---
ver: rpa2
title: Invisible Backdoor Attacks on Diffusion Models
arxiv_id: '2406.00816'
source_url: https://arxiv.org/abs/2406.00816
tags:
- diffusion
- image
- trigger
- backdoor
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an optimization framework to learn input-aware
  invisible triggers for backdoor attacks on diffusion models, applicable to both
  unconditional and conditional settings. Unlike prior methods using visible triggers,
  the proposed approach learns imperceptible perturbations via bi-level optimization,
  enhancing stealth and robustness.
---

# Invisible Backdoor Attacks on Diffusion Models

## Quick Facts
- arXiv ID: 2406.00816
- Source URL: https://arxiv.org/abs/2406.00816
- Reference count: 40
- One-line primary result: Optimization framework learns input-aware invisible triggers for backdoor attacks on diffusion models, achieving high utility and stealth across multiple datasets.

## Executive Summary
This paper introduces an optimization framework to learn input-aware invisible triggers for backdoor attacks on diffusion models, applicable to both unconditional and conditional settings. Unlike prior methods using visible triggers, the proposed approach learns imperceptible perturbations via bi-level optimization, enhancing stealth and robustness. For unconditional diffusion models, the framework learns a trigger distribution to generate target images from poisoned noise. For conditional models, it uses a neural network to create input-aware triggers in text-guided image editing/inpainting pipelines. Experiments show the backdoored models achieve high utility (similar FID to clean models) and high specificity (low MSE to target images) across datasets like CIFAR10, CELEBA-HQ, and MS COCO. The invisible triggers also enable model watermarking for ownership verification. The attack remains effective against various defenses, demonstrating its practical threat.

## Method Summary
The paper proposes a bi-level optimization framework for learning invisible backdoor triggers in diffusion models. The outer loop optimizes the diffusion model parameters to minimize the expected reconstruction loss on clean data while maximizing the likelihood of generating target images from poisoned data. The inner loop optimizes the trigger distribution or generator to minimize the reconstruction loss on poisoned data. For unconditional models, the trigger is a learned noise distribution added to the initial noise. For conditional models, a neural network generates input-aware triggers conditioned on text prompts. The framework uses DDIM sampling with a fixed number of steps (10 for CIFAR10, 3 for CELEBA-HQ, 5 for MS COCO) and ℓ∞ norm constraints on the triggers (0.04 to 0.4). The attack is stealthy as the triggers are imperceptible to human eyes and can be used for ownership verification.

## Key Results
- Backdoored diffusion models achieve high utility (FID ≈ 11.76 for CIFAR10) and specificity (MSE ≈ 3.07e-3) across poison rates (0.05 to 0.5).
- Invisible triggers are imperceptible and enable backdoor activation for both unconditional and conditional diffusion models.
- The attack remains effective against various defenses, demonstrating its practical threat.
- The framework can be used for model watermarking, providing a new application for backdoor attacks.

## Why This Works (Mechanism)
The bi-level optimization framework effectively learns imperceptible triggers by jointly optimizing the diffusion model and trigger distribution/generator. The inner loop ensures that the triggers are optimized to generate target images from poisoned data, while the outer loop ensures that the diffusion model can still generate high-quality images from clean data. The use of ℓ∞ norm constraints and imperceptible perturbations makes the triggers invisible to human eyes. The framework's flexibility allows it to be applied to both unconditional and conditional diffusion models, making it a versatile attack method. The attack's effectiveness against defenses demonstrates its practical threat in real-world scenarios.

## Foundational Learning
- **Diffusion Models**: Generative models that learn to denoise data step-by-step, needed to understand the attack target.
  - Quick check: Verify the diffusion model architecture and sampling process.
- **Bi-level Optimization**: A nested optimization framework where the outer problem depends on the solution of the inner problem, needed to optimize both the diffusion model and trigger distribution.
  - Quick check: Ensure the bi-level optimization implementation is correct and converges.
- **Trigger Optimization**: The process of learning imperceptible perturbations to activate the backdoor, needed to generate invisible triggers.
  - Quick check: Visualize the learned triggers and verify their imperceptibility.
- **ℓ∞ Norm Constraints**: Bounds on the maximum perturbation for each pixel, needed to ensure trigger invisibility.
  - Quick check: Monitor the ℓ∞ norm of the triggers during optimization.
- **DDIM Sampling**: A fast sampling method for diffusion models that uses a fixed number of denoising steps, needed for efficient trigger optimization.
  - Quick check: Verify the DDIM sampling implementation and its integration with the trigger optimization loop.
- **FID and MSE Metrics**: Quantitative measures of image quality and trigger specificity, needed to evaluate the attack's effectiveness.
  - Quick check: Compute FID and MSE on clean and poisoned data to assess utility and specificity.

## Architecture Onboarding
- **Component Map**: Diffusion model (unconditional/conditional) <- bi-level optimization <- trigger distribution/generator <- poisoned data
- **Critical Path**: Poisoned data -> trigger optimization (inner loop) -> diffusion model optimization (outer loop) -> backdoor activation
- **Design Tradeoffs**: Invisible triggers vs. attack effectiveness, model utility vs. trigger specificity, unconditional vs. conditional settings.
- **Failure Signatures**: Visible triggers (ℓ∞ norm too large), low utility (high FID), low specificity (high MSE), ineffective backdoor activation.
- **First Experiments**:
  1. Train backdoored diffusion model on CIFAR10 with poison rate 0.05, ℓ∞ norm 0.2, target 'Hat'.
  2. Evaluate FID and MSE on clean and poisoned data, target FID ≈ 11.76, MSE ≈ 3.07e-3.
  3. Generate samples with and without trigger, verify invisibility and backdoor activation.

## Open Questions the Paper Calls Out
None

## Limitations
- The trigger generator architecture for conditional settings is not fully specified, potentially impacting performance.
- DDIM sampling implementation details in the inner optimization loop are not fully specified, particularly regarding gradient computation.
- Exact replication is challenging without code or architectural details, relying on reproducing results from CIFAR10 with specific hyperparameters.

## Confidence
- High: The paper's main contributions and experimental results are well-described and supported by quantitative metrics.
- Medium: Some implementation details, such as the trigger generator architecture and DDIM sampling, are not fully specified, potentially affecting faithful reproduction.
- Low: The paper does not discuss potential limitations or failure modes in detail, which could impact real-world applicability.

## Next Checks
1. Implement and visualize the trigger generator architecture for the conditional case, ensuring it produces input-aware perturbations within the ℓ∞ norm constraints.
2. Verify the DDIM sampling implementation by checking gradient flow through sampling steps during trigger optimization.
3. Compare the bi-level optimization convergence behavior with the reported training curves to ensure proper trigger learning.