---
ver: rpa2
title: Large Language Models are Biased Because They Are Large Language Models
arxiv_id: '2406.13138'
source_url: https://arxiv.org/abs/2406.13138
tags:
- language
- bias
- llms
- large
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that harmful biases are an inevitable property
  of large language models (LLMs) due to their fundamental design. LLMs learn latent
  representations from vast amounts of human-generated text without distinguishing
  between normatively acceptable and unacceptable generalizations.
---

# Large Language Models are Biased Because They Are Large Language Models

## Quick Facts
- arXiv ID: 2406.13138
- Source URL: https://arxiv.org/abs/2406.13138
- Authors: Philip Resnik
- Reference count: 38
- Key outcome: Harmful biases are inevitable in LLMs due to their fundamental design and the hard-core interpretation of the distributional hypothesis

## Executive Summary
This paper argues that harmful biases are an inherent property of large language models because they learn latent representations from vast amounts of human-generated text without distinguishing between normatively acceptable and unacceptable generalizations. The author contends that current mitigation approaches like RLHF only substitute one set of biases for another without addressing the root problem. The solution proposed involves reconsidering the foundational assumptions of LLMs, particularly the hard-core interpretation of the distributional hypothesis, and incorporating more interpretable structure to distinguish between conventional meaning and contingent, context-dependent representations.

## Method Summary
The paper presents a conceptual analysis of why harmful biases are inevitable in LLMs, examining the fundamental design principles that lead to this outcome. It analyzes the hard-core interpretation of the distributional hypothesis and how it prevents models from distinguishing between legitimate linguistic meaning and harmful societal stereotypes. The paper also critiques existing bias mitigation approaches like RLHF, arguing they cannot overcome the underlying architectural limitations.

## Key Results
- LLMs inherently capture harmful stereotypes because they learn from all distributional patterns without normative discrimination
- RLHF and similar methods cannot eliminate harmful biases as they operate within the constraints of pre-trained representations
- The fundamental problem lies in the hard-core interpretation of the distributional hypothesis that equates meaning entirely with observable patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Harmful biases are inevitable in LLMs because they capture all distributional patterns in training data without distinguishing normatively acceptable from unacceptable generalizations.
- Mechanism: LLMs learn latent representations by optimizing cross-entropy loss to predict human-generated text. This forces the model to encode all statistical regularities—including harmful stereotypes—because it cannot differentiate between definitional meaning (normatively acceptable) and contingent societal biases (potentially harmful).
- Core assumption: The hard-core distributional hypothesis holds that linguistic meaning is entirely characterized by observable co-occurrence patterns.
- Evidence anchors:
  - [abstract] "LLMs learn latent representations from vast amounts of human-generated text without distinguishing between normatively acceptable and unacceptable generalizations"
  - [section 5] "LLMs, as they are currently constituted and trained, have no basis for distinguishing among these three distinct statements about nurses"
  - [corpus] Weak corpus evidence—most related papers focus on bias mitigation techniques rather than the foundational claim about inevitability
- Break condition: If models incorporate interpretable structure that explicitly distinguishes between conventional meaning and contingent, context-dependent representations (as proposed in section 7).

### Mechanism 2
- Claim: RLHF and other bias mitigation approaches cannot eliminate harmful biases because they operate within the constraints of pre-trained representations and face fundamental tradeoffs between safety and model utility.
- Mechanism: RLHF adjusts model parameters to align with human preferences but must maintain proximity to the original pre-trained model to preserve its powerful latent structure. This creates a tradeoff where pushing too hard against harmful biases risks losing the very capabilities that make LLMs useful.
- Core assumption: The pre-trained model's latent structure is so deeply interconnected that modifying it to remove specific biases inevitably affects other representations.
- Evidence anchors:
  - [section 6] "the formal definition of the optimization criterion also includes not straying too far from the original θ, so as not to disrupt the model's ability to do what it was trained for"
  - [section 6] "RLHF replaces one set of under-characterized biases...with another set of under-characterized biases, these inferred from the judgments and feedback of a different, far smaller number of people"
  - [corpus] Strong corpus evidence—multiple papers discuss limitations of RLHF and similar approaches
- Break condition: If new architectural approaches separate safety-critical reasoning from general language modeling, allowing independent optimization of bias mitigation without compromising utility.

### Mechanism 3
- Claim: The fundamental problem is the extreme interpretation of the distributional hypothesis that equates meaning entirely with observable patterns, ignoring the distinction between stable conventional meaning and context-dependent conveyed meaning.
- Mechanism: By treating all distributional patterns as equally valid sources of meaning, LLMs cannot distinguish between facts about word definitions (e.g., nurses are healthcare workers) and contingent societal stereotypes (e.g., nurses wear dresses). This conflation embeds harmful biases as inseparable from legitimate linguistic knowledge.
- Core assumption: Language has both conventional/stable meanings and context-dependent/conveyed meanings that should be treated differently in computational models.
- Evidence anchors:
  - [section 5] "LLMs have no way to distinguish the stuff that sucks from the stuff that doesn't"
  - [section 7] "re-thinking the very idea of bias mitigation, arguing instead for the alternative of creating LLM-like technologies where harmful bias-creating properties are not so baked into the models in the first place"
  - [corpus] Moderate corpus evidence—some papers discuss interpretability and structured approaches to meaning
- Break condition: If models successfully implement architectures that explicitly represent and reason about the distinction between conventional and conveyed meaning.

## Foundational Learning

- Concept: Distributional hypothesis and its hard-core vs. moderate interpretations
  - Why needed here: Understanding this distinction is crucial for grasping why LLMs cannot naturally distinguish harmful from benign patterns
  - Quick check question: What is the key difference between the hard-core interpretation (meaning = distribution) and the moderate interpretation (distribution is part of meaning)?

- Concept: Latent semantic analysis and representation learning
  - Why needed here: These techniques explain how LLMs capture nth-order co-occurrences and build abstract connections from indirect relationships
  - Quick check question: How does representation learning through dimensionality reduction enable LLMs to capture relationships between words that never co-occur directly?

- Concept: Bias as KL-divergence between prior and posterior distributions
  - Why needed here: This formalization helps understand how LLMs inherently contain biases as deviations from uniform priors
  - Quick check question: In the KL-divergence formulation of bias, what does it mean when the posterior distribution equals the prior distribution?

## Architecture Onboarding

- Component map: Text corpus → Embedding space construction → Parameter optimization → Bias embedding → Output generation
- Critical path: Pre-training phase (massive cross-entropy optimization on human text) → RLHF/fine-tuning phase (human feedback-based parameter adjustment) → Inference/usage phase (biased representations manifest in outputs)
- Design tradeoffs: Utility vs. safety (more bias mitigation reduces capabilities), scale vs. interpretability (larger models are more powerful but less transparent), generality vs. specificity (broad training captures more biases but enables more applications)
- Failure signatures: Overt bias in direct responses while covert bias persists in latent representations, bias amplification when mitigation techniques are applied, unexpected bias propagation across seemingly unrelated domains
- First 3 experiments:
  1. Test whether prompting an LLM with definitional vs. stereotypical statements about a concept (e.g., "nurses are healthcare workers" vs. "nurses wear dresses") produces different latent representations
  2. Apply RLHF to reduce bias in one domain and measure whether bias appears in unexpected domains, following Betley et al.'s methodology
  3. Compare representations of concepts with clear conventional meanings vs. those heavily influenced by social stereotypes to identify distinguishing features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RLHF or similar bias mitigation methods ever fully eliminate harmful biases in LLMs without compromising their core functionality?
- Basis in paper: [explicit] The paper argues that RLHF replaces one set of biases with another, but the underlying problematic structure remains intact.
- Why unresolved: The paper claims that the fundamental issue lies in the hard-core interpretation of the distributional hypothesis, which is baked into the model's architecture.
- What evidence would resolve it: Empirical studies demonstrating that RLHF or other methods can completely remove harmful biases without affecting the model's performance on standard benchmarks.

### Open Question 2
- Question: Is there a way to modify the foundational assumptions of LLMs to distinguish between normatively acceptable and unacceptable generalizations?
- Basis in paper: [explicit] The paper suggests that incorporating more interpretable structure to distinguish between conventional meaning and contingent, context-dependent representations could be a solution.
- Why unresolved: The paper does not provide a concrete method for achieving this distinction, only suggesting it as a potential avenue for future research.
- What evidence would resolve it: Development and validation of a new LLM architecture that successfully separates these types of generalizations and shows reduced harmful bias in downstream applications.

### Open Question 3
- Question: How can we empirically measure the real-world harm caused by LLM biases, beyond in vitro studies?
- Basis in paper: [explicit] The paper notes that most studies on LLM bias use in vitro methods and lack systematic, empirical studies on actual, real-world harms.
- Why unresolved: The paper highlights the difficulty in measuring real-world harm and suggests that more research is needed in this area.
- What evidence would resolve it: Longitudinal studies tracking the impact of LLM biases on real-world decision-making processes, such as hiring, lending, or criminal justice, and correlating these impacts with specific model behaviors.

## Limitations

- The claim that harmful biases are inevitable depends on accepting the hard-core interpretation of the distributional hypothesis, which is not universally accepted
- The proposed solution of incorporating interpretable structure remains largely conceptual without concrete implementation details
- The paper doesn't fully address whether different architectural approaches could break the claimed inevitability of bias

## Confidence

- **High Confidence**: The observation that LLMs capture all distributional patterns in training data, including harmful stereotypes, is well-established and empirically verified
- **Medium Confidence**: The claim that harmful biases are fundamentally inevitable due to LLM design principles is plausible but depends on accepting the hard-core distributional hypothesis
- **Medium Confidence**: The proposal to incorporate interpretable structure distinguishing conventional from conveyed meaning is conceptually sound but lacks concrete implementation details

## Next Checks

1. **Architectural Feasibility Test**: Implement a prototype model that explicitly represents the distinction between conventional meaning (stable definitions) and conveyed meaning (context-dependent usage) using modular architectures or structured representations. Measure whether this approach reduces harmful bias manifestations while preserving core capabilities.

2. **Distributional Hypothesis Stress Test**: Compare bias patterns in models trained with different interpretations of the distributional hypothesis—one using pure co-occurrence statistics versus one incorporating external knowledge bases or explicit semantic hierarchies. Determine whether bias inevitability holds across these different approaches.

3. **RLHF Enhancement Experiment**: Test whether combining RLHF with interpretability tools (such as activation steering or bias direction identification) can achieve meaningful bias reduction without the claimed utility tradeoffs. Follow Betley et al.'s methodology for measuring cross-domain bias propagation to assess whether mitigation in one area creates problems elsewhere.