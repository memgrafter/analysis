---
ver: rpa2
title: Generalising Battery Control in Net-Zero Buildings via Personalised Federated
  RL
arxiv_id: '2412.20946'
source_url: https://arxiv.org/abs/2412.20946
tags:
- energy
- learning
- buildings
- federated
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of optimal energy management in
  building-based microgrids using reinforcement learning (RL) and federated learning.
  The authors evaluate two common RL algorithms (PPO and TRPO) in different collaborative
  setups to manage distributed energy resources efficiently, with the goal of reducing
  energy costs and carbon emissions while ensuring privacy.
---

# Generalising Battery Control in Net-Zero Buildings via Personalised Federated RL

## Quick Facts
- arXiv ID: 2412.20946
- Source URL: https://arxiv.org/abs/2412.20946
- Reference count: 40
- This paper evaluates Federated TRPO for optimal energy management in building microgrids, achieving performance comparable to state-of-the-art methods without hyperparameter tuning.

## Executive Summary
This work addresses optimal energy management in building-based microgrids using reinforcement learning (RL) and federated learning. The authors evaluate Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO) in collaborative setups to manage distributed energy resources efficiently, aiming to reduce energy costs and carbon emissions while preserving privacy. Using a customized CityLearn environment with synthetic data, they simulate net-zero energy scenarios for microgrids composed of multiple buildings. Results show that Federated TRPO performs comparably to state-of-the-art federated RL methods without requiring hyperparameter tuning, demonstrating the feasibility of collaborative learning for achieving optimal control policies in energy systems.

## Method Summary
The authors use a customized CityLearn environment with synthetic microgrid data to train RL agents via federated learning. Each building runs a local agent with policy and value networks, optionally augmented with a personal encoding block for privacy-preserving personalization. Agents exchange model updates (not raw trajectories) using Federated Averaging (FedAvg), enabling collaborative learning while preserving data privacy. Two RL algorithms—PPO and TRPO—are evaluated, with TRPO's trust-region constraint providing stability in heterogeneous settings. The reward function balances energy cost and emissions, with penalties for infeasible actions. Experiments are conducted on 2- and 5-building environments, with and without personalization and feature grouping.

## Key Results
- Federated TRPO achieves performance comparable to state-of-the-art federated RL methods without hyperparameter tuning.
- Personalized federated learning with split-learning encoding reduces the generalization gap in heterogeneous building environments.
- The framework demonstrates the feasibility of collaborative RL for optimal control in energy systems, advancing sustainable smart grid goals.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Federated TRPO achieves near-optimal policies without hyperparameter tuning because its trust-region constraint stabilizes updates in heterogeneous microgrid settings.
- Mechanism: TRPO constrains policy updates to a Kullback-Leibler (KL) divergence bound, ensuring conservative changes that prevent destabilizing jumps in policy space—especially important when aggregating updates from buildings with varying solar, load, and pricing patterns.
- Core assumption: The trust-region constraint is sufficiently tight to prevent catastrophic policy shifts while still allowing meaningful learning progress.
- Evidence anchors:
  - [abstract] "Federated TRPO performs comparably to state-of-the-art federated RL methodologies without hyperparameter tuning."
  - [section] "TRPO explores the vast space by identifying suitable step sizes within a confined region, ensuring stable policy, monotonic improvement, and more effective exploration than other policy gradient methods."
- Break condition: If the KL bound is set too loosely, updates may become unstable; if too tight, learning slows or stalls.

### Mechanism 2
- Claim: Personalized federated learning with split-learning encoding improves generalization by allowing private features to remain local while sharing public feature updates.
- Mechanism: The "Personal Encoding" block captures household-specific attributes (e.g., number of occupants, appliance types) and produces a latent representation that augments shared features, enabling each agent to specialize without exposing sensitive data.
- Core assumption: Household-specific latent features are complementary to shared patterns and can be effectively combined without leakage.
- Evidence anchors:
  - [section] "This block represents a part of the network that remains local; the FedAvg process does not access it... This aligns with the concept of split-learning."
  - [abstract] "We included personalisation through split-learning... further shrinking the generalisation gap that often appears when controllers are deployed in heterogeneous conditions."
- Break condition: If the private encoding is too coarse, it may not capture useful heterogeneity; if too detailed, it may overfit.

### Mechanism 3
- Claim: Using synthetic, simplified microgrid data enables clearer policy interpretability and faster debugging of federated RL behaviors.
- Mechanism: By controlling solar/load profiles and pricing structures, the authors can create deterministic "optimal" policies (e.g., net-zero cost/emissions) against which learned policies can be directly compared, revealing suboptimality sources.
- Core assumption: Simplified profiles preserve essential decision-relevant dynamics without introducing confounding real-world noise.
- Evidence anchors:
  - [section] "We decided to generate synthetic data that allows us to understand clearly the learned policies through simple cases... focusing on representing homes that can be self-sustainable."
  - [section] "By design, our scenario leads to net-zero energy consumption."
- Break condition: If simplification removes critical nonlinearities, learned policies may not transfer to real systems.

## Foundational Learning

- Concept: Trust Region Policy Optimization (TRPO)
  - Why needed here: Ensures stable policy updates when aggregating across heterogeneous buildings; avoids the need for careful hyperparameter tuning.
  - Quick check question: Why does TRPO use a KL divergence constraint instead of a fixed learning rate?

- Concept: Federated Averaging (FedAvg) in RL
  - Why needed here: Allows agents to train locally on their own trajectories and share only model updates, preserving privacy while benefiting from diverse data.
  - Quick check question: What would happen if raw trajectories were shared instead of model updates?

- Concept: Split-learning personalization
  - Why needed here: Enables each building to encode private features locally, preventing privacy leakage while still contributing to a shared policy backbone.
  - Quick check question: How does the "personal encoding" block differ from a standard embedding layer?

## Architecture Onboarding

- Component map: Environment -> Agents (per building) -> Personal Encoding block (optional) -> Federated Trainer -> Updated model distributed back to agents

- Critical path:
  1. Each agent samples trajectories from its environment.
  2. Local networks (with optional personal encoding) process observations.
  3. Policy and value losses computed and gradients calculated.
  4. Gradients sent to federated aggregator.
  5. Federated averaging applied to update global model.
  6. Updated model distributed back to agents.

- Design tradeoffs:
  - Simplicity vs realism: Synthetic data simplifies debugging but may limit transfer.
  - Privacy vs performance: Split-learning protects privacy but may slightly reduce shared learning efficiency.
  - Fixed rounds vs adaptive: Fixed communication rounds simplify experimentation but may not be optimal for convergence.

- Failure signatures:
  - Reward plateaus but remains far from optimal → possible insufficient exploration or poor reward shaping.
  - High variance across seeds → instability in policy updates or insufficient communication rounds.
  - Degraded performance on shifted data → overfitting to training building profiles.

- First 3 experiments:
  1. Run PPO with default hyperparameters on 2-building environment; observe reward curves and compare to baseline.
  2. Enable split-learning personalization and re-run; check if variance across seeds decreases.
  3. Switch to TRPO (no tuning) and compare convergence speed and final reward to PPO baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Federated TRPO compare to other state-of-the-art federated RL methodologies in terms of convergence speed and final policy quality?
- Basis in paper: [explicit] The paper states that "Federated TRPO is comparable with state-of-the-art federated RL methodologies without hyperparameter tuning."
- Why unresolved: The paper does not provide a direct comparison with other state-of-the-art federated RL methods, such as FedAvg or FedProx, in terms of convergence speed or final policy quality.
- What evidence would resolve it: A detailed comparison study with other federated RL methods, including convergence speed metrics and policy quality assessments (e.g., energy cost, emissions, reward values), would resolve this question.

### Open Question 2
- Question: What is the impact of different personalization strategies (e.g., split-learning, grouping features) on the performance and generalization of federated RL agents in heterogeneous building environments?
- Basis in paper: [explicit] The paper mentions that "we included personalization through split-learning (Han et al., 2022), further shrinking the generalisation gap that often appears when controllers are deployed in heterogeneous conditions or occupancy patterns."
- Why unresolved: The paper does not provide a detailed analysis of the impact of different personalization strategies on performance and generalization, nor does it compare the effectiveness of split-learning versus other personalization techniques.
- What evidence would resolve it: An ablation study comparing the performance of federated RL agents with and without personalization strategies, and across different personalization techniques, would resolve this question.

### Open Question 3
- Question: How does the proposed federated RL framework scale to larger microgrids with dozens of buildings, and what are the challenges in terms of communication cost and convergence?
- Basis in paper: [inferred] The paper mentions that "we plan to scale experiments from five to dozens of buildings, testing how privacy, communication cost, and convergence behave in larger federations."
- Why unresolved: The paper does not provide experimental results on the scalability of the federated RL framework to larger microgrids, nor does it discuss the challenges and potential solutions for communication cost and convergence in such scenarios.
- What evidence would resolve it: Experimental results on the performance, communication cost, and convergence of the federated RL framework in larger microgrids with dozens of buildings would resolve this question.

## Limitations
- Relies on synthetic microgrid data with simplified solar and load profiles, which may not capture real-world nonlinearities and variability.
- The proposed framework's generalization to heterogeneous building stocks or different climate zones is not validated.
- Privacy benefits of split-learning are asserted but not rigorously quantified.

## Confidence

- **High confidence**: TRPO's stability in federated settings due to trust-region constraints; split-learning's theoretical privacy preservation.
- **Medium confidence**: Performance gains from personalization; synthetic data's sufficiency for debugging and interpretability.
- **Low confidence**: Transferability to real-world microgrids; privacy guarantees under active inference attacks.

## Next Checks
1. Test the framework on real-world building datasets to assess robustness to profile heterogeneity and noise.
2. Quantify information leakage in split-learning by simulating adversarial inference attacks on local features.
3. Perform an ablation study varying communication frequency and personalization strength to identify optimal federated training schedules.