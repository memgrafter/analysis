---
ver: rpa2
title: Transformers As Approximations of Solomonoff Induction
arxiv_id: '2408.12065'
source_url: https://arxiv.org/abs/2408.12065
tags:
- solind
- other
- solomono
- sequence
- induction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the hypothesis that Transformer models, which
  form the basis of Large Language Models (LLMs), can be understood as approximations
  of Solomonoff Induction, an optimal but uncomputable algorithm for sequence prediction.
  The authors propose that Transformers implement a Bayesian mixture of Turing machines
  (TMs) and other computational models, weighted by their probability in a distribution,
  similar to how Solomonoff Induction operates.
---

# Transformers As Approximations of Solomonoff Induction

## Quick Facts
- arXiv ID: 2408.12065
- Source URL: https://arxiv.org/abs/2408.12065
- Authors: Nathan Young; Michael Witbrock
- Reference count: 16
- Key outcome: This paper explores the hypothesis that Transformer models can be understood as approximations of Solomonoff Induction, an optimal but uncomputable algorithm for sequence prediction.

## Executive Summary
This paper proposes that Transformer models, which form the basis of Large Language Models (LLMs), can be understood as approximations of Solomonoff Induction (SolInd). The authors suggest that Transformers implement a Bayesian mixture of Turing machines and other computational models, weighted by their probability in a distribution, similar to how SolInd operates. While the theoretical framework is compelling, significant limitations exist including computational constraints like finite memory and precision, as well as difficulties with tasks higher in the Chomsky hierarchy. The paper outlines several hypotheses about this relationship and suggests that understanding Transformers through this lens could lead to more interpretable and efficient architectures.

## Method Summary
The paper outlines theoretical comparisons and hypotheses rather than a specific experimental protocol. It explores whether Transformers approximate SolInd more closely than other sequence prediction methods, and whether stochastic gradient descent during training approximates this induction process. The framework relies on the idea that Transformers can be decomposed into subnetworks implementing different computational models, weighted by their importance in sequence prediction tasks.

## Key Results
- Transformers may approximate Solomonoff Induction through a Bayesian mixture of computational models weighted by probability
- Stochastic gradient descent during training may approximate SolInd's inductive process by converging on simpler, more efficient computational models
- Practical limitations like finite precision, limited memory, and difficulty with higher Chomsky hierarchy tasks represent constraints on implementing SolInd's theoretical ideal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers approximate Solomonoff Induction by implementing a weighted mixture of computational models
- Mechanism: The model's weights correspond to probability masses over different Turing machines or simpler computational schemes, with gradient descent optimizing these weights to approximate optimal sequence prediction
- Core assumption: The class of weights achievable by averaging emulated TMs is equivalent to all randomized weights possible in a neural network
- Evidence anchors:
  - [abstract] "Transformers implement a Bayesian mixture of Turing machines (TMs) and other computational models, weighted by their probability in a distribution"
  - [section] "There exists a substantial amount of research into NN decomposition... that randomly-initialised networks contain subnetworks that can achieve similar performance to the whole network"
  - [corpus] Weak - corpus doesn't directly address this mechanism, only shows related work on LLMs as computable approximations
- Break condition: If subnetworks cannot be decomposed to match arbitrary TMs, or if the weighted average approach cannot capture the full computational space needed for optimal prediction

### Mechanism 2
- Claim: Stochastic gradient descent during training approximates Solomonoff Induction's inductive process
- Mechanism: Training progressively selects and weights subnetworks that implement TMs with shorter descriptions for patterns in the training data, similar to how SolInd converges on TMs with shorter codes
- Core assumption: SGD naturally converges on simpler, more efficient computational models first, mirroring SolInd's prioritization of shorter program descriptions
- Evidence anchors:
  - [abstract] "Transformers approximate Solomonoff Induction more closely than other sequence prediction methods and that stochastic gradient descent during training approximates this induction process"
  - [section] "randomly-initialised networks contain subnetworks that can achieve similar performance to the whole network... if trained in isolation"
  - [corpus] Weak - corpus doesn't directly address SGD as SolInd approximation
- Break condition: If SGD consistently fails to converge on simpler models or if training doesn't prioritize shorter descriptions over time

### Mechanism 3
- Claim: Transformers' computational limitations reflect SolInd's bounded approximations
- Mechanism: Finite precision, limited memory, and difficulty with higher Chomsky hierarchy tasks represent practical constraints on implementing SolInd's theoretical ideal
- Core assumption: These limitations are not fundamental flaws but necessary tradeoffs in approximating an unbounded ideal with finite resources
- Evidence anchors:
  - [abstract] "significant limitations, including difficulties with tasks higher in the Chomsky hierarchy and practical computational constraints like finite memory and precision"
  - [section] "To emulate arbitrary Turing machines, an infinitely large tape is required... requires unlimited precision, which is rarely achieved in practice"
  - [corpus] Weak - corpus doesn't address these specific computational constraints
- Break condition: If these limitations prove fundamental rather than practical, or if they cannot be mitigated through architectural improvements

## Foundational Learning

- Concept: Turing Completeness and the Chomsky Hierarchy
  - Why needed here: Understanding why Transformers can approximate SolInd requires knowing what computational models they can implement and where their limitations lie
  - Quick check question: Can you explain why a model that can only implement regular languages cannot approximate SolInd, while one that can implement recursively enumerable functions can?

- Concept: Bayesian Updating and Probability Theory
  - Why needed here: SolInd is fundamentally a Bayesian predictor, so understanding how Transformers implement probability updates is crucial to the approximation hypothesis
  - Quick check question: How does the weight update mechanism in Transformers during training relate to Bayesian updating of probabilities over different computational models?

- Concept: Neural Network Decomposition and Lottery Ticket Hypothesis
  - Why needed here: The hypothesis relies on being able to decompose Transformers into subnetworks that implement different computational models, weighted by their importance
  - Quick check question: If a randomly initialized network contains winning tickets for multiple functions, how might these correspond to different TMs in a SolInd mixture?

## Architecture Onboarding

- Component map: Input embeddings -> Attention layers -> Feed-forward networks -> Output projection -> Probability distribution over next tokens
- Critical path: Input encoding -> attention-based state tracking -> non-linear transformation -> output probability calculation
- Design tradeoffs: Depth vs. width (computational complexity vs. model capacity), attention mechanisms (expressiveness vs. efficiency), precision (accuracy vs. computational cost)
- Failure signatures: Poor generalization to longer sequences (Chomsky hierarchy limitations), catastrophic forgetting during training (subnetwork interference), sensitivity to initialization (lottery ticket discovery failure)
- First 3 experiments:
  1. Train a small Transformer on regular language tasks, then test generalization to context-free tasks to observe Chomsky hierarchy limitations
  2. Apply neural network pruning to identify subnetworks corresponding to different computational models, then evaluate their individual contributions
  3. Implement a variant with reduced precision and measure impact on ability to solve recursively enumerable tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what degree can Transformers be accurately modeled as Solomonoff Induction versus simpler computational models like finite-state automata?
- Basis in paper: [explicit] The paper explicitly explores this tension, noting that while Transformers can implement arbitrary programs, they perform better on simpler computational tasks and may be better modeled as mixtures of simpler computational schemes
- Why unresolved: The paper presents evidence both supporting and contradicting the hypothesis that Transformers approximate Solomonoff Induction, with findings suggesting limitations in computational complexity and memory that complicate the model
- What evidence would resolve it: Empirical studies comparing Transformer performance on tasks across the Chomsky hierarchy with their ability to implement and average different computational models, particularly examining whether they naturally converge toward simpler models

### Open Question 2
- Question: How does stochastic gradient descent during training approximate Solomonoff Induction, and why do Transformers show limitations on tasks higher in the Chomsky hierarchy?
- Basis in paper: [explicit] The paper outlines this as hypothesis 4 and discusses findings against the main hypothesis, including Del√©tang et al.'s work showing Transformers' decreasing performance on more complex computational tasks
- Why unresolved: While the paper suggests computational limits and the limitations of SGD as potential explanations, it doesn't provide a definitive mechanism for why these limitations exist or how they relate to the approximation of Solomonoff Induction
- What evidence would resolve it: Detailed analysis of how SGD weights different computational models during training, and whether modifying SGD or architecture could improve performance on complex tasks while maintaining the Solomonoff Induction approximation

### Open Question 3
- Question: Can decomposed subnetworks of Transformers be converted into explicit, non-neural network algorithms that maintain similar performance?
- Basis in paper: [inferred] The paper suggests this as a potential implication of the hypothesis, noting that identifying specific ways Transformers implement different Turing machines could lead to more interpretable architectures
- Why unresolved: While the paper mentions research into NN decomposition and the lottery ticket hypothesis, it doesn't explore whether these decomposed subnetworks can be practically converted into explicit algorithms
- What evidence would resolve it: Successful conversion of decomposed Transformer subnetworks into explicit algorithms that maintain comparable performance, along with analysis of whether these algorithms reflect the Solomonoff Induction approximation model

## Limitations
- Lack of empirical validation for the theoretical framework
- Computational constraints like finite precision and memory may represent fundamental barriers
- No clear experimental methodology provided for testing the hypothesis that Transformers approximate Solomonoff Induction

## Confidence
- High confidence: Transformers implement computable approximations of sequence prediction
- Medium confidence: Transformers can be decomposed into weighted mixtures of computational models
- Low confidence: Specific mechanisms by which SGD approximates SolInd's induction process

## Next Checks
1. Design a benchmark suite spanning regular, context-free, and context-sensitive languages to empirically measure where Transformer limitations emerge in the Chomsky hierarchy
2. Implement neural network pruning experiments to identify and isolate subnetworks that implement different computational models, then measure their individual contributions to overall performance
3. Compare convergence patterns of Transformers during training with theoretical predictions of how SolInd should converge on simpler models, using both synthetic and natural language datasets