---
ver: rpa2
title: Reliable Evaluation and Benchmarks for Statement Autoformalization
arxiv_id: '2406.07222'
source_url: https://arxiv.org/abs/2406.07222
tags:
- filter
- accuracy
- lean
- statements
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating and benchmarking
  statement autoformalization, the task of translating natural language mathematics
  into formal languages like Lean 4. The authors introduce BEq+, an improved evaluation
  metric, and ProofNetVerif, a new dataset with 3,752 annotated examples.
---

# Reliable Evaluation and Benchmarks for Statement Autoformalization

## Quick Facts
- arXiv ID: 2406.07222
- Source URL: https://arxiv.org/abs/2406.07222
- Authors: Auguste Poiroux; Gail Weiss; Viktor KunÄak; Antoine Bosselut
- Reference count: 17
- Primary result: BEq+ metric improves autoformalization evaluation with 45.1% accuracy on undergraduate mathematics

## Executive Summary
This paper addresses critical challenges in evaluating and benchmarking statement autoformalization - the task of translating natural language mathematics into formal languages like Lean 4. The authors identify systematic errors in existing evaluation metrics and datasets, then introduce BEq+, an improved evaluation metric, along with ProofNetVerif, a corrected dataset with 3,752 annotated examples. They develop two new benchmarks: ProofNet#, a corrected version of ProofNet, and RLM25, containing 619 pairs of research-level mathematics from six formalization projects. Through systematic experimentation, they demonstrate that current autoformalization techniques achieve up to 45.1% accuracy on undergraduate mathematics but struggle significantly with research-level content without proper context.

## Method Summary
The authors systematically address the evaluation problem by first analyzing existing datasets and metrics to identify sources of error. They introduce BEq+, which improves upon previous metrics by better handling structural variations in formal proofs. To create reliable benchmarks, they develop ProofNetVerif through careful annotation of undergraduate mathematics problems, correcting errors in the original ProofNet dataset. They also construct RLM25 by extracting and formalizing research-level mathematical statements from six active formalization projects. The evaluation framework tests various autoformalization approaches across both undergraduate and research-level mathematics, measuring performance with and without contextual information.

## Key Results
- BEq+ metric achieves more reliable evaluation than previous methods, reducing false negatives in autoformalization assessment
- Current techniques reach 45.1% accuracy on undergraduate mathematics but only 38.5% on research-level problems
- Context significantly improves performance on complex mathematical statements, particularly at research level
- ProofNetVerif provides 3,752 high-quality annotated examples for training and evaluation
- RLM25 establishes a new benchmark for research-level mathematics autoformalization with 619 pairs

## Why This Works (Mechanism)
The approach works by addressing fundamental flaws in how autoformalization systems are evaluated. BEq+ captures semantic equivalence more effectively by considering the mathematical meaning rather than just syntactic similarity. The improved datasets reduce noise from incorrect annotations that previously confounded results. By providing both undergraduate and research-level benchmarks, the framework enables systematic comparison across difficulty levels. The inclusion of context in evaluation better reflects real-world formalization scenarios where mathematical statements depend on definitions and theorems from surrounding text.

## Foundational Learning

**Lean 4 syntax and semantics**: Why needed - the primary formal language used in benchmarks; Quick check - can you parse a simple Lean theorem statement?

**BEq+ metric calculation**: Why needed - the core evaluation mechanism; Quick check - can you compute BEq+ score for two semantically equivalent but syntactically different formal statements?

**ProofNet dataset structure**: Why needed - understanding the baseline being corrected; Quick check - can you identify the components of a ProofNet entry?

**Mathematical formalization principles**: Why needed - to understand what makes autoformalization challenging; Quick check - can you explain why "there exists" statements are harder to formalize than "for all" statements?

## Architecture Onboarding

**Component map**: BEq+ metric -> ProofNetVerif dataset -> RLM25 benchmark -> Evaluation pipeline -> Performance analysis

**Critical path**: Natural language statement -> Formalization model -> BEq+ evaluation -> Accuracy calculation -> Benchmark comparison

**Design tradeoffs**: Precision vs. recall in metric design - favoring precision to avoid false positives; Dataset size vs. annotation quality - prioritizing quality for reliable evaluation; Undergraduate vs. research-level focus - providing both to measure progress across domains

**Failure signatures**: False negatives in BEq+ indicate semantic equivalence not captured by syntactic comparison; Low research-level performance reveals limitations in handling complex mathematical structures; Context dependence shows models struggle with implicit mathematical knowledge

**Three first experiments**:
1. Run baseline autoformalization model on ProofNet# and compare BEq+ scores with original ProofNet
2. Test same model with and without context on RLM25 to quantify context importance
3. Evaluate BEq+ on a small set of Coq problems to assess cross-system generalizability

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the generalizability of BEq+ beyond Lean 4 and undergraduate mathematics, the scalability of current approaches to handle advanced mathematical concepts and complex proof structures, and the transferability of evaluation metrics to other formal proof assistants like Coq or Isabelle.

## Limitations
- Generalizability concerns for BEq+ beyond Lean 4 and undergraduate mathematics domain
- Performance gap between undergraduate (45.1%) and research-level (38.5%) mathematics suggests limitations with advanced concepts
- Single formal system focus (Lean 4) raises questions about metric applicability to other proof assistants

## Confidence
- High confidence in dataset quality and basic metric improvements demonstrated on ProofNetVerif
- Medium confidence in RLM25 benchmark claims due to small sample size (619 pairs)
- Medium confidence in generalizability findings beyond tested formal system and mathematical domains

## Next Checks
1. Evaluate BEq+ on at least two additional formal proof systems (e.g., Coq, Isabelle) using the same underlying natural language statements to test metric generalizability
2. Expand RLM25 to at least 2,000 research-level pairs across more mathematical domains (topology, category theory, etc.) to better assess performance boundaries
3. Conduct ablation studies removing various context elements to quantify their impact on autoformalization accuracy, particularly for research-level problems