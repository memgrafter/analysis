---
ver: rpa2
title: Rethinking the Capacity of Graph Neural Networks for Branching Strategy
arxiv_id: '2402.07099'
source_url: https://arxiv.org/abs/2402.07099
tags:
- theorem
- graph
- branching
- learning
- gnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the expressive power of graph neural networks
  (GNNs) for representing strong branching (SB) scores in mixed-integer linear programming
  (MILP). While message-passing GNNs (MP-GNNs) are commonly used to approximate SB,
  the authors show through a counterexample that MP-GNNs cannot distinguish between
  two MILPs with different SB scores.
---

# Rethinking the Capacity of Graph Neural Networks for Branching Strategy

## Quick Facts
- **arXiv ID:** 2402.07099
- **Source URL:** https://arxiv.org/abs/2402.07099
- **Reference count:** 40
- **Key outcome:** Message-passing GNNs cannot distinguish MILPs with different strong branching scores, while second-order folklore GNNs can universally approximate these scores.

## Executive Summary
This paper investigates the expressive power of graph neural networks for approximating strong branching scores in mixed-integer linear programming. The authors prove that message-passing GNNs (MP-GNNs) cannot distinguish between two specific MILPs with different strong branching scores due to their limited expressive power. They then establish a universal approximation theorem showing that second-order folklore GNNs (2-FGNNs) can approximate strong branching scores for any MILP distribution with arbitrarily high accuracy. The theoretical results are validated with small-scale experiments demonstrating that 2-FGNNs can fit the strong branching scores of the counterexample MILPs while MP-GNNs cannot.

## Method Summary
The paper analyzes two GNN architectures: MP-GNNs operating on individual nodes and their immediate neighborhoods, and 2-FGNNs operating on pairs of nodes and their joint neighborhoods. The theoretical analysis proves that MP-GNNs cannot distinguish certain non-isomorphic MILP graphs with different strong branching scores, while 2-FGNNs can universally approximate strong branching scores for any MILP distribution. The authors validate these findings with numerical experiments using two specific MILP instances, training both architectures to minimize mean squared error between predicted and actual strong branching scores.

## Key Results
- MP-GNNs cannot distinguish two MILP instances with different strong branching scores, as demonstrated by a mathematical counterexample.
- 2-FGNNs can universally approximate strong branching scores for any MILP distribution with arbitrarily high accuracy.
- Small-scale experiments show 2-FGNNs achieve near-zero error on the counterexample MILPs while MP-GNNs fail to converge.

## Why This Works (Mechanism)

### Mechanism 1
MP-GNNs operate on individual nodes and their immediate neighborhoods. The counterexample shows two MILPs with different SB scores produce identical MP-GNN outputs because their node features and aggregated messages are indistinguishable at every layer. The SB score depends on deeper structural properties that require tracking relationships beyond immediate node neighborhoods.

### Mechanism 2
2-FGNNs operate on pairs of nodes and their joint neighborhoods, capturing relationships that MP-GNNs miss. The universal approximation theorem guarantees that for any MILP distribution, there exists a 2-FGNN that can approximate SB scores with arbitrary precision. The SB score computation can be expressed as a continuous function on the space of MILP graphs that respects the symmetry structure of the problem.

### Mechanism 3
The separation power of 2-FGNNs is equivalent to the 2-FWL test, which is stronger than SB score distinguishability. The proof establishes that if two MILPs have different SB scores, the 2-FWL test will distinguish them, and 2-FGNNs have the same separation power as 2-FWL. This creates a chain: different SB scores → distinguishable by 2-FWL → distinguishable by 2-FGNNs → can be approximated.

## Foundational Learning

- **Graph Neural Networks and their expressive power**: Understanding why MP-GNNs fail while 2-FGNNs succeed requires grasping the fundamental limitations of message-passing architectures. *Quick check:* What structural information about MILPs is accessible to MP-GNNs but not to WL tests?

- **Strong Branching and its computational complexity**: The motivation for using GNNs to approximate SB scores depends on understanding why SB is computationally expensive and what information it captures. *Quick check:* Why does SB require solving multiple LPs for each variable, and how does this relate to the MILP graph structure?

- **Universal approximation theorems and Stone-Weierstrass theorem**: The theoretical guarantee for 2-FGNNs relies on showing they can separate points and then applying a generalized Stone-Weierstrass theorem. *Quick check:* What conditions must a function class satisfy to universally approximate continuous functions on a compact space?

## Architecture Onboarding

- **Component map:** MILP graphs → 2-FGNN with pair-wise node features → output SB scores per variable. Key difference from MP-GNN: initial features are defined on node pairs (i,j) and (j1,j2) rather than single nodes.

- **Critical path:** Graph representation → pair-wise feature initialization → L layers of pair-wise message passing → final aggregation to node scores → SB score output

- **Design tradeoffs:** 2-FGNNs have higher computational complexity than MP-GNNs due to pair-wise operations, but gain expressive power. The tradeoff is between efficiency and approximation capability.

- **Failure signatures:** If training fails to converge, check whether the MILP distribution satisfies Assumption 3.1 (SB scores must be defined). If performance is poor on specific instances, those instances might be similar to the counterexample where MP-GNNs fail.

- **First 3 experiments:**
  1. Verify the counterexample: Train an MP-GNN on the two MILPs from section 3.1 and confirm it cannot fit both SB scores simultaneously.
  2. Test approximation quality: Generate random MILPs, compute exact SB scores, train a 2-FGNN, and measure approximation error.
  3. Ablation study: Train 2-FGNNs with varying numbers of layers and hidden dimensions to understand the relationship between model complexity and approximation accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
What is the required complexity/number of parameters in 2-FGNNs to achieve a given precision for approximating strong branching scores? The paper focuses on theoretical capabilities and small-scale numerical experiments, leaving the quantification of parameter requirements for a given precision as an unresolved question.

### Open Question 2
How do 2-FGNNs compare to other GNN architectures (e.g., higher-order GNNs) in terms of expressive power and efficiency for approximating strong branching scores? The paper only analyzes MP-GNNs and 2-FGNNs, leaving the performance of other GNN variants unexplored for this specific task.

### Open Question 3
Can the universal approximation theorem for 2-FGNNs be extended to other MILP-related learning tasks beyond strong branching score prediction? The paper focuses specifically on the expressive power of GNNs for strong branching scores, without exploring their applicability to other MILP-related tasks.

## Limitations
- Theoretical results rely on specific assumptions about MILP graph structures and continuous SB scores that may not hold universally.
- Empirical validation is limited to only two MILP instances, providing limited evidence of real-world applicability.
- Computational complexity of 2-FGNNs compared to MP-GNNs is not thoroughly analyzed, leaving open questions about practical scalability.

## Confidence

- **Universal approximation theorem for 2-FGNNs:** High confidence
- **Counterexample demonstrating MP-GNN limitations:** High confidence
- **Empirical validation with two MILP instances:** Medium confidence

## Next Checks

1. Test 2-FGNNs on larger, more diverse MILP datasets to verify generalization beyond the theoretical counterexample.

2. Compare computational efficiency of 2-FGNNs versus MP-GNNs across varying MILP sizes to quantify the expressiveness-efficiency tradeoff.

3. Investigate whether alternative GNN architectures (beyond MP-GNNs and 2-FGNNs) might offer intermediate expressiveness with better computational properties.