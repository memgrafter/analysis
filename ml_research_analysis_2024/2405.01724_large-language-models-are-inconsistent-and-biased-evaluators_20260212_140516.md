---
ver: rpa2
title: Large Language Models are Inconsistent and Biased Evaluators
arxiv_id: '2405.01724'
source_url: https://arxiv.org/abs/2405.01724
tags:
- score
- evaluation
- scores
- evaluators
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models (LLMs) exhibit significant bias and inconsistency
  when used as automatic evaluators for text summarization. The paper identifies three
  key biases: familiarity bias (preference for lower-perplexity text), score bias
  (preference for round numbers), and anchoring effects (multi-attribute judgments
  influenced by prior scores).'
---

# Large Language Models are Inconsistent and Biased Evaluators

## Quick Facts
- **arXiv ID**: 2405.01724
- **Source URL**: https://arxiv.org/abs/2405.01724
- **Reference count**: 40
- **Key result**: Proposed LLM evaluation recipe achieves Kendall's τ correlation of 0.220 on CNNDM and 0.308 on SAMSum, outperforming state-of-the-art methods

## Executive Summary
Large language models exhibit significant biases and inconsistencies when used as automatic evaluators for text summarization tasks. The paper identifies three key biases: familiarity bias (preference for lower-perplexity text), score bias (preference for round numbers), and anchoring effects (where earlier scores influence subsequent ones in multi-attribute judgments). Through systematic experimentation, the authors demonstrate these biases can be mitigated through specific prompt engineering techniques including 1-10 scoring granularity, temperature=0 settings, and single-attribute predictions, achieving substantially improved consistency and correlation with human judgments.

## Method Summary
The study evaluates LLM-based summarization using SummEval and RoSE datasets with GPT-3.5 and GPT-4 models. The authors systematically test various prompt configurations including different temperature settings, scoring granularities (1-5, 1-10, 1-100), and single vs. multi-attribute predictions. They measure performance using Kendall's τ correlation with human judgments and Krippendorff's α for inter-sample agreement. The analysis includes perplexity correlation studies, score distribution analysis, and conditional probability modeling to identify and quantify specific biases in LLM evaluation behavior.

## Key Results
- LLM evaluators show familiarity bias, preferring summaries with lower perplexity
- Score bias manifests as round number preference and underutilization of full score ranges
- Anchoring effects occur in multi-attribute judgments, with earlier scores influencing later ones
- Proposed mitigation recipe (1-10 granularity, temperature=0, single-attribute) achieves 0.220 Kendall's τ on CNNDM and 0.308 on SAMSum
- Temperature=0 with single-attribute predictions significantly reduces variance while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
LLM evaluators exhibit familiarity bias by preferring text with lower perplexity, mistaking familiarity for quality. Perplexity serves as a proxy for how "familiar" or "common" the text appears to the model. When evaluating summaries, GPT-4 assigns higher scores to summaries with lower perplexity, even though human evaluators do not show this pattern. This creates a systematic bias where fluent but potentially less informative summaries are overvalued.

### Mechanism 2
LLM evaluators show anchoring effects when making multi-attribute judgments, where earlier scores influence subsequent ones. Auto-regressive generation in LLMs means each output token is conditioned on previous tokens. When scoring multiple attributes in sequence, the first score creates a cognitive anchor that biases subsequent scores. The model over-adjusts from this anchor rather than evaluating each attribute independently.

### Mechanism 3
LLM evaluators show score bias with round number preference and underutilization of full score ranges. The model's token prediction probabilities are skewed toward round numbers (multiples of 5 or 10), leading to non-uniform score distributions. This creates artificial clustering of scores and reduces granularity. The model also fails to utilize the full range of available scores, similar to human grading patterns.

## Foundational Learning

- **Concept: Perplexity as a measure of text familiarity**
  - Why needed here: Understanding how perplexity relates to LLM evaluation bias is crucial for diagnosing why models prefer certain types of text
  - Quick check question: If a summary has unusually low perplexity but contains generic content, should it receive a high evaluation score?

- **Concept: Auto-regressive generation and conditioning**
  - Why needed here: Explains why sequential scoring in a single generation creates dependencies between attributes
  - Quick check question: Why would generating scores for "Coherence" and then "Relevance" in one prompt create different results than generating them separately?

- **Concept: Temperature scaling in LLM generation**
  - Why needed here: Temperature affects the diversity and quality of outputs, particularly for Chain-of-Thought prompting
  - Quick check question: Why might higher temperature improve Chain-of-Thought performance while harming simple prediction tasks?

## Architecture Onboarding

- **Component map**: Article text, summary text, prompt template -> LLM interface (OpenAI API) -> Scoring engine -> Evaluation metrics -> Analysis module

- **Critical path**:
  1. Prepare article and summary inputs
  2. Construct evaluation prompt with appropriate scoring granularity
  3. Generate LLM evaluation with configured temperature and CoT settings
  4. Parse numerical score from LLM output
  5. Calculate correlation with human judgments
  6. Analyze for bias patterns (perplexity correlation, score distribution, anchoring)

- **Design tradeoffs**:
  - Single vs. multiple attribute prediction: Multiple attributes are cheaper but suffer from anchoring effects
  - Temperature setting: Higher temperature increases diversity but may reduce consistency
  - Scoring granularity: Finer granularity allows better discrimination but may introduce score bias
  - CoT prompting: Can improve reasoning but requires temperature tuning and is more expensive

- **Failure signatures**:
  - High correlation between perplexity and scores indicates familiarity bias
  - Skewed score distributions with peaks at round numbers indicates score bias
  - High correlation between sequential attribute scores indicates anchoring effect
  - Low inter-sample agreement indicates inconsistency

- **First 3 experiments**:
  1. Compare perplexity of summaries across different score bins to detect familiarity bias
  2. Test different scoring granularities (1-5 vs 1-10 vs 1-100) to measure score bias and performance impact
  3. Generate multi-attribute scores in different orders to measure anchoring effects

## Open Questions the Paper Calls Out

### Open Question 1
Does the anchoring effect observed in multi-attribute judgments extend to other natural language processing tasks beyond text summarization? The study focuses specifically on text summarization tasks. It remains unclear whether this bias manifests similarly in other domains such as machine translation, question answering, or dialogue systems.

### Open Question 2
What is the optimal balance between scoring granularity and resistance to score bias in LLM evaluators? While the paper demonstrates that 1-10 scoring granularity performs best on average, it does not identify the precise point of diminishing returns or optimal granularity for different tasks and evaluation contexts.

### Open Question 3
How do different LLM architectures (beyond GPT models) compare in terms of bias and consistency when used as evaluators? The analysis primarily uses GPT-based models due to their superior performance, but acknowledges uncertainty about generalization to other architectures.

## Limitations

- Findings are limited to text summarization evaluation and may not generalize to other NLP tasks or domains
- The study relies on indirect perplexity measurements using a different model (text-davinci-003) rather than the evaluation models themselves
- The proposed mitigation recipe combines multiple independent parameters without isolating individual contributions

## Confidence

**High Confidence**: The existence of score bias (round number preference) and the effectiveness of 1-10 scoring granularity are well-supported by multiple experiments and clear statistical evidence. The anchoring effect demonstration through conditional probability analysis provides robust evidence for sequential score dependencies.

**Medium Confidence**: The familiarity bias mechanism is reasonably supported through perplexity correlation analysis, but the indirect measurement approach and potential confounds in the perplexity proxy reduce confidence. The temperature=0 recommendation for reducing variance is supported but could benefit from more extensive temperature sweep experiments.

**Low Confidence**: The generalizability of these specific biases and mitigation strategies to other evaluation domains, model architectures, or evaluation tasks beyond text summarization. The paper focuses exclusively on summarization evaluation, limiting broader applicability claims.

## Next Checks

1. **Cross-domain validation**: Apply the identified bias detection methods and mitigation recipe to at least two different evaluation domains (e.g., code generation and visual question answering) to test generalizability of findings.

2. **Component isolation experiment**: Systematically vary each component of the proposed recipe (temperature, granularity, single vs. multi-attribute) independently while holding others constant to quantify individual contribution effects.

3. **Perplexity validation study**: Directly measure the relationship between perplexity and evaluation scores using the same model (GPT-4) for both calculations, rather than using text-davinci-003 as a proxy, to validate the familiarity bias mechanism.