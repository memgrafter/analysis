---
ver: rpa2
title: Fantastic Biases (What are They) and Where to Find Them
arxiv_id: '2411.15051'
source_url: https://arxiv.org/abs/2411.15051
tags:
- bias
- biases
- data
- they
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the nature and impact of biases in deep learning
  and artificial intelligence systems, focusing on how these biases arise from data
  and model structures. It defines bias in general terms, distinguishing between useful
  structural biases and harmful social biases, and presents a taxonomy of common biases
  in machine learning, including reporting, automation, selection, representation,
  group attribution, and implicit biases.
---

# Fantastic Biases (What are They) and Where to Find Them

## Quick Facts
- arXiv ID: 2411.15051
- Source URL: https://arxiv.org/abs/2411.15051
- Reference count: 15
- One-line primary result: Biases in AI systems arise from data and model structures, requiring detection and mitigation strategies for fairer outcomes.

## Executive Summary
This paper examines how biases manifest in deep learning and artificial intelligence systems, distinguishing between useful structural biases and harmful social biases. The work presents a comprehensive taxonomy of biases including reporting, automation, selection, representation, group attribution, and implicit biases, with particular focus on natural language processing applications. The paper emphasizes that AI models learn correlations from skewed data distributions, leading to spurious patterns that can disadvantage certain groups, and outlines methods for detecting and mitigating these biases to ensure fairer AI systems that serve all populations equitably.

## Method Summary
The paper employs a conceptual framework for understanding AI biases through analysis of data distributions, model architectures, and evaluation methodologies. Methods include data analysis for demographic representation, counterfactual testing by modifying protected attributes, and performance evaluation across demographic subgroups. Mitigation strategies encompass oversampling, adversarial training with fairness constraints, data augmentation, and incorporating diverse human perspectives. The approach emphasizes continuous monitoring and adaptation to address evolving biases as language and social norms change over time.

## Key Results
- AI models learn harmful biases from data imbalances rather than intentional design, encoding societal prejudices
- Multiple bias types exist including selection bias from Western-centered internet data, temporal bias from outdated language, and confounding variable bias
- Mitigation requires a combination of data-level interventions (oversampling, augmentation) and model-level approaches (adversarial training, fairness constraints)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Biases in AI arise from skewed data distributions rather than intentional design.
- Mechanism: When training data overrepresents certain groups or patterns, the model learns spurious correlations that reflect those imbalances rather than causal relationships.
- Core assumption: Real-world data inherently contains societal biases that get encoded into models.
- Evidence anchors:
  - [abstract] "Deep Learning models tend to learn correlations of patterns on huge datasets... they can help to reach the most disadvantaged areas. However, such a universal systems must be able to represent society, without benefiting some at the expense of others."
  - [section] "In natural language processing (NLP), biases are everywhere, starting by the data... the main biases comes from a Selection Biases. The main part of the internet data is Western-centered."
  - [corpus] Found related papers on bias detection and mitigation, suggesting this is a recognized mechanism in the field.
- Break condition: If data is perfectly balanced and representative of all groups, this mechanism would not apply.

### Mechanism 2
- Claim: Model architecture and feature selection can embed biases through confounding variables.
- Mechanism: When features correlated with protected attributes are used for prediction, the model learns to use these as proxies for the protected attributes, perpetuating discrimination.
- Core assumption: Feature selection can introduce bias even when protected attributes are not explicitly used.
- Evidence anchors:
  - [abstract] "We have seen cases where these systems use gender, race, or even class information in ways that are not appropriate for resolving their tasks."
  - [section] "In old-school machine learning algorithms, selection bias at the feature level can be a problem. For instance, prioritizing features like income level and neighborhood in a loan approval model can inadvertently embed socioeconomic biases."
  - [corpus] Weak evidence - corpus contains related papers but none directly addressing feature selection bias mechanism.
- Break condition: If all confounding variables are removed and only causally relevant features are used.

### Mechanism 3
- Claim: Temporal shifts in language and social norms create temporal biases in models.
- Mechanism: Models trained on historical data encode outdated meanings and associations that no longer reflect current usage, leading to inappropriate outputs.
- Core assumption: Language evolves over time and models need to account for these changes.
- Evidence anchors:
  - [abstract] "The promises it holds strongly depend on their fair and universal use, such as access to information or education for all."
  - [section] "Temporal biases... the semantic meanings of the words change over time... Who would like a LLM having the mainstream early 60's vision of women?"
  - [corpus] Weak evidence - corpus contains related papers but none directly addressing temporal bias mechanism.
- Break condition: If models are continuously updated with current data to reflect evolving language.

## Foundational Learning

- Concept: Data distribution and sampling
  - Why needed here: Understanding how imbalanced data leads to biased models is fundamental to the paper's thesis
  - Quick check question: If a dataset contains 90% examples from one demographic group, what kind of bias might the resulting model exhibit?

- Concept: Confounding variables and causal inference
  - Why needed here: The paper distinguishes between spurious correlations and causal reasoning, requiring understanding of how confounding variables create false associations
  - Quick check question: Why might using neighborhood information as a feature in a loan approval model introduce bias, even if race is not explicitly used?

- Concept: Cultural and linguistic variation
  - Why needed here: The paper discusses how models trained on Western-centered data perform poorly on other cultural contexts
  - Quick check question: How might a language model trained primarily on English data struggle when deployed in multilingual contexts?

## Architecture Onboarding

- Component map: Data preprocessing → Model training → Bias detection → Bias mitigation → Deployment monitoring
- Critical path: Clean data → Train model → Test for biases → Apply mitigation strategies → Deploy with monitoring
- Design tradeoffs: Balancing model performance with fairness constraints often requires sacrificing some accuracy to reduce bias
- Failure signatures: Disproportionate error rates across demographic groups, reinforcement of stereotypes, poor performance on underrepresented groups
- First 3 experiments:
  1. Analyze training data distribution across demographic groups to identify selection biases
  2. Test model predictions on counterfactual examples where protected attributes are modified
  3. Compare model performance metrics across different demographic subgroups to identify bias patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific methods can be used to evaluate the fairness of AI models across different cultures and regions?
- Basis in paper: [explicit] The paper mentions that creating systems that behave fairly across all situations and cultures remains a significant challenge.
- Why unresolved: The paper does not provide specific methods for evaluating fairness across cultures and regions, only stating that it is a complex task.
- What evidence would resolve it: Research showing successful methods for cross-cultural fairness evaluation in AI models.

### Open Question 2
- Question: How can we effectively mitigate biases in generative AI models that are trained on internet data, which may contain inherent biases?
- Basis in paper: [explicit] The paper discusses the challenges of biases in generative AI models and mentions methods like oversampling, adversarial training, and data augmentation.
- Why unresolved: The paper does not provide a comprehensive solution for mitigating biases in generative AI models trained on internet data.
- What evidence would resolve it: Studies demonstrating effective bias mitigation techniques specifically for generative AI models trained on internet data.

### Open Question 3
- Question: What are the long-term impacts of biased AI models on society, and how can these impacts be measured and addressed?
- Basis in paper: [explicit] The paper highlights the societal impact of biased AI models, including reinforcing existing inequalities and discrimination.
- Why unresolved: The paper does not explore the long-term societal impacts of biased AI models or propose methods for measuring and addressing these impacts.
- What evidence would resolve it: Longitudinal studies on the societal impacts of biased AI models and effective strategies for addressing these impacts.

## Limitations
- Limited empirical validation of proposed mitigation strategies across different bias types
- Scalability challenges for bias detection methods when applied to large language models
- Quantification gaps in trade-offs between fairness improvements and model performance degradation

## Confidence
- Mechanism 1: High - Strong supporting literature and concrete NLP examples
- Mechanism 2: Medium - Theoretical understanding but limited empirical demonstrations
- Mechanism 3: Low - Primarily anecdotal support with limited direct evidence

## Next Checks
1. Conduct controlled experiments comparing model performance on balanced vs. imbalanced datasets across multiple demographic groups to quantify the relationship between data distribution and bias manifestation
2. Implement and evaluate the adversarial training approach described for fairness through blindness on a standard benchmark dataset to measure both fairness improvements and performance degradation
3. Perform longitudinal analysis of model outputs over time using datasets spanning different historical periods to empirically validate the temporal bias mechanism and test adaptation strategies