---
ver: rpa2
title: Evaluating Morphological Compositional Generalization in Large Language Models
arxiv_id: '2410.12656'
source_url: https://arxiv.org/abs/2410.12656
tags:
- word
- systematicity
- root
- turkish
- morphological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models exhibit limited morphological compositional
  generalization in agglutinative languages, particularly struggling with novel word
  roots and increasing morphological complexity. The study introduces generative and
  discriminative tasks to evaluate morphological productivity and systematicity, testing
  models like GPT-4 and Gemini on Turkish and Finnish.
---

# Evaluating Morphological Compositional Generalization in Large Language Models

## Quick Facts
- arXiv ID: 2410.12656
- Source URL: https://arxiv.org/abs/2410.12656
- Reference count: 40
- Primary result: Large language models show significant deficits in morphological compositional generalization, especially with novel word roots and increasing morphological complexity

## Executive Summary
This study evaluates how well large language models (LLMs) can generalize morphological rules to novel linguistic contexts, particularly in agglutinative languages like Turkish and Finnish. Through two novel tasks—morphological productivity (generating words from roots and affixes) and morphological systematicity (judging grammatical validity)—the research reveals that LLMs struggle significantly when presented with unseen word roots, despite performing better than chance on systematicity tasks. The performance gap widens with morphological complexity, and results consistently fall short of human capabilities, suggesting fundamental limitations in how these models handle compositional generalization.

## Method Summary
The researchers created test suites from Turkish and Finnish corpora, extracting morphologically complex words segmented into roots and bound morphemes. They generated out-of-distribution (OOD) versions using nonce word roots to test generalization to novel inputs. Two tasks were evaluated: productivity (generating valid words from given morphemes) and systematicity (judging grammatical validity). Models were tested using few-shot in-context learning (1, 3, 5 shots) with greedy decoding across multiple LLMs including GPT-4 and Gemini-1.5. Performance was measured using exact match accuracy for productivity and macro-F1 plus coherence scores for systematicity, with human evaluation serving as a baseline for comparison.

## Key Results
- LLMs exhibit sharp performance decline with increasing morphological complexity, dropping to near-zero accuracy while human performance remains stable
- Productivity task performance is severely impaired for OOD cases with novel word roots, with most valid generations failing task constraints
- Systematicity task shows moderate success (better than chance) but lacks human-level coherence, indicating models identify combinations without true systematic understanding

## Why This Works (Mechanism)

### Mechanism 1
LLMs fail morphological productivity with novel roots because they rely on surface-level memorization rather than abstract morphological rules. When given a known affix list and novel root, models generate invalid or unfaithful outputs that ignore task constraints. This occurs because morphological productivity requires applying affixes to unseen roots while preserving grammatical validity, but LLMs treat novel roots as outliers rather than inputs to compositional rules. The mechanism breaks if models are fine-tuned on nonce-root examples.

### Mechanism 2
Morphological complexity causes performance to drop sharply for LLMs but not humans because as morpheme count increases, the combinatorial space explodes. LLMs, lacking explicit compositional reasoning, fall back on shallow heuristics that fail in longer derivations. Humans, with internalized grammar, remain robust. This is tied to E-complexity (enumerative morphological complexity) correlating with performance degradation for LLMs but not humans. The mechanism breaks if explicit morpheme-order constraints or rule-based decoding are provided.

### Mechanism 3
Tokenization granularity does not explain morphological generalization failure because even when morphemes are tokenized at subword level, models still cannot compose valid derivations. The issue lies in compositional reasoning, not token alignment. Experiments with tokenizer-aligned versus morphologically-aligned morphemes yield similar poor results. The failure is due to lack of abstract morphological rule learning, not token-level representation. The mechanism breaks if character-level tokenization is used, though the model may still fail without compositional reasoning.

## Foundational Learning

- **Compositional generalization**: The ability to systematically recombine known components in novel ways. Why needed: The study hinges on whether models can generalize morphological rules to novel inputs; without this concept, tasks and results cannot be interpreted. Quick check: If a model knows "-ler" means plural and can apply it to "kitap", can it correctly derive "kitaplar" from "kitap" + "-ler" when never seeing "kitaplar" before?

- **Agglutinative morphology**: Languages where words are formed by stringing together morphemes, each representing a single grammatical function. Why needed: Turkish and Finnish are agglutinative; tasks require handling multiple morphemes per word. Misunderstanding this leads to incorrect assumptions about task difficulty. Quick check: In an agglutinative language, how many morphemes might a single word contain? (Answer: often 3-7 or more.)

- **Productivity vs systematicity**: Productivity measures generation ability (can you create new words?), while systematicity measures judgment ability (can you tell if a word is grammatical?). Why needed: The two experimental tasks measure different aspects of morphological generalization; conflating them would obscure which ability LLMs lack. Quick check: If a model can correctly judge "kitap+larsa" as grammatical but fails to generate it, which aspect is failing?

## Architecture Onboarding

- **Component map**: Data pipeline → Morphological analyzer → Test suite generator (ID/OOD) → Prompt generator (English/Turkish/Finnish) → LLM inference (few-shot) → Evaluation metrics (accuracy, macro-F1, coherence) → Human annotation (validation) → Analysis scripts (complexity, tokenization, context effects)
- **Critical path**: Test suite generation → Prompt template creation → Model evaluation (few-shot) → Result aggregation → Human validation → Complexity stratification analysis
- **Design tradeoffs**: Few-shot prompting trades prompt engineering cost against model performance; English instruction prompts trade instruction clarity against code-switching challenges; tokenizer alignment trades morpheme fidelity against model compatibility
- **Failure signatures**: Sharp accuracy drop with morpheme count; coherence score collapse despite moderate macro-F1; generation of grammatically invalid or unfaithful outputs; sensitivity to morpheme order in prompts
- **First 3 experiments**:
  1. Replicate the 5-shot productivity task on GPT-4 with shuffled vs correct morpheme order to confirm order sensitivity
  2. Run the systematicity task with language-specific negative sampling (e.g., no adjacent vowels in Turkish) to test heuristic shortcut exploitation
  3. Vary decoding strategy (temperature, top_p) on the productivity task to check if stochasticity improves compositionality

## Open Questions the Paper Calls Out

### Open Question 1
Do morphological generalization deficits persist across diverse language families beyond agglutinative languages? The paper explicitly acknowledges their study is limited to two agglutinative languages (Turkish and Finnish) and calls for further research on other language families. This remains unresolved because the current study's scope is confined to typologically similar agglutinative languages, leaving open whether these findings generalize to fusional, isolating, or polysynthetic languages. Conducting analogous experiments on languages from different morphological typologies (e.g., Mandarin, Arabic, or Inuktitut) using the same task framework would clarify if morphological generalization challenges are universal or language-specific.

### Open Question 2
Is the observed morphological generalization gap due to tokenization strategies or deeper compositional limitations in LLMs? The authors conducted tokenization-aligned experiments showing similar performance to morphologically-aligned tasks, suggesting tokenization may not be the core issue, but acknowledge this analysis was limited to subword-level tokenizers. This remains unresolved because while tokenization effects were partially ruled out, the analysis did not include character-level tokenizers, and the possibility remains that other tokenization-related factors could contribute to performance gaps. Systematic comparison of model performance across different tokenization schemes (character-level, word-piece, byte-level) on identical morphological tasks would isolate the impact of tokenization from compositional limitations.

### Open Question 3
How do morphological generalization abilities scale with model size and training data diversity? The study tested multiple model sizes (8B, 35B, 7B, 32B) but found performance gaps persisted even in the largest models, suggesting scaling alone may not resolve the issue, though the relationship remains unclear. This remains unresolved because while performance improved with model size, the study did not systematically analyze scaling laws or the impact of multilingual training data composition on morphological generalization. Training and evaluating models across a broader range of sizes while controlling for training data diversity (e.g., varying proportions of morphologically rich languages) would reveal whether scaling or data composition drives improvements.

### Open Question 4
What specific architectural or training modifications could enhance morphological compositional generalization? The consistent performance gaps across different models and tasks imply that current architectures may lack mechanisms for robust morphological compositionality, though the paper does not propose specific architectural changes. This remains unresolved because the study identifies the problem but does not explore architectural innovations (e.g., morpheme-aware attention, explicit morphological supervision) that could address these deficits. Comparative experiments testing models with morphological-aware architectural modifications (e.g., morpheme-level embeddings, explicit morphological constraints) against standard transformers would identify effective solutions.

## Limitations

- Morphological analyzer accuracy directly affects test suite quality, but segmentation precision is not independently validated
- Nonce word generation rules may not fully capture naturalistic linguistic constraints, potentially inflating difficulty
- Few-shot in-context learning without parameter updates means results may not generalize to fine-tuned models
- Human baseline comparison lacks demographic diversity and standardized training protocols

## Confidence

- **Morphological productivity failure on novel roots**: High confidence
- **Complexity-induced performance collapse**: Medium confidence  
- **Tokenization irrelevance**: Low confidence

## Next Checks

1. **Morphological analyzer ablation**: Rerun the systematicity task using gold-standard morphological annotations from linguistic experts on a subset of test items, comparing results with the automatic analyzer outputs to quantify segmentation error impact on model performance.

2. **Tokenization granularity experiment**: Implement character-level tokenization for the productivity task and compare performance with BPE and morpheme-aligned tokenizations across the same model instances, controlling for all other variables to isolate tokenization effects.

3. **Compositional scaffolding test**: Modify the productivity task to provide explicit morpheme ordering constraints in the prompt (e.g., "Add affixes in this order: root + A + B + C") and measure whether this scaffolding reduces the performance gap with humans, distinguishing between rule application and rule learning.