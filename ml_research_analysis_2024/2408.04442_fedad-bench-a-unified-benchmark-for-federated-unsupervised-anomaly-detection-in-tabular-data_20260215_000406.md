---
ver: rpa2
title: 'FedAD-Bench: A Unified Benchmark for Federated Unsupervised Anomaly Detection
  in Tabular Data'
arxiv_id: '2408.04442'
source_url: https://arxiv.org/abs/2408.04442
tags:
- anomaly
- detection
- data
- learning
- unsupervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedAD-Bench, a benchmark for evaluating unsupervised
  anomaly detection methods in federated learning (FL) settings. The authors systematically
  compare deep learning models across diverse tabular datasets, addressing the lack
  of standardized evaluation frameworks in this domain.
---

# FedAD-Bench: A Unified Benchmark for Federated Unsupervised Anomaly Detection in Tabular Data

## Quick Facts
- arXiv ID: 2408.04442
- Source URL: https://arxiv.org/abs/2408.04442
- Reference count: 32
- Primary result: Introduces a benchmark for evaluating unsupervised anomaly detection methods in federated learning settings using five models, four datasets, and five metrics.

## Executive Summary
This paper addresses the lack of standardized evaluation frameworks for unsupervised anomaly detection in federated learning (FL) by introducing FedAD-Bench. The benchmark systematically compares deep learning models across diverse tabular datasets, employing a robust set of evaluation metrics including Precision, Recall, AUROC, AUPR, and F1-Score. The authors redesign data splitting to exclude anomalies from training sets, enabling models to focus on learning normal data distributions without interference from abnormal samples.

The experimental results reveal that while centralized learning typically outperforms FL, federated setups can surpass centralized approaches in specific scenarios due to inherent regularization effects that help mitigate overfitting. The work establishes a standardized framework to guide future research and development in federated anomaly detection, addressing key challenges such as model aggregation strategies, data heterogeneity, and comprehensive evaluation methodologies.

## Method Summary
The benchmark evaluates five state-of-the-art anomaly detection models (DAE, DSEBM, DEEPSVDD, NeuTraLAD, MemAE) on four tabular datasets under both centralized and federated settings. Data is split such that anomalies are excluded from training sets, with 50% normal data for training and 50% normal data plus all anomalies for testing. Federated learning uses FedAvg and FedProx aggregation strategies across 3, 5, or 7 clients. Models are trained using Adam optimizer (lr=1e-4, weight_decay=1e-4) with batch sizes varying by dataset size. Performance is evaluated using five metrics: Precision, Recall, AUROC, AUPR, and F1 Score.

## Key Results
- Excluding anomalies from training data improves model performance by focusing learning on normal data distributions
- Federated learning's regularization effects can help mitigate overfitting, sometimes outperforming centralized learning
- Using multiple evaluation metrics provides more comprehensive and reliable assessment than single metrics

## Why This Works (Mechanism)

### Mechanism 1
Excluding anomalies from training data improves the model's ability to learn normal data distributions in unsupervised anomaly detection. By removing anomalies from the training set, the model focuses solely on learning the underlying distribution of normal data without interference from abnormal samples, leading to better anomaly detection performance. This assumes anomalies are rare and distinct enough from normal data that their exclusion does not harm the model's ability to generalize.

### Mechanism 2
Federated learning's inherent regularization effects can help mitigate overfitting, potentially leading to better generalization than centralized learning in some cases. The aggregation of local models in FL introduces a form of regularization by averaging out idiosyncratic features learned by individual clients, which can reduce overfitting to local data distributions. This assumes the local data distributions across clients are sufficiently diverse to provide a regularization effect through model aggregation.

### Mechanism 3
Using multiple evaluation metrics, including threshold-independent metrics like AUROC and AUPR, provides a more comprehensive and reliable assessment of anomaly detection performance than relying on a single threshold-dependent metric. Different metrics capture different aspects of model performance, with threshold-independent metrics providing a more robust evaluation across various operating points. This assumes the combination of multiple metrics provides a more complete picture of model performance than any single metric.

## Foundational Learning

- Concept: Unsupervised learning and anomaly detection
  - Why needed here: FedAD-Bench focuses on evaluating unsupervised anomaly detection methods in federated learning settings. Understanding the principles of unsupervised learning and anomaly detection is crucial for interpreting the benchmark's results and designing effective anomaly detection models.
  - Quick check question: What is the main difference between supervised and unsupervised learning in the context of anomaly detection?

- Concept: Federated learning and model aggregation
  - Why needed here: The benchmark evaluates anomaly detection methods in federated learning environments. Understanding how federated learning works, including the aggregation of local models, is essential for interpreting the benchmark's results and understanding the potential benefits and challenges of federated anomaly detection.
  - Quick check question: How does model aggregation in federated learning differ from traditional centralized training?

- Concept: Evaluation metrics for anomaly detection
  - Why needed here: FedAD-Bench employs a robust set of evaluation metrics to assess anomaly detection performance. Understanding the strengths and limitations of different metrics, including threshold-dependent and threshold-independent metrics, is crucial for interpreting the benchmark's results and designing effective evaluation strategies.
  - Quick check question: Why might relying solely on a threshold-dependent metric like F1-score be problematic for evaluating anomaly detection performance?

## Architecture Onboarding

- Component map: Datasets -> Models -> Evaluation metrics -> Federated learning setup
- Critical path: 1) Prepare datasets with class-based splitting (anomalies in test set only), 2) Train models on local data in federated or centralized settings, 3) Aggregate models using FedAvg or FedProx, 4) Evaluate performance using the robust set of metrics, 5) Analyze results and identify key challenges and insights
- Design tradeoffs: Number of clients vs. model performance (more clients may lead to smaller local datasets and potential performance degradation), Aggregation strategy (FedAvg vs. FedProx, with FedProx potentially offering better handling of heterogeneous data distributions), Evaluation metrics (balancing threshold-dependent and threshold-independent metrics for comprehensive assessment)
- Failure signatures: Poor performance on all metrics (fundamental issues with anomaly detection model or data preprocessing), Large discrepancy between threshold-dependent and threshold-independent metrics (potential bias or sensitivity to threshold choice), Significant performance drop when increasing number of clients (insufficient local data or model capacity)
- First 3 experiments: 1) Evaluate DAE on Arrhythmia dataset in both centralized and federated settings using 3 clients, 2) Compare DEEPSVDD and NeuTraLAD on Thyroid dataset in federated settings with varying clients (3, 5, 7), 3) Assess FedProx impact on MemAE performance on KDDCUP10 dataset with different Î¼ values

## Open Questions the Paper Calls Out

### Open Question 1
How do specialized aggregation strategies for complex models (e.g., DSEBM, MemAE) affect performance in federated unsupervised anomaly detection? The paper explicitly states that complex models require specialized aggregation strategies to handle extra modules and heuristics, and that this is a limitation of current approaches. This remains unresolved as the paper only mentions this limitation without exploring or testing specialized aggregation strategies.

### Open Question 2
How does dataset homogeneity impact the performance gap between centralized and federated learning in anomaly detection? The paper notes that KDD10 dataset's high homogeneity led to sustained performance in federated settings, and that FedProx effectively mitigated overfitting in this scenario. This remains unresolved as the paper observes this phenomenon but does not systematically analyze how different levels of dataset homogeneity affect the centralized vs. federated performance trade-off.

### Open Question 3
What is the optimal number of clients for federated unsupervised anomaly detection across different dataset sizes and model complexities? The paper shows that increasing the number of clients generally decreases performance, but notes that KDD10 maintained performance despite this trend. This remains unresolved as the paper only tests 3, 5, and 7 clients without establishing guidelines for choosing the optimal number based on dataset characteristics or model requirements.

## Limitations
- Restricted to tabular datasets, limiting applicability to other data modalities
- Choice of five specific anomaly detection models may not capture the full landscape of possible approaches
- Evaluation focuses on binary anomaly detection, which may not translate to multi-class scenarios

## Confidence
- Confidence in core claims: Medium
- Major uncertainties include: assumption that excluding anomalies from training always improves performance may not hold for all dataset characteristics, observed regularization benefits of FL depend on specific data distributions and may not generalize across all domains, optimal number of clients and aggregation strategy may be dataset-dependent

## Next Checks
1. Test FedAD-Bench with additional tabular datasets featuring different anomaly ratios and feature distributions to verify the robustness of observed FL regularization effects
2. Adapt the benchmark framework for non-tabular data (e.g., images or time series) to assess the generalizability of the evaluation methodology and findings
3. Extend the experimental setup to multi-class anomaly scenarios to evaluate whether insights about training data composition and FL benefits hold in more complex settings