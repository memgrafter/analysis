---
ver: rpa2
title: Densely Multiplied Physics Informed Neural Networks
arxiv_id: '2402.04390'
source_url: https://arxiv.org/abs/2402.04390
tags:
- pinn
- neural
- training
- equation
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of insufficient precision and
  incorrect outcomes in physics-informed neural networks (PINNs) when solving nonlinear
  partial differential equations (PDEs). The authors propose a novel architecture
  called densely multiplied PINN (DM-PINN), which multiplies the output of each hidden
  layer with the outputs of all subsequent hidden layers.
---

# Densely Multiplied Physics Informed Neural Networks

## Quick Facts
- arXiv ID: 2402.04390
- Source URL: https://arxiv.org/abs/2402.04390
- Authors: Feilong Jiang; Xiaonan Hou; Min Xia
- Reference count: 40
- Primary result: DM-PINN achieves L2 error of 2.57e-02 on Allen-Cahn equation, outperforming vanilla PINN (5.32e-01) while requiring less computational time per iteration

## Executive Summary
This paper addresses accuracy and stability issues in physics-informed neural networks (PINNs) when solving nonlinear partial differential equations (PDEs). The authors propose a densely multiplied PINN (DM-PINN) architecture that multiplies outputs of each hidden layer with all subsequent layer outputs, enhancing expressive power without adding trainable parameters. The method is evaluated on four benchmark PDE problems, showing significant accuracy improvements over standard PINN architectures.

## Method Summary
The paper introduces DM-PINN, a novel PINN architecture that uses element-wise multiplication between hidden layer outputs to create richer feature interactions. The architecture includes Batch Normalization for input normalization and a ResNet-style skip connection structure (SDM-PINN variant) to address potential gradient vanishing issues. The method is trained using the Adam optimizer with learning rates between 0.001-0.002 on four benchmark PDE problems using Latin Hypercube sampling for data points.

## Key Results
- DM-PINN achieves L2 error of 2.57e-02 on Allen-Cahn equation versus 5.32e-01 for vanilla PINN
- The method requires less computational time per iteration compared to baseline approaches
- Hessian eigenvalue analysis shows DM-PINN has smaller maximum eigenvalues, indicating less stiff gradient flow dynamics during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Element-wise multiplication of hidden layer outputs creates richer feature interactions without adding trainable parameters
- Mechanism: Each hidden layer's output is multiplied with all subsequent layer outputs, allowing information from earlier layers to be directly combined with later layer features
- Core assumption: The element-wise multiplication operation effectively combines features in a way that improves the network's expressive power for solving PDEs
- Evidence anchors: Abstract states the mechanism "can significantly improve the accuracy of PINNs" without introducing more trainable parameters

### Mechanism 2
- Claim: Skip densely multiplied structure prevents gradient vanishing problems while maintaining efficiency
- Mechanism: The skip densely multiplied PINN (SDM-PINN) uses modified architecture where multiplication operations are replaced with skip connections
- Core assumption: Skip connections can effectively mitigate gradient vanishing while preserving the benefits of multiplicative feature interactions
- Evidence anchors: Section 3 mentions adopting ResNet structure as residual blocks which "achieves the best performance"

### Mechanism 3
- Claim: Densely multiplied structure stabilizes training dynamics by reducing Hessian eigenvalue stiffness
- Mechanism: The multiplicative interactions create a smoother loss landscape with smaller maximum eigenvalues of the Hessian matrix
- Core assumption: Smaller maximum Hessian eigenvalues correlate with more stable training and better convergence
- Evidence anchors: Section 5 shows DM-PINN has smallest largest eigenvalue among models, indicating "less restriction in the required learning rate"

## Foundational Learning

- Concept: Physics-Informed Neural Networks (PINNs)
  - Why needed here: DM-PINN is a specific architecture for PINNs, so understanding how PINNs work is essential
  - Quick check question: What are the three main loss components in a PINN, and what does each enforce?

- Concept: Partial Differential Equations (PDEs) and their numerical solution
  - Why needed here: The paper evaluates DM-PINN on several PDE benchmark problems, requiring understanding of PDE formulation and solution
  - Quick check question: What are the typical boundary and initial conditions for the Allen-Cahn equation?

- Concept: Neural network architectures and training dynamics
  - Why needed here: DM-PINN modifies standard neural network architecture, so understanding basic NN concepts is crucial
  - Quick check question: What is the purpose of Batch Normalization and how does it help with gradient flow?

## Architecture Onboarding

- Component map: Input layer → Batch Normalization → Dense multiplicative blocks → Output layer
- Critical path: 1) Input normalization (Batch Normalization), 2) Feature extraction through dense multiplicative layers, 3) Output generation for PDE solution, 4) Loss computation combining PDE residuals, initial conditions, and boundary conditions
- Design tradeoffs: Multiplicative vs additive interactions (richer combinations but potential numerical issues), Dense vs sparse connections (more information flow but increased computational cost), Skip connections (improve gradient flow but may reduce multiplicative benefits)
- Failure signatures: Training instability or divergence (numerical issues with multiplication), Poor performance on boundary conditions (insufficient skip connections), Slow convergence or getting stuck in local minima (inappropriate learning rate for stiffer Hessian)
- First 3 experiments: 1) Implement basic DM-PINN on simple PDE (like 1D Poisson equation) and compare with vanilla PINN, 2) Test gradient flow by monitoring loss during training and checking for vanishing/exploding gradients, 3) Vary number of hidden layers and neurons to find optimal architecture for given PDE problem

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored including performance on higher-dimensional PDEs, combination with other advanced training techniques, and systematic analysis of layer count effects on performance.

## Limitations
- Limited generalization scope: Evaluation restricted to four specific PDE problems without testing on other PDE classes or higher-dimensional problems
- Numerical stability concerns: Densely multiplied structure could face instability issues in practice, particularly with deep networks or widely varying output scales
- Limited benchmarking: Comparisons primarily against vanilla PINN and modified MLP, lacking comprehensive evaluation against state-of-the-art PINN variants

## Confidence
- High confidence: Core architectural innovation (densely multiplied structure) is clearly defined and implementation details are sufficiently specified
- Medium confidence: Performance claims demonstrated through L2 error improvements, though more comprehensive benchmarking would strengthen assertions
- Low confidence: Training stability assertions based on Hessian eigenvalue analysis without systematic experiments across different learning rates, initialization schemes, or network depths

## Next Checks
1. **Scalability test**: Evaluate DM-PINN on higher-dimensional PDEs (2D/3D problems) to verify that multiplicative benefits persist and numerical stability is maintained
2. **Ablation study**: Systematically remove individual components (Batch Normalization, skip connections) to quantify their contribution to performance gains and identify which are essential versus beneficial
3. **Robustness analysis**: Test the method on PDEs with varying levels of noise in training data and with non-uniform sampling strategies to assess real-world applicability beyond controlled benchmark settings