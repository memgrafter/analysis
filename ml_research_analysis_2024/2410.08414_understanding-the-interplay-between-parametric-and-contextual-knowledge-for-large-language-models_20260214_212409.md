---
ver: rpa2
title: Understanding the Interplay between Parametric and Contextual Knowledge for
  Large Language Models
arxiv_id: '2410.08414'
source_url: https://arxiv.org/abs/2410.08414
tags:
- knowledge
- llms
- reasoning
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates how well large language models (LLMs) can
  integrate their internal parametric knowledge (PK) with external contextual knowledge
  (CK) when solving complex problems. To study this, the authors introduce ECHOQA,
  a new benchmark spanning scientific, factual, and commonsense knowledge, and design
  four reasoning types based on the relationships between PK and CK: Supportive, Complementary,
  Conflicting, and Irrelevant.'
---

# Understanding the Interplay between Parametric and Contextual Knowledge for Large Language Models

## Quick Facts
- arXiv ID: 2410.08414
- Source URL: https://arxiv.org/abs/2410.08414
- Reference count: 40
- Primary result: LLMs suppress parametric knowledge when contextual knowledge is present, even when it's complementary or irrelevant

## Executive Summary
This paper investigates how large language models (LLMs) integrate their internal parametric knowledge (PK) with external contextual knowledge (CK) when solving complex problems. The authors introduce ECHOQA, a new benchmark spanning scientific, factual, and commonsense knowledge, and design four reasoning types based on the relationships between PK and CK: Supportive, Complementary, Conflicting, and Irrelevant. The study reveals that LLMs tend to suppress their PK when CK is available, regardless of whether the CK is complementary or irrelevant to the task. This suppression occurs across different models, knowledge types, and reasoning scenarios, suggesting a fundamental limitation in how LLMs combine multiple knowledge sources.

## Method Summary
The authors create ECHOQA, a benchmark with 1,005 questions across scientific, factual, and commonsense domains, each with three versions representing different CK-PK relationships. They develop a framework to systematically fabricate CK using three methods: adding, varying, and dropping known facts. Four reasoning types are defined: Supportive (CK and PK both provide correct answers), Complementary (CK complements PK to form a complete answer), Conflicting (CK contradicts PK), and Irrelevant (CK provides unrelated information). The study evaluates multiple LLMs including Llama, Qwen, and GPT-4 using progressively-enforced instructions designed to encourage PK reliance, measuring accuracy, memorization ratio, and unknown ratio across different scenarios.

## Key Results
- LLMs consistently suppress parametric knowledge when contextual knowledge is present, even when CK is complementary or irrelevant
- Instruction-following capabilities can partially modulate LLMs' awareness of PK, but cannot fully restore it
- LLMs show better recall of popular knowledge and are more resistant to conflicts in commonsense knowledge compared to scientific or factual knowledge
- Complementary contextual knowledge actually increases uncertainty in LLMs, doubling the unknown ratio for most models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs suppress parametric knowledge (PK) when contextual knowledge (CK) is present, even when CK is complementary or irrelevant.
- **Mechanism**: The suppression occurs because LLMs prioritize context over internal knowledge, treating the context as the primary source of information for reasoning.
- **Core assumption**: The context window and CK are treated as more reliable or relevant than PK stored in model parameters.
- **Evidence anchors**:
  - [abstract] "Our results show that LLMs tend to suppress their PK when contextual information is available, even when it is complementary or irrelevant."
  - [section] "With complementary CK, LLMs show even increased uncertainty. Table 3 indicates that the complementary scientific CK 'confuses' most tested LLMs significantly, doubling the UR for Llama and Qwen models, comparing 'NI' with 'w/o K' column."

### Mechanism 2
- **Claim**: Instruction-following capabilities can modulate LLMs' awareness of PK, but cannot fully restore it.
- **Mechanism**: Progressive instructions that explicitly tell LLMs to trust their own knowledge or to "speak out loud" about their PK can increase reliance on PK, but a significant gap remains compared to when all knowledge is provided.
- **Core assumption**: LLMs can follow instructions to some degree, but their default behavior is to prioritize CK.
- **Evidence anchors**:
  - [abstract] "While tailored instructions can encourage LLMs to rely more on their PK, they still struggle to fully leverage it."
  - [section] "By asking LLMs to adopt PK in a progressively enforced tone, our reasoning instructions (Section 3.2) significantly bring up the performance, demonstrating the strong instruction-following capabilities of LLMs and showing that instructions can modulate perception of knowledge to some extent."

### Mechanism 3
- **Claim**: The popularity of knowledge in training corpus affects LLMs' ability to recall PK.
- **Mechanism**: LLMs are more likely to recall PK that corresponds to more popular entities or facts, likely because these appear more frequently in training data.
- **Core assumption**: Training corpus contains imbalanced representation of knowledge, with popular entities appearing more often.
- **Evidence anchors**:
  - [section] "Inspired by studies showing that LLMs lean on more popular entities, i.e., monthly associated Wikipedia page views (Mallen et al., 2023; Xie et al., 2024), we evaluate representative LLMs on ConflictQA with Conflicting Reasoning w.r.t, popularity of the topic entity in the question."
  - [section] "Intuitively, LLMs can recall memories better for more popular questions. This upward trend holds across all LLMs and our reasoning instructions."

## Foundational Learning

- **Concept**: Parametric vs Contextual Knowledge
  - **Why needed here**: Understanding the distinction between knowledge encoded in model parameters (PK) and knowledge provided in context (CK) is fundamental to grasping why LLMs struggle with integration.
  - **Quick check question**: What is the difference between parametric knowledge and contextual knowledge in LLMs?

- **Concept**: Knowledge Conflict Resolution
  - **Why needed here**: The paper investigates how LLMs handle conflicts between PK and CK, which requires understanding conflict resolution mechanisms.
  - **Quick check question**: How do LLMs typically behave when parametric knowledge conflicts with contextual knowledge?

- **Concept**: Instruction Following in LLMs
  - **Why needed here**: The study uses progressively-enforced instructions to modulate LLM behavior, requiring understanding of how well LLMs can follow instructions.
  - **Quick check question**: What types of instructions were used to encourage LLMs to rely more on their parametric knowledge?

## Architecture Onboarding

- **Component map**: Question → Knowledge Elicitation → CK Construction → Reasoning Instruction Application → Answer Generation → Evaluation
- **Critical path**: Question → Knowledge Elicitation → CK Construction → Reasoning Instruction Application → Answer Generation → Evaluation
- **Design tradeoffs**: The study prioritizes understanding PK-CK interplay over optimizing for practical performance, using controlled experiments that may not reflect real-world complexity.
- **Failure signatures**: LLMs showing high unknown ratios, low memorization ratios in conflicting scenarios, or performance degradation when complementary CK is added.
- **First 3 experiments**:
  1. Test LLMs on complementary reasoning tasks with neutral instructions to establish baseline suppression of PK.
  2. Apply progressively-enforced instructions to measure improvement in PK leverage.
  3. Test different CK fabrication methods (adding, varying, dropping) to understand their impact on PK awareness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the post-training process in LLMs influence their ability to integrate parametric knowledge (PK) with contextual knowledge (CK), and what specific training modifications could enhance this integration?
- Basis in paper: Inferred from the discussion on how LLMs are fine-tuned with instructions and optional context separately, which may hinder their ability to combine PK and CK effectively.
- Why unresolved: The paper hypothesizes that the current post-training approach encourages LLMs to rely on either PK or CK independently, rather than integrating both, but does not provide concrete evidence or specific training modifications to address this issue.
- What evidence would resolve it: Experimental results comparing the performance of LLMs trained with different post-training objectives that explicitly encourage the integration of PK and CK, such as joint reasoning tasks or multi-step reasoning that requires both knowledge sources.

### Open Question 2
- Question: To what extent does the popularity of entities in training data affect LLMs' ability to recall and leverage their parametric knowledge (PK), and how can this bias be mitigated?
- Basis in paper: Explicit from the finding that LLMs recall popular knowledge better, as evidenced by the higher memorization ratio for popular entities in the ConflictQA dataset.
- Why unresolved: While the paper identifies the correlation between entity popularity and PK recall, it does not explore the underlying mechanisms or propose solutions to mitigate this bias in LLMs.
- What evidence would resolve it: Detailed analysis of training data to quantify the impact of entity popularity on PK recall, and experiments testing whether balancing the frequency of knowledge in training data or using techniques like knowledge distillation can reduce this bias.

### Open Question 3
- Question: How does the method of fabricating conflicting contextual knowledge (CK) impact LLMs' ability to recall and leverage their parametric knowledge (PK), and which method is most effective for maintaining PK integrity?
- Basis in paper: Explicit from the observation that different methods of fabricating CK (adding, variation, dropping) affect LLMs' memorization ratio differently, with dropping being the least harmful to PK recall.
- Why unresolved: The paper identifies the varying impact of CK fabrication methods but does not provide a comprehensive evaluation of their long-term effects on PK integrity or suggest optimal strategies for fabricating CK in different scenarios.
- What evidence would resolve it: Comparative experiments across diverse knowledge domains and reasoning tasks to assess the impact of each CK fabrication method on PK recall, and the development of guidelines for selecting the most appropriate method based on the specific knowledge and task requirements.

## Limitations

- The ECHOQA benchmark uses manually curated and synthesized examples, which may not fully capture real-world knowledge conflict complexity
- The study relies on accuracy, memorization ratio, and unknown ratio metrics that may not capture nuanced aspects of knowledge integration
- The research focuses primarily on mainstream models (Llama, Qwen, GPT-4) and may not represent the full spectrum of model architectures
- The controlled experimental conditions may not reflect how LLMs handle knowledge in more complex, real-world scenarios with multiple information sources

## Confidence

- **High confidence**: The observation that LLMs suppress PK when CK is present, even when CK is complementary or irrelevant. This is consistently demonstrated across multiple models, knowledge types, and reasoning scenarios with clear quantitative evidence.
- **Medium confidence**: The effectiveness of progressively-enforced instructions in modulating PK awareness. While the results show improvement, the instructions may not fully restore PK reliance, and the effect may vary depending on instruction phrasing and context complexity.
- **Medium confidence**: The popularity bias in PK recall. The correlation between knowledge popularity and recall ability is demonstrated, but the underlying mechanisms and potential confounding factors require further investigation.

## Next Checks

1. Apply the ECHOQA benchmark methodology to real-world QA datasets with naturally occurring knowledge conflicts to validate whether the suppression effect persists outside controlled synthetic examples.

2. Systematically vary instruction phrasing, enforcement strength, and timing to determine the limits of instruction-following capabilities for PK awareness and identify optimal instruction strategies.

3. Test the PK-CK interplay across diverse domains (legal, medical, technical) to assess whether the suppression effect and popularity bias generalize beyond scientific, factual, and commonsense knowledge categories.