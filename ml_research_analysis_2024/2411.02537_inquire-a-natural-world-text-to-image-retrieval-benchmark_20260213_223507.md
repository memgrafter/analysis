---
ver: rpa2
title: 'INQUIRE: A Natural World Text-to-Image Retrieval Benchmark'
arxiv_id: '2411.02537'
source_url: https://arxiv.org/abs/2411.02537
tags:
- images
- retrieval
- image
- dataset
- species
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INQUIRE is a text-to-image retrieval benchmark designed to challenge
  multimodal vision-language models on expert-level queries. The benchmark includes
  iNaturalist 2024, a new dataset of five million natural world images, and 250 expert-level
  retrieval queries paired with all relevant images comprehensively labeled within
  iNat24, comprising 33,000 total matches.
---

# INQUIRE: A Natural World Text-to-Image Retrieval Benchmark

## Quick Facts
- arXiv ID: 2411.02537
- Source URL: https://arxiv.org/abs/2411.02537
- Reference count: 40
- Best models achieve mAP@50 below 50% on expert-level natural world queries

## Executive Summary
INQUIRE is a text-to-image retrieval benchmark designed to challenge multimodal vision-language models on expert-level queries about the natural world. The benchmark includes iNaturalist 2024, a new dataset of five million natural world images, and 250 expert-level retrieval queries paired with all relevant images comprehensively labeled within iNat24, comprising 33,000 total matches. Queries span categories such as species identification, context, behavior, and appearance, requiring nuanced image understanding and domain expertise. The benchmark evaluates two core retrieval tasks: INQUIRE-Fullrank, a full dataset ranking task, and INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed evaluation of a range of recent multimodal models demonstrates that INQUIRE poses a significant challenge, with the best models failing to achieve an mAP@50 above 50%. Reranking with more powerful multimodal models can enhance retrieval performance, yet there remains a significant margin for improvement.

## Method Summary
The benchmark evaluates text-to-image retrieval using CLIP-style models for initial ranking, followed by reranking with larger multimodal models like GPT-4o. The method uses iNat24 dataset (5M images, 10K species) and 250 expert-level text queries with comprehensive relevance labels. Evaluation is performed using mAP@50, nDCG@50, and MRR metrics across two tasks: full dataset ranking (INQUIRE-FULLRANK) and reranking top-100 retrievals (INQUIRE-RERANK).

## Key Results
- Best-performing models fail to achieve mAP@50 above 50% on expert-level queries
- Reranking with GPT-4o improves performance on every category compared to initial CLIP ranking
- Queries with scientific terminology are significantly more challenging, indicating models struggle with domain-specific language

## Why This Works (Mechanism)

### Mechanism 1
Multimodal models struggle with expert-level queries due to insufficient domain-specific knowledge despite strong general capabilities. Models trained on general web data lack the specialized vocabulary and fine-grained visual understanding required for expert-level natural world queries. Core assumption: Expert queries contain terminology and visual concepts not well-represented in standard pretraining datasets.

### Mechanism 2
Reranking with more powerful multimodal models provides significant performance improvement for text-to-image retrieval. Initial retrieval using efficient embeddings provides broad coverage, while reranking with expensive models refines the top results using deeper multimodal understanding. Core assumption: Top-100 retrievals contain most relevant images, making reranking computationally feasible while still improving quality.

### Mechanism 3
Performance degradation on queries with scientific terminology indicates models struggle with domain-specific language understanding. Models lack exposure to scientific vocabulary during training, making them unable to properly interpret and retrieve based on technical terms. Core assumption: Scientific terminology represents concepts that models need explicit training on to understand.

## Foundational Learning

- **Text-to-image retrieval vs. image captioning datasets**: Understanding the fundamental difference between retrieval (finding existing images) and captioning (generating descriptions) explains why Flickr30K/COCO benchmarks are insufficient for real-world retrieval. *Quick check*: Why does having exactly one relevant image per query in standard benchmarks not reflect real-world retrieval needs?

- **Multimodal embedding spaces and contrastive learning**: The paper relies heavily on CLIP-style models and their limitations, requiring understanding of how vision-language embeddings work. *Quick check*: How does cosine similarity in the joint embedding space enable zero-shot text-to-image retrieval?

- **Evaluation metrics for ranking tasks (AP@k, nDCG, MRR)**: The paper uses specialized ranking metrics rather than simple recall@k, requiring understanding of how to evaluate retrieval quality. *Quick check*: Why is Average Precision at k more informative than Recall at k for retrieval tasks with multiple relevant images?

## Architecture Onboarding

- **Component map**: Query processing pipeline → Embedding generation (text encoder) → Image indexing (pre-computed image embeddings) → Retrieval (nearest neighbor search) → Reranking (multimodal model) → Evaluation
- **Critical path**: 1) Pre-compute image embeddings using efficient model (CLIP ViT-H/14) 2) For each query, generate text embedding and retrieve top-k images 3) Rerank top-k using more powerful multimodal model 4) Evaluate using AP@k, nDCG@50, MRR
- **Design tradeoffs**: Efficiency vs. accuracy: Using CLIP for initial retrieval vs. GPT-4o for reranking; Comprehensiveness vs. labeling cost: Exhaustively labeling vs. sampling relevant images; Generalist vs. specialist models: Balancing broad knowledge with domain expertise
- **Failure signatures**: Low AP@k scores indicate poor ranking quality; Performance gaps between models suggest domain knowledge limitations; Scientific terminology queries performing worse than general queries indicate language understanding issues
- **First 3 experiments**: 1) Compare CLIP ViT-H/14 vs. SigLIP SO400m-14 on INQUIRE-FULLRANK to validate embedding quality differences 2) Test GPT-4o reranking on different-sized initial retrieval sets (top-50 vs. top-100) to find optimal reranking point 3) Evaluate performance on queries with vs. without scientific terminology to quantify domain knowledge impact

## Open Questions the Paper Calls Out

- How do domain-specific fine-tuning strategies affect retrieval performance on expert-level queries? The paper notes that models specifically trained on natural world data demonstrate degraded performance across all supercategories, but does not explore how to balance domain expertise with maintaining generalist capabilities.

- How can retrieval systems be optimized for interactive use with large image collections while maintaining expert-level accuracy? While the paper establishes benchmark tasks, it does not investigate techniques like approximate nearest neighbors, caching strategies, or adaptive ranking that would enable practical interactive use.

## Limitations

- Limited generalization to other domains - findings may not transfer to medical imaging, satellite imagery, or industrial inspection
- Reranking efficiency trade-offs not comprehensively evaluated - computational costs vs. performance improvements unclear
- Annotation completeness verification - methodology for ensuring exhaustive labeling not fully detailed

## Confidence

**High Confidence**: Benchmark construction methodology and evaluation framework are well-defined and reproducible. Observation that even best-performing models fail to achieve mAP@50 above 50% is supported by direct experimental evidence.

**Medium Confidence**: Mechanism explaining why expert-level queries challenge models (domain-specific knowledge gaps) is plausible but not definitively proven. Alternative explanations could also contribute to performance degradation.

**Low Confidence**: Claim that reranking "significantly" improves performance lacks quantification of practical utility. Magnitude of improvement relative to computational cost isn't established.

## Next Checks

1. **Domain Transferability Test**: Evaluate INQUIRE's best-performing models on a different specialized domain (e.g., medical imaging retrieval) to determine if performance degradation patterns are consistent across domains or specific to natural world imagery.

2. **Reranking Cost-Benefit Analysis**: Measure exact computational costs (API calls, latency) for GPT-4o reranking across different initial retrieval set sizes, then calculate the precision improvement per unit cost to identify optimal reranking thresholds.

3. **Annotation Gap Analysis**: For a sample of broad queries with high match counts, perform targeted manual verification to identify potential missing relevant images, establishing confidence bounds on the claimed comprehensive labeling.