---
ver: rpa2
title: 'MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning'
arxiv_id: '2406.09044'
source_url: https://arxiv.org/abs/2406.09044
tags:
- milora
- lora
- singular
- pissa
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MiLoRA improves parameter-efficient fine-tuning of LLMs by updating
  only the minor singular components of weight matrices while freezing the principal
  components. This approach preserves pretrained knowledge better than existing methods
  like LoRA and PiSSA, as shown by consistent performance gains across commonsense
  reasoning (+1.6 to +2.9 accuracy), math reasoning (+2.0 EM), instruction following
  (+2.9), and visual instruction tasks (+1.4).
---

# MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning

## Quick Facts
- **arXiv ID**: 2406.09044
- **Source URL**: https://arxiv.org/abs/2406.09044
- **Reference count**: 18
- **Primary result**: MiLoRA improves PEFT by updating only minor singular components while freezing principal components, showing +1.6 to +2.9 accuracy gains across multiple tasks.

## Executive Summary
MiLoRA addresses parameter-efficient fine-tuning of large language models by selectively updating only the minor singular components of weight matrices while preserving the principal components. This approach builds on LoRA's framework but modifies the initialization strategy to orthogonalize updates away from the pretrained knowledge-carrying directions. The method demonstrates consistent performance improvements across commonsense reasoning, math reasoning, instruction following, and visual instruction tasks, requiring no additional hyperparameters beyond rank selection. By constraining updates to less-optimized subspaces, MiLoRA achieves better preservation of pretrained knowledge while maintaining training and inference efficiency.

## Method Summary
MiLoRA performs parameter-efficient fine-tuning by decomposing weight matrices via Singular Value Decomposition (SVD) into principal and minor components. The method freezes principal singular components (large singular values) and initializes low-rank adaptation matrices exclusively from minor components. This orthogonal initialization ensures updates occur in subspaces less likely to contain critical pretrained knowledge, reducing catastrophic forgetting. The approach requires computing SVD during initialization but adds no runtime overhead during training or inference. MiLoRA maintains compatibility with standard LoRA training procedures while improving knowledge preservation through its selective updating strategy.

## Key Results
- Consistent performance gains across tasks: commonsense reasoning (+1.6 to +2.9 accuracy), math reasoning (+2.0 EM), instruction following (+2.9)
- Visual instruction tasks show +1.4 improvement over baseline methods
- Lower amplification factor (24.17) compared to LoRA (37.46) and PiSSA (3.18), indicating better preservation of pretrained knowledge
- No additional hyperparameters required beyond standard rank selection

## Why This Works (Mechanism)

### Mechanism 1
MiLoRA improves fine-tuning by restricting weight updates to minor singular components while preserving principal components. Weight matrix W is decomposed via SVD into principal (Wp) and minor (Wm) parts. Only Wm initializes low-rank matrices A and B, ensuring updates occur in a subspace orthogonal to principal knowledge-carrying directions. This works under the assumption that principal components encode essential pretrained knowledge while minor components capture noise or long-tail information. The method would fail if minor components actually encode important task-specific features.

### Mechanism 2
By initializing low-rank matrices from Wm rather than randomly, MiLoRA achieves better preservation of pretrained features. The initialization Wm = (Um√Σm)(√ΣmVm)⊤ = BmAm ensures that the update direction ∆W is naturally orthogonal to principal components, avoiding interference with well-learned features. This assumes orthogonal initialization to principal components reduces catastrophic forgetting during fine-tuning. The approach would break if learning dynamics require adjustment in principal component directions.

### Mechanism 3
MiLoRA reduces amplification factor compared to LoRA and PiSSA, leading to more stable fine-tuning. By constraining updates to minor singular directions, MiLoRA limits the magnitude of weight changes relative to the original matrix, as measured by the Frobenius norm of the projected update. Smaller amplification factors indicate better preservation of pretrained knowledge while still allowing effective adaptation. This would fail if tasks require large weight changes for effective learning.

## Foundational Learning

- **Singular Value Decomposition (SVD)**
  - Why needed here: SVD decomposes weight matrices into principal and minor components, which is the foundation for MiLoRA's selective updating strategy.
  - Quick check question: Given a matrix W, how would you decompose it into principal and minor components using SVD, and what determines which components are "principal"?

- **Low-rank adaptation (LoRA)**
  - Why needed here: MiLoRA builds directly on LoRA's framework of parameter-efficient fine-tuning by modifying the initialization strategy rather than the fundamental approach.
  - Quick check question: How does LoRA's initialization of low-rank matrices differ from MiLoRA's, and what problem does this difference address?

- **Catastrophic forgetting in fine-tuning**
  - Why needed here: MiLoRA's preservation of principal components is specifically designed to mitigate forgetting of pretrained knowledge during adaptation.
  - Quick check question: What metrics could you use to measure forgetting during fine-tuning, and how would you expect MiLoRA to perform on these metrics compared to standard LoRA?

## Architecture Onboarding

- **Component map**: Pre-trained weights W → SVD decomposition → Wp (frozen) + Wm → Bm and Am initialized from Wm → LoRA training on Bm and Am only

- **Critical path**: 
  1. Compute SVD of each target weight matrix
  2. Split into principal and minor components based on singular value magnitude
  3. Initialize Bm and Am from Wm = Um√Σm and √ΣmVm
  4. Apply standard LoRA training to only Bm and Am
  5. During inference, merge updates with frozen Wp

- **Design tradeoffs**:
  - Pros: Better preservation of pretrained knowledge, no additional hyperparameters beyond rank selection
  - Cons: Additional SVD computation cost at initialization, potential limitation if minor components contain important task features
  - Alternative: Could use random initialization with regularization to encourage orthogonal updates instead of explicit SVD

- **Failure signatures**:
  - Poor performance on tasks requiring modification of principal components
  - Increased training instability if minor components are insufficient for learning
  - SVD computation becoming bottleneck for very large matrices

- **First 3 experiments**:
  1. Replicate LoRA results on a simple task, then compare with MiLoRA using same rank and hyperparameters
  2. Measure forgetting on a held-out pretraining distribution during fine-tuning
  3. Analyze subspace similarity between weight updates and original matrix for LoRA vs MiLoRA

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal rank selection strategy for MiLoRA across different task types and model scales?
- **Basis in paper**: [inferred] The paper mentions that "MiLoRA does not require any hyperparameter except the rank r" and shows results using ranks of 32 and 64 for different tasks, but doesn't explore systematic rank selection.
- **Why unresolved**: The paper uses fixed ranks based on task complexity (32 for most tasks, 64 for math reasoning with large datasets) without exploring whether this is optimal or task-specific.
- **What evidence would resolve it**: Systematic experiments varying rank values across multiple task types and model sizes to identify patterns or rules for rank selection.

### Open Question 2
- **Question**: How does MiLoRA's performance compare to other PEFT methods when applied to non-Transformer architectures or non-LLM models?
- **Basis in paper**: [inferred] All experiments are conducted on Transformer-based LLMs and vision-language models, with no exploration of other architectures.
- **Why unresolved**: The paper focuses exclusively on LLMs and LLaVA-1.5, leaving uncertainty about MiLoRA's effectiveness on other model types like CNNs, RNNs, or vision-only models.
- **What evidence would resolve it**: Experiments applying MiLoRA to non-Transformer architectures and comparing performance with LoRA and other PEFT methods on equivalent tasks.

### Open Question 3
- **Question**: What is the theoretical relationship between the decay rate of singular values and the effectiveness of MiLoRA across different layers and model components?
- **Basis in paper**: [explicit] The paper observes that "minor matrix corresponds to the noisy or long-tail information" and shows that MiLoRA performs better than baselines, but doesn't analyze the singular value spectrum.
- **Why unresolved**: While the paper demonstrates empirical success, it doesn't investigate whether the effectiveness of MiLoRA correlates with specific patterns in the singular value distribution of weight matrices.
- **What evidence would resolve it**: Analysis of singular value spectra across different layers and components, correlating decay patterns with MiLoRA's performance improvements compared to other methods.

## Limitations
- The assumption that minor singular components capture "noisy or long-tail information" is not empirically validated across different architectures
- Computational overhead from SVD decomposition is mentioned but not quantified for very large models
- Missing ablation study on the optimal threshold for separating principal and minor components

## Confidence
- **High Confidence**: Experimental results showing consistent performance improvements across multiple tasks and datasets with statistical significance measures
- **Medium Confidence**: Theoretical justification for why minor components should be less critical for pretrained knowledge is plausible but not rigorously proven
- **Low Confidence**: Assertion that MiLoRA reduces catastrophic forgetting lacks direct measurement and comparison metrics

## Next Checks
1. Perform an ablation study testing different thresholds for separating principal and minor components (e.g., 0.001, 0.01, 0.05) to determine if the 0.01 threshold is optimal
2. Measure catastrophic forgetting directly by evaluating model performance on held-out pretraining distribution examples during fine-tuning, comparing MiLoRA against standard LoRA
3. Test MiLoRA's performance when applied to different layer types (attention matrices vs. MLP matrices) separately to determine if effectiveness varies by layer architecture