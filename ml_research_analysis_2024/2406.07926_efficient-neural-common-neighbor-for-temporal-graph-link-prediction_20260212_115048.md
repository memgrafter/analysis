---
ver: rpa2
title: Efficient Neural Common Neighbor for Temporal Graph Link Prediction
arxiv_id: '2406.07926'
source_url: https://arxiv.org/abs/2406.07926
tags:
- temporal
- tncn
- graph
- prediction
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles temporal link prediction by introducing Temporal
  Neural Common Neighbor (TNCN), which builds on a memory-based backbone and extends
  the Neural Common Neighbor (NCN) framework. The key insight is that link prediction
  is inherently pairwise, so TNCN explicitly captures common neighbors across multiple
  temporal hops.
---

# Efficient Neural Common Neighbor for Temporal Graph Link Prediction

## Quick Facts
- arXiv ID: 2406.07926
- Source URL: https://arxiv.org/abs/2406.07926
- Reference count: 40
- Primary result: State-of-the-art performance on 6/7 transductive and 3/7 inductive datasets, plus 1.8-30.3× speedup over graph-based methods

## Executive Summary
This paper introduces Temporal Neural Common Neighbor (TNCN), a memory-based framework for temporal link prediction that efficiently captures multi-hop common neighbors across time. TNCN extends the Neural Common Neighbor approach by using sparse tensor operations to extract arbitrary (i,j)-hop common neighbors without repeated subgraph computations. The method achieves state-of-the-art results on the Temporal Graph Benchmark while offering significant computational advantages over existing approaches.

## Method Summary
TNCN builds on a memory-based backbone that incrementally updates node embeddings with each event, avoiding repeated subgraph computations. It extends Neural Common Neighbor by efficiently extracting multi-hop common neighbors using sparse tensor operations. The model captures zero-hop through k-hop common neighbors for each node pair, aggregates their embeddings with sum pooling, and combines them with pairwise node representations through concatenation and MLP projection. This design enables TNCN to process each event once while retaining long-term temporal dependencies.

## Key Results
- Achieves state-of-the-art performance on 6/7 transductive and 3/7 inductive small/medium datasets
- Matches state-of-the-art on Review dataset in large-scale TGB evaluation
- Provides 1.8-30.3× speedup in inference over graph-based models while maintaining comparable speed to memory-based methods
- Ablation study confirms multi-hop and zero-hop common neighbors are critical for performance

## Why This Works (Mechanism)

### Mechanism 1
TNCN captures higher-order structural patterns by explicitly modeling multi-hop common neighbors across time. For each node pair (u,v), it extracts (i,j)-hop common neighbors from up to K temporal hops, aggregates their embeddings with sum pooling, and concatenates these with the pair-wise node embeddings. Core assumption: Common neighbor patterns—including zero-hop and higher-order hops—are discriminative for link prediction in temporal graphs.

### Mechanism 2
Memory-based backbone enables TNCN to process each event once while retaining long-term dependencies. TNCN updates node memory incrementally with messages from each event, avoiding repeated subgraph computations. Core assumption: Temporal link formation depends on both recent and historical interaction patterns, and memory updates are sufficient to encode these dependencies.

### Mechanism 3
Sparse tensor operations enable efficient extraction of arbitrary (i,j)-hop common neighbors without repeated graph traversals. TNCN builds a sparse adjacency tensor, uses self-multiplication to generate higher-order connectivity, and applies sparse hadamard product to extract multi-hop common neighbors in batch. Core assumption: The temporal graph is sparse enough for efficient sparse tensor representation, and GPU-accelerated tensor ops can outperform iterative neighbor enumeration.

## Foundational Learning

- Concept: Temporal graph representation learning with memory-based backbones
  - Why needed here: TNCN relies on incremental memory updates to encode evolving node states without recomputing full subgraphs
  - Quick check question: How does a memory update differ from a static GNN message passing step in terms of time and space complexity?

- Concept: Multi-hop common neighbor heuristics (CN, RA, AA)
  - Why needed here: TNCN generalizes these static link prediction heuristics into learnable neural embeddings across temporal hops
  - Quick check question: What is the difference between (1,1)-hop CN and (0,1)-hop CN in a temporal setting?

- Concept: Sparse tensor algebra for graph computations
  - Why needed here: TNCN uses sparse tensor multiplication and hadamard product to extract multi-hop common neighbors efficiently
  - Quick check question: Why is sparse tensor multiplication preferred over dense matrix multiplication for large-scale temporal graphs?

## Architecture Onboarding

- Component map: Event -> Memory update -> CN extraction -> Pair-wise representation -> MLP prediction
- Critical path: Each event triggers memory update, followed by CN extraction using sparse tensors, then combination with pairwise embeddings for final prediction
- Design tradeoffs: Memory-based backbone offers speed but may lose expressiveness compared to subgraph methods; sparse tensor ops trade exactness for scalability
- Failure signatures: Poor performance on non-bipartite high-surprise datasets; memory explosion if K is too large; tensor sparsity loss in dense graphs
- First 3 experiments:
  1. Replace sparse CN extraction with naive neighbor enumeration on a small dataset and compare runtime and accuracy
  2. Vary K (max hop count) and measure impact on performance and memory usage
  3. Remove multi-hop CN module and measure change in link prediction accuracy to confirm its contribution

## Open Questions the Paper Calls Out

### Open Question 1
Can TNCN effectively handle non-bipartite temporal graphs with high surprise values, such as the Comment dataset, without significant performance degradation? The paper identifies this as a limitation but does not provide a solution or modification to address this issue.

### Open Question 2
How does the performance of TNCN scale with the size of the temporal graph, particularly in terms of the number of nodes and edges, beyond the tested TGB datasets? The scalability analysis is limited to the TGB datasets, and the paper does not discuss performance on graphs with millions of nodes and edges.

### Open Question 3
What is the impact of different time encoding schemes on TNCN's performance, and can alternative encodings further enhance its ability to capture temporal patterns? The paper uses Time2Vec but does not explore other options or their impact on performance.

## Limitations

- TNCN struggles with non-bipartite datasets with high surprise values where prior events have diminished correlation with subsequent events
- Memory-based backbone may struggle with highly non-linear temporal patterns where historical context becomes irrelevant quickly
- Sparse tensor operations lose efficiency advantage in dense graphs or with extremely high hop counts

## Confidence

**High Confidence**: TNCN's core mechanism of using sparse tensor operations for efficient multi-hop CN extraction is well-supported by the ablation study and computational analysis. The claim that TNCN achieves state-of-the-art performance on 6/7 transductive datasets is substantiated by the experimental results.

**Medium Confidence**: The claim about TNCN's scalability advantage over subgraph-based methods is supported by inference speed comparisons, but the memory-based backbone's long-term effectiveness across diverse temporal patterns remains under-validated.

**Low Confidence**: The assertion that TNCN maintains comparable speed to memory-based methods while offering significant accuracy improvements requires additional validation, particularly on larger TGB datasets.

## Next Checks

1. **Hop Count Sensitivity Analysis**: Systematically vary K (maximum hop count) across 1-5 hops on all TGB datasets to quantify the accuracy-speed trade-off and identify optimal hop ranges for different graph types.

2. **Memory Decay Experiment**: Modify the GRU updater to include explicit memory decay mechanisms and compare performance on datasets with varying temporal dynamics to validate the memory backbone's adaptability.

3. **Dense Graph Stress Test**: Evaluate TNCN on artificially densified versions of TGB datasets to assess tensor sparsity retention and determine the computational break-even point with subgraph-based methods.