---
ver: rpa2
title: Simulation-Based Benchmarking of Reinforcement Learning Agents for Personalized
  Retail Promotions
arxiv_id: '2405.10469'
source_url: https://arxiv.org/abs/2405.10469
tags:
- agents
- customer
- agent
- training
- customers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks reinforcement learning (RL) agents for personalized
  coupon targeting in retail using simulated customer shopping data. The environment
  simulates customer purchase behavior and trains agents offline on batch data to
  target store-wide coupons, measuring revenue uplift and customer retention.
---

# Simulation-Based Benchmarking of Reinforcement Learning Agents for Personalized Retail Promotions

## Quick Facts
- arXiv ID: 2405.10469
- Source URL: https://arxiv.org/abs/2405.10469
- Reference count: 33
- Primary result: Contextual bandits and PPO significantly outperform static policies for personalized coupon targeting in simulated retail environments

## Executive Summary
This paper benchmarks reinforcement learning (RL) agents for personalized coupon targeting in retail using simulated customer shopping data. The environment simulates customer purchase behavior and trains agents offline on batch data to target store-wide coupons, measuring revenue uplift and customer retention. Experiments compare static policies against contextual bandits (LinTS, LinUCB, NB) and deep RL (PPO, DQN), showing contextual bandits and PPO significantly outperform static policies. Analysis reveals agents mostly target 50% off coupons to price-sensitive customers and 0% to price-insensitive ones, though some non-optimal coupons are still offered. The study provides a practical framework for simulating AI agents that optimize the full retail customer journey.

## Method Summary
The study creates a simulation environment using RetailSynth to model customer shopping behavior and train RL agents offline on batch data. The environment simulates 100,000 customers over 50 time steps, generating sparse transaction data that is summarized into customer features like price sensitivity and purchase history. Agents (LinTS, LinUCB, NB, PPO, DQN) are trained on this data to target store-wide coupons from a discrete action space (0%-50% discounts). Performance is evaluated by resuming simulation and measuring revenue uplift, customer retention, and category penetration over 20 time steps.

## Key Results
- Contextual bandits (LinTS, LinUCB) and PPO significantly outperform static coupon policies in terms of revenue and retention
- DQN underperforms other methods, likely due to overfitting sparse reward distributions
- Agents learn to assign higher discounts to price-sensitive customers and minimal discounts to price-insensitive ones
- Some non-optimal coupon assignments persist, suggesting room for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offline batch training with sparse transaction data can still yield RL agents that outperform static coupon policies.
- Mechanism: The environment simulates realistic customer shopping behavior using a mechanistic four-stage decision model. Even though transactions are sparse, the simulation accumulates rich trajectories that include customer features (e.g., price sensitivity, category purchase history) which the agents use to learn personalized coupon strategies. Contextual bandits and PPO exploit the linear reward structure, learning to assign higher discounts to price-sensitive customers and minimal discounts to price-insensitive ones.
- Core assumption: The synthetic data generated by RetailSynth accurately reflects real-world customer behavior and retains causal relationships between features and rewards.
- Evidence anchors:
  - [abstract]: "Our experiments revealed that contextual bandit and deep RL methods that are less prone to over-fitting the sparse reward distributions significantly outperform static policies."
  - [section]: "Based on our observation that price-insensitive customers still often receive large discounts, there do appear to be opportunities to improve agent performance on this benchmark."
  - [corpus]: Missing direct evidence; related works focus on trajectory generation and customer analytics, not specifically on coupon-targeting RL benchmarks.
- Break condition: If the simulation fails to capture true customer heterogeneity, or if the feature engineering misses key predictive signals, agents will not generalize to real-world deployments.

### Mechanism 2
- Claim: Simple linear contextual bandits perform as well as or better than deep RL methods on this task.
- Mechanism: The reward structure in this environment is largely linear with respect to customer features. LinTS and LinUCB efficiently explore and exploit this structure without overfitting sparse rewards. Deep RL methods like DQN overfit because they attempt to model a sparse, noisy reward signal with complex neural networks.
- Core assumption: The reward-to-action mapping is approximately linear and low-dimensional.
- Evidence anchors:
  - [abstract]: "Our experiments revealed that contextual bandit and deep RL methods that are less prone to over-fitting the sparse reward distributions significantly outperform static policies."
  - [section]: "Our results indicate that all the agents, except DQN, effectively learn to target coupons more effectively than a random policy."
  - [corpus]: Weak; no direct evidence about linear reward structures, but one related work uses RL for personalized simulation, suggesting simpler models can be effective.
- Break condition: If customer response to coupons becomes highly non-linear or high-dimensional, linear methods will underfit and deep RL may regain advantage.

### Mechanism 3
- Claim: Feature engineering and summarizing sparse transaction histories into customer-level features is critical for agent performance.
- Mechanism: Raw transaction data is sparse; summarizing into features like average purchase price, discount history, and category penetration gives the agents richer context to base decisions on. Mutual information scores confirm these features are predictive of revenue outcomes.
- Core assumption: Engineered features capture the essential latent state (e.g., price sensitivity) that determines customer response to coupons.
- Evidence anchors:
  - [section]: "We worked with a modest feature space to minimize the computational overhead of training and evaluating multiple agents... We prepared aggregated features from the transactional data and marketing activity logs, such as average purchase price, average purchase discount, etc."
  - [section]: "To verify the relevance of these features, we computed using mutual information scores... and verified the information content was non-negligible."
  - [corpus]: Missing; related works focus on trajectory generation but not feature engineering for RL agents.
- Break condition: If the feature space omits important predictive signals or overfits to simulation quirks, agents will not transfer to real-world deployments.

## Foundational Learning

- Concept: Sparse reward learning in RL
  - Why needed here: Customer transactions are infrequent; agents must learn from delayed and sparse reward signals without overfitting.
  - Quick check question: Why does DQN underperform PPO and contextual bandits in this benchmark?

- Concept: Contextual bandit algorithms (LinTS, LinUCB)
  - Why needed here: These methods efficiently handle the exploration-exploitation tradeoff in a linear reward setting and are less prone to overfitting sparse rewards.
  - Quick check question: What is the key difference between LinTS and LinUCB in how they balance exploration and exploitation?

- Concept: Reinforcement learning policy gradient methods (PPO)
  - Why needed here: PPO can optimize cumulative revenue over trajectories, not just immediate rewards, which is useful in this multi-step coupon targeting task.
  - Quick check question: How does PPO's use of a value function help stabilize training compared to REINFORCE?

## Architecture Onboarding

- Component map:
  - RetailSynth customer decision model (4-stage choice model) -> Simulation environment (state transitions, coupon application, reward calculation) -> Feature engineering pipeline (summarizing transactions into customer features) -> Agent training loop (offline batch learning) -> Evaluation loop (resume simulation, measure revenue, retention, category penetration)
- Critical path:
  1. Generate offline dataset via random coupon policy
  2. Engineer features from customer histories
  3. Train agents (contextual bandits or PPO) on the dataset
  4. Evaluate agents by resuming simulation and measuring key metrics
- Design tradeoffs:
  - Simpler models (LinTS, LinUCB) are faster to train and less prone to overfitting but may miss non-linear effects
  - Deep RL (PPO) can model more complex relationships but requires more data and careful hyperparameter tuning
  - Feature engineering trades off manual effort for better agent performance and interpretability
- Failure signatures:
  - Agents assign non-optimal coupons (e.g., 50% off to price-insensitive customers) → feature engineering or model misspecification
  - No improvement over static policy → environment or reward function not properly calibrated
  - High variance in performance across runs → insufficient training data or unstable hyperparameters
- First 3 experiments:
  1. Train LinTS and LinUCB with default hyperparameters on a small synthetic dataset; compare revenue lift vs static policy
  2. Vary the size of the training dataset (e.g., 10k vs 100k customers) and measure impact on agent performance
  3. Test a simple feature ablation study: remove engineered features one by one and observe impact on agent performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of RL agents change if the product catalog size were increased back to the original 26,176 products?
- Basis in paper: [inferred] The paper reduced the product catalog from 26,176 to 2,514 products to increase purchase frequency and manage computational costs. The original larger catalog likely created more sparse rewards due to fewer purchases per product.
- Why unresolved: The paper only tested with the reduced catalog size and did not explore the impact of catalog size on agent performance. Sparse rewards from a larger catalog could affect the performance of different RL algorithms differently.
- What evidence would resolve it: Re-running the experiments with the original catalog size and comparing the performance metrics (revenue, retention, category penetration) across different RL algorithms would show how catalog size affects agent learning and optimization.

### Open Question 2
- Question: Would using more sophisticated customer feature engineering improve the performance of neural-based RL agents like PPO and DQN?
- Basis in paper: [explicit] The paper notes that PPO and DQN agents performed poorly, possibly due to overfitting to sparse reward distributions and the limited feature space used. The authors mention that richer customer features might lead to improved performance.
- Why unresolved: The experiments used a modest feature space to minimize computational overhead. The impact of more complex feature engineering on neural-based agents' ability to handle sparse rewards and improve performance is untested.
- What evidence would resolve it: Implementing advanced feature engineering techniques (e.g., interaction features, embeddings) and retraining the neural-based agents to compare their performance against the baseline results would determine if richer features enhance their effectiveness.

### Open Question 3
- Question: How would the agents' performance be affected if the collection policy used for training data was not random but a learned policy?
- Basis in paper: [inferred] The paper used a random collection policy to gather training data, which the authors note is a relatively strong policy. The presence of pre-existing learning systems in real-world retail environments suggests that agents might be trained on data from non-random, learned policies.
- Why unresolved: The impact of training agents on data generated by learned policies versus random policies is not explored. A learned collection policy might introduce biases or correlations that affect how agents learn to optimize coupon targeting.
- What evidence would resolve it: Conducting experiments where agents are trained on data collected by a learned policy (instead of random) and comparing their performance metrics to those trained on random data would reveal the effects of different data collection strategies on agent learning.

## Limitations

- The simulation environment may not fully capture real-world customer behavior and reward structures
- Feature engineering relies on domain expertise and may miss important predictive signals
- Results are based on synthetic data; real-world validation is needed to confirm generalizability

## Confidence

- **High confidence**: The relative performance ranking of agents (contextual bandits and PPO outperforming static policies and DQN) within the simulation environment
- **Medium confidence**: The generalizability of these findings to real-world retail settings without further validation
- **Medium confidence**: The claim that simple linear methods outperform deep RL due to sparse rewards, as this is context-dependent

## Next Checks

1. **Real-world validation**: Test the trained agents on actual customer transaction data from a retail partner to measure performance lift beyond the simulation environment
2. **Feature importance analysis**: Conduct ablation studies removing engineered features one-by-one to quantify their contribution to agent performance and identify potential overfitting
3. **Reward structure sensitivity**: Modify the simulation's reward function to include non-linear components and observe whether contextual bandits maintain their performance advantage over deep RL methods