---
ver: rpa2
title: Robust Multi-bit Text Watermark with LLM-based Paraphrasers
arxiv_id: '2412.03123'
source_url: https://arxiv.org/abs/2412.03123
tags:
- text
- watermark
- will
- paraphrasing
- decoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a robust multi-bit text watermarking method
  using LLM-based paraphrasing. The approach fine-tunes a pair of LLM paraphrasers
  to behave differently, enabling watermark embedding by alternating between them
  at the sentence level.
---

# Robust Multi-bit Text Watermark with LLM-based Paraphrasers

## Quick Facts
- arXiv ID: 2412.03123
- Source URL: https://arxiv.org/abs/2412.03123
- Reference count: 15
- Over 99.99% detection AUC with small (1.1B) models while maintaining semantic fidelity

## Executive Summary
This paper introduces a novel multi-bit text watermarking method that leverages LLM-based paraphrasing to embed imperceptible watermarks. The approach fine-tunes a pair of LLM paraphrasers to behave differently, enabling watermark embedding by alternating between them at the sentence level. A trained LLM classifier then decodes the watermark, achieving exceptional detection accuracy while preserving text quality. The method demonstrates robustness against common text attacks and generalizes well to out-of-distribution data.

## Method Summary
The method uses two LLM paraphrasers fine-tuned to generate semantically similar but distinguishable text. These paraphrasers alternate at the sentence level to encode watermark bits, with a text classifier trained to decode the watermark. The system employs a co-training framework using PPO optimization, where the decoder acts as a reward model for encoder fine-tuning. Text is segmented into sentences, with each sentence corresponding to one watermark bit, providing a robust injection strategy resistant to word substitutions and paraphrasing attacks.

## Key Results
- Achieves >99.99% detection AUC with 1.1B parameter models
- Maintains high semantic similarity (low KL divergence) with original text
- Demonstrates robustness against word substitution and sentence paraphrasing attacks
- Generalizes well to out-of-distribution text data

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning two LLM paraphrasers to generate distinguishable outputs while maintaining semantic similarity creates a reliable encoding channel. The paraphrasers learn to produce texts with different semantic patterns that can be reliably classified.

### Mechanism 2
The co-training framework creates a virtuous cycle where encoder and decoder improve each other's performance through alternating optimization. The decoder provides reward signals for encoder training via PPO, leading to mutual improvement.

### Mechanism 3
Sentence-level segmentation provides robust watermark injection that survives common text perturbations. Word substitutions and paraphrasing attacks that don't significantly alter sentence structure cannot easily remove the watermark.

## Foundational Learning

- Concept: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - Why needed here: PPO is used to fine-tune the encoder paraphrasers based on rewards from the decoder classifier
  - Quick check question: How does PPO differ from standard supervised fine-tuning when optimizing language models for tasks where rewards are provided by another model?

- Concept: Text classification and semantic similarity metrics
  - Why needed here: The decoder must accurately classify between two watermark classes while maintaining semantic similarity
  - Quick check question: What are the key challenges in training a classifier to distinguish between two semantically similar text classes?

- Concept: Sentence segmentation and tokenization
  - Why needed here: The system relies on dividing text into segments at the sentence level for watermark embedding
  - Quick check question: How might different sentence segmentation strategies affect the robustness and capacity of the watermarking system?

## Architecture Onboarding

- Component map: Input text → Encoder (two paraphrasers θ0, θ1) → Watermarked text → Decoder (classifier θd) → Decoded watermark bits
- Critical path: Text generation → Segmentation → Classification → Bit extraction
- Design tradeoffs: Model size vs. performance, bit rate vs. stealth, robustness vs. fidelity
- Failure signatures: Low AUC scores, low bit accuracy, low similarity scores, training instability
- First 3 experiments:
  1. Train encoder and decoder on a small dataset with fixed watermark bits, verify basic functionality
  2. Test robustness against word substitution attacks with varying substitution ratios
  3. Evaluate performance on out-of-distribution text to assess generalization

## Open Questions the Paper Calls Out

- How does watermark detection performance scale with text length beyond 128 tokens?
- How robust is the system against sophisticated adversarial attacks targeting semantic preservation?
- What is the impact of using different LLM architectures beyond the evaluated 1.1B and 7B models?
- How does performance generalize to domain-specific text significantly different from C4 training data?
- Can the system support dynamic watermark codes that change based on contextual factors?

## Limitations

- The sentence-level segmentation strategy may limit watermark capacity compared to finer-grained approaches
- Evaluation uses relatively simple attack models that may not reflect sophisticated adversarial techniques
- Limited assessment of real-world adversarial scenarios beyond controlled attacks
- Questions about scalability and effectiveness with larger, more capable models remain

## Confidence

- High Confidence: The core mechanism of using two fine-tuned paraphrasers with distinguishable semantic patterns
- Medium Confidence: Robustness claims against word substitution and paraphrasing attacks
- Low Confidence: Stealthiness evaluation methodology and sample outputs

## Next Checks

1. Test watermarking performance with larger model sizes (7B-70B parameters) to assess scalability
2. Conduct experiments with more sophisticated adversarial attacks, including context-aware paraphrasing
3. Perform ablation studies to quantify the contribution of each component (PPO fine-tuning, JS divergence loss, sentence segmentation)