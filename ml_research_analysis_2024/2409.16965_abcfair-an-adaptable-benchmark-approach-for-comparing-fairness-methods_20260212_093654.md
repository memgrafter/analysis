---
ver: rpa2
title: 'ABCFair: an Adaptable Benchmark approach for Comparing Fairness Methods'
arxiv_id: '2409.16965'
source_url: https://arxiv.org/abs/2409.16965
tags:
- fairness
- methods
- sensitive
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ABCFair, a benchmark approach for comparing
  fairness methods that addresses the challenge of differing problem settings across
  methods. The approach adapts to real-world desiderata through flexible components
  for data, fairness methods, and evaluation.
---

# ABCFair: an Adaptable Benchmark approach for Comparing Fairness Methods

## Quick Facts
- arXiv ID: 2409.16965
- Source URL: https://arxiv.org/abs/2409.16965
- Authors: MaryBeth Defrance; Maarten Buyl; Tijl De Bie
- Reference count: 40
- Primary result: Benchmark approach enabling fair comparison of fairness methods across different problem settings using dual-label datasets and flexible evaluation components

## Executive Summary
This paper introduces ABCFair, a comprehensive benchmark approach for comparing fairness methods in machine learning that addresses the challenge of differing problem settings across methods. The approach features flexible components for data handling, fairness method application, and evaluation, validated on 10 methods, 7 fairness notions, 3 sensitive feature formats, and 2 output distributions. A key innovation is the use of dual-label datasets that contain both biased and unbiased labels, enabling evaluation of both traditional and unbiased accuracy. The benchmark reveals that preprocessing methods struggle to achieve low fairness violations compared to in-processing methods, and that more granular sensitive features lead to larger fairness violations.

## Method Summary
ABCFair provides a standardized framework for comparing fairness methods through three main components: Dataset (handling different sensitive feature formats and data loading), FairnessMethod (supporting pre-, in-, and post-processing approaches), and Evaluator (measuring multiple fairness notions across output formats). The benchmark is validated on real-world datasets including SchoolPerformance (856 samples with dual labels) and ACS folktables (1.1M-3.2M samples), using neural networks with 2-4 hidden layers and various fairness methods. Key innovations include dual-label evaluation that reveals traditional fairness-accuracy trade-offs disappear when measuring unbiased accuracy, and comprehensive evaluation across multiple fairness notions showing preprocessing limitations.

## Key Results
- Experiments on dual-label datasets reveal that methods performing well on traditional fairness-accuracy trade-offs actually perform worse on unbiased labels
- Preprocessing methods optimize for specific fairness notions more efficiently than in-processing methods, but in-processing methods improve all fairness notions simultaneously
- More granular sensitive features lead to larger fairness violations, and preprocessing methods struggle to achieve low fairness violations across all notions
- The benchmark enables fair comparison across methods designed for different problem settings through standardized evaluation components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual-label dataset enables unbiased evaluation of fairness methods by training on biased labels while measuring accuracy on unbiased labels.
- Mechanism: When models are trained on human-annotated (biased) labels but evaluated on actual outcomes (less biased), the traditional accuracy-fairness trade-off disappears and transforms into a synergy where higher fairness leads to higher unbiased accuracy.
- Core assumption: Human-annotated labels contain systematic bias that differs from the true outcomes being predicted.
- Evidence anchors:
  - [abstract]: "Experiments on a dual-label dataset reveal that methods performing well on the traditional fairness-accuracy trade-off actually perform worse on unbiased labels"
  - [section]: "The SchoolPerformance dataset [38], which is a dual label: it contains both biased and (less) unbiased labels. Biased labels were gathered by asking humans to predict school outcomes... These labels are assumed to be far more biased than the actual outcomes"
  - [corpus]: Weak - corpus neighbors discuss fairness methods but don't specifically address dual-label evaluation methodology
- Break condition: If the bias in human-annotated labels is minimal or if the unbiased labels themselves contain residual bias that wasn't accounted for.

### Mechanism 2
- Claim: ABCFair's configurable pipeline addresses comparability challenges by standardizing how fairness methods are evaluated across different problem settings.
- Mechanism: By providing flexible components for Data (handling different sensitive feature formats), FairnessMethod (supporting pre-, in-, and post-processing approaches), and Evaluator (measuring multiple fairness notions across output formats), the benchmark creates a common evaluation framework that enables fair comparison between methods designed for different desiderata.
- Core assumption: Different fairness methods are optimized for different problem settings, making direct comparison impossible without a standardized framework.
- Evidence anchors:
  - [abstract]: "The approach adapts to real-world desiderata through flexible components for data, fairness methods, and evaluation"
  - [section]: "To address the challenges raised in Sec. 2, we introduce a new benchmarking approach: ABCFair (Adaptable Benchmark for Comparing Fairness methods), that helps in both establishing qualitative comparability across methods and performing a quantitative evaluation"
  - [corpus]: Weak - corpus neighbors discuss fairness methods but don't address benchmarking challenges or standardization approaches
- Break condition: If the standardized framework cannot accurately capture the nuances of specific fairness methods or if methods require fundamentally incompatible evaluation approaches.

### Mechanism 3
- Claim: The benchmark's comprehensive evaluation across multiple fairness notions reveals that preprocessing methods struggle to achieve low fairness violations compared to in-processing methods.
- Mechanism: By evaluating methods across seven fairness notions (demographic parity, equalized opportunity, predictive equality, predictive parity, false omission rate parity, accuracy equality, F1-score equality) and three sensitive feature formats, the benchmark exposes limitations of preprocessing approaches that cannot directly optimize for all fairness notions.
- Core assumption: Different fairness notions capture different aspects of bias, and methods optimized for one notion may not perform well on others.
- Evidence anchors:
  - [abstract]: "Experiments on a dual-label dataset reveal that methods performing well on the traditional fairness-accuracy trade-off actually perform worse on unbiased labels"
  - [section]: "Key Finding 4: Preprocessing methods optimize for specific a fairness notion more efficiently than inprocessing, but those improve all fairness notions at once by improving on one"
  - [corpus]: Weak - corpus neighbors don't discuss comprehensive evaluation across multiple fairness notions or preprocessing limitations
- Break condition: If the fairness notions measured don't capture the relevant aspects of bias for the application domain or if preprocessing methods can be adapted to optimize for multiple notions simultaneously.

## Foundational Learning

- Concept: Fairness notions and their mathematical definitions (demographic parity, equalized opportunity, predictive equality, etc.)
  - Why needed here: The benchmark evaluates methods across seven different fairness notions, each with specific mathematical definitions that determine how bias is measured and what constraints methods optimize for
  - Quick check question: What is the key mathematical difference between demographic parity and equalized opportunity in terms of what statistics they require to be equal across groups?

- Concept: Sensitive feature formats (binary, categorical, parallel) and their implications for bias measurement
  - Why needed here: The benchmark supports three different sensitive feature formats, each affecting how bias is measured and what group comparisons are made, which directly impacts method comparability
  - Quick check question: How does measuring fairness with categorical sensitive features differ from binary features in terms of the number of group comparisons and statistical power?

- Concept: Pre-processing, in-processing, and post-processing fairness intervention stages
  - Why needed here: The benchmark includes methods from all three intervention stages, each with different requirements for data access and different mechanisms for bias mitigation, which affects their applicability and performance
  - Quick check question: What are the practical limitations of post-processing methods compared to pre-processing methods in terms of data access and privacy concerns?

## Architecture Onboarding

- Component map: Dataset -> FairnessMethod -> Evaluator
- Critical path: The typical execution flow is: 1) Initialize Dataset with appropriate sensitive feature format and data loading parameters, 2) Create and train a neural network model, 3) Apply FairnessMethod to modify data, training loop, or model output, 4) Use Evaluator to measure performance and fairness violations on validation/test data, 5) Generate comparison tables or trade-off plots.
- Design tradeoffs: The benchmark prioritizes flexibility and comparability over simplicity - supporting multiple sensitive feature formats and fairness notions adds complexity but enables more comprehensive comparisons. The choice to use soft scores for some methods versus hard decisions for others reflects real-world applicability versus theoretical purity.
- Failure signatures: Common failure modes include: methods not converging due to incompatible fairness strengths, evaluation metrics not being computed correctly for certain sensitive feature formats, and performance degradation when fairness constraints are too strict. The benchmark's modular design helps isolate these issues to specific components.
- First 3 experiments:
  1. Run the naive baseline (no fairness method) on ACSPublicCoverage dataset with binary sensitive features and demographic parity evaluation to establish baseline performance and fairness violation levels
  2. Apply Error Parity post-processing method on the same dataset to verify the benchmark's ability to handle post-processing approaches and compare performance to the baseline
  3. Switch to categorical sensitive features and repeat experiment 2 to test the framework's handling of different sensitive feature formats and observe any changes in fairness violations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific fairness notions (beyond demographic parity and equalized opportunity) are most effective at mitigating bias in real-world datasets?
- Basis in paper: [explicit] The paper mentions 7 fairness notions but focuses primarily on demographic parity and equalized opportunity, suggesting other notions may warrant further investigation.
- Why unresolved: The paper only briefly mentions additional fairness notions (predictive equality, predictive parity, false omission rate parity, accuracy equality, F1-score equality) without providing detailed comparative analysis.
- What evidence would resolve it: Comprehensive experimental results comparing the effectiveness of all 7 fairness notions across different datasets and sensitive feature formats.

### Open Question 2
- Question: How do fairness violations scale with the granularity of sensitive feature intersections in practical applications?
- Basis in paper: [explicit] The paper notes that more granular sensitive features lead to larger fairness violations but does not provide quantitative analysis of this relationship.
- Why unresolved: The paper only observes this phenomenon qualitatively through comparisons between binary, intersectional, and parallel formats without measuring the exact scaling behavior.
- What evidence would resolve it: Systematic experiments measuring fairness violations across different levels of sensitive feature granularity on multiple datasets.

### Open Question 3
- Question: What is the optimal strategy for determining fairness strength hyperparameters across different fairness methods?
- Basis in paper: [explicit] The paper notes that fairness methods have different hyperparameter scales and manually selected additional strengths, indicating this is an open challenge.
- Why unresolved: The paper uses manual selection for additional strengths and does not propose an automated or principled approach for hyperparameter tuning across different fairness methods.
- What evidence would resolve it: Development and validation of an automated hyperparameter selection method that consistently finds optimal fairness strengths across different methods and datasets.

## Limitations

- The dual-label evaluation approach relies on the assumption that human-annotated labels are systematically biased compared to actual outcomes, but the magnitude and nature of this bias across different domains remains unclear
- The benchmark's comprehensive evaluation across multiple fairness notions may not capture all relevant fairness concerns in specific application domains, potentially leading to overgeneralization of results
- The scalability of the benchmark to very large datasets and its computational requirements for training multiple methods across various configurations is not fully characterized

## Confidence

- High confidence in the methodological framework design and implementation, given the detailed component specifications and clear validation procedure
- Medium confidence in the dual-label evaluation results, as they depend on the specific characteristics of the SchoolPerformance dataset and may not generalize to other domains
- Medium confidence in the comparative results across fairness notions, as preprocessing methods' limitations may be dataset-specific rather than universal

## Next Checks

1. Replicate the dual-label evaluation on a different domain (e.g., healthcare or criminal justice) to test generalizability of the finding that traditional fairness-accuracy trade-offs disappear when using unbiased labels
2. Evaluate the benchmark's performance on a dataset with millions of samples to assess computational scalability and identify potential bottlenecks in the evaluation pipeline
3. Test whether the identified limitations of preprocessing methods hold when they are combined with in-processing techniques or when optimized for multiple fairness notions simultaneously