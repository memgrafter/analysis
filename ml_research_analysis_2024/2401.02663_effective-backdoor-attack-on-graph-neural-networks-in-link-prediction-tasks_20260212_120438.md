---
ver: rpa2
title: Effective backdoor attack on graph neural networks in link prediction tasks
arxiv_id: '2401.02663'
source_url: https://arxiv.org/abs/2401.02663
tags:
- node
- backdoor
- trigger
- attack
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a backdoor attack on Graph Neural Networks
  (GNNs) for link prediction tasks. The attack embeds a backdoor trigger into GNN
  models by poisoning the training data, causing the models to incorrectly predict
  links between two unlinked nodes when the trigger is present.
---

# Effective backdoor attack on graph neural networks in link prediction tasks

## Quick Facts
- arXiv ID: 2401.02663
- Source URL: https://arxiv.org/abs/2401.02663
- Authors: Jiazhu Dai; Haoyu Sun
- Reference count: 40
- Primary result: Achieves over 89% ASR with less than 1% BPD and 1% poisoning rate in black-box link prediction attack

## Executive Summary
This paper presents a novel backdoor attack targeting Graph Neural Networks (GNNs) for link prediction tasks. The attack embeds a stealthy trigger into the model by poisoning training data, causing the model to incorrectly predict links between specific unlinked nodes when the trigger is present. The method uses a single node as the trigger, generated based on statistical information of node features, and selects unlinked node pairs with sparse features for poisoning. Experimental results demonstrate high attack success rates (over 89%) with minimal impact on benign performance (less than 1% accuracy loss) and low poisoning rates (about 1%) in black-box scenarios.

## Method Summary
The proposed backdoor attack generates a trigger node by identifying feature dimensions with the least occurrence frequency in the training data and setting those features to 1. It then selects unlinked node pairs for poisoning using a scoring function based on the sum of absolute feature values. The attack injects the backdoor by connecting the trigger node to these selected node pairs and changing their link state to linked. GNN models (GAE, VGAE, ARGA, ARVGA) are then trained on this poisoned data. The attack operates in a black-box scenario where the attacker has access to the training data but not the model architecture or parameters.

## Key Results
- Achieves over 89% attack success rate (ASR) in black-box scenario
- Maintains less than 1% benign performance drop (BPD) on clean samples
- Uses only about 1% poisoning rate (p) of training samples
- Outperforms baseline methods in both attack effectiveness and stealthiness

## Why This Works (Mechanism)
The attack exploits the fact that GNNs learn to predict links based on patterns in the training data. By carefully selecting and poisoning specific node pairs that share similar (sparse) feature patterns, the attacker can embed a trigger that the model learns to associate with the presence of a link. When the trigger node appears in the graph, the poisoned patterns are activated, causing the model to incorrectly predict links for the target node pairs. The use of a single trigger node and sparse feature patterns makes the attack stealthy and effective.

## Foundational Learning
- **Graph Neural Networks (GNNs)**: Neural networks designed to operate on graph-structured data, aggregating information from neighboring nodes to learn node representations. Why needed: The attack targets GNNs specifically, exploiting their link prediction capabilities.
- **Link Prediction**: The task of predicting missing or future links in a graph based on existing structure and node features. Why needed: The attack focuses on compromising GNNs' link prediction performance.
- **Backdoor Attacks**: A class of attacks where the adversary embeds a trigger into the model during training, causing the model to behave maliciously when the trigger is present. Why needed: The paper's attack is a specific type of backdoor attack tailored for GNNs and link prediction.
- **Graph Poisoning**: The process of manipulating the training graph data to introduce vulnerabilities or biases into the learned model. Why needed: The attack poisons the training data by adding the trigger node and modifying link states.
- **Node Features**: Attributes or properties associated with nodes in a graph, used as input to GNNs. Why needed: The attack exploits patterns in node features to select targets and generate the trigger.
- **Black-box Attack Scenario**: An attack setting where the adversary has limited knowledge about the target model, typically only access to training data. Why needed: The attack is evaluated in a black-box scenario, making it more practical and challenging.

## Architecture Onboarding

**Component Map**: Training Data -> Trigger Generation -> Node Pair Selection -> Data Poisoning -> GNN Training -> Backdoored Model

**Critical Path**: Trigger generation and node pair selection are critical for attack effectiveness. The poisoning process must be carefully controlled to maintain stealthiness while ensuring the backdoor is embedded.

**Design Tradeoffs**: The attack balances between effectiveness (high ASR) and stealthiness (low BPD and poisoning rate). Using a single trigger node and sparse feature patterns helps maintain stealthiness but may limit the number of target node pairs.

**Failure Signatures**: Low ASR indicates poor trigger generation or node pair selection. High BPD suggests the poisoning is too aggressive, causing the model to overfit to the poisoned samples.

**First Experiments**:
1. Verify trigger generation by checking the feature distribution of the generated trigger node.
2. Validate node pair selection by examining the scores of selected pairs and their feature similarities.
3. Assess poisoning impact by comparing clean and poisoned model performance on a held-out validation set.

## Open Questions the Paper Calls Out
- How does the proposed backdoor attack scale to very large graphs with millions of nodes and edges, particularly in terms of computational efficiency and effectiveness?
- Can the proposed backdoor attack be adapted to work with weighted or directed graphs, or is it limited to unweighted, undirected graphs as presented in the paper?
- How resilient is the proposed backdoor attack to potential defenses against backdoor attacks, such as model pruning, input sanitization, or adversarial training?

## Limitations
- The attack is evaluated only on unweighted, undirected graphs with binary node features.
- The black-box attack scenario assumes the attacker has access to the training data, which may not always be the case.
- The method's effectiveness on larger, more complex graphs and real-world datasets is unknown.

## Confidence
- **High confidence**: The overall concept and methodology of the backdoor attack on GNNs for link prediction tasks.
- **Medium confidence**: The reported experimental results and performance metrics, as they are based on specific datasets and parameter settings.
- **Low confidence**: The method's effectiveness in real-world scenarios and its generalizability to other datasets and graph types.

## Next Checks
1. Replicate the experiments on additional datasets, including larger and more complex graphs, to assess the method's generalizability.
2. Evaluate the attack's performance in a gray-box or white-box scenario, where the attacker has more knowledge about the target model.
3. Investigate the potential defenses against the proposed backdoor attack, such as input sanitization, model regularization, or adversarial training.