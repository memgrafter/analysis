---
ver: rpa2
title: 'SiamTST: A Novel Representation Learning Framework for Enhanced Multivariate
  Time Series Forecasting applied to Telco Networks'
arxiv_id: '2407.02258'
source_url: https://arxiv.org/abs/2407.02258
tags:
- time
- series
- siamtst
- learning
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SiamTST is a novel representation learning framework for multivariate
  time series forecasting, combining a Siamese network with attention, channel-independent
  patching, and normalization techniques. It was evaluated on a real-world telecommunication
  dataset and demonstrated significant improvements in forecasting accuracy over existing
  methods.
---

# SiamTST: A Novel Representation Learning Framework for Enhanced Multivariate Time Series Forecasting applied to Telco Networks

## Quick Facts
- arXiv ID: 2407.02258
- Source URL: https://arxiv.org/abs/2407.02258
- Reference count: 24
- Key outcome: SiamTST combines Siamese network with attention, patching, and normalization to achieve state-of-the-art MTS forecasting on telecom data, with pre-training on multiple sectors improving performance by ~5%.

## Executive Summary
SiamTST introduces a novel representation learning framework for multivariate time series forecasting that leverages a Siamese network architecture with channel-independent patching and specialized normalization techniques. The method was evaluated on a real-world telecommunication dataset from Telenor Denmark, demonstrating significant improvements over existing methods. Notably, even a simple linear network with the same patching approach showed competitive performance, highlighting the strength of the patching mechanism itself. Pre-training experiments revealed that including more sectors during training improves forecasting performance by approximately 5%, establishing new benchmarks for MTS representation learning in telecommunications.

## Method Summary
SiamTST integrates a Siamese network with attention mechanisms, channel-independent patching, and normalization techniques (RMSNorm and QKNorm) for MTS forecasting. The method uses pre-training on masked time series modeling with a random masking ratio (0.15-0.55) and QKNorm in the self-attention mechanism. Fine-tuning is performed on individual sectors with frozen backbone parameters. The architecture treats each variable independently through patching, reducing computational complexity while maintaining feature-specific learning capabilities. The approach was specifically designed for and tested on a telecommunications dataset containing hourly aggregated metrics from 98 cell tower sectors.

## Key Results
- SiamTST achieved state-of-the-art performance on MTS forecasting for telecommunications data
- A simple linear network (LinearNet) with patching showed competitive performance, achieving second-best results
- Pre-training on multiple sectors improved performance by approximately 5% compared to single-sector training
- Channel-independent patching with RMSNorm and QKNorm provided stable attention and improved local pattern capture

## Why This Works (Mechanism)

### Mechanism 1
SiamTST improves forecasting by learning robust, general representations from multiple sectors before fine-tuning. The Siamese architecture forces the model to map different views of the same time series to similar embeddings, while pre-training on multiple sectors increases diversity of patterns learned. These pre-trained embeddings are then frozen during fine-tuning, so downstream tasks benefit from rich, general features. The core assumption is that representations learned from diverse sectors transfer well to individual sectors during fine-tuning. Evidence shows an increase in model performance when more sectors are included in the training data, with improvement practically flattening after 50 sectors. The break condition occurs if sectors are too heterogeneous or unrelated, causing learned representations to not transfer well during fine-tuning.

### Mechanism 2
Channel-independent patching with RMSNorm and QKNorm improves local pattern capture and stabilizes attention. Each variate is split into patches and processed independently, reducing computational complexity and allowing fine-grained, feature-specific learning. RMSNorm normalizes activations, while QKNorm normalizes queries and keys to avoid softmax saturation in attention, improving gradient flow and learning diverse attention patterns. The core assumption is that feature-specific, local processing yields better representation than joint processing of all variates. Evidence comes from the modified SDPA incorporating QKNorm making the softmax function less prone to saturation. The break condition is if temporal interactions between variates are critical for the task, as channel-independence could miss important cross-variable dependencies.

### Mechanism 3
Simple linear head (LinearNet) can compete with complex representation learning, but SiamTST still outperforms due to better embeddings. LinearNet treats each variate as a univariate series, processes patches, and uses a final linear layer for forecasting. Its performance shows that patching and linear models are strong baselines, but SiamTST's superior embeddings still provide an edge in complex patterns and long-term dependencies. The core assumption is that patch-based linear processing is a strong baseline for MTS forecasting. Evidence shows LinearNet outperforming several complex representation learning methods despite its simplicity. The break condition is if the task requires modeling cross-variable interactions or non-linear temporal patterns, where LinearNet's performance will degrade relative to Siamese-based methods.

## Foundational Learning

- **Time series patching and positional encoding**: Patching reduces sequence length and computational load, while positional encodings inject temporal order information for the Transformer. Quick check: How does positional encoding help the model understand the order of time steps after patching?

- **Siamese network and contrastive learning**: Siamese architecture trains the model to map different views of the same series to similar embeddings, encouraging robust, generalizable representations. Quick check: What is the role of the Siamese architecture in pre-training for MTS representation learning?

- **Normalization layers (RMSNorm, QKNorm)**: RMSNorm stabilizes training by rescaling activations, and QKNorm prevents attention softmax saturation, both improving convergence and representation quality. Quick check: How does QKNorm differ from standard attention normalization, and why is this important?

## Architecture Onboarding

- **Component map**: Input MTS → Channel-independent patching → Patch embedding (linear + positional) → Transformer encoder (RMSNorm + QKNorm + FFN) → Siamese loss (pre-training) or frozen embeddings + LinearNet head (fine-tuning)

- **Critical path**: Patching → Embedding → Transformer layers → Output (Siamese loss or forecast head)

- **Design tradeoffs**: Channel independence vs. joint modeling provides gains in computational efficiency and feature-specific learning but risks missing cross-variate interactions. RMSNorm + QKNorm vs. LayerNorm offers improved training stability but is less standard in MTS literature. Siamese pre-training vs. direct training provides better generalization but requires careful negative/positive pair sampling.

- **Failure signatures**: Poor generalization across sectors indicates issues with pre-training data diversity or Siamese loss balance. Vanishing gradients or unstable training suggests problems with normalization layers or QKNorm implementation. Overfitting to training sectors requires increased regularization or reduced model capacity.

- **First 3 experiments**: 1) Reproduce LinearNet baseline to confirm patching + linear head performance on your dataset. 2) Train SiamTST with channel-independent patching only (no Siamese loss) to isolate benefits of patching. 3) Pre-train SiamTST on 5 sectors, then fine-tune on a held-out sector to measure transfer performance.

## Open Questions the Paper Calls Out

### Open Question 1
What is the impact of varying the number of variates (N) and time steps (L) on SiamTST's performance? The paper uses a fixed N=13 and L=4 months of hourly data but does not explore the effect of varying these parameters. This remains unresolved because the authors do not conduct experiments with different values of N and L, limiting understanding of how these parameters affect model performance. Experiments with different values of N and L showing their effect on MAE and MSE would resolve this question.

### Open Question 2
How does SiamTST's performance compare to other models when dealing with missing data? While the paper mentions TS2Vec performed well despite missing input data, it does not compare SiamTST's performance with missing data. This remains unresolved because the authors do not conduct experiments with missing data to compare SiamTST's performance with other models. Experiments with artificially introduced missing data comparing SiamTST's performance with other models in terms of MAE and MSE would resolve this question.

### Open Question 3
What is the effect of using different pre-training tasks on SiamTST's performance? The paper uses a masked time series modeling task for pre-training but does not explore the effect of using different pre-training tasks. This remains unresolved because the authors do not conduct experiments with different pre-training tasks to compare their effect on SiamTST's performance. Experiments with different pre-training tasks, such as predicting future values or reconstructing the entire time series, showing their effect on SiamTST's performance in terms of MAE and MSE would resolve this question.

## Limitations

- The exact clustering methodology for sector selection is not fully specified, introducing uncertainty in reproducing the dataset preparation
- Performance drops significantly on handover-related features during test anomalies suggest potential overfitting to training patterns
- QKNorm benefits lack direct corpus support, representing a potential risk for overfitting to this specific dataset

## Confidence

- **High confidence**: Multi-sector pre-training improves generalization (supported by explicit ablation showing 5% improvement with increasing sectors up to 50)
- **Medium confidence**: SiamTST's overall architecture design delivers state-of-the-art performance (supported by comparative results, but limited by proprietary dataset access)
- **Low confidence**: QKNorm specifically contributes to performance gains (no direct corpus support for this normalization variant in MTS contexts)

## Next Checks

1. Replicate the LinearNet baseline on a publicly available MTS dataset (e.g., ETT or Traffic) to verify that channel-independent patching + linear head constitutes a strong baseline across domains.

2. Conduct ablation studies removing QKNorm while keeping all other components constant to isolate its contribution to performance improvements.

3. Test transfer performance by pre-training on one domain (e.g., traffic data) and fine-tuning on a different domain (e.g., energy data) to validate the generalizability of multi-sector pre-training benefits.