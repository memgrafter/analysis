---
ver: rpa2
title: 'Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective'
arxiv_id: '2402.14545'
source_url: https://arxiv.org/abs/2402.14545
tags:
- data
- training
- visual
- hallucinations
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that overly detailed training data can impair
  a model's ability to terminate generation timely, leading to multimodal hallucinations
  in large vision-language models. The authors analyze how models make end-of-sentence
  (EOS) decisions and find that models inherently assess textual completeness relative
  to visual input.
---

# Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective

## Quick Facts
- arXiv ID: 2402.14545
- Source URL: https://arxiv.org/abs/2402.14545
- Authors: Zihao Yue; Liang Zhang; Qin Jin
- Reference count: 17
- Primary result: Selective EOS Supervision and Scoring EOS Supervision reduce hallucinations by 26-27% on LLaVA-1.5

## Executive Summary
This paper addresses multimodal hallucinations in large vision-language models (LVLMs) by analyzing end-of-sentence (EOS) decision mechanisms. The authors identify that overly detailed training data impairs the model's ability to terminate generation timely, leading to generation beyond visual perception limits. They propose two methods: Selective EOS Supervision, a training objective that selectively optimizes EOS prediction, and Scoring EOS Supervision, a data filtering strategy that removes harmful training data based on EOS prediction impact.

## Method Summary
The paper proposes two complementary approaches to mitigate multimodal hallucinations. Selective EOS Supervision modifies the standard MLE objective by excluding the EOS token from the probability distribution during non-EOS positions, forcing the model to learn EOS decisions based on visual completeness assessment. Scoring EOS Supervision implements a data filtering strategy that removes training samples likely to impair EOS decision-making based on two metrics: Spos (measuring positive impact) and Sneg (measuring negative impact). The methods are evaluated on LLaVA models using CHAIR metrics for hallucination assessment.

## Key Results
- Selective EOS Supervision reduces sentence-level hallucinations by 26% and instance-level hallucinations by 27% on LLaVA-1.5
- Data filtering based on Scoring EOS Supervision significantly reduces hallucinations without requiring additional data
- Models inherently assess textual completeness relative to visual input when making EOS decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overly detailed training data impairs the model's ability to terminate generation timely, leading to multimodal hallucinations.
- Mechanism: When trained on data exceeding visual perception limits, models attempt to fit ground truth caption detail levels, generating text beyond capability limits.
- Core assumption: Model's visual perception limits are lower than detail level in some training data.
- Evidence anchors: Abstract identifies overly detailed data hinders timely termination; section 2.1 provides example of detailed captioning tasks; corpus analysis shows related work.

### Mechanism 2
- Claim: Model inherently assesses completeness of generated text relative to visual input when making EOS decisions.
- Mechanism: When predicting EOS, model relies more on all preceding sentences rather than current sentence, suggesting completeness assessment by comparing generated text with image.
- Core assumption: Model uses completeness assessment based on visual input for EOS decisions.
- Evidence anchors: Section 2.1 finds model assesses completeness by comparing generated text with image; section 2.2 shows EOS decision relates to current state of entire sequence.

### Mechanism 3
- Claim: Selective EOS Supervision and Scoring EOS Supervision unlock model's potential to make timely EOS decisions and reduce hallucinations.
- Mechanism: Modifying training objective to selectively preserve EOS prediction optimization and filtering harmful training data teaches model to terminate generation based on visual perception limits.
- Core assumption: Model's inherent potential to make proper EOS decisions based on visual perception can be unlocked through targeted training modifications.
- Evidence anchors: Abstract explores two methods to mitigate hallucinations; section 3.1 enables model to mitigate hallucinations through learning from regular instruction data; section 3.2 eliminates harmful training data that can impair sequence ending ability.

## Foundational Learning

- Concept: Saliency-based analysis of information flow
  - Why needed here: To understand how model uses context information when making EOS decisions
  - Quick check question: What does a high saliency score between two tokens indicate about their relationship in model's processing?

- Concept: Cross-entropy loss and probability distributions in language models
  - Why needed here: To understand how Selective EOS Supervision modifies standard training objective
  - Quick check question: How does excluding vEOS from probability distribution during non-EOS positions affect model's learning?

- Concept: Data filtering based on model predictions
  - Why needed here: To implement Scoring EOS Supervision method for identifying and removing harmful training data
  - Quick check question: What does a high Sneg score indicate about particular training data sample's impact on model's EOS decision ability?

## Architecture Onboarding

- Component map: Visual encoder -> Language decoder -> EOS token -> Training pipeline
- Critical path:
  1. Input image is encoded by visual encoder
  2. Visual features are combined with language model state
  3. Model generates tokens autoregressively
  4. EOS decision is made based on completeness assessment
  5. Training is performed with modified objective or filtered data
- Design tradeoffs:
  - Selective EOS Supervision vs. standard MLE: Improved hallucination mitigation at potential cost of some recall
  - Data filtering vs. using all data: Better EOS decision capability but potentially reduced training set size
  - Focus on generative tasks vs. broader VQA tasks: More targeted solution but narrower applicability
- Failure signatures:
  - No improvement in hallucination metrics after applying methods
  - Significant drop in recall or overall caption quality
  - Increased computational cost without proportional performance gains
- First 3 experiments:
  1. Implement saliency analysis to verify information flow patterns in EOS vs. non-EOS predictions
  2. Apply Selective EOS Supervision to pre-trained LVLM and measure changes in hallucination metrics
  3. Implement data filtering based on Scoring EOS Supervision and compare model performance with and without filtered data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does effectiveness of proposed methods vary across different model sizes and architectures beyond tested LLaVA and MiniGPT models?
- Basis in paper: [inferred] Paper tests methods on LLaVA-1.5 (7b and 13b) and MiniGPT-v2 (7b) but does not explore broader range of model sizes or architectures.
- Why unresolved: Paper's experiments are limited to specific model configurations, leaving open question of generalizability to other sizes and architectures.
- What evidence would resolve it: Systematic testing of methods on diverse set of model sizes (e.g., 1b, 3b, 13b) and architectures (e.g., different vision encoders, decoder-only vs. encoder-decoder) with consistent evaluation metrics.

### Open Question 2
- Question: What is long-term impact of proposed methods on model performance across diverse tasks beyond image captioning, such as visual question answering or image generation?
- Basis in paper: [explicit] Paper focuses on mitigating hallucinations in generative tasks like image captioning and does not explore broader task applicability.
- Why unresolved: Methods are evaluated only on caption generation tasks, leaving uncertainty about their effectiveness in other multimodal tasks that may have different hallucination patterns.
- What evidence would resolve it: Comprehensive evaluation of methods on suite of multimodal tasks including VQA, visual reasoning, and image generation, with both hallucination metrics and task-specific performance measures.

### Open Question 3
- Question: How do proposed methods affect model's ability to handle ambiguous or novel visual inputs that require inference beyond explicit visual features?
- Basis in paper: [inferred] Methods focus on preventing generation beyond visual perception limits, but paper does not address how this impacts model's ability to make reasonable inferences from ambiguous or novel inputs.
- Why unresolved: Paper does not explore trade-off between preventing hallucinations and maintaining model's capacity for reasonable inference from incomplete or ambiguous visual information.
- What evidence would resolve it: Experiments with intentionally ambiguous or novel visual inputs where ground truth is uncertain, measuring both hallucination reduction and model's ability to make plausible inferences.

## Limitations
- Limited evaluation to image captioning tasks, leaving uncertainty about generalization to other vision-language tasks
- Does not systematically characterize the threshold of detail that causes hallucination problems
- Data filtering may implicitly reduce effective training set size while claiming to use "regular" data

## Confidence

**High confidence** (Strong empirical support, clear mechanism):
- Observation that models rely more on full context when making EOS decisions compared to regular token predictions
- Effectiveness of both proposed methods in reducing hallucination metrics (CHAIRS and CHAIRI) on tested models
- Correlation between training on detailed data and increased hallucination rates

**Medium confidence** (Reasonable support but some gaps):
- Claim that overly detailed training data is primary cause of multimodal hallucinations across LVLMs
- Assumption that EOS decision mechanisms are fundamentally similar across different LVLM architectures
- Assertion that proposed methods unlock inherent model potential rather than simply changing optimization dynamics

**Low confidence** (Limited evidence or significant assumptions):
- Generalizability of findings to LVLM applications beyond image captioning
- Long-term stability of hallucination reduction after applying these methods
- Claim that data filtering preserves all essential learning signals while removing only harmful content

## Next Checks

1. **Cross-task validation**: Test Selective EOS Supervision and data filtering methods on LVLM tasks beyond image captioning (e.g., visual question answering, image retrieval) to verify if EOS-based hallucination mitigation generalizes across LVLM capability spectrum.

2. **Ablation study on data detail levels**: Systematically vary detail level in training data (rather than using binary detailed vs. regular split) to identify specific threshold where hallucination rates increase, providing clearer characterization of "overly detailed" boundary.

3. **Long-term stability analysis**: Evaluate hallucination rates across multiple epochs of continued training after applying proposed methods to determine whether improvements are stable or whether models revert to hallucinating behaviors over time.