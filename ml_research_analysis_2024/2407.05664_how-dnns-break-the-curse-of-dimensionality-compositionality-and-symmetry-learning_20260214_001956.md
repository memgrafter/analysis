---
ver: rpa2
title: 'How DNNs break the Curse of Dimensionality: Compositionality and Symmetry
  Learning'
arxiv_id: '2407.05664'
source_url: https://arxiv.org/abs/2407.05664
tags:
- bound
- networks
- functions
- have
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the fundamental challenge of learning complex,
  high-dimensional functions using deep neural networks (DNNs) by proposing a novel
  theoretical framework based on compositionality and symmetry learning. The authors
  introduce a generalization bound for Accordion Networks (AccNets), which are deep
  networks composed of shallow subnetworks, that depends on the F1-norms and Lipschitz
  constants of these subnetworks rather than the number of neurons or parameters.
---

# How DNNs break the Curse of Dimensionality: Compositionality and Symmetry Learning

## Quick Facts
- **arXiv ID**: 2407.05664
- **Source URL**: https://arxiv.org/abs/2407.05664
- **Reference count**: 40
- **Key outcome**: Proposes a generalization bound for Accordion Networks that depends on F1-norms and Lipschitz constants rather than parameters, showing DNNs can learn compositions of Sobolev functions efficiently and break the curse of dimensionality.

## Executive Summary
This paper presents a theoretical framework explaining how deep neural networks overcome the curse of dimensionality by leveraging compositional structure in target functions. The authors introduce Accordion Networks (AccNets) and prove generalization bounds that depend on the F1-norms and Lipschitz constants of shallow subnetworks rather than total parameters. They show that AccNets can efficiently learn compositions of Sobolev functions where traditional methods fail, with empirical validation matching theoretical predictions. The work identifies two phases in learning: first learning symmetries (dimensionality reduction) and then learning the target function in the reduced space.

## Method Summary
The paper studies Accordion Networks - deep networks composed of shallow subnetworks fℓ = Wℓσ(Vℓx + bℓ) with depth L. Generalization bounds are derived using covering number arguments that depend on F1-norms (Rℓ) and Lipschitz constants (ρℓ) of each subnetwork rather than total parameters. The framework applies to compositions of Sobolev functions f* = h∘g, showing AccNets can learn these efficiently when kernel methods or shallow networks cannot. Empirical validation uses synthetic data generated from compositions of Matérn kernels with input dimension 15, bottleneck 3, output 20, trained with L2 regularization and Adam optimizer over 3600 epochs in three phases with increasing weight decay.

## Key Results
- Generalization bound: p L(f) − q ˜LN(f) ≤ c0 p BρL:1rPL ℓ′=1 Rℓ′ ρℓ′ p dℓ′ + dℓ′−1N − 1 2 (1 + o(1)) + c1B p − log p/2 N
- AccNets can learn compositions of Sobolev functions with almost optimal rates, avoiding curse of dimensionality under smoothness and dimensionality assumptions
- Empirical scaling laws match theoretical predictions, with phase transitions observed depending on whether learning symmetries or the target function is harder

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep networks break the curse of dimensionality by learning compositionally structured functions rather than treating high-dimensional inputs as unstructured.
- Mechanism: Compositions of Sobolev functions can be learned efficiently with deep networks, whereas kernel methods or shallow networks cannot. If f* decomposes into h∘g where g reduces dimensionality and h is less regular, deep networks can learn g smoothly and h in the reduced space.
- Core assumption: The true function has compositional structure with intermediate low-dimensional representations that are smooth.
- Evidence anchors:
  - [abstract] "We show that deep neural networks (DNNs) can efficiently learn any composition of functions with bounded F1-norm, which allows DNNs to break the curse of dimensionality in ways that shallow networks cannot."
  - [section 4.1.1] "To break the curse of dimensionality, we need to assume some additional structure on the data or task which introduces an 'intrinsic dimension' that can be much lower than the input dimension."
- Break condition: If the function lacks compositional structure or if intermediate dimensions remain high despite composition, the advantage disappears.

### Mechanism 2
- Claim: The F1-norm regularization enables efficient learning by controlling parameter complexity without depending on width.
- Mechanism: The generalization bound depends on F1-norms of subnetworks rather than total parameters. This allows the network to grow wide for representational power while keeping complexity low, avoiding overfitting that would occur with parameter-count-based bounds.
- Core assumption: The learned function can be represented with bounded F1-norm across layers.
- Evidence anchors:
  - [section 3.2] "The generalization gap over any F1-ball can be uniformly bounded with high probability: Theorem 1. For any input distribution π supported on the L2 ball B(0, b) with radius b, we have with probability 1 − p..."
  - [section 3.1] "The generalization gap over any F1-ball can be uniformly bounded with high probability"
- Break condition: If the function requires exponentially large F1-norm to represent, the bound becomes vacuous.

### Mechanism 3
- Claim: The Lipschitz constant bound (rather than operator norm) captures the actual smoothness of learned functions, leading to tighter generalization bounds.
- Mechanism: By using Lipschitz constants of individual subnetworks instead of operator norms of weight matrices, the bound becomes independent of rank and captures the true smoothness properties. This is crucial for compositional functions where operator norms can explode even when Lipschitz constants remain bounded.
- Core assumption: The learned function has bounded Lipschitz constant across layers.
- Evidence anchors:
  - [section 3.2] "The fact that our bound depends on the Lipschitz constants rather than the operator norms ∥Wℓ∥op, ∥Vℓ∥op is thus a significant advantage."
  - [section 4.2] "By evaluating the complexities R, ˜R on this approximation, we obtain two bounds illustrating the tradeoff between these two complexities."
- Break condition: If the function has unbounded Lipschitz constant or if Lipschitz constant is much larger than operator norm in pathological cases.

## Foundational Learning

- Concept: Sobolev spaces and norms
  - Why needed here: The paper's theoretical framework relies on understanding function smoothness through Sobolev norms, which measure decay of Fourier/spherical harmonic coefficients and control learning rates.
  - Quick check question: If a function has finite Sobolev norm W^ν,2, what does this tell us about its differentiability and how does this relate to the ν parameter?

- Concept: Covering numbers and generalization bounds
  - Why needed here: The paper uses covering number arguments to derive generalization bounds for deep networks, which is a different approach from traditional VC-dimension or Rademacher complexity methods.
  - Quick check question: How does the covering number of a function class relate to the uniform convergence of empirical to population risk, and why is this particularly useful for compositional function classes?

- Concept: Composition of function spaces
  - Why needed here: The key insight is that deep networks learn compositions of simpler functions, and understanding how complexity measures behave under composition is essential to the theoretical results.
  - Quick check question: When composing two function classes F and G, how does the covering number of the composition relate to the covering numbers of the individual classes?

## Architecture Onboarding

- Component map:
  Input layer → multiple shallow subnetworks (Accordion structure) → composition through sequential application → output layer
  Each subnetwork fℓ = Wℓσ(Vℓx + bℓ) with separate weight matrices and biases
  F1-norm regularization applied across all subnetworks
  Lipschitz constant control (implicit or explicit) for each subnetwork

- Critical path:
  1. Initialize weights with appropriate scaling
  2. Train with L2 regularization to encourage small F1-norms
  3. Monitor Lipschitz constants (implicitly via large learning rates or explicitly via regularization)
  4. Ensure convergence to global minimum of regularized loss
  5. Verify test error scales as predicted by N^(-2/(2+r_max))

- Design tradeoffs:
  - Width vs depth: Very wide shallow subnetworks with the accordion structure vs traditional deep networks
  - F1-norm vs parameter norm: Using F1-norm regularization instead of L2 on parameters directly
  - Lipschitz control: Implicit via training dynamics vs explicit regularization terms

- Failure signatures:
  - If test error scales as N^(-2/din) instead of the improved rate, the compositional structure wasn't learned
  - If training fails to converge despite sufficient capacity, the F1-norm regularization might be too strong
  - If Lipschitz constants grow uncontrollably, operator-norm-based bounds become vacuous

- First 3 experiments:
  1. Train on synthetic data f* = h∘g with known compositional structure and measure scaling laws; compare to kernel methods and shallow networks
  2. Vary the smoothness parameters ν_g and ν_h to observe phase transitions between regimes where learning g is harder vs learning h is harder
  3. Implement F1-norm regularization explicitly and compare to standard L2 regularization on test error scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the F1-norm approximation results for Sobolev functions be extended to finite-width deep networks without relying on infinite-width assumptions?
- Basis in paper: [explicit] The paper notes that current approximation results require infinite width because bounded F1-norm and Lipschitzness preservation under approximation is not yet proven for finite-width networks.
- Why unresolved: The authors state they "were not able to prove that a function with bounded F1-norm and Lipschitz constant can be approximated by a sufficiently wide shallow networks with the same (or close) F1-norm and Lipschitz constant."
- What evidence would resolve it: A rigorous proof showing that for any Sobolev function with bounded F1-norm and Lipschitz constant, there exists a finite-width network achieving arbitrary approximation accuracy while preserving these norm bounds.

### Open Question 2
- Question: What is the optimal regularization strategy for controlling Lipschitz constants in deep networks when using F1-norm regularization?
- Basis in paper: [explicit] The authors discuss using weight decay for F1-norm control but rely on implicit bias for Lipschitz control, suggesting this is not fully understood.
- Why unresolved: The paper mentions that "Lipschitz constants Lip(fℓ) are difficult to optimize over" and proposes relying on implicit bias like large learning rates, but this remains heuristic.
- What evidence would resolve it: Empirical studies comparing different regularization schemes (weight decay, explicit Lipschitz regularization, learning rate schedules) on tasks requiring precise Lipschitz control, measuring generalization performance and approximation accuracy.

### Open Question 3
- Question: How does the depth dependence in generalization bounds scale for non-accordion architectures like standard ResNets with skip connections?
- Basis in paper: [inferred] The authors mention their bounds apply to ResNets by replacing Lipschitz constants with Lip(fℓ + id), but do not analyze the depth scaling differences.
- Why unresolved: The paper shows their bound grows linearly with depth for accordion networks but does not analyze whether skip connections change this scaling.
- What evidence would resolve it: A theoretical analysis proving depth scaling (linear vs sublinear vs exponential) for ResNets with skip connections, comparing to the accordion network case.

## Limitations
- The generalization bounds depend critically on F1-norms remaining bounded, which may not hold for functions learned on real data
- The compositionality assumption - that the target function decomposes into simpler components with intermediate low-dimensional representations - is restrictive and may not capture all real-world functions
- The analysis focuses on Sobolev spaces, which require strong smoothness assumptions that many practical functions may not satisfy

## Confidence
- High: The mechanism by which compositional structure enables breaking the curse of dimensionality is well-established theoretically
- Medium: The generalization bounds are mathematically rigorous but may be conservative in practice
- Medium: The empirical validation using synthetic data convincingly demonstrates the theoretical predictions

## Next Checks
1. Test the framework on real-world datasets beyond MNIST and WESAD to verify if the compositional structure assumption holds in practice
2. Quantify how sensitive the generalization bounds are to violations of the F1-norm and Lipschitz assumptions
3. Investigate whether alternative function classes (beyond Sobolev spaces) can benefit from similar compositional learning approaches