---
ver: rpa2
title: 'Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark'
arxiv_id: '2404.06859'
source_url: https://arxiv.org/abs/2404.06859
tags:
- task
- learning
- replay
- medical
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new benchmark for continual learning (CL)
  in the medical domain. The benchmark combines the challenges of new class arrivals
  and domain shifts, using a New Instances and New Classes (NIC) scenario.
---

# Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark

## Quick Facts
- arXiv ID: 2404.06859
- Source URL: https://arxiv.org/abs/2404.06859
- Reference count: 40
- This paper proposes a new benchmark for continual learning in medical imaging, introducing the RCLP method that achieves state-of-the-art performance with an average F1 score of 0.27 and a forgetting rate of only 2.4%.

## Executive Summary
This paper addresses the challenge of continual learning in the medical domain, specifically for multi-label classification tasks. It proposes a novel benchmark that combines new class arrivals and domain shifts, using the New Instances and New Classes (NIC) scenario. The benchmark is based on two datasets (NIH ChestX-ray14 and CheXpert) with nineteen classes across seven tasks. To tackle the complexities of this setting, the authors introduce Replay Consolidation with Label Propagation (RCLP), which integrates old knowledge into new samples, exploits the replay memory fully, and mitigates task interference using a Masking Loss. The results demonstrate that RCLP outperforms existing methods, achieving state-of-the-art performance on the proposed benchmark.

## Method Summary
The proposed method, RCLP, combines several key components to address the challenges of multi-label continual learning in the medical domain. It uses a DenseNet-121 backbone pre-trained on ImageNet and employs a replay memory buffer to store samples from previous tasks. During training, RCLP integrates old knowledge into new samples through label propagation, both in the forward and backward directions. It also uses a masking loss to prevent task interference and feature distillation to further improve performance. The method is trained using binary cross-entropy loss and the Adam optimizer with a learning rate of 0.0005. The proposed benchmark combines the NIH ChestX-ray14 and CheXpert datasets, with a total of nineteen classes across seven tasks.

## Key Results
- RCLP achieves an average F1 score of 0.27 on the proposed benchmark, outperforming existing methods.
- The forgetting rate of RCLP is only 2.4%, demonstrating its effectiveness in mitigating catastrophic forgetting.
- RCLP successfully handles the challenges of new class arrivals and domain shifts in the multi-label medical imaging setting.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RCLP's label propagation forward step integrates old task knowledge into new task samples, mitigating catastrophic forgetting.
- Mechanism: During training on new task samples, predictions from the previously optimized model are thresholded and used to fill in labels for classes not present in the current task. This creates pseudo-labels for old classes on new task samples.
- Core assumption: The old model's predictions are sufficiently accurate to serve as reliable pseudo-labels for old classes on new task samples.
- Evidence anchors:
  - [abstract]: "RCLP integrates old knowledge into new samples"
  - [section 4.1]: "During the training of task t, given a sample x ∈ Xt, the forward step adjusts the ground truth associated with each y ∈ Yt, relative to the labels L1, . . . , Lt−1, to integrate the knowledge of the previously optimized model fθt−1."
- Break condition: If the old model's performance degrades significantly, the pseudo-labels will be inaccurate, leading to corrupted training signals and worse performance.

### Mechanism 2
- Claim: RCLP's label propagation backward step consolidates new task knowledge into old replay memory samples.
- Mechanism: After training on a new task, samples from the replay memory (which originated from previous tasks) are passed through the updated model. Pseudo-labels for the new task's classes are generated and added to the ground truth targets of these replay samples.
- Core assumption: The updated model's predictions on old replay samples are reliable indicators of the presence/absence of new task classes.
- Evidence anchors:
  - [abstract]: "RCLP...exploits the replay memory fully"
  - [section 4.2]: "The procedure can be described formally as...after training the model on the new task t, the samples in the replay buffer...are given as input to the model, and the pseudo-labels relative to the current task are computed and added to the ground truth targets."
- Break condition: If the model's performance on old replay samples degrades, the pseudo-labels for new classes will be inaccurate, potentially corrupting the replay memory.

### Mechanism 3
- Claim: RCLP's masking loss prevents task interference by focusing the model's learning on relevant labels for each sample type.
- Mechanism: When training on new task samples, the loss is computed only for the current task's labels. When training on replay samples, the loss is computed only for the labels from the replay sample's originating task.
- Core assumption: The intersection between tasks (samples containing labels from multiple tasks) is not so large that masking the loss significantly degrades performance on multi-label samples.
- Evidence anchors:
  - [abstract]: "RCLP...mitigates task interference using a Masking Loss"
  - [section 4.3]: "When the model revisits old samples, the masking loss ensures that the model's outputs are only influenced by the labels of the old tasks. This effectively 'masks' the influence of the old samples on the new labels, preventing task interference."
- Break condition: If the task overlap is very high, masking the loss could prevent the model from learning useful correlations between labels from different tasks present in the same sample.

## Foundational Learning

- Concept: Continual Learning (CL) and Catastrophic Forgetting
  - Why needed here: The paper addresses the problem of catastrophic forgetting in a multi-label medical imaging setting where new classes and domains are introduced over time.
  - Quick check question: What is the main challenge that CL methods aim to solve, and why is it particularly problematic in the medical domain?

- Concept: Multi-Label Classification
  - Why needed here: The benchmark and proposed method are specifically designed for multi-label classification problems, where each sample can have multiple labels simultaneously.
  - Quick check question: How does multi-label classification differ from multi-class classification, and what unique challenges does it pose for CL?

- Concept: Domain Shift
  - Why needed here: The proposed benchmark includes scenarios where new data for previously seen classes comes from different domains (hospitals), requiring the model to adapt to domain shifts.
  - Quick check question: What is domain shift, and how can it affect the performance of a model trained on data from one domain when applied to data from a different domain?

## Architecture Onboarding

- Component map:
  DenseNet-121 backbone -> Label Propagation module (forward and backward steps) -> Masking Loss module -> Feature Distillation module -> Replay memory buffer

- Critical path:
  1. Train on new task samples with label propagation forward step and masking loss
  2. Update replay memory with new samples and label propagation backward step
  3. Train on replay memory samples with masking loss
  4. Apply feature distillation loss
  5. Repeat for each task

- Design tradeoffs:
  - Replay memory size vs. performance: Larger memory provides more diverse samples but increases computational cost
  - Threshold for label propagation vs. accuracy: Lower thresholds include more samples but may introduce noise
  - Masking loss vs. task interference: Strict masking prevents interference but may hinder learning label correlations

- Failure signatures:
  - High forgetting rate: Indicates inadequate consolidation of old knowledge
  - Poor performance on new tasks: Suggests insufficient learning of new classes
  - Instability during training: May indicate conflicts between label propagation and masking loss

- First 3 experiments:
  1. Validate label propagation forward step by comparing model performance with and without it on a simple 2-task scenario
  2. Test masking loss effectiveness by measuring task interference in a multi-label setting with high task overlap
  3. Evaluate replay memory consolidation by comparing feature distillation performance with and without the backward step of label propagation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RCLP perform on other medical imaging modalities beyond chest X-ray, such as MRI or CT scans?
- Basis in paper: [inferred] The paper mentions future work plans to evaluate RCLP on other scenarios like object detection and semantic segmentation, but doesn't specifically address other medical imaging modalities.
- Why unresolved: The current study only evaluates RCLP on chest X-ray images. Extending the evaluation to other medical imaging modalities would require collecting or accessing new datasets and re-training the model.
- What evidence would resolve it: Conducting experiments with RCLP on MRI or CT scan datasets for various medical tasks, and comparing its performance against existing methods in those domains.

### Open Question 2
- Question: What is the optimal memory size for the replay buffer in RCLP, and how does it impact performance?
- Basis in paper: [explicit] The paper mentions using a replay buffer size of 3% of the original dataset size, but doesn't explore the impact of different buffer sizes on performance.
- Why unresolved: The optimal memory size likely depends on factors like the number of tasks, class distribution, and dataset size. The current study uses a fixed buffer size without exploring its sensitivity.
- What evidence would resolve it: Conducting experiments with varying replay buffer sizes and analyzing their impact on RCLP's performance metrics like average F1 score and forgetting rate.

### Open Question 3
- Question: How does RCLP handle scenarios with more frequent domain shifts or class arrivals compared to the current benchmark?
- Basis in paper: [inferred] The current benchmark combines domain shifts and class arrivals, but the frequency and distribution of these changes might not represent all real-world scenarios. The paper doesn't explore RCLP's robustness to more frequent or extreme changes.
- Why unresolved: The benchmark's design choices regarding the frequency and distribution of domain shifts and class arrivals might not capture the full range of challenges in continual learning for medical imaging.
- What evidence would resolve it: Designing and evaluating RCLP on benchmarks with different frequencies and patterns of domain shifts and class arrivals, and analyzing its performance under those varying conditions.

## Limitations
- The optimal replay buffer size is not explored, and the current 3% size may not be ideal for all scenarios.
- The benchmark, while more complex than previous ones, is still relatively small-scale compared to real-world medical imaging datasets.
- The performance gains are based on a single backbone architecture (DenseNet-121), and may not generalize to other architectures.

## Confidence
- Mechanism 1 (Label Propagation Forward): High
- Mechanism 2 (Label Propagation Backward): Medium
- Mechanism 3 (Masking Loss): Medium

## Next Checks
1. Conduct an ablation study to isolate the contributions of each component of RCLP (label propagation, masking loss, feature distillation) on the benchmark.
2. Test RCLP's performance on a larger-scale multi-label medical imaging dataset to assess its scalability and generalization.
3. Evaluate RCLP's robustness to varying levels of task overlap and domain shift to better understand its limitations and failure modes.