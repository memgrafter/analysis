---
ver: rpa2
title: Transformer Based Planning in the Observation Space with Applications to Trick
  Taking Card Games
arxiv_id: '2404.13150'
source_url: https://arxiv.org/abs/2404.13150
tags:
- player
- search
- games
- game
- observation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Generative Observation Monte Carlo Tree Search
  (GO-MCTS), a method for addressing the challenges of applying traditional search
  algorithms to games of imperfect information, such as trick-taking card games. GO-MCTS
  performs MCTS in the observation space, using a generative model to provide state
  transitions based solely on the agent's observations, thereby avoiding issues like
  strategy fusion and non-locality.
---

# Transformer Based Planning in the Observation Space with Applications to Trick Taking Card Games

## Quick Facts
- arXiv ID: 2404.13150
- Source URL: https://arxiv.org/abs/2404.13150
- Reference count: 10
- Primary result: GO-MCTS improves upon baseline policies in Hearts, Skat, and The Crew through iterative transformer-based planning in observation space

## Executive Summary
This paper introduces Generative Observation Monte Carlo Tree Search (GO-MCTS), a novel approach for addressing the challenges of imperfect information games by performing search directly in the observation space rather than sampling explicit game states. The method uses a transformer-based generative model to predict next observations conditioned only on the observation history, thereby avoiding issues like strategy fusion and non-locality that plague traditional approaches. The authors demonstrate this approach in three trick-taking card games, showing significant improvements over baseline policies through an iterative self-play training process.

## Method Summary
GO-MCTS combines Monte Carlo Tree Search with a transformer-based generative model that predicts the next observation given the current observation history. The approach sidesteps the need for explicit state sampling by operating directly in observation space, using the transformer to generate transitions during search. Training occurs through an iterative process where the transformer is first trained on baseline data, then improved through population-based self-play where agents mix greedy and sampled policies. The method requires game-specific tokenization of observations but demonstrates strong performance across multiple trick-taking games without requiring hand-crafted heuristics.

## Key Results
- In Hearts, GO-MCTS outperforms the baseline xinxin bot by 1.74 points on average
- When extrapolated to 100-point games, this difference would equate to a 31.0 point advantage
- GO-MCTS provides new state-of-the-art results in both Hearts and The Crew
- The approach shows consistent improvements across multiple trick-taking card games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GO-MCTS avoids strategy fusion by operating in the observation space rather than sampling explicit game states.
- Mechanism: Instead of determinizing to a full game state, GO-MCTS uses a generative model to produce the next observation given the current observation history. This sidesteps the need to maintain consistency across multiple sampled states and prevents the agent from using incompatible strategies in different sampled worlds.
- Core assumption: The generative model can accurately approximate the distribution of possible next observations conditioned only on the observation history and the agent's actions.
- Evidence anchors:
  - [abstract] "This method performs the search within the observation space and advances the search using a model that depends solely on the agent's observations, thereby avoiding issues like strategy fusion and non-locality."
  - [section 3] "We present Generative Observation MCTS (GO-MCTS)...which utilizes MCTS on observation sequences generated by a game specific model...sidesteps the issue of not knowing the true underlying state of the game by performing search in the observation space."
- Break condition: If the generative model's predictions deviate significantly from the true observation distribution, the search will optimize toward unrealistic or impossible trajectories, degrading performance.

### Mechanism 2
- Claim: Transformers are effective generative models for trick-taking card games because gameplay resembles natural language sequences.
- Mechanism: The sequence of observations in trick-taking games (cards played, communication, game state changes) can be tokenized and modeled as a sequence generation problem, similar to how transformers handle language. The transformer learns to predict the next observation token given the history.
- Core assumption: The game's observation space can be effectively tokenized into discrete symbols that preserve meaningful sequential dependencies.
- Evidence anchors:
  - [section 3] "Transformers can be trained using only the raw observations as inputs...we chose to use transformers due to their strength in sequence generation in Natural Language Processing (NLP)...the game-play in trick-taking card games bears resemblance to natural language."
  - [section 5.1] "We used a PyTorch...implementation of the GPT2 architecture...The encoded sequence of observations by the agent are directly fed into the network, which it uses to predicts the next observation."
- Break condition: Games with continuous or highly complex observation spaces that cannot be effectively tokenized would not benefit from this approach.

### Mechanism 3
- Claim: Population-based self-play with iterative policy improvement stabilizes transformer training for imperfect information games.
- Mechanism: The training alternates between generating experience data using a mix of ArgmaxVal* (greedy) and sampled policies from previous iterations, then retraining the transformer on this data. This creates a curriculum that gradually improves the policy while maintaining diversity.
- Core assumption: A balance between exploitation (greedy selection) and exploration (sampling) in the self-play population prevents collapse to degenerate strategies.
- Evidence anchors:
  - [section 5.2] "We employed iterative policy improvement using neural fictitious selfplay...set one player to greedily select the action with the highest action-value estimate from the most recent network, and the rest of the players were set to sample from a policy previous iteration."
  - [section 6.1] "We then performed 10 iterations of experience generation and retraining...In the first experience generation, we set all players to follow the ArgMaxVal* policy. We did this to aggressively move the learning process into a space that resembled sensible play."
- Break condition: If the population becomes too homogeneous or the exploration rate is insufficient, the training may converge prematurely to suboptimal policies.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP) formulation
  - Why needed here: Understanding that the observation sequence can be treated as a state in an MDP helps justify why MCTS can work in the observation space.
  - Quick check question: Why can we treat the observation sequence as the state in GO-MCTS instead of the underlying game state?

- Concept: Transformer sequence modeling and attention mechanisms
  - Why needed here: The transformer must learn to generate the next observation token given a sequence of previous observations, which requires understanding self-attention and autoregressive generation.
  - Quick check question: How does the transformer use attention to predict the next observation in the sequence?

- Concept: Upper Confidence bounds applied to Trees (UCT) and Monte Carlo Tree Search
  - Why needed here: GO-MCTS uses MCTS with UCT for action selection during search, so understanding the exploration-exploitation tradeoff is crucial.
  - Quick check question: What role does the exploration constant C play in the UCT formula during GO-MCTS?

## Architecture Onboarding

- Component map: Game environment -> Observation tokenizer -> Transformer generative model -> MCTS search -> Action selector

- Critical path:
  1. Initialize transformer with game-specific vocabulary
  2. Generate initial training data via random or baseline play
  3. Train transformer on observation sequences
  4. Run GO-MCTS using trained transformer for planning
  5. Generate new self-play data using ArgmaxVal* + sampled policies
  6. Retrain transformer and repeat

- Design tradeoffs:
  - Model size vs. inference speed: Smaller transformers are faster but may capture less complex patterns
  - Exploration threshold in ArgmaxVal*: Higher thresholds are more conservative but may miss good actions
  - Training data diversity vs. quality: More diverse data helps generalization but may include poor quality examples

- Failure signatures:
  - Illegal action generation: Transformer predicts observations that imply impossible actions
  - Mode collapse: Transformer generates very similar observation sequences regardless of input
  - Search inefficiency: MCTS explores many branches that lead to illegal or low-value outcomes

- First 3 experiments:
  1. Train transformer on Hearts observation sequences and test next-observation prediction accuracy
  2. Run GO-MCTS in Hearts with a fixed opponent to verify it improves upon the base policy
  3. Compare ArgmaxVal* vs. GO-MCTS performance in a simplified trick-taking game with known optimal play

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the specific hyperparameter choices (such as the policy action selection threshold and the penalty for illegal moves) affect the performance and efficiency of GO-MCTS in different trick-taking card games?
- Basis in paper: [explicit] The paper mentions that the performance of GO-MCTS relies heavily on seemingly arbitrary parameter settings and that further refinement and exploration over these degrees of freedom could provide substantial improvements.
- Why unresolved: The paper does not provide a detailed analysis of how different hyperparameter settings impact the performance and efficiency of GO-MCTS in various trick-taking card games.
- What evidence would resolve it: Conducting systematic experiments varying hyperparameters such as the policy action selection threshold and the penalty for illegal moves in different trick-taking card games, and analyzing the impact on performance and efficiency.

### Open Question 2
- Question: How can the learning process and population dynamics be optimized to effectively utilize the additional computation provided by GO-MCTS while maintaining stability?
- Basis in paper: [explicit] The paper suggests that further work is needed to investigate how to effectively incorporate GO-MCTS into the learning process and how to form the population dynamics and learning updates to effectively utilize this additional computation while maintaining stability.
- Why unresolved: The paper does not provide a detailed exploration of how to optimize the learning process and population dynamics to effectively utilize GO-MCTS.
- What evidence would resolve it: Developing and testing new algorithms or modifications to existing algorithms that integrate GO-MCTS into the learning process and analyzing their impact on stability and performance.

### Open Question 3
- Question: How does the choice of observation encoding impact the effectiveness of the transformer in modeling the dynamics of different trick-taking card games?
- Basis in paper: [explicit] The paper mentions that the choice of observation encoding is crucial for the transformer to effectively model the dynamics of the game, and that different games may require different encoding schemes.
- Why unresolved: The paper does not provide a detailed analysis of how different observation encoding schemes impact the effectiveness of the transformer in different trick-taking card games.
- What evidence would resolve it: Conducting experiments with different observation encoding schemes in various trick-taking card games and analyzing their impact on the effectiveness of the transformer in modeling game dynamics.

## Limitations

- The approach requires substantial domain knowledge for game-specific tokenization and model architecture design, limiting its general applicability
- The reliance on transformer-based generative models may not scale efficiently to games with extremely large or complex observation spaces
- The iterative training process, though shown to work in practice, lacks theoretical guarantees about convergence or stability across different game types

## Confidence

- **High Confidence**: The core mechanism of performing MCTS in observation space (avoiding strategy fusion) is well-supported by the literature on imperfect information games and the empirical results show clear improvements over baseline methods.
- **Medium Confidence**: The claim that transformers are particularly well-suited for this task is supported by the results but relies heavily on empirical evidence rather than theoretical justification. The success could potentially be replicated with other sequence models.
- **Medium Confidence**: The population-based self-play training procedure appears effective but the specific hyperparameters and balance between exploration and exploitation were tuned for the tested games and may not generalize directly.

## Next Checks

1. **Cross-game Generalization**: Apply GO-MCTS to a different class of imperfect information games (e.g., poker variants or board games with hidden information) to test whether the approach requires significant retraining or can generalize with minimal modifications.

2. **Ablation Study on Transformer Architecture**: Systematically vary the transformer architecture (depth, width, attention heads) and training data size to identify the minimum viable configuration that maintains performance, providing insight into the approach's robustness to architectural choices.

3. **Failure Mode Analysis**: Deliberately test GO-MCTS in scenarios where the generative model makes prediction errors to quantify how these errors propagate through the search process and identify whether there are built-in mechanisms to detect or recover from such failures.