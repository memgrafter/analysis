---
ver: rpa2
title: What is Fair? Defining Fairness in Machine Learning for Health
arxiv_id: '2406.09307'
source_url: https://arxiv.org/abs/2406.09307
tags:
- fairness
- health
- bias
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews fairness definitions and their use in health-focused
  machine learning. It examines why models may be unfair, introduces group, individual,
  and causal fairness frameworks, and discusses their applications and incompatibilities.
---

# What is Fair? Defining Fairness in Machine Learning for Health

## Quick Facts
- arXiv ID: 2406.09307
- Source URL: https://arxiv.org/abs/2406.09307
- Reference count: 40
- Primary result: Reviews fairness definitions and their use in health-focused ML, examining why models may be unfair and introducing group, individual, and causal fairness frameworks

## Executive Summary
This paper reviews commonly used fairness notions within group, individual, and causal-based frameworks for health-focused machine learning applications. It examines why machine learning models may be unfair, introduces mathematical definitions of fairness, and discusses their applications and incompatibilities. The paper emphasizes that fairness is complex and context-dependent, requiring careful consideration of different frameworks and their trade-offs. It concludes by highlighting opportunities for future research, particularly in causal fairness and addressing unique challenges in health data.

## Method Summary
The paper provides a comprehensive review of fairness definitions in machine learning for health applications, categorizing them into group, individual, and causal frameworks. It examines the mathematical foundations of each framework, their practical applications in healthcare settings, and the inherent incompatibilities between different fairness criteria. The review includes healthcare-specific examples and discusses challenges unique to health data, such as missing protected attributes and proxy variables. The paper also identifies future research directions and provides guidance for practitioners on selecting appropriate fairness approaches based on their specific contexts.

## Key Results
- Fairness definitions in ML primarily fall into three categories: group fairness (requiring similar performance across protected groups), individual fairness (ensuring similar predictions for similar individuals), and causal fairness (using causal models to identify sources of unfairness)
- Different fairness definitions are often mathematically incompatible, requiring practitioners to make deliberate choices about which fairness criteria to prioritize
- Healthcare data presents unique challenges for fairness assessments, including missing or unreliable protected attributes, proxy variables, and intersectional subgroups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper introduces a comprehensive framework for fairness in health ML by categorizing definitions into group, individual, and causal fairness, each with specific criteria and real-world examples.
- Mechanism: By structuring fairness definitions around three distinct frameworks, the paper allows practitioners to select and apply the most appropriate fairness metric based on their specific context and objectives. The inclusion of mathematical formulations alongside practical health-focused examples bridges the gap between theoretical concepts and real-world implementation.
- Core assumption: Different health applications require different fairness approaches, and understanding the trade-offs between group, individual, and causal fairness is essential for effective implementation.
- Evidence anchors:
  - [abstract] "We review commonly used fairness notions within group, individual, and causal-based frameworks."
  - [section] "Existing definitions of fairness primarily fall into three categories: group fairness, individual fairness, and causal fairness."
  - [corpus] Weak evidence - corpus contains related papers on fairness definitions but lacks direct comparison to this paper's comprehensive framework approach.
- Break condition: The framework becomes ineffective if practitioners cannot understand or navigate the trade-offs between different fairness approaches, or if the mathematical formulations are too complex for practical application.

### Mechanism 2
- Claim: The paper demonstrates that different fairness definitions are often mathematically incompatible, requiring practitioners to make deliberate choices about which fairness criteria to prioritize.
- Mechanism: By explicitly showing the mathematical incompatibilities between different fairness metrics (e.g., independence vs. sufficiency), the paper forces practitioners to engage with the trade-offs inherent in fairness definitions rather than assuming multiple criteria can be simultaneously satisfied. This transparency about incompatibilities prevents misguided attempts to optimize for incompatible metrics.
- Core assumption: Practitioners need to understand that achieving all fairness criteria simultaneously is mathematically impossible, and must therefore make informed trade-offs based on their specific context.
- Evidence anchors:
  - [section] "Independence, sufficiency, and separation provide different perspectives on what it means for a model to be fair. Except under highly restrictive conditions, it is not possible for an algorithm to fulfill all criteria simultaneously."
  - [section] "The following pairs of criteria are incompatible in the sense that they cannot generally be simultaneously satisfied: independence and sufficiency, independence and separation, and separation and sufficiency."
  - [corpus] Moderate evidence - corpus contains papers discussing fairness trade-offs but lacks the specific mathematical incompatibility analysis presented in this paper.
- Break condition: This mechanism fails if practitioners ignore the incompatibility warnings and attempt to optimize for multiple incompatible fairness metrics, or if they lack the mathematical background to understand the implications of these incompatibilities.

### Mechanism 3
- Claim: The paper bridges the gap between ML fairness research and healthcare applications by providing healthcare-specific examples and addressing unique challenges in health data.
- Mechanism: By grounding abstract fairness concepts in concrete healthcare scenarios (e.g., disease diagnosis, risk prediction, patient triage) and addressing health-specific challenges like missing data, proxy variables, and intersectional subgroups, the paper makes fairness concepts accessible and applicable to healthcare practitioners who may lack ML expertise.
- Core assumption: Healthcare practitioners need context-specific examples and guidance to effectively implement ML fairness concepts in their work.
- Evidence anchors:
  - [abstract] "We review commonly used fairness notions within group, individual, and causal-based frameworks... highlight opportunities and challenges in operationalizing fairness in health-focused applications."
  - [section] "The data used in health applications most often measures and categorizes people and therefore encodes societal structures, injustices, and stereotypes, such as gender, racial, and age bias."
  - [corpus] Moderate evidence - corpus contains healthcare ML papers but lacks the specific focus on bridging ML fairness research with healthcare practice that this paper provides.
- Break condition: This mechanism fails if the healthcare examples are not sufficiently representative of diverse healthcare settings, or if the health-specific challenges addressed are not comprehensive enough to cover the range of issues practitioners face.

## Foundational Learning

- Concept: Mathematical formulation of group fairness metrics (statistical parity, equalized odds, predictive parity)
  - Why needed here: The paper provides mathematical definitions alongside conceptual explanations, requiring understanding of probability notation and conditional probability to fully grasp the fairness criteria.
  - Quick check question: What is the mathematical difference between statistical parity and equalized odds, and why can't both be satisfied simultaneously when outcome prevalence differs across groups?

- Concept: Directed acyclic graphs (DAGs) and causal inference concepts
  - Why needed here: The paper introduces causal fairness concepts that rely on understanding causal relationships, counterfactuals, and DAG representations of variable dependencies.
  - Quick check question: In the counterfactual fairness example with hospital readmission, how does the DAG structure help identify whether using emergency department visits as a predictor introduces unfair bias?

- Concept: Trade-offs and incompatibilities in optimization problems
  - Why needed here: The paper emphasizes that different fairness criteria create competing optimization objectives, requiring understanding of multi-objective optimization and Pareto optimality concepts.
  - Quick check question: If a model must balance between minimizing false negatives for a disease screening application and ensuring equal false positive rates across demographic groups, what mathematical framework would help identify the optimal trade-off?

## Architecture Onboarding

- Component map: Bias identification (data, model, deployment stages) -> Three fairness frameworks (group, individual, causal) -> Incompatibility analysis -> Healthcare-specific examples and challenges -> Future research directions and practical recommendations
- Critical path: Start with bias identification in the data and model stages, select appropriate fairness framework(s) based on application context, evaluate chosen fairness metrics, assess incompatibilities and make deliberate trade-offs, implement mitigation strategies if needed, validate fairness improvements across diverse subgroups
- Design tradeoffs: Group fairness is easier to implement but may mask individual-level discrimination; individual fairness requires complex similarity metrics and computational resources; causal fairness provides mechanistic understanding but requires reliable causal models that may be difficult to specify in observational health data
- Failure signatures: Model shows good group fairness metrics but individual cases reveal discrimination; fairness improvements lead to significant accuracy degradation; different fairness metrics provide contradictory assessments of the same model; healthcare practitioners report that fairness definitions don't align with their ethical priorities
- First 3 experiments:
  1. Implement statistical parity, equalized odds, and predictive parity calculations on a healthcare dataset and demonstrate their mathematical incompatibility when outcome prevalence differs across groups
  2. Create a simple DAG-based simulation showing how confounding variables can create spurious associations between protected attributes and model predictions, then demonstrate counterfactual fairness as a solution
  3. Implement a fair representation learning approach (e.g., Zemel et al.'s method) on a healthcare dataset and evaluate both fairness improvements and accuracy trade-offs across different fairness metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective methods for achieving causal fairness in health-focused machine learning applications?
- Basis in paper: [explicit] The paper discusses the growing interest in causal fairness but notes that its use in health applications is still in early stages, presenting both opportunities and challenges.
- Why unresolved: Causal fairness requires reliable causal models, which are difficult to specify in complex health contexts, and issues of identifiability can arise, making it hard to measure fairness uniquely from data.
- What evidence would resolve it: Empirical studies comparing different causal fairness methods in diverse health applications, demonstrating their effectiveness in identifying and mitigating sources of bias.

### Open Question 2
- Question: How can individual fairness be operationalized in health-focused applications given the lack of agreed-upon standards for quantifying similarity between individuals?
- Basis in paper: [explicit] The paper notes that individual fairness is just emerging in health applications due to the difficulty in appropriately quantifying similarity across individuals and the absence of agreed-upon standards.
- Why unresolved: The implementation of individual fairness depends on user-defined similarity metrics, which are context-dependent and lack standardization, especially in health settings.
- What evidence would resolve it: Development and validation of context-specific similarity metrics