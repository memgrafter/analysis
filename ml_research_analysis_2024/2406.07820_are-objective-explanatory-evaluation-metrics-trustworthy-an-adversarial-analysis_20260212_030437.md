---
ver: rpa2
title: Are Objective Explanatory Evaluation metrics Trustworthy? An Adversarial Analysis
arxiv_id: '2406.07820'
source_url: https://arxiv.org/abs/2406.07820
tags:
- explanation
- explanations
- causal
- adversarial
- gradcam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the reliability of objective evaluation metrics
  for explainable AI (XAI) methods. The authors propose a novel adversarial explanation
  technique called SHAPE (SHifted Adversaries using Pixel Elimination) based on causal
  notions of necessity and sufficiency.
---

# Are Objective Explanatory Evaluation metrics Trustworthy? An Adversarial Analysis

## Quick Facts
- arXiv ID: 2406.07820
- Source URL: https://arxiv.org/abs/2406.07820
- Reference count: 0
- Key outcome: SHAPE outperforms traditional XAI methods on causal metrics but produces less interpretable explanations, suggesting current evaluation metrics may be unreliable

## Executive Summary
This paper investigates the reliability of objective evaluation metrics for explainable AI (XAI) methods by introducing SHAPE (SHifted Adversaries using Pixel Elimination), an adversarial explanation technique based on causal notions of necessity and sufficiency. SHAPE generates importance maps by quantifying how model predictions change when pixels are eliminated, achieving superior performance on insertion and deletion games compared to popular methods like GradCAM and GradCAM++. However, human inspection reveals these explanations are less interpretable, raising questions about the validity of current causal metrics for XAI evaluation and highlighting the need for human involvement in assessment.

## Method Summary
SHAPE generates adversarial explanations by quantifying the necessity of pixels through their elimination. The method calculates the change in model prediction when specific pixels are removed, using Monte Carlo approximation over randomly generated masks. The importance score for each pixel is the expected change in prediction probability given that the pixel is not observed. Experiments compare SHAPE's performance against GradCAM, GradCAM++, and RISE using insertion and deletion games on ImageNet DET test set images with ResNet50, ResNet101, and VGG16 models.

## Key Results
- SHAPE achieves higher insertion AUC and lower deletion AUC than GradCAM and GradCAM++ on causal metrics
- SHAPE's importance maps highlight causally necessary pixels but are less human-interpretable than traditional methods
- The discrepancy between SHAPE's performance on causal metrics and human interpretability suggests current evaluation metrics may be unreliable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAPE generates adversarial explanations by quantifying the necessity of pixels through their elimination.
- Mechanism: SHAPE calculates the change in model prediction when specific pixels are removed, using Monte Carlo approximation over randomly generated masks. The importance score for each pixel is the expected change in prediction probability given that the pixel is not observed.
- Core assumption: Causal necessity can be approximated by pixel elimination and the resulting change in model prediction.
- Evidence anchors:
  - [abstract] "SHAPE generates importance maps by quantifying the change in model prediction when pixels are eliminated"
  - [section] "Our method creates explanations by quantifying the shift in model response when a specific portion of pixels are eliminated"
  - [corpus] Weak evidence - no direct mention of SHAPE or pixel elimination methodology
- Break condition: If the model's prediction doesn't change significantly when important pixels are removed, or if the pixel-level approach fails to capture true causal features.

### Mechanism 2
- Claim: SHAPE outperforms traditional XAI methods on causal metrics (insertion and deletion games) despite being less human-interpretable.
- Mechanism: SHAPE achieves higher insertion AUC and lower deletion AUC by focusing on pixel necessity rather than human interpretability. The adversarial nature of SHAPE explanations means they highlight pixels that are causally necessary for the model's decision but may not correspond to human intuition.
- Core assumption: Causal metrics (insertion and deletion games) are valid measures of explanation quality, and pixel-level necessity correlates with causal importance.
- Evidence anchors:
  - [abstract] "Experiments show that SHAPE outperforms popular methods like GradCAM and GradCAM++ on causal metrics (insertion and deletion games)"
  - [section] "Our analysis shows that SHAPE outperforms popular explanatory techniques like GradCAM and GradCAM++ in these tests"
  - [corpus] Weak evidence - no direct mention of SHAPE's performance on insertion/deletion metrics
- Break condition: If human interpretability becomes a critical requirement for explanation quality, or if the causal metrics themselves are proven unreliable.

### Mechanism 3
- Claim: Current causal metrics may not be reliable for evaluating XAI methods, as demonstrated by SHAPE's performance.
- Mechanism: SHAPE's superior performance on insertion and deletion metrics despite being less interpretable suggests that these metrics may be "fooled" by pixel-level necessity calculations rather than true causal explanations. The adversarial nature of SHAPE reveals potential flaws in using insertion/deletion games as evaluation metrics.
- Core assumption: The discrepancy between SHAPE's performance on causal metrics and human interpretability indicates a fundamental flaw in the evaluation metrics themselves.
- Evidence anchors:
  - [abstract] "This suggests current causal metrics may not be reliable for evaluating XAI methods, highlighting the need for human involvement in assessment"
  - [section] "However human inspection of the explanation map proves otherwise"
  - [corpus] Weak evidence - no direct discussion of the reliability of causal metrics
- Break condition: If additional evidence shows that SHAPE's pixel-level approach genuinely captures causal importance, or if alternative evaluation metrics confirm SHAPE's superiority.

## Foundational Learning

- Concept: Causal necessity and sufficiency in machine learning explanations
  - Why needed here: SHAPE is built on the theoretical foundations of causal necessity, using pixel elimination to quantify how necessary certain pixels are for the model's prediction
  - Quick check question: How does the concept of "necessary cause" from causal literature translate to pixel importance in image classification?

- Concept: Insertion and deletion games as evaluation metrics
  - Why needed here: These causal metrics are the primary evaluation method used in the paper to compare SHAPE with other XAI techniques
  - Quick check question: What is the difference between insertion and deletion games, and how do they relate to necessity and sufficiency?

- Concept: Adversarial attacks in machine learning
  - Why needed here: SHAPE is described as an "adversarial explanation" technique, extending the concept of adversarial attacks to the explanation space
  - Quick check question: How does the concept of adversarial attacks in traditional machine learning relate to the adversarial explanations proposed in SHAPE?

## Architecture Onboarding

- Component map:
  Image tensor -> Random binary masks (7x7) -> Upsampled masks -> Masked images -> Model inference -> Score calculation -> Monte Carlo aggregation -> Importance map

- Critical path:
  1. Generate random binary masks
  2. Apply masks to input image (element-wise multiplication)
  3. Feed masked images and original image to model
  4. Calculate change in prediction probability
  5. Aggregate importance scores using weighted sum of masks

- Design tradeoffs:
  - Mask size vs. computational efficiency: Smaller masks require more samples for accurate estimation
  - Pixel-level vs. region-level analysis: SHAPE focuses on individual pixels, potentially missing broader contextual information
  - Necessity vs. sufficiency: SHAPE emphasizes necessity, potentially at the expense of identifying sufficient features

- Failure signatures:
  - Low variance in deletion/insertion scores across different mask samples
  - Importance maps that are entirely black or white
  - Poor correlation between SHAPE scores and other explanation methods

- First 3 experiments:
  1. Compare SHAPE's deletion and insertion AUC scores with GradCAM on a small image dataset (e.g., CIFAR-10)
  2. Visualize SHAPE importance maps for different classes on the same image to identify patterns in pixel necessity
  3. Test SHAPE's robustness to adversarial attacks on the model itself (does removing SHAPE-highlighted pixels affect model accuracy?)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What alternative evaluation metrics could replace insertion and deletion games to more accurately assess the quality of visual explanations?
- Basis in paper: [explicit] The paper demonstrates that current causal metrics (insertion and deletion games) can be fooled by adversarial explanations like SHAPE, suggesting they are not reliable for evaluating XAI methods.
- Why unresolved: The paper does not propose specific alternative metrics, only highlighting the need for human involvement and questioning the current metrics' validity.
- What evidence would resolve it: Development and validation of new evaluation metrics that align better with human perception of explanation quality, tested across various XAI methods and datasets.

### Open Question 2
- Question: How can the concept of necessity and sufficiency be better integrated into visual explanation methods to avoid adversarial attacks?
- Basis in paper: [explicit] The paper uses necessity and sufficiency to create adversarial explanations (SHAPE) that outperform other methods on causal metrics but are not human-interpretable.
- Why unresolved: The paper shows that while necessity and sufficiency are theoretically sound, their implementation in SHAPE leads to adversarial explanations that do not increase trust or understanding.
- What evidence would resolve it: A method that uses necessity and sufficiency to generate explanations that are both theoretically sound and human-interpretable, validated through user studies.

### Open Question 3
- Question: What are the implications of adversarial explanations like SHAPE for the trustworthiness of deep learning models in critical applications?
- Basis in paper: [explicit] The paper raises concerns about the reliability of causal metrics and the need for human involvement in evaluating explanations, which is crucial for trust in applications like medical diagnosis and autonomous driving.
- Why unresolved: The paper does not explore the broader implications of adversarial explanations on model trustworthiness in real-world scenarios.
- What evidence would resolve it: Case studies or experiments showing how adversarial explanations impact user trust and decision-making in critical applications, and strategies to mitigate these effects.

### Open Question 4
- Question: How can human involvement be systematically incorporated into the evaluation of XAI methods to complement objective metrics?
- Basis in paper: [explicit] The paper concludes that human intervention and assessment are necessary for proper evaluation of visual explanations due to the limitations of current objective metrics.
- Why unresolved: The paper does not provide a framework or methodology for integrating human evaluation into the assessment process.
- What evidence would resolve it: A structured approach for combining human feedback with objective metrics, validated through experiments showing improved alignment with human understanding and trust.

## Limitations
- SHAPE's explanations are less human-interpretable than traditional methods, limiting their practical utility
- The paper relies solely on causal metrics for evaluation without incorporating human assessment
- The method's performance on real-world applications and diverse datasets is not explored

## Confidence

**Confidence Labels:**
- SHAPE's mechanism for generating adversarial explanations: Medium
- SHAPE's superior performance on causal metrics: Medium
- The unreliability of current causal metrics: Medium

## Next Checks

1. Replicate SHAPE's insertion and deletion scores on a small image dataset (e.g., CIFAR-10) using a simple CNN model
2. Conduct a user study comparing human interpretation of SHAPE explanations with GradCAM to validate the claim about interpretability
3. Test SHAPE's explanations against alternative evaluation metrics (e.g., Pointing Game, Sanity Checks) to assess metric reliability across different evaluation frameworks