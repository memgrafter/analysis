---
ver: rpa2
title: 'SAGEval: The frontiers of Satisfactory Agent based NLG Evaluation for reference-free
  open-ended text'
arxiv_id: '2411.16077'
source_url: https://arxiv.org/abs/2411.16077
tags:
- evaluation
- arxiv
- agent
- text
- criteria
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAGEval addresses the challenge of evaluating open-ended, reference-free
  natural language generation (NLG) outputs from large language models, particularly
  for structured content like forms, surveys, and quizzes. The framework introduces
  a critiquing agent (SAGE Agent) that validates and rectifies scores assigned by
  an LLM evaluator, addressing the lack of ground-truth references.
---

# SAGEval: The frontiers of Satisfactory Agent based NLG Evaluation for reference-free open-ended text

## Quick Facts
- arXiv ID: 2411.16077
- Source URL: https://arxiv.org/abs/2411.16077
- Reference count: 29
- Primary result: Achieves up to 20% improvement in correlation with human judgments for evaluating open-ended, reference-free NLG outputs

## Executive Summary
SAGEval introduces a novel framework for evaluating open-ended, reference-free natural language generation outputs using a critiquing agent (SAGE Agent) that validates and rectifies scores from an LLM evaluator. The framework addresses the challenge of evaluating structured content like forms, surveys, and quizzes without ground-truth references by expanding evaluation criteria beyond traditional dimensions. Experiments on 96 human-annotated surveys demonstrate significant improvements in alignment with human judgments compared to existing methods.

## Method Summary
SAGEval employs a dual-agent framework consisting of an Evaluator Agent based on G-Eval principles and a SAGE Agent that critiques and rectifies scores. The Evaluator Agent generates initial scores across eight predefined aspects (Accuracy, Semantic Diversity, Coherence, Relevancy, Audience Understandability, Audience Engagement, Fairness, and Sentiment/Tone type) using chain-of-thought reasoning and few-shot exemplars. The SAGE Agent then acts as a meta-evaluator, reviewing these scores and suggesting modifications or new scoring criteria to enhance evaluation coverage. This approach reduces dependency on labeled data while improving evaluation reliability for complex NLG tasks.

## Key Results
- Achieves up to 20% improvement in correlation with human judgments compared to G-Eval and CheckEval
- SAGE Agent successfully identifies gaps in scoring criteria, suggesting additions like creativity and content quality scores
- Effective at evaluating structured content like forms, surveys, and quizzes without ground-truth references

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The critiquing SAGE Agent rectifies scores by analyzing Evaluator Agent outputs and providing feedback without ground-truth references
- Mechanism: The SAGE Agent acts as a meta-evaluator that reviews scores from the Evaluator Agent and either corrects them or suggests modifications to scoring criteria definitions, enabling reference-free evaluation
- Core assumption: The SAGE Agent can effectively critique and improve scores without access to ground-truth references by leveraging its own judgment capabilities
- Evidence anchors: [abstract] "SAGEval utilizes a critiquing Agent to provide feedback on scores generated by LLM evaluators" and [section] "SAGE Agent is designed to objectively look at each instance of open-ended reference-free text to provide insights on how to rectify assigned scores by Evaluator Agent"

### Mechanism 2
- Claim: SAGEval improves correlation with human judgments by 20% compared to existing methods through comprehensive scoring criteria
- Mechanism: By expanding evaluation criteria beyond traditional dimensions (Coherence, Fluency, Relevancy, Consistency) to include Accuracy, Semantic Diversity, Audience Engagement, Fairness, and Sentiment/Tone type, SAGEval captures more nuanced aspects of open-ended text quality
- Core assumption: The expanded set of scoring criteria better captures the quality dimensions that humans consider when evaluating open-ended reference-free text
- Evidence anchors: [abstract] "Experiments on a dataset of 96 human-annotated surveys show that SAGEval achieves higher correlation with human judgments (up to 20% improvement)" and [section] "We expand the criteria typically used by LLM evaluators for scoring, and do not limit the Agents to judge on Coherence, Fluency, Relevancy, and Consistency only"

### Mechanism 3
- Claim: SAGEval identifies gaps in scoring criteria and suggests new dimensions like Creativity and Content Quality scores
- Mechanism: The SAGE Agent not only critiques existing scores but also analyzes whether predefined aspects comprehensively cover the evaluation task, suggesting additional scoring criteria when gaps are identified
- Core assumption: LLM evaluators can identify missing evaluation dimensions in open-ended text that human evaluators would also consider important
- Evidence anchors: [abstract] "The framework also identifies gaps in scoring criteria, proposing additions like creativity and content quality scores" and [section] "SAGE Agent is prompted to also ensure if the pre-defined scoring criteria is comprehensively covering the evaluation task"

## Foundational Learning

- **Role-based agent frameworks**: SAGEval relies on agents assuming specific roles (Evaluator Agent and SAGE Agent) to perform different aspects of the evaluation process
  - Why needed here: Different evaluation tasks require distinct capabilities and perspectives
  - Quick check question: What distinguishes the Evaluator Agent from the SAGE Agent in terms of their roles and responsibilities?

- **Chain-of-Thought (CoT) reasoning in LLMs**: The Evaluator Agent uses CoT reasoning to execute the evaluation task and provide reasoning for assigned scores
  - Why needed here: Enables systematic, step-by-step evaluation that mimics human reasoning processes
  - Quick check question: How does incorporating CoT reasoning improve the quality and consistency of scores assigned by the Evaluator Agent?

- **In-Context Learning (ICL) with exemplars**: Both agents use few-shot exemplars for ICL to understand how to format responses and perform their evaluation tasks
  - Why needed here: Helps agents understand expected output formats and evaluation approaches without extensive training
  - Quick check question: What role do the exemplars play in helping agents understand the expected output format and evaluation approach?

## Architecture Onboarding

- **Component map**: Open-ended text input -> Evaluator Agent (initial scoring) -> SAGE Agent (critique and rectification) -> Refined scores with feedback
- **Critical path**: 
  1. Receive open-ended text input
  2. Evaluator Agent generates initial scores across 8 predefined aspects
  3. SAGE Agent critiques these scores and suggests corrections
  4. Output final scores with potential new scoring criteria suggestions
- **Design tradeoffs**: Multiple agent calls vs. single comprehensive evaluation, expanded scoring criteria vs. evaluation complexity, self-reflection capability vs. computational cost
- **Failure signatures**: Consistent score disagreements between Evaluator and SAGE Agents, low correlation between SAGEval scores and human judgments, SAGE Agent suggests irrelevant or redundant scoring criteria
- **First 3 experiments**:
  1. Run Evaluator Agent alone on a sample dataset and measure correlation with human judgments
  2. Run SAGEval (both agents) on the same dataset and compare improvement in correlation
  3. Test SAGE Agent's ability to identify missing scoring criteria by analyzing a subset of data and comparing suggested criteria to human-identified gaps

## Open Questions the Paper Calls Out
- How does SAGEval's performance compare when evaluating open-ended text in different structural formats (e.g., JSON, tables) compared to its current evaluation of surveys/forms?
- Can SAGEval's SAGE Agent effectively identify and suggest new scoring criteria for open-ended text evaluation tasks beyond surveys/forms, such as code generation or creative writing?
- How does the cost of using SAGEval, particularly the multiple LLM calls for the SAGE Agent, compare to other LLM-based evaluation methods in terms of productionization and scalability?

## Limitations
- The 20% improvement is based on a relatively small dataset (96 samples), raising questions about generalizability
- Exact prompt engineering and exemplars are not fully specified, making reproduction difficult
- Framework's reliance on LLM agents introduces potential biases inherent in base models

## Confidence
- **High Confidence**: The core concept of using critiquing agents to improve reference-free NLG evaluation is well-grounded in existing research on agent-based evaluation systems
- **Medium Confidence**: The claim of 20% improvement in correlation with human judgments is promising but needs validation on larger and more diverse datasets
- **Low Confidence**: The scalability of SAGEval to other NLG tasks beyond forms, surveys, and quizzes is uncertain without additional experiments

## Next Checks
1. **Dataset Expansion and Diversity Test**: Replicate experiments on a significantly larger and more diverse dataset of open-ended reference-free text, including different domains and NLG tasks, to assess generalizability and robustness
2. **Prompt Engineering Sensitivity Analysis**: Systematically vary prompt templates and exemplars for both Evaluator and SAGE Agents to determine impact on evaluation quality and consistency
3. **Cross-Model Evaluation Consistency**: Test framework using different LLM architectures for Evaluator and SAGE Agents to evaluate how model choice affects evaluation outcomes and identify potential biases