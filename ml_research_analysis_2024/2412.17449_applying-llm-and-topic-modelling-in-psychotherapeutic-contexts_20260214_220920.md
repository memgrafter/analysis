---
ver: rpa2
title: Applying LLM and Topic Modelling in Psychotherapeutic Contexts
arxiv_id: '2412.17449'
source_url: https://arxiv.org/abs/2412.17449
tags:
- your
- topic
- what
- topics
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies BERTopic, a machine learning-based topic modeling
  tool, to analyze therapist remarks in psychotherapy sessions. The method involves
  preprocessing text data, constructing vector spaces using Sentence-Transformer embeddings,
  reducing dimensionality with UMAP, clustering with HDBSCAN, and optimizing topic
  representations using large language models.
---

# Applying LLM and Topic Modelling in Psychotherapeutic Contexts

## Quick Facts
- arXiv ID: 2412.17449
- Source URL: https://arxiv.org/abs/2412.17449
- Authors: Alexander Vanin; Vadim Bolshev; Anastasia Panfilova
- Reference count: 5
- Primary result: BERTopic identified 43 topics for classical therapists and 46 for modern therapists, with topic coherence improving from 0.429 to 0.47 and 0.377 to 0.431, respectively.

## Executive Summary
This study applies BERTopic, a transformer-based topic modeling technique, to analyze therapist remarks in psychotherapy sessions. The research compares classical therapists (Carl Rogers, Fritz Perls, Albert Ellis) with modern approaches (CBT, DBT, TA, ACT, etc.) to identify stable themes across therapeutic styles. Expert refinement combined with LLM optimization improves topic coherence and interpretability. The analysis reveals significant overlap between classical and modern therapeutic topics, particularly in areas like fear, anger, work-related anxiety, and relationships.

## Method Summary
The study applies BERTopic to transcribed psychotherapy sessions from YouTube recordings, using Sentence-Transformer embeddings for semantic representation. Text data is preprocessed and converted to embeddings, then dimensionality is reduced using UMAP before clustering with HDBSCAN. Topics are represented using c-TF-IDF and optimized through expert refinement and GPT-based labeling. The analysis compares 19 classical therapist sessions with 111 modern therapist sessions, yielding 43 and 46 topics respectively with improved coherence scores.

## Key Results
- BERTopic identified 43 topics for classical therapists and 46 for modern therapists
- Topic coherence improved from 0.429 to 0.47 (classical) and 0.377 to 0.431 (modern)
- Cosine similarity analysis revealed high overlap in topics between classical and modern therapists, particularly in fear, anger, work-related anxiety, and relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERTopic leverages transformer-based embeddings to capture semantic relationships without requiring extensive preprocessing like stop-word removal or lemmatization.
- Mechanism: The model uses Sentence-Transformer embeddings to construct a dense vector space where semantic similarity is preserved, enabling more nuanced topic clustering than traditional bag-of-words approaches.
- Core assumption: Transformer embeddings encode semantic meaning in a way that clustering algorithms can effectively identify topic groupings.
- Evidence anchors:
  - [abstract] "BERTopic, a machine learning technique that leverages transformer-based models to identify and extract meaningful topics from unstructured datasets"
  - [section] "We created the vector space from document embeddings obtained using language neural networks with the transformer architecture... allowing not only to effectively take into account semantic relationships in texts, but also to avoid many stages of text preprocessing"
- Break condition: If the embeddings fail to capture domain-specific terminology or if the clustering algorithm cannot handle the density of the vector space, topic coherence would degrade.

### Mechanism 2
- Claim: The combination of UMAP dimensionality reduction and HDBSCAN clustering enables stable topic formation from high-dimensional embeddings.
- Mechanism: UMAP reduces the embedding space to a manageable dimensionality while preserving local and global structure, and HDBSCAN identifies clusters at varying densities without requiring a predetermined number of clusters.
- Core assumption: The reduced dimensional space maintains sufficient information for clustering to identify meaningful topics.
- Evidence anchors:
  - [section] "Dimensionality reduction was performed using the umap-learn library... We chose the UMAP method with the following hyperparameters... HDBSCAN was used for the current research because it offered the most reliable and accurate text data clustering"
- Break condition: If UMAP's hyperparameters are poorly tuned or HDBSCAN's minimum cluster size is too restrictive, meaningful topics may be merged or discarded.

### Mechanism 3
- Claim: Expert refinement of automatically generated topics improves interpretability and coherence beyond what pure algorithmic clustering achieves.
- Mechanism: Human experts merge semantically similar clusters identified by the algorithm, remove low-value topics, and optimize topic representations using LLM-generated labels.
- Core assumption: Human expertise can identify meaningful patterns that algorithms might miss, especially in domain-specific contexts like psychotherapy.
- Evidence anchors:
  - [abstract] "Along with the automatic topical modeling by the BERTopic, the research involved an expert assessment of the findings and manual topic structure optimization"
  - [section] "Through expert interpretation of the topic modeling results, we were able to identify a large number of similar clusters in terms of the subjects' meanings... So as to combine the clusters we applied the merge_topics method"
- Break condition: If expert bias dominates the refinement process, the objectivity of the topic modeling may be compromised.

## Foundational Learning

- Concept: Vector space embeddings in NLP
  - Why needed here: Understanding how transformer models convert text into numerical representations is crucial for grasping how BERTopic works
  - Quick check question: What advantage do transformer-based embeddings have over traditional TF-IDF representations for topic modeling?

- Concept: Dimensionality reduction techniques
  - Why needed here: UMAP is used to make high-dimensional embedding spaces tractable for clustering while preserving semantic relationships
  - Quick check question: How does UMAP differ from PCA in preserving the structure of high-dimensional data?

- Concept: Clustering algorithms for text data
  - Why needed here: HDBSCAN is chosen for its ability to find clusters of varying densities and handle noise in text data
  - Quick check question: What is the key advantage of density-based clustering like HDBSCAN over centroid-based methods like k-means for topic modeling?

## Architecture Onboarding

- Component map: Data preprocessing → Sentence-Transformer embeddings → UMAP dimensionality reduction → HDBSCAN clustering → c-TF-IDF topic representation → Expert refinement → LLM optimization
- Critical path: Embedding generation → Dimensionality reduction → Clustering → Topic interpretation
- Design tradeoffs: Using pre-trained embeddings provides semantic richness but may include irrelevant information; HDBSCAN handles variable density but may merge related topics
- Failure signatures: Low topic coherence scores, excessive number of small clusters, or topics with unclear semantic boundaries
- First 3 experiments:
  1. Run BERTopic on a small subset of the data with default parameters to verify the pipeline works
  2. Compare UMAP and PCA dimensionality reduction on the same dataset to evaluate impact on clustering quality
  3. Test different HDBSCAN minimum cluster sizes to find the optimal balance between topic granularity and coherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do client topics and concerns compare to therapist topics across classical and modern therapeutic approaches?
- Basis in paper: [explicit] The paper suggests analyzing client topics in future work and notes that understanding common client needs could inform therapist training
- Why unresolved: The current study only analyzed therapist remarks, not client contributions
- What evidence would resolve it: Comparative analysis of client vs therapist topics using the same BERTopic methodology on client utterances

### Open Question 2
- Question: How do therapeutic topics evolve dynamically throughout individual therapy sessions?
- Basis in paper: [explicit] The authors propose studying "dynamic development of topics throughout a single therapeutic session" as future work
- Why unresolved: The current analysis only identified static topic clusters, not temporal topic progression
- What evidence would resolve it: Time-series analysis of topic distributions within therapy sessions using sliding windows or sequential topic modeling

### Open Question 3
- Question: What is the optimal integration strategy for digital assistants that provide real-time topic analysis and intervention suggestions during therapy sessions?
- Basis in paper: [inferred] The authors mention potential for "digital assistants for therapists" that could "analyze and record the most significant moments of a therapy session, offering therapists suggestions for further exploration and intervention"
- Why unresolved: The paper only conceptualizes this possibility without addressing implementation challenges or therapist acceptance
- What evidence would resolve it: Usability studies and randomized controlled trials comparing therapy outcomes with and without digital assistant support

## Limitations

- Dataset representativeness: The study relies on YouTube recordings, which may not represent typical therapeutic discourse due to selection bias, recording quality variations, and the specific therapists chosen (19 classical vs 111 modern sessions).
- Generalizability: Results are based on therapist remarks only, not patient-therapist interactions, limiting applicability to broader therapeutic contexts.
- Expert bias: Manual topic refinement introduces subjective interpretation that may not be fully reproducible across different expert panels.

## Confidence

- High confidence: Topic coherence improvements (0.429→0.47 for classical, 0.377→0.431 for modern) are directly measurable from the BERTopic implementation.
- Medium confidence: Overlap findings between classical and modern therapists are supported by cosine similarity analysis, but the semantic interpretation of these overlaps requires expert judgment.
- Medium confidence: The BERTopic methodology itself is well-established, though parameter choices (UMAP n_neighbors=15, HDBSCAN min_cluster_size=40) may affect results.

## Next Checks

1. Apply the same BERTopic pipeline to a different psychotherapy dataset (e.g., transcribed therapy sessions from clinical databases) to verify topic stability across contexts.
2. Systematically vary UMAP and HDBSCAN hyperparameters to determine their impact on topic coherence and cluster formation.
3. Have multiple therapy experts independently refine the topic clusters to assess inter-rater reliability and identify potential bias in the original refinement process.