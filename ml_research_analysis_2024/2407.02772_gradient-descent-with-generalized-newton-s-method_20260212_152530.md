---
ver: rpa2
title: Gradient descent with generalized Newton's method
arxiv_id: '2407.02772'
source_url: https://arxiv.org/abs/2407.02772
tags:
- learning
- rate
- training
- conference
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generalized Newton (GeN), a Hessian-informed
  optimization method that dynamically adapts learning rates for any base optimizer
  like SGD or Adam. By leveraging second-order Taylor expansion, GeN automatically
  computes optimal step sizes using only additional forward passes, avoiding expensive
  Hessian computations or hyperparameter tuning.
---

# Gradient descent with generalized Newton's method

## Quick Facts
- **arXiv ID**: 2407.02772
- **Source URL**: https://arxiv.org/abs/2407.02772
- **Reference count**: 40
- **Key outcome**: GeN achieves state-of-the-art performance across diverse tasks while requiring minimal hyperparameter tuning

## Executive Summary
This paper introduces Generalized Newton (GeN), a Hessian-informed optimization method that dynamically adapts learning rates for any base optimizer like SGD or Adam. By leveraging second-order Taylor expansion, GeN automatically computes optimal step sizes using only additional forward passes, avoiding expensive Hessian computations or hyperparameter tuning. The method is shown to achieve state-of-the-art performance across diverse tasks—image classification, language modeling, object detection, and recommendation systems—matching or surpassing carefully tuned schedulers while maintaining computational efficiency and scalability.

## Method Summary
GeN estimates optimal learning rates by fitting a quadratic loss model using second-order Taylor expansion, requiring only additional forward passes without explicit Hessian computation. The method generalizes to any base optimizer by applying second-order correction to pre-conditioned gradients, making it universally applicable. GeN achieves computational efficiency by estimating Hessian-vector products through curve fitting rather than expensive backpropagation operations, maintaining scalability to large models while providing automatic learning rate adaptation.

## Key Results
- Achieves state-of-the-art performance across 12 datasets including CIFAR-10, GLUE, and MovieLens-1M
- Matches or surpasses carefully tuned learning rate schedulers while requiring minimal hyperparameter tuning
- Demonstrates superior performance with large batch sizes compared to traditional optimizers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GeN accelerates convergence by automatically computing the optimal learning rate using second-order Taylor expansion, avoiding manual scheduler tuning.
- Mechanism: GeN estimates the optimal learning rate η* = G⊤gt / (gt⊤Htgt) by fitting a quadratic loss model using forward passes only, eliminating expensive Hessian computations.
- Core assumption: The loss function is well-approximated by a second-order Taylor expansion around the current point for small learning rates.
- Evidence anchors:
  - [abstract]: "automatically and dynamically selects the learning rate that accelerates the convergence"
  - [section]: "applying the Taylor expansion on the loss and leveraging (2.1), we can derive the loss improvement at the current iteration"
  - [corpus]: "Average neighbor FMR=0.428" (weak corpus evidence for second-order methods)
- Break condition: The second-order approximation breaks down for large learning rates or highly non-convex regions where higher-order terms dominate.

### Mechanism 2
- Claim: GeN generalizes to any base optimizer by applying the second-order correction to the pre-conditioned gradient, making it universally applicable.
- Mechanism: GeN transforms any base optimizer's update wt+1 = wt - ηtgt into wt+1 = wt - (g⊤Gt/gt⊤Hgt)gt, where gt is the pre-conditioned gradient from any optimizer.
- Core assumption: The pre-conditioned gradient gt from any optimizer can be used in the second-order Taylor framework.
- Evidence anchors:
  - [abstract]: "applies to any optimizer such as SGD and Adam, and covers the Newton-Raphson method as a sub-case"
  - [section]: "goptimt can come from any optimizer or even be random: e.g. we equip (3.2) with gSGDt to obtain GeN-SGD from SGD, or with gAdamt to obtain GeN-Adam from Adam"
  - [corpus]: "Average neighbor FMR=0.428" (weak corpus evidence for general applicability)
- Break condition: If the pre-conditioned gradient gt is poorly scaled or corrupted, the second-order correction may amplify errors.

### Mechanism 3
- Claim: GeN achieves computational efficiency by using forward passes instead of Hessian-vector products, making it scalable to large models.
- Mechanism: GeN estimates gt⊤Hgt and G⊤gt through curve fitting using multiple forward passes with different learning rates, avoiding the expensive Hessian-vector product computation.
- Core assumption: Forward passes without activation storage are sufficiently fast and memory-efficient compared to Hessian-vector products.
- Evidence anchors:
  - [abstract]: "only requires additional forward passes with almost zero computational overhead"
  - [section]: "we can estimate g⊤Htgt via the Hessian-vector product of Htgt, without directly accessing Ht"
  - [corpus]: "Average neighbor FMR=0.428" (weak corpus evidence for efficiency claims)
- Break condition: If forward passes become expensive due to model complexity or activation storage requirements, the efficiency advantage diminishes.

## Foundational Learning

- Concept: Second-order Taylor expansion
  - Why needed here: Provides the mathematical foundation for estimating the optimal learning rate by capturing curvature information in the loss landscape
  - Quick check question: Why does the second-order term η²/2 (gt⊤Hgt) matter for learning rate selection?

- Concept: Generalized inverse
  - Why needed here: Allows GeN to work with projected or pre-conditioned gradients by computing right inverses when the Hessian is not directly invertible
  - Quick check question: How does the generalized right inverse (gt⊤Ht)-1R differ from the standard matrix inverse?

- Concept: Finite difference methods
  - Why needed here: Provides the numerical technique for estimating Hessian-vector products using multiple forward passes instead of backpropagation
  - Quick check question: Why does using symmetric learning rates (-η, 0, η) improve the accuracy of finite difference estimates?

## Architecture Onboarding

- Component map:
  Base optimizer -> Pre-conditioned gradient gt -> Quadratic curve fitter -> Learning rate smoother -> Update scheduler

- Critical path:
  1. Standard forward pass computes loss L0
  2. Backpropagation computes pre-conditioned gradient gt
  3. Every Φ iterations, forward passes compute losses at ±ηt-1
  4. Quadratic fitting estimates optimal learning rate η*
  5. Smoothed learning rate update and parameter update

- Design tradeoffs:
  - Frequency vs. overhead: Higher Φ reduces computation but may miss important landscape changes
  - Batch size vs. estimation error: Larger batches reduce sub-sampling error in η* estimation
  - Forward passes vs. accuracy: More points in curve fitting improve accuracy but increase cost

- Failure signatures:
  - Divergence with small learning rates: Indicates poor quadratic fit or Hessian estimation issues
  - Oscillations in learning rate: Suggests unstable curvature estimation or insufficient smoothing
  - No improvement over base optimizer: May indicate that second-order information isn't beneficial for this problem

- First 3 experiments:
  1. Implement GeN-SGD on CIFAR10 with ResNet18 using Φ=1, compare convergence to SGD with tuned scheduler
  2. Test GeN-AdamW on GLUE benchmark with LoRA, verify performance matches published results
  3. Measure computational overhead of GeN vs. base optimizer with varying Φ on a large model (ViT)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GeN's performance scale with batch size, and what is the optimal batch size for different model architectures?
- Basis in paper: [explicit] The paper mentions that the sub-sampling error is dominant and that larger batch sizes improve performance, but doesn't provide systematic analysis of scaling behavior.
- Why unresolved: The experiments use fixed batch sizes (B=500 for most experiments) without exploring the trade-off between batch size and convergence speed across different model sizes and architectures.
- What evidence would resolve it: Systematic experiments varying batch sizes from small to very large (1000+) for different model architectures (ResNet, ViT, GPT2) showing convergence speed and final accuracy as functions of batch size.

### Open Question 2
- Question: What is the global convergence behavior of GeN compared to local convergence, and under what conditions does it achieve global optimality?
- Basis in paper: [explicit] The paper states "we remark that the learning rate in GeN is locally optimal but less is known about the global convergence speed" and provides local quadratic convergence analysis.
- Why unresolved: The theoretical analysis focuses on local convergence near the optimum, but deep learning loss landscapes are highly non-convex with multiple local minima, and it's unclear how GeN performs in escaping poor local minima.
- What evidence would resolve it: Empirical studies comparing GeN's ability to find better local minima compared to other optimizers across multiple random initializations and model architectures, plus theoretical analysis of global convergence properties.

### Open Question 3
- Question: How does GeN perform on extremely large-scale models (e.g., trillion-parameter models) and what are the practical limitations of the method?
- Basis in paper: [explicit] The paper discusses scalability and efficiency analysis for models up to 7B parameters, but doesn't test on the largest models or discuss limitations for trillion-parameter scale.
- Why unresolved: While the paper claims scalability, it doesn't address practical challenges like communication overhead in model-parallel training, memory constraints, or numerical stability for extremely large models.
- What evidence would resolve it: Experiments on the largest available models (7B+ parameters) in distributed settings, analysis of memory usage and communication patterns, and identification of practical bottlenecks or failure modes.

## Limitations
- Effectiveness relies heavily on second-order Taylor approximations in highly non-convex deep learning landscapes
- Computational efficiency claims assume forward passes without activation storage are negligible
- Robustness across diverse architectures and optimization landscapes remains to be thoroughly validated

## Confidence

- **High confidence**: The mathematical derivation of the learning rate formula using Taylor expansion is sound and the computational efficiency advantage over explicit Hessian computation is well-established
- **Medium confidence**: The empirical performance improvements across diverse tasks are demonstrated but may be sensitive to specific implementation details and hyperparameter choices (Φ parameter, smoothing coefficients)
- **Low confidence**: The claim of universal applicability to any base optimizer and the robustness to different batch sizes and model architectures requires more extensive validation across diverse settings

## Next Checks

1. **Break condition analysis**: Systematically test GeN's performance on tasks known to have highly non-convex loss landscapes (e.g., training deep residual networks with many layers) to identify where second-order approximations fail

2. **Computational overhead measurement**: Implement profiling to measure actual forward pass costs with and without activation storage, particularly for models with expensive attention mechanisms or large embedding layers

3. **Generalization robustness**: Test GeN across a wider range of optimizer combinations (SGD variants, Adam variants, second-order methods) and batch sizes to quantify the stability of learning rate estimation and performance consistency