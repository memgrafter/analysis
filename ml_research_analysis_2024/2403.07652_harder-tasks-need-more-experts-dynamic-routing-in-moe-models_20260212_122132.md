---
ver: rpa2
title: 'Harder Tasks Need More Experts: Dynamic Routing in MoE Models'
arxiv_id: '2403.07652'
source_url: https://arxiv.org/abs/2403.07652
tags:
- experts
- routing
- number
- more
- activated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a dynamic expert selection framework for Mixture
  of Experts (MoE) models, which adjusts the number of activated experts based on
  input difficulty. Unlike traditional MoE approaches that activate a fixed number
  of experts regardless of input complexity, this method dynamically selects experts
  based on confidence levels in expert selection for each input.
---

# Harder Tasks Need More Experts: Dynamic Routing in MoE Models

## Quick Facts
- arXiv ID: 2403.07652
- Source URL: https://arxiv.org/abs/2403.07652
- Authors: Quzhe Huang; Zhenwei An; Nan Zhuang; Mingxu Tao; Chen Zhang; Yang Jin; Kun Xu; Kun Xu; Liwei Chen; Songfang Huang; Yansong Feng
- Reference count: 16
- Primary result: Dynamic expert selection framework adjusts activated experts based on input difficulty, achieving 0.7% average improvement with <90% activated parameters

## Executive Summary
This paper introduces a dynamic expert selection framework for Mixture of Experts (MoE) models that adjusts the number of activated experts based on input difficulty. Unlike traditional MoE approaches that activate a fixed number of experts regardless of input complexity, this method dynamically selects experts based on confidence levels in expert selection for each input. The framework demonstrates substantial improvements over conventional Top-2 routing across various benchmarks while activating fewer parameters overall. The findings also reveal interesting layer-wise variation in expert activation requirements, with lower layers needing more experts than upper layers.

## Method Summary
The method implements a dynamic routing mechanism that computes a probability distribution over experts for each input token. If the highest probability exceeds a threshold p, only that expert is activated; otherwise, additional experts are progressively added until the cumulative probability exceeds p. This reduces unnecessary expert activation for simple inputs while ensuring sufficient experts for complex inputs. The model architecture follows LLaMA with 24 transformer layers, 1024 hidden dimension, 16 attention heads, and 16 experts per MoE layer. Training uses a combined loss function incorporating language modeling loss, load-balance loss, and dynamic loss to minimize routing entropy.

## Key Results
- Dynamic routing achieves an average 0.7% improvement over Top-2 routing across PIQA, Hellaswag, ARC-e, Commonsense QA, and BBH tasks
- Activates fewer than 90% of parameters compared to static routing while maintaining or improving performance
- Layer-wise analysis shows a gradual decrease in average experts activated per token from lower to upper layers
- Dynamic loss effectively minimizes routing entropy, leading to more confident expert selection over training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic routing activates fewer experts overall than fixed Top-2 while maintaining or improving performance by allocating experts based on input confidence.
- Mechanism: The model computes a probability distribution over experts for each input. If the highest probability exceeds a threshold p, only that expert is activated; otherwise, additional experts are progressively added until the cumulative probability exceeds p. This reduces unnecessary expert activation for simple inputs while ensuring sufficient experts for complex inputs.
- Core assumption: The routing network's probability output accurately reflects confidence in expert suitability for the input.
- Evidence anchors:
  - [abstract] "achieving an average improvement of 0.7% with less than 90% activated parameters"
  - [section] "Table 1 shows the performance of different models on downstream tasks... our proposed Dynamic Adaptive MoE demonstrates the best performance, achieving at least a 0.7% higher score on average compared to other models"
  - [corpus] Weak - corpus mentions related dynamic routing approaches but doesn't directly support this specific confidence-based mechanism
- Break condition: If the routing network's probability estimates don't correlate with actual expert performance, the dynamic selection would fail to allocate resources appropriately.

### Mechanism 2
- Claim: Layer-wise variation in expert activation requirements improves performance by avoiding overthinking in upper layers.
- Mechanism: Lower transformer layers activate more experts (up to 4) to capture rich shallow representations, while upper layers activate fewer experts (down to 1) to maintain generality and avoid overfitting to specific patterns.
- Core assumption: Different layers require different amounts of expert specialization, with lower layers needing more diverse processing and upper layers benefiting from simpler representations.
- Evidence anchors:
  - [section] "Another interesting finding is that the number of experts needed varies across different layers of the transformer... This may relate to the over-thinking phenomenon widely observed in deep neural networks"
  - [section] "Figure 4 displays the number of experts activated per token at different layers... we observe a gradual decrease in the average number of experts activated per token with increasing layer depth"
  - [corpus] Weak - corpus contains related work on layer-wise specialization but doesn't specifically mention overthinking mitigation
- Break condition: If the layer-wise pattern is reversed or uniform, the overthinking hypothesis would be invalidated.

### Mechanism 3
- Claim: Dynamic routing enables more efficient training by gradually reducing expert activation as training progresses.
- Mechanism: Early in training, more experts are activated per token to facilitate learning diverse representations; as training progresses, the model becomes more confident in its routing decisions, reducing the number of activated experts.
- Core assumption: The routing network's confidence increases with training, allowing for more selective expert activation without sacrificing performance.
- Evidence anchors:
  - [section] "Figure 3 shows the change in the average number of experts activated throughout the training process of 100B tokens... the number of experts activated per token decreases over time"
  - [section] "In the early stages of training, dynamic routing assigns more experts to each token, but after 60B tokens, the average number of activated experts is already less than 2"
  - [corpus] Weak - corpus mentions related training efficiency work but doesn't specifically address progressive expert reduction
- Break condition: If expert activation doesn't decrease with training or if performance degrades when reducing activation, the efficiency hypothesis would fail.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Understanding how MoE differs from dense models and how expert routing works is fundamental to grasping why dynamic routing provides benefits
  - Quick check question: In a standard MoE layer, how many experts are typically activated per token, and what determines which ones?

- Concept: Routing mechanisms and their limitations
  - Why needed here: The paper builds on Top-K routing; understanding its fixed nature and why it's suboptimal for varying input difficulties is crucial
  - Quick check question: What's the main limitation of Top-K routing when handling inputs of varying complexity?

- Concept: Confidence-based decision making in neural networks
  - Why needed here: The dynamic routing mechanism relies on interpreting probability distributions as confidence measures
  - Quick check question: How can probability outputs from a softmax layer be interpreted as confidence measures for decision making?

## Architecture Onboarding

- Component map: Input token -> Router network (probability computation) -> Expert selection logic (dynamic vs fixed) -> Expert computation -> Weighted combination -> Output

- Critical path:
  1. Token embedding enters MoE layer
  2. Router computes expert probabilities
  3. Dynamic selection logic determines which experts to activate
  4. Selected experts process the token
  5. Results are weighted and combined
  6. Loss is computed with dynamic and load-balance components

- Design tradeoffs:
  - Fixed vs dynamic expert activation: Fixed is simpler and more predictable; dynamic is more efficient but adds complexity
  - Threshold selection (p value): Lower thresholds activate more experts (potentially better performance but less efficiency); higher thresholds are more selective
  - Layer-wise variation: Adds architectural complexity but can improve performance by matching expert needs to layer functions

- Failure signatures:
  - Training instability: If dynamic routing causes extreme variance in activated experts, training may become unstable
  - Performance degradation: If the routing network doesn't accurately assess input difficulty, dynamic routing may underperform fixed routing
  - Load imbalance: Without proper load-balance loss, some experts may be underutilized while others are overloaded

- First 3 experiments:
  1. Implement dynamic routing with a fixed threshold and compare performance and efficiency against Top-2 routing on a small dataset
  2. Vary the threshold parameter p and measure its effect on expert activation patterns and downstream task performance
  3. Analyze layer-wise expert activation patterns to verify the expected decrease in expert activation from lower to upper layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic routing mechanism's performance scale with model size and training data?
- Basis in paper: [inferred] The paper mentions that their model is limited in size (374M parameters) and trained on 100B tokens, and references work by Dai et al. (2024) suggesting conclusions from smaller models can generalize to larger ones.
- Why unresolved: The paper explicitly states resource constraints limited their model size and training data, preventing direct testing on larger models.
- What evidence would resolve it: Training and evaluating the dynamic routing mechanism on models with significantly more parameters (e.g., 10B+) and substantially larger training datasets (e.g., 1T+ tokens) to compare performance gains against baseline MoE models.

### Open Question 2
- Question: What is the optimal threshold p for the dynamic routing mechanism across different tasks and model architectures?
- Basis in paper: [explicit] The paper explores different p values through inference on a pretrained model but acknowledges that training from scratch with various p values is resource-intensive.
- Why unresolved: The paper only provides inference results for different p values, not training results, and doesn't explore task-specific or architecture-specific optimal p values.
- What evidence would resolve it: Comprehensive training and evaluation of models with different p values across various tasks, model architectures, and sizes to determine optimal thresholds.

### Open Question 3
- Question: How does the layer-wise expert allocation in dynamic routing affect model interpretability and reasoning capabilities?
- Basis in paper: [explicit] The paper observes that dynamic routing activates more experts in lower layers and fewer in upper layers, which they attribute to mitigating overthinking, but doesn't explore interpretability implications.
- Why unresolved: The paper only speculates about the relationship between layer-wise allocation and overthinking without empirical analysis of how this affects the model's reasoning process or interpretability.
- What evidence would resolve it: Detailed analysis of attention patterns, feature representations, and reasoning chains across different layers when using dynamic routing versus fixed routing to understand how expert allocation affects model behavior.

## Limitations
- Evaluation limited to five downstream tasks from SuperGLUE benchmark, potentially overfitting to this specific setup
- Computational overhead of dynamic routing mechanism not quantified, which could offset efficiency gains
- Threshold parameter p=0.4 appears arbitrary without systematic exploration of its sensitivity to model performance

## Confidence
- **High Confidence**: The claim that dynamic routing achieves better performance with fewer activated parameters is well-supported by experimental results
- **Medium Confidence**: The layer-wise variation explanation linking to overthinking is supported empirically but remains somewhat speculative
- **Low Confidence**: The assumption that routing probabilities accurately reflect confidence in expert suitability lacks direct validation

## Next Checks
1. Evaluate the dynamic routing mechanism on tasks outside the SuperGLUE benchmark to assess generalization across diverse domains
2. Systematically vary the threshold parameter p across a range of values to determine optimal thresholds and sensitivity
3. Design an experiment where inputs of known difficulty levels are processed to verify correlation between routing probabilities and actual task complexity