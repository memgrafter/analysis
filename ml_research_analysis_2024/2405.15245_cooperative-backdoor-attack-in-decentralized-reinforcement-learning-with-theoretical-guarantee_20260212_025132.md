---
ver: rpa2
title: Cooperative Backdoor Attack in Decentralized Reinforcement Learning with Theoretical
  Guarantee
arxiv_id: '2405.15245'
source_url: https://arxiv.org/abs/2405.15245
tags:
- backdoor
- policy
- attack
- learning
- decentralized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a cooperative backdoor attack method for decentralized
  reinforcement learning called Co-Trojan. The core idea is to decompose a backdoor
  attack into multiple components according to the state space, with each malicious
  agent hiding one component in its policy.
---

# Cooperative Backdoor Attack in Decentralized Reinforcement Learning with Theoretical Guarantee

## Quick Facts
- arXiv ID: 2405.15245
- Source URL: https://arxiv.org/abs/2405.15245
- Reference count: 40
- This paper proposes Co-Trojan, the first provable cooperative backdoor attack in decentralized reinforcement learning

## Executive Summary
This paper introduces Co-Trojan, a novel cooperative backdoor attack method for decentralized reinforcement learning systems. The attack decomposes a backdoor trigger into multiple components, with each malicious agent hiding one component in its policy. When benign agents learn from all poisoned policies, the complete backdoor is assembled in their policies. The authors provide theoretical guarantees for the attack's success and demonstrate its effectiveness through experiments on Atari environments (Breakout and Seaquest). Co-Trojan achieves comparable attack performance to centralized backdoor attacks while being more difficult to detect.

## Method Summary
Co-Trojan operates by dividing the backdoor trigger into multiple components according to the state space, with each malicious agent responsible for hiding one component in its policy. The attack leverages the decentralized nature of the learning system, where benign agents learn from multiple other agents' policies. When benign agents learn from all malicious agents' policies, the complete backdoor is assembled in their policies. The method uses a surrogate model to approximate the benign agents' policies and optimizes the malicious agents' policies to maximize the surrogate model's performance on backdoor triggers while maintaining normal performance. Theoretical analysis proves that under certain conditions, the attack can successfully inject backdoors into benign agents' policies.

## Key Results
- Co-Trojan achieves comparable attack performance to centralized backdoor attacks while being more covert
- The attack maintains effectiveness across various poisoning conditions in decentralized learning environments
- Experimental results on Breakout and Seaquest Atari games demonstrate successful backdoor injection with high attack success rates
- The cooperative nature makes the attack more difficult to detect compared to traditional backdoor attacks

## Why This Works (Mechanism)
The attack exploits the inherent vulnerabilities in decentralized reinforcement learning systems where agents learn from multiple sources. By decomposing the backdoor into components distributed across multiple malicious agents, the attack leverages the aggregation effect when benign agents learn from all sources. The theoretical guarantee stems from the mathematical properties of policy optimization in decentralized settings, where the combination of multiple partially poisoned policies can result in a fully poisoned policy in the benign agents.

## Foundational Learning
- Decentralized reinforcement learning: Why needed - understanding the target system architecture; Quick check - agents learn from multiple other agents' policies
- Backdoor attacks in machine learning: Why needed - foundation for understanding attack objectives; Quick check - malicious triggers embedded in benign models
- Policy optimization in RL: Why needed - core mechanism for how agents learn; Quick check - gradient-based updates from other agents' policies
- Cooperative multi-agent systems: Why needed - understanding how distributed components can achieve collective goals; Quick check - multiple agents working together toward a common objective
- State space decomposition: Why needed - mechanism for distributing backdoor components; Quick check - dividing the environment into parts for different agents
- Surrogate modeling: Why needed - approximating target agents' learning dynamics; Quick check - model used to optimize malicious policies

## Architecture Onboarding

Component map: Malicious agents (each with partial backdoor) -> Benign agents' learning aggregation -> Complete backdoor in benign policies

Critical path: Backdoor decomposition -> Malicious policy optimization -> Benign agents' learning from multiple sources -> Backdoor activation

Design tradeoffs:
- Attack effectiveness vs. covertness: More components increase difficulty of detection but may reduce attack success rate
- Number of malicious agents vs. performance: More malicious agents can carry more complete backdoor information but increase detection risk
- Optimization complexity vs. theoretical guarantees: More sophisticated optimization provides better theoretical bounds but requires more computation

Failure signatures:
- Inconsistent performance across different states that should trigger the backdoor
- Unexpected policy correlations between malicious agents
- Performance degradation when certain agents are excluded from learning

First experiments:
1. Test backdoor injection success rate when varying the number of malicious agents
2. Measure detection difficulty by evaluating performance on clean vs. poisoned tasks
3. Analyze the impact of backdoor component size on attack effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes idealized learning conditions that may not hold in practical implementations
- Attack effectiveness depends on specific coordination patterns that may not generalize to all decentralized architectures
- Limited evaluation to Atari environments, which may not represent diverse real-world applications
- Assumes white-box knowledge of learning algorithms and benign agents' policies

## Confidence
- Theoretical guarantees: Medium - Proofs are sound under stated assumptions but practical applicability depends on idealized conditions
- Experimental results: High - Atari experiments clearly demonstrate effectiveness and comparison with baselines
- Generalizability claims: Low - Limited evaluation environments and specific attack assumptions restrict broader claims

## Next Checks
1. Test attack effectiveness across a broader range of RL environments beyond Atari games, including continuous control tasks and real-world applications
2. Evaluate the attack's robustness under partial observability conditions where malicious agents have incomplete information about the state space
3. Investigate detection methods that could identify the cooperative behavior patterns among malicious agents during the learning process