---
ver: rpa2
title: Using LLMs to Establish Implicit User Sentiment of Software Desirability
arxiv_id: '2408.01527'
source_url: https://arxiv.org/abs/2408.01527
tags:
- sentiment
- data
- user
- explanation
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates using LLMs for zero-shot sentiment analysis
  of implicit software desirability expressed through qualitative user experience
  data, addressing the challenge of quantifying user sentiment without explicit ratings.
  The research applies several LLMs (Claude Sonnet 3/3.5, GPT4, GPT4o) and traditional
  tools (TRBS, Vader) to Microsoft Product Desirability Toolkit data from 50 users
  of a gamification system.
---

# Using LLMs to Establish Implicit User Sentiment of Software Desirability

## Quick Facts
- arXiv ID: 2408.01527
- Source URL: https://arxiv.org/abs/2408.01527
- Reference count: 40
- This study demonstrates that LLMs significantly outperform traditional sentiment analysis tools for implicit sentiment detection in qualitative software desirability data, with GPT4o achieving a Pearson coefficient of 0.97.

## Executive Summary
This study investigates using large language models (LLMs) for zero-shot sentiment analysis of implicit software desirability expressed through qualitative user experience data. The research addresses the challenge of quantifying user sentiment without explicit ratings by applying several LLMs (Claude Sonnet 3/3.5, GPT4, GPT4o) and traditional tools to Microsoft Product Desirability Toolkit data from 50 users of a gamification system. The LLMs demonstrated statistically significant performance in detecting user sentiment, with GPT4o-Respondent achieving a Pearson coefficient of 0.97 and GPT4o-Avg5 showing strong performance with PC=0.92. All LLMs outperformed traditional sentiment analysis tools and provided confidence scores and explanations that enhanced understanding of user sentiment.

## Method Summary
The study employed a comparative analysis approach using Microsoft Product Desirability Toolkit data from 50 participants who evaluated a gamification system. Six different LLM models were tested: Claude Sonnet 3, Claude Sonnet 3.5, GPT4, GPT4o, and their variations. Each LLM was prompted to rate user sentiment on a 1-7 scale based on qualitative responses, with multiple variations including sentiment-only responses, confidence scores, and explanations. The LLM results were compared against traditional sentiment analysis tools (TRBS, Vader) and validated using statistical measures including Pearson correlation coefficients. The analysis examined both individual respondent scores and aggregated scores (average of 5 words and top 3 words) to determine optimal approaches for sentiment detection.

## Key Results
- GPT4o-Respondent achieved the highest performance with Pearson coefficient of 0.97 for individual sentiment detection
- GPT4o-Avg5 demonstrated strong overall performance with PC=0.92 when using average of 5 word scores
- All LLM models significantly outperformed traditional sentiment analysis tools (TRBS, Vader) in detecting implicit sentiment
- LLMs provided valuable confidence scores and explanations that enhanced understanding of user sentiment beyond numerical ratings

## Why This Works (Mechanism)
LLMs leverage their understanding of natural language semantics and contextual relationships to interpret implicit sentiment from qualitative data. Unlike traditional sentiment analysis tools that rely on predefined lexicons and rules, LLMs can understand nuanced expressions, sarcasm, and context-dependent sentiment. Their transformer architecture allows them to capture long-range dependencies in text and understand how different words and phrases relate to each other in expressing sentiment. The ability to generate explanations and confidence scores also provides transparency into the reasoning process, making the sentiment analysis more interpretable and actionable.

## Foundational Learning
- **Zero-shot learning**: Understanding how LLMs can perform tasks without explicit training data - needed to comprehend the methodology's approach to sentiment analysis without model fine-tuning; quick check: can the model perform basic sentiment analysis on new domains without additional training?
- **Implicit sentiment detection**: Recognizing sentiment that isn't explicitly stated through words like "good" or "bad" - needed to understand the core challenge being addressed; quick check: can the model identify positive sentiment in neutral-sounding phrases?
- **LLM confidence scoring**: How models can estimate their certainty in predictions - needed to evaluate the reliability of LLM outputs; quick check: does the confidence score correlate with actual accuracy?
- **Qualitative data analysis**: Processing non-numeric user feedback - needed to understand the data type being analyzed; quick check: can the model handle varied response lengths and styles?
- **Statistical validation**: Using Pearson correlation and other metrics to verify model performance - needed to assess the scientific rigor of the findings; quick check: are the statistical measures appropriate for the data distribution?
- **Transformer architecture**: Understanding the underlying technology enabling contextual understanding - needed to appreciate why LLMs excel at this task; quick check: does the model maintain context across long passages?

## Architecture Onboarding

Component Map:
Data Collection -> LLM Processing -> Sentiment Scoring -> Statistical Validation -> Confidence/Explanation Generation

Critical Path:
The critical path flows from qualitative user responses through LLM analysis to sentiment quantification, with the most time-sensitive step being the LLM processing of individual responses to generate timely insights for software evaluation.

Design Tradeoffs:
The study balances between using single LLM responses versus aggregated scores, trading off computational cost against accuracy. Single responses (GPT4o-Respondent) provide the highest accuracy but require more processing, while aggregated approaches offer faster processing with slightly reduced accuracy.

Failure Signatures:
Poor performance occurs when text is extremely short, when responses contain domain-specific jargon not well-represented in training data, or when user sentiment is highly nuanced and context-dependent. Traditional tools fail completely on implicit sentiment, while LLMs may show reduced confidence scores in challenging cases.

First Experiments:
1. Test LLM performance on varying response lengths to determine minimum text requirements
2. Compare single LLM versus ensemble approaches across different software domains
3. Validate confidence score reliability by testing on known sentiment benchmarks

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the length of PDT data impact the performance of LLMs in sentiment analysis?
- Basis in paper: [inferred] The paper mentions that the small amount of text in word/explanation pairs may have made it challenging for tools to match sentiment, yet LLMs still performed well.
- Why unresolved: The study focused on a specific dataset with a fixed length of text, so the impact of varying text lengths on LLM performance remains unexplored.
- What evidence would resolve it: Conducting experiments with PDT datasets of varying lengths and comparing LLM performance across these datasets would provide insights into the impact of text length.

### Open Question 2
- Question: What is the optimal approach for combining multiple LLM sentiment scores to improve accuracy?
- Basis in paper: [inferred] The study suggests that either GPT4o or Claude3.5 or a combination of both could serve as the basis for a sentiment analysis tool, but does not explore the optimal combination strategy.
- Why unresolved: The paper does not provide a detailed analysis of how to effectively combine scores from multiple LLMs to enhance overall accuracy.
- What evidence would resolve it: Experimenting with different combination strategies, such as weighted averages or ensemble methods, and evaluating their impact on sentiment analysis accuracy would clarify the optimal approach.

### Open Question 3
- Question: How does the order of PDT data affect the sentiment analysis performance of LLMs?
- Basis in paper: [explicit] The paper notes that Claude3 generated consistent base word scores only when pairs were sorted alphabetically by word.
- Why unresolved: The study only observed this effect with one LLM and did not systematically investigate the impact of data order on other LLMs or in different contexts.
- What evidence would resolve it: Conducting systematic experiments with various LLMs and data orderings to determine if and how order affects sentiment analysis performance would provide clarity.

### Open Question 4
- Question: To what extent does domain-specific training improve LLM performance in sentiment analysis of PDT data?
- Basis in paper: [explicit] The paper states that previous work found LLMs generally perform poorly on implicit sentiment analysis and domain-specific training was needed to improve performance.
- Why unresolved: The study used LLMs without domain-specific training, so the potential benefits of such training on PDT data remain untested.
- What evidence would resolve it: Training LLMs on domain-specific PDT data and comparing their performance with non-domain-trained models would reveal the extent of improvement from such training.

## Limitations
- Small sample size (50 users) limits generalizability across different software domains and user populations
- Domain specificity to gamification systems may not translate to other software evaluation contexts
- Potential biases in LLM responses and the computational cost implications of deploying these models at scale were not addressed

## Confidence
- High confidence: Core finding that LLMs significantly outperform traditional sentiment analysis tools for implicit sentiment detection (supported by Pearson coefficients of 0.97 and 0.92)
- Medium confidence: Generalizability of results beyond gamification context due to limited sample size and domain specificity
- Medium confidence: Reliability of LLM-generated explanations and confidence scores without independent validation

## Next Checks
1. Replicate the study across multiple software domains (productivity tools, social platforms, enterprise systems) with larger sample sizes (n>200) to test generalizability
2. Conduct a longitudinal study tracking how LLM sentiment detection performance evolves as models are updated or fine-tuned on new data
3. Perform a cost-benefit analysis comparing LLM-based sentiment analysis against traditional methods in terms of computational resources, time, and accuracy trade-offs for different organization sizes