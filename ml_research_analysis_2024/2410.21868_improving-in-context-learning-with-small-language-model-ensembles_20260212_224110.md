---
ver: rpa2
title: Improving In-Context Learning with Small Language Model Ensembles
arxiv_id: '2410.21868'
source_url: https://arxiv.org/abs/2410.21868
tags:
- supericl
- ensemble
- language
- slms
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Ensemble SuperICL, a method that improves
  in-context learning by leveraging the predictions and confidence scores of multiple
  fine-tuned small language models (SLMs) alongside a large language model (LLM).
  The approach constructs prompts that include SLM outputs for both in-context examples
  and test inputs, allowing the LLM to draw on specialized knowledge from multiple
  models.
---

# Improving In-Context Learning with Small Language Model Ensembles

## Quick Facts
- **arXiv ID**: 2410.21868
- **Source URL**: https://arxiv.org/abs/2410.21868
- **Reference count**: 12
- **Primary result**: Ensemble SuperICL improves ICL accuracy by 3-20 percentage points across GLUE benchmarks and medical domain tasks

## Executive Summary
This paper introduces Ensemble SuperICL, a method that enhances in-context learning by incorporating predictions and confidence scores from multiple fine-tuned small language models (SLMs) alongside a large language model (LLM). The approach constructs prompts that include SLM outputs for both in-context examples and test inputs, allowing the LLM to draw on specialized knowledge from multiple models. Experiments show that Ensemble SuperICL outperforms traditional ICL, single-SLM SuperICL, and majority vote baselines on four GLUE benchmark datasets and a medical domain labeling task, with accuracy improvements ranging from 3 to 20 percentage points.

## Method Summary
Ensemble SuperICL constructs enhanced prompts by appending predictions and confidence scores from multiple fine-tuned SLMs for both in-context examples and test inputs. The method samples 8-32 in-context examples from training data, passes them through selected SLMs (MobileBERT, flan-t5-base, ELECTRA-large, DeBERTa-large, RoBERTa-large, BART-large, T5-large) to obtain predictions and confidence scores (sigmoid of logit probabilities), then constructs prompts combining these outputs with the test input and an instruction prompt. The LLM (Llama3-8b-Instruct) processes these enhanced prompts to make final predictions using greedy decoding.

## Key Results
- Outperforms traditional ICL, single-SLM SuperICL, and majority vote baselines on GLUE datasets (SST-2, MRPC, MNLI, CoLA)
- Achieves 3-20 percentage point accuracy improvements across datasets, with largest gains on challenging tasks
- Ablation studies confirm all components (SLM predictions for examples, confidence scores, test input predictions) are essential for optimal performance
- Demonstrates effectiveness with SLMs fine-tuned on general tasks rather than domain-specific data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The LLM uses SLM predictions and confidence scores to triangulate correct responses and learn SLM reliability
- **Mechanism**: By observing multiple SLMs' predictions for the same in-context examples alongside true labels, the LLM builds a reliability model for each SLM and weighs predictions accordingly
- **Core assumption**: The LLM can effectively process and reason about structured ensemble outputs (predictions + confidence scores) in the prompt context
- **Evidence anchors**: Abstract mentions LLM draws on specialized knowledge; section explains motivation for context construction; corpus provides weak support
- **Break condition**: If the LLM cannot effectively parse the structured ensemble format or if SLM predictions are too noisy/unreliable, the triangulation benefit disappears

### Mechanism 2
- **Claim**: Ensembling multiple SLMs provides more diverse and robust predictions than any single SLM
- **Mechanism**: Different SLMs may capture different aspects of the task or have complementary strengths; the LLM can identify patterns across multiple predictions
- **Core assumption**: SLMs fine-tuned on similar tasks capture complementary rather than redundant information
- **Evidence anchors**: Abstract mentions leveraging predictions from multiple SLMs; section shows ensemble outperforms baselines; corpus provides weak support
- **Break condition**: If SLMs are too similar in their predictions (low diversity) or if some SLMs consistently provide misleading information, the ensemble benefit diminishes

### Mechanism 3
- **Claim**: Confidence scores help the LLM calibrate its trust in SLM predictions
- **Mechanism**: SLM confidence scores (sigmoid of logit probabilities) provide quantitative measure of prediction certainty; the LLM can use this to weigh predictions
- **Core assumption**: SLM confidence scores are meaningful and correlate with actual prediction accuracy
- **Evidence anchors**: Abstract mentions confidence scores; section explains confidence score representation; corpus provides weak support
- **Break condition**: If SLM confidence scores are poorly calibrated or if the LLM cannot effectively use them for weighting, this mechanism fails

## Foundational Learning

- **Concept: In-context learning (ICL)**
  - **Why needed here**: Ensemble SuperICL builds directly on ICL by enhancing the demonstration examples with SLM outputs
  - **Quick check question**: How does ICL differ from fine-tuning in terms of model adaptation and computational requirements?

- **Concept: Ensemble methods**
  - **Why needed here**: The core innovation combines multiple model predictions to improve accuracy beyond any single model
  - **Quick check question**: What are the key differences between majority voting and weighted voting in ensemble methods?

- **Concept: Prompt engineering**
  - **Why needed here**: Effective prompt design is critical for incorporating SLM predictions and confidence scores in a way the LLM can process
  - **Quick check question**: How does the format and structure of prompt content affect LLM performance on downstream tasks?

## Architecture Onboarding

- **Component map**: SLM ensemble → Context builder → LLM inference → Evaluation pipeline
- **Critical path**: SLM predictions → Context construction → LLM inference → Evaluation
- **Design tradeoffs**:
  - Number of SLMs vs. prompt length limits
  - SLM diversity vs. computational cost
  - Confidence score granularity vs. LLM processing capability
  - Prompt format complexity vs. robustness
- **Failure signatures**:
  - Performance worse than single SLM: suggests poor SLM selection or prompt format issues
  - High variance across seeds: suggests sensitivity to demonstration selection
  - Confidence score miscalibration: suggests SLM fine-tuning or confidence computation issues
- **First 3 experiments**:
  1. Replicate single SLM SuperICL results as baseline
  2. Test ensemble with two SLMs on a simple dataset (SST-2)
  3. Conduct ablation study removing confidence scores to measure their impact

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of Ensemble SuperICL scale with the number of small language models beyond five?
  - **Basis in paper**: [explicit] The paper tests up to 5 SLMs and notes that the optimal configuration varies considerably across datasets, but does not explore beyond this limit
  - **Why unresolved**: The study only considers combinations of 2-5 SLMs and suggests that the optimal number varies by dataset, leaving open whether performance continues to improve or plateaus with more models
  - **What evidence would resolve it**: Systematic experiments testing 6+ SLMs across multiple datasets would show whether performance gains continue, plateau, or potentially degrade with additional models

- **Open Question 2**: Can Ensemble SuperICL be effectively applied to text generation tasks beyond classification?
  - **Basis in paper**: [inferred] The paper focuses on classification tasks and mentions that SuperICL has shown positive results on text generation, suggesting potential generalization but not testing it
  - **Why unresolved**: While the authors suggest generalization to text generation based on related work, they only test on classification benchmarks and one medical domain task, leaving open whether the method works equally well for generation tasks
  - **What evidence would resolve it**: Direct testing of Ensemble SuperICL on generation benchmarks like summarization or dialogue would demonstrate whether the approach generalizes to these task types

- **Open Question 3**: What is the impact of using domain-specific versus general task fine-tuned SLMs for Ensemble SuperICL performance?
  - **Basis in paper**: [explicit] The paper tests using SLMs fine-tuned on general tasks (MNLI) for a medical domain task and finds improved performance over ICL, but does not systematically compare domain-specific versus general fine-tuning
  - **Why unresolved**: The study uses both domain-specific fine-tuned SLMs for benchmarks and general fine-tuned SLMs for the medical task, but does not conduct a controlled comparison to determine which approach yields better results
  - **What evidence would resolve it**: A direct comparison using the same SLMs fine-tuned on either domain-specific or general tasks for the same target task would reveal which fine-tuning strategy provides superior performance

## Limitations

- Limited generalizability beyond text classification tasks and tested datasets
- Heavy computational overhead from running multiple SLMs for every test example
- Fundamental dependency on LLM's ability to process and reason about structured ensemble outputs

## Confidence

- **High confidence** in core experimental results: Consistent improvements across multiple baselines and datasets with well-designed ablation studies
- **Medium confidence** in mechanism explanations: Plausible theoretical explanations but limited empirical validation and weak corpus support
- **Low confidence** in generalizability claims: Broad claims with limited evidence beyond tested datasets and sparse details on "general task" fine-tuning experiments

## Next Checks

1. **Ablation across diverse task types**: Test Ensemble SuperICL on non-classification tasks (regression, generation, QA) and datasets with different error distributions to validate generalizability claims

2. **LLM capability boundary analysis**: Systematically test the LLM's ability to process structured ensemble outputs by varying the number of SLMs, confidence score formats, and prediction quality to identify processing limits

3. **Computational efficiency evaluation**: Measure end-to-end inference time and cost of Ensemble SuperICL compared to standard ICL and single SLM approaches to quantify accuracy-cost tradeoffs across different numbers of SLMs and dataset sizes