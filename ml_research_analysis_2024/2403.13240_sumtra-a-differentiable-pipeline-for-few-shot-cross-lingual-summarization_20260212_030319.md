---
ver: rpa2
title: 'SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization'
arxiv_id: '2403.13240'
source_url: https://arxiv.org/abs/2403.13240
tags:
- language
- have
- sumtra
- shot
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes SUMTRA, a cross-lingual summarization model
  that combines a monolingual summarizer with a machine translation model in a differentiable
  pipeline. This approach leverages abundant monolingual summarization and translation
  resources to achieve strong zero-shot and few-shot performance, addressing the scarcity
  of cross-lingual summarization training data.
---

# SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization

## Quick Facts
- arXiv ID: 2403.13240
- Source URL: https://arxiv.org/abs/2403.13240
- Authors: Jacob Parnell; Inigo Jauregi Unanue; Massimo Piccardi
- Reference count: 33
- The paper proposes SUMTRA, a cross-lingual summarization model that combines a monolingual summarizer with a machine translation model in a differentiable pipeline. This approach leverages abundant monolingual summarization and translation resources to achieve strong zero-shot and few-shot performance, addressing the scarcity of cross-lingual summarization training data. SUMTRA outperforms equivalent multilingual language model baselines (mBART-50) in many languages with only 10% of the fine-tuning samples, achieving competitive or state-of-the-art results on two widely adopted datasets (CrossSum and WikiLingua) across high-, medium-, and low-resource languages.

## Executive Summary
SUMTRA addresses the challenge of cross-lingual summarization by creating a differentiable pipeline that combines separate monolingual summarization and translation models. The key innovation is using soft token outputs that enable end-to-end training while avoiding the non-differentiable argmax operation. This approach allows leveraging abundant monolingual resources for both summarization and translation, achieving strong zero-shot and few-shot performance across multiple languages with minimal cross-lingual training data.

## Method Summary
SUMTRA consists of two mBART-50 modules: a summarization model (SUM) and a translation model (TRA). The pipeline uses soft token outputs from SUM that are mixed with TRA's embedding layer, enabling differentiability. An auxiliary back-translation loss grounds the intermediate summary in the target language space. The model is fine-tuned end-to-end using a convex combination of losses, achieving competitive performance across high-, medium-, and low-resource languages with minimal fine-tuning samples.

## Key Results
- SUMTRA outperforms mBART-50 baselines on CrossSum and WikiLingua datasets
- Achieves strong zero-shot performance across 14 target languages
- Requires only 10% of fine-tuning samples compared to baselines while maintaining competitive performance
- Shows particular strength in low-resource language settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The soft token output from the summarization model enables end-to-end differentiability, allowing joint fine-tuning of the entire pipeline.
- Mechanism: By mixing the summarizer's probability vectors with the translator's embedding layer, the pipeline bypasses the non-differentiable argmax operation, preserving gradients through both models.
- Core assumption: Both models share a compatible vocabulary and embedding space to allow meaningful mixing of probabilities with embeddings.
- Evidence anchors:
  - [abstract]: "The proposed pipeline is completely differentiable end-to-end, allowing it to take advantage of few-shot fine-tuning"
  - [section 3]: "Since the soft predictions from the SUM module do not interrupt backpropagation, the whole network can be trained end-to-end."
  - [corpus]: Weak - no direct evidence of end-to-end training effectiveness beyond stated claims.
- Break condition: Vocabulary mismatch between models or incompatible embedding dimensions would prevent mixing and break differentiability.

### Mechanism 2
- Claim: The back-translation auxiliary loss grounds the intermediate summary in the target language space, preventing misalignment between summary and translation.
- Mechanism: A reverse translation of the target reference provides supervision for the summarizer, ensuring the English summary contains information that will translate well to the target language.
- Core assumption: Back-translation from target to source preserves enough semantic content to effectively guide summarization.
- Evidence anchors:
  - [section 3]: "we add an auxiliary training objective that encourages the predicted summary to adhere to the target more closely"
  - [section 5.4]: Qualitative example shows summaries with BT loss are more aligned with reference than without
  - [corpus]: Weak - no quantitative comparison of BT loss impact across languages
- Break condition: Poor quality of the back-translation model would provide misleading supervision signals.

### Mechanism 3
- Claim: The pipeline leverages abundant monolingual resources for both summarization and translation, achieving strong zero-shot performance without requiring cross-lingual summarization data.
- Mechanism: By separately training a high-quality English summarizer and a many-to-one translation model, the pipeline can generate reasonable summaries in the target language immediately, before any fine-tuning.
- Core assumption: Both the summarization and translation models are independently capable enough to produce acceptable intermediate outputs when combined.
- Evidence anchors:
  - [abstract]: "This approach allows reusing the many, publicly-available resources for monolingual summarization and translation, obtaining a very competitive zero-shot performance"
  - [section 4.2]: Uses mBART-50 variants pretrained on large monolingual corpora
  - [section 5.2]: Cross-domain experiments show reasonable generalization
- Break condition: Insufficient quality of either component model would propagate errors and degrade overall performance.

## Foundational Learning

- Concept: Differentiable computation graphs and backpropagation
  - Why needed here: The soft token mechanism relies on preserving gradients through the entire pipeline during training
  - Quick check question: What happens to gradients when you apply argmax to probability distributions?

- Concept: Transfer learning and catastrophic forgetting
  - Why needed here: The paper discusses how monolingual fine-tuning can degrade multilingual performance, which is relevant to understanding mBART-50-mono results
  - Quick check question: Why might fine-tuning a multilingual model on English data hurt its ability to generate other languages?

- Concept: Embedding spaces and token probability distributions
  - Why needed here: The soft token approach mixes probability vectors with embedding matrices, requiring understanding of how tokens are represented
  - Quick check question: How does multiplying a probability vector by an embedding matrix produce an "expected embedding"?

## Architecture Onboarding

- Component map:
  - Input document → Summarization model (SUM) → Soft tokens → Translation model (TRA) → Target language summary
  - Auxiliary back-translation module provides loss signal for SUM
  - Both SUM and TRA are mBART-50 variants with compatible vocabularies

- Critical path:
  1. Document tokenization and feeding to SUM
  2. SUM generates probability distribution over vocabulary
  3. Probability distribution mixed with TRA embedding layer to create soft embeddings
  4. Soft embeddings fed to TRA decoder
  5. TRA generates target language summary
  6. Both predictions compared to references for loss computation

- Design tradeoffs:
  - Memory vs. performance: Fine-tuning entire pipeline uses ~70% of 48GB GPU memory vs. single model
  - Speed vs. quality: Two-model pipeline is 1.15-1.87x slower than single model but achieves better few-shot results
  - Zero-shot vs. few-shot: Pipeline excels at few-shot but single model catches up with sufficient data

- Failure signatures:
  - Soft token mixing produces NaNs or infs (embedding dimension mismatch)
  - Translation quality degrades rapidly (summarizer not producing translatable content)
  - Memory errors during fine-tuning (batch size or sequence length too large)
  - Back-translation loss dominates and corrupts summarizer output (α hyperparameter issue)

- First 3 experiments:
  1. Verify soft token mechanism works: Run inference with and without soft token mixing, compare outputs
  2. Test back-translation supervision: Train with and without BT loss, compare zero-shot performance
  3. Validate catastrophic forgetting behavior: Fine-tune mBART-50-mono on English, measure cross-lingual performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SUMTRA pipeline performance change when using different base language models (e.g., PISCES) for the summarization and translation modules?
- Basis in paper: explicit
- Why unresolved: The authors state they plan to test model configurations with different base language models in future work, but have not done so yet.
- What evidence would resolve it: Experimental results comparing SUMTRA's performance using various base models for the summarization and translation modules.

### Open Question 2
- Question: What is the impact of using alternative fine-tuning strategies, such as adversarial training and reinforcement learning, on SUMTRA's performance?
- Basis in paper: explicit
- Why unresolved: The authors mention exploring alternative fine-tuning strategies in future work but have not implemented them yet.
- What evidence would resolve it: Experimental results comparing SUMTRA's performance using different fine-tuning strategies.

### Open Question 3
- Question: How does SUMTRA's performance change when trained with different monolingual summarization datasets (e.g., CNN/DM, XSum) instead of the English splits of the XLS datasets?
- Basis in paper: explicit
- Why unresolved: The authors explore training the SUM module with CNN/DM and XSum in an alternative to the English training splits of the XLS datasets but do not provide a comprehensive comparison.
- What evidence would resolve it: Experimental results comparing SUMTRA's performance using different monolingual summarization datasets for training the SUM module.

### Open Question 4
- Question: How does the choice of the α hyperparameter in the loss function affect SUMTRA's performance across different languages?
- Basis in paper: explicit
- Why unresolved: The authors explore the sensitivity of the performance to the α coefficient but do not provide a comprehensive analysis for all languages.
- What evidence would resolve it: Experimental results showing SUMTRA's performance using different α values for each language.

### Open Question 5
- Question: How does SUMTRA's performance change when using hard predictions instead of soft predictions at inference time?
- Basis in paper: explicit
- Why unresolved: The authors mention that using hard predictions at inference time is an option but do not provide a comprehensive comparison with soft predictions.
- What evidence would resolve it: Experimental results comparing SUMTRA's performance using hard and soft predictions at inference time.

## Limitations

- Limited quantitative evidence for the effectiveness of the back-translation auxiliary loss
- No comprehensive ablation study comparing differentiable pipeline versus stage-wise training
- Memory and speed tradeoffs are acknowledged but not thoroughly analyzed for different resource constraints

## Confidence

- Mechanism 1 (Differentiable Pipeline): Medium confidence
- Mechanism 2 (Back-Translation Loss): Low confidence
- Mechanism 3 (Resource Utilization): High confidence

## Next Checks

1. **Ablation on End-to-End Training:** Compare SUMTRA performance with a stage-wise variant where SUM and TRA are trained separately and only the combination weights are fine-tuned. This would isolate the contribution of differentiability versus model quality.

2. **Back-Translation Loss Sensitivity:** Systematically vary the α parameter controlling the BT loss weight and measure performance across different resource levels (high vs. medium vs. low resource languages) to determine optimal configurations.

3. **Memory-Accuracy Tradeoff Analysis:** Experiment with different batch sizes and sequence lengths during fine-tuning to quantify the relationship between memory constraints and performance degradation, particularly for low-resource languages where sample efficiency is critical.