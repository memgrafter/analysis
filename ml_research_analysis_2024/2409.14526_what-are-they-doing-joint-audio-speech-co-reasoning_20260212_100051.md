---
ver: rpa2
title: What Are They Doing? Joint Audio-Speech Co-Reasoning
arxiv_id: '2409.14526'
source_url: https://arxiv.org/abs/2409.14526
tags:
- speech
- audio
- they
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Joint Audio-Speech Co-Reasoning (JASCO)
  task, which requires deep co-reasoning from both audio and speech modalities, addressing
  limitations in current audio-language benchmarks that often rely on single-modality
  dominance or simple concatenation of information. The authors create the "What Are
  They Doing" dataset with 80 high-quality samples where audio and speech are irrelevant
  but complementary, forcing models to jointly reason about speaker actions.
---

# What Are They Doing? Joint Audio-Speech Co-Reasoning

## Quick Facts
- arXiv ID: 2409.14526
- Source URL: https://arxiv.org/abs/2409.14526
- Reference count: 37
- Primary result: Introduces JASCO task requiring deep co-reasoning from both audio and speech modalities

## Executive Summary
This paper introduces the Joint Audio-Speech Co-Reasoning (JASCO) task, which requires deep co-reasoning from both audio and speech modalities, addressing limitations in current audio-language benchmarks that often rely on single-modality dominance or simple concatenation of information. The authors create the "What Are They Doing" dataset with 80 high-quality samples where audio and speech are irrelevant but complementary, forcing models to jointly reason about speaker actions. They benchmark four popular Auditory Large Language Models (ALLMs) and find that Qwen2-Audio achieves the highest average Best-Mean score of 1.23 (out of 2), but all models show substantial room for improvement in true joint reasoning. The analysis reveals that most models exhibit a preference for speech over audio, with Qwen2-Audio achieving 50% both-dependent reasoning versus 77% speech-dependence for WavLLM.

## Method Summary
The authors created a novel "What Are They Doing" dataset with 80 samples where audio and speech are irrelevant but complementary, forcing models to jointly reason about speaker actions. They benchmarked four popular ALLMs (WavLLM, Qwen2-Audio, Qwen2.5-Audio, and Speech-Whisper-Llama) using both single-instruction and step-by-step thinking prompts. Model-as-judge evaluation was employed to assess prediction quality and modality dependence, analyzing whether reasoning came from both modalities or single sources. The evaluation framework scored predictions on alignment with reference answers and analyzed modality dependence patterns.

## Key Results
- Qwen2-Audio achieved the highest average Best-Mean score of 1.23 out of 2
- All models showed substantial room for improvement in true joint reasoning
- Qwen2-Audio achieved 50% both-dependent reasoning versus 77% speech-dependence for WavLLM
- Dataset design forces models to integrate both modalities rather than relying on single-modality dominance
- Model-as-judge evaluation provides reliable assessment of joint reasoning quality and modality dependence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint audio-speech co-reasoning forces models to integrate both modalities rather than relying on a single dominant source.
- Mechanism: The dataset design ensures audio and speech are irrelevant but complementary, creating a scenario where neither modality alone provides sufficient information for correct answers. This forces the model to perform deep reasoning across both inputs.
- Core assumption: Models can be evaluated on their ability to truly co-reason rather than concatenate or prioritize one modality.
- Evidence anchors:
  - [abstract] "audio and speech are irrelevant but complementary, forcing models to jointly reason about speaker actions"
  - [section II.A] "The audio information and speech information should be irrelevant. Under this condition, it is intuitive to determine which modality the reasoning depends on"
  - [corpus] Weak evidence - no direct citations found in neighboring papers about modality-irrelevant co-reasoning datasets
- Break condition: If audio and speech contain overlapping or dominant information in one modality, the model can succeed without true co-reasoning.

### Mechanism 2
- Claim: Model-as-judge evaluation provides reliable assessment of joint reasoning quality and modality dependence.
- Mechanism: LLM judges score predictions on alignment with reference answers and analyze whether reasoning comes from both modalities or a single source, providing a two-dimensional evaluation framework.
- Core assumption: LLMs can reliably judge the quality of reasoning and detect modality dependence better than automated metrics for open-ended tasks.
- Evidence anchors:
  - [section II.C] "we use the Model-As-Judge approach with a carefully designed prompt to compare the model predictions with the reference correct answers"
  - [section IV.A] "We scored the ALLMs' predictions using the Model-As-Judge method mentioned in section 2.C"
  - [corpus] Weak evidence - while LLM-as-judge is mentioned in corpus, no direct citations found for audio-speech modality dependence evaluation
- Break condition: If LLM judges are biased toward certain modalities or fail to detect subtle differences in reasoning quality.

### Mechanism 3
- Claim: Multi-encoder ALLM architecture enables joint audio-speech processing by fusing separate representations before LLM reasoning.
- Mechanism: Separate encoders extract acoustic and semantic information, which are concatenated and passed to an LLM with LoRA adaptors for co-reasoning, allowing the model to process both modalities simultaneously.
- Core assumption: The concatenation of separately encoded audio and speech representations provides sufficient information for the LLM to perform joint reasoning.
- Evidence anchors:
  - [section II.B] "pre-trained acoustic encoder (such as BEATs encoder) and semantic encoder (such as Whisper encoder) are used to respectively extract audio and speech information"
  - [section I] "These models typically use separate encoders to extract audio and speech representations, which are then concatenated and fed into a text-based LLM for reasoning"
  - [corpus] Moderate evidence - neighboring papers mention similar multi-encoder approaches for multimodal processing
- Break condition: If the concatenation operation loses critical information or if the LLM cannot effectively integrate the fused representations.

## Foundational Learning

- Concept: Audio and speech modality processing fundamentals
  - Why needed here: Understanding the distinction between audio (non-speech sounds) and speech (human speech with semantics and paralinguistics) is crucial for designing joint reasoning tasks
  - Quick check question: What is the difference between processing a dog barking sound versus processing the word "dog" spoken by a person?

- Concept: Multimodal fusion techniques
  - Why needed here: The paper relies on concatenating separately encoded audio and speech representations, requiring understanding of how different fusion strategies affect reasoning performance
  - Quick check question: What are the trade-offs between early fusion (before encoding) versus late fusion (after encoding) for multimodal tasks?

- Concept: Model-as-judge evaluation methodology
  - Why needed here: The evaluation framework uses LLMs to assess both answer quality and modality dependence, requiring understanding of how to design effective evaluation prompts
  - Quick check question: How would you design a prompt to determine whether an LLM's answer relied primarily on audio or speech information?

## Architecture Onboarding

- Component map: Audio/Speech → Separate Encoders → Concatenation → LLM with LoRA adaptors → Answer
- Critical path: Audio/Speech → Separate Encoders → Concatenation → LLM → Answer
  The most critical components are the separate encoders (must capture relevant information) and the concatenation operation (must preserve information for reasoning).
- Design tradeoffs:
  - Encoder selection: Specialized encoders (BEATs for audio, Whisper for speech) vs. unified encoders
  - Fusion strategy: Simple concatenation vs. more complex fusion mechanisms (e.g., attention-based)
  - LLM size: Larger models may perform better but are more resource-intensive
  - Prompt design: Step-by-step thinking instructions vs. direct questions
- Failure signatures:
  - Low both-dependent percentage indicates preference for single modality
  - Poor performance on samples where both modalities are equally important
  - Inability to capture key details even when using both modalities
  - Over-reliance on either audio-oriented or speech-oriented reasoning patterns
- First 3 experiments:
  1. Run the benchmark with a single instruction prompt to establish baseline performance before testing with multiple prompts
  2. Analyze modality dependence breakdown for each model to identify patterns of audio vs. speech preference
  3. Test with step-by-step thinking instructions disabled to see if performance improves or degrades on different sample types

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset Size and Generalization: The current dataset contains only 80 samples, which may be insufficient to comprehensively evaluate model performance across the full spectrum of joint audio-speech reasoning scenarios.
- Prompt Dependency: The evaluation relies heavily on specific prompt designs, including both single-instruction and step-by-step thinking approaches, suggesting that current results might reflect prompt sensitivity rather than true reasoning capability differences.
- Evaluation Methodology Limitations: While the model-as-judge approach provides detailed analysis of modality dependence, it introduces potential biases inherent in LLM-based evaluation and may have limitations in detecting nuanced reasoning patterns.

## Confidence

**High Confidence (☀️☀️☀️)**: The fundamental observation that current ALLMs show preference for speech over audio modalities is well-supported by the data, with consistent patterns across multiple models showing higher speech-dependence percentages compared to both-dependent reasoning.

**Medium Confidence (☀️☀️)**: The claim that Qwen2-Audio achieves the best joint reasoning performance is supported but requires additional validation, as the difference between models is relatively small (1.23 Best-Mean score) and the dataset size limits statistical power.

**Low Confidence (☀️)**: The assertion that the dataset design forces true joint reasoning is promising but requires further validation with larger datasets and more diverse sample types to confirm that models cannot succeed through single-modality dominance or simple concatenation strategies.

## Next Checks
1. **Dataset Expansion and Ablation Study**: Create a larger dataset (minimum 500 samples) with controlled variations in audio-speech complementarity levels. Test whether current models maintain their modality dependence patterns across different degrees of complementarity, and identify the minimum threshold where joint reasoning becomes necessary rather than optional.

2. **Prompt Robustness Testing**: Systematically vary prompt formulations across multiple dimensions (instruction clarity, thinking steps, question framing) to quantify the sensitivity of model performance to prompt design. This will help distinguish between genuine reasoning capabilities and prompt-dependent behaviors.

3. **Cross-Modal Dependency Analysis**: Implement a fine-grained analysis of modality contributions by creating hybrid samples where audio and speech information are progressively mixed or separated. Measure how model performance changes as the degree of modality interdependence varies, providing deeper insights into the nature of current models' reasoning limitations.