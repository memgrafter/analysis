---
ver: rpa2
title: Large Language Models as Narrative-Driven Recommenders
arxiv_id: '2410.13604'
source_url: https://arxiv.org/abs/2410.13604
tags:
- prompting
- qwen2
- llms
- llama
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores using large language models (LLMs) for narrative-driven
  movie recommendations. The authors evaluate 38 LLMs, comparing zero-shot, identity,
  and few-shot prompting on a crowdworker-annotated dataset from Reddit.
---

# Large Language Models as Narrative-Driven Recommenders

## Quick Facts
- arXiv ID: 2410.13604
- Source URL: https://arxiv.org/abs/2410.13604
- Reference count: 40
- One-line primary result: LLMs, particularly medium and large models, significantly outperform traditional methods like doc2vec for narrative-driven movie recommendations using zero-shot prompting

## Executive Summary
This study explores using large language models (LLMs) for narrative-driven movie recommendations, evaluating 38 LLMs across zero-shot, identity, and few-shot prompting strategies. The authors compare LLM performance against traditional methods like doc2vec using a crowdworker-annotated Reddit dataset. Results show that LLMs significantly outperform traditional methods, with medium-sized open-source models (e.g., Gemma 2 27B) performing competitively with larger models. Zero-shot prompting achieves strong results with minimal complexity, requiring no task-specific fine-tuning or extensive examples.

## Method Summary
The study evaluates 38 LLMs (both open- and closed-source) using three prompting strategies on a crowdworker-annotated Reddit dataset from r/MovieSuggestions containing 296 submissions and 778 unique mentioned movies. Models are prompted to generate JSON-formatted movie recommendations based on free-form user narratives, with responses compared to Reddit community recommendations using metrics like F1@10, NDCG@10, and inter-list diversity. The evaluation includes models ranging from small to large parameter counts, testing zero-shot, identity, and few-shot prompting approaches.

## Key Results
- LLMs significantly outperform traditional methods like doc2vec in narrative-driven movie recommendation
- Medium-sized open-source models (10-50B parameters) perform competitively with larger models and closed-source alternatives
- Zero-shot prompting achieves high performance across all model sizes, with minimal gains from more complex prompting strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve strong narrative-driven recommendation performance primarily through zero-shot prompting, without requiring complex prompt engineering.
- Mechanism: LLMs leverage their large-scale pretraining to directly interpret user narratives and generate relevant recommendations without needing explicit task-specific fine-tuning or extensive examples.
- Core assumption: Pretrained LLMs have sufficiently broad knowledge of movies and genres to handle narrative queries effectively out-of-the-box.
- Evidence anchors:
  - [abstract] "we observe no significant differences across prompting strategies for most models, underscoring the effectiveness of simple approaches such as zero-shot prompting"
  - [section] "zero-shot prompting achieves a high level of performance across all model size categories... indicating that additional prompting complexity with identity or few-shot prompting yields only minor gains"
  - [corpus] Weak - related work focuses on conversational LLMs or general recommender frameworks, but not specifically on zero-shot effectiveness for narrative-driven tasks.
- Break condition: If pretraining data lacks sufficient coverage of movies, genres, or narrative contexts, zero-shot performance would degrade significantly.

### Mechanism 2
- Claim: Medium-sized open-source models can perform competitively with larger closed-source models for narrative-driven recommendations.
- Mechanism: Model performance scales with parameter count up to a point, after which returns diminish for this specific task; medium models reach the performance plateau while remaining computationally efficient.
- Core assumption: The task complexity is within the capacity of medium-sized models, and further increases in size don't yield proportional accuracy gains for narrative interpretation.
- Evidence anchors:
  - [abstract] "medium-sized open-source models (e.g., Gemma 2 27B) perform competitively with similarly sized closed-source models (e.g., GPT-3.5 Turbo) and even larger open-source models"
  - [section] "medium-sized models with parameter counts between 10 and 50 billion can achieve almost equivalent accuracy as their larger counterparts"
  - [corpus] Weak - corpus includes work on LLM recommender frameworks but lacks direct comparisons of medium vs. large model performance on narrative tasks.
- Break condition: If the narrative complexity increases beyond the capacity of medium models, or if specific task nuances require larger models' broader context understanding.

### Mechanism 3
- Claim: LLMs exhibit recency bias, favoring more recent movies in recommendations.
- Mechanism: Training data distribution and model architecture create implicit preference for newer content, likely due to greater availability of recent movie information and higher popularity of contemporary releases.
- Core assumption: Training corpora contain disproportionate amounts of information about recent movies, and models learn to prioritize this information during generation.
- Evidence anchors:
  - [abstract] "we investigate release years of the recommended movies... our investigation indicates a noticeable preference for more recent movies, particularly those released after 2000"
  - [section] "a noticeable preference for more recent movies, particularly those released after 2000. This suggests that LLMs tend to favor newer content, possibly influenced by inherent biases in the training data"
  - [corpus] Weak - corpus mentions LLM biases in general but not specifically recency bias in recommendations.
- Break condition: If training data is balanced across time periods or if explicit de-biasing techniques are applied during inference.

## Foundational Learning

- Concept: Zero-shot vs. few-shot vs. fine-tuning prompting strategies
  - Why needed here: The paper demonstrates that zero-shot prompting performs as well as more complex approaches, which is counterintuitive for many practitioners
  - Quick check question: What is the key difference between zero-shot and few-shot prompting in the context of LLM-based recommendations?

- Concept: Model parameter scaling and performance relationships
  - Why needed here: Understanding why medium-sized models perform comparably to larger ones is crucial for practical deployment decisions
  - Quick check question: At what point does increasing model size stop providing significant performance gains for narrative-driven recommendations?

- Concept: Bias in LLM training data and its impact on recommendations
  - Why needed here: The paper identifies recency bias as a significant factor affecting recommendation quality
  - Quick check question: How might the temporal distribution of training data influence an LLM's tendency to recommend recent movies?

## Architecture Onboarding

- Component map: LLM model → Prompt template → Reddit submission text → JSON output parsing → Movie title matching → Evaluation metrics
- Critical path: User narrative → Prompt generation → LLM response → JSON parsing → Movie matching → F1@10 calculation
- Design tradeoffs: Model size vs. computational cost, zero-shot simplicity vs. potential accuracy gains from fine-tuning, open-source accessibility vs. closed-source performance
- Failure signatures: Invalid JSON format, incorrect number of recommendations, duplicate movies, movies outside release year constraints
- First 3 experiments:
  1. Test zero-shot prompting with various model sizes to establish baseline performance
  2. Implement identity prompting with different personas to assess persona impact
  3. Add few-shot examples to evaluate if performance improves beyond zero-shot results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on narrative-driven recommendations in domains other than movies, such as books, music, or restaurants?
- Basis in paper: [explicit] The paper acknowledges that their evaluation is restricted to a single domain and dataset from the r/MovieSuggestions subreddit, and suggests that their approaches could be validated in similar recommendation scenarios for other domains.
- Why unresolved: The study focuses solely on movie recommendations, so the generalizability of LLMs' effectiveness to other recommendation domains remains untested.
- What evidence would resolve it: Conducting experiments with LLMs on narrative-driven recommendation tasks in other domains (e.g., books, music, restaurants) and comparing their performance to traditional methods in those contexts.

### Open Question 2
- Question: What is the impact of different LLM configurations (e.g., temperature, sampling method, quantization) on recommendation quality and diversity?
- Basis in paper: [explicit] The paper mentions that they did not test different individual LLM configurations, including parameters such as sampling method, temperature, or quantization details, and employed all LLMs using their default parameter settings.
- Why unresolved: The study uses default settings for all LLMs, so the influence of these configurations on the output's variability, diversity, and performance is not explored.
- What evidence would resolve it: Running experiments with various LLM configurations and analyzing their effects on recommendation accuracy, diversity, and computational efficiency.

### Open Question 3
- Question: Do LLMs exhibit biases beyond recency bias in their recommendations, such as skews in topic or popularity?
- Basis in paper: [explicit] The paper investigates recency bias in LLMs and finds a preference for more recent movies, but does not explore other potential biases like skews in topic or popularity.
- Why unresolved: The study focuses on recency bias but does not examine other types of biases that may be inherent in LLM responses.
- What evidence would resolve it: Analyzing LLM recommendations for biases related to topics, popularity, or other factors by examining the content and characteristics of the recommended items across different user requests.

## Limitations

- The evaluation dataset is relatively small (296 submissions) and narrowly focused on movie recommendations from a single subreddit
- The study relies on Reddit community recommendations as ground truth, which may not represent optimal recommendations
- The analysis of recency bias is based on aggregate statistics rather than systematic investigation of underlying causes

## Confidence

**High Confidence**: The core finding that zero-shot prompting performs competitively with more complex approaches is well-supported by the experimental results showing no significant differences across prompting strategies. The observation that medium-sized models achieve performance comparable to larger models is also robust, with clear performance plateaus evident in the data.

**Medium Confidence**: The claim about recency bias is supported by statistical evidence but lacks deep analysis of the underlying mechanisms. The comparative advantage of LLMs over traditional methods like doc2vec is demonstrated, but the specific reasons for this advantage are not fully explored.

**Low Confidence**: The generalizability of results to other domains or recommendation types is not established. The paper does not adequately address potential safety concerns, such as the generation of inappropriate or biased recommendations, which could limit real-world applicability.

## Next Checks

1. **Cross-Domain Validation**: Test the same LLM models and prompting strategies on a different recommendation domain (e.g., books, restaurants, or music) to assess whether the zero-shot effectiveness and medium-model performance advantage generalize beyond movies.

2. **Fine-Tuning Experiment**: Implement a lightweight fine-tuning approach on a subset of the Reddit data and compare performance against zero-shot and few-shot prompting to determine if the performance ceiling observed in this study can be exceeded.

3. **Recency Bias Investigation**: Conduct a controlled experiment varying the release years mentioned in user narratives and analyzing how this affects recommendation patterns, to distinguish between inherent training data bias and context-dependent recommendation behavior.