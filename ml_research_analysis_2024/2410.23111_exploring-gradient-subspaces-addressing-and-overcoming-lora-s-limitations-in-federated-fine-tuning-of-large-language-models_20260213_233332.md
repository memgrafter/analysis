---
ver: rpa2
title: 'Exploring Gradient Subspaces: Addressing and Overcoming LoRA''s Limitations
  in Federated Fine-Tuning of Large Language Models'
arxiv_id: '2410.23111'
source_url: https://arxiv.org/abs/2410.23111
tags:
- lora
- learning
- federated
- fine-tuning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates limitations of LoRA-based federated learning
  approaches for large language models, demonstrating that rank inflation and poor
  stability lead to suboptimal performance. The authors show that direct weight averaging
  with GaLore optimization outperforms LoRA-based methods like FlexLoRA and FFA-LoRA
  across text and vision tasks.
---

# Exploring Gradient Subspaces: Addressing and Overcoming LoRA's Limitations in Federated Fine-Tuning of Large Language Models

## Quick Facts
- arXiv ID: 2410.23111
- Source URL: https://arxiv.org/abs/2410.23111
- Authors: Navyansh Mahla; Kshitij Sharad Jadhav; Ganesh Ramakrishnan
- Reference count: 10
- Key outcome: Direct weight averaging with GaLore optimization outperforms LoRA-based federated fine-tuning methods, achieving 4-12% higher ROUGE-L scores on text tasks and 60-84% F1 scores on brain tumor classification compared to FFA-LoRA's 0.16-0.31%.

## Executive Summary
This paper investigates fundamental limitations of LoRA-based federated learning approaches for large language models, demonstrating that rank inflation and poor stability lead to suboptimal performance. The authors propose FedFTG, a framework that uses GaLore optimization with direct weight averaging, which outperforms federated LoRA methods like FlexLoRA and FFA-LoRA across both text and vision tasks. Theoretical analysis shows that GaLore with direct aggregation achieves linear excess risk bounds compared to LoRA's quadratic bounds, providing better generalization and stability in federated settings.

## Method Summary
The authors propose FedFTG, a framework that fine-tunes lower MLP layers of large language models using GaLore optimization with direct weight averaging via FedAvg. Unlike LoRA-based methods that learn low-rank matrices separately on each client and suffer from rank inflation upon aggregation, GaLore projects gradient matrices into a low-rank subspace before optimization. The framework operates in federated settings with non-IID data, where clients perform local training for T iterations followed by global aggregation every Tagg steps. The approach is evaluated on MedQuAD and Dolly-15k text datasets (using ROUGE-L and BLEU-4 metrics) and a brain tumor classification dataset (using F1 scores), comparing against FlexLoRA and FFA-LoRA baselines.

## Key Results
- On MedQuAD and Dolly-15k datasets, FedFTG outperforms FlexLoRA and FFA-LoRA by 4-12% in ROUGE-L scores
- On brain tumor classification, FedFTG achieves 60-84% F1 scores compared to FFA-LoRA's 0.16-0.31%
- Theoretical analysis shows GaLore with direct aggregation has linear excess risk bounds (O(Stagg)) versus LoRA's quadratic bounds (O(N²S²t²agg))
- Direct weight averaging with GaLore demonstrates superior stability compared to rank-constrained LoRA approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct weight averaging with GaLore optimization achieves superior performance compared to LoRA-based federated fine-tuning methods.
- Mechanism: GaLore projects gradient matrices into a low-rank subspace before applying optimization techniques like AdamW or SGD, reducing memory footprint while maintaining effective gradient updates. When combined with direct weight averaging in FedAvg, this approach avoids the rank inflation and excess risk issues inherent in LoRA-based methods.
- Core assumption: The gradient matrices of lower MLP layers in transformer networks become low-rank during training, making them suitable for GaLore's subspace projection approach.
- Evidence anchors:
  - [abstract] "Our findings show that GaLore along with direct-weight aggregation is a more effective approach, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities."
  - [section] "Our theoretical insights are validated through experiments that reveal the suboptimal performance of LoRA-based methods in FL environments."
  - [corpus] Found 25 related papers. No direct evidence for GaLore's specific mechanism in federated settings found in corpus.
- Break condition: If gradient matrices do not become sufficiently low-rank during training, or if the rank of aggregated gradients increases significantly across clients with heterogeneous data.

### Mechanism 2
- Claim: LoRA-based methods suffer from rank inflation and poor stability in federated settings due to constrained subspace learning of low-rank matrices.
- Mechanism: When N clients with non-IID data each have LoRA matrices of rank r, direct aggregation causes the resulting matrix to have inflated rank up to N×r. FlexLoRA attempts to address this by decomposing aggregated matrices via SVD back to rank r, but this creates a bottleneck that restricts capturing diverse local semantics.
- Core assumption: Non-IID data across clients leads to different intrinsic ranks in learned representations, causing rank inflation upon aggregation.
- Evidence anchors:
  - [section] "Proposition 1 In FL scenarios like FlexLoRA, where parameter change matrices ∆W i from N clients are aggregated with each client i having an intrinsic rank ri, the globally aggregated parameter matrix exhibits rank inflation following each global aggregation step."
  - [section] "After aggregation, the resulting rank of the matrix is inflated (as shown in Proposition 1), capturing more comprehensive information from all the non-IID datasets."
  - [corpus] Found 25 related papers. No direct evidence for rank inflation mechanism in federated LoRA found in corpus.
- Break condition: If all clients have identical data distributions (IID), the rank inflation issue would be minimized as all LoRA matrices would learn similar subspaces.

### Mechanism 3
- Claim: GaLore with direct weight averaging provides linear excess risk bounds compared to LoRA's quadratic bounds, leading to better generalization and stability.
- Mechanism: Theorem 2 shows that for convex loss functions with bounded gradients, the excess risk for GaLore-based direct aggregation scales linearly with the number of global aggregation steps (O(Stagg)), while FFA-LoRA exhibits quadratic scaling (O(N²S²t²agg)). This linear relationship ensures stable performance as training progresses.
- Core assumption: The loss function is convex and the L2 norm of gradients is bounded across all clients and training iterations.
- Evidence anchors:
  - [section] "Theorem 2 For a convex loss function L, let ∆W ∗ denote the optimal weight matrix and α represent the learning rate. Assuming that the L2 norm of the gradient is bounded... the excess risk... can be expressed as follows: |L(∆Wagg) − L(∆W*)| ≤ αD²Stagg + c = O(Stagg)"
  - [section] "Theorem 1... can be expressed as follows: ≤ DN Stagg (αDN Stagg c + α/N ∥∆W*∥²) = O(N²S²t²agg)"
  - [corpus] Found 25 related papers. No direct evidence for excess risk bounds comparison found in corpus.
- Break condition: If the loss function is non-convex (as is typical in deep learning) or if gradient norms become unbounded during training.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: The paper addresses limitations of federated fine-tuning approaches for LLMs, so understanding the FL framework is essential for grasping the problem context and proposed solutions.
  - Quick check question: In federated learning, who performs the model updates and who performs the aggregation of these updates?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA is the primary baseline method being critiqued, and understanding its mechanism is crucial for comprehending why the proposed approach is superior.
  - Quick check question: In LoRA, what mathematical operation constrains the weight updates to a low-rank subspace?

- Concept: Gradient Subspace Projection
  - Why needed here: GaLore's core mechanism involves projecting gradients onto a low-rank subspace, which is fundamental to understanding why this approach works better than LoRA.
  - Quick check question: What is the primary benefit of projecting gradient matrices into a lower-dimensional subspace during optimization?

## Architecture Onboarding

- Component map: Client side (LLM model with frozen parameters except lower MLP layers, local dataset, GaLore optimizer) -> Server side (parameter aggregation using FedAvg, global model distribution) -> Communication (model parameters exchanged between clients and server) -> Training loop (Local training → parameter upload → global aggregation → parameter download → repeat)

- Critical path:
  1. Initialize global model on server
  2. Distribute model to all clients
  3. Each client performs local training using GaLore optimizer on lower MLP layers
  4. Clients upload updated parameters to server
  5. Server aggregates parameters using FedAvg
  6. Server distributes aggregated parameters back to clients
  7. Repeat steps 3-6 for specified number of rounds

- Design tradeoffs:
  - Memory vs. Performance: GaLore reduces memory usage but may slightly impact convergence speed compared to full parameter updates
  - Communication efficiency: Direct weight averaging requires more communication than LoRA-based methods but provides better performance
  - Privacy vs. Accuracy: Federated approach maintains privacy but may converge slower than centralized training

- Failure signatures:
  - Poor convergence: May indicate inappropriate learning rate or insufficient local training iterations
  - Client drift: Large discrepancies between client updates suggest non-IID data or insufficient aggregation frequency
  - Memory issues: Clients unable to handle model size despite GaLore optimization may need model compression

- First 3 experiments:
  1. Single client fine-tuning: Test GaLore optimizer on lower MLP layers with centralized data to establish baseline performance
  2. Two-client federated setup: Verify that FedFTG outperforms LoRA-based methods in minimal federated configuration
  3. Varying aggregation frequency: Test different Tagg values to find optimal balance between local training and global aggregation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific conditions cause the rank inflation observed in FlexLoRA's aggregated LoRA matrices to become unbounded in practice, and how can these conditions be characterized theoretically?
- Basis in paper: [explicit] The paper states "The equality on the left holds true when the ranks of all matrices share the same subspace, a condition that becomes unattainable when data samples are non-IID across clients."
- Why unresolved: The paper mentions rank inflation occurs with non-IID data but doesn't provide precise conditions under which it becomes unbounded or provide mathematical characterization of these conditions.
- What evidence would resolve it: Empirical measurements of rank inflation across varying degrees of non-IID data, coupled with theoretical bounds on how client data heterogeneity affects rank inflation rates.

### Open Question 2
- Question: How does GaLore's memory efficiency compare to LoRA when considering different LoRA rank configurations and varying numbers of clients in federated learning scenarios?
- Basis in paper: [inferred] The paper states GaLore "reduces optimizer state memory usage by projecting weight gradients onto a lower-dimensional subspace" but doesn't provide direct comparison with LoRA's memory footprint.
- Why unresolved: While the paper discusses GaLore's theoretical advantages, it lacks quantitative memory usage comparisons between GaLore and different LoRA configurations across various federated learning scenarios.
- What evidence would resolve it: Systematic measurements of memory usage for both approaches across different model sizes, LoRA ranks, and client counts in federated settings.

### Open Question 3
- Question: What are the theoretical guarantees for GaLore's performance in heterogeneous federated learning environments where clients have significantly different data distributions and computational capabilities?
- Basis in paper: [inferred] The paper mentions "Our analysis shows salient feature learning in federated settings similar to (Tian et al. 2024) which is for centralized cases" but doesn't extend this analysis to heterogeneous FL scenarios.
- Why unresolved: The paper's theoretical analysis focuses on convex losses and homogeneous settings, but real-world federated learning involves heterogeneous data and clients with varying capabilities.
- What evidence would resolve it: Extension of the theoretical analysis to cover non-convex losses, heterogeneous client data distributions, and varying client computational capabilities in federated settings.

## Limitations

- The paper's performance claims are based on comparison with only two specific LoRA variants (FlexLoRA and FFA-LoRA) rather than a broader range of federated fine-tuning approaches
- Experimental validation is limited to three specific datasets without testing on more diverse model architectures or task types to verify generalizability
- The theoretical analysis focuses on convex loss functions, which may not fully capture the behavior of deep learning models in practice

## Confidence

**High Confidence**: The core finding that direct weight averaging with GaLore optimization outperforms LoRA-based federated fine-tuning methods is well-supported by experimental results showing 4-12% improvements in ROUGE-L scores on text tasks and significant gains in vision tasks. The theoretical analysis of linear vs quadratic excess risk bounds provides strong theoretical justification.

**Medium Confidence**: The specific mechanism by which GaLore avoids rank inflation in federated settings is theoretically sound but would benefit from more extensive empirical validation across different data distributions and client counts. The claim about superior stability is supported but could be strengthened with longer training runs and more diverse failure mode analysis.

**Low Confidence**: The paper's assertion that GaLore with direct aggregation is universally superior to all LoRA variants is based on comparison with only two specific methods (FlexLoRA and FFA-LoRA). The performance on small test sets (1% for text, 5% for vision) may not be representative of true generalization capabilities.

## Next Checks

1. **Cross-dataset validation**: Test FedFTG on additional diverse datasets beyond the three presented to verify generalizability of the performance improvements and stability claims.

2. **Ablation study on GaLore parameters**: Systematically vary the low-rank projection dimensions and learning rates in GaLore to determine optimal configurations and understand sensitivity to hyperparameters.

3. **Long-term stability analysis**: Extend training beyond the current 3 epochs to evaluate whether the theoretical linear excess risk bounds hold over extended training periods and whether any drift or instability emerges in federated settings.