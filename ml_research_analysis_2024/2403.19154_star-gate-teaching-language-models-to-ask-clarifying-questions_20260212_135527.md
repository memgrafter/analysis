---
ver: rpa2
title: 'STaR-GATE: Teaching Language Models to Ask Clarifying Questions'
arxiv_id: '2403.19154'
source_url: https://arxiv.org/abs/2403.19154
tags:
- your
- questions
- responses
- arxiv
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STaR-GATE, a self-improvement algorithm that
  teaches language models to ask clarifying questions to resolve task ambiguity. The
  method uses synthetic persona-task prompts to simulate conversations between a Questioner
  model and a Roleplayer model, iteratively finetuning the Questioner to ask questions
  that maximize the probability of generating high-quality responses from an Oracle
  model.
---

# STaR-GATE: Teaching Language Models to Ask Clarifying Questions

## Quick Facts
- arXiv ID: 2403.19154
- Source URL: https://arxiv.org/abs/2403.19154
- Authors: Chinmaya Andukuri; Jan-Philipp Fränken; Tobias Gerstenberg; Noah D. Goodman
- Reference count: 35
- One-line primary result: Finetuned model generates responses preferred over initial model in 72% of tasks

## Executive Summary
This paper introduces STaR-GATE, a self-improvement algorithm that teaches language models to ask clarifying questions to resolve task ambiguity. The method uses synthetic persona-task prompts to simulate conversations between a Questioner model and a Roleplayer model, iteratively finetuning the Questioner to ask questions that maximize the probability of generating high-quality responses from an Oracle model. The approach includes a regularization term to ensure the model retains its ability to generate responses, not just ask questions. After two iterations, the finetuned model generates responses that are preferred over the initial model in 72% of tasks.

## Method Summary
STaR-GATE is a self-improvement algorithm that fine-tunes a Questioner model through simulated conversations with a Roleplayer. The algorithm iteratively selects the best conversations based on their ability to generate high-quality responses from an Oracle, then fine-tunes the Questioner on these examples while maintaining response generation capability through regularization. The process uses persona-task pairs where the Questioner doesn't know the persona, forcing it to ask clarifying questions to generate personalized responses.

## Key Results
- Finetuned model generates responses preferred over initial model in 72% of tasks
- Strongest performance improvements observed after one iteration of finetuning
- Models trained directly on gold responses suffer from hallucinations and lower win rates
- Response regularization prevents distribution shift and maintains response generation capability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-play with a fixed Roleplayer enables the Questioner to learn useful questions by maximizing the probability of generating gold responses from an Oracle.
- **Mechanism**: The Questioner simulates conversations with a Roleplayer whose persona is unknown to it. By iteratively fine-tuning on questions that increase the log probability of gold responses (generated by an Oracle with access to the persona), the Questioner learns to ask questions that elicit relevant preferences. The fixed Roleplayer ensures consistent simulation while the Oracle provides ground truth for optimization.
- **Core assumption**: The Roleplayer can accurately simulate human responses to questions about preferences, and the Oracle can generate high-quality gold responses based on the persona.
- **Evidence anchors**:
  - [abstract] "The Questioner is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an Oracle with access to the Roleplayer's latent preferences."
  - [section 3] "The objective of STaR-GATE is to maximize the expected log probability that the pretrained model QBASE assigns to the gold response gij, given the task ti and a simulated conversation sij between Q and R"
- **Break condition**: If the Roleplayer fails to simulate realistic human preferences or the Oracle generates poor gold responses, the optimization signal becomes unreliable and learning stalls.

### Mechanism 2
- **Claim**: Response regularization prevents the Questioner from forgetting how to generate responses and maintains the ability to provide personalized answers.
- **Mechanism**: After selecting the best conversations, the algorithm samples responses from the previous iteration's model and includes these in the fine-tuning process. This KL regularization term ensures the distribution of responses doesn't drift too far from the original model, preventing the Questioner from devolving into a question-asking-only model.
- **Core assumption**: Including self-generated responses during fine-tuning preserves the response-generation capability while still improving question-asking.
- **Evidence anchors**:
  - [abstract] "we encourage the LM to use the elicited information while avoiding distribution shift through response regularization"
  - [section 3] "we add to Equation 1 a regularization term preventing the distribution of responses (not questions) from moving too far from the previous iteration: KL(pQη−1 (r|ti, sij )||pQη (r|ti, sij ))"
- **Break condition**: If regularization is too strong, it may prevent the Questioner from improving question quality; if too weak, the model may lose its ability to generate useful responses.

### Mechanism 3
- **Claim**: Expert iteration with top-k trajectory selection efficiently optimizes the question-asking policy by focusing on the most promising simulated conversations.
- **Mechanism**: For each task-persona pair, multiple conversations are simulated. The algorithm selects the top conversation(s) based on the log probability of generating the gold response, then fine-tunes on these high-quality examples. This focuses learning on the most effective question strategies.
- **Core assumption**: The log probability of gold responses is a reliable proxy for question quality, and selecting the top-k trajectories provides sufficient diversity for learning.
- **Evidence anchors**:
  - [section 3] "we sample N trajectories of simulated conversations, sijn, using the current Qη. We then select the top-k trajectories (here, k = 1) based on the objective, and do supervised fine-tuning for this set from the initial QBASE"
- **Break condition**: If top-k selection is too restrictive (k=1), it may lead to overfitting to specific conversation patterns and reduce generalization.

## Foundational Learning

- **Concept**: Reinforcement learning through self-play
  - **Why needed here**: The algorithm needs to optimize a policy (question-asking) based on outcomes (quality of responses generated), which is a natural fit for reinforcement learning principles
  - **Quick check question**: How does the reward signal in STaR-GATE differ from traditional RLHF, and why is this distinction important for question-asking?

- **Concept**: Distribution shift and regularization
  - **Why needed here**: Fine-tuning on questions alone can cause the model to forget its original capabilities, so regularization techniques are needed to maintain balanced behavior
  - **Quick check question**: What would happen if we removed the response regularization term, and how would this manifest in the model's behavior?

- **Concept**: Expert iteration and imitation learning
  - **Why needed here**: The algorithm uses expert iteration by treating the best simulated conversations as "expert" examples for supervised fine-tuning, which is more stable than direct policy optimization
  - **Quick check question**: Why might expert iteration be preferred over direct policy gradient methods in this context?

## Architecture Onboarding

- **Component map**: Persona generation -> Task generation -> Gold response generation -> Conversation simulation -> Top-k selection -> Fine-tuning with regularization -> Win rate evaluation
- **Critical path**: Task → Persona generation → Gold response generation → Conversation simulation → Top-k selection → Fine-tuning with regularization → Win rate evaluation
- **Design tradeoffs**: 
  - Using a stronger Oracle (GPT-4) provides better learning signals but creates dependency on external models
  - Mixtral-8x7b-instruct Roleplayer offers good simulation quality but may not perfectly represent all human preference patterns
  - Mistral-7b-instruct Questioner balances capability with computational efficiency
  - Top-k=1 selection is computationally efficient but may limit diversity
- **Failure signatures**:
  - Win rates plateau below 50%: Likely indicates the Questioner is not learning to ask better questions
  - Win rates drop below baseline: Suggests distribution shift or hallucination issues
  - Gold response log probabilities don't increase: Indicates poor question quality or simulation issues
  - Model generates only questions: Response regularization is insufficient
- **First 3 experiments**:
  1. Run STaR-GATE with only question fine-tuning (no regularization) to observe degradation in response generation
  2. Test with different k values in top-k selection to find optimal balance between efficiency and diversity
  3. Replace the Roleplayer with a weaker model (e.g., Mistral-7b) to test generalization and identify failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of iterations (N) for the STaR-GATE algorithm to balance performance improvement with computational efficiency?
- Basis in paper: [inferred] The paper mentions "we observe the strongest performance improvements after one iteration of finetuning" and "conducting multiple iterations is still computationally more intensive than a single finetuning run."
- Why unresolved: The paper only experiments with N=3 iterations but notes that the strongest improvement occurs after just one iteration. There's no systematic exploration of whether fewer iterations could achieve similar results or if more iterations continue to provide benefits.
- What evidence would resolve it: A systematic study varying N from 1 to 5+ iterations, measuring both win rates and computational cost, would identify the point of diminishing returns.

### Open Question 2
- Question: How does the performance of STaR-GATE change when using different size models as the Oracle (e.g., comparing GPT-4 with smaller models like llama3-8b-instruct)?
- Basis in paper: [explicit] The paper discusses using GPT-4 as the Oracle and mentions an exploratory analysis using "llama3-8b-instruct for all STaR-GATE components (Oracle, Questioner, and Roleplayer)" which achieved 65% win rate.
- Why unresolved: The exploratory analysis only tested one alternative model. There's no systematic comparison across a range of model sizes to determine the minimum capability required for the Oracle.
- What evidence would resolve it: A controlled experiment comparing STaR-GATE performance using Oracles of varying sizes (e.g., 7B, 13B, 34B, 70B parameters) would reveal the threshold where Oracle quality significantly impacts Questioner improvement.

### Open Question 3
- Question: Does incorporating multiple Roleplayer models during training improve the Questioner's generalization ability compared to training with a single Roleplayer?
- Basis in paper: [explicit] The paper notes that "when replacing the Roleplayer from mixtral-7x8b-instruct with mistral-7b-instruct or gemma-7b-instruct, win rates eventually decrease" and suggests this "highlights the importance of including multiple Roleplayers directly during training."
- Why unresolved: The paper only tests generalization by evaluating on different Roleplayers after training on one. It doesn't investigate whether training on multiple Roleplayers simultaneously would improve robustness.
- What evidence would resolve it: An experiment training STaR-GATE with ensembles of 2-5 different Roleplayer models and comparing generalization performance against single-Roleplayer training would determine if diversity during training improves robustness.

### Open Question 4
- Question: What is the effect of varying the number of simulated conversations (n) per task-persona pair on the quality of the finetuned model?
- Basis in paper: [explicit] The paper states "we simulate n conversations (N = 10 for all experiments)" but doesn't explore how this hyperparameter affects results.
- Why unresolved: The choice of n=10 appears arbitrary, and there's no analysis of whether more or fewer conversations per pair would yield better or more efficient training.
- What evidence would resolve it: A study varying n from 1 to 20+ and measuring both win rates and training efficiency would identify the optimal number of conversations needed for effective finetuning.

### Open Question 5
- Question: How does the STaR-GATE approach perform on domains outside of everyday life tasks, such as specialized technical or professional domains?
- Basis in paper: [inferred] The paper evaluates on "everyday tasks" from the instruct-human-assistant-prompt-dataset but notes "we did not evaluate our model on other domains to assess whether finetuning affects performance across different areas."
- Why unresolved: The evaluation is limited to general conversational tasks, leaving open whether the approach transfers to domains requiring specialized knowledge or different interaction patterns.
- What evidence would resolve it: Testing STaR-GATE on specialized datasets (e.g., medical advice, legal consultation, technical support) and comparing performance to baseline models would reveal domain transferability.

## Limitations

- Performance relies heavily on the quality of synthetic persona-task pairs and gold responses generated by GPT-4
- Top-k=1 selection may limit diversity of question strategies learned, potentially causing overfitting
- Generalization to real-world tasks with genuine human ambiguity remains untested

## Confidence

**High Confidence**:
- Response regularization effectively prevents the Questioner from devolving into a pure question-asking model, supported by ablation studies

**Medium Confidence**:
- Oracle's ability to generate high-quality gold responses is assumed reliable but directly impacts learning signal
- Roleplayer's ability to accurately simulate human preferences is assumed but not extensively validated

**Low Confidence**:
- Generalization to real-world tasks with genuine ambiguity is claimed but not empirically demonstrated
- Long-term stability of learned question-asking behavior across multiple iterations is assumed but unverified

## Next Checks

1. **Real-world Task Transfer**: Test the finetuned Questioner on a separate set of real human tasks (not synthetic) to validate generalization, using human evaluation of question quality and response personalization.

2. **Ablation of Oracle Quality**: Replace GPT-4 with a weaker model (e.g., Mistral-7b) as the Oracle to quantify dependency on Oracle quality, measuring win rate drops and learning of useful question strategies.

3. **Top-k Diversity Analysis**: Systematically vary the top-k parameter (k=1, 3, 5, 10) and analyze diversity of question strategies learned using clustering or embedding analysis to correlate strategy diversity with performance metrics.