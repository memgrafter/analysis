---
ver: rpa2
title: 'How Much Data is Enough Data? Fine-Tuning Large Language Models for In-House
  Translation: Performance Evaluation Across Multiple Dataset Sizes'
arxiv_id: '2409.03454'
source_url: https://arxiv.org/abs/2409.03454
tags:
- translation
- training
- language
- data
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of fine-tuning large
  language models (LLMs) for organization-specific translation using translation memories
  (TMs). We fine-tune Llama 3 8B Instruct on TMs from a software organization across
  five language directions, analyzing the impact of varying training dataset sizes
  (1k to 207k segments) on translation quality.
---

# How Much Data is Enough Data? Fine-Tuning Large Language Models for In-House Translation: Performance Evaluation Across Multiple Dataset Sizes

## Quick Facts
- arXiv ID: 2409.03454
- Source URL: https://arxiv.org/abs/2409.03454
- Reference count: 24
- Fine-tuning Llama 3 8B Instruct on translation memories shows consistent quality improvement with larger datasets

## Executive Summary
This study investigates the effectiveness of fine-tuning large language models (LLMs) for organization-specific translation using translation memories (TMs). Llama 3 8B Instruct is fine-tuned on TMs from a software organization across five language directions, analyzing the impact of varying training dataset sizes (1k to 207k segments) on translation quality. Results show consistent performance improvement with larger datasets, with BLEU and COMET scores increasing by an average of 13 and 25 points respectively on the largest training set compared to the baseline model. Notably, models trained on only 1k and 2k examples perform worse than the baseline, but quality improves substantially as dataset size increases.

## Method Summary
The study fine-tunes Llama 3 8B Instruct on translation memories from a software organization across five language directions (all involving English). Five different dataset sizes are evaluated: 1k, 2k, 5k, 10k, and 207k segments. Translation quality is measured using BLEU and COMET scores, with additional comparison to Google Translate for English-Japanese. The study focuses on how dataset size affects translation quality and whether TMs can be effectively integrated with LLMs for domain-specific translation needs.

## Key Results
- BLEU and COMET scores increase by an average of 13 and 25 points respectively when using the largest dataset (207k segments) compared to the baseline model
- Models trained on only 1k and 2k segments perform worse than the baseline, highlighting the importance of sufficient training data
- Fine-tuning with larger datasets consistently improves translation quality across all five language directions

## Why This Works (Mechanism)
Fine-tuning large language models on domain-specific translation memories allows the model to learn organization-specific terminology, style, and context patterns that are not captured by general-purpose translation models. The translation memories provide parallel sentence pairs that enable the model to adapt its internal representations to better handle the specific vocabulary and linguistic patterns of the target domain.

## Foundational Learning
- **Translation Memories**: Why needed - Provide parallel text for supervised fine-tuning; Quick check - Verify TM segments are true translations, not paraphrases
- **BLEU Score**: Why needed - Automatic metric for translation quality; Quick check - Ensure reference translations are available for all test segments
- **COMET Score**: Why needed - Complementary metric that better captures semantic quality; Quick check - Verify model was trained on relevant human judgments
- **Dataset Size Impact**: Why needed - Critical factor for model performance; Quick check - Compare training curves across different sizes
- **Domain Adaptation**: Why needed - Improves translation quality for specific organizational needs; Quick check - Test on domain-specific terminology not in general corpora

## Architecture Onboarding
- **Component Map**: Translation Memory Data -> Fine-tuning Pipeline -> Llama 3 8B Instruct -> Evaluation Metrics
- **Critical Path**: TM preprocessing → Dataset creation → Model fine-tuning → Quality evaluation
- **Design Tradeoffs**: Balance between dataset size and computational cost; generalization vs. specialization
- **Failure Signatures**: Poor performance on small datasets (<5k segments); degradation on out-of-domain content
- **3 First Experiments**: 1) Test different learning rates during fine-tuning; 2) Evaluate on held-out test sets of varying sizes; 3) Compare performance across different domain types

## Open Questions the Paper Calls Out
None

## Limitations
- Results limited to five language directions, all involving English, restricting generalizability to other language pairs
- Single translation memory source from one software organization limits findings to that domain and data style
- Limited comparison with commercial translation tools (only English-Japanese with Google Translate)
- Lack of human evaluation prevents direct correlation between automatic metrics and human-perceived quality

## Confidence
- High: Performance consistently improves with larger training datasets
- Medium: Integration of TMs with LLMs is effective for domain-specific translation needs
- Medium: Fine-tuning with small datasets (1k, 2k segments) underperforms the baseline model

## Next Checks
1. Evaluate model performance on a wider range of language pairs, including non-English-centric combinations
2. Test fine-tuning on translation memories from multiple organizations and domains to assess generalization
3. Conduct human evaluation studies to validate the correlation between automatic metrics and actual translation quality