---
ver: rpa2
title: 'Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects'
arxiv_id: '2407.02430'
source_url: https://arxiv.org/abs/2407.02430
tags:
- texture
- arxiv
- link
- generation
- made
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Meta 3D TextureGen introduces a fast, feedforward method for generating
  high-quality, globally consistent textures for 3D objects in under 20 seconds. The
  approach uses two sequential networks: a geometry-aware text-to-image model that
  conditions on 3D features in 2D space to generate multi-view texture renders, followed
  by a UV-space inpainting network that completes the texture map and enhances resolution.'
---

# Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects

## Quick Facts
- arXiv ID: 2407.02430
- Source URL: https://arxiv.org/abs/2407.02430
- Authors: Raphael Bensadoun, Yanir Kleiman, Idan Azuri, Omri Harosh, Andrea Vedaldi, Natalia Neverova, Oran Gafni
- Reference count: 40
- Primary result: Achieves 78.5-81.9% user preference win rate over state-of-the-art methods for 3D texture generation

## Executive Summary
Meta 3D TextureGen introduces a fast, feedforward method for generating high-quality, globally consistent textures for 3D objects in under 20 seconds. The approach uses two sequential networks: a geometry-aware text-to-image model that conditions on 3D features in 2D space to generate multi-view texture renders, followed by a UV-space inpainting network that completes the texture map and enhances resolution. By conditioning on position and normal renders rather than depth, the method achieves better 3D consistency and eliminates artifacts like the "Janus effect." The system outperforms prior methods in user preference studies and quantitative metrics while running significantly faster.

## Method Summary
The method uses a two-stage diffusion-based approach for texture generation. Stage I employs a fine-tuned latent diffusion model to generate multi-view texture renders conditioned on text prompts and grids of position/normal renders. Stage II backprojects these outputs to UV space using weighted incidence blending and completes the texture map through an inpainting network. The approach trains on 260k textured 3D objects and generates 1024×1024 textures in under 20 seconds, with optional 4K upscaling via a texture enhancement network.

## Key Results
- Achieves 78.5%-81.9% win rate in user preference studies against prior methods
- Outperforms competitors with FID of 73.0 vs. 77.7-99.7 and KID of 3.6 vs. 3.8-10.7
- Runs in 19 seconds compared to 66-287 seconds for prior methods
- Eliminates the "Janus effect" artifact common in 3D texture generation

## Why This Works (Mechanism)
The core innovation is conditioning on 2D position and normal renders instead of depth maps, which provides global, view-independent coordinates and high-frequency surface orientation details. Position values enable point correspondence between views, encouraging consistent textures across different viewpoints. Normal renders encode local geometric features that guide the model in preserving anatomical details and preventing duplicated features. This approach eliminates the Janus effect by ensuring that features generated for one view maintain consistency when projected to other views.

## Foundational Learning
- **Position renders**: Global UV coordinates for each point on the object, providing point correspondence between views - needed for maintaining consistency across viewpoints
- **Normal renders**: Surface orientation vectors encoding high-frequency geometric details - needed for preserving local features and preventing texture duplication
- **UV-space backprojection**: Process of mapping multi-view renders back to texture coordinates using weighted incidence blending - needed to create complete texture maps from partial view coverage
- **Latent diffusion models**: Generative models that operate in compressed latent space rather than pixel space - needed for computational efficiency in texture generation
- **Multi-view conditioning**: Using multiple camera viewpoints (four at 90° intervals) to capture different aspects of the object - needed for comprehensive texture coverage

## Architecture Onboarding

**Component Map**: Text prompt + 3D object → Position/Normal renders → Stage I diffusion model → Multi-view texture renders → Stage II inpainting → UV texture map → Optional 4K enhancement

**Critical Path**: The two-stage diffusion pipeline is critical, with Stage I generating view-consistent texture renders and Stage II completing the UV map. The position and normal conditioning is essential for avoiding the Janus effect.

**Design Tradeoffs**: The method trades some local detail resolution for global consistency by using position/normal conditioning instead of depth. The four-viewpoint setup balances coverage with computational efficiency. UV-space inpainting handles complex geometries but may introduce seams at UV boundaries.

**Failure Signatures**: Poor global consistency with feature duplication (Janus effect) indicates issues with position/normal conditioning. Visible seams at UV island boundaries suggest problems with backprojection blending. Inconsistent textures across views indicate viewpoint configuration issues.

**First Experiments**:
1. Verify position and normal render generation produces normalized, aligned outputs for test geometries
2. Test Stage I diffusion model with simple prompts on basic geometries to confirm view consistency
3. Validate Stage II backprojection produces seamless UV maps on simple test cases

## Open Questions the Paper Calls Out
The paper identifies several areas requiring further investigation. A comprehensive ablation study comparing position/normal conditioning against other 3D representations like depth maps would strengthen the claims about 3D consistency benefits. The optimal configuration of camera viewpoints for different object geometries and prompt types remains unexplored. The method's performance on non-manifold or highly complex geometries with intricate topological features has not been thoroughly evaluated, despite claims of general applicability.

## Limitations
- Evaluation relies on a single synthetic benchmark dataset (54 Sketchfab objects) which may not represent real-world diversity
- User studies may be influenced by presentation order or prompt selection bias
- The method requires baking position and normal information into UV space, adding preprocessing complexity
- Performance on non-manifold geometries and highly complex topological features remains unproven

## Confidence
- **High Confidence**: Core architectural contribution (position/normal conditioning) and runtime improvements are well-supported
- **Medium Confidence**: Quantitative metrics and user preference results are based on limited dataset diversity
- **Low Confidence**: Claims about performance on arbitrary complex geometries lack comprehensive validation

## Next Checks
1. Replicate the user preference study with a more diverse dataset including real-world 3D models
2. Conduct ablation studies systematically comparing position, normal, and depth conditioning
3. Test the method on non-manifold geometries and highly complex topological structures to validate general applicability claims