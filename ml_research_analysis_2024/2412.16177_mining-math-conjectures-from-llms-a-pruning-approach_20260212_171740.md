---
ver: rpa2
title: 'Mining Math Conjectures from LLMs: A Pruning Approach'
arxiv_id: '2412.16177'
source_url: https://arxiv.org/abs/2412.16177
tags:
- conjecture
- group
- conjectures
- solg
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to generating mathematical
  conjectures using Large Language Models (LLMs) like ChatGPT, Claude, and Gemini.
  The method involves generating conjectures about the solubilizer, a relatively recent
  construct in group theory, and then pruning them by having the LLMs write GAP code
  to check for counterexamples.
---

# Mining Math Conjectures from LLMs: A Pruning Approach

## Quick Facts
- arXiv ID: 2412.16177
- Source URL: https://arxiv.org/abs/2412.16177
- Reference count: 40
- Primary result: LLMs can generate original mathematical conjectures about solubilizers, with 25.95% producing falsifiable conjectures via GAP counterexamples

## Executive Summary
This paper presents a novel approach to generating mathematical conjectures using Large Language Models (LLMs) like ChatGPT, Claude, and Gemini. The method involves generating conjectures about the solubilizer, a relatively recent construct in group theory, and then pruning them by having the LLMs write GAP code to check for counterexamples. The results show that LLMs are capable of producing original conjectures that are either plausible or falsifiable via counterexamples, though they exhibit limitations in code execution. Among 420 unique conjectures generated, 25.95% successfully found counterexamples, 9.52% produced conjectures with no counterexamples, and 64.52% resulted in non-executable code. ChatGPT performed best in generating plausible conjectures, while Claude was more effective at identifying counterexamples. The study demonstrates the potential of combining LLMs with computational tools to mine for mathematical conjectures, while also highlighting the need for robust error-checking and handling within the models.

## Method Summary
The methodology employs an iterative "guess-and-check" approach where LLMs generate conjectures about the solubilizer construct in group theory, then produce GAP code to verify these conjectures by searching for counterexamples. The process begins with a system prompt containing solubilizer literature and basic GAP code templates. LLMs generate conjectures and corresponding GAP code, which is executed on non-solvable groups up to order 1,000,000. If code fails to execute, the model receives error feedback and attempts revisions. False conjectures are added to the prompt to avoid repetition. Temperature parameters are tuned differently for conjecture generation (higher values for diversity) and code generation (lower values for correctness). This iterative process continues until a target number of unique conjectures is reached, with results classified into three categories based on code executability and counterexample findings.

## Key Results
- LLMs generated 420 unique conjectures about solubilizers, with 25.95% successfully finding counterexamples through GAP code execution
- 64.52% of generated conjectures resulted in non-executable code, representing a significant bottleneck in the methodology
- ChatGPT excelled at generating plausible conjectures while Claude was more effective at identifying counterexamples, suggesting complementary strengths across different LLM models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs can generate mathematically plausible conjectures by leveraging pattern recognition from their training data, even without explicit knowledge of the solubilizer concept.
- **Mechanism**: The model identifies structural patterns in group theory definitions and operations, then applies these patterns to the novel solubilizer definition to generate conjectures.
- **Core assumption**: The model's training data contains sufficient mathematical structure and group theory concepts to enable pattern transfer to unfamiliar constructs.
- **Evidence anchors**:
  - [abstract]: "LLMs are capable of producing original conjectures that, while not groundbreaking, are either plausible or falsifiable via counterexamples"
  - [section]: "This output shows three correct assertions about the subset, so we test to see if the model can expand further on it's understanding without more input"
  - [corpus]: Weak - the related papers focus on formal theorem proving and proof generation rather than creative conjecture generation, making direct evidence sparse
- **Break condition**: If the training data lacks sufficient mathematical structure or the construct is too far removed from known concepts, the model cannot generate plausible conjectures.

### Mechanism 2
- **Claim**: The "guess-and-check" pruning approach effectively filters out false conjectures by leveraging computational verification.
- **Mechanism**: LLMs generate conjectures and accompanying GAP code, which is executed to identify counterexamples. This creates a feedback loop where falsified conjectures are added to the prompt to avoid repetition.
- **Core assumption**: Computational tools like GAP can reliably verify or falsify conjectures within reasonable time limits.
- **Evidence anchors**:
  - [abstract]: "These conjectures are pruned by allowing the LLMs to generate counterexamples"
  - [section]: "If the code compiles and runs, then the outcome is recorded"
  - [corpus]: Moderate - related work on automated conjecturing exists but focuses on different verification methods
- **Break condition**: If code generation fails frequently or verification tools cannot handle the complexity of generated conjectures, the pruning approach becomes ineffective.

### Mechanism 3
- **Claim**: Temperature parameter tuning influences the diversity of generated conjectures while maintaining mathematical coherence.
- **Mechanism**: Higher temperature values during conjecture generation increase randomness in outputs, leading to more diverse conjectures, while lower values during code generation ensure syntactic correctness.
- **Core assumption**: Temperature settings can be optimized to balance creativity and correctness without sacrificing either completely.
- **Evidence anchors**:
  - [section]: "The temperature for the Claude model was set to 1 for conjecture generation and .1 for code generation"
  - [section]: "The values were generated by trial and error where the authors observed qualitatively the most consistent conjecture variation"
  - [corpus]: Weak - no direct evidence in related papers about temperature tuning for mathematical conjecture generation
- **Break condition**: If optimal temperature settings cannot be found or if high temperatures consistently produce incoherent outputs, the approach fails to generate useful conjectures.

## Foundational Learning

- **Concept**: Group theory fundamentals (normal subgroups, quotient groups, derived subgroups, Frattini subgroups)
  - Why needed here: The solubilizer construct is defined within group theory, and conjectures involve relationships between solubilizers and other group-theoretic concepts
  - Quick check question: Can you explain the difference between a normal subgroup and a characteristic subgroup, and how this relates to solubilizers?

- **Concept**: Computational algebra systems (GAP syntax and capabilities)
  - Why needed here: GAP is used to verify conjectures through counterexample generation, requiring understanding of its syntax and limitations
  - Quick check question: How would you write GAP code to check if a given subset of a group is actually a subgroup?

- **Concept**: Mathematical conjecture structure and proof techniques
  - Why needed here: Understanding what makes a conjecture interesting and how to approach proving or disproving it is essential for evaluating LLM outputs
  - Quick check question: What distinguishes a "trivial" conjecture from a "deep" one in group theory?

## Architecture Onboarding

- **Component map**: LLM (ChatGPT, Claude, Gemini) → Conjecture Generation → GAP Code Generation → GAP Execution → Result Classification → Prompt Update → Repeat
- **Critical path**: The conjecture generation and GAP code execution stages are most critical, as failures in either prevent the pruning mechanism from functioning
- **Design tradeoffs**: Using multiple LLMs provides diversity but increases complexity; relying on GAP limits the types of conjectures that can be verified but ensures computational rigor
- **Failure signatures**: High rates of non-executable code (>60%) indicate issues with code generation; repetitive conjectures suggest temperature tuning problems; inability to generate counterexamples may indicate overly simple conjectures
- **First 3 experiments**:
  1. Run a single LLM with a simple prompt containing basic group theory definitions and verify it can generate syntactically correct GAP code
  2. Test temperature parameter effects by generating multiple conjectures with varying temperature settings and analyzing diversity
  3. Implement the feedback loop by running a conjecture through GAP, then updating the prompt with falsified results and observing changes in subsequent outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a multi-modal approach combining different LLMs (e.g., one specialized in conjecture generation and another in code execution) improve the overall performance of mining mathematical conjectures compared to using a single model?
- Basis in paper: [inferred] The paper mentions that different LLMs performed differently in generating conjectures and executing code, suggesting that a combination of models could potentially yield better results.
- Why unresolved: The paper only tested each LLM individually and did not explore a multi-modal approach combining different models.
- What evidence would resolve it: Experimental results comparing the performance of a multi-modal approach to individual LLM performance in generating and validating conjectures.

### Open Question 2
- Question: What are the patterns of failure in code generation by LLMs, and can these patterns be used to improve prompting strategies to avoid common bugs and increase code executability?
- Basis in paper: [explicit] The paper notes that a significant portion of conjectures resulted in non-executable code, particularly for Claude and Gemini, and suggests that analyzing these failures could lead to better prompting.
- Why unresolved: The paper identifies the issue but does not provide a detailed analysis of the specific patterns of failure in code generation.
- What evidence would resolve it: A systematic analysis of failed code generation instances to identify common patterns and the development of improved prompting strategies based on these patterns.

### Open Question 3
- Question: How can the diversity of conjectures generated by LLMs be increased without sacrificing quality, and what methods can be employed to reduce redundancy in the outputs?
- Basis in paper: [explicit] The paper observes a high level of redundancy in the conjectures generated by the LLMs, with approximately 55.48% of conjectures being unique, and suggests that different approaches might be needed to increase diversity.
- Why unresolved: The paper acknowledges the redundancy issue but does not explore specific methods to enhance diversity while maintaining the quality of conjectures.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of various methods (e.g., multi-modal model interaction, integration with automated theorem provers) in increasing the diversity of conjectures while maintaining or improving their quality.

## Limitations
- High failure rate for code generation (64.52%) represents a significant bottleneck that limits practical utility
- Relies on a relatively obscure mathematical construct (solubilizer) with limited prior research, making it unclear if results generalize
- Labor-intensive iterative prompting process requires manual intervention and may not scale effectively

## Confidence

**High Confidence**: The empirical observation that LLMs can generate mathematically coherent conjectures about unfamiliar constructs, even if many are ultimately falsified. This is directly observable from the generated outputs and the GAP verification process.

**Medium Confidence**: The comparative performance differences between LLMs (ChatGPT excelling at generating plausible conjectures, Claude at finding counterexamples). While the data shows these patterns, the sample sizes for some comparisons are relatively small, and the performance may be sensitive to prompt engineering choices not fully explored.

**Low Confidence**: The claim that this approach represents a scalable methodology for mathematical discovery. The high rate of non-executable code and the labor-intensive nature of the iterative prompting process suggest significant barriers to practical deployment beyond this specific domain.

## Next Checks

1. **Cross-domain validation**: Test the same methodology on a well-established mathematical domain (like graph theory or number theory) where the ground truth of conjecture validity is better understood, to assess whether the high code failure rate persists.

2. **Prompt engineering ablation**: Systematically vary the prompt structure, temperature settings, and feedback mechanisms to identify which components most significantly impact code generation success rates and conjecture quality.

3. **Alternative verification methods**: Implement a secondary verification system using a different computational algebra system or formal proof assistant to determine whether the verification bottleneck is specific to GAP or represents a more general challenge in automated conjecture validation.