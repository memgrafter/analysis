---
ver: rpa2
title: A Simple Architecture for Enterprise Large Language Model Applications based
  on Role based security and Clearance Levels using Retrieval-Augmented Generation
  or Mixture of Experts
arxiv_id: '2407.06718'
source_url: https://arxiv.org/abs/2407.06718
tags:
- security
- experts
- could
- clearance
- role
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a role-based security architecture for enterprise
  large language model (LLM) applications, addressing the challenge of preventing
  unauthorized access to sensitive information. The proposed method filters documents
  in Retrieval-Augmented Generation (RAG) and experts in Mixture of Experts (MoE)
  models based on user roles and security clearance levels.
---

# A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts

## Quick Facts
- arXiv ID: 2407.06718
- Source URL: https://arxiv.org/abs/2407.06718
- Authors: Atilla Özgür; Yılmaz Uygun
- Reference count: 35
- Primary result: A role-based security architecture for enterprise LLM applications that prevents unauthorized access to sensitive information by filtering documents in RAG and experts in MoE based on user roles and security clearance levels.

## Executive Summary
This paper introduces a role-based security architecture for enterprise large language model (LLM) applications, addressing the challenge of preventing unauthorized access to sensitive information. The proposed method filters documents in Retrieval-Augmented Generation (RAG) and experts in Mixture of Experts (MoE) models based on user roles and security clearance levels. This ensures that only authorized users can access specific information, thereby preventing information leakage. The architecture is versatile, supporting RAG, MoE, or both, and is designed to work with local open-source LLMs or commercial models like OpenAI's ChatGPT. The primary result is a secure framework that enhances information access control in enterprise LLM applications, ensuring compliance with NATO clearance levels and role-based security protocols.

## Method Summary
The proposed method implements a role-based security architecture for enterprise LLM applications by filtering documents in RAG and experts in MoE based on user roles and security clearance levels. The approach involves setting up user-role mappings and document clearance levels, implementing RAG with document filtering using user roles and clearance levels, and training MoE models with multiple experts for each role and clearance level combination. The architecture ensures that only authorized users can access specific information, thereby preventing information leakage. The method is designed to work with local open-source LLMs or commercial models like OpenAI's ChatGPT, providing flexibility for enterprise deployment.

## Key Results
- Document filtering in RAG prevents unauthorized access to sensitive content by applying role and clearance-based filters at the vector database query stage.
- Expert filtering in MoE ensures users cannot query sensitive expert models by setting gating scores to zero for unauthorized experts based on their roles and clearance levels.
- Role and clearance mappings stored in enterprise directories enable dynamic, centralized access control, ensuring the LLM application always uses current permissions without hardcoding.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document filtering in RAG prevents unauthorized access to sensitive content.
- Mechanism: During retrieval, only documents accessible to the user's roles and clearance levels are included in the context passed to the LLM. This is enforced by applying role and clearance-based filters at the vector database query stage.
- Core assumption: The vector database supports fine-grained filtering on metadata such as role and clearance tags.
- Evidence anchors:
  - [abstract] "Using roles and security clearance level of the user, documents in RAG and experts in MoE are filtered."
  - [section] "While doing the inference using Retrieval-Augmented Generation (RAG), role information and security clearance levels should be used. Most of the vector databases allow use of filters. Using filters, only documents a user has access to should be returned."
  - [corpus] Weak: No direct corpus paper validates this exact filtering approach; related work focuses on security filtering but not role-based clearance filtering in RAG.
- Break condition: If the vector database cannot filter by user-specific metadata, unauthorized documents may be retrieved and exposed in the LLM prompt.

### Mechanism 2
- Claim: Expert filtering in MoE ensures users cannot query sensitive expert models.
- Mechanism: The router in the MoE architecture assigns gating scores of zero to experts the user is not authorized to access, based on their roles and clearance levels. Only permitted experts are activated and consulted.
- Core assumption: The MoE router can be modified to enforce access control policies before computing gating scores.
- Evidence anchors:
  - [abstract] "Using roles and security clearance level of the user, documents in RAG and experts in MoE are filtered."
  - [section] "In our role based access control architecture, gating scores of some experts will be zero according to the roles of the caller."
  - [corpus] Weak: No corpus evidence of role-based gating score suppression in MoE; related work discusses ensemble learning but not access control.
- Break condition: If the router cannot enforce zero gating scores, unauthorized experts may still be activated, leaking sensitive information.

### Mechanism 3
- Claim: Role and clearance mappings stored in enterprise directories enable dynamic, centralized access control.
- Mechanism: User-to-role and user-to-clearance mappings are retrieved from directory services (e.g., Active Directory, ERP databases) at runtime, ensuring the LLM application always uses current permissions without hardcoding.
- Core assumption: Enterprise directory services provide a reliable API for querying user roles and clearances.
- Evidence anchors:
  - [section] "For our LLM application, this user to role and user to security clearance levels mappings should be accessible from its programming interface."
  - [section] "This mapping information could be stored in LLM application’s own database or a web service could be provided to the application."
  - [corpus] Weak: No corpus evidence of this exact mapping integration; related work discusses RBAC but not in the context of LLM applications.
- Break condition: If directory service integration fails or is misconfigured, the application may default to incorrect or no access control.

## Foundational Learning

- Concept: NATO Security Clearance Levels (Cosmic Top Secret, Secret, Confidential, Restricted, Not Classified)
  - Why needed here: These levels define the sensitivity hierarchy used to filter documents and experts in the architecture.
  - Quick check question: What is the highest NATO clearance level mentioned in the paper, and why is it important for filtering?

- Concept: Role-Based Access Control (RBAC)
  - Why needed here: RBAC defines how users are grouped into roles, which in turn determine what resources (documents, experts) they can access.
  - Quick check question: How does the paper extend RBAC when combined with security clearance levels?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is the mechanism by which the LLM retrieves relevant documents from a vector database to augment its responses.
  - Quick check question: What is the key security vulnerability in RAG that the paper aims to address?

## Architecture Onboarding

- Component map:
  - User Directory Service -> Vector Database -> LLM Application -> MoE Router
- Critical path:
  1. User request arrives.
  2. Retrieve user roles and clearance from directory service.
  3. For RAG: Filter vector database query by user's access rights.
  4. For MoE: Set gating scores to zero for unauthorized experts.
  5. Generate response using filtered context or permitted experts.
- Design tradeoffs:
  - Using commercial LLMs with RAG simplifies deployment but may expose sensitive documents if filtering is imperfect.
  - Training local MoE models ensures data stays on-premises but increases upfront cost and complexity.
  - Strict filtering may reduce recall, impacting answer quality; balancing security and usefulness is critical.
- Failure signatures:
  - Unauthorized documents appearing in LLM responses.
  - Unauthorized experts being consulted by the MoE router.
  - Application failing to retrieve user roles or clearances from directory service.
- First 3 experiments:
  1. Set up a test directory service with mock users, roles, and clearances. Verify the application can retrieve and apply these mappings correctly.
  2. Populate a test vector database with documents tagged by role and clearance. Run RAG queries with different user profiles and confirm only authorized documents are returned.
  3. Build a minimal MoE router that enforces zero gating scores for unauthorized experts. Test with mock experts and user profiles to ensure only permitted experts are activated.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the proposed role-based security architecture in preventing unauthorized access to sensitive information in enterprise LLM applications?
- Basis in paper: [explicit] The paper proposes a role-based security architecture for enterprise LLM applications and claims it prevents information leakage.
- Why unresolved: The paper does not provide empirical evidence or performance metrics to demonstrate the effectiveness of the proposed architecture in real-world scenarios.
- What evidence would resolve it: Empirical studies or case studies showing the effectiveness of the proposed architecture in preventing unauthorized access to sensitive information in enterprise LLM applications.

### Open Question 2
- Question: How does the proposed architecture handle dynamic changes in user roles and security clearance levels?
- Basis in paper: [inferred] The paper mentions that the architecture should be able to handle changes in user roles and security clearance levels, but does not provide details on how this is achieved.
- Why unresolved: The paper does not provide information on how the architecture handles dynamic changes in user roles and security clearance levels, which is crucial for maintaining security in a dynamic enterprise environment.
- What evidence would resolve it: Detailed information on how the architecture handles dynamic changes in user roles and security clearance levels, including mechanisms for updating and propagating changes.

### Open Question 3
- Question: What are the performance implications of using the proposed architecture with large-scale enterprise LLM applications?
- Basis in paper: [inferred] The paper mentions that the architecture could be used with large-scale enterprise LLM applications, but does not provide information on the performance implications.
- Why unresolved: The paper does not provide information on the performance implications of using the proposed architecture with large-scale enterprise LLM applications, which is crucial for understanding its scalability and feasibility.
- What evidence would resolve it: Performance benchmarks or scalability studies showing the performance implications of using the proposed architecture with large-scale enterprise LLM applications.

## Limitations
- The proposed architecture assumes enterprise directory services can reliably provide user-role and user-clearance mappings at runtime, but no evidence is provided on how these integrations handle failure modes, latency, or synchronization issues across distributed systems.
- The paper does not specify how to handle documents or experts that span multiple clearance levels or roles, which could lead to over-restriction or unintended information leakage if not carefully designed.
- No empirical validation is presented; the security guarantees rely entirely on correct implementation of filtering logic, with no discussion of potential bypass attacks or adversarial prompts that could circumvent role-based restrictions.

## Confidence
- High Confidence: The core mechanism of filtering documents in RAG and experts in MoE based on user roles and clearances is logically sound and directly stated in the abstract and methodology sections.
- Medium Confidence: The feasibility of integrating enterprise directory services for dynamic access control is plausible but lacks specific implementation details or validation in the paper.
- Low Confidence: The effectiveness of the architecture in preventing information leakage under real-world conditions is not demonstrated; the paper presents a design without empirical testing or security audits.

## Next Checks
1. **Directory Service Integration Test**: Implement mock directory services with varying failure modes (latency, downtime, inconsistent data) and verify the LLM application gracefully handles these scenarios without defaulting to insecure behavior.
2. **Multi-Level Document/Expert Filtering**: Create test cases where documents or experts have multiple clearance levels or roles, and validate that the filtering logic correctly handles intersections, unions, and conflicts.
3. **Adversarial Prompting Simulation**: Conduct controlled experiments with adversarial prompts designed to trick the system into revealing unauthorized information, and assess whether role-based filtering can withstand such attacks.