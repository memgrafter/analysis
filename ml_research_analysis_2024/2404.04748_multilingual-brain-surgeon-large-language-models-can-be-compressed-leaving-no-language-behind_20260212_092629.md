---
ver: rpa2
title: 'Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving
  No Language Behind'
arxiv_id: '2404.04748'
source_url: https://arxiv.org/abs/2404.04748
tags:
- language
- languages
- compression
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of compressing multilingual large
  language models while maintaining performance across all languages, especially low-resource
  ones. Current compression methods typically use English-only calibration data, leading
  to significant performance drops in non-English languages.
---

# Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind

## Quick Facts
- arXiv ID: 2404.04748
- Source URL: https://arxiv.org/abs/2404.04748
- Reference count: 24
- Key outcome: MBS improves multilingual model compression by sampling calibration data proportionally to language distribution, significantly outperforming English-only calibration methods.

## Executive Summary
This paper addresses the critical challenge of compressing multilingual large language models while maintaining performance across all languages, particularly low-resource ones. Current compression methods typically use English-only calibration data, leading to significant performance degradation in non-English languages. The authors propose Multilingual Brain Surgeon (MBS), which samples calibration data proportionally to each language's distribution in the model's training set, and validate it across multiple compression methods including GPTQ, SparseGPT, and Wanda. Experiments on BLOOM models show that MBS consistently outperforms English-only calibration, especially for underrepresented languages, while also improving performance for well-represented languages like English.

## Method Summary
The Multilingual Brain Surgeon (MBS) method samples calibration data from different languages proportionally to their distribution in the model's training dataset, rather than using only English data. This proportional sampling is applied to various OBS/OBD-based compression methods including GPTQ (3-bit quantization), SparseGPT, and Wanda (50% pruning). The approach leverages the observation that the Hessian matrix in second-order optimization is language-dependent, and sampling proportionally ensures the approximation reflects the true multi-language error surface. The authors also analyze language interaction during compression, finding that languages with larger training proportions are more resistant to compression and that similar languages retain better performance when one is used for calibration.

## Key Results
- MBS outperforms English-only calibration across all tested compression methods (GPTQ, SparseGPT, Wanda) on BLOOM-7b1 and BLOOM-560m models
- For underrepresented languages like Bengali, Marathi, and Gujarati, MBS significantly reduces perplexity increases compared to English-only calibration
- Languages more similar to the calibration language (measured by cosine similarity) retain better performance after compression
- Languages with larger training proportions in the original model are more resistant to compression degradation

## Why This Works (Mechanism)

### Mechanism 1
Calibration data sampling proportional to training language distribution preserves multilingual performance better than English-only calibration. The Hessian matrix in OBS/OBD-based methods is language-dependent, and sampling calibration data from each language proportionally to its training set size ensures that the Hessian approximation reflects the true multi-language error surface rather than being biased toward English.

### Mechanism 2
Language similarity affects compression performance, with more similar languages experiencing less performance degradation when one is used for calibration. The pruning priority is determined by the Hessian matrix, and for similar languages, the input vectors have high cosine similarity, meaning the same parameters are likely to be prioritized for pruning, preserving performance in the similar target language.

### Mechanism 3
Languages with larger training proportions are more resistant to compression degradation. Larger training proportions mean the language's error surface has more weight in the total error, pushing the model state closer to that language's local minimum, making the model more robust to perturbations from pruning in that language.

## Foundational Learning

- **Second-order optimization and Hessian matrix approximation**: Why needed - MBS and all OBS/OBD-based compression methods rely on approximating second-order derivatives to determine parameter importance for pruning. Quick check - What is the relationship between the Hessian matrix and the error surface in neural network optimization?

- **Cosine similarity and vector alignment**: Why needed - The paper uses cosine similarity between language input vectors to predict compression performance based on language similarity. Quick check - How does cosine similarity differ from Euclidean distance in measuring vector similarity, and why is it more appropriate for this application?

- **Multilingual curse of dimensionality**: Why needed - Understanding why multilingual models perform worse than monolingual models helps explain why proportional calibration is necessary. Quick check - What is the "curse of multilinguality" and how does it affect multilingual model performance compared to monolingual models?

## Architecture Onboarding

- **Component map**: Calibration data sampler -> Compression method interface -> Evaluation harness -> Language similarity calculator
- **Critical path**: 1) Determine language distribution from training data 2) Sample calibration data according to distribution (MBS) or equally 3) Run compression method with sampled calibration data 4) Evaluate compressed model across all target languages 5) Compare performance against baselines
- **Design tradeoffs**: Proportional vs equal sampling (preserves true distribution but requires distribution knowledge vs simpler but may not reflect actual language importance), sampling size (larger samples give better Hessian approximation but increase computation time), language selection (some languages may be excluded due to resource constraints)
- **Failure signatures**: Performance drops in underrepresented languages despite MBS (insufficient sampling or languages too dissimilar from calibration), performance worse than English-only calibration (sampling bias or incorrect language distribution estimation), inconsistent results across compression methods (Hessian approximation sensitive to algorithm implementation details)
- **First 3 experiments**: 1) Implement MBS with proportional sampling on a small multilingual model and compare perplexity against English-only calibration across 3-4 languages 2) Test equal vs proportional sampling on the same model to quantify the benefit of distribution-aware sampling 3) Implement language similarity calculation and verify that more similar languages to the calibration language show less performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MBS change when applied to language pairs with very different proportions in the training data? The paper discusses how languages with larger proportions in the training data are more resistant to compression, but doesn't explore the interaction between languages of vastly different sizes.

### Open Question 2
Does the cosine similarity measure between languages remain effective for predicting compression performance across different model sizes and architectures? The authors propose a cosine similarity measure to predict performance drops, but only validate it on BLOOM models.

### Open Question 3
How does the language distribution in the calibration data affect the quality of the Hessian approximation in the OBD/OBS framework? The paper mentions that MBS improves the Hessian approximation by using proportionally sampled calibration data, but doesn't explore the relationship between calibration data distribution and Hessian quality.

### Open Question 4
What is the optimal number of segments to sample from each language for MBS calibration data? The paper uses 256 segments total but doesn't explore whether this number is optimal or how it affects performance.

## Limitations

- The study only evaluates on BLOOM models, limiting generalizability to other multilingual architectures
- The theoretical justification for proportional sampling relies on assumptions about error surface additivity that aren't empirically validated
- The language similarity analysis uses a simplified cosine similarity metric that may not capture the full complexity of cross-linguistic parameter interactions
- The exact language distribution proportions in the BLOOM training dataset are not provided, making exact reproduction difficult

## Confidence

**High Confidence**: The empirical results showing MBS outperforming English-only calibration across multiple compression methods on BLOOM models are well-supported with specific perplexity values and consistent improvements across 20 languages.

**Medium Confidence**: The theoretical mechanism explaining why proportional sampling works (error surface additivity and Hessian matrix approximation) is logically sound but lacks direct empirical validation.

**Low Confidence**: The generalizability of findings to other multilingual models beyond BLOOM, and the claim that training proportion directly determines compression resistance, remain weakly supported.

## Next Checks

1. Replicate proportional sampling validation by creating a controlled experiment with a small multilingual model where the exact language distribution is known, then verify that MBS sampling with proportional allocation outperforms both English-only and equal sampling methods across all languages.

2. Test language similarity predictions by selecting 5-6 language pairs with varying degrees of similarity, compressing the model using each language as calibration, and measuring the correlation between predicted performance drops (based on cosine similarity) and actual performance degradation.

3. Apply MBS to a different multilingual architecture (e.g., mT5 or XGLM) and compare performance against BLOOM results to assess whether the proportional sampling advantage holds across model families with different training data and architectures.