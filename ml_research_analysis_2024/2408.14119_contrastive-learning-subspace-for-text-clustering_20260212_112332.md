---
ver: rpa2
title: Contrastive Learning Subspace for Text Clustering
arxiv_id: '2408.14119'
source_url: https://arxiv.org/abs/2408.14119
tags:
- learning
- clustering
- contrastive
- text
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Subspace Contrastive Learning (SCL) for text
  clustering, which models cluster-wise relationships among instances through subspace
  modeling. Unlike existing contrastive learning-based methods that only focus on
  instance-wise semantic similarity, SCL constructs virtual positive samples using
  a self-expressive module and learns discriminative subspace representations.
---

# Contrastive Learning Subspace for Text Clustering

## Quick Facts
- arXiv ID: 2408.14119
- Source URL: https://arxiv.org/abs/2408.14119
- Authors: Qian Yong; Chen Chen; Xiabing Zhou
- Reference count: 31
- Key outcome: SCL achieves state-of-the-art NMI of 63.3 on AgNews, outperforming previous best (57.8)

## Executive Summary
This paper introduces Subspace Contrastive Learning (SCL), a novel approach for text clustering that models cluster-wise relationships through subspace modeling. Unlike existing contrastive learning methods that focus solely on instance-wise semantic similarity, SCL constructs virtual positive samples using a self-expressive module and learns discriminative subspace representations. The method demonstrates state-of-the-art or comparable performance across seven benchmark datasets, particularly excelling on datasets with fewer categories while avoiding the need for prior knowledge of category numbers.

## Method Summary
SCL jointly optimizes an encoder and self-expressive module in a contrastive learning framework. The encoder processes text input to produce representations, while the self-expressive module constructs virtual positive samples by expressing each sample as a linear combination of others. This approach captures cluster-wise relationships rather than just instance-wise similarities. The model uses an adaptive temperature mechanism and regularization to optimize the contrastive loss, enabling effective clustering without requiring prior knowledge of the number of categories.

## Key Results
- Achieves 63.3 NMI on AgNews dataset, surpassing previous best of 57.8
- Demonstrates consistent improvements on datasets with fewer categories
- Shows comparable performance to state-of-the-art methods on GoogleNews datasets despite having many categories
- Generalizes well to both transformer-based (DistilBERT) and shallow network architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCL learns discriminative subspace representations by enforcing cluster-wise relationships among instances
- Mechanism: SCL constructs virtual positive samples using a self-expressive module that expresses each sample as a linear combination of other samples, then applies contrastive learning to minimize distance between real samples and their virtual positives while maximizing distance to negatives
- Core assumption: Each sample can be represented by a linear combination of features lying in latent subspaces, and samples from the same subspace/cluster will have non-zero coefficients in each other's self-expressive representations
- Evidence anchors:
  - [abstract] "models cluster-wise relationships among instances" and "constructs virtual positive samples using a self-expressive module"
  - [section] "The hypothesis of this paper is that each sample can be represented by a linear combination of several features lying in a set of latent subspaces" and "The second part ∑i̸= j r (Ai j) is a regularization object to allow the coefficient matrix becoming sparse or low-rank"
- Break condition: If the self-expressive coefficients become too sparse or fail to capture meaningful cluster relationships, the virtual positive samples will not be reliable and contrastive learning will not effectively cluster samples

### Mechanism 2
- Claim: SCL avoids requiring prior knowledge of category numbers while still achieving effective clustering
- Mechanism: By learning cluster-aware representations through subspace modeling rather than explicit category labels, SCL can cluster data without knowing the number of clusters in advance
- Core assumption: Discriminative subspace representations that capture cluster-wise relationships will naturally separate into meaningful clusters during the clustering step, regardless of the number of clusters
- Evidence anchors:
  - [abstract] "without requiring prior knowledge of category numbers"
  - [section] "However, all of such methods require a key prior knowledge: the number of categories, which may not available in many real-world scenarios"
- Break condition: If the learned subspace does not sufficiently separate clusters or if the number of clusters is extremely large relative to sample size, clustering performance will degrade significantly

### Mechanism 3
- Claim: SCL's virtual positive sample construction is more efficient than traditional data augmentation methods
- Mechanism: Instead of encoding original samples and their augmented versions separately (requiring double encoding), SCL constructs virtual positives through matrix operations on already-encoded representations
- Core assumption: The self-expressive module can generate meaningful virtual positives that preserve semantic relationships while avoiding the computational cost of explicit data augmentation
- Evidence anchors:
  - [abstract] "has less complexity in positive sample construction"
  - [section] "while existing contrastive learning approaches would individually feed each original sample and its corresponding positive sample to the encoder, our SCL construct positive sample pairs, which can jointly feed them to the encoder"
- Break condition: If the matrix operations in self-expressive module become computationally prohibitive for large datasets or if the quality of virtual positives degrades with larger batch sizes

## Foundational Learning

- Concept: Contrastive learning fundamentals
  - Why needed here: SCL is built on contrastive learning principles but extends them to model cluster-wise rather than just instance-wise relationships
  - Quick check question: What is the key difference between instance-wise and cluster-wise contrastive learning in terms of positive sample selection?

- Concept: Subspace clustering theory
  - Why needed here: SCL's core hypothesis relies on data points lying in a union of low-dimensional subspaces, which forms the basis for self-expressive positive sample construction
  - Quick check question: How does the self-expressive representation enforce that samples from the same subspace have non-zero coefficients in each other's representations?

- Concept: Self-expression and sparse coding
  - Why needed here: The self-expressive module uses self-expression to construct virtual positive samples, which requires understanding how samples can be expressed as linear combinations of other samples
  - Quick check question: What role does the regularization term play in ensuring that non-zero coefficients only occur between samples from the same subspace?

## Architecture Onboarding

- Component map: Encoder → Self-expressive module → Contrastive learning module → Regularization
- Critical path: Text input → Encoder (produces E) → Self-expressive module (produces A and Ev) → Contrastive loss computation → Backpropagation through both modules
- Design tradeoffs: Joint optimization of encoder and self-expressive module vs. separate training; computational efficiency of virtual positive construction vs. quality of positive samples
- Failure signatures: Poor clustering performance indicates either the self-expressive module isn't capturing cluster relationships or the contrastive learning isn't effectively separating clusters; training instability may indicate temperature adaptation issues
- First 3 experiments:
  1. Implement encoder and self-expressive module, verify that affinity matrix A shows clear block-diagonal structure for synthetic data with known clusters
  2. Test temperature adaptation mechanism by monitoring τ during training on a small dataset
  3. Compare clustering performance with and without the self-expressive module on a benchmark dataset to validate its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SCL scale with increasingly large numbers of text categories?
- Basis in paper: [explicit] The authors note that "our method is not suitable to conduct text clustering when the given texts contain too many categories" and that "improvements of our approach are not significant on the three GoogleNews datasets, as there are too many categories defined in these dataset."
- Why unresolved: The paper does not provide systematic experiments or analysis of SCL's performance degradation as the number of categories increases, nor does it quantify at what point the method becomes ineffective.
- What evidence would resolve it: Empirical results showing SCL's performance (NMI, ACC) across datasets with varying numbers of categories (e.g., 2-200+ classes), with analysis of the relationship between category count and clustering quality.

### Open Question 2
- Question: What is the optimal strategy for selecting batch size in SCL to balance computational efficiency and clustering performance?
- Basis in paper: [explicit] The authors state that "the performance of our SCL model is sensitive to the training batch size, a small batch size may lead some samples can not find their subspace."
- Why unresolved: The paper does not provide systematic experiments or guidelines for choosing batch size, nor does it explain the relationship between batch size and the ability of samples to find their subspaces.
- What evidence would resolve it: Experimental results showing SCL's performance across different batch sizes, with analysis of the trade-offs between computational efficiency and clustering accuracy.

### Open Question 3
- Question: Can SCL be extended to handle multi-view or multi-modal text clustering scenarios?
- Basis in paper: [inferred] The paper focuses on single-view text clustering, but the concept of subspace clustering and contrastive learning could potentially be applied to scenarios where text is accompanied by other modalities (e.g., images, audio).
- Why unresolved: The authors do not explore or discuss the potential application of SCL to multi-view or multi-modal clustering tasks, nor do they provide any insights into how the method might need to be adapted for such scenarios.
- What evidence would resolve it: Experimental results comparing SCL's performance on single-view vs. multi-view text clustering tasks, along with any modifications or extensions made to the method to handle multiple modalities.

## Limitations
- Performance degrades on datasets with many categories due to uniform sampling challenges
- Sensitivity to batch size requires careful tuning to ensure samples can find their subspaces
- Missing implementation details for regularization function r(Aij) and exact hyperparameter values

## Confidence

- Mechanism 1 (Subspace representation learning): **High** - Well-supported by theoretical framework and experimental results showing improved NMI scores
- Mechanism 2 (Category-agnostic clustering): **Medium** - Validated on seven datasets but lacks ablation studies on the necessity of category number independence
- Mechanism 3 (Efficient positive sample construction): **Medium** - Conceptually sound but efficiency claims need quantitative validation against augmentation-based methods

## Next Checks
1. Conduct ablation studies to isolate the contribution of the self-expressive module by comparing performance with and without it on datasets of varying category counts
2. Measure computational efficiency by benchmarking runtime and memory usage against traditional data augmentation approaches on large-scale datasets
3. Test the model's robustness to different batch sizes and temperature adaptation schedules to identify optimal training configurations