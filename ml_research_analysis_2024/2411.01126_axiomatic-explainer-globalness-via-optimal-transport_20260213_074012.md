---
ver: rpa2
title: Axiomatic Explainer Globalness via Optimal Transport
arxiv_id: '2411.01126'
source_url: https://arxiv.org/abs/2411.01126
tags:
- globalness
- explanations
- distribution
- feature
- explainer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a measure of explainer globalness to quantify
  the diversity of explanations produced by post-hoc explainability methods. The authors
  propose six axiomatic properties that any globalness measure should satisfy, including
  non-negativity, continuity, convexity, bounds, and invariance to isometries.
---

# Axiomatic Explainer Globalness via Optimal Transport

## Quick Facts
- arXiv ID: 2411.01126
- Source URL: https://arxiv.org/abs/2411.01126
- Reference count: 40
- Primary result: Introduces Wasserstein Globalness (WG), a measure quantifying explainer diversity that satisfies six axiomatic properties and enables better explainer selection

## Executive Summary
This paper introduces a principled approach to measuring explainer globalness through Wasserstein distance, addressing the gap in quantifying the diversity of explanations produced by post-hoc explainability methods. The authors establish six axiomatic properties that any globalness measure should satisfy and prove that their proposed Wasserstein Globalness measure meets all criteria. Through extensive experiments on image, tabular, and synthetic datasets, they demonstrate that WG captures the spectrum of explanation diversity, improves selection among high-faithfulness explainers, and helps identify explainers that reflect the true globalness of the underlying data.

## Method Summary
The method computes Wasserstein Globalness (WG) by measuring the Wasserstein distance between the distribution of explanations generated by an explainer and a uniform baseline distribution. Users select a distance metric dE based on their specific explanation space (continuous for feature attribution, discrete for feature selection) and a baseline distribution UE. The framework is applied by sampling from the dataset, generating explanations with the target explainer, and computing the Wasserstein distance using approximations like sliced Wasserstein distance for high-dimensional spaces. The method is validated across multiple datasets (Divorce, NHANES, MNIST, CIFAR10, ImageNet200) using various explainers (Saliency, IG, DeepSHAP, Guided Backprop, SAGE) with smoothing operations.

## Key Results
- WG satisfies six axiomatic properties (non-negativity, continuity, convexity, bounds, isometry invariance) making it a robust measure of explainer globalness
- Empirical results show WG captures the spectrum of explanation diversity and improves selection among high-faithfulness explainers
- WG helps identify explainers that reflect the true globalness of the underlying data, with optimal smoothing parameters varying by dataset complexity
- The framework enables customization of distance metrics and baseline distributions for specific applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WG measures the diversity of explanations across a dataset, capturing explainer complexity
- Mechanism: WG computes the Wasserstein distance between the distribution of explanations generated by an explainer and a uniform baseline distribution
- Core assumption: A more diverse set of explanations indicates a more complex explainer that captures finer-grained distinctions in the data
- Evidence anchors:
  - [abstract] "WG captures the spectrum of explanation diversity, improves selection among high-faithfulness explainers, and helps identify explainers that reflect the true globalness of the underlying data."
  - [section] "Intuitively, an explainer with a higher globalness score generates fewer distinct explanations, identifies more globally-invariant features, and/or produces more similar explanations (Figure 1)."
  - [corpus] Weak corpus evidence - related papers focus on explainability methods but not directly on globalness measures
- Break condition: If the distance metric dE doesn't capture the relevant notion of similarity for the explanations, WG will not accurately reflect explainer complexity

### Mechanism 2
- Claim: WG satisfies six axiomatic properties that make it a robust measure of explainer globalness
- Mechanism: The axioms (non-negativity, continuity, convexity, bounds, and invariance to isometries) ensure WG is mathematically sound and interpretable
- Core assumption: These axioms capture the intuitive properties we want in a globalness measure and lead to meaningful comparisons between explainers
- Evidence anchors:
  - [abstract] "We establish the axiomatic properties that any such measure should possess and prove that our proposed measure, Wasserstein Globalness, meets these criteria."
  - [section] "We introduce six desirable properties that any globalness measure should satisfy... Building on these axioms, we propose a novel method, Wasserstein Globalness (WG), which quantifies explainer globalness..."
  - [corpus] Weak corpus evidence - related papers don't discuss axiomatic properties of explainability measures
- Break condition: If the axioms don't align with the intended use case or interpretation of globalness, WG may not be the appropriate measure

### Mechanism 3
- Claim: WG enables users to select appropriate distance metrics and baseline distributions based on their specific application
- Mechanism: By allowing users to choose dE and UE, WG can be tailored to capture the relevant notion of similarity between explanations for a given application
- Core assumption: Different applications may require different notions of explanation similarity, and WG's flexibility allows it to adapt to these needs
- Evidence anchors:
  - [abstract] "Notably, WG also enables users to choose a distance metric for explanations, allowing customization of how similarity is measured and specifying the transformations which WG remains invariant to."
  - [section] "The WG framework is highly flexible, requiring only the definition of a metric space (E, dE). This allows it to be applied for both feature attribution (continuous) and feature selection (discrete) cases by selecting an appropriate distance metric dE."
  - [corpus] Weak corpus evidence - related papers don't discuss customizable explainability metrics
- Break condition: If the user doesn't have a clear understanding of the relevant notion of similarity for their application, they may choose inappropriate dE and UE, leading to misleading WG scores

## Foundational Learning

- Concept: Wasserstein distance
  - Why needed here: WG is defined as the Wasserstein distance between the explanation distribution and a uniform baseline
  - Quick check question: What is the Wasserstein distance between two Dirac measures (point masses) located at different points in the explanation space?

- Concept: Axiomatic properties
  - Why needed here: WG is designed to satisfy six specific axiomatic properties
  - Quick check question: Which of the six axiomatic properties ensures that WG is invariant to transformations that preserve explanation similarity (isometries)?

- Concept: Feature attribution vs. feature selection
  - Why needed here: WG can be applied to both feature attribution (continuous explanations) and feature selection (discrete explanations)
  - Quick check question: What is the main difference between feature attribution and feature selection in terms of the explanation space (E)?

## Architecture Onboarding

- Component map:
  - Input: Dataset (X), Black-box model (F), Explainer (E)
  - Distance Metric (dE): User-defined based on explanation space and application
  - Baseline Distribution (UE): User-defined based on explanation space
  - Wasserstein Distance Solver: Approximates the Wasserstein distance between the explanation distribution and the baseline
  - Output: Wasserstein Globalness (WG) score

- Critical path:
  1. User selects distance metric dE and baseline distribution UE based on explanation space and application
  2. N samples are drawn from the dataset X
  3. The explainer E is applied to each sample to generate explanations
  4. The Wasserstein distance is computed between the empirical distribution of explanations and the baseline distribution using the selected distance metric
  5. The resulting Wasserstein distance is the WG score

- Design tradeoffs:
  - Computational complexity: Approximating the Wasserstein distance in high dimensions can be computationally expensive
  - Choice of distance metric: The choice of dE significantly impacts the interpretation of WG scores
  - Choice of baseline distribution: The choice of UE determines the reference scale for WG scores

- Failure signatures:
  - WG scores are unexpectedly high or low: This may indicate an inappropriate choice of distance metric or baseline distribution
  - WG scores don't correlate with intuitive notions of explainer complexity: This may indicate a mismatch between the axioms and the intended interpretation of globalness
  - WG scores are highly sensitive to small changes in explanations: This may indicate a lack of continuity in the distance metric or baseline distribution

- First 3 experiments:
  1. Compare WG scores for different explainers on a simple dataset (e.g., MNIST) to verify that WG captures intuitive differences in explainer complexity
  2. Vary the smoothing parameter σ for a smoothing-based explainer (e.g., SmoothGrad) and verify that WG increases as σ increases, indicating higher globalness
  3. Compare WG scores for explainers with similar faithfulness scores to verify that WG can differentiate between high-faithfulness explainers based on their complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of smoothing parameter σ in SmoothGrad and related methods affect the trade-off between explanation faithfulness and globalness across different datasets and model architectures?
- Basis in paper: [explicit] The paper empirically shows that increasing σ increases globalness but may decrease faithfulness on complex datasets like CIFAR10 and ImageNet
- Why unresolved: The experiments show mixed results, but the paper doesn't provide a theoretical framework for predicting when increased smoothing will help or hurt faithfulness
- What evidence would resolve it: Systematic experiments varying σ across a broader range of datasets with different complexity levels, model architectures (CNNs, Transformers, etc.), and tasks (classification, regression)

### Open Question 2
- Question: What is the optimal way to select the baseline distribution UE for feature attribution explainers when the explanation space is unbounded?
- Basis in paper: [explicit] The paper notes that for unbounded explanation spaces, UE must be bounded and suggests using a radius k based on the maximum norm across explanations
- Why unresolved: The paper's method for determining k is empirical and may not generalize well to all datasets and explainer combinations
- What evidence would resolve it: Comparative studies showing how different methods for selecting k affect globalness rankings of explainers across diverse datasets and explainer types

### Open Question 3
- Question: How does the choice of distance metric dE affect the sensitivity of Wasserstein Globalness to meaningful differences between explanations versus superficial variations?
- Basis in paper: [explicit] The paper shows that different distance metrics (Euclidean, Hamming, Kendall-Tau) can be used and discusses how this affects transformation invariance
- Why unresolved: The paper provides theoretical properties but doesn't empirically validate which metrics best capture meaningful explanation differences
- What evidence would resolve it: Empirical studies comparing how different distance metrics affect globalness rankings across various explanation types and whether those rankings align with human judgment of explanation quality

## Limitations
- The empirical validation primarily focuses on the faithfulness-vs-complexity tradeoff without establishing causal relationships with downstream task performance
- The choice of baseline distribution UE=Uniform(E) assumes minimal globalness corresponds to uniform explanation distribution, which may not hold for all explanation types
- The paper demonstrates WG's ability to distinguish between explainers but doesn't validate whether the most "globally appropriate" explainer actually provides better explanations for end-user decision-making

## Confidence
- High confidence in the theoretical framework and axiomatic properties of WG
- Medium confidence in the empirical results showing WG captures explanation diversity
- Medium confidence in the claim that WG improves explainer selection among high-faithfulness methods
- Low confidence in the practical utility of WG for real-world explainability applications without further user studies

## Next Checks
1. Conduct user studies to validate whether explainers selected by low WG scores actually lead to better human understanding and decision-making compared to high-faithfulness but higher-WG alternatives
2. Test WG on additional explanation types beyond feature attribution (e.g., counterfactual explanations, example-based explanations) to verify the framework's generalizability
3. Compare WG against alternative diversity measures (e.g., entropy-based metrics, mutual information) to establish its relative advantages and limitations for explainer selection