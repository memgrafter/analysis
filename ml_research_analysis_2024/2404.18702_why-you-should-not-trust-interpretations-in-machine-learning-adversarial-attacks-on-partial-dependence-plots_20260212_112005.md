---
ver: rpa2
title: 'Why You Should Not Trust Interpretations in Machine Learning: Adversarial
  Attacks on Partial Dependence Plots'
arxiv_id: '2404.18702'
source_url: https://arxiv.org/abs/2404.18702
tags:
- data
- plots
- feature
- adversarial
- insurance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper exposes how partial dependence plots (PD plots), widely
  used to interpret black-box models, can be manipulated via adversarial attacks.
  The authors propose an adversarial framework that modifies the original model to
  generate deceptive PD plots by exploiting extrapolation behavior in regions of sparse
  data.
---

# Why You Should Not Trust Interpretations in Machine Learning: Adversarial Attacks on Partial Dependence Plots

## Quick Facts
- arXiv ID: 2404.18702
- Source URL: https://arxiv.org/abs/2404.18702
- Authors: Xi Xin; Giles Hooker; Fei Huang
- Reference count: 16
- One-line primary result: Adversarial attacks can manipulate partial dependence plots to hide discriminatory behaviors while preserving model accuracy

## Executive Summary
This paper exposes significant vulnerabilities in partial dependence plots (PD plots), a widely-used interpretation method for black-box models. The authors propose an adversarial framework that can manipulate PD plots by exploiting extrapolation behavior in regions of sparse data, creating deceptive visualizations while maintaining the original model's predictive accuracy. Using real-world insurance and COMPAS datasets, they demonstrate that this attack can intentionally mask discriminatory patterns, making models appear neutral while retaining nearly all original predictions. The study highlights fundamental limitations of permutation-based interpretation methods and raises serious concerns about their reliability in critical decision-making contexts.

## Method Summary
The paper proposes an adversarial framework that modifies black-box models to generate deceptive PD plots by manipulating predictions in extrapolation regions. The approach constructs a classifier to distinguish between original feature space and extrapolation domain, then generates compensating outputs to create manipulated PD plots. The framework maintains original model accuracy while producing misleading interpretations. Experiments are conducted on insurance claims data and COMPAS recidivism data using XGBoost and neural network models respectively, with metrics including true positive rate (TPR) and fooling effectiveness (FE).

## Key Results
- Successfully fools PD plots for driver's age and vehicle value in insurance data while maintaining 95% of original model predictions
- Demonstrates ability to hide discriminatory patterns in COMPAS data, making models appear race-neutral while preserving accuracy
- Shows that fooling multiple PD plots simultaneously is feasible with a unified framework using classifier c1(x)
- Reveals that PD plots can be unreliable when features are strongly correlated, as they extrapolate beyond the data distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial framework manipulates PD plots by replacing predictions in extrapolation regions with compensating outputs.
- Mechanism: Classifier `c(x)` identifies instances in extrapolation domain, and model `g(x)` provides compensating outputs to produce deceptive PD plots.
- Core assumption: Feature of interest is not independent of other features in the dataset.
- Evidence anchors:
  - [abstract]: "This adversarial framework modifies the original black box model to manipulate its predictions for instances in the extrapolation domain."
  - [section 4.1]: "The framework predicts that an instance originates from the original feature space when c(x) = 0 and outputs the black-box prediction f(x) accordingly."
  - [corpus]: No direct evidence found.
- Break condition: If features are independent, the classifier `c(x)` cannot effectively distinguish extrapolation from real data.

### Mechanism 2
- Claim: The framework maintains original model's predictive accuracy while producing manipulated PD plots.
- Mechanism: The adversarial model `a(x) = (1 - c(x)) · f(x) + c(x) · g(x)` preserves `f(x)` predictions for non-extrapolation instances.
- Core assumption: The proportion of data in extrapolation domain is small enough that model accuracy is preserved.
- Evidence anchors:
  - [abstract]: "produces deceptive PD plots that can conceal discriminatory behaviors while preserving most of the original model's predictions."
  - [section 4.1]: "The model a(x) replaces the original machine learning model f(x), which aims to maintain the prediction accuracy of the original model and generate manipulated PD plots."
  - [corpus]: No direct evidence found.
- Break condition: If too much data is classified as extrapolation, model accuracy degrades significantly.

### Mechanism 3
- Claim: Multiple PD plots can be fooled simultaneously using a unified framework with classifier `c1(x)`.
- Mechanism: Classifier `c1(x)` allocates compensating outputs to each targeted feature based on which feature was permuted.
- Core assumption: Permuting different features leads to little overlap in the extrapolation region.
- Evidence anchors:
  - [section 4.4]: "We introduce a unified g(x) model designed to ensure consistent prediction outputs while manipulating q PD plots simultaneously."
  - [section 4.4]: "The inclusion of c1(x) only affects the stability or uncertainty of a(x)."
  - [corpus]: No direct evidence found.
- Break condition: If permuting different features leads to significant overlap in extrapolation regions, consistent predictions become impossible.

## Foundational Learning

- Concept: Partial Dependence Plots (PD plots)
  - Why needed here: PD plots are the target of adversarial attacks, so understanding their construction and limitations is essential.
  - Quick check question: What is the formula for estimating a PD function for a single feature?

- Concept: Extrapolation in correlated features
  - Why needed here: The adversarial attack exploits extrapolation behavior in regions of sparse data to manipulate PD plots.
  - Quick check question: Why do PD plots potentially yield misleading results when features are strongly dependent?

- Concept: Permutation-based interpretation methods
  - Why needed here: The paper focuses on vulnerabilities of permutation-based methods like PD plots.
  - Quick check question: How do permutation-based methods like PD plots differ from conditional methods like M-plots?

## Architecture Onboarding

- Component map:
  Original model `f(x)` -> Classifier `c(x)` -> Compensating model `g(x)` -> Adversarial model `a(x)`

- Critical path:
  1. Train `f(x)` on original data
  2. Generate augmenting sample for training `c(x)`
  3. Construct `c(x)` to identify extrapolation
  4. Define target PD outputs
  5. Compute compensating outputs for `g(x)`
  6. (Optional) Train `c1(x)` for multiple features
  7. Implement `a(x)` and validate effectiveness

- Design tradeoffs:
  - Higher threshold for `c(x)` preserves more original predictions but reduces fooling effectiveness
  - Fooling multiple PD plots introduces complexity but enables broader manipulation
  - Using compensating outputs vs. retraining model affects control over final predictions

- Failure signatures:
  - TPR (true positive rate) drops significantly, indicating loss of original model accuracy
  - Accuracy of attack remains low despite multiple iterations
  - Classifier `c1(x)` misclassifies frequently, causing inconsistent outputs

- First 3 experiments:
  1. Implement framework on simple simulated data with known correlations to verify basic functionality
  2. Test different thresholds for `c(x)` on real insurance data to find optimal balance between accuracy and fooling
  3. Attempt to fool two PD plots simultaneously on COMPAS data to validate multi-feature framework

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adversarial attacks on PD plots be detected or mitigated in practice?
- Basis in paper: [explicit] The paper discusses potential mitigation strategies, including using ICE curves, assessing feature dependencies, and considering alternative models like GAMs.
- Why unresolved: The paper presents mitigation strategies but does not provide a comprehensive evaluation of their effectiveness in real-world scenarios or against sophisticated attacks.
- What evidence would resolve it: Empirical studies testing the proposed mitigation strategies against various types of adversarial attacks on PD plots in different domains and contexts.

### Open Question 2
- Question: How do adversarial attacks on PD plots impact the reliability of model interpretation in high-stakes decision-making contexts?
- Basis in paper: [explicit] The paper demonstrates that PD plots can be manipulated to hide discriminatory behaviors, raising concerns about their reliability in critical decision-making scenarios.
- Why unresolved: The paper does not explore the specific implications of these attacks on decision-making processes in high-stakes domains like healthcare, criminal justice, or finance.
- What evidence would resolve it: Case studies or simulations examining the consequences of manipulated PD plots on real-world decisions and outcomes in high-stakes contexts.

### Open Question 3
- Question: Are there alternative interpretation methods that are more robust to adversarial attacks than PD plots?
- Basis in paper: [inferred] The paper suggests that all interpretation methods have limitations and proposes some alternatives like ICE plots and ALE plots.
- Why unresolved: The paper does not provide a comprehensive comparison of the robustness of different interpretation methods to adversarial attacks.
- What evidence would resolve it: Systematic evaluation of various interpretation methods, including PD plots, ICE plots, ALE plots, and others, against a range of adversarial attacks to determine their relative robustness.

## Limitations

- Effectiveness depends on feature correlations, as low correlations between target feature and other features reduce attack success
- Requires sufficient data in extrapolation domain, which may not exist in datasets with dense feature distributions
- Limited evaluation to only two datasets (insurance and COMPAS), requiring broader validation across domains
- Lack of extensive ablation studies examining impact of hyperparameters on attack effectiveness

## Confidence

Medium confidence in the proposed adversarial framework's effectiveness. The core mechanism relies on assumptions about feature correlations and extrapolation regions that may not hold in all real-world datasets. The experiments demonstrate successful manipulation in two distinct datasets, but generalizability requires further validation.

## Next Checks

1. Test the adversarial framework on additional datasets with varying feature correlation structures to establish robustness across different correlation regimes.

2. Conduct ablation studies varying the classifier threshold and grid point selection to quantify their impact on attack effectiveness and original model accuracy.

3. Implement the framework against alternative interpretation methods (e.g., SHAP values, individual conditional expectation plots) to evaluate whether the vulnerability extends beyond PD plots.