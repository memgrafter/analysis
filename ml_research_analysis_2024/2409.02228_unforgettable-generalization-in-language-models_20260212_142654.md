---
ver: rpa2
title: Unforgettable Generalization in Language Models
arxiv_id: '2409.02228'
source_url: https://arxiv.org/abs/2409.02228
tags:
- forgetting
- forget
- task
- accuracy
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether fine-tuning language models on randomized
  labels can effectively "forget" specific skills, and whether this forgetting generalizes
  across different task instances. The authors train models on tasks like entailment
  classification, commonsense reasoning, and question answering, then fine-tune them
  on randomized labels for those tasks.
---

# Unforgettable Generalization in Language Models

## Quick Facts
- **arXiv ID**: 2409.02228
- **Source URL**: https://arxiv.org/abs/2409.02228
- **Reference count**: 40
- **Primary result**: Fine-tuning on randomized labels can make models produce near-random predictions for individual examples, but generalization of forgetting varies widely across tasks and is largely determined by evaluation tasks rather than training tasks.

## Executive Summary
This paper investigates whether fine-tuning language models on randomized labels can effectively "forget" specific skills and whether this forgetting generalizes across different task instances. The authors train models on tasks like entailment classification, commonsense reasoning, and question answering, then fine-tune them on randomized labels for those tasks. They find that forgetting effectiveness varies widely across tasks: some tasks (like entailment classification) are easily forgotten and generalize well, while others (like physical commonsense reasoning and scientific question answering) are more resistant to forgetting, with models continuing to perform well even on examples similar to those in the training set. Surprisingly, the effectiveness of forgetting is largely determined by the tasks the model is evaluated on, not the task it was trained to forget. The degree of forgetting is not predicted by task difficulty, but weakly correlates with model confidence and the variance of LM representations of training data. The authors also show that even generalizable forgetting is shallow, as linear probes trained on the models' representations can still perform tasks reliably after forgetting.

## Method Summary
The authors use pre-trained Llama2-7B models and 21 binary multiple-choice tasks. They first fine-tune each model on its target task using early stopping on validation accuracy (learning rate 3e-3, 100 epochs max, batch size 3). Then they perform a second fine-tuning on the same training data but with randomized labels (learning rate 1e-4, 100 epochs max or until test accuracy < 0.5). They measure forgetting effectiveness using two metrics: forget gap (accuracy after forgetting - 0.5) and forget ratio ((fine-tuned accuracy - forgetting accuracy) / (fine-tuned accuracy - 0.5)). They also train linear probes on the models' hidden states before and after forgetting to assess whether underlying knowledge is truly removed.

## Key Results
- Fine-tuning on randomized labels can make models produce near-random predictions for individual training examples
- Forgetting effectiveness varies dramatically across tasks, with some tasks (entailment classification) easily forgotten while others (physical commonsense reasoning) resist forgetting
- The effectiveness of forgetting is largely determined by the evaluation tasks, not the task being trained to forget
- Forgetting is "shallow" - linear probes can still perform tasks reliably after forgetting, indicating underlying knowledge remains encoded

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on randomized labels can make models produce near-random predictions for individual examples in the training set.
- Mechanism: By repeatedly showing the model training examples paired with incorrect labels, the gradient updates push the model away from the original task-specific decision boundaries. This process disrupts the learned mapping between inputs and correct outputs for those specific examples.
- Core assumption: The model's internal representations and decision boundaries are plastic enough to be altered through continued exposure to incorrect labels.
- Evidence anchors:
  - [abstract] "Such LMs learn to generate near-random predictions for individual examples in the 'training' set used for forgetting."
  - [section 4] "During the forgetting phase, however, we observe several distinct categories of behavior (1) forget accuracy is very similar to the fine-tuned accuracy..."
- Break condition: If the model's representations are too robust or the learning rate too low, the randomized labels may not induce sufficient parameter updates to shift the decision boundaries away from the original task.

### Mechanism 2
- Claim: Generalization of forgetting (i.e., affecting new examples outside the training set) is determined more by the evaluation tasks than by the task being trained to forget.
- Mechanism: The fine-tuning process on randomized labels does not erase the underlying knowledge or capabilities entirely but instead alters the model's behavior in a task-dependent manner. The model's ability to generalize forgetting to new examples is influenced by the structure and nature of the evaluation tasks themselves.
- Core assumption: Different tasks rely on different aspects of the model's learned representations, and the fine-tuning process interacts with these aspects in complex, task-specific ways.
- Evidence anchors:
  - [abstract] "The degree of forgetting is largely determined by the tasks the model is evaluated on, not the task it was trained to forget."
  - [section 4] "As shown in Figure 3, we find that the effectiveness of the forgetting procedure is largely determined by the tasks that the model is evaluated onâ€”not the training task."
- Break condition: If the evaluation tasks are too dissimilar from the training task, the forgetting effects may not generalize as observed.

### Mechanism 3
- Claim: Even when models appear to forget a task (producing near-random outputs), the underlying knowledge is not truly removed but is instead "suppressed" or made less accessible.
- Mechanism: Linear probes trained on the model's hidden states can still perform the task accurately after the forgetting procedure. This suggests that the information necessary for the task is still encoded in the model's representations but is not being used by the model's output layer due to the fine-tuning process.
- Core assumption: The fine-tuning process on randomized labels primarily affects the model's output layer and decision boundaries, but does not significantly alter the intermediate representations learned during pre-training.
- Evidence anchors:
  - [abstract] "Finally, we show that even generalizable forgetting is shallow: linear probes trained on LMs' representations can still perform tasks reliably after forgetting."
  - [section 7] "The results are shown in Figure 6. We find that the fine-tuning procedure largely does not influence the probing effectiveness."
- Break condition: If the fine-tuning process significantly alters the intermediate representations, the linear probes may not be able to recover the original task performance.

## Foundational Learning

- Concept: Fine-tuning and its effects on model behavior
  - Why needed here: Understanding how fine-tuning alters a model's decision boundaries and representations is crucial for interpreting the forgetting results.
  - Quick check question: What is the primary difference between pre-training and fine-tuning in terms of their impact on a language model?

- Concept: Generalization in machine learning
  - Why needed here: The paper studies whether forgetting generalizes to new examples, which is a form of generalization. Understanding the factors that influence generalization is essential for interpreting the results.
  - Quick check question: What are some common factors that influence a model's ability to generalize to new examples?

- Concept: Linear probes and their use in analyzing model representations
  - Why needed here: The paper uses linear probes to show that the underlying knowledge is not truly removed even when the model appears to forget. Understanding how linear probes work is essential for interpreting this result.
  - Quick check question: How do linear probes help in understanding what information is encoded in a model's hidden states?

## Architecture Onboarding

- Component map: Pre-trained LM -> Fine-tuning on correct labels -> Fine-tuning on randomized labels -> Evaluation -> Linear probe training -> Evaluation
- Critical path: 1. Pre-train the language model 2. Fine-tune on the target task 3. Fine-tune on randomized labels for forgetting 4. Evaluate the model's performance on the task 5. Train linear probes on the model's hidden states 6. Evaluate the linear probes' performance
- Design tradeoffs:
  - Learning rate for forgetting: A higher learning rate may lead to faster forgetting but could also cause catastrophic forgetting of other tasks.
  - Number of epochs for forgetting: More epochs may lead to better forgetting but could also cause the model to overfit to the randomized labels.
  - Size of the training set for forgetting: A larger training set may lead to better forgetting but could also be more computationally expensive.
- Failure signatures:
  - If the model's accuracy on the task does not decrease after forgetting, the forgetting procedure may not be effective.
  - If the linear probes' accuracy on the task is significantly lower after forgetting, the underlying knowledge may have been truly removed.
  - If the model's accuracy on the task decreases significantly after fine-tuning, the fine-tuning procedure may be too aggressive.
- First 3 experiments:
  1. Fine-tune the pre-trained model on a simple task (e.g., entailment classification) and evaluate its performance on the test set.
  2. Fine-tune the model on randomized labels for the same task and evaluate its performance on the test set.
  3. Train linear probes on the model's hidden states before and after the forgetting procedure and compare their performance on the task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we predict which specific examples within a task will be forgotten versus retained during the random-label forgetting process?
- Basis in paper: [explicit] The paper explicitly states that while task-level predictors like model confidence and hidden state variance can predict forgetting at the task level, "we did not observe any correlation between any of the above metrics and models' behavior at the level of individual examples."
- Why unresolved: The authors found that different effects may dominate at the finer-grained level of individual examples, and that focusing on a narrow scope of same-task examples may reveal other effects that were too strong to see an effect with the current traits.
- What evidence would resolve it: Experiments that systematically vary example-level properties (difficulty, similarity to training examples, input patterns) and measure their correlation with forgetting outcomes. Testing whether example-level model confidence, representation similarity, or other features can predict forgetting would be particularly valuable.

### Open Question 2
- Question: What is the relationship between the order in which examples are learned and forgotten, and how does this relationship vary across different model architectures and pretraining procedures?
- Basis in paper: [explicit] The paper finds a "consistent, modest correlation between learning order and forgetting order" across tasks, but hypothesizes that "the lack of a stronger correlation may be due to the shallow nature of fine-tuning."
- Why unresolved: The study only examined Llama2 7B models and did not explore how different architectures or pretraining methods might affect the learning-forgetting relationship. The authors suggest that if learning is "shallow" (just aligning rather than teaching new capabilities), the learning order may be more related to the model's initial state.
- What evidence would resolve it: Experiments comparing learning-forgetting orders across different model architectures (varying size, pretraining objectives, training duration) would reveal whether this relationship is fundamental or an artifact of specific training procedures.

### Open Question 3
- Question: How does the depth and type of forgetting achieved by random-label fine-tuning compare to other unlearning methods in terms of both generalization and information removal?
- Basis in paper: [explicit] The paper shows that random-label forgetting is "at best shallow" since linear probes can still recover task performance after forgetting, and asks whether this is unique to their method or a general property of forgetting approaches.
- Why unresolved: The authors only tested one forgetting method (random-label fine-tuning) and one evaluation method (linear probes), leaving open whether other approaches might achieve deeper forgetting or whether the shallowness is an inherent limitation of current methods.
- What evidence would resolve it: Comparative studies of multiple forgetting methods (gradient ascent, representation manipulation, knowledge editing) using multiple evaluation techniques (probing at different layers, activation-based tests, downstream task performance) would reveal whether some methods achieve deeper forgetting than others.

## Limitations

- Forgetting effectiveness varies dramatically and unpredictably across tasks, with no clear predictors for why some tasks resist forgetting
- Current fine-tuning approaches produce only "shallow" forgetting that can be reversed by linear probes, suggesting true capability removal remains unachievable
- The mechanism by which evaluation task structure determines forgetting effectiveness is not well understood and appears to be a fundamental limitation of current approaches

## Confidence

- **High confidence**: The experimental methodology is sound and the core empirical findings (variable forgetting effectiveness across tasks, shallow forgetting detectable by probes) are robust.
- **Medium confidence**: The observation that forgetting effectiveness correlates weakly with model confidence and representation variance is supported but requires further validation.
- **Low confidence**: The theoretical explanation for why evaluation tasks determine forgetting effectiveness is speculative and not well-supported by current evidence.

## Next Checks

1. **Probe architecture sensitivity**: Test whether different probe architectures (MLP vs linear) or training procedures yield different conclusions about the depth of forgetting.
2. **Fine-tuning hyperparameter sweep**: Systematically vary learning rates and epochs for the forgetting phase to determine if optimal parameters exist for each task type.
3. **Cross-task generalization**: Evaluate whether models fine-tuned to forget one task show predictable forgetting patterns on structurally similar tasks, to better understand the relationship between task structure and forgetting effectiveness.