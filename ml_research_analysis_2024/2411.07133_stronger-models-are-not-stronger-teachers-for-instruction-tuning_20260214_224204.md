---
ver: rpa2
title: Stronger Models are NOT Stronger Teachers for Instruction Tuning
arxiv_id: '2411.07133'
source_url: https://arxiv.org/abs/2411.07133
tags:
- response
- base
- instruction
- generators
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the widely held assumption that larger or
  stronger language models make better teachers for instruction tuning. Through extensive
  experiments across five base models and twenty response generators from seven model
  families, the authors reveal the "Larger Models'' Paradox": larger response generators
  do not necessarily improve a base model''s instruction-following capabilities compared
  to their smaller counterparts.'
---

# Stronger Models are NOT Stronger Teachers for Instruction Tuning

## Quick Facts
- arXiv ID: 2411.07133
- Source URL: https://arxiv.org/abs/2411.07133
- Reference count: 40
- Primary result: Larger response generators do not necessarily improve base model instruction-following capabilities compared to smaller counterparts

## Executive Summary
This paper challenges the widely held assumption that larger or stronger language models make better teachers for instruction tuning. Through extensive experiments across five base models and twenty response generators from seven model families, the authors reveal the "Larger Models' Paradox": larger response generators do not necessarily improve a base model's instruction-following capabilities compared to their smaller counterparts. Notably, open-source models like Gemma-2-9b-it and Qwen2.5-72B-Instruct consistently outperform GPT-4 as response generators.

## Method Summary
The authors conducted comprehensive experiments across five base models and twenty response generators from seven different model families to evaluate the effectiveness of various response generators in instruction tuning. They compared traditional metrics for alignment data selection against their proposed Compatibility-Adjusted Reward (CAR) metric, which quantifies the risk-return trade-off in selecting response generators by incorporating both response quality (measured by reward models) and compatibility (measured by the average loss of responses on the base model). The experiments systematically tested different combinations of base models and response generators to identify patterns in instruction-following capability improvements.

## Key Results
- The "Larger Models' Paradox" shows larger response generators do not consistently improve base model instruction-following capabilities
- Open-source models like Gemma-2-9b-it and Qwen2.5-72B-Instruct consistently outperform GPT-4 as response generators
- CAR metric outperforms all baseline metrics in predicting response generator effectiveness without requiring actual instruction tuning

## Why This Works (Mechanism)
The effectiveness of smaller response generators over larger ones appears to stem from better compatibility with base models rather than raw response quality. When response generators produce outputs that align well with a base model's capabilities and expected output distribution, the resulting instruction tuning is more effective. The CAR metric captures this by balancing quality assessment through reward models with compatibility measurement through base model loss, providing a more accurate prediction of instruction-tuning effectiveness than quality metrics alone.

## Foundational Learning
- **Instruction Tuning**: A training paradigm where models learn to follow instructions through fine-tuning on curated datasets
  - Why needed: Enables models to generalize across diverse tasks and prompts
  - Quick check: Does the model generate appropriate responses to unseen instructions?
- **Response Generator Compatibility**: The degree to which a response generator's outputs align with a base model's capabilities
  - Why needed: Incompatible responses can degrade rather than improve base model performance
  - Quick check: Do the response generator's outputs match the base model's expected output distribution?
- **Reward Modeling**: Using learned models to estimate the quality of generated responses
  - Why needed: Provides automated quality assessment without human evaluation
  - Quick check: Does the reward model's ranking correlate with human judgments?

## Architecture Onboarding
- **Component Map**: Base Model <- Response Generator -> Quality Assessment (Reward Model + Compatibility Score) -> CAR Metric
- **Critical Path**: Response Generator selection → Quality assessment → CAR calculation → Prediction of instruction-tuning effectiveness
- **Design Tradeoffs**: Larger response generators offer higher quality but lower compatibility; smaller generators offer better compatibility but potentially lower quality
- **Failure Signatures**: Poor CAR predictions occur when compatibility and quality metrics diverge significantly
- **First Experiments**:
  1. Compare CAR predictions against actual instruction-tuning results across diverse model pairs
  2. Test CAR metric performance with different reward model configurations
  3. Evaluate CAR's sensitivity to different compatibility measurement approaches

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the generalizability of the Larger Models' Paradox across different instruction-tuning scenarios, the potential advantages of larger models in other aspects such as robustness or safety, and whether the CAR metric captures all relevant dimensions of response generator effectiveness beyond the tested scenarios.

## Limitations
- Experiments focus on specific instruction-tuning scenarios that may not generalize to all deployment contexts
- Study does not examine whether larger models might excel in other aspects like robustness or safety
- CAR metric validation is limited to a subset of baseline metrics and may not capture all relevant dimensions

## Confidence
- Larger Models' Paradox: High confidence in experimental findings, Medium confidence in broader implications
- CAR metric effectiveness: High confidence for tested scenarios, Low-Medium confidence for general applicability
- Open-source vs. proprietary comparison: Medium confidence, limited by potential confounding factors

## Next Checks
1. Test the CAR metric across additional model families and instruction domains not covered in the original study to assess generalizability
2. Conduct ablation studies on the compatibility and quality components of CAR to quantify their relative contributions
3. Evaluate response generators across multiple performance dimensions (including robustness, safety, and edge case handling) to determine if the paradox holds across all relevant metrics