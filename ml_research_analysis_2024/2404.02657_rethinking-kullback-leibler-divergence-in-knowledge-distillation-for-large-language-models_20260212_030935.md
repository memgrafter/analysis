---
ver: rpa2
title: Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large
  Language Models
arxiv_id: '2404.02657'
source_url: https://arxiv.org/abs/2404.02657
tags:
- teacher
- part
- epochs
- table
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper challenges the common belief that reverse Kullback-Leibler
  (RKL) divergence is superior to forward KL (FKL) for knowledge distillation in large
  language models (LLMs). Theoretical and empirical analysis shows that both divergences
  converge to the same objective after sufficient training epochs, but FKL and RKL
  focus on different parts of the distribution (head vs tail) at the beginning.
---

# Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models

## Quick Facts
- **arXiv ID:** 2404.02657
- **Source URL:** https://arxiv.org/abs/2404.02657
- **Reference count:** 19
- **Primary result:** Adaptive Kullback-Leibler (AKL) divergence improves knowledge distillation for LLMs by adaptively combining forward and reverse KL based on head/tail distribution gaps.

## Executive Summary
This paper challenges the conventional wisdom that reverse Kullback-Leibler (RKL) divergence is superior to forward KL (FKL) for knowledge distillation in large language models. Through theoretical analysis and empirical validation, the authors demonstrate that both divergences converge to the same optimization objective after sufficient training, contrary to popular belief about their mode-seeking vs mean-seeking properties. The key insight is that FKL and RKL focus on different parts of the distribution (head vs tail) at the beginning of training, which the proposed Adaptive Kullback-Leibler (AKL) divergence method exploits by adaptively combining them based on distribution gaps. Experiments show AKL outperforms baselines in both metric-based and GPT-4-based evaluations, improving both diversity and quality of generated responses.

## Method Summary
The paper proposes Adaptive Kullback-Leibler (AKL) divergence for knowledge distillation in LLMs. AKL works by calculating separate gaps for head and tail parts of the distribution, then assigning higher weights to the divergence (FKL or RKL) that has larger gap in that region. The method uses a threshold parameter μ (set to 0.5) to distinguish head from tail tokens, where tokens with probability above μ are considered head and below μ are tail. The gap function ϵ(p(z), q(z)) = |p(z) - q(z)| measures the difference between teacher and student distributions. During training, AKL adaptively allocates weights to combine FKL and RKL based on these gaps, allowing better alignment where needed most. The method was evaluated using GPT-2 (1.5B) as teacher and GPT-2 (120M) as student, along with LLaMA (6.7B) and TinyLLaMA (1.1B) configurations.

## Key Results
- AKL outperforms FKL and RKL baselines on Dolly, S-NI, and UnNI datasets with consistent improvements in Rouge-L and BERTscores
- GPT-4-based evaluation shows AKL improves both diversity and quality of generated responses compared to baselines
- Theoretical analysis proves FKL and RKL converge to the same objective in LLM KD, challenging common assumptions about their differences
- AKL demonstrates sensitivity to threshold parameter μ, with μ=0.5 showing optimal performance across experiments

## Why This Works (Mechanism)

### Mechanism 1
In KD for LLMs, FKL and RKL share the same optimization objective, forcing the student model to generate the same logits as the teacher model. The gradient derivation for both FKL and RKL shows that convergence occurs when qθ(Yj|y<t) = p(Yj|y<t) for all vocabulary items j. This equivalence holds under the assumption that both teacher and student distributions are discrete and defined over the same vocabulary set.

### Mechanism 2
FKL focuses on the head part and RKL focuses on the tail part of the teacher distribution at the beginning epochs. FKL loss weights tokens by p(z), giving higher priority to fitting areas with larger teacher probability (head). RKL loss weights by qθ(z), making it easier to fit areas where p(z) is small (tail). This head/tail focusing distinction is meaningful during early training stages but disappears after sufficient epochs (50+ in the paper).

### Mechanism 3
AKL improves distillation by adaptively combining FKL and RKL based on distribution gaps. The method calculates separate gaps for head and tail parts, then assigns higher weights to the divergence that has larger gap in that region, allowing better alignment where needed most. This adaptive weighting is particularly effective during typical training epochs (10-20) when head/tail distinctions remain meaningful.

## Foundational Learning

- **Concept:** KL divergence and its asymmetry
  - Why needed here: Understanding why FKL ≠ RKL is fundamental to grasping the paper's contributions
  - Quick check question: What is the key difference between FKL(p,q) and RKL(p,q) in terms of their mathematical formulation?

- **Concept:** Knowledge distillation in language models
  - Why needed here: The paper specifically addresses KD for LLMs, which has unique properties compared to other domains
  - Quick check question: Why don't the mean-seeking and mode-seeking behaviors of FKL and RKL hold in LLM KD as they do in other domains?

- **Concept:** Token-level distribution alignment
  - Why needed here: KD for LLMs works at the token level, requiring understanding of how distributions align at each generation step
  - Quick check question: How does the additive decomposition of KL divergence across tokens enable the theoretical analysis?

## Architecture Onboarding

- **Component map:** Teacher model (GPT-2 1.5B or LLaMA 6.7B) -> Student model (GPT-2 120M or TinyLLaMA 1.1B) -> AKL loss function -> Gap calculation module -> Training loop with Adam optimizer

- **Critical path:** 1. Teacher generates logits for input 2. Student generates logits for same input 3. AKL calculates head/tail gaps and weights 4. Loss is computed and backpropagated to student 5. Student parameters are updated

- **Design tradeoffs:** Computational overhead: AKL requires extra gap calculation but provides better alignment. Hyperparameter sensitivity: Threshold μ for head/tail split needs tuning. Memory usage: AKL requires slightly more GPU memory than baselines.

- **Failure signatures:** If μ is set too low, AKL may overfit to tail distribution. If μ is set too high, AKL may ignore important tail information. If teacher and student have very different architectures, AKL may not work as well.

- **First 3 experiments:** 1. Compare AKL vs FKL vs RKL on a small dataset with 3-5 epochs to verify head/tail focusing 2. Test different values of μ (0.3, 0.5, 0.7) to find optimal threshold 3. Compare AKL vs FKL+RKL with fixed weights to verify adaptive weighting is beneficial

## Open Questions the Paper Calls Out

### Open Question 1
How does the Adaptive Kullback-Leibler (AKL) divergence method perform when applied to larger language models beyond GPT-2 and LLaMA 6.7B, such as LLaMA-2 70B or other transformer-based models? The paper mentions that one limitation is not having conducted experiments on bigger language models due to limited resources. This remains unresolved as the paper did not perform experiments on larger models, leaving the scalability and effectiveness of AKL on bigger models unexplored.

### Open Question 2
What are the specific impacts of the threshold parameter μ on the performance of AKL across different datasets and tasks? The paper mentions sensitivity analysis of μ in AKL and shows results for different values of μ (0.45, 0.50, 0.55), but does not provide a comprehensive analysis across all datasets and tasks. This remains unresolved as the paper only shows sensitivity analysis for a few values of μ and does not explore its impact comprehensively across different datasets and tasks.

### Open Question 3
How does the computational overhead of AKL compare to other knowledge distillation methods in terms of training time and GPU memory usage when applied to larger datasets and models? The paper reports the computing costs of AKL in terms of GPU memory and training time for a specific experiment but does not provide a comprehensive comparison with other methods on larger datasets and models. This remains unresolved as the paper only provides a specific case study of computing costs and does not compare AKL's computational overhead with other methods on larger datasets and models.

## Limitations

- Theoretical analysis assumes discrete distributions and may not hold for continuous outputs or non-standard architectures
- Empirical validation focuses primarily on short-range language modeling tasks (up to 512 tokens), leaving questions about longer-form generation
- Evaluation relies on relatively small sample sizes (500 samples for testing) and introduces potential variability in GPT-4-based quality assessment

## Confidence

- **High Confidence:** The theoretical derivation showing FKL and RKL share the same convergence objective is mathematically rigorous and well-supported
- **Medium Confidence:** The empirical results demonstrating AKL's superiority over baselines are promising but limited in scope
- **Medium Confidence:** The claim about head/tail focusing at early epochs is supported by theoretical analysis but relies on the assumption that μ=0.5 is universally optimal

## Next Checks

1. **Longer Sequence Testing:** Evaluate AKL performance on extended generation tasks (2K+ tokens) to verify that head/tail focusing remains beneficial for longer-range dependencies and to test the method's scalability

2. **Architecture Ablation Study:** Test AKL with non-standard model architectures (e.g., continuous output distributions, non-token-level predictions) to verify the theoretical assumptions about discrete distributions and convergence conditions

3. **Threshold Sensitivity Analysis:** Systematically vary the head/tail threshold μ across a broader range (0.3 to 0.7) on multiple datasets to establish whether the claimed universal optimality of μ=0.5 holds across different domains and model scales