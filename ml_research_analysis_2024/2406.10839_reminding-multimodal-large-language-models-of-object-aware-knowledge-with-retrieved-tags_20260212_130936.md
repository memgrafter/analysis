---
ver: rpa2
title: Reminding Multimodal Large Language Models of Object-aware Knowledge with Retrieved
  Tags
arxiv_id: '2406.10839'
source_url: https://arxiv.org/abs/2406.10839
tags:
- image
- tags
- llav
- retrieved
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'TUNA introduces a tag-grounded visual instruction tuning framework
  with retrieval-augmentation to address three challenges in MLLMs: failure to identify
  novel objects/entities, mention of non-existent objects, and neglect of object details.
  The method retrieves object-aware tags from a large-scale external datastore and
  uses an image-aware tag encoder with adaptive weighting to enhance the multimodal
  connector''s mapping of out-of-distribution images.'
---

# Reminding Multimodal Large Language Models of Object-aware Knowledge with Retrieved Tags

## Quick Facts
- arXiv ID: 2406.10839
- Source URL: https://arxiv.org/abs/2406.10839
- Authors: Daiqing Qi; Handong Zhao; Zijun Wei; Sheng Li
- Reference count: 28
- TUNA outperforms baselines sharing the same language model and training data on 12 benchmarks, achieving notable improvements in object identification and detail recognition

## Executive Summary
TUNA addresses fundamental limitations in multimodal large language models (MLLMs) related to identifying novel objects, avoiding mention of non-existent objects, and capturing object details. The framework introduces a tag-grounded visual instruction tuning approach with retrieval augmentation, using object-aware tags retrieved from a large external datastore to enhance the multimodal connector's mapping of out-of-distribution images. By combining an image-aware tag encoder with adaptive weighting, TUNA significantly improves performance on standard benchmarks while demonstrating strong zero-shot capability on domain-specific tasks.

## Method Summary
TUNA implements a tag-grounded visual instruction tuning framework that retrieves object-aware tags from a large-scale external datastore and uses an image-aware tag encoder with adaptive weighting to enhance the multimodal connector's mapping of out-of-distribution images. The method fine-tunes models using instruction data with a learning rate of 2e-5 and batch size of 128, leveraging retrieved tags to provide object names, attributes, and contextual information that helps the LLM identify and describe novel objects even when the connector mapping fails.

## Key Results
- TUNA outperforms baselines sharing the same language model and training data on 12 benchmarks
- Notable improvements in object identification and detail recognition compared to state-of-the-art methods
- Demonstrates zero-shot capability on domain-specific tasks when provided with relevant datastores

## Why This Works (Mechanism)

### Mechanism 1
The multimodal connector's mapping bottleneck from insufficient training data causes failures on OOD images. The connector learns to map CLIP vision embeddings to LLM text embeddings during training, but when encountering objects/entities not present in the training data, this mapping becomes inaccurate, leading to incorrect responses about novel objects.

### Mechanism 2
Retrieval-augmented tag tokens provide object-aware knowledge that compensates for connector limitations. By retrieving tags associated with similar images from a large external datastore, TUNA provides the LLM with object names, attributes, and contextual information that helps identify and describe novel objects even when the connector mapping fails.

### Mechanism 3
Adaptive weighting of retrieved tags improves quality by focusing on relevant information. The adaptive weight tuner calculates relevance scores based on cosine similarity between tag text embeddings and input image visual features, then applies these weights to emphasize useful tags while suppressing irrelevant ones.

## Foundational Learning

- **Image-to-text mapping in multimodal models**: Understanding how multimodal connectors translate visual features to text embeddings is fundamental to grasping TUNA's approach. *Quick check: What happens when a connector encounters an image with objects not seen during training?*

- **Retrieval-augmented generation (RAG)**: TUNA builds on RAG principles but adapts them for object-aware tag retrieval rather than caption retrieval. *Quick check: How does TUNA's tag retrieval differ from traditional caption-based RAG approaches?*

- **Feature embedding spaces and similarity metrics**: Understanding how CLIP embeddings, text embeddings, and tag embeddings relate through cosine similarity is crucial for the adaptive weighting mechanism. *Quick check: How does cosine similarity between tag text embeddings and image visual features determine tag relevance?*

## Architecture Onboarding

- **Component map**: Vision encoder (frozen CLIP) → Multimodal connector → Adaptive weight tuner → Image-aware tag encoder → LLM; External datastore with image embeddings as keys and tags as values; Retrieval module that finds similar images and extracts associated tags

- **Critical path**: 1. Input image and instruction processed; 2. Similar images retrieved from datastore; 3. Tags extracted and encoded with image-aware tag encoder; 4. Adaptive weights applied to tag representations; 5. Combined with original multimodal features for LLM input

- **Design tradeoffs**: Large datastore provides better coverage but increases retrieval latency; Simple cosine similarity weighting is efficient but may miss complex relevance patterns; Frozen vision encoder ensures stability but limits adaptation to domain-specific features

- **Failure signatures**: Consistently irrelevant retrieved tags indicate datastore quality issues; Poor performance on known objects suggests connector training problems; Performance drops without adaptive weighting confirm its importance

- **First 3 experiments**: 1. Test baseline performance on OOD objects without retrieval augmentation; 2. Evaluate retrieval quality by examining top-k tags for various input images; 3. Compare performance with and without adaptive weighting on a subset of benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of TUNA scale with increasing datastore size beyond 15M image-text pairs? The paper only tests up to 15M image-text pairs and does not explore the performance gains or potential diminishing returns of scaling the datastore size further. Experiments comparing TUNA's performance using datastores of varying sizes (e.g., 30M, 50M, 100M) on the same benchmarks would clarify the scaling behavior.

### Open Question 2
How robust is TUNA to noisy or irrelevant tags when the retrieval process fails to find highly relevant images? While the paper demonstrates robustness to some degree, it does not explore the limits of this robustness or quantify the performance degradation when the retrieved tags are consistently irrelevant or noisy. Controlled experiments where the retrieval process is intentionally made to retrieve irrelevant or noisy tags, and the resulting performance degradation is measured, would provide insights into TUNA's robustness limits.

### Open Question 3
How does the performance of TUNA vary with different choices of vision encoders, especially those with better out-of-distribution generalization capabilities? The paper only uses CLIP as the vision encoder and does not explore the potential benefits of using alternative vision encoders with better OOD generalization. Experiments comparing TUNA's performance using different vision encoders (e.g., OpenCLIP, BLIP-2) on the same benchmarks would quantify the impact of the vision encoder choice.

## Limitations
- The effectiveness of TUNA heavily depends on the quality and coverage of the external datastore
- The adaptive weighting mechanism relies on simple cosine similarity, which may not capture complex semantic relationships between tags and images
- The framework is bottlenecked by the capability of CLIP and would benefit from more powerful vision-language models

## Confidence

- **Medium** for claims about connector bottleneck being the primary cause of OOD failures
- **Low** for claims about adaptive weighting effectiveness
- **Medium** for zero-shot domain-specific performance claims

## Next Checks

1. **Retrieval Quality Analysis**: Analyze the top-10 retrieved tags for a diverse set of OOD images to quantify relevance rates and determine whether the datastore actually contains useful information for novel objects.

2. **Connector Mapping Investigation**: Conduct controlled experiments comparing connector performance on in-distribution versus OOD images using metrics like embedding similarity and downstream task accuracy to validate whether connector mapping degradation is the primary bottleneck.

3. **Adaptive Weighting Robustness**: Test the adaptive weighting mechanism across different domains and object types to identify failure patterns, including analysis of when the weighting system makes incorrect prioritization decisions.