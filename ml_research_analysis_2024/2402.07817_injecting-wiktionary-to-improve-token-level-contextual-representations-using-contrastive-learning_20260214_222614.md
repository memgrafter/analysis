---
ver: rpa2
title: Injecting Wiktionary to improve token-level contextual representations using
  contrastive learning
arxiv_id: '2402.07817'
source_url: https://arxiv.org/abs/2402.07817
tags:
- embeddings
- step
- fine-tuning
- task
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work fine-tunes contextual token embeddings using contrastive
  learning on Wiktionary examples, where positive pairs consist of sentences containing
  the same word sense. Compared to self-supervised approaches, leveraging lexicon
  examples provides supervision that leads to embeddings better suited for lexical
  semantic tasks.
---

# Injecting Wiktionary to improve token-level contextual representations using contrastive learning

## Quick Facts
- arXiv ID: 2402.07817
- Source URL: https://arxiv.org/abs/2402.07817
- Reference count: 29
- This work fine-tunes contextual token embeddings using contrastive learning on Wiktionary examples, achieving state-of-the-art performance on WiC in the unsupervised setting.

## Executive Summary
This paper proposes fine-tuning contextual embeddings using contrastive learning on Wiktionary examples, where positive pairs consist of sentences containing the same word sense. The approach leverages lexicon examples as supervision, which leads to embeddings better suited for lexical semantic tasks compared to self-supervised methods. Experiments show the fine-tuned embeddings achieve state-of-the-art performance on the WiC task in the unsupervised setting and improve results on two new WiC test sets derived from Wiktionary and FrameNet examples. The method also provides modest gains for the downstream task of semantic frame induction.

## Method Summary
The method fine-tunes BERT token-level embeddings using a multiple-positive contrastive loss on Wiktionary examples, grouping sentences by word sense. The training data includes verbs with 1-10 senses from Wiktionary (13,118 verbs, 26,398 senses, 68,271 examples). After fine-tuning, PCA dimensionality reduction (100 components with whitening) is applied to the embeddings. The approach is evaluated on WiC classification using cosine similarity thresholds and on semantic frame induction using two-step clustering (X-means followed by group-average clustering).

## Key Results
- Achieves state-of-the-art performance on WiC task in unsupervised setting
- Improves results on two new WiC test sets derived from Wiktionary and FrameNet examples
- Shows modest gains on semantic frame induction task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning contextual embeddings using contrastive learning on Wiktionary examples reduces the variance of token representations for the same word sense.
- Mechanism: The multiple-positive contrastive loss pulls together embeddings of target tokens that appear in example sentences labeled with the same sense, while pushing apart embeddings from different senses. This creates tighter clusters in embedding space for tokens sharing a sense.
- Core assumption: Wiktionary examples are reliable enough to serve as supervision for distinguishing word senses, despite potential coverage and granularity mismatches.
- Evidence anchors:
  - [abstract] "Fine-tuning pre-trained language models (PLMs) using contrastive learning on Wiktionary examples, where positive pairs consist of sentences containing the same word sense."
  - [section 4] "Our training data includes the examples for all verbs having from 1 to 10 senses... In total, we obtained a dataset of 13,118 verbs having in total 26,398 senses, with a total of 68,271 examples."
  - [corpus] Weak: neighbor corpus contains no related contrastive learning papers with lexicon supervision; evidence comes solely from this paper's experiments.
- Break condition: If the lexicon's sense annotations are noisy or mismatched with the target domain, the contrastive signals may push embeddings in wrong directions, degrading performance.

### Mechanism 2
- Claim: Dimensionality reduction via PCA (with whitening) smooths high-dimensional embedding space flaws and improves similarity-based downstream tasks.
- Mechanism: High-dimensional BERT embeddings have uneven similarity distributions; PCA with whitening normalizes variance across dimensions, making cosine similarity more reliable for detecting sense similarity.
- Core assumption: The variance of cosine similarity is dominated by a few dimensions and PCA can re-balance this effect.
- Evidence anchors:
  - [section 4] "we also experiment with a simple PCA reduction of the PLM embeddings, with or without whitening."
  - [section 4] "we observe that PCA is beneficial when applied to plain BERT embeddings, and the improvements add up when applying both fine-tuning and PCA."
  - [corpus] Missing: no neighbor papers cited that validate PCA smoothing specifically for contextual embeddings; evidence is internal.
- Break condition: If the intrinsic dimensionality of the sense space is high, aggressive PCA reduction may discard discriminative dimensions, hurting performance.

### Mechanism 3
- Claim: Contrastive fine-tuning with lexicon examples generalizes beyond the training domain, improving WiC and frame induction tasks.
- Mechanism: The lexicon-based contrastive objective captures general lexical-semantic regularities that transfer to downstream sense-aware tasks, even when those tasks use different sense inventories (Wiktionary vs. FrameNet).
- Core assumption: The sense distinctions captured in Wiktionary overlap sufficiently with those in other lexical resources to provide transferable supervision.
- Evidence anchors:
  - [abstract] "improvements are also substantial for the two other test sets, which shows the method generalizes to other kinds of sense definitions, of varying granularity."
  - [section 4] "improvements are also substantial for the two other test sets, which shows the method generalizes to other kinds of sense definitions, of varying granularity."
  - [corpus] Weak: corpus contains no contrastive learning with lexicon supervision for WiC or frame induction; evidence is from this paper only.
- Break condition: If downstream tasks use sense inventories that diverge significantly from Wiktionary's granularity, the learned clusters may not align, reducing transfer benefit.

## Foundational Learning

- Concept: Contrastive learning with multiple positive examples.
  - Why needed here: The loss function requires grouping embeddings of the same sense together; multiple positives allow richer similarity structure than binary pairs.
  - Quick check question: What is the formula for the multiple-positive contrastive loss used in this paper?
- Concept: Token-level vs. sentence-level embeddings in PLMs.
  - Why needed here: The method operates on target token embeddings, not whole-sentence vectors; understanding how embeddings are extracted per token is essential.
  - Quick check question: How does the paper extract a single embedding for a target word that may be split into subword tokens?
- Concept: Dimensionality reduction impact on embedding geometry.
  - Why needed here: PCA with whitening is applied to embeddings before similarity computation; knowing how this affects cosine similarity is key to interpreting results.
  - Quick check question: Why does PCA with whitening help reduce the dominance of a few dimensions in cosine similarity?

## Architecture Onboarding

- Component map:
  Wiktionary extraction pipeline -> verb lemma set with examples -> sense grouping
  BERT base model -> token embedding extraction (last layer) -> optional PCA
  Contrastive loss computation -> optimizer -> fine-tuned BERT
  Downstream evaluators (WiC, frame induction) -> similarity thresholds or clustering algorithms
- Critical path:
  1. Load and parse Wiktionary examples per lemma.
  2. Group examples by sense ID.
  3. For each lemma, create a batch of up to 64 examples.
  4. Encode batch with BERT; extract target token embeddings.
  5. Compute similarity matrix; evaluate contrastive loss.
  6. Backpropagate and update BERT weights.
  7. After fine-tuning, apply PCA (if enabled) and evaluate on downstream tasks.
- Design tradeoffs:
  - Using only verbs limits training data but reduces noise; including all POS might increase coverage but hurt sense consistency.
  - Batch size capped at 64 per lemma to control memory but may underutilize large batches.
  - PCA reduces dimensions (100 comps) which speeds inference but may lose subtle sense distinctions.
- Failure signatures:
  - WiC accuracy plateaus or drops after fine-tuning -> lexicon supervision misaligned with WiC sense granularity.
  - Frame induction purity improves but B-Cubed F1 drops -> over-clustering at first step, recoverable in second step.
  - PCA with whitening degrades results -> loss of discriminative dimensions or mismatched sense geometry.
- First 3 experiments:
  1. Baseline: evaluate BERT base on WiC dev set without fine-tuning or PCA.
  2. Ablation: fine-tune BERT with contrastive loss but without PCA; compare WiC dev performance.
  3. Full: fine-tune BERT with contrastive loss + PCA (100 comps, whitening); evaluate WiC dev and frame induction dev.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning other pre-trained language models (e.g., BERT-large, RoBERTa, or T5) yield similar or better improvements on WiC and frame induction tasks compared to bert-base-uncased?
- Basis in paper: [explicit] The paper states that experiments were conducted only using bert-base-uncased, and suggests that fine-tuning other PLMs should be studied to prove the generalisability of the method.
- Why unresolved: The paper only reports results for bert-base-uncased, leaving open the question of whether the proposed method is effective across different PLMs.
- What evidence would resolve it: Experiments showing WiC and frame induction performance on bert-large, RoBERTa, T5, and other PLMs using the proposed fine-tuning approach.

### Open Question 2
- Question: Does incorporating examples from other parts of speech (e.g., nouns, adjectives, adverbs) in addition to verbs improve the performance of the fine-tuned embeddings on lexical semantic tasks?
- Basis in paper: [explicit] The paper mentions that the training dataset only includes verbal lemmas, and suggests verifying whether using lemmas of all parts of speech improves or worsens the fine-tuning results.
- Why unresolved: The paper only uses verbs for training, so the impact of including other parts of speech is unknown.
- What evidence would resolve it: Experiments comparing WiC and frame induction performance using fine-tuned embeddings trained on examples from all parts of speech versus only verbs.

### Open Question 3
- Question: Is there a significant difference in performance between using contrastive learning on Wiktionary examples versus using them for a different pre-training objective (e.g., masked language modeling or next sentence prediction)?
- Basis in paper: [inferred] The paper introduces a novel approach using contrastive learning on Wiktionary examples, but does not compare it to other potential uses of the lexicon data.
- Why unresolved: The paper only explores one method of leveraging Wiktionary examples, leaving open the question of whether contrastive learning is the most effective approach.
- What evidence would resolve it: Experiments comparing the proposed contrastive learning approach to fine-tuning methods using Wiktionary examples for other pre-training objectives on WiC and frame induction tasks.

### Open Question 4
- Question: How does the performance of the proposed method scale with the size of the Wiktionary dataset used for fine-tuning?
- Basis in paper: [inferred] The paper uses a subset of Wiktionary examples for training, but does not explore how performance changes with more or less training data.
- Why unresolved: The paper does not report experiments with varying amounts of Wiktionary data, so the relationship between dataset size and performance is unknown.
- What evidence would resolve it: Experiments showing WiC and frame induction performance as a function of the number of Wiktionary examples used for fine-tuning.

### Open Question 5
- Question: Can the proposed method be extended to create positive examples using Wiktionary sentences for distinct lemmas, and would this lead to further improvements on lexical semantic tasks?
- Basis in paper: [explicit] The paper mentions that a promising continuation of the work is to create positive examples using Wiktionary example sentences for distinct lemmas.
- Why unresolved: The paper does not explore this extension, so its potential benefits are unknown.
- What evidence would resolve it: Experiments using the proposed method with positive examples created from Wiktionary sentences of distinct lemmas, and comparing the results to the original approach on WiC and frame induction tasks.

## Limitations
- Reliance on Wiktionary as sole source of sense supervision introduces coverage and granularity issues
- Generalization claims across different sense inventories are based on relatively small test sets
- PCA dimensionality reduction lacks strong theoretical justification and may be dataset-specific

## Confidence
- **High confidence**: The core mechanism of using contrastive learning on lexicon examples to pull together same-sense embeddings is well-supported by experimental results on WiC and frame induction tasks.
- **Medium confidence**: The generalization claims to FrameNet and other sense inventories are supported but based on limited test sets. The PCA smoothing hypothesis lacks external validation.
- **Low confidence**: The assumption that excluding single-sense verbs with single examples improves quality rather than reducing coverage is not empirically validated.

## Next Checks
1. **Sense granularity alignment test**: Create a controlled experiment where WiC-like pairs are generated from Wiktionary examples with varying sense granularity (fine-grained vs coarse-grained) and measure how the fine-tuned embeddings' performance varies with sense inventory granularity to quantify the transfer limitation.

2. **Ablation on PCA components**: Systematically vary the number of PCA components (10, 50, 100, 200, 300) and test whether the optimal configuration is consistent across different downstream tasks, or if task-specific tuning is required.

3. **Cross-lingual transfer validation**: Apply the same fine-tuning procedure to non-English Wiktionaries (e.g., French, German) with similar scale and structure, and evaluate on language-specific WiC-like datasets to test the cross-lingual generalization claim.