---
ver: rpa2
title: Impact of Noisy Supervision in Foundation Model Learning
arxiv_id: '2403.06869'
source_url: https://arxiv.org/abs/2403.06869
tags:
- noise
- pre-training
- learning
- tasks
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of label noise in large-scale
  pre-training datasets on downstream task performance, a critical issue as foundation
  models become increasingly prevalent. Through extensive experiments on fully-supervised
  and contrastive pre-training with synthetic noisy ImageNet-1K, YFCC15M, and CC12M
  datasets, the authors demonstrate that while slight noise (up to 5-10%) can benefit
  in-domain performance, it consistently deteriorates out-of-domain generalization.
---

# Impact of Noisy Supervision in Foundation Model Learning

## Quick Facts
- arXiv ID: 2403.06869
- Source URL: https://arxiv.org/abs/2403.06869
- Authors: Hao Chen; Zihan Wang; Ran Tao; Hongxin Wei; Xing Xie; Masashi Sugiyama; Bhiksha Raj; Jindong Wang
- Reference count: 40
- Key outcome: This paper investigates the impact of label noise in large-scale pre-training datasets on downstream task performance, demonstrating that while slight noise (up to 5-10%) can benefit in-domain performance, it consistently deteriorates out-of-domain generalization. The authors propose NMTune, a novel regularization method that reshapes the pre-trained feature space to improve robustness, validated across popular vision and language models.

## Executive Summary
This paper addresses the critical issue of label noise in large-scale pre-training datasets for foundation models. Through extensive experiments on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, the authors demonstrate that while slight noise (up to 5-10%) can benefit in-domain performance, it consistently deteriorates out-of-domain generalization. The core insight is that pre-training noise reshapes the feature space's singular value spectrum, reducing dominant singular values and increasing spanning dimensions. To mitigate these effects, the authors propose NMTune, a novel tuning method that applies regularization terms to reshape the pre-trained feature space, applicable in both parameter-efficient (e.g., LoRA) and black-box tuning paradigms. The method shows significant improvements in generalization performance on both in-domain and out-of-domain tasks across popular vision and language models.

## Method Summary
The authors investigate pre-training noise effects through synthetic noise injection into standard datasets (ImageNet-1K, YFCC15M, CC12M) at various ratios (0-30%). They pre-train models on these noisy datasets and evaluate downstream performance using linear probing, LoRA, and full fine-tuning. To mitigate noise effects, they propose NMTune, which applies three regularization terms: consistency regularization (LMSE) to maintain pre-trained knowledge, covariance regularization (LCOV) to improve feature discriminability, and dominant singular value regularization (LSVD) to improve transferability. These regularizations reshape the feature space to counteract noise-induced distortions, making NMTune applicable for both black-box and parameter-efficient tuning paradigms.

## Key Results
- Pre-training noise up to 5-10% can benefit in-domain performance while consistently deteriorating out-of-domain generalization
- NMTune significantly improves generalization performance on both in-domain and out-of-domain tasks across vision and language models
- Feature space analysis reveals that pre-training noise reduces dominant singular values and increases spanning dimensions, explaining the ID/OOD performance trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training noise reshapes the feature space's singular value spectrum, reducing dominant singular values and increasing spanning dimensions, which improves ID performance but harms OOD performance.
- Mechanism: Noise in pre-training data causes the feature extractor to allocate more capacity to fitting noise structure, increasing the feature space's spanning dimension (higher SVE) and decreasing the dominant singular values (lower LSVR). This makes the model better at fitting the training distribution (ID) but less transferable to new distributions (OOD).
- Core assumption: The singular value spectrum of the feature space is a reliable proxy for measuring model generalization and transferability.
- Evidence anchors:
  - [abstract] "Through extensive experiments... we demonstrate that... noise consistently deteriorates out-of-domain generalization... examining the feature space's singular value spectrum, revealing that pre-training noise shapes the feature space differently, reducing dominant singular values and increasing spanning dimensions."
  - [section] "Definition 5.1 (Singular Value Entropy)... measures the flatness of the singular value distribution... Larger SVE values indicate that the feature space captures more structure in the data and thus spans more dimensions... Definition 5.2 (Largest Singular Value Ratio)... measures the variations in data captured by the singular vector corresponding to the largest singular value... LSVR measures the variations in data captured by the singular vector corresponding to the largest singular value σ1, which is related to the transferability of a model [62]."
  - [corpus] Corpus provides limited direct evidence for this mechanism, though related papers on noise resilience and graph-text alignment suggest similar feature-space reshaping phenomena.
- Break condition: If the singular value spectrum is not correlated with generalization performance, or if noise does not consistently reshape the feature space in the described manner.

### Mechanism 2
- Claim: NMTune uses regularization terms to reshape the pre-trained feature space, mitigating the negative effects of noise on downstream tasks.
- Mechanism: NMTune introduces three regularization terms: consistency regularization (LMSE) to maintain pre-trained knowledge, covariance regularization (LCOV) to improve feature discriminability, and dominant singular value regularization (LSVD) to improve transferability. These terms affine the feature space to counteract the noise-induced distortions.
- Core assumption: The proposed regularization terms effectively reshape the feature space to improve generalization without overfitting to noise.
- Evidence anchors:
  - [abstract] "To mitigate these effects, they propose a novel tuning method called NMTune, which applies regularization terms to reshape the pre-trained feature space... The method is validated across popular vision and language models, including APIs, showing significant improvements in generalization performance on both in-domain and out-of-domain tasks."
  - [section] "We propose three straightforward regularization terms on Z to encourage the pre-trained knowledge to be maintained and directly improve SVE and LSVR of the new feature space... The total objective on a downstream task thus becomes: L = LCE + λLNMTune = LCE + λ (LMSE + LCOV + LSVD)"
  - [corpus] Limited direct evidence in corpus, though related papers on noise-resilient learning suggest regularization can improve robustness.
- Break condition: If the regularization terms do not effectively reshape the feature space, or if they introduce other negative effects (e.g., overfitting to noise).

### Mechanism 3
- Claim: NMTune is effective for both black-box and parameter-efficient tuning paradigms, making it suitable for large foundation models with limited access.
- Mechanism: NMTune can be applied in two ways: (1) black-box tuning with an MLP hω transforming pre-trained features, and (2) parameter-efficient tuning with LoRA modules already capable of reshaping features. This flexibility allows NMTune to be used on models with varying levels of access.
- Core assumption: The proposed regularization terms can be effectively applied in both black-box and parameter-efficient tuning settings.
- Evidence anchors:
  - [abstract] "NMTune is applicable in both parameter-efficient (e.g., LoRA) and black-box tuning paradigms, making it suitable for large foundation models with limited access."
  - [section] "Per the analysis above, noise in pre-training can shape the feature space differently from pre-training on clean data... For LP, we introduce a multi-layer perceptron (MLP) hω transforming the pre-trained features into new feature space Z. For LoRA, since it inserts learnable models at intermediate layers of the pre-trained model, it is already capable to reshape the pre-trained features, and thus we do not utilize additional MLP on top of the pre-trained models."
  - [corpus] Limited direct evidence, though related papers on parameter-efficient tuning suggest such methods are effective for large models.
- Break condition: If the regularization terms are not effective in one or both tuning paradigms, or if the flexibility introduces other negative effects.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and its application to feature space analysis.
  - Why needed here: SVD is used to decompose the feature space and analyze the singular value spectrum, which is crucial for understanding how pre-training noise affects model generalization.
  - Quick check question: What is the relationship between the singular value spectrum of a feature space and the model's generalization performance?

- Concept: Regularization techniques in machine learning.
  - Why needed here: Regularization terms are used in NMTune to reshape the feature space and mitigate the negative effects of noise. Understanding these techniques is essential for implementing and understanding NMTune.
  - Quick check question: How do the proposed regularization terms (LMSE, LCOV, LSVD) work together to improve the feature space and generalization performance?

- Concept: Parameter-efficient tuning methods, such as LoRA.
  - Why needed here: LoRA is used as one of the tuning paradigms for NMTune, allowing it to be applied to large foundation models with limited access. Understanding LoRA is crucial for implementing and understanding this aspect of NMTune.
  - Quick check question: How does LoRA work, and how does it differ from traditional fine-tuning methods?

## Architecture Onboarding

- Component map: Pre-trained model -> Tuning method (MLP or LoRA) -> Regularization terms (LMSE, LCOV, LSVD) -> Downstream task performance
- Critical path: Load pre-trained model -> Apply chosen tuning method (MLP or LoRA) -> Apply regularization terms during training -> Evaluate on downstream tasks
- Design tradeoffs: Complexity of tuning method (MLP vs. LoRA) vs. effectiveness of regularization terms; strength of regularization (λ) vs. risk of overfitting to noise
- Failure signatures: Poor generalization performance, especially on OOD tasks; overfitting to noise; incorrect hyperparameter settings leading to ineffective regularization
- First 3 experiments:
  1. Implement and test NMTune with MLP on a small dataset to verify the regularization terms work as expected
  2. Implement and test NMTune with LoRA on a small dataset to verify the parameter-efficient tuning works as expected
  3. Compare the performance of NMTune with and without regularization terms on a larger dataset to verify the effectiveness of the regularization

## Open Questions the Paper Calls Out
1. How do the effects of pre-training noise transfer to non-classification tasks like generative modeling or reinforcement learning?
2. How does the beneficial effect of pre-training noise on in-domain tasks interact with different pre-training data scales?
3. What is the optimal noise ratio that maximizes the trade-off between in-domain performance benefits and out-of-domain generalization preservation?

## Limitations
- The synthetic noise injection method may not fully capture the complexity of real-world label noise distributions
- Empirical validation of the singular value spectrum as a proxy for generalization is limited across diverse model architectures and noise patterns
- The effectiveness of NMTune's regularization terms across black-box and parameter-efficient tuning paradigms needs more rigorous ablation studies

## Confidence

### Major Uncertainties
The paper's core claims about noise-induced feature-space reshaping rely heavily on the singular value spectrum as a proxy for generalization. While the theoretical framework is sound, empirical validation of this proxy across diverse model architectures and noise patterns is limited. The effectiveness of NMTune's regularization terms across black-box and parameter-efficient tuning paradigms needs more rigorous ablation studies. The synthetic noise injection method, while controlled, may not fully capture the complexity of real-world label noise distributions.

### Confidence Labels
- **High confidence**: The observation that pre-training noise degrades OOD generalization is well-supported by extensive experiments across multiple datasets and models.
- **Medium confidence**: The proposed mechanism linking singular value spectrum changes to generalization performance is theoretically plausible but needs more empirical validation across diverse noise patterns.
- **Medium confidence**: NMTune's effectiveness in improving generalization is demonstrated, but the relative contribution of each regularization term and their interaction effects require further investigation.

## Next Checks
1. Conduct ablation studies to isolate the individual contributions of LMSE, LCOV, and LSVD regularization terms to NMTune's performance.
2. Validate the singular value spectrum proxy by testing its correlation with generalization across diverse noise patterns beyond synthetic uniform noise.
3. Test NMTune's effectiveness on real-world noisy datasets with naturally occurring label noise rather than synthetic noise injection.