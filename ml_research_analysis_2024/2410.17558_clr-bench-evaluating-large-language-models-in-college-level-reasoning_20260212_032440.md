---
ver: rpa2
title: 'CLR-Bench: Evaluating Large Language Models in College-level Reasoning'
arxiv_id: '2410.17558'
source_url: https://arxiv.org/abs/2410.17558
tags:
- rationale
- question
- llms
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLR-Bench, a comprehensive benchmark for
  evaluating large language models (LLMs) on college-level reasoning tasks. The key
  innovation lies in assessing not just answer correctness but also the quality of
  the accompanying rationale.
---

# CLR-Bench: Evaluating Large Language Models in College-level Reasoning

## Quick Facts
- arXiv ID: 2410.17558
- Source URL: https://arxiv.org/abs/2410.17558
- Reference count: 40
- Large language models achieve only 39.00% on Q→AR metric despite 63.31% on Q→A

## Executive Summary
CLR-Bench introduces a comprehensive benchmark for evaluating large language models on college-level reasoning tasks across 16 disciplines. The key innovation is assessing both answer correctness and rationale quality through two novel metrics: Q→A (answer prediction) and Q→AR (joint answer and rationale evaluation). Experiments on 40 LLMs reveal that models often "guess" correct answers without genuine reasoning, as evidenced by a dramatic performance drop from 63.31% on Q→A to 39.00% on Q→AR. Notably, smaller models sometimes outperform larger ones on Q→AR, challenging assumptions about model size and reasoning ability.

## Method Summary
CLR-Bench evaluates LLMs using a standardized one-shot prompting approach on 1,018 multi-type questions across 16 disciplines. The benchmark employs two metrics: Q→A measures answer correctness through exact matching for discrete answers and semantic similarity for open-ended responses, while Q→AR requires both correct answers and accurate rationales, scored on a {0, 0.5, 1} scale. Expert-generated rationales are created through a hybrid approach using GPT-4o for initial drafts refined by human experts. The evaluation process uses exact matching for discrete answers and semantic similarity for text-based responses.

## Key Results
- LLMs achieve 63.31% accuracy on Q→A but only 39.00% on Q→AR, revealing significant guessing behavior
- Smaller models sometimes outperform larger models on Q→AR, challenging size-reliability correlation
- Performance varies significantly across question types, with open-ended questions showing the largest accuracy gaps
- GPT-4-turbo ranks highest but still shows substantial room for improvement in reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Q→AR metric reduces guessing by requiring both correct answer and accurate rationale
- Mechanism: Introduces strict evaluation criteria where models are penalized for correct answers with wrong rationales (score drops from 1.0 to 0.0)
- Core assumption: Models cannot reliably produce accurate rationales without genuine understanding
- Evidence anchors:
  - [abstract] "It shows a dramatic decrease in accuracy from 63.31% Q→A to 39.00% Q→AR"
  - [section] "A detailed analysis of Q→AR scores reveals that despite their relatively high accuracy in answering questions (Q→A), many LLMs struggle significantly to provide coherent and accurate rationales"
- Break condition: If models learn to generate plausible-sounding rationales without understanding, Q→A and Q→AR gap would narrow

### Mechanism 2
- Claim: One-shot prompting standardizes evaluation across model sizes
- Mechanism: Forces all models to use single example per question type, preventing larger models from leveraging extensive context
- Core assumption: Multiple examples create unfair advantage for larger models with more context processing capacity
- Evidence anchors:
  - [section] "We introduce a standardized one-shot setting in CLR-Bench to ensure uniformity across models, particularly for smaller LLMs"
  - [section] "By using one-shot examples, we also aim to highlight the fairness in CLR-Bench of evaluating the reasoning capabilities without potential over-reliance on multiple context examples"
- Break condition: If one-shot examples are insufficient for models to understand expected output format

### Mechanism 3
- Claim: Expert-guided rationale generation with GPT-4o improves efficiency while maintaining quality
- Mechanism: Hybrid approach where GPT-4o generates initial drafts that experts refine and verify
- Core assumption: Expert verification is necessary to ensure rationale quality and accuracy
- Evidence anchors:
  - [section] "This collaborative approach leverages the speed and coverage of GPT-4o while maintaining the nuanced accuracy that expert verification ensures"
  - [section] "Experts then review the output, making refinements or modifications as needed to meet high-quality standards"
- Break condition: If expert verification becomes bottleneck preventing dataset scaling

## Foundational Learning

- Concept: Multi-type question categorization (MC, MS, TF, OE, FB)
  - Why needed here: Different question types test different aspects of reasoning ability
  - Quick check question: What distinguishes MS questions from MC questions in terms of rationale requirements?

- Concept: Hierarchical topic graph construction
  - Why needed here: Ensures systematic coverage of subject matter across three levels of specificity
  - Quick check question: How many level-1 topics does the HTG contain according to the paper?

- Concept: Semantic similarity evaluation
  - Why needed here: Provides automated initial filtering for rationale evaluation before expert review
  - Quick check question: What threshold is used for semantic similarity in evaluating OE answers?

## Architecture Onboarding

- Component map: Dataset construction → One-shot prompting → Q→A/Q→R evaluation → Q→AR scoring → Leaderboard generation
- Critical path: Expert-curated topics → Question collection → GPT-4o rationale generation → Expert verification → Model evaluation → Results analysis
- Design tradeoffs: Expert involvement vs. automation efficiency; comprehensive evaluation vs. computational cost
- Failure signatures: Low Q→AR despite high Q→A indicates guessing behavior; poor performance on non-MC questions suggests reasoning limitations
- First 3 experiments:
  1. Test one-shot prompting effectiveness by comparing with few-shot results on sample questions
  2. Validate Q→AR scoring logic by manually evaluating sample predictions against rubric
  3. Benchmark small vs. large model performance to confirm size doesn't guarantee reasoning ability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Q→AR metric perform across different model families when evaluating open-ended questions compared to other question types?
- Basis in paper: [explicit] The paper mentions that open-ended questions require deeper reasoning and articulation, and performance drops significantly on these compared to multi-choice questions.
- Why unresolved: The paper provides a leaderboard comparing models but doesn't specifically analyze performance differences between question types within each model family.
- What evidence would resolve it: A detailed breakdown of Q→AR scores for each question type (MC, MS, TF, FB, OE) within each model family would show which types are most challenging and whether certain families perform better on specific question types.

### Open Question 2
- Question: What specific aspects of rationales cause the largest performance gaps between Q→A and Q→AR across different disciplines?
- Basis in paper: [explicit] The paper observes that models often "guess" correct answers without understanding rationales, with significant drops from Q→A to Q→AR scores.
- Why unresolved: The paper doesn't identify which components of rationales (coverage of knowledge points, reasoning steps, etc.) contribute most to the performance gap.
- What evidence would resolve it: A detailed error analysis categorizing rationale failures (e.g., missing key concepts, incorrect reasoning, incomplete explanations) for each discipline would reveal which aspects most impact Q→AR performance.

### Open Question 3
- Question: How does the hierarchical topic graph structure impact the distribution and difficulty of questions across disciplines?
- Basis in paper: [inferred] The paper describes constructing a hierarchical topic graph with 16 level-1, 40 level-2, and 26 level-3 topics to guide question collection, but doesn't analyze how this structure affects question difficulty.
- Why unresolved: The paper doesn't provide analysis on whether questions from different levels of the hierarchy vary in difficulty or whether certain topic areas are overrepresented.
- What evidence would resolve it: Statistical analysis of question difficulty and performance across different levels of the topic hierarchy would show whether the structure effectively balances coverage and whether certain topic areas need adjustment.

## Limitations
- Reliance on expert-verified rationales constrains dataset scalability and may introduce evaluation biases
- Small dataset size (1,018 questions) may not fully represent college-level reasoning breadth across all disciplines
- Semantic similarity threshold for open-ended answer evaluation is not explicitly defined

## Confidence
- High confidence: Q→A scores (63.31%) significantly exceed Q→AR scores (39.00%), indicating genuine reasoning limitations
- Medium confidence: Smaller models can outperform larger ones on Q→AR, requiring further validation across model families
- Medium confidence: One-shot prompting approach's effectiveness, lacking comparative analysis with few-shot settings

## Next Checks
1. Conduct controlled experiment comparing one-shot versus few-shot prompting across same questions to quantify impact on Q→A and Q→AR scores
2. Perform inter-annotator agreement analysis on subset of rationale evaluations to establish reliability of expert verification process
3. Expand dataset by 2-3x and re-evaluate model performance to assess whether observed patterns hold with increased sample size and coverage