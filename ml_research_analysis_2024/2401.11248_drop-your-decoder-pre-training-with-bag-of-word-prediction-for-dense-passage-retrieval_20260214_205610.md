---
ver: rpa2
title: 'Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage
  Retrieval'
arxiv_id: '2401.11248'
source_url: https://arxiv.org/abs/2401.11248
tags:
- pre-training
- retrieval
- dense
- decoder
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of masked auto-encoder
  (MAE) pre-training for dense passage retrieval, which requires additional decoder
  layers. The authors reveal that MAE pre-training with enhanced decoding improves
  term coverage of input tokens in dense representations.
---

# Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval

## Quick Facts
- arXiv ID: 2401.11248
- Source URL: https://arxiv.org/abs/2401.11248
- Authors: Guangyuan Ma; Xing Wu; Zijia Lin; Songlin Hu
- Reference count: 40
- Key outcome: Achieves state-of-the-art retrieval performance on MS-MARCO, Natural Questions, and TriviaQA with 67% training speed-up compared to standard MAE pre-training

## Executive Summary
This paper addresses the high computational cost of masked auto-encoder (MAE) pre-training for dense passage retrieval, which requires additional decoder layers. The authors reveal that MAE pre-training with enhanced decoding improves term coverage of input tokens in dense representations. They propose a simpler alternative: Bag-of-Word prediction, which directly compresses input lexicon terms into dense representations without extra decoders. This method achieves state-of-the-art retrieval performance on MS-MARCO, Natural Questions, and TriviaQA datasets, with a 67% training speed-up compared to standard MAE pre-training. The approach is highly interpretable and requires zero additional computational complexity.

## Method Summary
The paper proposes a novel pre-training approach for dense passage retrieval that replaces the computationally expensive masked auto-encoder (MAE) framework with a Bag-of-Word (BoW) prediction task. Instead of using separate decoder layers to reconstruct masked tokens, the method directly compresses input lexicon terms into dense representations through a simple yet effective compression mechanism. This approach maintains the benefits of term coverage improvement while eliminating the need for additional decoder parameters and computational overhead. The method leverages contrastive learning objectives to align query and passage representations effectively.

## Key Results
- Achieves state-of-the-art retrieval performance on MS-MARCO, Natural Questions, and TriviaQA datasets
- Provides 67% training speed-up compared to standard MAE pre-training
- Demonstrates high interpretability with zero additional computational complexity
- Maintains effective term coverage of input tokens in dense representations

## Why This Works (Mechanism)
The proposed Bag-of-Word prediction method works by directly compressing input lexicon terms into dense representations without requiring additional decoder layers. This approach leverages the inherent information in bag-of-words statistics to create effective dense representations for passage retrieval. By eliminating the decoder component, the method reduces computational complexity while maintaining or improving the quality of learned representations. The contrastive learning framework ensures that the compressed representations retain semantic information necessary for effective retrieval.

## Foundational Learning
- **Dense Passage Retrieval**: Understanding how dense representations enable efficient semantic search; needed to grasp why pre-training matters for retrieval performance; quick check: compare sparse vs dense retrieval on sample data
- **Masked Auto-Encoder Pre-training**: Knowledge of MAE frameworks and their computational requirements; needed to understand the motivation for the proposed alternative; quick check: implement basic MAE on sample text
- **Contrastive Learning**: Understanding how positive and negative examples are used to learn representations; needed to grasp the training objective; quick check: visualize embedding space with contrastive pairs
- **Bag-of-Words Models**: Understanding the statistical properties of word distributions; needed to appreciate how BoW statistics can be leveraged for representation learning; quick check: compute TF-IDF on sample corpus
- **Representation Compression**: Understanding trade-offs between compression and information retention; needed to evaluate the effectiveness of the proposed compression method; quick check: measure reconstruction quality at different compression rates

## Architecture Onboarding
- **Component Map**: Input text -> Lexicon term extraction -> Bag-of-Word compression -> Dense representation -> Contrastive learning objective -> Retrieval model
- **Critical Path**: The key computational path is Input text → Lexicon term extraction → Bag-of-Word compression → Dense representation, which directly feeds into the contrastive learning objective
- **Design Tradeoffs**: The method trades the reconstruction capability of MAE decoders for computational efficiency and interpretability, accepting that exact token reconstruction is not necessary for effective retrieval
- **Failure Signatures**: Potential failures include loss of fine-grained semantic information during compression, inadequate handling of rare terms, and suboptimal alignment between query and passage representations
- **First Experiments**: 
  1. Implement basic BoW compression on sample text and visualize resulting dense representations
  2. Compare retrieval performance of BoW-pretrained vs randomly initialized models on small dataset
  3. Measure computational efficiency gains by profiling training time for BoW vs MAE approaches

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- The generalizability of the approach across different domains beyond the three benchmark datasets remains unclear
- The long-term stability of performance gains needs further validation
- The method's effectiveness on extremely long documents or specialized domains has not been thoroughly evaluated

## Confidence
- Computational efficiency improvements: High
- Retrieval performance: Medium
- Interpretability and complexity claims: Medium

## Next Checks
1. Conduct cross-domain evaluation to verify the method's effectiveness beyond the three benchmark datasets
2. Perform extensive ablation studies to quantify the contribution of each component of the BoW prediction approach
3. Analyze the learned representations to provide quantitative evidence of interpretability and verify the zero additional computational complexity claim through detailed profiling