---
ver: rpa2
title: Mitigating LLM Hallucinations via Conformal Abstention
arxiv_id: '2405.01563'
source_url: https://arxiv.org/abs/2405.01563
tags:
- rcps
- baseline
- bound
- responses
- match
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of reducing hallucinations in
  large language models (LLMs) by developing a principled method for determining when
  an LLM should abstain from answering a query. The core idea is to use self-consistency,
  where the LLM evaluates the similarity of its own sampled responses for a given
  query, combined with conformal prediction techniques to establish a statistically
  rigorous abstention procedure.
---

# Mitigating LLM Hallucinations via Conformal Abstention

## Quick Facts
- **arXiv ID**: 2405.01563
- **Source URL**: https://arxiv.org/abs/2405.01563
- **Reference count**: 20
- **Primary result**: Conformal abstention with self-consistency significantly reduces hallucinations while minimizing abstention rates

## Executive Summary
This paper addresses the critical problem of LLM hallucinations by developing a principled method for determining when an LLM should abstain from answering. The approach combines self-consistency (evaluating similarity between sampled responses) with conformal prediction techniques to create a statistically rigorous abstention procedure. The method aims to bound the hallucination rate while minimizing the number of abstentions, providing theoretical guarantees on error rates. Experiments on Temporal Sequences and TriviaQA datasets using Gemini Pro demonstrate that this approach outperforms baseline methods using log-probability scores, particularly for longer responses.

## Method Summary
The core innovation is using conformal prediction to create an abstention mechanism based on LLM self-consistency. When a query is received, the LLM generates multiple responses and evaluates their pairwise similarity using another LLM. Conformal prediction then determines a threshold for when responses are sufficiently inconsistent to warrant abstention. The method uses match count and expected match count scores, with a calibration procedure to tune the similarity threshold. This approach provides statistical guarantees on the maximum hallucination rate while optimizing the abstention rate. The calibration process itself uses conformal prediction to ensure the match prediction accuracy meets specified confidence levels.

## Key Results
- Conformal abstention with self-consistency significantly outperforms log-probability baselines on both Temporal Sequences and TriviaQA datasets
- The method effectively bounds hallucination rates while maintaining lower abstention rates than baselines
- Performance gains are particularly pronounced for longer responses where log-probability scoring fails
- Calibration methodology provides theoretical guarantees on match prediction accuracy

## Why This Works (Mechanism)
The method exploits the insight that when an LLM is uncertain or hallucinating, its sampled responses will show high variability. By generating multiple responses and measuring their consistency, the system can identify when the model lacks confidence in its answer. Conformal prediction provides the statistical framework to turn this observation into a rigorous abstention decision with guaranteed error bounds. The calibration process ensures the similarity threshold is set appropriately for the specific LLM and dataset, avoiding both excessive abstention and insufficient hallucination detection.

## Foundational Learning

**Conformal prediction**: A statistical technique for constructing prediction sets with guaranteed coverage probabilities. Needed to provide theoretical guarantees on abstention error rates. Quick check: Verify that the chosen conformal method satisfies the required risk condition on the calibration set.

**Self-consistency**: Measuring the similarity between multiple sampled responses from the same LLM. Needed as a proxy for the LLM's confidence in its answer. Quick check: Confirm that response similarity correlates with actual answer correctness on a validation set.

**Risk condition**: A constraint that the expected loss on the calibration set must not exceed a specified threshold. Needed to ensure the abstention method provides valid statistical guarantees. Quick check: Calculate average calibration loss and verify it satisfies the risk bound.

## Architecture Onboarding

**Component map**: Query -> Multiple response generation -> Similarity evaluation -> Conformal threshold determination -> Abstain or answer decision

**Critical path**: The key computational steps are generating multiple responses (sampling), evaluating pairwise similarities (LLM calls), and computing conformal thresholds (statistical calculation). Response generation and similarity evaluation dominate the computational cost.

**Design tradeoffs**: The method trades computational cost (multiple LLM calls) for improved reliability. More samples improve abstention accuracy but increase latency and cost. The choice of similarity scoring function (match count vs expected match count) affects both performance and computational requirements.

**Failure signatures**: If the calibration set is too small, the method may violate risk conditions, leading to higher than expected hallucination rates. If the similarity evaluation prompt is poorly designed, the method may abstain too frequently or too rarely. Log-probability scoring failure on long responses is a known limitation of baseline methods.

**First experiments**:
1. Verify that response self-consistency correlates with answer correctness on a held-out validation set
2. Test calibration performance with varying calibration set sizes to identify minimum effective size
3. Compare abstention rates and hallucination bounds across different sampling temperatures for response generation

## Open Questions the Paper Calls Out

**Open Question 1**: What is the impact of different LLM self-consistency sampling temperatures on the effectiveness of conformal abstention? The paper uses temperature 0.9 but doesn't explore this parameter. Varying temperatures could affect response diversity and thus the reliability of self-consistency measures.

**Open Question 2**: How does the size of the calibration set impact the performance of the conformal abstention method, particularly for smaller datasets? While the paper uses subsamples of varying sizes, it doesn't provide detailed analysis of this relationship, which is crucial for practical deployment.

**Open Question 3**: Can the conformal abstention method be extended to other generative tasks beyond question-answering, such as text summarization or translation? The method is based on general principles but has only been tested on question-answering datasets, leaving its applicability to other tasks uncertain.

## Limitations
- Evaluation limited to two question-answering datasets with a single model (Gemini Pro), constraining generalizability
- Reliance on LLM-based similarity judgments introduces potential circularity despite conformal calibration
- Abstention mechanism may reduce practical utility if abstention rates are too high for real-world deployment
- Computational overhead of multiple LLM calls for each query may limit scalability

## Confidence

**High confidence**: Theoretical guarantees of conformal abstention bounds, comparison with log-probability baselines, calibration methodology

**Medium confidence**: Performance improvements on test datasets, effectiveness across different calibration set sizes, impact of answer length

**Low confidence**: Generalizability to other model families, domain transfer capabilities, real-world deployment scenarios

## Next Checks
1. Evaluate the method on diverse LLM architectures (GPT, Claude, open-source models) to assess cross-model generalizability
2. Test on multi-turn dialogue datasets to determine effectiveness in conversational contexts
3. Conduct human evaluation studies to validate whether abstention decisions align with expert judgment of when answers are unreliable