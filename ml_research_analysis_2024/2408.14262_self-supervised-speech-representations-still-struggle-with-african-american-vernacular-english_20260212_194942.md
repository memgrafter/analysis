---
ver: rpa2
title: Self-supervised Speech Representations Still Struggle with African American
  Vernacular English
arxiv_id: '2408.14262'
source_url: https://arxiv.org/abs/2408.14262
tags:
- speech
- features
- coraal
- american
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Despite success of self-supervised learning in improving ASR for
  low-resource languages, this study finds that SSL speech representations still underperform
  on African American Vernacular English (AAVE) compared to Mainstream American English
  (MAE). Using four SSL models (wav2vec 2.0, HuBERT, WavLM, and XLS-R) in a zero-shot
  ASR setting, researchers found that WER for AAVE utterances remained significantly
  higher than for MAE, and that WER increased with the presence of AAVE phonological
  and morphosyntactic features.
---

# Self-supervised Speech Representations Still Struggle with African American Vernacular English

## Quick Facts
- **arXiv ID:** 2408.14262
- **Source URL:** https://arxiv.org/abs/2408.14262
- **Reference count:** 0
- **Primary result:** SSL speech representations underperform on African American Vernacular English compared to Mainstream American English, even with diverse pretraining data.

## Executive Summary
This study examines whether self-supervised learning (SSL) speech representations can mitigate ASR disparities between African American Vernacular English (AAVE) and Mainstream American English (MAE). Despite the success of SSL models like wav2vec 2.0, HuBERT, WavLM, and XLS-R in improving ASR for low-resource languages, the researchers found these models still perform significantly worse on AAVE utterances. The study used zero-shot ASR evaluation and found that WER for AAVE remained higher than for MAE, with errors increasing for utterances containing more AAVE phonological and morphosyntactic features. Even when SSL models were pretrained on diverse spontaneous speech data, the performance gap persisted.

## Method Summary
The researchers evaluated four SSL speech representations (wav2vec 2.0, HuBERT, WavLM, and XLS-R) in a zero-shot ASR setting using the Fisher corpus. They compared WER for AAVE versus MAE utterances and analyzed error patterns across different AAVE linguistic features. The study measured how WER changed with the number of AAVE phonological and morphosyntactic features present in utterances. They also examined whether pretraining on diverse spontaneous speech data would reduce disparities, testing models with varying degrees of linguistic diversity in their pretraining corpora.

## Key Results
- WER for AAVE utterances remained significantly higher than for MAE across all four SSL models tested
- WER increased proportionally with the number of AAVE phonological and morphosyntactic features present in utterances
- SSL models pretrained on diverse spontaneous speech data still showed substantial AAVE-MAE performance gaps
- Error analysis revealed specific struggles with AAVE features like final consonant deletion and dialect-specific pronunciations

## Why This Works (Mechanism)
SSL models learn general speech representations from large amounts of unlabeled audio data through contrastive learning objectives. However, these models may not adequately capture the acoustic-phonetic patterns and phonological rules that characterize AAVE. The learning mechanism relies on predicting masked portions of speech from surrounding context, which may not effectively model the systematic phonological variations and morphosyntactic structures unique to AAVE. Since the pretraining process optimizes for general speech understanding across many varieties, it may underweight the specific patterns needed for accurate AAVE recognition.

## Foundational Learning
- **Self-supervised learning (SSL) in speech:** Learning representations from unlabeled audio data without human annotations. Why needed: Enables training on massive amounts of speech data without costly transcription. Quick check: Can the model learn meaningful representations from raw audio alone?
- **Phonological variation in dialects:** Systematic differences in sound patterns across language varieties. Why needed: AAVE has distinct phonological rules that affect pronunciation and word recognition. Quick check: Does the model recognize AAVE-specific pronunciation patterns?
- **Morphosyntactic features:** Grammatical structures and word formation patterns. Why needed: AAVE has unique grammatical constructions that affect word boundaries and pronunciation. Quick check: Can the model handle AAVE-specific syntactic structures?
- **Zero-shot ASR evaluation:** Testing ASR models without fine-tuning on target domain data. Why needed: Provides baseline performance assessment but may underestimate potential with adaptation. Quick check: How does performance compare to fine-tuned baselines?
- **Word Error Rate (WER):** Standard metric for ASR evaluation measuring sequence-level errors. Why needed: Quantifies overall recognition accuracy across different speech varieties. Quick check: What is the relative WER difference between dialects?
- **Dialectal variation in spontaneous speech:** Natural speech patterns that differ from standardized forms. Why needed: Real-world ASR must handle diverse speaking styles and pronunciations. Quick check: Does the model generalize across speaking styles?

## Architecture Onboarding

**Component Map:** Raw audio -> Feature extractor (CNN) -> Transformer encoder -> Masked prediction head (wav2vec 2.0 example)

**Critical Path:** Audio input → CNN feature extraction → Transformer contextualization → Masked prediction objective → Speech representation

**Design Tradeoffs:** SSL models balance between learning general speech patterns and capturing dialect-specific features. The masked prediction objective may favor majority dialect patterns over minority varieties. Pretraining on diverse data helps but may not be sufficient without explicit dialect representation.

**Failure Signatures:** Higher WER for utterances with more AAVE features, systematic errors on specific phonological patterns (like final consonant deletion), inability to recognize dialect-specific pronunciations even after diverse pretraining.

**3 First Experiments:**
1. Compare WER distributions for AAVE vs MAE utterances across different feature count thresholds
2. Analyze error patterns for specific AAVE phonological features (final consonant deletion, consonant cluster simplification)
3. Test whether fine-tuning on small amounts of AAVE data reduces the performance gap

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Zero-shot evaluation may underestimate potential improvements from fine-tuning on AAVE data
- Analysis limited to four SSL models and one specific AAVE-MAE corpus (Fisher)
- Qualitative error analysis without systematic quantification of feature-specific impacts
- May not capture full diversity of AAVE dialect sub-varieties

## Confidence

| Claim | Confidence |
|-------|------------|
| WER for AAVE utterances remains significantly higher than for MAE across all four SSL models | High |
| SSL pretraining alone is insufficient to mitigate AAVE-MAE ASR disparities | Medium |
| WER increases with presence of AAVE phonological and morphosyntactic features | Medium |

## Next Checks
1. Evaluate whether fine-tuning SSL models on small amounts of AAVE data can reduce the performance gap
2. Replicate analysis using additional AAVE corpora and SSL models pretrained on more diverse linguistic data
3. Conduct systematic quantitative analysis of which specific AAVE phonological and morphosyntactic features most strongly correlate with ASR errors