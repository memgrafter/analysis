---
ver: rpa2
title: Co-Trained Retriever-Generator Framework for Question Generation in Earnings
  Calls
arxiv_id: '2409.18677'
source_url: https://arxiv.org/abs/2409.18677
tags:
- questions
- question
- retriever
- generation
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a retriever-enhanced question generation framework
  for earnings calls, addressing the challenge of generating diverse, context-specific
  questions for professional settings. The core method combines a retriever to select
  relevant presentation segments with a generator to create questions, using a co-trained
  approach to optimize both components.
---

# Co-Trained Retriever-Generator Framework for Question Generation in Earnings Calls

## Quick Facts
- arXiv ID: 2409.18677
- Source URL: https://arxiv.org/abs/2409.18677
- Reference count: 5
- Primary result: BLEU-4 of 2.389 and ROUGE-2 of 7.255, demonstrating significant improvements in correctness and diversity of generated questions for earnings calls

## Executive Summary
This paper introduces a retriever-enhanced question generation framework specifically designed for earnings calls, addressing the challenge of generating diverse, context-specific questions for professional settings. The core method combines a retriever to select relevant presentation segments with a generator to create questions, using a co-trained approach to optimize both components. The framework demonstrates superior performance compared to baselines, with significant improvements in both correctness and diversity of generated questions, as measured by automatic metrics and human evaluation.

## Method Summary
The framework uses a retriever (Prompt-Based Retriever, ProRetriever) with Guanaco-33B to select relevant presentation segments, and a generator (Alpaca-Lora-7B) to create questions from these segments. The retriever and generator are co-trained using a text-to-text framework, with the retriever optimizing segment selection based on three-way relevance classification ("Highly Related", "Partially Related", "Not Related"), and the generator optimizing question generation through cross-entropy loss. The system processes earnings call transcripts by first identifying relevant segments through the retriever, then generating questions using the selected content, addressing token limitations of transformer models when dealing with lengthy presentations.

## Key Results
- BLEU-4 score of 2.389 and ROUGE-2 score of 7.255, showing significant improvement over baselines
- Human evaluation demonstrates comparable performance to professionals in Logic and Consistency (LC), though slightly lower in Professionalism (PF)
- Semantic Entropy (Sem-Ent) scores indicate improved diversity in generated questions compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The co-trained retriever-generator framework improves question generation by iteratively optimizing both components through a feedback loop where better retrieval leads to better question generation.
- Mechanism: The retriever selects relevant presentation segments, which are used by the generator to create reference questions. The quality of these questions (measured through cross-entropy loss) informs adjustments to the retriever's parameters, creating a co-training loop that improves both components.
- Core assumption: The quality of generated questions is directly correlated with the relevance of the retrieved presentation segments.
- Evidence anchors:
  - [abstract]: "Our methodology involves an exhaustive collection of earnings call transcripts and a novel annotation technique to classify potential questions."
  - [section]: "Improved retrieval boosts generation accuracy, with the consequent loss iteratively optimizing both models, enhancing the Retriever's unsupervised capability."
- Break condition: If the relationship between retrieval quality and generation quality becomes non-monotonic, the co-training loop could destabilize or produce worse results.

### Mechanism 2
- Claim: Using question types as control codes for the generator allows the model to produce more diverse and contextually appropriate questions.
- Mechanism: The retriever classifies each paragraph's relevance to a question using three labels ("Highly Related", "Partially Related", "Not Related"), and this classification guides the generator to produce questions that align with different types of analyst inquiries.
- Core assumption: Financial analysts ask questions that can be meaningfully categorized into distinct types based on their relationship to presentation content.
- Evidence anchors:
  - [abstract]: "With a core aim of generating a spectrum of potential questions that analysts might pose, we derive these directly from earnings call content."
  - [section]: "Our crafted prompt reads: Given a manager's presentation transcript during an earnings call and an analyst's query, discern if the query is deeply anchored, tangentially connected, or aloof from the manager's discourse?"
- Break condition: If the three-way classification system doesn't capture the actual diversity of analyst questions, the control mechanism becomes ineffective.

### Mechanism 3
- Claim: The retrieval-augmented approach addresses the token limitation problem of transformer-based models when dealing with lengthy earnings call presentations.
- Mechanism: Instead of processing entire presentations, the system identifies relevant segments through the retriever, allowing the generator to focus on specific content areas that are most likely to generate meaningful questions.
- Core assumption: Audience questions typically target specific segments of presentations rather than requiring full context.
- Evidence anchors:
  - [abstract]: "Given the extensive content of oral presentations, most transformer-based models encounter token constraints."
  - [section]: "Recognizing that audience questions typically target presentation specifics, we introduce a Co-Trained Retriever-Generator Framework."
- Break condition: If important contextual information is lost by only considering retrieved segments, the generated questions may miss critical connections or nuances.

## Foundational Learning

- Concept: Cross-entropy loss in sequence generation
  - Why needed here: The paper uses cross-entropy loss to measure the difference between generated questions and reference questions, which drives the training of both retriever and generator components.
  - Quick check question: How does cross-entropy loss differ from other loss functions when evaluating generated text quality?

- Concept: Text-to-text framework for relevance scoring
  - Why needed here: The retriever uses a text-to-text framework to compute relevance scores between questions and presentation snippets, which is central to the system's ability to select appropriate content.
  - Quick check question: What are the advantages of using a text-to-text framework over traditional embedding-based retrieval methods?

- Concept: Semantic entropy for diversity measurement
  - Why needed here: The paper evaluates diversity using semantic entropy, measuring how uniformly generated questions cover different semantic topics rather than just checking for n-gram overlap.
  - Quick check question: How does semantic entropy provide a more meaningful measure of question diversity compared to traditional n-gram diversity metrics?

## Architecture Onboarding

- Component map: Earnings call transcript -> Retriever (ProRetriever) -> Selected segments -> Generator (Alpaca-Lora-7B) -> Questions

- Critical path: Presentation transcript → Retriever → Selected segments → Generator → Questions
  - The retriever must select the top-k segments before the generator can produce questions

- Design tradeoffs:
  - Token limits (1400 for generator, 512 for retriever) vs. context completeness
  - Number of retrieved segments (k=6) vs. computational efficiency
  - Model size (7B generator, 33B retriever) vs. deployment feasibility
  - Fine-tuning fraction (0.24% generator, 1.24% retriever) vs. adaptation quality

- Failure signatures:
  - Low relevance scores from retriever indicate poor segment selection
  - BLEU/ROUGE scores significantly below baselines suggest generation quality issues
  - Semantic entropy near zero indicates lack of diversity in generated questions
  - High perplexity scores suggest unnatural language generation

- First 3 experiments:
  1. Baseline comparison: Run the same presentation through random retriever, BM25 retriever, and ProRetriever to compare question quality metrics
  2. Ablation study: Disable the retriever and feed entire presentation to generator to measure the impact of retrieval augmentation
  3. Diversity test: Generate questions from the same presentation using different retrievers and measure semantic entropy to verify diversity improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed framework perform when applied to other professional settings beyond earnings calls, such as academic conferences or political debates?
- Basis in paper: [inferred] The paper mentions that the framework is currently limited to earnings call transcripts and suggests extending the approach to other professional settings as future work.
- Why unresolved: The current study focuses exclusively on earnings calls, and the model's performance in other professional domains remains untested.
- What evidence would resolve it: Conducting experiments to apply the framework to transcripts from academic conferences, political debates, or other professional settings, and evaluating the quality of generated questions using the same metrics (BLEU, ROUGE, BERTScore, etc.).

### Open Question 2
- Question: What is the impact of varying the top-k parameter on the quality and diversity of generated questions?
- Basis in paper: [explicit] The paper mentions that top-k is set to 6 for all experiments but does not explore the effects of different top-k values.
- Why unresolved: The choice of top-k is crucial for determining how many relevant passages are selected by the retriever, which in turn affects the generator's output. The optimal value of top-k is not investigated.
- What evidence would resolve it: Performing experiments with different top-k values (e.g., 3, 6, 9, 12) and analyzing how these changes influence the correctness, diversity, and overall quality of the generated questions.

### Open Question 3
- Question: How does the model handle scenarios where training data is sparse or unavailable?
- Basis in paper: [inferred] The paper notes that the framework relies heavily on the availability of extensive training data and suggests that the quality of generated questions could degrade in settings with sparse data.
- Why unresolved: The current model's robustness and adaptability to low-resource settings are not explored.
- What evidence would resolve it: Testing the model on datasets with varying amounts of training data, including low-resource scenarios, and evaluating its performance using the established metrics to determine its effectiveness in such conditions.

### Open Question 4
- Question: Can the model generate questions that match the critical level of professionalism exhibited by human analysts?
- Basis in paper: [explicit] Human evaluation results indicate that while the model performs comparably to professionals in terms of logic and consistency, it falls short in professionalism.
- Why unresolved: The model's ability to generate questions that meet the high standards of professional analysts has not been fully achieved.
- What evidence would resolve it: Conducting further human evaluations with a larger and more diverse set of questions, comparing the model's output to questions from seasoned analysts, and identifying specific areas where the model can be improved to enhance professionalism.

## Limitations

- Annotation Strategy Ambiguity: The paper describes a novel annotation technique but provides insufficient detail on how analysts' questions were annotated for training the retriever, creating uncertainty about the consistency of the three-way classification system.
- Generalization Beyond Earnings Calls: The framework was developed and tested specifically on earnings call transcripts from S&P 500 companies, and its effectiveness for other professional domains remains untested.
- Resource Intensity: The use of large models (Guanaco-33B for retrieval, Alpaca-Lora-7B for generation) combined with LoRA adapters and co-training procedures requires substantial computational resources, raising questions about practical deployment feasibility given only modest performance improvements.

## Confidence

- High Confidence Claims:
  - The framework architecture (retriever + generator with co-training) is technically sound and implemented as described
  - The methodology for collecting and preprocessing earnings call transcripts is clearly specified
  - The evaluation metrics (BLEU, ROUGE, BERTScore, Semantic Entropy) are appropriately applied

- Medium Confidence Claims:
  - The reported performance improvements over baselines are accurate based on the described experiments
  - The three-way classification system for relevance scoring captures meaningful distinctions in analyst questions
  - The diversity improvements measured by semantic entropy are genuine and not artifacts of the evaluation method

- Low Confidence Claims:
  - The practical utility of the generated questions for real-world analysts (beyond automatic metrics)
  - The robustness of the framework to variations in earnings call structure and presentation styles
  - The effectiveness of the co-training approach compared to alternative optimization strategies

## Next Checks

1. **Annotation Consistency Audit**: Conduct inter-rater reliability testing on a subset of the dataset to verify that the three-way classification system produces consistent annotations across different annotators, validating whether the classification labels are objectively meaningful or subjective artifacts that could compromise the retriever's training.

2. **Cross-Domain Transfer Test**: Apply the trained framework to earnings calls from non-S&P 500 companies (such as small-cap stocks, international companies, or different industries) to assess whether the model maintains performance without additional fine-tuning, revealing whether the approach is genuinely capturing universal patterns or overfitting to the specific training domain.

3. **Baseline Ablation Analysis**: Systematically disable the retriever component and feed entire presentations directly to the generator, comparing performance against the full retriever-generator system to quantify the actual contribution of the retrieval-augmented approach versus simply increasing the generator's context window, addressing the core question of whether the complexity is justified.