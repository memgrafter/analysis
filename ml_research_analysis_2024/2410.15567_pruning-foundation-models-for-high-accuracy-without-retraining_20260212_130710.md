---
ver: rpa2
title: Pruning Foundation Models for High Accuracy without Retraining
arxiv_id: '2410.15567'
source_url: https://arxiv.org/abs/2410.15567
tags:
- pruning
- weights
- sparsegpt
- solution
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying large language
  models (LLMs) by proposing a post-training pruning method that avoids the need for
  expensive fine-tuning or retraining. The core method idea is to formulate and solve
  a Multiple Removal Problem (MRP) for layer-wise LLM compression, which simultaneously
  prunes multiple weights while maintaining accuracy.
---

# Pruning Foundation Models for High Accuracy without Retraining

## Quick Facts
- arXiv ID: 2410.15567
- Source URL: https://arxiv.org/abs/2410.15567
- Authors: Pu Zhao; Fei Sun; Xuan Shen; Pinrui Yu; Zhenglun Kong; Yanzhi Wang; Xue Lin
- Reference count: 13
- One-line primary result: Proposes MRP formulation achieving 4.278 perplexity on wikitext2 vs 5.698 from SparseGPT for LLaMA2-70B under 2:4 sparsity

## Executive Summary
This paper addresses the challenge of deploying large language models (LLMs) by proposing a post-training pruning method that avoids expensive fine-tuning or retraining. The core innovation is a Multiple Removal Problem (MRP) formulation that simultaneously prunes multiple weights while maintaining accuracy, unlike existing methods that rely on sequential weight freezing. Experiments across transformer and Mamba architectures demonstrate superior perplexity and zero-shot accuracy compared to state-of-the-art baselines, making LLM deployment more feasible on resource-constrained hardware.

## Method Summary
The proposed method formulates LLM pruning as a Multiple Removal Problem (MRP) that simultaneously prunes multiple weights while maintaining accuracy. Unlike Single Removal Problem (SRP)-based approaches that sequentially freeze weights, MRP updates all unpruned weights and considers interactions between pruned weights. The method uses layer-wise pruning with block processing to reduce memory requirements, making it feasible for large models on single GPUs. Two solution strategies are proposed: Solution S (simplified) and Solution M (optimal), with the latter incorporating full interactions between pruned weights through matrix inversion operations.

## Key Results
- Achieves 4.278 perplexity on wikitext2 versus 5.698 from SparseGPT for LLaMA2-70B under 2:4 sparsity
- Maintains zero-shot accuracy on multiple benchmarks (LAMBADA, Hellaswag, PIQA, ARC-Easy, ARC-Challenge, WinoGrade)
- Outperforms state-of-the-art methods across transformer and Mamba architectures
- Enables single-GPU computation through layer-wise block pruning strategy

## Why This Works (Mechanism)

### Mechanism 1: Simultaneous Weight Pruning
The MRP formulation allows simultaneous pruning of multiple weights, addressing the limitation of SRP-based methods that sequentially freeze weights. By minimizing the ℓ2 norm of output differences while simultaneously pruning multiple weights, the solution updates all unpruned weights without freezing, maintaining full interactions between pruned weights.

### Mechanism 2: Interaction-Aware Compensation
The optimal solution for MRP incorporates full interactions between pruned weights, unlike SRP which ignores these interactions. The solution includes terms that capture interactions between multiple pruned weights, leading to better compensation of pruning loss through optimal weight updates.

### Mechanism 3: Memory-Efficient Layer-Wise Processing
The layer-wise pruning strategy with block pruning reduces memory cost sufficiently to enable single-GPU computation while maintaining accuracy. By sequentially loading and pruning one block at a time, the method reduces memory requirements compared to whole-model pruning.

## Foundational Learning

- Concept: Hessian matrix computation and properties (positive semi-definite, invertibility)
  - Why needed here: The optimal solution requires computing (2xxT)^-1 and its modifications, which are central to the MRP formulation
  - Quick check question: Given a quadratic loss function L(w) = ||wx||^2, what is the Hessian matrix with respect to w?

- Concept: Lagrange multipliers and constrained optimization
  - Why needed here: The MRP formulation involves minimizing loss subject to sparsity constraints, requiring constrained optimization techniques
  - Quick check question: How do you set up a Lagrangian for minimizing a function subject to equality constraints?

- Concept: Matrix inversion and numerical stability
  - Why needed here: The method requires computing matrix inverses with dampening to ensure numerical stability
  - Quick check question: What is the effect of adding a small multiple of the identity matrix (γI) to a matrix before inversion?

## Architecture Onboarding

- Component map: Input data -> Calibration -> Layer-wise Block Pruning -> Weight Compensation -> Output Pruned Model
- Critical path: The matrix inversion step in weight compensation is the computational bottleneck, particularly for Solution M with multiple pruned weights
- Design tradeoffs: Solution M provides better accuracy but higher computational complexity versus Solution S which is faster but less accurate
- Failure signatures: Poor perplexity performance indicates issues with calibration data or mask selection; numerical instability suggests problems with matrix inversion dampening
- First experiments: 1) Verify matrix inversion stability with varying dampening ratios, 2) Compare Solution S vs Solution M on small model, 3) Test layer-wise processing memory usage on target GPU

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

- Numerical stability concerns with matrix inversions for very large matrices or ill-conditioned Hessian matrices
- Computational complexity that may become prohibitive for extremely sparse regimes or very large models
- Dependency on calibration data quality and representativeness for optimal pruning decisions

## Confidence

- High Confidence: Superiority of MRP over SRP-based methods is well-supported by experimental results
- Medium Confidence: Theoretical derivation is mathematically sound but practical implementation details are not fully specified
- Medium Confidence: Claim about single-GPU feasibility is plausible but specific memory usage numbers are not provided

## Next Checks

1. Implement matrix inversion with varying dampening ratios (γ) on synthetic data to determine stability threshold
2. Benchmark Solution M runtime for increasing numbers of pruned weights (k) to verify O(nk^3) scaling
3. Run pruning pipeline with different calibration datasets to quantify sensitivity to calibration data choice