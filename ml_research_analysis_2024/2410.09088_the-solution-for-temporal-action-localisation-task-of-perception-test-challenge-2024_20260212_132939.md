---
ver: rpa2
title: The Solution for Temporal Action Localisation Task of Perception Test Challenge
  2024
arxiv_id: '2410.09088'
source_url: https://arxiv.org/abs/2410.09088
tags:
- video
- action
- audio
- dataset
- localisation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the Temporal Action Localisation (TAL) task,
  aiming to accurately identify and classify actions within specific time intervals
  in untrimmed video sequences. The method leverages state-of-the-art video feature
  extractors (UMT and VideoMAEv2) and audio feature extractors (BEATs and CAV-MAE),
  training both multimodal and unimodal models, and fusing their predictions using
  Weighted Box Fusion (WBF).
---

# The Solution for Temporal Action Localisation Task of Perception Test Challenge 2024

## Quick Facts
- **arXiv ID:** 2410.09088
- **Source URL:** https://arxiv.org/abs/2410.09088
- **Reference count:** 13
- **Primary result:** Achieved mAP of 0.5498, securing first place in Perception Test Challenge 2024

## Executive Summary
This work presents a solution for the Temporal Action Localisation (TAL) task in the Perception Test Challenge 2024. The method combines multimodal and unimodal models using advanced feature extractors including UMT, VideoMAEv2, BEATs, and CAV-MAE. By augmenting training data with overlapping labels from Something-SomethingV2 and employing Weighted Box Fusion for prediction combination, the approach achieves state-of-the-art performance. The system successfully identifies and classifies actions within specific time intervals in untrimmed video sequences.

## Method Summary
The solution leverages state-of-the-art video feature extractors (UMT and VideoMAEv2) and audio feature extractors (BEATs and CAV-MAE) to train both multimodal and unimodal models. These models are then combined using Weighted Box Fusion (WBF) to produce final predictions. To improve generalisation, the training dataset is augmented with overlapping labels from the Something-SomethingV2 dataset. This comprehensive approach addresses the challenge of accurately identifying and classifying actions within specific time intervals in untrimmed video sequences.

## Key Results
- Achieved mean Average Precision (mAP) score of 0.5498
- Secured first place in the Perception Test Challenge 2024
- Demonstrated effective combination of multimodal and unimodal predictions

## Why This Works (Mechanism)
The method works by extracting rich multimodal features from videos using multiple state-of-the-art extractors, then combining these through both model-level fusion (training separate models) and prediction-level fusion (WBF). The augmentation with overlapping labels from a related dataset helps the model generalise better to the target domain by exposing it to similar action patterns.

## Foundational Learning

**Temporal Action Localisation (TAL)**
*Why needed:* Core task of identifying when actions occur in untrimmed videos
*Quick check:* Can the system output start/end times and action categories for events in video

**Weighted Box Fusion (WBF)**
*Why needed:* Combines predictions from multiple models while handling overlapping detections
*Quick check:* Are predicted boxes from different models properly weighted and merged

**Multimodal Feature Extraction**
*Why needed:* Combines visual and audio information for better action understanding
*Quick check:* Are both video and audio features effectively captured and utilised

## Architecture Onboarding

**Component Map**
Video/Audio Input -> Feature Extractors (UMT, VideoMAEv2, BEATs, CAV-MAE) -> Model Training (Multimodal/Unimodal) -> Prediction Fusion (WBF) -> Final Output

**Critical Path**
Feature extraction → Model training → WBF fusion → mAP evaluation

**Design Tradeoffs**
The use of multiple feature extractors increases computational cost but improves accuracy through diverse representations. Model augmentation with external dataset helps generalisation but may introduce label noise.

**Failure Signatures**
Poor performance on action classes not well-represented in training data or augmentation set. Degradation in temporal precision when actions have subtle visual/audio cues.

**First Experiments**
1. Test individual feature extractor performance without fusion
2. Evaluate WBF fusion effectiveness with different weight schemes
3. Measure impact of dataset augmentation on model generalisation

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on external dataset augmentation may limit generalisation to other action localisation scenarios
- Fusion strategy assumes independence between model predictions, which may not hold in practice
- mAP metric provides aggregate performance but may mask variations across action categories

## Confidence

| Claim | Confidence |
|-------|------------|
| mAP of 0.5498 demonstrates strong performance | High |
| Method achieves first place in Perception Test Challenge 2024 | High |
| State-of-the-art performance claims | Medium |
| Generalisation capability to other datasets | Low |

## Next Checks
1. Conduct ablation studies to quantify individual contributions of each feature extractor and the fusion strategy
2. Test model's generalisation capability on independent action localisation benchmarks (ActivityNet, THUMOS)
3. Perform temporal sensitivity analysis to evaluate performance across different action durations and temporal resolutions