---
ver: rpa2
title: 'Steering Away from Harm: An Adaptive Approach to Defending Vision Language
  Model Against Jailbreaks'
arxiv_id: '2411.16721'
source_url: https://arxiv.org/abs/2411.16721
tags:
- steering
- adversarial
- attack
- image
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of vision language models (VLMs)
  producing harmful content when exposed to adversarial attacks, particularly due
  to their vision capabilities creating new vulnerabilities. The core method, ASTRA,
  involves constructing transferable steering vectors representing the direction of
  harmful responses through image attribution, and applying adaptive activation steering
  to remove these directions at inference time.
---

# Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks

## Quick Facts
- arXiv ID: 2411.16721
- Source URL: https://arxiv.org/abs/2411.16721
- Authors: Han Wang; Gang Wang; Huan Zhang
- Reference count: 40
- Primary result: ASTRA framework effectively mitigates perturbation-based attacks while preserving model utility, achieving state-of-the-art performance across multiple models and baselines

## Executive Summary
Vision language models (VLMs) are vulnerable to jailbreak attacks that manipulate both visual and textual inputs to generate harmful content. This paper introduces ASTRA (Adaptive Steering Against Jailbreaks), a framework that constructs transferable steering vectors through image attribution and applies adaptive activation steering during inference. The method identifies visual tokens most associated with harmful outputs using ablation-based attribution, then removes harmful directions selectively based on calibrated activations. Extensive experiments show ASTRA achieves superior defense performance while maintaining model utility across three VLM architectures and multiple attack types.

## Method Summary
ASTRA constructs steering vectors by first generating adversarial images through PGD attacks, then using image attribution to identify visual tokens strongly associated with jailbreak behavior. The framework randomly ablates visual tokens and fits a Lasso surrogate model to estimate each token's impact on jailbreak probability, selecting top-k attributed tokens. Steering vectors are created by averaging difference vectors across multiple adversarial images. During inference, adaptive steering applies these vectors selectively based on the projection between calibrated activations (computed from benign inputs) and the steering vectors, ensuring steering is only applied when harmful directions are detected.

## Key Results
- ASTRA achieves substantial improvement in defending against attacks while maintaining high efficiency compared to baseline methods
- The framework exhibits strong transferability, defending against unseen attack types including structured-based, perturbation-based, and text-only attacks
- ASTRA preserves high model utility on standard benchmarks (MM-Vet, MM-Bench, XSTest) with minimal performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive activation steering using calibrated activations reduces harmful outputs while preserving benign utility
- Mechanism: The framework computes a projection between calibrated activations (hl - hl_0) and steering vectors vl, applying steering only when the projection is positive
- Core assumption: The calibrated activation space can effectively distinguish harmful from benign inputs based on their relationship to steering vectors
- Evidence anchors:
  - [abstract] "During inference, we perform the adaptive steering method that involves the projection between the steering vectors and calibrated activation, resulting in little performance drops on benign inputs while strongly avoiding harmful outputs under adversarial inputs."
  - [section 3.2] "we propose adaptive steering based on conditional projection: hl = hl - α · max( (hl - hl_0)⊤vl / ∥hl - hl_0∥∥vl∥ · ∥hl∥, 0) · vl / ∥vl∥"
  - [corpus] Weak - corpus contains related work on activation steering but doesn't specifically address the calibration approach

### Mechanism 2
- Claim: Image attribution identifies visual tokens most strongly associated with jailbreaks
- Mechanism: The framework randomly ablates visual tokens and fits a Lasso surrogate model to estimate the impact of each token on jailbreak probability
- Core assumption: The surrogate model can accurately attribute jailbreak behavior to specific visual tokens through linear approximation
- Evidence anchors:
  - [section 3.1] "we fit a linear surrogate model ˆf to analyze the influence of masking subsets of visual tokens on the likelihood of jailbreaks and select the visual tokens that are highly relevant for triggering the jailbreaking responses"
  - [section 3.1] "We define visual token ablation as the process of masking specific tokens in a visual input... Given an ablation vector g, the image attribution is expected to quantify the impact on the log probability of generating specified responses r"
  - [corpus] Moderate - corpus contains related work on activation steering but doesn't specifically address the image attribution approach

### Mechanism 3
- Claim: Steering vectors constructed from harmful feature extraction generalize across different attack intensities and types
- Mechanism: The framework constructs steering vectors by averaging difference vectors across multiple adversarial images, capturing common harmful directions that persist regardless of attack method
- Core assumption: Different attacks trigger a common harmful feature direction in the model's representation space
- Evidence anchors:
  - [section 3.1] "We utilize visual tokens with Top-k attribution scores from the surrogate model ˆf paired with the empty user query to construct the steering vectors"
  - [abstract] "ASTRA exhibits good transferability, defending against unseen attacks (i.e., structured-based attack, perturbation-based attack with project gradient descent variants, and text-only attack)"
  - [section 4.2] "Although models can be jailbroken by different types of attacks, eventually, there exists a certain direction in the feature space that represents the harmfulness"

## Foundational Learning

- Concept: Activation steering in transformer models
  - Why needed here: The defense framework relies on manipulating intermediate activations to steer model behavior, requiring understanding of how activations encode semantic information
  - Quick check question: How do steering vectors shift a language model's output distribution toward specified behaviors during inference?

- Concept: Image attribution techniques in computer vision
  - Why needed here: The framework uses ablation-based attribution to identify which visual tokens contribute to harmful outputs, requiring understanding of attribution methods
  - Quick check question: What is the relationship between token ablation and the ability to attribute model behavior to specific input features?

- Concept: Adversarial attack methodologies
  - Why needed here: The framework must defend against various attack types (perturbation-based, structured-based, text-only), requiring understanding of attack mechanisms
  - Quick check question: How do perturbation-based attacks differ from structured-based attacks in their approach to jailbreaking VLMs?

## Architecture Onboarding

- Component map: Adversarial image generation -> Random token ablation -> Lasso surrogate fitting -> Top-k token selection -> Vector averaging -> Steering vector construction
- Critical path: 1) Construct steering vectors using adversarial images and harmful instructions, 2) Compute calibration activation from benign image-text pairs, 3) During inference, apply adaptive steering to newly generated tokens only, 4) Evaluate defense effectiveness on both adversarial and benign inputs
- Design tradeoffs: Steering coefficient selection (fixed vs adaptive), number of adversarial images (defense vs construction cost), steering layer selection (semantic level)
- Failure signatures: Poor defense performance (calibration fails, surrogate poorly fits), utility degradation (aggressive steering on benign), transferability issues (attack-specific features)
- First 3 experiments: 1) Ablation study: Remove calibration activation to test its impact on defense effectiveness, 2) Layer sensitivity: Test steering at different transformer layers to find optimal layer, 3) Coefficient sweep: Test different steering coefficients to find optimal balance between defense and utility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of adversarial images needed for constructing effective steering vectors that balance defense performance and computational efficiency?
- Basis in paper: [explicit] "We examine how the number of adversarial images used for steering vector construction affects defense performance using LLaV A-v1.5. As shown in Fig. 9(a) and (b), increasing the number of adversarial images for steering vector construction leads to rapid convergence in defense performance, indicating that only a modest amount of adversarial image is required."
- Why unresolved: The paper shows convergence occurs but doesn't specify the exact optimal number. The term "modest amount" is qualitative and may vary across different models and attack types.
- What evidence would resolve it: Systematic experiments across different VLMs (varying model sizes, architectures) and attack types showing the relationship between number of adversarial images and defense performance metrics (toxicity score, attack success rate) would identify the optimal threshold.

### Open Question 2
- Question: How does the choice of steering layer impact the transferability of steering vectors across different types of attacks (perturbation-based, structured-based, text-only)?
- Basis in paper: [explicit] "We vary the selected steering layer to assess whether our framework can generalize across different layers. For simplicity, this ablation study uses linear steering... The results also indicate that our framework correctly identifies the harmfulness direction, enabling semantic manipulation through simple adjustments to the steering coefficient."
- Why unresolved: While the paper demonstrates that different layers can be used for steering, it doesn't specifically examine how layer selection affects cross-attack transferability - whether steering vectors from one attack type generalize effectively to other attack types.
- What evidence would resolve it: Comparative experiments testing steering vectors constructed at different layers against multiple attack types (not just the type used for construction) would reveal optimal layer selection for maximum transferability.

### Open Question 3
- Question: What is the relationship between the top-k attribution scores used for visual token selection and the effectiveness of the resulting steering vectors?
- Basis in paper: [explicit] "With attribution scores for each token, we extract the representation of those tokens strongly correlated with jailbreak. Additionally, we hope our steering vectors generalize rather than overfitting to specific instructions... Given a set D of (xv, Mask(xv))... we calculate the mean difference vector for a layer l as: vl = 1/|D| Σ (al(xv, xtemplate) - al(Mask(xv), xtemplate))"
- Why unresolved: The paper uses top-k attribution scores for visual token selection but doesn't systematically explore how different k values affect steering vector quality, defense performance, or potential overfitting to specific jailbreak patterns.
- What evidence would resolve it: Experiments varying k values (e.g., top-5, top-10, top-20, top-50) and measuring corresponding defense performance, model utility preservation, and generalization across different jailbreak types would establish optimal k selection criteria.

## Limitations

- The framework's effectiveness critically depends on accurate identification of harmful feature directions through image attribution, which may fail if the linear surrogate model poorly approximates the relationship between visual tokens and jailbreak behavior
- Computational overhead of the adaptive steering mechanism during inference requires additional computation through projection calculations and conditional activation modifications
- The transferability assumption may break down if different attack types exploit fundamentally different mechanisms, limiting defense against completely novel attack strategies

## Confidence

- High confidence: The framework's core concept of using image attribution to construct steering vectors and applying adaptive activation steering shows strong empirical results across multiple VLM models and attack types
- Medium confidence: The calibration activation approach for distinguishing benign from harmful inputs appears effective but may be sensitive to the quality of the benign training data and the specific activation layer chosen
- Low confidence: The claim of substantial efficiency improvements relative to baseline methods lacks detailed analysis of the computational overhead introduced by the adaptive steering mechanism during inference

## Next Checks

1. **Layer sensitivity analysis**: Systematically test the steering mechanism at different transformer layers to identify which layers capture the most relevant harmful features and optimize defense effectiveness

2. **Steering coefficient ablation**: Conduct a comprehensive sweep of steering coefficients (α values) to quantify the exact trade-off between defense effectiveness and utility preservation, identifying optimal values for different VLM architectures

3. **Generalization to novel attacks**: Test the framework's defense effectiveness against completely unseen attack types not used in steering vector construction to validate the claimed transferability of the approach