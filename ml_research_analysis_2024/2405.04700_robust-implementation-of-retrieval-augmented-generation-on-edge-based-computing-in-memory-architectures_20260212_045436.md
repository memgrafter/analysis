---
ver: rpa2
title: Robust Implementation of Retrieval-Augmented Generation on Edge-based Computing-in-Memory
  Architectures
arxiv_id: '2405.04700'
source_url: https://arxiv.org/abs/2405.04700
tags:
- data
- uni00000013
- uni00000011
- uni00000027
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Robust CiM-backed RAG (RoCR), a framework that
  uses Computing-in-Memory (CiM) to accelerate Retrieval-Augmented Generation (RAG)
  on edge devices. The key idea is to optimize the sentence embedding model via contrastive
  learning and noise-aware training to generate CiM-friendly embeddings that are robust
  against device variations in non-volatile memory (NVM) cells.
---

# Robust Implementation of Retrieval-Augmented Generation on Edge-based Computing-in-Memory Architectures

## Quick Facts
- arXiv ID: 2405.04700
- Source URL: https://arxiv.org/abs/2405.04700
- Reference count: 40
- Key outcome: RoCR improves RAG performance by up to 35% on edge CiM devices through optimized sentence embeddings

## Executive Summary
This paper introduces RoCR, a framework that enables efficient Retrieval-Augmented Generation (RAG) on edge devices using Computing-in-Memory (CiM) architectures. The key innovation is optimizing sentence embedding models through contrastive learning and noise-aware training to generate CiM-friendly embeddings that are robust against device variations in non-volatile memory cells. The approach addresses the challenge of running RAG on resource-constrained edge devices while maintaining accuracy despite the inherent noise and variability of CiM hardware.

## Method Summary
RoCR employs a two-stage optimization process to make sentence embeddings more suitable for CiM deployment. First, contrastive learning is used to create embeddings that better capture semantic relationships while being less sensitive to hardware noise. Second, noise-aware training explicitly accounts for CiM device variations during the embedding generation process. These optimizations allow the embeddings to maintain high retrieval quality even when deployed on imperfect CiM hardware, enabling efficient RAG inference on edge devices without significant accuracy degradation.

## Key Results
- Up to 35% improvement in RAG accuracy/F1/ROUGE metrics compared to baseline approaches
- Significant performance gains demonstrated across five different CiM devices
- Low hardware overhead during inference while maintaining robustness to device variations

## Why This Works (Mechanism)
The framework works by creating embeddings that are inherently more compatible with CiM hardware characteristics. Through contrastive learning, the embeddings develop semantic structures that are less susceptible to the noise patterns typical in non-volatile memory. The noise-aware training component further enhances this robustness by explicitly modeling the expected hardware imperfections during the embedding generation phase. This dual optimization approach ensures that the semantic information remains intact even when processed through imperfect CiM cells, allowing for accurate retrieval despite hardware limitations.

## Foundational Learning
- **Computing-in-Memory (CiM) basics**: Understanding how computation is performed directly in memory arrays using analog operations - needed to grasp why traditional embeddings fail on CiM hardware and how RoCR addresses these limitations
- **Contrastive learning for embeddings**: How pairs of similar and dissimilar samples are used to shape embedding spaces - needed to understand the optimization that makes embeddings more robust to noise
- **Non-volatile memory variations**: The inherent device-to-device and temporal variations in NVM cells - needed to appreciate why noise-aware training is critical for CiM deployment
- **RAG system components**: The interplay between retrieval and generation in RAG pipelines - needed to understand how embedding quality impacts overall system performance
- **Edge computing constraints**: Limited computational resources and power budgets on edge devices - needed to contextualize why CiM acceleration is valuable
- **Embedding robustness metrics**: Methods for quantifying how well embeddings maintain semantic relationships under noise - needed to evaluate the effectiveness of the proposed optimizations

## Architecture Onboarding
**Component map**: Input text -> Sentence embedding model -> CiM-friendly embeddings -> Approximate nearest neighbor search -> Retrieved documents -> LLM for generation

**Critical path**: Embedding generation and retrieval are the most critical operations, as they directly impact the quality of retrieved documents that feed into the generation model. The noise-aware training phase is computationally intensive but occurs offline.

**Design tradeoffs**: The framework trades some embedding precision for robustness to hardware noise, accepting slight semantic drift to ensure consistent performance across varying CiM devices. This approach prioritizes reliability over absolute accuracy in exchange for better generalization.

**Failure signatures**: Poor retrieval quality manifests as semantically unrelated documents being retrieved, often due to embedding degradation in CiM cells. Hardware variations can cause systematic biases in embedding space that degrade recall for certain semantic regions.

**First experiments**: 1) Test embedding quality on a single CiM device with controlled noise patterns to verify the contrastive learning benefits, 2) Evaluate retrieval accuracy with varying degrees of hardware noise to establish robustness bounds, 3) Compare end-to-end RAG performance against baseline embedding models across multiple datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to diverse CiM hardware platforms beyond the five tested devices remains uncertain
- Limited evaluation of non-accuracy metrics such as latency, power consumption, and model size
- No comprehensive analysis of how environmental factors like temperature affect embedding quality and retrieval performance

## Confidence
- **High confidence**: The 35% performance improvement over baselines on tested datasets and CiM devices is well-supported by experimental results
- **Medium confidence**: The assertion of low hardware overhead during inference is reasonable based on described optimizations, though analysis could be more comprehensive
- **Medium confidence**: The framework's robustness to device variations is demonstrated, but generalization to unseen CiM platforms requires further validation

## Next Checks
1. Test RoCR's performance across a broader range of CiM hardware platforms, including different non-volatile memory technologies (e.g., PCM, RRAM, MRAM) and emerging architectures, to assess generalizability
2. Evaluate the impact of environmental factors such as temperature fluctuations and voltage variations on embedding quality and overall RAG performance
3. Quantify the computational overhead of the contrastive learning and noise-aware training phases, and assess their scalability to larger embedding models or multi-domain adaptation scenarios