---
ver: rpa2
title: 'CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced
  Retrieval-Augmented Generation in Fact-Checking'
arxiv_id: '2408.08535'
source_url: https://arxiv.org/abs/2408.08535
tags:
- retrieval
- community
- fact-checking
- information
- percent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating structured knowledge
  graph data with retrieval-augmented generation (RAG) systems for fact-checking.
  The authors propose CommunityKG-RAG, a zero-shot framework that leverages community
  structures within knowledge graphs to improve retrieval accuracy.
---

# CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking

## Quick Facts
- arXiv ID: 2408.08535
- Source URL: https://arxiv.org/abs/2408.08535
- Authors: Rong-Ching Chang; Jiawei Zhang
- Reference count: 13
- One-line primary result: Zero-shot framework leveraging knowledge graph community structures achieves 56.24% accuracy on MOCHEG dataset, outperforming semantic retrieval (43.84%) and no retrieval (39.79%) baselines.

## Executive Summary
This paper addresses the challenge of integrating structured knowledge graph data with retrieval-augmented generation (RAG) systems for fact-checking. The authors propose CommunityKG-RAG, a zero-shot framework that leverages community structures within knowledge graphs to improve retrieval accuracy. The method constructs a knowledge graph from fact-checking articles, detects communities using the Louvain algorithm, and uses these communities to retrieve relevant context for LLM-based claim verification. The framework demonstrates superior performance compared to traditional methods, achieving 56.24% accuracy on the MOCHEG dataset, a significant improvement over baselines like semantic retrieval (43.84%) and no retrieval (39.79%). The approach is shown to be scalable and efficient, requiring no additional training while effectively utilizing multi-hop information pathways within the knowledge graph.

## Method Summary
CommunityKG-RAG is a zero-shot framework that leverages community structures within knowledge graphs to improve retrieval-augmented generation for fact-checking. The method constructs a KG from fact-checking articles using coreference resolution and entity relationship extraction, detects communities via the Louvain algorithm, and assigns BERT embeddings to nodes. Communities are then used for context retrieval through a hierarchical filtering process: first selecting top δ percent of communities by relevance score, then selecting top λ percent of sentences within those communities. The retrieved context is provided to an LLM for claim verification, achieving improved accuracy compared to traditional RAG approaches.

## Key Results
- CommunityKG-RAG achieves 56.24% accuracy on the MOCHEG dataset, outperforming semantic retrieval (43.84%) and no retrieval (39.79%) baselines.
- The framework demonstrates effective zero-shot performance without requiring additional training.
- Hierarchical filtering with community and sentence selection thresholds significantly improves retrieval relevance and claim verification accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Community structure detection via Louvain algorithm improves retrieval relevance by clustering semantically related entities.
- Mechanism: The Louvain algorithm identifies dense subgraphs (communities) within the knowledge graph. These communities act as semantic clusters where entities are more interconnected within the community than with the rest of the graph. By focusing retrieval on these communities, the framework captures richer contextual relationships.
- Core assumption: Community detection using modularity optimization preserves semantic coherence relevant to fact-checking tasks.
- Evidence anchors:
  - [abstract] "utilizes the multi-hop nature of community structures within KGs to significantly improve the accuracy and relevance of information retrieval."
  - [section 4.2] "This algorithm is instrumental in detecting and delineating communities within the graph G, by focusing on the optimization of modularity."
  - [corpus] Weak evidence: No direct corpus support for Louvain algorithm's effectiveness in this specific fact-checking context.
- Break condition: If the knowledge graph lacks community structure (highly sparse or random graph), modularity optimization fails to produce meaningful clusters.

### Mechanism 2
- Claim: Converting community node embeddings into averaged semantic representations enhances retrieval matching with claims.
- Mechanism: Each community's embedding is computed as the average of BERT embeddings of its constituent nodes. This aggregation creates a compact semantic representation of the community, enabling efficient similarity computation between claims and communities.
- Core assumption: Averaging node embeddings preserves the collective semantic meaning of the community.
- Evidence anchors:
  - [section 4.3] "The embedding representation of each community denoted as φ(m) is derived by averaging the BERT embeddings of the nodes within Em."
  - [section 4.3] "This approach aggregates the collective semantic attributes of the community, encapsulating a comprehensive semantic representation."
  - [corpus] No direct corpus evidence on the effectiveness of averaged embeddings for community representation.
- Break condition: If communities contain highly diverse or unrelated entities, averaging may dilute specific semantic signals.

### Mechanism 3
- Claim: Hierarchical filtering (top communities → top sentences) optimizes context relevance and reduces noise.
- Mechanism: The framework first selects top δ percent of communities by relevance score, then within those communities selects top λ percent of sentences. This two-stage filtering ensures that only the most contextually relevant information reaches the language model.
- Core assumption: Relevance scores based on dot product similarity effectively rank communities and sentences.
- Evidence anchors:
  - [section 4.3] "The relevance score r(c, m) between claim c and community m is calculated as the dot product between their embeddings."
  - [section 4.4] "To efficiently prioritize communities for deeper analysis, the top δ percent of communities, ranked by their relevance scores r(c, m), are selected."
  - [corpus] Weak evidence: No corpus data directly validating the effectiveness of hierarchical filtering.
- Break condition: If relevance scores are poorly calibrated, the filtering may exclude important context or include irrelevant information.

## Foundational Learning

- Concept: Modularity-based community detection
  - Why needed here: Enables identification of semantically coherent subgraphs within the knowledge graph that are relevant to fact-checking.
  - Quick check question: What metric does the Louvain algorithm optimize to identify communities?

- Concept: Graph node embeddings and aggregation
  - Why needed here: Provides a way to represent complex community structures as single semantic vectors for efficient similarity computation.
  - Quick check question: How is the embedding for a community computed from its constituent nodes?

- Concept: Hierarchical information retrieval
  - Why needed here: Allows for multi-stage filtering that balances precision and recall in selecting relevant context for fact-checking.
  - Quick check question: What are the two filtering stages in the CommunityKG-RAG framework?

## Architecture Onboarding

- Component map: Knowledge Graph Construction -> Community Detection -> Community Retrieval -> Top Community Selection -> Community-to-Sentence Selection -> LLM Integration

- Critical path: Knowledge Graph Construction → Community Detection → Community Retrieval → Top Community Selection → Community-to-Sentence Selection → LLM Generation

- Design tradeoffs:
  - Zero-shot vs. fine-tuning: Zero-shot approach avoids training costs but may underperform on highly specialized domains.
  - Community granularity: Larger communities capture more context but risk including noise; smaller communities are more focused but may miss relevant connections.
  - Embedding averaging: Simple and efficient but may lose specific entity relationships compared to more complex aggregation methods.

- Failure signatures:
  - Low accuracy: May indicate poor community detection or relevance scoring issues.
  - Slow inference: Could be caused by large knowledge graphs or inefficient community-to-sentence mapping.
  - Inconsistent results: Might suggest instability in community detection or embedding computation.

- First 3 experiments:
  1. Baseline comparison: Run CommunityKG-RAG with no retrieval and semantic retrieval baselines to establish performance gap.
  2. Community threshold ablation: Vary δ (top community selection percentage) to find optimal community granularity.
  3. Community-to-sentence selection ablation: Vary λ to determine optimal context depth for LLM prompting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of entity recognition affect the performance of CommunityKG-RAG?
- Basis in paper: [explicit] The paper mentions that the framework's effectiveness heavily relies on the quality of entity recognition and that there are prior works utilizing language models for entity recognition which could introduce hallucinations.
- Why unresolved: The paper uses REBEL, a seq2seq model based on Wikipedia data, and mentions that if the framework is applied to text significantly different from Wikipedia text, it might hinder performance. However, it doesn't provide empirical evidence on how different entity recognition methods or data domains affect the framework's performance.
- What evidence would resolve it: Comparative experiments using different entity recognition methods and data domains to measure the impact on CommunityKG-RAG's accuracy and efficiency.

### Open Question 2
- Question: What is the impact of using different backbone language models on the performance of CommunityKG-RAG?
- Basis in paper: [explicit] The paper conducts an ablation study comparing LLaMa2 7B and LLaMa3 8B models, showing varying improvements in accuracy. However, it doesn't explore a wider range of models or provide a comprehensive analysis of how model size and architecture affect performance.
- Why unresolved: The study only compares two models and doesn't consider other popular models or different sizes within the same model family. It also doesn't analyze the computational trade-offs between model performance and resource requirements.
- What evidence would resolve it: Extensive experiments using various backbone models of different sizes and architectures to measure accuracy, computational efficiency, and scalability of CommunityKG-RAG.

### Open Question 3
- Question: How does the selection of top communities and community-to-sentence selection thresholds affect the performance of CommunityKG-RAG?
- Basis in paper: [explicit] The paper conducts ablation studies on different thresholds for community and community-to-sentence selection, showing that increasing thresholds from 25% to 100% improves performance. However, it doesn't explore the optimal combination of thresholds or provide a detailed analysis of the trade-offs between context richness and noise introduction.
- Why unresolved: The study only tests a few discrete values for the thresholds and doesn't use techniques like grid search or cross-validation to find the optimal combination. It also doesn't discuss how the optimal thresholds might vary depending on the dataset or the specific fact-checking task.
- What evidence would resolve it: Systematic experiments using techniques like grid search or Bayesian optimization to find the optimal combination of community and community-to-sentence selection thresholds for different datasets and fact-checking tasks.

## Limitations

- The evaluation is constrained to a single fact-checking dataset (MOCHEG) with limited domain coverage, making generalization claims uncertain.
- The framework relies on modularity-based community detection, which can be sensitive to graph density and structure - if the knowledge graph lacks clear community structure, the method may fail.
- Averaged community embedding approach assumes semantic coherence within communities, but no corpus evidence validates this assumption for fact-checking contexts.

## Confidence

- **High Confidence**: The basic framework architecture (KG construction → community detection → retrieval → LLM prompting) is clearly specified and reproducible.
- **Medium Confidence**: The performance improvements over baselines (56.24% accuracy vs 43.84% semantic retrieval) are demonstrated but may be dataset-specific.
- **Low Confidence**: Claims about the general effectiveness of modularity-based community detection for fact-checking tasks lack supporting corpus evidence.

## Next Checks

1. **Cross-dataset validation**: Evaluate CommunityKG-RAG on additional fact-checking datasets (e.g., FEVER, HoVer) to test generalizability beyond MOCHEG.
2. **Community structure sensitivity**: Systematically vary knowledge graph construction parameters and Louvain algorithm settings to identify conditions where community detection fails or produces suboptimal results.
3. **Embedding aggregation comparison**: Compare averaged community embeddings against alternative aggregation methods (weighted averaging, attention-based pooling) to determine if the simple averaging approach is optimal.