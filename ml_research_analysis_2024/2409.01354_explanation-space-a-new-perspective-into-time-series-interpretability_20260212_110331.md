---
ver: rpa2
title: 'Explanation Space: A New Perspective into Time Series Interpretability'
arxiv_id: '2409.01354'
source_url: https://arxiv.org/abs/2409.01354
tags:
- time
- series
- domain
- explanation
- zero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpreting deep learning
  models for time series data, where the meaningful features are often not easily
  visualized in the time domain. The authors propose a method called "explanation
  space projection" that allows generating explanations in different domains (e.g.,
  frequency, time/frequency, min-zero, difference, decomposition) without retraining
  the model.
---

# Explanation Space: A New Perspective into Time Series Interpretability

## Quick Facts
- arXiv ID: 2409.01354
- Source URL: https://arxiv.org/abs/2409.01354
- Authors: Shahbaz Rezaei; Xin Liu
- Reference count: 38
- Primary result: Introduces explanation space projection method for time series interpretability without model retraining

## Executive Summary
This paper addresses the challenge of interpreting deep learning models for time series data by proposing a method called "explanation space projection." The approach allows generating explanations in different domains (frequency, time/frequency, min-zero, difference, decomposition) without retraining the model. By wrapping the trained model with a transformation layer, existing XAI methods can produce interpretable explanations that reveal different aspects of the data generation process. Experiments on nine XAI methods across various time series datasets demonstrate that different spaces can yield more sparse and faithful explanations depending on the data type, with frequency domain being particularly effective for periodic signals like FordA.

## Method Summary
The method involves training a model (ResNet or InceptionTime) on time domain data, then wrapping it with transformation layers that project inputs into different explanation spaces before classification. Five explanation spaces are introduced: frequency (via Fourier transform), time/frequency (wavelet transform), min-zero (baseline removal), difference (non-stationary feature extraction), and decomposition (SSA-based trend/seasonal separation). Existing XAI methods are applied to the wrapped model to generate attributions in each space. The approach leverages the property that M(F⁻¹(F(x))) = M(x), ensuring the model's predictions remain unchanged while explanation space changes. Evaluation metrics include faithfulness (percentage of samples where predictions flip when non-negligible attributions are masked), sparsity (bounded metric between 0 and 1), and robustness to perturbations.

## Key Results
- Different explanation spaces produce significantly different sparsity and faithfulness scores across datasets
- Frequency space explanations are more interpretable for FordA dataset with periodic signals
- Difference space is better suited for ECG signals with non-stationary characteristics
- The proposed sparsity metric outperforms Shannon entropy by being bounded and length-aware
- No single explanation space dominates across all metrics, indicating the need for application-specific selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanation spaces allow existing XAI methods to produce interpretable explanations in domains where the underlying features are more naturally represented.
- Mechanism: By wrapping a trained model with a transformation layer that projects inputs into a different domain (e.g., frequency, difference, min-zero), existing XAI methods can generate explanations in that domain without retraining the model. The key insight is that M(F⁻¹(z)) = M(x), so the model's predictions remain unchanged while the explanation space changes.
- Core assumption: The transformation functions F and F⁻¹ are one-to-one and preserve the model's decision boundary.
- Evidence anchors:
  - [abstract]: "The core idea is to wrap the trained model with a transformation layer that projects the input to a different domain before classification, enabling the use of existing XAI methods to produce interpretable explanations."
  - [section]: "Since M(F⁻¹(F(x))) = M(x), we can define z = F(x) as a new domain and the associated classifier is M′(z) = M(F⁻¹(z)) → y."
  - [corpus]: Weak evidence - no directly related papers found in corpus that specifically discuss this mechanism of explanation space projection.

### Mechanism 2
- Claim: Different explanation spaces reveal different aspects of the data generation process, making explanations more interpretable for specific types of time series.
- Mechanism: Each explanation space emphasizes different signal characteristics - frequency space highlights periodic components, difference space reveals trends and shapelets, min-zero space addresses baseline issues, and decomposition space separates trend/seasonality from shapelet features.
- Core assumption: Time series data has inherent characteristics that are more naturally represented in specific domains rather than time domain.
- Evidence anchors:
  - [abstract]: "Five explanation spaces are introduced, each suited for different types of time series (e.g., frequency for periodic signals, difference for non-stationary data)."
  - [section]: "Different spaces can reveal different aspects of data generation process, e.g. shapelet-based features in time domain and wave-based features in frequency."
  - [corpus]: Weak evidence - while related papers exist on time-frequency explanations, none specifically validate the multi-space approach for interpretability across different time series types.

### Mechanism 3
- Claim: The proposed sparsity metric improves interpretability assessment over traditional Shannon entropy by being bounded and length-aware.
- Mechanism: The new sparsity metric Spr(E(x)) = (Σ(1 - Ei(x))/n - 1)^β normalizes attributions and accounts for input space length, avoiding the accumulation issues of Shannon entropy in long sequences.
- Core assumption: A bounded, length-normalized metric provides better interpretability assessment than unbounded metrics like Shannon entropy.
- Evidence anchors:
  - [section]: "Our formulation (in equation (6)), however, indicates better sparsity since it takes the length into account" and "We introduce four desirable properties for a sparsity metric and then propose a new simple metric that satisfies all properties."
  - [corpus]: No evidence found in corpus about this specific sparsity metric formulation.

## Foundational Learning

- Concept: One-to-one transformations and their reversibility
  - Why needed here: The entire explanation space framework relies on being able to transform between domains and back without information loss.
  - Quick check question: Can you explain why F⁻¹(F(x)) = x is necessary for the explanation space approach to work?

- Concept: Time series decomposition (SSA - Singular Spectrum Analysis)
  - Why needed here: Decomposition space requires separating time series into trend, seasonal, and residual components for separate explanation.
  - Quick check question: What are the three main components typically obtained from time series decomposition?

- Concept: Fourier transform and its properties
  - Why needed here: Frequency space and time/frequency space rely on Fourier analysis to transform between time and frequency domains.
  - Quick check question: What is the relationship between time resolution and frequency resolution in the uncertainty principle?

## Architecture Onboarding

- Component map: Input -> Transformation Layer (F) -> Wrapped Model (M′ with F⁻¹) -> XAI Method (E) -> Attribution in Explanation Space

- Critical path:
  1. Transform input x → z = F(x)
  2. Pass z through M′ (which applies F⁻¹ internally)
  3. Apply XAI method E to get attribution in space z
  4. Present attribution alongside original time series for context

- Design tradeoffs:
  - Differentiable vs non-differentiable transformations: Differentiable transformations enable gradient-based XAI methods
  - Computational overhead: Each transformation adds computation, especially for complex spaces like time/frequency
  - Interpretability vs faithfulness: Some spaces may produce sparser explanations but less faithful ones

- Failure signatures:
  - Gradient-based XAI methods fail: Indicates non-differentiable transformation
  - Explanations don't change across spaces: Suggests the transformation isn't revealing meaningful structure
  - Poor faithfulness scores: Indicates the explanation space isn't aligned with model decision-making

- First 3 experiments:
  1. Implement min-zero space wrapper and verify it removes baseline artifacts on ElectricDevices dataset
  2. Compare frequency vs time domain explanations on FordA dataset using DeepLIFT
  3. Test difference space on ECGFiveDays dataset and measure sparsity improvement

## Open Questions the Paper Calls Out
- Does the proposed explanation space projection method work effectively for real-time or streaming time series data where model retraining is not feasible?
- How does the choice of explanation space affect the interpretability and usefulness of explanations in different domains, such as healthcare or finance, where interpretability is critical?
- What are the limitations of the sparsity metric proposed in the paper, and how does it compare to other metrics in terms of capturing the true interpretability of explanations?

## Limitations
- The approach assumes transformation functions are one-to-one and differentiable for gradient-based XAI methods
- The sparsity metric lacks empirical validation against human interpretability judgments
- The decomposition space implementation using SSA is not fully specified in the paper

## Confidence
- High confidence in the core mechanism of explanation space projection and its mathematical validity
- Medium confidence in the practical benefits across different time series types, as results are mixed
- Medium confidence in the sparsity metric's superiority over Shannon entropy due to limited empirical validation

## Next Checks
1. Test the frequency space explanation on a new periodic time series dataset not used in the paper to verify the generalizability of the approach
2. Implement the min-zero space wrapper and measure its impact on removing baseline artifacts in ElectricDevices dataset
3. Conduct a user study comparing explanations generated in time vs frequency domains to validate which produces more interpretable results for human analysts