---
ver: rpa2
title: Model Stealing for Any Low-Rank Language Model
arxiv_id: '2411.07536'
source_url: https://arxiv.org/abs/2411.07536
tags:
- algorithm
- distribution
- vectors
- learning
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of model stealing for low-rank
  language models, particularly Hidden Markov Models (HMMs) and their generalizations.
  The authors study this problem in the conditional query model, where a learner can
  query the model with a history of observations and receive samples from the conditional
  distribution of future observations.
---

# Model Stealing for Any Low-Rank Language Model

## Quick Facts
- **arXiv ID**: 2411.07536
- **Source URL**: https://arxiv.org/abs/2411.07536
- **Reference count**: 21
- **One-line primary result**: Efficient algorithm learns any low-rank language model through conditional queries in polynomial time

## Executive Summary
This paper addresses the problem of model stealing for low-rank language models, particularly Hidden Markov Models (HMMs) and their generalizations. The authors develop an algorithm that can learn any low-rank distribution through conditional queries, improving upon previous work that required additional restrictive assumptions. The core innovation involves representing conditional distributions using barycentric spanners among exponentially large vectors, combined with a sampling algorithm that iteratively solves convex optimization problems involving KL projection to prevent error compounding.

## Method Summary
The algorithm operates in two phases: first, it uses conditional queries to build a distribution close to the original; second, it applies dimensionality reduction and barycentric spanner construction to create a succinct representation. The sampling procedure then generates samples from this learned representation using KL projection to control error propagation. The approach handles arbitrary low-rank distributions without requiring additional structural assumptions, achieving polynomial query complexity and runtime in the rank, alphabet size, sequence length, and desired accuracy.

## Key Results
- Polynomial-time algorithm for learning low-rank language models via conditional queries
- Improved over previous work by removing restrictive assumptions about distribution structure
- Produces distributions η-close in total variation distance to the original
- Sampling algorithm runs in poly(S, O, T, log(1/η)) time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Barycentric spanners enable efficient representation of conditional distributions
- Mechanism: The algorithm constructs barycentric spanners among exponentially large vectors to represent conditional distributions at each timestep, allowing polynomial-sized representation of low-rank distributions
- Core assumption: The low-rank structure ensures conditional distributions lie in a low-dimensional subspace
- Evidence anchors:
  - [abstract]: "First, we represent the conditional distributions at each timestep by constructing barycentric spanners among a collection of vectors of exponentially large dimension"
  - [section 3.2.2]: "By Fact 3.2, there is a subset of d elements of A that is a (1, 0)-spanner for A"
  - [corpus]: Weak evidence - corpus neighbors focus on model stealing but not barycentric spanners specifically
- Break condition: If the distribution is not low-rank, the spanning sets would need to be exponentially large

### Mechanism 2
- Claim: KL projection prevents error compounding in sequential sampling
- Mechanism: At each sampling step, the algorithm solves convex optimization problems involving projection in relative entropy to prevent compounding of errors over sequence length
- Core assumption: KL divergence has contractive properties that prevent error multiplication
- Evidence anchors:
  - [abstract]: "for sampling from our representation, we iteratively solve a sequence of convex optimization problems that involve projection in relative entropy to prevent compounding of errors over the length of the sequence"
  - [section 3.3]: "The key to overcoming the error doubling issue above. In particular, in the same abstraction as above, we would just set z′ = z∗"
  - [section 4.2]: "Fact 4.10. Let T ⊂ RN be the N-dimensional simplex... Let z∗ = argmin z∈K KL(z∥v). Then for all z∈K, KL(z∥z∗)≤ KL(z∥v)"
- Break condition: If KL divergence loses its contractive property for the specific distributions involved

### Mechanism 3
- Claim: Dimensionality reduction preserves key distribution properties
- Mechanism: The algorithm uses a sampling-based dimensionality reduction that preserves TV distance and KL divergence between linear combinations of distributions
- Core assumption: Random sampling from a mixture distribution provides representative coordinates
- Evidence anchors:
  - [section 5.2]: "Lemma 5.10. Let D1, . . . , Dm be distributions on a set of elements... If k≥ 100mr4 log4(m/ (τ δγ))/γ 2 then with 1− δ probability... The vectors u1, . . . , u m are (r, γ)-representative for D1, . . . , Dm"
  - [section 3.2.1]: "Fact 3.4. For any distribution D over OT−t, in expectation over the random draws from D, for any subset A⊆Ot and real coefficients {ch}h∈A, E[∥∑h∈A chvh∥1] = ∥∑h∈A ch PrˆH[·|h]∥1"
  - [corpus]: No direct evidence - corpus focuses on model stealing but not dimensionality reduction techniques
- Break condition: If the sampling distribution D is poorly chosen, variance could be prohibitively large

## Foundational Learning

- Concept: Hidden Markov Models and low-rank distributions
  - Why needed here: The algorithm specifically targets HMMs and more general low-rank language models, so understanding their structure is fundamental
  - Quick check question: What does it mean for a distribution to be rank S in the context of language models?

- Concept: Barycentric spanners and linear algebra
  - Why needed here: The algorithm relies on constructing barycentric spanners to efficiently represent conditional distributions in a compressed form
  - Quick check question: How does a (C, γ)-spanner relate to the original vectors it approximates?

- Concept: KL divergence and its contractive properties
  - Why needed here: The algorithm uses KL projection to prevent error compounding, so understanding its mathematical properties is crucial
  - Quick check question: Why does projection in KL divergence prevent error multiplication while projection in TV distance does not?

## Architecture Onboarding

- Component map:
  Conditional query simulator -> Dimensionality reduction module -> Barycentric spanner calculator -> Representation learning engine -> Sampling algorithm

- Critical path:
  1. Use conditional queries to build a distribution close to the original
  2. Apply dimensionality reduction to obtain succinct representations
  3. Compute barycentric spanners for each timestep
  4. Build the full learned representation with transition information
  5. Sample from the learned representation using KL-projection-based change-of-basis

- Design tradeoffs:
  - Accuracy vs. computational efficiency: Higher accuracy requires more queries and computation
  - Representation size vs. approximation quality: Larger spanning sets provide better approximations
  - Dimensionality reduction vs. information loss: More samples in reduction preserve more information but cost more

- Failure signatures:
  - Poor spanner quality: Distribution samples show high variance or bias
  - Error accumulation in sampling: KL divergence between samples and true distribution grows with sequence length
  - Inconsistent pdf queries: Conditional probabilities don't sum to 1 or violate probability bounds

- First 3 experiments:
  1. Verify conditional query simulator produces distributions close to original in TV distance
  2. Test dimensionality reduction preserves linear combinations of distributions within error bounds
  3. Validate barycentric spanner computation finds spans with correct approximation quality

## Open Questions the Paper Calls Out
No open questions explicitly called out in the provided content.

## Limitations
- Theoretical guarantees without empirical validation or runtime benchmarks
- Numerical precision challenges with exponential-dimensional vectors in practical implementation
- No analysis of performance under restricted or noisy conditional query access

## Confidence
- **High Confidence**: The fundamental theoretical framework connecting low-rank distributions to barycentric spanners is well-established. The use of KL projection for error control follows standard information-theoretic principles.
- **Medium Confidence**: The polynomial complexity bounds appear sound given the assumptions, but practical constants and implementation challenges are not addressed.
- **Low Confidence**: The sampling algorithm's iterative convex optimization steps may face numerical precision issues when dealing with high-dimensional vectors in practice.

## Next Checks
1. Implement a simplified version of the algorithm on small HMMs (S ≤ 3, O ≤ 5, T ≤ 10) to verify the theoretical guarantees empirically and identify practical bottlenecks.
2. Test the error propagation behavior of the KL projection step by comparing sampled sequences against ground truth across multiple iterations to verify the claimed error bounds.
3. Evaluate the spanner approximation quality by measuring the trade-off between spanner size and approximation accuracy on synthetic low-rank distributions with known structure.