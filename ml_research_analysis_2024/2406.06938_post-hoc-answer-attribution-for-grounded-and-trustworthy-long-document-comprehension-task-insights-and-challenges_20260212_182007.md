---
ver: rpa2
title: 'Post-Hoc Answer Attribution for Grounded and Trustworthy Long Document Comprehension:
  Task, Insights, and Challenges'
arxiv_id: '2406.06938'
source_url: https://arxiv.org/abs/2406.06938
tags:
- answer
- document
- arxiv
- sentences
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of post-hoc answer attribution
  for long document comprehension, which involves identifying supporting sentences
  in a document for each sentence of an answer to an information-seeking question.
  The authors formulate this as a new task and refactor existing datasets to evaluate
  it due to the lack of appropriate datasets.
---

# Post-Hoc Answer Attribution for Grounded and Trustworthy Long Document Comprehension: Task, Insights, and Challenges

## Quick Facts
- **arXiv ID:** 2406.06938
- **Source URL:** https://arxiv.org/abs/2406.06938
- **Reference count:** 23
- **Primary result:** ADiOSAA improves attribution precision for abstractive answers in long document comprehension by decomposing answers and using entailment-based optimal selection.

## Executive Summary
This paper introduces a new task: post-hoc answer attribution for long document comprehension (LDC), which requires identifying supporting sentences in a document for each sentence of an answer to an information-seeking question. The authors refactor existing datasets (Citation Verifiability and Hagrid) to evaluate this task due to lack of appropriate datasets. They propose ADiOSAA, a system that decomposes answers into information units and uses a textual entailment model with an optimal selection strategy to find attributions. Results show that while retrieval-based methods perform well for top-1 predictions, ADiOSAA variants achieve higher precision for top-2 and top-4 predictions, indicating better performance on abstractive and compositional attributions. The paper highlights the need for more challenging long-form abstractive reading comprehension datasets.

## Method Summary
The authors propose ADiOSAA, a system that addresses post-hoc answer attribution through three main components: answer decomposition using ChatGPT to split answer sentences into information units, a textual entailment model (RoBERTa-L trained on DocNLI) to assess support relationships, and an optimal selection algorithm that greedily adds document sentences maximizing entailment probability until gains drop below threshold δ. The system is evaluated against retrieval-based baselines (BM25, GTR, MonoT5) on reformulated versions of Citation Verifiability and Hagrid datasets, measuring precision, recall, and F1@k for top-k predicted attributions per answer sentence.

## Key Results
- Retrieval-based methods (MonoT5, GTR, BM25) excel at top-1 attribution but fail for top-2 and top-4 predictions on abstractive answers.
- ADiOSAA variants achieve significantly higher precision for top-2 and top-4 predictions compared to retrieval baselines.
- The optimal selection algorithm with decomposition outperforms retrieval methods on compositional attributions.
- Current datasets show most answer sentences are extractive or only loosely abstractive, limiting the challenge for attribution systems.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADiOSAA's decomposition plus optimal selection strategy improves attribution precision for abstractive answers.
- Mechanism: The answer decomposer breaks sentences into information units; the optimal selection algorithm greedily adds document sentences that maximize entailment probability until gains drop below threshold δ.
- Core assumption: Each information unit in an abstractive answer can be mapped to one or more document sentences through textual entailment.
- Evidence anchors:
  - [abstract] "ADiOSAA variants achieve higher precision for top-2 and top-4 predictions, indicating better performance on abstractive and compositional attributions."
  - [section 3] "Algorithm 1 Optimal Selection Algorithm ... iteratively selects a sentence from the set of remaining source sentences that maximizes the probability of entailment until the entailment score keeps increasing above a threshold δ."
  - [corpus] "Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models" supports decomposition utility, though no direct entailment validation in neighbor corpus.
- Break condition: If optimal selection greedily adds non-supporting sentences, precision degrades; if δ too low, algorithm includes irrelevant sentences.

### Mechanism 2
- Claim: Retrieval-based methods excel at top-1 attribution but fail on multi-sentence support in abstractive answers.
- Mechanism: BM25, GTR, MonoT5 retrieve top-k document sentences per answer sentence; single best match suffices for extractive answers but not for compositional ones.
- Core assumption: Most answer sentences in current datasets are extractive or only loosely abstractive, so top-1 retrieval works.
- Evidence anchors:
  - [section 4] "While MonoT5-based retrieval system outperforms others for the top-1 prediction, ADiOSAA variants attain the highest precision when top 2 or 4 predictions are considered."
  - [section 5] "Retrieval-based systems are good at retrieving one attribution correctly but fail for the second (or more) one compared to our systems."
  - [corpus] No direct corpus evidence; inference from result comparison only.
- Break condition: If dataset becomes fully abstractive, retrieval baselines' top-1 advantage diminishes.

### Mechanism 3
- Claim: Decomposition mitigates over-decomposition and hallucination risks by constraining information units to document content.
- Mechanism: ChatGPT decomposes answer sentences into facts; answer decomposer ensures each unit aligns with source content before entailment.
- Core assumption: Decompositions that are too granular or hallucinated will be filtered out by entailment model's low confidence scores.
- Evidence anchors:
  - [section 3] "We prompt ChatGPT ... to decompose the given answer into its information units ... avoids relying on annotated data and achieves greater flexibility."
  - [section 6] "the decomposer might sometimes over-decompose a simple sentence, or generate hallucinated information units."
  - [corpus] "Atomic Fact Decomposition Helps Attributed Question Answering" supports decomposition utility; no hallucination mitigation detail in corpus.
- Break condition: If decomposition introduces hallucinated facts not in document, entailment scores drop, causing false negatives.

## Foundational Learning

- Concept: Textual entailment modeling with DocNLI.
  - Why needed here: To decide whether a document sentence supports an answer information unit.
  - Quick check question: Given premise "The capital of France is Paris" and hypothesis "Paris is in France," what would DocNLI output probability?
- Concept: Sparse vs dense retrieval (BM25 vs GTR).
  - Why needed here: Baseline methods for attribution; understand lexical vs semantic matching.
  - Quick check question: For query "capital of France," which method retrieves "Paris" better: BM25 or GTR?
- Concept: Greedy optimal selection algorithm.
  - Why needed here: To compose multiple document sentences that jointly support a compositional answer unit.
  - Quick check question: If δ=0.3 and entailment scores for adding sentences are [0.6, 0.7, 0.75], when does the algorithm stop?

## Architecture Onboarding

- Component map: Input (Q, A, D) triplet -> Answer decomposer (ChatGPT) -> Attributor (DocNLI-based entailment scorer) -> Optimal selection module (greedy algorithm with threshold δ) -> Output (supporting sentence indices per answer sentence)
- Critical path:
  1. Decompose A into units.
  2. For each unit, run optimal selection over D.
  3. Aggregate results and deduplicate.
- Design tradeoffs:
  - Decompose vs not: Decomposition increases precision for abstractive cases but adds latency and potential over-decomposition errors.
  - Top-k vs full D: Using only top-150 retrieved sentences speeds inference but risks missing relevant sentences outside the top-150.
  - δ threshold: Low δ improves recall but hurts precision; high δ improves precision but risks missing valid attributions.
- Failure signatures:
  - Precision drops when δ too low or decomposition over-granular.
  - Recall drops when δ too high or top-k retrieval too restrictive.
  - False positives when entailment model overconfident on noisy matches.
- First 3 experiments:
  1. Baseline: Run retrieval-only (MonoT5) on Verifiability test set; record @1, @2, @4 precision.
  2. Ablation: Run ADiOSAA with decomposition disabled; compare to baseline.
  3. Sensitivity: Sweep δ ∈ {0.1, 0.3, 0.5}; plot precision-recall curves for top-4 predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of answer decompositions impact the overall performance of the ADiOSAA system?
- Basis in paper: [inferred] The paper mentions that the answer decomposer might sometimes over-decompose a simple sentence or generate hallucinated information units, but does not provide a detailed analysis of how these errors affect the system's performance.
- Why unresolved: The paper acknowledges the limitations of the answer decomposer but does not quantify the impact of these errors on the final attribution results.
- What evidence would resolve it: A detailed error analysis of the answer decomposer, showing the types and frequencies of errors, and their correlation with the system's attribution performance.

### Open Question 2
- Question: How does the choice of the entailment model affect the performance of the ADiOSAA system?
- Basis in paper: [explicit] The paper uses RoBERTa-L trained on DocNLI as the entailment model but acknowledges that the performance of ADiOSAA depends on the attributor and suggests investigating the impact of the NLI model's performance on the final results.
- Why unresolved: The paper does not explore the use of different entailment models or provide a comparison of their performance on the task.
- What evidence would resolve it: Experiments comparing the performance of ADiOSAA with different entailment models, such as BERT, RoBERTa, or other NLI models, on the same datasets.

### Open Question 3
- Question: How does the performance of the ADiOSAA system vary with the length and complexity of the input document?
- Basis in paper: [inferred] The paper mentions that the task is challenging for long document comprehension but does not provide a detailed analysis of how the system's performance changes with document length or complexity.
- Why unresolved: The paper does not explore the relationship between document length, complexity, and the system's ability to accurately attribute answers.
- What evidence would resolve it: Experiments evaluating the performance of ADiOSAA on documents of varying lengths and complexities, and analyzing the correlation between these factors and the system's attribution accuracy.

## Limitations
- The performance gap between retrieval-based methods and ADiOSAA for top-2 and top-4 predictions is asserted but not thoroughly analyzed, making it unclear whether this is due to the nature of the answer or limitations of retrieval methods.
- The optimal selection algorithm's effectiveness is assumed based on the greedy strategy with a fixed threshold δ, but there's no sensitivity analysis showing how δ impacts results or whether alternative selection strategies might perform better.
- The claim that current datasets are insufficient for fostering trustworthy QA systems is based on observed performance gaps, but there's no evidence that the reformulated datasets themselves are inherently flawed or that new datasets would resolve the issue.

## Confidence

**High Confidence:** The observation that retrieval-based methods perform well for top-1 predictions but struggle for top-2 and top-4 is directly supported by the evaluation results and is a straightforward interpretation of the data.

**Medium Confidence:** The claim that ADiOSAA's decomposition plus optimal selection improves precision for abstractive answers is supported by the results, but the underlying reasons (e.g., decomposition quality, entailment accuracy) are not fully validated through ablations or error analysis.

**Low Confidence:** The assertion that current datasets are insufficient for fostering trustworthy QA systems is based on the observed performance gaps, but there's no evidence that the reformulated datasets themselves are inherently flawed or that new datasets would resolve the issue.

## Next Checks

1. **Ablation Study:** Run ADiOSAA without decomposition (i.e., use answer sentences directly) and compare precision@2 and precision@4 to assess the impact of decomposition on abstractive attribution.

2. **δ Sensitivity Analysis:** Sweep δ across a range (e.g., 0.1, 0.3, 0.5) and plot precision-recall curves for top-4 predictions to determine the optimal threshold and its impact on performance.

3. **Error Analysis:** Conduct a qualitative analysis of false positives and false negatives in ADiOSAA's attributions to identify whether errors stem from decomposition, entailment, or optimal selection, and propose targeted improvements.