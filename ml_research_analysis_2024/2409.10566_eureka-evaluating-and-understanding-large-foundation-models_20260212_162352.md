---
ver: rpa2
title: 'Eureka: Evaluating and Understanding Large Foundation Models'
arxiv_id: '2409.10566'
source_url: https://arxiv.org/abs/2409.10566
tags:
- claude
- llama
- language
- evaluation
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EUREKA is an open-source framework and benchmark suite designed
  for rigorous, reproducible evaluation of large foundation models (LFMs) across diverse
  language and multimodal capabilities. It addresses challenges like benchmark saturation,
  lack of transparency, and the need for comprehensive capability assessment.
---

# Eureka: Evaluating and Understanding Large Foundation Models

## Quick Facts
- arXiv ID: 2409.10566
- Source URL: https://arxiv.org/abs/2409.10566
- Reference count: 40
- Key outcome: EUREKA is an open-source framework and benchmark suite designed for rigorous, reproducible evaluation of large foundation models across diverse language and multimodal capabilities.

## Executive Summary
EUREKA addresses critical challenges in evaluating large foundation models by providing a modular, transparent framework and a carefully curated benchmark suite. The framework enables granular, disaggregated analysis of model capabilities across 12 state-of-the-art models, revealing that no single model dominates across all tasks. Key findings include persistent struggles with geometric reasoning, image understanding, and factuality, along with significant non-determinism and backward incompatibility issues. The open-source nature and comprehensive evaluation approach make EUREKA a valuable tool for understanding and improving AI systems.

## Method Summary
EUREKA provides a modular pipeline architecture where evaluation experiments are defined using configurable components (PromptProcessing, Inference, DataProcessing, EvalReporting, DataJoin) that can be composed into shareable pipelines. The framework includes EUREKA-BENCH, a benchmark suite selected to avoid saturation, featuring tasks like GeoMeter for geometric reasoning, MMMU for multimodal QA, and Kitab for information retrieval. Evaluations were conducted across 12 models including Claude 3 Opus, GPT-4 variants, Llama 3, and others, with disaggregated analysis across subcategories to uncover granular model weaknesses.

## Key Results
- No single "best" model across all capabilitiesâ€”different models excel in different tasks
- Persistent struggles with detailed image understanding, geometric reasoning, and factuality
- High non-determinism observed in several models, even with temperature set to zero
- Notable backward incompatibility in model updates within the same family

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular Pipeline Architecture Enables Transparent and Reproducible Evaluations
- Mechanism: The EUREKA framework uses a modular design where each experiment is defined using a Pipeline composed of configurable Components (PromptProcessing, Inference, DataProcessing, EvalReporting, DataJoin). This modularity allows flexible customization of evaluation pipelines while ensuring reproducibility through standardized JSONL logging of all steps and parameters.
- Core assumption: Modular, composable evaluation components reduce development overhead and increase transparency compared to monolithic evaluation scripts.
- Evidence anchors:
  - [abstract] "EUREKA provides a flexible library for composing these functionalities into shareable evaluation pipelines and gives full control to practitioners to handle and log the details of each experiment."
  - [section] "Each experiment under the EUREKA framework is defined using a Pipeline. These pipelines are comprised of a set of Components... This modular design not only improves readability, but also allows extendability and reusability."
- Break condition: If components become too tightly coupled or if logging fails to capture sufficient detail for reproduction, the transparency and reproducibility benefits diminish.

### Mechanism 2
- Claim: Benchmark Selection Based on Saturation Levels Reveals Meaningful Model Differences
- Mechanism: EUREKA-BENCH includes benchmarks where models have not reached saturation (performance < 80%), enabling discovery of meaningful differences between models at a capability level. This contrasts with saturated benchmarks where models perform near-perfectly, obscuring differentiation.
- Core assumption: Non-saturated benchmarks provide sufficient headroom for models to demonstrate varied capabilities and failure modes.
- Evidence anchors:
  - [abstract] "The inherent space for improvement in non-saturated benchmarks enables us to discover meaningful differences between models at a capability level."
  - [section] "To create space for deeper analysis, the benchmarks in EUREKA-BENCH are chosen such that either the whole benchmark, or an important experimental condition within that benchmark, remains challenging for even the most capable models."
- Break condition: If benchmarks become saturated over time without updates, or if model capabilities improve faster than benchmark design can adapt, the differentiation advantage disappears.

### Mechanism 3
- Claim: Disaggregated Analysis Across Subcategories Uncovers Granular Model Weaknesses
- Mechanism: Instead of reporting single aggregate scores, EUREKA disaggregates performance across important experimental conditions and subcategories of data. This reveals granular weaknesses of models for specific capabilities that would be hidden in overall metrics.
- Core assumption: Performance variations across subcategories are meaningful indicators of specific model strengths and weaknesses, not just statistical noise.
- Evidence anchors:
  - [abstract] "Such insights uncover granular weaknesses of models for a given capability and can then be further leveraged to plan more precisely on what areas are most promising for improvement."
  - [section] "Previous work in model evaluation and error analysis has shown that single-score evaluations can hide important failure modes. Here, we build upon the prior work and disaggregate performance across input attributes (i.e. subcategories of data) and experimental conditions relevant for the given capability."
- Break condition: If subcategories are poorly defined, too granular to provide actionable insights, or if performance differences are primarily due to noise rather than systematic weaknesses, the disaggregated approach loses value.

## Foundational Learning

- Concept: Modular Software Design and Component Composition
  - Why needed here: Understanding how to design, implement, and integrate modular components that can be composed into flexible evaluation pipelines.
  - Quick check question: How would you design a new Component that extends the existing Pipeline without breaking backward compatibility?

- Concept: Statistical Analysis and Error Metrics
  - Why needed here: Ability to calculate and interpret various performance metrics, standard errors, entropy measures for non-determinism, and backward compatibility metrics across multiple runs and conditions.
  - Quick check question: How would you determine if observed performance differences between models are statistically significant versus due to random variation?

- Concept: Multimodal AI Capabilities and Evaluation
  - Why needed here: Understanding the specific capabilities being evaluated (geometric reasoning, multimodal QA, object recognition/detection, spatial reasoning, navigation, counting) and how to assess them across different modalities.
  - Quick check question: What are the key differences between evaluating vision-only versus text-only versus vision-text conditions for spatial reasoning tasks?

## Architecture Onboarding

- Component map:
  Pipeline -> PromptProcessing -> Inference -> DataProcessing -> EvalReporting -> DataJoin

- Critical path:
  1. Data loading through appropriate DataLoaders
  2. Prompt preparation via PromptProcessing
  3. Model inference through configured Models
  4. Output post-processing via DataProcessing
  5. Metric calculation and aggregation through EvalReporting
  6. Result logging and reporting

- Design tradeoffs:
  - Flexibility vs. complexity: More modular components provide flexibility but increase system complexity
  - Transparency vs. performance: Extensive logging ensures reproducibility but may impact evaluation speed
  - Standardization vs. customization: Standardized components ensure consistency but may limit specialized use cases

- Failure signatures:
  - Missing or corrupted JSONL logs indicate logging failures
  - Component inheritance issues suggest problems with pipeline composition
  - Inconsistent performance across repeated runs points to non-determinism issues
  - High standard errors across subcategories indicate potential measurement problems

- First 3 experiments:
  1. Run a simple text-only benchmark (like IFEval) with one model to verify basic pipeline functionality
  2. Test a multimodal benchmark (like GeoMeter) with temperature=0 to check non-determinism detection
  3. Compare two model versions on the same benchmark to validate backward compatibility measurement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively evaluate the impact of memorization on model performance in non-saturated benchmarks?
- Basis in paper: [explicit] The paper discusses the challenge of benchmark saturation and mentions that even if test data is used in training, models still struggle to perform well on non-saturated benchmarks like those in EUREKA-BENCH. It also references the need for tools to distinguish memorization from genuine capability improvements.
- Why unresolved: Distinguishing between memorization and genuine learning is difficult without full transparency on training data. The paper acknowledges this challenge but does not provide a solution.
- What evidence would resolve it: Development of generalizable methods to detect memorization across different modalities and datasets, or studies showing clear performance drops when test data is recollected or expanded.

### Open Question 2
- Question: What is the source of non-determinism in large foundation models, and how can it be mitigated?
- Basis in paper: [explicit] The paper finds that several models exhibit high non-determinism, even with temperature set to zero and fixed seeds. It speculates on possible causes like GPU computation and Sparse Mixture of Experts, but does not confirm them.
- Why unresolved: The sources of non-determinism are not fully understood, and its impact on user experience and evaluation stability is significant but not quantified.
- What evidence would resolve it: Detailed studies isolating the causes of non-determinism, or development of models with guaranteed deterministic outputs.

### Open Question 3
- Question: What is the relationship between model size, architecture, and non-determinism?
- Basis in paper: [inferred] The paper observes that models like Llama 3 70B and Llama 3.1 70B are more deterministic than others, while speculating that Mixture of Experts architectures might contribute to non-determinism.
- Why unresolved: The paper does not provide a systematic study of how model size or architecture affects non-determinism.
- What evidence would resolve it: Comparative studies of deterministic vs. non-deterministic models across different architectures and sizes, or ablation studies isolating the impact of specific components.

## Limitations

- Evaluation is limited to 12 specific models and a curated set of benchmarks, which may not capture the full diversity of LFMs
- Performance variations across subcategories could be influenced by dataset characteristics rather than fundamental model weaknesses
- The modular framework design may introduce complexity that affects reproducibility across different implementation environments

## Confidence

High confidence in the modular pipeline architecture and its benefits for transparency and reproducibility
Medium confidence in the benchmark selection strategy based on saturation levels and long-term effectiveness
Medium confidence in the disaggregated analysis approach and practical significance of granular performance differences

## Next Checks

1. Replicate the evaluation pipeline with an additional set of models not included in the original study to verify generalizability of the framework and benchmark selection strategy
2. Conduct cross-dataset validation by testing model performance on similar tasks across multiple benchmarks to confirm whether observed weaknesses are consistent or dataset-specific
3. Perform real-world deployment testing of models identified as strong in specific capabilities to validate whether benchmark performance correlates with practical effectiveness