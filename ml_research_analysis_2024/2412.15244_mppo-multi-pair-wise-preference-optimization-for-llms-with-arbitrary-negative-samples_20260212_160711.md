---
ver: rpa2
title: 'MPPO: Multi Pair-wise Preference Optimization for LLMs with Arbitrary Negative
  Samples'
arxiv_id: '2412.15244'
source_url: https://arxiv.org/abs/2412.15244
tags:
- preference
- optimization
- training
- responses
- mppo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MPPO, a preference optimization method for
  LLMs that avoids the need for a reference model and better utilizes sparse multi-response
  preference data. Instead of relying on reward modeling or a reference model, MPPO
  fits the reward function using the average likelihood of model responses.
---

# MPPO: Multi Pair-wise Preference Optimization for LLMs with Arbitrary Negative Samples

## Quick Facts
- arXiv ID: 2412.15244
- Source URL: https://arxiv.org/abs/2412.15244
- Reference count: 3
- Key result: MPPO achieves state-of-the-art results on MT-Bench and outperforms DPO and ORPO on Arena-Hard

## Executive Summary
This paper introduces MPPO, a preference optimization method for LLMs that avoids the need for a reference model and better utilizes sparse multi-response preference data. Instead of relying on reward modeling or a reference model, MPPO fits the reward function using the average likelihood of model responses. The authors compare three implementation approaches—Point-wise, Pair-wise, and List-wise—and find that the Pair-wise approach, specifically the Pair-Multi-N-Merge variant, performs best. MPPO achieves state-of-the-art results on MT-Bench and outperforms existing methods like DPO and ORPO on Arena-Hard, demonstrating superior effectiveness in aligning LLMs with human preferences.

## Method Summary
MPPO (Multi Pair-wise Preference Optimization) is a preference optimization method for LLMs that uses the average likelihood of model responses as the reward function, eliminating the need for a separate reward model or reference model. The method processes sparse multi-response preference data where multiple model-generated responses are scored for each query. Three implementation approaches are explored: Point-wise (using absolute scores), Pair-wise (comparing two responses at a time), and List-wise (optimizing full rankings). The Pair-wise approach, particularly the Pair-Multi-N-Merge variant, achieves the best performance by designating the highest-scoring response as positive and treating all others as negative samples for optimization.

## Key Results
- MPPO achieves state-of-the-art performance on MT-Bench benchmark
- Outperforms DPO and ORPO on Arena-Hard benchmark with higher win rates
- Pair-wise implementation (Pair-Multi-N-Merge) shows superior performance compared to Point-wise and List-wise approaches
- Effectively handles sparse preference data with multiple responses per query

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using the average likelihood of model responses as the reward function enables preference optimization without requiring a separate reward model or reference model.
- Mechanism: MPPO approximates the reward function as the geometric mean of token likelihoods in a response, then directly optimizes the policy to maximize this reward. This bypasses the need to train a reward model or maintain a reference model during fine-tuning.
- Core assumption: The average likelihood of responses correlates with human preference judgments, even in sparse data scenarios.
- Evidence anchors:
  - [abstract] "MPPO fits the reward function using the average likelihood of model responses"
  - [section 3.2] "We proposed MPPO, which uses the average likelihood fitting of the model's responses as the reward function"
  - [corpus] Weak evidence - related papers focus on other preference optimization methods but don't directly support this likelihood-as-reward claim
- Break condition: If the correlation between average likelihood and human preferences breaks down (e.g., in tasks where likelihood doesn't capture semantic quality), the method would fail to optimize for true preferences.

### Mechanism 2
- Claim: The Pair-wise implementation approach achieves better performance than Point-wise or List-wise methods for sparse preference data.
- Mechanism: Pair-wise methods compare two responses at a time, using their scores to create a preference signal. This focuses optimization on pairwise preference information rather than absolute scores or full rankings.
- Core assumption: Pairwise preference information is more informative and stable than absolute scores or full rankings when data is sparse.
- Evidence anchors:
  - [abstract] "Through a comparison of Point-wise, Pair-wise, and List-wise implementations, we found that the Pair-wise approach achieves the best performance"
  - [section 5.2] "the Pair-wise method stands out, outperforming the List-wise and Point-wise methods in both MT-Bench and Arena-hard benchmarks"
  - [corpus] No direct support - corpus neighbors don't discuss these specific implementation tradeoffs
- Break condition: If pairwise comparisons become unreliable due to ambiguous preferences or if the ranking information contains valuable signals that pairwise methods discard.

### Mechanism 3
- Claim: In sparse data scenarios, focusing on a single optimal response (Pair-MNM) is more effective than considering multiple optimal responses through sampling (Pair-MCM).
- Mechanism: Pair-MNM designates the highest-scoring response as positive and treats all others as negative, creating a clear optimization target. Pair-MCM samples pairs of responses and adjusts suppression intensity based on relative scores, which can lead to inconsistent optimization signals.
- Core assumption: A single clear positive example provides more stable and effective optimization signals than multiple competing positive examples in sparse data.
- Evidence anchors:
  - [section 5.3] "Our experiments show that Pair-MCM does not perform better in sparse data scenarios. Thus, for multiple-answer preference optimization in such contexts, focusing on a single optimal response (Pair-MNM) and suppressing other samples is more effective"
  - [corpus] No direct support - corpus neighbors don't discuss this specific comparison
- Break condition: If the single highest-scoring response is not actually the best response, or if multiple responses are equally good and should be treated as positive examples.

## Foundational Learning

- Concept: Bradley-Terry ranking model
  - Why needed here: MPPO and many preference optimization methods use the Bradley-Terry model to convert reward differences into preference probabilities
  - Quick check question: What is the mathematical form of the Bradley-Terry model and how does it relate reward differences to preference probabilities?

- Concept: Geometric mean of token likelihoods
  - Why needed here: MPPO uses the geometric mean of token likelihoods as the reward function, which requires understanding how this metric behaves compared to arithmetic means
  - Quick check question: How does the geometric mean of token likelihoods differ from the arithmetic mean, and why might it be more appropriate for measuring response quality?

- Concept: Sparse preference data scenarios
  - Why needed here: The paper focuses on optimizing with sparse data (multiple responses per query but not dense pairwise comparisons), which is different from typical preference optimization setups
  - Quick check question: What distinguishes sparse preference data from dense preference data, and how does this affect the choice of optimization strategy?

## Architecture Onboarding

- Component map: Policy model (LLM) -> Average likelihood computation -> Implementation-specific processing (Pair-MNM) -> Bradley-Terry ranking loss -> Policy update
- Critical path: (1) Load preference dataset with multiple responses per query and their scores, (2) For each training batch, compute average likelihoods for each response, (3) Apply the chosen implementation (e.g., Pair-MNM) to create preference pairs, (4) Compute the optimization loss using the Bradley-Terry model, (5) Update the policy model parameters.
- Design tradeoffs: MPPO trades the stability and expressiveness of a learned reward model for computational efficiency and simplicity. The geometric mean reward function may be less expressive than learned rewards but is more stable and requires no additional training.
- Failure signatures: Poor performance on benchmarks despite reasonable training loss suggests the likelihood-based reward doesn't capture true preferences. High variance in training loss across different batches indicates instability in the sparse data handling.
- First 3 experiments:
  1. Implement Pair-Single on a small subset of UltraFeedback to verify basic functionality and establish baseline performance
  2. Compare Pair-MNS vs Pair-MNM on the same subset to understand the impact of collective vs separate handling of negative samples
  3. Test Point-CE implementation to confirm the hypothesis that absolute scores are less effective than pairwise preferences in sparse data scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MPPO's performance scale with the number of negative samples in sparse data scenarios?
- Basis in paper: [explicit] The authors note that MPPO can utilize any number of negative samples and compare different implementations (Pair-MNS, Pair-MNM, Pair-MCS, Pair-MCM) with varying numbers of responses.
- Why unresolved: The paper provides experimental results for N=3 but does not systematically explore how performance changes with different numbers of negative samples or identify the optimal number.
- What evidence would resolve it: A comprehensive study varying the number of negative samples across different implementations, showing performance curves and identifying the point of diminishing returns.

### Open Question 2
- Question: What is the theoretical relationship between response likelihood and human preference scores in MPPO?
- Basis in paper: [inferred] The authors use response likelihood as a reward function approximation but acknowledge that "there is only a positive correlation, not an exact correspondence, between the responses' average likelihood and reward values."
- Why unresolved: The paper uses likelihood as a proxy without establishing or validating the theoretical connection to human preferences, leaving the choice of this reward function as an empirical assumption.
- What evidence would resolve it: Mathematical analysis or empirical validation demonstrating how response likelihood correlates with human preference scores across different domains and model capabilities.

### Open Question 3
- Question: How does MPPO perform compared to RLHF methods when sufficient dense preference data is available?
- Basis in paper: [explicit] The authors position MPPO as an alternative to RLHF methods that require reward models and reference models, suggesting it is particularly suited for sparse data scenarios.
- Why unresolved: The paper focuses on sparse data scenarios and does not compare MPPO against traditional RLHF approaches when abundant preference data is available, leaving its relative performance unclear.
- What evidence would resolve it: Head-to-head comparisons between MPPO and RLHF methods using the same datasets with varying levels of data density, measuring both performance and computational efficiency.

### Open Question 4
- Question: How do different reward function formulations affect MPPO's performance across various domains?
- Basis in paper: [inferred] The authors use average likelihood as the reward function but acknowledge that "directly approximating these specific values may not be effective" and note the "inherent randomness of scores generated by GPT-4."
- Why unresolved: The paper commits to a specific reward function formulation without exploring alternatives or testing domain-specific adaptations that might be more effective for different types of tasks.
- What evidence would resolve it: Systematic experiments testing alternative reward function formulations (e.g., length-normalized likelihood, context-aware rewards) across multiple domains and measuring their impact on alignment quality.

## Limitations
- The reliance on average likelihood as a reward function may not capture true human preferences in all domains, particularly where likelihood and quality are poorly correlated
- The paper doesn't adequately address performance degradation in extremely sparse data scenarios or when preference scores are noisy
- Limited exploration of how different numbers of negative samples affect performance, with only N=3 tested

## Confidence

High confidence in: The basic implementation of MPPO using average likelihood fitting and its ability to optimize the policy model without requiring a separate reward model. The experimental setup and benchmark comparisons are clearly specified.

Medium confidence in: The claim that Pair-wise approaches outperform Point-wise and List-wise methods specifically for sparse preference data. While experimental results support this, the analysis of why pairwise methods work better in this context could be more rigorous.

Medium confidence in: The effectiveness of Pair-MNM over Pair-MCM in sparse data scenarios. The paper provides experimental evidence but doesn't explore the theoretical reasons for this difference or test intermediate approaches.

## Next Checks
1. Test MPPO on a benchmark where likelihood-based rewards are known to correlate poorly with human quality judgments (e.g., factual question answering or creative writing tasks) to assess the robustness of the average likelihood reward assumption.

2. Conduct an ablation study varying the number of responses per query and the quality of relative scores to determine the minimum data requirements for MPPO to outperform baselines like DPO.

3. Compare MPPO's performance when using arithmetic mean vs geometric mean of token likelihoods as the reward function to quantify the impact of this design choice on optimization effectiveness.