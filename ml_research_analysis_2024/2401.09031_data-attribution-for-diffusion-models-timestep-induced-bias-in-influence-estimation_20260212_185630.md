---
ver: rpa2
title: 'Data Attribution for Diffusion Models: Timestep-induced Bias in Influence
  Estimation'
arxiv_id: '2401.09031'
source_url: https://arxiv.org/abs/2401.09031
tags:
- samples
- training
- influence
- diffusion
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion models pose unique challenges for data attribution due
  to their sequential denoising timesteps and the influence of gradient norms on influence
  estimation. The authors introduce Diffusion-TracIn, an extension of TracIn that
  accounts for timestep dynamics, but find that gradient norms are highly dependent
  on timesteps, leading to bias in influence estimation.
---

# Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation

## Quick Facts
- **arXiv ID:** 2401.09031
- **Source URL:** https://arxiv.org/abs/2401.09031
- **Reference count:** 19
- **Primary result:** Diffusion-ReTrac reduces generally influential samples by 2/3 (from 69% to 20-25%) by mitigating timestep-induced bias in influence estimation

## Executive Summary
Diffusion models pose unique challenges for data attribution due to their sequential denoising timesteps and the influence of gradient norms on influence estimation. The authors introduce Diffusion-TracIn, an extension of TracIn that accounts for timestep dynamics, but find that gradient norms are highly dependent on timesteps, leading to bias in influence estimation. They propose Diffusion-ReTrac, which re-normalizes gradient information to mitigate this bias. Experiments show Diffusion-ReTrac outperforms Diffusion-TracIn in targeted attribution tasks, reducing the number of generally influential samples by 2/3 (from 69% to 20-25% distinct samples). Diffusion-ReTrac also successfully attributes MNIST test samples to MNIST training samples and identifies outliers, while Diffusion-TracIn often incorrectly attributes CIFAR-plane test samples to MNIST outliers due to their large gradient norms.

## Method Summary
The authors extend TracIn to diffusion models by accounting for timestep dynamics through Diffusion-TracIn, then introduce Diffusion-ReTrac with gradient normalization to mitigate timestep-induced bias. They train diffusion models using DDIM architecture on various datasets and implement sparse timestep sampling to approximate full trajectory influence estimation with minimal accuracy loss.

## Key Results
- Diffusion-ReTrac reduces generally influential samples from 69% to 20-25% distinct samples
- Successfully attributes MNIST test samples to MNIST training samples and identifies outliers
- Diffusion-TracIn incorrectly attributes CIFAR-plane test samples to MNIST outliers due to large gradient norms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Diffusion models exhibit timestep-induced bias in influence estimation due to gradient norms being highly dependent on training timesteps.
- **Mechanism:** In diffusion models, each denoising timestep has a distinct loss function. Gradient norms vary significantly across timesteps, with later timesteps (closer to noise) typically having larger norms. Since training samples are randomly assigned timesteps, samples trained on high-norm timesteps appear artificially influential to many test samples.
- **Core assumption:** The variance in gradient norms across timesteps is an artifact of the diffusion training dynamics rather than intrinsic properties of the training samples themselves.
- **Evidence anchors:**
  - [abstract] "samples' loss gradient norms are highly dependent on timestep"
  - [section 4.2] "samples' loss gradient norms tend to increase when the training timestep falls in the later range (towards noise)"
  - [corpus] Weak - corpus papers focus on data attribution broadly but don't directly address timestep-induced bias
- **Break condition:** If gradient norms were primarily determined by sample properties rather than timesteps, the bias would not manifest.

### Mechanism 2
- **Claim:** Re-normalizing gradient information mitigates timestep-induced bias by making influence estimation more localized and sample-specific.
- **Mechanism:** Diffusion-ReTrac normalizes both test sample and training sample gradients by their respective norms. This removes the effect of varying gradient magnitudes across timesteps, ensuring that influence scores reflect true sample relationships rather than timestep artifacts.
- **Core assumption:** The direction of gradients (not their magnitude) contains the meaningful signal for influence estimation.
- **Evidence anchors:**
  - [abstract] "Diffusion-ReTrac as a re-normalized adaptation that enables the retrieval of training samples more targeted to the test sample"
  - [section 5.2] "By Cauchy-Schwarz inequality, it can be noticed from Equation 4 that |TracIn(z,z')|≤∑ηt∥∇ℓ(wt,z')∥∥∇ℓ(wt,z)∥"
  - [section 6.2] "Diffusion-ReTrac retrieves proponents that bear greater visual resemblance to the test samples"
- **Break condition:** If gradient magnitude contains essential information beyond direction, normalization would remove useful signal.

### Mechanism 3
- **Claim:** Sparse timestep sampling can approximate full trajectory influence estimation with minimal accuracy loss.
- **Mechanism:** Instead of computing influence across all T timesteps, Diffusion-TracIn uses a reduced subset of evenly-spaced timesteps. This dramatically reduces computational cost while maintaining high correlation with full-timestep results.
- **Core assumption:** The diffusion trajectory contains redundant information across timesteps, and a sparse sample can capture the essential dynamics.
- **Evidence anchors:**
  - [section 5.1] "We leveragent evenly-spaced timesteps S ={t1,t 2,...,tnt}ranging from 1 to T"
  - [appendix E.1] "Spearman's rank correlation consistently exceeds 0.99 across various sparsity levels"
  - [corpus] Weak - corpus papers don't specifically address timestep sampling efficiency
- **Break condition:** If certain timesteps contain unique, non-redundant information critical for influence estimation, sparse sampling would miss important dynamics.

## Foundational Learning

- **Concept:** Diffusion models and denoising process
  - Why needed here: The paper extends TracIn to diffusion models, which operate over sequential timesteps rather than instantaneous input-output mappings. Understanding the denoising process is essential to grasp why timesteps matter for attribution.
  - Quick check question: In diffusion models, what does each timestep represent in the denoising process?

- **Concept:** Gradient-based influence estimation
  - Why needed here: Both Diffusion-TracIn and Diffusion-ReTrac build on TracIn's approach of using gradients to estimate training sample influence. The paper's innovation is adapting this to handle timestep dynamics.
  - Quick check question: How does TracIn approximate the influence of training samples on test predictions?

- **Concept:** Normalization techniques in machine learning
  - Why needed here: Diffusion-ReTrac introduces normalization to mitigate timestep-induced bias. Understanding when and why normalization helps is crucial for appreciating this design choice.
  - Quick check question: What problem does normalization solve in the context of gradient-based influence estimation?

## Architecture Onboarding

- **Component map:** Trained diffusion model -> Checkpoint storage -> Gradient computation engine -> Timestep sampling module -> Normalization layer -> Influence scoring module

- **Critical path:**
  1. Load trained diffusion model and checkpoints
  2. For each test sample, compute gradients across sampled timesteps
  3. For each training sample, compute gradients at their respective training timesteps
  4. Apply normalization (if using Diffusion-ReTrac)
  5. Aggregate influence scores across checkpoints and timesteps
  6. Rank training samples by influence for each test sample

- **Design tradeoffs:**
  - Computational efficiency vs. accuracy: Sparse timestep sampling reduces computation by ~20x with minimal accuracy loss
  - Sensitivity vs. robustness: Diffusion-TracIn is more sensitive to timestep artifacts but captures more information; Diffusion-ReTrac is more robust but may lose some signal
  - Granularity vs. interpretability: Computing influence at each timestep provides detailed attribution but results in many scores per image

- **Failure signatures:**
  - Generally influential samples appearing for diverse test samples (indicates timestep-induced bias)
  - High correlation between training timestep and influence score (indicates bias)
  - Minimal improvement from normalization (suggests bias isn't timestep-related)
  - Poor precision on targeted attribution tasks (indicates fundamental issues with influence estimation)

- **First 3 experiments:**
  1. Verify timestep-induced bias exists: Compute correlation between training timestep and gradient norm for random samples across checkpoints
  2. Test normalization effectiveness: Compare attribution results with and without normalization on a dataset with known outliers
  3. Validate sparse sampling: Compare influence rankings using full timesteps vs. sparse sampling on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for why later timesteps in diffusion models tend to induce larger gradient norms?
- Basis in paper: [inferred] The authors hypothesize this is related to later timesteps learning structure while earlier timesteps focus on finer details, but note this requires further theoretical exploration.
- Why unresolved: The paper only provides a hypothesis based on previous work about learning dynamics, without a rigorous theoretical explanation for the observed correlation between timesteps and gradient norms.
- What evidence would resolve it: A formal theoretical analysis showing how the diffusion model architecture and training objective lead to systematically larger gradients at certain timesteps would resolve this.

### Open Question 2
- Question: How would Diffusion-ReTrac perform on latent diffusion models like Stable Diffusion?
- Basis in paper: [explicit] The authors mention this as an "intriguing avenue for future research" in the limitations section, noting their methods are designed based on training process which may need modification for latent diffusion models.
- Why unresolved: The paper only evaluates on standard diffusion models using pixel-space representations, not latent diffusion models that operate in compressed latent spaces.
- What evidence would resolve it: Applying Diffusion-ReTrac to a latent diffusion model and comparing attribution results with the original model would show whether the timestep-induced bias persists in latent space.

### Open Question 3
- Question: What is the optimal normalization strategy for balancing between mitigating timestep-induced bias and preserving sample-specific gradient information?
- Basis in paper: [explicit] The authors explore "Guided Normalization" as a middle ground between full normalization and no normalization, but found it didn't show "immensely convincing improvement."
- Why unresolved: The paper settles on simple re-normalization but acknowledges this may involve trade-offs in suppressing important information, without identifying the optimal balance point.
- What evidence would resolve it: Systematic evaluation of different normalization thresholds and strategies on attribution quality metrics (precision, recall, distinctness) would identify the optimal normalization approach.

## Limitations

- The assumption that gradient magnitude contains no meaningful signal beyond direction for influence estimation requires further validation
- Sparse timestep sampling may miss critical dynamics at certain timesteps, particularly for complex attribution tasks
- Experiments focus on image-based diffusion models, leaving uncertainty about applicability to other domains like language models or scientific applications

## Confidence

**High Confidence:** The existence of timestep-induced bias is well-supported by the correlation between training timesteps and gradient norms. The experimental demonstration that Diffusion-TracIn incorrectly attributes CIFAR-plane samples to MNIST outliers due to their large gradient norms provides strong empirical evidence.

**Medium Confidence:** The effectiveness of Diffusion-ReTrac's normalization approach is supported by improved attribution specificity, but the mechanism assumes gradient direction is sufficient for influence estimation. This assumption, while reasonable, requires further validation.

**Low Confidence:** The sparse timestep sampling approach shows high correlation with full-timestep results, but this has only been demonstrated on image datasets. The claim of 20x computational reduction with minimal accuracy loss needs broader validation across different model architectures and data types.

## Next Checks

1. **Gradient Magnitude Validation:** Conduct controlled experiments comparing attribution results when using normalized vs. unnormalized gradients on datasets with known influence patterns. Specifically test whether removing magnitude information impacts the ability to distinguish between genuinely influential samples and timestep artifacts.

2. **Cross-Domain Applicability:** Apply Diffusion-ReTrac to non-image diffusion models (e.g., text-to-image or scientific data) to validate whether timestep-induced bias and the effectiveness of normalization generalize beyond the tested image domains.

3. **Timestep Sensitivity Analysis:** Systematically vary the number of sampled timesteps (beyond the sparse sampling tested) and measure the impact on attribution accuracy for different types of attribution tasks, particularly focusing on edge cases where sparse sampling might fail.