---
ver: rpa2
title: A Study on Effect of Reference Knowledge Choice in Generating Technical Content
  Relevant to SAPPhIRE Model Using Large Language Model
arxiv_id: '2407.00396'
source_url: https://arxiv.org/abs/2407.00396
tags:
- context
- knowledge
- sapphire
- physical
- technical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether retrieval-augmented generation with
  large language models can produce scientifically accurate technical content relevant
  to the SAPPhIRE model of causality. The core method uses retrieval-augmented generation,
  where external scientific knowledge is provided as context to the LLM to generate
  descriptions of physical interactions and their conditions.
---

# A Study on Effect of Reference Knowledge Choice in Generating Technical Content Relevant to SAPPhIRE Model Using Large Language Model

## Quick Facts
- arXiv ID: 2407.00396
- Source URL: https://arxiv.org/abs/2407.00396
- Reference count: 25
- The choice of reference knowledge significantly affects the accuracy of LLM-generated technical content for SAPPhIRE model constructs

## Executive Summary
This study investigates whether retrieval-augmented generation (RAG) with large language models can produce scientifically accurate technical content relevant to the SAPPhIRE model of causality. The research focuses on generating descriptions of physical interactions and their conditions for two scientific phenomena (vaporization and Seebeck effect) using RAG to suppress hallucinations. The core finding is that the choice of reference knowledge context significantly impacts response accuracy, with some contexts producing better results than others. ANOVA tests confirm statistical significance (p < 0.05) across multiple experimental conditions, demonstrating that carefully selected reference knowledge is crucial for generating accurate technical content for SAPPhIRE model constructs.

## Method Summary
The study employs retrieval-augmented generation using LangChain API with Chroma vector database to provide context to ChatGPT (GPT-4-Turbo, temperature 0.0) for generating technical content about scientific phenomena. Three numerical experiments compare responses with and without context, using different contexts as ground truth, and varying detail levels in reference knowledge. Cosine similarity scores between LLM responses and ground truth contexts serve as the accuracy metric, with ANOVA tests determining statistical significance of differences between response groups. The method focuses on two scientific phenomena and three reference contexts per phenomenon to evaluate how context choice affects grounded-ness of generated content.

## Key Results
- ANOVA tests show p < 0.05 significance for all experiments, indicating context choice significantly affects response accuracy
- Some reference knowledge contexts produce significantly better similarity scores than others for the same phenomena
- The choice of ground truth context itself affects similarity score outcomes, highlighting the importance of accurate reference selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation (RAG) significantly reduces hallucination in LLM outputs by providing grounding context.
- Mechanism: By embedding external scientific knowledge into the LLM prompt, RAG ensures responses are anchored to verified information rather than generated from the model's internal knowledge alone.
- Core assumption: The reference knowledge used in RAG is accurate and relevant to the SAPPhIRE construct being generated.
- Evidence anchors:
  - [abstract] "presents a method for hallucination suppression using Retrieval Augmented Generating with LLM"
  - [section] "Retrieval Augmented Generation (RAG) is a popular and effective method employed in multiple applications where a domain-specific knowledge base is used to ground an LLM's responses"
  - [corpus] Weak - corpus doesn't directly address RAG effectiveness for SAPPhIRE models
- Break condition: If the reference knowledge is inaccurate or irrelevant, the grounding effect fails and hallucinations may increase.

### Mechanism 2
- Claim: The choice of reference knowledge context significantly affects the accuracy of generated technical content.
- Mechanism: Different descriptions or levels of detail in reference knowledge produce varying similarity scores between LLM responses and ground truth, indicating some contexts generate more accurate content.
- Core assumption: One reference knowledge context is more scientifically accurate than others and serves as ground truth.
- Evidence anchors:
  - [abstract] "the selection of reference knowledge used in providing context to the LLM for generating the technical content is very important"
  - [section] "The result from this research shows that the selection of reference knowledge used in providing context to the LLM for generating the technical content is very important"
  - [section] "The outcome of this first part of the research shows that the choice of reference knowledge matters when generating LLM responses for SAPPhIRE construct"
- Break condition: If all reference knowledge contexts are equally accurate, the choice would not significantly affect output accuracy.

### Mechanism 3
- Claim: ANOVA tests can detect statistically significant differences in LLM response accuracy based on reference knowledge choice.
- Mechanism: By comparing similarity scores between response groups generated with different contexts, ANOVA identifies whether context choice creates meaningful differences in accuracy.
- Core assumption: The similarity score metric reliably measures grounded-ness of LLM responses to scientific truth.
- Evidence anchors:
  - [section] "A one-way ANOVA test was done to check the effect of 'context' knowledge in LLM responses"
  - [section] "Since the p-value in all the test cases in Numerical experiment 1 is less than 0.05...we can safely reject the null hypothesis"
  - [section] "The ANOVA test results of Numerical experiment 1 show that the 'context' used in the RAG method influences the responses generated by the LLM"
- Break condition: If the similarity score metric doesn't accurately reflect accuracy, ANOVA results would be misleading.

## Foundational Learning

- Concept: SAPPhIRE model of causality
  - Why needed here: The entire study focuses on generating technical content specifically relevant to SAPPhIRE model constructs
  - Quick check question: What are the seven layers of abstraction in the SAPPhIRE model?

- Concept: Retrieval-augmented generation (RAG) methodology
  - Why needed here: The study uses RAG as the primary method for hallucination suppression and context provision
  - Quick check question: How does RAG differ from standard prompt engineering in LLM applications?

- Concept: Cosine similarity for document embedding comparison
  - Why needed here: The study uses cosine similarity scores to measure how grounded LLM responses are to reference knowledge
  - Quick check question: What range of values does cosine similarity produce, and what do extreme values indicate?

## Architecture Onboarding

- Component map: User query → RAG context retrieval → LLM prompt construction → response generation → similarity scoring → statistical analysis
- Critical path: User query → RAG context retrieval → LLM prompt construction → response generation → similarity scoring → statistical analysis
- Design tradeoffs: The choice between using detailed vs. concise reference knowledge contexts affects both accuracy and computational efficiency; more detailed contexts may improve accuracy but increase processing time
- Failure signatures: Low similarity scores across all contexts may indicate the LLM is not properly grounded; inconsistent ANOVA results may suggest issues with the ground truth selection or similarity scoring methodology
- First 3 experiments:
  1. Test basic RAG functionality by generating responses with and without context for a simple scientific phenomenon
  2. Compare similarity scores across different reference knowledge contexts for the same phenomenon
  3. Vary the level of detail in reference knowledge and measure the impact on response accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on only two scientific phenomena, limiting external validity
- Similarity metric relies on cosine similarity of document embeddings without validation against human accuracy judgments
- Ground truth contexts are assumed correct but selection criteria are not fully transparent

## Confidence
- RAG reduces hallucinations: **Medium** - supported by mechanism but not directly tested against baseline without RAG
- Context choice affects accuracy: **High** - clear statistical significance with p < 0.05 across multiple experiments
- Similarity scores measure grounded-ness: **Low-Medium** - metric is used but not validated against human judgment of scientific accuracy

## Next Checks
1. **Ground truth validation**: Have domain experts evaluate whether the reference contexts designated as "ground truth" are indeed the most scientifically accurate representations of the phenomena, and test alternative ground truth selections.

2. **Cross-phenomenon replication**: Apply the same experimental design to at least 5-10 additional scientific phenomena spanning different domains to assess generalizability of the context choice effect.

3. **Human accuracy benchmark**: Compare similarity scores against human expert ratings of response accuracy to validate whether the metric truly captures scientific correctness rather than just semantic similarity.