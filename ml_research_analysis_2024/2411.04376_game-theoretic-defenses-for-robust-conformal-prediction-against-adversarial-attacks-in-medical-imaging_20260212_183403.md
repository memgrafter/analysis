---
ver: rpa2
title: Game-Theoretic Defenses for Robust Conformal Prediction Against Adversarial
  Attacks in Medical Imaging
arxiv_id: '2411.04376'
source_url: https://arxiv.org/abs/2411.04376
tags:
- uni00000011
- prediction
- uni00000019
- uni00000036
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of maintaining valid uncertainty
  quantification under adversarial attacks in medical imaging. The authors propose
  a game-theoretic framework that integrates conformal prediction with specialized
  defensive models trained against known attack types.
---

# Game-Theoretic Defenses for Robust Conformal Prediction Against Adversarial Attacks in Medical Imaging

## Quick Facts
- arXiv ID: 2411.04376
- Source URL: https://arxiv.org/abs/2411.04376
- Reference count: 40
- This paper addresses maintaining valid uncertainty quantification under adversarial attacks in medical imaging using a game-theoretic framework.

## Executive Summary
This paper proposes a game-theoretic framework that integrates conformal prediction with specialized defensive models to maintain valid uncertainty quantification under adversarial attacks in medical imaging. The approach uses maximum and minimum quantile thresholds across multiple attack types to ensure coverage under unknown threats, while framing the interaction between attacker and defender as a zero-sum game to identify optimal defensive strategies. Experiments on three MedMNIST datasets demonstrate that the proposed method maintains high empirical coverage (typically 90%+) while minimizing prediction set sizes, effectively preserving conformal prediction's validity guarantees even when calibration and test data are simultaneously attacked.

## Method Summary
The methodology involves training specialized defensive models against specific attack types using adversarially perturbed training data, then employing conservative quantile thresholding across attacks for unknown threats. For known attacks, defensive models are trained on attacked training data and conformal prediction is applied using attack-specific thresholds. For unknown attacks, maximum quantile thresholds across all attack types are used to construct prediction sets that maintain coverage. The framework formulates the defender-attacker interaction as a zero-sum game, constructing a payoff matrix where the defender selects from multiple defensive models and the attacker selects from multiple attack types. Nash equilibrium is computed using NashPy to identify optimal mixed defense strategies that minimize the worst-case prediction set size while preserving coverage guarantees.

## Key Results
- VRCP-I achieves the smallest prediction set sizes across all attack scenarios and datasets
- VRCP-C reaches the highest empirical coverage (consistently above 90%) while maintaining reasonable prediction set sizes
- APS demonstrates superior SSCV (Soft Set Conditional Validity) performance compared to other methods
- Game-theoretic analysis reveals optimal defensive strategies often converge to a single robust model rather than uniform combinations
- The framework effectively preserves conformal prediction's validity guarantees even when calibration and test data are simultaneously attacked

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The game-theoretic framework ensures robustness by treating the defender-attacker interaction as a zero-sum game, optimizing the worst-case prediction set size.
- Mechanism: By constructing a payoff matrix where the defender selects from multiple defensive models and the attacker selects from multiple attack types, the framework identifies the Nash equilibrium that minimizes the maximum prediction set size while maintaining coverage guarantees.
- Core assumption: The calibration and test sets are exchangeable, and the adversarial attacks and defensive models are properly defined with known attack types.
- Evidence anchors:
  - [abstract] "The game-theoretic analysis reveals that the optimal defensive strategy often converges to a singular robust model, outperforming uniform and simple strategies across all evaluated datasets."
  - [section] "By formulating the interplay between the defender and the adversary as a zero-sum game, we rigorously analyze the worst-case scenario—namely, the most adverse adversarial perturbation that maximally impacts prediction set size an adversary can select—and derive optimal mixed defense strategies that minimize prediction set ambiguity while preserving coverage."
  - [corpus] Weak - The corpus contains papers on adversarial robustness and conformal prediction but lacks direct discussion of game-theoretic frameworks for defensive strategy optimization.
- Break condition: The exchangeability assumption fails, or the attacker can access information about the defensive models, breaking the strategic uncertainty.

### Mechanism 2
- Claim: Conservative thresholding using maximum quantile thresholds across multiple attacks ensures coverage under unknown attacks.
- Mechanism: For each defensive model, the framework computes quantile thresholds under all possible attack scenarios on the calibration set, then selects the maximum threshold to construct prediction sets that maintain coverage regardless of the specific attack type.
- Core assumption: The set of potential attacks is known and bounded, and the calibration set can be attacked with each type to estimate appropriate thresholds.
- Evidence anchors:
  - [abstract] "Our approach uses maximum and minimum quantile thresholds across multiple attacks to ensure coverage under unknown threats."
  - [section] "By evaluating multiple potential adversarial attacks and selecting the most stringent quantile threshold, we ensure that the prediction sets remain valid even when the exact nature of the attack is unknown."
  - [corpus] Weak - While the corpus discusses adversarial attacks and defenses, it does not specifically address conservative thresholding strategies for unknown attacks in conformal prediction.
- Break condition: The actual attack is outside the assumed set of possible attacks, or the conservative thresholding becomes too restrictive, leading to impractical prediction set sizes.

### Mechanism 3
- Claim: Specialized defensive models trained against specific attack types improve prediction set efficiency while maintaining coverage.
- Mechanism: Separate defensive models are trained for each attack type using adversarially perturbed training data, allowing each model to learn robust features specific to its corresponding attack.
- Core assumption: The training data can be appropriately partitioned and attacked to create effective defensive models, and each defensive model can generalize to its specific attack type.
- Evidence anchors:
  - [abstract] "Our methodology involves training specialized defensive models against specific attack types and employing maximum and minimum classifiers to aggregate defenses effectively."
  - [section] "To tackle these questions, we develop specialized models by partitioning the training data and tailoring them to distinct attack types."
  - [corpus] Weak - The corpus discusses adversarial training but does not specifically address the use of specialized defensive models for different attack types in the context of conformal prediction.
- Break condition: The defensive models overfit to their specific attack types and fail to generalize, or the computational overhead of maintaining multiple models becomes prohibitive.

## Foundational Learning

- Concept: Conformal Prediction (CP) fundamentals
  - Why needed here: The entire framework builds on CP's coverage guarantees and prediction set construction methods
  - Quick check question: What is the key assumption that allows CP to provide distribution-free coverage guarantees?

- Concept: Adversarial attack types and defenses
  - Why needed here: The framework requires understanding different attack methods (FGSM, PGD, SPSA, CW) and corresponding defensive strategies
  - Quick check question: How do gradient-based attacks differ from gradient-free attacks in terms of computational requirements and effectiveness?

- Concept: Game theory and Nash equilibrium
  - Why needed here: The optimal defensive strategy is derived from a zero-sum game formulation between attacker and defender
  - Quick check question: What is the difference between pure and mixed strategies in the context of this game-theoretic framework?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline for medical imaging datasets -> Adversarial attack generators (FGSM, PGD, SPSA, CW) -> Defensive model training modules for each attack type -> Conformal prediction modules (APS, RSCP, VRCP variants) -> Game-theoretic optimization engine for strategy selection -> Evaluation and metrics computation (Coverage, Size, SSCV)

- Critical path:
  1. Load and preprocess medical imaging data
  2. Train normal and adversarial defensive models
  3. Generate calibration and evaluation datasets under various attacks
  4. Compute quantile thresholds and prediction set sizes
  5. Construct payoff matrix and solve for Nash equilibrium
  6. Apply optimal defensive strategy to test data

- Design tradeoffs:
  - Multiple defensive models vs. single robust model: Multiple models provide better specialization but increase computational overhead
  - Conservative thresholding vs. efficiency: Larger thresholds ensure coverage but produce less informative prediction sets
  - Game-theoretic optimization complexity vs. practical deployment: Finding Nash equilibrium can be computationally intensive for large strategy spaces

- Failure signatures:
  - Coverage drops below 1-α: Indicates exchangeability assumption violation or insufficient defensive capability
  - Prediction set sizes become excessively large: Suggests overly conservative thresholding or ineffective defensive models
  - Nash equilibrium solution favors uniform/random strategies: May indicate poorly defined payoff matrix or insufficient strategy differentiation

- First 3 experiments:
  1. Verify baseline CP coverage and size metrics on clean data across all datasets
  2. Test defensive models against their corresponding known attacks to confirm specialization
  3. Validate the game-theoretic framework by comparing prediction set sizes under optimal vs. random defensive strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the game-theoretic defensive strategy scale to continuous action spaces for both attacker and defender?
- Basis in paper: [explicit] The paper states "we have simplified strategy selection in our current work by restricting both attackers and defenders to a discrete action space" and mentions future work could "explore dynamic adaptation to a broader range of attacks"
- Why unresolved: The current implementation uses a discrete payoff matrix and enumerative approach to find Nash equilibria, which becomes computationally infeasible for continuous strategies
- What evidence would resolve it: Demonstration of the framework operating with continuous strategy spaces, showing computational tractability and performance comparison to the discrete approach

### Open Question 2
- Question: What is the performance impact when applying this framework to larger, more complex medical imaging datasets beyond MedMNIST?
- Basis in paper: [inferred] The paper acknowledges "future work may explore...extend the framework to other medical imaging datasets" and the current experiments are limited to relatively small 28×28 pixel MedMNIST datasets
- Why unresolved: The computational complexity of training multiple defensive models against multiple attack types, and the game-theoretic optimization, may not scale well to larger datasets with higher resolution images
- What evidence would resolve it: Experimental results showing coverage, prediction set sizes, and computational requirements on standard larger medical imaging datasets like NIH Chest X-rays or CheXpert

### Open Question 3
- Question: How do uncertainty-reducing adversarial training methods [56, 57, 58] perform when integrated with this conformal prediction framework?
- Basis in paper: [explicit] The discussion section mentions "future work may...investigate the integration of uncertainty-reducing adversarial training methods [56, 57, 58] to further enhance CP efficiency"
- Why unresolved: The paper acknowledges this as a potential future direction but does not evaluate any of these specific methods within their framework
- What evidence would resolve it: Implementation and evaluation showing improved prediction set sizes or SSCV metrics when incorporating these uncertainty-reducing techniques compared to standard adversarial training

## Limitations
- The framework assumes attacks are limited to five specific methods, which may not capture the full range of real-world adversarial strategies
- The zero-sum game formulation assumes complete information about attack capabilities and defensive model performance, which may not hold in practical scenarios
- The conservative thresholding approach, while ensuring coverage, may lead to overly large prediction sets that reduce practical utility

## Confidence
**High Confidence**: The core game-theoretic framework and its mathematical formulation are well-grounded. The experimental results showing VRCP-I achieving smallest prediction set sizes and VRCP-C reaching highest coverage are consistently supported across all three datasets.

**Medium Confidence**: The generalization of results to truly unknown attacks beyond the five tested types remains uncertain. While the conservative thresholding approach provides theoretical guarantees, its practical effectiveness against novel attack strategies is not fully validated.

**Low Confidence**: The assumption that calibration and test sets remain exchangeable under adversarial attacks is a critical but unverified assumption. The framework's performance when attackers can observe and adapt to defensive strategies is not explored.

## Next Checks
1. Test the framework against adaptive attacks where the adversary observes and responds to the defensive strategy in real-time, rather than static attack types.

2. Evaluate the computational overhead of maintaining multiple defensive models versus a single robust model across varying dataset sizes and model complexities.

3. Validate the exchangeability assumption by systematically introducing distribution shifts between calibration and test sets while maintaining adversarial perturbations.