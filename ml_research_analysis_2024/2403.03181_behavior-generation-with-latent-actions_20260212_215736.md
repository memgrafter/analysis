---
ver: rpa2
title: Behavior Generation with Latent Actions
arxiv_id: '2403.03181'
source_url: https://arxiv.org/abs/2403.03181
tags:
- vq-bet
- behavior
- action
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VQ-BeT, a behavior generation model that
  leverages vector quantization to handle multimodal continuous actions. Unlike prior
  methods that use k-means clustering, VQ-BeT employs a hierarchical residual vector
  quantization module to discretize actions, enabling better scalability and gradient
  flow.
---

# Behavior Generation with Latent Actions

## Quick Facts
- arXiv ID: 2403.03181
- Source URL: https://arxiv.org/abs/2403.03181
- Authors: Seungjae Lee; Yibin Wang; Haritheja Etukuru; H. Jin Kim; Nur Muhammad Mahi Shafiullah; Lerrel Pinto
- Reference count: 35
- Primary result: VQ-BeT outperforms state-of-the-art baselines while being 5× faster in inference

## Executive Summary
This paper introduces VQ-BeT, a behavior generation model that leverages vector quantization to handle multimodal continuous actions. Unlike prior methods that use k-means clustering, VQ-BeT employs a hierarchical residual vector quantization module to discretize actions, enabling better scalability and gradient flow. The model combines this discretization with a transformer architecture to predict action sequences from observations, supporting both conditional and unconditional tasks. Across seven simulated environments (manipulation, locomotion, autonomous driving) and real-robot experiments, VQ-BeT outperforms state-of-the-art baselines like BeT and Diffusion Policies in task success rates while being 5× faster in inference. The approach demonstrates strong performance in capturing behavior diversity and generalizing to long-horizon real-world tasks.

## Method Summary
VQ-BeT uses hierarchical residual vector quantization to discretize continuous actions into hierarchical discrete codes, which are then predicted by a transformer architecture. The model employs a MinGPT backbone to predict both primary and secondary codes from observation sequences, with an offset head to fine-tune the quantized actions for better fidelity. The approach uses weighted focal loss to balance learning between primary and secondary code predictions. The method supports both conditional and unconditional behavior generation tasks across manipulation, locomotion, and autonomous driving domains.

## Key Results
- VQ-BeT outperforms BeT and Diffusion Policies across seven simulated environments
- Achieves 5× faster inference than Diffusion Policies while maintaining or improving performance
- Demonstrates successful real-robot experiments on door opening, drawer manipulation, and block stacking tasks
- Shows robustness to codebook size variations, maintaining performance despite large increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical residual vector quantization enables better scalability for high-dimensional action spaces compared to k-means.
- Mechanism: The residual quantization structure decomposes the continuous action space into coarse and fine-grained representations. The primary codebook handles broad clustering while secondary layers refine within those clusters, avoiding the curse of dimensionality that plagues flat k-means approaches.
- Core assumption: The action space exhibits hierarchical structure where coarse modes can be captured first, followed by local refinements.
- Evidence anchors:
  - [abstract]: "Unlike prior methods that use k-means clustering, VQ-BeT employs a hierarchical residual vector quantization module to discretize actions, enabling better scalability and gradient flow."
  - [section 2.3]: "Residual VQ is a multi-stage vector quantizer which replaces each embedding of vanilla VQ-VAE with the sum of vectors from a finite layers of codebooks."

### Mechanism 2
- Claim: Single-pass inference provides 5× speedup over diffusion-based methods while maintaining performance.
- Mechanism: Unlike diffusion models that require multiple denoising steps, VQ-BeT directly predicts quantized action tokens in one forward pass. The autoregressive prediction of codes allows immediate action reconstruction without iterative refinement.
- Core assumption: The action prediction task can be solved accurately in a single forward pass without the gradual refinement that diffusion provides.
- Evidence anchors:
  - [abstract]: "Importantly, we demonstrate VQ-BeT's improved ability to capture behavior modes while accelerating inference speed 5× over Diffusion Policies."
  - [section 4.4]: "Unconditional C-BeT C-BESO CFG-BESO VQ-BeT Single step 22.6ms 25.9ms 41.7ms 22.8ms Multi step ✗ ✗ ✗ 23.3ms"

### Mechanism 3
- Claim: Focal loss weighting between primary and secondary codes enables better capture of multimodal behavior distributions.
- Mechanism: The focal loss with adjustable weight β between primary and secondary code prediction allows the model to balance between capturing coarse modes and fine-grained variations. This weighting leverages prior knowledge about the relative importance of different quantization levels.
- Core assumption: The relative importance of primary versus secondary codes varies across environments and datasets, requiring adaptive weighting.
- Evidence anchors:
  - [section 3.3]: "We adjust the weights between the primary code and secondary code learning losses, leveraging our priors about the latent space."
  - [section 4.6]: "We note that performance-wise, not using a residual VQ layer has a significant negative impact... A similar drop in performance shows up when we weigh the two VQ layers equally by setting β = 1"

## Foundational Learning

- Concept: Vector quantization for continuous action discretization
  - Why needed here: Actions are continuous vectors that are multimodal and high-dimensional, making direct prediction difficult. Vector quantization converts them into discrete tokens that transformers can handle effectively.
  - Quick check question: How does vector quantization differ from simple binning approaches like k-means?

- Concept: Transformer sequence modeling for behavior prediction
  - Why needed here: Behavior data exhibits temporal dependencies and correlations. Transformers can capture long-range dependencies in observation-action sequences better than recurrent architectures.
  - Quick check question: What advantage does the transformer's attention mechanism provide for modeling temporal behavior patterns?

- Concept: Hierarchical representation learning
  - Why needed here: The action space may have multiple levels of abstraction - coarse modes and fine-grained variations. Hierarchical quantization captures this structure more efficiently than flat representations.
  - Quick check question: Why might a single-layer vector quantization be insufficient for complex action spaces?

## Architecture Onboarding

- Component map: Observation → MinGPT → Code Prediction → Residual VQ Decoder → Action
- Critical path: Observation → MinGPT → Code Prediction → Residual VQ Decoder → Action
- Design tradeoffs:
  - Single-pass vs. multi-pass: VQ-BeT sacrifices the gradual refinement of diffusion for speed
  - Codebook size vs. expressivity: Larger codebooks capture more modes but require more data
  - Primary vs. secondary code weighting: Balancing coarse mode capture with fine details
- Failure signatures:
  - Poor mode coverage: May indicate insufficient codebook size or inappropriate β weighting
  - Unstable training: Could suggest learning rate issues or problems with code prediction balance
  - Low fidelity: May indicate offset head is not effective or decoder needs adjustment
- First 3 experiments:
  1. Train VQ-BeT on a simple environment (like BlockPush) with default hyperparameters to verify basic functionality
  2. Compare performance with and without the offset head to understand its impact on action fidelity
  3. Test different β values for code prediction weighting to find optimal balance for your specific dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does VQ-BeT's performance scale with dataset size and diversity beyond the evaluated benchmarks?
- Basis in paper: [inferred] The paper mentions potential applications in scaling to larger datasets but does not provide empirical results.
- Why unresolved: The current experiments focus on specific environments; scaling effects are only speculated.
- What evidence would resolve it: Systematic evaluation of VQ-BeT on progressively larger and more diverse datasets, measuring performance metrics and computational costs.

### Open Question 2
- Question: What are the trade-offs between codebook size and model performance in VQ-BeT?
- Basis in paper: [explicit] The paper includes an ablation study on codebook size but notes that performance remains robust despite large increases.
- Why unresolved: The study shows robustness but does not identify optimal codebook sizes or explore the impact of extremely large codebooks.
- What evidence would resolve it: Extensive experiments varying codebook sizes across diverse tasks, analyzing performance, entropy, and computational efficiency.

### Open Question 3
- Question: How does VQ-BeT's performance compare to other generative models in long-horizon tasks with real-world noise?
- Basis in paper: [explicit] The paper demonstrates VQ-BeT's effectiveness in real-world robot experiments but does not compare it to other generative models like diffusion policies in the same settings.
- Why unresolved: The real-world experiments focus on VQ-BeT vs. simpler baselines, lacking direct comparison to other generative models.
- What evidence would resolve it: Head-to-head comparison of VQ-BeT with diffusion policies and other generative models in long-horizon real-world tasks, measuring success rates and robustness to noise.

## Limitations
- Evaluation relies heavily on comparisons against limited baselines (BeT and Diffusion Policies) across seven specific environments
- Real-robot experiments involve relatively simple tasks that may not fully capture complex real-world challenges
- Paper does not thoroughly explore sensitivity to codebook sizes or investigate performance scaling with action dimensionality
- Lacks direct ablation studies comparing hierarchical VQ to k-means discretization approaches

## Confidence

- High Confidence: The computational speedup claims (5× faster inference) are directly supported by measured inference times in the paper and represent a straightforward empirical finding.
- Medium Confidence: The performance improvements over baselines are reasonably well-supported through task success metrics, though the limited scope of comparisons and potential variability across different environments introduces some uncertainty.
- Low Confidence: The claim about hierarchical residual VQ being fundamentally superior to k-means for scalability is primarily theoretical, as the paper does not provide direct ablation studies comparing these specific discretization approaches.

## Next Checks

1. Conduct direct ablation studies comparing VQ-BeT with standard k-means discretization on identical architectures to empirically validate the scalability claims.
2. Test VQ-BeT on more complex, long-horizon real-world tasks beyond the current simple manipulation tasks to better assess real-world applicability.
3. Perform sensitivity analysis on codebook sizes and β weighting parameters to understand their impact on performance across different environments and action space complexities.