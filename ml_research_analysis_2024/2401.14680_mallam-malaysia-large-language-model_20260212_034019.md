---
ver: rpa2
title: MaLLaM -- Malaysia Large Language Model
arxiv_id: '2401.14680'
source_url: https://arxiv.org/abs/2401.14680
tags:
- language
- dataset
- malaysian
- data
- mallam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MaLLaM is a family of 1.1B, 3B, and 5B parameter language models
  pretrained from scratch on a 349GB Malaysian dataset (90B tokens) using a custom
  BPE tokenizer. The models were trained on 80 A100 GPUs using the Ray distributed
  framework.
---

# MaLLaM -- Malaysia Large Language Model

## Quick Facts
- arXiv ID: 2401.14680
- Source URL: https://arxiv.org/abs/2401.14680
- Authors: Husein Zolkepli; Aisyah Razak; Kamarul Adha; Ariff Nazhan
- Reference count: 7
- Primary result: Family of 1.1B, 3B, and 5B parameter Malay language models pretrained from scratch on 349GB Malaysian dataset

## Executive Summary
MaLLaM is a family of large language models (1.1B, 3B, and 5B parameters) specifically designed for the Malaysian language. The models were pretrained from scratch on a 349GB dataset containing 90 billion tokens sourced from various Malaysian contexts including Wikipedia, government documents, social media, journals, and research papers. Using a custom BPE tokenizer trained on 85GB of deduplicated text, the models were trained on 80 A100 GPUs using the Ray distributed framework. Instruction-tuned versions were fine-tuned on a 20k-token Malay instruction dataset and demonstrated proficiency in multi-turn Malay QA, coding tasks, and recipe generation.

## Method Summary
The models were trained using causal language modeling with a Mistral architecture, context length 4096, batch size 24, learning rate 1e-4, and bfloat16 precision. The training utilized 80 A100 80GB GPUs through the Ray distributed framework. A custom BPE tokenizer with 32K vocabulary size was trained on 85GB deduplicated text. After pretraining for one epoch, instruction-tuned models were fine-tuned with 20480 context length, batch size 6, and learning rate 2e-5 on a 20k-token Malay instruction dataset.

## Key Results
- MaLLaM-5B achieved 27.79% accuracy on tatabahasa test set 1-shot tasks, competitive with existing models
- Instruction-tuned MaLLaM models demonstrated proficiency in multi-turn Malay QA, coding tasks, and recipe generation
- Models address the lack of native Malay LLMs and are available under a non-commercial license

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining from scratch on native Malay corpus removes English-centric bias better than fine-tuning English models
- Mechanism: Model learns language patterns directly from Malay text without inheriting English token structures or semantic priors
- Core assumption: BPE tokenization optimized for Malay yields shorter, more semantically meaningful subwords
- Evidence anchors: Custom BPE tokenizer goal to minimize token sizes; weak evidence for subword quality comparison
- Break condition: If dataset contains significant English content, pretraining from scratch offers no advantage

### Mechanism 2
- Claim: Larger model size (5B vs 1.1B) improves performance on Malay grammar tasks
- Mechanism: Increased parameter count allows better capture of grammatical rules and syntactic dependencies specific to Malay
- Core assumption: Model capacity scales predictably with language complexity
- Evidence anchors: MaLLaM-5B achieving 27.79% accuracy on 1-shot tasks; weak comparative scaling study
- Break condition: If dataset is too small or noisy, additional parameters lead to overfitting

### Mechanism 3
- Claim: Multi-turn instruction fine-tuning enables coherent Malay conversational QA and coding tasks
- Mechanism: Supervised fine-tuning aligns generation patterns with expected response formats in Malay
- Core assumption: Instruction dataset covers sufficient linguistic diversity and task types
- Evidence anchors: Instruction-tuned models demonstrated proficiency in multi-turn Malay QA, coding tasks, and recipe generation; moderate evidence from examples
- Break condition: If instruction dataset lacks domain coverage, models fail on unseen task types

## Foundational Learning

- Concept: Byte Pair Encoding (BPE) tokenization
  - Why needed here: Reduces vocabulary size while preserving morphological information in Malay
  - Quick check question: Does the BPE tokenizer split compound words into meaningful subwords without losing semantic coherence?

- Concept: Cross-entropy minimization in causal language modeling
  - Why needed here: Trains model to predict next token given all previous tokens, enabling coherent text generation
  - Quick check question: Is loss curve showing consistent decrease across all model sizes, or are some sizes plateauing early?

- Concept: Supervised instruction fine-tuning
  - Why needed here: Adapts pretrained model to follow instructions and generate contextually appropriate responses in Malay
  - Quick check question: Does fine-tuned model maintain grammatical accuracy while following complex multi-turn instructions?

## Architecture Onboarding

- Component map: Data preprocessing -> Tokenizer training -> Distributed pretraining -> Tatabahasa evaluation -> Instruction fine-tuning -> Multi-turn QA/coding evaluation
- Critical path: Data preprocessing -> Tokenizer training -> Distributed pretraining -> Evaluation -> Fine-tuning -> Deployment
- Design tradeoffs: Larger models give better performance but require more GPU memory and longer training; BPE vs SentencePiece affects tokenization efficiency; distributed training vs single-node training affects scalability
- Failure signatures: GPU failures during pretraining (resolved by daemon restart), learning rate instability (mitigated by temporary reduction), tokenization inefficiency (addressed by custom BPE)
- First 3 experiments:
  1. Run tokenizer evaluation: Compare token count reduction on Malay Wikipedia between custom BPE and SentencePiece
  2. Run small-scale pretraining: Train 1.1B model for 1000 steps and monitor loss curve stability
  3. Run instruction fine-tuning: Fine-tune 1.1B model on subset of instruction dataset and evaluate on simple Malay QA task

## Open Questions the Paper Calls Out

- What is the precise distribution of token sizes across the different components of the 349GB dataset (31.7B deduped text, 40.98B StarCoder, 14.98B MS Madlad 400, 1.58B instruction-tuned, 1.14B Malaysia journals)?
- How does performance of MaLLaM models on tatabahasa test set compare to other existing models when evaluated on larger number of shots (e.g., 5 or 10 shots)?
- What is the impact of 30% reduction in learning rate on final performance of 5-billion-parameter MaLLaM model?

## Limitations

- Evaluation framework presents significant limitations with only 27.79% accuracy on tatabahasa test set lacking comparison to baseline Malay models
- Instruction-tuning evaluation relies on qualitative examples rather than systematic metrics, making it difficult to assess true proficiency
- Dataset composition remains unclear with specific sources, quality filtering, and potential English content not detailed
- Model's performance on coding tasks is particularly uncertain given stated limited coding data in training corpus

## Confidence

- **High Confidence**: Technical implementation details of training pipeline, model family specification (1.1B, 3B, 5B parameters), and training methodology are well-documented and reproducible
- **Medium Confidence**: Claim that larger models improve grammatical accuracy is supported by tatabahasa evaluation but absolute performance levels and comparisons are unclear
- **Low Confidence**: Claims about instruction-tuned model proficiency in multi-turn Malay QA, coding tasks, and recipe generation based on qualitative examples without systematic evaluation

## Next Checks

1. Design and implement standardized benchmark for Malay instruction-following tasks with multiple-choice questions, code generation tasks with unit tests, and multi-turn dialogue scenarios; score instruction-tuned models and compare against base models and commercial Malay language models

2. Conduct ablation study comparing custom BPE tokenizer against SentencePiece and other tokenization methods on same Malay dataset; measure token count reduction, average token length for typical Malay sentences, and downstream language modeling perplexity

3. Evaluate base models on parallel Malay-English datasets to measure any residual English language capabilities; test whether models show preferential performance on Malay versus English content, and whether pretraining from scratch reduces English bias compared to English models fine-tuned on Malay data