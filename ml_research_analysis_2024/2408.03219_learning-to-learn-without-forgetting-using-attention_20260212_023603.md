---
ver: rpa2
title: Learning to Learn without Forgetting using Attention
arxiv_id: '2408.03219'
source_url: https://arxiv.org/abs/2408.03219
tags:
- learning
- task
- tasks
- classifier
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a transformer-based meta-optimizer for continual
  learning that learns task-specific weight updates while preventing catastrophic
  forgetting. The approach uses attention to selectively update classifier parameters
  based on importance scores derived from gradients, allowing selective adaptation
  to new tasks while preserving knowledge from previous ones.
---

# Learning to Learn without Forgetting using Attention

## Quick Facts
- arXiv ID: 2408.03219
- Source URL: https://arxiv.org/abs/2408.03219
- Reference count: 13
- Primary result: Transformer-based meta-optimizer for continual learning with 90.40% average FWT and -6.74% BWT on SplitMNIST

## Executive Summary
This paper introduces a transformer-based meta-optimizer for continual learning that learns task-specific weight updates while preventing catastrophic forgetting. The approach uses attention mechanisms to selectively update classifier parameters based on importance scores derived from gradients, allowing selective adaptation to new tasks while preserving knowledge from previous ones. Evaluated on SplitMNIST, RotatedMNIST, and SplitCIFAR-100 datasets, the method shows strong performance in both backward transfer (BWT) and forward transfer (FWT), outperforming most baseline continual learning methods.

## Method Summary
The method employs a meta-learned transformer-based optimizer that predicts weight updates for a classifier network. It takes current weights, their importance scores (computed from gradients on a support set), and a task representation as input. The transformer's self-attention layers learn which parameters should be updated for the current task while preserving weights important for previous tasks. The approach uses a pre-trained task encoder (on SVHN for MNIST experiments, ImageNet for CIFAR experiments) to extract task representations, and applies importance scoring to constrain updates to task-relevant parameters only.

## Key Results
- Achieves 90.40% average forward transfer (FWT) and -6.74% backward transfer (BWT) on SplitMNIST
- Demonstrates strong performance across SplitMNIST, RotatedMNIST, and SplitCIFAR-100 datasets
- Eliminates need for task identifiers and replay buffers while maintaining performance
- Outperforms most baseline continual learning methods in both BWT and FWT metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The transformer-based meta-optimizer learns task-specific weight updates by attending to relationships between parameters across the task stream, enabling selective adaptation while preventing catastrophic forgetting.
- **Mechanism**: The meta-optimizer takes the current weights, their importance scores (computed from gradients on the support set), and a task representation as input. The transformer's self-attention layers learn which parameters should be updated for the current task while preserving weights important for previous tasks. By zeroing out low-importance scores, the optimizer is constrained to modify only task-relevant weights, protecting existing knowledge.
- **Core assumption**: Parameter importance scores derived from gradients before any optimization accurately reflect which weights are crucial for the current task and should be protected during updates.
- **Evidence anchors**:
  - [abstract]: "The meta-learned optimizer uses attention to learn the complex relationships between model parameters across a stream of tasks, and is designed to generate effective weight updates for the current task while preventing catastrophic forgetting on previously encountered tasks."
  - [section]: "Scores below a certain threshold are zeroed out since they do not serve in solving the task at hand. This guarantees that we don't overwrite knowledge from previous tasks, thus avoiding forgetting."
- **Break condition**: If importance scores poorly estimate parameter criticality (e.g., in tasks with overlapping but distinct parameter usage patterns), the meta-optimizer may incorrectly modify or preserve weights, leading to forgetting or underutilization of parameters.

### Mechanism 2
- **Claim**: Pre-training the task encoder on large datasets (SVHN for MNIST experiments, ImageNet for CIFAR experiments) provides rich task representations that enable the transformer to generalize across diverse task streams.
- **Mechanism**: The pre-trained task encoder extracts meaningful features from the support set and computes a single vector representation characterizing the current task. This representation is invariant to permutations of examples and captures task-specific information. By concatenating this with weight embeddings, the transformer can condition its weight updates on both the current task's nature and the current parameter values.
- **Core assumption**: Task representations learned from related but distinct datasets (SVHN vs MNIST, ImageNet vs CIFAR) capture sufficient semantic similarity to enable effective transfer for continual learning.
- **Evidence anchors**:
  - [section]: "The task encoder (part A) is pre-trained on the SVHN dataset for SplitMNIST and RotatedMNIST, and on ImageNet for SplitCIFAR-100."
  - [section]: "After extracting meaningful features from the data, an averaging layer computes the mean of the transformed data along the KN examples, producing a vector representation gA(D(sp)i,m) that characterizes the current task."
- **Break condition**: If the pre-training dataset is too dissimilar from the target tasks, the task representations may lack discriminative power, causing the meta-optimizer to generate suboptimal weight updates.

### Mechanism 3
- **Claim**: Meta-learning the optimizer itself eliminates the need for hand-crafted hyperparameters (learning rate, weight decay, optimizer type) and enables task-specific optimization strategies that balance stability and plasticity.
- **Mechanism**: Instead of using a fixed optimizer like SGD or Adam, the meta-optimizer learns how to generate weight updates for each task. During meta-training, it learns to predict effective updates that achieve good performance on the current task while maintaining performance on previous tasks. This learned strategy automatically adapts to the task's characteristics without requiring manual hyperparameter tuning.
- **Core assumption**: The meta-optimizer can learn effective optimization strategies across diverse tasks that generalize better than hand-tuned hyperparameters.
- **Evidence anchors**:
  - [abstract]: "the proposed approach also eliminates the need to select optimizer-related hyperparameters for the classifier network, such as learning rate, weight decay, and optimizer type."
  - [section]: "Instead, the hyperparameters of the meta-optimizer proposed in this paper are selected only once at the beginning of the training, and the model itself automatically learns which optimization strategy is best suited for each task."
- **Break condition**: If the meta-optimizer overfits to the meta-training tasks, it may fail to generalize to new tasks, producing ineffective weight updates that don't balance stability and plasticity appropriately.

## Foundational Learning

- **Concept**: Gradient-based importance scoring
  - **Why needed here**: Importance scores derived from gradients indicate which parameters are critical for the current task. This allows the meta-optimizer to focus updates on task-relevant parameters while preserving knowledge stored in important parameters from previous tasks.
  - **Quick check question**: How are importance scores computed, and why does zeroing out low scores prevent catastrophic forgetting?

- **Concept**: Meta-learning with bi-level optimization
  - **Why needed here**: The approach frames continual learning as a meta-learning problem where the outer loop optimizes meta-parameters (including the meta-optimizer) and the inner loop adapts the classifier to each task. This enables fast adaptation with limited data while maintaining performance across the task stream.
  - **Quick check question**: What is the difference between the meta-learned parameters θ and the classifier weights W that are optimized by the meta-optimizer?

- **Concept**: Transformer self-attention mechanisms
  - **Why needed here**: The transformer's self-attention allows the meta-optimizer to learn complex relationships between all parameters simultaneously, capturing dependencies that sequential methods (like LSTMs) might miss. This is crucial for identifying which parameter updates will preserve previous knowledge while adapting to new tasks.
  - **Quick check question**: Why might a transformer be more suitable than an LSTM for learning parameter update strategies in continual learning?

## Architecture Onboarding

- **Component map**: Support set → Task encoder → Feature extractor → Transformer → Weight updates → Classifier adaptation → Query set evaluation → Meta-parameter update

- **Critical path**: Support set → Task encoder → Feature extractor → Transformer → Weight updates → Classifier adaptation → Query set evaluation → Meta-parameter update

- **Design tradeoffs**:
  - Using spatial vs sequential weight allocation: spatial allocation reduces memory usage but may lose some ordering information
  - Number of adaptation steps (Q): higher values improve performance but increase computation
  - Importance score threshold: higher thresholds protect more knowledge but may limit adaptation
  - Transformer depth and width: deeper models capture more complex relationships but increase training time

- **Failure signatures**:
  - High BWT with low FWT: meta-optimizer is too conservative, preserving old knowledge but failing to adapt
  - Low BWT with high FWT: meta-optimizer is too aggressive, adapting well but forgetting previous tasks
  - Both BWT and FWT low: poor meta-optimizer generalization or inadequate task representations
  - Extremely high accuracy on last task only: catastrophic forgetting of all previous tasks

- **First 3 experiments**:
  1. Run on SplitMNIST with K=5 support examples, verify that importance scores correlate with actual parameter usage across tasks
  2. Compare BWT/FWT when removing importance scores (should show catastrophic forgetting)
  3. Test transfer learning baseline (pre-trained task encoder + fine-tuning) to confirm meta-optimizer adds value beyond transfer learning alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of meta-learning larger transformer models on longer task streams compared to the current approach?
- Basis in paper: [explicit] The paper states "it would be interesting to meta-learn larger transformer models on longer streams, which should improve both BWT and FWT as it can meta-learn over more tasks" and "The primary contribution is the introduction of a meta-learned transformer-based optimizer."
- Why unresolved: The authors did not conduct experiments with larger transformer models on longer task streams due to computational constraints and potential memory issues.
- What evidence would resolve it: Empirical results comparing the performance (BWT, FWT, average accuracy) of the current approach with a meta-learned larger transformer model on longer task streams would provide evidence for the impact.

### Open Question 2
- Question: How does the proposed approach perform in online continual learning scenarios where data arrives in a streaming fashion rather than in batches?
- Basis in paper: [explicit] The paper mentions "given that the optimizer works with batches of data as input, the proposed approach could also be extended to online CL scenarios" and discusses the approach's ability to handle non-stationary data streams.
- Why unresolved: The authors did not evaluate the approach in an online setting, focusing instead on batch-based continual learning.
- What evidence would resolve it: Experimental results demonstrating the approach's performance (BWT, FWT, average accuracy) in an online continual learning setting with streaming data would provide evidence for its effectiveness in this scenario.

### Open Question 3
- Question: How does the proposed approach scale to more complex classifier architectures with a larger number of parameters?
- Basis in paper: [explicit] The paper states "One potential mitigation strategy involves focusing on optimizing only the last layers, as they encode more task-specific information compared to earlier layers" and discusses computational demands associated with larger architectures.
- Why unresolved: The authors did not conduct experiments with more complex classifier architectures due to memory constraints and potential scalability issues.
- What evidence would resolve it: Empirical results comparing the performance (BWT, FWT, average accuracy) of the current approach with the same approach applied to more complex classifier architectures would provide evidence for its scalability and effectiveness with larger models.

## Limitations

- Limited evaluation on task streams with high task similarity, leaving robustness to overlapping parameter usage unclear
- No thorough analysis of computational overhead compared to baseline methods, particularly for larger architectures
- Limited ablation studies on the necessity of pre-training task encoders on external datasets (SVHN/ImageNet)

## Confidence

- **High confidence**: The meta-optimizer architecture design and implementation details are well-specified
- **Medium confidence**: Claims about backward and forward transfer performance, though limited to specific datasets
- **Low confidence**: Claims about eliminating hyperparameter tuning without extensive ablation studies

## Next Checks

1. Implement ablation studies removing importance scoring to quantify its contribution to preventing forgetting
2. Test the method on task streams with high task similarity to evaluate robustness to overlapping parameter usage
3. Analyze computational overhead compared to baseline methods, particularly for larger architectures