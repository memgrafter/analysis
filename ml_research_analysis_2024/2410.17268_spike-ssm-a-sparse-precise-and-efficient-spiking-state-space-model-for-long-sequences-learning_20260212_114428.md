---
ver: rpa2
title: 'SPikE-SSM: A Sparse, Precise, and Efficient Spiking State Space Model for
  Long Sequences Learning'
arxiv_id: '2410.17268'
source_url: https://arxiv.org/abs/2410.17268
tags:
- spiking
- pmbc
- spike-ssm
- neuron
- iterations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPikE-SSM, a spiking state space model designed
  to address the challenge of efficient long-sequence modeling in spiking neural networks.
  The method incorporates a parallel max-min boundary compression (PMBC) strategy
  to enable efficient parallel inference, a novel refractory LIF neuron model with
  trainable dynamics to balance accuracy and sparsity, and hierarchical integration
  of the neuron model into the SSM block with trainable thresholds and refractory
  magnitudes.
---

# SPikE-SSM: A Sparse, Precise, and Efficient Spiking State Space Model for Long Sequences Learning

## Quick Facts
- arXiv ID: 2410.17268
- Source URL: https://arxiv.org/abs/2410.17268
- Reference count: 40
- Primary result: Achieves state-of-the-art accuracy-sparsity tradeoffs on long-sequence benchmarks with average network sparsity below 10%

## Executive Summary
SPikE-SSM addresses the challenge of efficient long-sequence modeling in spiking neural networks by combining parallel max-min boundary compression (PMBC) with a novel refractory LIF neuron model integrated into SSM blocks. The method achieves exceptional sparsity (as low as 0.07% on Path-X) while maintaining competitive accuracy across LRA benchmarks and WikiText-103. By leveraging parallelizable computations and trainable thresholds, SPikE-SSM enables biologically interpretable spiking dynamics without sacrificing the computational efficiency of traditional SSMs.

## Method Summary
SPikE-SSM integrates a refractory LIF neuron model with trainable dynamics into SSM blocks, using PMBC to enable parallel inference. The refractory mechanism introduces temporal dependencies through a sliding pulse that accumulates with each spike, while trainable thresholds vth and Uth allow systematic control of spiking rates. PMBC decomposes the iterative membrane potential update into parallel-computable components, achieving significant speedup by compressing temporal dependencies into boundary comparisons. The model is trained using AdamW optimizer with surrogate gradients to handle the non-differentiable Heaviside function.

## Key Results
- Achieves 0.07% sparsity on Path-X task while maintaining competitive accuracy
- Outperforms existing methods on WikiText-103 with fewer parameters
- Demonstrates average network sparsity below 10% across LRA benchmarks
- Shows nearly two-order acceleration at 8K sequence length using PMBC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PMBC enables parallel inference of spiking neuron dynamics by compressing temporal dependencies into boundary comparisons.
- Mechanism: PMBC decomposes the iterative membrane potential update into two components - a parallel-computable kernel k_t (convolution of input I with exponential τ sequence) and a history-dependent term m_t (computed via boundary compression). By iteratively refining upper and lower bounds on m_t, most spikes can be identified without serial iteration.
- Core assumption: The spiking signal s_t is binary (0 or 1), allowing boundary compression to work.
- Evidence anchors:
  - [abstract] "we propose a boundary compression strategy (PMBC) to accelerate the inference of the spiking neuron model, enabling parallel processing for long sequence learning"
  - [section] "The output membrane voltage u is iteratively computed by Eq. (6) since u_t depends on the spiking history from the previous time steps, notwithstanding the input current I can be obtained in parallel"
  - [corpus] Weak - related papers mention "parallel" and "spiking state space models" but don't discuss boundary compression specifically
- Break condition: If spiking signals were not binary or had multiple states, boundary compression would fail.

### Mechanism 2
- Claim: The refractory LIF neuron model improves sparsity while maintaining accuracy through trainable reset dynamics.
- Mechanism: The refractory mechanism introduces a sliding pulse R_t that accumulates with each spike and decays exponentially, making subsequent spiking more difficult. This creates temporal dependencies that enhance biological interpretability while the trainable thresholds vth and refractory magnitudes Uth allow optimization of the sparsity-accuracy tradeoff.
- Core assumption: Biological neurons exhibit refractory periods that are functionally important.
- Evidence anchors:
  - [abstract] "we propose a novel and concise neuron model incorporating reset-refractory mechanism to leverage the inherent temporal dimension for dynamic computing with biological interpretability"
  - [section] "In biological neurons, spiking is usually followed by a refractory period during which new spiking is more difficult. This mechanism improves the overall sparsity of the network"
  - [corpus] Weak - related papers mention "biological neuron parameters" but don't discuss refractory mechanisms specifically
- Break condition: If the refractory mechanism overly suppresses spiking, accuracy would degrade.

### Mechanism 3
- Claim: Hierarchical integration of the refractory neuron into the SSM block balances sparsity and accuracy through dynamic reset.
- Mechanism: The SPikE-SSM block replaces the original SSM non-linearity with the proposed refractory neuron, incorporating higher-level neuronal dynamics. Trainable thresholds vth and Uth in the block allow systematic control of spiking rates while maintaining the SSM's long-range dependency modeling capability.
- Core assumption: The SSM framework can accommodate spiking dynamics without losing its computational efficiency.
- Evidence anchors:
  - [abstract] "we hierarchically integrate the proposed neuron model to the original SSM block, and enhance the dynamics of SPikE-SSM by incorporating trainable thresholds and refractory magnitudes to balance accuracy and sparsity"
  - [section] "we integrate the proposed refractory neuron with soft reset mechanism and PMBC to the inherent SSM block, which aims to maintain both the high sparsity and excellent accuracy"
  - [corpus] Weak - related papers mention "spiking state space models" but don't discuss hierarchical integration specifically
- Break condition: If the spiking dynamics disrupt the SSM's state-space computations, the model would lose its efficiency advantages.

## Foundational Learning

- Concept: Leaky Integrate-and-Fire (LIF) neuron dynamics
  - Why needed here: Understanding the LIF model is essential for grasping how PMBC works and why the refractory mechanism improves sparsity
  - Quick check question: Why does the membrane potential u_t depend on spiking history in LIF neurons, and how does this create computational challenges?

- Concept: State Space Models (SSMs) and their diagonalization
-  - Why needed here: SSMs provide the computational framework that makes long-sequence modeling efficient; understanding their structure is key to seeing how spiking dynamics can be integrated
  - Quick check question: How does the recurrent view of SSMs (h_t = Āh_{t-1} + B̄x_t) relate to the original differential equation formulation?

- Concept: Fast Fourier Transform (FFT) acceleration
  - Why needed here: PMBC relies on FFT for parallel computation of the kernel k_t; understanding this connection is crucial for grasping the efficiency gains
  - Quick check question: Why does using FFT reduce the computational complexity from O(L²) to O(L log L) for computing the kernel?

## Architecture Onboarding

- Component map:
  - Input sequence → SSM block (producing y_t) → Refractory LIF neuron (producing s_t) → Conv1D + GLU → Output
  - PMBC operates within the refractory LIF neuron computation
  - Trainable parameters: vth, Uth, SSM parameters (A, B, C), Conv1D weights

- Critical path:
  1. Input normalization and SSM computation (parallelizable)
  2. PMBC boundary compression iterations (M iterations, parallel within each)
  3. Spiking signal generation and Conv1D+GLU processing
  4. Loss computation and gradient backpropagation

- Design tradeoffs:
  - M iterations in PMBC vs. accuracy: More iterations identify more spikes but increase computation time
  - Refractory magnitude τr vs. sparsity: Larger τr increases sparsity but may hurt accuracy
  - Trainable vth/Uth vs. fixed values: Trainable parameters improve accuracy but add optimization complexity

- Failure signatures:
  - Low accuracy despite high sparsity: PMBC may be identifying too few spikes, or refractory mechanism may be too strong
  - High computation time: M iterations in PMBC may be too large, or FFT implementation may be suboptimal
  - Training instability: Learning rates for vth/Uth may be inappropriate, or surrogate gradient may not approximate Heaviside function well

- First 3 experiments:
  1. Baseline test: Run SPikE-SSM on sMNIST with default parameters, verify ~99% accuracy and ~5-10% spiking rate
  2. PMBC iteration sensitivity: Vary M from 1 to 10, measure accuracy and computation time to find optimal tradeoff
  3. Refractory magnitude sweep: Vary τr from 0.5 to 0.99, measure sparsity-accuracy tradeoff curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the PMBC algorithm scale with increasing sequence length in terms of both computational efficiency and accuracy preservation?
- Basis in paper: [explicit] The paper mentions that the speedup ratio using PMBC increases with sequence length, achieving a nearly two-order acceleration at 8K. It also states that the maximum number of iterations of PMBC is no more than L, and that the actual number of iterations is much smaller than L.
- Why unresolved: While the paper demonstrates significant speedup at 8K sequence length, it does not provide a comprehensive analysis of how PMBC performance scales to much longer sequences (e.g., 64K or 128K). The relationship between iteration count, accuracy, and computational cost at extreme lengths is not fully characterized.
- What evidence would resolve it: Empirical results showing PMBC performance on sequence lengths beyond 8K, including accuracy retention, iteration count requirements, and comparative speedup ratios. Additionally, theoretical analysis of computational complexity scaling would help.

### Open Question 2
- Question: What is the impact of trainable refractory magnitude (τr) on the model's performance compared to fixed τr, and how does it affect the trade-off between sparsity and accuracy?
- Basis in paper: [explicit] The paper mentions that τ and τr are set to 0.1 and 0.9 respectively by default, and that ablation studies show SPikE-SSM with fixed τ and τr performs better than that with trainable τ and τr. However, the paper also discusses the refractory mechanism's role in improving sparsity.
- Why unresolved: The paper does not provide a detailed comparison between fixed and trainable τr, nor does it fully explore how different τr values affect the sparsity-accuracy trade-off across various tasks.
- What evidence would resolve it: Comparative experiments showing SPikE-SSM performance with different fixed τr values and trainable τr across multiple tasks, with detailed analysis of sparsity levels and accuracy metrics.

### Open Question 3
- Question: How does the choice of surrogate gradient function affect the training dynamics and final performance of SPikE-SSM, and are there specific gradient functions that are more suitable for different types of sequence modeling tasks?
- Basis in paper: [explicit] The paper mentions the use of a piecewise quadratic surrogate spiking function with α = 1, and notes that several surrogate gradient methods are proposed to enable training through gradient descent.
- Why unresolved: While the paper implements one specific surrogate gradient function, it does not explore the impact of different surrogate gradient functions on model performance or discuss whether certain functions might be more suitable for specific task types.
- What evidence would resolve it: Experiments comparing SPikE-SSM performance using different surrogate gradient functions (e.g., rectangular, triangular, and other piecewise functions) across various sequence modeling tasks, with analysis of training stability and convergence characteristics.

## Limitations

- PMBC convergence guarantees under non-ideal conditions (noisy inputs, approximate spike timing) remain unproven
- Biological plausibility claims are limited by simplified neuron dynamics that may not capture full neural complexity
- Long-sequence performance on real-world data beyond synthetic benchmarks needs validation

## Confidence

- **High confidence**: SPikE-SSM achieves state-of-the-art accuracy-sparsity tradeoffs on tested benchmarks
- **Medium confidence**: PMBC provides consistent speedup across different sequence lengths
- **Low confidence**: Biological interpretability claims given simplified neuron model assumptions

## Next Checks

1. Test PMBC convergence on noisy input sequences and measure accuracy degradation
2. Compare learned threshold dynamics against biological neuron response patterns
3. Benchmark on long real-world sequences (audio, video) to validate scalability claims