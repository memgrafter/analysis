---
ver: rpa2
title: 'PRE: A Peer Review Based Large Language Model Evaluator'
arxiv_id: '2401.15641'
source_url: https://arxiv.org/abs/2401.15641
tags:
- llms
- evaluation
- arxiv
- text
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PRE, a peer review-based framework for evaluating
  large language models (LLMs). PRE addresses the challenges of high cost, low generalizability,
  and inherent bias in existing LLM evaluation methods by using a peer review mechanism
  inspired by academic publication processes.
---

# PRE: A Peer Review Based Large Language Model Evaluator

## Quick Facts
- arXiv ID: 2401.15641
- Source URL: https://arxiv.org/abs/2401.15641
- Reference count: 40
- Primary result: PRE achieves 0.7443 precision, 0.7331 Kendall's tau, and 0.4998 Spearman correlation, outperforming baselines including GPT-4 itself

## Executive Summary
This paper introduces PRE, a peer review-based framework for evaluating large language models that addresses the limitations of existing evaluation methods including high cost, low generalizability, and inherent bias. The framework mimics academic peer review processes by selecting qualified LLM reviewers through a qualification exam, having them rate evaluatee outputs, and aggregating results using weighted voting based on qualification performance. Experimental results on text summarization tasks with 11 LLMs demonstrate that PRE significantly outperforms baseline methods, achieving the highest consistency with human preferences while reducing bias through aggregation of multiple diverse evaluators.

## Method Summary
PRE implements a three-module framework: qualification exam to select reliable LLM reviewers, peer review to collect ratings from qualified reviewers on evaluatee outputs, and chair decision to aggregate reviewer ratings using weighted voting. The qualification exam uses manual annotations or unsupervised methods to assess reviewer capability, with reviewers weighted by their Precision scores. The peer review module employs pairwise or pointwise rating modes depending on task requirements, and the chair decision module normalizes scores and aggregates using weighted voting. The framework is designed to be generalizable across tasks without requiring task-specific training data or reference outputs.

## Key Results
- PRE achieves 0.7443 precision, 0.7331 Kendall's tau, and 0.4998 Spearman correlation on text summarization tasks
- PRE outperforms baseline methods including ROUGE, BLEU, BERTScore, PandaLM, GPTScore, and single LLM evaluators
- Even without GPT-4 in the reviewer pool, PRE achieves comparable performance to GPT-4, demonstrating the effectiveness of the peer review mechanism
- PRE reduces bias by aggregating judgments from multiple LLMs, each with different biases, as validated by Preference Gap analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PRE reduces evaluation bias by aggregating judgments from multiple LLMs, each with different biases.
- Mechanism: Individual LLM biases (e.g., GPT-4 preferring outputs from its own series) are diversified when multiple reviewers are used. Aggregation with weighted voting based on qualification exam performance cancels out systematic biases.
- Core assumption: LLMs have different, uncorrelated biases, and aggregation with appropriate weighting reduces overall bias.
- Evidence anchors:
  - [abstract] "The study also validates the existence of bias when using a single LLM for evaluation and shows that PRE reduces this bias."
  - [section 5.2] "Figure 4 shows the heatmap distribution of the PG metric among seven powerful LLMs... The proportions of PG values greater than 0 are 66.67%, 57.14% and 76.19% respectively, which are all significantly higher than the 50% of the unbiased scenario."
  - [corpus] "Pre-review to Peer review: Pitfalls of Automating Reviews using Large Language Models" - suggests prior work recognizes bias issues in LLM-based review.
- Break condition: If all LLMs share a common bias (e.g., all trained on similar data), aggregation will not reduce bias.

### Mechanism 2
- Claim: PRE provides generalizable evaluation without task-specific training or reference data.
- Mechanism: Instead of requiring task-specific reference datasets or fine-tuning evaluators, PRE uses a qualification exam to select capable reviewer LLMs and then applies them to rate outputs in a zero-shot manner.
- Core assumption: LLMs possess general evaluation capabilities that can be validated through a qualification exam and then transferred to new tasks.
- Evidence anchors:
  - [abstract] "PRE can easily be generalized to different tasks and is highly cost efficient."
  - [section 3.2.1] "The purpose of the qualification exam data is to assess the evaluation abilities of reviewer candidate LLMs. With proper design, a single set of qualification exam data can reflect the general evaluation abilities of reviewer candidate LLMs, thus making the reviewer selection results generalizable to multiple tasks."
  - [corpus] "Auto-PRE: An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation" - similar approach validated in other work.
- Break condition: If evaluation capability is too task-specific, qualification exam performance won't transfer to new tasks.

### Mechanism 3
- Claim: Weighted voting based on qualification exam performance improves evaluation consistency with human preferences.
- Mechanism: Reviewer LLMs are weighted by their qualification exam Precision, giving more influence to more reliable evaluators. This weighted aggregation produces results more aligned with human judgments than unweighted methods.
- Core assumption: Qualification exam performance correlates with actual evaluation capability on the target task.
- Evidence anchors:
  - [section 3.2.3] "For pointwise and pairwise modes, we have different implementation details... The weight of reviewer LLM ð‘¤ð‘™ is determined by its performance in qualification exam."
  - [section 5.1] "Even the variant without GPT-4 model (PRE w/o GPT-4) achieves comparable evaluation results with GPT-4. This indicates that the peer review mechanism could effectively evaluate."
  - [section 4.4.3] "In the pointwise mode... we first need to normalize the original LLM output score... The weight of reviewer LLM ð‘¤ð‘™ is determined by its Precision ð‘ð‘™ in the qualification exam."
- Break condition: If qualification exam doesn't predict task performance, weighting by exam scores won't improve results.

## Foundational Learning

- Concept: LLM evaluation bias and its sources
  - Why needed here: Understanding bias is crucial for appreciating why PRE's peer review mechanism is necessary and how it addresses the problem
  - Quick check question: What are two sources of bias in LLM evaluation mentioned in the paper?

- Concept: Zero-shot evaluation vs. fine-tuned evaluation
  - Why needed here: PRE uses zero-shot evaluation through qualification exams rather than task-specific fine-tuning, which is a key design choice
  - Quick check question: How does PRE avoid the need for task-specific training data that other methods require?

- Concept: Aggregation methods for ensemble evaluation
  - Why needed here: PRE uses weighted voting based on qualification performance; understanding aggregation is key to the architecture
  - Quick check question: What two aggregation strategies does PRE use for pointwise vs. pairwise evaluation modes?

## Architecture Onboarding

- Component map:
  - Qualification Exam Module -> Peer Review Module -> Chair Decision Module
  - Prompt Design System -> Manual Annotation Interface

- Critical path:
  1. Qualification exam to select reviewers
  2. Peer review to collect ratings
  3. Aggregation to produce final scores
  4. Validation against human annotations

- Design tradeoffs:
  - Qualification exam vs. automatic qualification (Auto-Exam): Manual annotation provides better validation but costs more
  - Pairwise vs. pointwise rating: Pairwise shows slightly better performance but requires more computation
  - Weighting by qualification vs. equal weighting: Weighting improves performance but requires qualification exam

- Failure signatures:
  - Low Precision in qualification exam indicates poor reviewer selection
  - High variance in reviewer ratings suggests qualification exam isn't filtering well
  - Results not correlating with human preferences indicates aggregation method issues

- First 3 experiments:
  1. Run qualification exam with different pass thresholds (e.g., 55%, 60%, 65%) and measure impact on final evaluation quality
  2. Compare pairwise vs. 5-level pointwise vs. 100-level pointwise settings on a small subset of data
  3. Test aggregation with equal weights vs. qualification-based weights on the same reviewer set

## Open Questions the Paper Calls Out

- How does the PRE framework handle tasks beyond text summarization?
  - Basis in paper: [inferred] The paper mentions that PRE can be generalized to different tasks but does not provide specific examples or experiments.
  - Why unresolved: The paper focuses on text summarization tasks and does not explore the application of PRE to other types of tasks, leaving the generalizability of the framework untested.
  - What evidence would resolve it: Experiments demonstrating the effectiveness of PRE on a variety of tasks beyond text summarization would provide evidence for its generalizability.

- What is the impact of the size and diversity of the reviewer LLM pool on the performance of PRE?
  - Basis in paper: [explicit] The paper mentions that PRE uses a qualification exam to select reviewers but does not explore the effects of varying the number or diversity of reviewers.
  - Why unresolved: The paper does not investigate how the size and diversity of the reviewer pool affect the robustness and accuracy of PRE, which could be crucial for its practical implementation.
  - What evidence would resolve it: Experiments varying the number and diversity of reviewers in the qualification exam and analyzing the impact on PRE's performance would provide insights into this aspect.

- How does PRE address the potential for collusion or bias among reviewer LLMs?
  - Basis in paper: [inferred] The paper acknowledges the existence of bias in LLM evaluations and proposes using multiple reviewers to mitigate it, but does not detail mechanisms to prevent collusion or bias.
  - Why unresolved: The paper does not provide a detailed discussion on how PRE ensures that reviewer LLMs do not collude or introduce bias in their evaluations, which is critical for the framework's reliability.
  - What evidence would resolve it: A detailed analysis of the mechanisms within PRE to detect and prevent collusion or bias among reviewers would address this question.

## Limitations

- The study focuses exclusively on text summarization tasks, limiting generalizability to other domains like reasoning or coding
- The peer review mechanism's effectiveness depends heavily on the qualification exam design, which is not fully specified
- The cost efficiency claim assumes the qualification exam is a one-time cost, but scaling to many tasks may require multiple exams
- The framework's performance relative to human evaluation is measured through correlation rather than direct comparison of evaluation quality

## Confidence

- High Confidence: The bias reduction mechanism through aggregation of multiple LLMs is well-supported by experimental evidence showing bias in single LLM evaluation and its reduction with PRE
- Medium Confidence: The generalizability claim is supported by the zero-shot approach but limited by the single task evaluation
- Medium Confidence: The cost efficiency claim is plausible but depends on implementation details not fully specified in the paper

## Next Checks

1. **Cross-task validation:** Apply PRE to at least two additional NLP tasks (e.g., question answering and text generation) to verify the generalizability claim beyond text summarization
2. **Qualification exam robustness:** Test the framework with qualification exams of varying quality (e.g., using different pass thresholds or synthetic vs. real data) to measure sensitivity to exam design
3. **Bias analysis:** Conduct a systematic analysis of which types of biases are reduced versus which persist when using multiple LLMs in the peer review process