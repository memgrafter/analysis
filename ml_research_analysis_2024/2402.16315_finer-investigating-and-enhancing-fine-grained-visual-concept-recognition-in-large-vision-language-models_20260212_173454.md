---
ver: rpa2
title: 'Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition
  in Large Vision Language Models'
arxiv_id: '2402.16315'
source_url: https://arxiv.org/abs/2402.16315
tags:
- concept
- image
- attributes
- fine-grained
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large vision-language models (LVLMs) excel at general image understanding
  but struggle with fine-grained visual categorization (FGVC), exhibiting dramatic
  drops in accuracy when distinguishing between similar subcategories. Analysis reveals
  a modality gap: while LVLMs can leverage their parametric knowledge for classification
  given text descriptions, they fail to do so when provided with images of the same
  concepts.'
---

# Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models

## Quick Facts
- arXiv ID: 2402.16315
- Source URL: https://arxiv.org/abs/2402.16315
- Authors: Jeonghwan Kim; Heng Ji
- Reference count: 18
- Large vision-language models excel at general image understanding but struggle with fine-grained visual categorization, exhibiting dramatic drops in accuracy when distinguishing between similar subcategories.

## Executive Summary
Large vision-language models (LVLMs) demonstrate strong general image understanding capabilities but exhibit significant performance drops when tasked with fine-grained visual categorization (FGVC). Through systematic analysis, the authors identify a critical "modality gap" where LVLMs can leverage their parametric knowledge for classification when given text descriptions but fail to do so with corresponding images. To address this limitation, the paper introduces FINER, a comprehensive benchmark with multiple granularity levels and attribute-centric evaluation, along with an attribute-focused training mixture. The proposed approach significantly improves zero-shot FGVC performance across six benchmark datasets, with some models showing over 40 percentage point improvements in exact match scores.

## Method Summary
The authors construct FINER, a benchmark with multiple granularity levels (superordinate, coarse, fine) for six FGVC datasets, and evaluate LVLMs on classification and attribute generation tasks. They analyze the modality gap by comparing text-only and image-only performance, then develop an attribute-focused training mixture based on ATTR SEEK prompting. The training procedure involves instruction-tuning LLaVA-1.5 with LoRA on this mixture to improve zero-shot attribute generation and FGVC performance. The method explicitly generates visual attributes before classification, allowing LVLMs to leverage their parametric knowledge about concept-attribute relationships.

## Key Results
- LVLMs show significant performance gaps between text-only and image-only inputs for fine-grained classification, with text-only inputs often outperforming image-only inputs by substantial margins
- Instruction-tuning with attribute-focused training mixture improves zero-shot FGVC performance across six benchmark datasets
- Some models demonstrate over 40 percentage point improvements in exact match scores after attribute-focused fine-tuning
- The modality gap persists despite strong image captioning abilities, indicating LVLMs cannot effectively ground their knowledge in visual input

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LVLMs possess concept-attribute knowledge in their parameters but fail to leverage it when given image inputs due to modality gap.
- Mechanism: When provided with text descriptions of concepts, LVLMs can successfully classify fine-grained categories because the text modality can access the parametric knowledge. However, when given images, the visual modality cannot effectively bridge to this knowledge due to information loss during projection from visual to textual embedding space.
- Core assumption: The parametric knowledge within LVLMs contains sufficient concept-attribute information for fine-grained classification, and the modality gap is the primary bottleneck preventing its utilization.
- Evidence anchors:
  - [abstract] "Analysis reveals a modality gap: while LVLMs can leverage their parametric knowledge for classification given text descriptions, they fail to do so when provided with images of the same concepts."
  - [section 4.1] "In Figure 4, the results show that even with text-only input that contains the detailed physical attributes of a concept, LVLMs are capable of solving fine-grained visual classification, outperforming the image-only input."
  - [corpus] Weak - related papers focus on optimization strategies and understanding gaps but don't directly confirm this specific mechanism.

### Mechanism 2
- Claim: The loss of visual information after projection from visual embedding space to textual space significantly contributes to modality gap.
- Mechanism: CLIP-ViT-L/14 encodes visual information which is then projected to textual space through a learned projection layer. This projection process causes substantial loss of fine-grained visual details necessary for distinguishing similar subcategories, preventing effective cross-modal interplay.
- Core assumption: The projection from visual to textual embedding space is lossy for fine-grained visual attributes, and this loss directly impacts classification performance.
- Evidence anchors:
  - [section 4.3] "We use CLIP-ViT-L/14 as the image encoder and use LLaVA-1.5's projector. We freeze both the image encoder and the projector and finetune a multi-layer perceptron (MLP) layer on top for classification for 10 epochs... As shown in Figure 5, the loss of visual information encoded by the vision encoder leads to substantial drop in classification performance across the six FGVC tasks."
  - [abstract] "We also show that such constraints lead to diminished fine-grained understanding of the image, preventing these models from generating accurate and detailed visual attributes of the concepts that appear within an image."
  - [corpus] Weak - related papers discuss optimization strategies but don't specifically address projection-related information loss.

### Mechanism 3
- Claim: Explicitly generating visual attributes before classification enables LVLMs to leverage their parametric knowledge and improves fine-grained visual concept recognition.
- Mechanism: The ATTR SEEK prompting technique forces LVLMs to first generate a set of discriminative visual attributes visible in the concept image. This intermediate step allows the model to access and utilize its concept-attribute knowledge before making the final classification decision, effectively bridging the modality gap.
- Core assumption: Generating visual attributes activates the model's parametric knowledge about concept-attribute relationships, and this knowledge can then be effectively used for classification when provided as context.
- Evidence anchors:
  - [abstract] "To address this, the authors introduce FINER, a benchmark with multiple granularity levels and attribute-centric evaluation, along with an attribute-focused training mixture. Experiments show that instruction-tuning LVLMs to explicitly generate visual attributes before classification substantially improves zero-shot FGVC performance across six benchmark datasets, with some models improving by over 40 percentage points in exact match scores."
  - [section 5.3] "We construct an instruction-tuning mixture based on the ATTR SEEK prompting pipeline to improve the zero-shot attribute generation and FGVC performance... In Table 4, we finetune LLaVA-1.5 (7B) on the training mixture and see that the FINER-tuned model outperforms the direct finetuned counterpart that was simply trained to directly predict the concept label."
  - [corpus] Weak - related papers discuss fine-grained recognition but don't specifically validate this attribute-generation mechanism.

## Foundational Learning

- Concept: Modality gap in vision-language models
  - Why needed here: Understanding how different input modalities (text vs. image) are processed differently by LVLMs is crucial for diagnosing performance issues in fine-grained visual categorization.
  - Quick check question: If a model performs well on text-only fine-grained classification but poorly on image-only classification for the same concepts, what does this suggest about its processing of different modalities?

- Concept: Cross-modal representation alignment
  - Why needed here: The projection from visual to textual embedding space and how well these representations align determines whether visual information can effectively access textual parametric knowledge.
  - Quick check question: What happens to classification performance when you add a linear probe layer on top of projected visual embeddings versus original visual embeddings?

- Concept: Zero-shot learning in multimodal models
  - Why needed here: Understanding how LVLMs transfer knowledge from pre-training to downstream tasks without task-specific training is essential for evaluating their inherent capabilities and limitations.
  - Quick check question: How does the performance of a zero-shot LVLM on a new task compare to its performance when fine-tuned on that task, and what does this gap indicate?

## Architecture Onboarding

- Component map: Vision encoder (CLIP-ViT-L/14) → Projection layer → LLM component (Vicuna/LLaMA) → Output generator
- Critical path: Image → Vision encoder → Projection layer → LLM cross-attention → Generation
- Design tradeoffs: Larger models have more parametric knowledge but may be more prone to overfitting during fine-tuning. Simpler projection mechanisms preserve more visual information but may align less well with textual representations.
- Failure signatures: Significant performance gap between text-only and image-only inputs for the same concepts indicates modality gap. Poor performance on fine-grained categories despite good superordinate-level performance suggests insufficient preservation of fine-grained visual details.
- First 3 experiments:
  1. Compare text-only vs image-only classification performance on the same fine-grained concepts to quantify modality gap.
  2. Implement linear probing on projected vs original visual embeddings to measure information loss.
  3. Test ATTR SEEK prompting on a held-out dataset to validate attribute generation improves classification.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LVLMs be effectively trained to better leverage visual information without losing fine-grained details during the projection from visual to textual embedding spaces?
- Basis in paper: [explicit] The paper discusses the modality gap and visual information loss after projection (§4.3), showing that linear probing after projection leads to substantial drops in classification performance across FGVC tasks.
- Why unresolved: While the paper identifies the problem of visual information loss during projection, it does not propose specific architectural or training modifications to preserve visual details. The current approach of linear probing only demonstrates the problem rather than solving it.
- What evidence would resolve it: Experimental results comparing different projection methods (e.g., nonlinear projections, attention-based fusion, or preserving dual embedding spaces) against the baseline linear projection, showing maintained or improved FGVC performance while reducing the modality gap.

### Open Question 2
- Question: What is the optimal granularity level for attribute extraction and generation in LVLMs for FGVC tasks, and how does this vary across different object categories?
- Basis in paper: [inferred] The paper introduces FINER with multiple granularity levels and discusses attribute-centric evaluation, but does not systematically investigate the relationship between attribute granularity and classification performance across different object types.
- Why unresolved: The paper uses a fixed approach to attribute extraction and generation without exploring whether different granularity levels of attributes are more effective for different types of objects (e.g., animals vs. vehicles vs. plants).
- What evidence would resolve it: Comparative experiments showing FGVC performance across different granularity levels of attributes (e.g., basic vs. detailed vs. hyper-specific) for each object category, identifying optimal granularity settings for each type.

### Open Question 3
- Question: How can intra-concept variance in images (e.g., partial occlusions, different viewing angles) be better handled in LVLMs for FGVC, and what role do attributes play in this?
- Basis in paper: [explicit] Section 7 explicitly mentions intra-concept variance as a limitation, noting that images of single concepts can appear in various forms with parts partially occluded.
- Why unresolved: While the paper acknowledges this limitation and mentions that attributes are constructed to be visually-grounded, it does not explore methods to make attributes more robust to intra-concept variance or to adapt the model's attention to handle such variations.
- What evidence would resolve it: Experiments demonstrating improved FGVC performance on challenging images (with occlusions, unusual angles, or low quality) using methods that adapt attribute selection or model attention based on image characteristics.

## Limitations

- The paper does not explore alternative projection mechanisms that could preserve fine-grained visual information during the visual-to-textual embedding transformation
- The generalizability of attribute-focused training improvements across different LVLM architectures beyond LLaVA-1.5 is not thoroughly validated
- The approach does not explicitly address intra-concept variance in images, such as partial occlusions or different viewing angles, which can affect fine-grained classification

## Confidence

- **High Confidence**: The existence of a modality gap between text and image inputs for fine-grained classification is well-supported by empirical evidence showing dramatically different performance on the same concepts depending on input modality.
- **Medium Confidence**: The claim that this modality gap results primarily from information loss during visual-to-textual projection is plausible but not definitively proven, as alternative explanations (such as insufficient training data for fine-grained categories) were not thoroughly ruled out.
- **Medium Confidence**: The effectiveness of attribute-focused training and ATTR SEEK prompting is demonstrated on multiple benchmarks, but the specific mechanisms by which this approach bridges the modality gap require further investigation.

## Next Checks

1. **Projection Analysis**: Implement ablation studies comparing different projection mechanisms (e.g., linear vs non-linear projections) to quantify their impact on fine-grained visual information preservation and classification performance.

2. **Cross-Architecture Validation**: Test the FINER training approach on multiple LVLM architectures beyond LLaVA-1.5 (e.g., BLIP-2, IDEFICS) to assess generalizability and identify architecture-specific limitations.

3. **Long-term Stability Assessment**: Evaluate the performance of FINER-tuned models on held-out data after extended periods to determine whether improvements in fine-grained recognition are stable or subject to catastrophic forgetting.