---
ver: rpa2
title: 'BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities
  of Language Models in Multi-Agent Systems'
arxiv_id: '2408.15971'
source_url: https://arxiv.org/abs/2408.15971
tags:
- position
- tank
- game
- cooperation
- move
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The BattleAgentBench benchmark evaluates LLM cooperation and competition
  capabilities across seven fine-grained stages of increasing difficulty, from single-agent
  navigation to multi-agent hybrid cooperation-competition scenarios. The benchmark
  assesses 11 LLMs (4 API-based, 7 open-source) using metrics including Forward Distance,
  Format Accuracy, Move Accuracy, and Score.
---

# BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems

## Quick Facts
- arXiv ID: 2408.15971
- Source URL: https://arxiv.org/abs/2408.15971
- Reference count: 40
- Primary result: Benchmark evaluates LLM cooperation/competition across 7 difficulty stages with API models outperforming open-source models

## Executive Summary
BattleAgentBench is a comprehensive benchmark designed to evaluate the cooperation and competition capabilities of language models in multi-agent systems. The benchmark features seven fine-grained stages of increasing difficulty, ranging from single-agent navigation to complex multi-agent scenarios involving both cooperation and competition. It assesses 11 different LLMs, including both API-based models and open-source variants, using a combination of spatial navigation, movement accuracy, and collaborative task completion metrics.

The benchmark reveals significant performance gaps between API-based and open-source models, with the former demonstrating superior capabilities in handling complex multi-agent interactions. The evaluation framework measures multiple aspects of agent performance including forward distance, format accuracy, move accuracy, and overall task completion scores. The study provides valuable insights into the current limitations of LLMs in multi-agent scenarios and establishes a foundation for future research in developing more capable collaborative AI systems.

## Method Summary
The BattleAgentBench framework evaluates language models across seven progressive stages, starting with simple single-agent navigation tasks and advancing to complex hybrid scenarios involving both cooperation and competition. The benchmark uses a grid-based environment where agents must navigate, perform actions, and interact with other agents to achieve specific objectives. Performance is measured using four key metrics: Forward Distance (path efficiency), Format Accuracy (instruction following), Move Accuracy (correct action execution), and Score (overall task completion). The evaluation includes 11 LLMs (4 API-based and 7 open-source) tested across all seven stages, with particular focus on their ability to handle spatial reasoning, collaborative planning, and competitive dynamics in multi-agent settings.

## Key Results
- API-based models (claude3.5-sonnet, gpt-4o-mini) significantly outperform open-source models in multi-agent scenarios
- Only claude3.5-sonnet and gpt-4o-mini demonstrate effective cooperation capabilities
- Open-source models struggle particularly with spatial perception and movement accuracy
- Limited improvement from collaboration observed across most models tested

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its structured progression through increasingly complex multi-agent scenarios, which systematically reveals the limitations of current LLMs in handling spatial reasoning, collaborative planning, and competitive dynamics. The grid-based environment provides a controlled yet challenging setting where agents must process spatial information, execute precise movements, and coordinate with or compete against other agents. The multi-metric evaluation approach captures different aspects of agent performance, from basic navigation to complex social interactions, providing a comprehensive assessment of cooperation and competition capabilities.

## Foundational Learning
- Multi-agent system dynamics: Essential for understanding how agents interact, compete, and cooperate in shared environments
  - Why needed: Forms the theoretical basis for designing evaluation scenarios and interpreting results
  - Quick check: Can the model distinguish between cooperative and competitive scenarios and adjust behavior accordingly

- Spatial reasoning in LLMs: Critical for understanding how models process and navigate physical environments
  - Why needed: Spatial perception and movement accuracy are fundamental to multi-agent task success
  - Quick check: Does the model maintain accurate spatial awareness throughout navigation tasks?

- Collaborative planning: Important for evaluating how models coordinate actions with other agents
  - Why needed: Cooperation requires planning and executing actions that benefit multiple agents simultaneously
  - Quick check: Can the model generate action sequences that achieve shared goals with other agents?

- Competitive strategy formulation: Necessary for understanding how models handle adversarial scenarios
  - Why needed: Competition requires strategic thinking and the ability to optimize individual outcomes
  - Quick check: Does the model demonstrate strategic behavior when competing against other agents?

## Architecture Onboarding

Component map: Grid environment -> Agent observation processing -> Action selection -> Performance evaluation -> Multi-metric scoring

Critical path: Environment state → Agent perception → Decision-making → Action execution → Outcome measurement → Score calculation

Design tradeoffs: The benchmark prioritizes controlled, reproducible evaluation over ecological validity. Grid-based environments enable precise measurement of spatial and movement capabilities but may not capture the full complexity of real-world multi-agent interactions.

Failure signatures: Open-source models consistently fail at spatial perception and movement accuracy tasks. Most models show limited improvement from collaboration, suggesting either ineffective collaboration mechanisms or insufficient motivation for cooperative behavior. API-based models perform well on simple tasks but struggle with complex collaborative scenarios.

First experiments:
1. Test single-agent navigation baseline to establish individual agent capabilities
2. Evaluate simple cooperative scenarios with two agents and shared goals
3. Assess competitive scenarios where agents must optimize individual outcomes

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focus on grid-based navigation may not generalize to continuous or real-world environments
- Limited improvement from collaboration suggests potential issues with evaluation metrics or collaboration mechanisms
- Sample size of 11 models may not represent the full spectrum of available LLMs

## Confidence
- Model performance rankings: High
- Cooperation capability assessments: Medium
- Generalizability of results: Low

## Next Checks
1. Test the benchmark with continuous-space variants to assess real-world applicability
2. Conduct human evaluations to validate the automated metrics for cooperation quality
3. Test additional open-source models with enhanced spatial reasoning capabilities to establish performance baselines