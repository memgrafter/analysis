---
ver: rpa2
title: 'Learning to Rewrite: Generalized LLM-Generated Text Detection'
arxiv_id: '2408.04237'
source_url: https://arxiv.org/abs/2408.04237
tags:
- text
- rewrite
- human
- dataset
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Learning to Rewrite: Generalized LLM-Generated Text Detection
  Ran Li; Wei Hao; Weiliang Zhao; Junfeng Yang; Chengzhi Mao Columbia University1,
  Rutgers University2 {rl3424, wh2473, wz2665, jy2324}@columbia.edu, cm1838@rutgers.edu
  Large language models (LLMs) present significant risks when used to generate non-factual
  content and spread disinformation at scale. Detecting such LLM-generated content
  is crucial, yet current detectors often struggle to generalize in open-world contexts.'
---

# Learning to Rewrite: Generalized LLM-Generated Text Detection

## Quick Facts
- **arXiv ID**: 2408.04237
- **Source URL**: https://arxiv.org/abs/2408.04237
- **Reference count**: 32
- **Primary result**: Introduces L2R, a rewrite-based framework that detects AI-generated text with up to 23.04% higher AUROC than state-of-the-art methods across 21 domains and 4 LLMs.

## Executive Summary
This paper introduces Learning2Rewrite (L2R), a novel framework for detecting AI-generated text with exceptional generalization to unseen domains. The method leverages the insight that LLMs inherently modify AI-generated content less than human-written text when tasked with rewriting. By training LLMs to minimize alterations on AI-generated inputs, the framework amplifies the edit distance disparity between human and AI texts, yielding a more distinguishable and generalizable detection signal across diverse text distributions.

## Method Summary
L2R fine-tunes a Llama-3-8B rewrite model using LoRA with a calibrated loss to maximize edits on human text and minimize edits on AI text. The method uses diverse prompts to generate AI texts, capturing real-world variability. The core detection signal is the edit distance between input and rewritten text, with lower distances indicating AI generation. The calibration loss with a threshold prevents overfitting by only updating on hard examples, and LoRA reduces trainable parameters to improve out-of-distribution generalization.

## Key Results
- L2R outperforms state-of-the-art detection methods by up to 23.04% in AUROC for in-distribution tests.
- L2R achieves 37.26% higher AUROC for out-of-distribution tests compared to baselines.
- L2R demonstrates 48.66% better robustness under adversarial attacks (Decoherence, Rewrite).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a rewrite model with a calibrated loss amplifies the edit distance difference between human and AI texts, improving detection generalization.
- Mechanism: The rewrite model is fine-tuned to maximize edits on human texts and minimize edits on AI texts. A calibration loss with a threshold prevents overfitting by only updating weights on hard examples.
- Core assumption: Edit distance (or its differentiable proxy, loss) is a stable signal that differs systematically between human and AI-generated texts across domains.
- Evidence anchors:
  - [abstract] "By training LLMs to minimize alterations on AI-generated inputs, we amplify this disparity, yielding a more distinguishable and generalizable edit distance across diverse text distributions."
  - [section] "Without this loss, the model tends to overfit during fine-tuning as shown in Figure 4, where the model loss drastically decrease after 1500 steps, resulting in verbose rewrite even for LLM-generated text."
  - [corpus] Weak evidence; only related papers found, no direct citations.

### Mechanism 2
- Claim: Using diverse prompts during data generation captures the variability of real-world LLM usage, improving model robustness.
- Mechanism: A dataset of 200 varied rewrite prompts is used to generate AI texts, mimicking real-world diversity in prompt phrasing and instructions.
- Core assumption: Prompt diversity changes the distribution of generated text in ways that a detector trained on diverse prompts can generalize better to unseen prompts.
- Evidence anchors:
  - [abstract] "For each generated text, a prompt is randomly sampled from this dataset."
  - [section] "For Gemini rewrite, training on diversely-prompted dataset increases testing AUROC from 0.7302 to 0.7566. For Llama rewrite, AUROC increases from 0.7888 to 0.7970."
  - [corpus] Weak evidence; related papers found, no direct citations.

### Mechanism 3
- Claim: Reducing the number of trainable parameters via LoRA improves out-of-distribution (OOD) generalization by preventing overfitting.
- Mechanism: LoRA is used with reduced ranks (e.g., r=4, lora_alpha=8) to limit parameter updates during fine-tuning, trading some in-distribution (ID) performance for better OOD results.
- Core assumption: Fewer trainable parameters reduce model capacity to memorize training domain specifics, improving generalization to unseen domains.
- Evidence anchors:
  - [abstract] "Comparing with fine-tuning a Llama-3 model for naive text classification, L2R has 51.35% higher AUROC OOD despite leveraging the same number of parameters."
  - [section] "By adjusting the LoRA parameters r and lora_alpha, we define four fine-tuning configurations with the number of trainable parameters ranging from 851,968 to 6,815,744... As the number of parameters increase from 1 × 106 to 7 × 106, both L2R and RAIDAR show higher ID performance and lower OOD performance."
  - [corpus] Weak evidence; related papers found, no direct citations.

## Foundational Learning

- Concept: Edit distance and its use as a proxy for text similarity
  - Why needed here: Edit distance is the core metric used to distinguish human from AI-generated text by measuring how much a rewrite model changes the input.
  - Quick check question: What are the three types of operations counted in Levenshtein edit distance?

- Concept: Fine-tuning with calibrated loss and thresholding
  - Why needed here: Calibration loss with a threshold prevents overfitting by only updating on hard examples, improving generalization.
  - Quick check question: How does the calibration loss differ from standard cross-entropy loss during fine-tuning?

- Concept: LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning
  - Why needed here: LoRA reduces the number of trainable parameters, which empirically improves OOD generalization in this context.
  - Quick check question: What LoRA parameters control the rank and scaling of the adaptation matrix?

## Architecture Onboarding

- Component map: Rewrite model (LLaMA-3-8B) → Calibration loss → LoRA adapters → Detector classifier
- Critical path: Input text → Rewrite model → Edit distance calculation → Classification
- Design tradeoffs: Parameter reduction via LoRA improves OOD generalization but may slightly reduce ID accuracy; diverse prompts improve robustness but increase dataset complexity.
- Failure signatures: Overfitting (loss plateaus, verbose rewrites), poor OOD performance (low AUROC on unseen domains), sensitivity to prompt diversity (AUROC drops with non-diverse prompts).
- First 3 experiments:
  1. Train with standard cross-entropy loss vs. calibrated loss and compare AUROC on OOD data.
  2. Vary LoRA rank (r=2,4,8,16) and measure trade-off between ID and OOD AUROC.
  3. Train with single prompt vs. diverse prompts and evaluate on OOD dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of trainable parameters for L2R to achieve the best balance between in-distribution and out-of-distribution performance?
- Basis in paper: [inferred] The paper discusses the impact of different LoRA configurations with varying numbers of trainable parameters on L2R's performance, showing a trade-off between in-distribution and out-of-distribution AUROC scores.
- Why unresolved: While the paper provides insights into how different parameter configurations affect performance, it does not determine an optimal number that balances both in-distribution and out-of-distribution performance.
- What evidence would resolve it: Conducting a comprehensive study with a wider range of parameter configurations and evaluating their performance on both in-distribution and out-of-distribution datasets would help identify the optimal balance.

### Open Question 2
- Question: How does L2R perform when applied to multilingual text detection, beyond the English datasets used in the study?
- Basis in paper: [inferred] The paper focuses on English text detection and does not explore the model's performance on multilingual datasets.
- Why unresolved: The effectiveness of L2R on multilingual text detection remains unexplored, which is crucial for real-world applications where text may be in various languages.
- What evidence would resolve it: Testing L2R on multilingual datasets and comparing its performance across different languages would provide insights into its generalizability and effectiveness in multilingual contexts.

### Open Question 3
- Question: Can L2R be adapted to detect other types of AI-generated content, such as images or audio, beyond text?
- Basis in paper: [explicit] The paper focuses on detecting AI-generated text using LLMs, but does not explore its application to other types of media.
- Why unresolved: The potential of L2R to detect AI-generated content in other formats, such as images or audio, is not addressed, limiting its applicability to text-only scenarios.
- What evidence would resolve it: Adapting L2R's methodology to other media types and evaluating its performance on AI-generated images or audio would determine its versatility and effectiveness beyond text detection.

## Limitations

- The core detection signal (edit distance disparity) is assumed stable across domains, but empirical evidence is limited to the 21 tested domains.
- The calibration loss mechanism relies on a threshold derived from logistic regression that is not fully specified, creating potential reproducibility gaps.
- The trade-off between ID and OOD performance via LoRA parameter tuning lacks comprehensive ablation studies across different rank values.

## Confidence

- **High confidence**: The general framework of using rewrite-based edit distance for detection, supported by consistent AUROC improvements across multiple LLMs and domains.
- **Medium confidence**: The specific calibration loss implementation and its effectiveness in preventing overfitting, as the exact threshold derivation is unclear.
- **Low confidence**: The generalizability of the prompt diversity mechanism to real-world scenarios with potentially different prompt distributions than those used in training.

## Next Checks

1. **Threshold derivation validation**: Implement the exact logistic regression-based threshold calculation for the calibration loss and verify its impact on preventing overfitting compared to standard fine-tuning.
2. **Domain transfer robustness test**: Evaluate the detector on an additional set of 10-15 unseen domains (e.g., technical documentation, legal texts) to assess true OOD generalization beyond the M4 dataset.
3. **Prompt distribution stress test**: Systematically vary the diversity and phrasing of rewrite prompts during inference (beyond training data) to measure detector robustness to prompt distribution shifts.