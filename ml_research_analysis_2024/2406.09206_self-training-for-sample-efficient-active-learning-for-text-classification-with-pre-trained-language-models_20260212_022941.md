---
ver: rpa2
title: Self-Training for Sample-Efficient Active Learning for Text Classification
  with Pre-Trained Language Models
arxiv_id: '2406.09206'
source_url: https://arxiv.org/abs/2406.09206
tags:
- learning
- active
- self-training
- hast
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HAST, a self-training approach that improves
  sample efficiency in active learning for text classification. The method combines
  active learning's uncertainty-based instance selection with self-training's pseudo-label
  generation using high-confidence samples.
---

# Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models

## Quick Facts
- arXiv ID: 2406.09206
- Source URL: https://arxiv.org/abs/2406.09206
- Reference count: 28
- Achieves comparable classification results to previous work while using only 25% of the labeled data

## Executive Summary
This paper introduces HAST (Hybrid Active Self-Training), a method that combines active learning with self-training to improve sample efficiency in text classification. HAST uses uncertainty-based instance selection from active learning and pseudo-label generation from self-training, leveraging contrastive representation learning and k-nearest neighbors. The approach balances human-labeled and pseudo-labeled instances through weighting and demonstrates strong performance on four text classification benchmarks using small language models (110M parameters), achieving results comparable to previous work while using only 25% of the labeled data.

## Method Summary
HAST is a self-training approach for active learning in text classification that addresses the challenge of sample efficiency. The method integrates uncertainty-based instance selection from active learning with pseudo-label generation from self-training. It employs contrastive representation learning and k-nearest neighbors to assign pseudo-labels to high-confidence samples, then balances these against human-labeled instances through weighting. The approach is particularly effective when combined with SetFit models and demonstrates strong performance on benchmark datasets while significantly reducing the amount of labeled data required.

## Key Results
- HAST outperforms four reproduced self-training approaches on four text classification benchmarks
- Achieves classification results comparable to previous work while using only 25% of the labeled data
- Particularly effective with SetFit models, sometimes reaching near-final performance within the first few iterations

## Why This Works (Mechanism)
HAST combines the strengths of active learning and self-training to address sample efficiency in text classification. Active learning's uncertainty sampling identifies informative instances that would benefit most from human annotation, while self-training leverages high-confidence predictions to generate pseudo-labels. The contrastive representation learning and k-nearest neighbors provide a robust mechanism for pseudo-label assignment, while the weighting strategy balances the reliability of human-labeled and pseudo-labeled instances. This hybrid approach maximizes the information extracted from limited labeled data while maintaining classification accuracy.

## Foundational Learning
- Active Learning: Uncertainty-based instance selection to identify informative samples for labeling. Why needed: Maximizes the value of each human annotation by focusing on uncertain instances. Quick check: Review uncertainty sampling strategies (e.g., entropy, margin).
- Self-Training: Uses model predictions to generate pseudo-labels for unlabeled data. Why needed: Leverages model confidence to expand training data without additional human annotation. Quick check: Understand confidence thresholding and pseudo-label selection.
- Contrastive Learning: Learns representations by contrasting similar and dissimilar examples. Why needed: Provides meaningful embeddings for k-nearest neighbors pseudo-label assignment. Quick check: Review contrastive loss functions and augmentation strategies.
- K-Nearest Neighbors: Assigns pseudo-labels based on similarity in representation space. Why needed: Provides a simple yet effective method for pseudo-label generation using learned representations. Quick check: Understand distance metrics and neighbor selection.
- Instance Weighting: Balances contributions of human-labeled and pseudo-labeled instances during training. Why needed: Maintains training stability and prevents pseudo-label noise from dominating. Quick check: Review weighting strategies and their impact on convergence.

## Architecture Onboarding
- Component Map: Data → Active Learning (uncertainty sampling) → Contrastive Learning (representation) → K-NN (pseudo-labeling) → Weighted Training (human + pseudo) → Classification Model
- Critical Path: The most critical components are the active learning selection strategy and the contrastive learning representation, as they directly impact the quality of pseudo-labels and overall performance.
- Design Tradeoffs: The method trades increased computational overhead from self-training for reduced labeling costs. Using smaller models (110M parameters) improves efficiency but may limit performance on complex tasks.
- Failure Signatures: Poor pseudo-label quality due to inadequate contrastive representations or improper confidence thresholds could degrade performance. Over-reliance on pseudo-labels without sufficient human supervision may cause model drift.
- Three First Experiments: 1) Evaluate pseudo-label quality on a validation set to assess representation learning effectiveness. 2) Test different uncertainty sampling strategies to optimize instance selection. 3) Vary the weighting ratio between human and pseudo-labeled instances to find optimal balance.

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Experimental scope limited to small language models (110M parameters), raising questions about scalability to larger models
- Pseudo-label generation relies on k-nearest neighbors and contrastive learning without thorough discussion of potential noise issues
- Computational overhead of self-training process not explicitly discussed
- Claims about 25% data reduction based on comparisons with reproduced approaches rather than direct state-of-the-art comparisons

## Confidence
- High confidence: The experimental methodology and implementation details are clearly described and reproducible
- Medium confidence: The performance improvements and sample efficiency claims are supported by empirical results on the tested benchmarks
- Low confidence: The generalizability of HAST to larger language models, different task types, and real-world applications

## Next Checks
1. Evaluate HAST's performance on larger language models (1B+ parameters) to assess scalability and potential performance changes
2. Test HAST on diverse NLP tasks beyond text classification, such as named entity recognition or sentiment analysis, to evaluate generalizability
3. Conduct ablation studies to quantify the contribution of each component (active learning, self-training, contrastive learning) to overall performance