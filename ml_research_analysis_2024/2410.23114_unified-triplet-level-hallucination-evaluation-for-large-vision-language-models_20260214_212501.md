---
ver: rpa2
title: Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models
arxiv_id: '2410.23114'
source_url: https://arxiv.org/abs/2410.23114
tags:
- hallucination
- lvlms
- image
- evaluation
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified framework for evaluating object
  and relation hallucinations in Large Vision-Language Models (LVLMs) by analyzing
  (object, relation, object) triplets extracted from model responses. The authors
  construct Tri-HE, a novel benchmark that enables fine-grained evaluation of both
  types of hallucinations simultaneously.
---

# Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models

## Quick Facts
- arXiv ID: 2410.23114
- Source URL: https://arxiv.org/abs/2410.23114
- Reference count: 40
- Key outcome: Introduces Tri-HE benchmark for fine-grained evaluation of object and relation hallucinations in LVLMs, finding relation hallucination is more severe than object hallucination, and proposes effective training-free mitigation via self-alignment

## Executive Summary
This paper addresses the critical challenge of evaluating and mitigating hallucinations in Large Vision-Language Models (LVLMs) by introducing a unified triplet-level evaluation framework. The authors construct Tri-HE, a novel benchmark that enables fine-grained analysis of both object and relation hallucinations by extracting (object, relation, object) triplets from model responses. Through comprehensive evaluations across multiple LVLMs including GPT-4V, the study reveals that relation hallucination is consistently more severe than object hallucination. Based on these findings, the authors propose a simple yet effective training-free hallucination mitigation method using self-alignment through prompt engineering, which significantly reduces hallucination rates across models.

## Method Summary
The authors develop a triplet-level hallucination evaluation framework that extracts (object, relation, object) triplets from LVLM responses and compares them against ground truth scene graphs. They construct the Tri-HE benchmark using 300 images from the GQA dataset, generating 10 questions per image requiring commonsense reasoning via GPT-4V. For evaluation, they use either GPT-4 or NLI models to judge whether extracted triplets can be logically inferred from ground truth triplets. The framework calculates two hallucination rates: HalluQ (question-level) and HalluI (image-level). For mitigation, they propose a training-free method that first describes the image, then generates responses without direct visual access, leveraging the model's text-only LLM capabilities to reduce modality misalignment.

## Key Results
- Relation hallucination is consistently more severe than object hallucination across all evaluated LVLMs, including GPT-4V
- LLaVA-1.5 with the proposed self-alignment mitigation method outperforms all open-sourced counterparts and achieves comparable performance to GPT-4V on Tri-HE
- The mitigation method significantly reduces hallucination rates while maintaining reasonable response quality
- Hallucination rates correlate with response length, with longer responses showing higher hallucination rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Triplet-level hallucination evaluation provides more accurate and fine-grained assessment than object-level or sentence-level methods.
- Mechanism: By extracting (object, relation, object) triplets from LVLM responses and comparing them against ground truth scene graphs, the framework can distinguish between object hallucinations, relation hallucinations, and prediction errors at the semantic level.
- Core assumption: Triplets capture sufficient semantic information to uniquely identify hallucinations while being generalizable across different vision-language tasks.
- Evidence anchors:
  - [abstract] "conduct hallucination evaluation on (object, relation, object) triplets extracted from LVLMs' responses"
  - [section 3.1] "we first define, G = (V, E) as the scene graph of I, where V and E refer to all the objects existing in I and all the possible relations among existing objects"
  - [section 3.3] "knowledge graph Gθ extracted from Aθ via prompting GPT-4"
- Break condition: If the triplet extraction process fails to capture relevant semantic relationships or if scene graphs are incomplete, the evaluation may miss hallucinations.

### Mechanism 2
- Claim: Relation hallucination is more severe than object hallucination in LVLMs, particularly for object pairs the models are less familiar with.
- Mechanism: LVLMs tend to correctly identify objects but frequently assign incorrect relations between them, especially for infrequent object pairs in the training data.
- Core assumption: The severity of relation hallucination correlates with the frequency of object pairs in the training data and the model's reasoning capabilities.
- Evidence anchors:
  - [abstract] "relation hallucination is more severe than object hallucination across existing LVLMs, including GPT-4V"
  - [section 5.2] "all the LVLMs generate more relation hallucinations than object hallucinations. A possible explanation is that existing LVLMs lack reasoning abilities"
  - [section 5.3] "all the LVLMs have significantly lower relation hallucination rates on frequent object pairs they are familiar with"
- Break condition: If relation hallucination severity varies significantly across different types of vision-language tasks or model architectures.

### Mechanism 3
- Claim: Training-free hallucination mitigation through self-alignment via prompt engineering effectively reduces hallucination rates.
- Mechanism: By first describing the image (with visual access), then generating responses based solely on the description (without visual access), the model leverages its text-only LLM backbone to reduce modality misalignment.
- Core assumption: Disabling direct visual access forces the model to rely on its language understanding capabilities rather than potentially unreliable visual processing.
- Evidence anchors:
  - [abstract] "propose a simple training-free hallucination mitigation method using self-alignment through prompt engineering"
  - [section 5.4] "we disable the direct visual access of LVLMs to alleviate hallucinations resulting from modality misalignment"
  - [section 5.4] "we propose a training-free LVLM hallucination mitigation method via self-alignment [12, 40, 41]"
- Break condition: If the image description step itself introduces hallucinations or if the model cannot effectively reason from textual descriptions alone.

## Foundational Learning

- Concept: Scene graphs and knowledge graphs
  - Why needed here: The evaluation framework relies on comparing extracted triplets against ground truth scene graphs to identify hallucinations
  - Quick check question: Can you explain the difference between a scene graph (V, E) and a knowledge graph (V', E') as defined in the paper?

- Concept: Natural Language Inference (NLI) and triplet-level comparison
  - Why needed here: The paper uses NLI models to determine if extracted triplets can be logically inferred from ground truth triplets
  - Quick check question: How does the NLI-based judgment differ from the GPT-4-based judgment in terms of hallucination detection?

- Concept: Prompt engineering for multimodal models
  - Why needed here: The mitigation method relies on carefully designed prompts to first describe images and then generate responses without visual access
  - Quick check question: What is the purpose of the "triplet description" prompt variant in the hallucination mitigation method?

## Architecture Onboarding

- Component map: 
  - Image input → Scene graph extraction (from GQA dataset) → Question generation (via GPT-4V) → LVLM response generation → Triplet extraction (via GPT-4) → Hallucination judgment (via GPT-4 or NLI) → Hallucination rate calculation
  - Mitigation pipeline: Image input → LVLM response (with visual access) → Image description (triplet extraction) → LVLM response (without visual access) → Hallucination evaluation

- Critical path: 
  - For evaluation: Image → Scene graph → Question → LVLM response → Triplet extraction → Hallucination judgment → Metrics
  - For mitigation: Image → Image description → Response generation (eyes closed) → Evaluation

- Design tradeoffs:
  - Using GPT-4 for triplet extraction and judgment provides higher accuracy but increases computational cost and dependency on external APIs
  - The NLI-based approach is faster but less accurate according to the correlation study with human judgments
  - The mitigation method trades potential loss of visual detail for reduced hallucination through modality separation

- Failure signatures:
  - High object hallucination rates may indicate issues with object recognition in the LVLM
  - High relation hallucination rates suggest problems with the model's reasoning or understanding of spatial/temporal relationships
  - Inconsistent results between GPT-4 and NLI judges may indicate ambiguity in the hallucination definitions or extraction process

- First 3 experiments:
  1. Reproduce the baseline hallucination rates for MiniGPT-4 on the Tri-HE benchmark using both GPT-4 and NLI judges
  2. Test the triplet description mitigation method on MiniGPT-4 and compare hallucination rates with the baseline
  3. Analyze the correlation between response length and hallucination rates by truncating responses to different token counts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different hallucination mitigation strategies compare in effectiveness when applied to large diffusion models beyond LVLMs?
- Basis in paper: [explicit] The paper discusses extending the triplet-level evaluation to diffusion models and enhancing hallucination mitigation with stronger vision encoders and visual tools.
- Why unresolved: The current study focuses on LVLMs, and while it mentions potential extensions to diffusion models, it does not provide empirical results or comparisons for such models.
- What evidence would resolve it: Conducting experiments to evaluate and compare the effectiveness of various hallucination mitigation strategies on diffusion models using the triplet-level evaluation framework.

### Open Question 2
- Question: What is the impact of using different types of vision encoders on the hallucination rates in LVLMs?
- Basis in paper: [explicit] The paper suggests using stronger vision encoders to enhance hallucination mitigation in LVLMs.
- Why unresolved: The study does not explore or compare the impact of different vision encoders on hallucination rates, leaving the potential benefits and trade-offs unexplored.
- What evidence would resolve it: Performing experiments with various vision encoders to assess their impact on hallucination rates and comparing the results to determine the most effective encoder.

### Open Question 3
- Question: How does the triplet-level hallucination evaluation framework perform in real-world applications compared to traditional benchmarks?
- Basis in paper: [inferred] The paper introduces a novel triplet-level evaluation framework and benchmark, suggesting its potential for real-world applications.
- Why unresolved: While the framework is introduced and tested, its performance and applicability in real-world scenarios compared to traditional benchmarks are not evaluated.
- What evidence would resolve it: Implementing the triplet-level evaluation framework in real-world applications and comparing its performance and insights with those from traditional hallucination benchmarks.

## Limitations
- Heavy reliance on GPT-4 for both triplet extraction and hallucination judgment creates potential circular dependencies
- Assumes scene graphs from GQA dataset provide complete ground truth, which may not capture all valid interpretations of ambiguous visual scenes
- Mitigation method's effectiveness may vary across different image types and question domains not well-represented in the GQA dataset

## Confidence

**High Confidence**: The observation that relation hallucinations exceed object hallucinations across multiple LVLMs, supported by consistent results across different models and the logical explanation regarding reasoning capabilities.

**Medium Confidence**: The effectiveness of the training-free mitigation method, as results are based on comparisons with other open-sourced LVLMs but limited testing on diverse image types and reasoning tasks.

**Low Confidence**: The generalizability of the framework to non-English languages and cross-modal tasks beyond the GQA dataset, given the reliance on English-language scene graphs and question-answering format.

## Next Checks
1. **Generalizability Test**: Apply the evaluation framework to images from other datasets (e.g., Visual Genome, COCO) to verify consistent hallucination patterns across different visual domains and ground truth sources.

2. **Mitigation Robustness**: Test the self-alignment mitigation method on images with complex spatial relationships and temporal reasoning requirements to assess performance degradation in challenging scenarios.

3. **Cross-Lingual Evaluation**: Translate questions and responses to multiple languages and repeat the hallucination analysis to determine if the observed patterns hold across linguistic contexts.