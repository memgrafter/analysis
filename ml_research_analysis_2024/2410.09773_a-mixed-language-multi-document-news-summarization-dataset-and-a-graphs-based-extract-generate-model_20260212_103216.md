---
ver: rpa2
title: A Mixed-Language Multi-Document News Summarization Dataset and a Graphs-Based
  Extract-Generate Model
arxiv_id: '2410.09773'
source_url: https://arxiv.org/abs/2410.09773
tags:
- document
- summarization
- news
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the MLMD-news dataset, the first mixed-language
  multi-document news summarization dataset containing 10,992 source document cluster
  and target summary pairs across four languages (English, German, French, Spanish).
  The authors propose a graph-based extract-generate model that uses graph neural
  networks to extract key sentences from mixed-language document clusters and then
  generates summaries using pre-trained models.
---

# A Mixed-Language Multi-Document News Summarization Dataset and a Graphs-Based Extract-Generate Model

## Quick Facts
- arXiv ID: 2410.09773
- Source URL: https://arxiv.org/abs/2410.09773
- Authors: Shengxiang Gao; Fang nan; Yongbing Zhang; Yuxin Huang; Kaiwen Tan; Zhengtao Yu
- Reference count: 16
- Primary result: Introduces MLMD-news dataset and achieves SOTA ROUGE-1 of 39.16, ROUGE-2 of 9.64, and ROUGE-L of 34.02

## Executive Summary
This paper introduces MLMD-news, the first mixed-language multi-document news summarization dataset containing 10,992 source document cluster and target summary pairs across four languages (English, German, French, Spanish). The authors propose a graph-based extract-generate model that uses graph neural networks to extract key sentences from mixed-language document clusters and then generates summaries using pre-trained models. The proposed method achieves state-of-the-art performance with ROUGE-1 of 39.16, ROUGE-2 of 9.64, and ROUGE-L of 34.02 on the MLMD-news dataset, outperforming existing baselines including various abstractive models and large language models.

## Method Summary
The paper proposes a graph-based extract-generate model for mixed-language multi-document summarization. The approach constructs homogeneous graphs between sentences and heterogeneous graphs between sentences and words to capture complex relationships across multiple documents and languages. An extractor using Graph Attention Networks (GAT) learns sentence representations and identifies top-K key sentences. A generator based on mBART then creates summaries from these extracted sentences. The model incorporates a consistency loss using KL divergence between dynamic weights from the generator and scores from the extractor. The MLMD-news dataset was constructed from Multi-News through round-trip translation, containing 10,992 document clusters with varying language combinations.

## Key Results
- Achieves state-of-the-art ROUGE-1 score of 39.16, ROUGE-2 of 9.64, and ROUGE-L of 34.02 on MLMD-news dataset
- Outperforms existing abstractive models and large language models on mixed-language multi-document summarization task
- Demonstrates effectiveness of graph-based approach for modeling complex relationships across multiple documents and languages
- Shows sensitivity to top-K parameter, with performance improving as more sentences are extracted (up to computational limits)

## Why This Works (Mechanism)

### Mechanism 1
Graph neural networks effectively model complex relationships between sentences across multiple documents and languages. The model constructs homogeneous graphs (between sentences) and heterogeneous graphs (between sentences and words) to capture inter-sentence and intra-sentence relationships respectively, using GAT and heterogeneous GAT to learn sentence representations that incorporate both types of information. The core assumption is that relationships between sentences and words across mixed-language documents can be effectively represented as graph structures where nodes represent sentences/words and edges represent semantic connections. Break condition: If semantic relationships between sentences across different languages cannot be effectively captured by TF-IDF-based edge weights or if graph construction fails to preserve language-specific nuances.

### Mechanism 2
The extract-then-generate approach addresses excessive input length while preserving important information. The extractor uses learned sentence representations to identify and extract key sentences (top-K selection), reducing input to manageable size while maintaining critical information for summary generation. The core assumption is that key sentences extracted based on similarity to target summaries will contain sufficient information for generating high-quality summaries. Break condition: If extraction process fails to identify truly important sentences or if critical information is distributed across too many sentences to be captured by top-K selection.

### Mechanism 3
Consistency loss using KL divergence ensures alignment between extraction and generation processes. The generator produces dynamic weights representing probability of selecting each extracted sentence for summary generation, and KL divergence loss encourages these weights to align with extractor's predicted scores, creating consistency between components. The core assumption is that dynamic weights from generator should reflect importance scores from extractor, as both are making similar decisions about sentence importance. Break condition: If generator's dynamic weights diverge significantly from extractor's scores, indicating fundamental disconnect between extraction and generation processes.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and Graph Attention Networks (GATs)
  - Why needed here: The paper relies heavily on GAT for learning sentence representations from graph structures representing document relationships
  - Quick check question: What is the key difference between GAT and traditional GNNs, and why is this important for sentence representation learning?

- Concept: Cross-entropy loss and KL divergence
  - Why needed here: The model uses cross-entropy for the extractor and KL divergence for consistency loss, requiring understanding of both loss functions
  - Quick check question: How does KL divergence differ from cross-entropy, and why is it appropriate for measuring consistency between two probability distributions?

- Concept: Multi-language model capabilities (mBART, mT5)
  - Why needed here: The generator uses mBART for its multilingual understanding and generation capabilities
  - Quick check question: What specific features of mBART make it suitable for mixed-language summarization tasks compared to monolingual models?

## Architecture Onboarding

- Component map: Graph Construction → Extractor (with Cross-entropy loss) → Generator (with NLL and KL divergence losses) → Summary Output

- Critical path: Graph Construction → Extractor (with Cross-entropy loss) → Generator (with NLL and KL divergence losses) → Summary Output

- Design tradeoffs:
  - Using graph structures adds computational complexity but captures richer relationships than sequential models
  - Top-K extraction balances input length reduction against information preservation
  - Consistency loss adds training complexity but improves alignment between components

- Failure signatures:
  - Poor ROUGE scores indicate issues with either extraction quality or generation quality
  - High KL divergence loss values suggest misalignment between extractor and generator
  - Memory errors during graph construction may indicate need for smaller batch sizes or graph simplification

- First 3 experiments:
  1. Run with top-K=1 to verify basic functionality and identify if extraction is working
  2. Test with homogeneous graph only (remove heterogeneous graph) to measure impact of word-level relationships
  3. Disable consistency loss to measure its contribution to overall performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MLMD summarization models vary across different language combinations in the document clusters? The paper mentions that the MLMD-news dataset contains document clusters with varying language combinations, with 2-language clusters being most common and 4-language clusters being least common, but does not report performance differences across these combinations. This remains unresolved as the benchmark experiments report overall ROUGE scores but do not analyze how performance varies with different language combinations. Detailed performance analysis showing ROUGE scores for models across different language combinations (2-language, 3-language, 4-language clusters) would clarify how language diversity impacts summarization quality.

### Open Question 2
What is the optimal number of sentences to extract for mixed-language multi-document summarization, and how does this vary with document cluster characteristics? The paper conducts a parameter sensitivity analysis for top-K values but only tests up to K=13 due to GPU limitations, noting that higher values might improve performance but were not validated. This remains unresolved as the paper acknowledges that increasing top-K beyond 13 might yield better results but could not test this due to computational constraints. Comprehensive experiments testing a wider range of top-K values across different cluster sizes and language combinations would identify the optimal extraction strategy.

### Open Question 3
How does the proposed graph-based extract-generate model perform on low-resource languages compared to resource-rich languages? The paper acknowledges that the current MLMD-news dataset primarily includes resource-rich languages (English, German, French, Spanish) and suggests future work could extend to more languages, especially low-resource ones. This remains unresolved as the current dataset and experiments are limited to four resource-rich languages, so the model's effectiveness on low-resource language pairs remains untested. Experiments applying the model to document clusters containing low-resource languages with appropriate evaluation metrics would demonstrate its generalization capability across language resource levels.

## Limitations
- The generalizability of the graph-based approach to other mixed-language domains beyond news summarization remains uncertain
- The dataset construction through round-trip translation may introduce artifacts that don't reflect natural mixed-language document clusters
- The fixed top-K selection strategy may be suboptimal, as different document clusters likely require different numbers of extracted sentences
- The paper lacks human evaluation to assess quality and coherence of generated summaries, particularly for mixed-language content

## Confidence
- **High Confidence**: The mechanism of using graph neural networks to model sentence relationships across documents and languages is well-supported by the paper's methodology section and the established success of GNNs in NLP tasks
- **Medium Confidence**: The claim about achieving state-of-the-art performance is supported by benchmark results, but without comparisons to a broader range of recent LLMs or cross-lingual models, the full extent of the performance gains remains uncertain
- **Low Confidence**: The effectiveness of the consistency loss in aligning the extractor and generator is assumed but not empirically validated through ablation studies that would show its specific contribution to performance improvements

## Next Checks
1. Conduct an ablation study removing the consistency loss to quantify its specific contribution to the reported ROUGE scores, measuring changes in both performance metrics and KL divergence values

2. Perform human evaluation studies comparing summaries generated by the proposed model versus strong baselines, focusing on coherence, relevance, and language quality, particularly for summaries containing mixed-language content

3. Test the model's robustness by evaluating performance on document clusters with different numbers of documents (varying from 2 to 10) and different language combinations to assess scalability and generalizability