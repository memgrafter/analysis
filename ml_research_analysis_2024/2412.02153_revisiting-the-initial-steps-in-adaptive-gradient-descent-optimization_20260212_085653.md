---
ver: rpa2
title: Revisiting the Initial Steps in Adaptive Gradient Descent Optimization
arxiv_id: '2412.02153'
source_url: https://arxiv.org/abs/2412.02153
tags:
- adam
- initialization
- learning
- gradient
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability in Adam optimizer caused by
  zero initialization of the second-order moment (v0=0), which leads to aggressive
  sign-descent behavior in early training steps. The authors propose initializing
  v0 with non-zero values using either data-driven statistics from training data or
  random positive values from a scaled chi-squared distribution.
---

# Revisiting the Initial Steps in Adaptive Gradient Descent Optimization

## Quick Facts
- arXiv ID: 2412.02153
- Source URL: https://arxiv.org/abs/2412.02153
- Authors: Abulikemu Abuduweili; Changliu Liu
- Reference count: 40
- Primary result: Simple non-zero initialization of Adam's second-order moment (v0) significantly improves optimization stability and performance across multiple tasks

## Executive Summary
This paper identifies a fundamental instability in the Adam optimizer caused by zero initialization of the second-order moment (v0=0), which leads to sign-descent behavior in the first update step. The authors propose initializing v0 with non-zero values using either data-driven statistics from training data or random positive values from a scaled chi-squared distribution. This simple modification stabilizes the optimization process by preventing overly large initial updates. Experiments across image classification, language modeling, neural machine translation, and image generation tasks demonstrate that the proposed initialization improves Adam's performance and eliminates the need for learning rate warmup while achieving better stability and convergence.

## Method Summary
The paper proposes initializing the second-order moment estimate (v0) in Adam with non-zero values to prevent the optimizer from degenerating into sign-descent behavior during the first update step. Two initialization strategies are proposed: data-driven initialization using gradient statistics from a sample of training data, and random initialization using scaled chi-squared distributions. The initialization is parameterized by a scaling factor σ that controls the magnitude of the initial second-order moment estimates. The approach is tested on ResNet architectures for image classification, LSTMs for language modeling, Transformer models for machine translation, and GANs for image generation.

## Key Results
- Adam with data-driven initialization achieves 96.24% test accuracy on CIFAR-10, outperforming standard Adam (95.12%) and newer optimizers like AdaBelief (95.85%)
- The proposed initialization eliminates the need for learning rate warmup while achieving better stability and convergence
- Adam with proposed initialization achieves BLEU scores of 35.12 on IWSLT'14 DE-EN translation task, surpassing standard Adam (34.56) and other optimizers
- The method shows consistent improvements across diverse tasks including image classification, language modeling, machine translation, and image generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero initialization of the second-order moment (v0=0) causes Adam to exhibit sign-descent behavior in its first update step, leading to instability.
- Mechanism: With v0=0, the first Adam update simplifies to θ1 = θ0 - α·sign(g1), where the step size depends only on the learning rate rather than gradient magnitude. This creates overly large initial updates that push the optimizer away from initial regions in parameter space.
- Core assumption: The loss landscape at initialization is flat with small gradients, making sign-descent behavior particularly problematic.
- Evidence anchors:
  - [abstract] "the standard initialization of the second-order moment estimation (v0 =0) as a significant factor contributing to these limitations"
  - [section 2.2] "the first step of the optimization process degenerates into sign descent, where the magnitude of the step size depends solely on the learning rateα rather than the full gradient"
  - [corpus] Weak evidence - related papers discuss Adam convergence but not this specific initialization issue

### Mechanism 2
- Claim: Non-zero initialization of v0 stabilizes Adam by preventing excessive variance in early updates.
- Mechanism: Initializing v0 with data-driven statistics or random positive values ensures E[v0] > 0, which prevents the optimizer from taking pure sign-descent steps and maintains stable adaptive learning rates from the start.
- Core assumption: The steady-state value of v should be positive and reflect gradient statistics.
- Evidence anchors:
  - [section 2.3] "initializing the second-order momentv0 with non-zero values effectively prevents the first step of Adam from degenerating into sign descent"
  - [section 2.3] "E[vt] ≈ ¯g2 + σ2I" shows the expected steady-state value is positive
  - [corpus] No direct evidence in corpus papers about this specific mechanism

### Mechanism 3
- Claim: The proposed initialization reduces drift between initial and steady-state second-moment estimates.
- Mechanism: By initializing v0 closer to its steady-state value E[v∞] = ¯g2 + σ2I, the optimizer requires less adjustment during training, leading to more stable convergence.
- Core assumption: Smaller drift in second-moment estimates leads to more stable optimization.
- Evidence anchors:
  - [section 2.3] "driftvt(v0) = ∥E[v∞] − E[v0]∥" defines the drift metric
  - [section 2.3] "driftvt(v0 = ¯g2 + σ2I) = 0" shows zero drift with proper initialization
  - [corpus] No corpus evidence supporting this specific drift-based explanation

## Foundational Learning

- Concept: Stochastic gradient descent and adaptive gradient methods
  - Why needed here: Understanding how Adam differs from basic SGD and why adaptive methods need moment estimates
  - Quick check question: What is the key difference between SGD and Adam in terms of how they compute update steps?

- Concept: Exponential moving averages and bias correction
  - Why needed here: Adam uses exponential decay for both first and second moment estimates, and the paper discusses how initialization affects these
  - Quick check question: How do β1 and β2 parameters affect the bias correction terms in Adam's update equations?

- Concept: Loss landscape characteristics in deep learning
  - Why needed here: The paper assumes flat loss landscapes at initialization with small gradients, which is crucial for understanding why sign-descent is problematic
  - Quick check question: Why do neural networks typically have small gradients at the beginning of training?

## Architecture Onboarding

- Component map:
  - Optimizer state tracking (first and second moment estimates) -> Parameter update computation using adaptive learning rates -> Initialization module for second-order moment estimates -> Data preprocessing for gradient statistics computation

- Critical path:
  1. Before training: Compute gradient statistics from sample data (for data-driven init)
  2. Initialize optimizer with non-zero v0
  3. During training: Regular Adam updates proceed with stabilized initial steps
  4. Monitor convergence and final performance

- Design tradeoffs:
  - Data-driven vs random initialization: Data-driven is more principled but requires extra computation; random is simpler but less tailored
  - Initialization scale (σ parameter): Too small defeats the purpose, too large prevents proper adaptation
  - Compatibility with existing optimizer implementations: Must integrate cleanly without breaking other features

- Failure signatures:
  - If v0 is initialized too large: Poor early convergence, optimizer may be overly conservative
  - If v0 is initialized with wrong statistics: May not help stability, could even hurt performance
  - If implementation misses bias correction: Could lead to incorrect updates

- First 3 experiments:
  1. Compare training loss curves for Adam with v0=0 vs v0=rnd on a simple CNN task
  2. Measure update step sizes in first few iterations for different v0 initializations
  3. Test sensitivity to initialization scale parameter σ across multiple tasks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The theoretical explanation of sign-descent behavior relies on assumptions about flat loss landscapes at initialization that are not empirically verified
- While random initialization (rnd) shows empirical benefits, it lacks theoretical justification compared to the data-driven approach
- The paper does not explore the interaction between proposed initialization and learning rate warmup strategies, which are commonly used in practice

## Confidence

- **High Confidence**: The experimental results showing improved performance across multiple tasks with non-zero initialization are well-supported by the data.
- **Medium Confidence**: The theoretical explanation of why zero initialization causes sign-descent is logically sound but relies on assumptions about loss landscape characteristics that are not empirically verified.
- **Low Confidence**: The claim that the proposed initialization eliminates the need for learning rate warmup is based on limited experimental evidence and may not generalize to all tasks and architectures.

## Next Checks
1. Conduct ablation studies to isolate the contribution of initialization scale parameter σ versus the initialization strategy itself.
2. Test the proposed initialization across a broader range of architectures (Transformers, LSTMs) and tasks (reinforcement learning, GAN training) to assess generalizability.
3. Compare the proposed initialization with other recent Adam variants (AdaBelief, NAdam, RAdam) to establish relative performance improvements.