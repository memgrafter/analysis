---
ver: rpa2
title: Dependency Graph Parsing as Sequence Labeling
arxiv_id: '2410.17972'
source_url: https://arxiv.org/abs/2410.17972
tags:
- parsing
- dependency
- encodings
- encoding
- arcs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends sequence labeling to dependency graph parsing,
  introducing unbounded and bounded encodings for graphs with reentrancy and cycles.
  Positional encodings were less effective, while bracketing-based methods performed
  robustly, especially in dense semantic dependency parsing datasets.
---

# Dependency Graph Parsing as Sequence Labeling

## Quick Facts
- **arXiv ID:** 2410.17972
- **Source URL:** https://arxiv.org/abs/2410.17972
- **Reference count:** 40
- **Primary result:** Sequence labeling can parse dependency graphs with competitive accuracy to biaffine parsers, using bounded encodings (4k, 6k bits) for efficiency and multiplanarity (k > 2) for accuracy in dense graphs.

## Executive Summary
This work extends sequence labeling to dependency graph parsing, introducing unbounded and bounded encodings for graphs with reentrancy and cycles. Positional encodings were less effective, while bracketing-based methods performed robustly, especially in dense semantic dependency parsing datasets. Bounded encodings, using fixed bits per label, excelled in sparser enhanced Universal Dependencies parsing. Across datasets, these parsers achieved competitive or superior results to strong biaffine baselines, confirming that graph parsing is viable as sequence labeling. Increasing the number of subgraph planes (k > 2) notably improved performance for bounded encodings in denser graphs, even when coverage gains were small. The approach combines simplicity, flexibility, and strong empirical accuracy.

## Method Summary
The method frames dependency graph parsing as sequence labeling by encoding arcs as per-word labels. Unbounded encodings use position or bracketing information, while bounded encodings use fixed-length bit vectors per label. The approach employs Transformer-based encoders (XLM-RoBERTa, XLNet) or BiLSTMs with feed-forward decoders, and decoding relies on plane assignment and stack-based arc construction. Training uses AdamW with dataset-specific learning rates, and evaluation uses SDP toolkit for UF/LF metrics.

## Key Results
- Bounded encodings (B4k, B6k) outperformed unbounded ones in IWPT datasets.
- Multiplanarity (k > 2) improved accuracy in dense semantic dependency graphs.
- Sequence labeling parsers achieved competitive or superior F1 scores compared to biaffine baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph parsing can be encoded as sequence labeling by representing arcs with per-word labels.
- Mechanism: The encoding schemes (positional, bracketing, bounded) translate a dependency graph into a sequence of labels, one per word, so that standard sequence labeling models can learn the mapping from sentence to parse.
- Core assumption: There exists an injective encoding from the set of graphs to the label space, and the model can learn the corresponding decoding.
- Evidence anchors:
  - [abstract] "Experimental results on semantic dependency and enhanced UD parsing show that with a good choice of encoding, sequence-labeling dependency graph parsers combine high efficiency with accuracies close to the state of the art, in spite of their simplicity."
  - [section] "If we train a tagger to predict a function fΘ : V n → Ln (where V is the vocabulary of possible input tokens, and Θ are the model parameters) that associates each word w with the encoding of its parse, we can obtain the parse for a sentence w as E −1(fΘ(w))."
  - [corpus] No explicit corpus-level evidence; inferred from successful parsing results across multiple datasets.
- Break condition: If the encoding is not injective or the model fails to generalize from training encodings to unseen structures.

### Mechanism 2
- Claim: Bounded encodings with fixed bits per label enable efficient learning even in sparser graphs.
- Mechanism: By limiting each label to a fixed-length bit vector (4k or 6k bits), the label space becomes finite and compact, reducing sparsity and improving learning efficiency, especially for enhanced UD datasets.
- Core assumption: The dependency graph can be decomposed into a fixed number of relaxed 1-planar subgraphs (for 4k-bit) or left/right subgraph pairs (for 6k-bit) without breaking structural properties.
- Evidence anchors:
  - [abstract] "Bounded encodings, using fixed bits per label, excelled in sparser enhanced Universal Dependencies parsing."
  - [section] "It uses a sequence of 4 bits to encode the arcs related to the word wi that are in the jth subgraph."
  - [corpus] Empirical results in Table 2 show bounded encodings (B4k, B6k) outperform unbounded ones in IWPT datasets.
- Break condition: If the graph cannot be decomposed into the required number of subgraphs, leading to high theoretical coverage loss.

### Mechanism 3
- Claim: Multiplanarity (using k > 2) improves accuracy even when coverage gains are small.
- Mechanism: Allowing arcs to be distributed across more than two planes reduces the risk of conflicting dependencies in a single plane, thus enabling the model to capture more complex graph structures.
- Core assumption: Most dependency graphs are relaxed k-planar for small k, so increasing k only marginally increases coverage but reduces conflicts.
- Evidence anchors:
  - [abstract] "Increasing the number of subgraph planes (k > 2) notably improved performance for bounded encodings in denser graphs, even when coverage gains were small."
  - [section] "The rationale was the trend that most syntactic trees are 2-planar... however, since dependency graphs can be denser... we also experiment with adding a third plane..."
  - [corpus] Tables 1 and 2 show consistent improvements in accuracy for B4k and B6k when k is increased from 2 to 3 or 4.
- Break condition: If the graph is extremely dense or contains many cycles that require more planes than practical, the encoding may become inefficient.

## Foundational Learning

- Concept: Injective mapping between parse structures and label sequences.
  - Why needed here: Without injectivity, different graphs could map to the same label sequence, making the task ambiguous for the model.
  - Quick check question: If two different dependency graphs produce the same label sequence under a given encoding, what would happen during decoding?

- Concept: Multiplanarity and relaxed k-planarity.
  - Why needed here: These concepts define the class of graphs each encoding can handle; understanding them is key to choosing k.
  - Quick check question: In a 2-planar encoding, can two arcs in the same direction cross? Why or why not?

- Concept: Bit-vector decomposition of graph arcs.
  - Why needed here: Bounded encodings rely on grouping bits to represent arcs per subgraph; knowing how bits are allocated is essential for implementing the plane assignment algorithm.
  - Quick check question: In the 4k-bit encoding, what does each group of four bits represent?

## Architecture Onboarding

- Component map:
  - Encoder (Transformer or BiLSTM) -> Decoder (Feed-forward for arc labeling) -> Decoder (Feed-forward for dependency typing) -> Plane assignment + stack-based arc construction

- Critical path:
  1. Input sentence → contextualized embeddings (Eθ).
  2. Per-word labels predicted (Dϕ).
  3. Decode arcs from labels (encoding-specific algorithm).
  4. For each arc, predict dependency type (Dφ).
  5. Output labeled dependency graph.

- Design tradeoffs:
  - Bounded vs. unbounded: Bounded offers speed and simplicity but may sacrifice coverage; unbounded offers full coverage but slower inference.
  - k parameter: Higher k improves accuracy but increases computational cost and may overfit.
  - Encoder choice: Transformers give better accuracy but are slower; BiLSTMs are faster but less accurate.

- Failure signatures:
  - Low tagging accuracy: Encoder or decoder not learning.
  - Low oracle F-score: Encoding not injective or missing label types.
  - Well-formed trees but wrong arcs: Decoding algorithm bug.
  - Coverage loss: Plane assignment algorithm failing on dense graphs.

- First 3 experiments:
  1. Train and evaluate the 4-bit encoding (k=1) on a small treebank to verify basic pipeline works.
  2. Switch to 4k-bit with k=2 and test coverage and accuracy on a dense graph dataset.
  3. Compare bounded (B4k, B6k) vs. unbounded (B2) encodings on IWPT to observe speed-accuracy tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sequence labeling approach scale with increasing sentence length and graph density, and what are the theoretical limits of its practical applicability?
- Basis in paper: [inferred] The paper notes that unbounded encodings scale with sentence length and that performance degrades with higher graph density, but does not provide a formal analysis of scalability or limits.
- Why unresolved: The authors present empirical results across varied datasets but do not analyze asymptotic complexity or provide explicit scalability bounds.
- What evidence would resolve it: Experiments varying sentence length systematically and theoretical analysis of encoding/decoding complexity with respect to graph density.

### Open Question 2
- Question: What is the impact of different hyperparameter choices (e.g., number of encoder layers, attention mechanisms) on the relative performance of bounded versus unbounded encodings?
- Basis in paper: [explicit] The paper uses a fixed model architecture across all experiments but notes that "more powerful architectures might boost our results" without exploring these effects.
- Why unresolved: The experimental setup uses a single encoder configuration, preventing isolation of architectural impacts on encoding performance.
- What evidence would resolve it: Ablation studies varying encoder depth, attention heads, and other hyperparameters across different encoding types.

### Open Question 3
- Question: Can the proposed encodings be effectively adapted for meaning representations with nodes not directly corresponding to input words (e.g., abstract or implicit nodes)?
- Basis in paper: [explicit] The authors explicitly acknowledge this as a limitation, noting their work focuses on flavor (0) representations where nodes correspond to input words.
- Why unresolved: The authors recognize the gap but do not attempt to extend their encodings to handle more complex anchoring schemes.
- What evidence would resolve it: Successful application of modified encodings to flavor (1) or (2) representations, demonstrating preservation of parsing accuracy.

## Limitations

- The bounded encoding approach relies on the assumption that dependency graphs can be decomposed into a small number of relaxed k-planar subgraphs, which may not hold for highly dense or cyclic graphs.
- The study focuses primarily on English and a limited set of other languages, leaving open the question of how well these encodings generalize to typologically diverse languages.
- The empirical gains from increasing k are reported, but the marginal benefit diminishes and the computational cost rises.

## Confidence

- **High confidence**: The overall feasibility of dependency graph parsing as sequence labeling, supported by strong empirical results across multiple datasets and encoders.
- **Medium confidence**: The superiority of bounded encodings for sparse graphs and the benefits of multiplanarity for dense graphs.
- **Low confidence**: The robustness of the approach to highly non-planar or extremely dense graphs, and the scalability of multiplanarity beyond k=4 in practice.

## Next Checks

1. Evaluate the bounded and unbounded encodings on a typologically diverse set of languages, especially those with free word order or rich morphological marking, to test generalizability.
2. Test the approach on artificially generated dense or cyclic graphs (e.g., from AMR or UCCA) to assess the limits of multiplanarity and coverage.
3. Systematically compare training and inference times across bounded, unbounded, and biaffine baselines on the same hardware, especially as k increases, to quantify practical benefits.