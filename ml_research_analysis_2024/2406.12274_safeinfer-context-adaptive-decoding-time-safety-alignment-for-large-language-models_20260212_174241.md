---
ver: rpa2
title: 'SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language
  Models'
arxiv_id: '2406.12274'
source_url: https://arxiv.org/abs/2406.12274
tags:
- safety
- safe
- language
- base
- infer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SafeInfer tackles the problem of fragile safety alignment in large
  language models by proposing a two-phase context-adaptive decoding-time safety alignment
  strategy. It first amplifies safety through integration of safety demonstration
  vectors into model hidden states, then guides decoding using safety-optimized distributions
  to reduce harmful outputs.
---

# SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models

## Quick Facts
- arXiv ID: 2406.12274
- Source URL: https://arxiv.org/abs/2406.12274
- Reference count: 18
- Primary result: Reduces attack success rates (ASRs) on harmful outputs while preserving model utility

## Executive Summary
SafeInfer addresses the challenge of safety alignment in large language models by introducing a two-phase decoding-time strategy. It first amplifies safety through integration of safety demonstration vectors into model hidden states, then guides decoding using safety-optimized distributions to reduce harmful outputs. Evaluated across multiple datasets and model types, SafeInfer achieves the lowest attack success rates compared to baselines while maintaining model utility.

## Method Summary
SafeInfer employs a two-phase context-adaptive decoding-time safety alignment strategy. The first phase, safety amplification (SA), computes a safety amplification vector from safe demonstration examples and integrates it into the model's hidden states at a specific layer. The second phase, safety-guided decoding strategy (sGDS), combines the amplified model's distribution with a harmful model's distribution using a Union operation, then applies a subtraction mechanism to bias token selection toward safer outputs while preserving general capabilities.

## Key Results
- Achieves lowest attack success rates (ASRs) across multiple evaluation datasets
- Significantly reduces unsafe responses while preserving model utility
- Demonstrates strong effectiveness against jailbreak attacks and edited models
- Maintains safety alignment without substantial degradation of general capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating a safety amplification vector (SV) into hidden states shifts the latent space toward safer outputs
- Mechanism: SV computed from safe demonstrations by averaging influential attention head activations, then added to hidden state at specific layer
- Core assumption: Latent space contains representational capacity to encode safety features
- Evidence anchors: Abstract and section describing SA phase; weak corpus evidence
- Break condition: If attention heads aren't truly influential for safety or latent space doesn't encode safety features

### Mechanism 2
- Claim: sGDS reduces harmful outputs by combining amplified model distribution with harmful model distribution
- Mechanism: Fine-tunes harmful model Musf, uses Union operation to create combined distribution C, then subtracts scaled C from amplified model's distribution
- Core assumption: Harmful model reliably generates harmful content tokens
- Evidence anchors: Abstract and section describing sGDS; weak corpus evidence
- Break condition: If harmful model doesn't accurately represent harmful tendencies or scaling parameter λ is poorly tuned

### Mechanism 3
- Claim: Combination of SA and sGDS phases provides synergistic effect outperforming either phase alone
- Mechanism: SA modifies internal representations while sGDS modifies output distribution, addressing safety at both latent and surface levels
- Core assumption: Safety issues arise from both internal representations and output distribution
- Evidence anchors: Abstract and section describing combined approach; no direct corpus evidence
- Break condition: If safety issues are primarily at one level rather than both

## Foundational Learning

- Concept: Autoregressive language model decoding and probability distributions over tokens
  - Why needed here: Understanding token-by-token generation is crucial for grasping how SafeInfer modifies decoding process
  - Quick check question: In an autoregressive model, what determines the probability distribution for the next token given previous tokens?

- Concept: Attention mechanisms and influential heads in transformer models
  - Why needed here: SafeInfer identifies influential attention heads to compute safety amplification vector
  - Quick check question: What role do attention heads play in transformer models, and how might they be identified as influential for specific tasks?

- Concept: Kullback-Leibler divergence and probability distribution optimization
  - Why needed here: SafeInfer uses KL divergence in optimization function to create combined distribution C
  - Quick check question: What does Kullback-Leibler divergence measure between two probability distributions, and why might it be useful for creating a combined distribution?

## Architecture Onboarding

- Component map: Base model -> SV computation module -> Hidden state modification layer -> Harmful model Musf -> Combined distribution computation -> Final distribution modification -> Output generation

- Critical path: Input → SV computation → Hidden state modification → Output distribution computation → Token selection → Output

- Design tradeoffs:
  - Safety vs. utility: Ensuring safety while preserving general capabilities
  - Complexity vs. effectiveness: Two-phase approach vs. single-phase methods
  - Computational cost vs. safety gain: Additional computations in sGDS phase

- Failure signatures:
  - High ASR with low over-safety: sGDS too restrictive, base model may need more SA
  - Low ASR with high over-safety: sGDS too aggressive, need to reduce λ
  - No improvement over base model: SV computation or integration may be flawed

- First 3 experiments:
  1. Run SA phase only with various γ values on a small dataset to find optimal setting
  2. Run sGDS phase only with various λ values on the same dataset to find optimal setting
  3. Combine SA and sGDS with optimal parameters and compare to baselines on full evaluation suite

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal value of γ in SAFE INFER vary across different language model architectures and datasets?
- Basis in paper: Inferred from sensitivity analysis of γ on ASR and over-safety scores
- Why unresolved: Only tested on Llama-2 with fixed λ; optimal values might differ for other architectures
- What evidence would resolve it: Systematic grid search experiments across multiple model architectures and datasets

### Open Question 2
- Question: What is the impact of speculative sampling on computational efficiency of SAFE INFER across different datasets and model sizes?
- Basis in paper: Brief mention of speculative sampling reducing model calls for HarmEval dataset
- Why unresolved: Only one example provided; speedup might vary significantly with different configurations
- What evidence would resolve it: Comprehensive timing experiments across multiple datasets and model architectures

### Open Question 3
- Question: How does SAFE INFER perform against emerging jailbreak techniques that combine multiple attack vectors?
- Basis in paper: Tested against five jailbreak methods but acknowledges these represent different categories
- Why unresolved: Tested methods represent only specific approaches; new techniques might require different defensive strategies
- What evidence would resolve it: Experiments with combined jailbreak attacks and novel techniques

## Limitations

- Generalizability concerns to larger models beyond 7B parameters
- Limited exploration of hyperparameter sensitivity and optimal tuning across different architectures
- Assumption that safety issues can be effectively addressed at decoding time rather than requiring fundamental alignment changes

## Confidence

**High Confidence Claims:**
- SafeInfer reduces ASR compared to baseline models and other safety alignment methods
- Two-phase approach (SA + sGDS) outperforms either phase alone
- SafeInfer maintains general capabilities while improving safety
- Method is effective against jailbreak attacks and edited models

**Medium Confidence Claims:**
- Specific mechanism of SV integration meaningfully shifts latent space toward safety
- Union operation and distribution subtraction in sGDS effectively capture harmful tendencies
- Identified influential attention heads are truly representative of safety-relevant features

**Low Confidence Claims:**
- Generalizability to models significantly larger than 7B parameters
- Robustness to adversarial attacks beyond those tested in evaluation datasets
- Performance in real-world deployment scenarios with diverse user inputs

## Next Checks

1. **Layer Sensitivity Analysis**: Systematically vary the layer l where SV integration occurs and measure impact on both ASR and over-safety rates across different model sizes

2. **Cross-Domain Safety Transfer**: Evaluate effectiveness when safe demonstration examples come from different domain than evaluation datasets

3. **Long-Range Context Evaluation**: Test on extended conversations where harmful content emerges gradually through multi-turn interactions