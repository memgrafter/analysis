---
ver: rpa2
title: One-Shot Domain Incremental Learning
arxiv_id: '2403.16707'
source_url: https://arxiv.org/abs/2403.16707
tags:
- domain
- one-shot
- learning
- batch
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of one-shot domain incremental
  learning (DIL), where a neural network model must be updated to classify samples
  from a new domain with only a single example, while maintaining performance on the
  original domain. Existing DIL methods like Elastic Weight Consolidation (EWC) and
  Gradient Episodic Memory (GEM) fail in this setting due to the instability of batch
  normalization statistics caused by the single new sample.
---

# One-Shot Domain Incremental Learning

## Quick Facts
- arXiv ID: 2403.16707
- Source URL: https://arxiv.org/abs/2403.16707
- Reference count: 40
- Primary result: Proposes fixing batch normalization statistics to address instability in one-shot domain incremental learning

## Executive Summary
This paper tackles the challenge of one-shot domain incremental learning (DIL), where a neural network must adapt to classify samples from a new domain using only a single example while maintaining performance on the original domain. Existing DIL methods like Elastic Weight Consolidation (EWC) and Gradient Episodic Memory (GEM) fail in this setting due to the instability of batch normalization statistics caused by the single new sample. The authors identify that the moving averages of mean and variance in batch normalization layers become unreliable when only one new sample is available, leading to poor generalization and accuracy degradation.

The proposed solution involves fixing these statistics to their pre-update values and preventing their update during training on the new domain. This modification stabilizes training and improves performance without interfering with existing DIL methods. Experiments on MNIST, CIFAR-10, and RESISC-45 datasets demonstrate that this approach significantly improves accuracy on the new domain (up to 57% increase) while maintaining or even improving accuracy on the original domain. The results establish a strong baseline for one-shot DIL and highlight the importance of carefully handling batch normalization in such settings.

## Method Summary
The authors propose a simple yet effective modification to address the instability of batch normalization in one-shot domain incremental learning. The core idea is to fix the moving averages of mean and variance in batch normalization layers to their pre-update values when adapting to a new domain with only a single example. By preventing these statistics from being updated during training on the new domain, the method stabilizes the learning process and improves generalization. This approach can be combined with existing DIL methods like EWC and GEM without modification to those methods themselves. The solution directly targets the identified failure mode while maintaining the integrity of the original model's learned representations.

## Key Results
- Accuracy on new domains improved by up to 57% compared to baseline methods
- Original domain accuracy maintained or improved in most cases
- Demonstrated effectiveness across MNIST, CIFAR-10, and RESISC-45 datasets
- Simple modification that can be combined with existing DIL methods

## Why This Works (Mechanism)
The paper identifies that batch normalization becomes unstable in one-shot learning scenarios because the moving averages of mean and variance cannot be reliably estimated from a single sample. When the model attempts to update these statistics, it introduces significant noise that disrupts the learned feature distributions. By fixing these statistics to their pre-update values, the method preserves the stable feature normalization that the model has already learned, preventing the degradation that would otherwise occur from attempting to update these statistics with insufficient data.

## Foundational Learning

**Domain Incremental Learning**: A continual learning scenario where the model must adapt to new domains while maintaining performance on previous ones. Why needed: Forms the broader context for understanding the incremental adaptation challenge. Quick check: Does the model retain performance on previous domains while adapting?

**Batch Normalization**: A normalization technique that maintains moving averages of mean and variance during training. Why needed: The paper's solution directly addresses batch normalization's instability in one-shot settings. Quick check: Are batch normalization statistics being updated during domain adaptation?

**One-Shot Learning**: Learning from a single example, as opposed to few-shot or many-shot scenarios. Why needed: The specific challenge of adapting with minimal data is central to the paper's contribution. Quick check: Is the model successfully adapting with only one example from the new domain?

## Architecture Onboarding

**Component Map**: Input -> Batch Normalization Layers -> Neural Network Layers -> Output
The critical components are the batch normalization layers whose statistics must be stabilized.

**Critical Path**: Batch normalization statistics (mean/variance) -> Feature normalization stability -> Model adaptation success
The stability of batch normalization statistics is the determining factor for successful adaptation.

**Design Tradeoffs**: Fixed statistics prevent adaptation to domain shifts vs. preventing catastrophic forgetting
The method sacrifices potential adaptation to domain-specific feature distributions to maintain stability and prevent forgetting.

**Failure Signatures**: Significant accuracy drop on original domain when batch normalization statistics are updated
The paper demonstrates that updating batch normalization statistics with single samples leads to catastrophic forgetting.

**First Experiments**:
1. Test the approach on MNIST with one example from a new digit class
2. Apply the method to CIFAR-10 with one example from a new object category
3. Evaluate on RESISC-45 with one example from a new scene type

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Experimental evaluation limited to relatively small-scale datasets
- Performance on more complex vision tasks or larger domain shifts remains untested
- Reliance on fixed batch normalization statistics may not generalize to substantially different domain distributions
- Does not explore trade-offs between adaptation speed and final accuracy

## Confidence

**High Confidence**: The identification of batch normalization instability as a key failure mode in one-shot DIL, and the empirical demonstration of accuracy improvements on tested datasets

**Medium Confidence**: The claim that accuracy on original domains is maintained/improved, due to limited architectural and dataset diversity in experiments

**Low Confidence**: Generalizability to larger-scale problems and substantially different domain distributions

## Next Checks
1. Test the approach on larger, more complex datasets (e.g., ImageNet variants) with greater domain shifts to assess scalability
2. Evaluate performance across diverse network architectures beyond standard CNNs, including transformer-based models
3. Conduct ablation studies to quantify the contribution of fixed batch normalization statistics versus other components of the method