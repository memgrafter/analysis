---
ver: rpa2
title: 'XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided
  Context Optimization'
arxiv_id: '2403.09410'
source_url: https://arxiv.org/abs/2403.09410
tags:
- prompts
- prompt
- learning
- clinical
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XCoOp is a framework for explainable prompt learning in medical
  image diagnosis. It improves interpretability by aligning learnable prompts with
  clinical concepts at multiple levels, using knowledge from large language models
  when needed.
---

# XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization

## Quick Facts
- arXiv ID: 2403.09410
- Source URL: https://arxiv.org/abs/2403.09410
- Reference count: 30
- Primary result: Achieves 81.12% AUC on SkinCon dataset while providing visual and textual explanations for diagnoses

## Executive Summary
XCoOp introduces a framework for explainable prompt learning in medical image diagnosis by aligning learnable soft prompts with clinical concepts at multiple granularities. The method leverages vision-language models like CLIP, using both token-level and prompt-level alignment to make soft prompts semantically meaningful for medical practitioners. It addresses the challenge of missing concept annotations by eliciting medical knowledge from large language models like GPT-4, enabling application to diverse medical datasets. The framework achieves state-of-the-art diagnostic accuracy while providing interpretable explanations that align with how medical experts diagnose diseases.

## Method Summary
XCoOp builds on vision-language models by learning soft prompts for medical image classification while maintaining interpretability. The framework uses multi-granularity prompt alignment that works at both token-level (through contrastive learning between soft and hard prompt embeddings) and prompt-level (aligning full prompt embeddings via the text encoder). It incorporates global-local image-prompt alignment to capture both overall image patterns and disease-relevant regions. When clinical concept annotations are unavailable, the method elicits medical knowledge from large language models to generate clinically relevant prompts. The framework is trained using SGD with learning rate 0.032, warm-up, and cosine annealing, optimizing a combined loss function that balances soft-hard prompt alignment and image-prompt alignment.

## Key Results
- Achieves 81.12% AUC on SkinCon dataset for skin disease diagnosis
- Outperforms baseline prompt learning methods on Derm7pt, Pneumonia, and IU X-Ray datasets
- Provides both visual and textual explanations for model decisions through soft prompt alignment with clinical concepts
- Successfully applies to datasets with and without concept annotations by leveraging LLM knowledge elicitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-granularity prompt alignment (token-level + prompt-level) forces soft prompts to semantically align with clinical concepts, improving both accuracy and interpretability.
- Mechanism: Token-level alignment uses contrastive learning between soft and hard prompt embeddings to ensure each token in the soft prompt corresponds to a meaningful clinical concept. Prompt-level alignment aligns the full prompt embeddings via the text encoder to preserve global disease semantics.
- Core assumption: Clinical concept embeddings from the text encoder are semantically meaningful and can guide soft prompt learning.
- Evidence anchors:
  - [abstract] "leverages medical knowledge by aligning the semantics of images, learnable prompts, and clinical concept-driven prompts at multiple granularities"
  - [section] "To enhance the informativeness and explainability of the soft prompts by incorporating clinical semantics, we introduce a soft-hard prompt alignment module that aligns prompts at both the prompt level and token level."
- Break condition: If clinical concept embeddings are not semantically meaningful (e.g., if the text encoder fails to capture clinical semantics), alignment cannot improve interpretability.

### Mechanism 2
- Claim: Global-local image-prompt alignment mimics how medical experts diagnose by focusing on both overall image patterns and specific disease-relevant regions.
- Mechanism: The model computes similarity between global image features and soft prompt embeddings, and separately computes similarity between local patch features and soft prompt embeddings. Both are combined for final prediction.
- Core assumption: Disease diagnosis requires both global context and local discriminative features, and these can be captured by the image encoder's multi-level features.
- Evidence anchors:
  - [abstract] "our framework offers both visual and textual explanations for learned prompts"
  - [section] "Medical diagnosis typically hinges on various clinical symptoms observable within specific, localized regions in an image... we employ a global-local image-prompt alignment module to align the medical images and clinical concept-driven prompts at multiple levels."
- Break condition: If the image encoder fails to produce meaningful local features or if local features are not disease-relevant, the global-local alignment cannot improve diagnosis.

### Mechanism 3
- Claim: Eliciting medical knowledge from LLMs when concept annotations are unavailable enables the framework to be applied to diverse datasets without human labeling.
- Mechanism: When concept annotations are missing, the framework queries an LLM (e.g., GPT-4) with "What are the most useful visual concepts to distinguish [disease name] in a [image type]?" to generate clinical prompts.
- Core assumption: LLMs contain sufficient medical knowledge to generate clinically relevant concepts for disease diagnosis.
- Evidence anchors:
  - [abstract] "addresses the lack of valuable concept annotations by eliciting knowledge from large language models"
  - [section] "For the datasets lacking explicit concept annotations, we elicit medical knowledge from a large language model such as GPT4 [1] and create the corresponding clinical prompts."
- Break condition: If the LLM lacks relevant medical knowledge or provides inaccurate concepts, the generated clinical prompts will be ineffective.

## Foundational Learning

- Concept: Prompt learning in vision-language models
  - Why needed here: XCoOp is built on top of prompt learning techniques to adapt VLMs like CLIP to medical image diagnosis tasks.
  - Quick check question: What is the key difference between hard prompts and soft prompts in vision-language models?
- Concept: Contrastive learning
  - Why needed here: Token-level prompt alignment uses contrastive learning to align soft prompt tokens with clinical concept tokens in embedding space.
  - Quick check question: How does contrastive learning work to bring similar items closer and dissimilar items farther apart in embedding space?
- Concept: Multi-modal embeddings
  - Why needed here: The framework works with both image and text embeddings from CLIP, requiring understanding of how different modalities are represented and compared.
  - Quick check question: What is the typical dimensionality of CLIP image and text embeddings, and how are they compared?

## Architecture Onboarding

- Component map:
  - Text encoder (CLIP) -> Token-level alignment module -> Prompt-level alignment module -> Global-local image-prompt alignment module -> Prediction
- Critical path: Text encoder → Token-level alignment → Prompt-level alignment → Global-local image-prompt alignment → Prediction
- Design tradeoffs:
  - Using soft prompts vs. fine-tuning entire model: Soft prompts are more parameter-efficient but may have limited expressiveness
  - Token-level vs. prompt-level alignment: Token-level provides finer control but may be noisier; prompt-level is more stable but less granular
  - Global vs. local image features: Global features capture overall patterns but may miss local discriminative regions; local features are more specific but may lose context
- Failure signatures:
  - Poor performance: Soft prompts fail to learn meaningful representations; image encoder features are not disease-relevant
  - Lack of interpretability: Token-level alignment fails; clinical prompts are not semantically meaningful
  - High computational cost: Image encoder produces too many local features; alignment modules are too complex
- First 3 experiments:
  1. Compare performance with and without token-level alignment on a dataset with concept annotations
  2. Compare performance with and without LLM knowledge elicitation on a dataset without concept annotations
  3. Visualize soft prompt tokens before and after alignment to verify semantic changes

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The framework's reliance on LLM-elicited medical knowledge lacks empirical validation of concept quality and accuracy
- Soft prompt alignment mechanisms assume clinical concept embeddings are semantically meaningful without verification
- Explainability evaluation is limited, with insufficient detail on implementation and results of faithfulness, understandability, and plausibility metrics

## Confidence
**High Confidence Claims:**
- The framework's architecture and training procedure are technically sound
- Performance improvements over baseline prompt learning methods on datasets with concept annotations
- The concept of multi-granularity prompt alignment is valid

**Medium Confidence Claims:**
- The explainability benefits claimed for the framework
- The effectiveness of LLM knowledge elicitation for concept generation
- The clinical utility of the visual and textual explanations provided

**Low Confidence Claims:**
- The framework's performance on truly unannotated datasets
- The practical interpretability of soft prompts for medical practitioners
- The generalizability of results across diverse medical imaging domains

## Next Checks
1. **LLM Knowledge Quality Validation**: Conduct a blind evaluation where medical experts assess the quality and clinical relevance of LLM-generated concepts versus human-annotated concepts. Compare the diagnostic accuracy achieved using LLM-generated concepts against ground truth annotations to quantify the knowledge elicitation performance.

2. **Clinical Practitioner Study**: Recruit radiologists or other medical specialists to evaluate whether the soft prompt alignments actually improve their understanding of model decisions. Use think-aloud protocols or structured interviews to assess if the visual and textual explanations provided by XCoOp are clinically meaningful and actionable.

3. **Cross-Dataset Generalization Test**: Apply XCoOp to a medical imaging dataset where neither concept annotations nor disease labels are available, relying entirely on LLM-elicited concepts. Compare performance against traditional fine-tuning approaches to validate the framework's claims about applicability to unannotated data and its practical utility in real-world medical settings where annotated data is scarce.