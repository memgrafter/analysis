---
ver: rpa2
title: Towards a Benchmark for Large Language Models for Business Process Management
  Tasks
arxiv_id: '2410.03255'
source_url: https://arxiv.org/abs/2410.03255
tags:
- process
- performance
- task
- tasks
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the first benchmark for evaluating Large
  Language Models (LLMs) on Business Process Management (BPM) tasks, addressing the
  gap in domain-specific performance assessment. The study systematically compares
  seven open-source and closed-source LLMs across four BPM tasks: activity recommendation,
  RPA candidate identification, process question answering, and mining declarative
  process models.'
---

# Towards a Benchmark for Large Language Models for Business Process Management Tasks

## Quick Facts
- **arXiv ID:** 2410.03255
- **Source URL:** https://arxiv.org/abs/2410.03255
- **Reference count:** 13
- **Primary result:** First benchmark for evaluating LLMs on BPM tasks shows model size doesn't predict performance, and smaller models can match GPT-4 on specific tasks

## Executive Summary
This paper introduces the first benchmark for evaluating Large Language Models on Business Process Management tasks, addressing a critical gap in domain-specific AI assessment. The study systematically compares seven open-source and closed-source LLMs across four BPM tasks: activity recommendation, RPA candidate identification, process question answering, and mining declarative process models. Results demonstrate that model size alone does not determine performance, with smaller models like Llama3-7b and Mixtral-8x7b matching or exceeding GPT-4 in specific tasks. The research emphasizes the importance of task-specific model selection and highlights how prompt engineering significantly impacts LLM performance in BPM applications.

## Method Summary
The study creates task-specific datasets for four BPM applications: SAP Signavio Academic Models for activity recommendation, Grohs et al. 2023 datasets for RPA identification and declarative mining, and custom Camunda-based data for process QA. Seven LLMs (GPT-4, Phi-3 Medium, Claude 2, Falcon 2, Mixtral-8x7b, Llama 3, Yi-1.5) are evaluated using few-shot prompting with temperature=0 for reproducibility. Each task employs appropriate metrics: cosine similarity for activity recommendation, precision/recall/F1 for RPA and declarative mining, and ROUGE-L for process QA. The benchmark systematically compares model performance across all task combinations.

## Key Results
- Model size does not directly correlate with BPM task performance; smaller models can match or exceed larger ones
- GPT-4 shows stable performance across all tasks but is not the top performer in any single task
- Prompt engineering significantly affects LLM performance, with substantial variability across different template strategies
- Open-source models like Llama3-7b and Mixtral-8x7b can compete with or exceed GPT-4 on specific BPM tasks

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific benchmarking is necessary because generic NLP benchmarks do not capture unique BPM challenges. By creating tailored datasets and evaluation metrics for BPM-specific tasks, the benchmark exposes model limitations that generic benchmarks miss. Core assumption: LLM performance on generic NLP tasks does not transfer to domain-specific BPM tasks without adaptation. Evidence: "Currently, there are no BPM-specific benchmarks available, leaving it unclear which LLMs are suitable for which BPM task."

### Mechanism 2
Model size is not the sole determinant of BPM task performance; task-specific characteristics and prompt engineering matter more. By comparing models of different sizes (8B to 1,760B parameters) on the same BPM tasks, the study shows smaller models can match or exceed larger ones depending on task requirements. Core assumption: Larger models do not automatically perform better on all BPM tasks due to specialized nature of the tasks. Evidence: "Our experiments reveal that model size alone is insufficient to explain performance differences."

### Mechanism 3
Prompt engineering significantly affects LLM performance on BPM tasks, and different models respond differently to various prompt strategies. By testing multiple prompt templates across models, the study identifies optimal prompting strategies for each model-task combination. Core assumption: LLMs are sensitive to prompt structure, and this sensitivity varies by model architecture and task type. Evidence: "Performance of the LLMs is highly sensitive to the prompt templates used."

## Foundational Learning

- **Concept:** BPM task categorization (activity recommendation, RPA identification, process QA, declarative mining)
  - **Why needed here:** Understanding the four distinct BPM tasks is essential for interpreting benchmark results and their implications for model selection.
  - **Quick check question:** Can you explain the difference between activity recommendation and declarative process mining in BPM?

- **Concept:** LLM evaluation metrics (cosine similarity, precision/recall/F1, ROUGE-L)
  - **Why needed here:** Different BPM tasks require different evaluation metrics, and understanding these metrics is crucial for interpreting benchmark results.
  - **Quick check question:** Why is cosine similarity used for activity recommendation while F1-score is used for RPA identification?

- **Concept:** Prompt engineering strategies (few-shot, persona, step-by-step)
  - **Why needed here:** The effectiveness of different prompt templates varies by model and task, making prompt engineering a critical factor in LLM performance.
  - **Quick check question:** Which prompt template would you use for a model that performs poorly with few-shot examples but well with persona prompts?

## Architecture Onboarding

- **Component map:** Dataset preparation -> Model loading (HuggingFace + OpenAI API) -> Prompt template application -> Metric calculation -> Result aggregation

- **Critical path:**
  1. Dataset preparation for each BPM task
  2. Model loading and configuration (temperature=0 for reproducibility)
  3. Prompt template application and inference
  4. Metric calculation and result aggregation
  5. Cross-model and cross-task performance analysis

- **Design tradeoffs:** Open-source vs. closed-source models (transparency vs. performance), model size vs. performance (efficiency vs. capabilities), few-shot vs. persona prompts (generalizability vs. optimization)

- **Failure signatures:** Inconsistent results across runs (check temperature and random seed), poor performance on specific tasks (review dataset quality and prompt templates), memory errors with large models (verify GPU allocation and consider quantization)

- **First 3 experiments:**
  1. Baseline validation: Run GPT-4 on all tasks with few-shot prompts to establish performance benchmarks
  2. Model size comparison: Compare Llama3-7b and Phi3-14b on activity recommendation to test if smaller models can match larger ones
  3. Prompt sensitivity test: Apply persona and step-by-step prompts to Claude2-13b on RPA identification to identify optimal prompting strategies

## Open Questions the Paper Calls Out

### Open Question 1
How do LLMs perform on BPM tasks when using different prompt templates (few-shot, persona, step-by-step) beyond the few-shot examples tested in this study? The paper discusses that prompt templates significantly affect LLM performance but only focuses on few-shot prompting for the main analysis. Systematic performance comparisons of all three prompt templates across all four BPM tasks would resolve this question.

### Open Question 2
What is the impact of model size on BPM task performance beyond the parameter range tested (8B to 46.7B parameters), particularly when comparing to much smaller or larger models? The paper concludes that model size alone does not explain performance differences but does not test models outside this range. Performance benchmarking of additional models with parameter counts significantly below 8B and above 46.7B would resolve this question.

### Open Question 3
How do open-source LLMs perform on BPM tasks when fine-tuned on domain-specific BPM data versus using their base capabilities? The study uses base open-source models without domain-specific fine-tuning, noting that these models can compete with GPT-4, suggesting potential for further performance gains through fine-tuning. Comparative performance analysis of base versus fine-tuned open-source models would resolve this question.

### Open Question 4
What are the limitations of using ROUGE-L for evaluating process question answering in BPM contexts, and what alternative metrics might better capture model performance? The paper uses ROUGE-L but acknowledges limitations of strict output formatting requirements. Comparative evaluation using multiple metrics would determine which best captures BPM-specific comprehension.

## Limitations

- The benchmark covers only four BPM tasks, potentially missing other critical BPM applications such as process mining, conformance checking, or process optimization
- Evaluation uses only English-language datasets, limiting applicability to multilingual BPM contexts
- Prompt engineering relies on manually crafted templates that may not represent optimal strategies for all models or tasks

## Confidence

**High Confidence:**
- The need for domain-specific BPM benchmarks is well-supported by observed performance gaps
- Model size does not consistently predict performance across all BPM tasks
- Prompt engineering significantly affects LLM performance

**Medium Confidence:**
- GPT-4 demonstrates stable performance across all tasks but is not top performer in any single task
- Relative ranking of models varies significantly across tasks

**Low Confidence:**
- Exact impact of different prompt engineering strategies on long-term BPM deployment success
- Generalizability of findings to other BPM tasks beyond the four evaluated

## Next Checks

1. **Cross-lingual validation:** Test the benchmark with BPM datasets in multiple languages to assess whether observed performance patterns hold across linguistic contexts, particularly for multilingual process documentation and international BPM deployments.

2. **Longitudinal performance tracking:** Implement continuous evaluation framework to monitor how LLM performance on BPM tasks evolves over time with model updates, ensuring benchmark results remain valid as models improve.

3. **Real-world deployment validation:** Conduct pilot study deploying top-performing models from benchmark in actual BPM workflows within organizations, measuring not just task accuracy but also integration overhead, user satisfaction, and business impact to validate practical relevance.