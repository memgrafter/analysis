---
ver: rpa2
title: 'Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs
  Without Retraining'
arxiv_id: '2407.19126'
source_url: https://arxiv.org/abs/2407.19126
tags:
- pruning
- proj
- module
- output
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to pruning large language
  models (LLMs) that achieves superior performance without requiring model retraining.
  The method identifies a depth-2 pruning structure within Transformer-based LLMs
  and introduces two inference-aware pruning criteria based on output approximation.
---

# Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining

## Quick Facts
- arXiv ID: 2407.19126
- Source URL: https://arxiv.org/abs/2407.19126
- Reference count: 40
- Primary result: Achieves superior performance without retraining by identifying depth-2 pruning structures and using inference-aware metrics

## Executive Summary
This paper introduces a novel approach to structured pruning for large language models that eliminates the need for model retraining. By identifying depth-2 pruning structures within Transformer architectures and developing two inference-aware pruning criteria based on output approximation, the method achieves significant computational cost reductions while maintaining or improving model performance. The approach uses similarity-based metrics for attention blocks and second-moment-based metrics for depth-2 modules, outperforming traditional training-aware metrics like gradient and Hessian.

## Method Summary
The method identifies depth-2 pruning structures within Transformer models, treating attention and feed-forward modules as independent pruning units. Two inference-aware pruning criteria are introduced: a similarity-based metric that uses Jensen-Shannon divergence to identify redundant attention heads, and a second-moment-based metric that estimates channel importance through activation statistics. A two-step reconstruction technique is employed to mitigate pruning errors without requiring model retraining. The approach is evaluated on multiple models including LLaMA-7B, GPT-2, and Vicuna-7B across various datasets and sparsity levels.

## Key Results
- Achieves superior performance without requiring model retraining
- Outperforms traditional training-aware metrics (gradient and Hessian) using inference-aware criteria
- Maintains performance across various datasets and models including LLaMA-7B, GPT-2, and Vicuna-7B
- Significantly reduces computational costs and hardware requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depth-2 pruning structure in Transformer models preserves feature knowledge while reducing pruning complexity from residual connections.
- Mechanism: Identifies independent depth-2 modules (attention and feed-forward) that can be pruned without disrupting residual connections, preserving structural integrity while allowing parallel pruning decisions.
- Core assumption: Residual connections enforce strict channel alignment across modules, making output-channel pruning more effective than input-channel pruning.
- Evidence anchors: Section on depth-2 module identification and abstract on simplifying pruning process.

### Mechanism 2
- Claim: Similarity-based pruning criteria effectively identifies redundant attention heads without requiring dataset-based importance scores.
- Mechanism: Calculates pairwise head divergence using Jensen-Shannon divergence on attention scores, removing heads with similar information first to exploit natural redundancy.
- Core assumption: Attention heads are designed to capture correlations independently, creating natural redundancy that can be exploited through similarity analysis.
- Evidence anchors: Section on similarity-based pruning and conceptual alignment with filter pruning literature.

### Mechanism 3
- Claim: Second-moment-based pruning criteria provides a computationally efficient alternative to training-aware metrics while maintaining effectiveness.
- Mechanism: Uses second moments of activations to estimate channel importance, integrating information across multiple layers without requiring backpropagation or Hessian computation.
- Core assumption: Second moments provide sufficient information to estimate channel importance, and covariance matrix approximation works without explicit computation.
- Evidence anchors: Section on second-moment-based pruning and abstract on outperforming traditional metrics.

## Foundational Learning

- Concept: Depth-2 module identification in Transformer architectures
  - Why needed here: Understanding the depth-2 structure is crucial for implementing the pruning method correctly and preserving residual connections.
  - Quick check question: What are the two levels in an attention module's depth-2 structure, and how do they relate to pruning decisions?

- Concept: Jensen-Shannon divergence for similarity analysis
  - Why needed here: The similarity metric relies on JS divergence to quantify redundancy between attention heads.
  - Quick check question: How does Jensen-Shannon divergence differ from Kullback-Leibler divergence, and why is it preferred for this application?

- Concept: Second-moment statistics in neural networks
  - Why needed here: The pruning metric uses second moments to estimate channel importance without backpropagation.
  - Quick check question: How does the second moment of activations relate to the importance of a channel, and what assumptions are made about the input distribution?

## Architecture Onboarding

- Component map: Depth-2 module identification -> Pruning criteria module (similarity-based + second-moment-based) -> Pre-pruning recovery module -> Actual pruning
- Critical path: (1) Analyze model architecture to identify depth-2 modules, (2) Compute similarity matrices for attention heads using calibration data, (3) Calculate second-moment scores for feed-forward modules, (4) Apply pre-pruning recovery to reconstruct weights, (5) Perform actual pruning based on computed scores
- Design tradeoffs: Trades computational efficiency (avoiding retraining and second-order information) against potential accuracy loss (using simpler metrics instead of training-aware methods), with depth-2 structure constraint limiting flexibility but ensuring residual connection compatibility
- Failure signatures: (1) Poor depth-2 module identification leading to broken residual connections, (2) Incorrect similarity thresholds causing over-pruning or under-pruning of attention heads, (3) Inaccurate second-moment calculations due to poor covariance matrix estimation, (4) Insufficient pre-pruning recovery leading to error accumulation
- First 3 experiments:
  1. Verify depth-2 module identification by checking that pruning decisions preserve residual connection integrity and maintain output channel counts
  2. Test similarity-based pruning on a small attention-only model to validate that redundant heads are correctly identified and removal doesn't degrade performance
  3. Compare second-moment-based pruning against random pruning on a feed-forward-only model to confirm the metric provides meaningful importance scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the depth-2 pruning structure generalize to other transformer architectures beyond the standard ones tested?
- Basis in paper: The paper identifies depth-2 pruning structures in attention and feed-forward modules and demonstrates effectiveness on LLaMA-7B, GPT-2, Vicuna-7B, and LLaMA-13B.
- Why unresolved: The paper only tests on specific transformer architectures. The depth-2 pruning structure's applicability to other architectures (e.g., sparse transformers, vision transformers) remains unexplored.
- What evidence would resolve it: Experiments applying the method to a broader range of transformer architectures, including sparse and specialized variants.

### Open Question 2
- Question: What is the long-term stability of the pruning method on models undergoing continuous updates or fine-tuning?
- Basis in paper: The paper focuses on static pruning without retraining and demonstrates immediate post-pruning performance.
- Why unresolved: The paper doesn't address how the pruned model performs over time, especially if the model undergoes further updates or fine-tuning.
- What evidence would resolve it: Long-term studies tracking model performance after pruning and subsequent updates or fine-tuning sessions.

### Open Question 3
- Question: How does the pruning method affect the model's robustness to adversarial attacks or out-of-distribution data?
- Basis in paper: The paper evaluates pruning performance on standard benchmarks (perplexity, classification tasks) but does not address robustness to adversarial or out-of-distribution scenarios.
- Why unresolved: The paper does not investigate the impact of pruning on model robustness, a critical aspect for practical deployment.
- What evidence would resolve it: Rigorous testing of pruned models against adversarial attacks and out-of-distribution data to assess robustness changes.

### Open Question 4
- Question: Can the pruning method be extended to other types of neural networks beyond transformers?
- Basis in paper: The paper focuses exclusively on transformer-based models, but the underlying concepts of depth-2 pruning and output approximation might be applicable to other architectures.
- Why unresolved: The paper does not explore the applicability of the pruning method to other neural network architectures (e.g., CNNs, RNNs).
- What evidence would resolve it: Experiments applying the pruning method to various neural network architectures beyond transformers.

## Limitations

- Implementation complexity: The paper introduces sophisticated pruning techniques but lacks detailed implementation specifics, particularly for the two-step reconstruction technique and exact calculation methods for second-moment-based metrics
- Evaluation scope limitations: While demonstrating results on multiple models, evaluation is primarily focused on zero-shot task classification and perplexity metrics, with performance under different inference scenarios unexplored
- Scalability concerns: The method's effectiveness on larger models (beyond 7B parameters) or in production deployment scenarios is not validated, with computational overhead for very large models not discussed

## Confidence

- High confidence: The fundamental approach of using depth-2 pruning structure and inference-aware metrics is well-founded and theoretically sound
- Medium confidence: The effectiveness of similarity-based and second-moment-based metrics compared to traditional training-aware methods is demonstrated but could benefit from more extensive ablation studies
- Low confidence: The practical implementation details, particularly around the two-step reconstruction technique and handling of edge cases in module identification, are not sufficiently detailed for reliable reproduction

## Next Checks

1. **Module identification validation**: Implement and test the depth-2 module identification on a small Transformer model, verifying that pruning decisions preserve residual connection integrity and maintain proper output channel counts across all modules

2. **Metric effectiveness comparison**: Conduct controlled experiments comparing the proposed similarity-based and second-moment-based metrics against random pruning and magnitude-based pruning on attention-only and feed-forward-only models to isolate the effectiveness of each metric

3. **Reconstruction technique verification**: Implement the two-step reconstruction technique and evaluate its effectiveness in mitigating pruning errors by comparing performance with and without reconstruction on progressively larger pruning ratios (20%, 40%, 60%)