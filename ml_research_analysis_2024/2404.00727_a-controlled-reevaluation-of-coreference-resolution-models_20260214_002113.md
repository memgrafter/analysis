---
ver: rpa2
title: A Controlled Reevaluation of Coreference Resolution Models
arxiv_id: '2404.00727'
source_url: https://arxiv.org/abs/2404.00727
tags:
- language
- coreference
- computational
- more
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-evaluates five coreference resolution models by controlling
  for confounding factors such as language model choice and hyperparameter search
  space. The authors find that when controlling for these factors, the oldest model
  (C2F) performs competitively with newer models, and encoder-based models outperform
  decoder-based models at comparable sizes.
---

# A Controlled Reevaluation of Coreference Resolution Models

## Quick Facts
- arXiv ID: 2404.00727
- Source URL: https://arxiv.org/abs/2404.00727
- Authors: Ian Porada; Xiyuan Zou; Jackie Chi Kit Cheung
- Reference count: 0
- Primary result: Controlling for language model choice reveals that the oldest model (C2F) performs competitively with newer models, and encoder-based models outperform decoder-based models at comparable sizes

## Executive Summary
This paper reevaluates five coreference resolution models by systematically controlling for confounding factors such as language model choice and hyperparameter search space. The authors find that when these factors are held constant, the performance gap between models narrows significantly, with the oldest model (C2F) showing competitive performance and better out-of-domain generalization. Encoder-based models consistently outperform decoder-based models at comparable parameter sizes, suggesting that recent improvements in coreference resolution may be largely attributable to stronger language models rather than architectural innovations. The study highlights the importance of controlling for confounding factors when comparing model architectures in NLP.

## Method Summary
The authors reimplement five coreference resolution models (C2F, S2E, WLC, LingMess, and Link-Append) using Hugging Face Transformers, controlling for language model choice and hyperparameter search space. Encoder-based models use DeBERTa variants while decoder-based models use mT5 and Flan-T5. Models are trained on OntoNotes 5.0 with 25, 50, and 125 epochs using four different feed-forward network sizes. Performance is evaluated on OntoNotes development and test sets, OntoGUM for out-of-domain generalization, and GAP for a different coreference resolution task. The study uses permutation tests to assess statistical significance of performance differences.

## Key Results
- Controlling for language model choice reduces most of the performance gap between coreference resolution models
- Encoder-based models outperform decoder-based models at comparable parameter sizes
- The oldest model (C2F) generalizes best to out-of-domain textual genres
- LingMess achieves 81.7 CoNLL F1 on OntoNotes using a 561M parameter DeBERTa encoder, while decoder-based models require over 7x more parameters for similar performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlling for language model choice reduces most of the performance gap between coreference resolution models.
- Mechanism: When different coreference resolution models use different language models (e.g., BERT vs. RoBERTa), performance differences may be attributed to the language model rather than the model architecture itself. By standardizing the language model across all models, the true contribution of architectural differences becomes clearer.
- Core assumption: The language model is the dominant factor in determining coreference resolution performance when all other factors are held constant.
- Evidence anchors:
  - [abstract] "controlling for the choice of language model reduces most, but not all, of the increase in F1 score reported in the past five years."
  - [section 4.1] "we found the opposite to be true when controlling for the choice of language model"
  - [corpus] Weak evidence - only 5 related papers found, none specifically testing this mechanism
- Break condition: If language model choice is not the dominant factor in performance differences, or if architectural differences become significant when controlling for language model choice.

### Mechanism 2
- Claim: Encoder-based coreference resolution models outperform decoder-based models at comparable parameter sizes.
- Mechanism: Encoder-based models process the entire input sequence at once, allowing for better context understanding and more efficient computation compared to autoregressive decoder-based models. This efficiency advantage becomes more pronounced as model size increases.
- Core assumption: The architectural difference between encoder and decoder approaches is the primary driver of performance differences at comparable model sizes.
- Evidence anchors:
  - [abstract] "encoder-based CR models outperform more recent decoder-based models in terms of both accuracy and inference speed"
  - [section 3.2.1] "We do not directly compare encoder versus decoder-based CR models using the exact same language model because these classes of CR models in turn rely on language models of different architectures."
  - [corpus] Weak evidence - only 5 related papers found, none specifically testing this mechanism
- Break condition: If decoder-based models can match or exceed encoder performance at comparable sizes through architectural innovations or training techniques.

### Mechanism 3
- Claim: The oldest coreference resolution model (C2F) generalizes better to out-of-domain textual genres than more recent models of comparable size.
- Mechanism: C2F's simpler architecture and training approach may lead to better generalization across diverse text genres, while more recent models may be overfit to the specific characteristics of the OntoNotes training data.
- Core assumption: Model complexity and training duration negatively impact out-of-domain generalization performance.
- Evidence anchors:
  - [abstract] "the oldest CR model that we test generalizes the best to out-of-domain textual genres"
  - [section 4] "For all models in Table 3, full precision, recall, and test set results are available in our project's GitHub repository"
  - [corpus] Weak evidence - only 5 related papers found, none specifically testing this mechanism
- Break condition: If more recent models can achieve comparable or better out-of-domain generalization through architectural improvements or training techniques.

## Foundational Learning

- Concept: Coreference resolution
  - Why needed here: Understanding the task being evaluated is crucial for interpreting the results and implications of the study.
  - Quick check question: What is the difference between entity coreference and event coreference resolution?

- Concept: Language model pretraining
  - Why needed here: The choice of language model is a key confounding factor in the evaluation, and understanding how language models work is essential for interpreting the results.
  - Quick check question: How does the architecture of a language model (encoder vs. decoder) affect its performance on downstream tasks like coreference resolution?

- Concept: Statistical significance testing
  - Why needed here: The study uses permutation tests to determine if performance differences between models are statistically significant.
  - Quick check question: What is the purpose of using a permutation test in comparing coreference resolution models, and how does it differ from other statistical tests?

## Architecture Onboarding

- Component map:
  Language model (encoder or decoder) -> Task-specific head for coreference resolution -> Span representation method (weighted sum or first/last token embeddings) -> Coreference scoring mechanism -> Inference method (pairwise scoring or sequence generation)

- Critical path:
  1. Load and preprocess input document
  2. Encode document using language model
  3. Generate span representations
  4. Score spans for coreference relationships
  5. Cluster coreferring spans
  6. Output coreference clusters

- Design tradeoffs:
  - Encoder vs. decoder architecture: Encoders offer better context understanding but may struggle with long documents, while decoders can handle longer sequences but may be less efficient.
  - Span representation method: Weighted sum of all token embeddings captures more context but is computationally expensive, while first/last token embeddings are more efficient but may miss important information.
  - Training duration: Longer training may improve performance on the training set but could lead to overfitting and reduced out-of-domain generalization.

- Failure signatures:
  - Poor performance on out-of-domain datasets: Indicates overfitting to training data or lack of robustness in the model architecture.
  - High memory usage during inference: Suggests inefficient use of the language model or span representation method.
  - Slow inference speed: May indicate a need for architectural optimizations or more efficient span scoring methods.

- First 3 experiments:
  1. Compare the performance of C2F and LingMess on the OntoNotes development set using the same language model (DeBERTa-large) and hyperparameters.
  2. Evaluate the out-of-domain generalization of all models on the OntoGUM dataset, controlling for language model choice and hyperparameter search space.
  3. Test the efficiency of encoder-based and decoder-based models at different parameter sizes on the GAP dataset, measuring both accuracy and inference speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does model architecture versus language model quality drive performance improvements in coreference resolution?
- Basis in paper: [explicit] The authors conclude that "controlling for the choice of language model reduces most, but not all, of the increase in F1 score reported in the past five years"
- Why unresolved: While the study controls for language model choice and finds encoder-based models outperform decoder-based models at comparable sizes, it cannot completely isolate the effect of architectural innovations from improved language model quality
- What evidence would resolve it: A direct comparison using identical language models with different architectures, or systematic ablation studies of architectural components across models

### Open Question 2
- Question: How do coreference resolution models perform on languages and domains beyond English OntoNotes?
- Basis in paper: [explicit] The authors evaluate on OntoGUM and GAP datasets but acknowledge these are still English-language resources
- Why unresolved: The study focuses exclusively on English-language coreference resolution, limiting generalizability to multilingual contexts
- What evidence would resolve it: Systematic evaluation of the same models across multiple languages and diverse domain types

### Open Question 3
- Question: What is the optimal trade-off between model size, inference speed, and accuracy for practical deployment of coreference resolution systems?
- Basis in paper: [inferred] The authors note that encoder-based models are more efficient but also show LingMess achieves state-of-the-art performance when scaled to 1.5B parameters
- Why unresolved: The paper demonstrates different models optimize for different dimensions but doesn't provide a unified framework for balancing these factors
- What evidence would resolve it: Comprehensive analysis of the Pareto frontier across accuracy, speed, and memory usage across model sizes and architectures

## Limitations
- Focus on English-language coreference resolution limits generalizability to other languages
- Limited out-of-domain testing may not capture full generalization capabilities across diverse text genres
- Inability to use identical language models across encoder/decoder architectures constrains direct architectural comparisons

## Confidence
- High confidence: The finding that controlling for language model choice reduces performance differences between models is well-supported by the experimental design and results
- Medium confidence: The conclusion that encoder-based models outperform decoder-based models at comparable sizes is supported by the data but limited by architectural constraints
- Medium confidence: The observation that C2F generalizes best to out-of-domain data is based on limited out-of-domain testing

## Next Checks
1. Replicate the controlled evaluation methodology on a multilingual coreference resolution dataset to test the generalizability of findings across languages
2. Conduct an ablation study to isolate the specific architectural differences between encoder and decoder models that contribute to performance gaps
3. Evaluate model performance on additional out-of-domain datasets with diverse text genres to better understand generalization capabilities across the entire model spectrum