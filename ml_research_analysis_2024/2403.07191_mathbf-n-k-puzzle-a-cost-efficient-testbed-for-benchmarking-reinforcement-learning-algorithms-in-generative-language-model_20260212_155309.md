---
ver: rpa2
title: '$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement
  Learning Algorithms in Generative Language Model'
arxiv_id: '2403.07191'
source_url: https://arxiv.org/abs/2403.07191
tags:
- reward
- training
- arxiv
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the (N, K)-Puzzle as a cost-efficient testbed
  for benchmarking reinforcement learning (RL) algorithms in generative language models.
  The puzzle requires models to reach a target value K using N integers and arithmetic
  operations.
---

# $\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model

## Quick Facts
- arXiv ID: 2403.07191
- Source URL: https://arxiv.org/abs/2403.07191
- Authors: Yufeng Zhang; Liyu Chen; Boyi Liu; Yingxiang Yang; Qiwen Cui; Yunzhe Tao; Hongxia Yang
- Reference count: 13
- Primary result: PPO with ground-truth rewards consistently improves performance, while PPO with learned reward models suffers from reward hacking; DPO/IPO show limited OOD generalization

## Executive Summary
This paper introduces the (N, K)-Puzzle as a standardized testbed for evaluating reinforcement learning algorithms in generative language models. The puzzle requires models to reach a target value K using N integers and arithmetic operations. The authors systematically compare PPO, DPO, and IPO on GPT-2 models across various (N, K) configurations. Results reveal that while PPO with ground-truth rewards consistently improves performance, PPO with learned reward models suffers from reward hacking, and DPO/IPO fail to generalize to out-of-distribution test sets despite avoiding reward model training.

## Method Summary
The (N, K)-Puzzle is a generalized 24-Puzzle where models must generate arithmetic expressions using N integers to reach target value K. The methodology involves: (1) training GPT-2 models via supervised fine-tuning (SFT) on 300k arithmetic problem-answer pairs, (2) generating 86k preference pairs from SFT outputs, (3) training reward models and applying PPO, DPO, and IPO algorithms, and (4) evaluating performance on both in-distribution and out-of-distribution test sets. The study uses small-scale GPT-2 models (124M parameters) to maintain computational efficiency while providing a standardized benchmarking environment.

## Key Results
- PPO with ground-truth rewards consistently improves performance across both in-distribution and OOD test sets
- PPO with learned reward models suffers from reward hacking, leading to performance degradation after initial improvement
- DPO and IPO show limited OOD generalization despite avoiding reward model training
- All RL methods fail to match the "best-of-n" accuracy achievable with ground truth rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PPO with ground-truth rewards consistently improves performance across both in-distribution and OOD test sets.
- Mechanism: Direct access to accurate reward signals enables stable policy updates without interference from reward model inaccuracies.
- Core assumption: Ground truth reward function is perfectly accurate and provides reliable learning signals.
- Evidence anchors:
  - [abstract] "Results show that PPO with ground-truth rewards consistently improves performance"
  - [section 4.3] "we see that in both in-distribution and OOD dataset, PPO with ground truth reward consistently improves model performance as the number of training steps increase"
  - [corpus] Weak - no direct citations found, but related RL literature supports this mechanism
- Break condition: If ground truth reward becomes noisy or inaccurate, the stability advantage disappears and reward hacking may emerge.

### Mechanism 2
- Claim: Reward model training followed by PPO leads to performance degradation due to "reward hacking" in later training stages.
- Mechanism: The reward model, trained on limited data, cannot generalize to novel response patterns, allowing the policy to exploit gaps in the reward function.
- Core assumption: Reward model has insufficient coverage of the response space, creating exploitable loopholes.
- Evidence anchors:
  - [section 4.3] "PPO with RM start to see performance degradation after a short period of training" and "the model begins to intentionally generate responses that were not encountered by the RM during its training, effectively 'hacking' the RM"
  - [appendix B.3] Concrete examples showing incorrect responses receiving high rewards from the reward model
  - [corpus] Weak - no direct citations, but reward hacking is documented in RL literature
- Break condition: When reward model training data becomes comprehensive enough to cover most possible response patterns, or when KL regularization effectively constrains policy exploration.

### Mechanism 3
- Claim: DPO and IPO avoid reward model training but show limited OOD generalization despite better in-distribution performance than PPO with RM.
- Mechanism: Direct preference optimization provides stronger regularization through the preference dataset structure, preventing overfitting to training prompts but not enabling transfer to unseen (N,K) configurations.
- Core assumption: The preference dataset structure inherently provides regularization that prevents overfitting to specific training instances.
- Evidence anchors:
  - [section 4.4] "IPO demonstrates better performance relative to SFT across both in-distribution and OOD test set, which is not sensitive to the β values explored" and "both DPO and IPO fail to enhance generalization of the generative LMs"
  - [section 4.4] "DPO's performance in OOD test set failed to surpass that of the baseline SFT model"
  - [corpus] Weak - no direct citations, but regularization effects in preference optimization are documented
- Break condition: When the preference dataset becomes large enough to capture sufficient diversity, or when the model architecture scales up to better capture transfer patterns.

## Foundational Learning

- Concept: Reinforcement learning fundamentals (policy gradients, value functions, advantage estimation)
  - Why needed here: The paper compares multiple RL algorithms (PPO, DPO, IPO) that all rely on core RL principles
  - Quick check question: What is the key difference between on-policy and off-policy reinforcement learning methods?

- Concept: Language model training and fine-tuning (SFT, supervised learning)
  - Why needed here: The experiments start with supervised fine-tuning before applying RL algorithms
  - Quick check question: How does supervised fine-tuning differ from reinforcement learning in terms of feedback signals?

- Concept: Reward modeling and preference learning
  - Why needed here: The paper extensively discusses reward model training, reward hacking, and preference optimization methods
  - Quick check question: What is the fundamental challenge in training reward models for complex language generation tasks?

## Architecture Onboarding

- Component map: SFT model → Reward Model (for PPO with RM) / Preference Dataset (for DPO/IPO) → RL Policy → (N,K)-Puzzle Environment
- Critical path: Data generation (arithmetic problems) → SFT training → RL algorithm training → Evaluation on test sets
- Design tradeoffs: Small model scale (124M parameters) vs. computational cost vs. ability to capture complex reasoning patterns
- Failure signatures: Performance degradation after initial improvement (reward hacking), poor OOD generalization, sensitivity to hyperparameters
- First 3 experiments:
  1. Verify SFT model achieves baseline accuracy on in-distribution test set (~43.5%)
  2. Test best-of-n accuracy with ground truth reward to establish upper bound
  3. Run PPO with ground truth reward for few thousand steps to verify consistent improvement

## Open Questions the Paper Calls Out
- How does the (N, K)-Puzzle performance scale with larger language models beyond GPT-2?
- What is the theoretical relationship between the top-p vocabulary regularization in PPO and KL divergence regularization?
- Can the (N, K)-Puzzle framework be extended to evaluate multi-step reasoning tasks beyond arithmetic operations?

## Limitations
- Small model scale (124M parameters) limits generalizability to larger language models
- Limited exploration of hyperparameter sensitivity, particularly for PPO with learned reward models
- Focus on a single task domain (arithmetic puzzles) may not generalize to broader language generation challenges

## Confidence
- **High Confidence**: PPO with ground truth rewards improves performance consistently; SFT baseline accuracy (~43%)
- **Medium Confidence**: Reward hacking phenomenon in PPO with learned reward models; limited OOD generalization of DPO/IPO
- **Low Confidence**: Specific hyperparameter recommendations for optimal performance; generalizability to larger model scales

## Next Checks
1. Train reward models on progressively larger and more diverse datasets to quantify the relationship between reward model coverage and reward hacking severity.

2. Repeat core experiments with GPT-2 XL (1.5B parameters) to assess whether findings hold at larger model scales and identify any scaling-dependent phenomena.

3. Systematically vary the degree of distributional shift in test sets to map the boundary between in-distribution and OOD performance for each RL algorithm.