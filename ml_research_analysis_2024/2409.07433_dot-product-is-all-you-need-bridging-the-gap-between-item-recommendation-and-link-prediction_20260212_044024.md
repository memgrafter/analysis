---
ver: rpa2
title: 'Dot Product is All You Need: Bridging the Gap Between Item Recommendation
  and Link Prediction'
arxiv_id: '2409.07433'
source_url: https://arxiv.org/abs/2409.07433
tags:
- recommendation
- link
- item
- prediction
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges item recommendation and link prediction by showing
  that item recommendation can be treated as a link prediction problem, where users
  and items are entities in a knowledge graph connected by the "interactsWith" relation.
  The authors test three popular factorisation-based link prediction models (DistMult,
  CP, and ComplEx) on three widely-used recommendation datasets (Gowalla, Yelp 2018,
  and Amazon Book) without any modifications to their architectures.
---

# Dot Product is All You Need: Bridging the Gap Between Item Recommendation and Link Prediction

## Quick Facts
- arXiv ID: 2409.07433
- Source URL: https://arxiv.org/abs/2409.07433
- Reference count: 40
- Primary result: Item recommendation can be treated as link prediction, achieving competitive performance with existing recommendation models

## Executive Summary
This paper bridges the gap between item recommendation and link prediction by reformulating the recommendation problem as a link prediction task on knowledge graphs. The authors demonstrate that existing factorisation-based link prediction models (DistMult, CP, ComplEx) can be directly applied to recommendation datasets without architectural modifications, achieving competitive performance with state-of-the-art recommendation systems. The key insight is that user-item interactions can be represented as "interactsWith" relations in a knowledge graph, allowing recommendation to be solved using established link prediction techniques.

## Method Summary
The authors treat item recommendation as a link prediction problem by mapping users and items to entities in a knowledge graph connected by a single "interactsWith" relation. Three popular link prediction models (DistMult, CP, ComplEx) are trained on three widely-used recommendation datasets (Gowalla, Yelp 2018, Amazon Book) using standard link prediction techniques including negative sampling and various training strategies. The models are evaluated using modified recommendation metrics (Recall@20, nDCG@20) without any architectural changes to the original link prediction implementations.

## Key Results
- Link prediction models achieve competitive performance with ten state-of-the-art recommendation models
- Models consistently place in the top-5 for Recall@20 and nDCG@20 metrics
- Higher embedding sizes generally improve recommendation performance
- No architectural modifications needed to adapt link prediction models to recommendation task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating item recommendation as link prediction enables direct reuse of existing knowledge graph embedding models without architectural modifications.
- Mechanism: Users and items are mapped to entities in a knowledge graph with a single relation type "interactsWith", allowing link prediction models to operate directly on user-item interaction data.
- Core assumption: The factorization-based scoring functions used in link prediction are sufficient for modeling user-item interactions in recommendation.
- Evidence anchors: The paper demonstrates that existing link prediction models can be directly applied to recommendation tasks without modifications.

### Mechanism 2
- Claim: Higher embedding sizes improve recommendation performance when applying link prediction models to recommendation tasks.
- Mechanism: Increasing the dimensionality of entity embeddings allows for more expressive representations, capturing finer-grained patterns in user-item interactions.
- Core assumption: The improvement from larger embeddings follows the same pattern as in pure link prediction tasks.
- Evidence anchors: Empirical results show performance improvements with larger embedding sizes.

### Mechanism 3
- Claim: The dot product scoring function is sufficient for both link prediction and recommendation tasks.
- Mechanism: Both tasks involve predicting the strength of relationships between entities, which can be captured by the similarity of their embeddings measured through dot product.
- Core assumption: The underlying interaction patterns in both knowledge graphs and user-item interactions can be effectively modeled through embedding similarity.
- Evidence anchors: The competitive performance of dot product-based models on recommendation tasks.

## Foundational Learning

- Concept: Knowledge graph embedding models (TransE, DistMult, CP, ComplEx)
  - Why needed here: These models provide the foundation for treating recommendation as link prediction
  - Quick check question: Can you explain how TransE's scoring function differs from DistMult's and why this matters for recommendation?

- Concept: Matrix factorization and collaborative filtering
  - Why needed here: Recommendation systems traditionally use matrix factorization, which is mathematically equivalent to the scoring functions used in link prediction models
  - Quick check question: How does the dot product in matrix factorization relate to the tri-linear dot product in DistMult?

- Concept: Negative sampling and training objectives in link prediction
  - Why needed here: Understanding how link prediction models handle missing links is crucial for adapting them to recommendation
  - Quick check question: What is the difference between 1vsAll and KvsAll training strategies in link prediction?

## Architecture Onboarding

- Component map: Data preprocessing (user-item to knowledge graph triples) -> Link prediction model (DistMult/CP/ComplEx) -> Training loop with negative sampling -> Evaluation using recommendation metrics
- Critical path: Data conversion → Model training with negative sampling → Evaluation on held-out interactions
- Design tradeoffs: Using existing link prediction models avoids architectural modifications but may miss recommendation-specific optimizations
- Failure signatures: Poor performance on sparse datasets, overfitting with large embedding sizes, failure to capture sequential patterns
- First 3 experiments:
  1. Train DistMult on Gowalla dataset with default embedding size (64) and compare to baseline recommendation models
  2. Vary embedding size from 64 to 1024 on Yelp 2018 dataset to observe performance trends
  3. Test different training strategies (1vsAll vs KvsAll) on Amazon Book dataset to find optimal training approach

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability to domains with complex interaction patterns beyond bipartite graphs
- Single relation type assumption may oversimplify recommendation scenarios with rich relational structures
- Static modeling approach may not capture temporal dynamics in user preferences

## Confidence

**High Confidence**: The core claim that item recommendation can be reformulated as a link prediction problem is well-supported by the mathematical framework and empirical results.

**Medium Confidence**: The claim about higher embedding sizes improving performance has some empirical support but requires more systematic investigation across different dataset characteristics.

**Low Confidence**: The assertion that "dot product is all you need" for bridging recommendation and link prediction may be overstated, as the paper doesn't thoroughly explore more complex scoring functions.

## Next Checks

1. Test the approach on sequential recommendation datasets (e.g., LastFM, Taobao) and multi-relational recommendation scenarios to assess generalizability beyond bipartite interactions.

2. Systematically evaluate the impact of introducing additional relation types beyond "interactsWith" (e.g., "similarTo" between items, "friendOf" between users) to understand the limits of the single-relation assumption.

3. Conduct experiments on datasets with temporal information to assess how well the static link prediction models handle dynamic recommendation scenarios where user preferences evolve over time.