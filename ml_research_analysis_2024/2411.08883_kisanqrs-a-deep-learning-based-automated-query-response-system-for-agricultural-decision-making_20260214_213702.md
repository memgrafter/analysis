---
ver: rpa2
title: 'KisanQRS: A Deep Learning-based Automated Query-Response System for Agricultural
  Decision-Making'
arxiv_id: '2411.08883'
source_url: https://arxiv.org/abs/2411.08883
tags:
- query
- queries
- cluster
- similarity
- kisanqrs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces KisanQRS, a deep learning-based automated
  query-response system for agricultural decision-making. The system addresses challenges
  in traditional farmers' helpline centers by providing quick and accurate responses
  using NLP techniques.
---

# KisanQRS: A Deep Learning-based Automated Query-Response System for Agricultural Decision-Making

## Quick Facts
- arXiv ID: 2411.08883
- Source URL: https://arxiv.org/abs/2411.08883
- Reference count: 5
- Achieves 96.58% F1-score for query mapping and 96.20% NDCG for answer retrieval

## Executive Summary
KisanQRS introduces a deep learning-based automated query-response system designed to improve agricultural decision-making by addressing limitations in traditional farmers' helpline centers. The system leverages natural language processing techniques, specifically combining semantic and lexical similarities of queries through a threshold-based clustering method and an LSTM model for query mapping. By integrating SBERT embeddings with Jaccard similarity, KisanQRS achieves high accuracy in both query clustering and answer retrieval, making it a practical solution for enabling farmers to make informed decisions about their farming practices.

## Method Summary
The system processes agricultural query data through multiple stages: preprocessing to clean and normalize queries, threshold-based clustering using a weighted combination of SBERT cosine similarity and Jaccard token similarity, LSTM-based query mapping with SBERT embeddings, and answer retrieval through cluster-based ranking. The approach uses a dataset of 34 million KCC call logs from 2018-2020, with 300,000 samples from 5 Indian states for model training and 10,000 samples for answer retrieval evaluation. The clustering algorithm employs a linear search method that adds each query to the first cluster exceeding a similarity threshold, while the query mapping stage uses an LSTM model trained on cluster labels derived from the preprocessed data.

## Key Results
- Achieves 96.58% F1-score for query mapping in Andhra Pradesh state
- Competitive NDCG score of 96.20% for answer retrieval
- Outperforms traditional techniques in both clustering quality (Silhouette Score 0.82) and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic and lexical similarity integration improves query clustering accuracy over purely lexical methods.
- Mechanism: Combines cosine similarity of contextual embeddings (semantic) with Jaccard index (lexical) into a weighted sum to capture both meaning and keyword overlap.
- Core assumption: High λ (0.78–0.81) gives better results because SBERT embeddings correlate more strongly with cluster labels than Jaccard alone.
- Evidence anchors:
  - [abstract] "integrates semantic and lexical similarities of farmers queries"
  - [section] "empirical analysis indicates that the best results are achieved when λ is between 0.78 and 0.81"
- Break condition: If queries are very short and keyword-heavy, Jaccard similarity dominates and embeddings add noise; tuning λ is critical.

### Mechanism 2
- Claim: Threshold-based clustering with linear search is faster and sufficiently accurate for this dataset.
- Mechanism: Iterates through queries once, adding each to the first cluster exceeding a similarity threshold; skips expensive graph-based clustering.
- Core assumption: After preprocessing, similar queries share identical keyword sets, so ordering doesn't matter much; similarity transitivity holds.
- Evidence anchors:
  - [section] "our proposed algorithm provides a framework for clustering queries based on semantic and lexical similarities. It is an effective algorithm that is fast and simple to implement"
  - [section] runtime comparison shows 12 sec vs 14-19 min for alternatives
- Break condition: If query similarity distribution is less uniform or transitive relationships break, clusters may fragment or merge incorrectly.

### Mechanism 3
- Claim: LSTM with SBERT embeddings yields better query mapping than other feature sets.
- Mechanism: SBERT provides context-aware embeddings; LSTM captures sequential dependencies; together they outperform MLP, GRU, and traditional ML on this classification task.
- Core assumption: The KCC dataset contains queries with clear semantic meaning; short-phrase queries benefit from contextual embeddings.
- Evidence anchors:
  - [section] "LSTM with SBERT is performing better as compared to other models" and "LSTM model consistently demonstrates superior performance"
  - [section] "SBERT is based on BERT architecture which is a transformer-based model. Transformer-based models consider the context of a word while extracting embeddings"
- Break condition: If queries are extremely short and keyword-driven, BoW or TF-IDF may match performance; overfitting possible with high-dimensional embeddings.

## Foundational Learning

- Concept: Sentence-BERT (SBERT) transformer embeddings
  - Why needed here: Provides contextual, dense vector representations of queries for semantic similarity computation.
  - Quick check question: What is the dimensionality of SBERT embeddings used in the system?

- Concept: Jaccard similarity for lexical overlap
  - Why needed here: Captures token-level similarity between queries; complements semantic similarity from embeddings.
  - Quick check question: How is Jaccard similarity defined between two token sets A and B?

- Concept: Long Short-Term Memory (LSTM) networks for sequence classification
  - Why needed here: Learns temporal dependencies in query embeddings to map them to correct clusters.
  - Quick check question: What is the advantage of LSTM over plain dense layers for this task?

## Architecture Onboarding

- Component map: Data Preprocessing → Query Clustering (threshold-based) → Query Mapping (LSTM) → Answer Retrieval (clustering + ranking)
- Critical path: User query → preprocessing → SBERT embedding → LSTM inference → cluster lookup → candidate answer selection → ranking → top-K output
- Design tradeoffs: Speed vs accuracy in clustering (threshold-based vs DBSCAN), embedding size vs model complexity (768 vs smaller), K value vs NDCG
- Failure signatures: Low silhouette scores indicate poor clusters; low LSTM accuracy indicates mismatched labels; NDCG drop indicates retrieval issues
- First 3 experiments:
  1. Vary λ (0.5 to 0.9) and observe silhouette score and clustering runtime.
  2. Test LSTM vs GRU with same embeddings and measure F1-score.
  3. Run answer retrieval with K=1,3,5 and plot NDCG to find sweet spot.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold value for the clustering algorithm in different agricultural domains or languages?
- Basis in paper: [explicit] The paper experiments with threshold values of 0.80, 0.90, and 0.95, but does not explore a wider range or different domains.
- Why unresolved: The optimal threshold may vary depending on the specific characteristics of the dataset, such as query formulation patterns or language differences.
- What evidence would resolve it: Conducting experiments with a wider range of threshold values on datasets from different agricultural domains or languages, and comparing the clustering performance.

### Open Question 2
- Question: How does the performance of KisanQRS compare to human experts in terms of accuracy and relevance of responses?
- Basis in paper: [inferred] The paper focuses on the performance of KisanQRS compared to traditional techniques, but does not compare it to human experts.
- Why unresolved: Human experts may provide more accurate and relevant responses, especially for complex or nuanced queries that require domain knowledge and reasoning.
- What evidence would resolve it: Conducting a comparative study between KisanQRS and human experts, evaluating their performance on a set of queries with ground truth answers provided by domain experts.

### Open Question 3
- Question: How does the performance of KisanQRS generalize to other languages and regions outside of India?
- Basis in paper: [explicit] The paper focuses on the KCC dataset from India and does not explore the performance of KisanQRS on datasets from other regions or languages.
- Why unresolved: The performance of KisanQRS may be affected by differences in language, query formulation patterns, and domain-specific knowledge across regions.
- What evidence would resolve it: Evaluating the performance of KisanQRS on datasets from other countries or regions, and comparing the results to the performance on the KCC dataset.

## Limitations
- Limited generalizability beyond the Kisan Call Centre dataset
- Heavy reliance on SBERT embeddings may not perform well with extremely short or keyword-focused queries
- Threshold-based clustering may struggle with non-uniform query similarity distributions

## Confidence
- **High Confidence**: Query mapping performance metrics (F1-score of 96.58% and accuracy of 97.12%) and clustering evaluation metrics (Silhouette Score of 0.82, Calinski-Harabasz Index of 4125.52) are well-documented with clear methodology.
- **Medium Confidence**: Answer retrieval performance (NDCG of 96.20% and MAP of 96.34%) relies on cluster-based ranking, which may be sensitive to cluster quality and query distribution.
- **Low Confidence**: The system's effectiveness with languages other than English and its performance on datasets with significantly different query patterns remain unverified.

## Next Checks
1. **Cross-dataset Validation**: Test KisanQRS on agricultural query datasets from different regions or languages to evaluate generalizability beyond the Kisan Call Centre data.

2. **Query Length Sensitivity Analysis**: Systematically evaluate system performance on queries of varying lengths (short phrases vs. longer questions) to identify potential limitations of the SBERT+LSTM approach.

3. **Ablation Study on Similarity Components**: Conduct controlled experiments varying λ from 0.5 to 0.9 to quantify the relative contribution of semantic vs. lexical similarity components to overall system performance.