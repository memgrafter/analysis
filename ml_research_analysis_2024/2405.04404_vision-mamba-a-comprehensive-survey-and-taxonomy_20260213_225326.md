---
ver: rpa2
title: 'Vision Mamba: A Comprehensive Survey and Taxonomy'
arxiv_id: '2405.04404'
source_url: https://arxiv.org/abs/2405.04404
tags:
- mamba
- arxiv
- image
- state
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey and taxonomy of Vision
  Mamba, a state space model (SSM) for computer vision tasks. Mamba combines the advantages
  of convolutional neural networks and transformers while addressing their limitations.
---

# Vision Mamba: A Comprehensive Survey and Taxonomy

## Quick Facts
- arXiv ID: 2405.04404
- Source URL: https://arxiv.org/abs/2405.04404
- Authors: Xiao Liu; Chenxu Zhang; Lei Zhang
- Reference count: 40
- Key outcome: Comprehensive survey and taxonomy of Vision Mamba, a state space model for computer vision tasks

## Executive Summary
This paper presents a comprehensive survey and taxonomy of Vision Mamba, a state space model (SSM) that combines the advantages of convolutional neural networks and transformers while addressing their limitations. Mamba uses a selective state space model with time-varying parameters and a hardware-aware algorithm for efficient training and inference. The paper categorizes Mamba variants according to downstream application scenarios, including general vision tasks, multi-modal tasks, and vertical-domain tasks like remote sensing and medical image analysis. The survey discusses the predecessors, recent advances, and impact of Mamba on various domains, providing insights into the principles and technical details of each method.

## Method Summary
The survey method involves reviewing existing literature on Mamba and its variants in computer vision tasks, analyzing different categories of Mamba models according to their specific applications, and summarizing key findings, challenges, and future research directions. The authors categorize Mamba models into general vision, multi-modal, and vertical-domain tasks, discussing their technical details, advantages, and limitations. The survey also highlights the potential of Mamba in handling long sequences, capturing long-range dependencies, and achieving linear computational complexity.

## Key Results
- Mamba merges time-varying parameters into SSMs to obtain selective state space models (S6) for efficient training and inference
- Hardware-aware algorithms enable Mamba to scale efficiently on GPUs by avoiding memory bottlenecks through segmented computation
- Cross-scan strategies in VMamba extend Mamba to 2D visual data while preserving spatial structure and global relevance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mamba replaces self-attention with a selective state space model that can handle long sequences in linear time.
- **Mechanism:** The selective mechanism parameterizes the state space matrices (B, C, ∆) as functions of the input, enabling time-varying behavior that mimics attention-like selectivity without the quadratic cost.
- **Core assumption:** Time-varying parameters can effectively learn token-level importance without explicit pairwise comparisons.
- **Evidence anchors:**
  - [abstract] "merges time-varying parameters into SSMs and formulates a hardware-aware algorithm for efficient training and inference"
  - [section] "Mamba merges the selectivity mechanism into the state space model to obtain Selective State Space Models (S6)"
- **Break condition:** If the selective parameters cannot generalize across different input distributions, performance will degrade.

### Mechanism 2
- **Claim:** Hardware-aware state expansion allows Mamba to scale efficiently on GPUs by avoiding memory bottlenecks.
- **Mechanism:** Mamba uses a scanning algorithm instead of convolution, computing sequences in segments and combining them iteratively. This avoids the need to store intermediate states and leverages fast SRAM memory for discretization and recursion.
- **Core assumption:** GPUs can efficiently execute the segmented scan without sacrificing parallelization benefits.
- **Evidence anchors:**
  - [abstract] "hardware-aware algorithm for efficient training and inference"
  - [section] "hardware-aware parallel algorithm in cyclic mode" and "Mamba takes advantage of the GPU's HBM and fast SRAM IOs"
- **Break condition:** If the scan segmentation introduces too much overhead, the theoretical speedup will not materialize.

### Mechanism 3
- **Claim:** Cross-scan strategies in VMamba enable 2D visual data to be processed without losing spatial structure.
- **Mechanism:** VMamba employs a cross-scan module that traverses spatial dimensions in four directions (left-to-bottom-right, bottom-right-to-top-left, etc.), converting non-causal visual data into sequential patches while preserving global relevance.
- **Core assumption:** Directional scanning can capture the full spatial context without requiring full attention.
- **Evidence anchors:**
  - [abstract] "extends Mamba from natural language domain to visual domain"
  - [section] "VMamba [68] proposes a cross-scan strategy to bridge the gap between 1D array and 2D sequence scanning"
- **Break condition:** If the cross-scan misses critical long-range dependencies, performance will fall below attention-based models.

## Foundational Learning

- **Concept:** State Space Models (SSMs) as continuous-time dynamical systems
  - Why needed here: Mamba builds directly on SSM theory, so understanding the A, B, C, D matrices and discretization is essential for grasping how Mamba works.
  - Quick check question: What is the role of the A matrix in the continuous-time SSM formulation?
- **Concept:** Linear complexity vs. quadratic complexity in sequence modeling
  - Why needed here: Mamba's main advantage over Transformers is its linear scaling, so engineers must understand why attention is O(n²) and how SSMs avoid this.
  - Quick check question: How does the convolution kernel K in SSMs enable parallel computation?
- **Concept:** Hardware-aware algorithm design
  - Why needed here: Mamba's performance depends on efficient GPU utilization, so understanding memory hierarchy (HBM vs. SRAM) is critical.
  - Quick check question: Why does Mamba recompute intermediate states during backpropagation instead of storing them?

## Architecture Onboarding

- **Component map:** Input projection -> Patch embedding -> Positional embedding -> Cross-scan (if 2D) -> Sequential blocks (Mamba + MLP) -> Final output
- **Critical path:**
  1. Input → Patch embedding → Positional embedding → Cross-scan (if 2D)
  2. Cross-scan output → Sequential blocks (Mamba + MLP) → Final output
- **Design tradeoffs:**
  - Linear complexity vs. expressiveness: Mamba may miss some long-range dependencies that attention captures.
  - Hardware efficiency vs. flexibility: The scan algorithm is optimized for GPUs but may be less flexible than attention.
  - Bidirectional vs. unidirectional: Bidirectional scanning captures more context but may introduce spurious associations.
- **Failure signatures:**
  - Performance degrades on tasks requiring complex global reasoning.
  - Training instability when selective parameters become too extreme.
  - Memory usage spikes if scan segmentation is not properly implemented.
- **First 3 experiments:**
  1. Compare Mamba vs. Transformer on a small image classification task to measure speed and accuracy trade-offs.
  2. Test different scanning strategies (cross-scan, local scan, Hilbert scan) on a semantic segmentation dataset to evaluate spatial context capture.
  3. Measure GPU memory usage and throughput for varying sequence lengths to validate hardware-aware scaling claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Mamba architectures be optimized for causal visual data processing, given their inherent unidirectional nature?
- Basis in paper: [explicit] The paper discusses Mamba's challenges in handling non-causal visual data and the need for scanning mechanisms to bridge the gap between 1D arrays and 2D sequences.
- Why unresolved: While various scanning strategies are proposed (e.g., cross-scan, continuous 2D scanning), their effectiveness in capturing the full spatial context of images while maintaining computational efficiency remains an open question.
- What evidence would resolve it: Comparative studies evaluating different scanning mechanisms on diverse visual tasks, analyzing their impact on performance, efficiency, and ability to capture long-range dependencies.

### Open Question 2
- Question: What is the optimal architecture for combining Mamba with other neural network modules (e.g., CNN, Transformer) to maximize performance and efficiency?
- Basis in paper: [explicit] The paper highlights the potential of hybrid models combining Mamba with CNNs or Transformers, but also mentions the challenges of integrating these architectures due to their fundamental differences.
- Why unresolved: The paper suggests that the optimal combination of Mamba with other architectures depends on the specific task and data characteristics, but a comprehensive understanding of the trade-offs and best practices is lacking.
- What evidence would resolve it: Systematic ablation studies and architectural explorations to identify the most effective ways to integrate Mamba with other modules, considering factors like task type, data modality, and computational constraints.

### Open Question 3
- Question: How can Mamba be scaled effectively to handle extremely large models and datasets while maintaining its computational advantages?
- Basis in paper: [inferred] The paper mentions the importance of following the "law of scale" for large models but does not provide specific insights into how Mamba can be scaled effectively.
- Why unresolved: As Mamba is a relatively new architecture, its behavior and limitations at scale are not yet fully understood. Scaling up Mamba models presents challenges in terms of memory consumption, training stability, and maintaining its linear complexity.
- What evidence would resolve it: Empirical studies on scaling Mamba models to different sizes, analyzing their performance, computational requirements, and memory usage. Investigation of techniques like model parallelism, sparse attention, and efficient training algorithms tailored for Mamba.

## Limitations

- The paper lacks direct empirical comparisons between Mamba variants and other state-of-the-art approaches in specific tasks, making it difficult to quantify performance gains.
- Some implementation details of mentioned variants are not fully specified, which could lead to reproducibility challenges.
- Claims about the superiority of specific scanning strategies (cross-scan vs. local scan) lack direct experimental support in the survey.

## Confidence

- **High confidence**: The general framework of Mamba as a selective state space model and its advantages over transformers (linear complexity, hardware efficiency) are well-established in the literature.
- **Medium confidence**: The categorization of Mamba variants by application domain is reasonable, but the relative performance and practical utility of each variant in specific tasks remain uncertain without empirical validation.
- **Low confidence**: Claims about the superiority of specific scanning strategies (cross-scan vs. local scan) lack direct experimental support in the survey.

## Next Checks

1. Conduct a controlled experiment comparing Mamba variants with transformers on a standardized image classification benchmark, measuring both accuracy and computational efficiency across varying sequence lengths.
2. Implement and test different scanning strategies (cross-scan, local scan, Hilbert scan) on a semantic segmentation task to evaluate their effectiveness in capturing spatial context and preserving global relevance.
3. Measure GPU memory usage and throughput for Mamba models with varying sequence lengths and model sizes to validate the hardware-aware scaling claims and identify potential bottlenecks in practical deployment.