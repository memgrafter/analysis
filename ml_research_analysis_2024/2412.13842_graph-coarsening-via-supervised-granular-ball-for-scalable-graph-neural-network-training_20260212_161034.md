---
ver: rpa2
title: Graph Coarsening via Supervised Granular-Ball for Scalable Graph Neural Network
  Training
arxiv_id: '2412.13842'
source_url: https://arxiv.org/abs/2412.13842
tags:
- graph
- granular-ball
- coarsening
- nodes
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Supervised Granular-Ball Graph Coarsening
  (SGBGC), a method to enhance the scalability of Graph Neural Networks (GNNs) by
  compressing large graphs while preserving key structural and label information.
  The core idea is to iteratively split graphs into granular-balls based on node label
  purity, using these as super vertices to construct a coarsened graph.
---

# Graph Coarsening via Supervised Granular-Ball for Scalable Graph Neural Network Training

## Quick Facts
- arXiv ID: 2412.13842
- Source URL: https://arxiv.org/abs/2412.13842
- Reference count: 40
- Key outcome: SGBGC achieves up to 20x graph size reduction while maintaining accuracy comparable to training on original graph

## Executive Summary
This paper introduces Supervised Granular-Ball Graph Coarsening (SGBGC), a method to enhance the scalability of Graph Neural Networks (GNNs) by compressing large graphs while preserving key structural and label information. The core idea is to iteratively split graphs into granular-balls based on node label purity, using these as super vertices to construct a coarsened graph. SGBGC adaptively performs splitting without requiring a predefined coarsening rate, addressing the inefficiency and lack of adaptability in existing methods. Experimental results demonstrate that SGBGC achieves accuracy comparable to training on the original graph, reduces graph size by up to 20 times, and exhibits superior robustness to noise.

## Method Summary
SGBGC enhances GNN scalability through a two-stage process: initial coarse partitioning using √N centers followed by fine-grained binary splitting based on purity thresholds. The method iteratively divides the graph into granular-balls - connected subgraphs where nodes share homogeneous labels - using local shortest-path distances instead of global pairwise computations. This approach preserves spectral properties and label information while avoiding full distance matrix calculations. The final coarsened graph is constructed by projecting original nodes onto granular-balls, maintaining essential structural relationships while significantly reducing graph size for more efficient GNN training.

## Key Results
- Achieves accuracy comparable to original graph training while reducing graph size by up to 20×
- Outperforms existing methods (SCAL, CMGC, GCOND) in both accuracy and noise robustness
- Demonstrates O(N log N) time complexity through avoidance of full pairwise distance calculations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The purity threshold-based splitting ensures label consistency is preserved in coarsened graphs, improving classification accuracy.
- Mechanism: Nodes are iteratively split into granular-balls based on label homogeneity. The purity threshold T = 1 enforces that all nodes in a granular-ball share the same label before stopping the split.
- Core assumption: Maintaining homogeneous label clusters in the coarsened graph preserves discriminative power for downstream classification tasks.
- Evidence anchors:
  - [abstract] "Experimental results demonstrate that our method achieves accuracy comparable to training on the original graph."
  - [section] "High purity T in a granular-ball indicates strong label consistency, preserving or enhancing the discriminative properties of the original graph."
- Break condition: If the purity threshold is too low, heterogeneous label mixtures degrade model performance; if too high, over-splitting may occur and reduce coarsening efficiency.

### Mechanism 2
- Claim: Granular-ball computing avoids full pairwise distance computations, reducing time complexity while preserving local structural relationships.
- Mechanism: Each granular-ball is a connected subgraph centered on high-degree nodes; node assignments are based on shortest path distances within the subgraph, not global pairwise distances.
- Core assumption: Local shortest-path distances within connected subgraphs sufficiently capture structural similarity for coarsening without needing global computations.
- Evidence anchors:
  - [abstract] "SGBGC reduces time complexity by avoiding the need to calculate distances between all node pairs within the graph."
  - [section] "Our method has demonstrated superior noise resistance in robustness experiments, outperforming existing graph coarsening techniques."
- Break condition: If graph diameter is large, local shortest-path distances may not reflect global structure, potentially leading to poor coarsening.

### Mechanism 3
- Claim: Adaptive coarsening without predefined ratios allows the method to scale effectively to graphs of varying sizes and complexities.
- Mechanism: The number of initial granular-balls is set to √N, and splitting continues until purity thresholds are met, dynamically determining the final coarsening ratio.
- Core assumption: √N initial centers provide a good balance between granularity and computational cost across diverse graph sizes.
- Evidence anchors:
  - [abstract] "Our algorithm can adaptively perform splitting without requiring a predefined coarsening rate."
  - [section] "This adaptability enhances the flexibility of graph coarsening."
- Break condition: If the graph has highly irregular structure, √N may be suboptimal, leading to either excessive or insufficient coarsening.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their scalability challenges.
  - Why needed here: SGBGC is designed to enhance GNN scalability by reducing graph size while preserving key information.
  - Quick check question: What are the main computational bottlenecks in training GNNs on large graphs?

- Concept: Spectral graph theory and graph coarsening fundamentals.
  - Why needed here: Understanding how coarsening preserves spectral properties is key to grasping why SGBGC works.
  - Quick check question: How does preserving spectral properties in a coarsened graph affect GNN performance?

- Concept: Label purity and its role in classification tasks.
  - Why needed here: SGBGC uses label purity thresholds to guide the splitting of granular-balls, directly impacting classification accuracy.
  - Quick check question: Why is maintaining label homogeneity important in graph coarsening for classification?

## Architecture Onboarding

- Component map: Original graph G = (V, E, X, Y) -> Initial coarse partitioning -> Fine-grained binary splitting -> Granular-ball graph construction -> Coarsened graph ˜G = (˜V, ˜E, ˜X) and labels ˜Y -> GNN training on ˜G

- Critical path:
  1. Coarse partitioning (O(√N(N+M)) time)
  2. Fine-grained splitting until purity threshold met
  3. Granular-ball graph construction via projection matrix P
  4. GNN training on coarsened graph

- Design tradeoffs:
  - Purity threshold vs. coarsening ratio: Higher purity yields better accuracy but less reduction.
  - √N initial centers vs. granularity: Balances computational cost and representation quality.
  - Local shortest-path distances vs. global distances: Faster but may miss global structure.

- Failure signatures:
  - Accuracy drops sharply: Purity threshold too low or over-splitting.
  - Memory usage still high: Insufficient coarsening or graph too dense.
  - Training time increases: Too many granular-balls or inefficient splitting.

- First 3 experiments:
  1. Run SGBGC on Cora dataset with default parameters; verify graph size reduction and accuracy preservation.
  2. Vary purity threshold T; observe trade-off between coarsening ratio and classification accuracy.
  3. Compare SGBGC runtime and memory usage against SCAL and CMGC on a medium-sized graph (e.g., Citeseer).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the purity threshold T impact the trade-off between graph compression ratio and downstream task performance?
- Basis in paper: [explicit] The paper mentions that SGBGC uses a purity threshold T = 1 but does not explore how varying this threshold affects results
- Why unresolved: The paper only tests with T = 1 and doesn't provide ablation studies on different purity thresholds
- What evidence would resolve it: Systematic experiments varying T from 0.5 to 1.0 with corresponding performance and compression metrics

### Open Question 2
- Question: Can SGBGC be effectively extended to heterogeneous graphs with multiple edge types and node features?
- Basis in paper: [inferred] The paper only evaluates SGBGC on homogeneous citation networks without considering heterogeneous graph structures
- Why unresolved: The methodology is described for undirected graphs but doesn't address how to handle different edge types or heterogeneous node features
- What evidence would resolve it: Experiments on heterogeneous graph datasets like OGB-HET showing comparable performance to specialized heterogeneous GNN methods

### Open Question 3
- Question: What is the theoretical limit of graph compression achievable with SGBGC while maintaining acceptable performance?
- Basis in paper: [explicit] The paper mentions up to 20x compression but doesn't explore the theoretical limits or identify breaking points
- Why unresolved: The experiments stop at practical compression ratios without exploring extreme cases where graph structure might be too compressed to preserve information
- What evidence would resolve it: Experiments testing SGBGC at compression ratios of 50x, 100x, or higher to identify the point where accuracy significantly degrades

## Limitations
- The method's performance heavily depends on the purity threshold T=1, which may not generalize well to graphs with different label distributions or multi-label scenarios.
- The adaptive coarsening approach using √N initial centers is heuristic and may not be optimal for all graph structures.
- The robustness claims against label noise need more systematic evaluation across different noise levels and types.

## Confidence
- **High confidence**: The core mechanism of using purity-based granular-ball splitting for label preservation is sound and well-supported by experimental evidence.
- **Medium confidence**: The computational efficiency claims are supported by theoretical analysis but require more extensive empirical validation across diverse graph types and sizes.
- **Medium confidence**: The robustness claims against label noise are demonstrated but need more comprehensive testing with varying noise levels and distributions.

## Next Checks
1. **Purity threshold sensitivity**: Systematically evaluate SGBGC's performance across different purity thresholds (T ∈ [0.5, 1.0]) on multiple datasets to understand the trade-off between accuracy and coarsening ratio.
2. **Noise robustness evaluation**: Conduct comprehensive experiments with varying levels (5%, 10%, 20%) and types (random vs. targeted) of label noise to validate robustness claims.
3. **Scalability testing**: Test SGBGC on graphs with 1M+ nodes to validate the claimed O(N log N) complexity and identify potential bottlenecks in extremely large-scale scenarios.