---
ver: rpa2
title: 'Surveying the Landscape of Image Captioning Evaluation: A Comprehensive Taxonomy,
  Trends and Metrics Analysis'
arxiv_id: '2408.04909'
source_url: https://arxiv.org/abs/2408.04909
tags:
- image
- lexsim
- captioning
- conference
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work surveys over 70 image captioning evaluation metrics and
  their usage in hundreds of papers, providing the first comprehensive taxonomy and
  trend analysis. Despite the large number of metrics, most studies rely on only five
  dominant metrics (BLEU, METEOR, ROUGE, CIDEr, SPICE), which show only weak correlation
  with human ratings.
---

# Surveying the Landscape of Image Captioning Evaluation: A Comprehensive Taxonomy, Trends and Metrics Analysis

## Quick Facts
- arXiv ID: 2408.04909
- Source URL: https://arxiv.org/abs/2408.04909
- Reference count: 40
- Most studies rely on only five dominant metrics (BLEU, METEOR, ROUGE, CIDEr, SPICE), which show only weak correlation with human ratings

## Executive Summary
This comprehensive survey analyzes over 70 image captioning evaluation metrics and their usage across hundreds of papers. The study reveals that despite the abundance of metrics, research overwhelmingly relies on just five dominant metrics that poorly align with human judgment. The authors propose ENSEMBEVAL, a linear regression-based ensemble method that combines diverse metrics to achieve significantly better correlation with human ratings. The work provides the first taxonomy of human evaluation frameworks and analyzes usage patterns, revealing declining human evaluation and inadequate reporting of significance and inter-annotator agreement. The ensemble approach and open-source implementation enable future research on more sophisticated multi-aspect evaluation methods.

## Method Summary
The authors conducted a comprehensive survey of image captioning evaluation metrics by analyzing hundreds of papers and identifying over 70 distinct metrics. They performed correlation analysis between metric scores and human ratings across multiple datasets, then developed ENSEMBEVAL - a linear regression-based ensemble method that learns optimal weights for combining diverse metrics. The approach uses sequential feature selection with cross-validation to identify the most complementary subset of metrics, then trains on one human ratings dataset and evaluates generalization across five additional datasets. The study also created a taxonomy of human evaluation frameworks and analyzed usage patterns in the literature.

## Key Results
- Five dominant metrics (BLEU, METEOR, ROUGE, CIDEr, SPICE) show only weak correlation with human ratings despite their widespread use
- ENSEMBEVAL ensemble method trained on one human ratings dataset achieves improved correlation across five additional datasets
- Lesser-known metrics often correlate better with human ratings than the five dominant metrics
- Use of human evaluation in captioning research has declined significantly over time
- Most papers fail to report significance testing and inter-annotator agreement statistics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining multiple metrics improves correlation with human ratings compared to using any single metric
- Mechanism: Diverse metrics capture different aspects of caption quality (e.g., lexical similarity, semantic similarity, visual grounding). A weighted ensemble can balance these aspects to better match human judgment patterns
- Core assumption: Human ratings reflect a combination of multiple quality dimensions that individual metrics capture only partially
- Evidence anchors:
  - [abstract]: "combining a diverse set of metrics can enhance correlation with human ratings"
  - [section]: "we show that an ensemble of selected metrics...achieves enhanced correlation with human ratings"
  - [corpus]: Weak - only 25 related papers found, average FMR 0.494 suggests moderate relevance
- Break condition: If human ratings are based on a single dimension, or if metrics are highly redundant, ensemble benefit disappears

### Mechanism 2
- Claim: Lesser-known metrics often correlate better with human ratings than the five dominant metrics
- Mechanism: Dominant metrics (BLEU, METEOR, ROUGE, CIDEr, SPICE) focus primarily on lexical/semantic similarity to references, missing other important aspects like visual grounding or fluency. Newer metrics address these gaps
- Core assumption: The popularity of metrics does not correlate with their effectiveness in matching human judgment
- Evidence anchors:
  - [abstract]: "five dominant metrics...show only weak correlation with human ratings"
  - [section]: "popular metric groups have faced increasing criticism...while the recently popularized reference-free metrics have been challenged"
  - [corpus]: Weak - limited corpus coverage for this specific claim
- Break condition: If human ratings primarily value lexical/semantic similarity, or if newer metrics perform poorly

### Mechanism 3
- Claim: The ensemble approach generalizes across different human rating datasets
- Mechanism: Linear regression trained on one dataset learns coefficients that capture general patterns in how different metrics relate to human judgment, allowing application to new datasets
- Core assumption: Human rating patterns are consistent enough across datasets that learned weights transfer effectively
- Evidence anchors:
  - [section]: "trained on one human ratings dataset, achieves improved correlation across five additional datasets"
  - [section]: "the learned coefficients should generalize across datasets"
  - [corpus]: Weak - limited evidence about cross-dataset generalization in related work
- Break condition: If different datasets have fundamentally different rating criteria or scales

## Foundational Learning

- Concept: Feature selection algorithms for metric ensemble construction
  - Why needed here: With 20+ metrics available, selecting the most complementary subset is crucial for effective ensembling
  - Quick check question: What criterion does sequential feature selection use to add metrics to the ensemble?

- Concept: Correlation metrics (Kendall's tau, Pearson)
  - Why needed here: Different correlation measures are used for different datasets based on their rating scale properties
  - Quick check question: When would you use Kendall's tau vs Pearson correlation?

- Concept: Cross-validation for model selection
  - Why needed here: Determining the optimal stopping point for feature selection requires validation on held-out data
  - Quick check question: How does the epsilon parameter control feature selection termination?

## Architecture Onboarding

- Component map:
  - Human rating datasets -> metric computation -> correlation analysis -> feature selection -> linear regression ensemble -> evaluation on held-out datasets

- Critical path:
  1. Load and preprocess human rating datasets
  2. Compute metric scores for all candidates
  3. Perform feature selection to identify optimal metric subset
  4. Train linear regression model on selected metrics
  5. Evaluate ensemble performance on held-out datasets

- Design tradeoffs:
  - Simple linear ensemble vs. more complex models (better interpretability vs. potential performance)
  - Reference-based vs. reference-free metrics (coverage of different caption aspects)
  - Standardization of metric scores (ensures fair weighting)

- Failure signatures:
  - Poor cross-dataset generalization (weights don't transfer)
  - High redundancy among selected metrics (low diversity benefit)
  - Negative coefficients for otherwise good metrics (compensation for redundancy)

- First 3 experiments:
  1. Compare ensemble performance using all metrics vs. selected subset
  2. Test different feature selection stopping criteria (epsilon values)
  3. Evaluate ensemble with different correlation measures across datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do metrics that combine reference-based and reference-free approaches compare to existing ensemble methods in terms of correlation with human ratings?
- Basis in paper: [explicit] The authors propose ENSEMBEVAL as a linear regression-based ensemble method and demonstrate improved correlation, but suggest future research on "more sophisticated multi-aspect evaluation methods."
- Why unresolved: The paper only demonstrates a simple linear ensemble approach; more complex combinations of reference-based and reference-free metrics remain unexplored
- What evidence would resolve it: Empirical comparison of correlation with human ratings between ENSEMBEVAL and novel ensemble methods incorporating both metric types

### Open Question 2
- Question: Does the performance gap between widely-used metrics and lesser-known metrics persist across different languages and cultural contexts?
- Basis in paper: [inferred] The authors note that speakers of different languages "tend to describe different entities" and reference cross-linguistic studies, suggesting evaluation metrics may have language-specific limitations
- Why unresolved: The paper's experiments focus exclusively on English datasets, leaving multilingual generalization unexamined
- What evidence would resolve it: Correlation analysis between metric scores and human ratings across diverse language datasets

### Open Question 3
- Question: What is the optimal balance between diversity and redundancy when selecting metrics for ensemble evaluation?
- Basis in paper: [explicit] The authors use feature selection to identify diverse metrics but acknowledge "strong correlations among certain metrics" and assign negative coefficients to mitigate redundancy
- Why unresolved: The paper uses a heuristic approach to diversity selection but doesn't explore systematic trade-offs between metric diversity and ensemble performance
- What evidence would resolve it: Controlled experiments varying the diversity-redundancy balance and measuring resulting correlation with human ratings

## Limitations

- The ensemble approach's cross-dataset generalization remains uncertain, as learned weights may not fully represent the diversity of human rating patterns across all captioning tasks
- The linear ensemble assumes additive contributions from metrics, potentially missing complex interactions between different evaluation dimensions
- The study focuses on correlation with human ratings but doesn't address whether improved correlation translates to better model selection or training signal

## Confidence

- **High confidence**: The observation that dominant metrics show weak correlation with human ratings is well-supported by the comprehensive analysis of hundreds of papers
- **Medium confidence**: The ensemble method's cross-dataset generalization is supported but requires broader validation across more diverse datasets and tasks
- **Medium confidence**: The taxonomy of human evaluation frameworks provides valuable insights, though usage pattern analysis may be limited by incomplete reporting in papers

## Next Checks

1. Test ensemble generalization on specialized captioning tasks (medical imaging, multimodal reasoning) not represented in the current human rating datasets
2. Evaluate whether ensemble-based evaluation leads to improved model training outcomes, not just better correlation scores
3. Compare linear ensemble performance against non-linear methods (e.g., neural networks) to assess if the simple approach is sufficient or if more complex models are warranted