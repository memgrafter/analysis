---
ver: rpa2
title: 'Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse
  Models'
arxiv_id: '2404.18796'
source_url: https://arxiv.org/abs/2404.18796
tags:
- answer
- question
- provided
- reference
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accurately evaluating large
  language model (LLM) generations, which is difficult due to the complexity of assessing
  free-form outputs. Current evaluations often rely on using a single large model
  like GPT-4 as a judge, which can be costly, introduce intra-model bias, and may
  be unnecessary.
---

# Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models

## Quick Facts
- arXiv ID: 2404.18796
- Source URL: https://arxiv.org/abs/2404.18796
- Reference count: 21
- Primary result: A Panel of LLM Evaluators (PoLL) outperforms single large model judging with higher correlation to human judgments and 7x lower cost

## Executive Summary
This paper addresses the challenge of accurately evaluating large language model (LLM) generations by proposing a novel approach that uses a Panel of LLM Evaluators (PoLL) instead of a single large model like GPT-4. The authors demonstrate that a diverse panel of smaller models from different families can achieve better correlation with human judgments while being significantly more cost-effective. Through extensive experiments across six datasets and three judge settings, PoLL shows higher Cohen's kappa scores and better rank correlation with human preferences compared to single large model judging. The approach reduces intra-model bias and provides more consistent evaluations, making it a practical alternative for LLM evaluation tasks.

## Method Summary
The method constructs a Panel of LLM Evaluators (PoLL) by combining multiple smaller models from different families to judge LLM outputs instead of using a single large model. For each evaluation task, different LLM models generate outputs which are then scored independently by each panel member. The individual scores are aggregated using a voting function (maximum for binary decisions like QA, average for graded evaluations like Chatbot Arena). The approach is tested across six datasets including Natural Questions, TriviaQA, HotpotQA, and Chatbot Arena Hard, comparing PoLL's performance against individual judges and human annotations using correlation metrics like Cohen's kappa and Kendall Tau.

## Key Results
- PoLL achieves higher Cohen's kappa scores than single large judges (0.763 vs 0.627 on Natural Questions)
- PoLL shows better rank correlation with human preferences on Chatbot Arena Hard compared to individual judges
- The approach is over 7x less expensive than using a single large model judge while maintaining or improving evaluation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A panel of diverse LLM evaluators reduces intra-model bias compared to a single large model judge
- Mechanism: Different LLM families have different training data and architectural biases. When multiple models from disjoint families vote on correctness, individual model biases cancel out, leading to more consistent and less biased judgments
- Core assumption: Models from different families have sufficiently different biases that averaging their judgments reduces overall bias
- Evidence anchors:
  - [abstract] "exhibits less intra-model bias due to its composition of disjoint model families"
  - [section 4.4] "Intra-model scoring bias is reduced by pooling judgements across a panel of heterogeneous evaluator models"
  - [corpus] Weak - no direct corpus evidence for bias cancellation mechanism

### Mechanism 2
- Claim: Smaller models in a panel can outperform a single large model judge on certain tasks
- Mechanism: For tasks requiring pattern matching or fuzzy string comparison, smaller models may avoid "overthinking" and simply perform the required matching operation, while larger models may over-analyze and introduce noise
- Core assumption: Larger models don't always provide better judgments for simpler tasks
- Evidence anchors:
  - [section 4.3] "GPT-4 is a relatively weak judge, exhibiting high variance with minor changes to the prompt" and optimized prompt improved performance
  - [table 3] GPT-4 with optimized prompt achieves 0.725 kappa vs 0.627 with standard prompt
  - [corpus] Weak - no direct corpus evidence for "overthinking" hypothesis

### Mechanism 3
- Claim: Pooling judgments across multiple models provides more stable and consistent evaluations than single-model judging
- Mechanism: Individual model judgments have variance; averaging across multiple independent judgments reduces overall variance and increases reliability
- Core assumption: Individual model judgments are sufficiently independent and their errors are uncorrelated
- Evidence anchors:
  - [section 4.4] "PoLL has the smallest spread in scores, with a standard deviation of 2.2, compared to EM and individual judges"
  - [section 4.2] PoLL achieves higher correlation (0.917 Pearson) than individual judges
  - [corpus] Weak - no direct corpus evidence for variance reduction mechanism

## Foundational Learning

- Concept: Cohen's kappa statistic
  - Why needed here: Used to measure inter-rater reliability between LLM judges and human annotators
  - Quick check question: What does a Cohen's kappa value of 0.8 indicate about agreement between two raters?

- Concept: Kendall Tau and Pearson correlation
  - Why needed here: Used to measure rank correlation between LLM judge rankings and human preference rankings
  - Quick check question: How does Kendall Tau differ from Pearson correlation in measuring rank agreement?

- Concept: Fuzzy string matching vs exact string matching
  - Why needed here: Understanding the difference is crucial for designing effective judge prompts and evaluating model outputs
  - Quick check question: Why might fuzzy string matching be more appropriate than exact matching for evaluating LLM-generated answers?

## Architecture Onboarding

- Component map: Question -> Retrieved evidence snippets -> Reference answer -> Model generation -> Panel of evaluators -> Voting mechanism -> Final judgment score

- Critical path:
  1. Retrieve evidence snippets for question
  2. Generate answers from test models
  3. Panel of evaluators independently score each answer
  4. Aggregate scores through voting function
  5. Compare to human judgments for evaluation

- Design tradeoffs:
  - Model diversity vs. computational cost
  - Voting mechanism choice (max vs average)
  - Prompt engineering for different judge models
  - Balancing between bias reduction and accuracy

- Failure signatures:
  - High variance in panel judgments indicates model disagreement
  - Systematic bias toward certain answer patterns
  - Poor correlation with human judgments despite high agreement within panel

- First 3 experiments:
  1. Compare single judge (GPT-4) vs PoLL on KILT-NQ dataset
  2. Test different voting mechanisms (max vs average) on multi-hop QA
  3. Vary panel composition (different combinations of 3 models) and measure correlation with human judgments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PoLL compare to single large judges in settings beyond QA and Chatbot Arena, such as math or reasoning tasks?
- Basis in paper: Inferred
- Why unresolved: The paper states that further work is needed to see how broadly applicable PoLL is, for example, in math or reasoning evaluations, where language models often struggle.
- What evidence would resolve it: Experiments comparing PoLL to single large judges on math or reasoning datasets, showing performance differences and potential benefits or limitations of PoLL in these settings.

### Open Question 2
- Question: What is the optimal composition of a PoLL in terms of the number and types of models to include, considering both quality and cost?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that the task of 'panel selection', or identifying the best models to include in PoLL in terms of quality and cost, is left to future work.
- What evidence would resolve it: Experiments varying the number and types of models in PoLL, measuring performance and cost, to determine the optimal composition that balances quality and cost-effectiveness.

### Open Question 3
- Question: How sensitive is PoLL's performance to the voting function used for aggregating scores across the judges?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that they consider two different voting functions (max and average pooling) but does not explore the sensitivity of PoLL's performance to these or other potential voting functions.
- What evidence would resolve it: Experiments comparing PoLL's performance using different voting functions (e.g., max, average, median, weighted) across various tasks and datasets, to determine the impact of the voting function on overall performance.

## Limitations
- The intra-model bias analysis relies heavily on synthetic KILT datasets rather than real-world evaluation scenarios
- The variance reduction mechanism assumes independence between model judgments which may not hold if models share similar training data
- The "overthinking" hypothesis for why larger models perform poorly lacks direct evidence and remains speculative

## Confidence

**High Confidence**: The correlation improvements with human judgments are well-supported by the data across multiple datasets and evaluation metrics.

**Medium Confidence**: The intra-model bias reduction mechanism is plausible based on experimental results, but the synthetic nature of KILT datasets limits generalizability.

**Low Confidence**: The "overthinking" hypothesis for why larger models perform poorly on certain tasks lacks direct evidence and remains speculative.

## Next Checks

1. **Real-world Bias Analysis**: Test PoLL on human-evaluated datasets from diverse domains (not just synthetic KILT data) to verify intra-model bias reduction in practical scenarios.

2. **Model Independence Validation**: Conduct correlation analysis between individual model judgments in PoLL to quantify the independence assumption underlying the variance reduction mechanism.

3. **Cost Structure Sensitivity**: Replicate the cost analysis using different pricing models and cloud providers to verify the claimed 7x cost reduction holds across various deployment scenarios.