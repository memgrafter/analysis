---
ver: rpa2
title: A Short Note on Evaluating RepNet for Temporal Repetition Counting in Videos
arxiv_id: '2411.08878'
source_url: https://arxiv.org/abs/2411.08878
tags:
- repnet
- counting
- repetition
- videos
- transrac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses widespread confusion and inconsistent evaluation
  of the RepNet model for temporal repetition counting in videos across multiple papers.
  The authors clarify that a modified version of RepNet was incorrectly reported in
  prior work, leading to poor performance results.
---

# A Short Note on Evaluating RepNet for Temporal Repetition Counting in Videos

## Quick Facts
- arXiv ID: 2411.08878
- Source URL: https://arxiv.org/abs/2411.08878
- Reference count: 12
- Key outcome: RepNet with multi-speed evaluation achieves strong performance across multiple datasets (MAE 0.3083, OBOA 0.7047 on Countix)

## Executive Summary
This paper addresses widespread confusion and inconsistent evaluation of the RepNet model for temporal repetition counting in videos across multiple papers. The authors clarify that a modified version of RepNet was incorrectly reported in prior work, leading to poor performance results. They demonstrate that the original RepNet model, using multi-speed evaluation (playing videos at 1x, 2x, 3x, 4x, and 5x speeds), achieves strong performance across multiple datasets. The authors release evaluation code and checkpoints to standardize RepNet evaluation and highlight that a 2020 model with ResNet-50 backbone still performs competitively against more recent methods.

## Method Summary
The authors evaluate the original RepNet model with ResNet-50 backbone at 112×112 resolution on three datasets: Countix, UCFRep, and RepCount-A. They implement multi-speed evaluation by processing each video at 1×, 2×, 3×, 4×, and 5× speeds, selecting the highest scoring prediction. The evaluation uses both per-frame periodicity scores and period length predictions with a threshold of τ = 0.5 for RepCount-A. The authors provide checkpoints and evaluation code on GitHub to standardize RepNet evaluation across the research community.

## Key Results
- On Countix dataset: MAE of 0.3083 and OBOA of 0.7047
- On UCFRep dataset: MAE of 0.2088 and OBOA of 0.7333
- On RepCount-A dataset: MAE of 0.3308 and OBOA of 0.5329
- RepNet outperforms more recent methods despite being from 2020

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The original RepNet model can handle videos with more than 32 action periods through multi-speed evaluation without requiring architectural modifications.
- Mechanism: By playing videos at different speeds (1×, 2×, 3×, 4×, and 5×), the model effectively increases the maximum period length it can predict from 32 to 160 frames. This allows the same ResNet-50 backbone to process longer repetition cycles by effectively "stretching" the temporal context.
- Core assumption: The model's temporal receptive field scales proportionally with the playback speed while maintaining feature quality.
- Evidence anchors:
  - [section]: "This is achieved by playing the video at different speeds, rather than modifying the model. This technique is described in the original RepNet paper [1] as Multi-speed Evaluation (Section 3.5)."
  - [abstract]: "They demonstrate that the original RepNet model, using multi-speed evaluation (playing videos at 1x, 2x, 3x, 4x, and 5x speeds), achieves strong performance"

### Mechanism 2
- Claim: The original RepNet model performs competitively against more recent methods despite being from 2020.
- Mechanism: RepNet's architecture (ResNet-50 backbone at 112×112 resolution) combined with its multi-speed evaluation technique provides sufficient representational capacity for temporal repetition counting tasks across diverse datasets.
- Core assumption: The fundamental problem of temporal repetition counting hasn't evolved to require significantly more complex architectures since 2020.
- Evidence anchors:
  - [section]: "We find that RepNet outperforms more modern methods such as [8]. Please note that we are reporting Off-by-One Accuracy here as opposed to Off-by-One Error in the original RepNet paper as most papers report accuracy on these benchmarks now."
  - [section]: "We find that RepNet still performs well for videos with gaps in repetitions. This is contrary to what has been reported in the TransRAC paper"

### Mechanism 3
- Claim: Using both periodicity predictions and period length prediction scores provides more accurate frame-level repetition detection in videos with gaps.
- Mechanism: The model combines two complementary signals - per-frame periodicity (pi) and period length confidence (li) - to determine whether a frame should be counted as repeating. This dual-signal approach is more robust than using either signal alone.
- Core assumption: Periodicity and period length predictions capture different aspects of the repetition structure, and their combination provides better discrimination.
- Evidence anchors:
  - [section]: "We found using both periodicity predictions pi and period length prediction scores li to give a more accurate measure of whether a frame should be considered repeating or not."
  - [section]: "To evaluate in this setting, we use both the per-frame periodicity score and confidence of predicted period to determine whether a particular frame is to be counted as repeating or not."

## Foundational Learning

- Concept: Temporal repetition counting in videos
  - Why needed here: The paper evaluates a model for counting repeated actions in video sequences, which requires understanding temporal patterns and periodicity
  - Quick check question: What is the difference between counting repetitions in continuous vs. gapped sequences?

- Concept: Multi-speed evaluation technique
  - Why needed here: This is the key mechanism that allows RepNet to handle longer periods without architectural changes
  - Quick check question: How does playing a video at 5× speed effectively allow the model to detect 160-frame periods when its native capacity is 32 frames?

- Concept: Off-by-One Accuracy (OBOA) and Mean Absolute Error (MAE) metrics
  - Why needed here: These are the evaluation metrics used to compare RepNet performance against other methods
  - Quick check question: Why does the paper mention that α = 0.1 is used in some implementations but α = 0 in the original RepNet paper?

## Architecture Onboarding

- Component map: Video input → Frame sampling at multiple speeds → Feature extraction → Periodicity/period length prediction → Aggregation across speeds → Final count prediction
- Critical path: Video input → Frame sampling at multiple speeds → Feature extraction → Periodicity/period length prediction → Aggregation across speeds → Final count prediction
- Design tradeoffs: Uses relatively simple ResNet-50 backbone (lower computational cost) vs. more complex modern architectures, trades model complexity for evaluation strategy sophistication
- Failure signatures: Poor performance on datasets with long periods (if multi-speed evaluation is not properly implemented), incorrect counts when periodicity and period length signals conflict, degradation when videos have significant gaps in repetitions
- First 3 experiments:
  1. Run RepNet on a simple continuous repetition video at 1× speed and verify period length prediction
  2. Test multi-speed evaluation by running the same video at 2×, 3×, 4×, and 5× speeds and compare period length predictions
  3. Evaluate RepNet on a gapped repetition video and verify the combined periodicity/period length counting mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much does multi-speed evaluation contribute to RepNet's performance compared to architectural modifications for handling longer repetition periods?
- Basis in paper: [explicit] The paper demonstrates that multi-speed evaluation (playing videos at 1x, 2x, 3x, 4x, and 5x speeds) allows RepNet to handle periods up to 160 frames without modifying the model architecture, yet many papers modified the model instead
- Why unresolved: The paper doesn't provide ablation studies comparing multi-speed evaluation against architectural modifications, nor does it quantify the exact performance gain from multi-speed evaluation alone
- What evidence would resolve it: Controlled experiments showing RepNet performance with and without multi-speed evaluation, and comparison against models with architectural modifications but without speed augmentation

### Open Question 2
- Question: Why does RepNet maintain competitive performance despite using a smaller ResNet-50 backbone and lower resolution (112×112) compared to newer methods?
- Basis in paper: [explicit] The paper notes that "a 2020 model trained with ResNet-50 backbone evaluated at 112 × 112 resolution still has strong performance" compared to modern methods using larger backbones and higher resolutions
- Why unresolved: The paper doesn't investigate the specific architectural or training factors that contribute to RepNet's efficiency and effectiveness, nor does it explain why architectural complexity hasn't translated to proportional performance gains
- What evidence would resolve it: Detailed architectural analysis comparing feature extraction efficiency, ablation studies with different backbone sizes and resolutions, and investigation of what specific design choices enable RepNet's efficiency

### Open Question 3
- Question: How robust is RepNet's performance across different types of repetitive actions and real-world conditions?
- Basis in paper: [explicit] The paper evaluates RepNet on three datasets (Countix, UCFRep, RepCount-A) with varying characteristics including videos with gaps in repetitions, but doesn't systematically analyze performance across action types or environmental conditions
- Why unresolved: The evaluation doesn't break down performance by action category, environmental conditions, or video quality, leaving questions about generalization across different repetitive actions
- What evidence would resolve it: Detailed per-category performance analysis, experiments with videos containing occlusions or varying lighting conditions, and systematic testing across diverse repetitive action types

### Open Question 4
- Question: What is the optimal threshold (τ) for determining repeating frames in videos with gaps, and how sensitive is RepNet's performance to this parameter?
- Basis in paper: [explicit] The paper uses τ = 0.5 for RepCount-A dataset but notes "We found using both periodicity predictions pi and period length prediction scores li to give a more accurate measure" without exploring threshold sensitivity
- Why unresolved: The paper doesn't provide threshold sensitivity analysis or optimization, leaving questions about whether the chosen threshold is optimal for different datasets or conditions
- What evidence would resolve it: Comprehensive threshold sensitivity analysis showing performance across different τ values, comparison of different threshold selection strategies, and investigation of whether optimal thresholds vary by dataset or action type

## Limitations
- Limited ablation studies on why the modified RepNet architecture performed poorly in prior work
- The evaluation methodology assumes perfect video speed manipulation, which may not be feasible for all video sources
- Relies on pre-existing RepNet checkpoints without providing details about their training process or hyperparameters

## Confidence
- High confidence in the claim that RepNet with multi-speed evaluation performs well on Countix, UCFRep, and RepCount-A
- Medium confidence in the claim that the original RepNet outperforms more recent methods
- Low confidence in the explanation of why the modified RepNet architecture performed poorly without more detailed architectural analysis

## Next Checks
1. Replicate the evaluation on Countix using the released code and checkpoints to verify the reported MAE of 0.3083 and OBOA of 0.7047
2. Test RepNet's performance on a new dataset with significantly longer repetition periods (>160 frames) to verify the limits of multi-speed evaluation
3. Implement and compare the exact modified RepNet architecture from TransRAC to understand the specific changes that degraded performance