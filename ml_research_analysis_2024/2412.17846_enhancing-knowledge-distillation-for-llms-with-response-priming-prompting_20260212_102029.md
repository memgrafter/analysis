---
ver: rpa2
title: Enhancing Knowledge Distillation for LLMs with Response-Priming Prompting
arxiv_id: '2412.17846'
source_url: https://arxiv.org/abs/2412.17846
tags:
- teacher
- prompting
- student
- eggs
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces response-priming prompting strategies for
  knowledge distillation to improve the performance of student language models. The
  method fine-tunes a Llama 3.1 8B model by distilling knowledge from a quantized
  Llama 3.1 405B teacher model using novel prompting strategies including Teacher
  prompting, Ground Truth prompting, and Confidence prompting, along with LoRA optimization.
---

# Enhancing Knowledge Distillation for LLMs with Response-Piming Prompting

## Quick Facts
- arXiv ID: 2412.17846
- Source URL: https://arxiv.org/abs/2412.17846
- Reference count: 10
- Key outcome: Ground Truth prompting yields 55% accuracy improvement on GSM8K for Llama 3.1 8B student models

## Executive Summary
This paper introduces response-priming prompting strategies for knowledge distillation to improve student language model performance. The method fine-tunes a Llama 3.1 8B model by distilling knowledge from a quantized Llama 3.1 405B teacher model using novel prompting strategies including Teacher prompting, Ground Truth prompting, and Confidence prompting, along with LoRA optimization. Evaluated on GSM8K, the approach shows that Ground Truth prompting yields a 55% accuracy improvement over unprompted distillation, with student models exhibiting better reasoning behaviors and more focused self-attention patterns in deeper layers.

## Method Summary
The method involves generating teacher responses using three prompting strategies: Teacher prompting (teacher acts as an actual teacher), Ground Truth prompting (teacher told it's generating training data), and Confidence prompting (teacher double-checks answers). These responses are used to create a transfer set for knowledge distillation. The student model (Llama 3.1 8B) is fine-tuned using LoRA with combined loss function combining reverse KL divergence (soft loss) and cross-entropy (hard loss), optimized using Bayesian optimization for α=0.61 and temperature=5.9 over 2 epochs.

## Key Results
- Ground Truth prompting yields 55% accuracy improvement on GSM8K compared to unprompted distillation
- Student models show better reasoning behaviors and more focused self-attention patterns in deeper layers
- All prompting strategies outperform unprompted distillation baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ground Truth prompting improves student model accuracy by 55% over unprompted distillation
- Mechanism: Teacher model produces more structured reasoning steps when told it's generating training data
- Core assumption: Teacher model's reasoning can be shaped through personification prompts
- Evidence anchors: Abstract and section describing Ground Truth prompting approach

### Mechanism 2
- Claim: Teacher prompting improves student reasoning through explicit step-by-step explanation
- Mechanism: Teacher model explains reasoning process clearly when prompted to teach
- Core assumption: LLMs can produce structured reasoning outputs when explicitly prompted to teach
- Evidence anchors: Abstract and section on Teacher prompting strategy

### Mechanism 3
- Claim: Confidence prompting increases student confidence in correct predictions through mode-seeking behavior
- Mechanism: Teacher double-checking creates peaked distributions encouraging student confidence
- Core assumption: Reverse KL divergence's mode-seeking property can be leveraged through teacher behavior
- Evidence anchors: Section on Confidence prompting and reverse KL divergence

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: Entire approach relies on transferring knowledge from large teacher to smaller student
  - Quick check question: What is the primary difference between knowledge distillation and standard supervised learning?

- Concept: Prompt engineering
  - Why needed here: Method manipulates how teacher model generates responses through prompting strategies
  - Quick check question: How does chain-of-thought prompting differ from standard prompting in terms of model output?

- Concept: Self-attention mechanisms
  - Why needed here: Analysis of attention patterns is crucial for understanding why prompting strategies work
  - Quick check question: What does high entropy in self-attention distributions indicate about a model's understanding of input sequences?

## Architecture Onboarding

- Component map: Quantized Llama 3.1 405B Instruct (teacher) -> Llama 3.1 8B Instruct (student) -> LoRA fine-tuning module -> GSM8K evaluation

- Critical path: 1) Generate teacher responses with prompting strategies 2) Create transfer set from teacher logits 3) Apply LoRA fine-tuning to student model 4) Evaluate student performance on GSM8K

- Design tradeoffs: Using quantized teacher model (6-bit vs 16-bit), 2 epochs of fine-tuning, zero-shot evaluation, 8-bit LoRA weights vs full fine-tuning

- Failure signatures: Student performance plateaus below baseline, attention entropy remains high in final layers, self-attention focus doesn't decrease in later layers, attention similarity remains low throughout layers

- First 3 experiments: 1) Compare unprompted distillation vs. Ground Truth prompting on small GSM8K subset 2) Test different temperature values (1.0, 5.9, 11.0) 3) Evaluate different LoRA rank values (2, 4, 8)

## Open Questions the Paper Calls Out
1. How generalizable are response-priming prompting strategies across different domains and reasoning tasks beyond GSM8K?
2. What is the relationship between self-attention patterns and reasoning quality, and can this relationship be used to predict student model performance?
3. How should hyper-parameters like α and temperature be optimally selected for knowledge distillation with different teacher-student model pairs?
4. What are the risks and limitations of using LLM outputs in knowledge distillation, particularly regarding propagation of biases or errors?

## Limitations
- Exact prompt templates for response-priming strategies are not provided, making faithful reproduction difficult
- Evaluation scope is narrow, testing only on GSM8K benchmark without examining generalization to other reasoning tasks
- Attention analysis is somewhat qualitative, showing patterns but not establishing causal relationships

## Confidence
- High Confidence: Ground Truth prompting yields 55% accuracy improvement on GSM8K
- Medium Confidence: Response-priming prompting strategies produce better reasoning behaviors and more focused self-attention patterns
- Low Confidence: Confidence prompting increases student model confidence in correct predictions through mode-seeking behavior

## Next Checks
1. Conduct ablation study on prompting strategies to determine relative contributions of each strategy versus LoRA optimization
2. Evaluate distilled student models on additional reasoning benchmarks beyond GSM8K to assess generalization
3. Perform rigorous quantitative analysis of attention distributions with statistical tests to establish causal relationships between attention patterns and performance improvements