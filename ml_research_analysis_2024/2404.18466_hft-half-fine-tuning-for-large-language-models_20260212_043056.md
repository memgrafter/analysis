---
ver: rpa2
title: 'HFT: Half Fine-Tuning for Large Language Models'
arxiv_id: '2404.18466'
source_url: https://arxiv.org/abs/2404.18466
tags:
- fine-tuning
- parameters
- llama
- knowledge
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Half Fine-Tuning (HFT) to address catastrophic
  forgetting in large language models (LLMs) during sequential fine-tuning. HFT randomly
  freezes half of the model parameters during each fine-tuning stage while updating
  the other half, allowing the model to retain previously learned knowledge while
  adapting to new tasks.
---

# HFT: Half Fine-Tuning for Large Language Models

## Quick Facts
- arXiv ID: 2404.18466
- Source URL: https://arxiv.org/abs/2404.18466
- Reference count: 40
- Primary result: HFT randomly freezes half of model parameters during fine-tuning to mitigate catastrophic forgetting while achieving 30% training time reduction

## Executive Summary
This paper introduces Half Fine-Tuning (HFT), a simple yet effective approach to address catastrophic forgetting in large language models during sequential fine-tuning. HFT randomly freezes half of the model parameters during each fine-tuning stage while updating the other half, allowing the model to retain previously learned knowledge while adapting to new tasks. The approach is evaluated across supervised fine-tuning, direct preference optimization, and continual learning settings, demonstrating significant improvements in mitigating forgetting while maintaining or enhancing performance on downstream tasks.

## Method Summary
HFT implements a parameter-level mask that randomly selects half of the parameters in each transformer layer category (self-attention, feed-forward, layer normalization) to freeze during training, while updating the remaining half. This approach is mathematically equivalent to optimizing with an L2 regularization term that penalizes changes to frozen parameters. The method is evaluated on LLAMA 2-7B/13B models across multiple fine-tuning scenarios including standard supervised fine-tuning on TÃœLU V2, direct preference optimization on UltraFeedback, and continual learning on TRACE.

## Key Results
- HFT achieves comparable or better performance than full fine-tuning on general abilities benchmarks (MMLU, GSM8K, BBH, TyDiQA, TruthfulQA, HumanEval, AlpacaEval 2.0)
- HFT preserves basic knowledge significantly better than full fine-tuning, with large improvements on NaturalQuestion, TriviaQA, and HotpotQA
- HFT reduces training time by approximately 30% in standard supervised fine-tuning while maintaining performance
- In continual learning settings, HFT shows improvements in both Overall Performance (OP) and Backward Transfer (BWT) metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HFT works by applying a parameter-level mask that regularizes the fine-tuning process, preventing extreme deviations from pre-trained parameters.
- Mechanism: HFT selects half of the parameters to update while freezing the other half. This is mathematically equivalent to optimizing FFT loss with an L2 regularization term that penalizes changes to the frozen parameters.
- Core assumption: The regularization from parameter masking stabilizes the sparse fine-tuned model and prevents catastrophic forgetting.
- Evidence anchors:
  - [abstract]: "We provide a feasibility analysis from the perspective of optimization and interpret the parameter selection operation as a regularization term."
  - [section 3]: "HFT could be regarded as exerting a parameter-level mask to vanilla FFT... solving the constrained optimization problem is equivalent to solving the following unconstrained optimization problem"
  - [corpus]: Weak - no direct corpus evidence found for the optimization-based regularization mechanism
- Break condition: If the regularization term becomes too strong, it could prevent effective learning of new tasks. If the parameter selection becomes correlated with task-specific parameters, the freezing mechanism loses its protective effect.

### Mechanism 2
- Claim: HFT preserves basic knowledge by keeping a portion of pre-trained parameters frozen during fine-tuning.
- Mechanism: By freezing half of the parameters in each training round, HFT maintains the knowledge stored in those parameters while allowing the other half to adapt to new tasks. The frozen parameters act as "anchors" to the pre-trained knowledge.
- Core assumption: The frozen parameters contain critical knowledge that would otherwise be overwritten during full fine-tuning.
- Evidence anchors:
  - [abstract]: "where half of the parameters are selected to learn new tasks while the other half are frozen to remain previous knowledge."
  - [section 2]: "When selectively restoring half parameters to the pre-trained LLAMA 2-7 B model... we witness the remarkable recovery of basic knowledge."
  - [corpus]: Weak - limited corpus evidence for the specific knowledge preservation mechanism
- Break condition: If the frozen parameters are randomly selected without regard to their importance, the knowledge preservation effect may be minimal. If the new task requires modification of the frozen parameters, performance will suffer.

### Mechanism 3
- Claim: HFT achieves efficiency gains by reducing the number of trainable parameters by approximately 50%.
- Mechanism: By only updating half of the parameters during each fine-tuning stage, HFT reduces computational load and training time while maintaining performance.
- Core assumption: The performance degradation from halving trainable parameters is offset by the regularization benefits and knowledge preservation.
- Evidence anchors:
  - [abstract]: "HFT achieves comparable or better performance than full fine-tuning with approximately 30% reduction in training time"
  - [section 4.3]: "HFT could shorten the training time by 30% in standard SFT"
  - [corpus]: Weak - no direct corpus evidence for the efficiency gains
- Break condition: If the model requires more than 50% of parameters to be updated for effective learning, performance will degrade significantly. If hardware or software optimizations don't account for the reduced parameter count, efficiency gains may be minimal.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why traditional fine-tuning causes loss of previously learned knowledge is essential to appreciate HFT's solution
  - Quick check question: What happens to pre-trained knowledge when all parameters are updated during fine-tuning?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: HFT is a form of parameter-efficient fine-tuning that freezes some parameters while updating others
  - Quick check question: How does freezing parameters during fine-tuning differ from methods like LoRA or adapters?

- Concept: Optimization with constraints
  - Why needed here: HFT's mechanism can be understood through constrained optimization theory, where parameter selection acts as a constraint
  - Quick check question: How does adding a constraint to an optimization problem affect the solution space and stability?

## Architecture Onboarding

- Component map:
  Transformer layers with three blocks: self-attention -> feed-forward -> layer normalization
  Embedding and lm_head layers (typically not frozen)
  Parameter selection mechanism at category-level (matrices within each block)
  Training loop that alternates between frozen and trainable parameter sets

- Critical path:
  1. Load pre-trained model and identify parameter categories
  2. For each fine-tuning round, randomly select half of parameters per category to freeze
  3. Train only on unfrozen parameters while maintaining frozen parameters
  4. Evaluate performance on both new tasks and basic knowledge preservation

- Design tradeoffs:
  - Freezing more parameters increases knowledge preservation but may limit learning capacity
  - Freezing fewer parameters increases learning flexibility but risks catastrophic forgetting
  - Category-level selection vs layer-level vs model-level selection affects granularity and performance
  - Embedding and lm_head updates are typically preserved for knowledge-intensive tasks

- Failure signatures:
  - Performance degradation on new tasks indicates insufficient parameter updates
  - Loss of basic knowledge indicates frozen parameters aren't effectively preserving knowledge
  - Training instability suggests poor parameter selection or learning rate issues
  - Efficiency gains not materializing suggests implementation overhead or suboptimal hardware utilization

- First 3 experiments:
  1. Compare HFT vs full fine-tuning on a single task to verify performance maintenance
  2. Test different parameter selection ratios (25%, 50%, 75%) to find optimal balance
  3. Implement sequential fine-tuning with HFT to validate catastrophic forgetting mitigation

## Open Questions the Paper Calls Out
None

## Limitations
- Optimization interpretation: The mathematical equivalence between HFT and L2 regularization is not rigorously proven
- Parameter selection strategy: Random selection is used but optimal strategy remains unexplored
- Generalizability: All experiments use LLAMA 2 models, effectiveness on other architectures is unknown

## Confidence
**High Confidence**: HFT reduces trainable parameters by approximately 50%; HFT achieves 30% training time reduction in standard SFT; HFT shows improvements in continual learning settings (OP and BWT metrics)

**Medium Confidence**: HFT prevents catastrophic forgetting during sequential fine-tuning; HFT maintains or improves performance on general abilities benchmarks; Parameter selection at category-level is more effective than layer-level or model-level

**Low Confidence**: The mathematical equivalence between HFT and L2 regularization; Optimal parameter selection ratio is exactly 50%; HFT's effectiveness generalizes to all types of fine-tuning tasks

## Next Checks
1. **Ablation Study on Parameter Selection Strategy**: Compare random selection against structured approaches (e.g., freezing task-irrelevant parameters, freezing lower-layer parameters) to determine if the random selection is optimal.

2. **Cross-Architecture Validation**: Implement HFT on GPT-style models and smaller language models to test generalizability beyond LLAMA 2.

3. **Long-Term Sequential Fine-Tuning**: Conduct experiments with more than two sequential fine-tuning stages to evaluate whether HFT's effectiveness degrades over multiple rounds of fine-tuning.