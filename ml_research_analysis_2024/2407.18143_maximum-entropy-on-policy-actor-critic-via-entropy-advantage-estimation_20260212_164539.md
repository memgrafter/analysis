---
ver: rpa2
title: Maximum Entropy On-Policy Actor-Critic via Entropy Advantage Estimation
arxiv_id: '2407.18143'
source_url: https://arxiv.org/abs/2407.18143
tags:
- entropy
- policy
- eapo
- advantage
- maxent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method called Entropy Advantage Policy Optimisation
  (EAPO) for maximum entropy reinforcement learning in on-policy actor-critic settings.
  The key idea is to separately estimate the task and entropy objectives using different
  discount factors and GAE, allowing for better control over the entropy reward's
  impact.
---

# Maximum Entropy On-Policy Actor-Critic via Entropy Advantage Estimation

## Quick Facts
- arXiv ID: 2407.18143
- Source URL: https://arxiv.org/abs/2407.18143
- Authors: Jean Seong Bjorn Choe; Jong-Kook Kim
- Reference count: 40
- One-line primary result: EAPO improves policy optimisation performance in MuJoCo and Procgen tasks by separately estimating task and entropy objectives.

## Executive Summary
This paper introduces Entropy Advantage Policy Optimisation (EAPO), a method for maximum entropy reinforcement learning in on-policy actor-critic settings. EAPO extends PPO and TRPO by separately estimating task and entropy objectives using different discount factors and Generalized Advantage Estimation (GAE), replacing the conventional advantage estimate with a soft advantage estimate. The method aims to address the difficulty of managing entropy rewards in practice, particularly the correlation between entropy return and episode length in episodic tasks.

## Method Summary
EAPO is a maximum entropy reinforcement learning algorithm that modifies the standard advantage estimation in PPO and TRPO by incorporating a separate entropy advantage estimate. The method uses distinct discount factors and GAE parameters for task and entropy objectives, and introduces a separate entropy critic network with PopArt normalization. The soft advantage estimate is computed by combining the value advantage and entropy advantage, which is then used in the policy update objective. EAPO is evaluated on MuJoCo continuous control tasks, Procgen benchmark environments, and MiniGrid-DoorKey-8x8 hard exploration task.

## Key Results
- EAPO consistently outperforms or matches PPO on Procgen benchmark environments, with a mean normalised score of 0.54±0.06 compared to PPO's 0.42±0.07.
- On MuJoCo tasks, EAPO achieves competitive performance while improving policy stochasticity and exploration.
- EAPO successfully solves the MiniGrid-DoorKey-8x8 hard exploration task, though results are sensitive to hyperparameters.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating the task and entropy objectives using distinct discount factors prevents entropy reward inflation and improves control over policy stochasticity.
- Mechanism: By using a lower discount factor γH for entropy estimation, the method reduces the effective horizon of the entropy return, mitigating the correlation between entropy reward and episode length that can cause unstable behavior.
- Core assumption: The entropy return in episodic tasks is correlated with episode length, leading to overestimation or underestimation of the true entropy advantage.
- Evidence anchors:
  - [abstract] "we hypothesise that this is due to the difficulty of managing the entropy reward in practice"
  - [section 2] "in an episodic setting, the entropy return is largely correlated to the episode's length"
  - [corpus] Weak—most related work focuses on diffusion-based policies or energy-based models, not on-policy entropy control.
- Break condition: If the task is infinite-horizon with sparse rewards, the episode length correlation may not apply, reducing the benefit of separate discount factors.

### Mechanism 2
- Claim: Using GAE separately for each objective (task and entropy) reduces variance in the soft advantage estimate, enabling stable on-policy updates.
- Mechanism: The entropy advantage is estimated using GAE with its own λH and γH, analogous to the value advantage estimation, yielding a variance-reduced soft advantage that can be substituted into PPO/TRPO objectives.
- Core assumption: The entropy advantage function can be estimated with the same algorithmic structure as the value advantage function, enabling reuse of existing GAE machinery.
- Evidence anchors:
  - [section 4.2] "utilise the Generalised Advantage Estimation (GAE) [Schulman et al., 2015b] for a variance-reduced estimation of the entropy advantage"
  - [section 4.4] "integrate the entropy advantage with the standard advantage estimate"
- Break condition: If λH is set too high, variance reduction may be insufficient, causing noisy updates that destabilize training.

### Mechanism 3
- Claim: A separate entropy critic network with PopArt normalization stabilizes learning by handling scale differences between entropy and value estimates.
- Mechanism: The entropy critic shares hidden layers with the value critic but uses an independent output head, and PopArt normalizes both value and entropy estimates to comparable scales, preventing numerical instability.
- Core assumption: The entropy and value returns have different magnitudes and dynamics, requiring normalization to ensure balanced learning.
- Evidence anchors:
  - [section 4.3] "we employ the PopArt normalisation [van Hasselt et al., 2016] to address the scale difference of entropy and return estimates"
- Break condition: If the environment reward is sparse or highly variable, normalization may not fully address the scale mismatch, leading to degraded performance.

## Foundational Learning

- Concept: Maximum Entropy Reinforcement Learning (MaxEnt RL)
  - Why needed here: Understanding MaxEnt RL is essential to grasp why entropy regularization is added to the objective and how it differs from standard RL.
  - Quick check question: What is the role of the temperature parameter τ in MaxEnt RL, and how does it balance the task and entropy objectives?

- Concept: Advantage Actor-Critic (A2C) and Generalized Advantage Estimation (G