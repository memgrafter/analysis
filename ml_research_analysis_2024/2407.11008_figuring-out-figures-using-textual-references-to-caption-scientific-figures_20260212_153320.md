---
ver: rpa2
title: 'Figuring out Figures: Using Textual References to Caption Scientific Figures'
arxiv_id: '2407.11008'
source_url: https://arxiv.org/abs/2407.11008
tags:
- figure
- captions
- performance
- caption
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of generating captions for scientific
  figures, a task more complex than general image captioning due to the need for precise
  data extraction and domain-specific language. The authors propose a model combining
  a CLIP-based image encoder with a GPT-2 decoder, augmented with textual metadata
  (title, abstract, and in-text references) encoded by SciBERT.
---

# Figuring out Figures: Using Textual References to Caption Scientific Figures

## Quick Facts
- arXiv ID: 2407.11008
- Source URL: https://arxiv.org/abs/2407.11008
- Reference count: 25
- One-line primary result: Incorporating textual metadata (title, abstract, in-text references) improves scientific figure captioning, with text-only models slightly outperforming image-text models.

## Executive Summary
This work tackles the challenge of generating captions for scientific figures, a task more complex than general image captioning due to the need for precise data extraction and domain-specific language. The authors propose a model combining a CLIP-based image encoder with a GPT-2 decoder, augmented with textual metadata (title, abstract, and in-text references) encoded by SciBERT. Experiments show that incorporating textual references significantly improves caption quality over image-only approaches, achieving BLEU scores of 4.92 and ROUGE-L of 0.36 on original captions. Notably, a text-only variant (SciBERT+GPT-2) slightly outperforms the image-text model, suggesting the image encoder’s limited utility in the current setup. This highlights the importance of leveraging contextual paper metadata and points to opportunities for improving figure-specific visual encoders.

## Method Summary
The authors use a CLIP-ViT/B-32 + SciBERT + GPT-2 encoder-decoder architecture with cross-attention to generate captions for scientific figures. The model is trained end-to-end on the SCICAP dataset (416,000 graph plots from 290,000 arXiv papers) with additional metadata from METASCICAP. Training uses teacher forcing with AdamW (lr=5e-5) for 15 epochs, and captions are generated via top-p sampling (p=0.9). The approach is evaluated using BLEU and ROUGE-L F1 metrics.

## Key Results
- Incorporating textual metadata (title, abstract, in-text references) significantly improves caption quality over image-only approaches.
- A text-only variant (SciBERT+GPT-2) slightly outperforms the image-text model, suggesting the image encoder’s limited utility.
- BLEU score of 4.92 and ROUGE-L of 0.36 achieved on original captions, significantly outperforming previous CNN+LSTM baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Textual metadata (title, abstract, references) provides essential context that improves caption generation more than image features alone.
- Mechanism: The model leverages SciBERT to encode scientific text metadata, then fuses it with CLIP embeddings to generate captions. This allows the model to draw from the broader paper context when describing figures, rather than relying solely on visual features.
- Core assumption: Figure captions are heavily influenced by surrounding paper context, and this context is not fully captured by visual features alone.
- Evidence anchors:
  - [abstract]: "incorporates textual metadata from the original paper relevant to the figure, such as the title, abstract, and in-text references"
  - [section]: "we supplement our input with textual information, including paper metadata and in-text references"
  - [corpus]: Weak evidence—only general mentions of related SciCap work, no direct evidence of metadata boosting performance in cited papers.
- Break condition: If metadata is noisy or irrelevant, the model may learn to ignore it, reducing its benefit.

### Mechanism 2
- Claim: The transformer architecture (CLIP+GPT-2) outperforms previous CNN+LSTM models on figure captioning due to its ability to model long-range dependencies and integrate multimodal inputs.
- Mechanism: CLIP provides rich image embeddings, while GPT-2 with cross-attention can attend to both visual and textual features, generating captions that are more coherent and contextually relevant.
- Core assumption: Modern transformer models are better suited for complex multimodal tasks like scientific figure captioning than older CNN+LSTM approaches.
- Evidence anchors:
  - [abstract]: "use a variant of a CLIP+GPT-2 encoder-decoder model with cross-attention to generate captions conditioned on the image"
  - [section]: "GPT-2 is a decoder-only model, which we augment to add encoder-decoder cross attention to the final hidden states of the encoder output"
  - [corpus]: Weak evidence—no cited paper in the corpus directly compares CLIP+GPT-2 to CNN+LSTM on this task.
- Break condition: If the dataset is too small or the model overfits, transformer advantages may not materialize.

### Mechanism 3
- Claim: The text-only model (SciBERT+GPT-2) slightly outperforms the image-text model because figure captions often closely mirror surrounding text, making visual features less critical.
- Mechanism: The model learns to rely heavily on metadata, potentially bypassing image processing, especially if captions are derivable from textual references alone.
- Core assumption: Many scientific figure captions are essentially textual summaries of content already present in the paper, reducing the need for visual understanding.
- Evidence anchors:
  - [section]: "a text-only variant (SciBERT+GPT-2) slightly outperforms the image-text model, suggesting the image encoder’s limited utility"
  - [section]: "one explanation may be a confounding variable" (referring to decoder size differences)
  - [corpus]: No direct evidence in corpus papers supporting this specific mechanism.
- Break condition: If figures contain novel visual information not present in text, the text-only model will fail to capture it.

## Foundational Learning

- Concept: Multimodal learning (combining text and vision)
  - Why needed here: Scientific figures often contain both visual data and textual descriptions; effective captioning requires integrating both modalities.
  - Quick check question: What are the two main components of a multimodal encoder-decoder model used for figure captioning?

- Concept: Scientific domain adaptation (SciBERT)
  - Why needed here: Scientific text has specialized vocabulary and structure; domain-specific pretraining improves understanding and generation quality.
  - Quick check question: Why is SciBERT preferred over general BERT for encoding scientific paper metadata?

- Concept: BLEU and ROUGE evaluation metrics
  - Why needed here: These metrics quantify caption quality by comparing generated text to reference captions, guiding model selection and tuning.
  - Quick check question: How do BLEU and ROUGE differ in evaluating caption generation performance?

## Architecture Onboarding

- Component map:
  - Input: Figure image (224x224), paper title (≤100 chars), abstract (≤150 chars), in-text references (variable)
  - Encoders: CLIP-ViT/B-32 (image), SciBERT (text)
  - Fusion: Concatenated embeddings from both encoders
  - Decoder: GPT-2 (or DistilGPT-2) with cross-attention
  - Output: Generated caption via top-p sampling (p=0.9)
  - Evaluation: BLEU, ROUGE-L, qualitative inspection

- Critical path:
  1. Preprocess figure and metadata
  2. Encode image with CLIP, text with SciBERT
  3. Concatenate embeddings
  4. Feed into GPT-2 decoder with cross-attention
  5. Generate caption using top-p sampling
  6. Evaluate with BLEU/ROUGE

- Design tradeoffs:
  - Image encoder size: ViT-B/32 vs. ViT-L/14 (larger may overfit or underfit)
  - Text truncation: Limited context window forces prioritization of metadata
  - Decoder choice: GPT-2 vs. DistilGPT-2 (performance vs. memory)
  - Metadata inclusion: More metadata can help but may introduce noise

- Failure signatures:
  - Low BLEU/ROUGE: Model not learning from either modality effectively
  - High text-only performance, low image-text performance: Model ignoring image features
  - Memorization of references: Model regurgitating text without understanding visuals
  - Overfitting: Poor generalization to unseen figures

- First 3 experiments:
  1. Train text-only baseline (SciBERT+GPT-2) to establish upper bound of metadata utility
  2. Train image-only baseline (CLIP+GPT-2) to measure visual feature contribution
  3. Train full multimodal model (CLIP+SciBERT+GPT-2) and compare performance to baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can image encoders be improved to better capture figure-specific features beyond natural images?
- Basis in paper: [inferred] The authors note that CLIP, trained on natural images, may struggle with synthetic scientific figures, and that the text-only model outperforms the image-text model.
- Why unresolved: The paper suggests that resizing and normalization may hinder figure-specific feature extraction, and that alternative methods like autovectorizing SVG inputs could be explored, but these were not tested.
- What evidence would resolve it: Experiments comparing CLIP with figure-specific encoders (e.g., trained on synthetic figures) or preprocessing methods (e.g., SVG input) on scientific figure captioning tasks.

### Open Question 2
- Question: What is the impact of normalizing textual metadata (title, abstract, references) on model performance for scientific figure captioning?
- Basis in paper: [explicit] The authors observe that their model performs comparably on normalized captions but significantly better on original captions, suggesting normalization may remove domain-specific knowledge.
- Why unresolved: The paper hypothesizes that normalization removes useful tokens (e.g., equations, numbers) but does not test normalized metadata as input.
- What evidence would resolve it: Experiments training and evaluating models on normalized metadata to see if performance on normalized captions improves.

### Open Question 3
- Question: How does the similarity between in-text references and gold captions affect the model’s reliance on image features?
- Basis in paper: [explicit] The authors note that if references are very similar to captions, the model may "memorize" and ignore image features, as shown in Figure 4.
- Why unresolved: The paper identifies this as a potential confounding factor but does not quantify its prevalence or impact on performance.
- What evidence would resolve it: Analysis of reference-caption similarity across the dataset and ablation studies removing highly similar references to measure changes in image feature utilization.

## Limitations
- The image encoder (CLIP) may be suboptimal for synthetic scientific figures, as it was trained on natural images.
- The text-only model slightly outperforms the image-text model, raising questions about the necessity of visual features in this task.
- BLEU and ROUGE metrics may not fully capture the quality of scientific figure captions, particularly their accuracy in describing visual data trends.

## Confidence
- **High confidence**: The incorporation of textual metadata (title, abstract, references) improves caption quality over image-only approaches. This is supported by direct experimental results showing higher BLEU and ROUGE-L scores when metadata is included.
- **Medium confidence**: The transformer architecture (CLIP+GPT-2) provides advantages over previous CNN+LSTM models. While the mechanism is sound, there's limited direct comparative evidence in the cited corpus.
- **Low confidence**: The claim that the text-only model slightly outperforms the image-text model due to captions closely mirroring surrounding text. This interpretation is speculative and the decoder size difference is a significant confounding factor not fully addressed.

## Next Checks
1. Conduct an ablation study systematically removing each metadata component (title, abstract, references) to quantify their individual contributions to caption quality.
2. Evaluate generated captions using additional metrics beyond BLEU and ROUGE-L, such as semantic similarity scores or human evaluation focusing on accuracy of data trend description.
3. Test the model on figures containing novel visual information not present in the paper text to determine if the text-only approach fails when captions cannot be derived from metadata alone.