---
ver: rpa2
title: Towards a Framework for Evaluating Explanations in Automated Fact Verification
arxiv_id: '2403.20322'
source_url: https://arxiv.org/abs/2403.20322
tags:
- explanations
- explanation
- linguistics
- association
- arguments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper introduces a formal framework for evaluating
  rationalizing explanations in automated fact verification, addressing the lack of
  systematic evaluation methods for increasingly complex explanation structures. The
  framework defines three classes of explanations - free-form, deductive, and argumentative
  - each with associated properties and evaluation metrics.
---

# Towards a Framework for Evaluating Explanations in Automated Fact Verification

## Quick Facts
- arXiv ID: 2403.20322
- Source URL: https://arxiv.org/abs/2403.20322
- Authors: Neema Kotonya; Francesca Toni
- Reference count: 0
- Introduces a formal framework for evaluating rationalizing explanations in automated fact verification

## Executive Summary
This position paper addresses the critical need for systematic evaluation methods for rationalizing explanations in automated fact verification. The authors propose a formal framework that defines three classes of explanations - free-form, deductive, and argumentative - each with associated properties and evaluation metrics. By establishing properties like coherence, non-circularity, relevance, non-redundancy, and dialectical faithfulness, the framework provides a structured approach to assess explanation quality. Through illustrations using fact-checking examples, the work demonstrates how this formalization can improve model transparency and accountability in high-stakes applications where explanations are crucial for user trust.

## Method Summary
The framework introduces a systematic approach to evaluating rationalizing explanations in automated fact verification by defining three explanation classes (free-form, deductive, and argumentative) and their associated properties. Each explanation type is evaluated using specific metrics: coherence (Coh) for free-form explanations, relevance measures (RelWEAK, RelSTRONG) for deductive explanations, and acceptability (Acc) and circularity (Cir) scores for argumentative explanations. The method focuses on per-prediction rationalizing explanations that offer reasons for individual predictions, grounded in automated fact verification tasks where explanations are crucial for user trust.

## Key Results
- Defines three explanation classes with associated properties and metrics for systematic evaluation
- Introduces quantitative measures for properties like coherence, relevance, and dialectical faithfulness
- Demonstrates framework application through fact-checking examples showing systematic evaluation of explanation validity
- Provides concrete metrics enabling empirical assessment of different explanation types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework's structured approach to evaluating rationalizing explanations addresses the opacity of complex NLP models by providing systematic evaluation methods.
- Mechanism: By defining three classes of explanations (free-form, deductive, and argumentative) with associated properties and evaluation metrics, the framework creates a standardized way to assess explanation quality across different levels of structural complexity.
- Core assumption: Different types of explanations require different evaluation properties and metrics, and these can be formally defined and measured.
- Evidence anchors:
  - [abstract] "we advocate for a formal framework for key concepts and properties about rationalizing explanations to support their evaluation systematically"
  - [section] "We define a number of properties for our forms of rationalizing explanations... properties tailored to free-form, structured and argumentative explanations"
  - [corpus] Corpus neighbors show related work on fact-checking and explanation evaluation, but lack systematic frameworks like this one
- Break condition: If the defined properties and metrics cannot effectively capture the quality of explanations across different tasks or if the framework becomes too rigid to accommodate novel explanation types.

### Mechanism 2
- Claim: The framework's properties like coherence, non-circularity, and dialectical faithfulness provide concrete criteria for evaluating explanation validity.
- Mechanism: These properties create measurable standards that distinguish between superficial and substantive explanations by examining internal consistency, logical structure, and dialectical strength.
- Core assumption: Explanation quality can be quantified through measurable properties that reflect human reasoning standards.
- Evidence anchors:
  - [abstract] "we propose coherence, non-circularity, relevance, non-redundancy, and dialectical faithfulness as key properties"
  - [section] "We devise a metric for free-form explanation, Coh, relating to the property of coherence" and similar metrics for deductive and argumentative explanations
  - [corpus] Limited corpus evidence for similar property-based evaluation frameworks in NLP explanation literature
- Break condition: If the properties prove too subjective to measure consistently or fail to capture explanation quality in real-world applications.

### Mechanism 3
- Claim: The framework's application to automated fact verification demonstrates its practical utility in high-stakes domains requiring explanation transparency.
- Mechanism: By focusing on a knowledge-intensive task where explanations are crucial for user trust, the framework shows how systematic evaluation can improve model accountability in critical applications.
- Core assumption: Fact verification is representative of other high-stakes NLP tasks where explanation quality directly impacts user trust and decision-making.
- Evidence anchors:
  - [abstract] "Focusing on the automated fact verification task, we provide illustrations of the use and usefulness of our formalization"
  - [section] "We focus on per-prediction rationalizing explanations, which are specific to and offer reasons for individual predictions. We ground our analysis and paradigm on automated fact verification"
  - [corpus] Corpus neighbors show fact-checking as a common application area, supporting the relevance of this focus
- Break condition: If the framework's evaluation metrics don't generalize well to other NLP tasks or fail to improve user trust in fact verification systems.

## Foundational Learning

- Concept: Properties of explanations (coherence, non-circularity, relevance, non-redundancy, dialectical faithfulness)
  - Why needed here: These properties form the basis for the framework's evaluation metrics and distinguish between different types of explanations
  - Quick check question: Can you explain how coherence differs from non-circularity in the context of deductive explanations?

- Concept: Argumentation frameworks and dialectical strength
  - Why needed here: Argumentative explanations require understanding of attack/support relations and how they contribute to explanation quality
  - Quick check question: How does dialectical faithfulness relate to model prediction confidence in argumentative explanations?

- Concept: Free-form vs. structured explanations
  - Why needed here: The framework distinguishes between different levels of explanation structure, each requiring different evaluation approaches
  - Quick check question: What additional properties emerge when moving from free-form to deductive explanations?

## Architecture Onboarding

- Component map:
  - Explanation classes (free-form, deductive, argumentative)
  - Properties per class (coherence, non-circularity, relevance, non-redundancy, dialectical faithfulness)
  - Evaluation metrics per property (Coh, RelWEAK, RelSTRONG, Red, Acc, Cir)
  - Application domain (automated fact verification)

- Critical path:
  1. Define explanation class based on task requirements
  2. Apply relevant properties to evaluate explanation quality
  3. Calculate metric scores for each property
  4. Aggregate scores to assess overall explanation validity

- Design tradeoffs:
  - Flexibility vs. standardization: The framework aims to be general but may need task-specific adaptations
  - Property comprehensiveness vs. practical measurability: Some properties may be difficult to quantify accurately
  - Single-task focus vs. generalization: Strong performance in fact verification doesn't guarantee success in other domains

- Failure signatures:
  - Low coherence scores indicating contradictory propositions
  - High circularity scores suggesting logical loops in explanations
  - Inconsistent dialectical strength scores across similar predictions

- First 3 experiments:
  1. Apply framework to existing fact verification datasets and compare explanation quality scores with human judgments
  2. Test framework's generalizability by applying it to other NLP tasks (e.g., sentiment analysis with explanations)
  3. Evaluate framework's sensitivity by creating explanations with known quality issues and measuring metric detection rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we evaluate the effectiveness of argumentative explanations when the prediction confidence is low?
- Basis in paper: [explicit] The authors discuss dialectical faithfulness and acceptability properties for argumentative explanations, but focus primarily on high and top confidence predictions.
- Why unresolved: The paper does not provide concrete evaluation metrics or methods for assessing argumentative explanations when the model has low confidence in its prediction.
- What evidence would resolve it: Empirical studies comparing different evaluation metrics for argumentative explanations across varying confidence levels, or a formal definition of what constitutes a "good" argumentative explanation in low-confidence scenarios.

### Open Question 2
- Question: How do we measure the impact of culturally or linguistically dependent preferences on the evaluation of rationalizing explanations?
- Basis in paper: [inferred] The authors mention that their framework is modular and customizable to accommodate cultural and linguistic preferences, but do not provide specific methods for measuring this impact.
- Why unresolved: The paper acknowledges the importance of cultural and linguistic factors but does not offer concrete metrics or evaluation methods to assess their influence on explanation quality.
- What evidence would resolve it: Comparative studies of explanation evaluation across different cultural or linguistic groups, or the development of culturally-aware evaluation metrics for rationalizing explanations.

### Open Question 3
- Question: How can we extend the evaluation metrics for deductive explanations to account for more complex relations beyond simple binary relations?
- Basis in paper: [explicit] The authors mention that compound layered deductive explanations could be acquired by considering multiple semantics for the relation, but do not explore this in detail.
- Why unresolved: The current evaluation metrics for deductive explanations assume simple binary relations, which may not capture the complexity of real-world explanations.
- What evidence would resolve it: Development of evaluation metrics that can handle compound or multi-faceted relations in deductive explanations, validated through empirical studies on complex explanation structures.

## Limitations
- The framework's reliance on specific explanation structures may not capture all forms of human reasoning or explanation styles
- Some properties like coherence and dialectical faithfulness may vary across different cultural or domain contexts, affecting evaluation consistency
- Computational complexity of evaluating complex argumentative explanations could limit practical deployment in real-time systems

## Confidence
- Framework effectiveness for automated fact verification: High
- Framework generalization to other NLP tasks: Medium
- Computational efficiency for real-time systems: Low

## Next Checks
1. Conduct user studies to assess whether metric scores align with human judgment of explanation quality across diverse populations
2. Test framework generalization by applying it to explanation evaluation in sentiment analysis and question answering tasks
3. Measure computational overhead when evaluating explanations in large-scale fact verification systems