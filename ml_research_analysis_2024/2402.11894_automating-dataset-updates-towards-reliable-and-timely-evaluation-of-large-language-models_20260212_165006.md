---
ver: rpa2
title: Automating Dataset Updates Towards Reliable and Timely Evaluation of Large
  Language Models
arxiv_id: '2402.11894'
source_url: https://arxiv.org/abs/2402.11894
tags:
- answer
- question
- evaluation
- leakage
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a framework for automating dataset updates
  to enable reliable and timely evaluation of large language models (LLMs). The framework
  addresses benchmark leakage by generating new evaluation data through two strategies:
  mimicking (generating similar samples) and extending (generating samples at different
  cognitive levels using Bloom''s taxonomy).'
---

# Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models

## Quick Facts
- arXiv ID: 2402.11894
- Source URL: https://arxiv.org/abs/2402.11894
- Authors: Jiahao Ying; Yixin Cao; Yushi Bai; Qianru Sun; Bo Wang; Wei Tang; Zhaojun Ding; Yizhe Yang; Xuanjing Huang; Shuicheng Yan
- Reference count: 40
- Primary result: Framework automates dataset updates for reliable LLM evaluation using mimicking and extending strategies to address benchmark leakage

## Executive Summary
This paper presents a framework for automating dataset updates to enable reliable and timely evaluation of large language models (LLMs). The framework addresses benchmark leakage by generating new evaluation data through two strategies: mimicking (generating similar samples) and extending (generating samples at different cognitive levels using Bloom's taxonomy). The approach maintains evaluation stability while enabling fine-grained analysis of model capabilities. Extensive experiments on MMLU and BIG-Bench benchmarks show that the method effectively mitigates overestimation issues caused by data leakage, with high stability across multiple iterations.

## Method Summary
The framework uses two LLM-based strategies for dataset updates: mimicking generates similar samples based on original data while preserving stylistic and contextual essence, and extending expands existing samples at varying cognitive levels using Bloom's taxonomy adaptation. The process involves data ingestion of seed samples, question generation through both strategies, quality filtering to remove incorrect or duplicate samples, and performance evaluation. The method demonstrates adaptability to various LLM backbones beyond GPT-based models and provides systematic difficulty control through cognitive-level categorization.

## Key Results
- Mimicking strategy effectively alleviates overestimation issues in leaked datasets
- Extending strategy enables difficulty control and cognitive-level analysis across Remember, Apply, Analyze, and Evaluate levels
- High stability demonstrated across multiple iterations with consistent performance
- Method successfully mitigates benchmark leakage effects while maintaining evaluation reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mimicking strategy preserves stylistic and contextual essence of original data while generating unseen samples
- Mechanism: LLM-based prompt generation creates new samples that maintain format and knowledge patterns of seed samples
- Core assumption: LLMs can generate similar yet distinct samples that preserve essential characteristics
- Evidence anchors:
  - [abstract] "mimicking strategy to generate similar samples based on original data, preserving stylistic and contextual essence"
  - [section] "mimic the provided examples to generate *one* different but high-quality sample following the task description"
  - [corpus] Weak - only 5/25 neighbors with FMR > 0.4, suggesting limited direct relevance
- Break condition: If generated samples fail to maintain core characteristics or introduce significant errors

### Mechanism 2
- Claim: Extending strategy enables difficulty control through Bloom's taxonomy adaptation
- Mechanism: Systematic categorization of questions across cognitive levels (Remember, Apply, Analyze, Evaluate) allows targeted difficulty adjustment
- Core assumption: Higher cognitive levels inherently increase question difficulty
- Evidence anchors:
  - [abstract] "extending strategy that further expands existing samples at varying cognitive levels by adapting Bloom's taxonomy"
  - [section] "neither too difficult nor too easy an exam can fairly judge students' learning status"
  - [corpus] Weak - no direct corpus evidence supporting Bloom's taxonomy correlation with difficulty
- Break condition: If cognitive level assignment doesn't correlate with actual difficulty

### Mechanism 3
- Claim: Two strategies provide stability through iterative generation
- Mechanism: Multiple iterations of dataset updates show consistent performance across different LLM backbones
- Core assumption: LLMs can generate stable datasets across multiple iterations
- Evidence anchors:
  - [section] "The mimicking strategy proves effective in alleviating overestimation. In most cases, compared to the original leaked dataset, our updated dataset exhibits no significant overestimation issues"
  - [section] "Both mimicking and extending strategies show a high level of stability"
  - [corpus] Weak - limited corpus evidence on iterative stability of LLM-generated datasets
- Break condition: If performance variance exceeds acceptable thresholds across iterations

## Foundational Learning

- Concept: Bloom's Taxonomy
  - Why needed here: Provides systematic framework for categorizing cognitive levels and controlling difficulty
  - Quick check question: What are the four cognitive levels used in this framework and how do they differ in complexity?

- Concept: Benchmark Leakage
  - Why needed here: Understanding contamination issues in LLM evaluation datasets
  - Quick check question: How does benchmark leakage affect LLM performance evaluation?

- Concept: LLM-as-an-Examiner
  - Why needed here: Methodology for evaluating free-form generated questions
  - Quick check question: What are the three dimensions used in LLM-as-an-Examiner evaluation?

## Architecture Onboarding

- Component map:
  - Data Ingestion → Question Generation (Mimicking/Extending) → Quality Filtering → Evaluation → Performance Analysis
  - Key components: Seed data storage, LLM prompt templates, quality validation system, evaluation metrics

- Critical path:
  - Seed data → Generation → Filtering → Validation → Evaluation
  - Most time-consuming: Generation and filtering steps

- Design tradeoffs:
  - Quality vs. quantity: Stricter filtering reduces dataset size but improves reliability
  - Cost vs. coverage: More LLM generations increase cost but provide better coverage
  - Specificity vs. generalization: Domain-specific knowledge improves quality but reduces adaptability

- Failure signatures:
  - High variance across iterations indicates instability
  - Low diversity in generated samples suggests overfitting
  - Poor human evaluation scores indicate quality issues

- First 3 experiments:
  1. Generate 10 samples using mimicking strategy and verify format preservation
  2. Generate 10 samples using extending strategy across all cognitive levels
  3. Run stability test with 3 iterations and measure performance variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we more precisely control the difficulty of generated questions beyond using cognitive levels and entity popularity?
- Basis in paper: [explicit] The paper mentions "there is room for further exploration into more granular methods of setting question difficulty" in the limitations section.
- Why unresolved: The authors acknowledge that their current approach using cognitive levels and popularity is promising but not optimal for fine-grained difficulty control.
- What evidence would resolve it: Experimental results comparing different granular difficulty control methods (e.g., incorporating specific knowledge domains, reasoning steps, or linguistic complexity measures) against the current approach.

### Open Question 2
- Question: Does the filtering process during mimic dataset generation introduce bias in question difficulty distribution?
- Basis in paper: [explicit] The paper states "we filtered out samples where the answers were incorrect" which "may introduce variations in the complexity of the mimicked questions."
- Why unresolved: The authors recognize this as a limitation but don't analyze the extent or impact of this potential bias.
- What evidence would resolve it: Analysis comparing the difficulty distribution and content of original vs. filtered datasets, and evaluation of how this affects model performance across difficulty levels.

### Open Question 3
- Question: How does the performance of models on extended datasets correlate with their performance on original datasets across different cognitive levels?
- Basis in paper: [explicit] The paper shows performance variations across cognitive levels but doesn't analyze the correlation between original and extended dataset performance.
- Why unresolved: The authors demonstrate that extending strategy reveals performance variations but don't explore whether these variations are consistent with original dataset performance.
- What evidence would resolve it: Correlation analysis between model performance on original datasets and their performance on extended datasets at each cognitive level, potentially revealing which models are more robust across difficulty levels.

## Limitations
- Framework effectiveness depends on quality of LLM-generated samples without detailed filtering methodology
- Correlation between Bloom's taxonomy levels and actual difficulty remains unverified empirically
- Paper lacks comprehensive error analysis for generated samples that fail quality checks

## Confidence
- **High Confidence**

## Next Checks
1. Generate 10 samples using mimicking strategy and verify format preservation
2. Generate 10 samples using extending strategy across all cognitive levels
3. Run stability test with 3 iterations and measure performance variance