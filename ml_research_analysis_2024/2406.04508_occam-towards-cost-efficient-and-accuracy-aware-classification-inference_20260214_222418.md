---
ver: rpa2
title: 'OCCAM: Towards Cost-Efficient and Accuracy-Aware Classification Inference'
arxiv_id: '2406.04508'
source_url: https://arxiv.org/abs/2406.04508
tags:
- accuracy
- cost
- occam
- image
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of cost-efficient and accuracy-aware
  image classification by developing a principled framework, OCCAM, for optimal classifier
  assignment under budget constraints. The authors address the tradeoff between inference
  cost and accuracy in using machine learning models by formulating an integer linear
  programming problem to maximize accuracy subject to a user-specified cost budget.
---

# OCCAM: Towards Cost-Efficient and Accuracy-Aware Classification Inference

## Quick Facts
- **arXiv ID:** 2406.04508
- **Source URL:** https://arxiv.org/abs/2406.04508
- **Reference count:** 40
- **Primary result:** Achieves up to 40% cost reduction with minimal accuracy drop across CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet-1K

## Executive Summary
This paper introduces OCCAM, a framework for optimal classifier assignment in image classification that balances accuracy and inference cost. The key insight is formulating classifier selection as an integer linear programming problem under budget constraints. OCCAM uses an unbiased, low-variance accuracy estimator based on nearest neighbor interpolation in feature space, leveraging the well-separated nature of image classification tasks. The method demonstrates significant cost savings (up to 40%) compared to baselines while maintaining competitive accuracy across multiple benchmark datasets.

## Method Summary
OCCAM operates by first precomputing success probabilities for each classifier on a labeled sample using nearest neighbor distances in feature space. For each new query, it estimates accuracy using the nearest neighbor's success probability, assuming Lipschitz continuity. The framework then formulates an ILP problem to maximize estimated accuracy subject to a cost budget, with variance-based regularization to mitigate overestimation bias. The method is evaluated on CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet-1K using ResNet and SwinV2 models, with the ILP solved using the HiGHS solver.

## Key Results
- Achieves up to 40% cost reduction compared to using the single largest model (SwinV2-B)
- Outperforms baselines including single best model, random selection, and model-based accuracy prediction
- Demonstrates effectiveness across multiple datasets (CIFAR-10, CIFAR-100, Tiny ImageNet, ImageNet-1K)
- Shows robustness with minimal accuracy drop despite significant cost savings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OCCAM uses a Lipschitz continuity assumption to estimate classifier accuracy on unseen queries via nearest neighbor interpolation in a labeled sample space.
- Mechanism: Given that real image datasets are well-separated, OCCAM precomputes success probabilities (SP) for classifiers on a labeled sample. For a new query, it finds its nearest neighbor in the sample and uses the corresponding SP as an estimate. The Lipschitz continuity of both the oracle classifier and the ML classifiers ensures that this estimate is unbiased and has low variance.
- Core assumption: Image classification tasks are well-separated in the feature space used, meaning instances from different classes are far apart under the chosen metric.
- Evidence anchors:
  - [abstract] "unbiased and low-variance accuracy estimator with asymptotic guarantees, leveraging the well-separated nature of image classification tasks"
  - [section] "we can show that the success probability function SPi(x) ... is also Lipschitz continuous"
  - [corpus] Weak evidence; no direct mention of Lipschitz continuity or nearest neighbor accuracy estimation in the top related papers.
- Break condition: If the image dataset contains significant label noise or the feature space does not preserve class separation, the nearest neighbor estimate becomes unreliable.

### Mechanism 2
- Claim: OCCAM formulates the optimal model portfolio selection as an integer linear programming problem to maximize estimated accuracy under a cost budget.
- Mechanism: For each query and classifier, OCCAM has an estimated success probability. It then sets up an ILP where binary variables indicate whether a classifier is assigned to a query. The objective maximizes the sum of estimated success probabilities, subject to constraints that the total cost does not exceed the budget and each query is assigned exactly one classifier.
- Core assumption: The estimated success probabilities are accurate enough to guide the ILP toward a high-performing model portfolio.
- Evidence anchors:
  - [abstract] "computes the optimal solution by solving an integer linear programming problem"
  - [section] "we formulate the problem of finding the optimal model portfolio as an integer linear programming (ILP) problem"
  - [corpus] No direct evidence; the related papers focus on different optimization approaches (e.g., quantization, dual-agent frameworks) rather than ILP-based model selection.
- Break condition: If the estimation error is large, the ILP may select suboptimal classifiers, leading to accuracy loss despite the optimization.

### Mechanism 3
- Claim: OCCAM uses regularization in the ILP objective to mitigate overestimation bias from the accuracy estimator.
- Mechanism: OCCAM subtracts a term proportional to the estimator's standard deviation from the success probability estimate in the ILP objective. This penalizes selecting classifiers with high-variance estimates, reducing the risk of assigning them to queries where they might fail.
- Core assumption: The variance of the estimator can be reliably estimated on a validation set and used to regularize the selection.
- Evidence anchors:
  - [abstract] "compute the optimal classifier assignment strategy ... subject to a given cost budget by solving an integer linear programming (ILP) problem"
  - [section] "we optimize the objective ... in Equation (4), where Ïƒi is the standard deviation of the estimator dSPi"
  - [corpus] No direct evidence; the related papers do not mention variance-based regularization in ILP formulations for model selection.
- Break condition: If the validation set is too small or not representative, the estimated standard deviation may not reflect true estimator variance, leading to insufficient or excessive regularization.

## Foundational Learning

- **Concept:** Lipschitz continuity
  - Why needed here: To justify that success probability estimates from nearest neighbors are close to true values for well-separated data.
  - Quick check question: What property must a function have so that its value at a point is close to its value at a nearby point, with the difference bounded by a constant times the distance?

- **Concept:** Integer Linear Programming (ILP)
  - Why needed here: To formally encode the optimal model portfolio selection problem with binary assignment variables and linear constraints.
  - Quick check question: In an ILP for model selection, what type of constraint ensures that each query is assigned exactly one classifier?

- **Concept:** Well-separated datasets
  - Why needed here: To ensure that nearest neighbor interpolation in feature space provides accurate success probability estimates.
  - Quick check question: What characteristic of a dataset ensures that instances from different classes are far apart under a given metric, making nearest neighbor estimates reliable?

## Architecture Onboarding

- **Component map:** Preprocessing (extract features, precompute nearest neighbors and success probabilities) -> Estimation (compute unbiased, low-variance accuracy estimates) -> Optimization (solve ILP to assign classifiers under budget) -> Regularization (penalize high-variance estimators)
- **Critical path:** For a new query, the system must: (1) extract its features, (2) find its nearest neighbor in the precomputed sample, (3) retrieve the corresponding success probability estimates, (4) pass these to the ILP solver, and (5) output the assigned classifier. The ILP solving time is typically the bottleneck.
- **Design tradeoffs:** OCCAM trades off estimation accuracy (controlled by sample size and feature quality) for computational efficiency (ILP solving is NP-hard but practical solvers exist). It also trades off potential overestimation bias (mitigated by regularization) for the ability to handle diverse classifier pools.
- **Failure signatures:** High variance in accuracy estimates (due to small sample size or poor feature space), ILP solver failing to find a feasible solution (unrealistic budget), or poor feature space causing nearest neighbors to be from the wrong class.
- **First 3 experiments:**
  1. Run OCCAM on a small, clean dataset (e.g., CIFAR-10) with a few classifiers to verify the accuracy estimation and ILP assignment logic.
  2. Vary the sample size used for estimation and measure the impact on final accuracy to understand the bias-variance tradeoff.
  3. Test OCCAM with a challenging dataset (e.g., ImageNet-1K) to validate scalability and the effectiveness of the regularization term.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does OCCAM's performance scale with extremely large numbers of classes (e.g., 10,000+ classes) where the well-separated assumption may break down?
- Basis in paper: [inferred] The paper demonstrates effectiveness on ImageNet-1K (1000 classes) but acknowledges the well-separated assumption as critical. The discussion section mentions extending to other tasks but doesn't address high-dimensional class spaces.
- Why unresolved: The paper doesn't test on datasets with more than 1000 classes, and the theoretical guarantees rely on the well-separated property which may not hold for very large class sets.
- What evidence would resolve it: Empirical testing on datasets with 10,000+ classes (e.g., JFT-300M, OpenImages) comparing OCCAM's accuracy-cost tradeoffs to baselines, and analysis of nearest neighbor distances in high-dimensional class spaces.

### Open Question 2
- Question: Can OCCAM be extended to handle multi-label classification tasks where each image can have multiple ground truth labels simultaneously?
- Basis in paper: [explicit] The paper explicitly states "we mainly demonstrate the effectiveness of OCCAM on the image classification task" and identifies this as a potential extension in the discussion section.
- Why unresolved: The current formulation assumes a single ground truth label per image, and the success probability estimator is defined for single-label scenarios. Multi-label accuracy estimation would require fundamentally different probability modeling.
- What evidence would resolve it: Modified OCCAM implementation for multi-label tasks (e.g., COCO, Pascal VOC) with experimental validation showing cost reduction while maintaining per-label accuracy, and theoretical analysis of how the estimator needs to be adapted.

### Open Question 3
- Question: What is the optimal sample size K for OCCAM's accuracy estimator in terms of the bias-variance tradeoff, and how does this depend on dataset characteristics?
- Basis in paper: [explicit] The paper investigates sample size effects in Figure 4b but uses fixed parameters (K=40, s=1000 for Tiny ImageNet) without providing a principled method for determining optimal K.
- Why unresolved: The paper shows that accuracy improves with sample size but doesn't provide guidance on choosing K optimally, and the tradeoff between computational cost of sampling and estimation accuracy remains unclear.
- What evidence would resolve it: Systematic experiments varying K and s across multiple datasets to identify optimal ratios, theoretical analysis of the bias-variance tradeoff in the estimator, and practical guidelines for different dataset sizes and class complexities.

## Limitations
- The well-separated dataset assumption, while intuitive for image classification, lacks rigorous empirical validation across diverse datasets and feature spaces.
- The variance regularization approach assumes that validation set estimates accurately reflect test-time behavior, but the paper doesn't analyze the sensitivity to validation set size or representativeness.
- The ILP formulation, while principled, may become computationally prohibitive for large-scale deployments with many classifiers and queries.

## Confidence

- **High Confidence:** The core ILP formulation for budget-constrained model selection is mathematically sound and well-supported by the results.
- **Medium Confidence:** The accuracy estimation mechanism works well for the tested datasets, but its generalizability to more challenging scenarios remains to be seen.
- **Low Confidence:** The claims about asymptotic guarantees and the effectiveness of variance regularization lack thorough empirical validation.

## Next Checks
1. Test OCCAM on datasets with known label noise or fine-grained classes to evaluate the breakdown point of the well-separated assumption.
2. Conduct ablation studies varying the validation set size to quantify the impact on variance estimation accuracy and final performance.
3. Benchmark OCCAM's computational efficiency against alternative approaches (e.g., greedy heuristics) on large-scale deployments to assess practical viability.