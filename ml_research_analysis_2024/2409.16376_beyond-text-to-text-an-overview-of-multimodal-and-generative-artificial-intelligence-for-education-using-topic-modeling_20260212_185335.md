---
ver: rpa2
title: 'Beyond Text-to-Text: An Overview of Multimodal and Generative Artificial Intelligence
  for Education Using Topic Modeling'
arxiv_id: '2409.16376'
source_url: https://arxiv.org/abs/2409.16376
tags:
- education
- https
- generative
- research
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study used topic modeling to analyze 4175 research articles
  on generative AI in education, aiming to understand the research landscape across
  different modalities. The analysis identified 38 topics organized into 14 thematic
  areas, with a predominant focus on text-to-text models (especially ChatGPT) compared
  to other modalities like text-to-speech, text-to-image, and speech-to-text.
---

# Beyond Text-to-Text: An Overview of Multimodal and Generative Artificial Intelligence for Education Using Topic Modeling

## Quick Facts
- arXiv ID: 2409.16376
- Source URL: https://arxiv.org/abs/2409.16376
- Reference count: 40
- Primary result: Topic modeling of 4175 educational AI papers revealed dominant focus on text-to-text models (especially ChatGPT) compared to other modalities like text-to-speech, text-to-image, and speech-to-text.

## Executive Summary
This study used topic modeling to analyze 4175 research articles on generative AI in education, aiming to understand the research landscape across different modalities. The analysis identified 38 topics organized into 14 thematic areas, with a predominant focus on text-to-text models (especially ChatGPT) compared to other modalities like text-to-speech, text-to-image, and speech-to-text. The findings highlight a research gap, showing that while large language models dominate educational research, other promising AI technologies remain underexplored. The results suggest a need for more balanced attention across AI modalities and educational levels, and emphasize opportunities for future exploration of multimodal technologies to fully realize AI's transformative potential in education.

## Method Summary
The study employed BERTopic, a topic modeling approach, to analyze abstracts from 4175 educational AI research articles. The method involved three main steps: creating transformer-based embeddings using a sentence transformer trained on academic papers, reducing dimensionality with UMAP, and clustering with HDBSCAN. Topics were generated using class-based TF-IDF, and the analysis included qualitative interpretation to organize the 38 identified topics into 14 thematic areas. Parameter search via random sampling was used to optimize clustering quality, with 5000 iterations evaluating combinations of UMAP and HDBSCAN hyperparameters using the DBCV index.

## Key Results
- 38 interpretable topics were organized into 14 thematic areas covering educational AI research
- Text-to-text models (especially ChatGPT) dominated the research landscape compared to other modalities
- Text-to-speech was the next most common modality after language models, with text-to-image and speech-to-text remaining significantly underexplored

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Topic modeling with BERTopic captures latent thematic structure in multimodal AI education research.
- Mechanism: Abstracts are embedded using a transformer trained on academic papers, then clustered via UMAP + HDBSCAN, with topics identified by class-based TF-IDF. This allows identification of dominant research themes (e.g., LLMs in medical education, text-to-speech support) and reveals gaps in multimodal exploration.
- Core assumption: Abstracts sufficiently represent research focus and modality coverage.
- Evidence anchors:
  - [abstract] States that 38 interpretable topics were organized into 14 thematic areas using BERTopic.
  - [section] Describes the three-step process: embeddings → dimensionality reduction → clustering → c-TF-IDF topic extraction.
  - [corpus] Found 25 related papers via FMR scoring, showing relevant neighboring literature exists.
- Break condition: If abstracts are too brief or modality-specific terms are rare, clustering may fail to separate multimodal from unimodal research cleanly.

### Mechanism 2
- Claim: Parameter search via random sampling identifies optimal hyperparameters for clustering quality.
- Mechanism: 5000 random combinations of UMAP and HDBSCAN parameters are evaluated using DBCV index and topic count constraints. This avoids local optima and balances topic interpretability with coverage.
- Core assumption: DBCV index correlates with meaningful topic separation in academic abstracts.
- Evidence anchors:
  - [section] Describes sampling 5000 iterations and selecting solution minimizing out-of-topic documents while maximizing DBCV.
  - [abstract] Reports 54 initial topics, reduced to 38 after merging/removal, suggesting parameter optimization worked.
  - [corpus] Neighbor titles include multimodal AI and generative AI reviews, indicating domain relevance.
- Break condition: If DBCV favors over-clustering or under-clustering, thematic clarity may degrade.

### Mechanism 3
- Claim: Focusing on product-agnostic modality terms (e.g., "text-to-speech") avoids vendor bias and captures broader research landscape.
- Mechanism: Search string excludes brand names, relying on modality keywords to identify research scope. This surfaces underexplored technologies relative to dominant LLMs.
- Core assumption: Modality terms are consistently used across papers regardless of vendor.
- Evidence anchors:
  - [abstract] Notes that "text-to-speech was the next common keyword after language models," supporting modality-based coverage.
  - [section] States that no service names were included to allow unbiased review.
  - [corpus] FMR neighbor titles include "Generative AI and Its Impact on Personalized Intelligent Tutoring Systems," suggesting breadth.
- Break condition: If modality terms are inconsistently applied or papers use only vendor names, coverage gaps emerge.

## Foundational Learning

- Concept: Topic modeling and embedding techniques
  - Why needed here: Understanding how BERTopic transforms abstracts into topics is critical for interpreting results and diagnosing model behavior.
  - Quick check question: What is the role of c-TF-IDF in BERTopic topic generation?

- Concept: Hyperparameter optimization for clustering
  - Why needed here: Selecting UMAP/HDBSCAN parameters directly impacts topic coherence and interpretability.
  - Quick check question: How does DBCV index guide parameter selection in this study?

- Concept: Text preprocessing and stopword removal
  - Why needed here: Removing stopwords and irrelevant terms improves topic clarity and reduces noise.
  - Quick check question: Why might forcing out-of-topic abstracts into topics introduce errors?

## Architecture Onboarding

- Component map: Data ingestion (Dimensions.ai) → Abstract preprocessing → Embedding (sentence-transformer) → Dimensionality reduction (UMAP) → Clustering (HDBSCAN) → Topic labeling (c-TF-IDF) → Thematic mapping
- Critical path: Embedding → Clustering → Topic extraction; failure here invalidates downstream analysis
- Design tradeoffs: Broader search terms increase coverage but risk noise; excluding vendor names avoids bias but may miss some studies
- Failure signatures: Low DBCV index, high out-of-topic rate, or topics with high entropy indicate poor clustering
- First 3 experiments:
  1. Vary UMAP neighbor count and minimum distance to see effect on topic granularity
  2. Test HDBSCAN min_cluster_size and min_samples to balance topic count vs. coherence
  3. Compare results using different sentence transformers (e.g., all-mpnet-base-v2 vs. other academic models)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal generative AI technologies be effectively integrated into educational practices beyond text-to-text models to enhance personalized learning and creativity?
- Basis in paper: [explicit] The paper identifies a research gap where text-to-speech, text-to-image, and other multimodal capabilities are underexplored compared to text-to-text models like ChatGPT, despite their potential for personalized learning, problem-solving, and creativity.
- Why unresolved: The paper highlights the need for balanced attention across AI modalities but does not provide specific frameworks or evidence for effective integration of multimodal technologies in diverse educational contexts.
- What evidence would resolve it: Empirical studies demonstrating successful implementation and measurable impact of multimodal AI tools (e.g., text-to-speech, text-to-image) in classrooms across different subjects and age groups, along with guidelines for educators on their pedagogical use.

### Open Question 2
- Question: What are the ethical and academic integrity challenges specific to multimodal generative AI in education, and how can they be addressed?
- Basis in paper: [explicit] The paper discusses concerns about academic integrity with text-to-text models like ChatGPT and mentions the need for ethically sound AI integration, but it does not explore challenges unique to multimodal AI, such as deepfakes or manipulated visual/audio content.
- Why unresolved: While the paper acknowledges ethical concerns, it lacks a detailed analysis of how multimodal AI might introduce new risks (e.g., misinformation through AI-generated videos or images) and what policies or detection methods could mitigate these risks.
- What evidence would resolve it: Research identifying specific ethical risks posed by multimodal AI in education, along with proposed solutions such as detection tools, ethical guidelines, or regulatory frameworks tailored to these technologies.

### Open Question 3
- Question: How can educators and researchers develop and implement AI literacy frameworks that encompass both text-to-text and multimodal generative AI tools?
- Basis in paper: [explicit] The paper emphasizes the need for broad AI literacy to protect student agency and enable informed use of diverse AI technologies, but it does not outline specific strategies or frameworks for achieving this.
- Why unresolved: The paper calls for AI literacy but does not provide concrete models or examples of how educators can teach students to critically engage with both traditional and emerging multimodal AI tools.
- What evidence would resolve it: Development and validation of comprehensive AI literacy curricula or frameworks that include hands-on experience with multimodal AI tools, coupled with assessments of their effectiveness in improving student understanding and responsible use of AI.

## Limitations

- The analysis depends on abstracts adequately capturing modality coverage, which may underestimate research in non-text domains if terminology varies
- Temporal scope (post-2014) may not fully capture the recent surge in multimodal AI research following advances in foundation models
- The 14 thematic areas involve subjective judgment in topic merging and classification that could vary with different analytical approaches

## Confidence

- High Confidence: The methodological framework (BERTopic with parameter optimization) is sound and the finding that text-to-text models dominate educational AI research is well-supported by the data and consistent with recent literature trends.
- Medium Confidence: The identification of specific underexplored modalities (text-to-speech, text-to-image, speech-to-text) relies on keyword matching that may not capture all relevant research, particularly studies using multimodal approaches without explicit modality labeling.
- Medium Confidence: The thematic organization into 14 areas represents a reasonable interpretation but involves subjective judgment in topic merging and classification that could vary with different analytical approaches.

## Next Checks

1. **Modality Coverage Validation**: Manually review a stratified sample of papers from underrepresented modality topics to verify whether they truly represent limited research activity or whether modality terms were inconsistently applied in the corpus.

2. **Temporal Analysis**: Repeat the topic modeling on 1-2 year rolling windows to assess whether the dominance of text-to-text models is changing over time as multimodal capabilities advance.

3. **Author Expertise Mapping**: Cross-reference topic assignments with author publication histories to determine whether the modality distribution reflects researcher expertise gaps or genuine differences in technological readiness and application potential.