---
ver: rpa2
title: 'RLZero: Direct Policy Inference from Language Without In-Domain Supervision'
arxiv_id: '2412.05718'
source_url: https://arxiv.org/abs/2412.05718
tags:
- video
- learning
- reward
- policy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RLZero introduces a framework for zero-shot policy inference from
  language instructions without in-domain supervision. The method leverages pretrained
  video generative models to imagine task-relevant trajectories from language prompts,
  projects these imagined frames to real agent observations via semantic similarity
  retrieval, and uses pretrained behavior foundation models with a closed-form imitation
  learning solution to output executable policies.
---

# RLZero: Direct Policy Inference from Language Without In-Domain Supervision

## Quick Facts
- arXiv ID: 2412.05718
- Source URL: https://arxiv.org/abs/2412.05718
- Authors: Harshit Sikchi; Siddhant Agarwal; Pranaya Jajoo; Samyak Parajuli; Caleb Chuck; Max Rudolph; Peter Stone; Amy Zhang; Scott Niekum
- Reference count: 40
- One-line primary result: Achieves 83.2% win rate against baselines in 25 language tasks across continuous control domains without in-domain supervision

## Executive Summary
RLZero introduces a framework for zero-shot policy inference from language instructions without requiring in-domain supervision. The method leverages pretrained video generative models to imagine task-relevant trajectories from language prompts, projects these imagined frames to real agent observations via semantic similarity retrieval, and uses pretrained behavior foundation models with a closed-form imitation learning solution to output executable policies. Experiments across 25 language tasks in continuous control domains (Walker, Cheetah, Quadruped, Stickman) show RLZero achieves a 83.2% win rate against baselines, outperforming image-language and video-language reward methods without requiring test-time training.

## Method Summary
RLZero operates through a three-stage pipeline: imagination, projection, and imitation. First, it generates imagined video trajectories from language prompts using pretrained video generation models. Second, it projects these imagined frames into the agent's observation space using semantic similarity retrieval with vision-language embeddings. Third, it uses a pretrained behavior foundation model to compute a closed-form solution that extracts a policy matching the projected state visitation distribution. The method requires only an offline dataset of reward-free interactions and pretrained foundation models, enabling zero-shot policy generation without any in-domain task supervision.

## Key Results
- Achieves 83.2% win rate against baselines (TD3, IQL) across 25 language tasks in continuous control domains
- Outperforms image-language and video-language reward methods without requiring test-time training
- Enables zero-shot cross-embodiment transfer, successfully imitating complex human-like behaviors from YouTube or AI-generated videos in 2D/3D humanoid environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Closed-form imitation learning enables zero-step policy inference without gradient updates.
- Mechanism: The successor measure-based behavioral foundation model encodes behaviors into latent space. Given a reward function, the corresponding latent parameter is computed via closed-form linear regression, bypassing iterative optimization.
- Core assumption: Learned successor features span relevant rewards and offline dataset is sufficiently diverse.
- Evidence anchors:
  - [abstract] "Finally, an agent pretrained in the target environment with unsupervised RL instantly imitates the projected observation sequence through a closed-form solution."
  - [section 4.3] "At test time, the policy for any given reward function (r(s)) can then be obtained with such a BFM (zero-shot, with no additional experiential data or gradient updates) using a closed-form solution."
- Break condition: If offline dataset lacks behavioral diversity or successor features fail to span reward space, closed-form solution produces poor policies.

### Mechanism 2
- Claim: Semantic similarity retrieval grounds imagined video frames into agent's observation space.
- Mechanism: Imagined frames from video generation model are projected to real observations by finding nearest neighbors in embedding space (e.g., CLIP/SigLIP).
- Core assumption: Vision-language embedding space generalizes to agent's domain and captures relevant state information.
- Evidence anchors:
  - [abstract] "Next, these imagined observations are projected into the target environment domain."
  - [section 4.2] "We propose to use semantic-similarity based retrieval to obtain nearest frames in the dataset of the agent's prior environmental interactions dO to project individual frames of imagined trajectories to real observations as shown in Fig 2."
- Break condition: If embedding space doesn't generalize to agent's domain (e.g., due to background distractors), retrieval fails and produces incorrect state matches.

### Mechanism 3
- Claim: Video language models enable zero-shot task specification without in-domain annotations.
- Mechanism: VLMs generate imagined video trajectories conditioned on language prompts using internet-scale training, bypassing need for labeled task trajectories.
- Core assumption: VLMs learned rich prior mapping from language to behaviors that generalizes to agent's domain.
- Evidence anchors:
  - [abstract] "Large-scale multimodal foundation models [79] provide us with the first part of the solution. Trained on large amounts of internet data, they can generate video segments that communicate what performing a task entails."
  - [section 4.1] "Large video-language foundation models (ViFMs) help lift that requirement by training on vast amounts of internet videos, thus giving us a rich prior of grounding language commands to videos."
- Break condition: If VLM's training distribution doesn't overlap with agent's domain or generation quality is poor, imagined trajectories will be unrealistic or irrelevant.

## Foundational Learning

- Concept: Successor features and successor measures in reinforcement learning
  - Why needed here: They provide representation that can be linearly combined to represent any reward function, enabling closed-form policy inference.
  - Quick check question: What is the mathematical form of the successor measure, and how does it relate to the successor feature representation?

- Concept: Distribution matching and imitation learning
  - Why needed here: Policy inference step minimizes divergence between policy's state visitation distribution and expert (imagined) distribution.
  - Quick check question: How does KL divergence between state visitation distributions lead to closed-form solution for zimit?

- Concept: Multimodal video-language models and embedding spaces
  - Why needed here: Enable generation of imagined video trajectories from language prompts and semantic projection of these frames into agent's observation space.
  - Quick check question: How do video-language models like InternVideo2 and embedding models like SigLIP handle modality gap between video and text?

## Architecture Onboarding

- Component map:
  Video generation model (VM) -> Embedding model (SigLIP) -> Behavioral foundation model (BFM) -> Policy extraction

- Critical path:
  1. Generate imagined video from language prompt using VM
  2. Project each imagined frame to real observations via semantic similarity retrieval
  3. Compute closed-form z-parameter using successor features and projected states
  4. Extract and evaluate the policy

- Design tradeoffs:
  - Small video generation model limits imagination quality but reduces compute cost
  - Semantic similarity retrieval is interpretable but computationally expensive; vector databases could speed it up
  - Closed-form inference is fast but depends on quality of BFM and offline dataset

- Failure signatures:
  - Poor policy performance → Check imagined video quality, retrieval accuracy, and BFM training
  - Retrieval mismatches → Inspect embedding space generalization and background distractors
  - Closed-form solution fails → Verify offline dataset diversity and successor feature span

- First 3 experiments:
  1. Validate imagined video quality: Generate trajectories for set of prompts and inspect visually
  2. Test retrieval accuracy: For held-out set of real frames, check if nearest neighbor in embedding space matches ground truth
  3. Evaluate BFM closed-form inference: Given synthetic reward, check if inferred policy matches expected behavior on simple task

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on quality of pretrained video generation models and vision-language embeddings
- Sensitivity to offline dataset size and diversity for training effective behavior foundation models
- Limited empirical validation of closed-form inference mechanism's assumptions about successor feature span

## Confidence
- Closed-form imitation learning mechanism: Medium
- Semantic similarity retrieval grounding: Medium
- Zero-shot cross-embodiment transfer: Low
- Overall framework effectiveness: Medium

## Next Checks
1. Test closed-form inference sensitivity to offline dataset size and diversity by systematically varying these parameters and measuring policy performance degradation
2. Evaluate retrieval grounding accuracy quantitatively by creating benchmark where ground truth frame matches are known, measuring retrieval precision and recall
3. Conduct cross-embodiment transfer experiments with systematic ablation of VLM generation quality to isolate its contribution to success