---
ver: rpa2
title: 'X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated
  Cross-lingual Instructions'
arxiv_id: '2405.19744'
source_url: https://arxiv.org/abs/2405.19744
tags:
- instruction
- uni00000048
- languages
- language
- uni00000051
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of aligning large language models
  in low-resource languages by proposing a method to automatically generate cross-lingual
  instruction tuning data. The core idea is to first fine-tune a language model to
  generate English instructions for multilingual texts, then refine these instructions
  through iterative evaluation and diversification.
---

# X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions

## Quick Facts
- arXiv ID: 2405.19744
- Source URL: https://arxiv.org/abs/2405.19744
- Authors: Chong Li; Wen Yang; Jiajun Zhang; Jinliang Lu; Shaonan Wang; Chengqing Zong
- Reference count: 34
- Key outcome: X-Instruction dataset of 320k cross-lingual samples significantly outperforms baselines in low-resource languages, with 90.6% zero-shot instruction following capability

## Executive Summary
This paper addresses the challenge of aligning large language models in low-resource languages by proposing an automated method to generate cross-lingual instruction tuning data. The approach leverages English instruction generation as a bridge to bootstrap low-resource language capability, then iteratively refines the data quality through synthetic evaluation. The resulting X-Instruction dataset contains 320k high-quality samples across 10 languages and demonstrates superior performance compared to existing methods like Alpaca-MT and Bactrian-M, even approaching ChatGPT-level quality in some cases.

## Method Summary
The X-Instruction pipeline consists of three main stages: (1) generating English instructions for multilingual web texts, (2) iteratively refining these samples through synthetic rating datasets and evaluator training, and (3) diversifying the instruction types via k-means clustering. The method fine-tunes a base model to generate English instructions from non-English texts, then uses these as pseudo-training data while progressively improving quality through multiple refinement iterations. The final dataset is sampled to ensure diversity across instruction types, enabling effective cross-lingual instruction tuning.

## Key Results
- X-Instruction models achieve 71.2% win rate against Alpaca-MT and 67.6% against Bactrian-M in pairwise comparisons
- Models maintain 90.6% response quality when performing zero-shot instruction following in target languages
- Outperforms ChatGPT in 5 out of 10 languages according to GPT-4 pairwise evaluation
- Shows consistent improvements across multilingual benchmarks (XNLI, XCOPA, XStoryCloze) for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual instruction generation exploits better English generation performance to bootstrap low-resource language capability.
- Mechanism: The pipeline first fine-tunes the model to generate English instructions from low-resource language texts, then uses these as pseudo-training data. This leverages the higher-quality English generation while still enabling responses in the target language.
- Core assumption: The language model can generate semantically aligned English instructions even when given foreign language texts as context, and the model can subsequently generate appropriate responses in the target language when given these English instructions.
- Evidence anchors:
  - [abstract] "Specifically, the language model first learns to generate appropriate English instructions according to the natural web texts in other languages as responses."
  - [section] "Based on the observation, we first adopt the cross-lingual instruction following format like Figure 1(b), in which the languages of instructions and responses are not the same, to exploit the understanding ability rapidly learned in low-resource languages and better generation performance in English."

### Mechanism 2
- Claim: Iterative refinement with synthetic ratings progressively improves the quality of cross-lingual instruction samples.
- Mechanism: The system generates candidate samples, then iteratively trains an evaluator using synthetic rating datasets (best vs. worst outputs), which then filters better samples for the next iteration. This creates a self-improving loop.
- Core assumption: The evaluator trained on synthetic ratings can distinguish between good and bad cross-lingual instruction samples, and that the quality gap between different output versions allows meaningful training.
- Evidence anchors:
  - [section] "In the k-th iteration, we first synthesize a pseudo-rating dataset... to train the evaluator that outputs three-level ratings... the quality of output from the instruction following model improves by tuning on higher quality X-Instruction samples, which reduces the gap between the second-level sample and the highest one in the pseudo-rating dataset."

### Mechanism 3
- Claim: Zero-shot instruction following in the target language emerges from cross-lingual training without explicit multilingual instruction data.
- Mechanism: Models trained on English instructions with target language responses learn to generalize the instruction-following capability to the target language itself, enabling zero-shot performance.
- Core assumption: The cross-lingual training format induces a generalization from understanding English instructions to understanding instructions in the target language, even though the training never explicitly pairs target language instructions with target language responses.
- Evidence anchors:
  - [abstract] "models tuned on cross-lingual instruction following samples can follow the instruction in the output language without further tuning and maintain 90.6% response quality."
  - [section] "Table 5 shows the performance of zero-shot generation in ten languages. Compared with the vanilla cross-lingual generation, the win rate of X-Instruction models under zero-shot evaluation only drops by 5.7% on average."

## Foundational Learning

- Concept: Cross-lingual instruction tuning format
  - Why needed here: The method relies on having instruction in one language and response in another, which is non-standard and requires understanding how this format differs from monolingual instruction tuning.
  - Quick check question: What is the key difference between X-Instruction and standard instruction tuning data format?

- Concept: Iterative refinement with synthetic data
  - Why needed here: The quality improvement loop depends on generating synthetic ratings and training evaluators, which is a specific technique not common in all instruction tuning pipelines.
  - Quick check question: How does the synthetic rating dataset construction work in the first iteration versus subsequent iterations?

- Concept: Diversity sampling via clustering
  - Why needed here: The final diversification step uses k-means clustering on instruction embeddings to ensure varied instruction types, which is crucial for good generalization.
  - Quick check question: Why is it important to sample the same amount from each cluster rather than sampling uniformly from all data?

## Architecture Onboarding

- Component map: Seed data preparation -> Base model instruction generation -> Iterative refinement with evaluator training -> Clustering-based diversification -> Final instruction tuning
- Critical path: The iterative refinement process is the most resource-intensive step, requiring multiple rounds of evaluator training and large-scale candidate generation
- Design tradeoffs: Using English instructions leverages higher-quality generation but requires generalization to target language instructions; iterative refinement improves quality but adds computational cost; clustering for diversity prevents overfitting but adds complexity
- Failure signatures: Low win rates against baselines indicate evaluator training issues; poor zero-shot performance suggests broken generalization; low diversity indicates clustering parameter problems
- First 3 experiments:
  1. Test base model's ability to generate English instructions from non-English texts
  2. Run one iteration of refinement and measure improvement in win rate
  3. Test zero-shot instruction following in target language after cross-lingual training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact impact of increasing the number of iterations in the X-Instruction Refinement process beyond 3 iterations?
- Basis in paper: [explicit] The paper states "The number of iterations is set to 3 by default, which is investigated in Section 5.4.1" and shows win rates increase with more iterations but quality improvement saturates at 3 iterations.
- Why unresolved: The paper only investigates up to 3 iterations and doesn't explore whether additional iterations could further improve quality or if performance would degrade.
- What evidence would resolve it: Experimental results comparing model performance with 4, 5, or more refinement iterations against baseline models and human evaluation metrics.

### Open Question 2
- Question: How does the performance of X-Instruction models change when applied to languages not included in the original 10-language dataset?
- Basis in paper: [inferred] The paper mentions "we set the number of iterations to 3 by default" and evaluates on 10 languages, but acknowledges "our method is only applied to 10 languages in this work and other languages can be investigated in the future."
- Why unresolved: The study is limited to 10 languages (5 medium-resource and 5 low-resource), leaving uncertainty about generalization to other language families or resource levels.
- What evidence would resolve it: Experimental results comparing X-Instruction models on a diverse set of additional languages, including both related and unrelated language families.

### Open Question 3
- Question: What is the minimum quantity of high-quality seed data required to achieve comparable performance to the current X-Instruction models?
- Basis in paper: [explicit] The paper states "the quality of the model responses will increase with more samples used and is close to saturation at 32k" and investigates different rating levels of samples.
- Why unresolved: While the paper shows improvement up to 32k samples, it doesn't determine the minimum threshold for effective cross-lingual instruction tuning or whether smaller, higher-quality seed datasets could achieve similar results.
- What evidence would resolve it: Controlled experiments varying seed dataset sizes (e.g., 1k, 3k, 10k, 32k) while maintaining consistent quality levels, measuring performance degradation and resource efficiency trade-offs.

## Limitations

- The evaluation relies entirely on synthetic GPT-4 judgments and a single round of human evaluation, lacking independent verification
- The zero-shot performance claim of 90.6% maintenance is based on synthetic comparisons that may not reflect real-world effectiveness
- The iterative refinement process depends heavily on synthetic ratings, with no validation that the evaluator learns meaningful quality distinctions

## Confidence

**High Confidence**: The core pipeline architecture is well-defined and the iterative refinement mechanism is theoretically sound.

**Medium Confidence**: Claims of outperforming established baselines are supported by synthetic evaluations but lack independent verification.

**Low Confidence**: Superiority over ChatGPT is based on synthetic GPT-4 comparisons, creating a circular evaluation problem.

## Next Checks

1. **Independent Human Evaluation**: Conduct blind human evaluation comparing X-Instruction models against both baselines and ChatGPT across multiple languages and task types.

2. **Zero-shot Robustness Testing**: Test zero-shot instruction following capability with genuinely out-of-distribution instructions not seen during training.

3. **Domain Generalization Analysis**: Evaluate models on held-out domains from the web corpus to assess whether clustering-based diversity prevents overfitting to the training distribution.