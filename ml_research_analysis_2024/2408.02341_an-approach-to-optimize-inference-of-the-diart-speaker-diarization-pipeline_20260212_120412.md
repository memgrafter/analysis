---
ver: rpa2
title: An approach to optimize inference of the DIART speaker diarization pipeline
arxiv_id: '2408.02341'
source_url: https://arxiv.org/abs/2408.02341
tags:
- embedding
- pruning
- latency
- pyannote
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of optimizing inference latency
  in online speaker diarization, specifically for the DIART pipeline, where the embedding
  model dominates the overall latency. The core method idea involves applying various
  inference optimization techniques to the pyannote/embedding model, including knowledge
  distillation, structured and unstructured pruning, layer fusion, quantization, and
  conversion to ONNX format.
---

# An approach to optimize inference of the DIART speaker diarization pipeline

## Quick Facts
- arXiv ID: 2408.02341
- Source URL: https://arxiv.org/abs/2408.02341
- Reference count: 40
- One-line primary result: Optimized DIART pipeline achieves up to 82.9% of original latency while maintaining similar accuracy through knowledge distillation, layer fusion, and quantization

## Executive Summary
This paper addresses the problem of optimizing inference latency in online speaker diarization, specifically for the DIART pipeline where the embedding model dominates overall latency. The authors apply various inference optimization techniques to the pyannote/embedding model, including knowledge distillation, structured and unstructured pruning, layer fusion, quantization, and conversion to ONNX format. The study evaluates these techniques on a subset of the VoxConverse testset, measuring latency, diarization error rate (DER), and model size. Results show that knowledge distillation reduces latency by 10% and model size by 18% but increases DER by 5%, while layer fusion and quantization further reduce latency by 7% without worsening accuracy. Pruning has no effect on latency, and ONNX conversion unexpectedly increases latency by 40%.

## Method Summary
The authors apply a series of inference optimization techniques to the pyannote/embedding model in the DIART pipeline. First, they train a reduced model using knowledge distillation with a teacher factor λ=1000 on the AMI corpus for 79 epochs. Then they apply structured and unstructured pruning, layer fusion, int8 quantization, and ONNX conversion to this reduced model using PyTorch APIs. The optimized models are evaluated on a subset of four VoxConverse test files totaling ~20 minutes, measuring mean and standard deviation of processing time per chunk (latency), diarization error rate (DER), and model size. The evaluation is performed on an Intel Xeon Gold 5215 CPU.

## Key Results
- Knowledge distillation reduces latency by 10% and model size by 18%, but increases DER by 5%
- Layer fusion and quantization further reduce latency by 7% without worsening accuracy
- Pruning has no effect on latency, memory requirements, or accuracy
- ONNX conversion increases latency by 40% instead of decreasing it
- Optimized models achieve up to 82.9% of original latency while maintaining similar accuracy levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation improves latency by reducing model size and computational complexity while preserving most accuracy.
- Mechanism: A smaller student model is trained to mimic the output of a larger teacher model. The student has fewer layers and parameters, which reduces the number of operations during inference.
- Core assumption: The student model can learn to approximate the teacher's performance with fewer resources, and the loss of accuracy is acceptable for the gain in speed.
- Evidence anchors:
  - [abstract]: "knowledge distillation reduces latency by 10% and model size by 18%, but increases diarization error (DER) by 5%"
  - [section]: "By training a reduced pyannote/embedding model with knowledge distillation, the inference speed could be increased and the memory requirements reduced"
  - [corpus]: No direct evidence, but related work in the field shows similar improvements
- Break condition: If the student model cannot achieve acceptable accuracy, or if the reduction in latency is negligible compared to the increase in DER.

### Mechanism 2
- Claim: Layer fusion improves latency by reducing the number of sequential operations during inference.
- Mechanism: Successive layers (e.g., convolution followed by ReLU) are combined into a single operation, reducing the overhead of multiple separate operations.
- Core assumption: The fused operation can be executed more efficiently than the separate operations, and the accuracy is not significantly affected.
- Evidence anchors:
  - [abstract]: "Layer fusion and quantization further reduce latency by 7% without worsening accuracy"
  - [section]: "Layer fusion leads to a further reduction in latency of 7%. The number of model parameters remains constant at 3.56 million due to the layer fusion, as the layers are only combined but not reduced. This means that the speedup cannot be explained by a smaller model. However, layer fusion reduces the overhead and fewer separate operations need to be executed sequentially"
  - [corpus]: No direct evidence, but common optimization technique in deep learning literature
- Break condition: If the fused operation cannot be executed more efficiently, or if the accuracy degrades significantly.

### Mechanism 3
- Claim: Quantization improves latency by reducing the precision of weights and activations, which reduces memory usage and computational cost.
- Mechanism: Weights are converted from float32 to int8, reducing the memory required per weight from 4 bytes to 1 byte. This also reduces the computational cost of operations on these weights.
- Core assumption: The reduced precision does not significantly impact the accuracy, and the hardware can efficiently execute operations on the quantized data.
- Evidence anchors:
  - [abstract]: "quantization and layer fusion further reduce latency by 7% without worsening accuracy"
  - [section]: "The int8 quantization transfers the data type of all weights of the model from float32 to int8. This reduces the memory requirement per weight from 4 Bytes to 1 Byte. The activations remain unaffected by the applied quantization. The reduced bit size also reduces the effort required for the corresponding computing operations on the hardware. As a result, quantization usually leads to an additional inference speedup, which in the case of this work is approximately 7%"
  - [corpus]: No direct evidence, but well-established technique in deep learning optimization
- Break condition: If the reduced precision significantly impacts accuracy, or if the hardware cannot efficiently execute operations on the quantized data.

## Foundational Learning

- Concept: Speaker diarization
  - Why needed here: Understanding the task that the pipeline is performing is crucial for evaluating the impact of optimization techniques on accuracy.
  - Quick check question: What is the goal of speaker diarization, and why is low latency important in some scenarios?

- Concept: Knowledge distillation
  - Why needed here: This is a key technique used in the paper to reduce model size and improve latency. Understanding how it works is essential for evaluating its effectiveness.
  - Quick check question: How does knowledge distillation work, and what are the potential trade-offs between accuracy and latency?

- Concept: Model quantization
  - Why needed here: This is another important optimization technique used in the paper. Understanding how it works and its impact on accuracy and latency is crucial.
  - Quick check question: What is model quantization, and how does reducing the precision of weights and activations affect model performance?

## Architecture Onboarding

- Component map: Audio input → pyannote/segmentation → pyannote/embedding → Speaker labels output
- Critical path: pyannote/embedding is the bottleneck, so optimizing it is critical
- Design tradeoffs: Accuracy vs. latency (some techniques improve latency but may reduce accuracy), model size vs. latency (smaller models may be faster but less accurate), hardware compatibility (some optimizations may not be supported on all hardware)
- Failure signatures: Significant increase in DER (indicates optimization technique negatively impacting accuracy), no improvement in latency (indicates optimization technique not effective), increase in model size (indicates optimization technique not reducing model size as expected)
- First 3 experiments:
  1. Apply knowledge distillation to pyannote/embedding and measure the impact on latency and DER
  2. Apply layer fusion to the distilled model and measure the impact on latency
  3. Apply quantization to the fused model and measure the impact on latency and DER

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does converting the pyannote/embedding model to ONNX format increase latency by 40% instead of improving it?
- Basis in paper: [explicit] The paper notes that converting to ONNX format significantly increases latency, contrary to expectations, and suggests this may be due to the DIART pipeline not being able to utilize hardware optimizations with the ONNX model from PyTorch's onnx.export function.
- Why unresolved: The paper acknowledges the unexpected increase in latency but does not provide a detailed analysis or explanation for why the ONNX conversion did not leverage hardware optimizations as anticipated.
- What evidence would resolve it: Further analysis of the DIART pipeline's compatibility with ONNX models and the specific hardware optimizations that are not being utilized would clarify the reasons behind the increased latency.

### Open Question 2
- Question: Can the accuracy of the knowledge-distilled model be improved to match the baseline model through additional training with more datasets?
- Basis in paper: [inferred] The paper suggests that the 5% increase in DER due to knowledge distillation might be reduced by more extensive training with additional datasets, implying that further training could potentially improve accuracy.
- Why unresolved: The paper does not conduct experiments with additional datasets or extended training to verify if the accuracy can be brought closer to the baseline model's performance.
- What evidence would resolve it: Conducting experiments with additional training data and evaluating the DER of the knowledge-distilled model would provide evidence on whether its accuracy can be improved to match the baseline.

### Open Question 3
- Question: Why does structured and unstructured pruning not lead to any significant improvement in latency, memory requirements, or accuracy?
- Basis in paper: [explicit] The paper observes that neither structured nor unstructured pruning has a significant impact on latency, memory requirements, or accuracy, despite increasing model sparsity.
- Why unresolved: The paper does not explore the underlying reasons for the lack of impact from pruning methods, such as whether the pruned parameters are critical to model performance or if the hardware does not efficiently handle sparse models.
- What evidence would resolve it: Analyzing the pruned model's structure and performance characteristics, along with testing on different hardware that supports sparse computations, would provide insights into why pruning does not yield the expected improvements.

## Limitations
- Evaluation limited to single hardware platform (Intel Xeon Gold 5215 CPU)
- Test corpus restricted to only four audio files totaling ~20 minutes from VoxConverse
- Knowledge distillation implementation lacks detailed hyperparameter specifications
- Pruning methods' effectiveness could not be conclusively determined due to hardware limitations

## Confidence
- Knowledge distillation results: High (latency reduction of 10%, model size reduction of 18%, but 5% increase in DER)
- Layer fusion and quantization results: High (7% latency reduction without accuracy loss)
- Pruning results: Medium (no latency improvement, but may be due to hardware constraints)
- ONNX conversion results: Low (unexpected 40% latency increase, lack of detailed analysis)

## Next Checks
1. **Hardware Platform Verification**: Replicate the optimization experiments across multiple hardware configurations (different CPU architectures and GPUs) to confirm that the observed latency improvements are not platform-specific and to investigate why ONNX conversion increased latency on the tested hardware.

2. **Extended Corpus Evaluation**: Evaluate the optimized models on a larger, more diverse test set spanning multiple speaker diarization datasets to verify that the latency improvements and accuracy trade-offs hold across different acoustic conditions and speaker scenarios.

3. **Pruning Configuration Analysis**: Systematically test different pruning strategies (varying pruning ratios, targeted modules, and fine-tuning procedures) to determine whether the lack of latency improvement is due to the pruning approach or implementation details, and identify optimal pruning configurations for this specific model architecture.