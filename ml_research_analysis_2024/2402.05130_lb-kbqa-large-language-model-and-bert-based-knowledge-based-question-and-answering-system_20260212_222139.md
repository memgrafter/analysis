---
ver: rpa2
title: 'LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and
  Answering System'
arxiv_id: '2402.05130'
source_url: https://arxiv.org/abs/2402.05130
tags:
- language
- intent
- module
- question
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LB-KBQA, a Knowledge-Based Question Answering
  system that combines large language models (LLMs) and BERT to address the problem
  of unseen intents in KBQA. The system integrates rule-based, BERT-based, and LLM-based
  components for intent recognition, along with adaptive learning to handle linguistic
  diversity and newly appeared intents.
---

# LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System

## Quick Facts
- arXiv ID: 2402.05130
- Source URL: https://arxiv.org/abs/2402.05130
- Reference count: 24
- Primary result: 90% accuracy on financial domain KBQA with hybrid LLM+BERT system

## Executive Summary
LB-KBQA is a Knowledge-Based Question Answering system that addresses the challenge of unseen intents by combining large language models (LLMs) and BERT in a hierarchical architecture. The system integrates rule-based, BERT-based, and LLM-based components for intent recognition, along with adaptive learning to handle linguistic diversity and newly appeared intents. Experiments on a financial domain dataset demonstrate that the hybrid approach significantly outperforms individual components, achieving 90% accuracy compared to 60% without BERT and 85% without adaptive learning.

## Method Summary
The system uses a five-component architecture: language preprocessing, intent recognition (hierarchical fallback from rule-based to BERT to LLM), response generation, adaptive learning, and query library extension. The intent recognition module first attempts rule-based matching, then uses BERT-based semantic similarity if rule-based fails, and finally employs an LLM as a fallback for cases below a similarity threshold. The adaptive learning component engages in multi-turn dialogue with users to confirm and correct intent recognition errors, continuously expanding the intent library. The system was evaluated on a financial domain dataset using a knowledge graph built with neo4j and Tushare financial data.

## Key Results
- Achieves 90% accuracy on test set of 100 samples (50 simple, 50 complex questions)
- Removing BERT component reduces accuracy to 60%, demonstrating its critical role in semantic similarity matching
- Removing adaptive learning module reduces accuracy to 85%, showing its contribution to handling unseen intents
- Hierarchical fallback architecture outperforms individual component approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LB-KBQA's intent recognition succeeds by fusing three complementary models to handle unseen intents.
- Mechanism: The system uses a rule-based model for fast domain-specific matching, a BERT-based model for high-dimensional semantic representation, and an LLM-based model as a fallback for handling unseen or linguistically diverse intents. The three models work in sequence: rule-based first, then BERT-based if rule-based fails, and finally LLM-based if BERT-based similarity falls below a threshold.
- Core assumption: Different types of unseen intents (linguistic diversity vs. completely new intents) require different modeling approaches, and a hierarchical fallback structure can capture all cases.
- Evidence anchors:
  - [abstract] "The system integrates rule-based, BERT-based, and LLM-based components for intent recognition"
  - [section] "The rule-based model has the advantage of being fast and scenario-oriented, but it cannot handle queries with complex semantics... if the rule-based model fails to make a prediction, the query is passed to the question embedding component... However... a large language model is employed as a fallback approach"
- Break condition: If the threshold for LLM activation is set too high, some unseen intents may never trigger the LLM fallback, leading to missed cases.

### Mechanism 2
- Claim: Adaptive learning enables the system to continuously improve its intent recognition capability for unseen intents.
- Mechanism: When users indicate dissatisfaction with answers, the system engages in multi-turn dialogue with the LLM to confirm whether the issue is intent-related and what the correct intent should be. The confirmed intent is then added to the intent library along with the question vector, expanding the system's knowledge base over time.
- Core assumption: User feedback is reliable and can be used to accurately identify and correct intent recognition errors.
- Evidence anchors:
  - [section] "the large language model will engage in a multi-turn dialogue with the user... to confirm... what the user believes the correct intent is. Once the user provides the correct intent, the intent label is updated together with the question vector"
  - [section] "the system can achieve rapid adaptation to unknown intents"
- Break condition: If users frequently provide incorrect feedback or if the system cannot distinguish between intent recognition failures and answer generation failures, the adaptive learning may introduce incorrect intents.

### Mechanism 3
- Claim: BERT-based semantic representation is crucial for discovering unseen intents through vector similarity matching.
- Mechanism: The BERT model converts questions into context vectors that capture high-dimensional semantic information. These vectors are compared with a similarity question vector base using cosine similarity to find the most relevant intent. This approach can identify semantically similar questions even when they use different wording.
- Core assumption: BERT's contextual embeddings can capture semantic similarity between questions even when surface forms differ significantly.
- Evidence anchors:
  - [section] "the question embedding component, which extracts high-dimensional semantic information using the BERT model. The input query is converted to a context vector for further similarity calculation"
  - [section] "Our evaluation results demonstrate that the BERT-based question representation model and the similar question vector library are crucial for discovering unseen intents. The experimental results show that the accuracy decreased by 0.3 after removing the BERT module"
- Break condition: If BERT embeddings fail to capture domain-specific semantic nuances, the similarity matching may miss relevant intents.

## Foundational Learning

- Concept: Cosine similarity for vector comparison
  - Why needed here: The system uses cosine similarity to measure how close a new question's BERT embedding is to existing question vectors in the intent library
  - Quick check question: If two vectors have cosine similarity of 0.9, are they considered more or less similar than vectors with cosine similarity of 0.5?

- Concept: Vector space embeddings for semantic representation
  - Why needed here: BERT converts natural language questions into fixed-dimensional vectors that capture semantic meaning, enabling the system to compare questions based on meaning rather than exact wording
  - Quick check question: What property of BERT embeddings allows semantically similar questions to have similar vector representations even when they use different words?

- Concept: Multi-turn dialogue systems
  - Why needed here: The adaptive learning module uses multi-turn dialogue with users to confirm and correct intent recognition errors, requiring understanding of conversational flow and context
  - Quick check question: In a multi-turn dialogue for intent correction, what information must be preserved between turns to maintain context?

## Architecture Onboarding

- Component map: Language Preprocessing -> Intent Recognition (Rule-based -> BERT-based -> LLM fallback) -> Response Generation -> User Answer -> Adaptive Learning (if needed) -> Query Library Extension
- Critical path: User question → Language Preprocessing → Intent Recognition → Knowledge Graph Query → Response Generation → User answer. If answer is unsatisfactory, Adaptive Learning may modify the Intent Recognition component for future queries
- Design tradeoffs: The hierarchical fallback structure (rule → BERT → LLM) trades off speed for accuracy, with rule-based being fastest but least flexible, BERT being moderate speed and flexibility, and LLM being slowest but most capable of handling unseen cases
- Failure signatures: If accuracy drops significantly when removing BERT, this indicates reliance on semantic similarity matching. If adaptive learning module removal causes only minor accuracy decrease, it suggests limited user feedback utilization
- First 3 experiments:
  1. Test accuracy with each component removed individually to identify critical components
  2. Measure threshold sensitivity by varying the cosine similarity threshold for LLM fallback activation
  3. Evaluate adaptive learning effectiveness by simulating user feedback and measuring intent library growth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LB-KBQA scale with increasing domain complexity and knowledge base size?
- Basis in paper: [inferred] The paper mentions using a financial domain dataset and a knowledge graph, but does not explore scalability to larger or more complex domains.
- Why unresolved: The current experiments are limited to a single financial domain with 100 samples. The paper does not investigate how the system would perform with larger knowledge bases or more diverse domains.
- What evidence would resolve it: Experiments evaluating LB-KBQA on multiple domains with varying sizes and complexities of knowledge bases, measuring performance metrics such as accuracy, response time, and resource utilization.

### Open Question 2
- Question: What is the impact of different pre-trained language models on the performance of the intent recognition module?
- Basis in paper: [explicit] The paper mentions using BERT and a large language model (LLM) but does not compare different LLM architectures or pre-trained models.
- Why unresolved: The choice of LLM can significantly affect the system's ability to recognize unseen intents and generate appropriate responses. However, the paper does not explore the impact of different LLM choices on overall performance.
- What evidence would resolve it: Comparative experiments using various pre-trained language models (e.g., GPT-3, GPT-4, T5) in the intent recognition module, measuring their impact on accuracy, handling of unseen intents, and response quality.

### Open Question 3
- Question: How does the adaptive learning module perform in real-world scenarios with diverse user feedback and evolving intents?
- Basis in paper: [explicit] The paper describes the adaptive learning module and its ability to update the intent library based on user feedback, but does not evaluate its performance in dynamic, real-world settings.
- Why unresolved: The effectiveness of the adaptive learning module depends on its ability to handle diverse user feedback and adapt to evolving intents over time. The paper's evaluation is limited to a controlled test set.
- What evidence would resolve it: Long-term deployment of LB-KBQA in real-world applications, collecting user feedback and measuring the system's ability to adapt to new intents, maintain accuracy, and improve over time through continuous learning.

## Limitations
- Evaluation based on single financial-domain dataset with 100 test samples, limiting generalizability
- Limited implementation details for key components like rule-based model architecture and exact LLM prompts
- Adaptive learning relies on user feedback quality, which isn't fully characterized
- 90% accuracy claim comes from relatively small test set that may not capture edge cases

## Confidence
- **High confidence**: Hierarchical fallback architecture effectiveness, supported by ablation experiments
- **Medium confidence**: Adaptive learning mechanism effectiveness, dependent on user feedback quality
- **Low confidence**: Generalizability across domains, given single financial dataset evaluation

## Next Checks
1. **Ablation testing on real user data**: Deploy the system in a live environment with diverse user queries to measure actual performance degradation when removing BERT vs. LLM components
2. **Threshold sensitivity analysis**: Systematically vary the cosine similarity threshold for LLM fallback activation and measure accuracy tradeoffs across different query complexity levels
3. **User feedback reliability test**: Conduct experiments where simulated users intentionally provide incorrect intent corrections to measure how the adaptive learning module handles noisy feedback