---
ver: rpa2
title: Reduced storage direct tensor ring decomposition for convolutional neural networks
  compression
arxiv_id: '2405.10802'
source_url: https://arxiv.org/abs/2405.10802
tags:
- networks
- neural
- tensor
- pruning
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel low-rank compression method for convolutional
  neural networks (CNNs) based on reduced storage direct tensor ring decomposition
  (RSDTR). The method offers higher circular mode permutation flexibility and achieves
  large parameter and FLOPS compression rates while preserving classification accuracy.
---

# Reduced storage direct tensor ring decomposition for convolutional neural networks compression

## Quick Facts
- arXiv ID: 2405.10802
- Source URL: https://arxiv.org/abs/2405.10802
- Authors: Mateusz Gabor; Rafał Zdunek
- Reference count: 29
- Key outcome: Novel low-rank compression method (RSDTR) for CNNs achieving high parameter and FLOPS compression while preserving accuracy

## Executive Summary
This paper introduces Reduced Storage Direct Tensor Ring (RSDTR) decomposition, a novel low-rank compression method for convolutional neural networks. RSDTR decomposes each convolutional kernel into smaller factors and uses an exhaustive search procedure to find the optimal circular mode-permutation and divisor for each layer. The method offers higher circular mode permutation flexibility compared to existing approaches and achieves large parameter and FLOPS compression rates while preserving classification accuracy.

The experiments on CIFAR-10 and ImageNet datasets demonstrate that RSDTR outperforms state-of-the-art compression methods in terms of parameter compression ratio (PCR), FLOPS compression ratio (FCR), and classification accuracy. For example, on ResNet-20, RSDTR achieves a PCR of 3.5 and an FCR of 1.19 while maintaining high accuracy, surpassing other methods including tensorized TR approaches. The method is efficient, with compression times under 97 seconds on an Intel Core i9-12900H CPU, and provides strong compression ratios for both parameters and FLOPS across various CNN architectures.

## Method Summary
RSDTR compresses CNNs by decomposing each convolutional kernel into smaller factors using tensor ring decomposition. The method uses an exhaustive search procedure to find the optimal circular mode-permutation and divisor for each layer, replacing each convolutional layer with a four-sublayer structure of contractions and 1D convolutions. The compressed networks can be fine-tuned from decomposed factors rather than training from scratch, as was done in previous TR approaches. The decomposition is controlled by a prescribed relative error parameter that determines the rank of the TR decomposition.

## Key Results
- RSDTR achieves parameter compression ratios up to 3.5x and FLOPS compression ratios up to 1.19x on ResNet-20
- Outperforms state-of-the-art compression methods including tensorized TR approaches on both CIFAR-10 and ImageNet datasets
- Maintains high classification accuracy with minimal drop (0.2-1.1%) across various CNN architectures
- Compression process is efficient, completing in under 97 seconds on an Intel Core i9-12900H CPU

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RSDTR achieves lower FLOPS than tensorized TR approaches while maintaining or improving accuracy.
- Mechanism: By using reduced storage direct tensor ring decomposition, RSDTR finds the optimal circular mode-permutation and divisor for each layer, resulting in fewer FLOPS compared to tensorized TR methods that often increase FLOPS despite parameter reduction.
- Core assumption: The optimal permutation and rank selection in RSDTR leads to a more efficient computational structure than the fixed, hand-designed structures in tensorized TR.
- Evidence anchors:
  - [abstract] "large parameter and FLOPS compression rates, while preserving a good classification accuracy"
  - [section 4] "Comparing it with the FLOPS complexity of the tensorized TR approach... we can define the following FLOPS complexity ratio: ρ = RDI1I2 + RD ˜I1I2 R(J1J2 + C + T + O1O2) + D2I1I2"
  - [corpus] Weak evidence; no direct comparison found in corpus.
- Break Condition: If the optimal permutation and rank selection does not consistently lead to lower FLOPS, or if the overhead of finding these optimal parameters negates the FLOPS savings.

### Mechanism 2
- Claim: RSDTR enables fine-tuning from decomposed factors, unlike tensorized TR methods which require training from scratch.
- Mechanism: RSDTR uses the TR decomposition algorithm to decompose the pre-trained convolutional kernels, allowing the compressed network to be fine-tuned from these decomposed factors rather than requiring complete retraining.
- Core assumption: The decomposed factors from TR decomposition serve as a good initialization for fine-tuning, preserving more information than training from scratch.
- Evidence anchors:
  - [section 1] "compressed networks can be fine-tuned from decomposed factors instead of training them from scratch, as was done in the previous TR approaches"
  - [corpus] Weak evidence; no direct evidence in corpus.
- Break Condition: If the decomposed factors do not provide a good initialization, leading to poor performance during fine-tuning compared to training from scratch.

### Mechanism 3
- Claim: RSDTR achieves higher parameter compression ratios (PCR) than pruning methods while maintaining or improving accuracy.
- Mechanism: By decomposing the convolutional kernel into smaller factors using TR decomposition, RSDTR reduces the number of parameters more effectively than pruning methods, which remove connections but may not achieve the same level of compression.
- Core assumption: The TR decomposition can approximate the original convolutional kernel with fewer parameters while preserving essential features.
- Evidence anchors:
  - [abstract] "large parameter and FLOPS compression rates, while preserving a good classification accuracy"
  - [section 5.2.1] "RSDTR outperforms all compared pruning approaches in terms of PCR, FCR, and the drop in classification accuracy"
  - [corpus] Weak evidence; no direct comparison found in corpus.
- Break Condition: If the TR decomposition cannot effectively approximate the original kernel, leading to significant accuracy loss or insufficient parameter reduction.

## Foundational Learning

- Tensor Ring (TR) Decomposition:
  - Why needed here: TR decomposition is the core mathematical technique used by RSDTR to decompose convolutional kernels into smaller, more efficient factors.
  - Quick check question: What is the fundamental difference between TR decomposition and other tensor decomposition methods like CP or Tucker?

- Circular Mode Permutation:
  - Why needed here: RSDTR leverages the circular dimensional permutation invariance property of TR to find the optimal permutation that minimizes storage complexity.
  - Quick check question: How does circular mode permutation affect the storage complexity of TR-decomposed tensors?

- 2D Convolution and Its Implementation in PyTorch:
  - Why needed here: Understanding how 2D convolution is implemented in PyTorch is crucial for implementing the RSDTR algorithm, which replaces standard convolutions with a pipeline of contractions and 1D convolutions.
  - Quick check question: How does the Conv3D class in PyTorch differ from the Conv2D class, and why is it used in RSDTR?

## Architecture Onboarding

- Component map:
  - TR decomposition algorithm -> Exhaustive search procedure -> Four-sublayer structure -> Fine-tuning

- Critical path:
  1. Decompose each convolutional kernel using RSDTR
  2. Replace the original convolutional layer with the four-sublayer structure
  3. Fine-tune the compressed network

- Design tradeoffs:
  - Higher compression ratio vs. potential accuracy loss: RSDTR aims for high compression while maintaining accuracy, but there may be a tradeoff.
  - Computational overhead of exhaustive search: Finding the optimal permutation and divisor adds computational cost during compression.

- Failure signatures:
  - Accuracy degradation: If the TR decomposition does not effectively approximate the original kernel.
  - Increased FLOPS: If the optimal permutation and rank selection do not lead to lower FLOPS.
  - Long compression times: If the exhaustive search procedure is too computationally expensive.

- First 3 experiments:
  1. Compress a simple CNN (e.g., ResNet-20) on CIFAR-10 using RSDTR and compare the compression ratio and accuracy to the baseline.
  2. Vary the prescribed relative error (ϵp) in the TR algorithm and observe its effect on compression ratio and accuracy.
  3. Compare the performance of RSDTR to other compression methods (e.g., pruning, tensorized TR) on a larger CNN (e.g., ResNet-18) on ImageNet.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the discussion and future work section, several areas for further investigation are implied:
- Scalability of RSDTR to deeper neural networks beyond ResNet-34
- Combination of RSDTR with quantization techniques for even greater model compression
- Comparison with emerging tensor network methods like tree tensor networks (TTN) or matrix product operators (MPO)

## Limitations

- The exhaustive search procedure for optimal circular mode-permutation and divisor may become computationally prohibitive for larger networks or higher-dimensional data.
- The method's performance on tasks beyond image classification (e.g., object detection, segmentation) remains unexplored.
- The paper lacks comparison with recent state-of-the-art compression methods beyond tensorized TR approaches, such as knowledge distillation or quantization-based methods.

## Confidence

- **High Confidence**: The claim that RSDTR achieves higher parameter compression ratios (PCR) than pruning methods while maintaining or improving accuracy is supported by direct experimental comparisons in section 5.2.1.
- **Medium Confidence**: The claim that RSDTR achieves lower FLOPS than tensorized TR approaches while maintaining or improving accuracy is based on theoretical analysis but lacks direct experimental comparisons.
- **Low Confidence**: The claim that RSDTR enables fine-tuning from decomposed factors, unlike tensorized TR methods which require training from scratch, is stated but lacks supporting evidence or experimental validation.

## Next Checks

1. Implement both RSDTR and a tensorized TR compression method on the same CNN architecture and dataset, then measure and compare the FLOPS of the compressed models to validate the claim of lower FLOPS complexity in RSDTR.

2. Train two versions of a compressed CNN using RSDTR: one fine-tuned from decomposed factors and another trained from scratch using the same decomposed factors as initialization. Compare their performance to isolate the impact of fine-tuning versus from-scratch training.

3. Apply RSDTR to compress a larger CNN (e.g., ResNet-50) on ImageNet and measure the compression time and computational overhead of the exhaustive search procedure. Analyze how the search time scales with network depth and input dimensionality to assess the method's practicality for real-world applications.