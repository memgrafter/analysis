---
ver: rpa2
title: 'RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large
  Language Models'
arxiv_id: '2409.19886'
source_url: https://arxiv.org/abs/2409.19886
tags:
- llms
- routerdc
- accuracy
- router
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of efficiently assembling multiple
  large language models (LLMs) by learning a router to select the most suitable LLM
  for each query. The authors propose RouterDC, a query-based router trained using
  dual contrastive learning with two novel contrastive losses: sample-LLM contrastive
  loss to pull query embeddings close to top-performing LLMs while pushing away from
  bottom-performing ones, and sample-sample contrastive loss to improve training stability
  by encouraging similar queries to have similar embeddings.'
---

# RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models

## Quick Facts
- arXiv ID: 2409.19886
- Source URL: https://arxiv.org/abs/2409.19886
- Authors: Shuhao Chen; Weisen Jiang; Baijiong Lin; James T. Kwok; Yu Zhang
- Reference count: 40
- Primary result: RouterDC achieves 3.98% better accuracy than individual top LLMs on in-distribution tasks and 1.90% on out-of-distribution tasks

## Executive Summary
RouterDC addresses the challenge of efficiently assembling multiple large language models (LLMs) by learning a query-based router that selects the most suitable LLM for each input. The method uses dual contrastive learning with two novel losses: sample-LLM contrastive loss to align query embeddings with high-performing LLMs while pushing away from low-performers, and sample-sample contrastive loss to improve training stability by encouraging similar queries to have similar embeddings. Experimental results show RouterDC outperforms individual top-performing LLMs by 3.98% on in-distribution tasks and 1.90% on out-of-distribution tasks while being 6× faster than ensemble methods.

## Method Summary
RouterDC trains a small encoder (mDeBERTaV3-base) to generate query embeddings that determine routing probabilities through cosine similarity with learnable LLM embeddings. The training uses dual contrastive learning: sample-LLM contrastive loss pulls query embeddings toward top-performing LLMs and away from bottom-performing ones, while sample-sample contrastive loss clusters similar queries together. The router is trained on a dataset of queries and answers, using a scoring function to evaluate LLM outputs and identify top-k and bottom-k performers for each query. RouterDC is parameter-efficient (less than 100M parameters) and computation-efficient, requiring only one LLM call during inference.

## Key Results
- RouterDC achieves 3.98% better accuracy than individual top-performing LLMs on in-distribution tasks
- RouterDC achieves 1.90% better accuracy than individual top-performing LLMs on out-of-distribution tasks
- RouterDC is 6× faster than ensemble voting methods while being parameter-efficient with less than 100M parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sample-LLM contrastive loss effectively pulls query embeddings toward high-performing LLMs while pushing them away from low-performing ones
- Mechanism: For each query, the loss computes a softmax over cosine similarities between the query embedding and embeddings of top-performing LLMs (positive set) versus bottom-performing LLMs (negative set). The router parameters are updated to increase similarity with positives and decrease with negatives.
- Core assumption: The scoring function accurately reflects LLM performance on each query, and there is sufficient signal in the top-k/bottom-k separation to guide learning.
- Evidence anchors:
  - [abstract]: "Experimental results show that RouterDC achieves state-of-the-art performance, outperforming individual top-performing LLMs by 3.98% on in-distribution tasks and 1.90% on out-of-distribution tasks"
  - [section]: "we propose a sample-LLM contrastive loss to pull the query embedding (extracted by the encoder) close to the embeddings of top LLMs while pushing far away from the embeddings of bottom-performing LLMs"
  - [corpus]: Weak - no direct corpus evidence found for this specific contrastive design
- Break condition: If the scoring function is noisy or if multiple LLMs have similar scores that are not well-separated, the contrastive signal becomes weak and the router cannot reliably distinguish which LLMs to route to.

### Mechanism 2
- Claim: The sample-sample contrastive loss stabilizes training by encouraging the encoder to generate similar embeddings for similar queries
- Mechanism: Queries are clustered into groups; for each query, the loss pulls its embedding closer to a randomly chosen in-group query while pushing it away from out-group queries. This encourages the encoder to map semantically similar queries to nearby regions in embedding space.
- Core assumption: Unsupervised clustering of query embeddings produces meaningful groups that capture semantic similarity relevant to LLM routing decisions.
- Evidence anchors:
  - [abstract]: "we cluster all the training queries into multiple groups and design a sample-sample contrastive loss to maximize the similarity between queries in the same group"
  - [section]: "we empirically find that training the router by minimizing the sample-LLM contrastive loss alone is not stable as similar queries can have dissimilar embeddings and be assigned to different LLMs"
  - [corpus]: Weak - no direct corpus evidence found for this specific clustering+contrastive approach
- Break condition: If clustering is poor (e.g., overlapping clusters, incorrect number of clusters), the contrastive loss may push apart queries that should be routed similarly, degrading router performance.

### Mechanism 3
- Claim: The router's parameter efficiency (less than 100M parameters) and inference efficiency (6× faster than ensemble voting) make it practical for real-world deployment while maintaining high accuracy
- Mechanism: The router uses a small language model encoder (mDeBERTaV3-base with 86M parameters) and learnable LLM embeddings. During inference, only the selected LLM is called once, avoiding the need to call all candidate LLMs.
- Core assumption: A small encoder has sufficient capacity to distinguish between LLMs' strengths across diverse tasks, and the learned LLM embeddings capture each LLM's overall performance profile.
- Evidence anchors:
  - [abstract]: "RouterDC is parameter-efficient (has less than 100M parameters) and computation-efficient (without backpropagating the gradients through LLMs) in training. Moreover, RouterDC is also efficient in inference (6× faster than V oting)"
  - [section]: "RouterDC contains less than 100M parameters (that is, the encoder model E(x; w) is small and the number of parameters of LLM embeddings{k1, . . . ,kT } are negligible), thus it is parameter-efficient"
  - [corpus]: Weak - no direct corpus evidence found for this specific efficiency claim
- Break condition: If the small encoder lacks sufficient capacity to capture task-LLM relationships, or if the LLM embeddings cannot adequately represent the diverse strengths of candidate LLMs, the router's accuracy will degrade despite its efficiency.

## Foundational Learning

- Concept: Contrastive learning and its application to representation learning
  - Why needed here: The router's training relies on pulling together similar items (query-LLM pairs, query-query pairs) and pushing apart dissimilar ones to learn effective routing embeddings
  - Quick check question: What is the key difference between contrastive learning and traditional classification approaches in representation learning?

- Concept: Embedding similarity measures (cosine similarity) and their role in routing decisions
  - Why needed here: The router uses cosine similarity between query embeddings and LLM embeddings to determine routing probabilities
  - Quick check question: How does cosine similarity differ from Euclidean distance, and why might it be preferred for high-dimensional embeddings?

- Concept: Kullback-Leibler divergence and its use as a loss function for probability distribution alignment
  - Why needed here: Understanding why the authors rejected KL divergence (used in ZOOTER) in favor of contrastive losses requires knowing its limitations for this routing problem
  - Quick check question: In what scenarios might KL divergence be a poor choice for training a router compared to contrastive approaches?

## Architecture Onboarding

- Component map: Query -> Encoder -> Query embedding -> Cosine similarity with LLM embeddings -> Softmax -> Routing probabilities -> Selected LLM

- Critical path:
  1. Query → Encoder → Query embedding
  2. Query embedding → Cosine similarity with all LLM embeddings
  3. Similarities → Softmax → Routing probabilities
  4. Highest probability LLM selected for inference

- Design tradeoffs:
  - Small encoder (86M params) vs. larger models: Better efficiency but potentially less expressive
  - Number of positive LLMs (K+) vs. negative LLMs (K-): More positives capture uncertainty but may weaken contrastive signal
  - Clustering granularity (N) vs. training stability: More clusters improve semantic separation but may overfit

- Failure signatures:
  - Router consistently selects the same LLM regardless of query content (encoder not learning)
  - Router's accuracy matches random selection (loss not optimizing correctly)
  - Router performs well on training data but poorly on test data (overfitting)
  - Router's accuracy degrades when any single LLM is removed (insufficient redundancy in positive set)

- First 3 experiments:
  1. Train RouterDC with only sample-LLM contrastive loss (λ=0) and observe training stability and accuracy compared to full dual-contrastive approach
  2. Vary K+ (number of positive LLMs) from 1 to 5 and measure impact on accuracy and routing consistency
  3. Compare RouterDC's routing decisions against ground-truth LLM performance on a held-out validation set to verify correct routing behavior

## Open Questions the Paper Calls Out
- How does RouterDC's performance scale when using much larger LLMs (e.g., 70B+ parameters) as candidates?
- Can RouterDC effectively handle chat contexts and conversational history rather than just individual queries?
- How sensitive is RouterDC to hyperparameter choices beyond the ranges explored in the sensitivity analysis?

## Limitations
- Limited empirical validation of contrastive design choices without ablation studies examining individual component impacts
- Weak evidence for sample-sample contrastive necessity based on a single empirical observation without rigorous quantitative comparison
- Unclear generalizability beyond the specific LLM set used in experiments (seven Mistral and Llama models)

## Confidence
- High confidence in parameter efficiency claims: The architectural specification directly supports the stated parameter count and computational advantages
- Medium confidence in accuracy improvements: Reported gains depend on scoring function quality and may not generalize to different LLM sets
- Low confidence in training stability claims: The assertion that dual contrastive learning is necessary is based on limited empirical evidence

## Next Checks
1. Ablation study on contrastive components: Train RouterDC with (a) only sample-LLM contrastive loss, (b) only sample-sample contrastive loss, and (c) neither, comparing training stability and final accuracy
2. Sensitivity analysis of scoring function: Systematically vary the scoring function's tolerance for correct answers and measure the impact on router performance and contrastive signal quality
3. Cross-architecture generalization test: Replace the seven Mistral/Llama models with a different set of candidate LLMs and evaluate whether RouterDC maintains comparable accuracy improvements without retraining the encoder