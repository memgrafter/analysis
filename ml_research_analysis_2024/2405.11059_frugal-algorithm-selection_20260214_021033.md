---
ver: rpa2
title: Frugal Algorithm Selection
arxiv_id: '2405.11059'
source_url: https://arxiv.org/abs/2405.11059
tags:
- algorithm
- learning
- selection
- instance
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of reducing the computational
  cost of training algorithm selection models by minimizing the number of algorithm-instance
  runs needed for training data collection. The authors propose three key strategies:
  uncertainty-based instance selection, augmenting predictors with timeout predictors,
  and using dynamic timeouts that increase incrementally.'
---

# Frugal Algorithm Selection

## Quick Facts
- arXiv ID: 2405.11059
- Source URL: https://arxiv.org/abs/2405.11059
- Reference count: 40
- One-line primary result: Achieves same predictive performance as passive learning while requiring only 10-60% of the labelling cost

## Executive Summary
This paper addresses the computational cost of training algorithm selection models by minimizing the number of algorithm-instance runs needed for training data collection. The authors propose three key strategies: uncertainty-based instance selection, augmenting predictors with timeout predictors, and using dynamic timeouts that increase incrementally. These strategies are combined in eight different configurations and evaluated across six ASLib datasets. Results show that frugal algorithm selection can achieve the same predictive performance as passive learning while requiring only 10-60% of the labelling cost. Dynamic timeouts and timeout predictors provide the most significant cost reductions, with uncertainty-based instance selection showing no clear advantage over random selection. The best-performing configuration combines timeout predictors with dynamic timeouts.

## Method Summary
The frugal algorithm selection approach combines uncertainty-based instance selection, timeout predictors, and dynamic timeouts to reduce training costs. The method uses binary random forest classifiers (100 estimators, Gini impurity, max depth 2^31, min samples split 2, bootstrap=True) trained on incrementally selected instances. Eight configurations are tested combining uncertainty/random instance selection, timeout predictors, and fixed/dynamic timeouts. The approach is evaluated using 10-fold cross-validation with 5 runs per fold on six ASLib datasets, measuring PAR10 performance ratio compared to passive learning baseline.

## Key Results
- Achieved same predictive performance as passive learning while requiring only 10-60% of labelling cost
- Dynamic timeouts and timeout predictors provided the most significant cost reductions
- Uncertainty-based instance selection showed no clear advantage over random selection
- Best-performing configuration combined timeout predictors with dynamic timeouts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uncertainty-based instance selection reduces training cost by prioritizing instances where the model is least confident, which are more likely to improve predictive performance when labeled.
- Mechanism: The algorithm trains an initial model on a small random subset, then queries instances where the confidence is lowest, iteratively refining the model with potentially informative examples.
- Core assumption: The informativeness of an instance correlates with its prediction uncertainty, and labeling uncertain instances yields greater improvements in model accuracy.
- Evidence anchors:
  - [abstract] "We approach this problem in three ways: using active learning to decide based on prediction uncertainty"
  - [section] "Uncertainty-based querying is based on the premise that, by focusing on the data points where the model's predictions are least confident, the model is expected to learn from the most uncertain data points."

### Mechanism 2
- Claim: Timeout predictors enable the model to avoid expensive runs that are likely to time out, saving resources without losing useful information.
- Mechanism: Dedicated random forest classifiers predict whether an algorithm will time out on a given instance; the main voting mechanism excludes predicted timeouts from consideration, reducing wasted runtime.
- Core assumption: Predicting timeouts is a simpler learning task than full algorithm ranking, and timeout prediction accuracy is high enough to justify exclusion.
- Evidence anchors:
  - [abstract] "augmenting the algorithm predictors with a timeout predictor"
  - [section] "We enhance our base machine learning architecture of binary classifiers... with dedicated timeout predictors: additional random forest classifiers, one per algorithm, whose task is to predict whether an unseen instance will time out for a specific algorithm."

### Mechanism 3
- Claim: Dynamic timeouts reduce labeling cost by starting with short cutoffs and only extending them when model performance plateaus.
- Mechanism: The timeout begins small and is incrementally increased, allowing many cheap early queries and delaying expensive long runs until necessary.
- Core assumption: Many instance-algorithm pairs time out only at longer cutoffs; early short runs capture the majority of useful information.
- Evidence anchors:
  - [abstract] "collecting training data using a progressively increasing timeout"
  - [section] "The dynamic timeout strategy begins with an initially defined timeout period and incrementally increases it up to a maximum of one hour."

## Foundational Learning

- Concept: Active learning and uncertainty sampling
  - Why needed here: The method relies on querying instances based on model uncertainty rather than random selection to improve cost efficiency.
  - Quick check question: How does uncertainty sampling differ from random sampling in terms of selecting which instances to label next?

- Concept: Binary classification with pairwise voting for multi-class selection
  - Why needed here: The AS model uses a collection of binary classifiers to compare algorithm pairs, then aggregates votes to choose the best algorithm.
  - Quick check question: Why is a voting mechanism used instead of training a single multi-class classifier?

- Concept: Random forest hyperparameters and pruning strategies
  - Why needed here: The experiments fix random forest settings (e.g., 100 estimators, Gini impurity) and remove features with >20% missing values.
  - Quick check question: What is the purpose of removing features with more than 20% missing values before training?

## Architecture Onboarding

- Component map: Feature extraction -> Binary classifier pool -> Timeout predictor pool -> Instance selector -> Dynamic timeout controller -> Voting aggregator

- Critical path:
  1. Initialize with small random training set.
  2. Train binary classifiers and timeout predictors.
  3. Select next batch of instances via selector.
  4. Run selected algorithms on instances up to current timeout.
  5. Update models with new labels.
  6. Evaluate on validation set; if plateau, increase timeout.
  7. Repeat until budget exhausted or convergence.

- Design tradeoffs:
  - Uncertainty-based vs random selection: uncertainty gives better theoretical informativeness but costs more in feature extraction; random is cheaper but may query less useful instances.
  - Fixed vs dynamic timeout: dynamic saves early cost but adds complexity in plateau detection; fixed is simpler but may waste resources.
  - Timeout predictors vs no predictors: predictors reduce wasted time but require extra model training and may mispredict.

- Failure signatures:
  - Poor performance despite low labeling cost: uncertainty selection may not be more informative than random in this setting, or timeout predictors may exclude too many algorithms.
  - High variance in results across folds: insufficient training data or unstable classifier training; check random seed and data balance.
  - Runtime unexpectedly high: dynamic timeout increase too aggressive or plateau detection too lax.

- First 3 experiments:
  1. Compare uncertainty-based vs random instance selection with all other components fixed, measure runtime ratio and training cost.
  2. Test timeout predictors alone vs without, keeping instance selection and timeout fixed.
  3. Compare dynamic vs fixed timeout strategies, measuring how much labeling cost is saved and at what accuracy cost.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Uncertainty-based instance selection showed no clear advantage over random selection in experiments, contrary to theoretical expectations
- Effectiveness of timeout predictors depends on the distribution of timeout behaviors across instances
- Dynamic timeout strategy performance depends on specific plateau detection rules that are not fully specified

## Confidence
- Cost reduction claims (10-60% of passive learning): High
- Uncertainty sampling effectiveness: Medium
- Timeout predictor generalization: High
- Dynamic timeout reproducibility: Low

## Next Checks
1. Replicate uncertainty sampling advantage: Run a controlled experiment comparing uncertainty-based vs random instance selection on a synthetic dataset where the correlation between uncertainty and true informativeness is artificially strong, to verify if the method works as theoretically expected.

2. Stress-test timeout predictors: Evaluate timeout predictor performance when the timeout distribution is skewed (e.g., most instances time out at very short or very long cutoffs) to understand the method's robustness to different runtime characteristics.

3. Characterize plateau detection sensitivity: Systematically vary the performance plateau detection threshold in the dynamic timeout strategy and measure how it affects both cost savings and predictive accuracy, to identify optimal settings for different dataset characteristics.