---
ver: rpa2
title: LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet
arxiv_id: '2408.15221'
source_url: https://arxiv.org/abs/2408.15221
tags:
- human
- attacks
- arxiv
- harmful
- automated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Recent LLM defenses show strong robustness against single-turn
  automated attacks, but we find they are vulnerable to multi-turn human jailbreaks.
  Our human red teamers bypass defenses with over 70% attack success rate on HarmBench,
  significantly exceeding automated attacks.
---

# LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet

## Quick Facts
- arXiv ID: 2408.15221
- Source URL: https://arxiv.org/abs/2408.15221
- Authors: Nathaniel Li; Ziwen Han; Ian Steneker; Willow Primack; Riley Goodside; Hugh Zhang; Zifan Wang; Cristina Menghini; Summer Yue
- Reference count: 40
- Primary result: Human red teamers bypass LLM defenses with >70% success rate in multi-turn jailbreaks

## Executive Summary
This paper demonstrates that current LLM defenses, while robust against single-turn automated attacks, are vulnerable to multi-turn human jailbreaks. Human red teamers achieve over 70% attack success rate on HarmBench, significantly exceeding automated attacks that achieve less than 10% success. The study reveals that defenses relying on single-turn adversarial training and shallow refusal mechanisms fail when attackers engage across multiple conversation turns. Additionally, the research shows that machine unlearning defenses can be circumvented to recover sensitive knowledge through carefully crafted multi-turn conversations.

## Method Summary
The researchers conducted human red teaming across 537 HarmBench behaviors using 6-8 experienced red teamers who engaged in multi-turn conversations with four different LLM defenses (RR, LAT, DERTA, CYGNET). Each red teamer could submit up to three jailbreaks per behavior, with successful jailbreaks validated by reviewers and a GPT-4o classifier. The study also ran six automated attack methods (AutoDAN, GPTFuzzer, PAIR, Zero-Shot, AutoPrompt, GCG) against the same defenses for comparison. The researchers developed a taxonomy of jailbreak tactics through dozens of commercial red teaming engagements and created the Multi-Turn Human Jailbreaks (MHJ) dataset containing 2,912 prompts across 537 jailbreaks.

## Key Results
- Human red teamers achieved >70% attack success rate on HarmBench defenses
- Automated attacks achieved <10% success rate on the same defenses
- Multi-turn jailbreaks successfully recovered unlearned biosecurity knowledge from machine unlearning defenses
- 91.62% of successful HarmBench submissions required more than one conversation turn

## Why This Works (Mechanism)

### Mechanism 1
Multi-turn jailbreaks exploit temporal dynamics of LLM safety mechanisms primarily trained on single-turn interactions. Current defenses use single-turn adversarial training and shallow refusal training that is only a few tokens deep. When attackers engage across multiple turns, they can gradually build context and manipulate the model's understanding before delivering harmful requests.

### Mechanism 2
Human red teamers can adapt strategies dynamically based on model responses while automated attacks follow fixed patterns. Humans can observe when defenses trigger (like gibberish responses or refusals), adjust their approach in real-time, use multiple tactics sequentially, and leverage the model's own responses to craft more effective follow-up prompts.

### Mechanism 3
Machine unlearning defenses can be circumvented through carefully crafted multi-turn jailbreaks that recover unlearned knowledge. Even when models undergo unlearning to remove specific hazardous knowledge, attackers can use multi-turn conversations to reconstruct or infer this knowledge through indirect means, contextual framing, or knowledge reconstruction techniques.

## Foundational Learning

- **Multi-turn conversational dynamics**: Understanding how models handle extended conversations that build context over time is crucial since current vulnerabilities stem from failure to track conversation history across turns.
- **Adversarial attack taxonomy**: Understanding different jailbreak tactics (Direct Request, Obfuscation, Hidden Intention Streamline, etc.) is essential for developing both attacks and defenses against various exploitation methods.
- **Machine unlearning limitations**: The success against unlearning defenses shows that knowledge removal is not absolute and can be circumvented through indirect methods, revealing theoretical limits of complete knowledge removal vs obfuscation.

## Architecture Onboarding

- **Component map**: LLM defenses (circuit breakers, representation engineering, unlearning) -> Attack methods (human red teamers, automated attacks) -> Evaluation framework (HarmBench, WMDP-Bio) -> Dataset collection (MHJ with metadata and tactics)
- **Critical path**: Deploy defense → run attacks (human or automated) → classify responses as harmful/benign → calculate ASR → analyze tactics used
- **Design tradeoffs**: Single-turn defenses are easier to implement but vulnerable to multi-turn attacks; multi-turn defenses are more complex but provide better protection; automated attacks are cheaper but less effective than human attacks
- **Failure signatures**: High ASR for human attacks vs low ASR for automated attacks indicates multi-turn vulnerability; successful recovery of unlearned knowledge indicates unlearning circumvention; gibberish responses followed by harmful content indicate circuit breaker bypass
- **First 3 experiments**:
  1. Replicate single-turn vs multi-turn ASR comparison on a simple defense to confirm the vulnerability pattern
  2. Test different combinations of jailbreak tactics against a specific defense to identify most effective approaches
  3. Attempt to recover unlearned knowledge from an unlearning defense using both direct and indirect multi-turn approaches

## Open Questions the Paper Calls Out

1. Does the effectiveness of human red teaming tactics transfer across different LLM architectures and training paradigms?
2. What is the relationship between the number of conversation turns and jailbreak success rate in multi-turn attacks?
3. How does the cost-effectiveness of automated multi-turn jailbreak attacks compare to human red teaming when scaled to large datasets?
4. To what extent does domain-specific knowledge enhance the effectiveness of jailbreak attacks?
5. How does the order and combination of jailbreak tactics affect success rates against different defenses?

## Limitations

- Small sample size of 6-8 red teamers may not capture full diversity of attack strategies
- Evaluation focused primarily on LLaMA-3-8B-instruct and four specific defense mechanisms, limiting generalizability
- Current automated attacks lag behind human performance, but this gap may narrow as methods evolve

## Confidence

**High Confidence:**
- Multi-turn human jailbreaks significantly outperform single-turn automated attacks against current LLM defenses (ASR >70% vs <10% for automated attacks)
- Current LLM defenses show specific vulnerabilities to multi-turn conversational attacks that exploit temporal dynamics
- Machine unlearning defenses can be circumvented to recover sensitive knowledge through carefully crafted multi-turn conversations

**Medium Confidence:**
- The proposed taxonomy of jailbreak tactics comprehensively captures the range of human attack strategies
- The MHJ dataset provides a representative sample of multi-turn jailbreak capabilities
- The identified vulnerabilities will persist across future LLM architectures and defense mechanisms

**Low Confidence:**
- The exact mechanisms by which multi-turn jailbreaks exploit temporal dynamics are fully understood
- The current gap between human and automated attacks will remain significant as attack methods evolve
- All types of machine unlearning defenses are equally vulnerable to the demonstrated recovery techniques

## Next Checks

1. Replicate ASR gap with expanded defender pool: Conduct the same human vs automated attack comparison using 2-3 additional LLM defenses not included in the original study to verify whether the >60% ASR advantage for human attacks generalizes across defense types.

2. Test adaptive automated attack methods: Implement a simple adaptive automated attack that can detect when defenses trigger and modify its approach accordingly, then compare its performance against non-adaptive automated attacks to assess whether the human advantage is primarily due to adaptivity rather than multi-turn capability alone.

3. Validate unlearning circumvention across knowledge types: Attempt to recover multiple types of unlearned knowledge (beyond biosecurity) using the same multi-turn jailbreak techniques to determine whether the demonstrated vulnerability to knowledge recovery is specific to certain knowledge domains or represents a general weakness in machine unlearning approaches.