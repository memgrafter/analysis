---
ver: rpa2
title: From Brazilian Portuguese to European Portuguese
arxiv_id: '2408.07457'
source_url: https://arxiv.org/abs/2408.07457
tags:
- translation
- merged
- mbart-50
- chatgpt
- portuguese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the resource imbalance between Brazilian Portuguese
  (BP) and European Portuguese (EP) by developing translation systems from BP to EP.
  The authors created a manually curated test set of 500 sentences across five domains
  with two reference translations per sentence.
---

# From Brazilian Portuguese to European Portuguese

## Quick Facts
- arXiv ID: 2408.07457
- Source URL: https://arxiv.org/abs/2408.07457
- Reference count: 23
- Primary result: mBART-50 fine-tuned on merged dataset achieved best performance among fine-tuned models, but ChatGPT 3.5 Turbo outperformed all fine-tuned models on manually curated test set

## Executive Summary
This study addresses the resource imbalance between Brazilian Portuguese (BP) and European Portuguese (EP) by developing translation systems from BP to EP. The authors created a manually curated test set of 500 sentences across five domains with two reference translations per sentence. They fine-tuned multilingual models (M2M100 and mBART-50) using parallel data from movie subtitles and TED Talks, finding that the mBART-50 model fine-tuned on a merged dataset performed best among the fine-tuned models. However, ChatGPT 3.5 Turbo demonstrated superior generalization across domains compared to all fine-tuned models.

## Method Summary
The authors developed translation systems from Brazilian Portuguese to European Portuguese by fine-tuning multilingual models (M2M100 and mBART-50) on parallel corpora extracted from movie subtitles and TED Talks transcripts. They created a manually curated test set of 500 sentences across five domains (scientific, legislative, social media, literature, and wiki) with two reference translations per sentence. The models were evaluated using automatic metrics (BLEU, TER, COMET) and human evaluation, with results compared against ChatGPT 3.5 Turbo and a "do nothing" baseline where BP text is used directly as EP.

## Key Results
- mBART-50 fine-tuned on merged dataset (subtitles + TED Talks) achieved the best performance among fine-tuned models
- ChatGPT 3.5 Turbo outperformed all fine-tuned models on the manually curated test set
- "Doing nothing" (using BP text directly) often outperformed model translations, especially when compared to the more divergent DeepL reference
- Fine-tuned models showed better performance on their training domains but struggled with cross-domain generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning multilingual models on parallel BP-EP corpora enables them to learn variety-specific mappings.
- Mechanism: The model's parameters are adjusted to minimize translation loss on BP-EP pairs, causing the model to internalize orthographic, lexical, and syntactic differences between the two varieties.
- Core assumption: Parallel corpora contain sufficient variety-specific signal for the model to learn meaningful mappings.
- Evidence anchors:
  - [abstract] "We experimented with various models by fine-tuning existing Large Language Models using parallel data extracted from movie subtitles and TED Talks transcripts in both Brazilian and European Portuguese."
  - [section 4.2] "Since these multilingual models did not have the distinction between BP and EP, just having a standard 'Portuguese' language, we had to fine-tune these models with parallel corpora in order to make them learn the distinction between these varieties."
  - [corpus] "Found 25 related papers (using 8)" indicates strong domain relevance; however, the corpus evidence is weak for specific fine-tuning success metrics.
- Break condition: If parallel corpora are too small, noisy, or lack variety-specific examples, the model may not learn effective mappings and may default to the dominant variety.

### Mechanism 2
- Claim: Using a merged dataset (subtitles + TED Talks) improves generalization across domains compared to single-source training.
- Mechanism: Training on diverse sources exposes the model to a broader range of linguistic patterns, improving its ability to handle unseen domain-specific vocabulary and syntax.
- Core assumption: Diversity in training data correlates with improved cross-domain translation quality.
- Evidence anchors:
  - [section 5.1] "As such, the models that seem the most promising so far are the Merged mBART-50 and the Merged M2M100."
  - [section 5.1] "It is interesting to note that the merged datasets were very close to the score of the top models or even constituted that top score."
  - [corpus] Weak evidence; corpus analysis does not directly support domain diversity impact.
- Break condition: If the merged dataset is not balanced or contains conflicting patterns, it may confuse the model rather than improve generalization.

### Mechanism 3
- Claim: Manual translation references are more source-faithful than machine-generated ones, affecting evaluation outcomes.
- Mechanism: Human translators preserve more of the original sentence structure and meaning, while machine translations may introduce more changes to improve fluency, leading to different reference standards.
- Core assumption: Reference translation style influences automatic metric scores and human evaluation outcomes.
- Evidence anchors:
  - [section 3.2] "For the manually created reference, we made the minimal number of changes necessary to the original text. [...] This resulted in a translation that is significantly more distant from the original text compared to the manually constructed reference."
  - [section 3.3] "Notably, the DeepL reference achieved a higher fluency score compared to the manual reference."
  - [corpus] Weak evidence; corpus analysis does not directly support reference style impact.
- Break condition: If the model is optimized for machine-reference style but evaluated against human references (or vice versa), it may appear to perform worse than it actually does on the target domain.

## Foundational Learning

- Concept: Fine-tuning multilingual models
  - Why needed here: To adapt pre-trained models to distinguish BP and EP instead of treating them as a single language.
  - Quick check question: What happens to a multilingual model's parameters during fine-tuning on a specific language pair?

- Concept: Automatic evaluation metrics (BLEU, TER, COMET)
  - Why needed here: To quantitatively assess translation quality across multiple dimensions (precision, edit distance, semantic similarity).
  - Quick check question: How does COMET differ from BLEU in evaluating translation quality?

- Concept: Domain adaptation
  - Why needed here: To ensure the model generalizes across scientific, legislative, social media, literary, and wiki domains.
  - Quick check question: Why might a model trained only on subtitles perform poorly on scientific texts?

## Architecture Onboarding

- Component map: mBART-50/m2M100 model → tokenizer → parallel BP-EP corpus → fine-tuning loop → evaluation (NLP-Telescope) → human evaluation
- Critical path: Data preparation → model fine-tuning → evaluation on test sets → comparison with ChatGPT baseline
- Design tradeoffs: Merged dataset provides better generalization but may dilute domain-specific signals; single-domain training offers higher accuracy in that domain but poor cross-domain performance
- Failure signatures: Low BLEU/TER scores on test sets; high evaluation loss during training; poor human evaluation scores; failure to distinguish BP/EP in outputs
- First 3 experiments:
  1. Fine-tune mBART-50 on subtitles only; evaluate on subtitles and TED Talks test sets
  2. Fine-tune mBART-50 on merged dataset; evaluate on same test sets
  3. Fine-tune M2M100 on merged dataset; compare performance against mBART-50

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance vary across different Portuguese language domains when using specialized training data for each domain?
- Basis in paper: [inferred] The paper shows different models perform better on different domains (subtitles vs TED talks) but doesn't test domain-specific fine-tuning approaches.
- Why unresolved: The experiments only tested general domain fine-tuning versus mixed domain training, without exploring specialized domain adaptation strategies.
- What evidence would resolve it: Training separate models for each domain (scientific, legislative, social media, literature, wiki) and comparing their performance against general models would show if domain-specific fine-tuning improves translation quality.

### Open Question 2
- Question: What is the optimal ratio of Brazilian Portuguese to European Portuguese data for fine-tuning multilingual models when the goal is BP-to-EP translation?
- Basis in paper: [inferred] The paper uses balanced datasets but doesn't experiment with different ratios of BP-EP parallel data.
- Why unresolved: The study only used balanced datasets without exploring how different proportions of BP-EP data affect translation quality.
- What evidence would resolve it: Systematically varying the ratio of BP-EP parallel data during fine-tuning (e.g., 90-10, 70-30, 50-50, 30-70) and measuring translation performance would identify the optimal data balance.

### Open Question 3
- Question: How do different tokenization strategies affect translation quality between closely related language varieties like BP and EP?
- Basis in paper: [explicit] The paper mentions that models were trained "with tokenized data" and compares to models "without tokenization" in related work.
- Why unresolved: The paper doesn't investigate the impact of different tokenization approaches on BP-EP translation quality.
- What evidence would resolve it: Comparing models trained with different tokenization strategies (character-level, subword-level, word-level) and measuring their impact on translation quality would show the optimal tokenization approach for BP-EP translation.

## Limitations

- Limited parallel data: The study relies on movie subtitles and TED Talks as primary parallel corpora, which may not capture the full range of BP-EP differences, particularly in specialized domains like legal or scientific texts
- Reference translation bias: The study uses two different reference translations (manual and DeepL-generated) with different characteristics, creating potential evaluation inconsistencies
- GPT-4 comparison limitations: The comparison with ChatGPT 3.5 Turbo may be unfair since GPT models were trained on much larger, more diverse datasets including BP-EP distinctions

## Confidence

**High confidence**: The finding that fine-tuned models perform reasonably well on their training domains, with the merged dataset approach showing consistent benefits.

**Medium confidence**: The claim that mBART-50 merged dataset performs best overall, and the superiority of ChatGPT 3.5 Turbo, due to small margins and potential reference style bias.

**Low confidence**: The practical significance of the performance differences for real-world applications, given that "doing nothing" often outperforms model translations.

## Next Checks

1. **Reference reconciliation experiment**: Retrain and evaluate models using only the manual reference translation for both training and evaluation to eliminate reference style bias. Compare results against the current mixed-reference evaluation to quantify the impact of reference translation style on performance metrics.

2. **Data augmentation validation**: Create an expanded parallel corpus by automatically aligning additional BP-EP text pairs from web sources, then fine-tune models on this augmented dataset. Measure whether the performance gap with ChatGPT narrows, helping determine if the current gap reflects training data limitations versus fundamental model architecture constraints.

3. **Human evaluation on "do nothing" baseline**: Conduct human evaluation comparing BP text, fine-tuned model translations, and ChatGPT outputs for the same sentences. This would directly test whether current approaches provide meaningful improvements over simply accepting BP text, addressing the paper's finding that doing nothing often outperforms model translations.