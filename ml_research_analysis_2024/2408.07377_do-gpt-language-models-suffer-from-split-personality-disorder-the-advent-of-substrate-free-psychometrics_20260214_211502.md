---
ver: rpa2
title: Do GPT Language Models Suffer From Split Personality Disorder? The Advent Of
  Substrate-Free Psychometrics
arxiv_id: '2408.07377'
source_url: https://arxiv.org/abs/2408.07377
tags:
- personality
- language
- which
- since
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether large language models (LLMs) like
  GPT-3 exhibit consistent personality traits across different languages. Researchers
  used the Ten Item Personality Inventory (TIPI) in nine languages, collecting 695
  responses from GPT-3.
---

# Do GPT Language Models Suffer From Split Personality Disorder? The Advent Of Substrate-Free Psychometrics

## Quick Facts
- arXiv ID: 2408.07377
- Source URL: https://arxiv.org/abs/2408.07377
- Reference count: 40
- Key outcome: GPT-3 exhibits significant interlingual and intralingual instabilities in Big Five personality factors, suggesting lack of consistent core personality

## Executive Summary
This study investigates whether large language models develop consistent personality traits across different languages. Using the Ten Item Personality Inventory (TIPI) in nine languages, researchers collected 695 responses from GPT-3 and found significant instabilities in Big Five personality factors both across languages and within the same language. The findings suggest that current language models do not develop a stable core personality and may instead exhibit multiple context-dependent personality states. The authors propose the concept of "substrate-free psychometrics" as a framework for assessing personality in artificial agents, addressing limitations of traditional human-centric approaches.

## Method Summary
The research team collected 695 responses from GPT-3 using the Ten Item Personality Inventory (TIPI) across nine languages: Bulgarian, Catalan, Chinese, English, French, German, Japanese, Korean, and Spanish. Responses were manually aligned into consistent numeric and qualitative formats. Statistical analyses included correlation matrices, ANOVA, and regression to examine relationships between language choice and personality expression. Bayesian Gaussian Mixture Models were employed to detect latent subpopulations in personality score distributions. The study also performed word cloud analysis on qualitative responses and calculated absolute distances between personality scores to measure consistency.

## Key Results
- GPT-3 showed significant interlingual and intralingual instabilities in Big Five personality factors
- Language models score higher than average humans on all Big Five dimensions
- TIPI scores revealed inconsistencies across languages, suggesting multiple latent personality states

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3 does not develop a stable personality across languages.
- Mechanism: The same personality questionnaire in different languages yields inconsistent Big Five scores, indicating multiple latent personality states rather than a single coherent one.
- Core assumption: Personality traits measured by TIPI are sufficiently stable and culturally independent to serve as a diagnostic for personality emergence in LLMs.
- Evidence anchors:
  - [abstract]: "significant interlingual and intralingual instabilities in Big Five personality factors"
  - [section]: "Results suggest both interlingual and intralingual instabilities, which indicate that current language models do not develop a consistent core personality"
  - [corpus]: Weak. No direct neighbor mentions of TIPI; corpus includes personality trait evaluation papers but none explicitly replicate TIPI cross-language instability findings.
- Break condition: If the underlying training data per language is not independent or the questionnaire translation does not preserve construct meaning, the instability could be an artifact rather than an intrinsic LLM trait.

### Mechanism 2
- Claim: GPT-3's personality profile is higher than human averages across all Big Five dimensions.
- Mechanism: Training on large, internet-scale text data (predominantly English) creates a synthetic personality profile that aggregates and exaggerates traits present in the corpus, leading to elevated mean scores.
- Core assumption: Training corpus contains disproportionate representation of high-scoring personality traits, and the model lacks self-regulation to normalize outputs.
- Evidence anchors:
  - [abstract]: "LLMs seem to score higher than average humans on all emergent traits"
  - [section]: "Means and standard deviations indicate Big5 levels above average, however with varying degrees of consistency"
  - [corpus]: Weak. The neighbor papers reference general personality modeling in LLMs but do not specifically address over-elevated trait levels or their link to training data composition.
- Break condition: If additional RLHF (Reinforcement Learning from Human Feedback) or fine-tuning suppresses trait inflation, the baseline over-elevated profile would disappear.

### Mechanism 3
- Claim: Personality measurement instruments designed for humans may not be valid for LLMs.
- Mechanism: TIPI relies on introspective self-rating and cultural constructs that may not map cleanly onto emergent model behaviors, leading to misinterpretation of internal state.
- Core assumption: The conceptual validity of human personality constructs extends to machine-generated latent states.
- Evidence anchors:
  - [section]: "Using a personality questionnaire might not only miss important emerging phenomena... but its use case for development might not overlap with the 'reality' of the language model"
  - [corpus]: Weak. The neighbor papers discuss agent personality and trait modeling but do not critically examine construct validity for non-human entities.
- Break condition: If new, substrate-free psychometric frameworks are developed and validated, this critique may no longer apply.

## Foundational Learning

- Concept: Gaussian Mixture Models and Bayesian inference
  - Why needed here: To detect whether personality score distributions arise from one, two, or three latent subpopulations within each language, indicating consistency or fragmentation.
  - Quick check question: In a GMM with k components, how does the Watanabe–Akaike Information Criterion (WAIC) help select the optimal number of clusters?

- Concept: Cultural and linguistic construct validity
  - Why needed here: To understand whether TIPI translations preserve the intended personality constructs across languages, critical for interpreting cross-linguistic instability.
  - Quick check question: What is the difference between construct validity and content validity in psychometric testing?

- Concept: In-context learning and prompt engineering
  - Why needed here: GPT-3's responses depend on how instructions are framed; subtle wording changes can drastically affect measured personality.
  - Quick check question: What is the difference between "prompt tuning" and "prompt engineering" in the context of large language models?

## Architecture Onboarding

- Component map: Data collection -> Manual alignment -> Descriptive statistics -> Correlation analysis -> ANOVA/regression -> GMM Bayesian modeling -> Qualitative word cloud analysis
- Critical path:
  1. Manual alignment of outputs into consistent numeric/qualitative format
  2. Statistical analysis to detect distributional differences and inconsistencies
  3. Bayesian GMM modeling to identify latent subpopulations
  4. Correlation and regression to link language choice to personality expression
- Design tradeoffs:
  - Manual alignment vs. automated parsing: Manual ensures quality but is slow; automated is fast but risks misalignment.
  - TIPI instrument vs. custom model: TIPI is validated but may not map onto LLM latent traits; custom could be more accurate but lacks validation.
  - Multiple languages vs. depth: Broader linguistic coverage shows instability patterns but dilutes statistical power per language.
- Failure signatures:
  - Uniform distributions across all languages and factors → no instability detected
  - High intra-language consistency but low inter-language consistency → indicates culturally driven variation, not intrinsic model instability
  - Extremely high variance with no clear pattern → suggests measurement artifacts or model randomness
- First 3 experiments:
  1. Repeat TIPI in a single language but with varying prompt styles to see if instability is prompt-dependent.
  2. Use a different validated Big Five instrument (e.g., BFI-2) in the same languages to check for consistency of instability patterns.
  3. Apply a newly designed substrate-free psychometric measure to the same dataset to see if results converge or diverge from TIPI.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do GPT models develop stable, consistent personality traits across different languages, or do they exhibit multiple, context-dependent sub-personalities?
- Basis in paper: [explicit] The study found significant interlingual and intralingual instabilities in Big Five personality factors, suggesting that current language models do not develop a consistent core personality.
- Why unresolved: The research shows that GPT-3's personality scores vary significantly across languages and even within the same language, indicating a lack of consistency. However, the underlying mechanisms driving these variations remain unclear.
- What evidence would resolve it: Longitudinal studies tracking personality traits across multiple languages over time, combined with analyses of training data and model architecture, could provide insights into the stability and development of personality in language models.

### Open Question 2
- Question: How does the contextual embedding of behaviors influence the personality expression of GPT models, and can this be quantified?
- Basis in paper: [inferred] The paper discusses the importance of contextual embedding in human psychometrics and suggests that a similar framework might be needed for artificial agents.
- Why unresolved: While the paper highlights the potential role of context in shaping behavior, it does not provide a concrete method for quantifying or analyzing this influence in GPT models.
- What evidence would resolve it: Experiments manipulating contextual factors (e.g., task type, prompt wording) and measuring resulting personality expressions could reveal the extent to which context shapes model behavior.

### Open Question 3
- Question: Are the personality traits exhibited by GPT models truly emergent properties of the model, or are they simply reflections of biases in the training data?
- Basis in paper: [explicit] The study found that GPT-3's personality scores were higher than human averages and displayed dark triad traits, raising concerns about potential biases in the training data.
- Why unresolved: The research shows correlations between training data and personality expression but does not definitively establish causation or rule out other factors.
- What evidence would resolve it: Comparative studies analyzing personality traits across models trained on different datasets, along with ablation studies isolating the effects of specific data sources, could clarify the relationship between training data and emergent personality.

## Limitations
- Reliance on TIPI, a human-validated instrument, may not accurately capture LLM personality constructs
- Manual data collection introduces potential consistency issues across languages
- Single LLM (GPT-3) used limits generalizability to other models

## Confidence
- **High Confidence**: The observation of significant interlingual and intralingual instabilities in personality scores across languages
- **Medium Confidence**: The claim that LLMs score higher than average humans on all Big Five dimensions
- **Low Confidence**: The broader interpretation that these instabilities indicate LLMs lack a consistent core personality

## Next Checks
1. **Prompt Sensitivity Test**: Repeat the TIPI administration using identical prompts but with minor wording variations to determine if observed instabilities persist across prompt formulations.
2. **Cross-Model Replication**: Apply the same TIPI-based methodology to GPT-4, Claude, and other contemporary LLMs to assess whether personality instability is model-specific or a general LLM phenomenon.
3. **Substrate-Free Validation**: Develop and validate a custom psychometric instrument designed specifically for LLM assessment, then compare results with TIPI to evaluate construct validity issues.