---
ver: rpa2
title: 'Evolution of Thought: Diverse and High-Quality Reasoning via Multi-Objective
  Optimization'
arxiv_id: '2412.07779'
source_url: https://arxiv.org/abs/2412.07779
tags:
- answer
- reasoning
- answers
- quality
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Evolution of Thought (EoT), a multi-objective
  optimization framework for enhancing reasoning in multi-modal large language models
  (MLLMs). The framework addresses the challenge of local optima convergence and inefficiency
  in current reasoning path optimization methods by formulating reasoning as a multi-objective
  problem balancing answer quality and diversity.
---

# Evolution of Thought: Diverse and High-Quality Reasoning via Multi-Objective Optimization

## Quick Facts
- arXiv ID: 2412.07779
- Source URL: https://arxiv.org/abs/2412.07779
- Authors: Biqing Qi; Zhouyi Qian; Yiang Luo; Junqi Gao; Dong Li; Kaiyan Zhang; Bowen Zhou
- Reference count: 40
- Key outcome: Introduces Evolution of Thought (EoT), a multi-objective optimization framework using NSGA-II and Condensation-Aggregation to enhance reasoning in MLLMs by balancing quality and diversity, achieving superior performance on vision-language and language reasoning tasks.

## Executive Summary
This paper introduces Evolution of Thought (EoT), a multi-objective optimization framework for enhancing reasoning in multimodal large language models. EoT addresses the challenge of local optima convergence in current reasoning path optimization methods by formulating reasoning as a multi-objective problem balancing answer quality and diversity. The framework employs the Non-dominated Sorting Genetic Algorithm II (NSGA-II) with crossover and mutation operations to generate diverse high-quality solutions, combined with a Condensation-Aggregation mechanism to cluster and eliminate redundant paths while improving information sharing.

## Method Summary
EoT uses the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to optimize reasoning paths by balancing answer quality and diversity. The framework generates an initial set of candidate answers using an MLLM, then applies NSGA-II's non-dominated sorting to rank solutions across multiple layers based on quality and novelty metrics. Crossover and mutation operations create diverse offspring, which are then refined through a Condensation-Aggregation mechanism that clusters similar answers and eliminates redundancies while preserving high-quality information. The process iterates over multiple generations to converge on optimal reasoning paths.

## Key Results
- EoT achieves superior performance compared to competitive baselines on vision-language and language reasoning tasks (MathVista, Math-Vision, GSM8K).
- Significant improvements in accuracy metrics, with enhanced Pass@4 and Pass@8 scores while maintaining computational efficiency.
- The framework demonstrates effectiveness across different MLLM architectures including Qwen2VL, Llava, and Phi-3.5.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** NSGA-II balances answer quality and diversity by using non-dominated sorting to maintain a Pareto front of solutions.
- **Mechanism:** Non-dominated sorting ranks solutions across multiple layers, ensuring that solutions are selected based on their dominance in both quality and diversity, preventing early convergence to local optima.
- **Core assumption:** The non-dominated sorting approach in NSGA-II can effectively manage the trade-off between conflicting objectives (quality and diversity) without prioritizing one over the other.
- **Evidence anchors:**
  - [abstract]: "We introduce the Non-dominated Sorting Genetic Algorithm II for multi-objective optimization, utilizing crossover and mutation operators to promote greater diversity in reasoning solutions."
  - [section 3.2]: "To achieve this, we introduce the non-dominated sorting [6], which concentrates high-quality candidates on the Pareto front, balancing the trade-offs between objectives."
  - [corpus]: Weak evidence; related works mention multi-objective optimization but not specifically NSGA-II's application to reasoning.
- **Break condition:** If the dominance relationships are incorrectly calculated or if the sorting algorithm fails to properly rank solutions, the Pareto front may not represent a true balance between quality and diversity, leading to suboptimal solutions.

### Mechanism 2
- **Claim:** The Condensation-Aggregation (CA) mechanism reduces redundancy and enhances information sharing among parent nodes.
- **Mechanism:** The CA mechanism clusters candidate answers based on semantic and textual similarity, removes low-quality clusters, and aggregates the remaining high-quality answers to produce a concise and accurate final answer.
- **Core assumption:** Clustering based on edit distance and semantic similarity can effectively identify and eliminate redundant answers while preserving the most valuable information.
- **Evidence anchors:**
  - [abstract]: "we propose a Condensation-Aggregation mechanism to cluster and eliminate redundant paths, facilitate improved information sharing among parent nodes, and ultimately enhance both the efficiency and quality of the reasoning process."
  - [section 3.3]: "Specifically, in the condensation stage, we measure the distance D between different candidate answers A and A′ in A, based on their edit distance DEd(A, A′) and semantic distance DSE(A, A′)."
  - [corpus]: Weak evidence; related works discuss information sharing but not specifically the CA mechanism.
- **Break condition:** If the clustering algorithm fails to accurately group similar answers or if the quality assessment is flawed, the CA mechanism may eliminate valuable answers or retain redundant ones, reducing the overall quality and efficiency of the reasoning process.

### Mechanism 3
- **Claim:** The multi-objective optimization (MOO) formulation improves the model's reasoning capabilities by considering both quality and diversity.
- **Mechanism:** By formulating reasoning as a MOO problem, the model is encouraged to explore a wider range of solutions, balancing the trade-off between finding correct answers and generating diverse reasoning paths.
- **Core assumption:** The MOO formulation can effectively guide the model to explore a broader solution space, leading to improved generalization and adaptability.
- **Evidence anchors:**
  - [abstract]: "EoT formulates reasoning as a multi-objective optimization (MOO) problem, balancing both answer quality and diversity."
  - [section 3.1]: "To help MLLMs balance answer quality and diversity during the optimization process, we formalize this task as a MOO problem, acknowledging its inherent multi-objective nature."
  - [corpus]: Weak evidence; related works mention multi-objective optimization but not specifically its application to reasoning tasks.
- **Break condition:** If the MOO formulation is poorly designed or if the objectives are not properly weighted, the model may prioritize one objective over the other, leading to suboptimal performance in terms of either quality or diversity.

## Foundational Learning

- **Concept: Non-dominated Sorting**
  - Why needed here: Non-dominated sorting is essential for ranking solutions in a multi-objective optimization problem, ensuring that solutions are selected based on their dominance in both quality and diversity.
  - Quick check question: How does non-dominated sorting differ from traditional sorting methods, and why is it particularly useful for multi-objective optimization?

- **Concept: Genetic Algorithms**
  - Why needed here: Genetic algorithms provide a framework for exploring a large solution space through crossover and mutation operations, which are used in EoT to generate diverse and high-quality reasoning paths.
  - Quick check question: What are the key components of a genetic algorithm, and how do they contribute to the exploration and exploitation of the solution space?

- **Concept: Clustering Algorithms**
  - Why needed here: Clustering algorithms are used in the CA mechanism to group similar candidate answers based on semantic and textual similarity, enabling the elimination of redundant answers and the aggregation of high-quality ones.
  - Quick check question: What are the different types of clustering algorithms, and how do they differ in terms of their assumptions and applications?

## Architecture Onboarding

- **Component map:** User query prompt (pq) -> MLLM function (f) -> Candidate answer set (A) -> Non-dominated sorting -> Crossover and mutation operations -> Condensation-Aggregation (CA) mechanism -> Final optimized answers
- **Critical path:**
  1. Initialize candidate answer set (A) with N initial answers generated by the MLLM.
  2. Calculate quality (MQ) and novelty (MN) scores for each answer in A.
  3. Perform non-dominated sorting to rank solutions based on MQ and MN scores.
  4. Select K parent candidates from the highest-ranked non-dominated levels.
  5. Generate NC crossover offspring and NM mutation offspring using the selected parent candidates.
  6. Update the candidate answer set (A) with the new offspring.
  7. Repeat steps 2-6 for T generations.
  8. Apply the CA mechanism to cluster, eliminate redundant answers, and aggregate high-quality ones.
  9. Return the final optimized set of candidate answers.
- **Design tradeoffs:**
  - Balancing the number of candidate answers (N) and the number of generations (T) to achieve optimal performance without excessive computational cost.
  - Determining the appropriate crossover-to-mutation ratio (r) and the number of selected candidate parents (K) to maintain diversity while ensuring high-quality solutions.
  - Choosing the number of clusters (κ) and the number of clusters to drop (m) in the CA mechanism to effectively eliminate redundant answers without losing valuable information.
- **Failure signatures:**
  - Poor performance on reasoning tasks, indicating that the model is not effectively exploring the solution space or balancing quality and diversity.
  - Excessive computational cost, suggesting that the number of candidate answers (N) or the number of generations (T) is too high.
  - Loss of diversity in the candidate answer set, indicating that the crossover and mutation operations are not generating sufficiently diverse offspring.
- **First 3 experiments:**
  1. Evaluate the model's performance on a simple reasoning task with a small number of candidate answers (N) and generations (T) to establish a baseline.
  2. Vary the crossover-to-mutation ratio (r) and the number of selected candidate parents (K) to determine the optimal configuration for maintaining diversity and quality.
  3. Apply the CA mechanism with different numbers of clusters (κ) and clusters to drop (m) to assess its impact on the model's performance and efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of crossover-to-mutation ratio affect the performance of EoT in different reasoning task complexities?
- Basis in paper: The paper mentions that the configuration with r = 1:5 and K = 3 demonstrates the best performance on the more challenging Math-Vision dataset, suggesting that the ratio impacts performance differently based on task complexity.
- Why unresolved: The paper does not provide a comprehensive analysis of how different crossover-to-mutation ratios affect EoT's performance across a wider range of task complexities or datasets.
- What evidence would resolve it: Conducting experiments with varying crossover-to-mutation ratios across multiple datasets with different levels of complexity would provide insights into the optimal settings for each scenario.

### Open Question 2
- Question: What is the impact of the number of clusters and the number of clusters to drop on the efficiency and accuracy of the Condensation-Aggregation mechanism?
- Basis in paper: The paper discusses the CA mechanism's role in improving model performance by clustering and eliminating redundant paths, but does not fully explore how different numbers of clusters or dropped clusters affect outcomes.
- Why unresolved: The paper suggests that the CA mechanism improves performance with increased diversity, but does not detail the specific impact of varying the number of clusters or the number of clusters to drop.
- What evidence would resolve it: Analyzing the performance of EoT with different numbers of clusters and varying numbers of dropped clusters across multiple datasets would clarify the optimal settings for the CA mechanism.

### Open Question 3
- Question: How does the EoT framework scale with larger datasets and more complex reasoning tasks compared to other methods?
- Basis in paper: The paper demonstrates EoT's effectiveness on specific datasets like MathVista, Math-Vision, and GSM8K, but does not explore its scalability to larger datasets or more complex tasks.
- Why unresolved: While EoT shows promise in the experiments conducted, its performance and efficiency on larger or more complex datasets remain untested, leaving questions about its scalability.
- What evidence would resolve it: Testing EoT on larger datasets with more complex reasoning tasks and comparing its performance and efficiency to other methods would provide insights into its scalability.

## Limitations
- The exact implementation details of the Condensation-Aggregation mechanism, particularly the clustering algorithm and quality assessment criteria, are not fully specified, which could impact reproducibility.
- The framework is primarily evaluated on mathematical reasoning tasks, leaving questions about its generalizability to other reasoning domains.
- The paper lacks comprehensive ablation studies to isolate the contribution of individual components (NSGA-II vs CA mechanism vs their combination).

## Confidence

**High Confidence:** The fundamental premise that multi-objective optimization can improve reasoning path diversity and quality is well-supported. The NSGA-II algorithm is a proven approach in optimization literature, and the basic framework design follows established patterns.

**Medium Confidence:** The specific implementation details and hyperparameters (number of candidates, generations, crossover/mutation ratios) appear reasonable but would benefit from more thorough justification and sensitivity analysis. The reported performance improvements over baselines seem plausible given the methodology.

**Low Confidence:** The exact mechanism by which the Condensation-Aggregation process improves information sharing is not fully explained. The clustering approach and quality assessment criteria need more rigorous validation to ensure they're not introducing bias or removing valuable information.

## Next Checks

1. **Ablation Study Validation:** Conduct experiments to isolate the contribution of NSGA-II vs Condensation-Aggregation vs their combination. Specifically, test EoT without the CA mechanism and with simplified clustering to quantify its impact on performance.

2. **Generalization Testing:** Evaluate the framework on non-mathematical reasoning tasks (e.g., commonsense reasoning, logical inference) to assess its generalizability beyond the current dataset scope. Compare performance drops relative to baseline methods.

3. **Hyperparameter Sensitivity Analysis:** Systematically vary key hyperparameters (number of candidate answers, generations, crossover/mutation ratios) to identify optimal configurations and determine robustness to parameter changes. Document performance variance across the parameter space.