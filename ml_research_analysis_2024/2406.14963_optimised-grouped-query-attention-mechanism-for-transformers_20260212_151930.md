---
ver: rpa2
title: Optimised Grouped-Query Attention Mechanism for Transformers
arxiv_id: '2406.14963'
source_url: https://arxiv.org/abs/2406.14963
tags:
- grouping
- group
- size
- asymgqa
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AsymGQA, an activation-informed approach
  for asymmetrically grouping multi-head attention into grouped-query attention to
  improve model performance while maintaining hardware efficiency. The method leverages
  cosine similarity between attention head activations to guide the search for optimal
  grouping configurations, allowing for asymmetric group sizes.
---

# Optimised Grouped-Query Attention Mechanism for Transformers

## Quick Facts
- arXiv ID: 2406.14963
- Source URL: https://arxiv.org/abs/2406.14963
- Authors: Yuang Chen; Cheng Zhang; Xitong Gao; Robert D. Mullins; George A. Constantinides; Yiren Zhao
- Reference count: 40
- Primary result: AsymGQA improves MMLU accuracy by up to 7.5% compared to neighbor grouping

## Executive Summary
This paper introduces AsymGQA, an activation-informed approach for asymmetrically grouping multi-head attention into grouped-query attention to improve model performance while maintaining hardware efficiency. The method leverages cosine similarity between attention head activations to guide the search for optimal grouping configurations, allowing for asymmetric group sizes. Experiments on LLaMA-2-7B and OPT-1.3B models show that AsymGQA consistently outperforms baseline neighbour grouping across multiple tasks, with up to 7.5% accuracy improvement on MMLU.

## Method Summary
AsymGQA converts MHA to GQA by first computing activation similarity between attention heads using cosine similarity, then searching for optimal asymmetric groupings guided by these similarity scores. The search algorithm iteratively merges similar heads while preserving important functional relationships, allowing for varying group sizes across different heads. After grouping, the model undergoes fine-tuning to recover performance lost during the conversion process. The approach can be applied to both key and value layers independently, creating asymmetric group configurations that optimize the performance-efficiency tradeoff.

## Key Results
- AsymGQA LLaMA-2-7B with average group size of 4 achieves 7.2% higher MMLU accuracy than neighbor grouping
- Consistent performance improvements across QNLI, MNLI, SST2, and MMLU tasks
- Performance degrades with larger group sizes, indicating optimal tradeoffs at smaller group configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Activation-informed similarity guides grouping to preserve model quality during GQA conversion
- Mechanism: The search algorithm uses cosine similarity between attention head activations to determine which heads should be grouped together. Heads with similar activation patterns are merged, leading to more coherent feature extraction and stable gradient updates during fine-tuning.
- Core assumption: Similar activation patterns indicate that heads process comparable aspects of input data, so merging them preserves functional coherence
- Evidence anchors: [abstract] "Specifically, our contributions are as follows: We introduce AsymGQA, an activation-informed fusion approach for converting MHA into GQA models, which delivers superior model performance within the same computational constraints."

### Mechanism 2
- Claim: Asymmetric grouping allows better optimization of the performance-efficiency tradeoff
- Mechanism: By allowing group sizes to vary, the algorithm can create smaller groups for more important heads and larger groups for less important ones, effectively prioritizing computational resources
- Core assumption: Not all attention heads contribute equally to model performance, and asymmetric grouping can better capture this heterogeneity
- Evidence anchors: [abstract] "AsymGQA models significantly outperform the GQA baseline. For example, LLaMA-2-7B with an average group size of 4 has an increase in accuracy of 7.2% on MMLU compared to naive MHA to GQA conversion."

### Mechanism 3
- Claim: Fine-tuning after grouping recovers performance lost during the conversion process
- Mechanism: The model undergoes additional training after the grouping operation to adapt its parameters to the new grouped attention structure
- Core assumption: The loss of information from merging attention heads can be partially recovered through fine-tuning, though not completely
- Evidence anchors: [abstract] "For example, AsymGQA LLaMA-2-7B has an accuracy increase of 7.5% on MMLU compared to neighbour grouping."

## Foundational Learning

- Concept: Multi-Head Attention (MHA) mechanism
  - Why needed here: Understanding MHA is fundamental to grasping why GQA is an optimization and how AsymGQA improves upon it
  - Quick check question: In MHA, how many key/value pairs does each query head have access to, and how does this change in GQA?

- Concept: Cosine similarity and vector space representations
  - Why needed here: The method relies on measuring activation similarity using cosine similarity between vectors
  - Quick check question: What range of values can cosine similarity produce, and what do the extremes represent?

- Concept: Search optimization and local minima
  - Why needed here: The grouping algorithm uses a search strategy that must navigate the space of possible groupings while avoiding poor local optima
  - Quick check question: What techniques does the paper mention for avoiding local minima in the search process?

## Architecture Onboarding

- Component map: Search algorithm -> Activation similarity computation -> Grouping mechanism -> Fine-tuning process
- Critical path: The most time-consuming operation is the search algorithm, which must compute activation similarities across all head pairs for each iteration
- Design tradeoffs: The main tradeoff is between search thoroughness (more iterations, better groupings) and computational cost
- Failure signatures: Poor performance relative to baselines indicates either bad groupings (search failed) or insufficient fine-tuning
- First 3 experiments:
  1. Implement neighbour grouping baseline and verify performance degradation compared to original MHA
  2. Implement activation-informed symmetric grouping and compare against neighbour grouping on a small model
  3. Add asymmetric grouping capability and test if varied group sizes improve performance on the same small model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal group size for AsymGQA across different model sizes and tasks?
- Basis in paper: [inferred] The paper shows that larger group sizes lead to diminishing hardware efficiency returns and performance degradation. It suggests group sizes of 2 or 3 may be ideal for real-world applications, but this is not systematically studied.
- Why unresolved: The paper only provides results for specific group sizes (1, 2, 3, 4, 6) and models (LLaMA-2-7B, OPT-1.3B). A comprehensive study across different model sizes and tasks is needed to determine the optimal group size.
- What evidence would resolve it: A systematic study varying group sizes across different model sizes and tasks, measuring both performance and hardware efficiency, would help determine the optimal group size for AsymGQA.

### Open Question 2
- Question: How does AsymGQA compare to other attention mechanisms like multi-query attention or sparse attention in terms of performance and hardware efficiency?
- Basis in paper: [explicit] The paper focuses on comparing AsymGQA to GQA with neighbor grouping, but does not compare it to other attention mechanisms.
- Why unresolved: The paper only provides a comparison between AsymGQA and GQA with neighbor grouping, but does not explore how AsymGQA performs compared to other attention mechanisms like multi-query attention or sparse attention.
- What evidence would resolve it: A comparative study of AsymGQA against other attention mechanisms like multi-query attention or sparse attention, measuring both performance and hardware efficiency, would help determine its relative advantages and disadvantages.

### Open Question 3
- Question: How does the choice of similarity metric affect the performance of AsymGQA?
- Basis in paper: [explicit] The paper mentions that they use cosine similarity between activation vectors to measure the similarity between key (value) layers, but does not explore other similarity metrics.
- Why unresolved: The paper only uses cosine similarity between activation vectors as the similarity metric, but does not explore how other similarity metrics like Euclidean distance or learned similarity functions would affect the performance of AsymGQA.
- What evidence would resolve it: A study comparing the performance of AsymGQA using different similarity metrics like cosine similarity, Euclidean distance, or learned similarity functions would help determine the impact of the choice of similarity metric on its performance.

## Limitations

- The search algorithm requires substantial computation to evaluate activation similarities across all head pairs
- Performance gains are highly dependent on the fine-tuning process, which may not fully recover information lost during grouping
- The method's effectiveness is primarily demonstrated on LLaMA-2-7B and OPT-1.3B architectures, limiting generalizability

## Confidence

**High Confidence Claims**:
- Activation-informed similarity outperforms weight-based similarity for guiding groupings
- AsymGQA consistently outperforms neighbour grouping across multiple tasks and model sizes
- Performance degrades with larger average group sizes

**Medium Confidence Claims**:
- Asymmetric grouping provides benefits over symmetric approaches
- The 7.5% MMLU improvement is robust
- The search algorithm effectively finds good local optima

## Next Checks

1. **Cross-architecture validation**: Test AsymGQA on a different transformer architecture (e.g., BERT or GPT-2) to verify the approach generalizes beyond LLaMA-2 and OPT models.

2. **Ablation on search parameters**: Systematically vary the search algorithm parameters (n, k, pacc, preset) to determine their impact on final performance and identify the most critical factors for success.

3. **Zero-shot transfer evaluation**: Evaluate models converted with AsymGQA on tasks not seen during fine-tuning to assess whether the grouping preserves general reasoning capabilities or merely overfits to specific datasets.