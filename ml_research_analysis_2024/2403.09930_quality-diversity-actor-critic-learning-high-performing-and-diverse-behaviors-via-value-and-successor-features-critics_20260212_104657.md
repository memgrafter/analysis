---
ver: rpa2
title: 'Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors
  via Value and Successor Features Critics'
arxiv_id: '2403.09930'
source_url: https://arxiv.org/abs/2403.09930
tags:
- skill
- skills
- performance
- qdac
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Quality-Diversity Actor-Critic (QDAC) is an off-policy actor-critic
  algorithm that learns high-performing and diverse behaviors in continuous control
  tasks. It combines a value function critic (for performance) and a successor features
  critic (for diversity) via constrained optimization, using a Lagrange multiplier
  to balance both objectives.
---

# Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics

## Quick Facts
- arXiv ID: 2403.09930
- Source URL: https://arxiv.org/abs/2403.09930
- Authors: Luca Grillotti; Maxence Faldor; Borja G. León; Antoine Cully
- Reference count: 40
- One-line primary result: QDAC learns high-performing and diverse behaviors in continuous control tasks, outperforming baselines by 15% in diversity and 38% in performance.

## Executive Summary
Quality-Diversity Actor-Critic (QDAC) is an off-policy actor-critic algorithm that learns high-performing and diverse behaviors in continuous control tasks. It combines a value function critic (for performance) and a successor features critic (for diversity) via constrained optimization, using a Lagrange multiplier to balance both objectives. QDAC outperforms existing Quality-Diversity baselines on six locomotion tasks, achieving 15% more diverse behaviors and 38% higher performance. It also shows competitive adaptation to perturbed environments and enables hierarchical skill reuse.

## Method Summary
QDAC is an off-policy actor-critic algorithm that leverages value function and successor features critics to learn diverse and high-performing behaviors. The algorithm optimizes a Lagrangian combining return maximization and skill execution accuracy, balanced by a state-skill conditioned Lagrange multiplier λ(s,z). A model-based variant (QDAC-MB) uses a recurrent state space model for improved sample efficiency and skill representation.

## Key Results
- QDAC outperforms baselines on six locomotion tasks, achieving 15% more diverse behaviors and 38% higher performance.
- QDAC shows competitive adaptation to perturbed environments and enables hierarchical skill reuse.
- Qualitative results reveal adaptive behaviors such as negative-velocity locomotion and angle-relative movement.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QDAC learns to execute diverse skills by optimizing a Lagrangian combining value function and successor features critics.
- Mechanism: The actor maximizes a trade-off between return (via value critic) and skill execution accuracy (via successor features critic), balanced by a state-skill conditioned Lagrange multiplier λ(s,z).
- Core assumption: Successor features can approximate the expected features under a policy, and minimizing ∥(1−γ)ψ(s,z)−z∥2 upper bounds the distance to desired skill.
- Evidence anchors:
  - [abstract] "combines a value function critic (for performance) and a successor features critic (for diversity) via constrained optimization, using a Lagrange multiplier to balance both objectives."
  - [section 4.1] Proposition proving ∥Eπz[ϕ(s,a)]−z∥2 ≤ Eπz[∥(1−γ)ψ(s,z)−z∥2].
  - [corpus] No direct evidence; assumption relies on theoretical bound.
- Break condition: If the successor features network fails to approximate expected features accurately, the diversity constraint becomes ineffective.

### Mechanism 2
- Claim: The Lagrange multiplier λ(s,z) adapts to balance quality and diversity dynamically per state-skill pair.
- Mechanism: λ(s,z) is trained via cross-entropy loss to increase when the skill execution constraint is violated and decrease when satisfied, focusing the actor on return or diversity accordingly.
- Core assumption: The binary indicator y in the Lagrange multiplier update accurately reflects whether the skill execution constraint is satisfied.
- Evidence anchors:
  - [section 4.2] "The parameters θλ are optimized so that λ(s, z) increases when the actor is unable to execute the desired skill z, to put more weight on executing the skill. Conversely, the parameters θλ are optimized so that λ(s, z) decreases when the actor is able to execute the desired skill z, to put more weight on maximizing the return."
  - [section 4.1] Equation 5 defining the cross-entropy loss for λ.
  - [corpus] No direct evidence; relies on empirical success in experiments.
- Break condition: If the threshold δ is poorly chosen, λ may oscillate or converge to extreme values, destabilizing learning.

### Mechanism 3
- Claim: QDAC-MB (model-based variant) leverages world models to improve sample efficiency and skill representation.
- Mechanism: A recurrent state space model (RSSM) learns latent dynamics; rollouts in imagination enable massive parallel skill sampling and straight-through gradient computation.
- Core assumption: The latent dynamics learned by RSSM are sufficiently accurate to generate meaningful imagined rollouts for policy improvement.
- Evidence anchors:
  - [section C.2] Description of QDAC-MB using RSSM and imagination rollouts.
  - [section 5.4.1] QDAC-MB outperforms QDAC on most tasks, suggesting model-based benefits.
  - [corpus] No direct evidence; assumption relies on DreamerV3's success in similar domains.
- Break condition: If the world model's predictions deviate significantly from reality, imagined rollouts may mislead policy training.

## Foundational Learning

- Concept: Value functions and Bellman equations
  - Why needed here: QDAC uses a value function critic V(s,z) to estimate expected return under a skill-conditioned policy, requiring understanding of value iteration and Bellman backup.
  - Quick check question: What is the Bellman equation for the value function Vπ(s) in an infinite-horizon discounted MDP?

- Concept: Successor features and their relationship to expected features
  - Why needed here: QDAC uses successor features ψ(s,z) to characterize expected behavior under a policy, and the bound ∥(1−γ)ψ(s,z)−z∥2 is central to the diversity constraint.
  - Quick check question: How does the successor features Bellman equation differ from the value function Bellman equation?

- Concept: Constrained optimization and Lagrange multipliers
  - Why needed here: QDAC frames the quality-diversity trade-off as a constrained optimization problem, using Lagrange multipliers to balance the two objectives dynamically.
  - Quick check question: In the context of QDAC, what role does the Lagrange multiplier λ(s,z) play in the actor's objective function?

## Architecture Onboarding

- Component map:
  - Actor network: skill-conditioned policy π(a|s,z)
  - Value critic network: V(s,z) estimating expected return
  - Successor features critic network: ψ(s,z) estimating expected features
  - Lagrange multiplier network: λ(s,z) balancing quality and diversity
  - Replay buffer: stores (s,a,r,ϕ,s',z) transitions
  - (For QDAC-MB) World model: learns latent dynamics for imagination rollouts

- Critical path:
  1. Sample skill z uniformly
  2. Interact with environment using π(a|s,z), store transition
  3. Update Lagrange multiplier λ(s,z) via cross-entropy loss
  4. Update value critic V(s,z) via Bellman error minimization
  5. Update successor features critic ψ(s,z) via Bellman error minimization
  6. Update actor π(a|s,z) via SAC with adjusted objective incorporating λ(s,z)
  7. (For QDAC-MB) Train world model on real data, generate imagination rollouts, update critics and actor in imagination

- Design tradeoffs:
  - Balancing quality vs diversity: fixed λ vs adaptive λ(s,z)
  - Sample efficiency: model-free (QDAC) vs model-based (QDAC-MB)
  - Skill representation: predefined features vs learned unsupervised features
  - Computational cost: additional successor features critic and Lagrange multiplier network

- Failure signatures:
  - Value critic diverges: unstable returns, NaN values in V(s,z)
  - Successor features critic fails: inability to execute diverse skills, distance to skill remains high
  - Lagrange multiplier saturates: λ(s,z) → 0 or 1, losing balance between quality and diversity
  - Model-based variant fails: poor world model predictions, imagined rollouts don't match reality

- First 3 experiments:
  1. Train QDAC on Ant feet contact task, evaluate distance and performance profiles vs DCG-ME baseline
  2. Test QDAC's ability to execute negative-velocity skills on Walker velocity task, visualize heatmaps
  3. Apply QDAC-MB to Humanoid jump task, compare sample efficiency and skill execution vs QDAC

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QDAC perform on tasks with non-ergodic environments, such as Atari games?
- Basis in paper: [inferred] The authors mention applying QDAC to non-ergodic environments like Atari games as a future direction.
- Why unresolved: The paper only evaluates QDAC on continuous control locomotion tasks using the Brax physics engine.
- What evidence would resolve it: Running QDAC on a suite of Atari games and comparing its performance to other algorithms on these tasks.

### Open Question 2
- Question: Can QDAC discover task-agnostic skills by learning the feature function in an unsupervised manner?
- Basis in paper: [explicit] The authors suggest learning the feature function in an unsupervised manner as an exciting future direction.
- Why unresolved: The paper uses manually defined feature functions to guide the diversity search.
- What evidence would resolve it: Implementing an unsupervised feature learning method (e.g., autoencoder) and integrating it into QDAC, then evaluating the discovered skills on various downstream tasks.

### Open Question 3
- Question: How does the choice of feature function affect the quality and diversity of the generated solutions in QDAC?
- Basis in paper: [inferred] The authors mention that the choice of feature definition strongly influences the quality and diversity of the generated solutions.
- Why unresolved: The paper only evaluates QDAC on a limited set of feature functions.
- What evidence would resolve it: Conducting an ablation study on QDAC by systematically varying the feature functions and measuring the impact on quality and diversity metrics across different tasks.

## Limitations
- The empirical validation of the theoretical bound linking successor features to expected features is limited, with the bound's tightness and practical impact not extensively evaluated.
- Key hyperparameters like the distance threshold δ and the weighting of the quality-diversity trade-off are not extensively explored in ablation studies.
- The comparison with baselines is comprehensive but limited to six tasks from a single physics engine (Brax), raising questions about generalizability.

## Confidence
- **Medium**: Core QDAC algorithm (uncertainties around theoretical bound validation and hyperparameter sensitivity)
- **High**: QDAC-MB variant (builds on established DreamerV3 foundations)

## Next Checks
1. **Theoretical Bound Validation**: Empirically evaluate the tightness of the bound ∥Eπz[ϕ(s,a)]−z∥2 ≤ Eπz[∥(1−γ)ψ(s,z)−z∥2] across different skill spaces and MDPs.
2. **Hyperparameter Sensitivity**: Systematically vary the distance threshold δ and the initial Lagrange multiplier λ to assess their impact on convergence and performance.
3. **Generalization Study**: Test QDAC on continuous control tasks from other frameworks (e.g., MuJoCo, PyBullet) to evaluate robustness across physics engines and task distributions.