---
ver: rpa2
title: Getting the most out of your tokenizer for pre-training and domain adaptation
arxiv_id: '2402.01035'
source_url: https://arxiv.org/abs/2402.01035
tags: []
core_contribution: This paper examines how tokenizer design impacts LLM efficiency
  and performance in code generation tasks. The authors analyze the effects of tokenizer
  vocabulary size, pre-tokenization regular expressions, and training data on compression
  rates, generation speed, and downstream performance.
---

# Getting the most out of your tokenizer for pre-training and domain adaptation

## Quick Facts
- arXiv ID: 2402.01035
- Source URL: https://arxiv.org/abs/2402.01035
- Reference count: 40
- Specialized code tokenizers can compress code sequences by over 40% compared to standard tokenizers while maintaining or improving performance

## Executive Summary
This paper examines how tokenizer design impacts large language model efficiency and performance in code generation tasks. Through systematic experiments varying vocabulary size, pre-tokenization regular expressions, and training data composition, the authors demonstrate that specialized tokenizers can significantly improve compression rates while maintaining downstream performance. The study shows that when fine-tuning pre-trained models on substantial code data (>50 billion tokens), changing the tokenizer leads to meaningful gains in generation speed and effective context size. The authors provide practical recommendations for tokenizer design, finding that GPT-4's pre-tokenization regular expression offers an optimal balance of compression and performance.

## Method Summary
The authors systematically evaluate different tokenizer designs through a series of controlled experiments. They train Byte-Pair Encoding (BPE) tokenizers with varying vocabulary sizes (10k to 256k), pre-tokenization schemes (GPT-2, GPT-4, Punct, Identity), and training data mixes (70% code, 30% English). These tokenizers are then used to fine-tune base models (GPT-2 XL 1.5B and Llama 2 7B variants) for 500 billion tokens on code data. The evaluation uses code generation benchmarks HumanEval and MBPP, measuring compression rates (Normalized Sequence Length, bytes per token), generation speed, and task performance (Pass@1, Pass@100, Compile@1 metrics). The study also investigates tokenizer switching in pre-trained models using Fast Vocabulary Transfer and tests token healing techniques to address boundary issues.

## Key Results
- Specialized code tokenizers achieve over 40% compression compared to standard tokenizers while maintaining or improving code generation performance
- GPT-4's pre-tokenization regular expression provides the best balance of compression and downstream performance across multiple metrics
- When fine-tuning on more than 50 billion tokens, changing the tokenizer in pre-trained models yields significant gains in generation speed and effective context size with negligible impact on task performance
- Larger vocabulary sizes (up to 100k) provide better compression and slightly better performance, while very large vocabularies (256k) offer diminishing returns
- Token healing is crucial for maintaining performance when using tokenizers with less defined boundaries like the Identity tokenizer

## Why This Works (Mechanism)
Tokenizers fundamentally determine how input text is converted into discrete units that models process. The choice of tokenization scheme affects both the information density (compression rate) and the semantic granularity of the input representation. Different pre-tokenization regular expressions create varying levels of separation between syntax and semantics, which influences how easily models can learn compositional reasoning. When switching tokenizers in pre-trained models, the embedding space must be remapped, which requires substantial training data to recover performance. The interaction between tokenization and model architecture creates trade-offs between compression efficiency, generation speed, and task-specific performance.

## Foundational Learning
**Byte-Pair Encoding (BPE)**: A data compression algorithm that iteratively merges frequent character pairs to build a vocabulary. Why needed: BPE provides a balance between character-level and word-level tokenization, crucial for handling code with diverse identifiers and syntax. Quick check: Verify that BPE training converges and produces a reasonable vocabulary size for the target domain.

**Pre-tokenization Regular Expressions**: Patterns that split text into atomic units before BPE processing. Why needed: Different schemes (GPT-2, GPT-4, Punct, Identity) create varying levels of token granularity, directly impacting compression and semantic representation. Quick check: Compare token counts and semantic preservation across different pre-tokenization schemes on sample code.

**Fast Vocabulary Transfer (FVT)**: A technique for remapping embedding spaces when switching tokenizers in pre-trained models. Why needed: Changing tokenizers disrupts the learned embedding space, requiring efficient transfer methods to maintain performance during fine-tuning. Quick check: Monitor embedding similarity metrics before and after FVT application.

**Token Healing**: A technique to handle tokenization artifacts at prompt boundaries during generation. Why needed: Prevents invalid code generation when tokens span across prompt-generation boundaries, especially critical for tokenizers with less defined boundaries. Quick check: Verify generated code compiles and passes unit tests after implementing token healing.

**Normalized Sequence Length (NSL)**: A metric comparing tokenized sequence length to a baseline tokenizer. Why needed: Provides standardized measure of compression efficiency across different tokenizer designs. Quick check: Calculate NSL consistently across all tokenizer configurations on the same validation set.

## Architecture Onboarding

**Component Map**: Code Data -> Tokenizer (BPE + Pre-tokenization) -> Embedding Layer -> Model Architecture -> Output Generation -> Evaluation Metrics

**Critical Path**: Tokenizer Design → Embedding Space → Model Performance → Compression Efficiency

**Design Tradeoffs**: 
- Vocabulary Size: Larger vocabularies provide better compression but increase memory usage and slow inference
- Pre-tokenization Scheme: Stronger separation (Punct) simplifies language generation but may reduce compression; weaker separation (Identity) maximizes compression but complicates learning
- Training Data Mix: More code data improves code-specific performance but may reduce generalization to natural language

**Failure Signatures**:
- Poor downstream performance when switching tokenizers: Typically occurs with insufficient training data (<50B tokens)
- Tokenization errors at prompt boundaries: Leads to invalid code generation, especially with Identity tokenizer
- Excessive memory usage: Often caused by very large vocabulary sizes (>100k)

**First Experiments**:
1. Train baseline BPE tokenizers with different configurations and calculate compression metrics on held-out code samples
2. Fine-tune base models with different tokenizers for 50B tokens and evaluate performance recovery curves
3. Test token healing implementation by generating code sequences and verifying compilation success

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the precise mechanism by which changing tokenizers impacts the embedding space of a pre-trained model, and how can this be quantified?
- Basis in paper: The paper mentions that "changing a tokenizer has large effect on the weights" and discusses Fast Vocabulary Transfer (FVT) as a method to map the old embedding space onto the new.
- Why unresolved: While the paper shows that tokenizer changes impact performance, it does not delve into the specific mathematical or architectural reasons behind these changes in the embedding space.
- What evidence would resolve it: A detailed analysis of how the embedding weights are adjusted during FVT or when using other methods, along with empirical data showing the correlation between these adjustments and downstream performance.

### Open Question 2
- Question: How does the choice of pre-tokenization regular expression influence the ability of a model to learn compositional reasoning, and can this be measured directly?
- Basis in paper: The paper introduces the Punct tokenizer to test whether a stronger separation between syntax and semantics simplifies the language generation task, and finds that GPT-4's pre-tokenization regular expression offers a good balance of compression and performance.
- Why unresolved: The paper suggests that pre-tokenization impacts downstream performance, but does not provide a direct measure of how it influences the model's ability to reason compositionally.
- What evidence would resolve it: Experiments designed to specifically test the model's compositional reasoning on tasks where tokenization plays a crucial role, comparing models with different pre-tokenization schemes.

### Open Question 3
- Question: What is the optimal strategy for token healing, and how does it interact with different tokenizer designs?
- Basis in paper: The paper introduces token healing as a technique to address biases introduced by tokenization at prompt boundaries and shows that it has a large impact on tokenizers with less defined boundaries, such as the Identity tokenizer.
- Why unresolved: While the paper demonstrates the effectiveness of token healing, it does not explore the optimal strategy for its implementation or how it interacts with different tokenizer designs.
- What evidence would resolve it: A comprehensive study of different token healing strategies, their impact on various tokenizer designs, and their effect on downstream performance across different tasks.

## Limitations
- The study focuses primarily on code generation tasks (HumanEval and MBPP benchmarks), limiting generalizability to other domains
- The exact implementation details of pre-tokenization regular expressions are not fully specified, making exact reproduction challenging
- The computational requirements are substantial (up to 500 billion tokens), creating practical barriers for verification
- The exact data composition used to train base models is not precisely specified, affecting reproducibility of baseline comparisons

## Confidence
- Claims about relative performance improvements between different pre-tokenization schemes: Medium (due to unspecified regex patterns)
- Claims about compression rates and their impact on generation speed: High (well-measured and directly observable)
- Claims about optimal vocabulary sizes for different use cases: Medium (based on specific experimental conditions)
- Claims about universal tokenizer design principles: Low (limited to code generation domain)

## Next Checks
1. Implement and test the specific pre-tokenization regular expressions (GPT-2, GPT-4, Punct) to verify their exact impact on compression rates and downstream performance
2. Reproduce the tokenizer switching experiments with smaller-scale training (5-50B tokens) to confirm the claimed convergence behavior and performance recovery patterns
3. Test the recommended GPT-4 pre-tokenization scheme on additional code generation benchmarks beyond HumanEval and MBPP to validate robustness across different coding tasks