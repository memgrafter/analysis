---
ver: rpa2
title: 'MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series'
arxiv_id: '2405.19327'
source_url: https://arxiv.org/abs/2405.19327
tags:
- data
- arxiv
- training
- language
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The MAP-Neo project addresses the lack of truly open, transparent,\
  \ and high-performance bilingual large language models by releasing a 7B parameter\
  \ model trained on 4.5T tokens of high-quality data. The core innovation lies in\
  \ providing full transparency across the entire development pipeline\u2014including\
  \ data cleaning, pre-training code, model architecture, checkpoints, and evaluation\
  \ frameworks\u2014while achieving state-of-the-art performance on benchmarks like\
  \ C-EVAL, MMLU, GSM8K, and HumanEval."
---

# MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series

## Quick Facts
- **arXiv ID:** 2405.19327
- **Source URL:** https://arxiv.org/abs/2405.19327
- **Reference count:** 40
- **Primary result:** 7B parameter bilingual LLM trained on 4.5T tokens, achieving state-of-the-art performance on benchmarks while providing full transparency across development pipeline

## Executive Summary
MAP-Neo is a 7B parameter bilingual large language model that addresses the lack of truly open, transparent, and high-performance models. The project introduces the NEO Scaling Law, which better predicts training dynamics for large, diverse datasets compared to prior scaling laws. MAP-Neo achieves state-of-the-art performance on benchmarks like C-EVAL, MMLU, GSM8K, and HumanEval while providing complete transparency through open-sourcing of data cleaning pipelines, pre-training code, model checkpoints, and evaluation frameworks.

## Method Summary
MAP-Neo employs a two-stage pre-training strategy using a 4.5T token corpus (Matrix Data Pile) with a transformer decoder architecture enhanced by multi-query attention, RoPE embeddings, RMSNorm, and SwiGLU activation. The model undergoes fundamental phase training (3,726B tokens) followed by decay phase training (778B tokens) with adjusted tokenizer settings. Alignment is achieved through supervised fine-tuning and iterative direct preference optimization. The entire development pipeline is open-sourced, including data cleaning procedures and evaluation frameworks.

## Key Results
- Achieves state-of-the-art performance on C-EVAL, MMLU, GSM8K, and HumanEval benchmarks
- Outperforms previous transparent LLMs and rivals closed-source models
- Introduces NEO Scaling Law with better predictions for large, diverse dataset training dynamics
- Sets new standard for open research with full transparency across development pipeline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The NEO Scaling Law better predicts training dynamics for large, diverse datasets compared to prior scaling laws.
- Mechanism: Introduces an additional regularization term (-d · log(D)) to account for the effect of diverse high-quality corpora on loss reduction, which is not captured by Chinchilla Law's B/D^β term.
- Core assumption: The training data contains multiple high-quality corpora sources, and their diversity leads to a faster decrease in loss than predicted by the Chinchilla Law.
- Evidence anchors: [abstract], [section 8.2], [corpus]

### Mechanism 2
- Claim: Full transparency across the entire development pipeline leads to improved model performance and reproducibility.
- Mechanism: By open-sourcing all details, including data cleaning, pre-training code, model architecture, checkpoints, and evaluation frameworks, the research community can validate, reproduce, and build upon the work, leading to faster progress and higher-quality models.
- Core assumption: The research community values transparency and will actively engage with the open-sourced materials to validate and improve the model.
- Evidence anchors: [abstract], [section 1], [corpus]

### Mechanism 3
- Claim: The two-stage pre-training strategy (fundamental phase and decay phase) improves model performance, especially on code generation tasks.
- Mechanism: The fundamental phase trains the model on a vast corpus of generic texts to develop general text generation capabilities, while the decay phase focuses on enhancing the reliability of the model's generated content by incorporating high-quality data and mode code data.
- Core assumption: The two-stage pre-training strategy effectively separates the acquisition of general abilities from the refinement of specific skills, leading to improved overall performance.
- Evidence anchors: [section 6], [section 6.2], [corpus]

## Foundational Learning

- **Large Language Models (LLMs)**: Understanding the basic concepts and capabilities of LLMs is crucial for comprehending the significance and challenges of building transparent and high-performance bilingual LLMs.
  - Why needed here: Understanding the basic concepts and capabilities of LLMs is crucial for comprehending the significance and challenges of building transparent and high-performance bilingual LLMs.
  - Quick check question: What are the key differences between open-source and closed-source LLMs, and why is transparency important in LLM development?

- **Scaling Laws**: Scaling laws are essential for predicting training configuration and optimizing the loss under computational resource constraints. Understanding the NEO Scaling Law and its advantages over prior scaling laws is crucial for efficient LLM training.
  - Why needed here: Scaling laws are essential for predicting training configuration and optimizing the loss under computational resource constraints.
  - Quick check question: How does the NEO Scaling Law differ from the Chinchilla Law, and what is the rationale behind introducing the additional regularization term (-d · log(D))?

- **Tokenization**: Tokenization plays a critical role in LLM training and performance. Understanding the tokenizer settings and their impact on different languages and tasks is essential for optimizing model performance.
  - Why needed here: Tokenization plays a critical role in LLM training and performance.
  - Quick check question: How do tokenizer settings, such as vocabulary size and sentence-piece length, affect the performance of bilingual LLMs on different tasks and languages?

## Architecture Onboarding

- **Component map**: Tokenizer → Model Architecture → Pre-training (fundamental phase) → Pre-training (decay phase) → Alignment (SFT and DPO) → Evaluation

- **Critical path**: Tokenizer → Model Architecture → Pre-training (fundamental phase) → Pre-training (decay phase) → Alignment (SFT and DPO) → Evaluation

- **Design tradeoffs**:
  - Tokenizer: Balancing computational efficiency and model performance by setting the vocabulary size to 64000 and constraining the max sentence-piece length to 16.
  - Model Architecture: Choosing between multi-head attention and multi-query attention based on model scale and ablation studies.
  - Pre-training: Allocating data between fundamental and decay phases to optimize general ability acquisition and specific skill enhancement.
  - Alignment: Balancing instruction-following abilities and chat abilities in the two-phase SFT approach.

- **Failure signatures**:
  - Tokenizer: Poor performance on specific languages or tasks due to suboptimal settings.
  - Model Architecture: Instability during training or suboptimal performance due to inappropriate design choices.
  - Pre-training: Slow convergence or suboptimal performance due to inadequate data allocation or scaling law predictions.
  - Alignment: Lack of alignment with human preferences or degradation of base model capabilities.

- **First 3 experiments**:
  1. Test the tokenizer with a small sample of bilingual text to ensure proper tokenization and identify any issues.
  2. Train a small-scale model (e.g., 250M parameters) using the fundamental phase pre-training strategy to validate the scaling law predictions and identify any issues.
  3. Evaluate the aligned model on a subset of benchmarks to ensure proper alignment and identify any areas for improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the NEO Scaling Law perform when applied to smaller models (e.g., 1-2B parameters) trained on diverse, high-quality datasets compared to homogeneous datasets?
- Basis in paper: [inferred] The authors suggest NEO Scaling Law is designed for large, diverse datasets but acknowledge limited evidence since few models are pre-trained across multiple large high-quality corpora.
- Why unresolved: The paper focuses on MAP-Neo's 7B model and comparisons with DeepSeek's 7B and 67B models. No experiments are shown for smaller models on diverse datasets.
- What evidence would resolve it: Experiments training 1-2B parameter models on both diverse and homogeneous datasets, comparing NEO Scaling Law predictions against actual loss curves and against Chinchilla Law predictions.

### Open Question 2
- Question: What is the impact of the tokenizer "remove extra whitespaces" parameter on model performance for different domains (e.g., code vs. natural language) when trained with large, diverse datasets?
- Basis in paper: [explicit] The authors discovered that disabling "remove extra whitespaces" significantly improved code metrics during training, while initially leaving it enabled caused code metrics to fluctuate.
- Why unresolved: The paper only discusses this issue for MAP-Neo's training and doesn't explore how different tokenizer settings affect performance across various domains or dataset compositions.
- What evidence would resolve it: Systematic experiments varying tokenizer settings (particularly "remove extra whitespaces") across different domain-specific datasets and measuring downstream performance in each domain.

### Open Question 3
- Question: How does the performance of MAP-Neo's document conversion pipeline compare to commercial OCR solutions when processing documents in low-resource languages or with complex layouts?
- Basis in paper: [inferred] The authors present a comprehensive document conversion framework but only evaluate it implicitly through the quality of their pre-training corpus. No direct comparison to commercial solutions is provided.
- Why unresolved: The paper describes the pipeline's components and capabilities but doesn't benchmark it against established commercial OCR systems or evaluate its effectiveness on challenging document types.
- What evidence would resolve it: Head-to-head comparisons of MAP-Neo's pipeline against commercial OCR solutions on benchmark datasets containing low-resource languages, complex layouts, and degraded document quality.

## Limitations

- The NEO Scaling Law's superiority over Chinchilla is primarily demonstrated through one model series, lacking extensive empirical validation across different model sizes and dataset compositions.
- The transparency claims, while impressive in scope, lack formal reproducibility validation through independent replication attempts.
- The claimed community benefits from open-sourcing are speculative, with no documented evidence of community adoption or derivative works.

## Confidence

- **High confidence**: Technical implementation details (architecture choices, training configurations, benchmark results) are well-documented and internally consistent.
- **Medium confidence**: NEO Scaling Law's theoretical advantages are plausible but empirical evidence for diverse dataset scenarios is limited.
- **Low confidence**: Claims about community impact from open-sourcing are speculative with no evidence of community adoption.

## Next Checks

1. **Scaling Law Validation**: Train multiple MAP-Neo variants with controlled data diversity levels (e.g., 100% English vs 50/50 English-Chinese) to empirically test whether the -d·log(D) term accurately predicts performance differences attributable to data diversity.

2. **Tokenizer Sensitivity Analysis**: Conduct systematic ablation studies varying vocabulary size (32k, 64k, 128k) and max sentence-piece length (8, 16, 32) specifically for code generation tasks to determine if the chosen settings are optimal or merely adequate.

3. **Pre-training Strategy Comparison**: Train parallel models using (a) single-stage pre-training, (b) curriculum learning, and (c) the proposed two-stage approach with identical total compute budgets to quantify the actual performance gains from the decay phase.