---
ver: rpa2
title: 'On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the
  Era of LLMs'
arxiv_id: '2407.19200'
source_url: https://arxiv.org/abs/2407.19200
tags: []
core_contribution: 'This paper presents a comprehensive survey of NLP model interpretability
  from the stakeholders'' perspective, addressing three key questions: why we need
  interpretability, what we are interpreting, and how we interpret. The authors propose
  four perspectives on the need for interpretability: algorithmic (for developers),
  business (for decision-makers), scientific (for researchers), and social (for society).'
---

# On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the Era of LLMs

## Quick Facts
- arXiv ID: 2407.19200
- Source URL: https://arxiv.org/abs/2407.19200
- Authors: Nitay Calderon; Roi Reichart
- Reference count: 40
- Primary result: Comprehensive survey of NLP model interpretability from stakeholder perspective reveals significant disparities between developer and non-developer needs

## Executive Summary
This paper presents a comprehensive survey of NLP model interpretability through the lens of different stakeholder needs, analyzing thousands of papers from 2015-2024. The authors identify four stakeholder perspectives (algorithmic, business, scientific, and social) and their distinct interpretability requirements. Through large-scale LLM annotation of papers, they reveal significant disparities between NLP developers and non-developer users, as well as between research fields. The study finds that while feature attributions remain dominant, natural language explanations are gaining popularity with LLMs, and accessibility drives cross-domain adoption more than technical superiority.

## Method Summary
The paper employs an LLM annotation pipeline to analyze thousands of NLP interpretability papers from 2015-2024, using Semantic Scholar API for retrieval. Papers are annotated for relevance, field, paradigm, mechanism, scope, accessibility, and causality properties using a zero-shot prompting approach. Manual verification on random samples ensures annotation quality, with correction rules applied to address systematic biases. The analysis reveals trends in interpretability paradigms and stakeholder needs across NLP and non-NLP fields.

## Key Results
- Significant disparities exist between NLP developers and non-developer stakeholders in interpretability method preferences
- Natural language explanations are rapidly gaining popularity, especially with the rise of LLMs (66.7% of 2024 NLP papers employ LLMs)
- Method accessibility drives cross-domain adoption more than technical superiority
- Feature attributions remain the dominant paradigm, but non-NLP fields show different preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stakeholder-aligned interpretability methods improve adoption across domains.
- Mechanism: When interpretability methods are designed with explicit stakeholder needs (algorithmic, business, scientific, social), they are more likely to be adopted in relevant fields outside NLP.
- Core assumption: Different stakeholder groups have distinct interpretability requirements that can be mapped to specific method properties.
- Evidence anchors:
  - [abstract] "overlook the needs and perspectives of explanation stakeholders"
  - [section] "By identifying different stakeholders' specific requirements and concerns, we can foster practical interpretability methods that align with their expectations"
  - [corpus] Weak - corpus shows many papers but lacks stakeholder classification
- Break condition: If stakeholder mapping proves inconsistent across domains or if method properties don't predict adoption patterns.

### Mechanism 2
- Claim: LLM capabilities drive paradigm shifts in interpretability methods.
- Mechanism: Strong text generation capabilities of LLMs enable new interpretability paradigms (like natural language explanations) that were previously impractical.
- Core assumption: Generation quality is the limiting factor for natural language explanations adoption.
- Evidence anchors:
  - [abstract] "the introduction of LLMs in the last two years has drastically improved the capabilities of NLP models"
  - [section] "The introduction of LLMs in the last two years has drastically improved the capabilities of NLP models"
  - [corpus] Strong - shows 66.7% of 2024 NLP papers employ LLMs
- Break condition: If generation quality improvements plateau or if other factors (like faithfulness) prove more limiting.

### Mechanism 3
- Claim: Method accessibility drives cross-domain adoption more than technical superiority.
- Mechanism: Non-ML domains prefer interpretability methods with well-documented code and simple interfaces over technically superior but complex methods.
- Core assumption: Technical expertise availability varies significantly across domains.
- Evidence anchors:
  - [section] "non-developers favor methods that do not require advanced technical skills, as generating textual explanations can be done through simple prompting"
  - [section] "LIME and SHAP packages are widely used across many domains beyond NLP"
  - [corpus] Strong - shows clustering and classic ML methods common outside NLP
- Break condition: If domain experts' technical capabilities improve or if method complexity becomes standardized.

## Foundational Learning

- Concept: Stakeholder perspective in interpretability
  - Why needed here: The paper's central contribution is analyzing interpretability through stakeholder lenses
  - Quick check question: Can you name the four stakeholder perspectives and their primary interpretability needs?

- Concept: Interpretability paradigm properties
  - Why needed here: The paper categorizes methods by properties like scope, access, and presentation
  - Quick check question: What's the difference between model-agnostic and model-specific interpretability methods?

- Concept: Causal-based vs correlational methods
  - Why needed here: The paper distinguishes between methods providing causal insights vs mere correlations
  - Quick check question: Why are causal-based methods considered more faithful than correlational ones?

## Architecture Onboarding

- Component map:
  Semantic Scholar API -> LLM Annotation Pipeline -> Manual Verification -> Analysis & Visualization

- Critical path:
  1. Query construction and retrieval
  2. LLM annotation of retrieved papers
  3. Manual verification of sample annotations
  4. Analysis of trends and patterns
  5. Visualization of results

- Design tradeoffs:
  - LLM annotation vs manual annotation (speed vs accuracy)
  - Broad vs narrow interpretability definitions
  - Field-specific vs general categorization schemes

- Failure signatures:
  - Low agreement between LLM and human annotations
  - Inconsistent field classifications
  - Missing paradigm annotations

- First 3 experiments:
  1. Test LLM annotation accuracy on a random sample of 100 papers
  2. Validate field classification consistency across multiple annotators
  3. Verify paradigm classification against known benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can interpretability methods be adapted to better serve non-developer stakeholders outside the NLP community?
- Basis in paper: [explicit] The paper highlights significant differences in interpretability method preferences between NLP developers and non-developer stakeholders, particularly in fields like healthcare, neuroscience, and social sciences.
- Why unresolved: While the paper identifies the disparity, it doesn't provide concrete solutions for adapting interpretability methods to better meet the needs of non-developer stakeholders.
- What evidence would resolve it: Empirical studies comparing the effectiveness of different interpretability methods for non-developer stakeholders across various fields, or case studies of successful adaptations.

### Open Question 2
- Question: What is the optimal balance between model-specific and model-agnostic interpretability methods for different stakeholder groups?
- Basis in paper: [explicit] The paper discusses the trade-offs between model-specific and model-agnostic methods, noting that non-developers often prefer the latter due to ease of use.
- Why unresolved: The paper doesn't provide a framework for determining the optimal balance based on stakeholder needs and use cases.
- What evidence would resolve it: Comparative studies of stakeholder satisfaction and task performance using different balances of model-specific and model-agnostic methods.

### Open Question 3
- Question: How can the faithfulness of natural language explanations generated by LLMs be improved?
- Basis in paper: [explicit] The paper notes the increasing use of natural language explanations with LLMs but also mentions concerns about their faithfulness.
- Why unresolved: While the paper identifies the issue, it doesn't propose specific solutions for improving the faithfulness of LLM-generated explanations.
- What evidence would resolve it: Development and evaluation of techniques to enhance the faithfulness of LLM-generated explanations, such as incorporating causal reasoning or aligning explanations with model decision-making processes.

## Limitations
- Reliance on LLM annotations may introduce systematic biases despite manual verification
- Limited empirical validation of whether identified stakeholder needs actually drive method adoption
- Relatively small sample size of non-NLP interpretability papers for cross-domain comparisons

## Confidence
- Stakeholder perspective claims: Medium confidence (limited empirical validation)
- Trend analysis results: High confidence for general patterns, Medium confidence for specific quantitative claims
- NLP vs non-NLP comparison: Medium confidence (small sample size of non-NLP papers)

## Next Checks
1. Conduct user studies with actual practitioners from each stakeholder group to validate whether identified needs align with their real-world interpretability requirements
2. Perform head-to-head comparison of LLM annotations with expert manual annotations on a larger validation set to quantify systematic biases
3. Track adoption rates of different interpretability methods in non-NLP domains over time to verify if accessibility correlates with actual usage patterns