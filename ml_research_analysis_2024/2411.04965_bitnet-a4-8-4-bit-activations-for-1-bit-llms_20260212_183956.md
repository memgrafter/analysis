---
ver: rpa2
title: 'BitNet a4.8: 4-bit Activations for 1-bit LLMs'
arxiv_id: '2411.04965'
source_url: https://arxiv.org/abs/2411.04965
tags:
- bitnet
- quantization
- activations
- training
- projection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BitNet a4.8, a method that enables 4-bit
  activations for 1-bit LLMs. The key innovation is a hybrid quantization and sparsification
  strategy that selectively applies 4-bit quantization to inputs of attention and
  feed-forward layers while using sparsification with 8-bit quantization for intermediate
  states.
---

# BitNet a4.8: 4-bit Activations for 1-bit LLMs

## Quick Facts
- arXiv ID: 2411.04965
- Source URL: https://arxiv.org/abs/2411.04965
- Authors: Hongyu Wang, Shuming Ma, Furu Wei
- Reference count: 22
- Key result: Enables 4-bit activations for 1-bit LLMs with performance comparable to 1.58-bit baselines while activating only 55% of parameters

## Executive Summary
This paper introduces BitNet a4.8, a method that enables 4-bit activations for 1-bit LLMs through a hybrid quantization and sparsification strategy. The approach selectively applies 4-bit quantization to attention and feed-forward network inputs while using 8-bit quantization with top-K sparsification for intermediate states containing outlier channels. Trained via a two-stage recipe requiring minimal additional training tokens, BitNet a4.8 achieves performance comparable to BitNet b1.58 while being faster at inference and supporting 3-bit KV cache.

## Method Summary
BitNet a4.8 employs a hybrid quantization approach where 4-bit quantization is applied to inputs of attention and feed-forward layers, while intermediate states undergo 8-bit quantization combined with top-K sparsification (50% sparsity). The model is trained from W1.58A8 to W1.58A4 using a two-stage recipe: first with 8-bit activations for 95B tokens, then continuing with 4-bit activations for 5B tokens using optimizer state reuse. Squared ReLU and GLU activation functions are used to achieve high activation sparsity without performance loss.

## Key Results
- Achieves performance comparable to BitNet b1.58 with only 55% of parameters activated
- Supports 3-bit KV cache with negligible accuracy loss
- Maintains performance while requiring only a few additional training tokens for adaptation
- Demonstrates faster inference speed compared to equivalent 1.58-bit models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid quantization and sparsification reduces outlier-induced quantization error while preserving accuracy
- Mechanism: Selectively applies 4-bit quantization to Gaussian-like inputs (attention and FFN) and uses 8-bit quantization with top-K sparsification for intermediate states containing outlier channels
- Core assumption: Outlier channels are concentrated in intermediate states, not in inputs to attention/FFN layers
- Evidence anchors: [abstract] "we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization"; [section] "Inputs to the attention and FFN layers typically follow a Gaussian-like distribution, while activations before the FFN down projections and the output projections in attention have more outlier channels"
- Break condition: If outlier channels are distributed across all layers rather than concentrated in intermediate states

### Mechanism 2
- Claim: Two-stage training enables efficient transition from 8-bit to 4-bit activations with minimal performance loss
- Mechanism: Model first trained with 8-bit activations for 95B tokens, then continues training with hybrid quantization/sparsification for 5B tokens using optimizer state reuse
- Core assumption: Model can adapt to lower bit-width activations with minimal additional training data
- Evidence anchors: [abstract] "BitNet a4.8 is trained with a two-stage recipe from W1.58A8 to W1.58A4... requires only a few training tokens to adapt"; [section] "BitNet a4.8 quickly adapts to 4-bit and sparse activations with only a few training tokens while having negligible loss on performance"
- Break condition: If model cannot adapt to 4-bit activations without significant performance degradation

### Mechanism 3
- Claim: Squared ReLU and GLU activation functions enable higher activation sparsity without performance loss
- Mechanism: Squared ReLU applied to FFN layers achieving over 80% sparsity in down projection inputs, while GLU gates further increase sparsity in up projections
- Core assumption: Activation sparsity can be increased significantly without degrading model performance
- Evidence anchors: [section] "with squared ReLU, the inputs to the down projection achieve over 80% sparsity with minimal impact on performance"; [section] "the outputs of gate projection ReLU2(XWTgate) exhibit high activation sparsity as well (e.g., 67.5% for 7B models)"
- Break condition: If increasing sparsity beyond certain thresholds causes significant performance degradation

## Foundational Learning

- Concept: Quantization error and its impact on model performance
  - Why needed here: Understanding how low-bit quantization introduces errors, especially for outlier values, is crucial for grasping why the hybrid approach is necessary
  - Quick check question: Why do outlier values cause larger quantization errors than typical values?

- Concept: Activation distributions in neural networks
  - Why needed here: The method relies on understanding that different layers have different activation distribution patterns (Gaussian vs. long-tailed)
  - Quick check question: How would you identify which layers have outlier-heavy distributions?

- Concept: Sparsification techniques and their trade-offs
  - Why needed here: The method uses top-K sparsification for intermediate states, requiring understanding of how sparsification works and its benefits/drawbacks
  - Quick check question: What is the trade-off between sparsity level and computational efficiency?

## Architecture Onboarding

- Component map: Input layer (4-bit quantization) -> Attention and FFN inputs (4-bit quantization) -> Intermediate states (8-bit quantization with top-K sparsification) -> Gate projections (Squared ReLU) -> Output layer (mixed precision)
- Critical path: The attention and FFN computation path is critical, as these are the main computational bottlenecks and benefit most from the 4-bit activations
- Design tradeoffs:
  - Precision vs. efficiency: 4-bit vs 8-bit vs 16-bit quantization
  - Sparsity vs. accuracy: Higher sparsity reduces computation but may affect performance
  - Training complexity vs. inference efficiency: Two-stage training adds complexity but enables better inference performance
- Failure signatures:
  - Performance degradation: Indicates quantization errors are too high
  - Training divergence: May occur with improper quantization parameters
  - Memory issues: Could arise from incorrect KV cache quantization
- First 3 experiments:
  1. Verify distribution patterns across different layers using a pre-trained BitNet b1.58 model
  2. Test different sparsity levels (e.g., 30%, 50%, 70%) to find optimal trade-off
  3. Compare full 4-bit quantization vs hybrid approach on a small model to validate the mechanism

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important questions about the scalability and generalizability of the approach.

## Limitations
- Relies heavily on the assumption that outlier channels are concentrated in intermediate states rather than in inputs to attention and FFN layers
- Limited evidence supporting the claim that only "a few training tokens" are needed for adaptation from 8-bit to 4-bit activations
- Long-term stability of high sparsity levels (over 80%) across extended training or different tasks remains uncertain

## Confidence

**High Confidence**: The core methodology of hybrid quantization (4-bit for attention/FFN inputs, 8-bit with sparsification for intermediates) is well-defined and the implementation details are sufficiently specified for reproduction.

**Medium Confidence**: The performance claims on benchmark tasks are reasonable given the methodology, but the limited comparison to other quantization approaches and the narrow task set reduce confidence in generalizability.

**Low Confidence**: The foundational assumptions about activation distributions and the transferability of the two-stage training approach lack strong empirical support in the literature, making these critical premises vulnerable to invalidation.

## Next Checks

1. **Distribution Validation**: Conduct systematic analysis of activation distributions across all layers in BitNet b1.58 models trained on different datasets to verify whether outlier channels are indeed concentrated in intermediate states as claimed.

2. **Transfer Learning Robustness**: Test the two-stage training approach across multiple model sizes (7B, 13B, 70B) and different initialization points to determine whether the "few training tokens" claim holds consistently.

3. **Sparsity Stability Analysis**: Implement long-term training stability tests with varying sparsity levels (60%, 70%, 80%, 90%) to identify the precise thresholds where performance degradation begins.