---
ver: rpa2
title: 'MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs'
arxiv_id: '2407.01509'
source_url: https://arxiv.org/abs/2407.01509
tags:
- arxiv
- score
- instruction
- component
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MIA-Bench, a new benchmark designed to evaluate
  multimodal large language models (MLLMs) on their ability to strictly adhere to
  complex instructions. The benchmark consists of 400 image-prompt pairs, each crafted
  to challenge the models' compliance with layered instructions in generating accurate
  responses that satisfy specific requested patterns.
---

# MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs

## Quick Facts
- arXiv ID: 2407.01509
- Source URL: https://arxiv.org/abs/2407.01509
- Authors: Yusu Qian; Hanrong Ye; Jean-Philippe Fauconnier; Peter Grasch; Yinfei Yang; Zhe Gan
- Reference count: 16
- Key outcome: GPT-4o achieved the best score of 88.58 on MIA-Bench, highlighting the benchmark's effectiveness in evaluating instruction adherence.

## Executive Summary
This paper introduces MIA-Bench, a new benchmark designed to evaluate multimodal large language models' (MLLMs) ability to strictly adhere to complex instructions. The benchmark consists of 400 image-prompt pairs with layered instructions across eight categories. Using GPT-4o as a judge model, the authors evaluate state-of-the-art MLLMs and find significant variations in performance. They also explore supervised fine-tuning with additional instruction-tuning data to improve instruction adherence without compromising performance on other tasks.

## Method Summary
MIA-Bench is constructed using 400 image-prompt pairs from sources like COCO 2017, SBU, TextVQA, and Flickr, covering diverse content categories. Each prompt contains layered instructions across eight categories (description, length limit, mention, genre, grammar, math, perspective, and OCR). A judge model (GPT-4o) evaluates responses based on strict adherence to sub-instructions using a weighted scoring template. The authors also conduct supervised fine-tuning experiments using additional instruction-tuning data constructed from 1000 COCO images to improve instruction adherence capabilities.

## Key Results
- GPT-4o achieved the highest score of 88.58 on MIA-Bench, significantly outperforming other state-of-the-art MLLMs
- Models showed varying performance across different instruction categories, with description and length limit being the most challenging
- Supervised fine-tuning on instruction-tuning data showed promising improvements in instruction adherence
- Rankings on MIA-Bench differ significantly from rankings on other multimodal benchmarks, suggesting instruction adherence is a distinct capability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using a judge model (GPT-4o) to evaluate responses based on strict adherence to sub-instructions ensures objective scoring.
- **Mechanism:** The judge model decomposes each instruction into sub-components, assigns weights, and scores responses on compliance with each sub-instruction. This structured approach reduces subjective bias and provides granular feedback.
- **Core assumption:** The judge model can accurately parse and evaluate the compliance of responses to complex, layered instructions.
- **Evidence anchors:**
  - [abstract] "Evaluation results from a wide array of state-of-the-art MLLMs reveal significant variations in performance, highlighting areas for improvement in instruction fidelity."
  - [section] "To automatically evaluate MLLMs’ performance with the proposed MIA-Bench at scale, we adopt GPT-4o (OpenAI, 2024) as a judge model to score MLLMs’ responses on each instruction and return a total score based on different criteria mentioned above."
- **Break condition:** If the judge model cannot reliably interpret the instruction intent or if the instruction phrasing is ambiguous, the scoring may be inconsistent or unfair.

### Mechanism 2
- **Claim:** Supervised fine-tuning (SFT) on instruction-tuned data improves instruction adherence without harming other tasks.
- **Mechanism:** Additional training data with diverse, complex instructions is generated and used to fine-tune models. This targeted training enhances the model's ability to follow instructions while maintaining performance on other benchmarks.
- **Core assumption:** The additional SFT data effectively captures the complexity and diversity of real-world instructions.
- **Evidence anchors:**
  - [abstract] "we create extra training data and explore supervised fine-tuning to enhance the models’ ability to strictly follow instructions without compromising performance on other tasks."
  - [section] "Using LLaV A-NeXT as the backbone, we train the model for 1 epoch on the constructed diverse instruction-tuning (DIT) data."
- **Break condition:** If the SFT data is not representative or diverse enough, the model may overfit to specific instruction patterns and fail on unseen instructions.

### Mechanism 3
- **Claim:** Comparing rankings on MIA-Bench with other multimodal benchmarks reveals that instruction adherence is a distinct capability.
- **Mechanism:** The paper compares the rankings of models on MIA-Bench with their rankings on other multimodal benchmarks. The discrepancies highlight that excelling in general multimodal tasks does not necessarily translate to superior instruction adherence.
- **Core assumption:** The rankings on other benchmarks are accurate and reflective of the models' general capabilities.
- **Evidence anchors:**
  - [section] "Our findings reveal a discrepancy between the two sets of rankings. Notably, InternVL-Chat-V1.5, which holds the highest meta-ranking among the five MLLMs on the other benchmarks, ranks the lowest on MIA-Bench."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.453, average citations=0.0. Top related titles: Empowering Reliable Visual-Centric Instruction Following in MLLMs, MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks..."
- **Break condition:** If the other benchmarks are not comprehensive or if the models' performance on those benchmarks is not stable, the comparison may not accurately reflect the distinction between instruction adherence and general multimodal capabilities.

## Foundational Learning

- **Concept: Multimodal Large Language Models (MLLMs)**
  - **Why needed here:** Understanding the capabilities and limitations of MLLMs is crucial for designing effective benchmarks and training methods.
  - **Quick check question:** What are the key differences between text-only LLMs and MLLMs in terms of their architecture and training data?

- **Concept: Instruction Following and Adherence**
  - **Why needed here:** The ability to strictly follow complex instructions is the core focus of MIA-Bench. Understanding what constitutes instruction adherence and how to measure it is essential.
  - **Quick check question:** How does instruction adherence differ from general task completion in the context of MLLMs?

- **Concept: Supervised Fine-Tuning (SFT)**
  - **Why needed here:** SFT is used to improve the instruction adherence capabilities of MLLMs. Understanding the principles and best practices of SFT is crucial for effective implementation.
  - **Quick check question:** What are the key considerations when designing SFT data for improving instruction adherence in MLLMs?

## Architecture Onboarding

- **Component map:** Image-Prompt Pairs -> Judge Model (GPT-4o) -> Evaluation Scores -> SFT Data Generation -> Fine-tuned Models
- **Critical path:** Image collection and annotation → Instruction generation and categorization → Response generation from MLLMs → Judge model evaluation → Performance analysis and SFT
- **Design tradeoffs:** Using GPT-4o as the judge model ensures objective scoring but may introduce bias towards its own responses; generating diverse SFT data improves instruction adherence but requires significant effort and may not be fully representative; focusing on strict instruction adherence may limit the model's ability to generalize to more open-ended tasks.
- **Failure signatures:** Inconsistent scoring by the judge model due to ambiguous instructions; overfitting to specific instruction patterns during SFT; poor performance on open-ended tasks due to over-emphasis on strict instruction adherence.
- **First 3 experiments:**
  1. Evaluate the performance of different MLLMs on a subset of MIA-Bench instructions to identify the most challenging categories.
  2. Generate a small set of SFT data and fine-tune a model to assess the impact on instruction adherence.
  3. Compare the rankings of models on MIA-Bench with their rankings on other multimodal benchmarks to validate the distinctness of instruction adherence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal data mixture for supervised fine-tuning to maximize instruction adherence while maintaining performance on other benchmarks?
- Basis in paper: [explicit] The paper mentions that SFT using a combination of DIT data and LLaVA Visual Instruct 150K dataset did not significantly improve model performance on tested benchmarks, but further analysis is needed to determine the optimal mixture.
- Why unresolved: The paper only explores one data mixture (DIT + LLaVA Visual Instruct 150K) and finds it ineffective, but does not systematically test different ratios or combinations of instruction-tuning data with other datasets.
- What evidence would resolve it: Systematic experiments testing various ratios of DIT data to LLaVA Visual Instruct 150K, or testing other instruction-tuning datasets like IFEval or InfoBench, to identify the optimal mixture that maximizes instruction adherence while preserving general performance.

### Open Question 2
- Question: How does instruction adherence performance generalize to more complex or ambiguous real-world instructions beyond the controlled MIA-Bench prompts?
- Basis in paper: [inferred] The paper acknowledges that "the real world presents an infinite variety of instructions, many of which may pose significant challenges for MLLMs," suggesting limitations in how well benchmark performance translates to real-world scenarios.
- Why unresolved: MIA-Bench uses carefully crafted, answerable prompts, but real-world instructions may be more ambiguous, contradictory, or context-dependent, which could reveal additional limitations not captured by the benchmark.
- What evidence would resolve it: Evaluation of MLLMs on a diverse collection of real-world instruction-following tasks from actual user interactions, measuring both adherence to instructions and robustness to ambiguous or contradictory prompts.

### Open Question 3
- Question: What is the relationship between instruction adherence capabilities and other multimodal reasoning abilities, and can instruction adherence be improved without sacrificing reasoning performance?
- Basis in paper: [explicit] The paper notes a discrepancy between MIA-Bench rankings and meta-rankings on other benchmarks (MME, MMMU, MMBench, MMVet, HallusionBench, MathVista), suggesting that instruction adherence is a distinct capability from general multimodal reasoning.
- Why unresolved: While the paper demonstrates that instruction adherence is separate from other capabilities, it does not explore whether improvements in instruction adherence through methods like SFT negatively impact reasoning performance, or whether both can be improved simultaneously.
- What evidence would resolve it: Comprehensive evaluation of models after instruction adherence fine-tuning on both MIA-Bench and a suite of reasoning benchmarks, with correlation analysis to determine if improvements in one domain come at the expense of the other.

## Limitations

- The benchmark relies on GPT-4o as both a participant and judge, creating potential circular validation
- The supervised fine-tuning experiments are limited in scope, with only 1 epoch of training on 1000 images
- The benchmark's focus on strict instruction adherence may not capture all aspects of real-world instruction following
- The effectiveness of the SFT approach on other instruction-tuning datasets remains unexplored

## Confidence

**High Confidence:** The benchmark construction methodology, dataset composition, and evaluation framework are well-documented and reproducible. The performance rankings of different MLLMs on MIA-Bench appear consistent and well-supported by the evidence.

**Medium Confidence:** The effectiveness of the supervised fine-tuning approach in improving instruction adherence is supported by initial results, but the limited scope of experiments prevents strong generalization. The claim that instruction adherence is a distinct capability separate from general multimodal performance is supported by comparative rankings but would benefit from additional validation.

**Low Confidence:** The generalizability of MIA-Bench to real-world applications and its ability to capture all aspects of instruction adherence remain uncertain without broader testing across diverse domains and instruction types.

## Next Checks

1. **Judge Model Validation:** Run MIA-Bench evaluations using multiple judge models (including open-source alternatives to GPT-4o) to assess consistency and identify potential biases in the scoring system.

2. **Cross-Cultural Instruction Testing:** Evaluate MLLM performance on MIA-Bench using instructions crafted by diverse annotators from different cultural backgrounds to test the benchmark's generalizability.

3. **Long-Term Instruction Adherence Study:** Conduct a longitudinal study comparing the instruction adherence capabilities of SFT models before and after fine-tuning, using both MIA-Bench and real-world instruction-following tasks to validate practical improvements.