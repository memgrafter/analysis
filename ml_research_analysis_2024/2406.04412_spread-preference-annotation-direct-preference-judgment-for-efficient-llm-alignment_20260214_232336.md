---
ver: rpa2
title: 'Spread Preference Annotation: Direct Preference Judgment for Efficient LLM
  Alignment'
arxiv_id: '2406.04412'
source_url: https://arxiv.org/abs/2406.04412
tags:
- preference
- data
- learning
- selfee
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SELFEE, a framework that aligns large language
  models using only a small amount of human-labeled preference data. The core idea
  is to iteratively generate responses and derive preference labels from the model's
  own logits, then refine these labels using a noise-aware learning algorithm.
---

# Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment

## Quick Facts
- **arXiv ID**: 2406.04412
- **Source URL**: https://arxiv.org/abs/2406.04412
- **Reference count**: 40
- **Primary result**: SELFEE improves AlpacaEval2.0 win rate by 16.4% using only 3.3% of Ultrafeedback labels

## Executive Summary
SELFEE is a framework for aligning large language models with human preferences using minimal human-labeled preference data. The method iteratively generates responses and derives preference labels directly from model logits, then refines these labels using a noise-aware learning algorithm. By propagating human preference knowledge from small seed datasets through self-generated preference data, SELFEE achieves strong performance without external reward models or extensive human annotation. The approach demonstrates significant improvements across multiple benchmarks while maintaining robustness across different model sizes.

## Method Summary
SELFEE aligns LLMs through an iterative process starting with a small seed preference dataset. The method generates response pairs from the current model, derives preference labels from logits using a reference model, and applies confidence-based label refinement to reduce noise. These self-generated preference pairs are then used to fine-tune the model via Direct Preference Optimization (DPO). The process repeats for multiple iterations, each time expanding the preference dataset and improving alignment. A key innovation is the decoupled noise detection mechanism that approximates predictions of a more aligned model through linear logit interpolation to better identify noisy labels.

## Key Results
- Achieved 16.4% improvement on AlpacaEval2.0 win rate using only 3.3% of Ultrafeedback preference labels
- Outperformed state-of-the-art baselines including PairRM and LLM-as-judge across multiple benchmarks
- Demonstrated effectiveness without seed data and across different model sizes including Mistral-7B and Phi-2
- Maintained performance while controlling for length bias in generated responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SELFEE leverages intrinsic model preference by deriving labels directly from LLM logits, bypassing external reward modeling or in-context learning.
- Mechanism: During data expansion, two responses are sampled from the current model and a preference is computed from their logits using the reference model as a baseline.
- Core assumption: The model's own logits reflect its inherent preference structure and correlate with human preferences.
- Evidence anchors: Abstract states SELFEE "derive[s] the preference label from the logits of LLM to explicitly extract the model's inherent preference."
- Break condition: If the logits are poorly calibrated or the reference model is misaligned, the self-derived labels will propagate incorrect preferences.

### Mechanism 2
- Claim: Self-refinement reduces noise in preference labels by smoothing based on model confidence.
- Mechanism: Confidence is measured by the current model's preference probability; low-confidence pairs are smoothed toward random preference.
- Core assumption: Confidence in a preference judgment correlates with label correctness, and smoothing improves robustness.
- Evidence anchors: Paper introduces "confidence-based refinement of preference labels to reduce the risk of noise in preference learning."
- Break condition: If the confidence estimator is miscalibrated or the smoothing parameter is too aggressive, learning will be too slow or ineffective.

### Mechanism 3
- Claim: Decoupled noise detection approximates a more aligned model's predictions to better identify noisy labels.
- Mechanism: The logit of the current model is linearly extrapolated toward the reference model to mimic a stronger aligned model's predictions.
- Core assumption: Linearly interpolated logits approximate predictions of a more strongly aligned model, improving noise detection.
- Evidence anchors: Paper proposes "using a linearly extrapolated prediction between the current and reference models."
- Break condition: If the linear approximation poorly represents a more aligned model, noise detection will be inaccurate.

## Foundational Learning

- Concept: Preference Learning (Bradley-Terry model)
  - Why needed here: SELFEE relies on pairwise preference judgments to fine-tune the model toward human-aligned outputs.
  - Quick check question: How does the Bradley-Terry model translate pairwise preferences into a probability of one response being preferred over another?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: SELFEE uses DPO to update the model from the self-generated preference data without explicit reward modeling.
  - Quick check question: In DPO, what role does the reference model play in shaping the preference probability?

- Concept: Label Smoothing
  - Why needed here: Used in the self-refinement stage to reduce the impact of low-confidence preference labels.
  - Quick check question: How does label smoothing adjust the target distribution when a label is suspected to be noisy?

## Architecture Onboarding

- Component map: Initial SFT model → seed preference dataset → iteration loop (response generation → preference judgment → self-refinement → DPO fine-tuning) → evaluation
- Critical path: Generate responses → derive preference labels from logits → identify noisy pairs → apply label smoothing → train with DPO
- Design tradeoffs: Using self-derived labels speeds up data generation but risks propagating model biases; adding refinement steps increases training time but improves label quality
- Failure signatures: Low confidence detection fails if logits are poorly calibrated; excessive smoothing can stall learning; incorrect reference model alignment can misguide preference derivation
- First 3 experiments:
  1. Run one iteration of response generation and preference judgment without refinement; measure win rate improvement.
  2. Add self-refinement with fixed smoothing; compare win rate and response length bias.
  3. Introduce decoupled noise detection with different linear interpolation parameters; evaluate impact on win rate and length control.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does SELFEE's preference judgment method based on LLM logits generalize across different model architectures and sizes?
- Basis in paper: The paper states that SELFEE successfully improves alignment for various models including Mistral-7B and Phi-2, and performs well without seed data.
- Why unresolved: While the paper demonstrates effectiveness across different models, it doesn't explore the limits of this generalization - e.g., whether it works for much smaller models, or models with different architectures like encoder-decoder vs decoder-only.
- What evidence would resolve it: Experiments testing SELFEE on a wide range of model sizes (from small to large) and architectures, with quantitative comparisons of performance.

### Open Question 2
- Question: What is the impact of SELFEE on the diversity of model responses over multiple iterations?
- Basis in paper: The paper notes that SELFEE tends to increase response length and improve win rates, but doesn't analyze whether it affects the diversity or variety of generated responses.
- Why unresolved: While SELFEE improves alignment and performance metrics, it's unclear if this comes at the cost of reduced response diversity, which could be problematic for certain applications.
- What evidence would resolve it: Analysis of response diversity metrics (e.g., n-gram diversity, semantic similarity measures) across iterations of SELFEE training.

### Open Question 3
- Question: How does the de-coupled noise detection mechanism scale with the amount of preference data available?
- Basis in paper: The paper introduces de-coupled noise detection using a linearly extrapolated prediction between current and reference models, claiming it improves noise identification.
- Why unresolved: The paper only tests this mechanism with limited data (3.3% of Ultrafeedback). It's unclear how well it performs when more or less preference data is available.
- What evidence would resolve it: Experiments varying the amount of seed preference data (e.g., 0.5%, 3.3%, 10%, 50%, 100%) and measuring the effectiveness of the de-coupled noise detection at each level.

## Limitations

- SELFEE's effectiveness relies on empirical comparisons that lack independent replication and are based on the authors' own evaluation pipeline
- The method's generalization to domains beyond instruction following and different preference distributions is not demonstrated
- The correlation between self-derived preference labels and human preferences is not directly validated outside of final task performance

## Confidence

- **High confidence**: The core algorithmic framework (iterative preference generation + DPO training) is clearly described and reproducible
- **Medium confidence**: Relative improvements over baselines are well-documented but comparisons are limited to single dataset and model size
- **Low confidence**: The claim that logit-based self-preference judgments reliably approximate human preferences is only weakly supported

## Next Checks

1. **Ablation of label smoothing parameters**: Systematically vary the confidence threshold K and smoothing intensity to measure their impact on win rate and length bias

2. **Independent evaluation of self-derived labels**: Use a held-out set of human-annotated preferences to directly compare the accuracy of SELFEE's self-derived labels versus external reward model predictions

3. **Cross-dataset generalization**: Apply SELFEE to a different preference learning dataset (e.g., Anthropic's HH-RLHF) and measure performance on a corresponding benchmark