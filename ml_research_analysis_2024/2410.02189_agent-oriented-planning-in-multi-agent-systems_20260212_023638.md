---
ver: rpa2
title: Agent-Oriented Planning in Multi-Agent Systems
arxiv_id: '2410.02189'
source_url: https://arxiv.org/abs/2410.02189
tags:
- agent
- agents
- task
- sub-task
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AOP, a framework for agent-oriented planning
  in multi-agent systems. AOP addresses the challenges of decomposing user queries
  into sub-tasks and assigning them to appropriate agents.
---

# Agent-Oriented Planning in Multi-Agent Systems

## Quick Facts
- arXiv ID: 2410.02189
- Source URL: https://arxiv.org/abs/2410.02189
- Reference count: 40
- Key outcome: AOP achieves 43.7% accuracy on Husky-QA, outperforming best baseline (39.6%)

## Executive Summary
This paper introduces AOP (Agent-Oriented Planning), a framework for agent-oriented planning in multi-agent systems. AOP addresses the challenge of decomposing user queries into sub-tasks and assigning them to appropriate agents. The framework incorporates three design principles: solvability, completeness, and non-redundancy. AOP uses a fast decomposition and allocation process followed by evaluation via a reward model and a feedback loop for continuous improvement. Experiments on numerical reasoning datasets show that AOP significantly outperforms single-agent systems and existing planning strategies for multi-agent systems.

## Method Summary
AOP is a framework for agent-oriented planning in multi-agent systems that decomposes user queries into sub-tasks and allocates them to appropriate agents. The framework follows three design principles: solvability (ensuring tasks can be resolved by single agents), completeness (capturing all key information), and non-redundancy (avoiding overlapping content). The method involves a fast decomposition and allocation process, followed by evaluation using a reward model to assess sub-task solvability without actual agent calls. A detector identifies missing information or redundancy in sub-tasks, and a feedback loop with representative works enables continuous improvement. The framework was evaluated on the Husky-QA dataset and numerical reasoning tasks, showing significant improvements over single-agent and existing multi-agent planning approaches.

## Key Results
- AOP achieves 43.7% accuracy on Husky-QA dataset compared to 39.6% for the best baseline
- The framework outperforms single-agent systems and existing planning strategies for multi-agent systems
- Ablation studies demonstrate the effectiveness of the reward model, detector, and representative works mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AOP uses a reward model to predict whether sub-tasks can be resolved by single agents without actual agent calls
- Mechanism: The reward model is trained on sub-task/agent pairs with ground truth scores from an LLM scorer. It embeds the sub-task and agent description, then predicts a solvability score. Low scores trigger replanning or task decomposition
- Core assumption: Sub-task/agent pair embeddings capture sufficient information to predict resolution quality
- Evidence anchors:
  - [abstract]: "AOP incorporates a reward model designed to efficiently evaluate the solvability of sub-tasks without requiring actual agent calls"
  - [section 4.2]: "We introduce a reward model that provides efficient evaluations of the solvability of sub-tasks, which aims to predict the quality of the agents' responses to sub-tasks without necessitating actual agent calls"
  - [corpus]: Weak - no direct corpus evidence on reward model effectiveness
- Break condition: If the reward model's embeddings don't capture agent capability nuances, predictions will be unreliable

### Mechanism 2
- Claim: The detector identifies missing information and redundancy in sub-tasks using role-play prompting
- Mechanism: The detector acts as a plan evaluator that extracts key elements from the original query and checks if sub-tasks cover all requirements. It also checks for redundant information between sub-tasks
- Core assumption: LLM-based role-play prompting can effectively detect completeness and redundancy issues
- Evidence anchors:
  - [abstract]: "a detector is utilized to identify the missing key information or redundant content in the decomposed sub-tasks"
  - [section 4.4]: "we involve a detector, implemented by providing a role-play prompt to LLMs, to evaluate both the completeness and non-redundancy"
  - [corpus]: Weak - no direct corpus evidence on detector effectiveness
- Break condition: If the LLM-based detector fails to capture nuanced completeness requirements, important information may be missed

### Mechanism 3
- Claim: Representative works mechanism stores successful sub-task/agent pairs for future task modification
- Mechanism: When a sub-task is successfully resolved by an agent, it's added to that agent's representative works. New sub-tasks are compared to these works to determine if they need re-description or detailed planning
- Core assumption: Similar sub-tasks indicate similar solvability requirements
- Evidence anchors:
  - [abstract]: "we propose to construct a set of representative works that record the sub-tasks it has completely resolved"
  - [section 4.3]: "we propose to construct a set of representative works that record the sub-tasks it has completely resolved"
  - [corpus]: Weak - no direct corpus evidence on representative works effectiveness
- Break condition: If sub-tasks are superficially similar but require different approaches, representative works may lead to incorrect modifications

## Foundational Learning

- Concept: Multi-agent task decomposition and allocation
  - Why needed here: AOP is fundamentally about breaking down user queries into sub-tasks and assigning them to appropriate agents
  - Quick check question: What are the three design principles AOP follows for task decomposition?

- Concept: Reward model training and inference
  - Why needed here: The reward model predicts sub-task solvability without actual agent calls, requiring understanding of embedding-based prediction
  - Quick check question: How does the reward model combine sub-task and agent description embeddings?

- Concept: Role-play prompting for evaluation
  - Why needed here: The detector uses role-play prompting to evaluate plan completeness and redundancy
  - Quick check question: What are the two main evaluation criteria for the detector?

## Architecture Onboarding

- Component map: User query -> Fast decomposition -> Reward model evaluation -> Detector check -> Agent execution -> Feedback loop
- Critical path: User query → Fast decomposition → Reward model evaluation → Detector check → Agent execution → Feedback loop
- Design tradeoffs:
  - Fast decomposition vs. thorough planning: Initial fast approach trades some accuracy for speed
  - Reward model complexity vs. efficiency: More complex models could improve accuracy but reduce speed
  - Detector reliance on LLM: Effective but adds computational overhead
- Failure signatures:
  - Low accuracy: Likely reward model or detector failures
  - High token usage: May indicate inefficient sub-task decomposition
  - Long execution times: Could suggest suboptimal agent allocation
- First 3 experiments:
  1. Compare accuracy with and without reward model on a small dataset
  2. Test detector effectiveness by manually introducing completeness issues
  3. Evaluate representative works impact by comparing with and without this mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reward model handle scenarios where no single agent can fully resolve a sub-task?
- Basis in paper: [explicit] Section 4.2 describes the reward model's role in evaluating solvability but doesn't detail handling cases where all agents fail
- Why unresolved: The paper mentions that the meta-agent needs to "replan" in such cases but doesn't specify the replanning strategy or how to determine when all agents are insufficient
- What evidence would resolve it: Empirical results showing success rates of different replanning strategies, or a detailed algorithm for handling complete solvability failures

### Open Question 2
- Question: What is the impact of different similarity thresholds in the representative works mechanism on overall system performance?
- Basis in paper: [inferred] Section 4.3 mentions a similarity threshold for updating representative works but doesn't explore how threshold selection affects outcomes
- Why unresolved: The paper states a threshold is used but doesn't provide sensitivity analysis or optimal threshold determination methodology
- What evidence would resolve it: Experimental results showing performance across different threshold values, or an adaptive threshold selection algorithm

### Open Question 3
- Question: How does the system handle conflicting or contradictory responses from different agents?
- Basis in paper: [inferred] The framework mentions collecting responses from multiple agents but doesn't detail conflict resolution mechanisms
- Why unresolved: While the reward model evaluates individual agent responses, there's no discussion of how to reconcile conflicting information from different agents
- What evidence would resolve it: A conflict resolution algorithm, or experimental results showing how the system handles contradictory agent outputs

### Open Question 4
- Question: What are the computational limits of the detector component as task complexity increases?
- Basis in paper: [explicit] Section 4.4 introduces the detector but doesn't discuss its computational complexity or scalability
- Why unresolved: The paper mentions the detector's effectiveness but doesn't analyze its computational requirements or performance degradation with complex tasks
- What evidence would resolve it: Computational complexity analysis, scalability experiments, or performance benchmarks at different task complexity levels

## Limitations
- The effectiveness of individual components (reward model, detector, representative works) lacks direct corpus evidence
- Critical implementation details for reward model training and system prompts remain unclear
- No discussion of how the system handles conflicting responses from different agents

## Confidence
- Reward model effectiveness: Low confidence (weak corpus evidence)
- Detector effectiveness: Low confidence (weak corpus evidence)
- Overall framework performance: Medium confidence (results shown but component validation lacking)

## Next Checks
1. **Reward model validation**: Create a small validation set with ground truth solvability scores and test whether the reward model accurately predicts which agent-subtask pairs will succeed without actual execution.

2. **Detector effectiveness test**: Manually introduce completeness and redundancy issues into decomposed plans and verify whether the detector reliably catches these problems across different query types.

3. **Representative works impact**: Compare AOP performance with and without the representative works mechanism on a subset of tasks to isolate its contribution to overall accuracy improvements.