---
ver: rpa2
title: Mobile Video Diffusion
arxiv_id: '2412.07583'
source_url: https://arxiv.org/abs/2412.07583
tags:
- video
- temporal
- diffusion
- blocks
- unet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MobileVD, the first mobile-optimized video
  diffusion model, addressing the high computational demands that limit video diffusion
  models on mobile devices. Starting from Stable Video Diffusion (SVD), the authors
  reduce memory and computational costs through frame resolution reduction, multi-scale
  temporal representations, and novel pruning techniques to reduce channels and temporal
  blocks.
---

# Mobile Video Diffusion

## Quick Facts
- arXiv ID: 2412.07583
- Source URL: https://arxiv.org/abs/2412.07583
- Reference count: 40
- Primary result: MobileVD achieves 523x efficiency improvement (1817.2 vs 4.34 TFLOPs) with FVD 149 vs 171

## Executive Summary
This paper introduces MobileVD, the first mobile-optimized video diffusion model that addresses the high computational demands limiting video diffusion models on mobile devices. Starting from Stable Video Diffusion (SVD), the authors reduce memory and computational costs through frame resolution reduction, multi-scale temporal representations, and novel pruning techniques to reduce channels and temporal blocks. They also employ adversarial fine-tuning to reduce denoising to a single step. MobileVD achieves 523x efficiency improvement while maintaining reasonable quality, generating 14x512x256 px video clips in 1.7 seconds on a Xiaomi-14 Pro.

## Method Summary
MobileVD optimizes SVD for mobile deployment through five key techniques: (1) low-resolution finetuning at 512×256 pixels instead of 1024×576, (2) temporal multiscaling using max-pooling to reduce temporal dimension, (3) optimized cross-attention that removes unnecessary computations when conditioning on single tokens, (4) channel funneling that reduces intermediate channel dimensions through funnel matrices, and (5) temporal block pruning that removes less important temporal components through learned importance values. The model is trained with adversarial fine-tuning to reduce denoising steps to one, followed by additional training with temporal block pruning.

## Key Results
- 523x efficiency improvement (1817.2 vs 4.34 TFLOPs) while maintaining reasonable quality
- Generates 14x512x256 px video clips in 1.7 seconds on Xiaomi-14 Pro
- FVD score increases from 149 to 171 (slight quality drop)
- 14.4x reduction in parameters from 1.17B to 81.1M
- 22.4x reduction in inference latency (24.1s to 1.7s)

## Why This Works (Mechanism)

### Mechanism 1: Channel funneling
- Claim: Reduces model width while preserving quality by merging reduced-dimension intermediate representations into weight matrices
- Mechanism: Funnel matrices F1 and F2 reduce channel dimensionality between affine layers, then merge with corresponding weight matrices at inference
- Core assumption: Reduced-rank approximation maintains sufficient representational capacity
- Evidence: [abstract] mentions "two novel pruning schema to reduce channels"; [section 3.3] describes funneling method; corpus evidence is weak

### Mechanism 2: Temporal block pruning
- Claim: Removes less important temporal components while maintaining visual quality through learned importance values
- Mechanism: Each temporal block gets importance value qi, blocks are sampled based on inclusion probabilities during training, only top-n blocks kept at inference
- Core assumption: Constrained optimization balances block importance with budget constraint
- Evidence: [abstract] mentions "two novel pruning schema"; [section 3.4] details importance value definition; corpus evidence is weak

### Mechanism 3: Adversarial fine-tuning
- Claim: Enables single-step generation by training discriminator to distinguish real from generated videos
- Mechanism: Discriminator uses encoder features from denoising UNet, generator trained with adversarial loss to produce realistic videos in one pass
- Core assumption: Good initialization from standard diffusion training is crucial for fast convergence
- Evidence: [abstract] mentions "adversarial finetuning to reduce denoising to single step"; [section 3.1] follows SF-V framework; corpus shows some evidence from related work

## Foundational Learning

- Concept: Spatio-temporal UNet architecture
  - Why needed here: Understanding base architecture (SVD) is crucial for knowing what components can be optimized
  - Quick check question: What are the main components of a spatio-temporal UNet and how do they differ from standard 2D UNets?

- Concept: Cross-attention in conditional diffusion models
  - Why needed here: Optimized cross-attention removes unnecessary computations when context is single token, significantly impacting mobile latency
  - Quick check question: How does cross-attention differ from self-attention in diffusion models, and why is it computationally wasteful when conditioning on a single token?

- Concept: Low-rank matrix approximation and singular value decomposition
  - Why needed here: Channel funneling uses truncated singular decomposition for initialization
  - Quick check question: How does truncated singular decomposition approximate a matrix with lower rank, and what guarantees does Eckart-Young-Mirsky theorem provide?

## Architecture Onboarding

- Component map: Input image → VAE encoding → repeated latents + noise → UNet denoising (with optimizations) → latent output → VAE decoding → video output
- Critical path: Image → VAE encoder → UNet with temporal blocks → VAE decoder → Video
- Design tradeoffs: Resolution reduction trades quality for speed; temporal multiscaling trades some FVD quality for latency reduction; channel funneling trades representational capacity for parameter reduction
- Failure signatures: Out-of-memory errors indicate insufficient optimization; excessive FVD degradation indicates over-pruning; visual artifacts may indicate poor adversarial fine-tuning
- First 3 experiments:
  1. Measure FLOPs and latency of base SVD model at target resolution to establish baseline
  2. Apply optimized cross-attention and verify memory constraints on device are resolved
  3. Test temporal multiscaling with different factors and measure impact on FVD and latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal fun-factor for channel funnels that balances model compression with minimal quality degradation?
- Basis in paper: [explicit] The paper discusses impact of fun-factor values on model performance
- Why unresolved: Only tests fun-factors of 0.25, 0.5, 0.75, and 1.0, leaving gap in understanding precise optimal value
- What evidence would resolve it: Comprehensive ablation study testing wider range of fun-factor values

### Open Question 2
- Question: How does channel funneling compare to other compression methods like low-rank decomposition in efficiency and quality trade-offs on mobile devices?
- Basis in paper: [explicit] Compares funneling to truncated singular decomposition and finds funneling faster on device
- Why unresolved: Only compares to one other method and doesn't explore quantization or pruning
- What evidence would resolve it: Systematic comparison with other model compression techniques on various mobile devices

### Open Question 3
- Question: Can temporal block pruning be extended to other video diffusion models or generative models?
- Basis in paper: [explicit] Introduces novel pruning technique for temporal blocks in SVD model
- Why unresolved: Only applies technique to SVD model, doesn't explore other video diffusion or generative models
- What evidence would resolve it: Applying technique to other video diffusion models and generative models like image or audio generation

## Limitations

- Weak corpus evidence for key mechanisms like channel funneling and temporal block pruning
- Exact breakdown of efficiency contributions from each optimization technique is not provided
- Limited testing on diverse video datasets beyond UCF-101
- Some implementation details (like exact "motion bucket" implementation) are unclear

## Confidence

- High confidence: Basic architecture optimizations (resolution reduction, cross-attention optimization)
- Medium confidence: Temporal multiscaling, channel funneling, and adversarial fine-tuning mechanisms
- Low confidence: Exact quantitative contributions of each optimization and generalization to other video datasets

## Next Checks

1. **Ablation study replication**: Independently verify FVD degradation patterns when varying temporal multiscaling factors and channel funneling ratios to confirm reported 20-unit FVD increase thresholds.

2. **Memory profiling validation**: Measure actual memory usage on target mobile hardware before and after each optimization to confirm paper's claims about memory reduction and on-device feasibility.

3. **Generalization testing**: Evaluate MobileVD on diverse video datasets beyond UCF-101 (e.g., Kinetics, Something-Something) to assess whether optimizations maintain quality across different video types and domains.