---
ver: rpa2
title: 'FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language Model
  Mathematical Reasoning'
arxiv_id: '2410.06304'
source_url: https://arxiv.org/abs/2410.06304
tags:
- step
- hallucination
- reasoning
- fg-prm
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FG-PRM, a fine-grained approach to detect
  and mitigate hallucinations in LLM mathematical reasoning. It defines a taxonomy
  of six hallucination types and uses an automated method to generate synthetic datasets
  for training.
---

# FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning

## Quick Facts
- arXiv ID: 2410.06304
- Source URL: https://arxiv.org/abs/2410.06304
- Authors: Ruosen Li; Ziming Luo; Xinya Du
- Reference count: 35
- This paper introduces FG-PRM, a fine-grained approach to detect and mitigate hallucinations in LLM mathematical reasoning. It defines a taxonomy of six hallucination types and uses an automated method to generate synthetic datasets for training. Experiments show FG-PRM achieves over 5% higher F1 scores in detection and improves verification accuracy by over 3% on GSM8K and MATH benchmarks compared to baseline reward models.

## Executive Summary
This paper introduces FG-PRM, a fine-grained approach to detect and mitigate hallucinations in LLM mathematical reasoning. It defines a taxonomy of six hallucination types and uses an automated method to generate synthetic datasets for training. Experiments show FG-PRM achieves over 5% higher F1 scores in detection and improves verification accuracy by over 3% on GSM8K and MATH benchmarks compared to baseline reward models.

## Method Summary
FG-PRM uses a novel automated hallucination generation process where an LLM creates synthetic reasoning steps containing specific hallucination types based on a six-category taxonomy. The system generates 64 candidate solutions per problem, then six specialized PRMs (one per hallucination type) score each step. These scores are aggregated using log-sum to rank solutions and select the most reliable answer. The approach is trained on synthetic data rather than human annotations, significantly reducing labeling costs while maintaining high detection accuracy.

## Key Results
- FG-PRM achieves over 5% higher F1 scores in hallucination detection compared to baseline reward models
- Verification accuracy improves by over 3% on both GSM8K and MATH benchmarks
- Trained on only 12K synthetic instances, outperforms models trained on 400K human-labeled data
- All six hallucination types contribute to performance, with "incorrect symbols" and "wrong operations" being most prevalent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FG-PRM detects hallucinations at step-level granularity rather than chain-level, allowing finer-grained error localization.
- Mechanism: The model assigns a binary score to each reasoning step indicating whether it contains a hallucination, instead of scoring the whole chain.
- Core assumption: Errors in intermediate steps propagate to final answer; catching them early improves correction.
- Evidence anchors:
  - [abstract] "PRMs assess each step. PRMs have demonstrated superior performance in many scenarios since they can provide more granular feedback and effectively guide models’ reasoning process."
  - [section 3.2.2] "FG-PRM generates an aggregate reward for the solution y of the input question x"
  - [corpus] Weak - no direct citations in corpus neighbors about step-level reward models.
- Break condition: If hallucinations are highly context-dependent across steps, isolated step scoring may miss compound errors.

### Mechanism 2
- Claim: Automated hallucination generation reduces annotation cost while providing balanced fine-grained training data.
- Mechanism: LLM synthesizes negative reasoning steps with controlled hallucination types, then trains PRMs on this synthetic dataset.
- Core assumption: LLM-generated hallucinated steps are realistic enough to train detectors effectively.
- Evidence anchors:
  - [abstract] "we propose an automated method for generating fine-grained hallucination data using LLMs."
  - [section 3.2.1] "we develop a novel method to automatically generate fine-grained hallucination data using LLMs."
  - [corpus] Weak - corpus neighbors discuss hallucination detection but not automated synthetic data generation.
- Break condition: If generated hallucinations do not match real error patterns, model may overfit synthetic data.

### Mechanism 3
- Claim: Six-category taxonomy improves detection precision by distinguishing error types rather than binary correctness.
- Mechanism: Separate PRM classifiers trained per hallucination type detect nuanced reasoning failures.
- Core assumption: Different hallucination types exhibit distinct linguistic or logical patterns that models can learn to distinguish.
- Evidence anchors:
  - [section 2] "we develop a fine-grained taxonomy that categorizes hallucinations based on their nature and manifestation"
  - [section 3.2.2] "RΦ comprises six PRMs, Rϕ1…Rϕ6, each corresponding to a specific type of hallucination in our taxonomy."
  - [corpus] No corpus neighbors discuss six-category hallucination taxonomies explicitly.
- Break condition: If hallucination types overlap significantly, multi-class classification may be unnecessary overhead.

## Foundational Learning

- Concept: Cross-entropy loss for binary classification
  - Why needed here: FG-PRM trains individual PRMs using binary classification at each reasoning step
  - Quick check question: What loss function would you use to train a PRM that predicts "hallucination present/absent" for a single step?

- Concept: Monte Carlo tree search for automated data generation
  - Why needed here: Alternative to FG-PRM's LLM-based hallucination injection, mentioned in related work
  - Quick check question: How does Monte Carlo tree search differ from LLM-based synthetic data generation for supervision?

- Concept: Chain-of-thought reasoning evaluation
  - Why needed here: FG-PRM evaluates intermediate reasoning steps in multi-step math problems
  - Quick check question: Why is evaluating each reasoning step more informative than only evaluating the final answer?

## Architecture Onboarding

- Component map:
  - Question input → Reasoning step sequence → Six PRMs (one per hallucination type) → Aggregated score → Verification selection
  - Synthetic data generator: LLM + rules → Hallucination-labeled dataset → PRM training

- Critical path:
  - Generate synthetic hallucinated steps → Train six PRMs → Aggregate scores → Select best solution from candidates

- Design tradeoffs:
  - Granular supervision (six PRMs) vs. single multi-class model: Higher accuracy but more parameters
  - Synthetic data vs. human annotation: Lower cost but potential distribution mismatch
  - Step-level vs. chain-level scoring: Better error localization but requires step segmentation

- Failure signatures:
  - Low recall on certain hallucination types: Generated hallucinations may not match real errors
  - Poor transfer to out-of-distribution data: Synthetic data distribution differs from real reasoning chains
  - Aggregation issues: Log-sum reward may be dominated by long sequences

- First 3 experiments:
  1. Ablation study: Train FG-PRM with only one hallucination type vs. all six types
  2. Data scaling: Compare FG-PRM trained on 12k synthetic vs. 400k human-labeled data
  3. Transfer test: Train on GSM8K synthetic data, evaluate on MATH benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FG-PRM scale with increasing dataset sizes beyond 12K instances?
- Basis in paper: [explicit] The paper notes that FG-PRM trained on 12K instances outperforms Math-Shepherd's PRM trained on 400K data, but doesn't explore performance beyond 12K.
- Why unresolved: The paper focuses on demonstrating FG-PRM's effectiveness with limited data rather than exploring scalability limits.
- What evidence would resolve it: Comparative experiments showing FG-PRM performance across different dataset sizes (e.g., 12K, 50K, 100K, 400K) to determine if the advantage persists or plateaus.

### Open Question 2
- Question: Can FG-PRM be effectively adapted to domains beyond mathematical reasoning, such as scientific QA or commonsense reasoning?
- Basis in paper: [inferred] The paper mentions plans to extend FG-PRM to other domains but doesn't provide experimental results.
- Why unresolved: The paper only validates FG-PRM on mathematical benchmarks (GSM8K and MATH) without testing other domains.
- What evidence would resolve it: Empirical results showing FG-PRM performance on scientific QA datasets (e.g., PubMedQA) or commonsense reasoning benchmarks (e.g., CommonsenseQA).

### Open Question 3
- Question: How do different hallucination generation methods (e.g., Monte Carlo tree search vs. LLM-based synthesis) compare in terms of data quality and cost-effectiveness?
- Basis in paper: [explicit] The paper mentions OmegaPRM's Monte Carlo tree search approach but doesn't compare it with their LLM-based method.
- Why unresolved: The paper presents their automated hallucination generation method without benchmarking against alternative approaches.
- What evidence would resolve it: Comparative experiments measuring hallucination injection success rates, data quality metrics, and computational costs between LLM