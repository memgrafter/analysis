---
ver: rpa2
title: Statistical Analysis of the Impact of Quaternion Components in Convolutional
  Neural Networks
arxiv_id: '2409.00140'
source_url: https://arxiv.org/abs/2409.00140
tags:
- quaternion
- layer
- initialization
- fully
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a statistical analysis of how different components
  in Quaternion-Valued Convolutional Neural Networks (QCNNs) affect performance. The
  authors conduct factorial experiments on MNIST and CIFAR-10 datasets, testing combinations
  of activation functions (split vs fully quaternion ReLU), fully connected layers
  (quaternion inner product vs fully connected), initialization methods (Glorot/He
  channel-wise vs fully quaternion), and model sizes.
---

# Statistical Analysis of the Impact of Quaternion Components in Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2409.00140
- Source URL: https://arxiv.org/abs/2409.00140
- Reference count: 40
- Primary result: QCNNs achieve similar or better accuracy than real-valued models while training in fewer epochs through optimal component combinations

## Executive Summary
This paper presents a comprehensive statistical analysis of how different components in Quaternion-Valued Convolutional Neural Networks affect performance. The authors conducted factorial experiments on MNIST and CIFAR-10 datasets, testing combinations of activation functions, fully connected layers, initialization methods, and model sizes. The analysis reveals significant interaction effects between these components that impact classification accuracy. The proposed Fully Quaternion ReLU activation function outperforms existing methods, and QCNNs achieve similar or better accuracy than real-valued models while training in fewer epochs. The study demonstrates that QCNNs can be more parameter-efficient, with models using 4x fewer parameters achieving comparable performance to larger models when components are properly combined.

## Method Summary
The study employs a factorial design of experiments with 4 factors: activation function (Split Quaternion ReLU vs Fully Quaternion ReLU), fully connected layer type (Quaternion Inner Product vs Quaternion Fully Connected), initialization method (channel-wise Xavier/He vs fully quaternion), and model size. Experiments were conducted on MNIST and CIFAR-10 datasets with 10 replicates per combination for MNIST and 8 replicates for CIFAR-10. A 4-way ANOVA analysis was performed to identify significant main effects and interaction effects, followed by Tukey HSD test for pairwise comparisons to determine optimal component combinations.

## Key Results
- Significant interaction effects between quaternion CNN components were found (F = 5.521, p-value = 4.751×10−10)
- Fully Quaternion ReLU activation function outperformed existing methods by 8.9% on CIFAR-10
- QCNNs achieved best performance in fewer training epochs compared to real-valued models
- Models using 4x fewer parameters achieved comparable performance to larger models when components were properly combined

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fully quaternion ReLU (FQReLU) activation function outperforms split quaternion ReLU (SQReLU) by preserving quaternion algebra structure rather than treating real and imaginary parts separately.
- Mechanism: FQReLU applies the activation based on the quaternion phase (angle), keeping the vector part intact when phase is within [0, π/2], whereas SQReLU independently thresholds each component. This preserves phase relationships that encode useful information for quaternion convolution operations.
- Core assumption: Quaternion phase contains meaningful information that should be preserved rather than independently thresholded across components.
- Evidence anchors: Abstract states "We propose a novel Fully Quaternion ReLU activation function which exploits the unique properties of quaternion algebra to improve model performance." Section defines FQReLU as applying activation based on quaternion phase.

### Mechanism 2
- Claim: Interaction effects between quaternion CNN components significantly impact classification accuracy, making component selection interdependent rather than independent.
- Mechanism: The statistical analysis reveals that combinations of activation functions, initialization methods, fully connected layers, and parameter counts interact to produce non-additive effects on accuracy. For example, the best performance occurs when FQReLU is combined with QIP layers and QHe/QXavier initialization, rather than any single component being universally optimal.
- Core assumption: The performance of quaternion CNN components depends on their interaction rather than just individual contributions.
- Evidence anchors: Abstract mentions "The analysis reveals significant interaction effects between these components that impact classification accuracy." Section reports "The 4-way ANOVA study concluded that the overall interaction between the four factors was statistically significant (F = 5.521, p-value = 4.751×10−10)".

### Mechanism 3
- Claim: Quaternion CNNs achieve similar or better accuracy than real-valued models while training in fewer epochs due to the richer representation space of quaternions.
- Mechanism: The four-dimensional quaternion representation allows more information to be encoded in each weight and activation, potentially leading to faster convergence. The statistical analysis shows quaternion models achieve best performance in fewer epochs on average compared to real-valued counterparts.
- Core assumption: The higher-dimensional quaternion representation provides more efficient information encoding that accelerates learning.
- Evidence anchors: Abstract states "QCNNs achieve similar or better accuracy than real-valued models while training in fewer epochs." Section notes "The vast majority of quaternion models achieved their best performance in fewer training epochs than the real-valued model."

## Foundational Learning

- Concept: Quaternion algebra and operations
  - Why needed here: The paper builds quaternion-valued neural networks, requiring understanding of quaternion multiplication, conjugation, and phase computation for implementing FQReLU and convolution operations.
  - Quick check question: How does quaternion multiplication differ from complex number multiplication, and why does this matter for neural network operations?

- Concept: Analysis of Variance (ANOVA) and statistical interaction effects
  - Why needed here: The paper uses 4-way ANOVA to analyze how different quaternion CNN components interact to affect performance, requiring understanding of factorial designs and interaction effects.
  - Quick check question: What is the difference between main effects and interaction effects in ANOVA, and why are interaction effects particularly important for this study?

- Concept: Neural network initialization methods
  - Why needed here: The paper compares quaternion initialization methods (QHe, QXavier) with channel-wise real-valued methods, requiring understanding of how initialization affects training dynamics and gradient flow.
  - Quick check question: How do He and Xavier initialization methods address the vanishing/exploding gradient problem, and what modifications are needed for quaternion-valued networks?

## Architecture Onboarding

- Component map: Input quaternion mapping (RGB→quaternion) -> Quaternion convolution layers with left-sided discrete convolution -> Channel-wise average pooling layers -> Activation function application -> Fully connected layer computation -> Backpropagation with quaternion derivatives

- Critical path:
  1. Input quaternion mapping (RGB→quaternion)
  2. Quaternion convolution operations
  3. Activation function application
  4. Pooling operations
  5. Fully connected layer computation
  6. Backpropagation with quaternion derivatives

- Design tradeoffs:
  - FQReLU vs SQReLU: FQReLU preserves phase relationships but adds computational complexity; SQReLU is simpler but may lose information
  - QIP vs QFC: QIP produces real-valued outputs, potentially limiting representational capacity; QFC maintains quaternion representation but may be more parameter-intensive
  - Fully quaternion vs channel-wise initialization: Fully quaternion methods are designed for the algebra but may be more complex to implement

- Failure signatures:
  - Poor convergence: May indicate inappropriate initialization or activation function choice for the data distribution
  - Overfitting with quaternion models: Could suggest the quaternion representation is too expressive for the dataset size
  - No performance improvement over real-valued: Might indicate the data doesn't benefit from quaternion representation or implementation issues

- First 3 experiments:
  1. Implement MNIST classification with SQReLU + QIP + channel-wise Xavier initialization to establish baseline performance
  2. Replace activation with FQReLU while keeping other components constant to measure activation impact
  3. Switch to QHe initialization with FQReLU and QIP to test the optimal combination identified in the analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the interaction effect between QCNN components remain consistent across different image classification datasets beyond MNIST and CIFAR-10?
- Basis in paper: The paper acknowledges that "Because of the No Free Lunch theorems, one could say that our results might change with different problems" and suggests testing on other domains like natural language processing, time series modeling, or generative tasks.
- Why unresolved: The study only tested two datasets (MNIST and CIFAR-10) and focused on image classification tasks.
- What evidence would resolve it: Experimental results from QCNN models tested on additional image datasets (e.g., ImageNet, SVHN) or other domains (text, audio, time series) showing consistent or varying interaction effects.

### Open Question 2
- Question: How do fully quaternion initialization methods compare to channel-wise initialization methods when applied to deeper QCNN architectures with more than three convolution layers?
- Basis in paper: The paper found that fully quaternion initialization methods outperformed channel-wise methods for the CIFAR-10 dataset (deeper model with 3 convolution layers), but noted that for smaller models like MNIST, there was no statistically significant difference.
- Why unresolved: The study only tested models with 2-3 convolution layers, leaving uncertainty about how initialization methods perform in much deeper architectures.
- What evidence would resolve it: Experimental results comparing initialization methods across QCNN architectures with varying depths (e.g., 5-10+ convolution layers) showing performance differences.

### Open Question 3
- Question: Does the FQReLU activation function maintain its performance advantage over SQReLU when applied to QCNN models with different numbers of activation function modules?
- Basis in paper: The paper found that FQReLU outperformed SQReLU by 8.9% on CIFAR-10 (3 activation modules) but only showed a slight improvement (<0.5%) on MNIST (1 activation module), suggesting performance depends on how extensively activation functions are used.
- Why unresolved: The study only tested models with 1-3 activation function modules, leaving uncertainty about performance in architectures with more or fewer activation modules.
- What evidence would resolve it: Experimental results comparing FQReLU and SQReLU across QCNN architectures with varying numbers of activation function modules (e.g., 0, 1, 2, 4, 5+) showing performance trends.

## Limitations

- The statistical claims rely on ANOVA assumptions of normally distributed residuals and homogeneous variances
- The study focuses on classification accuracy but doesn't extensively explore other metrics like robustness to noise or adversarial examples
- The quaternion representation's benefits may be dataset-specific, as the study only examines MNIST and CIFAR-10

## Confidence

- **High Confidence**: The statistical significance of interaction effects between quaternion CNN components is well-supported by the ANOVA results.
- **Medium Confidence**: The practical superiority of FQReLU over SQReLU is demonstrated but could benefit from more diverse datasets.
- **Medium Confidence**: The claim about parameter efficiency is supported but needs more extensive comparison across different model architectures.

## Next Checks

1. Replicate the ANOVA analysis with additional image classification datasets (e.g., CIFAR-100, SVHN) to verify the consistency of interaction effects across different data distributions.

2. Conduct ablation studies measuring the impact of each quaternion component independently to separate main effects from interaction effects more clearly.

3. Implement cross-validation with different random seeds to ensure the statistical significance findings are robust to data partitioning variations.