---
ver: rpa2
title: Improving Linear System Solvers for Hyperparameter Optimisation in Iterative
  Gaussian Processes
arxiv_id: '2405.18457'
source_url: https://arxiv.org/abs/2405.18457
tags:
- estimator
- linear
- pathwise
- marginal
- likelihood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper improves hyperparameter optimisation in iterative Gaussian\
  \ processes by introducing three key techniques applicable across linear system\
  \ solvers: a pathwise gradient estimator that reduces solver iterations and amortises\
  \ prediction costs, warm starting solvers with previous solutions to accelerate\
  \ convergence, and early stopping with limited compute budgets that synergises with\
  \ warm starting. These techniques provide speed-ups of up to 72\xD7 when solving\
  \ to tolerance and decrease average residual norms by up to 7\xD7 when stopping\
  \ early."
---

# Improving Linear System Solvers for Hyperparameter Optimisation in Iterative Gaussian Processes

## Quick Facts
- arXiv ID: 2405.18457
- Source URL: https://arxiv.org/abs/2405.18457
- Reference count: 40
- Primary result: Up to 72× speed-ups in solving linear systems for hyperparameter optimization, with pathwise estimators reducing iterations and warm starting accelerating convergence

## Executive Summary
This paper addresses the computational bottleneck in iterative Gaussian process hyperparameter optimization by improving linear system solvers. The authors introduce three techniques applicable across solver types: a pathwise gradient estimator that reduces solver iterations through better initialization, warm starting that accelerates convergence by reusing previous solutions, and early stopping that works synergistically with warm starting to accumulate progress across optimization steps. These techniques provide significant speed-ups while maintaining or improving predictive performance.

## Method Summary
The paper introduces three key techniques for accelerating hyperparameter optimization in Gaussian processes. First, a pathwise gradient estimator uses probe vectors sampled from N(0, H⁻¹θ) instead of N(0, I), reducing the distance from initialization to solution. Second, warm starting initializes linear system solvers with solutions from previous optimization steps, leveraging the typically small changes in hyperparameters between steps. Third, early stopping with limited compute budgets allows solver progress to accumulate across steps when warm starting is used. These techniques are evaluated across multiple linear solvers (conjugate gradients, accelerated proximal point, stochastic gradient descent) and UCI regression datasets.

## Key Results
- Up to 72× speed-ups when solving linear systems to tolerance
- Pathwise estimator reduces initial distance to solution, accelerating convergence
- Warm starting introduces negligible bias while significantly reducing iterations
- Early stopping with warm starting achieves better residual norms than starting from scratch
- Low residual norms are not always necessary for good predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pathwise gradient estimator reduces solver iterations by moving solutions closer to the origin, accelerating convergence.
- Mechanism: The pathwise estimator uses probe vectors with second moment H⁻¹θ (sampled via f(x) + ε), whereas the standard estimator uses probe vectors with second moment I. The RKHS distance from origin to solution for pathwise is constant (n), while for standard it scales with trace(H⁻¹θ), which grows as noise precision increases during optimization.
- Core assumption: The solver initialization at zero and the relationship between probe vector distributions hold throughout optimization.
- Evidence anchors:
  - [abstract] "a pathwise gradient estimator, which reduces the required number of solver iterations"
  - [section] "we analyse and adapt these techniques, and show that they can be applied to accelerate different linear solvers, obtaining speed-ups of up to 72×"
  - [corpus] No direct evidence for this mechanism in corpus papers, weak support.
- Break condition: If probe vector distributions change significantly or if the assumption about RKHS distance breaks down, the convergence advantage may disappear.

### Mechanism 2
- Claim: Warm starting linear system solvers with previous solutions accelerates convergence with negligible bias.
- Mechanism: Since hyperparameters change minimally between optimization steps, the linear system solutions also change minimally. Using the previous solution as initialization reduces the initial distance to the new solution, decreasing required iterations.
- Core assumption: The coefficient matrix Hθ changes slowly enough between steps that the previous solution remains a good approximation to the new solution.
- Evidence anchors:
  - [abstract] "warm starting linear system solvers with the solution from the previous step, which leads to faster solver convergence"
  - [section] "we expect that the solution to inner-loop linear systems also does not change much between consecutive steps"
  - [corpus] "Improving Iterative Gaussian Processes via Warm Starting Sequential Posteriors" directly supports this mechanism.
- Break condition: If hyperparameters change drastically between steps, or if the matrix changes significantly, warm starting may introduce substantial bias or fail to provide convergence benefits.

### Mechanism 3
- Claim: Early stopping combined with warm starting allows solver progress to accumulate across optimization steps.
- Mechanism: When a limited compute budget prevents reaching tolerance, warm starting allows each solver to begin where the previous one left off. This creates an amortization effect where progress accumulates across optimization steps rather than restarting from zero each time.
- Core assumption: The solutions across consecutive steps are sufficiently correlated that warm starting provides meaningful progress accumulation.
- Evidence anchors:
  - [abstract] "early stopping linear system solvers after a limited computational budget, which synergises with warm starting, allowing solver progress to accumulate over multiple marginal likelihood steps"
  - [section] "warm starting allows the linear system solver to accumulate solver progress across marginal likelihood steps"
  - [corpus] No direct evidence in corpus papers, weak support.
- Break condition: If the hyperparameter changes cause solutions to diverge significantly, or if the compute budget is too small to make meaningful progress even with warm starting.

## Foundational Learning

- Concept: Linear system solvers and their convergence criteria
  - Why needed here: The paper relies on understanding how different solvers (CG, AP, SGD) behave under different initialization and stopping conditions
  - Quick check question: What is the difference between relative residual norm and absolute residual norm, and why is relative norm used as the convergence criterion?

- Concept: Gaussian process regression and marginal likelihood optimization
  - Why needed here: The paper operates within the GP framework, where hyperparameter optimization requires solving linear systems to compute gradients
  - Quick check question: How does the marginal likelihood gradient involve both linear solves and trace estimation, and why is Hutchinson's estimator used?

- Concept: Pathwise conditioning and random features
  - Why needed here: The pathwise estimator requires understanding how to efficiently sample from GP posteriors and how random features approximate the kernel
  - Quick check question: How do random Fourier features enable efficient sampling from GP priors, and what is the relationship between feature count and approximation quality?

## Architecture Onboarding

- Component map: Data → Kernel matrix → Linear systems → Gradient estimates → Hyperparameter updates → Repeat
- Critical path: Data → Kernel matrix → Linear systems → Gradient estimates → Hyperparameter updates → Repeat
- Design tradeoffs:
  - Standard vs pathwise estimator: Standard has lower variance but slower convergence; pathwise has constant initialization distance but requires random feature approximation
  - Warm starting vs no warm starting: Warm starting accelerates convergence but may introduce bias; no warm starting is unbiased but slower
  - Tolerance vs early stopping: Reaching tolerance ensures accuracy but may be computationally infeasible; early stopping saves time but may sacrifice predictive performance
- Failure signatures:
  - Solver divergence: Check learning rate, initialization, and matrix conditioning
  - Slow convergence: Verify probe vector distribution and warm start effectiveness
  - Poor predictive performance: Examine residual norms vs actual prediction quality
- First 3 experiments:
  1. Compare standard vs pathwise estimator on a small dataset (POL) with CG solver, measuring iterations to tolerance
  2. Test warm starting on the same setup, comparing convergence speed and checking for bias in hyperparameter trajectories
  3. Apply early stopping with warm starting on a larger dataset (3DROAD), measuring residual norm evolution and predictive performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal number of probe vectors in the pathwise estimator scale with dataset size and dimensionality?
- Basis in paper: [explicit] The paper shows that 64 probe vectors provides good predictive performance but uses only 16 for optimization, and mentions that using 64 instead of 16 increases runtime by only ~10%
- Why unresolved: The paper doesn't systematically study how the optimal number of probe vectors changes with dataset characteristics, only showing results for a fixed choice across different datasets
- What evidence would resolve it: Empirical studies showing predictive performance and computational efficiency across a range of dataset sizes (n) and dimensionalities (d) while varying the number of probe vectors

### Open Question 2
- Question: What is the theoretical relationship between residual norm and predictive performance, and why do low residuals not always guarantee good predictions?
- Basis in paper: [explicit] The paper finds that "low residual norms are not always necessary to obtain good predictive performance" and notes this as an "unexpected yet interesting observation"
- Why unresolved: The paper observes this phenomenon empirically but doesn't provide theoretical analysis of why the residual norm might be a poor proxy for predictive quality
- What evidence would resolve it: Theoretical analysis connecting residual norm to generalization error, or empirical studies showing specific conditions under which residual norms fail to predict predictive performance

### Open Question 3
- Question: How does warm starting affect the convergence rate and final solution quality when hyperparameters change substantially between iterations?
- Basis in paper: [inferred] The paper assumes hyperparameters don't change much between consecutive steps when justifying warm starting, but doesn't test scenarios with large hyperparameter changes
- Why unresolved: The analysis assumes small hyperparameter changes between iterations, but real optimization trajectories may involve larger jumps, especially early in training
- What evidence would resolve it: Experiments systematically varying the magnitude of hyperparameter changes and measuring both convergence speed and final solution quality with and without warm starting

## Limitations
- The pathwise gradient estimator's mechanism relies on assumptions about probe vector distributions and RKHS distances that may not hold in all scenarios
- Warm starting introduces potential bias that is claimed to be negligible but not rigorously quantified
- The early stopping approach assumes a linear relationship between residual norm and predictive performance that may break down for certain datasets or kernel configurations

## Confidence
- High confidence: The warm starting mechanism is well-supported by the related work "Improving Iterative Gaussian Processes via Warm Starting Sequential Posteriors"
- Medium confidence: The pathwise gradient estimator mechanism has theoretical justification but lacks empirical validation in the corpus
- Low confidence: The early stopping synergy with warm starting is described but not thoroughly validated

## Next Checks
1. Measure the actual variance reduction from pathwise estimation across different noise precision regimes to verify the claimed convergence advantage
2. Quantify the bias introduced by warm starting across multiple optimization trajectories to assess the "negligible bias" claim
3. Test the residual norm vs predictive performance relationship on datasets with different characteristics (e.g., high noise, non-smooth functions) to validate the early stopping assumptions