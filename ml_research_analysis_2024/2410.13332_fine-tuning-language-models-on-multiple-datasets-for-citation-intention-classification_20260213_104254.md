---
ver: rpa2
title: Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification
arxiv_id: '2410.13332'
source_url: https://arxiv.org/abs/2410.13332
tags:
- dataset
- datasets
- primary
- citation
- auxiliary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a multi-task learning (MTL) framework for
  citation intention classification (CIC) that fine-tunes pretrained language models
  (PLMs) on multiple datasets to improve performance on a primary dataset. A data-driven
  task relation learning (TRL) method controls the contribution of auxiliary datasets
  to prevent negative transfer.
---

# Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification

## Quick Facts
- arXiv ID: 2410.13332
- Source URL: https://arxiv.org/abs/2410.13332
- Authors: Zeren Shui; Petros Karypis; Daniel S. Karls; Mingjian Wen; Saurav Manchanda; Ellad B. Tadmor; George Karypis
- Reference count: 33
- Key outcome: Multi-task learning with task relation learning improves CIC performance by 7-11% on small datasets

## Executive Summary
This paper introduces a multi-task learning framework for citation intention classification that fine-tunes pretrained language models on multiple datasets simultaneously. The framework uses a data-driven task relation learning method to control the contribution of auxiliary datasets and prevent negative transfer. Experiments on three CIC datasets demonstrate that the proposed approach outperforms state-of-the-art models, particularly on smaller datasets where it achieves 7-11% improvement. The paper also proposes a position-aware readout function that leverages citation position within context to improve classification accuracy.

## Method Summary
The method involves jointly fine-tuning a pretrained language model (PLM) encoder on a primary CIC dataset and one or more auxiliary CIC datasets. A task relation learning (TRL) module computes a contribution weight (λ) for each auxiliary dataset based on information gain between label spaces. The PLM parameters are shared across all tasks while task-specific MLPs predict citation intentions. The CITED HERE readout function marks citation positions in context to capture positional information. The overall loss is a weighted combination of primary and auxiliary dataset losses, with λ controlling the auxiliary contribution.

## Key Results
- MTL framework with TRL improves macro-F1 by 7-11% on small datasets (ACL, KIM) compared to single-dataset training
- Position-aware CITED HERE readout function outperforms CLS and MEAN readout functions on ACL and KIM datasets
- TRL method successfully avoids negative transfer while being computationally more efficient than grid search
- On larger dataset (SciCite), performance gains are modest (1.5%) but still competitive with state-of-the-art models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint fine-tuning on multiple CIC datasets improves PLM performance on the primary dataset
- Mechanism: Sharing the PLM encoder parameters across related tasks enables knowledge transfer between datasets that share citation context input space
- Core assumption: Citation contexts from different scientific domains share sufficient linguistic and semantic similarity to benefit from joint training
- Evidence anchors:
  - [abstract]: "fine-tuning with additional datasets can improve the PLMs' generalization performance on the primary dataset"
  - [section]: "It is intuitive to assume leveraging the supervision signal of one dataset to fine-tune a PLM can improve its performance on another dataset"
  - [corpus]: Weak evidence - corpus shows related work on dataset generalization but no direct evidence for CIC multi-dataset benefits
- Break condition: If citation contexts across domains are too dissimilar or if label spaces have minimal semantic overlap

### Mechanism 2
- Claim: Task Relation Learning (TRL) method effectively controls negative transfer while being computationally efficient
- Mechanism: Information gain between auxiliary and primary label spaces quantifies task relevance without requiring expensive grid search
- Core assumption: Information gain is a reliable proxy for task similarity in classification tasks with different label spaces
- Evidence anchors:
  - [section]: "Information gain quantifies the uncertainty reduction when we group the primary instances by their predicted label in the auxiliary label space"
  - [section]: "The TRL method chooses a small value of λ that avoids performance degradation on the primary dataset"
  - [corpus]: Moderate evidence - corpus contains related work on task weighting but not specifically using information gain for this purpose
- Break condition: If information gain calculation is skewed by imbalanced label distributions or if tasks are semantically related but have low information gain

### Mechanism 3
- Claim: Position-aware CITED HERE readout function improves classification accuracy, especially on small datasets
- Mechanism: Explicitly marking citation positions in context provides positional information that helps distinguish citation intentions
- Core assumption: The position of a citation within its context contains discriminative information for predicting citation intentions
- Evidence anchors:
  - [section]: "We find that the position of the citation within the context is an informative signal for predicting citation intentions"
  - [section]: "CITED HERE readout function significantly outperforms the other two readout functions" on ACL and KIM datasets
  - [corpus]: Weak evidence - corpus shows position-aware methods in other domains but no direct evidence for CIC applications
- Break condition: If citation positions are randomly distributed or if the context window is too small to capture meaningful positional patterns

## Foundational Learning

- Concept: Information Theory and Entropy
  - Why needed here: TRL method relies on entropy and information gain calculations to measure task relevance
  - Quick check question: Can you calculate the entropy of a binary classification dataset where 70% of samples belong to class A and 30% to class B?

- Concept: Multi-task Learning and Negative Transfer
  - Why needed here: Understanding how task relatedness affects joint training performance is crucial for designing the MTL framework
  - Quick check question: In a multi-task learning setup, would you expect better performance when training on two highly related tasks or two unrelated tasks?

- Concept: Transformer-based Language Models and Fine-tuning
  - Why needed here: The paper builds on PLMs like BERT and SciBERT, requiring understanding of how fine-tuning works
  - Quick check question: What is the difference between fine-tuning all layers of a PLM versus using adapters for parameter-efficient fine-tuning?

## Architecture Onboarding

- Component map: Citation Context -> PLM Encoder -> Readout Function -> Task-specific MLP -> Citation Intention Prediction

- Critical path:
  1. Preprocess citation contexts and insert <CITED HERE> marker
  2. PLM encodes contexts to contextualized token embeddings
  3. Readout function aggregates embeddings to fixed-length vectors
  4. Task-specific MLPs predict intentions
  5. Compute losses and backpropagate through shared PLM

- Design tradeoffs:
  - Shared vs separate PLM parameters: Sharing enables knowledge transfer but may cause interference
  - λ coefficient selection: TRL method vs grid search - accuracy vs computational efficiency
  - Readout function choice: CLS/MEAN (simple) vs CITED HERE (more accurate but requires position information)

- Failure signatures:
  - Performance degradation when adding auxiliary datasets (negative transfer)
  - High variance in results across different λ values
  - Minimal improvement when using CITED HERE on large datasets

- First 3 experiments:
  1. Fine-tune BERT on KIM dataset alone, then with ACL as auxiliary (using grid search for λ)
  2. Implement TRL method and compare λ values with grid search results
  3. Compare CLS, MEAN, and CITED HERE readout functions on ACL dataset

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the traditional sense. However, several implicit questions emerge from the discussion and experimental setup:
- How does the effectiveness of the task relation learning (TRL) method vary with the number and diversity of auxiliary datasets?
- What is the impact of dataset size on the negative transfer phenomenon in multi-task learning for citation intention classification?
- How does the CITED HERE readout function perform compared to other position-aware approaches for citation intention classification?

## Limitations

- Limited to three CIC datasets, raising questions about generalizability to other domains
- Position-aware readout function shows diminishing returns on larger datasets
- TRL method's reliance on information gain may not capture all relevant task relationships

## Confidence

- Multi-task learning framework effectiveness: High (small datasets), Medium (large datasets)
- TRL method computational efficiency: Medium
- CITED HERE readout function: High (small datasets), Low (large datasets)

## Next Checks

1. **Cross-dataset robustness test**: Apply the MTL framework with TRL to additional CIC datasets (e.g., from different scientific domains) to validate generalization beyond the three studied datasets.

2. **Label space sensitivity analysis**: Systematically vary the label space structures of auxiliary datasets to determine whether TRL's information gain calculations remain reliable when label spaces have different granularities or semantic relationships.

3. **Computational overhead measurement**: Implement both TRL and grid search methods to measure actual training time differences across various dataset combinations and model sizes, quantifying the claimed efficiency benefits.