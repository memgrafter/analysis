---
ver: rpa2
title: Graph Neural Networks for Heart Failure Prediction on an EHR-Based Patient
  Similarity Graph
arxiv_id: '2411.19742'
source_url: https://arxiv.org/abs/2411.19742
tags:
- graph
- patient
- data
- attention
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a novel approach using graph neural networks
  (GNNs) and a Graph Transformer (GT) to predict heart failure (HF) incidence on a
  patient similarity graph constructed from electronic health records (EHRs). The
  GT model demonstrated superior performance (F1 score: 0.5361, AUROC: 0.7925, AUPRC:
  0.5168) compared to other GNN architectures and baseline models.'
---

# Graph Neural Networks for Heart Failure Prediction on an EHR-Based Patient Similarity Graph

## Quick Facts
- arXiv ID: 2411.19742
- Source URL: https://arxiv.org/abs/2411.19742
- Reference count: 38
- One-line primary result: Graph Transformer model achieved F1 score of 0.5361, AUROC of 0.7925, and AUPRC of 0.5168 for heart failure prediction

## Executive Summary
This study introduces a novel approach using graph neural networks (GNNs) and a Graph Transformer (GT) to predict heart failure (HF) incidence on a patient similarity graph constructed from electronic health records (EHRs). The GT model demonstrated superior performance compared to other GNN architectures and baseline models, with medication data identified as the most relevant clinical feature for HF prediction. The graph-based interpretability analysis revealed that each classified patient requires careful analysis of its attributes, which may enable the identification of patients at high risk for developing HF and reveal novel patient profiles associated with the disease.

## Method Summary
The study constructs a patient similarity graph from EHR data using K-Nearest Neighbors (KNN) with cosine similarity on patient embeddings generated from 300-dimensional skip-gram embeddings of medical codes (ICD-9, NDC). Three GNN architectures (GraphSAGE, GAT, and Graph Transformer) are trained for binary node classification with focal loss to handle class imbalance. The models are evaluated using F1 score, AUROC, and AUPRC metrics. Patient-level embeddings are created by averaging visit-level embeddings, and the graph is constructed using K=3 nearest neighbors.

## Key Results
- Graph Transformer achieved the best performance with F1 score: 0.5361, AUROC: 0.7925, AUPRC: 0.5168
- Medication data was identified as the most relevant clinical feature for HF prediction
- Graph-based interpretability analysis enabled identification of high-risk patients and novel disease-associated profiles
- The patient similarity graph approach outperformed flat-feature models by leveraging relational information between patients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based architectures outperform traditional flat-feature models by leveraging relational information between patients.
- Mechanism: Patient similarity graphs encode clinical relationships that flat features cannot capture, allowing GNNs to learn from neighborhood structures rather than treating each patient as an isolated instance.
- Core assumption: Patient clinical similarity correlates with predictive value for HF outcomes.
- Evidence anchors:
  - [abstract]: "The GT model offered enhanced interpretability due to the use of patient relationships in the graph structure."
  - [section]: "Graph-based approaches such as GNNs provide an effective framework for predicting HF. By leveraging a patient similarity graph, GNNs can capture complex relationships in EHR data, potentially improving prediction accuracy and clinical interpretability."
  - [corpus]: No direct support; the corpus papers focus on related graph methods but don't explicitly compare relational vs. flat features.
- Break condition: If patient similarity does not correlate with HF risk, the graph structure becomes irrelevant and performance degrades to random.

### Mechanism 2
- Claim: Graph Transformer's attention mechanism captures long-range dependencies better than GraphSAGE or GAT in clinical similarity graphs.
- Mechanism: GT uses self-attention over the entire graph neighborhood, allowing it to weigh distant but clinically relevant patient relationships more effectively than local aggregation methods.
- Core assumption: Clinical similarity relationships in the graph are not strictly local and benefit from global attention patterns.
- Evidence anchors:
  - [section]: "The GT model demonstrated the best performance (F1 score: 0.5361, AUROC: 0.7925, AUPRC: 0.5168)."
  - [section]: "Graph Transformer (GT), based on a more advanced attention mechanism."
  - [corpus]: Weak support; corpus papers mention GT variants but don't provide comparative evidence for this specific clinical use case.
- Break condition: If the similarity graph topology is highly local with little long-range signal, simpler aggregation methods may perform equally well.

### Mechanism 3
- Claim: Medication embeddings provide the most discriminative features for HF prediction due to their granularity and NDC standardization.
- Mechanism: Raw NDC medication codes preserve fine-grained pharmaceutical information that ICD-9 codes may abstract away, and the skip-gram embedding captures co-occurrence patterns specific to HF treatment pathways.
- Core assumption: Medication usage patterns are more directly indicative of HF status than diagnoses or procedures alone.
- Evidence anchors:
  - [section]: "Medication data was found to be the most relevant clinical feature for HF prediction."
  - [section]: "Prescription patterns were diverse, indicating the importance of pharmacotherapy in HF classification."
  - [corpus]: No direct support; corpus focuses on graph methods generally but not on the relative importance of medication vs. other features.
- Break condition: If medication data is sparse or if diagnoses/procedures are more temporally proximal to HF onset, this advantage may diminish.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their variants (GraphSAGE, GAT, GT)
  - Why needed here: The paper compares these architectures for patient similarity graphs, so understanding their differences is critical for interpreting results.
  - Quick check question: What is the key architectural difference between GraphSAGE and Graph Attention Network that could affect their performance on clinical data?
- Concept: Attention mechanisms and their interpretability in GNNs
  - Why needed here: The interpretability analysis relies on attention weights to explain predictions, so understanding how attention works in graph contexts is essential.
  - Quick check question: How do attention weights in Graph Transformer differ from simple neighbor aggregation in GraphSAGE?
- Concept: Class imbalance handling in machine learning
- Why needed here: The dataset is imbalanced (28% positive), and the paper uses focal loss and weighted BCE, so understanding these techniques is crucial for replicating results.
  - Quick check question: Why might focal loss be preferred over standard BCE for imbalanced medical datasets?

## Architecture Onboarding

- Component map: MIMIC-III data preprocessing → Patient embeddings (ICD-9, NDC) → Patient similarity graph (KNN) → GNN model (GraphSAGE/GAT/GT) → Prediction layer → Evaluation metrics (F1, AUROC, AUPRC)
- Critical path: Graph construction → GNN training with appropriate loss → Evaluation on masked test set → Interpretability analysis
- Design tradeoffs:
  - Graph construction: K=3 neighbors balances local structure vs. computational cost; larger K may capture more relationships but increase noise.
  - GNN choice: GT offers better attention but higher complexity; GraphSAGE is simpler but may miss long-range patterns.
  - Loss function: Focal loss helps with imbalance but may overfit to minority class.
- Failure signatures:
  - Low F1 with high AUROC: Model predicts mostly negatives correctly but misses positive cases (high false negatives).
  - Similar performance across all GNN types: Graph structure may not be capturing meaningful relationships.
  - Poor performance despite good embeddings: Issue likely in graph construction or hyperparameter tuning.
- First 3 experiments:
  1. Reconstruct the patient similarity graph with K=3 and visualize a small subgraph to verify structure makes clinical sense.
  2. Train GraphSAGE with BCE loss to establish baseline performance before testing more complex architectures.
  3. Compare attention weight distributions between TP and FP instances to validate interpretability claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do graph-based GNN models compare to non-graph models when accounting for temporal dependencies in EHR data?
- Basis in paper: [explicit] The paper notes that "most of these architectures disregard the relational information underlying EHR data, treating medical information as a flat-structured bag of features"
- Why unresolved: The study focused on patient similarity graphs without incorporating temporal dynamics, and did not compare against temporal models like RNNs or temporal GNNs
- What evidence would resolve it: Direct comparison of GNN models with temporal models (RNNs, temporal GNNs) on the same dataset, measuring both predictive performance and interpretability

### Open Question 2
- Question: What is the impact of using different graph construction methods (e.g., dynamic vs static graphs) on HF prediction performance?
- Basis in paper: [inferred] The paper uses a static patient similarity graph but mentions "Future work could investigate alternative graph representations, such as dynamic or heterogeneous graphs"
- Why unresolved: The study only used a single graph construction method (KNN on patient embeddings) without exploring alternative approaches
- What evidence would resolve it: Systematic comparison of different graph construction methods (dynamic graphs, heterogeneous graphs, different similarity metrics) on the same prediction task

### Open Question 3
- Question: How does the performance of GNN models vary across different healthcare settings and populations?
- Basis in paper: [explicit] "Important limitations should be noted. First, the data resource was restricted to MIMIC-III, and data from other hospitals should also be evaluated in future studies"
- Why unresolved: The study was limited to a single ICU dataset (MIMIC-III) without validation on external datasets
- What evidence would resolve it: Replication of the study using EHR data from multiple hospitals, different healthcare systems, and diverse patient populations

## Limitations
- Performance metrics are moderate (F1: 0.5361, AUROC: 0.7925, AUPRC: 0.5168), suggesting room for improvement
- Reliance on MIMIC-III data from a single healthcare system limits generalizability to other populations
- Graph construction using K=3 nearest neighbors is relatively sparse and may miss important patient relationships
- Interpretability analysis relies on attention weights that may not always correspond to clinically meaningful features
- No comparison against simpler baseline models like logistic regression or random forests with the same features

## Confidence
- **High confidence**: The core methodology of using GNNs on patient similarity graphs is sound and technically feasible. The claim that medication data was most relevant for HF prediction is supported by the data analysis presented.
- **Medium confidence**: The comparative performance of Graph Transformer over other GNN architectures, while demonstrated, may depend heavily on specific hyperparameters and graph construction choices. The interpretability findings are promising but require clinical validation.
- **Low confidence**: The generalizability of these results to other healthcare systems and patient populations remains uncertain without external validation. The clinical utility of the attention-based explanations needs verification by domain experts.

## Next Checks
1. **External validation**: Apply the trained model to an independent EHR dataset from a different healthcare system to assess generalizability and identify any dataset-specific biases or limitations.
2. **Baseline comparison**: Implement and compare against simpler models (e.g., logistic regression, random forests) using the same patient embeddings without graph structure to quantify the added value of the graph-based approach.
3. **Clinical validation study**: Conduct a prospective study where clinicians use the model's predictions and attention-based explanations to identify high-risk patients, measuring whether this improves early intervention rates compared to standard care protocols.