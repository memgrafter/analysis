---
ver: rpa2
title: Averaging log-likelihoods in direct alignment
arxiv_id: '2406.19188'
source_url: https://arxiv.org/abs/2406.19188
tags:
- reward
- step
- averaging
- direct
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of length-invariance in direct
  alignment methods for Large Language Models (LLMs). While cross-entropy loss used
  in supervised training is length-invariant due to token-wise averaging, direct alignment
  methods that optimize from preference data using contrastive losses involving log-likelihoods
  are not length-invariant.
---

# Averaging log-likelihoods in direct alignment

## Quick Facts
- **arXiv ID**: 2406.19188
- **Source URL**: https://arxiv.org/abs/2406.19188
- **Reference count**: 16
- **Primary result**: Length-normalized direct alignment achieves better length-quality tradeoffs but exhibits increased reward hacking

## Executive Summary
This paper addresses a fundamental discrepancy in direct alignment methods for Large Language Models (LLMs): while cross-entropy loss in supervised training is length-invariant due to token-wise averaging, direct alignment methods that optimize from preference data using contrastive losses are not length-invariant. This occurs because log-likelihoods depend on sequence length while token-wise averaging in cross-entropy makes it invariant. The authors introduce a principled approach to make direct alignment length-invariant by normalizing log-likelihoods by sequence length in the loss function. They validate this approach empirically on a summarization task, showing that length-normalized methods achieve higher rewards for shorter completions and improved trade-offs between generation length and quality, though at the cost of increased reward hacking behaviors.

## Method Summary
The paper introduces length-normalization to direct alignment methods by dividing log-likelihoods by sequence length in the loss function. For a preference pair (y+, y-) given input x, the length-averaged loss becomes: ℓavg_h(x, y+, y−; π) = h(β(1/|y+| ln π(y+|x)/πref(y+|x) − 1/|y−| ln π(y−|x)/πref(y−|x))). This transformation is theoretically justified as composing the averaging operator (geometric mean of token probabilities) with the optimality operator. The empirical study compares standard DPO and IPO with their length-averaged counterparts on the Reddit TL;DR summarization task using Llama2-7B as the policy model and a Bradley-Terry reward model. The authors sweep β values and measure reward-quality tradeoffs while monitoring for reward hacking behaviors.

## Key Results
- Length-normalized direct alignment achieves higher rewards for shorter completions compared to standard methods
- The averaging approach improves the trade-off between generation length and quality, moving closer to Pareto efficiency
- Length-normalization leads to increased reward hacking, particularly manifesting as never-ending punctuation patterns
- The optimal β values differ between standard and averaged methods, suggesting different sensitivity to hyperparameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct alignment methods are not length-invariant because they use log-likelihoods without normalization, while cross-entropy loss in supervised training is length-invariant due to token-wise averaging.
- Mechanism: The log-likelihood of a sequence is not length-invariant because it depends on the absolute length of the sequence. Token-wise averaging in cross-entropy loss makes it invariant to sequence length.
- Core assumption: The token probabilities in the autoregressive model are independent of sequence length.
- Evidence anchors:
  - [abstract]: "completions have various lengths, and the log-likelihood is not length-invariant. On the other side, the cross-entropy loss used in supervised training is length-invariant, as batches are typically averaged token-wise."
  - [section]: "So, it is, in practice, averaged token-wise."
- Break condition: If the token probabilities are not independent of sequence length, the mechanism breaks down.

### Mechanism 2
- Claim: The proposed averaging operator transforms a policy into one with geometric mean token probabilities, making it length-invariant.
- Mechanism: By taking the geometric mean of token probabilities, the averaging operator ensures that the sequence probability is invariant to the number of tokens.
- Core assumption: The geometric mean of token probabilities is a valid way to represent the sequence probability.
- Evidence anchors:
  - [abstract]: "we introduce a new averaging operator, to be composed with the optimality operator giving the best policy for the underlying RL problem. It translates into averaging the log-likelihood within the loss."
  - [section]: "πF (y|x) = (π(y|x))1/|y| Q|y|t=1 π(yi|x, y<i)"
- Break condition: If the geometric mean does not accurately represent the sequence probability, the mechanism breaks down.

### Mechanism 3
- Claim: Normalizing log-likelihoods by sequence length in the loss function makes direct alignment length-invariant.
- Mechanism: By dividing the log-likelihood by the sequence length, the contribution of each token to the loss is equalized, making the loss invariant to sequence length.
- Core assumption: The log-likelihood is linear in the number of tokens.
- Evidence anchors:
  - [abstract]: "translates into averaging the log-likelihood within the loss."
  - [section]: "ℓavg h (x, y+, y−; π) = h(β( 1 |y+| ln π(y+|x) πref(y+|x) − 1 |y−| ln π(y−|x) πref(y−|x) ))"
- Break condition: If the log-likelihood is not linear in the number of tokens, the mechanism breaks down.

## Foundational Learning

- Concept: Length-invariance in loss functions
  - Why needed here: To understand why direct alignment methods are not length-invariant and how to make them so.
  - Quick check question: What is the difference between the cross-entropy loss and the log-likelihood loss in terms of length-invariance?

- Concept: Geometric mean
  - Why needed here: To understand how the averaging operator works and why it makes the policy length-invariant.
  - Quick check question: What is the geometric mean of a set of numbers, and how does it differ from the arithmetic mean?

- Concept: Autoregressive models
  - Why needed here: To understand how the policy is defined and how the log-likelihood is calculated.
  - Quick check question: What is an autoregressive model, and how is it used in language modeling?

## Architecture Onboarding

- Component map: Policy model -> Averaging operator -> Optimality operator -> Loss function -> Bradley-Terry reward model

- Critical path:
  1. Train the policy model on a supervised dataset
  2. Apply the averaging operator to the policy model
  3. Optimize the averaged policy model using the optimality operator and the loss function
  4. Evaluate the performance of the optimized policy model

- Design tradeoffs:
  - Length-invariance vs. reward hacking: Making the policy length-invariant may lead to reward hacking, as observed in the experiments
  - Computational efficiency vs. accuracy: The averaging operator may increase the computational cost of training, but it improves the accuracy of the policy model

- Failure signatures:
  - Reward hacking: The policy model generates sequences with never-ending punctuation or other undesirable patterns
  - Length bias: The policy model generates sequences that are too short or too long, depending on the task

- First 3 experiments:
  1. Train the policy model on a supervised dataset and evaluate its performance
  2. Apply the averaging operator to the policy model and evaluate its performance
  3. Optimize the averaged policy model using the optimality operator and the loss function, and evaluate its performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does length normalization affect reward hacking behaviors in direct alignment methods beyond summarization tasks?
- Basis in paper: [explicit] The authors observe increased reward hacking with averaging, particularly evident in never-ending punctuation patterns, and note this as a potential area for refinement.
- Why unresolved: The study only examined reward hacking on a summarization task, and the phenomenon may manifest differently across various domains and tasks.
- What evidence would resolve it: Comparative experiments applying length-normalized direct alignment across multiple tasks (e.g., dialogue, translation, code generation) while systematically measuring reward hacking incidence and severity.

### Open Question 2
- Question: Does averaging log-likelihoods improve the generalization of direct alignment methods across datasets with varying completion lengths?
- Basis in paper: [inferred] The authors hypothesize that averaging the likelihood could make the choice of β more invariant to the dataset, especially with varying lengths of completions, but their experiment does not provide empirical evidence on this.
- Why unresolved: The experimental validation was limited to a single summarization dataset, preventing assessment of cross-dataset performance stability.
- What evidence would resolve it: Controlled experiments training and evaluating on multiple datasets with systematically varied completion lengths, comparing standard vs. averaged methods' performance variance.

### Open Question 3
- Question: How does length normalization in direct alignment compare to length-aware regularization approaches in RLHF when optimizing for human preferences?
- Basis in paper: [explicit] The authors suggest that length normalization could be more broadly applicable to RLHF, presenting it as an interesting future research direction.
- Why unresolved: The paper focuses exclusively on direct alignment methods and does not compare them to RLHF approaches with length-aware regularization.
- What evidence would resolve it: Direct empirical comparison between length-normalized direct alignment methods and RLHF methods incorporating length-aware regularization (e.g., KL penalties, length-based reward shaping) on the same preference datasets.

## Limitations

- Empirical validation is limited to a single summarization task and one model family (Llama2-7B), without investigation across other task types or model architectures
- The reward hacking analysis is primarily qualitative without quantitative metrics for measuring the frequency and severity of the phenomenon
- The paper does not discuss computational implications of length-normalization during training or inference

## Confidence

- **High confidence**: The theoretical derivation showing why log-likelihoods are not length-invariant and how normalization fixes this is mathematically sound. The geometric mean interpretation is well-established.
- **Medium confidence**: The empirical results showing improved length-quality tradeoffs are convincing but limited in scope. The claim that averaging improves Pareto efficiency is supported but could benefit from broader validation.
- **Low confidence**: The assertion that reward hacking is an inherent consequence of averaging rather than a hyperparameter or implementation issue is not fully established. The paper doesn't rule out that different β values or regularization could mitigate this.

## Next Checks

1. **Cross-task validation**: Replicate the experiments on instruction following and chat tasks (e.g., using datasets like Alpaca, ShareGPT) to verify whether length-normalization consistently improves Pareto efficiency across task types.

2. **Quantitative reward hacking analysis**: Implement metrics to count specific reward hacking patterns (e.g., consecutive punctuation, repeated tokens) and measure their frequency across different β values to determine if there's an optimal range that balances length-invariance with behavioral quality.

3. **Computational efficiency benchmarking**: Measure wall-clock training time and memory usage for both standard and length-normalized methods across varying sequence lengths to quantify the practical costs of implementing length-invariance.