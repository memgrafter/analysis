---
ver: rpa2
title: The Illusion of State in State-Space Models
arxiv_id: '2404.08819'
source_url: https://arxiv.org/abs/2404.08819
tags:
- state
- ssms
- tracking
- problems
- l-uniform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the theoretical expressive power of state-space
  models (SSMs) for state tracking compared to transformers. While SSMs are designed
  to handle sequential and stateful problems, the authors prove that SSMs, like transformers,
  cannot express computation outside the complexity class TC0.
---

# The Illusion of State in State-Space Models

## Quick Facts
- arXiv ID: 2404.08819
- Source URL: https://arxiv.org/abs/2404.08819
- Reference count: 26
- Primary result: State-space models (SSMs) have similar expressiveness limitations to transformers, unable to track state beyond TC0 complexity class.

## Executive Summary
This paper analyzes the theoretical expressive power of state-space models (SSMs) for state tracking compared to transformers. While SSMs are designed to handle sequential and stateful problems, the authors prove that SSMs, like transformers, cannot express computation outside the complexity class TC0. This means SSMs cannot solve inherently sequential problems like permutation composition, which underlies many state-tracking tasks such as tracking chess moves, evaluating code, or tracking entities in narratives. The authors show this both theoretically by proving SSMs can be simulated in TC0, and empirically by demonstrating that SSMs struggle to learn permutation composition in practice. They also propose minimal extensions to SSMs that increase their expressive power for state tracking.

## Method Summary
The paper combines theoretical analysis with empirical evaluation. Theoretically, the authors prove that linear SSMs can be simulated in the TC0 complexity class by reducing them to matrix powering operations. Empirically, they evaluate how well S4 and Mamba SSMs can learn permutation composition - a state-tracking problem that is NC1-complete and requires remembering input history. The evaluation compares SSMs against transformers and RNNs on sequences drawn from groups A5, A4 × Z5, and Z60, measuring accuracy as sequence length increases. The paper also proposes two extensions to SSMs (RNN-SSM and WFA-SSM) that theoretically increase expressive power beyond TC0.

## Key Results
- SSMs cannot express computation outside the TC0 complexity class, similar to transformers
- SSMs struggle to learn permutation composition, requiring depth that grows with sequence length
- Simple extensions (RNN-SSM with nonlinearity, WFA-SSM with input-dependent matrices) can increase SSM expressive power beyond TC0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSMs cannot express computation outside TC0 complexity class, limiting their ability to track state.
- Mechanism: The convolutional form of SSMs can be reduced to matrix powering operations that are in TC0. This means SSMs can only solve problems solvable by constant-depth threshold circuits.
- Core assumption: The convolutional form of SSMs computes the same function as the recurrent form.
- Evidence anchors:
  - [abstract]: "Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class TC^0."
  - [section 4.1]: "The convolutional form of the linear SSM layer defines the same h_1, ..., h_n but is unrolled as a summation of terms..."
  - [corpus]: Weak - related papers discuss SSM expressiveness but don't directly address the TC0 limitation mechanism.
- Break condition: If the convolutional and recurrent forms differ significantly in practice due to floating-point precision issues, the TC0 simulation argument may not hold.

### Mechanism 2
- Claim: The state in SSMs is an illusion because they cannot simulate recurrent computation like RNNs.
- Mechanism: SSMs use fixed matrices that get powered, while RNNs use input-dependent state transitions. This fundamental difference means SSMs cannot track state changes that require remembering arbitrary input sequences.
- Core assumption: Input-dependent state transitions are necessary for true state tracking.
- Evidence anchors:
  - [abstract]: "Despite its recurrent formulation, the 'state' in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers"
  - [section 2.1]: "SSMs consist of state-space layers, which can be thought of as simplified RNN layers."
  - [corpus]: Weak - related papers discuss SSMs vs RNNs but don't directly address the input-dependence requirement for state tracking.
- Break condition: If a new architecture variant of SSMs can incorporate input-dependent transitions without breaking parallelizability, this mechanism would be invalidated.

### Mechanism 3
- Claim: Simple extensions to SSMs can increase expressive power beyond TC0 for state tracking.
- Mechanism: Adding nonlinearities (making SSMs more like RNNs) or making the transition matrix input-dependent (making SSMs more like WFAs) allows SSMs to solve problems outside TC0.
- Core assumption: These extensions don't introduce practical implementation problems.
- Evidence anchors:
  - [section 5.1]: "An RNN-SSM layer with a step activation function can be defined as follows: h_i = sgn(A h_i-1 + B x_i)"
  - [section 5.2]: "Another completely different way to get greater expressive power is to let A matrix to be input-dependent."
  - [corpus]: Weak - related papers discuss SSM extensions but don't directly address the TC0 expressiveness limitation or solutions.
- Break condition: If these extensions cause severe vanishing gradients or other training issues, the mechanism would fail in practice.

## Foundational Learning

- Concept: Circuit complexity and complexity classes (TC0, NC1)
  - Why needed here: The paper's main theoretical contribution relies on classifying SSMs within circuit complexity classes to establish their expressiveness limitations.
  - Quick check question: What is the key difference between problems in TC0 vs NC1 that makes NC1-complete problems like S5 unsolvable by SSMs?

- Concept: Word problems on finite monoids and groups
  - Why needed here: State tracking is formalized as word problems on finite monoids, allowing algebraic analysis of which problems are "hard" to solve.
  - Quick check question: Why is the word problem for S5 NC1-complete, and how does this relate to SSMs' inability to track chess moves?

- Concept: Matrix operations and their computational complexity
  - Why needed here: The proof that SSMs can be simulated in TC0 relies on matrix powering being in TC0, and extensions rely on iterated matrix products.
  - Quick check question: Why can iterated matrix products not generally be computed in TC0, while matrix powers can?

## Architecture Onboarding

- Component map: Input sequence → Projection matrices (B, C) → State update (A matrix) → Output computation
- Critical path: For state tracking tasks, the critical path is the state update mechanism. Linear SSMs use fixed A matrices, limiting their ability to track state changes that depend on input history.
- Design tradeoffs:
  - Expressiveness vs parallelizability: SSM extensions that increase expressive power (RNN-SSM, WFA-SSM) may reduce parallelizability
  - Training stability vs expressiveness: Nonlinear extensions may cause vanishing gradients
  - Memory efficiency vs expressiveness: Input-dependent A matrices may require more parameters
- Failure signatures:
  - Poor performance on tasks requiring state tracking (chess, code evaluation, entity tracking)
  - Need for depth that grows with sequence length for permutation composition
  - Similar performance to transformers on state tracking tasks
- First 3 experiments:
  1. Train a standard S4 model and an RNN-SSM on the A5 word problem task. Compare minimum depth required for good performance as sequence length increases.
  2. Implement the WFA-SSM extension and evaluate its performance on permutation composition vs standard S4.
  3. Measure the parallelizability (wall-clock training time) of the extended SSM variants compared to standard S4.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed RNN-SSM and WFA-SSM architectures achieve the same level of parallelism as the original SSMs while maintaining their enhanced expressive power for state tracking?
- Basis in paper: The paper discusses that these extensions may negatively impact parallelism as well as learning dynamics.
- Why unresolved: The authors mention this as a competing practical concern but do not provide empirical evidence or theoretical analysis to determine if these architectures can be parallelized effectively on modern hardware.
- What evidence would resolve it: Empirical studies comparing the parallelism and performance of RNN-SSM and WFA-SSM architectures against the original SSMs and transformers on state tracking tasks would provide insights into their practical viability.

### Open Question 2
- Question: How do the learning dynamics of RNN-SSM and WFA-SSM compare to the original SSMs and transformers, particularly in terms of gradient vanishing and convergence speed?
- Basis in paper: The authors express concern that the iterated product of matrices in WFA-SSM may lead to vanishing gradient issues, and that adding nonlinearities in RNN-SSM might affect learning dynamics.
- Why unresolved: While the paper mentions these potential issues, it does not provide empirical evidence or theoretical analysis to quantify the impact on learning dynamics.
- What evidence would resolve it: Experimental studies comparing the training stability, convergence speed, and final performance of RNN-SSM and WFA-SSM against the original SSMs and transformers on various tasks would shed light on their learning dynamics.

### Open Question 3
- Question: Are there other minimal extensions to SSMs that could increase their expressive power for state tracking without significantly compromising parallelism or learning dynamics?
- Basis in paper: The paper proposes two extensions (RNN-SSM and WFA-SSM) but suggests these may come with practical drawbacks. The authors view it as an open question whether other SSM-like models with greater expressivity can balance expressivity, parallelism, and learning dynamics.
- Why unresolved: The authors acknowledge the limitations of their proposed extensions and express interest in exploring alternative SSM-like models that could achieve a better balance between expressivity and practical considerations.
- What evidence would resolve it: Research exploring novel architectural modifications to SSMs, followed by empirical evaluation of their expressive power, parallelism, and learning dynamics on state tracking tasks, would provide insights into the feasibility of developing SSM-like models with enhanced capabilities.

## Limitations

- The theoretical proof assumes perfect numerical precision, which may not hold in practical implementations
- The empirical evaluation focuses on permutation composition, which may not capture all aspects of real-world state tracking
- The proposed extensions (RNN-SSM and WFA-SSM) lack empirical validation of their practical effectiveness and training stability

## Confidence

- High Confidence: The theoretical proof that linear SSMs cannot solve NC1-complete problems like permutation composition. The circuit complexity arguments are rigorous and well-established.
- Medium Confidence: The empirical demonstration that SSMs struggle with permutation composition, and the general argument that SSM expressiveness limitations will affect real-world state tracking tasks.
- Low Confidence: The practical effectiveness of proposed SSM extensions and their ability to solve real-world state tracking problems without significant training difficulties.

## Next Checks

1. **Extension Training Stability**: Implement and train the proposed RNN-SSM and WFA-SSM extensions on permutation composition tasks while monitoring training curves, gradient norms, and convergence behavior. This would validate whether the theoretical expressiveness gains translate to practical learnability.

2. **Cross-Domain State Tracking**: Test SSM variants on diverse state tracking tasks beyond permutation composition, such as entity tracking in stories, program evaluation with control flow, or multi-agent state tracking. This would reveal whether the theoretical limitations generalize to practical scenarios.

3. **Hybrid Architecture Evaluation**: Create hybrid models that combine SSMs with explicit state tracking mechanisms (like external memory or attention over state variables) and compare their performance to pure SSMs and transformers on challenging state tracking benchmarks. This would test whether the limitations are fundamental or can be circumvented through architectural modifications.