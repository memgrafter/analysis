---
ver: rpa2
title: Lean and Mean Adaptive Optimization via Subset-Norm and Subspace-Momentum with
  Convergence Guarantees
arxiv_id: '2411.07120'
source_url: https://arxiv.org/abs/2411.07120
tags:
- proj
- memory
- noise
- have
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces two techniques for memory-efficient optimization\
  \ in large-scale deep learning: Subset-Norm and Subspace-Momentum. Subset-Norm reduces\
  \ the memory footprint of adaptive step sizes from O(d) to O(\u221Ad) by partitioning\
  \ parameters into subsets that share adaptive step sizes."
---

# Lean and Mean Adaptive Optimization via Subset-Norm and Subspace-Momentum with Convergence Guarantees

## Quick Facts
- arXiv ID: 2411.07120
- Source URL: https://arxiv.org/abs/2411.07120
- Authors: Thien Hang Nguyen; Huy Le Nguyen
- Reference count: 40
- Primary result: AdamSNSM achieves Adam's perplexity in half the tokens while using 20% of the memory

## Executive Summary
This paper introduces two complementary techniques for memory-efficient optimization in large-scale deep learning: Subset-Norm and Subspace-Momentum. Subset-Norm reduces the memory footprint of adaptive step sizes from O(d) to O(√d) by partitioning parameters into subsets that share adaptive step sizes. Subspace-Momentum compresses momentum by applying it in a low-dimensional subspace while using standard SGD in the orthogonal complement. The combined method, AdamSNSM, demonstrates that large language models can be trained with significantly reduced memory requirements without sacrificing convergence speed or final performance.

## Method Summary
The paper proposes AdamSNSM, which combines Subset-Norm and Subspace-Momentum techniques. Subset-Norm partitions model parameters into subsets and shares adaptive step sizes within each subset, reducing memory from O(d) to O(√d). Subspace-Momentum applies momentum only in a low-dimensional subspace (via SVD projection) while using standard SGD in the orthogonal complement, further reducing memory usage. The method is evaluated on LLaMA models (60M-1B parameters) trained on C4 dataset, showing that AdamSNSM achieves the same validation perplexity as Adam while using only 20% of the memory footprint and requiring approximately half the training tokens.

## Key Results
- AdamSNSM achieves Adam's validation perplexity in ~6.8B tokens vs 13.1B tokens for Adam
- Memory footprint reduced to 20% of Adam's (0.84GB vs 4.99GB for 1B model)
- Minimal hyperparameter tuning required compared to existing memory-efficient optimizers
- Strong stability demonstrated across multiple LLaMA model sizes

## Why This Works (Mechanism)
The two techniques work synergistically to reduce memory while maintaining convergence. Subset-Norm exploits the observation that many parameters in deep networks have similar gradients, allowing them to share adaptive step sizes without significantly impacting convergence. Subspace-Momentum recognizes that most momentum information is captured in a low-dimensional subspace, allowing the majority of parameters to use simple SGD while maintaining the benefits of momentum in the critical subspace. Together, these methods reduce both the adaptive step size memory (from first-order moments) and momentum memory (from second-order moments) to a fraction of the original requirements.

## Foundational Learning
- **Parameter partitioning**: Required to group parameters into subsets for shared adaptive step sizes. Quick check: Verify partition sizes and ensure 2D parameters are handled appropriately.
- **SVD projection**: Needed to identify the low-dimensional subspace for momentum application. Quick check: Confirm projection rank and update frequency maintain computational efficiency.
- **Memory accounting**: Essential for quantifying optimizer state reduction. Quick check: Track memory usage for both adaptive steps and momentum separately.
- **Convergence analysis**: Provides theoretical guarantees under standard assumptions. Quick check: Verify all assumptions hold in practice and monitor convergence behavior.

## Architecture Onboarding

**Component Map**
AdamSNSM -> Subset-Norm partitioning -> Shared adaptive steps
AdamSNSM -> Subspace-Momentum SVD -> Projected momentum updates

**Critical Path**
1. Partition parameters into subsets for Subset-Norm
2. Apply adaptive step sizes within each subset
3. Compute SVD for momentum subspace
4. Update momentum in subspace, use SGD in complement
5. Apply combined updates to parameters

**Design Tradeoffs**
- Memory vs. convergence: Subset-Norm trades some adaptivity for memory reduction
- Projection frequency vs. accuracy: Subspace-Momentum balances subspace updates with computational overhead
- Partition size vs. performance: Larger subsets save more memory but may reduce adaptivity

**Failure Signatures**
- Poor convergence: Indicates Subset-Norm partitioning is too coarse or subspace is poorly chosen
- Memory not reduced: Suggests incorrect implementation of parameter sharing or projection mechanism
- Training instability: May result from incorrect subspace projection updates or learning rate issues

**First Experiments**
1. Implement and test Subset-Norm with varying partition sizes to find optimal balance
2. Verify Subspace-Momentum with different projection ranks (128, 512) on small model
3. Combine both techniques and compare memory usage against theoretical predictions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to LLaMA models (60M-1B parameters) on C4 dataset only
- Subset-Norm heuristic for 2D parameter partitioning not fully specified
- No comparison with other memory-efficient optimizers on larger-scale models
- Theoretical convergence guarantees may not fully capture practical behavior

## Confidence
- **High confidence**: Memory reduction claims (20% of Adam's footprint) and convergence guarantees under stated assumptions
- **Medium confidence**: Training efficiency claims (halving token requirements) based on single-dataset evaluation
- **Low confidence**: Generalizability to larger models (>1B parameters) and other domains beyond LLMs on C4

## Next Checks
1. Reproduce Subset-Norm and Subspace-Momentum components separately to isolate implementation issues
2. Verify memory usage calculations by instrumenting optimizer state allocation during training
3. Extend evaluation to include larger model sizes and additional datasets to test generalizability