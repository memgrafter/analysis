---
ver: rpa2
title: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided
  Interventions
arxiv_id: '2401.09395'
source_url: https://arxiv.org/abs/2401.09395
tags:
- question
- reasoning
- inches
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates the robustness of large language models (LLMs)
  on math and coding tasks by introducing perturbed versions of questions from GSM8K
  and HumanEval datasets. A general ontology of perturbations was created, covering
  44 categories that modify aspects like logic, computation, and representation.
---

# Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions

## Quick Facts
- arXiv ID: 2401.09395
- Source URL: https://arxiv.org/abs/2401.09395
- Reference count: 40
- Key outcome: Introducing perturbed questions reveals significant performance drops (>30 points) in LLMs' mathematical and coding reasoning abilities

## Executive Summary
This paper evaluates the robustness of large language models (LLMs) on math and coding tasks by introducing perturbed versions of questions from GSM8K and HumanEval datasets. A general ontology of perturbations was created, covering 44 categories that modify aspects like logic, computation, and representation. Using a semi-automatic pipeline with GPT-4 and human verification, two datasets—GSMORE and HUMANEVAL-CORE—were generated with 216 and 219 perturbed questions, respectively. Evaluations across multiple open and closed-source LLMs showed significant performance drops when answering perturbed questions, revealing vulnerabilities in their reasoning and problem-solving abilities. Closed-source models like GPT-4 and o1-preview outperformed open-source models, but all models struggled particularly with concept analysis and formulation adjustment perturbations.

## Method Summary
The paper employs a semi-automatic perturbation generation pipeline using GPT-4 followed by human validation. The method creates perturbed questions by modifying one or two aspects of original GSM8K and HumanEval questions according to a general ontology covering eight aspects: information, query, values, toolbox, structure, representation, answer, and answer format. Each perturbation type targets specific reasoning capabilities. The resulting GSMORE and HUMANEVAL-CORE datasets contain 216 and 219 perturbed questions respectively, which are then evaluated across multiple open and closed-source LLMs using standardized prompt templates. Human annotators verify the logical coherence and solvability of generated perturbations.

## Key Results
- Performance drops of over 30 points observed when LLMs answer perturbed questions versus original questions
- Closed-source models (GPT-4, o1-preview) significantly outperform open-source models across all perturbation categories
- All models show particular difficulty with concept analysis and formulation adjustment perturbations
- Representational perturbations reveal format rigidity issues in LLM reasoning
- Models struggle with critical thinking perturbations involving misinformation and contradictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ontological perturbations expose structural gaps in LLM reasoning by changing one aspect at a time
- Mechanism: The ontology isolates eight aspects and applies perturbations only to one at a time, ensuring the change is unidirectional and controlled
- Core assumption: Perturbing a single aspect while keeping others fixed isolates the specific reasoning capability being tested
- Evidence anchors:
  - [abstract] "introduce (i) a general ontology of perturbations for math and coding questions"
  - [section] "Each perturbation changes only one or two aspects of the original question"
- Break condition: If multiple aspects are altered simultaneously, the isolated effect is lost and attribution becomes ambiguous

### Mechanism 2
- Claim: Perturbations that preserve underlying logic but change representation reveal format rigidity in LLMs
- Mechanism: Representational perturbations keep the same logical structure but alter how information or answers are encoded, testing whether the model can adapt to new formats without changing reasoning
- Core assumption: LLMs encode logic and representation separately, so changing one should not break the other
- Evidence anchors:
  - [abstract] "modification of the encoding of the question or solution while preserving the underlying logic"
  - [section] "Representational Perturbations retain the logical structure... only to exclusively change the representation or encoding"
- Break condition: If the model conflates format with meaning, it will fail even when logic is unchanged

### Mechanism 3
- Claim: Adding misinformation or contradictions tests error detection and robustness to noisy inputs
- Mechanism: Perturbations introduce plausible but incorrect information or logical conflicts to see if the model can identify and ignore them
- Core assumption: Robust reasoning requires not just correct inference but also the ability to reject invalid premises
- Evidence anchors:
  - [abstract] "Critical Thinking, which includes scenarios where...inconsistencies"
  - [section] "Critical Thinking: Identification of noise, inaccuracies and inconsistencies"
- Break condition: If the model overfits to training data, it may accept contradictory information as valid

## Foundational Learning

- Concept: Chain of Thought reasoning
  - Why needed here: Many perturbations require multi-step reasoning; CoT helps models track intermediate steps and avoid short-circuiting to wrong answers
  - Quick check question: Can the model solve a simple 2-step arithmetic problem only if prompted to "think step by step"?

- Concept: Abstract vs concrete reasoning
  - Why needed here: Some perturbations replace concrete values with variables, testing whether the model can manipulate abstract symbols rather than memorize number patterns
  - Quick check question: Given 2 + 3 = 5, can the model infer x + y = z for arbitrary x, y, z?

- Concept: Symbolic manipulation
  - Why needed here: Certain perturbations ask for equations or code transformations that preserve relationships without computing numeric results
  - Quick check question: Can the model derive 2X = 3Y from knowing 4 = 6 in the same ratio?

## Architecture Onboarding

- Component map: Perturbation generator -> GPT-4 candidate filter -> Human verifier -> Gold answer annotator -> LLM evaluation pipeline
- Critical path: Perturbation generation -> Validation (automatic + human) -> Answer annotation -> Automated LLM scoring -> Manual spot-check
- Design tradeoffs: Semi-automatic pipeline trades scalability for higher perturbation quality; human verification is slow but ensures logical coherence and instruction adherence
- Failure signatures: GPT-4 generating nonsensical questions, human annotators disagreeing on solvability, LLMs failing to follow explicit reasoning constraints (e.g., "no keyword" prompts)
- First 3 experiments:
  1. Generate 5 perturbed questions for Remove Constraint and run through GPT-4 + human filter; verify logical coherence
  2. Evaluate one closed-source LLM (e.g., GPT-4) on original vs. perturbed set; measure performance drop
  3. Apply "WhatIf" perturbation to a simple math problem; check if model can correctly adjust answer under new condition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do performance drops correlate with the number of reasoning steps required?
- Basis in paper: [inferred] from the lack of correlation between step count and performance decline observed in experiments
- Why unresolved: The experiments showed no definitive correlation between the number of reasoning steps and performance, but the study didn't explore if specific step types or reasoning patterns matter more than step count
- What evidence would resolve it: A detailed analysis mapping specific reasoning patterns (e.g., recursive reasoning, cross-referencing) to performance drops would clarify if step complexity or type matters more than quantity

### Open Question 2
- Question: What is the relative impact of open-source vs. closed-source models on handling enhanced perturbation types?
- Basis in paper: [explicit] from the observation that open-source models don't experience as significant a performance drop as closed-source models when handling enhanced types
- Why unresolved: The study notes the difference but doesn't explore the underlying reasons, such as architectural differences or training data exposure
- What evidence would resolve it: Comparative studies of model architectures and training data composition would reveal why open-source models handle enhanced perturbations differently

### Open Question 3
- Question: How does the difficulty of original questions influence the performance on perturbed variants?
- Basis in paper: [explicit] from the observation that performance appears to diminish based on the inherent difficulty of the original question in GSM8K
- Why unresolved: While the study notes a correlation, it doesn't quantify the exact relationship between original question difficulty and perturbation impact
- What evidence would resolve it: A systematic analysis correlating original question difficulty metrics (e.g., step count, concept complexity) with performance drops on perturbed variants would clarify this relationship

### Open Question 4
- Question: What are the specific limitations of fine-tuned models in concept analysis tasks?
- Basis in paper: [explicit] from the observation that fine-tuned models (e.g., CodeLlama, Metamath) show enhanced performance in logic alteration and representational perturbations but worse in concept analysis
- Why unresolved: The study identifies the issue but doesn't explore the underlying reasons, such as task-specific fine-tuning limiting broader reasoning capacities
- What evidence would resolve it: Comparative studies of fine-tuned models vs. general models on concept analysis tasks, focusing on reasoning patterns and adaptability, would reveal the specific limitations

## Limitations
- The perturbation pipeline's reliance on GPT-4 for initial generation may introduce biases that human verification cannot fully eliminate
- The evaluation framework tests robustness but doesn't establish whether improvements in perturbed task performance transfer to real-world problem-solving scenarios
- The paper doesn't provide the exact prompt templates used for each perturbation type, limiting reproducibility

## Confidence
- **High Confidence**: The methodology for creating perturbations is well-defined and the performance degradation across models is consistently observed across multiple perturbation types and datasets
- **Medium Confidence**: The claim that representational perturbations specifically reveal format rigidity is supported but could be strengthened with more detailed analysis
- **Low Confidence**: The assertion that certain perturbation categories are universally more challenging across all models needs more granular analysis

## Next Checks
1. **Transfer Learning Validation**: Train models on a subset of perturbed questions and test whether performance improvements generalize to unseen perturbations within the same category
2. **Error Pattern Analysis**: Conduct a systematic error analysis categorizing failure modes to identify whether certain perturbation types consistently trigger specific reasoning failures
3. **Human Performance Benchmark**: Compare LLM performance on perturbed questions against human performance on the same tasks to establish whether observed performance drops represent genuine reasoning limitations or artifacts of the evaluation framework