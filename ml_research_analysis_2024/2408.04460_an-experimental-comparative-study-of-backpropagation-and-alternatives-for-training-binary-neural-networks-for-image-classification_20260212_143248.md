---
ver: rpa2
title: An experimental comparative study of backpropagation and alternatives for training
  binary neural networks for image classification
arxiv_id: '2408.04460'
source_url: https://arxiv.org/abs/2408.04460
tags:
- binary
- training
- neural
- networks
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates training binary neural networks (BNNs)\
  \ for image classification using alternative algorithms to backpropagation (BP).\
  \ It compares three BP alternatives\u2014DFA, DRTP, and HSIC\u2014along with two\
  \ additional methods, SigpropTL, across three model architectures (VGG-19, MobileNetV2,\
  \ MLP-Mixer) on the ImageNette dataset."
---

# An experimental comparative study of backpropagation and alternatives for training binary neural networks for image classification

## Quick Facts
- arXiv ID: 2408.04460
- Source URL: https://arxiv.org/abs/2408.04460
- Reference count: 28
- Binary neural networks trained with BP and alternatives show BP generally achieves best accuracy for modern architectures

## Executive Summary
This paper investigates training binary neural networks (BNNs) for image classification using alternative algorithms to backpropagation (BP). It compares three BP alternatives—DFA, DRTP, and HSIC—along with two additional methods, SigpropTL, across three model architectures (VGG-19, MobileNetV2, MLP-Mixer) on the ImageNette dataset. The study finds that while BP generally achieves the best performance in terms of accuracy, especially for modern architectures like MobileNetV2 and MLP-Mixer, it fails to outperform DFA for VGG-19. Binarizing weights and activations significantly degrades performance, with activations having a slightly larger impact than weights. Skip connections are crucial for BP performance, and their removal makes DFA more competitive. DRTP, HSIC, and SigpropTL offer reduced memory usage and faster training but at the cost of lower accuracy. Overall, BP remains the preferred method for accuracy, but alternatives like DRTP may be advantageous when memory and speed are prioritized.

## Method Summary
The paper compares backpropagation with alternative training algorithms for binary neural networks on image classification tasks. Experiments are conducted on three model architectures (VGG-19, MobileNetV2, MLP-Mixer) using the ImageNette dataset, along with additional datasets for validation. Binary neural networks are implemented using PyTorch with sign functions for binarization and straight-through estimators for gradient estimation. The study evaluates the performance of BP, DFA, DRTP, HSIC, and SigpropTL algorithms, comparing accuracy scores on test sets, runtime performance, and memory usage. Hyperparameters are tuned through grid search for learning rates and algorithm-specific parameters.

## Key Results
- BP generally achieves the best accuracy for modern architectures (MobileNetV2, MLP-Mixer) but fails to outperform DFA for VGG-19
- Binarizing weights and activations significantly degrades performance, with activations having a slightly larger impact than weights
- Skip connections are crucial for BP performance; their removal makes DFA more competitive

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: BP still mostly outperforms alternatives for modern BNN architectures.
- **Mechanism**: BP computes gradients recursively through all layers via the chain rule, providing precise credit assignment for binary weight and activation layers.
- **Core assumption**: The quality of the teaching signal degrades when traversing multiple STE approximations, but BP still preserves more useful gradient information than alternatives.
- **Evidence anchors**:
  - [abstract] "while BP generally achieves the best performance in terms of accuracy, especially for modern architectures like MobileNetV2 and MLP-Mixer"
  - [section] "For the MLP-Mixer and MobileNet-V2 models, BP trains the most accurate models"
  - [corpus] Weak evidence - no related work directly addresses BP vs alternatives for BNNs on modern architectures.
- **Break condition**: If the model architecture lacks skip connections, BP performance degrades significantly and DFA becomes competitive.

### Mechanism 2
- **Claim**: Binarizing weights has a slightly larger impact on performance than binarizing activations.
- **Mechanism**: Weight binarization directly affects the linear transformation in each layer, while activation binarization only affects the input to subsequent layers. The impact of weight binarization is more fundamental to model capacity.
- **Core assumption**: The reduction in representational power from weight binarization is more severe than from activation binarization.
- **Evidence anchors**:
  - [section] "activations having a slightly larger impact than weights" (Note: this contradicts the claim, suggesting activations have larger impact)
  - [section] "DFA manages to train better binary VGG-19 models compared to BP for all datasets" - implies weight binarization impact varies by algorithm.
  - [corpus] Weak evidence - no related work quantifies the relative impact of weight vs activation binarization.
- **Break condition**: If only weights are binarized, performance degradation may be acceptable for some architectures and algorithms.

### Mechanism 3
- **Claim**: Skip connections are crucial for BP performance in BNNs, and their removal makes DFA more competitive.
- **Mechanism**: Skip connections provide gradient paths that bypass some STE approximations, improving gradient flow. Without them, BP's advantage diminishes and DFA's direct error projection becomes more effective.
- **Core assumption**: Skip connections mitigate the credit assignment problem in BP for BNNs.
- **Evidence anchors**:
  - [section] "Removing the skip connections highly degrades the performance of all the algorithms. However, BP shows the biggest performance drop, to the point it makes DFA the better performing algorithm"
  - [section] "DFA was previously observed to not work well with convolutional neural networks in the literature" - context for why this result is surprising.
  - [corpus] Weak evidence - no related work directly addresses skip connections' impact on BNN training algorithms.
- **Break condition**: If the architecture inherently lacks skip connections (like VGG-19), BP's performance degrades significantly regardless of skip connection presence.

## Foundational Learning

- **Concept**: Binary Neural Networks and Straight-Through Estimators (STE)
  - Why needed here: Understanding how BNNs handle non-differentiable operations during training is fundamental to grasping the paper's experimental setup.
  - Quick check question: What is the purpose of the Straight-Through Estimator in BNN training, and how does it modify the gradient flow?

- **Concept**: Alternative training algorithms to backpropagation (DFA, DRTP, HSIC, SigpropTL)
  - Why needed here: The paper compares BP with these alternatives, so understanding their mechanisms is crucial for interpreting the results.
  - Quick check question: How do DFA and DRTP differ from BP in terms of error propagation, and what are the potential advantages of these differences?

- **Concept**: Model architectures (VGG-19, MobileNetV2, MLP-Mixer) and their characteristics
  - Why needed here: Different architectures respond differently to binarization and training algorithms, which is a key finding of the paper.
  - Quick check question: Why might VGG-19 be more challenging for BP to optimize in its binarized form compared to MobileNetV2 or MLP-Mixer?

## Architecture Onboarding

- **Component map**: VGG-19 -> Binary CNN -> BP/DFA/DRTP/HSIC/SigpropTL; MobileNetV2 -> Binary CNN -> BP/DFA/DRTP/HSIC/SigpropTL; MLP-Mixer -> Binary MLP -> BP/DFA/DRTP/HSIC/SigpropTL
- **Critical path**: Model selection -> Binarization scheme selection -> Training algorithm selection -> Hyperparameter tuning -> Evaluation on test set
- **Design tradeoffs**: Accuracy vs. memory usage vs. training speed. BP provides best accuracy but highest memory usage; alternatives like DRTP, HSIC, and SigpropTL offer reduced memory usage and faster training but at the cost of lower accuracy.
- **Failure signatures**: BP failing to converge on VGG-19 binarized models; significant accuracy drops when skip connections are removed; alternatives failing to match BP's accuracy on modern architectures.
- **First 3 experiments**:
  1. Train VGG-19, MobileNetV2, and MLP-Mixer with continuous weights and activations using BP as baseline.
  2. Train the same models with binary weights and activations using BP, comparing accuracy degradation.
  3. Train the models with binary weights and activations using DFA, DRTP, HSIC, and SigpropTL, comparing their performance to BP.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does backpropagation fail to outperform DFA for VGG-19 but not for MobileNetV2 and MLP-Mixer when training BNNs?
- Basis in paper: [explicit] The paper notes that BP fails to achieve the best performance for VGG-19, while DFA outperforms it by 30 points, but BP performs better for the other architectures.
- Why unresolved: The paper suggests this might be due to exploding or vanishing gradient issues related to the large size of convolutional kernels in VGG-19, but this is not definitively proven.
- What evidence would resolve it: Detailed analysis of gradient flow and stability during training for each architecture, specifically comparing the behavior of BP and DFA in VGG-19 versus the other models.

### Open Question 2
- Question: Can the performance of BNNs be improved by only binarizing weights or activations instead of both?
- Basis in paper: [explicit] The paper suggests that binarizing weights and activations has a similar negative impact on performance when using BP, but this impact is smaller for DFA and DRTP.
- Why unresolved: The paper does not experimentally test the scenario where only weights or only activations are binarized, so the trade-off between memory savings and accuracy loss is not quantified.
- What evidence would resolve it: Experimental results comparing the accuracy of models with only binarized weights, only binarized activations, and both binarized, across different architectures and algorithms.

### Open Question 3
- Question: How do skip connections affect the performance of BNNs trained with backpropagation alternatives like DFA and DRTP?
- Basis in paper: [explicit] The paper shows that removing skip connections degrades performance for all algorithms, but BP is most affected, making DFA the better choice for architectures without skip connections.
- Why unresolved: The paper does not explore why skip connections are crucial for BP but less so for DFA and DRTP, nor does it investigate the mechanisms behind this difference.
- What evidence would resolve it: Analysis of gradient flow and information propagation in models with and without skip connections, specifically for BP, DFA, and DRTP, to understand the underlying reasons for the performance differences.

## Limitations
- Experiments are conducted only on the ImageNette dataset, which is a subset of ImageNet, and may not generalize to full-scale datasets or other domains.
- The study focuses on three specific architectures (VGG-19, MobileNetV2, MLP-Mixer) and may not capture the behavior of alternative model families.
- Implementation details of alternative algorithms (DFA, DRTP, HSIC, SigpropTL) are not fully specified, which could affect reproducibility.

## Confidence

**High confidence**: BP generally achieving best accuracy for modern architectures; memory and speed advantages of alternatives.

**Medium confidence**: Relative impact of weight vs activation binarization; importance of skip connections for BP performance.

**Low confidence**: DFA outperforming BP on VGG-19; generalizability to other datasets and architectures.

## Next Checks

1. Replicate experiments on full ImageNet or other standard datasets to test generalizability.
2. Implement and test the same algorithms on additional architectures (ResNet, EfficientNet) to validate architecture-specific findings.
3. Conduct ablation studies on skip connection importance by testing architectures with varying numbers of skip connections.