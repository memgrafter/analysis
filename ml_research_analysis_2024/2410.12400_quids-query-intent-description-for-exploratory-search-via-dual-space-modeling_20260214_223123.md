---
ver: rpa2
title: 'QUIDS: Query Intent Description for Exploratory Search via Dual Space Modeling'
arxiv_id: '2410.12400'
source_url: https://arxiv.org/abs/2410.12400
tags:
- intent
- query
- documents
- relevant
- irrelevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating user-facing natural
  language query intent descriptions in exploratory search, where search engines must
  infer intent from retrieved documents due to vague queries. The authors propose
  QUIDS, a dual-space modeling approach that uses contrastive learning in both representation
  and disentangling spaces to isolate relevant intent information while suppressing
  irrelevant content.
---

# QUIDS: Query Intent Description for Exploratory Search via Dual Space Modeling

## Quick Facts
- arXiv ID: 2410.12400
- Source URL: https://arxiv.org/abs/2410.12400
- Authors: Yumeng Wang; Xiuying Chen; Suzan Verberne
- Reference count: 40
- Primary result: Dual-space modeling with contrastive learning significantly outperforms state-of-the-art baselines on query intent description generation for exploratory search

## Executive Summary
QUIDS addresses the challenge of generating user-facing natural language query intent descriptions in exploratory search, where search engines must infer intent from retrieved documents due to vague queries. The authors propose a dual-space modeling approach that uses contrastive learning in both representation and disentangling spaces to isolate relevant intent information while suppressing irrelevant content. Enhanced by intent-driven hard negative sampling, QUIDS significantly outperforms state-of-the-art baselines across ROUGE, BERTScore, and human/LLM evaluations, improving both the quality of intent descriptions and transparency in the retrieval process.

## Method Summary
QUIDS employs a dual-encoder architecture with cross-attention to jointly model relevant and irrelevant documents for query intent description generation. The model uses contrastive learning in two spaces: representation space (separating relevant/irrelevant document embeddings) and disentangling space (decoding intent descriptions). Intent-driven hard negative sampling augments training data by selecting semantically similar irrelevant documents. The dual cross-attention decoder attends to both relevant and irrelevant documents simultaneously, with the final representation emphasizing relevant content through contrastive learning.

## Key Results
- QUIDS significantly outperforms state-of-the-art baselines on ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore metrics
- Ablation studies confirm dual-space modeling and hard negative sampling contribute to performance gains
- Human and LLM evaluations show QUIDS generates more fluent, factually aligned, and inclusion-focused intent descriptions
- Model demonstrates superior performance on exploratory queries compared to informational queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-space contrastive learning enables the model to learn distinct representations for relevant and irrelevant information, improving intent description quality.
- Mechanism: The model uses two separate spaces - a representation space for encoding relevant/irrelevant documents and a disentangling space for decoding intent descriptions. In the representation space, relevant document embeddings are pulled together while being pushed away from irrelevant embeddings. In the disentangling space, the decoder learns to attend to relevant information while suppressing irrelevant content.
- Core assumption: Relevant and irrelevant documents have distinct semantic features that can be effectively separated through contrastive learning.
- Evidence anchors:
  - [abstract]: "dual-space contrastive learning to isolate intent-relevant information while suppressing irrelevant content"
  - [section 3.3]: "contrastive learning is performed in the representation space to differentiate between embeddings for relevant and irrelevant documents"
- Break condition: If relevant and irrelevant documents share too many overlapping semantic features, the contrastive separation becomes ineffective.

### Mechanism 2
- Claim: Intent-driven hard negative sampling improves the model's ability to distinguish between relevant and irrelevant information.
- Mechanism: During training, the model augments irrelevant documents by selecting those semantically similar to relevant documents (hard negatives) based on their embeddings. This creates a more challenging training scenario where the model must learn finer distinctions between relevant and irrelevant content.
- Core assumption: Including harder negative samples during training improves the model's discriminative capabilities.
- Evidence anchors:
  - [section 3.2]: "we design a method to choose intent-aware hard negative samples based on semantic similarity"
  - [section 4.2]: "Enhanced by intent-driven hard negative sampling, the model significantly outperforms state-of-the-art baselines"
- Break condition: If the threshold for selecting hard negatives is too low, the model may overfit to irrelevant content.

### Mechanism 3
- Claim: The dual-encoder architecture with cross-attention layers enables query-aware modeling of relevant and irrelevant intent spaces.
- Mechanism: The model employs two cross-encoders that jointly encode query-document pairs, capturing the relationship between queries and both relevant and irrelevant documents. During decoding, dual cross-attention layers allow the decoder to attend to both relevant and irrelevant documents simultaneously, with the final representation being a combination that emphasizes relevant content.
- Core assumption: Joint encoding of query-document pairs captures more nuanced relationships than document-only encoding.
- Evidence anchors:
  - [section 3.3.1]: "we implement a dual encoder architecture, with each encoder being a cross-encoder, so that each cross-encoder directly models the relevance of a relevant or irrelevant document given a query"
  - [section 3.3.2]: "we design a contrastive decoder by adding an additional cross attention so that it can attend to both relevant and irrelevant documents"
- Break condition: If the cross-attention mechanism becomes too complex, it may lead to overfitting or computational inefficiency.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To enable the model to learn meaningful representations by pulling together similar items (relevant documents) and pushing apart dissimilar items (relevant vs. irrelevant documents)
  - Quick check question: How does the model ensure that relevant documents are closer together than relevant and irrelevant documents in the representation space?

- Concept: Cross-attention mechanisms
  - Why needed here: To allow the decoder to attend to both relevant and irrelevant documents simultaneously, enabling nuanced intent description generation
  - Quick check question: What happens to the decoder's attention weights when it processes irrelevant documents?

- Concept: Data augmentation with hard negatives
  - Why needed here: To create a more challenging training scenario that improves the model's ability to distinguish between relevant and irrelevant information
  - Quick check question: How does the model select which irrelevant documents to include as hard negatives?

## Architecture Onboarding

- Component map: Query → Dual Encoder → Representation Space Contrastive Learning → Decoder with Dual Cross-Attention → Disentangling Space Contrastive Learning → Intent Description

- Critical path: Query → Dual Encoder → Representation Space Contrastive Learning → Decoder with Dual Cross-Attention → Disentangling Space Contrastive Learning → Intent Description

- Design tradeoffs:
  - Complexity vs. performance: Dual-space modeling provides better results but increases model complexity
  - Hard negative selection: Stricter thresholds improve discrimination but may limit training data
  - Cross-attention depth: Deeper attention layers capture more nuance but increase computational cost

- Failure signatures:
  - Poor ROUGE scores despite high BERTScore: Indicates semantic similarity without lexical alignment
  - Low exclusion scores in human evaluation: Suggests model is including irrelevant information
  - Performance degradation with longer documents: May indicate truncation issues or attention limitations

- First 3 experiments:
  1. Compare single-space vs. dual-space modeling on ROUGE and BERTScore metrics
  2. Test different hard negative selection thresholds to find optimal discrimination
  3. Evaluate cross-attention ablation to determine impact on intent quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QUIDS perform on extremely long documents that exceed the 1024-token limit?
- Basis in paper: [inferred] The paper mentions that documents longer than 1024 tokens are truncated and shows minimal performance differences across input length categories.
- Why unresolved: The paper only evaluates documents up to the truncation limit and doesn't explore performance on significantly longer documents that would be truncated.
- What evidence would resolve it: Testing QUIDS on documents 2-5 times the token limit to measure degradation in performance metrics like ROUGE and BERTScore.

### Open Question 2
- Question: Can the dual-space contrastive learning approach be extended to handle multi-turn conversational queries?
- Basis in paper: [explicit] The conclusion mentions future work aims to extend the approach to conversational search.
- Why unresolved: The paper only evaluates single-query scenarios and doesn't explore how the dual-space modeling would handle conversational context and history.
- What evidence would resolve it: Evaluating QUIDS on conversational search datasets like TREC Conversational Assistant Track or CANARD, measuring performance across multiple query turns.

### Open Question 3
- Question: How sensitive is QUIDS to the choice of margin parameter t in the irrelevant feature representation space?
- Basis in paper: [explicit] The paper sets t=1 in equation (2) but doesn't explore sensitivity to this hyperparameter.
- Why unresolved: The paper only uses one fixed value and doesn't analyze how different margin values affect the model's ability to distinguish relevant from irrelevant content.
- What evidence would resolve it: Conducting ablation studies with different margin values (e.g., 0.5, 1.5, 2.0) and measuring impact on ROUGE scores and the exclusion metric.

## Limitations

- Evaluation relies heavily on automatic metrics and LLM-based human evaluation rather than extensive real user studies
- Model shows better performance on exploratory queries compared to informational queries, limiting generalizability
- Dual-space approach significantly increases model complexity and computational requirements

## Confidence

- Dual-space contrastive learning effectiveness: High
- Intent-driven hard negative sampling benefits: Medium-High
- Superiority over state-of-the-art baselines: High

## Next Checks

1. **Cross-domain validation:** Test QUIDS on different query intent description datasets (beyond Q2ID) to assess generalization across domains and query types, particularly focusing on informational queries where the model showed relatively weaker performance.

2. **User study with real search scenarios:** Conduct a user study where participants use QUIDS-generated descriptions in actual exploratory search sessions, measuring task completion rates, satisfaction, and whether the descriptions help users refine their queries effectively.

3. **Ablation of hard negative sampling thresholds:** Systematically vary the hard negative selection threshold (0.8 to 1.0 cosine similarity) and document selection size (1-10 documents) to determine optimal configurations and assess sensitivity to these hyperparameters across different query distributions.