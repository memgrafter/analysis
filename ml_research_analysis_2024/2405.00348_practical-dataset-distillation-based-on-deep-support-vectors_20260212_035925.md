---
ver: rpa2
title: Practical Dataset Distillation Based on Deep Support Vectors
arxiv_id: '2405.00348'
source_url: https://arxiv.org/abs/2405.00348
tags:
- dataset
- data
- distillation
- loss
- practical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a practical dataset distillation method that
  addresses the challenge of limited dataset access in real-world scenarios. By combining
  Deep KKT (DKKT) loss, which incorporates model knowledge, with Distribution Matching
  (DM) loss, the authors synthesize a condensed dataset that outperforms existing
  methods when only a small fraction of the original data is available.
---

# Practical Dataset Distillation Based on Deep Support Vectors

## Quick Facts
- **arXiv ID**: 2405.00348
- **Source URL**: https://arxiv.org/abs/2405.00348
- **Reference count**: 19
- **Primary result**: A dataset distillation method that achieves higher accuracy than existing methods when only 1% of the original data is available

## Executive Summary
This paper proposes a practical dataset distillation method that addresses the challenge of limited dataset access in real-world scenarios. By combining Deep KKT (DKKT) loss, which incorporates model knowledge, with Distribution Matching (DM) loss, the authors synthesize a condensed dataset that outperforms existing methods when only a small fraction of the original data is available. The key idea is to leverage Deep Support Vectors (DSVs) extracted from pretrained models to enhance the distillation process, even with minimal access to the training data. Experiments on the CIFAR-10 dataset show that the proposed method achieves higher accuracy compared to DM and DSV-based approaches, especially in settings with limited data access (e.g., 1% of the dataset). The results demonstrate the effectiveness of integrating model knowledge through DKKT loss to improve dataset distillation performance in practical scenarios.

## Method Summary
The proposed method combines two key components: Deep KKT (DKKT) loss and Distribution Matching (DM) loss. DKKT loss leverages model knowledge by incorporating Deep Support Vectors (DSVs) extracted from pretrained models, which represent critical data points that influence model decision boundaries. The DM loss ensures that the synthesized dataset maintains statistical properties similar to the original data distribution. By optimizing these two losses simultaneously, the method synthesizes a condensed dataset that captures essential information from the original data while being significantly smaller in size. The approach is particularly effective when only a limited portion of the original dataset is accessible, as it can still produce high-quality distilled datasets by leveraging pretrained model knowledge through DSVs.

## Key Results
- The proposed method outperforms DM and DSV-based approaches on CIFAR-10, especially with limited data access (e.g., 1% of the dataset)
- Higher accuracy is achieved compared to existing methods in low-data regimes
- Demonstrates effectiveness of integrating model knowledge through DKKT loss in dataset distillation

## Why This Works (Mechanism)
The method works by leveraging model knowledge through Deep Support Vectors (DSVs) extracted from pretrained models. These DSVs represent critical data points that lie close to decision boundaries and have significant influence on model predictions. By incorporating DKKT loss that utilizes these DSVs, the method can synthesize distilled datasets that capture essential information even when only a small fraction of the original data is available. The combination with DM loss ensures that the synthesized dataset maintains the statistical properties of the original distribution, leading to improved performance in downstream tasks.

## Foundational Learning
- **Deep Support Vectors (DSVs)**: Critical data points extracted from pretrained models that lie near decision boundaries and significantly influence model predictions. Needed to capture model-specific knowledge for distillation.
- **Distribution Matching (DM) loss**: Ensures statistical similarity between synthesized and original datasets. Critical for maintaining data distribution properties in the distilled dataset.
- **Deep KKT (DKKT) loss**: Incorporates model knowledge through DSVs into the optimization process. Required for leveraging pretrained model information in dataset distillation.
- **Dataset distillation**: Process of synthesizing a small, condensed dataset that can train models to achieve similar performance as models trained on the full original dataset. Fundamental concept being improved upon.
- **Limited data access scenarios**: Real-world situations where only a fraction of the original training data is available. Context that motivates the practical application of this method.

## Architecture Onboarding

**Component Map**: Original Data -> Pretrained Model (DSV Extraction) -> DKKT+DM Loss Optimization -> Distilled Dataset

**Critical Path**: The optimization process involves three main stages: (1) Extracting DSVs from a pretrained model, (2) Computing both DKKT and DM losses between the synthesized and original data, and (3) Updating the synthesized dataset parameters through gradient descent to minimize the combined loss.

**Design Tradeoffs**: The method trades computational complexity for improved performance by incorporating DKKT loss alongside DM loss. While this adds overhead compared to pure DM methods, it enables better performance especially in low-data regimes. The approach also requires access to a pretrained model, which may not always be available but provides valuable model knowledge when accessible.

**Failure Signatures**: The method may fail when the pretrained model is poorly aligned with the target task or when the original dataset distribution is highly complex and cannot be adequately captured with limited access. Additionally, if the DSV extraction process is not robust, it may lead to suboptimal distilled datasets.

**First Experiments**:
1. Replicate the CIFAR-10 results with 1% data access to verify the claimed performance improvements
2. Test the method on a different dataset (e.g., CIFAR-100) to evaluate generalizability
3. Compare the computational overhead of the full DKKT+DM approach versus DM-only baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Limited experimental validation to only CIFAR-10 dataset, raising questions about generalizability
- No exploration of the impact of using suboptimal or mismatched pretrained models for DSV extraction
- Computational overhead introduced by DKKT component compared to pure DM methods is not quantified

## Confidence
- **High confidence** in the core methodology of combining DKKT and DM losses for dataset distillation
- **Medium confidence** in the reported performance gains given limited experimental validation
- **Low confidence** in the claimed practical superiority without additional benchmarks on diverse datasets

## Next Checks
1. Cross-dataset validation: Test the method on CIFAR-100, SVHN, and Fashion-MNIST to verify robustness across data distributions
2. Pretrained model sensitivity analysis: Systematically evaluate the impact of different pretrained model architectures and training regimes
3. Computational overhead quantification: Measure and compare wall-clock time and memory requirements versus DM-only baselines