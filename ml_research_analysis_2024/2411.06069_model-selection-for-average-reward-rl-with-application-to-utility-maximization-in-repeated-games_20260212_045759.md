---
ver: rpa2
title: Model Selection for Average Reward RL with Application to Utility Maximization
  in Repeated Games
arxiv_id: '2411.06069'
source_url: https://arxiv.org/abs/2411.06069
tags:
- regret
- policy
- reward
- opponent
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes MRBEAR, an online model selection algorithm
  for average reward reinforcement learning (RL). MRBEAR builds on regret balancing
  and elimination techniques to learn an optimal policy for a Markov Decision Process
  (MDP) belonging to one of M model classes of varying complexity, without knowing
  which class is well-specified.
---

# Model Selection for Average Reward RL with Application to Utility Maximization in Repeated Games

## Quick Facts
- arXiv ID: 2411.06069
- Source URL: https://arxiv.org/abs/2411.06069
- Reference count: 40
- One-line primary result: MRBEAR achieves regret only linearly worse than the optimal model class by combining multiplicative regret balancing with misspecification elimination.

## Executive Summary
This work introduces MRBEAR, an online model selection algorithm for average reward reinforcement learning that learns an optimal policy across multiple model classes of varying complexity without knowing which class is well-specified. Building on regret balancing and elimination techniques, MRBEAR maintains an active set of model classes and uses a misspecification test to eliminate poorly specified classes while balancing regret across active ones. The algorithm achieves a regret bound of $\tilde{O}(MC_{m^*}^2 B_{m^*}(T,\delta))$, where the additional cost of model selection scales only linearly in $M$, the number of model classes.

## Method Summary
MRBEAR extends the regret balancing and elimination framework to the average reward setting by maintaining an active set of model classes and using a misspecification test to eliminate poorly specified ones. The algorithm runs a base algorithm (PMEVI-DT) on each model class and balances regret multiplicatively so that the regret of each active class stays within a constant factor of the optimal class's regret. The misspecification test checks whether the sum of confidence bounds and empirical rewards exceeds the maximum empirical reward across all classes by a margin dependent on the number of samples. By treating each possible opponent memory limit as a separate model class, MRBEAR achieves exponential improvement in repeated game settings compared to learning directly in the most general model class.

## Key Results
- MRBEAR achieves regret of $\tilde{O}(MC_{m^*}^2 B_{m^*}(T,\delta))$ where $C_{m^*}$ is the complexity of the simplest well-specified model class
- In repeated games, MRBEAR obtains regret bound of $\tilde{O}(M(\text{sp}(h^*) B^{m^*} A^{m^*+1})^{3/2} \sqrt{T})$ compared to $\tilde{O}(\sqrt{AMBm^*T})$ for direct learning
- Lower bound proves exponential dependency on opponent memory $m^*$ is unavoidable in repeated games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MRBEAR achieves regret only linearly worse than the optimal model class by combining multiplicative regret balancing with misspecification elimination.
- Mechanism: The algorithm maintains an active set of model classes, eliminates misspecified classes using a high-probability test, and balances regret multiplicatively so that the regret of each active class stays within a constant factor of the optimal class's regret.
- Core assumption: The misspecification test (inequality 4) correctly identifies misspecified classes with high probability under the event G, and the regret guarantees Bi(T) are accurate for well-specified classes.
- Evidence anchors:
  - [abstract]: "The algorithm maintains a set of active model classes and uses a misspecification test to eliminate classes that are not well-specified, while balancing regret across active classes."
  - [section 3]: The misspecification test checks whether Bi(Ni,k,δ) + ∑t∈Ni, kRt/Ni,k ≥ maxj≥i ∑t∈Nj,kRt − 2ch*/Nj/k.
  - [section 4]: Under event G, all well-specified classes remain active, and lemma 4.2 proves multiplicative regret balance.
- Break condition: If the misspecification test fails to eliminate a misspecified class (false negative) or eliminates a well-specified class (false positive), the regret bound degrades significantly.

### Mechanism 2
- Claim: The regret decomposition into individual model class regrets allows MRBEAR to pause and resume base algorithms across different model classes without harming regret guarantees.
- Mechanism: Since base algorithms like PMEVI-DT don't exploit transitions between epochs, MRBEAR can interleave epochs from different model classes and still maintain valid regret bounds for each.
- Core assumption: The base algorithm's regret guarantee holds regardless of which model class is active at any given iteration, as long as the underlying MDP structure is preserved.
- Evidence anchors:
  - [section 4]: "Any planning effects from a policy must be fully realized by the end of an epoch; so running a base learning algorithm in a selected model class for a single epoch is sufficient to generate an accurate sample of a policy's performance."
  - [section 3]: "After ending an epoch due to the doubling trick, they do not pay attention to or exploit that the first state of the next epoch is a successor of the last state, action pair of the previous epoch."
  - [section 4]: "Therefore the use of t ∈ Ni,K(T) in above expression is valid."
- Break condition: If a base algorithm does depend on state transitions between epochs, the regret decomposition becomes invalid.

### Mechanism 3
- Claim: The exponential improvement in the repeated game application comes from MRBEAR learning the opponent's memory limit simultaneously with optimal policy, avoiding the need to learn in the most general model class.
- Mechanism: By treating each possible memory limit as a separate model class, MRBEAR can achieve regret in O(M√A³(m*+1)B³m*T) instead of O(√AMBm*T) when learning directly in the most general class.
- Core assumption: The opponent's policy induces a weakly communicating MDP for all model classes with memory m ≥ m*, and the realizability assumption holds (some model class is well-specified).
- Evidence anchors:
  - [section 5]: "Model selection instead allows us to learn the opponent's memory limit simultaneously with learning an optimal policy, by treating the state space induced by each potential memory limit as a separate model class."
  - [corollary 5.6]: "In our setting, the size of the state space (i.e., (A × B)m) increases exponentially with respect to the memory m. Therefore, instead of having a regret bound in O(√AMBm*T) by directly learning in CM, MRBEAR enjoys from a regret in O(M√A³(m*+1)B³m*T) which is an exponential improvement with respect to the memory."
  - [section 5]: "The opponent can condition their choice in each game on the past 0 ≤ m* < M plays, where the upper limit M is known to us but the true limit m* is not."
- Break condition: If the opponent's policy doesn't induce a well-defined MDP for any of the model classes, or if the realizability assumption fails.

## Foundational Learning

- Concept: Weakly communicating MDPs and their properties (diameter, span of optimal bias)
  - Why needed here: The regret bounds for PMEVI-DT and the analysis of MRBEAR depend on understanding the complexity measures of MDPs, particularly sp(h*) and its relationship to diameter D.
  - Quick check question: Can you explain why sp(h*) ≤ D for any MDP, and provide an example where sp(h*) < D?

- Concept: Regret balancing and elimination techniques from online learning
  - Why needed here: MRBEAR is built on the regret balancing and elimination framework, which requires understanding how to maintain regret balance across multiple algorithms and how to eliminate poorly performing ones.
  - Quick check question: What is the key difference between additive and multiplicative regret balancing, and why does MRBEAR use multiplicative balancing?

- Concept: De Bruijn sequences and their applications in constructing hard instances
  - Why needed here: The lower bound proof for the repeated game setting uses de Bruijn sequences to construct opponent policies that force any algorithm to have high regret.
  - Quick check question: Can you construct a de Bruijn sequence for B={0,1} and m=3, and explain how it would be used to create a hard instance for a learning algorithm?

## Architecture Onboarding

- Component map: MRBEAR -> Misspecification test -> Active set management -> PMEVI-DT base algorithms -> Model classes
- Critical path:
  1. Warm-up phase: Run each base algorithm for c5h* iterations to bring well-specified algorithms into their guarantee region
  2. Epoch loop: For each epoch k:
     - Check misspecification for all active classes using inequality 4
     - Eliminate misspecified classes from active set
     - Select model class with smallest Bi(Ni,k-1) and run one epoch of its base algorithm
     - Update iteration counts and proceed to next epoch
  3. Termination: When total iterations reach T, return final policy from the active model class with smallest regret
- Design tradeoffs:
  - Misspecification test frequency vs. computational overhead: Less frequent testing reduces overhead but may delay elimination of misspecified classes
  - Choice of α in regret balancing: Controls how tightly regret is balanced across active classes (α ∈ [1/2, 3/4] in MRBEAR)
  - Base algorithm selection: PMEVI-DT provides optimal regret bounds but requires specific assumptions about confidence regions
- Failure signatures:
  - Regret grows faster than O(MCm*²Bm*(T,δ)): Indicates misspecification test is failing to eliminate misspecified classes or regret balance is breaking
  - Active set shrinks to empty: Misspecification test is too aggressive, eliminating well-specified classes (false positives)
  - Active set remains too large: Misspecification test is not aggressive enough, keeping misspecified classes (false negatives)
- First 3 experiments:
  1. Implement MRBEAR with a single model class (M=1) and verify it reduces to PMEVI-DT with the same regret bound
  2. Test MRBEAR with two model classes where one is clearly misspecified and verify the misspecified class is eliminated
  3. Implement the repeated game application with known opponent memory and verify MRBEAR achieves the claimed improvement over learning in the most general model class

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dependence on the number of model classes M in the regret bound be made additive rather than multiplicative?
- Basis in paper: The authors explicitly note that their regret bound scales linearly in M and suggest this as an open direction.
- Why unresolved: The current analysis shows a multiplicative dependence, and proving an additive bound would require new techniques beyond regret balancing and elimination.
- What evidence would resolve it: A rigorous proof showing that the model selection cost is additive in M, or a lower bound demonstrating that multiplicative dependence is unavoidable.

### Open Question 2
- Question: Can MRBEAR be extended to an anytime algorithm without requiring prior knowledge of an upper bound on the span of optimal bias sp(h*)?
- Basis in paper: The authors mention this as an attractive direction, noting that their current algorithm requires ch* as input.
- Why unresolved: The doubling trick used in PMEVI-DT requires knowing ch*, and adapting this to work without it would require novel algorithmic modifications.
- What evidence would resolve it: A modified version of MRBEAR that works without ch* and maintains the same regret guarantees, or a lower bound showing why this is impossible.

### Open Question 3
- Question: Can the framework for repeated games be extended to handle self-play analysis, where both players use MRBEAR?
- Basis in paper: The authors explicitly identify this as a promising avenue for future work.
- Why unresolved: The current analysis focuses on a learner facing a fixed unknown opponent strategy, while self-play would require analyzing the dynamics when both players adapt simultaneously.
- What evidence would resolve it: A theoretical analysis of equilibrium behavior when MRBEAR is used in self-play, or empirical simulations showing convergence properties.

## Limitations

- The algorithm requires knowing an upper bound on the span of optimal bias sp(h*) to set the confidence parameter ch*, which may not be available in practice
- The exponential improvement claim for repeated games assumes the realizability assumption holds (some model class is well-specified), which may not be satisfied for all opponent strategies
- The misspecification test relies on high-probability bounds that may fail in worst-case scenarios, potentially leading to incorrect elimination of well-specified classes

## Confidence

- **High Confidence**: Regret bound of O(MCm*²Bm*(T,δ)) when G holds and misspecification test works correctly. This follows directly from the regret balancing analysis in section 4.
- **Medium Confidence**: Exponential improvement claim for repeated games. While the analysis is sound under stated assumptions, the practical magnitude depends heavily on the unknown opponent memory m* and the realizability assumption.
- **Medium Confidence**: Lower bound proof showing exponential dependency on m* is unavoidable. The construction using de Bruijn sequences is valid, but the analysis assumes specific conditions on the learning algorithm.

## Next Checks

1. Implement a controlled experiment with two model classes where one is deliberately misspecified and verify the misspecification test correctly eliminates it with high probability across multiple runs.
2. Test MRBEAR with a base algorithm that does exploit state transitions between epochs to verify the regret decomposition breaks as predicted when this assumption is violated.
3. Construct and verify a de Bruijn sequence for B={0,1} and m=3, then implement the lower bound construction to confirm the exponential regret lower bound holds in simulation.