---
ver: rpa2
title: 'GRAF: Graph Retrieval Augmented by Facts for Romanian Legal Multi-Choice Question
  Answering'
arxiv_id: '2412.04119'
source_url: https://arxiv.org/abs/2412.04119
tags:
- legal
- question
- answer
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first open-source Romanian legal MCQA
  dataset (JuRO), a structured legal corpus (CROL), and the first Romanian legal knowledge
  graph (Law-RoG). It proposes GRAF, a novel Graph Retrieval Augmented by Facts method
  that outperforms state-of-the-art MCQA approaches in most legal branches.
---

# GRAF: Graph Retrieval Augmented by Facts for Romanian Legal Multi-Choice Question Answering

## Quick Facts
- arXiv ID: 2412.04119
- Source URL: https://arxiv.org/abs/2412.04119
- Reference count: 40
- First open-source Romanian legal MCQA dataset (JuRO) with state-of-the-art accuracy up to 60.09% on promotion exams

## Executive Summary
This paper introduces GRAF, a novel Graph Retrieval Augmented by Facts method for Romanian legal multi-choice question answering. The framework combines claim graph extraction, knowledge graph sampling, and fact-aware retrieval with self-attention to score answer choices. GRAF achieves up to 60.09% accuracy on promotion exams, surpassing state-of-the-art MCQA approaches across civil, penal, and administrative law domains. The work provides new resources including the first open-source Romanian legal MCQA dataset (JuRO), structured legal corpus (CROL), and legal knowledge graph (Law-RoG) with 160k nodes and 320k edges.

## Method Summary
GRAF processes question-answer pairs by first extracting claims using an LLM to identify entities and relations, forming a claim graph. The framework then samples a relevant sub-graph from the knowledge graph using BM25 retrieval and breadth-first search, limiting to ~50 entities. A Graph Attention Network (GAT) encodes the KG entities and relations with separate attention vectors for nodes and edges, capturing relational topological information. The final answer scoring uses self-attention over cosine similarity scores between encoded claims and KG relations, producing probability distributions over answer choices.

## Key Results
- Achieves 60.09% accuracy on Romanian legal promotion exams, outperforming state-of-the-art MCQA approaches
- Demonstrates consistent performance gains across civil, penal, and administrative law domains
- Provides the first open-source Romanian legal MCQA dataset (JuRO) with 10,836 questions from three examination types

## Why This Works (Mechanism)

### Mechanism 1
GRAF uses claim graph extraction to decompose each (question, choice) pair into its underlying claims, allowing targeted knowledge retrieval from the KG. The framework prompts an LLM to identify named entities and relations between them in each question-choice pair, forming a claim graph. This claim graph is then aligned with the sampled KG sub-graph using cosine similarity between encoded claims and relations.

### Mechanism 2
The knowledge graph sampling via BM25 retrieval and breadth-first search limits computational complexity while capturing relevant legal context. The framework preprocesses text (tokenization, lemmatization), uses BM25 to select top-k entities from the KG, then performs BFS to retrieve neighboring nodes and edges up to depth 1, limiting total entities to ~50.

### Mechanism 3
The Graph Attention Network (GAT) with modified node and edge attention captures relational topological information in the KG, improving claim-to-KG alignment. The framework encodes KG entities and relations using a language encoder, then applies GAT with separate attention vectors for nodes (aN) and edges (aE), computing attention coefficients for both node-node (eijN) and node-edge (eijE) relationships.

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) and Graph Attention Networks (GATs)
  - Why needed here: GRAF uses GAT to encode the knowledge graph structure and capture relational topological information between legal entities
  - Quick check question: How does GAT differ from standard GNNs, and why is this difference important for encoding legal knowledge graphs with both nodes and edges?

- **Concept**: Information Retrieval (IR) techniques, specifically BM25
  - Why needed here: GRAF uses BM25 retrieval to identify the most relevant entities from the large legal knowledge graph before sampling the sub-graph
  - Quick check question: What is the main advantage of BM25 over simpler TF-IDF approaches for entity retrieval in legal documents?

- **Concept**: Cosine similarity for vector alignment
  - Why needed here: GRAF computes alignment between encoded claims and KG relations using cosine similarity to identify relevant information for answer scoring
  - Quick check question: Why is cosine similarity preferred over Euclidean distance for comparing high-dimensional embeddings in NLP tasks?

## Architecture Onboarding

- **Component map**: Question → Claim graph extraction → KG sampling → KG encoding → Alignment computation → Aggregation → Answer scoring
- **Critical path**: Question → Claim graph extraction → KG sampling → KG encoding → Alignment computation → Aggregation → Answer scoring
- **Design tradeoffs**:
  - Using LLM for claim extraction provides flexibility but introduces computational cost and potential hallucination
  - BM25 + BFS sampling balances relevance and computational efficiency but may miss some relevant entities
  - GAT captures relational structure but requires careful parameter tuning
  - Self-attention allows flexible information aggregation but adds complexity

- **Failure signatures**:
  - Poor performance across all legal domains suggests issues with claim extraction or KG construction
  - Domain-specific failures indicate sampling may be missing relevant entities for that domain
  - Inconsistent performance between encoder-based and LLM-based baselines suggests issues with the alignment mechanism

- **First 3 experiments**:
  1. Test claim graph extraction quality by manually evaluating extracted claims on sample questions
  2. Compare BM25 sampling with random sampling to verify retrieval effectiveness
  3. Test GAT encoding by comparing performance with and without relational attention (using only node features)

## Open Questions the Paper Calls Out

- How does the performance of GRAF compare to state-of-the-art models when the backbone model is scaled to sizes beyond 11B parameters, such as with Llama 3.1 70B or larger variants?
- What is the impact of using alternative lightweight solutions for claim graph extraction, such as training a smaller language model or distilling Mixtral to a small language model, on the overall performance of GRAF?
- How does the performance of GRAF vary across different legal domains and examination types when the model is trained on a dataset that is specifically curated for each domain or examination type, rather than the entire JuRO dataset?

## Limitations
- Performance heavily depends on the quality of claim graph extraction, which relies on an LLM that may hallucinate or miss critical legal relationships
- Lacks ablation studies to isolate the contribution of each component (claim extraction, KG sampling, GAT encoding)
- Knowledge graph construction process using a single LLM may introduce factual errors or incomplete legal coverage

## Confidence
- **High confidence**: The methodological framework (claim graph extraction → KG sampling → GAT encoding → alignment scoring) is clearly specified and technically sound
- **Medium confidence**: The reported performance improvements over baselines, though results show consistent gains across multiple legal domains
- **Low confidence**: Claims about GRAF's ability to handle low-resource languages beyond Romanian, as the evaluation is confined to a single language

## Next Checks
1. Conduct ablation study to systematically disable each component (claim extraction, KG sampling, GAT encoding) to quantify individual contributions to overall performance
2. Test GRAF on legal MCQA datasets in other low-resource languages to validate claims about broader applicability
3. Manually verify a sample of extracted claims and KG relationships to assess the quality and reliability of the automated construction process