---
ver: rpa2
title: Autoregressive Large Language Models are Computationally Universal
arxiv_id: '2410.03170'
source_url: https://arxiv.org/abs/2410.03170
tags:
- will
- system
- string
- context
- turing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes that autoregressive decoding of transformer-based
  language models can realize universal computation without external intervention
  or modification of model weights. The key insight is that extended autoregressive
  decoding, where emitted tokens are appended to the input sequence, allows a language
  model to simulate a Lag system, which is computationally universal.
---

# Autoregressive Large Language Models are Computationally Universal

## Quick Facts
- arXiv ID: 2410.03170
- Source URL: https://arxiv.org/abs/2410.03170
- Authors: Dale Schuurmans; Hanjun Dai; Francesco Zanini
- Reference count: 6
- Primary result: Autoregressive decoding of transformer-based language models can realize universal computation without external intervention or modification of model weights.

## Executive Summary
This paper establishes that autoregressive decoding of transformer-based language models can achieve universal computation by simulating a Lag system, which is computationally universal. The key insight is that extended autoregressive decoding, where emitted tokens are appended to the input sequence, allows a language model to simulate the behavior of a Lag system. The authors prove that any Turing machine can be simulated by a restricted (2,2)-Lag system with 2027 production rules, and demonstrate this by developing a system prompt that drives gemini-1.5-pro-001 to correctly apply each of these rules under deterministic decoding.

## Method Summary
The authors construct a universal (2,2)-Lag system L(U15,2) from a universal Turing machine U15,2 with 15 states and 2 symbols, resulting in 2027 production rules over 262 symbols. They develop an invertible mapping between triples and token pairs to create a system prompt Sgemini containing all 2027 rules. Using gemini-1.5-pro-001 with temperature=0 and a sliding window of length 2, they verify that the model correctly applies each production rule under extended autoregressive decoding, thereby simulating the execution of the universal Turing machine.

## Key Results
- Autoregressive decoding of transformer-based language models can realize universal computation without external intervention or modification of model weights
- Extended autoregressive decoding simulates Lag system computation by appending emitted tokens to the input sequence as the context window advances
- gemini-1.5-pro-001 with extended autoregressive (greedy) decoding is a general-purpose computer by the Church-Turing thesis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extended autoregressive decoding simulates Lag system computation by appending emitted tokens to the input sequence as the context window advances.
- Mechanism: For each iteration, the model generates a token from the current context, appends it to the sequence, and shifts the context window forward by one position. This creates a circular queue behavior where tokens rotate through the context.
- Core assumption: The language model's deterministic behavior (temperature=0) ensures that the same context always produces the same output token.
- Evidence anchors: [abstract] "autoregressive decoding of a transformer-based language model can realize universal computation, without external intervention or modification of the model's weights"; [section 2] "we consider a generalization of autoregressive decoding where, given a long input, emitted tokens are appended to the end of the sequence as the context window advances"

### Mechanism 2
- Claim: A Lag system with specific production rules can simulate any Turing machine's bidirectional memory access patterns.
- Mechanism: The Lag system uses control tokens that orbit the memory string in a circular queue. Leftward movement uses O(n) iterations while rightward movement uses O(n³) iterations, but both are achievable through carefully designed rule sets.
- Core assumption: The Lag system can distinguish between different control token types (L, l, R, r) and use them to coordinate complex memory access patterns.
- Evidence anchors: [section 4.2] "a rule set Rright allows memory access location to be moved one position clockwise in O(n³) iterations (Lemma 6)"; [section 6] "Algorithm 4 operating on L produced by Algorithm 4 simulates the execution of Algorithm 3 for a given Turing machine T on any input"

### Mechanism 3
- Claim: Extended autoregressive decoding of a 4-gram model can simulate any restricted (2,2)-Lag system, which is Turing universal.
- Mechanism: The language model generates 2-4 tokens per context window, allowing it to represent the output of Lag system rules that produce either 1 or 2 symbols. The system prompt encodes all 2027 production rules from the universal Lag system.
- Core assumption: The language model's context window of size 2 can capture the necessary state information to apply each production rule correctly.
- Evidence anchors: [section 8] "we use gemini-1.5-pro-001, which is a publicly released large language model... developed a single system prompt that drives this model to correctly execute each of the 2027 rules"; [section 7] "Algorithm 2 operating on L(U15,2) is able to simulate the execution of any Turing machine T on any input"

## Foundational Learning

- Concept: Lag systems and their computational universality
  - Why needed here: Understanding how Lag systems work is essential to grasping why extended autoregressive decoding can achieve Turing completeness
  - Quick check question: What is the key difference between a Lag system and a Tag system that makes Lag systems computationally universal?

- Concept: Extended autoregressive decoding and context window management
  - Why needed here: This is the mechanism that allows language models to process arbitrarily long sequences by appending generated tokens
  - Quick check question: How does extended autoregressive decoding differ from standard autoregressive decoding when the input length exceeds the context window?

- Concept: Turing machine simulation and finite memory techniques
  - Why needed here: The proof relies on simulating Turing machines using only finite memory through delimiter symbols and dynamic memory allocation
  - Quick check question: Why is it necessary to use a delimiter symbol when simulating a Turing machine with finite memory?

## Architecture Onboarding

- Component map: Input sequence processor -> Rule engine -> Token generator -> State tracker -> System prompt manager
- Critical path: Initialize with input sequence and system prompt -> Extract 2-token context window from current position -> Apply matching production rule to generate output tokens -> Append output tokens to sequence and advance window -> Repeat until halting condition met
- Design tradeoffs: Context window size vs. rule expressiveness (larger windows could simplify rules but increase computational cost); Deterministic vs. stochastic generation (deterministic ensures reproducibility but may limit flexibility); System prompt size vs. model capacity (larger rule sets require more context but may exceed model limits)
- Failure signatures: Non-deterministic outputs (indicates temperature or seed issues); Context window misalignment (causes incorrect rule application); System prompt overflow (model cannot process all 2027 rules); Rule application failures (context doesn't match any production rule)
- First 3 experiments: 1. Verify deterministic token generation (test that the same 2-token context always produces the same output tokens); 2. Validate sliding window behavior (confirm that the context window advances correctly after each token generation); 3. Test rule application accuracy (check that a subset of production rules are correctly applied given their contexts)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the Church-Turing thesis falsifiable in principle, as suggested by the authors?
- Basis in paper: [explicit] The authors conclude with an "admittedly extreme" conjecture that the Church-Turing thesis is falsifiable.
- Why unresolved: The Church-Turing thesis is a philosophical assertion about the nature of computation, not a formal mathematical statement that can be proven or disproven within the framework of existing mathematics.
- What evidence would resolve it: A demonstration of a physical system capable of computation beyond Turing machines would falsify the thesis. Conversely, a rigorous mathematical proof showing that all conceivable computational models are equivalent to Turing machines would strengthen the thesis.

### Open Question 2
- Question: Can a single system prompt be developed that works across multiple large language models to simulate the universal Lag system L(U15,2)?
- Basis in paper: [inferred] The authors mention this as a "possibility worth considering" but have not yet conducted the necessary verifications on other language models.
- Why unresolved: The current proof relies on a specific system prompt (Sgemini) for gemini-1.5-pro-001. Different language models may have varying architectures, training data, and parameter configurations that affect their ability to interpret and execute the same prompt.
- What evidence would resolve it: Successful application of a single system prompt to multiple large language models, demonstrating their ability to simulate L(U15,2) with extended autoregressive decoding.

### Open Question 3
- Question: Can the 2027 production rules in the universal Lag system L(U15,2) be further compressed to simplify verification for other language models?
- Basis in paper: [explicit] The authors state that "it seems like even this rule set can likely be compressed further, leading to easier verifications for other language models."
- Why unresolved: The current rule set is a result of a direct reduction from the specific universal Turing machine U15,2. Alternative reductions or more efficient encodings might lead to a smaller set of rules.
- What evidence would resolve it: Development and successful application of a more compact set of production rules that can still simulate L(U15,2) with extended autoregressive decoding on a large language model.

## Limitations
- Context window dependency: The proof relies heavily on the language model's context window size being exactly 2 tokens
- Determinism assumption: Assumes perfect deterministic behavior under greedy decoding, which may not hold in practice
- System prompt completeness: The exact content and structure of the critical system prompt Sgemini are not provided

## Confidence
- High Confidence: The theoretical framework establishing that Lag systems can simulate Turing machines is well-founded in computational theory literature
- Medium Confidence: The demonstration that gemini-1.5-pro-001 can correctly apply all 2027 production rules under deterministic decoding is supported by verification, but implementation details are lacking
- Low Confidence: Generalizability to other language models and robustness under real-world conditions remains uncertain

## Next Checks
1. Implement systematic verification of each of the 2027 production rules using the provided universal Lag system construction, creating a test suite that feeds each possible 2-token context to the language model and verifies correct output tokens under greedy decoding
2. Test the approach with multiple language models having different context window sizes, tokenization schemes, and architectural designs, starting with smaller models where exhaustive testing is feasible
3. Execute complex Turing machine computations requiring thousands of iterations to verify that the autoregressive decoding process maintains consistency over extended runs, monitoring for context window misalignment and rule application errors