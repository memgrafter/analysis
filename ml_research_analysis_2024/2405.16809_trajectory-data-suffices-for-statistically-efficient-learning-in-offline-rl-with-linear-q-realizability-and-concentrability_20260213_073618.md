---
ver: rpa2
title: "Trajectory Data Suffices for Statistically Efficient Learning in Offline RL\
  \ with Linear $q^\u03C0$-Realizability and Concentrability"
arxiv_id: '2405.16809'
source_url: https://arxiv.org/abs/2405.16809
tags:
- traj
- lemma
- holds
- such
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper resolves an open problem in offline reinforcement learning\
  \ (RL) under linear q\u03C0-realizability, showing that trajectory data (full sequences\
  \ of state-action-reward tuples) enables statistically efficient learning where\
  \ individual transitions do not. The key insight is that linearly q\u03C0-realizable\
  \ MDPs can be approximated by linear MDPs when \"skipping\" over low-range states."
---

# Trajectory Data Suffices for Statistically Efficient Learning in Offline RL with Linear $q^π$-Realizability and Concentrability

## Quick Facts
- **arXiv ID**: 2405.16809
- **Source URL**: https://arxiv.org/abs/2405.16809
- **Authors**: Volodymyr Tkachuk; Gellért Weisz; Csaba Szepesvári
- **Reference count**: 40
- **Primary result**: Trajectory data enables statistically efficient learning in offline RL under linear qπ-realizability, while individual transitions do not.

## Executive Summary
This paper resolves a fundamental open problem in offline reinforcement learning by showing that trajectory data (full state-action-reward sequences) enables statistically efficient learning under linear qπ-realizability, whereas individual transitions do not. The key insight is that linearly qπ-realizable MDPs can be approximated by linear MDPs when skipping over low-range states. With trajectory data, the learner can simulate these skipped paths and achieve polynomial sample complexity in the feature dimension, horizon, and concentrability coefficient - independent of the state space size. The paper provides a learner that outputs ε-optimal policies with high probability using poly(d,H,C_conc)/ε² samples.

## Method Summary
The learner works by considering all possible "skipping mechanisms" (parameterized by G ∈ G), computing confidence sets for q-value parameters at each stage, and selecting the mechanism that guarantees tight q-estimates while maximizing value from the initial state. For each candidate skipping mechanism G, the algorithm computes confidence sets ΘG,h for q-value parameters using least-squares targets derived from trajectory data. It then solves an optimization problem that filters out mechanisms leading to loose q-value estimates and selects the one maximizing initial state value. The final policy is greedy with respect to the selected parameters. The main technical challenge is ensuring the true skipping mechanism is included among those considered.

## Key Results
- Achieves poly(d,H,C_conc)/ε² sample complexity for learning ε-optimal policies under linear qπ-realizability with trajectory data
- Demonstrates that trajectory data is necessary and sufficient for statistically efficient learning in this setting
- Shows that linear qπ-realizability can be transformed into approximate linear MDP structure through skipping mechanisms
- Proves that concentrability, combined with trajectory data, prevents exponential blow-up in sample complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trajectory data enables skipping over low-range states, transforming a linearly qπ-realizable MDP into an approximately linear MDP.
- Mechanism: When states with low range (states where action choice has little impact on value) are skipped, the resulting MDP behaves like a linear MDP. With trajectory data, the learner can simulate these skipped paths by summing rewards along the skipped states.
- Core assumption: The MDP is (η,L2)-approximately linear qπ-realizable (Assumption 1) and has low-range states that can be skipped without significant value loss.
- Evidence anchors:
  - [abstract]: "The main tool that makes this result possible is due to Weisz et al. [2023], who demonstrate that linear MDPs can be used to approximate linearly qπ-realizable MDPs."
  - [section 4.1]: "The hope in this setting is that learning a good policy will be possible without requiring a sample size that scales with the number of states in the MDP."
  - [corpus]: Missing direct evidence - the neighbor papers don't explicitly discuss this skipping mechanism.
- Break condition: If the MDP has no low-range states, or if the feature space cannot represent the skipping mechanism effectively.

### Mechanism 2
- Claim: The learner can estimate parameters for all possible skipping mechanisms (guesses G ∈ G) using trajectory data.
- Mechanism: For each possible skipping mechanism G, the learner computes confidence sets ΘG,h for q-value parameters at each stage h. This is done by considering all possible parameter sequences in the Cartesian product of confidence sets for later stages, and finding least-squares predictors that guarantee tight q-value estimates.
- Core assumption: The learner has access to full trajectory data (Assumption 2) and concentrability holds (Assumption 3).
- Evidence anchors:
  - [section 4.3]: "Each full length trajectory traj j = (sj t,a j t,r j t)t∈ [H+1]j ∈ [n] can be used to create the following least-squares target..."
  - [section 4.4]: "Optimization Problem 1... considers all guesses for the possible values of ¯G."
  - [corpus]: Weak evidence - neighbor papers discuss trajectory data but not this specific parameter estimation approach.
- Break condition: If the confidence sets ΘG,h become too large (due to insufficient data or high concentrability), making q-value estimates too loose.

### Mechanism 3
- Claim: The optimization problem selects the skipping mechanism that balances tight q-value estimation with high policy value from the initial state.
- Mechanism: The optimization problem (Optimization Problem 1) filters out skipping mechanisms that lead to loose q-value estimates (via constraint Eq. 14) and then selects the mechanism that maximizes value from the initial state (via objective ¯vθ † 1(s1)).
- Core assumption: The true skipping mechanism (based on the true q-value parameters) is included among those considered and passes the tightness constraint.
- Evidence anchors:
  - [section 4.4]: "The reason we can do this is because for G = ¯G we can show that this condition passes (with high probability)."
  - [section 5]: "To bound (II) = vπ ⋆ ¯G (s1) − ¯vθ ′ 1(s1): This term can be bound by approximately zero due to Optimization Problem 1 being optimistic from the start state."
  - [corpus]: Missing evidence - neighbor papers don't discuss this specific optimization approach.
- Break condition: If the true skipping mechanism is rejected due to the tightness constraint, or if no skipping mechanism achieves both tight estimates and high value.

## Foundational Learning

- Concept: Linear qπ-realizability - the action-value function of every policy is linear with respect to a given d-dimensional feature function.
  - Why needed here: This is the core assumption that allows the MDP to be approximated by a linear MDP when skipping low-range states.
  - Quick check question: If qπ(s,a) = ⟨φ(s,a),ψh(π)⟩ for all policies π and states (s,a), what does this tell us about the structure of the MDP?

- Concept: Concentrability - bounds how much the state-action distribution of any policy can differ from the data distribution.
  - Why needed here: This ensures that the data covers the MDP well enough for learning, preventing policies from accessing regions with insufficient data coverage.
  - Quick check question: If concentrability coefficient Cconc = 1, what does this tell us about the relationship between the data distribution and all possible policy distributions?

- Concept: Trajectory data vs individual transitions - full sequences of state-action-reward tuples vs individual transitions.
  - Why needed here: Trajectory data enables the learner to simulate skipped paths by summing rewards along the skipped states, which is essential for transforming the MDP into an approximately linear MDP.
  - Quick check question: How would the learner's ability to estimate q-values differ if it only had individual transitions instead of full trajectories?

## Architecture Onboarding

- Component map: Data processor -> Parameter estimator -> Optimization solver -> Policy generator
- Critical path: Data → Parameter estimation → Optimization → Policy output
- Design tradeoffs:
  - Large G (many skipping mechanisms) vs computational efficiency
  - Tight confidence sets (small ΘG,h) vs high probability of including true parameters
  - Conservative estimates (higher β) vs tight q-value estimates
- Failure signatures:
  - Optimization Problem 1 has no feasible solution (tightness constraint too strict)
  - Selected policy π' performs poorly (skipping mechanism rejected true mechanism)
  - High sample complexity despite theoretical guarantees
- First 3 experiments:
  1. Verify parameter estimation works for a simple linear MDP with known skipping mechanism
  2. Test optimization problem selects the correct skipping mechanism on a small MDP
  3. Evaluate policy performance on a larger MDP with varying concentrability coefficients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the optimization problem at the heart of the learner be solved efficiently in polynomial time?
- Basis in paper: [explicit] The paper explicitly states "We are not aware of any computationally efficient implementation of Optimization Problem 1" and "it is left as an open problem whether computationally efficient learning is possible in the setting we considered."
- Why unresolved: The optimization problem involves considering all possible skipping mechanisms G ∈ G and finding parameters that satisfy multiple constraints, which appears computationally intractable with current methods.
- What evidence would resolve it: A polynomial-time algorithm that solves Optimization Problem 1, or a proof that no such algorithm exists under standard complexity assumptions.

### Open Question 2
- Question: Is the statistical rate of Theorem 1 optimal, or can it be improved?
- Basis in paper: [explicit] "Another limitation of our work originates from our setting underpinning our result... we are not sure if our statistical rate in Theorem 1 is optimal. Showing a matching lower bound or improving the rate is left for future work."
- Why unresolved: The paper establishes a sample complexity bound but does not provide matching lower bounds, leaving open the possibility of tighter rates.
- What evidence would resolve it: A lower bound matching the upper bound in Theorem 1, or an improved upper bound with better sample complexity.

### Open Question 3
- Question: Does the learner's performance degrade under weaker assumptions such as only linear qπ-realizability for the target policy or only feature space coverage?
- Basis in paper: [inferred] The paper notes that "under many variations of weaker assumptions (for instance: general data, or linear qπ-realizability of only one policy, or only coverage of the feature space), polynomial statistical rates have been shown to be impossible to achieve by any learner."
- Why unresolved: The paper's results rely on strong assumptions (linear qπ-realizability for all policies, trajectory data, concentrability), but doesn't explore the boundaries of these requirements.
- What evidence would resolve it: A modified learner that works under weaker assumptions with polynomial sample complexity, or a lower bound showing impossibility under those weaker assumptions.

## Limitations
- Computational efficiency remains an open question - the optimization problem may be intractable for large MDPs
- Theoretical analysis assumes exact linear qπ-realizability and concentrability, which may not hold in practice
- The approximation quality depends on problem-specific properties not fully characterized in the paper

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Main sample complexity bound (Theorem 1) and core insight about trajectory data | High |
| Approximation argument transforming qπ-realizable MDPs to linear MDPs | Medium |
| Practical implementation details and computational feasibility | Low |

## Next Checks
1. Implement Optimization Problem 1 on a small MDP with known linear qπ-realizability structure to verify the algorithm can identify the correct skipping mechanism and produce reasonable policies.
2. Conduct experiments varying the concentrability coefficient to empirically validate the stated sample complexity dependence on C_conc.
3. Test the algorithm's sensitivity to violations of linear qπ-realizability by introducing controlled amounts of model misspecification and measuring performance degradation.