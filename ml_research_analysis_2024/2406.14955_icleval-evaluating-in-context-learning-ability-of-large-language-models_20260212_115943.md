---
ver: rpa2
title: 'ICLEval: Evaluating In-Context Learning Ability of Large Language Models'
arxiv_id: '2406.14955'
source_url: https://arxiv.org/abs/2406.14955
tags:
- uni00000013
- uni00000025
- output
- input
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ICLEval introduces a systematic benchmark for evaluating in-context
  learning (ICL) ability in LLMs by decomposing it into exact copying and rule learning
  sub-abilities. The benchmark contains 12 tasks with 2,040 samples across unstructured
  and structured contexts, testing copying, format rules, order rules, statistics
  rules, and list mapping.
---

# ICLEval: Evaluating In-Context Learning Ability of Large Language Models

## Quick Facts
- arXiv ID: 2406.14955
- Source URL: https://arxiv.org/abs/2406.14955
- Authors: Wentong Chen; Yankai Lin; ZhenHao Zhou; HongYun Huang; Yantao Jia; Zhao Cao; Ji-Rong Wen
- Reference count: 40
- Key outcome: Introduces a systematic benchmark for evaluating in-context learning (ICL) ability in LLMs by decomposing it into exact copying and rule learning sub-abilities.

## Executive Summary
ICLEval introduces a systematic benchmark for evaluating in-context learning (ICL) ability in LLMs by decomposing it into exact copying and rule learning sub-abilities. The benchmark contains 12 tasks with 2,040 samples across unstructured and structured contexts, testing copying, format rules, order rules, statistics rules, and list mapping. Experiments with models from 1.1B to 70B parameters reveal that ICL ability is widespread but not solely determined by model size; smaller models can match larger ones. Surprisingly, ICL ability—especially copying—develops rapidly in early pretraining stages and stabilizes, while rule learning improves more slowly. Analysis of bad cases identifies four key factors influencing ICL: distinguishing ability, inherent preferences, attention points capacity, and tokenizer effects.

## Method Summary
ICLEval evaluates ICL ability through a benchmark with 12 tasks and 2,040 samples covering unstructured and structured contexts. Tasks include copying (string completion, dictionary search), format rules (format check, format cloning, format conversion), order rules (order check, order adjustment), statistics rules (duplication check, de-duplication, count & navigation, relation analysis), and list mapping (numbers' list mapping). The evaluation uses exact match scores, with format cloning focusing only on format correctness. Models are tested using n-shot examples with greedy decoding, without additional prompts or sampling. Pretraining dynamics are analyzed by evaluating checkpoints at different training stages to track ICL ability development.

## Key Results
- ICL ability is widespread across model sizes, with smaller models sometimes matching larger ones
- ICL abilities, particularly copying, develop early in pretraining (around 200B tokens) and stabilize
- Four key factors influence ICL performance: distinguishing ability, inherent preferences, attention points capacity, and tokenizer effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL ability emerges early in pretraining and stabilizes after ~200B tokens.
- Mechanism: The model quickly acquires the capacity to match prefixes and copy content, which is foundational to ICL. This early stabilization contrasts with knowledge acquisition, which continues to grow.
- Core assumption: Copying and prefix matching are simpler learning tasks than acquiring factual knowledge or handling complex reasoning.
- Evidence anchors:
  - [abstract] "ICL abilities, particularly copying, develop early in the pretraining process and stabilize afterward."
  - [section 3.3] "The abilities of ICL exhibit rapid growth in the initial stage, before about 200B tokens. However, after this point, their growth becomes slow and eventually stops."
- Break condition: If pretraining corpus lacks repetitive patterns or prefix-matching examples, early ICL emergence may not occur.

### Mechanism 2
- Claim: Model size is not the sole determinant of ICL ability; smaller models can match larger ones.
- Mechanism: ICL depends on pattern recognition and copying ability, which are not strictly proportional to parameter count. Some smaller models may have favorable inductive biases or training data distributions.
- Core assumption: Pattern copying and rule learning can be learned with fewer parameters if the model architecture and data support it.
- Evidence anchors:
  - [abstract] "model size is not the sole determinant of ICL efficacy. Surprisingly, we observe that ICL abilities, particularly copying, develop early in the pretraining process and stabilize afterward."
  - [section 3.2] "larger models often exhibit stronger ICL ability, but some smaller models can also be compared with the larger ones."
- Break condition: If ICL ability strongly correlates with attention capacity or context length, larger models may dominate.

### Mechanism 3
- Claim: Four key factors influence ICL: distinguishing ability, inherent preferences, attention points capacity, and tokenizer effects.
- Mechanism: Distinguishing ability affects how well models handle similar strings. Inherent preferences bias model outputs toward certain formats. Attention points capacity limits how many context tokens can be effectively used. Tokenizer effects cause mismatches between human and model interpretations of text.
- Core assumption: These four factors are independent and collectively explain observed ICL performance variance.
- Evidence anchors:
  - [abstract] "we analyze some bad cases and discover that the ICL abilities are influenced by distinguishing ability, inherent preferences, attention points capacity, and tokenizer."
  - [section 3.4] Discusses each factor with examples and bad cases.
- Break condition: If models are trained with robust tokenization or if context length is not a bottleneck, these factors may not dominate.

## Foundational Learning

- Concept: Pattern matching and prefix copying.
  - Why needed here: ICL fundamentally relies on recognizing patterns in context and copying or transforming content accordingly.
  - Quick check question: Can the model accurately predict the continuation of a repeated string or pattern in context?

- Concept: Rule extraction from examples.
  - Why needed here: Beyond copying, ICL requires learning abstract rules from in-context examples to apply to new inputs.
  - Quick check question: Given several examples of format conversion, can the model generalize to convert a new example correctly?

- Concept: Token-level vs character-level processing.
  - Why needed here: Models process text at the token level, which can differ from human perception, especially for short strings or symbols.
  - Quick check question: Does the model handle strings with unusual tokenizations (e.g., short hashes) accurately?

## Architecture Onboarding

- Component map: Tokenizer → Embedding → Transformer layers → Attention heads → Output head. ICL ability depends on attention heads for pattern matching and copying.
- Critical path: Input tokenization → Context encoding → Attention-based pattern matching → Output generation. Failures often occur at tokenization or attention steps.
- Design tradeoffs: Larger models have more attention capacity but may develop stronger inherent preferences. Smaller models may be more flexible but limited in context handling.
- Failure signatures: Poor copying accuracy, format prediction bias, degraded performance with many similar context items, sensitivity to tokenization.
- First 3 experiments:
  1. Test string completion accuracy on simple repeated patterns across different model sizes.
  2. Evaluate format prediction accuracy with varied in-context examples to measure inherent preferences.
  3. Measure performance drop when increasing the number of similar keys in dictionary search to assess distinguishing ability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the rapid development of ICL ability in early pretraining stages depend on specific pretraining objectives or data distributions?
- Basis in paper: [explicit] The paper observes that ICL abilities, particularly copying, develop early in pretraining and stabilize afterward, but the reasons for this phenomenon are not fully explained.
- Why unresolved: The paper does not explore whether this pattern is influenced by the type of pretraining data or the objectives used during training.
- What evidence would resolve it: Experiments varying pretraining data distributions or objectives (e.g., masked language modeling vs. causal language modeling) to see if they affect the timing and stability of ICL ability development.

### Open Question 2
- Question: How does the "attention points capacity" limitation observed in count & navigation tasks generalize to other complex ICL tasks?
- Basis in paper: [explicit] The paper identifies that models struggle with tasks requiring attention to multiple elements, suggesting a capacity limitation.
- Why unresolved: The paper only tests this hypothesis in count & navigation tasks and does not investigate whether it applies to other ICL tasks like list mapping or relation analysis.
- What evidence would resolve it: Testing the same hypothesis across a broader range of ICL tasks with varying numbers of elements or dependencies to confirm if attention capacity is a universal bottleneck.

### Open Question 3
- Question: What role does tokenizer behavior play in the performance differences observed in structured vs. unstructured copying tasks?
- Basis in paper: [explicit] The paper notes that tokenizer effects significantly impact copying ability, especially in structured contexts.
- Why unresolved: The paper does not provide a detailed analysis of how specific tokenizer behaviors (e.g., tokenization of similar keys) affect model performance.
- What evidence would resolve it: Systematic experiments comparing different tokenizers or analyzing how tokenization patterns correlate with performance in structured copying tasks.

## Limitations

- Benchmark may not fully capture real-world ICL complexity due to controlled synthetic nature
- Pretraining dynamics analysis relies on checkpoint evaluation rather than longitudinal tracking
- Limited range of model families tested may not represent full landscape of ICL capabilities

## Confidence

- **High Confidence:** Benchmark construction methodology and ICL decomposition is methodologically sound
- **Medium Confidence:** Model size not being sole determinant requires further validation across broader model landscape
- **Medium Confidence:** Four-factor analysis based on qualitative failure case examination may not be comprehensive

## Next Checks

1. **Cross-Domain Transfer:** Validate whether observed ICL abilities generalize beyond synthetic benchmark by testing on real-world datasets requiring in-context learning, such as few-shot code completion or text-to-SQL tasks.

2. **Pretraining Dynamics Verification:** Conduct longitudinal studies tracking individual models throughout pretraining to confirm whether ICL abilities truly stabilize after 200B tokens, controlling for potential confounding factors like architecture changes or learning rate schedules.

3. **Factor Isolation Experiments:** Design controlled experiments to isolate impact of each proposed factor (distinguishing ability, inherent preferences, attention capacity, tokenization) on ICL performance, such as ablating attention mechanisms or testing with different tokenization strategies.