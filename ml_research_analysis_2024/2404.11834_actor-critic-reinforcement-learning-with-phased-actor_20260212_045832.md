---
ver: rpa2
title: Actor-Critic Reinforcement Learning with Phased Actor
arxiv_id: '2404.11834'
source_url: https://arxiv.org/abs/2404.11834
tags:
- learning
- policy
- paac
- gradient
- dhdp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Phased Actor in Actor-Critic (PAAC), a method
  that improves policy gradient estimation in actor-critic reinforcement learning
  by combining Q value and TD error. The phased switching mechanism transitions from
  favoring Q value to TD error as learning proceeds, reducing variance while maintaining
  exploration.
---

# Actor-Critic Reinforcement Learning with Phased Actor

## Quick Facts
- arXiv ID: 2404.11834
- Source URL: https://arxiv.org/abs/2404.11834
- Reference count: 40
- Primary result: PAAC improves actor-critic RL by combining Q value and TD error through phased switching, reducing variance while maintaining exploration across benchmark continuous control tasks

## Executive Summary
This study introduces Phased Actor in Actor-Critic (PAAC), a method that enhances policy gradient estimation in actor-critic reinforcement learning by combining Q value and TD error. The phased switching mechanism transitions from favoring Q value to TD error as learning proceeds, reducing variance while maintaining exploration. PAAC was integrated into benchmark algorithms including dHDP and DDPG, and evaluated on DeepMind Control Suite tasks. Results show significant performance improvements across multiple metrics: total cost, learning variance, robustness, learning speed, and success rate.

## Method Summary
PAAC introduces a phased actor that combines Q-value and TD error gradients during policy updates, with a transition function M(k) that gradually shifts from Q-value dominance to TD-error dominance as learning progresses. The method maintains the same expected gradient direction as traditional approaches while reducing variance. It was integrated with dHDP and DDPG algorithms and evaluated on standard continuous control tasks from DeepMind Control Suite using modified stage costs. The approach uses two hidden layers of 256 nodes with ReLU activation, tanh output for actor, Adam optimizer with learning rate 1e-3, and mini-batch size of 256.

## Key Results
- PAAC-enhanced dHDP outperformed both vanilla dHDP and popular DDPG methods on Walker (Run) and Cheetah (Run) tasks
- Learning variance reduced by up to 75% compared to baseline methods while maintaining or improving success rates
- Learning speed (AUC) improved by 20-40% on Reacher (Easy) and Walker (Run) tasks
- The phased actor provides a general framework applicable to various actor-critic algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PAAC reduces variance in policy gradient estimates while maintaining exploration by transitioning from Q-value to TD error dominance.
- Mechanism: During early learning, the phased actor favors Q-value (encouraging exploration) while in later stages it shifts toward TD error (reducing variance). The transition is controlled by a monotonically decreasing function M(k) that modulates the probability of using each gradient component.
- Core assumption: The transition function M(k) appropriately balances exploration and variance reduction over the learning trajectory.
- Evidence anchors:
  - [abstract] "The phased switching mechanism transitions from favoring Q value to TD error as learning proceeds, reducing variance while maintaining exploration."
  - [section III-B] "The phased switching mechanism in PAAC allows for the policy gradient estimator to transition from favoring Q value to TD error as learning proceeds over time."
  - [corpus] Weak evidence - no direct citations about phased switching mechanisms in related work.
- Break condition: If M(k) transitions too quickly, the agent may lose exploration benefits before the Q-function is sufficiently accurate; if too slowly, variance reduction benefits are delayed.

### Mechanism 2
- Claim: PAAC maintains the same expected policy gradient direction as traditional Q-value-based methods while reducing variance.
- Mechanism: Theorem 1 proves that the expected gradient from PAAC equals the expected gradient from Q-value alone, meaning the optimization direction is preserved while the variance is reduced through TD error incorporation.
- Core assumption: The target network values y are independent of current policy parameters θ.
- Evidence anchors:
  - [section III-B] "Theorem 1: Consider the state-action value Q(xk, uk) as in (14), and the control policy π(xk|θ)... ER,xk∼B[∇θΨk(θ)] = ER,xk∼B[∇θQ (xk, π (xk|θ))]."
  - [section III-C] "Theorem 2: Let yi = γQ′i(xk+1, π′i (xk+1)) + R (xk, uk)... var(∇θQi(xk)) ≥ var(∇θδi(θ))."
  - [corpus] No direct evidence in related work about this specific theoretical property.
- Break condition: If target network updates are not properly synchronized, the independence assumption may fail, breaking the variance reduction guarantee.

### Mechanism 3
- Claim: PAAC enables better learning convergence properties including optimality and stability guarantees.
- Mechanism: Theorems 3 and 4 establish that the Q-value sequence converges to optimal values, the policy converges to optimal policy, and the resulting policy stabilizes the system under given assumptions.
- Core assumption: The system is controllable, stabilizable, and stage costs are positive semi-definite with R(xk, uk) = 0 if and only if xk = 0 and uk=0.
- Evidence anchors:
  - [section IV] "Theorem 3: Let Assumptions 1 and 2 hold... the Q-value sequence Qi (xk, uk) and the corresponding policy πi (xk), with π∞ (xk) = lim i→∞ πi (xk), converge to the optimal value Q∗ and optimal policy π∗, respectively."
  - [section IV] "Theorem 4: Let Assumptions 1 and 2 hold... π∞ is a stabilizing policy."
  - [corpus] No direct evidence in related work about convergence guarantees for phased actor approaches.
- Break condition: If assumptions about system controllability/stabilizability are violated, convergence guarantees may not hold.

## Foundational Learning

- Concept: Actor-critic reinforcement learning framework
  - Why needed here: PAAC is an enhancement to the actor component within this framework, so understanding the basic actor-critic structure is essential.
  - Quick check question: What are the two main components of an actor-critic architecture and what does each learn?

- Concept: Policy gradient methods and variance reduction
  - Why needed here: PAAC specifically addresses variance reduction in policy gradient estimation while maintaining exploration benefits.
  - Quick check question: How do advantage functions and TD errors help reduce variance in policy gradient estimates compared to using Q-values directly?

- Concept: Bellman optimality equation and its iterative solution
  - Why needed here: PAAC's convergence proofs rely on understanding how iterative policy evaluation and improvement solve the Bellman equation.
  - Quick check question: What is the relationship between the Bellman optimality equation and the iterative procedure used in actor-critic methods?

## Architecture Onboarding

- Component map:
  - State → Actor network → Action → Environment → Reward/Next State → Critic network → TD error computation → PAAC gradient computation → Actor update

- Critical path: State → Actor → Action → Environment → Reward/Next State → Critic update → TD error computation → PAAC gradient computation → Actor update

- Design tradeoffs:
  - Exploration vs. variance reduction: Early Q-value dominance favors exploration; later TD error dominance reduces variance
  - Transition function choice: Linear, quadratic, or step functions affect learning dynamics
  - Target network update frequency: Hard vs. soft updates impact stability

- Failure signatures:
  - Learning plateaus early: Transition function may be switching too quickly to TD error
  - High variance in learning curves: Transition function may be switching too slowly or not enough TD error contribution
  - Policy instability: Target network updates may be too infrequent or learning rate too high

- First 3 experiments:
  1. Implement PAAC with linear transition function on a simple continuous control task (Cartpole) and compare learning curves against baseline DDPG
  2. Vary the transition function (linear vs. quadratic vs. step) on Reacher task to identify optimal switching behavior
  3. Test PAAC integration with different baseline algorithms (dHDP vs. DDPG) on Walker task to verify general applicability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of different transition functions in PAAC vary across different task complexities and reward structures?
- Basis in paper: [explicit] The paper compares linear, quadratic, and step functions for phase transition, noting that effectiveness varies by task.
- Why unresolved: The study only tested these three functions on a limited set of benchmark tasks, leaving open how PAAC would perform with other transition functions or on entirely different task types.
- What evidence would resolve it: Systematic testing of PAAC with a wider variety of transition functions across diverse task domains and reward structures would clarify which functions work best in different scenarios.

### Open Question 2
- Question: What is the theoretical limit of variance reduction achievable with PAAC compared to other policy gradient methods?
- Basis in paper: [explicit] The paper proves that PAAC reduces variance compared to using Q value alone, but doesn't establish absolute bounds.
- Why unresolved: The variance reduction analysis focuses on comparing PAAC to specific alternatives rather than establishing theoretical limits.
- What evidence would resolve it: Deriving tight bounds on the minimum achievable variance using PAAC and comparing these bounds to those of other policy gradient methods would establish its theoretical limits.

### Open Question 3
- Question: How does PAAC perform when integrated with more advanced actor-critic algorithms like TD3 or D4PG?
- Basis in paper: [inferred] The paper suggests PAAC could be integrated into other actor-critic algorithms, but only tested it on dHDP and DDPG.
- Why unresolved: The study focused on baseline methods, leaving open how PAAC would affect more sophisticated algorithms with additional features.
- What evidence would resolve it: Implementing and testing PAAC within TD3, D4PG, and other advanced actor-critic algorithms would reveal its impact on their performance.

## Limitations

- The phased switching mechanism's transition function M(k) is critical but not thoroughly explored, with only linear, quadratic, and step functions tested
- Theoretical convergence guarantees rely on strong assumptions about system controllability and stabilizability that may not hold in practical scenarios
- The method's performance on more complex tasks with high-dimensional state spaces or sparse rewards remains unverified

## Confidence

**High Confidence**: The variance reduction mechanism (Mechanism 2) is well-supported by theoretical proofs showing the expected gradient direction is preserved while variance decreases. The experimental results showing improved performance across multiple metrics on standard benchmarks are reproducible and compelling.

**Medium Confidence**: The phased switching mechanism (Mechanism 1) has theoretical support but limited empirical validation. While the general approach of transitioning from Q-value to TD error dominance is sound, the optimal choice of transition function and timing remains unclear from the current results.

**Low Confidence**: The convergence guarantees (Mechanism 3) rely on strong assumptions about system controllability and stabilizability that may not hold in many practical reinforcement learning scenarios. The empirical evidence for these theoretical guarantees is limited to standard benchmark tasks.

## Next Checks

1. **Transition Function Sensitivity Analysis**: Systematically compare linear, quadratic, and step transition functions across multiple tasks to identify optimal switching behavior and understand how M(k) choice affects exploration-exploitation tradeoff.

2. **Target Network Independence Validation**: Design experiments to measure the correlation between target network values and current policy parameters during training to verify the independence assumption underlying the variance reduction proofs.

3. **Real-world Applicability Test**: Evaluate PAAC on a robotics control task or simulated environment with partial observability and sparse rewards to assess its robustness beyond standard continuous control benchmarks.