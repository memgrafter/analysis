---
ver: rpa2
title: Uncovering Latent Chain of Thought Vectors in Language Models
arxiv_id: '2409.14026'
source_url: https://arxiv.org/abs/2409.14026
tags:
- steering
- language
- injection
- reasoning
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether activation space perturbations can
  encode Chain-of-Thought (CoT) reasoning patterns in Language Models (LMs). The authors
  derive steering vectors by contrasting LM activations from CoT and direct-answer
  prompts on a corpus of reasoning questions.
---

# Uncovering Latent Chain of Thought Vectors in Language Models

## Quick Facts
- arXiv ID: 2409.14026
- Source URL: https://arxiv.org/abs/2409.14026
- Authors: Jason Zhang; Scott Viteri
- Reference count: 8
- Primary result: Activation-space steering vectors achieve up to 1.74 percentage points improvement over CoT prompting on reasoning benchmarks

## Executive Summary
This work demonstrates that Chain-of-Thought (CoT) reasoning patterns can be encoded as activation space perturbations and injected into Language Models (LMs) at inference time without natural language prompting. The authors derive steering vectors by contrasting LM activations from CoT and direct-answer prompts on reasoning questions, then inject these vectors into Llama3 8B Instruct and Mistral 7B v0.2 Instruct. Results show competitive or superior performance compared to traditional CoT prompting on multiple benchmarks including GSM8k, MMLU, ARC AI2, and AGI Eval, with the approach generalizing to unseen datasets and qualitatively inducing more structured reasoning outputs.

## Method Summary
The method extracts layer activations from a reference model when processing contrasting prompts (CoT vs direct-answer) on reasoning questions, then computes steering vectors by averaging activations across token positions and questions and subtracting direct-answer from CoT activations. These vectors are injected into target models at inference time using PyTorch hooks, with either single injection (once during input processing) or continuous injection (at every token generation step). The approach uses varying layer indices and coefficients for different models and injection strategies, determined through empirical grid search.

## Key Results
- Activation-space steering achieves up to 1.74 percentage points improvement over CoT prompting on reasoning benchmarks
- Strong performance on AGI-Eval's SAT Math section, despite not being present in the steering vector training data
- Generalizes to unseen datasets and qualitatively induces more structured reasoning outputs
- Continuous injection yields slightly higher performance on average compared to single injection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Steering vectors capture the difference in activation patterns between CoT and direct-answer prompting, encoding reasoning behavior in the residual stream.
- Mechanism: The method subtracts average activations from CoT vs direct-answer prompts on reasoning questions, creating a vector that represents "reasoning mode." Injecting this vector shifts the model's activations toward the CoT distribution.
- Core assumption: The activation difference between CoT and direct-answer processing captures generalizable reasoning patterns rather than dataset-specific artifacts.
- Evidence anchors: [abstract] "These findings suggest that neural network activations can encode reasoning patterns"; [section] "v(q) = Et [a(l)t(qCoT)] − Et [a(l)t(qdirect)]" shows the explicit derivation of steering vectors from activation differences.

### Mechanism 2
- Claim: Continuous injection with lower coefficients avoids over-steering while maintaining reasoning behavior throughout generation.
- Mechanism: The steering vector is repeatedly injected at each token step with coefficient c=0.5 (Mistral) or c=1 (Llama3), providing ongoing guidance without overwhelming the original activations.
- Core assumption: Small, repeated perturbations are more effective than large single perturbations for maintaining behavioral changes during generation.
- Evidence anchors: [section] "For example, with Llama3 8B Instruct we use (l = 16, c = 20) for single injection and (l = 13, c = 1) for continuous injection"; [section] "The lower coefficients for continuous injection reflect the need to avoid over-steering during repeated applications".

### Mechanism 3
- Claim: Steering vectors generalize to unseen datasets and reasoning tasks beyond the training corpus.
- Mechanism: The vectors are derived from diverse reasoning benchmarks (Big Bench Lite, MMLU, GSM8k) and show performance improvements on AGI Eval's SAT Math section, which wasn't in the training distribution.
- Core assumption: Reasoning patterns are universal enough to transfer across different problem types and domains.
- Evidence anchors: [abstract] "Moreover, the approach demonstrates strong performance on AGI-Eval's SAT Math section, a benchmark whose data was not present in the steering vector data distribution"; [section] Results table shows consistent performance improvements across multiple benchmarks.

## Foundational Learning

- Concept: Activation space manipulation
  - Why needed here: Understanding how modifying neural activations can change model behavior without retraining
  - Quick check question: What is the difference between modifying activations versus modifying weights in a neural network?

- Concept: Residual stream in transformers
  - Why needed here: Steering vectors are injected into the residual stream, so understanding this architecture is crucial
  - Quick check question: Where in the transformer architecture are steering vectors injected, and why this location?

- Concept: Contrastive learning
  - Why needed here: The steering vectors are derived by contrasting CoT vs direct-answer activations
  - Quick check question: How does the contrastive approach help isolate reasoning-specific activation patterns?

## Architecture Onboarding

- Component map: Question → Activation extraction → Vector computation → Injection → Generation → Evaluation
- Critical path: The method flows from preparing contrasting prompts, extracting layer activations, computing steering vectors, injecting them via PyTorch hooks, and evaluating reasoning performance.
- Design tradeoffs: Single injection vs continuous injection (timing), coefficient magnitude (strength of steering), layer selection (which layer to modify)
- Failure signatures: Degraded performance on reasoning tasks, generation instability, failure to generalize to new datasets
- First 3 experiments:
  1. Verify activation extraction works by comparing CoT vs direct-answer activations on a small dataset
  2. Test single injection on a simple benchmark (GSM8k) with varying coefficients
  3. Compare single vs continuous injection strategies on the same benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of steering vectors generalize to larger language models beyond Llama3 8B and Mistral 7B v0.2 Instruct?
- Basis in paper: [inferred] The paper tests the approach on Llama3 8B and Mistral 7B v0.2 Instruct, suggesting potential for larger models but not testing them directly.
- Why unresolved: The study is limited to specific model sizes, and there is no evidence provided about the effectiveness of the approach on larger models.
- What evidence would resolve it: Testing the steering vector approach on larger models like Llama3 70B or GPT-4 would provide evidence of its scalability and effectiveness.

### Open Question 2
- Question: What are the long-term effects of continuous vs. single injection strategies on model performance and reasoning capabilities?
- Basis in paper: [explicit] The paper notes that continuous injection yields slightly higher performance on average but does not explore long-term effects.
- Why unresolved: The study focuses on immediate performance impacts without investigating how these strategies affect model behavior over extended use.
- What evidence would resolve it: Conducting longitudinal studies to assess the impact of each strategy on model reasoning and performance over time would provide insights into their long-term effects.

### Open Question 3
- Question: How do steering vectors affect the interpretability and transparency of model reasoning processes?
- Basis in paper: [inferred] The paper discusses the qualitative steering towards structured reasoning but does not address interpretability.
- Why unresolved: The focus is on performance metrics rather than understanding how steering vectors influence the transparency of reasoning.
- What evidence would resolve it: Analyzing the interpretability of model outputs with and without steering vectors would clarify their impact on transparency.

### Open Question 4
- Question: Can steering vectors be adapted to induce other complex reasoning patterns beyond Chain-of-Thought?
- Basis in paper: [inferred] The study is specific to CoT reasoning, suggesting potential for other patterns but not exploring them.
- Why unresolved: The research is limited to CoT reasoning, and there is no exploration of other reasoning patterns.
- What evidence would resolve it: Experimenting with steering vectors to induce different reasoning patterns, such as analogical reasoning or creative problem-solving, would demonstrate their versatility.

## Limitations

- The paper demonstrates steering vector effectiveness but doesn't fully explain why activation differences capture reasoning patterns versus other factors like lexical or formatting differences
- Empirical nature of hyperparameter selection (layer indices and coefficients) limits theoretical understanding of optimal configurations
- Unclear how the approach would perform on tasks requiring different cognitive processes beyond mathematical and logical reasoning

## Confidence

- **High confidence**: The core empirical finding that activation-space steering can improve reasoning performance compared to traditional CoT prompting (supported by multiple benchmark results)
- **Medium confidence**: The claim that steering vectors generalize to unseen datasets (supported by AGI Eval results but limited test distribution)
- **Medium confidence**: The mechanism that contrastive activation differences capture reasoning patterns (plausible but not directly validated through ablation studies)

## Next Checks

1. Conduct ablation studies comparing steering vectors derived from different layer depths to identify which layers contain the most reasoning-relevant information
2. Test steering vector transfer to non-mathematical reasoning tasks (e.g., commonsense reasoning, multi-step inference) to validate generality claims
3. Implement a controlled experiment comparing steering vectors from lexically-similar but reasoning-different prompts to isolate whether the vectors capture reasoning versus surface features