---
ver: rpa2
title: Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory
arxiv_id: '2405.19024'
source_url: https://arxiv.org/abs/2405.19024
tags:
- learning
- reward
- inverse
- problem
- curl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first theoretical framework for inverse
  reinforcement learning (IRL) in concave utility reinforcement learning (CURL) problems.
  CURL generalizes standard RL by using concave functions of state occupancy measures
  instead of linear reward expectations, enabling representation of various important
  applications like imitation learning, constrained MDPs, and human-regularized RL.
---

# Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory

## Quick Facts
- arXiv ID: 2405.19024
- Source URL: https://arxiv.org/abs/2405.19024
- Authors: Mustafa Mert Çelikok; Frans A. Oliehoek; Jan-Willem van de Meent
- Reference count: 40
- This paper introduces the first theoretical framework for inverse reinforcement learning (IRL) in concave utility reinforcement learning (CURL) problems, proving that standard IRL methods fail for CURL because classical Bellman equations do not hold.

## Executive Summary
This paper establishes that inverse reinforcement learning for concave utility reinforcement learning (I-CURL) problems cannot be solved using standard IRL methods because CURL invalidates classical Bellman equations and optimal policies cannot be induced by stationary reward functions. The authors prove that CURL problems are equivalent to a subclass of mean-field games (MFGs), allowing them to formulate I-CURL as an inverse game theory problem. They define a feasible reward set based on exploitability conditions in the equivalent MFG and present a saddle-point formulation that reduces the problem to a convex-concave game, enabling gradient-based solutions.

## Method Summary
The method formulates I-CURL as an inverse game theory problem by establishing the equivalence between CURL and mean-field games. The key insight is that any CURL problem can be transformed into an MFG where the reward function depends on the population distribution via the gradient of the concave utility function. The feasible reward set for I-CURL is defined as all rewards that make the expert policy an equilibrium in the equivalent MFG, characterized by exploitability conditions. An equivalent saddle-point formulation transforms this into a convex-concave game, enabling gradient-based optimization to find reward functions that rationalize the observed optimal policy.

## Key Results
- Standard IRL methods fail for CURL because the classical Bellman equations do not hold and optimal CURL policies cannot be induced by stationary reward functions
- CURL problems are equivalent to a subclass of mean-field games, enabling inverse game theory formulation
- The feasible reward set for I-CURL is both necessary and sufficient for rationalizing optimal CURL policies, defined by exploitability conditions in the equivalent MFG

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CURL problems cannot be reduced to standard RL by redefining stationary rewards
- Mechanism: The concave utility function F introduces non-linearities in the objective that prevent Bellman equation satisfaction
- Core assumption: The optimal CURL policy requires non-stationary or non-linear reward representations
- Evidence anchors:
  - [abstract] "CURL invalidates the classical Bellman equations"
  - [section 2.1] "Lemma 4 There exists an MDP M with concave utility F such that there can be no stationary reward R ∈ RS×A with argmaxdπ∈Kγ ⟨dπ, R⟩ = argmaxdπ∈Kγ F (dπ)"
- Break condition: If F becomes linear or can be decomposed into linear components

### Mechanism 2
- Claim: CURL problems are equivalent to mean-field games, enabling inverse game theory formulation
- Mechanism: The equivalence mapping ∇F(dπ) creates a reward function R(s,a,d) that depends on population distribution, transforming CURL into a strategic game
- Core assumption: The mean-field assumption holds (single agent's policy changes don't affect overall distribution)
- Evidence anchors:
  - [abstract] "CURL problems are equivalent to a subclass of mean-field games (MFGs)"
  - [section 2.2] "Theorem 1 of Geist et al. [2] proves that every CURL problem is in fact such an MFG with the reward R(·, ·, d) = ∇F (d)"
- Break condition: If the mean-field assumption breaks down (large population effects)

### Mechanism 3
- Claim: Feasible reward set for I-CURL is defined by exploitability conditions in the equivalent MFG
- Mechanism: The set RB = {R ∈ RS×A×Kγ |ϕ(πE; R) = 0 } captures all rewards that make the expert policy an equilibrium, ensuring rationalization
- Core assumption: The exploitability condition correctly characterizes equilibrium behavior
- Evidence anchors:
  - [abstract] "We propose a new definition for the feasible rewards for I-CURL by proving that this problem is equivalent to an inverse game theory problem"
  - [section 3.1] "Proposition 8 (Feasible Reward Set for I-CURL) The set of feasible reward functions for the I-CURL problem formulated as the individual-level IGT in CURL-MFGs, B = ( ¯G, πE, dE), is defined as RB = {R ∈ RS×A×Kγ |ϕ(πE; R) = 0 }"
- Break condition: If the exploitability metric fails to capture all equilibrium properties

## Foundational Learning

- Concave Utility Reinforcement Learning
  - Why needed here: Understanding how CURL generalizes standard RL and why Bellman equations break
  - Quick check question: What property of concave utility functions prevents standard RL reduction?

- Mean-field Games
  - Why needed here: The equivalence between CURL and MFGs enables the inverse game theory formulation
  - Quick check question: How does the reward function R(s,a,d) in MFGs differ from standard RL rewards?

- Inverse Game Theory
  - Why needed here: Provides the framework for inferring reward functions from equilibrium behavior in strategic settings
  - Quick check question: What is the difference between population-level and individual-level IGT?

## Architecture Onboarding

- Component map:
  CURL-MFG solver -> Expert policy estimator -> Feasible reward set computation -> Gradient-based optimization

- Critical path:
  1. Estimate expert policy πE from demonstrations
  2. Compute ∇F(dπE) for the equivalent MFG
  3. Solve minθ maxd U(θ,d; dE) to find feasible rewards
  4. Validate that ϕ(πE; θ) ≈ 0

- Design tradeoffs:
  - Function class complexity vs. computational tractability
  - Sample complexity vs. estimation accuracy
  - Gradient-based vs. combinatorial search methods

- Failure signatures:
  - High exploitability values indicate infeasible reward functions
  - Poor policy estimation leads to incorrect reward inference
  - Non-convergence in the saddle-point optimization

- First 3 experiments:
  1. Validate the equivalence between CURL and MFGs on a simple gridworld
  2. Test the feasible reward set computation on known reward functions
  3. Evaluate the empirical I-CURL approach with varying dataset sizes

## Open Questions the Paper Calls Out
None

## Limitations
- The saddle-point optimization may not converge or may get stuck in local optima, especially with complex function classes for the reward function
- The empirical estimation of the expert policy πE and occupancy measure dE from finite data may lead to a significantly different feasible reward set R̂B compared to the true set RB
- The framework relies on the mean-field assumption, which may not hold in systems with significant population effects or non-stationarities

## Confidence
- Theoretical framework: High - The CURL-MFG equivalence and impossibility results are rigorously proven
- Saddle-point formulation: High - The convex-concave game reduction is mathematically sound
- Practical implementation: Medium - Gradient-based methods may face convergence and generalization challenges

## Next Checks
1. **Convergence analysis**: Empirically evaluate the saddle-point optimization convergence rates and sensitivity to learning rates across different function classes
2. **Robustness to estimation error**: Test the framework's performance when expert policies are estimated from finite, noisy demonstrations versus known policies
3. **Function class generalization**: Compare the recovered reward functions across increasing complexity of parameterized function classes (linear → neural networks) on benchmark CURL problems