---
ver: rpa2
title: Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language
  Models
arxiv_id: '2406.17115'
source_url: https://arxiv.org/abs/2406.17115
tags:
- hallucination
- evaluation
- image
- response
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Hallucination benchmark Quality Measurement
  framework (HQM) for Large Vision-Language Models (LVLMs), focusing on assessing
  both reliability and validity of existing hallucination benchmarks. The proposed
  framework uses indicators like test-retest reliability, parallel-forms reliability,
  criterion validity, and coverage of hallucination types to measure the quality of
  benchmarks.
---

# Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models

## Quick Facts
- arXiv ID: 2406.17115
- Source URL: https://arxiv.org/abs/2406.17115
- Reference count: 40
- One-line primary result: Introduces HQM framework to assess hallucination benchmark quality, constructs HQH benchmark, and demonstrates current LVLMs still have significant hallucination issues

## Executive Summary
This paper addresses the critical issue of evaluating the quality of hallucination benchmarks for Large Vision-Language Models (LVLMs). The authors propose a Hallucination benchmark Quality Measurement framework (HQM) that assesses both reliability and validity of existing benchmarks using psychometric principles. Based on this framework, they construct a High-Quality Hallucination Benchmark (HQH) that demonstrates superior performance compared to existing benchmarks when evaluated on over 10 representative LVLMs, including GPT-4o and Gemini-1.5-Pro. The results reveal that current LVLMs still exhibit significant hallucination issues, with GPT-4o having a hallucination rate of over 15% and more than half of the models exceeding 40%.

## Method Summary
The authors construct the HQM framework by adapting psychometric principles to evaluate hallucination benchmark quality through reliability (test-retest and parallel-forms) and validity (criterion and coverage) metrics. They systematically evaluate 7 existing hallucination benchmarks (POPE, AMBER, HallusionBench, OpenCHAIR, MMHal, GA VIE) using these metrics, identifying significant reliability issues particularly in closed-ended tasks susceptible to response bias. Based on these findings, they construct the HQH benchmark with improved reliability and validity by employing binary hallucination judgments based on detailed image information rather than scoring approaches. The HQH is then evaluated on 11 representative LVLMs, including GPT-4o and Gemini-1.5-Pro, demonstrating its effectiveness in measuring hallucination rates.

## Key Results
- HQH benchmark demonstrates superior reliability (test-retest correlation: 0.96) and validity compared to existing benchmarks
- Current LVLMs still exhibit significant hallucination issues, with GPT-4o having hallucination rate of 15.83% and more than half exceeding 40%
- Closed-ended benchmarks show response bias (acquiescence bias with 63.33% yes-ratio in MiniGPT4-LLaMa-2) affecting reliability
- Binary judgment approach based on detailed image information achieves better reliability than scoring-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Test-retest reliability suffers when hallucination scoring relies on external LLMs like GPT.
- Mechanism: GPT introduces randomness across repeated tests, causing inconsistent hallucination scores for similar model responses.
- Core assumption: Current LLMs lack the stability and consistency needed for reliable hallucination grading.
- Evidence anchors:
  - [abstract] "hallucination score, which leverages external LLMs like GPT [26] to assign a specific score to the hallucination level of model response... resulting in inconsistent scores across repeated or parallel tests"
  - [section 3.2] "GPT may produce inconsistent scores for similar model responses in repeated or parallel tests, negatively impacting reliability"
  - [corpus] Weak evidence - corpus contains related hallucination detection papers but none specifically addressing GPT scoring instability in repeated tests
- Break condition: When hallucination scoring shifts from probabilistic judgment to deterministic verification (e.g., binary classification based on detailed image information).

### Mechanism 2
- Claim: Closed-ended tasks introduce response bias (acquiescence/dissent/position bias) that affects reliability.
- Mechanism: Models tend to favor certain answer types regardless of visual content, making accuracy metrics unreliable indicators of hallucination.
- Core assumption: Models have inherent response biases toward yes/no or specific options that don't reflect true understanding.
- Evidence anchors:
  - [abstract] "benchmarks of closed-ended tasks offer efficient automated evaluation but exhibit certain deficiencies in reliability since LVLMs are susceptible to response bias [36] introduced by task settings"
  - [section 3.2] "MiniGPT4-LLaMa-2 and Otter suffer from significant acquiescence bias, i.e., the tendency to answer 'yes', with much higher yes-ratios than ground truth"
  - [corpus] Moderate evidence - corpus includes "TUBench" which benchmarks LVLMs on trustworthiness with unanswerable questions, suggesting bias issues exist
- Break condition: When task design eliminates forced-choice options or when models demonstrate consistent answer patterns aligned with visual evidence.

### Mechanism 3
- Claim: Binary hallucination judgment based on detailed image information improves both reliability and validity.
- Mechanism: Simplifying evaluation from scoring to binary classification reduces the complexity gap between LLMs and human evaluators.
- Core assumption: Detailed image information can be provided to LLMs to make accurate yes/no judgments about hallucination.
- Evidence anchors:
  - [abstract] "we employ a simplified process: given detailed image information, the model only needs to determine whether the response is hallucinated"
  - [section 4.2] "By extracting binary hallucination judgments from GPT's responses, we calculate the hallucination rate as the evaluation metric"
  - [corpus] Weak evidence - corpus lacks papers specifically addressing simplified binary judgment approaches for hallucination evaluation
- Break condition: When hallucination detection requires nuanced grading beyond binary classification or when detailed image information is insufficient for accurate judgment.

## Foundational Learning

- Concept: Psychometrics and reliability/validity measurement
  - Why needed here: The framework adapts psychometric principles to evaluate benchmark quality, requiring understanding of test reliability and validity concepts
  - Quick check question: What are the two main types of reliability measured in the HQM framework?
- Concept: Response bias in forced-choice tasks
  - Why needed here: Understanding how acquiescence, dissent, and position biases affect closed-ended benchmark reliability
  - Quick check question: Which type of bias occurs when models consistently answer "yes" regardless of content?
- Concept: Hallucination classification taxonomy
  - Why needed here: The framework evaluates coverage across 8 hallucination types, requiring understanding of different hallucination categories
  - Quick check question: What are the three most challenging hallucination types for LVLMs according to the results?

## Architecture Onboarding

- Component map: HQM framework → Reliability (test-retest, parallel-forms) → Validity (criterion, coverage) → HQH benchmark construction → Evaluation pipeline
- Critical path: Data collection → Manual review → Evaluation metric design → GPT-assisted binary judgment → Hallucination rate calculation
- Design tradeoffs: Open-ended vs closed-ended tasks (reliability vs validity), scoring vs binary judgment (complexity vs accuracy), GPT-3.5 vs GPT-4 (cost vs performance)
- Failure signatures: Inconsistent scores across repeated tests, significant response length variations under parallel tests, low correlation with human evaluation
- First 3 experiments:
  1. Test-retest reliability experiment: Run same model on same benchmark with different seeds and measure score correlation
  2. Parallel-forms reliability experiment: Create equivalent prompts and compare evaluation results
  3. Criterion validity experiment: Manual review of random samples and correlation with automated metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the HQM framework be extended to measure other dimensions of benchmark quality beyond reliability and validity?
- Basis in paper: Explicit - The authors state "Our proposed HQM framework represents an initial attempt to assess benchmark quality, focusing on reliability and validity, without yet considering other dimensions that may influence benchmark quality."
- Why unresolved: The paper acknowledges that other dimensions of benchmark quality exist but does not explore what these dimensions might be or how they could be measured.
- What evidence would resolve it: Development and testing of additional quality metrics that could complement reliability and validity in the HQM framework, along with empirical validation of their effectiveness.

### Open Question 2
- Question: What specific prompt engineering techniques could be employed to reduce response bias in closed-ended benchmarks?
- Basis in paper: Explicit - The authors discuss response bias issues in closed-ended benchmarks but do not provide specific solutions to mitigate this bias.
- Why unresolved: While the paper identifies the problem of response bias in closed-ended tasks, it does not propose concrete methods to address this issue through prompt engineering or other techniques.
- What evidence would resolve it: Empirical studies comparing different prompt engineering approaches and their effectiveness in reducing response bias, along with measurable improvements in benchmark reliability.

### Open Question 3
- Question: How can the hallucination scoring process be improved to achieve better alignment between LLM evaluations and human judgments?
- Basis in paper: Explicit - The authors state that "scoring inherently involves a degree of subjectivity, it is beyond the capabilities of current LLMs like GPT to consistently and accurately grade the degree of hallucination in responses to the same level as human evaluators."
- Why unresolved: The paper identifies a fundamental limitation in using LLMs for hallucination scoring but does not propose specific improvements to bridge the gap between LLM and human evaluations.
- What evidence would resolve it: Development of new hallucination scoring methodologies that better align with human judgment, along with comparative studies demonstrating improved consistency between LLM and human evaluations.

## Limitations
- The framework relies heavily on GPT-based hallucination scoring, which may introduce evaluation bias that is not fully characterized
- Limited validation against human experts for the final HQH benchmark, with human evaluation only mentioned for criterion validity measurement
- The binary judgment approach may oversimplify complex hallucination scenarios that require nuanced assessment

## Confidence
- High confidence: The framework's adaptation of psychometric principles to benchmark evaluation (test-retest and parallel-forms reliability)
- Medium confidence: The effectiveness of binary judgment approach for hallucination detection, given limited comparison with traditional scoring methods
- Medium confidence: The superiority of HQH over existing benchmarks, as results are primarily based on correlation metrics rather than comprehensive human validation

## Next Checks
1. Conduct a comprehensive human evaluation study comparing HQH results with expert annotations across all 8 hallucination types
2. Test the robustness of the binary judgment approach by introducing edge cases with partial hallucinations or ambiguous visual information
3. Evaluate whether the reliability improvements persist when using different external LLM graders (e.g., Claude or Llama) instead of GPT