---
ver: rpa2
title: 'A Theory of Response Sampling in LLMs: Part Descriptive and Part Prescriptive'
arxiv_id: '2402.11005'
source_url: https://arxiv.org/abs/2402.11005
tags:
- number
- value
- person
- ideal
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study identifies a value bias in LLM sampling: responses systematically
  deviate from the statistically likely value toward an "ideal" value implicit in
  the model''s training. This bias was measured by asking LLMs to report average,
  ideal, and sampled values for 36 everyday categories.'
---

# A Theory of Response Sampling in LLMs: Part Descriptive and Part Prescriptive

## Quick Facts
- arXiv ID: 2402.11005
- Source URL: https://arxiv.org/abs/2402.11005
- Authors: Sarath Sivaprasad; Pramod Kaushik; Sahar Abdelnabi; Mario Fritz
- Reference count: 40
- Primary result: LLMs exhibit systematic value bias in sampling, shifting responses toward "ideal" values rather than statistical averages

## Executive Summary
This study identifies a systematic value bias in large language model (LLM) sampling behavior, where responses systematically deviate from statistically likely values toward an "ideal" value implicit in the model's training. The authors demonstrate this bias across 36 everyday categories, showing that 24 out of 36 samples were biased toward the ideal value (p = 0.033). The research reveals that LLMs can learn and apply new value systems through in-context prompting without fine-tuning, and that this bias manifests in prototype judgments where exemplars are rated as more prototypical when they align with ideal values rather than statistical averages.

## Method Summary
The study employed GPT-4 to investigate value bias through three main experimental approaches: implicit bias evaluation across 36 everyday categories where the model was prompted to report average, ideal, and sampled values; in-context learning with a novel concept "glubbing" where explicit value grades were provided; and prototype evaluation where exemplars were rated on average, ideal, and prototypicality dimensions. Experiments were run across multiple iterations (10 for implicit bias, 100 for in-context learning) with temperature settings of 0.8 and 0.0 for comparison. The authors used statistical tests including Mann-Whitney U, binomial tests, and correlation analysis to assess significance and quantify the deviation from average toward ideal values using the α metric.

## Key Results
- 24 out of 36 samples were biased toward the ideal value (p = 0.033) when measuring deviation from average using the α metric
- In in-context learning, samples were significantly higher than the average when the value system was positive (p < .001), and lower when negative
- 39 of 46 exemplars were rated as more prototypical than their average value in the direction of the ideal (p < 0.001)
- Human and LLM value systems were not correlated (Pearson r = -0.02), suggesting misalignment in ethical judgment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit a systematic value bias in sampling that shifts responses from statistically likely values toward an internally encoded "ideal" value.
- Mechanism: The autoregressive sampling process, while not goal-driven, is influenced by learned distributional patterns in training data that encode value-laden associations. These patterns bias the model toward generating tokens that align with the "ideal" representation of a concept rather than its statistical average.
- Core assumption: The training corpus contains sufficient value-laden examples to establish implicit value systems for various concepts.
- Evidence anchors:
  - [abstract] "responses systematically deviate from the statistically likely value toward an 'ideal' value implicit in the model's training"
  - [section] "The autoregressive sampling process, while not goal-driven, is influenced by learned distributional patterns in training data that encode value-laden associations"
  - [corpus] Weak evidence - corpus contains related papers on prescriptive AI and decision-making, but no direct evidence of value bias in LLMs
- Break condition: If the training data lacks consistent value associations for a concept, the bias would not manifest or would be random.

### Mechanism 2
- Claim: LLMs can learn and apply new value systems through in-context prompting without fine-tuning.
- Mechanism: The model's attention mechanisms can dynamically adjust sampling based on the value system presented in the prompt, treating the grades as contextual information that influences token selection probability.
- Core assumption: The model's attention mechanisms are sensitive to value-related information provided in context.
- Evidence anchors:
  - [abstract] "the model learned a new concept ('glubbing') and, when given a distribution and explicit value grades, produced samples significantly higher than the average when the value system was positive"
  - [section] "The model's attention mechanisms can dynamically adjust sampling based on the value system presented in the prompt"
  - [corpus] No direct evidence - corpus mentions prescriptive AI but not in-context learning of value systems
- Break condition: If the prompt structure doesn't clearly establish the value system, the model may not apply the bias correctly.

### Mechanism 3
- Claim: Value bias manifests in prototype judgments, where models rate exemplars as more prototypical when they align with the ideal value rather than statistical average.
- Mechanism: The model's concept representations include both statistical and value-based dimensions, with the value dimension influencing judgments of typicality when evaluating exemplars.
- Core assumption: Concept representations in the model contain both statistical and value-based dimensions that can be independently accessed.
- Evidence anchors:
  - [abstract] "39 of 46 exemplars were rated as more prototypical than their average value in the direction of the ideal (p < 0.001)"
  - [section] "The model's concept representations include both statistical and value-based dimensions"
  - [corpus] Weak evidence - corpus contains related work on concept representations but not specifically on prototype judgments
- Break condition: If the exemplar description doesn't clearly indicate value-related features, the bias may not manifest in prototype judgments.

## Foundational Learning

- Concept: Statistical vs. prescriptive components of concepts
  - Why needed here: Understanding how LLMs separate descriptive (statistical) and prescriptive (ideal) aspects of concepts is crucial for interpreting the value bias mechanism
  - Quick check question: How would you distinguish between a statistically average value and an ideal value for a concept like "exercise frequency"?

- Concept: In-context learning and prompt engineering
  - Why needed here: The experiments demonstrate that LLMs can learn new value systems through prompting without fine-tuning, requiring understanding of how context influences model behavior
  - Quick check question: What prompt design elements would you use to establish a new value system for a previously unknown concept?

- Concept: Prototype theory and exemplar-based reasoning
  - Why needed here: The prototype judgment experiments rely on understanding how humans and models evaluate typicality, which involves both statistical and value-based considerations
  - Quick check question: How would you design an experiment to test whether a model's prototype judgments are driven by statistical averages or ideal values?

## Architecture Onboarding

- Component map: Input processing -> Prompt encoding -> Value system extraction -> Attention adjustment -> Token sampling -> Response generation
- Critical path: Prompt → Value system extraction → Attention adjustment → Token sampling → Response generation
- Design tradeoffs:
  - Temperature settings affect the strength of value bias manifestation
  - Prompt structure impacts how clearly the value system is communicated
  - Concept complexity influences whether statistical vs. ideal values are accessible
- Failure signatures:
  - Random or inconsistent bias direction when value system is unclear
  - No bias manifestation when training data lacks value associations
  - Overcorrection toward ideal values in prompts with mixed valence
- First 3 experiments:
  1. Test value bias on a new concept with explicit value grades in the prompt
  2. Compare prototype judgments for exemplars with clear ideal vs. average values
  3. Evaluate how temperature settings affect the strength of value bias manifestation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does value bias in LLMs extend beyond the specific categories tested to other domains like financial or legal contexts?
- Basis in paper: [inferred]
- Why unresolved: The paper only tested value bias in 36 everyday categories (e.g., TV watching, exercise, parking tickets). It did not explore whether this bias manifests in domains where decisions have higher stakes, such as financial advising, legal judgments, or medical recommendations.
- What evidence would resolve it: Testing LLMs on prompts related to financial risk assessment, legal sentencing recommendations, or medical treatment choices, then measuring if samples systematically deviate toward "ideal" values in those domains.

### Open Question 2
- Question: Can value bias in LLMs be reduced or eliminated through fine-tuning or adversarial training?
- Basis in paper: [inferred]
- Why unresolved: The paper demonstrates the presence of value bias but does not investigate whether this bias can be mitigated. It would be valuable to know if fine-tuning on balanced datasets or using adversarial prompts could reduce the tendency to sample toward ideal values.
- What evidence would resolve it: Conducting experiments where LLMs are fine-tuned on datasets designed to neutralize value bias, then re-running the same tests to see if α values decrease or become statistically insignificant.

### Open Question 3
- Question: How does the magnitude of value bias in LLMs compare to that in humans across the same categories?
- Basis in paper: [explicit]
- Why unresolved: The paper compares the direction of bias (sign of α) between humans and LLMs but does not quantify the magnitude of deviation. It is unclear whether LLMs exhibit stronger or weaker value bias than humans in the same contexts.
- What evidence would resolve it: Measuring and comparing the absolute values of α for both humans and LLMs across the same categories, then performing statistical tests to determine if the bias magnitude differs significantly between the two.

## Limitations

- Experiments were conducted exclusively with GPT-4, limiting generalizability to other LLM architectures
- Value bias measurement relies on self-reported values from the model, which may not reflect actual internal representations
- Training data composition and its relationship to value systems remains opaque
- Temperature settings (0.8 and 0.0) represent only two points in a continuous spectrum

## Confidence

**High Confidence Claims:**
- The existence of a measurable value bias in GPT-4 sampling (α > 0 in 24/36 categories)
- The ability to manipulate sampling direction through in-context value system prompts
- The demonstration that prototypes are rated as more prototypical when they align with ideal values

**Medium Confidence Claims:**
- The mechanism by which value bias operates through learned distributional patterns
- The generalizability of these findings to other LLM architectures
- The practical implications for downstream tasks like summarization

**Low Confidence Claims:**
- The complete absence of correlation between human and LLM value systems (r = -0.02)
- The claim that LLMs "lack deliberate reasoning" while exhibiting systematic heuristics
- The assertion that this bias is "implicit in the model's training" without direct corpus analysis

## Next Checks

1. **Cross-model replication**: Conduct the same value bias experiments across multiple LLM architectures (e.g., Claude, Llama, Gemini) to determine whether the bias is architecture-specific or universal. This would involve running the 36-category implicit bias test and the glubbing in-context learning experiment on each model, comparing α values and sampling patterns.

2. **Temperature sensitivity analysis**: Systematically vary temperature settings across a fine-grained range (0.0 to 1.5 in 0.1 increments) while measuring value bias strength. This would reveal whether the bias operates independently of randomness parameters or whether it's amplified or diminished at different temperature levels.

3. **Human alignment verification**: Conduct a controlled experiment where human participants rate the same categories and exemplars used in the study, then perform a more rigorous correlation analysis with larger sample sizes and multiple human raters per category. This would validate whether the observed lack of correlation is robust or an artifact of the experimental design.