---
ver: rpa2
title: Rejecting Hallucinated State Targets during Planning
arxiv_id: '2410.07096'
source_url: https://arxiv.org/abs/2410.07096
tags:
- training
- targets
- steps
- state
- leap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucinations in generative
  models used for planning in reinforcement learning agents, which can lead to delusional
  behaviors and safety concerns. The authors propose a target feasibility evaluator
  that learns to identify and reject infeasible targets proposed by generators, thereby
  improving planning safety and performance.
---

# Rejecting Hallucinated State Targets during Planning

## Quick Facts
- arXiv ID: 2410.07096
- Source URL: https://arxiv.org/abs/2410.07096
- Reference count: 40
- This paper addresses hallucinations in generative models used for planning in RL agents, proposing a target feasibility evaluator that learns to identify and reject infeasible targets to improve safety and performance.

## Executive Summary
This paper tackles the problem of hallucinations in generative models used for planning in reinforcement learning agents. Hallucinations—generating targets that are infeasible under the agent's policy—can lead to delusional behaviors and safety concerns. The authors propose a target feasibility evaluator that learns to identify and reject infeasible targets proposed by generators, thereby improving planning safety and performance. The evaluator is designed to be minimally intrusive, automatically learn feasibility from interactions, and handle different time horizons. Key technical innovations include an off-policy compatible learning rule, a distributional architecture for learning feasibility across multiple time horizons, and data augmentation strategies using hindsight relabeling. Experiments on various decision-time and background planning agents show significant reductions in delusional behaviors and improved performance when the evaluator is integrated.

## Method Summary
The method introduces a target feasibility evaluator that learns to identify and reject infeasible targets proposed by generators in planning agents. The evaluator uses a distributional architecture (C51-style) to output a histogram over possible hitting times, estimating the probability that a target is feasible within a given horizon. It learns via an off-policy compatible recursive rule that estimates the distribution of Dπ(s, g⊙), the first timestep where target g⊙ is reached under policy π starting from state s. Data augmentation through hindsight relabeling strategies (episode, generate, pertask) exposes the evaluator to both experienced and hallucinated targets, enabling it to learn feasibility across diverse scenarios. The evaluator acts as a minimally intrusive add-on, observing agent interactions without altering the agent's policy or generator architecture.

## Key Results
- The target feasibility evaluator significantly reduces delusional behaviors in planning agents across multiple environments and agent types.
- Data augmentation through hindsight relabeling strategies enables the evaluator to learn feasibility for hallucinated targets, improving its rejection accuracy.
- The approach generalizes across different planning agents, including decision-time planners (Skipper, LEAP) and background planners (Dyna), with consistent performance improvements.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The evaluator learns to reject infeasible targets by estimating the distribution of Dπ(s, g⊙), the first timestep where target g⊙ is reached under policy π starting from state s.
- Mechanism: Uses an off-policy compatible learning rule that recursively estimates Dπ(s, g⊙) = 1 + Dπ(s′, g⊙), with terminal states set to ∞ or 0 depending on whether they satisfy the target. A distributional C51-style architecture outputs a histogram over [1, 2, …, T] to learn τ-feasibility probabilities for multiple horizons simultaneously.
- Core assumption: The feasibility of a target is intrinsic to the state-policy-target tuple, not heuristic, and can be learned from interaction data via hindsight relabeling.
- Evidence anchors:
  - [abstract] "target feasibility evaluator that learns to identify and reject infeasible targets proposed by generators"
  - [section 4.2] "learning rule to indirectly learn the targets' feasibility by directly learning the distribution of Dπ(s, g⊙)"
  - [corpus] Weak: no external corpus evidence for distributional architecture in feasibility learning
- Break condition: If the replay buffer lacks sufficient diverse source-target pairs (e.g., all targets are far in episode, missing local ones), the histogram cannot learn accurate probabilities → delusional evaluations.

### Mechanism 2
- Claim: Hindsight relabeling strategies generate training data beyond experienced transitions, exposing the evaluator to hallucinated targets it must learn to reject.
- Mechanism: Three relabeling strategies—episode (same trajectory), generate (just-in-time candidate targets), pertask (across episode same task)—expand source-target pair diversity. Mixtures balance efficiency vs. hallucination exposure.
- Core assumption: Exposure bias between planning behavior (all generated targets) and training (only experienced) can be corrected by synthetically relabeling transitions.
- Evidence anchors:
  - [abstract] "data augmentation based on hindsight relabeling"
  - [section 4.3] "two assistive hindsight relabeling strategies performing data augmentation to provide training data beyond those collected via interactions"
  - [corpus] Weak: corpus mentions relabeling but no evidence it addresses hallucinations specifically
- Break condition: If generator never produces G.1/G.2 targets during training, generate and pertask add no hallucinated examples → evaluator never learns to reject them.

### Mechanism 3
- Claim: The evaluator acts as a minimally intrusive add-on: it learns by observing agent interactions without altering the agent's policy or generator architecture.
- Mechanism: Evaluator receives paired (state, target) inputs from the agent's replay buffer and generator outputs, updates its feasibility model, and outputs reject/accept decisions to the planning loop. No gradients flow back to agent or generator.
- Core assumption: Feasibility estimation can be learned offline from logged data; the agent's planning behavior remains unchanged except for target rejection.
- Evidence anchors:
  - [abstract] "without the need to change the agent or its generator"
  - [section 4.1] "minimally intrusive … generally applicable to existing TAP agents"
  - [corpus] Weak: corpus does not discuss observer-style add-ons in RL
- Break condition: If agent's state/target representations change during training, evaluator's fixed input mapping becomes stale → inaccurate feasibility predictions.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formalism and state transition dynamics
  - Why needed here: The evaluator reasons about whether a target state can be reached under the agent's policy; this requires understanding P(s′|s,a) and the notion of feasible vs. infeasible transitions.
  - Quick check question: In an MDP with states S={A,B,C}, actions {left,right}, and deterministic transitions A→B (right), B→C (right), is target C feasible from A under a policy that always moves right? Why?

- Concept: Distributional RL and C51-style output heads
  - Why needed here: The evaluator outputs a probability distribution over hitting times Dπ(s,g⊙); C51's discrete support histogram enables simultaneous learning of multi-horizon feasibility probabilities.
  - Quick check question: If the support is [1,2,3,4,5] and the learned histogram is [0.1,0.2,0.3,0.4,0.0], what is the estimated probability that the target is 3-feasible?

- Concept: Hindsight Experience Replay (HER) and relabeling strategies
  - Why needed here: HER provides the data augmentation mechanism that exposes the evaluator to both experienced and hallucinated targets, overcoming exposure bias.
  - Quick check question: Given a trajectory s0→s1→s2 (terminal), which relabeling strategy would pair s0 with s2 as a source-target pair?

## Architecture Onboarding

- Component map:
  - Target generator (external to evaluator)
  - State encoder g(s) → target embedding
  - Evaluator neural net: input (state embedding, target embedding), output histogram over [1..T]
  - Feasibility threshold τ (configurable per agent)
  - HER relabeling module (episode, generate, pertask mixtures)
  - Decision filter: reject if histogram probability at τ below ε

- Critical path:
  1. Agent interacts → stores (s,a,r,s′,done) in replay
  2. HER relabels transitions with source-target pairs
  3. Evaluator samples minibatch → computes Dπ estimate via recursive rule
  4. Evaluator updates weights (off-policy compatible)
  5. During planning, evaluator scores each generated target
  6. Reject targets below feasibility threshold → feed remaining to agent

- Design tradeoffs:
  - Histogram support length T vs. computational cost: larger T captures longer horizons but increases forward pass time.
  - Relabeling mixture ratios: more generate increases hallucination coverage but may hurt G.0 learning efficiency.
  - Off-policy compatibility vs. on-policy accuracy: off-policy enables learning from logged data but may lag behind current policy.

- Failure signatures:
  - High rejection rate of all targets → histogram peaks at ∞ for all inputs → evaluator never saw valid G.0 examples.
  - Low rejection rate but performance drop → evaluator fails to identify hallucinations; check relabeling mixture and threshold.
  - Slow convergence → insufficient diverse source-target pairs; increase pertask sampling or adjust episode length.

- First 3 experiments:
  1. Run evaluator alone on fixed replay buffer with episode relabeling; verify histogram learns τ-feasibility for known reachable/unreachable targets.
  2. Add generate JIT relabeling; confirm evaluator starts rejecting hallucinated targets proposed by generator.
  3. Attach evaluator to a simple 1-step Dyna agent; measure reduction in delusional updates and performance gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hallucinations in action-based targets (not state-based) impact planning agents, and what evaluation strategies could address these?
- Basis in paper: [explicit] The paper discusses state-based targets and explicitly states that future work should investigate agents proposing "targets" that are in nature "actions," which do not directly correspond to reaching states.
- Why unresolved: The paper focuses on state-based targets and does not explore action-based target hallucinations or their impact on planning.
- What evidence would resolve it: Experimental results showing the prevalence and impact of action-based target hallucinations in various planning agents, and the effectiveness of different evaluation strategies in mitigating these issues.

### Open Question 2
- Question: What is the optimal balance between episode, generate, and pertask relabeling strategies for different types of hallucinations and planning agents?
- Basis in paper: [explicit] The paper discusses various relabeling strategies (episode, future, generate, pertask) and their effectiveness in addressing different types of hallucinations (G.0, G.1, G.2), but does not provide a definitive answer on the optimal balance.
- Why unresolved: The effectiveness of each strategy depends on the specific environment, agent architecture, and type of hallucinations encountered, requiring further experimentation to determine optimal combinations.
- What evidence would resolve it: Comparative studies across multiple environments and agents, systematically varying the proportions of each relabeling strategy and measuring their impact on hallucination reduction and planning performance.

### Open Question 3
- Question: How does the evaluator's performance scale with increasing state space complexity and partial observability in environments like Atari?
- Basis in paper: [inferred] The paper mentions applying the evaluator to DreamerV2 on Atari environments but notes preliminary results show negligible performance gains due to state representation stabilization challenges.
- Why unresolved: The paper's experiments are primarily conducted on controlled environments (SSM, RDS) with simpler state spaces, and the DreamerV2 results are preliminary due to computational limitations.
- What evidence would resolve it: Extended experiments on Atari and other complex environments, measuring the evaluator's performance as state space complexity and partial observability increase, and identifying any performance bottlenecks or adaptations needed.

## Limitations

- The distributional C51-style architecture assumes discrete hitting-time support [1..T], but no evidence is provided that this discretization is sufficient for continuous or very long-horizon domains.
- Relabeling strategies (generate, pertask, episode) are heuristic and their optimal mixture ratios are not derived; performance may be sensitive to these choices.
- The claim that the evaluator is "minimally intrusive" assumes no feedback from evaluator to agent or generator, but real-world deployment may require tighter integration.

## Confidence

- High confidence: The core mechanism of using hindsight relabeling to learn feasibility distributions is well-supported by the described architecture and learning rule.
- Medium confidence: The claim that this approach generalizes across different planning agents (Skipper, LEAP, Dyna) is supported by experiments, but may not hold for all agent types.
- Low confidence: The assertion that the evaluator can handle arbitrary time horizons without architectural changes is plausible but untested for very long horizons.

## Next Checks

1. **Stress test relabeling diversity**: Systematically vary the mixture ratios of generate, pertask, and episode relabeling strategies to identify optimal configurations and test evaluator robustness to data scarcity.
2. **Cross-agent transfer evaluation**: Train the evaluator on one agent type (e.g., Skipper) and evaluate on a novel agent (e.g., a new background planner) to test true generalization.
3. **Long-horizon feasibility testing**: Extend experimental horizons beyond those reported to verify the distributional architecture's accuracy and stability for T > 100 steps.