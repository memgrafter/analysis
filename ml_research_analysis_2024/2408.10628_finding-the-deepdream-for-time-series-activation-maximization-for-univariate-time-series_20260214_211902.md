---
ver: rpa2
title: 'Finding the DeepDream for Time Series: Activation Maximization for Univariate
  Time Series'
arxiv_id: '2408.10628'
source_url: https://arxiv.org/abs/2408.10628
tags:
- time
- series
- activation
- data
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sequence Dreaming, an adaptation of activation
  maximization techniques for time series data to enhance interpretability of deep
  learning models. The method generates synthetic time series that maximally activate
  specific neurons by optimizing input sequences through gradient-based approaches
  with various regularization techniques including exponential smoothing, total variation,
  and smoothness factors.
---

# Finding the DeepDream for Time Series: Activation Maximization for Univariate Time Series

## Quick Facts
- arXiv ID: 2408.10628
- Source URL: https://arxiv.org/abs/2408.10628
- Authors: Udo Schlegel; Daniel A. Keim; Tobias Sutter
- Reference count: 16
- Primary result: Sequence Dreaming achieves 95% test accuracy on FordA dataset while generating realistic, interpretable time series that maximize specific neuron activations

## Executive Summary
This paper introduces Sequence Dreaming, an adaptation of activation maximization techniques from image processing to time series data. The method generates synthetic time series that maximally activate specific neurons in deep learning models through gradient-based optimization with regularization. The approach is evaluated on the FordA time series classification dataset using a ResNet model, demonstrating the ability to produce smooth, realistic sequences that target specific regions of model activations. The method advances interpretability for time series models, which is crucial for safety-critical applications where understanding model decisions is essential.

## Method Summary
Sequence Dreaming adapts activation maximization to time series by optimizing input sequences through gradient ascent to maximize neuron activation, while applying regularization techniques including exponential smoothing, total variation, and smoothness factors. The method uses a ResNet architecture trained on univariate time series data, with the optimization process constrained by multiple regularization terms to prevent unrealistic outputs. Generated sequences are evaluated using Mahalanobis distance in activation space rather than raw time series comparison, providing a more meaningful assessment of whether the sequences elicit the intended neural response. The approach can target different activation regions by adjusting initialization and hyperparameters, producing either class-centered prototypes or border/maximum activation sequences.

## Key Results
- Sequence Dreaming generates realistic, smooth time series that target specific regions of model activations
- The method achieves 95% test accuracy on the FordA dataset using a ResNet model
- Mahalanobis distance analysis shows generated sequences align with desired activation patterns while maintaining plausibility
- Visual evaluation confirms the method produces interpretable temporal features learned by neural networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient ascent on input sequences, combined with multiple regularization terms, can generate synthetic time series that strongly activate target neurons without overfitting to noise.
- Mechanism: The method starts from a data sample or random initialization and iteratively updates the input using gradients of the activation score. Regularization terms (L2 norm, total variation, smoothness factor) constrain the optimization, preventing the input from diverging into unrealistic patterns while still maximizing neuron activation.
- Core assumption: The trained model's gradients with respect to its inputs are informative and stable enough to guide the generation of realistic, high-activation sequences.
- Evidence anchors: [abstract] "The method generates synthetic time series that maximally activate specific neurons by optimizing input sequences through gradient-based approaches with various regularization techniques..."
- Break condition: If gradients become unstable or lead to extreme, unrealistic sequences despite regularization, the approach fails to produce interpretable results.

### Mechanism 2
- Claim: Mahalanobis distance analysis on activations provides a more meaningful evaluation of generated sequences than direct time series comparison.
- Mechanism: Instead of measuring how close generated sequences are to training data in raw space (which may be misleading), the method evaluates whether the activations produced by generated sequences fall within the distribution of training activations. This captures whether the sequence elicits the intended neural response in context.
- Core assumption: The model's internal representations (activations) are the true locus of interest for interpretability, not the raw input similarity.
- Evidence anchors: [section] "The Mahalanobis distance measures the distance between a point and a distribution, considering the correlations between variables... We believe that applying this measure directly to the time series data is not ideal..."
- Break condition: If the model's activations are unstable or highly sensitive to small input changes, Mahalanobis distance in activation space may not reliably indicate plausibility.

### Mechanism 3
- Claim: Sequence Dreaming can target either class centers or border/maximum activations by adjusting hyperparameters and initialization, revealing different aspects of the model's decision boundaries.
- Mechanism: By initializing from class-centered samples and tuning regularization weights, the method can either produce sequences near the center of a class's activation distribution (prototypes) or sequences that push activations toward the boundary or maximum (hard examples). This flexibility exposes both typical and extreme model behavior.
- Core assumption: The activation landscape contains distinct regions (center vs. border) that can be navigated through controlled optimization, and these regions correspond to meaningful model behavior.
- Evidence anchors: [abstract] "The results show that our proposed Sequence Dreaming approach demonstrates targeted activation maximization for different use cases so that either centered class or border activation maximization can be generated."
- Break condition: If the activation landscape is too flat or noisy, the method cannot reliably distinguish or target different regions, reducing interpretability value.

## Foundational Learning

- Concept: Activation Maximization and DeepDream for images
  - Why needed here: Sequence Dreaming adapts these techniques from image to time series domains, so understanding the original mechanisms is essential for grasping the modifications required.
  - Quick check question: What is the primary goal of Activation Maximization in the context of neural networks?

- Concept: Regularization in optimization (L2, total variation, smoothing)
  - Why needed here: These techniques prevent overfitting during gradient ascent and ensure generated sequences remain realistic and interpretable.
  - Quick check question: How does total variation regularization differ from simple L2 norm regularization?

- Concept: Mahalanobis distance and outlier detection
  - Why needed here: Used to evaluate whether generated sequences are plausible by measuring their distance from the training distribution in activation space.
  - Quick check question: Why might Mahalanobis distance be preferred over Euclidean distance for evaluating generated time series?

## Architecture Onboarding

- Component map: Trained time series classification model (ResNet) -> Sequence Dreaming optimizer (gradient ascent with regularization) -> Evaluation module (Mahalanobis distance, PCA projections) -> Hyperparameter grid search manager
- Critical path: 1. Load trained model and dataset 2. Select target neuron/class and initialization strategy 3. Optimize input sequence using gradient-based method with regularization 4. Evaluate generated sequences using Mahalanobis distance and PCA 5. Visualize results and compare to baselines
- Design tradeoffs:
  - Gradient ascent vs. gradient descent: Ascent maximizes activation directly but risks instability; descent with target activation provides more control but requires careful hyperparameter tuning
  - Regularization strength: Stronger regularization yields smoother, more realistic sequences but may reduce activation maximization effectiveness
  - Evaluation in activation space vs. raw input space: Activation space is more meaningful for interpretability but may hide input-level artifacts
- Failure signatures:
  - Generated sequences with extreme outliers or discontinuities despite regularization
  - Mahalanobis distance indicating generated sequences are far from training distribution in activation space
  - Gradients becoming NaN or exploding during optimization
- First 3 experiments:
  1. Generate class-centered sequences for a simple binary dataset and verify they fall within the Mahalanobis distance bounds of training activations
  2. Generate border/maximum activation sequences and confirm they appear as outliers in activation space but still elicit high activation
  3. Compare Sequence Dreaming results to baseline methods (e.g., Ellis et al.) using both visual and quantitative metrics (Mahalanobis distance, PCA plots)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Sequence Dreaming compare to other attribution-based methods for time series XAI in terms of interpretability and plausibility of generated sequences?
- Basis in paper: [explicit] The paper discusses using attributions more heavily in the process as a future work direction and notes that selecting a working attribution technique is not trivial.
- Why unresolved: The paper focuses on comparing Sequence Dreaming with activation maximization techniques but does not directly compare it with attribution-based methods like Integrated Gradients or SHAP for time series.
- What evidence would resolve it: Empirical studies comparing Sequence Dreaming-generated sequences with those from attribution-based methods on the same datasets, measuring both interpretability (by human experts) and plausibility (using quantitative metrics like outlier detection).

### Open Question 2
- Question: Can genetic or evolutionary algorithms outperform gradient-based approaches in Sequence Dreaming for generating more diverse and effective activation-maximizing time series?
- Basis in paper: [explicit] The paper explicitly suggests this as a future work direction, citing works by Nguyen et al. and Xiao and Kreiman.
- Why unresolved: The paper uses gradient-based optimization and does not experiment with evolutionary algorithms for this task.
- What evidence would resolve it: Comparative experiments applying genetic algorithms to the same time series classification tasks, measuring the diversity and effectiveness of generated sequences against gradient-based Sequence Dreaming results.

### Open Question 3
- Question: How sensitive is Sequence Dreaming to the choice of hyperparameters across different time series datasets, and can we develop automated methods to select optimal parameters?
- Basis in paper: [explicit] The paper notes that hyperparameters are highly sensitive to the dataset, particularly weighting parameters, and mentions the extensive grid search required.
- Why unresolved: While the paper performs grid search, it does not explore automated hyperparameter optimization or provide a systematic study of sensitivity across multiple datasets.
- What evidence would resolve it: Systematic experiments varying hyperparameters across multiple diverse time series datasets, combined with automated hyperparameter optimization techniques like Bayesian optimization to assess sensitivity and develop guidelines.

## Limitations
- Lack of detailed architectural specifications and hyperparameter configurations critical for faithful reproduction
- Evaluation relies heavily on visual inspection and Mahalanobis distance in activation space, which may not capture all aspects of sequence quality
- Effectiveness across diverse time series domains beyond FordA remains unproven

## Confidence
- High confidence: The core mechanism of gradient-based activation maximization with regularization is well-established and theoretically sound
- Medium confidence: The adaptation to time series data is reasonable, but effectiveness across diverse datasets is unverified
- Medium confidence: Mahalanobis distance in activation space provides meaningful evaluation, though this assumption requires further validation
- Low confidence: The claim that Sequence Dreaming reveals distinct model behavior regions (center vs. border) is supported by limited evidence

## Next Checks
1. Implement and validate the ResNet architecture on FordA to confirm the reported 95% accuracy before proceeding with Sequence Dreaming
2. Generate sequences for multiple initialization strategies (random, class-centered, border) and systematically compare their activation patterns using PCA projections
3. Test Sequence Dreaming on additional time series datasets (e.g., ECG, accelerometer data) to assess generalizability and robustness across different temporal patterns and domain characteristics