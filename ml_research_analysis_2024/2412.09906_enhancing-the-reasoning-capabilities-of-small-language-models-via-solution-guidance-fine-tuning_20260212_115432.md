---
ver: rpa2
title: Enhancing the Reasoning Capabilities of Small Language Models via Solution
  Guidance Fine-Tuning
arxiv_id: '2412.09906'
source_url: https://arxiv.org/abs/2412.09906
tags:
- reasoning
- data
- language
- fine-tuning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enhancing the reasoning capabilities
  of small language models (SLMs) under resource constraints. The authors introduce
  a new reasoning strategy called Solution Guidance (SG), which focuses on problem
  understanding and decomposition at the semantic and logical levels, rather than
  specific computations.
---

# Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning

## Quick Facts
- arXiv ID: 2412.09906
- Source URL: https://arxiv.org/abs/2412.09906
- Authors: Jing Bi; Yuting Wu; Weiwei Xing; Zhenjie Wei
- Reference count: 5
- Primary result: Achieves 43.7% accuracy on GSM8K with only 1,000 training examples vs 34.4% with 30,000 CoT samples

## Executive Summary
This paper introduces Solution Guidance (SG) and Solution-Guidance Fine-Tuning (SGFT) to enhance reasoning capabilities of small language models under resource constraints. The approach replaces computation-heavy Chain-of-Thought (CoT) steps with higher-level semantic and logical decomposition, focusing on problem understanding rather than specific calculations. By fine-tuning a small language model to generate accurate problem-solving guidances using minimal SG training data, the method achieves significant performance improvements while using only 3% of the training data required by traditional CoT methods.

## Method Summary
The method involves three main steps: (1) using GPT-4o to generate SG training data from questions by decomposing problems into semantic and logical components, (2) fine-tuning a small language model using LISA fine-tuning method with the generated SG data, and (3) using collaborative inference where the SG-fine-tuned model generates guidance that is fed to a response model along with the original question to produce final answers. This plug-and-play paradigm allows any SLM to benefit from improved reasoning without requiring additional fine-tuning, preserving general capabilities while achieving better reasoning performance.

## Key Results
- SGFT achieves 43.7% accuracy on GSM8K with only 1,000 training examples
- Traditional CoT fine-tuning requires 30,000 examples to achieve 34.4% accuracy on same dataset
- SGFT demonstrates significant data efficiency, using only 3% of training data compared to CoT methods
- The approach works across multiple reasoning benchmark datasets including SVAMP, MultiArith, StrategyQA, and CommonsenseQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method improves reasoning performance by replacing computation-heavy CoT steps with higher-level semantic and logical decomposition.
- Mechanism: Solution Guidance (SG) shifts the reasoning task from generating detailed computational steps to producing problem decomposition strategies. This reduces noise from arithmetic errors and focuses on understanding the problem structure, enabling the response model to apply its reasoning capabilities more cleanly.
- Core assumption: The response model retains strong enough arithmetic and reasoning ability to perform accurate calculations when given a correct decomposition strategy.
- Evidence anchors:
  - [abstract] "SG focuses on problem understanding and decomposition at the semantic and logical levels, rather than specific computations, which can effectively improve the SLMs' generalization and reasoning abilities."
  - [section] "SG offers higher-level guidance. SG focuses on providing a framework for problem-solving rather than detailing the exact computational process."
  - [corpus] Weak anchor - corpus neighbors discuss general reasoning enhancement but do not directly support the specific mechanism of decomposing SG versus CoT.
- Break condition: If the response model's arithmetic capabilities are too weak to execute the guidance steps accurately, or if the SG generation model fails to produce correct problem decompositions, the method will degrade in effectiveness.

### Mechanism 2
- Claim: The approach achieves high performance with significantly less training data by focusing on problem understanding rather than full step-by-step reasoning.
- Mechanism: Fine-tuning the guidance model on SG data teaches it to identify problem-solving patterns and objectives without needing to replicate the full CoT chain, which requires much more data to generalize across diverse problem types.
- Core assumption: The guidance model can learn to generalize problem-solving patterns from a small dataset, and the response model can reliably execute the guidance.
- Evidence anchors:
  - [abstract] "With only a small amount of SG training data, SGFT can fine-tune a SLM to produce accurate problem-solving guidances, which can then be flexibly fed to any SLM as prompts, enabling it to generate correct answers directly."
  - [section] "Our method uses one model for high-level guidance—understanding the problem and providing strategies—while the other model generates answers based on this guidance."
  - [corpus] Weak anchor - corpus neighbors focus on reasoning enhancement but do not explicitly address data efficiency via problem decomposition.
- Break condition: If the small dataset fails to cover sufficient problem patterns, the guidance model may generate incomplete or incorrect guidance, leading to poor performance.

### Mechanism 3
- Claim: The plug-and-play paradigm allows any SLM to benefit from SG without requiring additional fine-tuning, preserving general capabilities.
- Mechanism: By decoupling the guidance generation from the answer generation, the response model does not undergo reasoning-specific fine-tuning, which can erode general language capabilities, while still benefiting from improved reasoning through the guidance.
- Core assumption: The response model maintains its general language and reasoning abilities without degradation when used in a prompt-based SG workflow.
- Evidence anchors:
  - [abstract] "These guidances can then be flexibly fed to any SLM as prompts, enabling it to generate correct answers directly."
  - [section] "This division of labor allows the models to retain their general capabilities while efficiently addressing complex reasoning tasks."
  - [corpus] Weak anchor - corpus neighbors do not discuss capability preservation in multi-model reasoning workflows.
- Break condition: If the response model's general capabilities degrade due to exposure to SG prompts or if the SG prompts are too specific, the plug-and-play advantage is lost.

## Foundational Learning

- Concept: Fine-tuning with LoRA and LISA
  - Why needed here: To efficiently adapt the guidance model with minimal parameter changes, enabling fine-tuning on consumer GPUs.
  - Quick check question: How does LISA differ from standard LoRA in terms of memory usage and training speed?

- Concept: In-context learning vs. few-shot vs. zero-shot
  - Why needed here: To understand how different prompt strategies affect the guidance model's ability to generate accurate SG.
  - Quick check question: What experimental evidence supports the superiority of few-shot over zero-shot in this SGFT context?

- Concept: Problem decomposition in reasoning tasks
  - Why needed here: To grasp how SG abstracts problem-solving into semantic and logical steps rather than computational ones.
  - Quick check question: How does SG handle problems requiring both logical reasoning and arithmetic computation?

## Architecture Onboarding

- Component map:
  Teacher model (GPT-4o) -> Guidance model (SLM) -> Response model (SLM)

- Critical path:
  1. Generate SG data using GPT-4o
  2. Fine-tune guidance model with SG data and context/few-shot examples
  3. Generate SG for new questions using fine-tuned guidance model
  4. Feed SG + question to response model to produce answer

- Design tradeoffs:
  - Data efficiency vs. coverage: Fewer SG examples reduce data needs but may miss rare problem types
  - Model separation vs. end-to-end fine-tuning: Preserves general capabilities but requires two models
  - Guidance abstraction level: Too abstract SG may lack actionable steps; too detailed may reintroduce noise

- Failure signatures:
  - Guidance model outputs SG with missing or incorrect problem decomposition
  - Response model fails to interpret SG correctly or performs faulty arithmetic
  - Both models degrade in performance when used collaboratively

- First 3 experiments:
  1. Compare zero-shot vs. few-shot SG generation accuracy on a held-out dataset
  2. Evaluate SG-guided answer accuracy vs. direct fine-tuning on the same model
  3. Test cross-model inference (different guidance and response models) to assess plug-and-play robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum size threshold for small language models to benefit from Solution Guidance fine-tuning without losing general capabilities?
- Basis in paper: [explicit] The paper states that SGFT aims to enhance reasoning capabilities of small language models (SLMs) with fewer than 10 billion parameters while preserving their general capabilities.
- Why unresolved: The paper tests models between 7-10B parameters but doesn't explore the lower boundary where SGFT might start becoming ineffective or where it might negatively impact general capabilities.
- What evidence would resolve it: Systematic experiments testing SGFT across a range of model sizes from 1B to 10B parameters, measuring both reasoning performance and general capability retention.

### Open Question 2
- Question: How does the effectiveness of Solution Guidance scale with increasing complexity of reasoning tasks beyond mathematical and commonsense domains?
- Basis in paper: [explicit] The paper demonstrates SGFT effectiveness on GSM8K, SV AMP, MultiArith, StrategyQA, and CommonsenseQA datasets, but doesn't explore more complex reasoning domains.
- Why unresolved: The paper focuses on relatively structured mathematical and commonsense reasoning tasks. It's unclear whether the SG approach would generalize to more complex domains requiring multi-modal reasoning, abstract thinking, or domain-specific knowledge.
- What evidence would resolve it: Experiments applying SGFT to diverse reasoning domains such as scientific reasoning, legal reasoning, or multi-step planning tasks, comparing performance against CoT and other methods.

### Open Question 3
- Question: What is the optimal ratio between guidance model and response model sizes for achieving the best reasoning performance in collaborative inference?
- Basis in paper: [explicit] The paper uses cross-model inference with different combinations of SG and response models, finding that SG-fine-tuned models as guidance paired with untrained models as response performed best.
- Why unresolved: While the paper identifies that SG-fine-tuned models work better as guidance, it doesn't systematically explore the optimal size relationship between guidance and response models, or whether larger guidance models consistently yield better results.
- What evidence would resolve it: Controlled experiments varying both guidance and response model sizes independently (e.g., 1B/3B/7B/10B combinations) while measuring reasoning accuracy to identify optimal size pairings.

## Limitations
- The effectiveness of SGFT heavily depends on the quality of the SG data generated by GPT-4o, which is not directly verifiable without access to the exact prompt templates and examples used.
- The paper claims significant data efficiency improvements, but the comparison is limited to GSM8K and may not generalize across all reasoning domains or model architectures.
- The separation between guidance and response models, while theoretically sound, may face practical challenges in maintaining consistent performance across different SLM pairs.

## Confidence

**High Confidence**: The core claim that SGFT achieves better performance with less training data compared to traditional CoT fine-tuning is supported by experimental results on GSM8K and other benchmark datasets.

**Medium Confidence**: The mechanism by which SG improves reasoning through higher-level semantic and logical decomposition is plausible but relies on assumptions about the response model's arithmetic capabilities that are not directly tested.

**Low Confidence**: The generalizability of SGFT's data efficiency claims to other reasoning tasks beyond those tested in the experiments.

## Next Checks

1. Test SGFT on a diverse set of reasoning tasks not covered in the original experiments to assess generalizability across problem types.

2. Conduct ablation studies varying the abstraction level of SG to determine the optimal balance between high-level guidance and actionable steps.

3. Evaluate the impact of SGFT on models with varying arithmetic capabilities to identify the threshold where the approach breaks down.