---
ver: rpa2
title: Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation
  QA
arxiv_id: '2407.15353'
source_url: https://arxiv.org/abs/2407.15353
tags:
- document
- retrieval
- documents
- answer
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAG-EDA, a customized retrieval augmented
  generation (RAG) framework designed to address the challenges of applying off-the-shelf
  RAG systems to electronic design automation (EDA) tool documentation question-answering
  (QA). The authors identify that general-purpose text embedding models, rerankers,
  and generators lack domain-specific knowledge in EDA, leading to poor retrieval
  and generation performance.
---

# Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA

## Quick Facts
- **arXiv ID**: 2407.15353
- **Source URL**: https://arxiv.org/abs/2407.15353
- **Reference count**: 40
- **Primary result**: Introduces RAG-EDA, a domain-customized RAG framework for EDA tool documentation QA that outperforms state-of-the-art systems on both ORD-QA and commercial EDA tool benchmarks

## Executive Summary
This paper addresses the challenge of applying general-purpose RAG systems to specialized domains by introducing RAG-EDA, a customized framework for electronic design automation (EDA) tool documentation question-answering. The authors identify that off-the-shelf RAG components lack domain-specific knowledge, leading to poor performance in retrieval and generation. They propose three domain-specific techniques: contrastive learning for text embeddings, a reranker distilled from a proprietary LLM, and a two-stage generator training approach. The paper also introduces ORD-QA, a benchmark dataset of 90 high-quality question-document-answer triplets for OpenROAD documentation, to evaluate their framework and support future research.

## Method Summary
RAG-EDA incorporates three domain-specific techniques to improve EDA documentation QA. First, it uses contrastive learning to fine-tune text embedding models on EDA terminology, enabling better semantic understanding of domain-specific concepts. Second, it employs a reranker distilled from a proprietary LLM to filter weakly-related documents that share similar semantics but lack relevance. Third, it utilizes a two-stage training approach for the generator LLM, first pre-training on EDA textbooks for domain knowledge and then instruction tuning on QA pairs for task-specific capabilities. The framework is evaluated on both the newly introduced ORD-QA benchmark and a commercial EDA tool documentation dataset.

## Key Results
- RAG-EDA significantly outperforms state-of-the-art RAG systems on both ORD-QA and commercial EDA tool documentation QA datasets
- The framework achieves superior performance in BLEU, ROUGE-L, and UniEval metrics compared to baseline approaches
- ORD-QA benchmark demonstrates high quality with 90 carefully curated question-document-answer triplets based on OpenROAD documentation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Domain-specific contrastive learning improves text embedding quality for EDA terms
- **Mechanism**: Constructs triplets of queries with the same EDA terminology (positive) and similar structures but different terminologies (hard negatives), learning to map semantically related EDA concepts closer while pushing apart non-related ones
- **Core assumption**: The contrastive loss formulation effectively captures semantic relationships in specialized domains when trained on appropriately curated triplets
- **Evidence anchors**: [abstract] "a contrastive learning scheme for text embedding model fine-tuning"; [section] "For one EDA-domain user query xi, we denote its positive and hard negative samples by xi+ and xi−, respectively"
- **Break condition**: If the triplet dataset lacks sufficient diversity in EDA terminology or the contrastive loss fails to converge, the embedding model will not improve semantic understanding of EDA concepts

### Mechanism 2
- **Claim**: Finetuning a reranker with GPT-4-filtered documents improves weak document filtering in EDA QA
- **Mechanism**: GPT-4 labels candidate documents as relevant or weakly-related for each question, then this labeled data trains a reranker via contrastive learning to distinguish between relevant and weakly-related documents even when they share similar semantics
- **Core assumption**: GPT-4's ability to filter weakly-related documents transfers effectively to a smaller reranker model through contrastive learning
- **Evidence anchors**: [abstract] "a reranker distilled from proprietary LLM"; [section] "we employ GPT-4 with task-specific prompt to differentiate relevant documents (Ci+) and weakly-related documents (Ci−)"
- **Break condition**: If GPT-4's filtering decisions are inconsistent or the reranker cannot learn the distinction from limited examples, the filtering performance will degrade

### Mechanism 3
- **Claim**: Two-stage training (domain pre-train + instruction tuning) creates a generator with EDA expertise
- **Mechanism**: The generator first pre-trains on EDA textbooks to acquire domain knowledge, then instruction tunes on QA pairs to learn the specific task of answering EDA tool documentation questions
- **Core assumption**: Pre-training on domain textbooks followed by instruction tuning on QA data creates a generator that combines domain knowledge with task-specific capabilities
- **Evidence anchors**: [abstract] "a generative LLM fine-tuned with high-quality domain corpus"; [section] "we propose a two-stage training scheme, namely, domain-knowledge pre-train and task-specific instruction tuning"
- **Break condition**: If the pre-training corpus lacks coverage of relevant EDA concepts or the instruction tuning data is insufficient, the generator will fail to acquire necessary domain knowledge or task skills

## Foundational Learning

- **Concept**: Contrastive learning for text embeddings
  - **Why needed here**: Standard embeddings lack semantic understanding of EDA-specific terminology, leading to poor retrieval
  - **Quick check question**: How does contrastive learning help the model distinguish between "Primary Input" and "Primary Output" when they appear in similar contexts?

- **Concept**: Cross-encoder architecture for reranking
  - **Why needed here**: The reranker must process query-document pairs to calculate relevance scores and filter weakly-related documents
  - **Quick check question**: Why is a cross-encoder better than a bi-encoder for distinguishing relevant vs weakly-related documents?

- **Concept**: Instruction tuning for task-specific LLM behavior
  - **Why needed here**: General LLMs lack the ability to answer EDA tool documentation questions with proper domain knowledge and logical inference
  - **Quick check question**: How does instruction tuning differ from standard fine-tuning when adapting an LLM for EDA QA?

## Architecture Onboarding

- **Component map**: Query → Custom embedding → Hybrid retrieval → Reranker → Generator → Answer
- **Critical path**: Query → Custom embedding → Hybrid retrieval → Reranker → Generator → Answer
- **Design tradeoffs**:
  - Hybrid retrieval vs pure semantic: Better for queries with specific keywords vs semantic understanding
  - Contrastive learning vs standard fine-tuning: Better semantic understanding but requires curated triplet data
  - Two-stage generator training vs single stage: Better domain knowledge but requires more training data and time
- **Failure signatures**:
  - Poor retrieval recall: Check custom embedding model performance on EDA terminology
  - Weakly-related documents passing through: Check reranker training data quality and contrastive learning setup
  - Incorrect or hallucinated answers: Check generator domain knowledge and instruction tuning effectiveness
- **First 3 experiments**:
  1. Evaluate custom embedding model recall@10 on a small set of EDA queries vs baseline embeddings
  2. Test reranker ability to filter a manually labeled set of relevant vs weakly-related document pairs
  3. Compare generator BLEU scores on a small QA dataset before and after two-stage training

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of RAG-EDA scale with larger or more diverse EDA tool documentation corpora?
  - **Basis in paper**: [explicit] The paper evaluates RAG-EDA on OpenROAD and a commercial EDA tool, but does not explore performance across different sizes or types of documentation
  - **Why unresolved**: The current evaluation is limited to specific datasets, and the scalability and generalizability of the approach to larger or more varied documentation have not been tested
  - **What evidence would resolve it**: Experiments showing RAG-EDA's performance on documentation of varying sizes, complexity, and domains within EDA would clarify its scalability and robustness

- **Open Question 2**: What are the long-term maintenance and update requirements for the domain-specific models in RAG-EDA as EDA tools and their documentation evolve?
  - **Basis in paper**: [inferred] The paper introduces domain-specific fine-tuning techniques but does not discuss how these models would be maintained or updated as EDA tools and documentation change over time
  - **Why unresolved**: The dynamic nature of EDA tools and their documentation suggests that continuous updates to the models may be necessary, but the paper does not address this aspect
  - **What evidence would resolve it**: A study or framework detailing the process and frequency of updates needed for the models as documentation evolves would provide insights into the sustainability of RAG-EDA

- **Open Question 3**: How does the performance of RAG-EDA compare to human experts in answering complex EDA tool documentation questions?
  - **Basis in paper**: [inferred] The paper focuses on quantitative metrics and comparisons with other RAG systems but does not benchmark against human performance in answering EDA-related questions
  - **Why unresolved**: Without a comparison to human experts, it is unclear how well RAG-EDA performs in real-world scenarios where expert knowledge is crucial
  - **What evidence would resolve it**: A comparative study where RAG-EDA and human experts answer the same set of complex EDA questions, with performance evaluated on accuracy and completeness, would provide a clearer picture of its effectiveness

## Limitations

- Limited dataset size (90 and 60 examples) raises questions about generalization beyond the specific documentation used
- Proprietary nature of the LLM used for reranker distillation prevents full reproduction and validation of this component
- No ablation studies isolating the contribution of each domain-specific technique

## Confidence

- **High confidence**: The overall framework design and motivation for domain customization in RAG systems
- **Medium confidence**: The effectiveness of contrastive learning for text embeddings and two-stage generator training
- **Low confidence**: The reranker distillation approach due to lack of transparency about the proprietary model

## Next Checks

1. **Ablation study**: Evaluate each component (custom embedding, reranker, generator) separately to quantify individual contributions to performance gains
2. **Dataset expansion**: Test RAG-EDA on additional EDA documentation and benchmark datasets to verify generalization beyond the current evaluation sets
3. **Open-source reranker baseline**: Replace the proprietary LLM-based reranker with an open-source alternative to validate the approach without access to the proprietary model