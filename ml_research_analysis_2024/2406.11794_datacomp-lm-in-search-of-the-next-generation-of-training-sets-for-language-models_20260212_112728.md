---
ver: rpa2
title: 'DataComp-LM: In search of the next generation of training sets for language
  models'
arxiv_id: '2406.11794'
source_url: https://arxiv.org/abs/2406.11794
tags:
- data
- dataset
- https
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DataComp for Language Models (DCLM) is the first large-scale benchmark
  for language model training data curation, addressing the challenge of standardizing
  dataset comparisons across different compute scales and model architectures. The
  benchmark provides a standardized corpus of 240T tokens from Common Crawl, fixed
  pretraining recipes, and a broad evaluation suite to isolate the effects of dataset
  design.
---

# DataComp-LM: In search of the next generation of training sets for language models

## Quick Facts
- **arXiv ID:** 2406.11794
- **Source URL:** https://arxiv.org/abs/2406.11794
- **Reference count:** 40
- **Primary result:** Model-based filtering enables 7B models to achieve 64% MMLU accuracy with 40% less compute

## Executive Summary
DataComp-LM addresses the critical challenge of standardizing language model training dataset comparisons by introducing the first large-scale benchmark for data curation. The benchmark provides a standardized 240T token corpus from Common Crawl, fixed pretraining recipes, and a comprehensive evaluation suite to isolate dataset design effects. Through extensive ablation studies, the research demonstrates that model-based filtering, particularly using fastText classifiers trained on high-quality reference data, is essential for assembling effective training sets. The resulting DCLM-BASELINE dataset enables training of 7B parameter models that achieve state-of-the-art performance while using significantly less compute than previous approaches.

## Method Summary
The benchmark establishes a standardized evaluation framework by curating a 240T token corpus from Common Crawl, defining fixed pretraining recipes, and implementing a broad evaluation suite. The core methodology involves systematic dataset curation through multiple filtering stages, including deduplication, content quality assessment, and model-based filtering using classifiers trained on reference datasets. The research employs ablation studies to evaluate different filtering strategies and dataset compositions, with particular emphasis on model-based filtering techniques. All experiments use consistent training protocols and evaluation metrics to ensure fair comparisons across different dataset designs.

## Key Results
- Model-based filtering with fastText classifiers trained on high-quality data is critical for dataset quality
- 7B parameter models achieve 64% MMLU accuracy using only 2.6T tokens with model-based filtering
- DCLM-BASELINE dataset achieves 6.6 percentage point improvement over previous state-of-the-art while using 40% less compute
- The curated dataset enables training of models competitive with larger models like Llama 3 8B and Mistral-7B-v0.3

## Why This Works (Mechanism)
The effectiveness of model-based filtering stems from its ability to systematically identify and retain high-quality text while removing low-quality or irrelevant content. FastText classifiers trained on curated reference datasets can efficiently score and filter massive amounts of web data, creating a positive feedback loop where higher-quality training data leads to better filtering models. This approach overcomes the limitations of simple heuristic filtering by learning complex quality patterns from human-curated examples, enabling the creation of training sets that capture the full distribution of high-quality web text.

## Foundational Learning
- **Language model pretraining fundamentals**: Understanding transformer architectures and pretraining objectives is essential for interpreting dataset quality impacts
- **Text classification and filtering**: Knowledge of classification algorithms and filtering strategies is crucial for implementing model-based approaches
- **Evaluation metrics and benchmarks**: Familiarity with MMLU, HELM, and other evaluation frameworks is needed to assess model performance
- **Web data curation**: Understanding web crawling, deduplication, and content extraction techniques is necessary for working with large-scale datasets
- **Compute efficiency optimization**: Knowledge of how dataset quality impacts training efficiency and final model performance

## Architecture Onboarding
- **Component map**: Common Crawl -> Preprocessing -> Deduplication -> Quality Filtering -> Model-based Filtering -> DCLM-BASELINE dataset -> Model Training -> Evaluation
- **Critical path**: Model-based filtering stage is critical as it directly determines dataset quality and downstream model performance
- **Design tradeoffs**: Balancing filtering strictness against data quantity, computational cost of filtering versus training efficiency gains, and generalization versus overfitting to specific quality metrics
- **Failure signatures**: Poor filtering leads to degraded model performance, excessive filtering reduces coverage and diversity, and contamination causes evaluation metric inflation
- **First experiments**: 1) Compare different classifier architectures for filtering, 2) Evaluate contamination impact on benchmark results, 3) Test filtering effectiveness across different model scales

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on English language data limits cross-lingual applicability
- Computational cost of model-based filtering at scale remains challenging
- Potential biases introduced by filtering process not fully characterized
- Limited evaluation of long-term model performance stability
- May not capture all downstream capabilities needed for real-world applications

## Confidence
- **High confidence**: Effectiveness of model-based filtering approach, specific DCLM-BASELINE dataset composition
- **Medium confidence**: Generalizability across different model architectures and scales
- **Medium confidence**: Claim of representing "next generation" methodology

## Next Checks
1. Replicate filtering approach across different language families and non-English datasets to test cross-lingual generalizability
2. Scale up model-based filtering to 100B+ parameter models to evaluate efficiency gains at different scales
3. Conduct longitudinal study tracking model performance decay over extended training periods to validate long-term effectiveness of curated datasets