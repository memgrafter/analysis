---
ver: rpa2
title: 'KnowPC: Knowledge-Driven Programmatic Reinforcement Learning for Zero-shot
  Coordination'
arxiv_id: '2408.04336'
source_url: https://arxiv.org/abs/2408.04336
tags:
- learning
- player
- transition
- action
- knowpc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KnowPC, a knowledge-driven programmatic reinforcement
  learning method for zero-shot coordination. The core idea is to represent agent
  policies as interpretable programs rather than black-box neural networks, addressing
  the lack of interpretability and logic in traditional approaches.
---

# KnowPC: Knowledge-Driven Programmatic Reinforcement Learning for Zero-shot Coordination

## Quick Facts
- arXiv ID: 2408.04336
- Source URL: https://arxiv.org/abs/2408.04336
- Authors: Yin Gu; Qi Liu; Zhi Li; Kai Zhang
- Reference count: 40
- Primary result: Knowledge-driven programmatic RL method for ZSC shows superior performance in novel layouts compared to DRL baselines

## Executive Summary
This paper introduces KnowPC, a knowledge-driven programmatic reinforcement learning method for zero-shot coordination (ZSC) in the Overcooked environment. The core innovation is representing agent policies as interpretable programs rather than black-box neural networks, addressing the lack of interpretability and logic in traditional approaches. KnowPC uses an extractor to discover environmental transition rules from multi-agent interaction trajectories, a reasoner to deduce preconditions for action primitives based on this knowledge, and a program synthesizer to generate programs that meet logical constraints. The method demonstrates comparable or superior performance to advanced DRL methods in zero-shot coordination, with particular robustness when environment layouts change.

## Method Summary
KnowPC combines three components to generate interpretable policies for zero-shot coordination. First, an extractor analyzes multi-agent trajectories to identify environmental transition rules by computing action entropy and filtering player-caused transitions. Second, a reasoner builds a transition graph from these rules and infers preconditions for each action primitive by analyzing conjunctive conditions. Third, a program synthesizer uses a genetic algorithm to evolve programs that satisfy these preconditions, dramatically reducing the search space. The resulting programmatic policies are fully interpretable, allowing for easier understanding and debugging by human partners, while maintaining competitive performance in Overcooked tasks.

## Key Results
- KnowPC achieves comparable or superior performance to advanced DRL methods in zero-shot coordination tasks
- The method demonstrates robust performance when Overcooked environment layouts change, while DRL baselines fail
- Programmatic policies are fully interpretable, enabling human understanding and debugging
- KnowPC shows particular strength in ZSC+ scenarios involving coordination with unseen partners in novel layouts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extractor identifies player-caused transitions by using action entropy thresholds to filter out environmental noise
- Mechanism: For each transition, the extractor computes action entropy H(p). Transitions with H(p) below a threshold δ are considered player-caused, as their action distributions are concentrated
- Core assumption: Transitions with low action entropy are more likely caused by the player than by the teammate or environment
- Evidence anchors:
  - [abstract] "The extractor discovers environmental transition knowledge from multi-agent interaction trajectories"
  - [section] "The smaller the entropy, the more concentrated the probability distribution. If the H(p) of a transition is relatively small, it is very likely caused by the player."
- Break condition: If the threshold δ is set too low, important transitions may be missed; if too high, environmental noise may be misclassified as player-caused

### Mechanism 2
- Claim: Reasoner infers preconditions by analyzing conjunctive conditions in the transition graph
- Mechanism: The reasoner builds a transition graph from extracted transitions. For each interaction point node, it identifies conjunctive conditions (nodes pointing to the same action) and uses them as preconditions for the action primitive
- Core assumption: If two conditions are conjunctive for a transition, both must be true for the action to be valid
- Evidence anchors:
  - [abstract] "The reasoner deduces the preconditions of each action primitive based on the transition knowledge."
  - [section] "Definition 1. Since there exists a T and the prerequisites of T are Ij and Ik, Ij and Ik are each other's conjunctive conditions."
- Break condition: If the transition graph is incomplete or incorrectly constructed, the inferred preconditions may be invalid or insufficient

### Mechanism 3
- Claim: Program synthesizer uses inferred preconditions to constrain the search space, making program discovery efficient
- Mechanism: The synthesizer runs a genetic algorithm to evolve programs, but only considers programs whose action primitives satisfy the preconditions inferred by the reasoner. This drastically reduces the search space
- Core assumption: Constraining the search space to programs that satisfy logical preconditions will yield high-performing, interpretable policies
- Evidence anchors:
  - [abstract] "To tackle it, The reasoner uses the identified transition rules to determine the prerequisites of transitions, thereby establishing the preconditions for certain actions."
  - [section] "These preconditions can be used to guide the synthesizer in generating reasonable programs, significantly reducing the search space."
- Break condition: If the preconditions are too restrictive, the synthesizer may miss optimal programs; if too loose, efficiency gains are lost

## Foundational Learning

- **Concept: Reinforcement Learning with sparse rewards**
  - Why needed here: KnowPC operates in environments with sparse rewards (only receiving reward when delivering soup), which is challenging for standard RL methods
  - Quick check question: How does sparse reward affect the learning signal for an RL agent, and why is this a problem for neural policies?

- **Concept: Program synthesis and genetic algorithms**
  - Why needed here: KnowPC uses a genetic algorithm to search for high-performing programs that conform to the DSL and inferred preconditions
  - Quick check question: What are the key components of a genetic algorithm, and how do they apply to program synthesis in KnowPC?

- **Concept: Transition system analysis**
  - Why needed here: KnowPC's extractor and reasoner rely on understanding the environment's transition dynamics to infer valid preconditions and synthesize interpretable policies
  - Quick check question: How can transition rules be extracted from multi-agent trajectories, and what information is needed to distinguish player-caused from environmental transitions?

## Architecture Onboarding

- **Component map**: Extractor -> Reasoner -> Program Synthesizer
- **Critical path**:
  1. Collect multi-agent trajectories in Overcooked environment
  2. Extractor processes trajectories to identify transition rules and player-caused changes
  3. Reasoner builds transition graph and infers preconditions for each action primitive
  4. Program synthesizer runs genetic algorithm to evolve programs that satisfy preconditions
  5. Evaluate programs in self-play and select Pareto-optimal set for ZSC evaluation

- **Design tradeoffs**:
  - Interpretability vs. expressiveness: Restricting the DSL to a small set of primitives makes programs interpretable but may limit expressiveness
  - Efficiency vs. completeness: Constraining the search space with preconditions improves efficiency but may miss some optimal programs
  - Abstraction vs. precision: Inferring preconditions from transition rules provides abstraction but may not capture all nuances of the environment

- **Failure signatures**:
  - Extractor: High false positive rate in identifying player-caused transitions, leading to incorrect preconditions
  - Reasoner: Incomplete or incorrect transition graph, resulting in missing or invalid preconditions
  - Program Synthesizer: Genetic algorithm fails to converge, or synthesized programs do not satisfy preconditions

- **First 3 experiments**:
  1. Validate extractor: Compare identified player-caused transitions against ground truth on a small Overcooked layout
  2. Validate reasoner: Manually verify inferred preconditions for each action primitive against known environment dynamics
  3. Validate synthesizer: Compare performance of synthesized programs with and without precondition constraints on a simple Overcooked task

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the environmental transition rules discovered by the extractor generalize across different layouts, and can they be used to improve zero-shot coordination performance in novel environments?
  - Basis in paper: [explicit] The paper discusses the extractor's role in discovering environmental transition rules from multi-agent interaction trajectories and mentions the use of these rules in the reasoner to deduce preconditions for action primitives
  - Why unresolved: While the paper demonstrates the effectiveness of KnowPC in zero-shot coordination tasks, it does not explicitly address the generalizability of the discovered transition rules to new layouts or environments
  - What evidence would resolve it: Experiments showing KnowPC's performance on a variety of novel layouts or environments, comparing the results with and without the use of discovered transition rules

- **Open Question 2**: Can the reasoning algorithm be extended to handle more complex program structures, such as nested if-then-else statements or loops, to further improve the expressiveness and performance of the synthesized programs?
  - Basis in paper: [inferred] The paper describes the reasoning algorithm as focusing on interaction point nodes and their conjunctive conditions, but does not explicitly address the handling of more complex program structures
  - Why unresolved: The current DSL and reasoning algorithm may have limitations in expressing and reasoning about more complex decision-making logic, potentially hindering performance in certain scenarios
  - What evidence would resolve it: Experiments comparing KnowPC's performance using the current DSL and reasoning algorithm with an extended version that supports more complex program structures

- **Open Question 3**: How does the choice of exploration strategy during training (e.g., ϵ-greedy) affect the quality and diversity of the discovered programs, and can alternative exploration methods lead to improved performance?
  - Basis in paper: [explicit] The paper mentions the use of ϵ-greedy exploration during training, but does not explore alternative exploration strategies
  - Why unresolved: The exploration strategy plays a crucial role in the agent's ability to discover diverse and high-performing programs, but the impact of different exploration methods on KnowPC's performance is not investigated
  - What evidence would resolve it: Experiments comparing KnowPC's performance using different exploration strategies (e.g., Boltzmann exploration, Upper Confidence Bound) and analyzing the resulting program diversity and quality

## Limitations

- The method relies on action entropy thresholds to identify player-caused transitions, but the optimal threshold is not discussed, which could lead to incorrect knowledge bases
- The precondition inference approach using conjunctive conditions may miss important temporal or sequential dependencies in the environment
- The genetic algorithm's efficiency in exploring the constrained search space is not quantitatively analyzed, raising questions about scalability to complex layouts

## Confidence

- **High confidence**: The overall framework of KnowPC (extractor → reasoner → synthesizer) is sound and addresses a clear need for interpretable policies in ZSC. The empirical results showing superior performance in ZSC+ (novel layouts) are convincing
- **Medium confidence**: The transition extraction and precondition inference mechanisms are well-motivated, but their robustness to noisy or incomplete data is not thoroughly evaluated. The choice of entropy threshold and the completeness of the transition graph are potential failure points
- **Low confidence**: The program synthesis component, while promising, lacks detailed analysis of the search space reduction and convergence properties. The paper does not discuss how to handle cases where the DSL is insufficient to express optimal policies

## Next Checks

1. **Transition extraction ablation**: Run the extractor with different entropy thresholds δ on a known Overcooked layout and measure the accuracy of identified player-caused transitions against ground truth
2. **Precondition inference completeness**: Manually inspect the inferred preconditions for each action primitive and compare them against the true environment dynamics. Identify any missing or incorrect preconditions
3. **Program synthesis efficiency**: Measure the convergence rate and final performance of the genetic algorithm with and without precondition constraints on a simple Overcooked task. Quantify the search space reduction achieved by using preconditions