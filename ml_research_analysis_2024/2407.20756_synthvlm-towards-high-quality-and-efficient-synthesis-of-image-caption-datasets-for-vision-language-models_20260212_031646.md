---
ver: rpa2
title: 'SynthVLM: Towards High-Quality and Efficient Synthesis of Image-Caption Datasets
  for Vision-Language Models'
arxiv_id: '2407.20756'
source_url: https://arxiv.org/abs/2407.20756
tags:
- data
- image
- arxiv
- language
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SynthVLM introduces a novel pipeline that synthesizes high-quality
  image-caption pairs by generating images from high-quality captions using diffusion
  models, rather than relying on web-sourced data. This approach improves data quality
  by eliminating noise, watermarks, and artifacts, and ensures precise image-text
  alignment through combined CLIPScore and SSIM evaluation.
---

# SynthVLM: Towards High-Quality and Efficient Synthesis of Image-Caption Datasets for Vision-Language Models

## Quick Facts
- arXiv ID: 2407.20756
- Source URL: https://arxiv.org/abs/2407.20756
- Reference count: 40
- Primary result: SynthVLM-7B/13B achieve SOTA vision-language performance using only 100K synthetic image-caption pairs, outperforming LLaVA-558K on benchmarks while using 18% of the training data.

## Executive Summary
SynthVLM introduces a novel pipeline that synthesizes high-quality image-caption pairs by generating images from high-quality captions using diffusion models, rather than relying on web-sourced data. This approach improves data quality by eliminating noise, watermarks, and artifacts, and ensures precise image-text alignment through combined CLIPScore and SSIM evaluation. Using only 100K curated synthetic pairs, SynthVLM-trained models achieve state-of-the-art vision-language understanding performance, outperforming baselines like LLaVA on benchmarks while using only 18% of the training data. The synthetic dataset also preserves language abilities and supports real-world applications, offering an efficient, privacy-safe, and high-quality alternative to web-scraped data.

## Method Summary
SynthVLM employs a two-stage pipeline for synthesizing high-quality image-caption datasets. First, it curates high-quality captions from web sources (LAION, CC, SBU) and model-generated data (DataComp via BLIP2), filtering low-quality pairs using CLIPScore and additional metrics. Second, it generates images from these curated captions using Stable Diffusion XL at 1024x1024 resolution, then evaluates and selects the best pairs using a weighted combination of CLIPScore (semantic alignment) and SSIM (image quality preservation). The final curated dataset contains 100K image-caption pairs, which is used to train LLaVA-7B/13B models through projector alignment and supervised fine-tuning stages.

## Key Results
- SynthVLM-7B and SynthVLM-13B achieve SOTA performance on vision-language benchmarks including SQA, MMVet, and MMLU
- Models trained on only 100K synthetic pairs outperform LLaVA-558K trained on 500K+ real pairs
- High-quality synthetic data preserves language abilities while improving vision-language understanding
- Synthetic approach eliminates privacy concerns associated with web-scraped data
- 18% data reduction (33MB vs 27GB) achieves superior performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models generate higher-quality images than real-world data by avoiding watermarks, blurriness, and other artifacts.
- Mechanism: The synthesis pipeline uses Stable Diffusion XL to generate images from curated captions, producing images at 1024x1024 resolution that are then evaluated for quality and alignment.
- Core assumption: Diffusion models can generate images that are visually superior and semantically aligned with text descriptions.
- Evidence anchors:
  - [abstract] "SynthVLM introduces a novel pipeline that synthesizes high-quality image-caption pairs by generating images from high-quality captions using diffusion models, rather than relying on web-sourced data."
  - [section] "SynthVLM produces images at a resolution of 1024x1024, effectively addressing the low-resolution issues present in existing datasets."

### Mechanism 2
- Claim: Combining CLIPScore and SSIM provides a robust metric for selecting high-quality, well-aligned image-caption pairs.
- Mechanism: CLIPScore measures semantic alignment between images and captions, while SSIM measures image quality preservation after resizing.
- Core assumption: CLIPScore and SSIM together capture both semantic alignment and visual quality in a complementary way.
- Evidence anchors:
  - [abstract] "ensures precise image-text alignment through combined CLIPScore and SSIM evaluation"
  - [section] "For a given image ð¼ with a resolution of 1024 Ã— 1024, we first resize it to 336 Ã— 336, then interpolate to restore it back to 1024 Ã— 1024. The SSIM value is then computed between the resized image and the original image to quantify the loss introduced by the resizing process."

### Mechanism 3
- Claim: High-quality synthetic data preserves language abilities while improving vision-language understanding.
- Mechanism: By using high-quality captions as input and ensuring precise alignment, the model learns both strong visual understanding and maintains language capabilities.
- Core assumption: The quality of the synthetic data directly translates to the quality of learned representations without compromising language understanding.
- Evidence anchors:
  - [abstract] "SynthVLM-7B and SynthVLM-13B attain SOTA performance on the MMLU benchmark, demonstrating that the high-quality SynthVLM-100K dataset preserves language abilities."
  - [section] "Furthermore, SynthVLM-7B and SynthVLM-13B attain SOTA performance on the MMLU benchmark, demonstrating that the high-quality SynthVLM-100K dataset preserves language abilities."

## Foundational Learning

- Concept: Diffusion models
  - Why needed here: Understanding how diffusion models work is crucial for grasping why they can generate high-quality images for the training pipeline.
  - Quick check question: What is the fundamental difference between diffusion models and traditional GANs in image generation?

- Concept: Contrastive learning (CLIP)
  - Why needed here: CLIPScore is used to measure image-text alignment, so understanding contrastive learning is essential for understanding this metric.
  - Quick check question: How does CLIP learn to align images and text in the same embedding space?

- Concept: Image quality metrics (SSIM)
  - Why needed here: SSIM is used to ensure image quality is preserved during preprocessing, which is critical for maintaining data quality.
  - Quick check question: What aspects of image quality does SSIM measure that other metrics like PSNR might miss?

## Architecture Onboarding

- Component map: Caption curation -> Image generation -> Quality evaluation -> Model training

- Critical path: Caption curation â†’ Image generation â†’ Quality evaluation â†’ Model training

- Design tradeoffs:
  - Using synthetic data eliminates privacy concerns but requires expensive generation
  - Smaller dataset size (100K) vs. traditional datasets (500K+) balances efficiency with potential coverage
  - High resolution generation (1024x1024) vs. computational cost

- Failure signatures:
  - Low CLIPScore values indicate poor semantic alignment
  - Low SSIM scores suggest quality degradation during preprocessing
  - Model performance drops on real-world benchmarks indicate synthetic data limitations

- First 3 experiments:
  1. Generate a small batch (1K) of images and manually inspect quality and alignment
  2. Compare CLIPScore distributions between synthetic and real datasets
  3. Train a small model on the synthetic data and evaluate on a held-out vision-language benchmark

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Quality and diversity of synthetic images may degrade over time as diffusion models face distribution shifts
- 100K dataset size, while effective, may not generalize to all vision-language tasks requiring rare object categories
- Reliance on CLIPScore for both caption curation and image selection introduces potential bias if CLIP's alignment capabilities degrade
- Computational cost of generating high-resolution images (1024x1024) remains substantial

## Confidence

- High Confidence: The claim that SynthVLM-7B/13B outperform LLaVA-558K on vision-language benchmarks is well-supported by the experimental results across multiple datasets (MMVet, SQA, MMLU).
- Medium Confidence: The assertion that diffusion models generate inherently higher-quality images than real-world data is supported by the quality metrics but lacks direct comparison studies with other synthetic data approaches.
- Medium Confidence: The claim that 100K synthetic pairs can replace 500K+ real pairs is supported by benchmark results but may not hold for all downstream tasks or emerging vision-language applications.

## Next Checks

1. **Cross-Domain Generalization**: Test SynthVLM models on specialized vision-language benchmarks (e.g., medical imaging, satellite imagery) to assess whether the synthetic data's limited scope affects performance on niche domains.

2. **Long-Term Quality Assessment**: Generate synthetic images using newer diffusion models (e.g., SD3, Midjourney v6) and evaluate whether CLIPScore and SSIM metrics maintain their correlation with human judgment over time.

3. **Efficiency vs. Coverage Trade-off**: Conduct ablation studies varying dataset size (50K, 200K, 500K) to determine the optimal balance between computational efficiency and task coverage for different vision-language applications.