---
ver: rpa2
title: Improving Cross-lingual Representation for Semantic Retrieval with Code-switching
arxiv_id: '2403.01364'
source_url: https://arxiv.org/abs/2403.01364
tags:
- language
- cross-lingual
- code-switching
- sentence
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses cross-lingual semantic retrieval (SR) for FAQ
  systems in e-commerce, where existing methods do not leverage downstream task features
  or handle code-switching well. The authors propose an alternative cross-lingual
  pre-trained model (PTM) that uses code-switching and incorporates a similarity loss
  during continual pre-training to better align queries and labels.
---

# Improving Cross-lingual Representation for Semantic Retrieval with Code-switching

## Quick Facts
- **arXiv ID**: 2403.01364
- **Source URL**: https://arxiv.org/abs/2403.01364
- **Reference count**: 31
- **Primary result**: Code-switched continual pre-training with similarity loss improves Recall@30 on cross-lingual FAQ retrieval by up to 5 percentage points

## Executive Summary
This paper addresses cross-lingual semantic retrieval (SR) for FAQ systems in e-commerce, where existing methods struggle with code-switched user queries and lack task-specific pre-training. The authors propose a novel approach that generates code-switched data using bilingual dictionaries and trains a model with alternating language modeling (ALM) plus a similarity loss during continual pre-training. Experiments on three business corpora (AliExpress, LAZADA, DARAZ) and four open datasets show consistent improvements over state-of-the-art methods, with Recall@30 scores exceeding baselines by up to 5 percentage points.

## Method Summary
The method involves generating code-switched data by replacing tokens in queries and labels with bilingual dictionary entries from frequently used languages on Alibaba's cross-border e-commerce platform. A cross-lingual PTM (e.g., mBERT or XLM-R) is then pre-trained on this code-switched corpus using a weighted combination of XMLM loss (masked token prediction) and similarity loss (cosine similarity ranking). The pre-trained model is fine-tuned on each language's SR training set for 3 epochs, with hyperparameters including λ=0.2 for similarity loss weight, 10% code-switching rate, and 1e-5 learning rate.

## Key Results
- Recall@30 improvements of 5.5% on LAZADA and 5.8% on DARAZ over baselines
- Consistent gains across three business corpora and four open datasets
- Enhanced robustness on open datasets like AskUbuntu and Tatoeba
- Improved semantic textual similarity performance measured by Spearman's rank correlation

## Why This Works (Mechanism)

### Mechanism 1
Providing similarity loss during continual pre-training gives the PTM explicit SR signals that it otherwise misses. The model is trained to pull the vector of a query close to its matching label and push it away from other labels in the same batch, directly mimicking the downstream retrieval objective.

### Mechanism 2
Code-switched training data better prepares the model for real user queries that mix languages. Random token-level replacement using bilingual dictionaries creates input resembling the e-commerce domain's actual mix of languages (e.g., English words in Malay or Thai queries).

### Mechanism 3
Alternating Language Modeling (ALM) on code-switched data improves cross-lingual alignment compared to vanilla MLM. MLM predicts masked tokens in sentences where some tokens are replaced with target-language equivalents, forcing the model to align representations across languages at the token level.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - Why needed here: Baseline cross-lingual PTM training relies on MLM; understanding it is key to extending to XMLM
  - Quick check question: In MLM, how is the loss computed for a masked token?

- **Concept: Cosine Similarity for Retrieval**
  - Why needed here: Both training (similarity loss) and inference use cosine similarity to rank query-label pairs
  - Quick check question: What is the formula for cosine similarity between two vectors?

- **Concept: Bilingual Dictionaries for Code-Switching**
  - Why needed here: Code-switched data is generated by token-level replacement using these dictionaries
  - Quick check question: How would you handle multi-word expressions when replacing tokens?

## Architecture Onboarding

- **Component map**: Code-switched data generation -> Continual pre-training (XMLM + similarity loss) -> Fine-tuning on SR corpus
- **Critical path**: Code-switched data generation → Continual pre-training (XMLM + similarity loss) → Fine-tuning on SR corpus
- **Design tradeoffs**: Code-switching rate (higher rates improve realism but may degrade fluency), λ (XMLM weight) (controls balance between token prediction and similarity learning), batch size (larger batches improve negative sampling for similarity loss)
- **Failure signatures**: Overfitting to code-switched training data (poor generalization), collapse of similarity loss (all vectors become identical), token replacement introducing invalid words (low recall)
- **First 3 experiments**: 1) Ablation: Train with XMLM only, no similarity loss. 2) Ablation: Train with similarity loss but no code-switching. 3) Hyperparameter sweep: Vary λ and code-switching rate to find optimal configuration

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Data generation realism: The code-switched data is generated via bilingual dictionary token replacement, which may not capture natural code-switching patterns (e.g., multi-word expressions, syntactic constraints, or context-dependent word choice).
- Downstream task generalization: Improvements on Tatoeba are modest, suggesting gains may be task-specific or domain-dependent rather than a general improvement in cross-lingual alignment.
- Hyperparameter sensitivity: The effectiveness of the similarity loss and code-switching rate depends on careful tuning of λ and the replacement rate, but sensitivity analysis is not reported.

## Confidence
- **High confidence**: The claim that incorporating similarity loss during continual pre-training improves Recall@30 on business corpora is well-supported by experimental results.
- **Medium confidence**: The claim that code-switching improves robustness to mixed-language queries is supported by performance gains, but the synthetic generation method may not fully capture real-world code-switching.
- **Low confidence**: The claim that XMLM + similarity loss is the first application of code-switching to cross-lingual SR is not rigorously substantiated.

## Next Checks
1. **Fluency and realism check**: Sample 100 generated code-switched queries from the LAZADA corpus and have native speakers rate them for naturalness and code-switching authenticity. Compare distributions to actual user queries to quantify generation fidelity.

2. **Cross-task generalization check**: Evaluate the pre-trained model on a non-FAQ retrieval task (e.g., cross-lingual document retrieval from TREC or CLEF) to test whether similarity loss improvements transfer beyond FAQ-style semantic retrieval.

3. **Ablation on code-switching rate**: Train models with code-switching rates of 5%, 10%, 15%, and 20% on the AliExpress corpus and plot Recall@30 vs. rate to identify the optimal rate and test whether gains are robust to hyperparameter variation.