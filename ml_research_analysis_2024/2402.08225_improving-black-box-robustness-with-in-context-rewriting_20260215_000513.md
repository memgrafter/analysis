---
ver: rpa2
title: Improving Black-box Robustness with In-Context Rewriting
arxiv_id: '2402.08225'
source_url: https://arxiv.org/abs/2402.08225
tags:
- augmentation
- performance
- task
- llm-tta
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes LLM-TTA, a black-box test-time augmentation
  method that uses large language models (LLMs) to generate semantic-preserving text
  augmentations for improving out-of-distribution (OOD) robustness in text classification.
  The method employs two LLM-based augmentation strategies: zero-shot paraphrasing
  and in-context rewriting (ICR), where ICR prompts the LLM to rewrite test inputs
  to resemble in-distribution (ID) exemplars.'
---

# Improving Black-box Robustness with In-Context Rewriting

## Quick Facts
- **arXiv ID:** 2402.08225
- **Source URL:** https://arxiv.org/abs/2402.08225
- **Reference count:** 40
- **One-line primary result:** LLM-TTA improves BERT's average OOD accuracy by 4.30 percentage points without regressing ID performance.

## Executive Summary
This paper proposes LLM-TTA, a black-box test-time augmentation method that uses large language models (LLMs) to generate semantic-preserving text augmentations for improving out-of-distribution (OOD) robustness in text classification. The method employs two LLM-based augmentation strategies: zero-shot paraphrasing and in-context rewriting (ICR), where ICR prompts the LLM to rewrite test inputs to resemble in-distribution (ID) exemplars. LLM-TTA significantly improves BERT and T5 models' OOD accuracy across sentiment, toxicity, and news topic classification tasks, with BERT's average OOD accuracy improving by 4.30 percentage points without regressing ID performance. The method is effective across low- and high-resource settings and does not require OOD labels or model access. An entropy-based selective augmentation strategy further reduces the average number of LLM augmentations by 57.76% while maintaining performance gains.

## Method Summary
LLM-TTA uses LLMs (specifically Stable Beluga 2) to generate semantic-preserving augmentations for test-time augmentation in text classification. The method has two main strategies: zero-shot paraphrasing where the LLM paraphrases inputs without exemplars, and in-context rewriting (ICR) where the LLM rewrites inputs to match provided in-distribution exemplars. For selective augmentation, the method uses entropy thresholds to determine which test inputs should be augmented, reducing computational cost by only augmenting uncertain predictions. The approach is black-box, requiring no model access or retraining, and works across different task model sizes and resource settings.

## Key Results
- BERT's average OOD accuracy improves by 4.30 percentage points using ICR without ID performance regression
- LLM-TTA outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks
- Entropy-based selective augmentation reduces LLM augmentations by 57.76% on average while maintaining performance gains
- ICR is the best-performing augmentation function, with BERT's OOD performance improving by an average of 4.30 percentage points

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-generated augmentations outperform conventional word-level or translation-based augmentations because they produce more semantically faithful and diverse paraphrases.
- **Mechanism:** LLMs leverage large-scale pretraining to understand subtle contextual cues, allowing them to rephrase text while preserving meaning and introducing stylistic diversity that matches in-distribution patterns.
- **Core assumption:** The LLM's pretraining corpus and instruction tuning provide it with sufficient semantic understanding to generate faithful augmentations across diverse tasks.
- **Evidence anchors:**
  - [abstract] "LLM-TTA outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks"
  - [section 3.2] "We hypothesize that the LLM-generated augmentations will outperform conventional augmentation functions"
  - [corpus] Weak - no direct citations supporting this specific mechanism

### Mechanism 2
- **Claim:** In-Context Rewriting (ICR) improves OOD robustness by rewriting test inputs to resemble in-distribution exemplars, leveraging the LLM's in-context learning ability.
- **Mechanism:** ICR prompts the LLM to rewrite OOD inputs to match the style and structure of provided ID exemplars, effectively translating OOD inputs into ID-like forms the model was trained on.
- **Core assumption:** The LLM can infer distributional differences from in-context exemplars and generate rewrites that preserve semantics while matching ID style.
- **Evidence anchors:**
  - [section 3.2] "ICR, where the LLM rewrites the input to be more like a set of ID exemplars provided in the prompt"
  - [section 5.1] "ICR is the best-performing augmentation function, with BERT's OOD performance improving by an average of 4.30 percentage points"
  - [corpus] Weak - no direct citations supporting ICR effectiveness

### Mechanism 3
- **Claim:** Entropy-based selective augmentation reduces computational cost by only augmenting test inputs where the model is uncertain, based on the entropy of the predicted class distribution.
- **Mechanism:** High entropy indicates model uncertainty, suggesting the original prediction might be unreliable and could benefit from TTA. Low entropy inputs are likely already correct, making augmentation unnecessary.
- **Core assumption:** There is a correlation between prediction entropy and the likelihood of prediction error, making entropy a useful proxy for identifying inputs that need augmentation.
- **Evidence anchors:**
  - [section 3.3] "We explore whether only augmenting test inputs in which the model is uncertain in its prediction"
  - [section 5.3] "Lower entropy has been observed to be correlated with correct predictions in machine learning models"
  - [corpus] Weak - cites Grandvalet & Bengio (2004) but no direct citations for this specific application

## Foundational Learning

- **Concept:** Test-Time Augmentation (TTA)
  - Why needed here: TTA is the core technique being enhanced with LLM-generated augmentations; understanding its mechanics is crucial for implementing LLM-TTA.
  - Quick check question: What are the three main steps in TTA, and how does aggregation combine predictions from augmented inputs?

- **Concept:** In-Context Learning
  - Why needed here: ICR relies on the LLM's ability to learn from provided exemplars within the prompt; understanding this mechanism is essential for crafting effective ICR prompts.
  - Quick check question: How does in-context learning differ from fine-tuning, and what are the limitations of this approach?

- **Concept:** Entropy as Uncertainty Measure
  - Why needed here: Entropy-based selective augmentation uses entropy to identify uncertain predictions; understanding entropy in classification contexts is necessary for implementing this efficiency mechanism.
  - Quick check question: How is entropy calculated for a probability distribution, and why is higher entropy associated with greater uncertainty?

## Architecture Onboarding

- **Component map:** Task model (BERT/T5/Falcon) -> LLM (Stable Beluga 2) -> Aggregation function -> Entropy threshold calculator

- **Critical path:**
  1. Receive test input
  2. Calculate prediction entropy (for selective augmentation)
  3. If above threshold, generate N augmentations using LLM
  4. Get predictions for original input + augmentations
  5. Aggregate predictions to final output

- **Design tradeoffs:**
  - More augmentations generally improve performance but increase cost
  - Selective augmentation reduces cost but may miss some beneficial augmentations
  - ICR requires ID exemplars but can outperform zero-shot paraphrasing
  - Larger task models benefit less from TTA, suggesting diminishing returns

- **Failure signatures:**
  - Performance regression on ID data (over-augmentation)
  - No improvement on OOD data (poor augmentation quality or insufficient diversity)
  - High computational cost without commensurate gains (ineffective selective augmentation)
  - Task model predictions remain unchanged across augmentations (model is invariant to perturbations)

- **First 3 experiments:**
  1. Run LLM-TTA with ICR on a small subset of OOD test data to verify basic functionality and observe performance improvements
  2. Test different numbers of augmentations (1, 2, 4) to find the optimal tradeoff between performance and cost
  3. Implement entropy-based selective augmentation and compare performance and augmentation rate against default (all inputs augmented)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do model architecture and pretraining objectives influence LLM-TTA effectiveness beyond parameter count?
- **Basis in paper:** [inferred] The authors note that BERT (smaller model) benefits most from TTA while larger models like T5 and Falcon benefit less, suggesting factors beyond parameter count are at play.
- **Why unresolved:** The paper attempts to isolate parameter count using Pythia models but finds no clear relationship. The authors conjecture that pretraining objective and corpus size may play larger roles but don't test these hypotheses.
- **What evidence would resolve it:** Systematic ablation studies varying pretraining objectives (masked LM vs causal LM) and corpus sizes while keeping architecture constant, then measuring TTA effectiveness.

### Open Question 2
- **Question:** What makes certain classes more susceptible to TTA-induced prediction changes than others?
- **Basis in paper:** [explicit] Figure 5 shows variance across classes, with some like positive/neutral sentiment gaining more "new correct" predictions while others like negative sentiment have more "new mistakes."
- **Why unresolved:** The authors observe the variance but don't investigate underlying causes. They note that TTA can hurt performance for some classes while improving overall performance.
- **What evidence would resolve it:** Detailed error analysis comparing characteristics of instances that changed predictions vs those that didn't, examining factors like input length, vocabulary overlap with ID data, or semantic complexity.

### Open Question 3
- **Question:** How can we more effectively predict which test inputs will benefit from augmentation to optimize selective augmentation strategies?
- **Basis in paper:** [explicit] The authors explore entropy-based selective augmentation but note it underperforms full augmentation while reducing computation, suggesting room for improvement.
- **Why unresolved:** The paper only explores entropy as a heuristic and acknowledges that more work is needed to better classify which examples will benefit from augmentation.
- **What evidence would resolve it:** Development and evaluation of alternative selection criteria (e.g., embedding distance to ID distribution, model uncertainty across multiple heads, or learned classifiers) compared against entropy-based selection.

## Limitations

- Black-box evaluation effectiveness remains theoretical as the study uses accessible task models
- ICR's dependence on ID exemplars introduces practical constraints with limited guidance on exemplar selection
- Limited experimental scope to three classification tasks may not generalize to other domains
- Computational cost analysis focuses on augmentation count rather than absolute resource consumption

## Confidence

**High Confidence:** The core finding that LLM-generated augmentations (particularly ICR) outperform conventional augmentation methods across multiple tasks and models is well-supported by the experimental results.

**Medium Confidence:** The entropy-based selective augmentation mechanism's effectiveness is moderately supported but shows significant variability across tasks, requiring careful calibration for new domains.

**Low Confidence:** The generalizability of results to other task types and the scalability to larger, more complex models or different language pairs remains uncertain due to limited experimental scope.

## Next Checks

1. **Ablation Study on Exemplar Quality:** Systematically vary the number and diversity of in-distribution exemplars provided to ICR to quantify the relationship between exemplar quality and OOD performance improvements.

2. **Cross-Domain Transferability:** Apply LLM-TTA to a new task domain (e.g., medical text classification or legal document analysis) to test whether the observed improvements generalize beyond the tested sentiment/toxicity/news domains.

3. **Resource Consumption Benchmarking:** Measure actual GPU/CPU time and API costs for different augmentation strategies across various input lengths to provide concrete deployment guidance for practitioners considering LLM-TTA implementation.