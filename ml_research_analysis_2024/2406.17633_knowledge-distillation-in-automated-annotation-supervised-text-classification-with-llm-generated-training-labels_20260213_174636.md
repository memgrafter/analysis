---
ver: rpa2
title: 'Knowledge Distillation in Automated Annotation: Supervised Text Classification
  with LLM-Generated Training Labels'
arxiv_id: '2406.17633'
source_url: https://arxiv.org/abs/2406.17633
tags:
- data
- labels
- gpt-4
- human
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that supervised text classifiers fine-tuned
  with GPT-4-generated labels perform comparably to those trained on human-annotated
  labels across 14 real-world classification tasks from computational social science
  research. The median F1 performance gap between models trained on GPT-4 labels and
  human labels is only 0.039, while models trained on GPT-4 labels achieve median
  F1 scores nearly identical to GPT-4 few-shot models (difference of 0.006).
---

# Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels

## Quick Facts
- arXiv ID: 2406.17633
- Source URL: https://arxiv.org/abs/2406.17633
- Authors: Nicholas Pangakis; Samuel Wolken
- Reference count: 40
- One-line primary result: Supervised text classifiers fine-tuned with GPT-4-generated labels perform comparably to those trained on human-annotated labels across 14 real-world classification tasks.

## Executive Summary
This study demonstrates that supervised text classifiers fine-tuned with GPT-4-generated labels perform comparably to those trained on human-annotated labels across 14 real-world classification tasks from computational social science research. The median F1 performance gap between models trained on GPT-4 labels and human labels is only 0.039, while models trained on GPT-4 labels achieve median F1 scores nearly identical to GPT-4 few-shot models (difference of 0.006). The approach offers a fast, efficient, and cost-effective alternative to human annotation, though validation against human labels remains essential. GPT-4 few-shot and GPT-trained models show significantly higher recall but lower precision compared to human-trained models.

## Method Summary
The study evaluates supervised text classification performance when fine-tuned with LLM-generated labels versus human-labeled data. Using 14 text classification tasks from CSS research, the authors validate GPT-4 few-shot performance on 250 samples, then generate 1000 GPT-4 labels per task. They fine-tune BERT-family models (BERT, RoBERTa, DistilBERT) and XLNet/Mistral-7B with varying training sizes using grid search over learning rate, batch size, and epochs. All models are evaluated on held-out test sets using accuracy, F1, precision, and recall. The primary comparison is between models trained on 250 human labels, 1000 human labels, 1000 GPT-4 labels, and GPT-4 few-shot performance.

## Key Results
- Median F1 performance gap between models trained on GPT-4 labels and human labels is only 0.039
- Models trained on GPT-4 labels achieve median F1 scores nearly identical to GPT-4 few-shot models (difference of 0.006)
- GPT-4 few-shot and GPT-trained models show significantly higher recall but lower precision compared to human-trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised classifiers fine-tuned on GPT-4-generated labels achieve comparable performance to those trained on human labels.
- Mechanism: Knowledge distillation from a large "teacher" LLM (GPT-4) to smaller "student" models, where the student models learn to replicate the teacher's classification decisions.
- Core assumption: The teacher LLM's labels capture the same underlying classification criteria as human annotations, and the student models can effectively learn from these surrogate labels.
- Evidence anchors:
  - [abstract]: "Our findings indicate that supervised classification models fine-tuned on LLM-generated labels perform comparably to models fine-tuned with labels from human annotators."
  - [section]: "Across all 14 classification tasks, DistilBERT and BERT fine-tuned on 1000 human-samples are the highest performing models... Not far behind, however, is the GPT-4 few-shot model (0.592 median F1) and BERT fine-tuned on 1000 GPT-labeled samples (0.586 median F1)."
  - [corpus]: Weak evidence. The corpus mentions related work on LLM-generated labels but does not directly address the knowledge distillation mechanism or performance comparison with human labels.
- Break condition: The teacher LLM's labels do not align with human annotation criteria, or the student models cannot effectively learn from the surrogate labels.

### Mechanism 2
- Claim: Models trained on surrogate labels from GPT-4 perform similarly to labels from GPT-4 with few-shot in-context learning.
- Mechanism: The supervised models trained on GPT-4 labels approximate the few-shot model's performance, offering a cost-effective alternative by avoiding per-sample queries to the teacher LLM.
- Core assumption: The student models can capture the few-shot model's decision-making process through training on its generated labels.
- Evidence anchors:
  - [abstract]: "models trained on surrogate labels from GPT-4 demonstrate very similar validation performance as labels from GPT-4 with few-shot in-context learning."
  - [section]: "models trained on surrogate labels from GPT-4 demonstrate very similar validation performance as labels from GPT-4 with few-shot in-context learning."
  - [corpus]: Weak evidence. The corpus includes related work on few-shot learning but does not specifically address the comparison between few-shot models and models trained on their generated labels.
- Break condition: The student models cannot effectively approximate the few-shot model's performance, or the cost savings are negligible.

### Mechanism 3
- Claim: GPT-4 few-shot models and supervised models trained on GPT-4 generated labels produce high recall but lower precision compared to human-trained models.
- Mechanism: The teacher LLM's labeling strategy prioritizes recall, leading to higher recall but lower precision in both the few-shot and student models.
- Core assumption: The teacher LLM's labeling decisions favor recall over precision, and this bias is transferred to the student models during training.
- Evidence anchors:
  - [abstract]: "GPT-4 few-shot and GPT-trained models show significantly higher recall but lower precision compared to human-trained models."
  - [section]: "GPT-4 few-shot (0.8 median recall) as well as DistilBERT and BERT fine-tuned on GPT-labels (both with 0.746 median recall) achieve significantly better median recall than any model fine-tuned with human labels."
  - [corpus]: Weak evidence. The corpus does not provide direct evidence for the recall-precision tradeoff in LLM-generated labels.
- Break condition: The recall-precision tradeoff is not observed in the student models, or the tradeoff is not beneficial for the target application.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Understanding the transfer of knowledge from a large "teacher" model (GPT-4) to smaller "student" models is crucial for grasping the core mechanism of this approach.
  - Quick check question: What is the main advantage of using knowledge distillation in this context?

- Concept: Few-shot Learning
  - Why needed here: Recognizing how LLMs can perform classification tasks with minimal labeled examples is essential for understanding the alternative approach (few-shot in-context learning) compared in this study.
  - Quick check question: How does few-shot learning differ from traditional supervised learning?

- Concept: Precision-Recall Tradeoff
  - Why needed here: Understanding the balance between precision and recall is important for interpreting the results and determining the suitability of LLM-generated labels for different tasks.
  - Quick check question: When would a high recall but lower precision be preferable to high precision but lower recall?

## Architecture Onboarding

- Component map:
  - Teacher LLM (GPT-4) -> Student Models (BERT, RoBERTa, DistilBERT, XLNet, Mistral-7B) -> Human-labeled data (ground truth) -> Validation data

- Critical path:
  1. Validate the teacher LLM's performance on a small subset of human-labeled data.
  2. Generate training labels for the student models using the validated teacher LLM.
  3. Fine-tune the student models on the teacher's labels.
  4. Evaluate the student models' performance on a held-out set of human-labeled data.

- Design tradeoffs:
  - Cost vs. Performance: Using a smaller, cheaper student model may sacrifice some performance compared to the teacher LLM, but offers significant cost savings.
  - Recall vs. Precision: The teacher LLM's labeling strategy may prioritize recall, leading to higher recall but lower precision in the student models.

- Failure signatures:
  - Poor performance on validation data: Indicates issues with the teacher LLM's labels or the student models' ability to learn from them.
  - High variance in performance across tasks: Suggests that the teacher LLM's performance is inconsistent across different classification tasks.

- First 3 experiments:
  1. Validate the teacher LLM's performance on a small subset of human-labeled data for each task.
  2. Generate training labels for the student models using the validated teacher LLM.
  3. Fine-tune a student model (e.g., BERT) on the teacher's labels and evaluate its performance on a held-out set of human-labeled data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the results generalize to languages other than English and to non-text classification tasks (e.g., image or audio classification)?
- Basis in paper: [inferred] The paper focuses exclusively on English-language text classification tasks from computational social science research, with no exploration of multilingual or multimodal applications.
- Why unresolved: The study's corpus is limited to English-language CSS applications, and the methodology is specifically designed for text data. The performance of LLM-generated labels and knowledge distillation approaches for other languages or data modalities remains untested.
- What evidence would resolve it: Replicating the study's methodology across diverse languages and data types (images, audio, video) would reveal whether the comparable performance between LLM-generated and human labels extends beyond English text classification.

### Open Question 2
- Question: How does the performance of knowledge distillation from LLMs vary with different model architectures (e.g., decoder-only vs. encoder-decoder) and parameter scales beyond those tested?
- Basis in paper: [explicit] The paper tests several BERT-family models and two additional architectures (XLNet, Mistral-7B), but acknowledges this is a limited sample and compares performance across different model sizes.
- Why unresolved: The study only examines a subset of available model architectures and parameter scales. The relative effectiveness of knowledge distillation from GPT-4 could differ significantly for other model families or scales not tested in the experiments.
- What evidence would resolve it: Systematic testing of knowledge distillation across a broader range of model architectures (including decoder-only models like GPT variants) and parameter scales would reveal the robustness and limitations of the approach across the model landscape.

### Open Question 3
- Question: What is the optimal balance between noise tolerance and data quality when filtering GPT-generated labels using consistency scores?
- Basis in paper: [explicit] The ablation experiments show that removing low-consistency (noisy) labels slightly reduces F1 scores (by 0.004 on average), suggesting the supervised models are fairly robust to noise, but the optimal filtering threshold is not explored.
- Why unresolved: The study uses a binary consistency threshold (1.0 vs. <1.0) and shows minimal performance differences, but does not explore whether partial noise tolerance or different consistency thresholds might yield better results for specific tasks or model types.
- What evidence would resolve it: Systematic experimentation with varying consistency thresholds and analysis of the trade-off between training data volume and label quality would identify optimal filtering strategies for different classification scenarios.

## Limitations
- Results are limited to English-language text classification tasks from computational social science research and may not generalize to other domains or languages
- The study does not address potential bias propagation from GPT-4's training data to the downstream models
- Performance may vary with different LLM teachers, prompting strategies, or model architectures not tested in the experiments

## Confidence

**High Confidence**: The finding that supervised models trained on GPT-4 labels perform comparably to human-labeled models, with a median F1 gap of only 0.039. This is directly supported by the comparative F1 scores presented in the results section and is the central empirical finding of the study.

**Medium Confidence**: The claim that models trained on GPT-4 labels achieve performance nearly identical to GPT-4 few-shot models (difference of 0.006). While the numerical difference is small, this finding depends on the specific tasks and prompting strategies used, and may not hold across different domains or LLM versions.

**Medium Confidence**: The observation that GPT-4 few-shot and GPT-trained models show higher recall but lower precision. The evidence is based on median recall scores (0.8 for few-shot, 0.746 for GPT-trained) compared to human-trained models, but the study does not provide a systematic analysis of when this tradeoff is beneficial or detrimental.

## Next Checks

1. **Cross-Model Validation**: Replicate the study using different LLM teachers (e.g., Claude, Llama) to determine whether the observed performance patterns are specific to GPT-4 or represent a more general phenomenon in LLM-generated labels.

2. **Bias and Domain Transfer Analysis**: Conduct a systematic examination of potential bias propagation from GPT-4's training data and test the models on out-of-domain classification tasks to assess generalizability beyond CSS research contexts.

3. **Longitudinal Performance Monitoring**: Implement a framework to track model performance over time as GPT-4's knowledge base updates, determining whether the knowledge distillation approach maintains its effectiveness across model versions.