---
ver: rpa2
title: 'Rethinking the Relationship between Recurrent and Non-Recurrent Neural Networks:
  A Study in Sparsity'
arxiv_id: '2404.00880'
source_url: https://arxiv.org/abs/2404.00880
tags:
- dynamical
- neural
- function
- systems
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a unified perspective showing that both
  Recurrent Neural Networks (RNNs) and Multi-Layer Perceptrons (MLPs) can be represented
  as iterative maps - discrete dynamical systems. The authors introduce a generalized
  notation (Sequential2D) to express these networks as block non-linear functions.
---

# Rethinking the Relationship between Recurrent and Non-Recurrent Neural Networks: A Study in Sparsity

## Quick Facts
- arXiv ID: 2404.00880
- Source URL: https://arxiv.org/abs/2404.00880
- Reference count: 7
- Key outcome: This paper establishes a unified perspective showing that both Recurrent Neural Networks (RNNs) and Multi-Layer Perceptrons (MLPs) can be represented as iterative maps - discrete dynamical systems.

## Executive Summary
This paper presents a unified mathematical framework that reveals the fundamental equivalence between recurrent and non-recurrent neural networks. By representing both RNNs and MLPs as iterative maps, the authors demonstrate that their apparent differences are primarily due to sparsity patterns rather than computational capability. The framework, called Sequential2D, provides a generalized notation for expressing these networks as block non-linear functions, showing that traditional architectural distinctions are largely arbitrary. Numerical experiments on MNIST classification reveal that the placement of trainable parameters has surprisingly little impact on training performance compared to the total number of parameters.

## Method Summary
The authors introduce a generalized notation (Sequential2D) to express both RNNs and MLPs as block non-linear functions in iterative maps. They implement a four-layer MLP with layer sizes [2500, 500, 200, 100, 10] and train it using Adam optimizer (learning rate 1e-3, batch size 64) for 100 epochs on MNIST. The key comparison is between this standard architecture and randomized dynamical systems with identical parameter counts but different sparsity patterns. The MNIST dataset is preprocessed with resizing to 50x50 pixels, normalization with N(0.1307, 0.3081), and optional transformations like random erasing and perspective transformations.

## Key Results
- Both RNNs and MLPs can be represented as iterative maps with identical block non-linear structures
- The apparent differences between RNNs and MLPs are primarily due to their sparsity patterns
- Random allocation of trainable parameters performs similarly to traditional layer-wise arrangements for MNIST classification
- Final testing accuracy was virtually indistinguishable at 92.7% between standard and randomized architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Both RNNs and MLPs can be represented as iterative maps with identical block non-linear structures.
- Mechanism: The paper constructs a unified notation (Sequential2D) where RNNs and MLPs are expressed as fixed block non-linear functions. For RNNs, the function is repeated identically across iterations, while for MLPs, different functions are applied in sequence.
- Core assumption: The mathematical equivalence holds regardless of specific activation functions and weight matrices used.
- Evidence anchors: [abstract] "many common neural network models, such as Recurrent Neural Networks (RNN), Multi-Layer Perceptrons (MLP), and even deep multi-layer transformers, can all be represented as iterative maps."

### Mechanism 2
- Claim: The difference between RNNs and MLPs is primarily due to their sparsity patterns, not their fundamental computational capability.
- Mechanism: RNNs have a sparse structure where only the state transition function is repeated, while MLPs have a sparse structure where only the weight matrices change between layers.
- Core assumption: Sparsity patterns are the primary determinant of network behavior.
- Evidence anchors: [abstract] "the apparent differences between RNNs and MLPs are largely due to their sparsity patterns"

### Mechanism 3
- Claim: Random allocation of trainable parameters performs similarly to traditional layer-wise arrangements for MNIST classification.
- Mechanism: Numerical experiments show that when comparing a standard MLP architecture with a "randomized" dynamical system that has the same number of trainable parameters but different sparsity patterns, both achieve similar performance on MNIST classification.
- Core assumption: The MNIST classification task is sufficiently simple that different architectural arrangements can achieve similar performance.
- Evidence anchors: [abstract] "Numerical experiments on MNIST classification reveal that the placement of trainable parameters (sparse vs. dense) has a surprisingly small impact on training performance compared to the total number of parameters"

## Foundational Learning

- Concept: Iterative maps and discrete dynamical systems
  - Why needed here: The paper's core insight is that both RNNs and MLPs can be understood as iterative maps, so understanding this mathematical framework is essential.
  - Quick check question: Can you explain the difference between an iterative map and a standard feedforward network in terms of state transitions?

- Concept: Function composition and block non-linear functions
  - Why needed here: The Sequential2D notation relies on composing functions and representing them as block structures, which is fundamental to understanding the unified representation.
  - Quick check question: How does the block non-linear function notation generalize standard matrix operations?

- Concept: Sparsity patterns and their computational implications
  - Why needed here: The paper argues that the difference between RNNs and MLPs is primarily due to sparsity patterns, so understanding how sparsity affects computation is crucial.
  - Quick check question: What are the computational advantages and disadvantages of sparse vs. dense weight matrices?

## Architecture Onboarding

- Component map: Sequential2D module -> block non-linear function structure -> iteration mechanism
- Critical path: 1) Understand the Sequential2D implementation, 2) Implement the block non-linear function structure, 3) Create the iteration mechanism, 4) Experiment with different sparsity patterns, 5) Test on classification tasks
- Design tradeoffs: The unified representation allows for more flexible architectures but may sacrifice some of the specific optimizations that have been developed for traditional RNNs and MLPs.
- Failure signatures: If the model fails to converge or performs poorly, check: 1) Incorrect implementation of the block non-linear function, 2) Inappropriate sparsity patterns for the task, 3) Insufficient parameter count for the complexity of the problem
- First 3 experiments:
  1. Implement a basic Sequential2D module and verify it can represent a standard MLP
  2. Modify the Sequential2D to implement an RNN and verify it produces the same results as a traditional RNN implementation
  3. Create a "randomized" architecture with the same number of parameters as the MLP but different sparsity patterns and test on MNIST classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the placement of trainable parameters in a neural network affect its training performance, especially when comparing traditional layer-wise architectures to randomized architectures?
- Basis in paper: [explicit] The paper shows that despite major structural changes, randomized and layered architectures have little training variation in MNIST classification experiments.
- Why unresolved: While the paper demonstrates similar training performance between different architectures, the underlying mechanisms driving this phenomenon remain unclear.
- What evidence would resolve it: Systematic experiments comparing various parameter placements and architectures across different datasets and tasks, along with analysis of the resulting learned representations.

### Open Question 2
- Question: What is the relationship between the number of iterations in a dynamical system representation of a neural network and its training performance?
- Basis in paper: [inferred] The paper notes that the number of iterations and the number of layers are related but distinct concepts, and that exploring this relationship provides fertile ground for exploration.
- Why unresolved: The paper demonstrates that the number of iterations can be decoupled from the architecture, but does not investigate how varying the number of iterations affects training performance for different architectures.
- What evidence would resolve it: Experiments training dynamical system representations with varying numbers of iterations and analyzing the resulting performance and convergence properties.

### Open Question 3
- Question: How can concepts from dynamical systems theory, such as Koopman operators, be leveraged to improve the training and analysis of neural networks?
- Basis in paper: [explicit] The paper suggests that Koopman operators are an active area of mathematical research that are precisely concerned with data-oriented dynamical systems, and that their proposed approach allows immediate leverage of this body of work.
- Why unresolved: While the paper proposes this connection, it does not explore the practical application of Koopman operators to neural network training or analysis.
- What evidence would resolve it: Developing and testing methods that incorporate Koopman operator theory into neural network training algorithms and analyzing their performance and interpretability.

## Limitations
- Empirical validation is confined to a single classification task (MNIST)
- Theoretical framework may not capture the full complexity of real-world applications
- Generalization to more complex tasks beyond MNIST classification remains untested

## Confidence
- High Confidence: The mathematical equivalence between RNNs and MLPs as iterative maps
- Medium Confidence: The claim that sparsity patterns are the primary determinant of network behavior
- Medium Confidence: The assertion that random parameter placement performs similarly to traditional architectures

## Next Checks
1. Replicate on CIFAR-10 to assess whether findings generalize to more complex image classification tasks
2. Test architectures with skip connections to evaluate whether sparsity pattern theory holds with different architectural constraints
3. Test the framework on a sequence modeling task (e.g., character-level language modeling) to determine if iterative map representation captures temporal dependencies