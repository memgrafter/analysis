---
ver: rpa2
title: Off-Policy Evaluation in Markov Decision Processes under Weak Distributional
  Overlap
arxiv_id: '2402.08201'
source_url: https://arxiv.org/abs/2402.08201
tags:
- overlap
- policy
- assumption
- distributional
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies off-policy evaluation in Markov decision processes
  when the target and behavior policies induce stationary state distributions with
  unbounded density ratios. The authors introduce a truncated doubly robust (TDR)
  estimator that controls instability by truncating these density ratios.
---

# Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap

## Quick Facts
- arXiv ID: 2402.08201
- Source URL: https://arxiv.org/abs/2402.08201
- Authors: Mohammad Mehrabi; Stefan Wager
- Reference count: 40
- Primary result: Truncated doubly robust estimator achieves optimal rates under weak distributional overlap when density ratio is square-integrable, and slower rates when not

## Executive Summary
This paper addresses off-policy evaluation in Markov decision processes when the target and behavior policies induce stationary state distributions with unbounded density ratios. The authors introduce a truncated doubly robust (TDR) estimator that controls instability by truncating these density ratios. Under weak distributional overlap assumptions, the TDR estimator achieves a 1/√T rate of convergence when the density ratio is square-integrable, matching previous results under strong overlap. When the density ratio is not square-integrable, TDR remains consistent but converges at a slower rate that depends on the tail decay of the density ratio. The authors prove that this rate is minimax optimal for a class of MDPs characterized by mixing conditions. Numerical experiments show that appropriate truncation significantly improves performance compared to standard doubly robust methods when strong distributional overlap does not hold.

## Method Summary
The paper proposes a truncated doubly robust (TDR) estimator for off-policy evaluation in MDPs. The method involves collecting trajectories under a behavior policy, estimating Q-functions and density ratios separately, then constructing the TDR estimator by truncating the density ratios at a level τt that depends on the trajectory length t. The truncation rate is chosen based on the weak distributional overlap exponent δ, with optimal rates being τt = t^(1/(1+δ)) or τt = T^(1/(1+δ)) for 0<δ<1. The paper also proposes Lepski's method for data-driven truncation selection when the overlap parameters are unknown. Theoretical analysis establishes convergence rates and minimax optimality under mixing conditions, while experiments demonstrate improved performance over standard doubly robust methods when strong overlap fails.

## Key Results
- TDR estimator achieves 1/√T convergence rate when density ratio is square-integrable (δ > 1), matching DR estimator under strong overlap
- When density ratio is not square-integrable (0 < δ ≤ 1), TDR converges at rate T^(-2δ/(1+δ)), which is minimax optimal
- Under mixing conditions with geometric mixing parameter t0 and policy overlap with parameter ζπ, weak distributional overlap holds with δ = 1/(ζπt0)
- Numerical experiments on two MDP setups show TDR significantly outperforms DR when strong overlap fails, with optimal truncation levels improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Truncation of density ratios stabilizes off-policy evaluation by removing extreme weights that would otherwise dominate the estimator's variance.
- Mechanism: When the stationary state distribution ratio ω(s) = p_e(s)/p_b(s) can be arbitrarily large, standard doubly robust estimators become unstable because importance weights can explode. By truncating these weights at a level τt, the estimator controls the variance contribution from rare but high-impact states, trading a small bias for a large variance reduction.
- Core assumption: The density ratio has bounded tail probabilities—specifically, the weak distributional overlap condition ensures that large values of ω(s) occur with sufficiently low probability.
- Evidence anchors:
  - [abstract] "In this paper, we re-visit the task of off-policy evaluation in MDPs under a weaker notion of distributional overlap, and introduce a class of truncated doubly robust (TDR) estimators which we find to perform well in this setting."
  - [section 3] "As discussed in the introduction, the main focus of this paper is in extending our methodological and formal understanding of doubly robust methods in MDPs to the case where strong distributional overlap may not hold, i.e., where ω(s) may grow unbounded."
- Break condition: If the tail decay of ω(s) is too slow (δ ≤ 0), truncation cannot control the bias-variance tradeoff, and the estimator may become inconsistent.

### Mechanism 2
- Claim: The TDR estimator recovers the 1/√T convergence rate when the density ratio is square-integrable (δ > 1).
- Mechanism: Under weak distributional overlap with δ > 1, the density ratio ω(s) has finite second moments. Truncation at appropriate rates (τt = t^α or τt = T^α with α ≥ 1/2) ensures that the bias introduced by truncation vanishes asymptotically while the variance remains controlled at the 1/T rate, matching the doubly robust estimator under strong overlap.
- Core assumption: The density ratio ω(s) is square-integrable under the behavior policy, and the truncation rate grows slowly enough to not introduce asymptotic bias.
- Evidence anchors:
  - [abstract] "When the distribution ratio of the target and data-collection policies is square-integrable (but not necessarily bounded), our approach recovers the large-sample behavior previously established under strong distributional overlap."
  - [section 3.1] "When δ > 1, i.e., when the density ratio is square integrable, our estimator recovers the rate of convergence of the DR estimator previously only established under strong distributional overlap."
- Break condition: If the density ratio is not square-integrable (δ ≤ 1), the estimator converges slower than 1/√T and cannot recover the optimal rate.

### Mechanism 3
- Claim: The TDR estimator is minimax rate-optimal under mixing conditions, even when strong overlap fails.
- Mechanism: When the MDP satisfies geometric mixing (Assumption 7) and policy overlap (Assumption 3), weak distributional overlap automatically holds with exponent δ = 1/(ζπt0). The TDR estimator's convergence rate T^(-2/(1+ζπt0)) matches the minimax lower bound for this class of MDPs, showing that truncation does not sacrifice statistical efficiency in this setting.
- Core assumption: The MDP is geometrically mixing with parameter t0, and policy overlap holds with parameter C_η = exp(ζπ).
- Evidence anchors:
  - [section 4] "Furthermore, it turns out that the polynomial exponent in the rate of convergence is optimal in a minimax sense... our results imply that, as long as we can estimate bQ_e and ω̂ at reasonably fast rates, off-policy estimation in MDPs under geometric mixing is no harder than off-policy evaluation in strongly regenerative MDPs."
  - [section 4.1] Theorem 4.1 establishes that under mixing and policy overlap, δ-weak distributional overlap holds with δ = 1/(ζπt0).
- Break condition: If the MDP lacks mixing properties or policy overlap, the connection to minimax optimality breaks down.

## Foundational Learning

- Concept: Stationary distributions in MDPs
  - Why needed here: The analysis relies on the ratio of stationary state distributions under different policies (ω(s) = p_e(s)/p_b(s)), which only exists when both policies induce stationary distributions.
  - Quick check question: What conditions guarantee that a policy in an MDP induces a unique stationary distribution?

- Concept: Mixing conditions for Markov chains
  - Why needed here: The paper uses mixing assumptions (ρ-mixing and geometric mixing) to control temporal dependence in the data and to derive distributional overlap from mixing properties.
  - Quick check question: How does geometric mixing with parameter t0 imply that the total variation distance between distributions contracts exponentially?

- Concept: Martingale central limit theorems
  - Why needed here: The asymptotic normality results for the TDR estimator rely on martingale central limit theorems applied to the truncated importance-weighted sums.
  - Quick check question: What are the Lindeberg conditions for a triangular martingale array to satisfy a central limit theorem?

## Architecture Onboarding

- Component map:
  - Data collection -> Q-function estimation -> Density ratio estimation -> Truncation selection -> TDR estimator -> Evaluation

- Critical path:
  1. Collect trajectory data under behavior policy
  2. Estimate Q-functions via temporal difference learning
  3. Estimate density ratios via moment matching
  4. Select truncation level (theory-guided or data-driven)
  5. Construct TDR estimator with chosen truncation
  6. Evaluate performance on test data

- Design tradeoffs:
  - Aggressive truncation (small τt) reduces variance but increases bias; conservative truncation (large τt) does the opposite
  - Theory-guided truncation requires knowledge of mixing and overlap parameters; data-driven truncation is more practical but may be suboptimal
  - Separate estimation of Q-functions and density ratios enables modular implementation but introduces additional estimation error

- Failure signatures:
  - If truncation is too aggressive: high bias, underestimation of true value
  - If truncation is too conservative: high variance, unstable estimates similar to standard DR
  - If mixing assumptions fail: weak distributional overlap may not hold, invalidating theoretical guarantees
  - If policy overlap fails: density ratios may be unbounded, causing estimator breakdown

- First 3 experiments:
  1. Replicate Experiment 1: MDP Setup 1 with known ω(·), compare TDR vs DR across trajectory lengths T ∈ {50, 600, 7200, 86400}
  2. Test Lepski's method: Apply Algorithm 1 to select truncation rate on the same MDP Setup 1 data
  3. Try different mixing parameters: Vary β in Setup 1 to change mixing rate and observe how truncation performance changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between weak distributional overlap and the tail decay rate of the density ratio function ω(s) under general MDP settings?
- Basis in paper: [explicit] The paper defines δ-weak distributional overlap and proves it holds under mixing conditions with δ = 1/(ζπt0), but doesn't provide a complete characterization of when and how this relationship holds.
- Why unresolved: The paper only establishes this relationship for a specific class of MDPs with geometric mixing, leaving open whether similar results hold for other mixing conditions or non-geometric settings.
- What evidence would resolve it: A general theorem showing how the tail decay rate of ω(s) relates to mixing conditions across different classes of MDPs, including counterexamples where such relationships fail.

### Open Question 2
- Question: How does the performance of the TDR estimator scale when both the Q-function and density ratio estimates are learned from data rather than being known?
- Basis in paper: [inferred] The paper validates performance with estimated models in experiments but doesn't provide theoretical guarantees for the fully data-driven setting where both components are learned simultaneously.
- Why unresolved: The theoretical analysis assumes oracle knowledge of either the Q-function or density ratio, and the experiments only show empirical performance without theoretical bounds.
- What evidence would resolve it: Theoretical analysis establishing error rates for the TDR estimator when both Q-function and density ratio are estimated from data, along with finite-sample bounds.

### Open Question 3
- Question: What is the optimal truncation strategy when the mixing rate and weak distributional overlap exponent are unknown?
- Basis in paper: [explicit] The paper proposes Lepski's method for data-driven truncation selection but doesn't characterize its optimality or compare it to other potential strategies.
- Why unresolved: While Lepski's method is shown to work empirically, there's no theoretical analysis of its optimality or comparison to alternative data-driven approaches.
- What evidence would resolve it: A minimax lower bound for data-driven truncation selection and comparison of Lepski's method to other adaptive strategies under various problem settings.

### Open Question 4
- Question: How does the TDR estimator perform in MDPs with non-stationary or time-varying dynamics?
- Basis in paper: [inferred] The paper assumes stationary MDP dynamics throughout, but many real-world applications involve non-stationary environments.
- Why unresolved: All theoretical results and experiments are based on stationary MDP assumptions, leaving the behavior in non-stationary settings unexplored.
- What evidence would resolve it: Extension of the TDR framework to non-stationary MDPs with corresponding theoretical guarantees and experimental validation.

### Open Question 5
- Question: What is the impact of policy overlap violations on the TDR estimator's performance, and how can it be addressed?
- Basis in paper: [explicit] The paper assumes policy overlap throughout but doesn't analyze what happens when this assumption is violated or how to handle such cases.
- Why unresolved: While the paper focuses on distributional overlap, it doesn't address the interaction between policy overlap violations and distributional overlap issues.
- What evidence would resolve it: Analysis of TDR performance under policy overlap violations and development of robust variants that can handle both types of overlap failures simultaneously.

## Limitations

- Theoretical guarantees depend critically on the validity of weak distributional overlap assumptions and accuracy of density ratio estimation
- Theoretical analysis assumes access to well-behaved estimators for Q-functions and density ratios, but doesn't fully characterize impact of estimation errors
- Numerical experiments are limited to two synthetic MDP setups and don't explore robustness to model misspecification or non-stationary environments

## Confidence

- **High confidence**: The mechanism by which truncation stabilizes off-policy evaluation through variance reduction is well-established and supported by both theory and experiments. The minimax lower bound matching the upper bound for the TDR estimator under mixing conditions is a strong theoretical result.
- **Medium confidence**: The recovery of 1/√T rates under square-integrability (δ > 1) is theoretically sound, but the practical benefits depend on the accuracy of density ratio estimation and the choice of truncation level. The connection between mixing conditions and weak distributional overlap is theoretically rigorous but may be sensitive to parameter choices.
- **Low confidence**: The empirical performance comparisons between TDR and standard DR estimators are based on limited experiments with synthetic data. The robustness of the method to more complex, real-world MDPs with partial observability or non-stationary dynamics remains unclear.

## Next Checks

1. **Robustness to estimation errors**: Conduct experiments where the Q-function and density ratio estimators are intentionally degraded to assess the impact on TDR performance. Compare against standard DR and other off-policy evaluation methods under varying levels of estimation error.

2. **Real-world applicability**: Apply the TDR estimator to a real-world RL benchmark (e.g., OpenAI Gym or MuJoCo) with known ground truth values. Evaluate performance across different policy overlaps and mixing conditions to test the practical relevance of the theoretical assumptions.

3. **Alternative truncation strategies**: Investigate data-driven truncation selection methods beyond Lepski's approach, such as cross-validation or online adaptation. Compare the statistical efficiency and computational cost of these methods against theory-guided truncation rates.