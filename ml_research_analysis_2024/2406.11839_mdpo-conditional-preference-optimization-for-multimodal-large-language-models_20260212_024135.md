---
ver: rpa2
title: 'mDPO: Conditional Preference Optimization for Multimodal Large Language Models'
arxiv_id: '2406.11839'
source_url: https://arxiv.org/abs/2406.11839
tags:
- preference
- mdpo
- multimodal
- arxiv
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multimodal large language
  model (MLLM) alignment, specifically the issue of unconditional preference in multimodal
  direct preference optimization (DPO). The authors propose mDPO, a method that tackles
  the problem of models overlooking the image condition during preference optimization.
---

# mDPO: Conditional Preference Optimization for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2406.11839
- Source URL: https://arxiv.org/abs/2406.11839
- Reference count: 12
- Primary result: mDPO significantly outperforms standard DPO in reducing hallucination rates on multimodal benchmarks

## Executive Summary
This paper addresses a critical limitation in multimodal direct preference optimization (DPO) where models tend to overlook image conditions during training, leading to hallucinations. The authors propose mDPO, a method that combines standard DPO with conditional preference optimization (CoPO) and anchored preference optimization (AncPO) to ensure models properly attend to both visual and language cues. Through extensive experiments on two MLLMs of different sizes and three benchmarks, mDPO demonstrates significant improvements in overall performance and hallucination reduction compared to standard DPO.

## Method Summary
MDPO addresses unconditional preference in multimodal DPO by introducing two key innovations: conditional preference optimization that forces the model to attend to image conditions through contrastive learning, and a reward anchor that prevents the likelihood of chosen responses from decreasing. The method uses LoRA adapters with specific hyperparameters (α=128, rank=64) and trains for 3 epochs with batch size 32 and learning rate 0.00001. The approach combines three losses: standard DPO, CoPO which contrasts responses conditioned on original vs corrupted images, and AncPO which adds a reward anchor to maintain positive rewards for chosen responses.

## Key Results
- On MMHalBench, mDPO improves overall score from 2.28 to 2.96 and reduces hallucination rate from 0.56 to 0.42 on the 3B model
- mDPO consistently outperforms standard DPO across different model scales (3B and 7B) and various question types
- The method shows effectiveness across different scales of preference data and demonstrates robustness to data size variations

## Why This Works (Mechanism)

### Mechanism 1
Standard DPO fails to condition on the image because it maximizes preference scores using only language tokens, ignoring visual input. The model can exploit this by treating images as noise to improve preference margins. This occurs because the preference label is defined by both modalities, but the optimization objective doesn't enforce joint conditioning. Evidence shows DPO (No Image) performs similarly to standard DPO on MMHalBench, demonstrating the unconditional preference problem.

### Mechanism 2
MDPO forces image attention by introducing CoPO, which contrasts responses conditioned on two different images. The CoPO loss maximizes the preference margin between responses on original images versus corrupted versions. Since the response is held constant, the model must rely on image features to satisfy the preference. The corruption strategy (0-20% cropping) creates hard negatives that are recognizably worse but share enough structure to provide meaningful contrastive signal.

### Mechanism 3
The reward anchor prevents likelihood collapse by forcing the reward of chosen responses to be positive. The AncPO loss adds a hinge on the reward value, keeping the likelihood of chosen responses from dropping. This is particularly important in multimodal settings where models can "cheat" by ignoring images and reducing response likelihood to inflate preference margins. The anchor ensures the model maintains reasonable likelihood values for high-quality responses.

## Foundational Learning

- **Multimodal conditional likelihood decomposition**: Understanding how image terms can be marginalized away if not explicitly regularized helps explain why models ignore images during optimization.
  - Quick check: If the image encoder outputs a constant vector, what happens to the multimodal likelihood term?

- **Bradley-Terry paired comparison model**: Knowing that DPO is derived from Bradley-Terry explains why the loss is a sigmoid over log-likelihood ratios and why the reference model matters.
  - Quick check: What happens to the DPO loss if the reference model π_ref is identical to the current model π_θ?

- **Contrastive learning loss form**: Understanding the hinge behavior of contrastive losses helps tune corruption levels and interpret CoPO gradients.
  - Quick check: If f(x) = f(x⁺), what is the gradient of the contrastive loss?

## Architecture Onboarding

- **Component map**: Data loader → image preprocessing (resize/crop) + question text tokenizer → Multimodal encoder (SigLIP backbone + LLM adapter) → Preference pair construction module (standard + CoPO + AncPO) → LoRA adapters on cross-attention and LM heads → Distributed trainer (32 batch size, cosine LR schedule) → Evaluation harness (MMHalBench, Object HalBench, AMBER)

- **Critical path**: Sample preference pair → Generate corrupted image for CoPO → Compute three losses (DPO, CoPO, AncPO) → Aggregate with equal weights and backprop through LoRA layers → Periodically evaluate on hallucination benchmarks

- **Design tradeoffs**: Corruption level vs negative difficulty (too easy → no signal; too hard → trivial shortcuts), anchor value sensitivity (too low → minimal effect; too high → over-constraint), weight sharing risks (separate LoRA is safer)

- **Failure signatures**: Image-ignoring collapse (MMHalBench score stops improving while Object HalBench CHAIRs plateaus), anchor overfit (training loss decreases but validation reward distribution narrows sharply), negative collapse (random images make MMHalBench score match standard DPO)

- **First 3 experiments**: 1) Run mDPO without CoPO to confirm conditional preference is main driver, 2) Vary crop percentage (0-20%, 20-50%, >50%) and measure CoPO loss magnitude, 3) Set δ ∈ {-1,0,1} and track likelihood of chosen responses and overall MMHalBench score

## Open Questions the Paper Calls Out

- How does mDPO effectiveness vary across different sizes and architectures of multimodal large language models? The current study focuses on two specific models and would benefit from experiments across a wider range of model sizes (e.g., 1B, 13B, 30B) and architectures (e.g., SigLIP, CLIP, Flamingo).

- Can the conditional preference optimization approach be extended to other multimodal learning scenarios beyond image and text? The current study focuses on image-text modalities and would benefit from applying the approach to other multimodal combinations like video-audio.

- What are the limitations of mDPO in terms of task and setting diversity? The current study evaluates on a limited set of tasks and would benefit from testing on more complex multimodal scenarios involving multiple images, videos, in-context learning, or risk-sensitive domains.

## Limitations
- Empirical claims rely heavily on synthetic hallucination benchmarks that may not fully capture real-world multimodal reasoning capabilities
- Preference data quality and diversity from Silkie with LLA-va-Instruct-150K instructions could significantly impact results
- Long-term stability of improvements across extended training or different data distributions is not demonstrated

## Confidence

**High Confidence**: Identification of unconditional preference problem is well-supported by comparison showing DPO (No Image) performs similarly to standard DPO. CoPO mechanism forcing image attention through contrastive learning is theoretically sound and empirically validated.

**Medium Confidence**: Effectiveness of reward anchor in preventing likelihood collapse is demonstrated but could be stronger. The specific choice of δ=0 as default and its sensitivity across data distributions needs more exploration.

**Low Confidence**: Generalization of mDPO benefits to real-world applications and robustness to different types of multimodal tasks beyond evaluated benchmarks. No testing on tasks requiring complex temporal reasoning, abstract visual concepts, or significant domain shift.

## Next Checks

1. **Corruption Method Ablation**: Systematically vary image corruption strategy beyond simple cropping (e.g., blur, occlusion, color distortion) and measure impact on CoPO loss magnitude and downstream MMHalBench performance to determine if 0-20% crop is truly optimal.

2. **Anchor Value Sensitivity**: Conduct comprehensive sweep of anchor values δ ∈ {-2, -1, 0, 1, 2} across different data distributions and model scales to identify optimal settings and determine if δ=0 default is universally applicable.

3. **Real-world Task Transfer**: Evaluate mDPO on real-world multimodal tasks such as visual question answering on OK-VQA or Science QA that require complex reasoning beyond hallucination detection to assess whether improvements on synthetic benchmarks translate to practical performance gains.