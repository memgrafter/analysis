---
ver: rpa2
title: Resource-Adaptive Successive Doubling for Hyperparameter Optimization with
  Large Datasets on High-Performance Computing Systems
arxiv_id: '2412.02729'
source_url: https://arxiv.org/abs/2412.02729
tags:
- rasda
- training
- gpus
- asha
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RASDA combines spatial doubling and temporal halving for large-scale
  HPO on HPC. It doubles GPU allocation to promising trials at milestones while halving
  under-performing trials.
---

# Resource-Adaptive Successive Doubling for Hyperparameter Optimization with Large Datasets on High-Performance Computing Systems

## Quick Facts
- arXiv ID: 2412.02729
- Source URL: https://arxiv.org/abs/2412.02729
- Reference count: 40
- RASDA achieves 0.84+ weak scaling efficiency on 1,024 GPUs and outperforms ASHA by up to 1.9x in runtime

## Executive Summary
This paper introduces Resource-Adaptive Successive Doubling Algorithm (RASDA), a novel hyperparameter optimization method designed for large-scale deep learning on high-performance computing systems. RASDA combines spatial doubling of GPU resources with temporal halving of trials, enabling efficient HPO on terabyte-scale scientific datasets. The method demonstrates significant runtime improvements over ASHA while maintaining or improving solution quality across computer vision, computational fluid dynamics, and additive manufacturing domains.

## Method Summary
RASDA is a multi-fidelity hyperparameter optimization algorithm that operates on Ray Tune's ResourceChangingScheduler interface. It launches multiple trials with initial GPU allocations and periodically evaluates their performance. At predefined milestones, promising trials receive doubled GPU resources (spatial doubling) while underperforming trials have their resources halved (temporal halving). The algorithm uses PyTorch DDP for data-parallel training with automatic batch size scaling and learning rate adjustment when resources change. RASDA integrates with existing HPO frameworks like BOHB and can be configured with different scaling factors for resource allocation.

## Key Results
- Outperforms ASHA by up to 1.9x in runtime across CV, CFD, and AM domains
- Achieves 0.84+ weak scaling efficiency on 1,024 GPUs
- First systematic HPO method applied to a terabyte-scale scientific dataset (8.3 TB CFD data)
- Maintains or exceeds solution quality through implicit batch size scheduling
- First 1.2x speedup on 128 GPUs for 60 GB AM dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RASDA outperforms ASHA by combining spatial doubling with temporal halving.
- Mechanism: RASDA doubles GPU allocation to promising trials while halving resources for underperforming trials at rung milestones. This leverages data-parallel training to accelerate model convergence.
- Core assumption: More GPUs for a trial leads to faster training without degrading model performance.
- Evidence anchors:
  - [abstract] "RASDA outperforms ASHA by a factor of up to 1.9 with respect to the runtime."
  - [section] "The developed method is suitable for problems that involve large scientific datasets... This work introduces a novel method, the Resource-Adaptive Successive Doubling Algorithm (RASDA), that leverages both HPC features to perform HPO efficiently at scale."
  - [corpus] No direct corpus evidence on spatial doubling, but multiple neighbor papers discuss multi-fidelity HPO which aligns with this concept.
- Break condition: If doubling GPUs causes severe GPU underutilization or communication bottlenecks that outweigh training speed gains.

### Mechanism 2
- Claim: RASDA maintains or improves solution quality through implicit batch size scheduling.
- Mechanism: As more GPUs are allocated to a trial, batch size increases geometrically. This mimics learning rate annealing and helps generalization by avoiding sharp minima.
- Core assumption: Increasing batch size over training time is equivalent to decaying learning rate for generalization.
- Evidence anchors:
  - [abstract] "the solution quality of final ASHA models is maintained or even surpassed by the implicit batch size scheduling of RASDA."
  - [section] "McCandlish et al. empirically studied large batch training... Based on these findings, the following two insights can be derived..." followed by discussion of batch size scaling benefits.
  - [corpus] No direct corpus evidence on batch size scheduling, but neighbor papers discuss multi-fidelity HPO which often involves varying batch sizes.
- Break condition: If batch size scaling becomes too aggressive and causes training instability or poor generalization.

### Mechanism 3
- Claim: RASDA achieves high weak scaling efficiency on large GPU counts.
- Mechanism: By efficiently reallocating GPUs from terminated trials to promoted trials, RASDA keeps most GPUs busy throughout the HPO process, avoiding the straggler problem of ASHA.
- Core assumption: Asynchronous promotion/demotion can be implemented efficiently without excessive overhead.
- Evidence anchors:
  - [abstract] "RASDA achieves 0.84+ weak scaling efficiency on 1,024 GPUs."
  - [section] "The advantage of the ASHA and RASDA scheduler is that they both perform asynchronous halving and doubling, i.e., top-performing trials are promoted to the next rung even if not all trials in the current rung have reached their milestones."
  - [corpus] No direct corpus evidence on weak scaling, but neighbor papers discuss distributed HPO which relates to this concept.
- Break condition: If communication overhead or resource allocation delays become significant at large scales.

## Foundational Learning

- Concept: Successive Halving Algorithm
  - Why needed here: RASDA builds on ASHA's successive halving foundation, so understanding the basic algorithm is essential.
  - Quick check question: In successive halving, how are underperforming trials eliminated at each rung?

- Concept: Data-Parallel Training
  - Why needed here: RASDA's resource doubling mechanism relies on data-parallel training to accelerate promising trials.
  - Quick check question: How does data-parallel training synchronize gradients across multiple GPUs?

- Concept: Hyperparameter Optimization (HPO)
  - Why needed here: RASDA is specifically designed for HPO on large datasets where full training is expensive.
  - Quick check question: Why is full training typically required to evaluate HPO configurations, and how do multi-fidelity methods address this?

## Architecture Onboarding

- Component map: Ray Tune framework -> ResourceChangingScheduler interface -> PyTorch DDP -> NCCL backend -> Application-specific training loops

- Critical path:
  1. Ray Tune launches trials with initial GPU allocation
  2. Training progresses and reports metrics to Ray head node
  3. At milestones, RASDA scheduler decides which trials to promote/demotion
  4. ResourceChangingScheduler reallocates GPUs accordingly
  5. Training continues with new resource allocation

- Design tradeoffs:
  - GPU allocation granularity vs. resource fragmentation
  - Batch size scaling aggressiveness vs. training stability
  - Frequency of resource reallocation vs. overhead
  - Number of trials vs. GPU utilization

- Failure signatures:
  - GPU underutilization indicates inefficient resource reallocation
  - Training instability after resource doubling suggests batch size issues
  - Poor scaling efficiency suggests communication bottlenecks
  - Suboptimal solutions suggest premature trial termination

- First 3 experiments:
  1. Compare RASDA vs ASHA on a small CV dataset with 8-16 GPUs to verify basic functionality and speed-up
  2. Test batch size scaling with different reduction/scaling factors to find optimal configuration
  3. Evaluate weak scaling from 64 to 256 GPUs on a representative dataset to verify scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RASDA compare to other state-of-the-art HPO methods like Optuna or SMAC on large-scale datasets?
- Basis in paper: [explicit] The paper mentions compatibility with Optuna and SMAC but only provides a comparison with BOHB and ASHA.
- Why unresolved: The authors only benchmark RASDA against ASHA and BOHB, leaving out other popular HPO tools that could provide a more comprehensive comparison.
- What evidence would resolve it: Experimental results comparing RASDA's runtime and solution quality against Optuna, SMAC, and other leading HPO frameworks on the same CV, CFD, and AM datasets.

### Open Question 2
- Question: What is the optimal strategy for dynamically adjusting the scaling and reduction factors (sf and rf) during an HPO run?
- Basis in paper: [inferred] The paper uses fixed values of sf=rf=2 and mentions that larger values can lead to instability, but does not explore adaptive adjustment of these parameters.
- Why unresolved: The authors acknowledge that the choice of these factors impacts performance but do not investigate whether dynamically adapting them based on trial progress could yield better results.
- What evidence would resolve it: Experiments showing the impact of different fixed vs. adaptive sf and rf values on runtime and solution quality across multiple datasets and models.

### Open Question 3
- Question: How does RASDA's resource allocation strategy scale beyond 1,024 GPUs, and what are the practical limits?
- Basis in paper: [explicit] The paper demonstrates scalability up to 1,024 GPUs but does not explore larger configurations.
- Why unresolved: While the paper shows good scaling efficiency at 1,024 GPUs, modern and future supercomputers have significantly more GPUs, and it's unclear how RASDA would perform at those scales.
- What evidence would resolve it: Performance benchmarks of RASDA on exascale systems with 10,000+ GPUs, including weak and strong scaling studies.

### Open Question 4
- Question: How sensitive is RASDA to different learning rate scaling strategies when increasing batch size?
- Basis in paper: [explicit] The paper mentions using linear scaling for SGD and square-root scaling for ADAM but does not explore alternative strategies.
- Why unresolved: The authors use specific learning rate scaling rules but do not investigate whether other strategies (e.g., warmup-based or adaptive scaling) might yield better results.
- What evidence would resolve it: Experiments comparing different learning rate scaling strategies (linear, square-root, warmup-based) across various models and datasets to determine optimal approaches.

## Limitations

- The paper demonstrates strong scaling up to 1,024 GPUs but lacks extensive validation beyond this point, leaving uncertainty about performance on exascale systems
- Resource allocation decisions are based on simple thresholding without considering GPU utilization patterns, which may lead to suboptimal scaling on heterogeneous architectures
- The implicit batch size scheduling mechanism, while theoretically sound, may not generalize to all neural network architectures or loss landscapes

## Confidence

- **High confidence** in runtime improvements (1.9x over ASHA): Multiple independent runs across three domains with consistent speed-ups provide strong empirical evidence
- **Medium confidence** in solution quality improvements: While results show maintained or improved accuracy, the claim of "even surpassed" quality lacks statistical significance testing across multiple random seeds
- **Medium confidence** in weak scaling efficiency: The 0.84+ efficiency claim is based on single-point measurements at 1,024 GPUs without extensive validation across different scales

## Next Checks

1. **Cross-architecture validation**: Test RASDA on transformer-based models (BERT, ViT) and graph neural networks to verify generalizability beyond the CV, CFD, and AM domains studied

2. **Extreme scale testing**: Evaluate RASDA on 2,048+ GPUs to identify potential communication bottlenecks and resource fragmentation issues not visible at 1,024 GPUs

3. **Robustness to noisy evaluations**: Introduce controlled noise into metric reporting to test whether RASDA's resource allocation decisions remain stable under realistic HPO conditions where evaluations have variance