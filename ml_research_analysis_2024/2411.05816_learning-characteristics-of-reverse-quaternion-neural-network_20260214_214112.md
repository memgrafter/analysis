---
ver: rpa2
title: Learning Characteristics of Reverse Quaternion Neural Network
arxiv_id: '2411.05816'
source_url: https://arxiv.org/abs/2411.05816
tags:
- neural
- quaternion
- rqnn
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Reverse Quaternion Neural Network (RQNN),
  a novel architecture that leverages the non-commutative property of quaternion multiplication
  by reversing the order of weight and input signal application. The study investigates
  RQNN's learning characteristics, comparing its performance to traditional quaternion
  neural networks (QNN) and real-valued neural networks (Real-NN).
---

# Learning Characteristics of Reverse Quaternion Neural Network

## Quick Facts
- arXiv ID: 2411.05816
- Source URL: https://arxiv.org/abs/2411.05816
- Authors: Shogo Yamauchi; Tohru Nitta; Takaaki Ohnishi
- Reference count: 17
- RQNN achieves learning speeds comparable to QNN and significantly faster than Real-NN, with all three models converging to similar levels of accuracy

## Executive Summary
This paper introduces the Reverse Quaternion Neural Network (RQNN), a novel architecture that leverages the non-commutative property of quaternion multiplication by reversing the order of weight and input signal application. The study investigates RQNN's learning characteristics, comparing its performance to traditional quaternion neural networks (QNN) and real-valued neural networks (Real-NN). Experimental results show that RQNN achieves learning speeds comparable to QNN and significantly faster than Real-NN, with all three models converging to similar levels of accuracy. Notably, RQNN demonstrates distinct rotational generalization capabilities, learning different rotation representations than QNN despite identical network configurations and training conditions.

## Method Summary
The paper introduces RQNN, which reverses the quaternion multiplication order in neural network computations. Instead of the standard S*W (input times weight), RQNN uses W*S (weight times input). The authors conducted three main experiments: comparing learning speeds across QNN, RQNN, and Real-NN on a quaternion learning task; testing rotation generalization on line rotation data; and evaluating rotation representation differences on point cloud rotation data. All experiments used identical network architectures except for the weight update rules, with learning rate 0.1 and sigmoid activation for most tests.

## Key Results
- RQNN achieves learning speeds comparable to QNN and significantly faster than Real-NN across all tested architectures
- RQNN and QNN converge to similar accuracy levels despite different rotation representations
- RQNN learns distinctly different rotation representations than QNN, with average angle differences of 102.5 degrees in output rotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RQNN exploits the non-commutative nature of quaternion multiplication by reversing the order of weight and input application
- Mechanism: In RQNN, the weight is applied before the input signal (W * S), whereas in standard QNN, the input is applied before the weight (S * W). This order reversal alters the network's parameter updates and the resulting rotation representations
- Core assumption: The non-commutative property of quaternion multiplication meaningfully affects learning dynamics when reversed
- Evidence anchors: [abstract] "leverages the non-commutative property of quaternion multiplication by reversing the order of weight and input signal application"

### Mechanism 2
- Claim: RQNN achieves comparable learning speed to QNN but captures different rotation representations
- Mechanism: The reversed multiplication order changes the gradient flow during backpropagation, resulting in different learned parameters that still enable fast convergence but represent rotations differently
- Core assumption: Reversed order still allows efficient gradient-based learning despite altering the parameter update path
- Evidence anchors: [abstract] "RQNN achieves learning speeds comparable to QNN and significantly faster than Real-NN"

### Mechanism 3
- Claim: RQNN's distinct rotation representations are measurable through Euler angle differences
- Mechanism: The learned quaternion weights encode rotations differently due to the reversed multiplication order, leading to different outputs even with identical inputs and training conditions
- Core assumption: Quaternion-based rotation encoding is sensitive to multiplication order
- Evidence anchors: [abstract] "RQNN demonstrates distinct rotational generalization capabilities, learning different rotation representations than QNN"

## Foundational Learning

- Concept: Quaternion algebra and non-commutativity
  - Why needed here: RQNN fundamentally depends on reversing the order of quaternion multiplication, so understanding quaternion properties is essential
  - Quick check question: Given two quaternions q1 = 1 + 2i + 3j + 4k and q2 = 2 + i + j + k, compute q1*q2 and q2*q1 to verify they differ

- Concept: Quaternion backpropagation derivation
  - Why needed here: RQNN modifies the standard quaternion backpropagation equations, requiring understanding of how gradients propagate through quaternion layers
  - Quick check question: In RQNN, write the weight update formula for a weight connecting hidden neuron m to output neuron n, showing the reversed multiplication order

- Concept: Euler angle representation of rotations
  - Why needed here: Experimental evaluation compares rotations via Euler angles, so interpreting results requires familiarity with this representation
  - Quick check question: Convert the quaternion q = 0.707 + 0.707i + 0j + 0k to Euler angles (roll, pitch, yaw)

## Architecture Onboarding

- Component map: Input layer (quaternion inputs) -> Hidden layers (Reverse quaternion neurons W*S) -> Output layer (quaternion outputs with split-type activation)
- Critical path:
  1. Forward pass: Input → W*S → activation → next layer → ... → output
  2. Loss computation: Quaternion error between output and target
  3. Backward pass: Gradients computed using reversed order equations
  4. Parameter update: Weights and biases adjusted using RQNN-specific rules
- Design tradeoffs:
  - RQNN vs QNN: Similar speed, different rotation representations
  - RQNN vs Real-NN: Faster convergence, better spatial representation
  - Activation function: Split-type used, but other quaternion activations possible
- Failure signatures:
  - No convergence: Learning rate too high/low, poor initialization
  - Identical outputs to QNN: Multiplication order not properly implemented
  - Unstable training: Numerical issues with quaternion gradient computation
- First 3 experiments:
  1. Implement RQNN and QNN with identical architecture and training data, verify learning curves converge similarly
  2. Train both on rotation task, measure output angle differences using Eq. (22)
  3. Compare Euler angles of rotation between QNN and RQNN outputs for test data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mathematical properties of RQNN cause its distinct rotational generalization compared to QNN?
- Basis in paper: [explicit] The paper states RQNN "can obtain a different rotation representation from the existing models" and shows experimental differences, but does not explain the mathematical mechanism
- Why unresolved: The paper demonstrates behavioral differences but doesn't provide theoretical analysis of why the reversed weight application leads to different rotation representations
- What evidence would resolve it: Mathematical proof showing how the weight reversal affects the quaternion product composition and rotation matrix derivation

### Open Question 2
- Question: Does RQNN maintain its learning speed advantage over Real-NN when scaling to deeper architectures or more complex datasets?
- Basis in paper: [inferred] The experiments only tested a single three-layer architecture on simple synthetic data; the authors note this as a future challenge
- Why unresolved: The paper only tested on simple 4-8-4 and 1-6-1 architectures with synthetic data, not complex real-world problems
- What evidence would resolve it: Systematic experiments varying network depth, width, and dataset complexity comparing convergence speed across all three architectures

### Open Question 3
- Question: Under what conditions (activation functions, initialization schemes, optimization algorithms) does RQNN show maximum performance advantage over QNN?
- Basis in paper: [explicit] The paper notes that "it is important to further investigate the learning representation characteristics of the RQNN" and mentions activation functions as a factor
- Why unresolved: Experiments were limited to sigmoid activation and Adam optimizer; no systematic ablation studies were conducted
- What evidence would resolve it: Comprehensive experiments testing different activation functions (ReLU, tanh, complex-valued activations), initialization methods, and optimizers to identify optimal configurations for RQNN vs QNN

## Limitations
- Experimental validation limited to synthetic data (quaternion learning task with only four examples, line and point cloud rotations)
- Does not explore hyperparameter sensitivity or compare against other quaternion network variants beyond standard QNN
- Practical relevance to real-world applications not demonstrated

## Confidence
- **High Confidence**: RQNN achieves learning speeds comparable to QNN and significantly faster than Real-NN, with all models converging to similar accuracy levels
- **Medium Confidence**: RQNN demonstrates distinct rotational generalization capabilities with different rotation representations than QNN
- **Low Confidence**: The mechanism by which reversed quaternion multiplication order fundamentally alters learning dynamics is not fully explained

## Next Checks
1. **Generalization Test**: Train RQNN and QNN on a larger, more diverse quaternion dataset (e.g., 100+ examples with varying rotation angles) to verify if the learning speed advantage and distinct rotation representations persist

2. **Real-World Application**: Apply RQNN to a practical 3D spatial processing task (e.g., point cloud classification or 3D object pose estimation) and compare performance against QNN and Real-NN baselines to assess practical value beyond synthetic experiments

3. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rates, network architectures, and activation functions for RQNN, QNN, and Real-NN to determine if RQNN's advantages are robust across different training configurations or specific to the chosen experimental setup