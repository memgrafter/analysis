---
ver: rpa2
title: Graph Data Condensation via Self-expressive Graph Structure Reconstruction
arxiv_id: '2403.07294'
source_url: https://arxiv.org/abs/2403.07294
tags:
- graph
- dataset
- structure
- synthetic
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel graph data condensation method, GCSR,
  which addresses the problem of generating small synthetic graphs that preserve essential
  information for training GNNs. The core idea is to explicitly incorporate the original
  graph structure into the condensing process and capture nuanced interdependencies
  between condensed nodes by reconstructing an interpretable self-expressive graph
  structure.
---

# Graph Data Condensation via Self-expressive Graph Structure Reconstruction

## Quick Facts
- arXiv ID: 2403.07294
- Source URL: https://arxiv.org/abs/2403.07294
- Authors: Zhanyu Liu; Chaolv Zeng; Guanjie Zheng
- Reference count: 40
- Primary result: Proposes GCSR method achieving superior node classification accuracy by explicitly incorporating original graph structure and reconstructing self-expressive graph structure

## Executive Summary
This paper introduces GCSR, a novel graph data condensation method that generates small synthetic graphs preserving essential information for training GNNs. The key innovation is explicitly incorporating the original graph structure into the condensing process and capturing nuanced interdependencies between condensed nodes through self-expressive graph structure reconstruction. GCSR achieves significant improvements in node classification accuracy across five real-world graph datasets compared to existing methods.

## Method Summary
GCSR is a three-module framework for graph data condensation. The initialization module uses message passing to generate node features incorporating graph structure information and creates a probabilistic adjacency matrix as a regularizer. The self-expressive reconstruction module leverages the self-expressive property of graph data to derive an interpretable graph structure using a closed-form solution. The update module refines synthetic data by matching training dynamics through multi-step gradient matching and updating the regularizer via bootstrapping. This approach explicitly maintains the original graph structure and inter-node correlations while capturing nuanced relationships between condensed nodes.

## Key Results
- GCSR achieves significant improvements in node classification accuracy compared to existing graph condensation methods
- The method demonstrates superior performance across five real-world graph datasets (Citeseer, Cora, Ogbn-arxiv, Flickr, Reddit)
- GCSR successfully maintains inter-class similarity of the original graph while reducing data size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GCSR achieves superior performance by explicitly incorporating the original graph structure into the condensing process.
- Mechanism: The initialization module uses message passing to generate node features that incorporate graph structure information, and the probabilistic adjacency matrix derived from the original graph serves as a regularizer in the self-expressive reconstruction module.
- Core assumption: Graph structure information is valuable for downstream GNN tasks and can be effectively transferred from the original to the synthetic graph.
- Evidence anchors:
  - [abstract]: "explicitly incorporating the original graph structure into the condensing process"
  - [section]: "we utilize the k-order node feature for node initialization and the probabilistic adjacency matrix derived from the original graph structure to initialize the regularization term"
  - [corpus]: Weak - the corpus contains related papers but doesn't directly address the mechanism of incorporating original graph structure.

### Mechanism 2
- Claim: GCSR captures nuanced interdependencies between condensed nodes by reconstructing an interpretable self-expressive graph structure.
- Mechanism: The self-expressive reconstruction module leverages the self-expressive property of graph data to derive an interpretable graph structure using a closed-form solution, which measures the similarity between nodes.
- Core assumption: Nodes within the same feature subspace can represent each other, and this property can be exploited to reconstruct graph structure.
- Evidence anchors:
  - [abstract]: "capturing the nuanced interdependencies between the condensed nodes by reconstructing an interpretable self-expressive graph structure"
  - [section]: "we reconstruct an explicit and interpretable graph structure using a closed-form expression"
  - [corpus]: Weak - the corpus contains related papers but doesn't directly address the mechanism of self-expressive graph structure reconstruction.

### Mechanism 3
- Claim: GCSR achieves better generalization across different GNN architectures by maintaining the original graph structure and inter-node correlations.
- Mechanism: The update module uses multi-step gradient matching to update node features and bootstrapping to update the regularizer, ensuring adaptability to training dynamics and preventing overfitting.
- Core assumption: Maintaining the original graph structure and inter-node correlations leads to better generalization across different GNN architectures.
- Evidence anchors:
  - [abstract]: "maintaining the inter-class similarity of the original graph"
  - [section]: "the synthetic data S to learn the training dynamics observed in models trained with the real data T"
  - [corpus]: Weak - the corpus contains related papers but doesn't directly address the mechanism of maintaining original graph structure for generalization.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GCSR is designed for condensing graph data for training GNNs.
  - Quick check question: What is the main difference between GNNs and traditional neural networks?

- Concept: Self-expressive property of graph data
  - Why needed here: GCSR leverages this property to reconstruct the graph structure.
  - Quick check question: How does the self-expressive property help in learning graph structure?

- Concept: Message passing in graphs
  - Why needed here: GCSR uses message passing for node initialization to incorporate graph structure information.
  - Quick check question: What is the role of message passing in GNNs?

## Architecture Onboarding

- Component map: Initialization Module -> Self-expressive Reconstruction Module -> Update Module
- Critical path: Initialization ‚Üí Self-expressive Reconstruction ‚Üí Update
- Design tradeoffs:
  - Message passing vs. random initialization: Message passing incorporates graph structure but may be computationally expensive.
  - Closed-form solution vs. iterative optimization: Closed-form is faster but may be less flexible.
- Failure signatures:
  - Poor node classification accuracy: Could indicate issues with any of the three modules.
  - Unstable training: Could indicate issues with the update module, particularly the bootstrapping.
- First 3 experiments:
  1. Compare node classification accuracy with and without message passing initialization.
  2. Compare graph structure reconstruction with and without the probabilistic adjacency matrix regularization.
  3. Test the effect of different update rates (ùúè and ùõæ) on the final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GCSR change when using different GNN architectures for condensation versus testing?
- Basis in paper: [explicit] The paper discusses cross-architecture performance and shows that SGC is the simplest but surpasses others in all methods. It also mentions that different GNN models show similar low-pass filtering behaviors.
- Why unresolved: While the paper provides initial results, it doesn't fully explore the impact of using different GNN architectures for both condensation and testing phases.
- What evidence would resolve it: Conducting extensive experiments with various combinations of GNN architectures for condensation and testing phases, and analyzing the resulting performance differences.

### Open Question 2
- Question: How does the performance of GCSR scale with larger graphs and different condensation ratios?
- Basis in paper: [inferred] The paper focuses on node classification tasks and uses specific datasets with fixed condensation ratios. It doesn't explore the scalability of GCSR to larger graphs or varying condensation ratios.
- Why unresolved: The scalability of GCSR to larger graphs and different condensation ratios is not thoroughly investigated, which is crucial for real-world applications.
- What evidence would resolve it: Conducting experiments with larger graphs and varying condensation ratios to evaluate the performance and scalability of GCSR.

### Open Question 3
- Question: How does the interpretability of the learned graph structure in GCSR compare to other graph condensation methods?
- Basis in paper: [explicit] The paper highlights the interpretability of the self-expressive reconstruction module and shows visualizations of the learned graph structure. It also mentions that the learned similarity matrix Z possesses interpretability as it explicitly indicates the weight to which the node feature of one node is represented by the other nodes.
- Why unresolved: While the paper provides some insights into the interpretability of the learned graph structure, a comprehensive comparison with other graph condensation methods is not provided.
- What evidence would resolve it: Conducting a detailed analysis and comparison of the interpretability of the learned graph structures in GCSR and other graph condensation methods, including visualizations and quantitative metrics.

## Limitations
- The effectiveness relies heavily on the validity of the self-expressive property assumption for condensed graphs
- The closed-form solution may oversimplify complex node interdependencies
- The message passing initialization assumes k-hop neighborhood information is always beneficial without clear guidance on selecting k

## Confidence
- **High confidence**: The core claim that explicitly incorporating original graph structure improves condensation quality is well-supported by experimental results showing consistent improvements over baseline methods.
- **Medium confidence**: The claim about capturing nuanced interdependencies between condensed nodes is supported by results but the interpretability of the reconstructed structure could be further validated.
- **Low confidence**: The assertion that the closed-form solution provides optimal reconstruction is theoretical; empirical comparison with iterative methods is limited.

## Next Checks
1. **Cross-graph type validation**: Test GCSR on graphs with varying homophily levels (low, medium, high) to validate whether the self-expressive property holds across different structural patterns.
2. **Ablation on reconstruction method**: Compare the closed-form self-expressive reconstruction against iterative optimization approaches on a subset of datasets to quantify the trade-off between efficiency and reconstruction quality.
3. **Multi-architecture generalization**: Evaluate GCSR-generated condensed graphs using GNNs with different message passing mechanisms (GraphSAGE, GAT, GIN) to verify claims about architectural agnosticism.