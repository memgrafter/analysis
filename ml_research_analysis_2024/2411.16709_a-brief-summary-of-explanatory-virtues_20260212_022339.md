---
ver: rpa2
title: A Brief Summary of Explanatory Virtues
arxiv_id: '2411.16709'
source_url: https://arxiv.org/abs/2411.16709
tags:
- explanation
- virtues
- theory
- explanatory
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews Explanatory Virtues (EVs) from philosophy, psychology,
  and cognitive science, and applies them to eXplainable AI (XAI). It categorizes
  EVs along epistemic/pragmatic and ontological dimensions, drawing from Keas (2018)
  taxonomy while adding a coverage category.
---

# A Brief Summary of Explanatory Virtues

## Quick Facts
- arXiv ID: 2411.16709
- Source URL: https://arxiv.org/abs/2411.16709
- Authors: Ingrid Zukerman
- Reference count: 6
- One-line primary result: Reviews explanatory virtues from philosophy, psychology, and cognitive science, applying them to XAI with a focus on local explanations and adding coverage virtues to Keas (2018) taxonomy.

## Executive Summary
This paper reviews explanatory virtues (EVs) from philosophy, psychology, and cognitive science, applying them to eXplainable AI (XAI). It categorizes EVs along epistemic/pragmatic and ontological dimensions, drawing from Keas (2018) taxonomy while adding a coverage category. For local XAI explanations, evidential virtues ensure explanations justify model predictions, coherential virtues align with accepted knowledge, and coverage virtues measure explanation applicability. Aesthetic virtues prefer brevity, while diachronic virtues are less relevant for XAI. The paper demonstrates how these virtues guide explanation generation and evaluation, with unification showing promise in user studies.

## Method Summary
The paper synthesizes existing literature on explanatory virtues from multiple disciplines and systematically applies these concepts to the XAI domain. It builds upon Keas's (2018) taxonomy of explanatory virtues, extending it by adding a coverage category and evaluating each virtue's applicability to local XAI explanations. The analysis considers how different virtues can guide the generation and evaluation of explanations, identifying which virtues are most relevant for XAI contexts.

## Key Results
- Categorizes explanatory virtues along epistemic/pragmatic and ontological dimensions for XAI applications
- Adds coverage virtues as a distinct category to Keas (2018) taxonomy
- Identifies unification as showing promise in user studies for XAI explanations
- Distinguishes between evidential, coherential, and coverage virtues for local XAI explanations
- Concludes that diachronic virtues have limited relevance for XAI applications

## Why This Works (Mechanism)
The framework works by providing a structured approach to evaluate and generate explanations in XAI based on established philosophical principles. By mapping traditional explanatory virtues to XAI contexts, the paper creates a systematic way to assess explanation quality beyond technical metrics. The categorization helps identify which virtues are most relevant for different types of explanations (local vs. global) and provides guidance for explanation designers to balance competing virtues when generating explanations.

## Foundational Learning

1. **Evidential Virtues** (why needed: ensure explanations are factually grounded and justified)
   - Quick check: Can the explanation be empirically verified against the model's predictions?

2. **Coherential Virtues** (why needed: ensure explanations align with established knowledge)
   - Quick check: Does the explanation maintain internal consistency and compatibility with accepted theories?

3. **Coverage Virtues** (why needed: measure explanation's applicability across different cases)
   - Quick check: How broadly does the explanation apply to similar instances or related domains?

4. **Aesthetic Virtues** (why needed: prefer simpler, more comprehensible explanations)
   - Quick check: Is the explanation concise and free from unnecessary complexity?

5. **Diachronic Virtues** (why needed: evaluate explanation's evolution over time)
   - Quick check: Can the explanation adapt and remain relevant as understanding evolves?

## Architecture Onboarding

Component Map: Evidence Generation -> Coherence Checking -> Coverage Assessment -> Aesthetic Optimization -> Diachronic Evaluation

Critical Path: The core explanation generation process flows through evidential virtues (ensuring factual accuracy) → coherential virtues (ensuring alignment with knowledge) → coverage virtues (ensuring applicability), with aesthetic virtues applied throughout for simplicity.

Design Tradeoffs: The framework requires balancing competing virtues, such as choosing between depth (evidential) and simplicity (aesthetic), or between conservativeness (coherential) and innovation (coverage).

Failure Signatures: Explanations may fail when they prioritize one virtue category at the expense of others, such as highly accurate but incomprehensible explanations, or simple but inaccurate explanations.

First Experiments:
1. Compare user comprehension of explanations generated with vs. without virtue-based guidance
2. Test whether unification-focused explanations improve user trust compared to traditional feature-importance explanations
3. Evaluate whether prioritizing aesthetic virtues improves explanation adoption in real-world XAI applications

## Open Questions the Paper Calls Out
None

## Limitations
- Low confidence in applying diachronic virtues (durability, fruitfulness, smoothness) to XAI due to temporal requirements
- Claims about unification showing promise in user studies lack specific methodological details or citations
- Does not address potential conflicts between different virtue categories when they suggest competing explanation approaches
- Lacks empirical validation of the framework's effectiveness in real-world XAI applications

## Confidence
High confidence: Categorization of epistemic vs. pragmatic virtues and specific virtues within each category are well-grounded in philosophical literature. Core argument that explanatory virtues guide XAI explanation generation is strongly supported.

Medium confidence: Extension to include coverage virtues is reasonable but may need more theoretical justification. Application of aesthetic virtues to XAI is plausible but could use more detailed exploration.

Low confidence: Claims about diachronic virtues' relevance to XAI and specific user study evidence for unification require additional substantiation.

## Next Checks
1. Conduct systematic literature review to identify existing empirical studies on explanatory virtues in XAI contexts, particularly regarding unification and user comprehension.

2. Design controlled experiments testing the framework's effectiveness by comparing XAI explanations generated with and without virtue-based guidance, measuring user understanding and trust.

3. Develop formal criteria for resolving conflicts between different virtue categories when they suggest competing explanation approaches.