---
ver: rpa2
title: Investigating the impact of 2D gesture representation on co-speech gesture
  generation
arxiv_id: '2406.15111'
source_url: https://arxiv.org/abs/2406.15111
tags:
- gesture
- gestures
- speech
- diffgesture
- pose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of gesture representation dimensionality
  on co-speech gesture generation. The authors compare 2D and 3D pose representations
  when training a diffusion-based generative model for generating co-speech gestures.
---

# Investigating the impact of 2D gesture representation on co-speech gesture generation

## Quick Facts
- arXiv ID: 2406.15111
- Source URL: https://arxiv.org/abs/2406.15111
- Authors: Teo Guichoux; Laure Soulier; Nicolas Obin; Catherine Pelachaud
- Reference count: 40
- Primary result: 3D pose representation outperforms 2D in co-speech gesture generation across FGD, BC, and Diversity metrics

## Executive Summary
This paper investigates how gesture representation dimensionality affects co-speech gesture generation quality. The authors compare 2D and 3D pose representations when training a diffusion-based generative model (DiffGesture) on the TED Gesture-3D dataset. They find that directly generating 3D gestures yields superior performance compared to generating 2D gestures and lifting them to 3D using VideoPose3D. The study reveals that depth information is crucial for maintaining gesture-speech synchrony and diversity, while speech conditioning helps 3D generation but harms 2D generation due to increased ambiguity.

## Method Summary
The study trains DiffGesture, a denoising diffusion probabilistic model, on both 2D and 3D pose data extracted from the TED Gesture-3D dataset. 2D poses are obtained using OpenPose on YouTube videos, while 3D poses are inferred using VideoPose3D. The model is trained with and without speech conditioning to examine its impact. Generated gestures are evaluated using FrÃ©chet Gesture Distance (FGD), Beat Consistency Score (BC), and Diversity metrics. For the 2D approach, generated 2D sequences are lifted to 3D using VideoPose3D before evaluation. The study systematically compares performance across these different training and generation conditions.

## Key Results
- DiffGesture 3D outperforms DiffGesture 2D + VP3D across all metrics (higher FGD, lower BC, lower diversity)
- Removing speech conditioning improves 2D-generated gesture quality but not 3D-generated quality
- 2D-to-3D lifting introduces an inductive bias that collapses the natural ambiguity in pose-to-gesture mapping, reducing output diversity
- Depth information in 3D poses is critical for maintaining kinematic beats and gesture-speech synchrony

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 3D pose representation yields better speech-gesture synchrony than 2D because it captures depth cues that align naturally with prosodic features.
- Mechanism: 3D coordinates encode full spatial articulation, allowing the generative model to map speech patterns (pitch, rhythm, emphasis) directly onto natural gesture amplitude and directionality. 2D coordinates lack depth, forcing the model to infer missing spatial information from noisy correlations, degrading timing alignment.
- Core assumption: Depth information in 3D poses has a direct mapping to speech prosody that 2D cannot capture.
- Evidence anchors:
  - [abstract] "the 2D keypoints were estimated using OpenPose [7] on TED YouTube videos" and "3D poses can further be inferred from these estimated 2D key-points using a third-party 2D to 3D lifting model [9, 34, 37]"
  - [section] "DiffGesture 2D + VP3D performs worse than the original DiffGesture 3D in terms of FGD, BC, and diversity" and "the drop in BC between DiffGesture 3D and DiffGesture 2D + VP3D... reduces the number of kinematic beats"
  - [corpus] Weak - no corpus papers directly address depth vs synchrony
- Break condition: If speech features that drive gesture rhythm are independent of depth (e.g., purely tonal), 2D could perform equally well.

### Mechanism 2
- Claim: Deterministic 2D-to-3D lifting collapses the inherent 1-to-many ambiguity into a single pseudo-ground-truth, reducing output diversity.
- Mechanism: For any 2D pose, multiple 3D poses are geometrically possible. A deterministic lifter (VideoPose3D) maps each 2D pose to exactly one 3D pose, eliminating natural variation. The generative model trained on these collapsed sequences learns a narrower distribution, producing less diverse gestures.
- Core assumption: Real-world co-speech gestures exhibit multiple valid 3D interpretations for the same 2D pose, and preserving this ambiguity is necessary for diversity.
- Evidence anchors:
  - [abstract] "the derived 3D pose estimation is essentially a pseudo-ground truth, with the actual ground truth being the 2D motion data"
  - [section] "the one-to-many relationship between 2D keypoints and their 3D counterparts" and "the distribution resulting from lifting 2D sequences is tighter than the distribution directly generated in 3D, explaining the high FGD and low diversity"
  - [corpus] Weak - no corpus papers quantify ambiguity collapse
- Break condition: If the deterministic lifter's output matches real distribution statistics, diversity loss would be minimal.

### Mechanism 3
- Claim: Speech conditioning adds ambiguity when training on 2D data, degrading alignment between gesture and speech content.
- Mechanism: Speech-to-gesture mapping is already one-to-many in 3D (multiple gestures fit the same speech). Reducing to 2D amplifies this ambiguity, making it harder for the model to learn consistent speech-gesture correspondences. Removing speech conditioning in 2D training recovers quality, indicating that the added conditioning confuses the 2D model.
- Core assumption: The 2D representation cannot disambiguate speech-gesture relationships as effectively as 3D.
- Evidence anchors:
  - [section] "DiffGesture 2D shows a higher FGD value than DiffGesture (3D->2D) and Uncond. DiffGesture 2D, suggesting that the speech condition reduces the quality of the gestures generated in 2D"
  - [section] "2D gestures can be synthesized without the speech condition while still closely aligning with the target distribution in terms of FGD and diversity"
  - [corpus] Weak - no corpus papers explicitly test speech conditioning removal
- Break condition: If speech features are well-captured in 2D (e.g., through planar velocity patterns), conditioning might still help.

## Foundational Learning

- Concept: Diffusion probabilistic models and classifier-free guidance
  - Why needed here: The study compares performance of a denoising diffusion model (DiffGesture) under different pose dimensionalities; understanding how diffusion models learn from conditional vs unconditional data is essential to interpret the FGD and BC differences.
  - Quick check question: How does classifier-free guidance balance fidelity and diversity in a diffusion model, and why might removing the speech condition help when training on 2D data?

- Concept: Pose representation invariance and coordinate normalization
  - Why needed here: The authors use directional vectors normalized to unit length centered on the root joint. Knowing why this representation is chosen (invariance to bone length, reduced sensitivity to root rotation) helps explain why 3D remains richer than 2D even with normalization.
  - Quick check question: What information is lost when converting 3D directional vectors to 2D by removing the depth axis, and how might this affect downstream generative quality?

- Concept: Evaluation metrics for gesture generation (FGD, BC, Diversity)
  - Why needed here: The paper uses three metrics to quantify naturalness, synchrony, and diversity. Understanding how each metric is computed and what it captures is critical for interpreting why 2D-to-3D lifting underperforms.
  - Quick check question: Why does the Beat Consistency Score drop when generating 2D gestures and lifting them to 3D, and what does this imply about the relationship between depth information and kinematic beat extraction?

## Architecture Onboarding

- Component map:
  - DiffGesture (DDPM) -> pose sequence generator
  - VideoPose3D (TCN) -> 2D->3D lifter
  - Pose encoder -> feature extractor for FGD/Diversity
  - Audio encoder (CNN) -> speech feature extractor
  - Metrics modules: FGD, BC, Diversity calculators

- Critical path:
  1. Train DiffGesture on 2D or 3D data with/without speech conditioning
  2. Generate synthetic pose sequences
  3. If 2D generated, lift to 3D with VideoPose3D
  4. Compute FGD, BC, Diversity against ground truth 3D sequences
  5. Compare across experimental conditions

- Design tradeoffs:
  - 2D training is cheaper and leverages abundant in-the-wild data but suffers from depth ambiguity and lifter-induced bias.
  - 3D training yields richer representations but requires either expensive MoCap or less accurate lifting models.
  - Speech conditioning improves diversity in 3D but may harm 2D due to added ambiguity.

- Failure signatures:
  - High FGD + low Diversity: lifter is collapsing distribution or model is mode-collapsed.
  - Low BC: depth cues critical for timing; lifting smooths motion and removes kinematic beats.
  - Low FGD but high BC: plausible gestures but poor temporal alignment with speech.

- First 3 experiments:
  1. Retrain DiffGesture 2D on the TED Gesture-3D dataset (without depth axis) and evaluate FGD/BC/Diversity vs 3D baseline.
  2. Generate 2D sequences, lift with VideoPose3D, compare to ground truth 3D in all metrics.
  3. Repeat steps 1-2 but mask speech input during training (unconditional generation) to test conditioning effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the quality of co-speech gesture generation improve when using intermediate 2.5D representations (depth as separate channel) instead of pure 2D or 3D representations?
- Basis in paper: [inferred] The paper explores the impact of 2D vs 3D representations and suggests that 3D representations are more suitable. An intermediate 2.5D representation could potentially capture depth information while maintaining some of the benefits of 2D representations.
- Why unresolved: The paper only compares pure 2D and 3D representations. 2.5D representations are not explored.
- What evidence would resolve it: Training DiffGesture with 2.5D representations and comparing FGD, BC, and Diversity scores to the 2D and 3D results.

### Open Question 2
- Question: How does the choice of 2D-to-3D lifting model affect the quality of generated gestures compared to using VideoPose3D?
- Basis in paper: [explicit] The paper acknowledges that using VideoPose3D introduces an inductive bias due to its deterministic nature and that this bias affects the quality of generated gestures.
- Why unresolved: The paper only uses VideoPose3D for 2D-to-3D lifting and does not compare it to other lifting models.
- What evidence would resolve it: Training DiffGesture with 2D data and lifting to 3D using different models (e.g., ExPose, XNect) and comparing the results to those obtained with VideoPose3D.

### Open Question 3
- Question: Does the impact of gesture representation dimensionality vary across different types of speech content (e.g., narrative vs. dialogue)?
- Basis in paper: [inferred] The paper uses the TED Gesture-3D dataset which contains mostly narrative speech content. The impact of representation dimensionality might differ for other types of speech.
- Why unresolved: The paper does not analyze the effect of speech content type on the impact of gesture representation dimensionality.
- What evidence would resolve it: Training and evaluating DiffGesture on datasets with different speech content types (e.g., dialogue, conversation) and comparing the results to those obtained with the TED dataset.

## Limitations
- Deterministic 2D-to-3D lifting may artificially constrain diversity, making fair comparison with 3D generation difficult
- Study only examines one diffusion-based architecture and dataset, limiting generalizability
- No analysis of how different speech content types might affect the impact of gesture representation dimensionality

## Confidence
- **High confidence**: The finding that 3D pose representation outperforms 2D in all three evaluation metrics (FGD, BC, Diversity)
- **Medium confidence**: The explanation that depth information is crucial for maintaining kinematic beats and gesture-speech synchrony
- **Medium confidence**: The claim that speech conditioning degrades 2D gesture quality but not 3D due to increased ambiguity

## Next Checks
1. Test alternative 2D-to-3D lifting approaches: Compare deterministic VideoPose3D with probabilistic lifting methods to determine if diversity limitations are due to the lifting method rather than the 2D representation itself.

2. Cross-dataset validation: Evaluate the same 2D vs 3D comparison on a different co-speech gesture dataset (e.g., Trinity Gesture Dataset) to verify that findings generalize beyond TED talks.

3. Ablation of depth components: Systematically remove depth-related features from 3D poses (e.g., z-axis velocity, depth-based beat detection) to quantify how much of the performance gap is specifically due to depth information versus other 3D-specific characteristics.