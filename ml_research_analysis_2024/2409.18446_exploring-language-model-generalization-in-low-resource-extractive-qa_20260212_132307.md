---
ver: rpa2
title: Exploring Language Model Generalization in Low-Resource Extractive QA
arxiv_id: '2409.18446'
source_url: https://arxiv.org/abs/2409.18446
tags:
- dataset
- language
- performance
- datasets
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why large language models (LLMs) struggle
  to generalize to closed-domain extractive question answering (EQA) tasks without
  in-domain training. The authors hypothesize that the performance drop stems from
  both architectural limitations and dataset complexity.
---

# Exploring Language Model Generalization in Low-Resource Extractive QA

## Quick Facts
- **arXiv ID**: 2409.18446
- **Source URL**: https://arxiv.org/abs/2409.18446
- **Reference count**: 40
- **One-line primary result**: Large language models struggle to generalize to closed-domain extractive QA without in-domain training due to architectural limitations, tokenization issues, and dataset complexity differences.

## Executive Summary
This paper investigates why large language models (LLMs) struggle to generalize to closed-domain extractive question answering (EQA) tasks without in-domain training. The authors hypothesize that the performance drop stems from both architectural limitations and dataset complexity. Through a series of controlled experiments, they find that (1) LLMs often fail to produce answer spans matching the length distribution of gold data in closed domains, with even large models overfitting to training data; (2) tokenization schemes significantly impact a model's ability to discriminate domain-specific word senses, with byte-level BPE (used by RoBERTa and Falcon) underperforming compared to SentencePiece with Unigram tokenization; (3) scaling model parameters does not guarantee better cross-domain generalization, and models like BERT, though smaller, sometimes outperform larger variants; (4) closed-domain datasets are quantitatively and qualitatively very different from open-domain ones, as confirmed by dataset similarity metrics (FDA, TEXT/TASK embeddings) and perplexity analysis, with higher perplexity correlating with lower performance. Overall, the findings suggest that improving cross-domain EQA requires better tokenization strategies, more robust sense discrimination, and awareness of dataset characteristics, rather than simply increasing model size.

## Method Summary
The study employs a zero-shot evaluation approach where models are fine-tuned on the open-domain SQuAD dataset and then tested on out-of-domain (OOD) datasets including DuoRC, CUAD, COVID-QA, and TechQA. The authors conduct controlled experiments analyzing answer length distributions, sense discrimination capabilities using polysemy datasets, model scaling effects with BERT variants, and dataset similarity using FDA and TEXT/TASK embeddings. They also measure perplexity correlations and evaluate multiple architectures including BiDAF, QANet, BERT, RoBERTa, Falcon, Platypus, Gemma, and Mistral.

## Key Results
- LLMs consistently produce shorter answer spans in closed domains compared to gold data, indicating length distribution mismatch
- Tokenization schemes significantly impact sense discrimination, with byte-level BPE underperforming SentencePiece with Unigram tokenization
- Scaling model parameters does not guarantee better cross-domain generalization, and smaller models like BERT sometimes outperform larger variants
- Closed-domain datasets show significantly higher perplexity and are quantitatively/qualitatively different from open-domain datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling model parameters does not guarantee better cross-domain generalization for EQA.
- Mechanism: Increasing model size may lead to overfitting on training data, causing models to memorize patterns rather than generalize to new domains.
- Core assumption: Larger models have more capacity to memorize training data, potentially reducing their ability to adapt to out-of-domain tasks.
- Evidence anchors:
  - [abstract] "Scaling model parameters is not always effective for cross domain generalization"
  - [section 3.2.1] "scaling model parameters does not guarantee better cross-domain generalization, and models like BERT, though smaller, sometimes outperform larger variants"
  - [corpus] Weak evidence - limited comparative studies on model scaling impact across diverse domains
- Break condition: If larger models demonstrate consistent improvement across multiple out-of-domain datasets without overfitting.

### Mechanism 2
- Claim: Tokenization schemes significantly impact a model's ability to discriminate domain-specific word senses.
- Mechanism: Different tokenization strategies (e.g., byte-level BPE vs SentencePiece with Unigram) affect how well models can distinguish between different meanings of words in specific domains.
- Core assumption: Tokenization directly influences the model's understanding of word semantics in context.
- Evidence anchors:
  - [abstract] "Certain LLMs, despite showing strong overall performance, display weaknesses in meeting basic requirements as discriminating between domain-specific senses of words which we link to pre-processing decisions"
  - [section 3.2.2] "tokenization schemes significantly impact a model's ability to discriminate domain-specific word senses, with byte-level BPE (used by RoBERTa and Falcon) underperforming compared to SentencePiece with Unigram tokenization"
  - [corpus] Weak evidence - limited exploration of tokenization impact across diverse domain datasets
- Break condition: If tokenization differences show minimal impact on model performance across various domains.

### Mechanism 3
- Claim: Closed-domain datasets are quantitatively and qualitatively different from open-domain ones, affecting model performance.
- Mechanism: Differences in dataset characteristics (e.g., answer length, complexity, terminology) create challenges for models trained on open-domain data.
- Core assumption: Dataset characteristics directly influence model performance when generalizing to new domains.
- Evidence anchors:
  - [abstract] "Closed-domain datasets are quantitatively much different than open-domain EQA datasets and current LLMs struggle to deal with them"
  - [section 3.3.1] "Closed-domain datasets are quantitatively and qualitatively very different from open-domain ones, as confirmed by dataset similarity metrics (FDA, TEXT/TASK embeddings) and perplexity analysis"
  - [corpus] Moderate evidence - dataset similarity metrics and perplexity analysis provide quantitative support
- Break condition: If models show similar performance across diverse datasets regardless of their characteristics.

## Foundational Learning

- Concept: Dataset similarity metrics (FDA, TEXT/TASK embeddings)
  - Why needed here: To quantify the differences between open-domain and closed-domain datasets, explaining why models struggle to generalize.
  - Quick check question: How do FDA and TEXT/TASK embeddings capture dataset characteristics, and what do their results indicate about model performance?

- Concept: Tokenization schemes (byte-level BPE, SentencePiece with Unigram)
  - Why needed here: To understand how different tokenization strategies affect a model's ability to process and understand domain-specific language.
  - Quick check question: What are the key differences between byte-level BPE and SentencePiece with Unigram tokenization, and how do they impact model performance?

- Concept: Model scaling and overfitting
  - Why needed here: To grasp why simply increasing model size doesn't always lead to better performance, especially in cross-domain scenarios.
  - Quick check question: How does model scaling potentially lead to overfitting, and what are the implications for cross-domain generalization?

## Architecture Onboarding

- Component map:
  Models: BERT -> RoBERTa -> Falcon -> Platypus -> Gemma -> Mistral
  Datasets: SQuAD (open-domain) -> DuoRC, CUAD, COVID-QA, TechQA (closed-domain)
  Metrics: Exact Match (EM), F1 score -> perplexity, dataset similarity metrics
  Experiments: Answer length analysis -> sense discrimination -> architecture examination -> dataset similarity analysis

- Critical path:
  1. Fine-tune models on open-domain dataset (SQuAD)
  2. Evaluate zero-shot performance on closed-domain datasets
  3. Analyze performance gaps using various metrics and experiments
  4. Identify architectural and dataset-related factors contributing to performance issues

- Design tradeoffs:
  - Model size vs. generalization: Larger models may overfit, while smaller models might lack capacity
  - Tokenization strategy: Different schemes impact sense discrimination and overall performance
  - Dataset characteristics: Open-domain training may not prepare models for closed-domain complexities

- Failure signatures:
  - Consistently shorter answer spans in closed domains
  - Poor sense discrimination for domain-specific terms
  - High perplexity scores on closed-domain datasets
  - Large performance gaps between open and closed-domain tasks

- First 3 experiments:
  1. Answer length analysis: Compare predicted vs. gold answer lengths across datasets
  2. Sense discrimination test: Evaluate model's ability to distinguish domain-specific word senses
  3. Dataset similarity analysis: Use FDA, TEXT/TASK embeddings, and perplexity to quantify dataset differences

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis of tokenization impact relies on comparisons across pre-existing architectures rather than controlled experiments with identical model backbones
- Sense discrimination experiments use a relatively small polysemy dataset that may not capture full complexity of domain-specific language
- Correlation between perplexity and performance does not establish causation - higher perplexity could be a symptom rather than a cause of poor performance

## Confidence
- **High confidence**: The finding that closed-domain datasets differ significantly from open-domain ones (supported by multiple dataset similarity metrics)
- **Medium confidence**: The observation that larger models don't necessarily perform better in cross-domain generalization (based on comparative experiments across multiple model sizes)
- **Low confidence**: The specific claim that byte-level BPE tokenization is the primary cause of poor sense discrimination (based on indirect comparisons rather than controlled experiments)

## Next Checks
1. **Controlled tokenization experiment**: Implement identical model architectures (e.g., BERT-base) with different tokenization schemes (BPE vs SentencePiece) and evaluate their performance on domain-specific sense discrimination tasks to isolate tokenization effects.

2. **Perplexity intervention study**: Fine-tune models on in-domain data and measure how perplexity changes correlate with performance improvements, testing whether perplexity is causal or merely indicative of other factors.

3. **Answer length distribution analysis**: Systematically compare predicted vs. gold answer length distributions across domains and model sizes to determine whether length mismatch is a consistent failure mode and whether specific architectural modifications could address it.