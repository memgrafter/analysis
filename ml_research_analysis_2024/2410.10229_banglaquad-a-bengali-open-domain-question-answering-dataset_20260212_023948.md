---
ver: rpa2
title: 'BanglaQuAD: A Bengali Open-domain Question Answering Dataset'
arxiv_id: '2410.10229'
source_url: https://arxiv.org/abs/2410.10229
tags:
- bengali
- question
- dataset
- answering
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BanglaQuAD, a high-quality Bengali question-answering
  dataset with 30,808 human-annotated pairs, constructed from Bengali Wikipedia articles
  by native speakers. To address the lack of quality Bengali QA datasets, the authors
  developed an annotation tool, BnAnno, that allows direct annotation from unstructured
  text without pre-processing.
---

# BanglaQuAD: A Bengali Open-domain Question Answering Dataset

## Quick Facts
- arXiv ID: 2410.10229
- Source URL: https://arxiv.org/abs/2410.10229
- Reference count: 0
- Primary result: Introduces BanglaQuAD, a high-quality Bengali QA dataset with 30,808 human-annotated pairs

## Executive Summary
BanglaQuAD addresses the critical need for high-quality Bengali question-answering datasets by providing 30,808 human-annotated QA pairs constructed from Bengali Wikipedia articles by native speakers. The dataset covers diverse topics across 14 categories and includes various question types and unanswerable questions to improve QA system robustness. The authors developed BnAnno, a novel annotation tool that allows direct annotation from unstructured text without pre-processing requirements, making local dataset construction more efficient. Experimental results show that baseline models (BanglaBERT and IndicBERT) achieve lower performance on BanglaQuAD compared to existing Bengali datasets, indicating its challenging nature.

## Method Summary
The paper introduces BanglaQuAD, a Bengali open-domain QA dataset constructed through human annotation by native speakers using the BnAnno annotation tool. The process involves curating 658 Bengali Wikipedia articles across 14 major topics and 114 subcategories, followed by annotation of question-answer pairs directly from the articles' text. The annotation tool allows annotators to work with unstructured text without pre-processing requirements. The dataset is split into training, validation, and test sets, and baseline models (BanglaBERT and IndicBERT) are fine-tuned and evaluated on the dataset using Exact Match and F1 metrics.

## Key Results
- BanglaQuAD contains 30,808 high-quality human-annotated QA pairs from Bengali Wikipedia articles
- Baseline models (BanglaBERT and IndicBERT) achieve lower performance on BanglaQuAD compared to existing Bengali QA datasets
- Dataset covers 14 major topics and 114 subcategories, ensuring diverse topic coverage
- Includes unanswerable questions to improve QA system robustness
- Achieves substantial inter-annotator agreement (Cohen's kappa of 0.79) for answer span selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Native speaker annotation produces higher quality QA pairs than translation-based datasets
- Mechanism: Native Bengali speakers construct questions and answers directly from Bengali text, avoiding translation errors and preserving grammatical correctness
- Core assumption: Native speakers can better identify relevant facts and create natural-sounding questions in Bengali
- Evidence anchors:
  - [abstract] "Typically, existing approaches construct the dataset by directly translating them from English to Bengali, which produces noisy and improper sentence structures"
  - [section] "Human annotators are employed to construct question-answer pairs from passages of the curated articles. This ensures that the data is of the best possible quality"
  - [corpus] Corpus evidence shows related datasets suffer from translation artifacts, supporting the need for native annotation
- Break condition: If annotators lack domain knowledge or if annotation guidelines are unclear, quality could degrade

### Mechanism 2
- Claim: Diverse topic coverage improves QA system generalization
- Mechanism: Dataset construction from 658 Wikipedia articles across 14 categories and 114 subcategories ensures broad vocabulary and topic variety
- Core assumption: Wider topic coverage forces models to learn more generalizable patterns rather than overfitting to narrow domains
- Evidence anchors:
  - [abstract] "BanglaQuAD contains 30,808 high-quality human-annotated question-answer pairs constructed from Bengali Wikipedia articles by native speakers"
  - [section] "This selection process ensures a broader coverage of topics, resulting in a wide range of vocabulary from different topics"
  - [corpus] Corpus shows Bengali QA systems typically have limited topic diversity, making this mechanism novel
- Break condition: If article selection becomes too narrow or biased, diversity benefits diminish

### Mechanism 3
- Claim: BnAnno annotation tool enables efficient local dataset construction
- Mechanism: Tool supports direct annotation from unstructured text without pre-processing requirements, reducing annotation friction
- Core assumption: Lower annotation barriers increase participation and dataset size
- Evidence anchors:
  - [abstract] "we propose an annotation tool that facilitates question-answering dataset construction on a local machine"
  - [section] "The standard cdQA tool requires the users to pre-process the data into SQuAD-like format in order to use the tool. In contrast, in BnAnno the user can copy any text document to the tool and start the annotation right away"
  - [corpus] Limited corpus evidence, but tool design addresses known bottleneck in QA dataset creation
- Break condition: If tool usability issues emerge or if unstructured text handling proves unreliable

## Foundational Learning

- Concept: Question Answering over unstructured text
  - Why needed here: BanglaQuAD is designed for extractive QA tasks where systems must locate answer spans in passages
  - Quick check question: Can you explain the difference between extractive QA and generative QA?

- Concept: Dataset quality metrics (EM, F1, Cohen's kappa)
  - Why needed here: Paper uses these metrics to evaluate both model performance and annotation agreement
  - Quick check question: What does a Cohen's kappa of 0.79 indicate about inter-annotator agreement?

- Concept: Low-resource language challenges
  - Why needed here: Bengali is considered low-resource, making high-quality datasets critical for NLP advancement
  - Quick check question: Why do low-resource languages face more challenges in NLP compared to high-resource languages?

## Architecture Onboarding

- Component map: Bengali Wikipedia articles → Article curation → Annotation tool (BnAnno) → JSON output → Training/validation split → Baseline model evaluation
- Critical path: Article selection → Annotation process → Quality control → Dataset construction → Model training and evaluation
- Design tradeoffs: Human annotation provides quality but is slower than automated methods; diverse topics increase complexity but improve generalization
- Failure signatures: Poor model performance on test set indicates dataset difficulty; low inter-annotator agreement suggests annotation guideline issues
- First 3 experiments:
  1. Evaluate baseline model performance on BanglaQuAD test set to establish benchmark
  2. Test BnAnno tool with small document sample to verify annotation workflow
  3. Analyze question type distribution to ensure coverage matches design goals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BanglaQuAD's performance on diverse question types compare to existing Bengali QA datasets?
- Basis in paper: [explicit] The paper mentions that BanglaQuAD includes diverse question types and unanswerable questions to improve QA system robustness, and baseline models achieve lower performance on BanglaQuAD compared to existing datasets.
- Why unresolved: While the paper reports baseline model performance, it does not provide a detailed breakdown of performance across different question types or directly compare performance on question type diversity between BanglaQuAD and existing datasets.
- What evidence would resolve it: A detailed performance analysis comparing baseline models' accuracy on different question types (what, when, where, who, why, how) across BanglaQuAD and existing Bengali QA datasets would resolve this.

### Open Question 2
- Question: What is the impact of including unanswerable questions on the overall quality and robustness of BanglaQuAD?
- Basis in paper: [explicit] The paper states that BanglaQuAD includes unanswerable questions to improve QA system robustness and mentions an inter-annotator agreement of 0.79 for answer span selection, including unanswerable questions.
- Why unresolved: The paper does not provide quantitative analysis of how the inclusion of unanswerable questions affects model performance or dataset quality metrics.
- What evidence would resolve it: A comparative analysis of model performance and dataset quality metrics with and without unanswerable questions would resolve this.

### Open Question 3
- Question: How does the coverage of Bengali-specific topics and terminologies in BanglaQuAD compare to other Bengali QA datasets?
- Basis in paper: [explicit] The paper states that existing datasets lack topics and terminologies related to the Bengali language and people, while BanglaQuAD covers 14 major topics including Bengali-specific domains.
- Why unresolved: The paper does not provide a direct comparison of Bengali-specific topic coverage between BanglaQuAD and existing datasets.
- What evidence would resolve it: A detailed analysis comparing the coverage of Bengali-specific topics and terminologies in BanglaQuAD versus existing Bengali QA datasets would resolve this.

## Limitations

- Language-Specific Constraints: Dataset construction relies heavily on Bengali Wikipedia articles, which may not fully represent the broader Bengali language landscape
- Annotation Quality Uncertainty: Cohen's kappa of 0.79 was calculated across only 60 samples from a single annotator pair
- Dataset Size Constraints: With 30,808 pairs from 658 articles, represents approximately 47 question-answer pairs per article on average

## Confidence

**High Confidence** (Multiple evidence anchors, well-established mechanisms):
- Native speaker annotation produces higher quality QA pairs than translation-based approaches
- Diverse topic coverage improves QA system generalization
- Lower baseline performance on BanglaQuAD indicates dataset difficulty

**Medium Confidence** (Single evidence anchor, emerging mechanisms):
- BnAnno annotation tool enables efficient local dataset construction
- The challenging nature of BanglaQuAD relative to existing datasets

**Low Confidence** (No direct evidence, speculative mechanisms):
- Dataset coverage represents comprehensive Bengali language diversity
- The specific difficulty distribution across question types is well-characterized

## Next Checks

1. **Inter-Annotator Agreement Validation**: Calculate Cohen's kappa across a larger sample (minimum 200 pairs) from multiple annotator pairs to verify the consistency of annotation quality across the entire dataset.

2. **Model Performance Replication**: Reproduce the baseline model evaluation using different hyperparameters and training procedures to confirm that the reported performance gaps are robust and not sensitive to implementation details.

3. **Coverage Analysis**: Conduct a comprehensive vocabulary and topic coverage analysis comparing BanglaQuAD against both Bengali Wikipedia as a whole and existing Bengali QA datasets to quantify the claimed diversity improvements.