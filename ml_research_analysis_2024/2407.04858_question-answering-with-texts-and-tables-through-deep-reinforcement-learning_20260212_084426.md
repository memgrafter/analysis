---
ver: rpa2
title: Question Answering with Texts and Tables through Deep Reinforcement Learning
arxiv_id: '2407.04858'
source_url: https://arxiv.org/abs/2407.04858
tags:
- question
- bm25
- tables
- retriever
- reader
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of answering multi-hop questions
  that require information from both text and tables in an open-domain setting. The
  authors propose a novel deep reinforcement learning (DRL) architecture that sequentially
  selects among three pre-trained modules: Text Retriever, Table Retriever, and Reader.'
---

# Question Answering with Texts and Tables through Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.04858
- Source URL: https://arxiv.org/abs/2407.04858
- Reference count: 38
- Primary result: Proposed DRL architecture achieves F1-score of 19.03 on OTT-QA dataset for multi-hop QA over texts and tables

## Executive Summary
This paper addresses the challenge of answering multi-hop questions requiring information from both text and tables in open-domain settings. The authors propose a novel deep reinforcement learning architecture that sequentially selects among three pre-trained modules: Text Retriever, Table Retriever, and Reader. The agent learns when to use each component through experience without requiring labeled action sequences. The system achieves competitive performance with existing iterative approaches while offering flexibility for modular improvements.

## Method Summary
The proposed method employs a deep reinforcement learning agent that chooses between pre-trained Text Retriever, Table Retriever, and Reader modules sequentially. The agent receives a state representation consisting of averaged embeddings from question and retrieved content blocks, then selects actions to either retrieve more text, retrieve tables, or generate an answer. Training uses delayed rewards based on final answer quality compared to ground truth. The architecture allows swapping individual components without retraining the RL agent, enabling system improvements as better models become available.

## Key Results
- Achieves F1-score of 19.03 on OTT-QA dataset, comparable to existing iterative systems
- Demonstrates sequential decision-making can effectively coordinate between different retrieval and reading modules
- Shows flexibility for modular improvement by incorporating additional components or replacing existing modules
- Performance varies significantly based on retriever quality, with Tri-encoder underperforming BM25

## Why This Works (Mechanism)

### Mechanism 1
Reinforcement learning enables adaptive module selection for multi-hop QA without requiring labeled action sequences. The DRL agent learns optimal sequences of retrieval actions through trial-and-error, maximizing cumulative reward based on final answer quality. Core assumption: reward signal based on F1-score provides sufficient feedback for learning effective retrieval strategies. Break condition: sparse or noisy reward may lead to suboptimal or random action selection.

### Mechanism 2
Sequential decision-making with pre-trained modules allows modular system improvement. The architecture decouples decision-making from content understanding, allowing individual modules to be swapped or upgraded without retraining the RL agent. Core assumption: the RL agent can generalize across different module behaviors. Break condition: if the RL agent's policy becomes too tightly coupled to specific module characteristics, it may fail when modules are significantly changed.

### Mechanism 3
State representation using averaged embeddings captures sufficient information for action selection. The agent observes a sequence of 11 vectors (question + 10 blocks), where each block is represented by averaging embeddings of its passages. Core assumption: averaging embeddings within blocks preserves sufficient semantic information. Break condition: if critical information is lost through averaging, the agent may make poor decisions based on incomplete state understanding.

## Foundational Learning

- Concept: Reinforcement learning fundamentals (states, actions, rewards, policies)
  - Why needed here: The system's core innovation is using DRL to learn when to invoke different QA modules without labeled action sequences
  - Quick check question: What is the difference between model-free and model-based reinforcement learning, and which approach does this paper use?

- Concept: Multi-hop reasoning and information retrieval
  - Why needed here: The QA task requires finding and combining information from multiple text and table passages in sequence
  - Quick check question: How does iterative retrieval differ from single-pass retrieval in open-domain QA?

- Concept: Transformer-based embeddings and semantic search
  - Why needed here: The state representation relies on MPNet and Tri-encoder embeddings to convert text into numerical vectors for the RL agent
  - Quick check question: What is the key difference between dense retrieval (DPR) and sparse retrieval (BM25) approaches?

## Architecture Onboarding

- Component map: Question → RL Agent → Action → Retriever → Blocks → (repeat) → Reader → Answer → Reward
- Critical path: Question flows through RL Agent decision-making, triggering Retriever actions to gather Blocks, with iterative retrieval until Reader generates final Answer, followed by Reward calculation
- Design tradeoffs: Fixed action sequences (baselines) vs. learned policies (RL): simplicity vs. adaptability; Embedding averaging vs. sequence encoding: computational efficiency vs. information preservation; Limited retrieval steps (3) vs. unlimited: noise control vs. completeness
- Failure signatures: RL agent consistently chooses only one action type (convergence to trivial solution); Performance drops significantly when switching retrievers; State representation fails to distinguish between different retrieval needs
- First 3 experiments: 1) Implement baseline fixed action sequences using BM25 retriever to establish performance floor; 2) Train DQN agent with MLP network using BM25 to verify RL approach works with simple architecture; 3) Compare PPO vs DQN performance with the same network architecture to identify optimal training algorithm

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the DRL agent change when trained for longer periods with the BM25 retriever? The paper mentions that only 100k training steps were used with BM25 due to higher inference time, but suggests this may be insufficient. This remains unresolved due to time constraints preventing multiple runs or longer training sessions.

### Open Question 2
Would breaking tables into smaller content sets (e.g., rows) improve the performance of table retrieval in the sequential approach? The paper discusses that table retrieval proved noisy, especially in iterative search, as tables often contain excessive data with relevant information typically in a single row. This modification was not implemented to test its impact.

### Open Question 3
How would the DRL agent perform if integrated with the COS architecture, which employs diverse retrieval strategies? The paper suggests that integrating the DRL agent with the COS architecture could enhance performance, as COS excels in OTT-QA by employing diverse retrieval strategies. This integration was not implemented or tested in the study.

## Limitations
- Relatively low F1-score of 19.03 indicates significant room for improvement compared to non-sequential methods
- Performance highly sensitive to retriever quality, with Tri-encoder trained on single-hop data underperforming BM25
- Reliance on averaged embeddings for state representation may lose critical information needed for optimal action selection

## Confidence

**Confidence Levels:**
- **High Confidence**: The modular architecture design and its flexibility for component replacement is well-supported by the paper's methodology and implementation details
- **Medium Confidence**: The sequential decision-making framework using DRL is theoretically sound, but empirical validation is limited to a single dataset
- **Low Confidence**: Claims about the effectiveness of embedding averaging for state representation lack sufficient empirical validation, particularly regarding information preservation

## Next Checks

1. Conduct ablation studies comparing the averaging-based state representation against sequence-aware alternatives (e.g., LSTM/Transformer encoders) to quantify information loss
2. Test the trained RL agent with significantly different retriever modules to evaluate true modularity and generalization across component variations
3. Implement a hybrid approach combining sequential decisions with non-sequential modules to empirically validate the suggested future direction for improved performance