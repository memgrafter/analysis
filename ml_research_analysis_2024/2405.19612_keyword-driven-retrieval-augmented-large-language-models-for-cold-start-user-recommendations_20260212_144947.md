---
ver: rpa2
title: Keyword-driven Retrieval-Augmented Large Language Models for Cold-start User
  Recommendations
arxiv_id: '2405.19612'
source_url: https://arxiv.org/abs/2405.19612
tags:
- user
- keywords
- restaurant
- candidates
- cold-start
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses cold-start user recommendation by leveraging
  large language models (LLMs) augmented with keyword-driven retrieval. The proposed
  KALM4Rec framework operates in two stages: first, a Message Passing on Graph (MPG)
  model retrieves candidate items using keyword embeddings and a TF-IRF weighting
  scheme; second, LLMs re-rank these candidates via structured prompts incorporating
  user and item keywords.'
---

# Keyword-driven Retrieval-Augmented Large Language Models for Cold-start User Recommendations

## Quick Facts
- arXiv ID: 2405.19612
- Source URL: https://arxiv.org/abs/2405.19612
- Reference count: 20
- Key outcome: KALM4Rec framework combines MPG retrieval with LLM re-ranking to achieve up to 14.54% precision@20 on TripAdvisor and 61.69% precision@1 on Yelp-Edinburgh for cold-start recommendations

## Executive Summary
This paper addresses cold-start user recommendation by leveraging large language models (LLMs) augmented with keyword-driven retrieval. The proposed KALM4Rec framework operates in two stages: first, a Message Passing on Graph (MPG) model retrieves candidate items using keyword embeddings and a TF-IRF weighting scheme; second, LLMs re-rank these candidates via structured prompts incorporating user and item keywords. Experiments on Yelp and TripAdvisor datasets show that KALM4Rec significantly outperforms baseline retrieval methods, with MPG achieving up to 14.54% precision@20 on TripAdvisor and 12.73% on Yelp-London. LLM re-ranking further improves performance, with Gemini Pro achieving precision@1 scores up to 61.69% on Yelp-Edinburgh. The study also highlights the importance of keyword order and prompt design, showing that few-shot prompts with ordered keywords yield the best results.

## Method Summary
KALM4Rec is a two-stage cold-start recommendation framework. First, MPG retrieval constructs a heterogeneous graph between keywords and items, using TF-IRF weighting to estimate keyword-item importance. This retrieves top-20 candidates per user using sBERT embeddings. Second, LLMs re-rank these candidates via structured prompts containing user keywords, candidate item keywords (ordered by TF-IRF scores), and few-shot examples. The framework uses SpaCy for keyword extraction from reviews and supports both zero-shot and few-shot prompting strategies with models like Gemini Pro and GPT-3.5.

## Key Results
- MPG retrieval achieves 14.54% precision@20 on TripAdvisor and 12.73% on Yelp-London
- LLM re-ranking with Gemini Pro reaches 61.69% precision@1 on Yelp-Edinburgh
- Few-shot prompts with ordered keywords outperform zero-shot and shuffled keyword approaches
- KALM4Rec significantly outperforms baseline Jaccard similarity and LLM-only methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Keyword-driven retrieval (MPG) reduces token usage while preserving semantic context.
- Mechanism: MPG constructs a heterogeneous graph between keywords and items, using TF-IRF weighting to estimate keyword-item importance. This replaces full-text reviews with compressed keyword sets, enabling efficient similarity search.
- Core assumption: Keywords extracted from reviews (noun phrases) retain sufficient user preference information for recommendation tasks.
- Evidence anchors:
  - [abstract] "using keywords instead of entire reviews to prompt LLMs can enhance the efficiency and accuracy of recommendations while minimizing token usage."
  - [section] "Using extracted keywords exclusively, we retain the contextual essence of the reviews while still describing the user profile well."
  - [corpus] Weak evidence: no direct comparison of keyword vs full-text token counts in corpus.
- Break condition: If extracted keywords omit critical preference signals, MPG's semantic matching will fail, leading to irrelevant candidate retrieval.

### Mechanism 2
- Claim: LLM re-ranking with ordered keywords and few-shot prompts improves recommendation quality.
- Mechanism: LLMs receive structured prompts containing user keywords, candidate item keywords, and few-shot examples. Ordered keywords (by TF-IRF score) help LLMs prioritize important features during reasoning.
- Core assumption: LLMs can effectively interpret and rank candidates based on keyword-based user and item profiles.
- Evidence anchors:
  - [abstract] "the importance of keyword order and prompt design, showing that few-shot prompts with ordered keywords yield the best results."
  - [section] "keywords are ranked by their TF-IRF scores. These keywords help LLMs capture nuanced user preferences and item attributes, enabling high-quality re-ranking."
  - [corpus] Moderate evidence: corpus includes related work on RAG and keyword-based prompting but no direct keyword-order ablation.
- Break condition: If LLM's reasoning capacity is exceeded by keyword complexity or if few-shot examples are unrepresentative, re-ranking performance degrades.

### Mechanism 3
- Claim: TF-IRF weighting scheme balances keyword importance across users and items.
- Mechanism: TF-IRF combines term frequency within an item's keyword set with inverse relevance frequency across all items, similar to TF-IDF but adapted for recommendation context.
- Core assumption: Keyword importance in user reviews correlates with item relevance for that user.
- Evidence anchors:
  - [section] "We introduce a scheme named TF-IRF to measure the importance score of a keyword to an item, which is similar to the TF-IDF idea..."
  - [section] "For an item node, information is generated by summing up the information of the adjacency keyword node..."
  - [corpus] Weak evidence: corpus lacks detailed TF-IRF performance analysis.
- Break condition: If keyword distributions are highly skewed or if rare but important keywords are undervalued, TF-IRF weighting becomes ineffective.

## Foundational Learning

- Concept: Graph neural networks for recommendation
  - Why needed here: MPG relies on graph-based information propagation between keywords and items without training parameters
  - Quick check question: How does LightGCN's weighted sum aggregator differ from standard GCN convolution in terms of parameter efficiency?

- Concept: Large language model prompting strategies
  - Why needed here: KALM4Rec uses zero-shot and few-shot prompting to guide LLM re-ranking with keyword inputs
  - Quick check question: What's the key difference between in-context learning and fine-tuning for LLM adaptation?

- Concept: Information retrieval metrics (precision, recall, NDCG)
  - Why needed here: Evaluation of both retrieval and re-ranking stages requires understanding of ranking quality metrics
  - Quick check question: Why might P@1 be more critical than P@20 for cold-start scenarios?

## Architecture Onboarding

- Component map: SpaCy keyword extractor → sBERT vectorizer → MPG graph construction → Candidate retrieval → Prompt generator → LLM re-ranker → Final recommendations
- Critical path: Keyword extraction → MPG retrieval → LLM re-ranking
- Design tradeoffs:
  - Efficiency vs accuracy: Keyword extraction reduces token usage but may lose nuance
  - Retrieval vs ranking: MPG focuses on recall while LLM re-ranking optimizes precision
  - Prompt complexity vs LLM capability: More structured prompts help but increase token costs
- Failure signatures:
  - Low recall in retrieval stage: Check keyword extraction quality and TF-IRF weighting
  - LLM re-ranking produces irrelevant results: Verify keyword ordering and few-shot example quality
  - Performance varies across cities: Investigate dataset characteristics and keyword distributions
- First 3 experiments:
  1. Compare MPG with Jaccard similarity baseline on keyword overlap
  2. Test zero-shot vs few-shot LLM re-ranking performance
  3. Evaluate impact of keyword order (shuffled vs TF-IRF ordered) on re-ranking quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different keyword extraction methods (beyond SpaCy's noun phrase extraction) impact the performance of KALM4Rec in cold-start scenarios?
- Basis in paper: [inferred] The paper uses SpaCy to extract keywords from reviews but does not explore alternative keyword extraction techniques or compare their effectiveness.
- Why unresolved: The paper focuses on a specific keyword extraction method without investigating whether other approaches (e.g., dependency parsing, TF-IDF-based selection, or semantic keyword extraction) could yield better representations of user preferences and item characteristics.
- What evidence would resolve it: Comparative experiments testing KALM4Rec with different keyword extraction methods (e.g., TF-IDF, semantic embeddings, or domain-specific extraction) on the same datasets would clarify which approach optimizes performance.

### Open Question 2
- Question: What is the optimal balance between retrieval precision and recall for KALM4Rec's two-stage pipeline, and how does this balance affect downstream re-ranking performance?
- Basis in paper: [explicit] The paper notes that "The goal of candidates retrieval is not only to achieve a high ranking (precision) but also to retrieve as many correct candidates as possible (recall)" but does not systematically explore the trade-off between these metrics.
- Why unresolved: While the paper demonstrates that MPG improves both precision and recall, it doesn't investigate how varying the number of retrieved candidates affects LLM re-ranking effectiveness or whether there's an optimal retrieval threshold.
- What evidence would resolve it: Systematic experiments varying the number of retrieved candidates (e.g., top-10, top-50, top-100) and measuring their impact on LLM re-ranking performance would identify optimal retrieval parameters.

### Open Question 3
- Question: How does KALM4Rec perform on datasets with different user-item interaction patterns, such as those with sparse versus dense interaction matrices?
- Basis in paper: [inferred] The experiments use Yelp and TripAdvisor datasets but don't analyze how interaction density affects performance, despite cold-start being fundamentally a sparsity problem.
- Why unresolved: The paper demonstrates effectiveness on specific datasets but doesn't characterize performance across varying levels of user-item interaction density, which is crucial for understanding generalizability.
- What evidence would resolve it: Experiments on datasets with systematically varied interaction densities (e.g., artificially sparsified versions of existing datasets or datasets with different interaction patterns) would reveal how interaction patterns affect KALM4Rec's effectiveness.

## Limitations
- The TF-IRF weighting scheme lacks detailed empirical validation against alternative keyword importance measures
- No direct comparison of token efficiency between keyword-based and full-text approaches
- Performance sensitivity to keyword extraction quality is not thoroughly investigated

## Confidence
- **High**: MPG retrieval effectiveness (14.54% P@20 on TripAdvisor, 12.73% on Yelp-London)
- **Medium**: LLM re-ranking improvements with ordered keywords and few-shot prompts (up to 61.69% P@1 on Yelp-Edinburgh)
- **Low**: TF-IRF weighting scheme effectiveness and keyword extraction quality impact

## Next Checks
1. **Ablation Study**: Compare KALM4Rec performance when using full reviews versus extracted keywords to quantify the claimed token efficiency gains and verify if semantic information is preserved.
2. **Keyword Order Sensitivity**: Systematically test re-ranking performance with randomized keyword orders versus TF-IRF ordered keywords across multiple LLM models to confirm the importance of keyword ranking.
3. **Cross-Dataset Generalization**: Evaluate KALM4Rec on datasets with different domain characteristics (e.g., non-English reviews, different recommendation contexts) to assess the framework's robustness beyond the tested Yelp and TripAdvisor data.