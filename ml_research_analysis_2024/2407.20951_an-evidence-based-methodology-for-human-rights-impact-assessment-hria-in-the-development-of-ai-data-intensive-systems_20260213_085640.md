---
ver: rpa2
title: An evidence-based methodology for human rights impact assessment (HRIA) in
  the development of AI data-intensive systems
arxiv_id: '2407.20951'
source_url: https://arxiv.org/abs/2407.20951
tags:
- data
- rights
- human
- impact
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a human rights impact assessment (HRIA) methodology
  tailored for AI data-intensive systems, addressing the need for measurable, context-specific
  regulatory tools beyond existing ethical frameworks. The approach is grounded in
  empirical analysis of over 700 data protection authority decisions across six European
  countries, identifying concrete rights affected by data processing.
---

# An evidence-based methodology for human rights impact assessment (HRIA) in the development of AI data-intensive systems

## Quick Facts
- arXiv ID: 2407.20951
- Source URL: https://arxiv.org/abs/2407.20951
- Reference count: 0
- An evidence-based HRIA methodology for AI data-intensive systems grounded in empirical analysis of 700+ DPA decisions

## Executive Summary
This study develops a human rights impact assessment (HRIA) methodology tailored for AI data-intensive systems, addressing the need for measurable, context-specific regulatory tools beyond existing ethical frameworks. The approach is grounded in empirical analysis of over 700 data protection authority decisions across six European countries, identifying concrete rights affected by data processing. The proposed model enables iterative, quantifiable risk assessment by evaluating likelihood and severity of impacts on rights like privacy, non-discrimination, and autonomy, using structured scoring tables. Testing on a small-scale case (Hello Barbie) demonstrated its ability to guide design choices and reduce risks, while application to large-scale projects (e.g., smart cities) highlighted the need for integrated, third-party assessments to capture cumulative socio-technical impacts.

## Method Summary
The methodology combines qualitative empirical legal research with structured quantitative risk assessment. It begins with planning and scoping questions, then collects data through analysis of DPA decisions to identify relevant human rights. Structured likelihood-severity scoring tables (using 1-4 scales) are populated to calculate overall impact, visualized via radial graphs. Mitigation measures are applied iteratively, with residual risk re-assessed and compared to initial scores. The model was tested on two case studies: a connected doll (Hello Barbie) and a smart city project (Sidewalk Labs), with additional guidance provided for large-scale multi-factor scenarios requiring integrated assessment and independent third-party review.

## Key Results
- Empirical analysis of 700+ DPA decisions across six European countries identified concrete rights affected by data processing
- Structured likelihood-severity tables enable quantifiable, repeatable risk scoring for AI systems
- Iterative mitigation and re-assessment demonstrated effectiveness in guiding design choices and reducing risks
- Large-scale applications revealed need for integrated HRIA approaches and third-party assessments for complex scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Empirical analysis of DPA decisions grounds the HRIA in actual rights affected by data-intensive systems.
- Mechanism: By mining over 700 real-world decisions, the model identifies concrete rights and violations rather than relying on theoretical constructs.
- Core assumption: DPA decisions consistently reveal the interplay between data use and human rights, even if human rights are not always explicitly cited.
- Evidence anchors:
  - [abstract] "empirical analysis of over 700 data protection authority decisions across six European countries"
  - [section 4.2] "analysis of the documents did find common ground between them in their approach to human rights and freedoms"
- Break condition: If DPA decisions systematically underrepresent certain rights, the model will miss them.

### Mechanism 2
- Claim: Structured likelihood-severity tables produce a repeatable, comparable risk score.
- Mechanism: Combining exposure, probability, gravity, and effort into combinatorial tables yields cardinal L and S values that can be aggregated into an overall impact rating.
- Core assumption: The four-point scales and combinatorial logic adequately capture risk complexity.
- Evidence anchors:
  - [section 5.2] "The expected impact of the identified risks is assessed by considering both the likelihood and the severity of the expected consequences, using a four-step scale"
  - [section 6.1.2.4] "Table of envisaged risks" and "Comparative risk impact analysis table"
- Break condition: If risks are highly multidimensional and resist ordinal reduction, the model will oversimplify.

### Mechanism 3
- Claim: Iterative mitigation and re-assessment closes the loop between design and rights protection.
- Mechanism: After initial scoring, mitigation measures and excluding factors are applied, residual risk is recomputed, and outcomes are visualized via radial graphs for clear before/after comparison.
- Core assumption: Mitigation steps can meaningfully alter L and S values, and visual comparison drives better design choices.
- Evidence anchors:
  - [section 5.2] "After the first adoption of the appropriate measures to mitigate the risk, further rounds of assessment can be conducted"
  - [section 6.1.3] "The following table shows the assessment of the different impacts, comparing the results before and after the adoption of mitigation measures"
- Break condition: If mitigation measures are infeasible or ineffective, re-assessment will show negligible change.

## Foundational Learning

- Concept: Data Protection Authority (DPA) jurisprudence
  - Why needed here: The model relies on DPA decisions as the empirical evidence base; understanding their scope and limitations is critical.
  - Quick check question: What is the main legal instrument referenced in European DPA decisions, and why does it matter for this model?

- Concept: Likelihood-severity combinatorial matrices
  - Why needed here: The model's risk scoring depends on correctly populating and interpreting these matrices.
  - Quick check question: How do exposure and probability combine to yield the overall likelihood score?

- Concept: Radial graph visualization
  - Why needed here: The final output is a radial graph comparing impacts across rights; interpreting it is essential for decision-making.
  - Quick check question: In a radial graph, what does a longer spoke represent?

## Architecture Onboarding

- Component map: Planning & Scoping (description, human rights context, stakeholder ID) -> Data Collection & Analysis (risk ID, likelihood table, severity table, impact table, radial graph) -> Mitigation & Re-assessment (exclude/mitigate, residual scoring, before/after comparison) -> Large-scale Multi-factor Add-on (integrated HRIA, independent third-party review)

- Critical path:
  1. Define product/service and collect baseline data
  2. Identify relevant rights via DPA jurisprudence
  3. Populate likelihood and severity tables
  4. Compute overall impact and generate radial graph
  5. Apply mitigation, re-score, and compare

- Design tradeoffs:
  - Granularity vs. scalability: detailed per-right scoring is more accurate but slower for large projects.
  - Internal vs. external assessment: self-assessment is faster but may lack independence for multi-factor cases.
  - Closed vs. open dialogue content: closed sets reduce risk but may limit user experience.

- Failure signatures:
  - Missing rights: no impact recorded for a known relevant right → likely due to DPA decision bias.
  - Uniform scores: all risks rated low/medium → likely data collection or scaling issue.
  - No mitigation effect: residual scores unchanged after mitigation → mitigation measures ineffective or incorrectly applied.

- First 3 experiments:
  1. Single-feature toy (e.g., connected speaker): run full HRIA, verify radial graph output.
  2. Multi-feature device (e.g., AI assistant with camera): test likelihood/severity table population, check combinatorial logic.
  3. Simple multi-factor scenario (e.g., smart home kit): attempt integrated assessment, evaluate need for third-party review.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the HRIA model be adapted to assess AI applications that involve multiple interconnected devices and services, such as those found in smart cities?
- Basis in paper: [explicit] The paper discusses the challenges of assessing large-scale, multi-factor AI scenarios, particularly in the context of smart cities.
- Why unresolved: The paper acknowledges the need for an integrated HRIA approach but does not provide specific guidance on how to adapt the model for such complex scenarios.
- What evidence would resolve it: Case studies and practical examples of applying the HRIA model to assess interconnected AI systems in smart cities, along with detailed methodologies for integrating the assessments of individual components.

### Open Question 2
- Question: What are the most effective methods for engaging stakeholders in the HRIA process, particularly in the context of AI applications that may impact diverse and potentially vulnerable populations?
- Basis in paper: [explicit] The paper emphasizes the importance of stakeholder engagement in the HRIA process but does not provide specific guidance on how to effectively engage diverse and vulnerable populations.
- Why unresolved: The paper highlights the need for inclusivity and transparency in stakeholder engagement but does not offer concrete strategies for reaching and involving different groups, especially those who may be less familiar with AI technologies or less likely to participate in consultations.
- What evidence would resolve it: Research on best practices for stakeholder engagement in AI impact assessments, including case studies and practical tools for reaching and involving diverse and vulnerable populations.

### Open Question 3
- Question: How can the HRIA model be used to assess the potential for AI applications to perpetuate or exacerbate existing biases and discrimination, particularly in areas such as hiring, lending, and criminal justice?
- Basis in paper: [explicit] The paper acknowledges the potential for AI to perpetuate discrimination but does not provide specific guidance on how to assess and mitigate this risk within the HRIA model.
- Why unresolved: While the paper discusses the importance of addressing bias and discrimination in AI, it does not offer concrete methods for identifying and evaluating these risks within the HRIA framework.
- What evidence would resolve it: Research on methods for detecting and measuring bias in AI systems, along with case studies of applying the HRIA model to assess and mitigate discrimination risks in specific domains like hiring, lending, and criminal justice.

## Limitations
- Empirical scope limited to six European countries, potentially missing rights concerns prevalent in non-European contexts
- Scalability challenges revealed when applying methodology to large-scale, multi-factor scenarios like smart cities
- Mitigation effectiveness remains largely theoretical, depending on practical implementation of proposed measures

## Confidence
- High Confidence: The empirical methodology of deriving rights from actual DPA decisions is methodologically sound and well-documented
- Medium Confidence: The model's effectiveness in guiding real-world design decisions is supported by the Hello Barbie case study, but this is a single, relatively simple example
- Low Confidence: The generalizability of the model beyond European legal frameworks and the practical effectiveness of mitigation measures in diverse real-world contexts have not been validated

## Next Checks
1. Cross-jurisdictional Validation: Apply the HRIA methodology to AI projects in non-European contexts (e.g., US, Asia-Pacific) and compare the rights identified and risk assessments with those derived from European DPA decisions.
2. Independent Replication: Have an independent team apply the HRIA methodology to the same case studies (Hello Barbie and a smart city project) without author guidance.
3. Longitudinal Effectiveness Study: Track AI projects that implemented HRIA-guided design changes over 6-12 months to assess whether the predicted risk reductions materialize in practice.