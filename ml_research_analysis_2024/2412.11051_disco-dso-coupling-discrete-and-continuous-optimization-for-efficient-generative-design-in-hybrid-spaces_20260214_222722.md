---
ver: rpa2
title: 'DisCo-DSO: Coupling Discrete and Continuous Optimization for Efficient Generative
  Design in Hybrid Spaces'
arxiv_id: '2412.11051'
source_url: https://arxiv.org/abs/2412.11051
tags:
- decision
- disco-dso
- optimization
- learning
- latexit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of black-box optimization in hybrid
  discrete-continuous and variable-length spaces, which arises in applications like
  decision tree learning and symbolic regression. The proposed DisCo-DSO method jointly
  optimizes discrete and continuous design variables by learning a joint distribution
  over them using an autoregressive model and deep reinforcement learning, in contrast
  to standard decoupled approaches that optimize these components separately.
---

# DisCo-DSO: Coupling Discrete and Continuous Optimization for Efficient Generative Design in Hybrid Spaces

## Quick Facts
- arXiv ID: 2412.11051
- Source URL: https://arxiv.org/abs/2412.11051
- Authors: Jacob F. Pettit; Chak Shing Lee; Jiachen Yang; Alex Ho; Daniel Faissol; Brenden Petersen; Mikel Landajuela
- Reference count: 40
- Primary result: DisCo-DSO significantly outperforms decoupled baselines in sample efficiency and generalization for black-box optimization in hybrid discrete-continuous spaces

## Executive Summary
This paper addresses the challenge of black-box optimization in hybrid spaces containing both discrete and continuous variables of variable length, which is common in applications like decision tree learning and symbolic regression. The proposed DisCo-DSO method introduces a novel approach that jointly optimizes discrete and continuous design variables by learning a joint distribution over them using an autoregressive model combined with deep reinforcement learning. Unlike standard decoupled approaches that optimize discrete and continuous components separately, DisCo-DSO leverages the coupling between these variable types to achieve superior performance with fewer function evaluations.

The experimental results demonstrate that DisCo-DSO achieves state-of-the-art performance on two benchmark tasks: decision tree policy optimization for reinforcement learning and symbolic regression. The method shows significant improvements in both sample efficiency and generalization compared to baseline approaches. For instance, in symbolic regression, DisCo-DSO achieves an average reward of 0.7045 on the test set compared to 0.6400 for the best decoupled method, while using fewer function evaluations. These results validate the effectiveness of jointly modeling the discrete-continuous coupling in hybrid optimization spaces.

## Method Summary
DisCo-DSO addresses black-box optimization in hybrid spaces by learning a joint distribution over discrete and continuous variables using an autoregressive model. The method employs deep reinforcement learning to optimize this joint distribution, allowing for simultaneous optimization of both variable types rather than treating them as separate components. The autoregressive model captures the dependencies between discrete and continuous variables, while the reinforcement learning component guides the optimization process toward high-performing designs. This coupling approach enables more efficient exploration of the hybrid search space compared to traditional decoupled methods that optimize discrete and continuous variables independently.

## Key Results
- DisCo-DSO significantly outperforms decoupled baselines in sample efficiency and generalization
- In symbolic regression, achieves 0.7045 average reward vs 0.6400 for best decoupled method on test set
- Achieves superior performance with fewer function evaluations across benchmark tasks
- Demonstrates effectiveness in both decision tree policy optimization and symbolic regression applications

## Why This Works (Mechanism)
DisCo-DSO works by leveraging the inherent coupling between discrete and continuous variables in hybrid spaces. Traditional decoupled approaches treat these variable types separately, missing potential synergies and dependencies between them. By learning a joint distribution through an autoregressive model, DisCo-DSO captures the complex relationships between discrete decisions (like tree structure) and continuous parameters (like split thresholds). The deep reinforcement learning component then optimizes this joint distribution, allowing the method to explore the hybrid search space more effectively by considering how changes in discrete structure affect continuous parameters and vice versa. This joint optimization approach enables more efficient navigation of the solution space and better generalization to unseen problems.

## Foundational Learning
- **Autoregressive models**: These models capture sequential dependencies by predicting each variable conditioned on previous ones. Why needed: To model the joint distribution over discrete and continuous variables that may have complex dependencies. Quick check: Verify the model can accurately predict the next variable given previous variables in the sequence.
- **Deep reinforcement learning**: This approach learns policies through interaction with an environment to maximize cumulative reward. Why needed: To optimize the joint distribution learned by the autoregressive model toward high-performing designs. Quick check: Ensure the RL agent can learn to improve its policy over successive iterations.
- **Hybrid optimization spaces**: These are search spaces containing both discrete and continuous variables, often with variable length. Why needed: Many real-world optimization problems involve both types of variables with complex interactions. Quick check: Confirm the method can handle variable-length representations and mixed variable types.
- **Black-box optimization**: This refers to optimization problems where the objective function is not analytically known and can only be evaluated through expensive function calls. Why needed: The method must work without gradient information or analytical properties of the objective. Quick check: Verify performance degrades gracefully as function evaluation budget decreases.
- **Sample efficiency**: This measures how many function evaluations are needed to find good solutions. Why needed: In black-box optimization, function evaluations are often expensive, making sample efficiency crucial. Quick check: Compare the number of function evaluations needed to reach target performance against baseline methods.
- **Generalization in optimization**: This refers to the ability of optimized solutions to perform well on unseen instances or distributions. Why needed: In many applications, we want solutions that work well across variations rather than overfitting to specific training instances. Quick check: Evaluate performance on held-out test sets or distributions different from training data.

## Architecture Onboarding

**Component Map:**
Autoregressive Model -> Deep RL Agent -> Joint Distribution -> Hybrid Space Exploration -> Function Evaluations -> Performance Feedback

**Critical Path:**
The critical path flows from the autoregressive model through the deep RL agent to generate candidate solutions, which are evaluated through function calls. The performance feedback is then used to update the RL policy, which in turn updates the autoregressive model parameters. This iterative loop continues until convergence or budget exhaustion.

**Design Tradeoffs:**
The main tradeoff involves the complexity of the autoregressive model versus computational efficiency. More complex models can capture finer-grained dependencies but require more training data and computational resources. The reinforcement learning component introduces additional hyperparameters that must be tuned for optimal performance. There's also a balance between exploration (trying new areas of the search space) and exploitation (refining known good solutions).

**Failure Signatures:**
- Poor performance on either discrete or continuous components suggests the coupling mechanism isn't effectively capturing dependencies
- Slow convergence may indicate the autoregressive model architecture is too simple or the RL agent isn't effectively exploring the space
- Overfitting to training instances suggests insufficient regularization or limited diversity in the exploration strategy
- Computational bottlenecks typically arise from the autoregressive model's inference time during candidate generation

**First 3 Experiments:**
1. **Ablation study**: Compare DisCo-DSO against versions that optimize discrete and continuous components separately to quantify the benefit of joint optimization.
2. **Scalability test**: Evaluate performance as the dimensionality of the hybrid space increases to understand computational limits.
3. **Generalization benchmark**: Test on held-out distributions or problem instances not seen during training to measure true generalization capability.

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope is narrow, focusing primarily on decision tree optimization and symbolic regression tasks
- Computational overhead of joint modeling may limit scalability to very large hybrid spaces
- Paper lacks detailed analysis of trade-offs between sample efficiency gains and increased computational complexity
- Claims about broad applicability to all black-box optimization problems in hybrid spaces lack comprehensive validation

## Confidence

**High Confidence:**
- Core experimental results showing DisCo-DSO outperforming decoupled baselines in tested domains are well-supported

**Medium Confidence:**
- Generalizability claims to broader black-box optimization problems are reasonable but not comprehensively validated
- Theoretical justification exists but limited empirical breadth across domains

**Low Confidence:**
- Assertion about DisCo-DSO being "widely applicable" across all black-box optimization scenarios in hybrid spaces is premature

## Next Checks
1. **Scalability Testing**: Evaluate DisCo-DSO on larger-scale hybrid optimization problems with higher-dimensional discrete and continuous variable spaces to assess computational feasibility and performance degradation patterns.

2. **Cross-Domain Generalization**: Apply DisCo-DSO to at least three additional distinct black-box optimization domains (e.g., hyperparameter tuning for neural architectures, robotic control policy optimization, and chemical compound design) to validate the claimed broad applicability.

3. **Ablation Studies**: Conduct systematic ablation experiments to quantify the individual contributions of the autoregressive modeling component versus the deep reinforcement learning component to DisCo-DSO's performance gains, and identify which hybrid space characteristics most benefit from the joint optimization approach.