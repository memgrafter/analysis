---
ver: rpa2
title: Transformers are Multi-State RNNs
arxiv_id: '2401.06104'
source_url: https://arxiv.org/abs/2401.06104
tags:
- tokens
- multi-state
- size
- layer
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that decoder-only transformers can be interpreted
  as infinite multi-state RNNs, with their key-value cache representing an unbounded
  hidden state. By introducing TOVA, a simple compression policy that drops tokens
  based on their attention scores, the authors show that pretrained transformers can
  be converted into bounded multi-state RNNs.
---

# Transformers are Multi-State RNNs

## Quick Facts
- arXiv ID: 2401.06104
- Source URL: https://arxiv.org/abs/2401.06104
- Authors: Matanel Oren; Michael Hassid; Nir Yarden; Yossi Adi; Roy Schwartz
- Reference count: 40
- One-line primary result: Decoder-only transformers can be interpreted as infinite multi-state RNNs, and compression policies like TOVA can reduce KV cache size by 1/8-1/4 while maintaining near-topline performance.

## Executive Summary
This work demonstrates that decoder-only transformers can be reinterpreted as infinite multi-state RNNs, where the key-value cache represents an unbounded hidden state. The authors introduce TOVA, a simple compression policy that drops tokens based on their attention scores, and show that pretrained transformers can be converted into bounded multi-state RNNs. Experiments on four long-range tasks with three LLM families (LLaMA-2, Mistral, Yi) reveal that TOVA significantly outperforms existing compression policies, maintaining near-topline performance while using only 1/8-1/4 of the original cache size. This translates to up to 88% reduction in memory consumption and 4.8x higher throughput.

## Method Summary
The authors reinterpret decoder-only transformers as infinite multi-state RNNs and propose TOVA, a compression policy that drops tokens with the lowest attention scores during decoding. TOVA is compared against Window, Window+4, and H2O policies on four long-range tasks using three LLM families. The evaluation measures memory reduction, throughput gains, and task performance across varying context sizes (64 to 4096 tokens). The approach requires no model retraining and works by modifying the KV cache during auto-regressive decoding.

## Key Results
- TOVA outperforms all existing compression policies, maintaining near-topline performance using 1/8-1/4 of the original context size
- Memory consumption reduced by up to 88% with 4.8x higher throughput
- Demonstrates that pretrained transformers behave as finite multi-state RNNs in practice, despite being trained as infinite ones
- Performance degradation observed for tasks requiring substantial context (long text generation, retrieval QA)

## Why This Works (Mechanism)

### Mechanism 1
Decoder-only transformers can be reinterpreted as infinite multi-state RNNs where the key-value cache represents an unbounded hidden state. The KV cache grows with each decoding step, storing representations of all previous tokens. This corresponds to a multi-state RNN where each token adds a new state row to the hidden state matrix.

### Mechanism 2
Pretrained transformers often behave as finite MSRNNs, not utilizing their full infinite state capacity. When compressing the KV cache using policies like TOVA, performance remains near the full model despite dropping tokens, indicating that many tokens are not essential for accurate generation.

### Mechanism 3
TOVA compression policy outperforms existing methods by selecting tokens to drop based solely on their attention scores. At each decoding step, TOVA computes attention scores from the current query to all cached tokens plus the current token, then drops the token with the lowest score.

## Foundational Learning

- Concept: Recurrent Neural Networks (RNNs) and their state transition mechanisms
  - Why needed here: Understanding MSRNNs requires grasping how RNNs maintain and update hidden states across time steps
  - Quick check question: In a standard RNN, what two inputs does each layer receive at time t, and what two outputs does it produce?

- Concept: Transformer self-attention and KV caching
  - Why needed here: TOVA operates on attention scores from cached KV pairs, so understanding how these are computed and stored is essential
  - Quick check question: How are the K and V matrices typically computed and cached during autoregressive decoding?

- Concept: Compression policies and their evaluation metrics
  - Why needed here: TOVA is compared against Window, Window+i, and H2O policies, so understanding their mechanisms and evaluation criteria is crucial
  - Quick check question: What is the key difference between the Window and Window+i compression policies?

## Architecture Onboarding

- Component map: Transformer layers with self-attention heads -> KV cache storage -> TOVA compression module -> Inference engine
- Critical path: 1) Forward pass through transformer layers 2) Attention score computation for current token against cached tokens 3) TOVA selection of lowest-scoring token to drop 4) Cache update with new token representation 5) Output generation
- Design tradeoffs: Compression ratio vs. performance (higher compression risks quality degradation), token selection strategy (attention-based vs. heuristic-based), layer-wise vs. head-wise attention aggregation
- Failure signatures: Performance degradation when cache size is too small for the task, inconsistent behavior across different transformer architectures, attention score noise causing suboptimal token selection
- First 3 experiments: 1) Implement basic TOVA on a small LLaMA-2 model with PG-19 language modeling to verify compression works 2) Compare TOVA against Window+4 policy on the same setup to establish performance baseline 3) Test TOVA on a long-range understanding task (SQuALITY) to evaluate generalization across task types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical basis for why transformer decoders behave as finite multi-state RNNs in practice, despite being trained as infinite ones?
- Basis in paper: Inferred
- Why unresolved: The paper shows empirical evidence that transformers behave as finite MSRNNs but does not provide a theoretical explanation for this behavior.
- What evidence would resolve it: A mathematical proof or extensive empirical study showing the conditions under which transformers converge to finite MSRNN behavior during training and inference.

### Open Question 2
- Question: How does the TOVA policy's layer-wise variant compare to the head-wise variant in terms of computational efficiency and performance across different model architectures and tasks?
- Basis in paper: Explicit
- Why unresolved: While the paper mentions that the layer-wise variant of TOVA performs better, it does not provide a detailed comparison of computational efficiency between the two variants or explore their performance across diverse architectures.
- What evidence would resolve it: Comprehensive experiments comparing the computational costs and performance of both variants across various transformer architectures and tasks.

### Open Question 3
- Question: What are the implications of the observed importance of the first token and specific POS tags (e.g., possessive nouns) on the design of compression policies and transformer architectures?
- Basis in paper: Explicit
- Why unresolved: The paper identifies the importance of certain tokens but does not explore how this knowledge can be leveraged to improve compression policies or influence the design of future transformer architectures.
- What evidence would resolve it: Studies showing how incorporating this knowledge into model design affects performance, or experiments demonstrating improved compression policies based on token importance.

## Limitations

- The theoretical interpretation of transformers as infinite MSRNNs is not fully explored in terms of architectural implications
- TOVA's reliance on attention scores lacks theoretical grounding for why attention scores reliably indicate token importance
- Evaluation scope is limited and does not systematically test edge cases where compression might fail

## Confidence

- High Confidence: Empirical demonstration that TOVA outperforms existing compression policies and achieves significant memory reduction while maintaining near-topline performance
- Medium Confidence: Theoretical interpretation of transformers as infinite multi-state RNNs
- Low Confidence: Claim that pretrained transformers "often behave in practice as finite multi-state RNNs"

## Next Checks

1. Systematically evaluate TOVA on tasks known to require precise long-range dependencies, such as mathematical reasoning or factual recall tasks, to test whether attention scores reliably indicate token importance across all task types.

2. Compare TOVA performance across different transformer variants (e.g., encoder-decoder, encoder-only) to validate whether the infinite MSRNN interpretation applies universally or is specific to decoder-only architectures.

3. Implement variants of TOVA that use alternative token selection criteria (e.g., positional information, token frequency, or learned importance scores) and compare their performance against the attention-based approach to determine whether attention scores are uniquely effective.