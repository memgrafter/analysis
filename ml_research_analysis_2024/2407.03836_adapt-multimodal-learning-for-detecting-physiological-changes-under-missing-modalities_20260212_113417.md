---
ver: rpa2
title: 'ADAPT: Multimodal Learning for Detecting Physiological Changes under Missing
  Modalities'
arxiv_id: '2407.03836'
source_url: https://arxiv.org/abs/2407.03836
tags:
- modalities
- adapt
- missing
- learning
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses detecting physiological changes from multimodal
  data (video, audio, biomedical signals) under missing modalities. ADAPT uses an
  anchoring strategy to align all modalities to a rich anchor space, followed by a
  Masked Multimodal Transformer that fuses features while handling missing inputs
  via masking.
---

# ADAPT: Multimodal Learning for Detecting Physiological Changes under Missing Modalities

## Quick Facts
- arXiv ID: 2407.03836
- Source URL: https://arxiv.org/abs/2407.03836
- Authors: Julie Mordacq; Leo Milecki; Maria Vakalopoulou; Steve Oudot; Vicky Kalogeiton
- Reference count: 15
- Primary result: ADAPT achieves up to 4% better accuracy than state-of-the-art methods for physiological change detection with missing modalities

## Executive Summary
This paper introduces ADAPT, a multimodal learning framework for detecting physiological changes when some input modalities are missing. The method uses an anchoring strategy to align all modalities to a rich anchor space (video) followed by a Masked Multimodal Transformer that fuses features while handling missing inputs via masking. Experiments on stress detection and pilot loss-of-consciousness datasets show ADAPT outperforms state-of-the-art methods by up to 4% in accuracy, with robustness to missing modalities (performance drop <8% in most cases) and balanced detection of positive/negative classes even under severe imbalance.

## Method Summary
ADAPT uses a two-stage training approach. First, modality-specific encoders are aligned to a video anchor through contrastive learning, reducing computational complexity from O(M²) to O(M). Second, a Masked Multimodal Transformer fuses the aligned features while handling missing modalities through attention masking. The framework incorporates modality dropout during training to improve robustness, randomly masking up to M-1 modalities in each training view. The final classification uses the [CLS] token from the transformer output.

## Key Results
- ADAPT achieves up to 4% better accuracy than state-of-the-art methods on stress detection and pilot loss-of-consciousness datasets
- Robustness to missing modalities with performance drop <8% in most cases when modalities are absent
- Balanced detection of positive/negative classes even under severe class imbalance conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADAPT achieves modality-agnostic representation by anchoring all modalities to a single "rich" modality (video) via contrastive learning.
- Mechanism: Each modality encoder produces embeddings that are projected and aligned to the anchor modality's feature space using symmetric contrastive loss. This alignment forces modality-specific features into a shared latent space without requiring pairwise modality losses, reducing computational complexity from O(M²) to O(M).
- Core assumption: The anchor modality (video) provides the richest and most discriminative features for the downstream task, and its representation space can adequately capture semantic information from all other modalities.
- Evidence anchors:
  - [abstract] "aligning all modalities in the space of the strongest, richest modality (called anchor)"
  - [section 3.1] "anchor is the video, as it can capture visually distinguishable physiological changes"
  - [corpus] No direct evidence; assumption about anchor richness is task-specific and not universally validated
- Break condition: If the chosen anchor modality is not truly the richest or most discriminative for the task, alignment will be suboptimal and degrade performance, especially when the anchor is missing.

### Mechanism 2
- Claim: The Masked Multimodal Transformer can handle missing modalities during both training and inference by applying modality-aware masking at the attention level.
- Mechanism: A binary masking matrix Z determines which modalities are available per sample. During attention computation, only available modalities contribute to the weighted sum of values, effectively ignoring missing ones without requiring explicit imputation or reconstruction.
- Core assumption: The attention mechanism's flexibility allows the model to dynamically adapt to variable input configurations while preserving inter- and intra-modality relationships.
- Evidence anchors:
  - [abstract] "leveraging both inter- and intra-modality correlations while handling missing modalities"
  - [section 3.2] "We consider one sub-layer with one head ( h = 1) for simplicity. We use a masking binary matrix Z ∈ R(M +1)×(M +1)"
  - [corpus] Weak evidence; masking in multimodal transformers is conceptually supported but empirical validation in medical settings is sparse
- Break condition: If too many modalities are missing simultaneously, the remaining modalities may not provide sufficient information for the task, leading to performance collapse.

### Mechanism 3
- Claim: Modality dropout during training improves robustness by preventing the model from overfitting to specific modality combinations.
- Mechanism: During each training step, two augmented views of the input are created by randomly masking up to M-1 modalities in each view. This forces the model to learn representations that are invariant to modality availability and reduces reliance on any single modality.
- Core assumption: Randomly masking modalities during training exposes the model to a distribution of missing-modality scenarios, improving generalization to unseen configurations at test time.
- Evidence anchors:
  - [section 3.2] "we mitigate the model's over-reliance on a single modality while enhancing its robustness in the absence of modalities through an augmentation technique called modality dropout"
  - [corpus] No direct evidence; modality dropout is adapted from vision-language literature without domain-specific validation
- Break condition: If the dropout rate is too aggressive, the model may fail to learn modality-specific features that are critical for the task.

## Foundational Learning

- Concept: Contrastive learning for representation alignment
  - Why needed here: To align heterogeneous modality embeddings into a shared space without requiring pairwise loss computations
  - Quick check question: What is the computational complexity of pairwise contrastive loss vs. anchor-based alignment for M modalities?

- Concept: Masked attention in transformers
  - Why needed here: To dynamically handle variable input configurations without requiring modality imputation
  - Quick check question: How does the masking matrix Z modify the attention score computation in the scaled dot-product attention?

- Concept: Modality dropout as a data augmentation strategy
  - Why needed here: To improve robustness to missing modalities by exposing the model to diverse availability patterns during training
  - Quick check question: What is the difference between modality dropout and standard dropout in terms of what gets masked?

## Architecture Onboarding

- Component map: Modality-specific encoders (Hiera for video, BYOL-A for audio, 1D CNN for biomedical signals) -> Projection heads -> Anchor-based contrastive alignment -> Masking -> Transformer -> [CLS] token -> Classifier

- Critical path: Encoder → Projection → Anchor alignment → Masking → Transformer → [CLS] token → Classifier

- Design tradeoffs:
  - Anchoring to video vs. learning a shared encoder: Anchoring avoids the need for modality-specific adaptation layers but assumes video is always the most informative
  - Modality dropout rate: Higher rates improve robustness but may degrade performance on complete data
  - Transformer depth and width: Deeper models may capture richer interactions but increase computational cost and overfitting risk

- Failure signatures:
  - High variance in performance across different missing-modality scenarios
  - Degraded performance when the anchor modality is missing
  - Model overfits to specific modality combinations (indicated by poor generalization to unseen missing patterns)

- First 3 experiments:
  1. Ablation study: Train without anchoring to verify if modality alignment is necessary for performance
  2. Sensitivity analysis: Vary the modality dropout rate during training to find the optimal robustness-performance tradeoff
  3. Anchor modality swap: Replace video with audio or biomedical signals as the anchor to test if the choice of anchor affects performance significantly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ADAPT perform on datasets with more than three modalities?
- Basis in paper: [inferred] The paper mentions ADAPT can be extended to other modalities by adding their respective encoders, but only evaluates two and three modality scenarios.
- Why unresolved: The experiments only test two and three modality scenarios (video, audio, biomedical signals), leaving performance on datasets with more modalities unexplored.
- What evidence would resolve it: Experimental results on datasets with 4+ modalities would demonstrate scalability and performance in higher-dimensional multimodal settings.

### Open Question 2
- Question: How does ADAPT's performance degrade with increasing levels of missing data?
- Basis in paper: [inferred] The paper shows robustness to missing modalities but only tests complete removal scenarios, not gradual degradation.
- Why unresolved: The experiments only consider complete removal of modalities, not partial or gradually increasing missing data scenarios.
- What evidence would resolve it: Experiments varying the percentage of missing data for each modality would reveal ADAPT's performance curve under different levels of data availability.

### Open Question 3
- Question: How does the choice of anchor modality affect performance in different medical tasks?
- Basis in paper: [explicit] The paper states "anchor is the video, as it can capture visually distinguishable physiological changes; however, any modality can be the anchor" and tests video vs audio as anchors.
- Why unresolved: While the paper tests video and audio as anchors, it doesn't explore how different anchor choices affect performance across various medical detection tasks.
- What evidence would resolve it: Comparative experiments using different modalities as anchors across multiple medical detection tasks would reveal optimal anchor selection strategies.

## Limitations
- The anchoring strategy's reliance on video as the "richest" modality is a critical assumption that may not generalize across all physiological change detection tasks
- The modality dropout technique is adapted from vision-language literature without domain-specific validation for physiological signals
- The paper does not provide sufficient detail on exact augmentation parameters and temperature schedules used in contrastive learning

## Confidence
**High Confidence Claims:**
- The overall framework architecture (modality-specific encoders + masked transformer) is technically sound and well-implemented
- Performance improvements over baselines on both StressID and LOC datasets are empirically demonstrated
- The mechanism for handling missing modalities through masking is correctly implemented and effective

**Medium Confidence Claims:**
- The anchoring strategy's effectiveness in aligning modalities to a shared space
- The robustness improvements from modality dropout are well-supported but domain-specific validation is limited
- The computational complexity reduction from O(M²) to O(M) through anchoring is theoretically valid

**Low Confidence Claims:**
- The universal optimality of video as the anchor modality across all physiological change detection tasks
- The specific augmentation parameters and their impact on final performance
- The generalization of modality dropout from vision-language to physiological signal domains

## Next Checks
1. **Anchor Modality Ablation**: Systematically evaluate ADAPT's performance when using different anchor modalities (audio, biomedical signals) across both datasets to quantify the impact of anchor selection on overall effectiveness.

2. **Dropout Sensitivity Analysis**: Conduct a comprehensive study varying the modality dropout rate during training (e.g., 0%, 10%, 25%, 50%, 75%) to identify the optimal tradeoff between robustness and performance on complete data.

3. **Missing Modality Stress Test**: Design experiments with controlled missing-modality scenarios (e.g., specific combinations of missing modalities) to test whether ADAPT maintains performance when critical modality combinations are absent.