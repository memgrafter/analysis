---
ver: rpa2
title: Handling Large-scale Cardinality in building recommendation systems
arxiv_id: '2401.09572'
source_url: https://arxiv.org/abs/2401.09572
tags:
- systems
- recommendation
- size
- performance
- sharing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of incorporating high-cardinality
  UUID features in recommendation systems, which typically leads to model degradation
  and increased size due to sparsity. The authors propose a bag-of-words approach
  combined with layer sharing to reduce model size while improving performance.
---

# Handling Large-scale Cardinality in building recommendation systems

## Quick Facts
- arXiv ID: 2401.09572
- Source URL: https://arxiv.org/abs/2401.09572
- Authors: Dhruva Dixith Kurra; Bo Ling; Chun Zh; Seyedshahin Ashrafzadeh
- Reference count: 3
- Primary result: 25x model size reduction and 10% recall@500 improvement using bag-of-words and layer sharing

## Executive Summary
This paper addresses the challenge of incorporating high-cardinality UUID features in recommendation systems, which typically leads to model degradation and increased size due to sparsity. The authors propose a bag-of-words approach combined with layer sharing to reduce model size while improving performance. The bag-of-words method replaces high-cardinality user IDs with a proxy based on their previously ordered stores, reducing dimensionality and model size by 25x. Layer sharing allows the query and item towers to share embedding table layers, enabling information exchange and capturing more nuanced relationships. Offline and online experiments on Uber use cases demonstrate significant improvements, with a 10% increase in recall@500 and up to 30% improvement in lower recall values (k=100, 200).

## Method Summary
The proposed approach replaces high-cardinality eater_uuid with a bag-of-words representation based on previously ordered store_uuids, reducing dimensionality. Layer sharing is then applied to share embedding table layers for UUID features between query and item towers, enabling information exchange. The method is evaluated against a Deep Matrix Factorization baseline using four months of Uber Eats order data, with one week for validation. Models use AdaGrad optimizer, cross-entropy loss, and 32-dimensional embeddings. Performance is measured using Hit Rate (Recall) at various cutoffs, with Hit Rate@500 as the primary metric.

## Key Results
- 25x reduction in model size through dimensionality reduction
- 10% increase in recall@500 compared to baseline
- Up to 30% improvement in lower recall values (k=100, 200)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bag-of-Words approach reduces model size by replacing high-cardinality user IDs with lower-cardinality store IDs
- Mechanism: Replaces eater_uuid (hundreds of millions) with a bag of previously ordered store_uuids (millions), effectively reducing dimensionality
- Core assumption: User behavior can be adequately represented by their ordered stores rather than direct user ID
- Evidence anchors:
  - [abstract] "bag-of-words approach... replace high-cardinality user IDs with a proxy based on their previously ordered stores, reducing dimensionality"
  - [section] "if we can replace the eater_uuid with a proxy, we can solve the issue of model size"
  - [corpus] Weak - no direct evidence in corpus papers about BoW for UUID replacement
- Break condition: If user ordering patterns are too diverse or if users frequently order from new stores not in their history

### Mechanism 2
- Claim: Layer sharing between query and item towers enables information exchange and captures nuanced relationships
- Mechanism: Shares embedding table layers for UUID features between two towers, allowing towers to exchange information about high-cardinal features
- Core assumption: Sharing UUID embeddings between towers improves relationship capture without harming tower independence
- Evidence anchors:
  - [abstract] "layer sharing allows the query and item towers to share embedding table layers, enabling information exchange"
  - [section] "we enforce embedding table-layer sharing for UUID features between the two towers, allowing them to share information"
  - [corpus] Weak - corpus papers focus on ranking model scaling but don't discuss layer sharing for UUID features
- Break condition: If sharing causes interference between towers or if feature distributions differ significantly between query and item

### Mechanism 3
- Claim: Combined approach achieves 25x reduction in model size and 10% improvement in recall@500
- Mechanism: BoW reduces dimensionality while layer sharing reduces parameters; together they improve performance and efficiency
- Core assumption: The benefits of dimensionality reduction and parameter sharing compound rather than conflict
- Evidence anchors:
  - [abstract] "resulting in promising results demonstrating our approach's effectiveness in optimizing recommendation systems"
  - [section] "We achieved a substantial reduction in model size... 10% increase in recall@500"
  - [section] "Our analysis of the results highlights the superior performance demonstrated by the newly proposed Bag of Words (BoW) models"
- Break condition: If model compression degrades critical feature representations or if layer sharing creates optimization conflicts

## Foundational Learning

- Concept: High-cardinality feature handling
  - Why needed here: UUID features have cardinality in hundreds of millions, causing sparsity and model bloat
  - Quick check question: What's the cardinality difference between eater_uuid and store_uuid in this system?

- Concept: Two-tower embedding architectures
  - Why needed here: The paper builds on TTE models, modifying them with BoW and layer sharing
  - Quick check question: How do query and item towers typically interact in standard TTE models?

- Concept: Matrix factorization vs deep learning approaches
  - Why needed here: The paper contrasts with deep matrix factorization baseline, showing advantages of their approach
  - Quick check question: What are the key limitations of matrix factorization for high-cardinality features?

## Architecture Onboarding

- Component map: User input → Store UUID sequence → Shared embeddings → Transformer → Similarity score
- Critical path: User input → Store UUID sequence → Shared embeddings → Transformer → Similarity score
- Design tradeoffs:
  - Model size vs. expressiveness: BoW trades direct user representation for dimensionality reduction
  - Tower independence vs. information sharing: Layer sharing enables information flow but may create dependencies
  - Training speed vs. convergence quality: Shared layers may converge faster but require careful optimization
- Failure signatures:
  - Poor recall@k values indicate BoW representation isn't capturing user preferences
  - Training instability suggests layer sharing is causing interference
  - Slow convergence indicates suboptimal embedding sharing strategy
- First 3 experiments:
  1. Baseline TTE with UUID features vs. BoW-only model to isolate dimensionality reduction impact
  2. Layer sharing ablation: Shared vs. separate UUID embeddings with identical BoW input
  3. Sequence length sensitivity: Vary number of historical stores in BoW representation and measure impact on recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bag-of-words approach perform on extremely sparse user histories where the number of previously ordered stores is very low?
- Basis in paper: [inferred] The paper discusses replacing eater_uuid with a bag-of-words proxy based on previously ordered stores, but does not explore the performance impact when user histories are sparse.
- Why unresolved: The paper does not provide experiments or analysis on edge cases where user history data is limited.
- What evidence would resolve it: Empirical results comparing model performance with varying levels of user history sparsity would clarify this limitation.

### Open Question 2
- Question: What is the impact of the layer sharing technique on model interpretability and the ability to debug recommendation failures?
- Basis in paper: [inferred] The paper introduces layer sharing between query and item towers but does not discuss its effects on model transparency or debugging capabilities.
- Why unresolved: The authors focus on performance metrics but do not address how the shared layers affect understanding model decisions.
- What evidence would resolve it: Studies comparing the interpretability of shared vs. separate tower architectures would provide insights.

### Open Question 3
- Question: How does the proposed approach scale when applied to recommendation systems with even larger cardinality features, such as item descriptions or user-generated content?
- Basis in paper: [explicit] The paper mentions that UUID features have exceptionally high cardinality but does not explore scaling to other high-cardinality features.
- Why unresolved: The experiments focus on UUID features, and the authors do not discuss the applicability to other types of high-cardinality data.
- What evidence would resolve it: Performance evaluations on recommendation systems incorporating diverse high-cardinality features would demonstrate scalability.

## Limitations
- Limited generalizability beyond the Uber Eats use case
- Lack of comparison with alternative dimensionality reduction methods like hashing or quantization
- No discussion of cold-start scenarios where users have minimal order history

## Confidence
- Bag-of-Words effectiveness: Medium
- Layer sharing benefits: Medium
- Combined performance improvement: Medium
- Model size reduction claims: Low

## Next Checks
1. Implement and test alternative cardinality reduction methods (hashing, quantization) on the same dataset to benchmark the BoW approach
2. Conduct ablation studies varying the number of historical stores in the BoW representation to determine optimal sequence length
3. Test the approach on a different high-cardinality dataset (e.g., video/music streaming) to evaluate generalizability