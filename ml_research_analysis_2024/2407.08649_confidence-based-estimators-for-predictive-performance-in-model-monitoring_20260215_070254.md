---
ver: rpa2
title: Confidence-based Estimators for Predictive Performance in Model Monitoring
arxiv_id: '2407.08649'
source_url: https://arxiv.org/abs/2407.08649
tags:
- confidence
- calibration
- accuracy
- performance
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how to monitor the predictive performance of
  a deployed machine learning model when ground truth labels are unavailable. It focuses
  on confidence-based methods, particularly Average Confidence (AC), which estimates
  model accuracy as the average of confidence scores.
---

# Confidence-based Estimators for Predictive Performance in Model Monitoring

## Quick Facts
- arXiv ID: 2407.08649
- Source URL: https://arxiv.org/abs/2407.08649
- Reference count: 24
- Key outcome: Average Confidence (AC) provides an unbiased and consistent estimator of model accuracy under calibration assumptions, with valid uncertainty quantification via Poisson binomial distribution.

## Executive Summary
This paper addresses the challenge of monitoring predictive performance in deployed machine learning models when ground truth labels are unavailable. The authors propose and analyze Average Confidence (AC), a simple yet theoretically grounded method that estimates accuracy as the average of confidence scores. Under the assumption of calibration, AC is proven to be an unbiased and consistent estimator with theoretically valid confidence intervals derived from the Poisson binomial distribution. The study demonstrates through extensive experiments that AC performs competitively against more complex methods, particularly when models maintain good calibration under covariate shift. A key finding is the strong correlation between estimation error and calibration error, highlighting the importance of maintaining model calibration for effective unsupervised performance monitoring.

## Method Summary
The study evaluates confidence-based performance estimation methods, primarily Average Confidence (AC), Difference of Confidences (DoC), and Average Thresholded Confidence (ATC). Using simulated datasets with controlled covariate shifts, the authors train multiple classifiers (logistic regression, Naive Bayes, KNN, SVM, random forest, XGBoost, LightGBM) and apply isotonic regression calibration. They generate test sets with varying proportions of "easy" and "hard" samples to simulate shifts, then compute confidence-based estimates using both calibrated and uncalibrated scores. Performance is measured through mean absolute estimation error compared against true accuracies, with additional analysis of the relationship between estimation and calibration errors.

## Key Results
- AC is theoretically proven to be an unbiased and consistent estimator of accuracy when confidence scores are calibrated.
- Poisson binomial distribution provides valid confidence intervals for AC estimates, with experimental validation showing roughly valid coverage.
- AC consistently outperforms or matches more complex methods (DoC, ATC) across multiple classifiers and shift scenarios.
- Strong correlation (Pearson coefficients ~0.8-0.9) exists between estimation error and calibration error, suggesting calibration maintenance is crucial for accurate monitoring.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Average Confidence (AC) is an unbiased estimator of model accuracy under the calibration assumption.
- Mechanism: When a model is perfectly calibrated, the confidence score for each prediction equals the probability that the prediction is correct. Therefore, averaging confidence scores across predictions yields the expected accuracy.
- Core assumption: The model's confidence scores are perfectly calibrated, meaning that Pp(x,y)(C = 1 | S = s) = s for all s in the confidence score range.
- Evidence anchors:
  - [abstract]: "under certain general assumptions, it is an unbiased and consistent estimator of model accuracy with many desirable properties"
  - [section 4.1]: Provides theoretical proof that E[¯SZ] = Accf(pt(x, y)) when confidence scores are calibrated
  - [corpus]: Weak - only 0 citations for related papers
- Break condition: The calibration assumption fails when confidence scores do not align with empirical probabilities, which is common in practice.

### Mechanism 2
- Claim: AC provides valid confidence intervals for accuracy estimates using the Poisson binomial distribution.
- Mechanism: When predictions are independent and confidence scores are calibrated, the number of correct predictions follows a Poisson binomial distribution. This allows exact calculation of confidence intervals for accuracy estimates.
- Core assumption: Predictions are independent and confidence scores are calibrated.
- Evidence anchors:
  - [abstract]: "AC's uncertainty estimates can be derived using the Poisson binomial distribution, yielding valid confidence intervals"
  - [section 4.2]: Describes using CDF of Poisson binomial distribution to estimate 95% confidence intervals
  - [section 5.1.3]: Experimental validation shows Poisson binomial approach produces roughly valid CIs
- Break condition: Predictions are not independent or confidence scores are miscalibrated.

### Mechanism 3
- Claim: Estimation error correlates strongly with calibration error, suggesting maintaining calibration is crucial.
- Mechanism: When confidence scores become miscalibrated under covariate shift, AC's estimates become less accurate. The strong correlation between estimation error and calibration error implies that improving calibration improves accuracy estimation.
- Core assumption: Calibration error increases under covariate shift, which affects estimation accuracy.
- Evidence anchors:
  - [abstract]: "study highlights a strong correlation between estimation error and calibration error, suggesting that maintaining model calibration under covariate shift is crucial"
  - [section 5.2.2]: Tables show strong Pearson correlation coefficients between estimation and calibration errors
  - [section 6.3]: Discussion acknowledges that calibration error tends to increase under covariate shift
- Break condition: Calibration error remains stable under shift or estimation error depends on factors other than calibration.

## Foundational Learning

- Concept: Probability calibration
  - Why needed here: AC relies on the assumption that confidence scores represent true probabilities of correctness. Understanding calibration is essential to know when AC will work well.
  - Quick check question: If a model predicts 100 examples with confidence 0.8, how many should be correct if the model is perfectly calibrated?

- Concept: Poisson binomial distribution
  - Why needed here: The theoretical foundation for deriving confidence intervals with AC comes from modeling correct predictions as a Poisson binomial distribution.
  - Quick check question: What distribution describes the sum of independent Bernoulli trials with different success probabilities?

- Concept: Covariate shift vs concept shift
  - Why needed here: AC works under covariate shift but fails under concept shift. Understanding the difference helps determine when AC is applicable.
  - Quick check question: If input data distribution changes but the relationship between inputs and outputs stays the same, is this covariate shift or concept shift?

## Architecture Onboarding

- Component map: Data collection -> Calibration check -> Estimation -> Uncertainty quantification -> Monitoring
- Critical path: Data collection → Calibration check → Estimation → Uncertainty quantification → Monitoring
- Design tradeoffs:
  - Window size: Larger windows reduce variance but slow detection of changes
  - Calibration method: More sophisticated calibration may improve estimates but adds complexity
  - Independence assumption: May need to handle correlated predictions in practice
- Failure signatures:
  - Systematic bias in estimates suggests calibration issues
  - Large variance suggests small window size or independence violations
  - Estimates diverging from control limits suggests concept shift
- First 3 experiments:
  1. Test AC on perfectly calibrated synthetic data with known accuracy to verify unbiasedness
  2. Test AC with varying window sizes to understand variance-accuracy tradeoff
  3. Compare AC against DoC and ATC on calibrated data with known shifts to validate performance claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Average Confidence (AC) as an estimator change when the calibration error is non-linear or exhibits complex patterns across the confidence spectrum?
- Basis in paper: [explicit] The paper demonstrates a strong correlation between estimation error and calibration error for AC, but focuses on linear or near-linear calibration errors.
- Why unresolved: The experiments primarily use simple synthetic datasets where calibration errors are relatively linear. Real-world data may have more complex, non-linear calibration errors that could affect AC's performance differently.
- What evidence would resolve it: Experiments using real-world datasets with known, complex calibration errors, or synthetic datasets designed to have non-linear calibration errors, to compare AC's performance against other methods.

### Open Question 2
- Question: Can the Poisson binomial distribution approach used in AC be extended or adapted to handle cases where the calibration assumption is significantly violated, such as under severe concept shift?
- Basis in paper: [inferred] The paper shows that AC performs well under the calibration assumption and provides valid confidence intervals using the Poisson binomial distribution, but acknowledges that no estimator can prevail under concept shift.
- Why unresolved: The paper does not explore modifications to the Poisson binomial approach to handle cases where the calibration assumption is severely violated, which is a common scenario in real-world applications.
- What evidence would resolve it: Development and testing of modified Poisson binomial approaches or alternative statistical methods that can handle significant calibration errors, possibly through robust statistical techniques or machine learning-based calibration adjustments.

### Open Question 3
- Question: How do different calibration techniques (beyond temperature scaling and isotonic regression) affect the performance of confidence-based estimators like AC under covariate shift?
- Basis in paper: [explicit] The paper mentions that earlier studies have focused on temperature scaling for calibration and suggests that other calibration techniques might yield different results, especially under covariate shift.
- Why unresolved: The experiments in the paper use isotonic regression for calibration, and while it mentions the potential benefits of other techniques, it does not empirically compare the impact of different calibration methods on the performance of confidence-based estimators.
- What evidence would resolve it: Comparative experiments using various calibration techniques (e.g., Bayesian binning, ensemble methods) across different datasets and shift scenarios to evaluate their impact on the performance of AC and other confidence-based estimators.

## Limitations
- Reliance on the calibration assumption, which may not hold in practice when models face real-world data shifts
- Experiments conducted on simulated data with controlled shifts rather than real-world deployment scenarios
- Independence assumption for Poisson binomial calculations may be violated when predictions are correlated

## Confidence
- **High confidence**: Theoretical proof of AC as unbiased estimator under calibration assumption
- **Medium confidence**: Empirical performance comparisons showing AC outperforming alternatives
- **Medium confidence**: Correlation between estimation error and calibration error

## Next Checks
1. Test AC on real-world datasets with documented covariate shifts to validate performance outside controlled simulations
2. Evaluate AC's performance when confidence scores are miscalibrated by varying amounts to understand sensitivity to calibration quality
3. Investigate the impact of prediction correlations on AC's uncertainty estimates by introducing structured dependencies in the data