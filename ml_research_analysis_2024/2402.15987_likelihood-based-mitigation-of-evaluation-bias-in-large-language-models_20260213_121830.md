---
ver: rpa2
title: Likelihood-based Mitigation of Evaluation Bias in Large Language Models
arxiv_id: '2402.15987'
source_url: https://arxiv.org/abs/2402.15987
tags:
- bias
- likelihood
- evaluation
- criteria
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses likelihood bias in LLM-based evaluation, where
  models overrate high-likelihood outputs and underrate low-likelihood ones compared
  to human judgments. The authors propose a method to quantify and mitigate this bias
  by identifying highly biased instances and using them as few-shot examples for in-context
  learning.
---

# Likelihood-based Mitigation of Evaluation Bias in Large Language Models

## Quick Facts
- arXiv ID: 2402.15987
- Source URL: https://arxiv.org/abs/2402.15987
- Authors: Masanari Oi; Masahiro Kaneko; Ryuto Koike; Mengsay Loem; Naoaki Okazaki
- Reference count: 10
- Key outcome: This paper addresses likelihood bias in LLM-based evaluation, where models overrate high-likelihood outputs and underrate low-likelihood ones compared to human judgments. The authors propose a method to quantify and mitigate this bias by identifying highly biased instances and using them as few-shot examples for in-context learning. Experiments on data-to-text and GEC tasks show that GPT-3.5 and Llama2-13B exhibit significant likelihood bias, particularly for non-intrinsic evaluation criteria. The proposed mitigation method successfully reduces bias (BiasScore close to zero) and improves evaluation performance in many cases, with significant improvements in evaluation correlation with human scores.

## Executive Summary
This paper investigates likelihood bias in LLM-based evaluators, where models overrate high-likelihood outputs and underrate low-likelihood ones compared to human judgments. The authors propose a method to quantify this bias using a BiasScore metric and mitigate it through in-context learning with carefully selected few-shot examples. Experiments on data-to-text and GEC tasks demonstrate significant bias in GPT-3.5 and Llama2-13B, particularly for non-intrinsic evaluation criteria, with successful bias reduction and improved evaluation performance after mitigation.

## Method Summary
The method quantifies likelihood bias by calculating Likelihood Scores (LS) and Unfairness Scores (US) for each instance, then measuring their correlation as BiasScore. To mitigate bias, the approach selects highly biased instances (highest RS(t) values) as few-shot examples for in-context learning, where human gold-standard scores replace the LLM's biased scores. This correction is applied during evaluation of test instances, with results showing reduced BiasScore and improved correlation with human judgments.

## Key Results
- GPT-3.5 and Llama2-13B exhibit significant likelihood bias, particularly for non-intrinsic evaluation criteria (relevance, data coverage)
- Non-intrinsic criteria show much higher BiasScore than intrinsic criteria (fluency, text structure)
- Bias mitigation through in-context learning successfully reduces BiasScore close to zero
- Evaluation performance significantly improves in correlation with human scores after bias mitigation
- The proposed method effectively corrects the tendency to overrate high-likelihood outputs and underrate low-likelihood ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs overrate high-likelihood outputs and underrate low-likelihood outputs compared to human judgments.
- Mechanism: The likelihood of a text, which is intrinsic to the output, influences the evaluation score. This causes bias because likelihood is affected by superficial differences (e.g., word order) that do not necessarily correlate with text quality.
- Core assumption: Likelihood fluctuations due to superficial differences in sentences can negatively impact evaluation scores based on meaning criteria.
- Evidence anchors:
  - [abstract] "they might overrate sentences with higher likelihoods while underrating those with lower likelihoods"
  - [section] "the likelihood calculated by the LLM fluctuates due to superficial differences in sentences, such as word order and sentence structure, even for sentences with identical meaning"
  - [corpus] Weak corpus evidence; related papers focus on likelihood in different contexts (e.g., precipitation downscaling, OOD detection) rather than evaluation bias.
- Break condition: If the likelihood of a text does not correlate with superficial differences or if the evaluation criteria are not influenced by likelihood.

### Mechanism 2
- Claim: Using highly biased instances as few-shot examples for in-context learning mitigates likelihood bias.
- Mechanism: By selecting instances with the highest RS(t) (a measure of bias), and using them as few-shot examples with human gold-standard scores, the LLM learns to correct its bias.
- Core assumption: In-context learning can effectively adjust the LLM's evaluation behavior based on a small number of examples.
- Evidence anchors:
  - [section] "Our bias reduction method identifies and utilizes highly biased instances as few-shot examples for in-context learning"
  - [section] "our proposed method successfully mitigates this bias, also improving evaluation performance (in terms of correlation of models with human scores) significantly"
  - [corpus] No direct corpus evidence for this specific mechanism.
- Break condition: If the selected instances do not represent the bias well or if in-context learning is ineffective for this task.

### Mechanism 3
- Claim: Intrinsic evaluation criteria are less prone to likelihood bias than non-intrinsic criteria.
- Mechanism: Intrinsic criteria (e.g., fluency, text structure) are concerned with text quality alone and are independent of the input, while non-intrinsic criteria (e.g., relevance, data coverage) are dependent on external factors in the input.
- Core assumption: Likelihood is intrinsic to the output text and thus more related to intrinsic evaluation criteria.
- Evidence anchors:
  - [section] "there is a marked difference in BiasScore between non-intrinsic and intrinsic criteria: non-intrinsic criteria are much more prone to bias"
  - [section] "likelihood is intrinsic to the output text, and thus, likelihood is strongly related to intrinsic criteria"
  - [corpus] No direct corpus evidence for this specific mechanism.
- Break condition: If the distinction between intrinsic and non-intrinsic criteria is not clear or if the evaluation task does not fit this paradigm.

## Foundational Learning

- Concept: Spearman's rank correlation coefficient
  - Why needed here: Used to quantify the correlation between likelihood scores and unfairness scores to measure bias.
  - Quick check question: What does a Spearman's rank correlation coefficient of 1, -1, or 0 indicate in the context of likelihood bias?
- Concept: In-context learning
  - Why needed here: Used to mitigate bias by providing the LLM with a small number of examples to adjust its evaluation behavior.
  - Quick check question: How does in-context learning differ from fine-tuning in terms of data efficiency and effectiveness for bias mitigation?
- Concept: Evaluation metrics (e.g., BLEU, ROUGE)
  - Why needed here: Provides context for why LLM-based evaluators are preferred over traditional metrics.
  - Quick check question: What are the limitations of traditional evaluation metrics like BLEU and ROUGE that motivate the use of LLM-based evaluators?

## Architecture Onboarding

- Component map:
  - LLM evaluator (e.g., GPT-3.5, Llama2-13B) -> Likelihood score calculator -> Unfairness score calculator -> Bias score calculator -> Bias mitigation module (few-shot examples selector and in-context learning)
- Critical path:
  1. Calculate likelihood score (LS) for each instance
  2. Calculate unfairness score (US) by comparing LLM scores with human scores
  3. Calculate bias score (BiasScore) as the correlation between LS and US
  4. If bias is significant, select highly biased instances and use them as few-shot examples for in-context learning
  5. Re-evaluate instances and measure improvement in bias and evaluation performance
- Design tradeoffs:
  - Using in-context learning limits the number of examples that can be used but is more data-efficient than fine-tuning
  - Approximating likelihood using Llama2-13B for GPT-3.5 may introduce some error
  - Focusing on intrinsic vs. non-intrinsic criteria may not capture all aspects of bias
- Failure signatures:
  - High BiasScore but no improvement after mitigation (indicates ineffective few-shot examples or in-context learning)
  - Significant increase in computational cost without corresponding improvement in evaluation performance
  - BiasScore close to zero but evaluation performance decreases (indicates over-correction or misalignment with human judgments)
- First 3 experiments:
  1. Measure BiasScore for a dataset with known bias to verify the quantification method
  2. Apply bias mitigation to a dataset and measure the change in BiasScore and evaluation performance
  3. Compare the effectiveness of in-context learning vs. fine-tuning for bias mitigation on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do instruction tuning and model size impact likelihood bias in LLM-based evaluators?
- Basis in paper: [explicit] The authors state: "In future work, we aim to investigate the relationship between other biases in LLM-based evaluators... Additionally, we plan to examine the impact of instruction-tuning and model size on likelihood bias."
- Why unresolved: The paper focuses on mitigating likelihood bias using in-context learning but does not explore how different training approaches (instruction tuning) or model scales affect the presence and magnitude of likelihood bias.
- What evidence would resolve it: Systematic experiments comparing likelihood bias across models of different sizes (e.g., GPT-3.5 vs GPT-4) and training paradigms (standard vs instruction-tuned) using the proposed BiasScore metric.

### Open Question 2
- Question: Is fine-tuning more effective than in-context learning for mitigating likelihood bias, and what is the optimal amount of training data needed?
- Basis in paper: [inferred] The authors acknowledge limitations: "One solution to them is fine-tuning the model instead of in-context learning. It is therefore necessary to explore whether fine-tuning works better than in-context learning and how much data we need."
- Why unresolved: The paper only explores in-context learning for bias mitigation and notes computational and token limitations. The effectiveness of fine-tuning versus in-context learning for this specific bias remains unexplored.
- What evidence would resolve it: Comparative experiments measuring BiasScore and evaluation performance for both fine-tuning and in-context learning approaches, varying the amount of training data used for fine-tuning.

### Open Question 3
- Question: What is the relationship between likelihood bias and other identified biases in LLM-based evaluators (self-enhancement bias and verbosity bias)?
- Basis in paper: [explicit] The authors state: "In future work, we aim to investigate the relationship between other biases in LLM-based evaluators, such as self-enhancement bias (Zheng et al., 2023) and verbosity bias (Zheng et al., 2023; Saito et al., 2023), and likelihood bias."
- Why unresolved: While the paper introduces likelihood bias and proposes a mitigation method, it does not investigate how this bias interacts with or contributes to other known biases in LLM evaluation.
- What evidence would resolve it: Correlation analysis between BiasScore (likelihood bias) and metrics measuring self-enhancement and verbosity biases across multiple datasets and tasks, exploring potential causal relationships.

## Limitations
- The causal relationship between likelihood scores and evaluation judgments is not fully established
- Effectiveness of in-context learning for bias mitigation may vary depending on few-shot example selection and evaluation task
- Approximation of GPT-3.5 likelihood scores using Llama2-13B introduces potential measurement error
- Generalizability beyond specific tasks (data-to-text and GEC) and evaluation criteria tested remains uncertain

## Confidence

**High Confidence**: The observation that LLMs exhibit likelihood bias (overrating high-likelihood outputs and underrating low-likelihood ones compared to human judgments) is well-supported by experimental results across multiple tasks and criteria. The distinction between intrinsic and non-intrinsic evaluation criteria and their differential susceptibility to bias is clearly demonstrated.

**Medium Confidence**: The effectiveness of the proposed bias mitigation method shows significant improvements in many cases, but the magnitude of improvement varies across tasks and criteria. The mechanism by which few-shot examples correct bias through in-context learning is plausible but not fully validated for all types of bias.

**Low Confidence**: The generalizability of results beyond the specific tasks (data-to-text and GEC) and evaluation criteria tested remains uncertain. The long-term stability of bias mitigation and potential unintended consequences of the correction approach require further investigation.

## Next Checks

1. **Cross-task validation**: Apply the bias quantification and mitigation method to additional NLP tasks (e.g., summarization, dialogue systems) with different types of evaluation criteria to assess generalizability beyond data-to-text and GEC.

2. **Longitudinal bias stability test**: Evaluate whether the mitigated bias remains stable over multiple evaluation rounds and whether the LLM gradually reverts to biased behavior without periodic retraining or recalibration.

3. **Ablation study on few-shot example selection**: Systematically vary the number of few-shot examples (beyond the 8 used) and the selection criteria (beyond RS(t) ranking) to determine optimal parameters for bias mitigation and identify potential overfitting to specific bias patterns.