---
ver: rpa2
title: 'CUTE: Measuring LLMs'' Understanding of Their Tokens'
arxiv_id: '2409.15452'
source_url: https://arxiv.org/abs/2409.15452
tags:
- tasks
- word
- llms
- spelling
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CUTE, a benchmark designed to evaluate Large
  Language Models' (LLMs) understanding of token composition and character-level manipulation.
  While LLMs typically process text as multi-character tokens without direct character
  access, CUTE tests whether they can infer orthographic knowledge.
---

# CUTE: Measuring LLMs' Understanding of Their Tokens
## Quick Facts
- arXiv ID: 2409.15452
- Source URL: https://arxiv.org/abs/2409.15452
- Authors: Lukas Edman; Helmut Schmid; Alexander Fraser
- Reference count: 25
- Key outcome: CUTE benchmark reveals LLMs struggle with character-level manipulation tasks despite strong spelling abilities, showing incomplete understanding of token composition.

## Executive Summary
This paper introduces CUTE, a benchmark designed to evaluate whether Large Language Models truly understand the orthographic structure of the tokens they process. While LLMs typically operate on multi-character tokens without direct character access, CUTE tests if they can infer and manipulate the underlying character-level composition. The benchmark includes tasks for spelling, character membership, orthographic vs. semantic similarity, and text manipulation at both character and word levels. Experiments on 12 popular models (7B-132B parameters) reveal that while models excel at spelling tasks, they struggle significantly with character-level manipulation compared to word-level tasks, suggesting that scaling alone may not resolve these fundamental limitations in token understanding.

## Method Summary
CUTE consists of 200 example-answer pairs across four task types: spelling, character membership, orthographic similarity, and text manipulation. Each task is evaluated at both character and word levels, creating eight subtasks total. The benchmark tests models' ability to spell tokens, identify constituent characters, distinguish orthographic from semantic similarity, and perform insertion, deletion, substitution, and swapping operations. Models are evaluated using exact string matching with zero-shot prompting, without instruction-tuning. The benchmark includes both alphabetic and numeric tokens to test generalization across token types.

## Key Results
- LLMs perform strongly on spelling tasks (averaging 89.4% accuracy) but show significant performance gaps on character-level manipulation tasks compared to word-level tasks
- Performance differences between character and word levels reach up to 72.8% on insertion tasks, with larger models showing better overall performance but persistent gaps
- Models with longer training (Llama3, Command-R+) outperform newer models on orthographic similarity tasks, suggesting instruction tuning improves token understanding
- Aya shows slight improvements over Command-R on character-level tasks after multilingual instruction tuning

## Why This Works (Mechanism)
None provided

## Foundational Learning
- Tokenization in LLMs: Understanding how subword tokenization (like BPE or WordPiece) breaks text into tokens that may contain multiple characters
- Character-level vs. word-level processing: Recognizing the distinction between models that operate on character sequences versus those that work with pre-tokenized units
- Orthographic vs. semantic similarity: Distinguishing between visual/spelling similarity and meaning-based similarity in token representations
- Zero-shot evaluation methodology: Understanding how models are tested without task-specific fine-tuning or examples
- Exact string matching as evaluation metric: Recognizing the limitations of strict string comparison for assessing model outputs that may be semantically equivalent but syntactically different

## Architecture Onboarding
- Component map: Input prompt → LLM token processing → Task execution → String output → Exact match evaluation
- Critical path: Tokenization affects how models perceive character relationships, which determines their ability to perform character-level manipulations
- Design tradeoffs: Zero-shot evaluation vs. instruction-tuned performance; exact string matching vs. semantic similarity evaluation; character-level vs. word-level task design
- Failure signatures: Models that can spell tokens but cannot manipulate individual characters; large performance gaps between character and word level tasks; inability to generalize character manipulation across token types
- First experiments: 1) Test spelling vs. character manipulation performance on the same tokens, 2) Compare exact match vs. semantic similarity evaluation, 3) Evaluate cross-token generalization for character manipulation tasks

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does scaling parameter count alone sufficiently improve LLMs' performance on character-level manipulation tasks, or are architectural modifications needed?
- Basis in paper: [explicit] The paper states "It remains to be seen if noticeable effects arise as the vocabulary size approaches the number of characters" and notes that "the performance gap between word and character level tasks is large" for insertion and swapping tasks, suggesting scaling alone may not be sufficient.
- Why unresolved: The paper observes that larger models generally perform better but notes that "for insertion and swapping, the performance gap between word and character level tasks is large" despite scaling. The relationship between parameter count, training data, and character-level task performance needs more systematic investigation.
- What evidence would resolve it: Controlled experiments comparing models with identical architectures but different parameter counts on character-level tasks, plus architectural modifications specifically designed for character-level processing.

### Open Question 2
- Question: How does multilingual instruction tuning affect LLMs' ability to perform character-level tasks compared to English-centric models?
- Basis in paper: [explicit] The paper notes "It is not yet clear why [Command-R+ and Llama3] perform so well on [orthographic similarity]" and observes that "Aya makes slight improvements over Command-R on the character-level" after multilingual instruction tuning.
- Why unresolved: While the paper observes some performance differences, it cannot definitively attribute improvements to multilingual instruction tuning versus additional English data or other factors. The specific mechanisms by which multilingual training affects character-level understanding remain unclear.
- What evidence would resolve it: Direct comparison of character-level task performance between otherwise identical models trained with and without multilingual instruction tuning, controlling for other variables.

### Open Question 3
- Question: What is the optimal vocabulary size for balancing subword tokenization efficiency with character-level understanding in LLMs?
- Basis in paper: [explicit] The paper states "There appear to be no noticeable effects of vocabulary size from the results shown" and notes that while Llama 3 performs well with 100k vocabulary, larger vocabularies (Gemma) don't improve performance.
- Why unresolved: The paper only tested a limited range of vocabulary sizes (7B-132B parameters, 32-256k vocabulary) and found no clear pattern. The relationship between vocabulary size and character-level understanding may be non-linear or depend on other factors not controlled in the study.
- What evidence would resolve it: Systematic evaluation of models across a wider range of vocabulary sizes (including approaches to character-level tokenization) while holding other factors constant.

## Limitations
- The benchmark only tests 12 models across five families, which may not represent the full diversity of LLM architectures and training approaches
- Exact string matching evaluation may be overly strict given LLMs' tendency toward paraphrasing and variation in token representations
- Zero-shot prompting without instruction-tuning may underestimate models' true capabilities, as many contemporary LLMs are optimized for instruction-following
- The benchmark does not explore whether performance differences stem from architectural choices, training data composition, or tokenization strategies

## Confidence
- High Confidence: The core finding that LLMs perform better on word-level manipulation tasks than character-level ones is well-supported by the experimental results, with consistent patterns across multiple model families and task types.
- Medium Confidence: The claim that scaling alone may not solve token composition limitations is reasonable but requires more extensive testing across additional model sizes and families to establish definitively.
- Medium Confidence: The interpretation that poor character-level performance indicates incomplete understanding of token composition is plausible but could also reflect limitations in how the benchmark tasks are framed or evaluated.

## Next Checks
1. Test additional model families (including decoder-only, encoder-decoder, and hybrid architectures) and parameter sizes beyond the 7B-132B range to determine if the observed patterns generalize across the LLM landscape.
2. Implement evaluation metrics that account for semantic equivalence and paraphrasing (e.g., using embedding-based similarity) rather than exact string matching to assess whether models provide functionally correct answers in different formats.
3. Conduct controlled experiments varying prompt formatting, few-shot examples, and instruction-tuning to determine the extent to which performance differences reflect inherent model limitations versus prompting strategy effects.