---
ver: rpa2
title: 'TrajGEOS: Trajectory Graph Enhanced Orientation-based Sequential Network for
  Mobility Prediction'
arxiv_id: '2412.19092'
source_url: https://arxiv.org/abs/2412.19092
tags:
- location
- user
- trajectory
- graph
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a trajectory graph enhanced orientation-based
  sequential network (TrajGEOS) for next location prediction. The model constructs
  a global trajectory graph from historical check-in data and applies hierarchical
  graph convolution to capture location and user embeddings, which are then used as
  additional features in downstream modules.
---

# TrajGEOS: Trajectory Graph Enhanced Orientation-based Sequential Network for Mobility Prediction

## Quick Facts
- arXiv ID: 2412.19092
- Source URL: https://arxiv.org/abs/2412.19092
- Reference count: 40
- Achieves 27.2%, 24.9%, and 13.2% in Recall@1 on NYC, TKY, and Dallas datasets respectively

## Executive Summary
This paper proposes TrajGEOS, a novel deep learning framework for next location prediction in location-based social networks. The model integrates hierarchical graph convolution to capture global location relationships and user-specific long-term preferences, an orientation-based module to learn mid-term preferences from recent trajectories, and a multi-task learning strategy that leverages category prediction as an auxiliary task. Extensive experiments on three real-world datasets demonstrate that TrajGEOS significantly outperforms state-of-the-art methods, achieving substantial improvements in recall metrics.

## Method Summary
TrajGEOS constructs a global trajectory graph from historical check-in data and applies hierarchical graph convolution to learn location and user embeddings. The model then combines these long-term preferences with short-term sequential patterns captured by GRU and mid-term preferences extracted through an orientation module that uses position-aware attention on recent trajectories. A multi-task learning approach simultaneously predicts next location and next category, with the category prediction serving as an auxiliary task to enhance location prediction accuracy. The model is trained end-to-end with an adaptive loss weighting mechanism.

## Key Results
- Achieves 27.2%, 24.9%, and 13.2% in Recall@1 on NYC, TKY, and Dallas datasets respectively
- Outperforms state-of-the-art methods by significant margins across all evaluation metrics
- Demonstrates effectiveness of combining long-term (graph-based), short-term (sequential), and mid-term (orientation-based) preference modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical graph convolution captures both global location relationships and user-specific long-term preferences
- Core assumption: Global trajectory graph can reveal implicit location relationships not obvious in sequential data
- Break condition: Graph construction fails to capture meaningful transitions in sparse datasets

### Mechanism 2
- Claim: Orientation module captures mid-term preferences by leveraging recent trajectory sequences with position-aware attention
- Core assumption: User mobility patterns have a mid-term component not captured by short-term or long-term modules
- Break condition: Recent trajectories too short or noisy for effective attention mechanism

### Mechanism 3
- Claim: Multi-task learning (next location + next category) improves location prediction by leveraging auxiliary information
- Core assumption: Next location and category tasks share useful intermediate representations
- Break condition: Category prediction task becomes too noisy or uncorrelated with location prediction

## Foundational Learning

- **Graph Neural Networks and GraphSAGE**: To process trajectory graph and extract meaningful location/user embeddings
  - Quick check: How does GraphSAGE aggregate information from node neighbors for location embeddings?

- **Recurrent Neural Networks and GRUs**: To model sequential nature of user trajectories and capture short-term patterns
  - Quick check: What role does GRU hidden state play in modeling sequential dependencies?

- **Attention Mechanisms and Position Embeddings**: To weigh recent trajectories based on relative position and recency
  - Quick check: How do sine/cosine position embeddings help attention distinguish sequence steps?

## Architecture Onboarding

- **Component map**: Global trajectory graph -> EGraphSAGE -> location embeddings -> GraphSAGE on user subgraphs -> user embeddings
  Trajectory embedding -> GRU (short-term) + Orientation module (mid-term) -> combined with long-term
  Combined preferences + user ID -> MLP -> next location and category prediction

- **Critical path**: 1) Construct global trajectory graph from training data
  2) Apply EGraphSAGE for location embeddings
  3) Apply GraphSAGE on user subgraphs for long-term preferences
  4) Embed current/recent trajectories -> GRU + Orientation module
  5) Combine all preferences + user ID -> MLP prediction

- **Design tradeoffs**: Graph dropout (0.5) vs. over-smoothing; multi-task learning complexity vs. performance; orientation module complexity vs. mid-term pattern capture

- **Failure signatures**: Poor performance on sparse datasets; over-smoothing in location embeddings; misalignment between short-term and mid-term signals

- **First 3 experiments**: 1) Remove graph learning module and measure performance drop
  2) Remove orientation module and test mid-term preference importance
  3) Remove multi-task learning to evaluate category prediction benefit

## Open Questions the Paper Calls Out
- Scalability verification on dense mobility datasets like population-scale mobile phone data
- Investigation of friendship networks contribution to next location prediction in LBSNs
- Analysis of performance variation across different location types and implications for model optimization

## Limitations
- Novel architectural components lack direct validation through ablation studies
- Performance on sparse datasets not thoroughly evaluated
- Multi-task learning approach may introduce noise if category prediction is unreliable

## Confidence
- **High confidence**: Hierarchical graph convolution mechanism for capturing long-term preferences
- **Medium confidence**: Orientation module's ability to capture mid-term preferences
- **Medium confidence**: Multi-task learning approach improving overall performance

## Next Checks
1. Perform detailed ablation studies removing each novel component to quantify individual contributions
2. Test model robustness on sparse datasets to verify graph learning mechanism
3. Compare proposed EGraphSAGE layer and orientation module against established alternatives