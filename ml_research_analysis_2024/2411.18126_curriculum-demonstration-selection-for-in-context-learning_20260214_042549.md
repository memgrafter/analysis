---
ver: rpa2
title: Curriculum Demonstration Selection for In-Context Learning
arxiv_id: '2411.18126'
source_url: https://arxiv.org/abs/2411.18126
tags:
- arxiv
- demonstrations
- learning
- code
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Curriculum Demonstration Selection (CDS),
  a novel demonstration selection method for in-context learning (ICL) that leverages
  curriculum learning principles. Unlike existing methods that rely solely on similarity,
  CDS partitions the training set by difficulty levels and selects demonstrations
  from easy to difficult, ensuring a diverse range of complexities.
---

# Curriculum Demonstration Selection for In-Context Learning

## Quick Facts
- arXiv ID: 2411.18126
- Source URL: https://arxiv.org/abs/2411.18126
- Authors: Duc Anh Vu; Nguyen Tran Cong Duy; Xiaobao Wu; Hoang Minh Nhat; Du Mingzhe; Nguyen Thanh Thong; Anh Tuan Luu
- Reference count: 40
- Primary result: CDS improves LLM performance by 3-6% over baseline methods by selecting demonstrations spanning multiple difficulty levels

## Executive Summary
This paper introduces Curriculum Demonstration Selection (CDS), a novel demonstration selection method for in-context learning that leverages curriculum learning principles. Unlike existing methods that rely solely on similarity, CDS partitions the training set by difficulty levels and selects demonstrations from easy to difficult, ensuring a diverse range of complexities. This approach enables large language models (LLMs) to learn from varied task difficulties within the training set. Experiments across three benchmarks—MATH (mathematical reasoning), ARC-Challenge (commonsense reasoning), and Mercury (code generation)—demonstrate that CDS consistently outperforms baseline methods, including random selection and similarity-based approaches like KATE. Notably, CDS shows significant improvements in solving harder problems, with accuracy gains of up to 6% on challenging tasks.

## Method Summary
CDS is a demonstration selection method for in-context learning that partitions the training set by difficulty levels and selects demonstrations from easy to difficult. The method uses human-annotated complexity scores or grade levels to partition data, then retrieves one demonstration from each partition using either similarity-based retrieval (CLS embeddings with negative Euclidean distance) or random selection. Demonstrations are shuffled before inference to prevent ordering bias. The approach is evaluated across three benchmarks (MATH, ARC-Challenge, Mercury) with nine LLMs using five-shot settings and greedy decoding with Chain-of-Thought prompting.

## Key Results
- CDS consistently outperforms baseline methods (random selection and KATE) across all three benchmarks
- Accuracy improvements of up to 6% on challenging tasks, particularly for harder problems
- Effectiveness demonstrated across nine LLMs including Llama-2, Llama-3, Mistral, and Qwen
- Demonstration ordering (easy-to-hard vs. random) shows no significant performance difference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CDS improves LLM performance by exposing the model to demonstrations spanning multiple difficulty levels during inference.
- Mechanism: The method partitions the training set by difficulty (using human-annotated complexity scores or grade levels) and retrieves one demonstration from each partition. This creates a diverse set of demonstrations that collectively cover the complexity spectrum of the task.
- Core assumption: LLMs benefit from seeing examples of varying difficulty during in-context learning, not just similar or uniformly difficult ones.
- Evidence anchors:
  - [abstract] "CDS partitions the training set by difficulty levels and selects demonstrations from easy to difficult, ensuring a diverse range of complexities."
  - [section] "CDS ensures that selected demonstrations cover a wide spectrum of difficulty levels, enabling LLMs to progressively learn from varied complexities within the training set."
  - [corpus] Found 25 related papers mentioning curriculum learning and demonstration selection, suggesting active research in this area but limited direct comparison to CDS specifically.

### Mechanism 2
- Claim: The similarity-based retrieval component enhances demonstration relevance while maintaining diversity.
- Mechanism: After partitioning by difficulty, CDS uses CLS embeddings and negative Euclidean distance to select the most similar demonstration from each difficulty level, balancing relevance with complexity diversity.
- Core assumption: Relevant demonstrations from each difficulty level provide better context than either random selection or similarity-only approaches.
- Evidence anchors:
  - [abstract] "The retrieval process can either be similarity-based or random. For similarity-based retrieval, we use CLS embeddings from a pre-trained Transformers model to represent the sentences."
  - [section] "The algorithm then selects demonstrations that are most similar to the test question from each difficulty level by calculating the negative Euclidean distance."

### Mechanism 3
- Claim: Ordering demonstrations from easy to hard (or random shuffling) doesn't significantly impact performance, indicating robustness to presentation order.
- Mechanism: The method includes an optional reordering step (E2H) but experiments show random shuffling performs similarly, suggesting the model benefits primarily from the diversity of demonstrations rather than their sequential presentation.
- Core assumption: The diversity of difficulty levels matters more than the order in which demonstrations are presented.
- Evidence anchors:
  - [section] "As demonstrated in Table 3, ordering the demonstrations from easy to hard (E2H) has no significant effect on performance compared to random shuffling."

## Foundational Learning

- Concept: Curriculum learning principles
  - Why needed here: CDS directly applies curriculum learning by structuring demonstrations from easy to difficult, helping models build understanding progressively
  - Quick check question: What is the core principle behind curriculum learning that CDS leverages?

- Concept: In-context learning (ICL) mechanics
  - Why needed here: Understanding how LLMs use demonstrations during inference is essential for designing effective demonstration selection strategies
  - Quick check question: How does demonstration selection impact ICL performance according to the paper?

- Concept: Difficulty measurement and partitioning
  - Why needed here: CDS relies on accurately partitioning data by difficulty using human-annotated metadata or performance metrics
  - Quick check question: What methods does CDS use to determine difficulty levels for partitioning?

## Architecture Onboarding

- Component map:
  Difficulty measurer -> Partition training data into k difficulty levels using human-annotated complexity scores
  Retrieval function -> Select demonstrations from each partition using similarity-based or random retrieval
  Demonstration pool -> Aggregate selected demonstrations from all difficulty levels
  Shuffling module -> Randomly shuffle demonstrations to prevent ordering bias
  LLM inference engine -> Execute in-context learning with selected demonstrations

- Critical path:
  1. Partition training set by difficulty using measurer
  2. For each test instance, retrieve one demonstration from each difficulty partition
  3. Aggregate demonstrations into pool and shuffle
  4. Construct prompt with demonstrations and test question
  5. Execute LLM inference with greedy decoding (or specified decoding strategy)

- Design tradeoffs:
  - Fixed k=5 demonstrations balances computational cost with performance, but may not be optimal for all tasks
  - Human-annotated difficulty metadata provides reliable partitioning but limits applicability to datasets with such metadata
  - Similarity-based retrieval adds computational overhead but improves relevance compared to random selection

- Failure signatures:
  - Poor performance on harder problems suggests difficulty partitioning may be inaccurate
  - Inconsistent improvement across different LLMs may indicate retrieval function or demonstration diversity issues
  - No improvement over baselines suggests the diversity benefit isn't being realized by the specific LLM architecture

- First 3 experiments:
  1. Test CDS with random retrieval vs similarity retrieval on a small subset to validate retrieval function impact
  2. Run CDS with varying k values (3, 5, 7 demonstrations) to find optimal demonstration count
  3. Compare CDS performance on easy vs hard problems separately to verify the claimed difficulty-level benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CDS performance scale with different shot settings (e.g., 3-shot, 10-shot, or more)?
- Basis in paper: [explicit] The paper mentions using a fixed five-shot setting and acknowledges this as a limitation, suggesting future work could explore varying the number of demonstrations.
- Why unresolved: The current experiments only tested CDS with five demonstrations per task, leaving uncertainty about optimal shot settings for different task complexities.
- What evidence would resolve it: Experiments varying the number of demonstrations (e.g., 3, 5, 10) across multiple benchmarks to identify performance trends and optimal shot settings.

### Open Question 2
- Question: Can CDS be effectively applied to domains without predefined difficulty metadata?
- Basis in paper: [explicit] The paper notes that CDS relies on predefined metadata (e.g., grade levels) for curriculum construction and identifies this as a limitation when such metadata is unavailable.
- Why unresolved: The paper doesn't demonstrate alternative methods for estimating task complexity in metadata-free scenarios, leaving uncertainty about CDS applicability in such domains.
- What evidence would resolve it: Experiments applying CDS to domains without difficulty metadata using alternative complexity estimation methods (e.g., model-based difficulty scoring or task-specific heuristics).

## Limitations
- Reliance on human-annotated difficulty metadata limits applicability to datasets without such annotations
- Experimental scope limited to three benchmarks and nine LLMs, raising questions about generalizability
- Fixed five-shot setting doesn't explore whether different tasks benefit from varying demonstration counts

## Confidence

**High Confidence**: The core mechanism of partitioning by difficulty and selecting diverse demonstrations is well-supported by experimental results, showing consistent improvements across all three benchmarks and multiple LLMs.

**Medium Confidence**: The assertion that CDS specifically improves performance on harder problems is supported by data, but analysis could be more granular regarding which specific difficulty levels contribute most to improvement.

**Low Confidence**: Scalability claims beyond tested nine LLMs and three benchmarks remain speculative, with insufficient evidence about effectiveness on different task domains or smaller models.

## Next Checks

1. **Difficulty Partitioning Validation**: Run CDS on a dataset with known difficulty distributions (like CIFAR-100 or ImageNet subsets) where difficulty can be measured through model performance rather than human annotation, to verify the partitioning mechanism works independently of metadata availability.

2. **Cross-Model Sensitivity Analysis**: Systematically test CDS across a broader range of model sizes and architectures (including smaller models like 7B and 1B parameters) to identify whether the method's effectiveness correlates with model scale or specific architectural features.

3. **Demonstration Count Optimization**: Conduct ablation studies varying the number of demonstrations (k=3, k=5, k=7, k=10) on each benchmark to determine whether the fixed k=5 choice is optimal or if different tasks benefit from different demonstration counts.