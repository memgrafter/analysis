---
ver: rpa2
title: Enhancing RL Safety with Counterfactual LLM Reasoning
arxiv_id: '2409.10188'
source_url: https://arxiv.org/abs/2409.10188
tags:
- safety
- policy
- action
- learning
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unsafe behavior in reinforcement
  learning (RL) policies and the difficulty of explaining such behavior. The core
  method combines model checking with counterfactual large language model (LLM) reasoning.
---

# Enhancing RL Safety with Counterfactual LLM Reasoning

## Quick Facts
- arXiv ID: 2409.10188
- Source URL: https://arxiv.org/abs/2409.10188
- Authors: Dennis Gross; Helge Spieker
- Reference count: 24
- Primary result: Combines model checking with LLM reasoning to improve RL policy safety through counterfactual explanations

## Executive Summary
This paper addresses the problem of unsafe behavior in reinforcement learning (RL) policies and the difficulty of explaining such behavior. The core method combines model checking with counterfactual large language model (LLM) reasoning. The approach uses Storm model checker to identify safety-critical states, then employs an LLM to explain why unsafe actions were taken and suggest alternatives. Experiments on a robotic cleaning agent environment show that LLM-suggested alternatives can improve safety metrics compared to the original policy. Specifically, using natural language environment descriptions with GPT-4-turbo reduced the probability of running out of energy from 0.603 to 0.406. The approach successfully explains failures in approximately 3/4 of cases and improves safety while providing human-readable explanations for policy decisions.

## Method Summary
The approach combines formal verification with LLM reasoning to improve RL policy safety. It first constructs an induced deterministic Markov Decision Process (DTMC) from the trained RL policy and environment, then uses Storm model checker to verify safety properties specified in Probabilistic Computation Tree Logic (PCTL). Safety-critical states that lead to violations are extracted and presented to an LLM along with the environment description. The LLM provides human-readable explanations for the unsafe actions and suggests alternative actions. These alternatives are then incorporated back into the policy, and safety metrics are re-evaluated to measure improvement. The method was tested on a robotic cleaning agent environment where the LLM's natural language explanations yielded better safety improvements than formal MDP encodings.

## Key Results
- LLM with natural language environment descriptions reduced energy depletion probability from 0.603 to 0.406
- The approach successfully explains failures in approximately 3/4 of cases
- Natural language descriptions outperformed formal MDP encodings (0.406 vs 0.660 probability)
- Provides human-readable explanations for policy decisions while improving safety metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Storm model checker can accurately identify safety-critical states that lead to violations through the trained RL policy.
- Mechanism: The approach incrementally builds the induced DTMC from the MDP using the trained policy, then verifies it against PCTL formulas to find states that directly lead to safety violations.
- Core assumption: The trained RL policy is deterministic and the induced DTMC can be fully constructed for verification.
- Evidence anchors:
  - [abstract] "We verify the policy's safety using the Storm model checker [13] and the PCTL formula."
  - [section] "We incrementally build the induced DTMC of the policy π and the MDP M as follows... The resulting DTMC D induced by M and π is fully deterministic, with no open action choices, and is passed to the model checker Storm for verification."
- Break condition: If the RL policy is stochastic or the state space is too large to construct the full DTMC, the model checking approach fails.

### Mechanism 2
- Claim: LLM can explain why unsafe actions were taken and suggest safer alternatives at safety-critical states.
- Mechanism: The LLM receives the environment description, state-action pair leading to violation, and outputs a human-readable explanation plus an alternative action that could improve safety.
- Core assumption: The LLM has been trained on sufficient data to understand the environment context and provide sensible counterfactual explanations.
- Evidence anchors:
  - [abstract] "For each extracted state, we provide the RL environment information and the state with its action leading to a safety violation to the LLM, asking it to explain the mistake and suggest a safer alternative."
  - [section] "Example 2 (LLM Ouput). The agent left the dirty room that nobody was taking care of. An alternative action would be to use the clean action."
- Break condition: If the LLM cannot parse the environment description or state representation correctly, or if it lacks understanding of safety concepts.

### Mechanism 3
- Claim: Replacing unsafe actions with LLM-suggested alternatives at critical states improves overall policy safety metrics.
- Mechanism: After obtaining alternative actions from the LLM, the approach rebuilds the DTMC with these alternatives and re-verifies the safety properties, showing improved safety measurements.
- Core assumption: The LLM-suggested alternatives are valid actions in the MDP and will actually improve safety when substituted.
- Evidence anchors:
  - [abstract] "Then, we reverify the policy with the action alternatives concerning safety properties in the RL environment."
  - [section] "Table 1. Different safety policy repair methods and the reachability probability of the unsafe behavior... LLM with a natural language explanation of the underlying environment improved safety performance to avoid running out of energy."
- Break condition: If LLM suggestions are invalid actions or if the safety improvement is not statistically significant across multiple runs.

## Foundational Learning

- Markov Decision Processes (MDPs)
  - Why needed here: The RL environment is modeled as an MDP, and the approach requires understanding state transitions, actions, and policies to build the DTMC for verification.
  - Quick check question: What is the difference between an MDP and the induced DTMC when a deterministic policy is applied?

- Probabilistic Computation Tree Logic (PCTL)
  - Why needed here: PCTL formulas are used to specify and verify safety properties of the RL policy during model checking.
  - Quick check question: How would you express "the probability of eventually running out of energy is less than 0.5" in PCTL?

- Counterfactual reasoning
  - Why needed here: The LLM provides counterfactual explanations by suggesting alternative actions that could have prevented safety violations.
  - Quick check question: What is the key difference between a counterfactual explanation and a standard explanation in the context of RL policy decisions?

## Architecture Onboarding

- Component map:
  RL environment (MDP model) -> Trained RL policy -> Storm model checker -> LLM (GPT-4-turbo API) -> PCTL safety property specification -> DTMC construction and verification pipeline

- Critical path:
  1. Build induced DTMC from MDP + policy
  2. Verify against PCTL safety properties using Storm
  3. Extract safety-critical state-action pairs
  4. Query LLM for explanations and alternatives
  5. Rebuild DTMC with LLM alternatives
  6. Re-verify safety metrics

- Design tradeoffs:
  - Deterministic vs. stochastic policy handling (current approach assumes deterministic)
  - State space size vs. model checking feasibility
  - LLM prompt engineering vs. explanation quality
  - Natural language vs. formal specification for environment description

- Failure signatures:
  - Model checking timeout or memory error (state space too large)
  - LLM returns invalid action or fails to parse input
  - Safety metrics worsen after applying LLM alternatives
  - Inconsistent explanations across multiple LLM queries for same state

- First 3 experiments:
  1. Verify original trained policy safety using Storm with PCTL formula for running out of energy
  2. Extract safety-critical states and query LLM with natural language environment description
  3. Rebuild DTMC with LLM alternatives and re-verify safety metrics, comparing to baseline

## Open Questions the Paper Calls Out

- Open Question 1: What are the limitations of using natural language descriptions versus formal MDP encodings when interacting with LLMs for policy safety improvements?
  - Basis in paper: [explicit] The paper explicitly compares LLM performance with natural language environment descriptions versus PRISM-encoded MDPs, finding that natural language yields better results (0.406 vs 0.660 probability of running out of energy) and speculates that LLMs are more trained on natural language than formal code.
  - Why unresolved: While the paper observes better performance with natural language descriptions, it doesn't systematically investigate why this difference occurs or what aspects of the natural language description are most critical for successful LLM reasoning.
  - What evidence would resolve it: Controlled experiments varying the level of detail, structure, and terminology in natural language descriptions while measuring LLM performance, or comparative analysis of how LLMs process natural language versus formal specifications.

- Open Question 2: How does the quality of LLM-generated explanations and alternatives vary across different types of safety violations?
  - Basis in paper: [explicit] The paper reports that approximately 3/4 of LLM explanations were deemed correct for energy depletion scenarios, but doesn't provide similar analysis for other safety properties like "wrong charge" or "wrong room switch."
  - Why unresolved: The paper only explicitly evaluates the quality of explanations for one safety property (running out of energy), leaving uncertainty about whether LLM performance is consistent across different types of safety violations.
  - What evidence would resolve it: Systematic evaluation of LLM explanation quality across all safety properties studied, including detailed error analysis of incorrect explanations to identify patterns.

- Open Question 3: What is the generalizability of this approach to different RL environments and policy architectures?
  - Basis in paper: [inferred] The approach is demonstrated on a single robotic cleaning agent environment with a specific policy architecture (deep Q-learning with 4 hidden layers), and the authors mention limitations to "MDP environments" and "memoryless policies."
  - Why unresolved: The paper only tests on one environment and policy type, and while it mentions limitations, it doesn't empirically investigate how performance varies across different environments or policy architectures.
  - What evidence would resolve it: Testing the approach on multiple diverse RL environments (different state spaces, action spaces, and dynamics) and with various policy architectures (policy gradients, actor-critic methods, etc.) to establish the breadth of applicability.

## Limitations
- The approach is limited to MDP environments and memoryless policies, restricting its applicability to more complex RL scenarios
- Model checking scalability remains a concern for environments with large state spaces
- The paper only demonstrates effectiveness on a single robotic cleaning agent environment, limiting generalizability

## Confidence
- Medium: The core mechanism of using Storm for identifying safety-critical states is well-established in model checking literature, but its application to RL policy verification is novel and requires further validation. The LLM's ability to provide meaningful counterfactual explanations is demonstrated but not rigorously evaluated - the paper reports success in approximately 3/4 of cases but doesn't provide detailed error analysis for the remaining failures.

## Next Checks
1. Test the approach on multiple RL environments with varying complexity to assess scalability limits
2. Conduct ablation studies to quantify the contribution of LLM explanations versus alternative action suggestions
3. Perform statistical significance testing across multiple runs to validate the reported safety improvements