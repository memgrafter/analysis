---
ver: rpa2
title: Zero-shot forecasting of chaotic systems
arxiv_id: '2409.15771'
source_url: https://arxiv.org/abs/2409.15771
tags:
- systems
- time
- chaotic
- series
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work benchmarks foundation models on 135 chaotic systems,
  comparing their zero-shot forecasting ability to custom-trained baselines (NBEATS,
  TiDE, etc.). Models like Chronos match or exceed trained models in short-term accuracy
  (VPT ~ 1 Lyapunov time), especially when training data is limited.
---

# Zero-shot forecasting of chaotic systems

## Quick Facts
- arXiv ID: 2409.15771
- Source URL: https://arxiv.org/abs/2409.15771
- Reference count: 40
- Foundation models achieve competitive zero-shot forecasting of chaotic systems without explicit training

## Executive Summary
This work benchmarks foundation models on 135 chaotic systems, comparing their zero-shot forecasting ability to custom-trained baselines (NBEATS, TiDE, etc.). Models like Chronos match or exceed trained models in short-term accuracy (VPT ~ 1 Lyapunov time), especially when training data is limited. Zero-shot models also capture long-term attractor geometry (fractal dimension, KL divergence) after point forecasts fail, via context parroting and in-context learning. Larger models generalize better, and accuracy improves with longer context even beyond decorrelation timescales. Performance degrades with non-stationarity. Fine-tuning Chronos requires large-scale retraining due to domain shift. The results show foundation models can effectively forecast chaotic systems without explicit training, highlighting their potential for probing complex nonlinear dynamics.

## Method Summary
The study uses Chronos, a time series foundation model, to perform zero-shot forecasting on 135 chaotic dynamical systems from the dysts dataset. Each system is evaluated with 20 initial conditions, providing 512 timepoints for context and 300 for testing. Chronos generates forecasts using in-context learning with context lengths up to 512 points, while baseline models (NBEATS, TiDE, NVAR, Transformer, LSTM) are trained with hyperparameter tuning on lookback windows. Performance is measured using sMAPE, VPT, and attractor similarity metrics (correlation dimension, KL divergence).

## Key Results
- Zero-shot Chronos models achieve VPT ~ 1 Lyapunov time, matching or exceeding trained baselines
- Larger Chronos models (710M parameters) generalize better than smaller ones (8M parameters)
- Context parroting enables long-term attractor reconstruction after point forecasts fail
- Performance improves with longer context beyond decorrelation timescales

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Zero-shot forecasting performance improves with foundation model size.
- **Mechanism:** Larger models generalize better due to their ability to learn universal features from vast training data, which capture generic structures present in chaotic time series.
- **Core assumption:** The training corpus contains sufficient diversity to learn transferable representations for chaotic dynamics.
- **Evidence anchors:**
  - [abstract] "larger models produce better forecasts"
  - [section 5.1] "Scaling of performance with model size indicates that the larger models exhibit better generalization properties"
  - [corpus] Weak - no corpus evidence directly supports this claim
- **Break condition:** If the training data lacks diversity in chaotic-like dynamics, larger models won't necessarily generalize better.

### Mechanism 2
- **Claim:** Context parroting enables zero-shot forecasting of chaotic attractors.
- **Mechanism:** The model identifies subsequences in the context that resemble the recent history and repeats them, effectively capturing attractor geometry even after point forecasts fail.
- **Core assumption:** Chaotic attractors have repeating patterns that can be matched in the context window.
- **Evidence anchors:**
  - [abstract] "identify context parroting as a simple mechanism"
  - [section 5.3] "much of Chronos's performance arises from its ability to parrot context sequences"
  - [corpus] No direct corpus evidence supporting this mechanism
- **Break condition:** If the chaotic system lacks repeating motifs in the context window, parroting becomes ineffective.

### Mechanism 3
- **Claim:** In-context learning improves with longer context windows beyond decorrelation timescales.
- **Mechanism:** Longer contexts allow the model to learn the natural measure (stationary distribution) of the attractor, even when individual points become decorrelated.
- **Core assumption:** Chaotic attractors have well-defined stationary distributions that can be learned from extended contexts.
- **Evidence anchors:**
  - [abstract] "accuracy improves with longer context even beyond decorrelation timescales"
  - [section 5.4] "random shuffles provide better forecasts than shorter context baselines"
  - [corpus] No corpus evidence directly supporting this claim
- **Break condition:** If the system is non-stationary, longer contexts may degrade performance due to distribution shift.

## Foundational Learning

- **Concept:** Takens' Embedding Theorem
  - **Why needed here:** Explains how univariate models can forecast multivariate chaotic systems by leveraging delay embeddings to reconstruct state space.
  - **Quick check question:** Can a univariate time series contain enough information to reconstruct a multidimensional attractor?

- **Concept:** Lyapunov Time and Predictability
  - **Why needed here:** Provides the fundamental timescale for chaotic predictability and benchmarks model performance.
  - **Quick check question:** What is the relationship between Lyapunov exponent and the timescale over which forecasts become unreliable?

- **Concept:** Natural Measure and Ergodic Theory
  - **Why needed here:** Underlies the in-context learning mechanism where longer contexts help learn the stationary distribution of attractor states.
  - **Quick check question:** How does the ergodicity of chaotic attractors enable learning of statistical properties from time series data?

## Architecture Onboarding

- **Component map:** Tokenized time series context -> T5 transformer with scaling/quantization -> Generated future time series tokens
- **Critical path:**
  1. Tokenize input time series using Chronos' scaling/quantization layer
  2. Feed tokens into T5 transformer
  3. Generate output tokens autoregressively
  4. Convert tokens back to continuous values
- **Design tradeoffs:**
  - Context length vs. computational cost (quadratic scaling)
  - Model size vs. generalization ability
  - Univariate vs. multivariate forecasting (channel independence)
  - Tokenization granularity vs. information retention
- **Failure signatures:**
  - Poor performance on non-stationary systems
  - Strong dependence on initial conditions
  - Degradation when context lacks repeating patterns
  - Out-of-distribution generalization issues
- **First 3 experiments:**
  1. Test zero-shot forecasting on a simple chaotic system (Lorenz) with varying context lengths to verify in-context learning
  2. Compare performance across different Chronos model sizes on the same chaotic system to verify scaling law
  3. Test forecasting on a non-stationary variant of a chaotic system to identify distribution shift sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific architectural modifications to Chronos (e.g., different positional encoding, multi-variate handling) would most improve its performance on chaotic systems?
- **Basis in paper:** [explicit] The paper notes Chronos's current univariate design and mentions potential improvements like rotary positional encoding and multi-variate handling.
- **Why unresolved:** The paper only speculates on potential modifications and doesn't experimentally test them.
- **What evidence would resolve it:** Controlled experiments testing different architectural variants of Chronos on the chaotic systems benchmark.

### Open Question 2
- **Question:** How does the performance of zero-shot forecasting scale with the complexity of the chaotic system (e.g., dimensionality, Lyapunov exponent)?
- **Basis in paper:** [inferred] The paper mentions 135 chaotic systems but doesn't analyze performance trends across different system complexities.
- **Why unresolved:** The paper presents aggregate results but doesn't explore relationships between system properties and forecasting performance.
- **What evidence would resolve it:** Detailed analysis correlating forecasting metrics with system properties across the benchmark dataset.

### Open Question 3
- **Question:** What is the minimum amount of fine-tuning data required for Chronos to significantly improve its performance on chaotic systems?
- **Basis in paper:** [explicit] The paper attempted fine-tuning but found it ineffective, suggesting the chaotic systems dataset differs from Chronos's training data.
- **Why unresolved:** The paper only tried one fine-tuning approach with a fixed dataset size and didn't explore the scaling relationship.
- **What evidence would resolve it:** Systematic experiments varying the amount of fine-tuning data to find the threshold for performance improvement.

## Limitations

- Context parroting mechanism remains observational rather than causally proven
- Limited academic context with weak citation networks in corpus analysis
- Direct performance comparisons may be confounded by architectural differences between zero-shot and trained models

## Confidence

**High confidence:** Chronos achieves competitive short-term forecasting (VPT ~ 1 Lyapunov time) on chaotic systems, supported by direct quantitative comparisons across 135 systems and multiple baselines.

**Medium confidence:** Scaling law results showing larger models perform better, following established foundation model trends but requiring validation across diverse chaotic system types.

**Low confidence:** Mechanistic explanation of context parroting as the dominant forecasting strategy, given the observational nature of the evidence and lack of causal isolation.

## Next Checks

1. Conduct ablation experiments comparing Chronos performance with shuffled context windows versus ordered windows to quantify the contribution of parroting versus genuine dynamical learning.

2. Test Chronos on synthetic chaotic systems with known attractor structures (e.g., modified Lorenz or RÃ¶ssler systems) to verify that attractor reconstruction via context parroting works as hypothesized.

3. Implement a controlled experiment comparing Chronos to trained models with identical tokenization and architectural constraints to isolate whether performance differences stem from zero-shot capability versus model design choices.