---
ver: rpa2
title: 'PCEvE: Part Contribution Evaluation Based Model Explanation for Human Figure
  Drawing Assessment and Beyond'
arxiv_id: '2409.18260'
source_url: https://arxiv.org/abs/2409.18260
tags:
- part
- pceve
- contribution
- figure
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of providing interpretable explanations
  for human figure drawing (HFD) assessment models, particularly for autism spectrum
  disorder diagnosis. The proposed PCEvE (Part Contribution Evaluation based model
  Explanation) framework uses Shapley Value to measure the contribution of each body
  part to model decisions, offering part-based explanations through contribution histograms.
---

# PCEvE: Part Contribution Evaluation Based Model Explanation for Human Figure Drawing Assessment and Beyond

## Quick Facts
- arXiv ID: 2409.18260
- Source URL: https://arxiv.org/abs/2409.18260
- Reference count: 40
- Primary result: PCEvE framework provides part-based model explanations using Shapley Value for human figure drawing assessment, achieving 98.0% ±2.7% accuracy on ASD Screening and 96.5% accuracy on SCAT datasets

## Executive Summary
This paper introduces PCEvE (Part Contribution Evaluation based model Explanation), a framework that provides interpretable explanations for human figure drawing (HFD) assessment models by evaluating the contribution of each body part to model decisions. The method uses Shapley Value to measure how much each part contributes to model predictions, offering explanations through contribution histograms rather than traditional pixel-level attribution maps. PCEvE generates sample-level, class-level, and task-level explanations, making model decisions more interpretable for tasks like autism spectrum disorder diagnosis. The approach is validated across multiple HFD datasets and demonstrates applicability to photo-realistic fine-grained classification tasks.

## Method Summary
PCEvE is a model explanation framework that uses Shapley Value to measure the contribution of each body part to model decisions in human figure drawing assessment. The method first detects or annotates body parts in input images, then generates all possible combinations of parts by masking individual parts. For each combination, the model is evaluated and Shapley Value is calculated to determine each part's marginal contribution across all combinations. The framework aggregates these contributions to create histograms at sample, class, and task levels, providing comprehensive explanations of which body parts drive model predictions. The approach is validated on ASD Screening, SCAT, and Stanford Cars datasets using various pre-trained models.

## Key Results
- Achieves 98.0% ±2.7% accuracy on ASD Screening dataset using 5-fold cross-validation
- Demonstrates 96.5% accuracy on SCAT dataset with part-based explanations
- Shows clearer explanations compared to traditional pixel-level attribution methods through part contribution histograms
- Validates applicability to photo-realistic fine-grained classification (Stanford Cars) with YOLOv3 part detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCEvE provides more interpretable explanations than pixel-level attribution methods by mapping model decisions to semantically meaningful body parts
- Mechanism: Instead of showing which pixels are important, PCEvE uses part detection to group pixels into body parts and measures Shapley Value contributions at the part level, creating a direct histogram showing which body parts drive the model's decision
- Core assumption: Users can interpret semantic parts more easily than raw pixel importance maps, and body parts in human figure drawings correspond to meaningful features that experts use in assessment
- Evidence anchors:
  - [abstract] "Unlike existing attribution-based XAI approaches, the PCEvE provides a straightforward explanation of a model decision, i.e., a part contribution histogram"
  - [section 2.1] "These pixel-level attributions approaches show critical limitations... force a human to interpret the semantic information of a region in an image, leading to additional costs and time for users"
  - [corpus] Weak evidence - no corpus papers directly compare part-based vs pixel-based XAI interpretability

### Mechanism 2
- Claim: Shapley Value provides fair and comprehensive part contribution measurement by considering all possible combinations of parts
- Mechanism: The framework generates all 2^K combinations of parts (where K is the number of parts), measures model performance for each combination, and calculates each part's marginal contribution across all combinations using the Shapley Value formula
- Core assumption: Shapley Value axioms (efficiency, symmetry, dummy player, linearity) ensure fair contribution measurement that accurately reflects each part's importance to model decisions
- Evidence anchors:
  - [section 3.1] "The Shapley Value is a metric to evaluate the contribution of a player in a coalitional game... we use it to fairly measure how much each part contributes to a model decision"
  - [section 3.2] "Since the Shapley Value fairly measures contributions from all players in a coalition game, we use it to fairly measure how much each part contributes to a model decision"
  - [corpus] Weak evidence - no corpus papers validate Shapley Value application to part-based model explanation

### Mechanism 3
- Claim: PCEvE's three-level explanation framework (sample, class, task) provides comprehensive understanding of model behavior across different abstraction levels
- Mechanism: Sample-level shows which parts matter for individual predictions, class-level aggregates across samples of the same class to identify class-specific patterns, and task-level combines all classes to show overall model focus
- Core assumption: Different levels of abstraction provide complementary insights that together give a more complete picture of model behavior than any single level could
- Evidence anchors:
  - [abstract] "Furthermore, the PCEvE expands the scope of explanations beyond the conventional sample-level to include class-level and task-level insights"
  - [section 3.4] "The C-PCEvE quantifies the contribution of each part within a specific class... The T-PCEvE framework extends the C-PCEvE to the entire dataset D"
  - [corpus] Weak evidence - no corpus papers demonstrate multi-level explanation frameworks for XAI

## Foundational Learning

- Shapley Value from cooperative game theory
  - Why needed here: Provides mathematical framework for fairly measuring part contributions by considering all possible combinations
  - Quick check question: Can you explain why Shapley Value is considered "fair" using its four axioms (efficiency, symmetry, dummy player, linearity)?

- Part detection and annotation
  - Why needed here: Required to map pixel regions to semantically meaningful body parts that can be analyzed individually
  - Quick check question: What challenges might arise when using off-the-shelf part detectors on human figure drawings versus photo-realistic images?

- T-SNE for feature visualization
  - Why needed here: Used in sanity checks to visualize feature spaces of different parts and validate that most contributing parts create more distinct class clusters
  - Quick check question: How would you interpret T-SNE visualizations showing compact clusters for most contributing parts versus intermingled samples for least contributing parts?

## Architecture Onboarding

- Component map:
  Part detection/annotations -> Part combination generator -> Model inference engine -> Shapley Value calculator -> Aggregation layers -> Visualization module

- Critical path:
  1. Detect/annotate parts -> 2. Generate all part combinations -> 3. Run model on each combination -> 4. Calculate Shapley Values -> 5. Aggregate and visualize

- Design tradeoffs:
  - Accuracy vs interpretability: Using detected parts vs ground truth
  - Computational cost vs comprehensiveness: All 2^K combinations vs sampling
  - Granularity vs clarity: Number of parts defined impacts histogram readability

- Failure signatures:
  - Very flat histograms suggest model doesn't focus on any particular part
  - Unexpected most-contributing parts may indicate model has learned spurious correlations
  - High computational time for datasets with many parts
  - Poor performance when part detector quality is low

- First 3 experiments:
  1. Run PCEvE on a simple binary classification task with known ground truth (e.g., distinguishing drawings with/without specific parts) to validate it identifies expected patterns
  2. Compare PCEvE explanations with human expert assessments on a small subset to check alignment with domain knowledge
  3. Test PCEvE with different numbers of parts (e.g., 3 vs 7 vs 18) on SCAT dataset to find optimal granularity that balances interpretability and computational cost

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided content.

## Limitations

- Computational complexity grows exponentially with the number of parts (2^K combinations), making it impractical for images with many parts
- Lacks user studies validating that part-based explanations are actually more interpretable than traditional pixel-level attribution methods
- Limited validation of framework's effectiveness across domains beyond human figure drawings and fine-grained classification

## Confidence

**High Confidence**: The core methodology of using Shapley Value to measure part contributions is mathematically sound and the computational results on benchmark datasets are reproducible. The classification accuracy claims (98.0% ±2.7% for ASD Screening, 96.5% for SCAT) are well-supported by experimental results.

**Medium Confidence**: The claim that part-based explanations are more interpretable than pixel-level attributions relies on theoretical reasoning rather than empirical user studies. The framework's applicability to domains beyond human figure drawings is demonstrated on Stanford Cars but needs further validation.

**Low Confidence**: The assertion that PCEvE provides comprehensive understanding through three-level explanations lacks validation showing that users actually benefit from all three levels versus any single level of explanation.

## Next Checks

1. **User Study Design**: Conduct a controlled experiment comparing human comprehension of PCEvE part-based explanations versus traditional pixel-level saliency maps for model decisions. Measure interpretation time, accuracy, and user preference across both novice and expert participants.

2. **Scalability Analysis**: Implement the PCEvE framework on datasets with varying numbers of parts (3, 7, 18) and measure computational time, memory usage, and explanation quality. Evaluate whether sampling strategies or approximation methods can maintain explanation quality while reducing computational cost.

3. **Cross-Domain Applicability**: Apply PCEvE to additional fine-grained classification tasks beyond Stanford Cars, such as medical imaging or satellite imagery, where part-based explanations could provide domain-relevant insights. Compare explanation quality and computational efficiency across different domain types.