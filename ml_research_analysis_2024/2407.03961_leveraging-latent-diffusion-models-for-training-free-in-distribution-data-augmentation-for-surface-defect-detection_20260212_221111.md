---
ver: rpa2
title: Leveraging Latent Diffusion Models for Training-Free In-Distribution Data Augmentation
  for Surface Defect Detection
arxiv_id: '2407.03961'
source_url: https://arxiv.org/abs/2407.03961
tags:
- images
- data
- samples
- detection
- diag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DIAG introduces a training-free diffusion-based in-distribution
  anomaly generation pipeline for surface defect detection. It uses Latent Diffusion
  Models (LDMs) guided by human-in-the-loop multimodal prompts (text + localized region)
  to synthesize realistic defect images that closely resemble real anomalies.
---

# Leveraging Latent Diffusion Models for Training-Free In-Distribution Data Augmentation for Surface Defect Detection

## Quick Facts
- arXiv ID: 2407.03961
- Source URL: https://arxiv.org/abs/2407.03961
- Authors: Federico Girella; Ziyue Liu; Franco Fummi; Francesco Setti; Marco Cristani; Luigi Capogrosso
- Reference count: 31
- One-line primary result: Training-free LDM-based in-distribution augmentation improves defect detection AP by up to 28% vs state-of-the-art.

## Executive Summary
DIAG introduces a training-free diffusion-based pipeline for in-distribution anomaly generation in surface defect detection. It leverages Latent Diffusion Models guided by expert multimodal prompts (text + localized region) to synthesize realistic defect images that closely resemble real anomalies. Unlike prior methods that produce out-of-distribution synthetic defects, DIAG generates in-distribution samples without requiring real positive data or fine-tuning. Evaluated on KSDD2, DIAG improves Average Precision by 18% over previous augmentation when real anomalies are available, and by 28% when they are missing.

## Method Summary
DIAG uses Latent Diffusion Models (LDMs) with multimodal conditioning to generate in-distribution defect images for training data augmentation. The method requires only negative images, expert-provided text descriptions of defect types, and localization masks from positive samples. These inputs are fed to SDXL inpainting to generate synthetic defect images. A ResNet-50 classifier is then trained on the original negatives plus the generated positives. The approach operates in a zero-shot manner without fine-tuning the LDM, relying on its pre-trained generalization to produce realistic defects. Evaluation uses Average Precision (AP), Precision, Recall, and Fr´echet Inception Distance (FID) to measure detection performance and image quality.

## Key Results
- DIAG achieves 18% higher AP than state-of-the-art augmentation when real anomalies are available.
- With missing anomalies, DIAG improves AP by 28% over baseline methods.
- DIAG attains the lowest FID scores, confirming superior visual fidelity to real defects.
- The method operates in a training-free, zero-shot manner without fine-tuning LDMs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DIAG's human-in-the-loop pipeline generates in-distribution defects by combining expert-provided text prompts and localized masks with latent diffusion models.
- Mechanism: Domain experts supply anomaly descriptions and spatial masks, which guide the inpainting process in SDXL. This conditioning ensures that generated defects match real-world defect appearance and location patterns.
- Core assumption: Expert knowledge accurately captures the distribution of real defects, and SDXL can translate this multimodal input into realistic images.
- Evidence anchors:
  - [abstract] "we implement a human-in-the-loop pipeline, where domain experts provide multimodal guidance to the model through text descriptions and region localization"
  - [section III-B] "domain experts will provide textual descriptions Da of what different anomalies may look like... regions where these anomalies may appear on the defect-free samples will be designated"
- Break condition: If expert prompts are vague, incomplete, or inconsistent with real defects, the generated images will drift out of distribution and fail to improve detection performance.

### Mechanism 2
- Claim: Training-free operation avoids fine-tuning costs while leveraging pre-trained LDM generalization to the target defect domain.
- Mechanism: DIAG uses SDXL, a pre-trained latent diffusion model, directly for inpainting without domain-specific fine-tuning. The zero-shot approach relies on the model's learned priors to produce high-quality defect images from prompts.
- Core assumption: SDXL's latent space contains sufficient semantic variety to represent the target defect types without additional training.
- Evidence anchors:
  - [abstract] "Remarkably, our approach operates in a zero-shot manner, avoiding time-consuming fine-tuning procedures"
  - [section III-B] "the above information will be fed to a text-conditioned LDM to perform inpainting... The LDM is then conditioned on this information to inpaint plausible anomalies"
- Break condition: If the target defect types are too domain-specific or far from SDXL's pretraining distribution, the zero-shot inpainting will produce artifacts or unrealistic defects.

### Mechanism 3
- Claim: In-distribution synthetic defects reduce false positives by narrowing the decision boundary between normal and anomalous samples.
- Mechanism: DIAG generates defects that are statistically similar to real anomalies (low FID), allowing the ResNet-50 classifier to learn tighter, more accurate boundaries. Out-of-distribution defects from traditional augmentation widen boundaries and increase false alarms.
- Core assumption: The classifier benefits more from high-fidelity, in-distribution synthetic defects than from noisy, out-of-distribution ones.
- Evidence anchors:
  - [abstract] "unlike conventional image generation techniques... generate realistic defect images that closely resemble real anomalies"
  - [section IV-C] "DIAG achieves the highest AP yet (.924), surpassing the .782 set by the previous state-of-the-art data augmentation pipeline"
  - [section IV-D] "DIAG can generate images that are very similar to the ones originally present in the dataset, resulting in the lowest FID out of all the other methodologies"
- Break condition: If the synthetic defects, despite being in-distribution, fail to cover the full diversity of real defects, the classifier will overfit to the generated subset and miss novel defect types.

## Foundational Learning

- Concept: Latent Diffusion Models (LDMs)
  - Why needed here: DIAG operates in latent space to reduce computational cost and improve generation speed while maintaining high fidelity.
  - Quick check question: What is the main advantage of using LDMs over pixel-space diffusion models in DIAG?

- Concept: Multimodal conditioning in generative models
  - Why needed here: DIAG conditions generation on both text and spatial masks to ensure defects match expert descriptions and appear in plausible locations.
  - Quick check question: How does DIAG combine textual and spatial conditioning during inpainting?

- Concept: Binary cross-entropy loss for anomaly detection
  - Why needed here: DIAG trains a ResNet-50 classifier using BCE loss on real negatives and DIAG-generated positives, framing defect detection as a binary classification problem.
  - Quick check question: Why is BCE loss appropriate for the anomaly detection task in DIAG?

## Architecture Onboarding

- Component map: Negative images (In) -> Expert prompts (Da) and masks (Ma) -> SDXL inpainting -> Generated positives (Ia) -> ResNet-50 classifier -> AP/FID evaluation
- Critical path: 1. Collect In, Da, Ma; 2. Feed to SDXL for inpainting → Ia; 3. Train ResNet-50 on In + Ia; 4. Evaluate on real test set
- Design tradeoffs:
  - Zero-shot vs fine-tuned LDM: avoids fine-tuning cost but depends on LDM's generalization
  - Prompt quality vs diversity: better prompts yield more realistic defects but may limit variety
  - Number of augmented samples: more images improve coverage but increase training time and risk repetition
- Failure signatures:
  - Low AP despite high FID: defects are realistic but don't match real defect diversity
  - High AP but many false positives: classifier overfits to synthetic defect patterns
  - Slow or unstable inpainting: SDXL struggles with certain prompt/mask combinations
- First 3 experiments:
  1. Generate Ia with fixed prompts and masks; verify FID < 1.0 vs real positives
  2. Train ResNet-50 with In + Ia (Naug=80); check AP vs MemSeg baseline
  3. Vary Naug (80, 100, 120); observe AP trend and check for overfitting signs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of generated defect images vary with different levels of noise in the textual prompts provided by domain experts?
- Basis in paper: [explicit] The paper mentions that DIAG uses textual prompts to guide the generation of defect images, and it notes the stochastic nature of Latent Diffusion Models (LDMs) can lead to variability in image quality.
- Why unresolved: The paper does not provide a detailed analysis of how variations in the clarity or specificity of textual prompts affect the quality of the generated images.
- What evidence would resolve it: Conducting experiments where the quality of generated images is systematically evaluated against different prompt variations, including noise levels, would provide insights into the robustness of the prompt-to-image quality relationship.

### Open Question 2
- Question: What is the impact of using different types of localization masks (e.g., rectangular, irregular) on the fidelity and realism of the generated defect images?
- Basis in paper: [explicit] The paper discusses the use of localization masks to guide the placement of defects in the images, but does not explore the effects of different mask shapes on the final output.
- Why unresolved: The choice of mask shape could influence how well the generated defects blend with the background and how realistic they appear, but this aspect is not investigated in the paper.
- What evidence would resolve it: Comparative studies using various mask shapes and evaluating the resulting images for realism and fidelity would clarify the influence of mask type on image quality.

### Open Question 3
- Question: How does the performance of DIAG scale with increasing complexity of the defect types described in the prompts?
- Basis in paper: [explicit] The paper highlights that DIAG uses prompts to describe defect types, but does not assess how the complexity of these descriptions affects the model's performance.
- Why unresolved: Understanding the relationship between prompt complexity and model performance is crucial for determining the practical limits of DIAG's applicability.
- What evidence would resolve it: Analyzing the model's performance across a range of prompt complexities, from simple to highly detailed defect descriptions, would reveal the scalability and limitations of DIAG in handling complex defect scenarios.

## Limitations
- Results are based on a single dataset (KSDD2), limiting generalizability to other defect detection domains.
- Quality and coverage of generated defects depend heavily on expert-provided prompts and masks, which may not be reliable in all settings.
- Zero-shot approach assumes pre-trained LDM generalization, which may fail for highly domain-specific or novel defect types.
- Computational cost and time for generating large-scale defect datasets are not discussed, potentially limiting scalability.

## Confidence
- High confidence in the core mechanism: Combining expert prompts and localized masks with latent diffusion models to generate in-distribution defects.
- Medium confidence in the zero-shot generalization: Relies on SDXL's pre-trained priors, which may not always generalize to unseen defect types.
- Medium confidence in performance gains: Results are promising but dataset-specific and may not transfer to other domains.

## Next Checks
1. **Dataset Generalization**: Evaluate DIAG on multiple surface defect datasets (e.g., MVTec AD, KolektorSDD) to assess performance consistency and robustness.
2. **Expert Prompt Reliability**: Conduct a user study to measure the impact of prompt quality and consistency on defect generation and detection performance.
3. **Computational Scalability**: Measure the time and resource requirements for generating large-scale defect datasets and training classifiers, comparing against fine-tuning baselines.