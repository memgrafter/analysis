---
ver: rpa2
title: Multi-modal Transfer Learning between Biological Foundation Models
arxiv_id: '2406.14150'
source_url: https://arxiv.org/abs/2406.14150
tags:
- expression
- multi-modal
- sequence
- protein
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IsoFormer, the first multi-modal model connecting
  DNA, RNA, and protein sequences. The approach leverages pre-trained modality-specific
  encoders and a cross-attention-based aggregation module to produce multi-modal embeddings
  for predicting RNA transcript isoform expression across human tissues.
---

# Multi-modal Transfer Learning between Biological Foundation Models

## Quick Facts
- arXiv ID: 2406.14150
- Source URL: https://arxiv.org/abs/2406.14150
- Reference count: 40
- Primary result: Multi-modal model connecting DNA, RNA, and protein sequences outperforms single-modality baselines on RNA isoform expression prediction

## Executive Summary
This paper introduces IsoFormer, the first multi-modal model connecting DNA, RNA, and protein sequences for predicting RNA transcript isoform expression across human tissues. The approach leverages pre-trained modality-specific encoders and a cross-attention-based aggregation module to produce multi-modal embeddings. IsoFormer achieves state-of-the-art performance on the GTEx dataset, demonstrating that combining multiple biological modalities improves prediction accuracy compared to single-modality approaches. The model is open-sourced, enabling further research in multi-modal biological sequence modeling.

## Method Summary
IsoFormer uses pre-trained encoders for DNA (Enformer), RNA (NT), and protein (ESM-2-150M) sequences to generate modality-specific embeddings. These embeddings are aggregated through a cross-attention module with residual connections, applied three times to enable bidirectional information flow between modalities. The aggregated embeddings are then passed through global average pooling and a linear head to predict expression levels across 30 human tissues. The model is trained using MSE loss on GTEx bulk RNA-seq data with approximately 170k transcripts, and performance is evaluated using R² and Spearman correlation metrics.

## Key Results
- IsoFormer achieves R² of 0.53 and Spearman correlation of 0.72 on RNA isoform expression prediction
- Performance improves with additional modalities, with best results using all three (DNA, RNA, protein)
- Using Enformer as DNA encoder yields better performance than NT due to task alignment with expression prediction
- Pre-trained encoders are crucial for performance, enabling both intra-modality and inter-modality transfer

## Why This Works (Mechanism)

### Mechanism 1
Multi-modal transfer learning outperforms single-modality models because embeddings capture complementary sequence features. IsoFormer uses pre-trained encoders for DNA, RNA, and protein modalities to generate modality-specific embeddings, which are aggregated via cross-attention to combine regulatory elements from DNA with stability features from RNA and functional information from protein sequences. This multi-modal embedding captures relationships that no single modality can encode alone. Core assumption: Pre-trained encoders already encode modality-specific knowledge relevant to the target task. Evidence: Performance increases from one modality to two combined, and best performance is achieved with all three modalities together. Break condition: If pre-trained encoders do not encode relevant biological features, cross-attention aggregation will not improve performance.

### Mechanism 2
Using a task-specific pre-trained DNA encoder (Enformer) improves transfer learning compared to a general DNA encoder (NT). Enformer was pre-trained on gene expression prediction tasks using chromatin accessibility and transcription factor binding data, providing embeddings aligned with expression-level prediction that are directly relevant to the RNA isoform expression task. This results in better initialization and faster convergence than using NT, which was pre-trained via self-supervised masked language modeling. Core assumption: Pre-training task alignment between encoder and target task leads to better downstream performance. Evidence: IsoFormer reaches state-of-the-art performance by using Enformer as the base DNA encoder. Break condition: If the pre-training task of Enformer does not share enough similarity with isoform expression prediction, the performance gain may not materialize.

### Mechanism 3
Cross-attention aggregation allows the model to focus on modality-specific regions relevant to the task, enabling interpretable specialization. The aggregation module applies cross-attention from each modality to the others, with residual connections preserving modality-specific information. This allows each modality's encoder to attend to the most relevant parts of other modalities, such as RNA encoder layers specializing to attend more to 3'UTR regions when predicting isoform expression, reflecting biological relevance of these regions for RNA stability and degradation. Core assumption: Cross-attention can learn biologically meaningful attention patterns that improve prediction accuracy. Evidence: Different layers specialize to capture specific features relative to isoform splicing and expression when finetuning using three modalities. Break condition: If cross-attention does not learn meaningful patterns, aggregation will not improve performance over simple concatenation.

## Foundational Learning

- **Masked Language Modeling (MLM) in biological sequence models**
  - Why needed here: MLM is the primary self-supervised pre-training method used for NT and ESM models. Understanding how MLM works is essential to grasp why these models can transfer knowledge to new tasks without labeled data.
  - Quick check question: What is the objective function used in MLM, and how does it encourage the model to learn meaningful sequence representations?

- **Cross-attention mechanism in transformer architectures**
  - Why needed here: Cross-attention is the core operation in IsoFormer's aggregation module. It allows each modality's embeddings to attend to relevant parts of other modalities, enabling information fusion.
  - Quick check question: How does cross-attention differ from self-attention, and why is it particularly useful for multi-modal integration?

- **Alternative splicing and its relationship to RNA isoform expression**
  - Why needed here: The target task involves predicting RNA isoform expression levels across tissues. Understanding alternative splicing is crucial to appreciate why DNA sequence alone is insufficient for this task.
  - Quick check question: Why can the same DNA sequence produce multiple RNA isoforms, and what biological factors determine which isoforms are expressed in a given tissue?

## Architecture Onboarding

- **Component map**: Input → DNA/RNA/Protein encoders → Cross-attention aggregation → Global average pooling → Linear head → Output
- **Critical path**: Input → Encoders → Cross-attention aggregation → Global average pooling → Linear head → Output
- **Design tradeoffs**:
  - Using pre-trained encoders vs training from scratch: Pre-trained encoders provide better initialization and faster convergence but limit flexibility in architectural changes
  - Cross-attention vs other aggregation methods: Cross-attention is more interpretable and performs better than Perceiver Resampler or linear projection, but is computationally more expensive
  - Modality completeness: The model can handle missing modalities, but performance improves with more modalities present
- **Failure signatures**:
  - Poor performance across all tissues: Likely issue with encoder initialization or aggregation module
  - Good performance on some tissues but poor on others: Dataset imbalance or tissue-specific features not captured
  - Model overfits quickly: Reduce model capacity or add regularization
  - No improvement from adding modalities: Check if encoders are properly pre-trained or if cross-attention is implemented correctly
- **First 3 experiments**:
  1. Train IsoFormer with only DNA input using Enformer encoder to establish baseline performance and verify encoder functionality
  2. Train IsoFormer with DNA + RNA modalities to test if adding RNA improves performance over DNA-only baseline
  3. Train full IsoFormer with all three modalities to evaluate maximum performance and verify cross-attention aggregation works as expected

## Open Questions the Paper Calls Out

- **Open Question 1**: Can IsoFormer's architecture be extended to incorporate additional biological sequence modalities beyond DNA, RNA, and proteins, such as epigenetic markers or metabolites?
  - Basis in paper: The paper demonstrates flexibility in using different encoders for each modality and mentions that the aggregation module can handle missing modalities.
  - Why unresolved: The current implementation focuses specifically on DNA, RNA, and protein sequences without exploring adaptation to other biological data types.
  - What evidence would resolve it: Experiments demonstrating IsoFormer's performance when integrating additional biological modalities, such as DNA methylation patterns or metabolomic data.

- **Open Question 2**: How does IsoFormer's performance scale with increasing amounts of training data, and is there a point of diminishing returns?
  - Basis in paper: The model was trained on approximately 170k unique transcripts but doesn't explore how performance changes with different dataset sizes.
  - Why unresolved: The relationship between training data volume and model performance remains unclear, which is crucial for determining practical resource requirements.
  - What evidence would resolve it: Systematic experiments varying the size of the training dataset while measuring performance metrics to reveal scaling behavior.

- **Open Question 3**: What is the minimum amount of labeled data required to effectively fine-tune IsoFormer for a new biological task?
  - Basis in paper: The paper demonstrates successful transfer learning from pre-trained encoders but doesn't investigate how much task-specific labeled data is needed for effective fine-tuning.
  - Why unresolved: While the model leverages pre-training, the amount of task-specific labeled data required for effective adaptation to new tasks is unclear.
  - What evidence would resolve it: Controlled experiments systematically reducing the amount of labeled data available during fine-tuning for various downstream tasks.

## Limitations

- **Pre-training alignment assumption**: The performance benefit from using Enformer over NT relies on the assumption that gene expression pre-training is more relevant than general MLM for the target task, which is plausible but not rigorously validated.
- **Cross-attention interpretability**: The biological interpretability of cross-attention patterns is limited, with claims about specialization based on correlation rather than causal analysis of attention weights.
- **Modality completeness**: The model assumes access to all three modalities (DNA, RNA, protein) for each transcript, which may not hold in practice due to incomplete annotations.

## Confidence

- **High Confidence**: The core technical implementation of IsoFormer (pre-trained encoders + cross-attention aggregation + expression prediction head) is well-specified and reproducible. The improvement over single-modality baselines is clearly demonstrated through ablation studies.
- **Medium Confidence**: The claims about mechanism 2 (Enformer vs NT performance difference) and mechanism 3 (cross-attention specialization) are supported by the presented results but rely on assumptions about pre-training task alignment and attention pattern interpretation that are not fully validated.
- **Low Confidence**: The claim that multi-modal aggregation captures complementary sequence features (mechanism 1) is intuitive but lacks rigorous validation, as the paper shows correlation between modality addition and performance improvement but does not analyze what specific features each modality contributes.

## Next Checks

1. **Encoder embedding analysis**: Compare the distribution and quality of embeddings from Enformer vs NT on held-out expression prediction tasks using nearest neighbor analysis, clustering, or supervised classification to quantify whether Enformer embeddings are better aligned with expression-level features.

2. **Cross-attention ablation**: Remove the cross-attention aggregation entirely and compare performance to the full model. Test whether the same performance can be achieved through simpler aggregation methods (concatenation + linear projection) with sufficient training data to validate whether cross-attention is truly necessary.

3. **Missing modality robustness**: Systematically evaluate model performance when one or more modalities are missing to test whether the model can still make reasonable predictions with partial information and identify which modalities are most critical for maintaining performance across different tissue types.