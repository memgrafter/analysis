---
ver: rpa2
title: Benchmarking and Building Zero-Shot Hindi Retrieval Model with Hindi-BEIR and
  NLLB-E5
arxiv_id: '2409.05401'
source_url: https://arxiv.org/abs/2409.05401
tags:
- dataset
- retrieval
- figure
- multilingual
- hindi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hindi-BEIR, the first comprehensive retrieval
  benchmark for Hindi, comprising 15 datasets across seven tasks. The authors address
  the challenge of evaluating retrieval models in Hindi by creating diverse datasets
  through translation, adaptation, and synthetic generation.
---

# Benchmarking and Building Zero-Shot Hindi Retrieval Model with Hindi-BEIR and NLLB-E5

## Quick Facts
- arXiv ID: 2409.05401
- Source URL: https://arxiv.org/abs/2409.05401
- Authors: Arkadeep Acharya; Rudra Murthy; Vishwajeet Kumar; Jaydeep Sen
- Reference count: 40
- Primary result: NLLB-E5 achieves 48.57 NDCG@10 on Hindi-BEIR, outperforming existing multilingual retrievers without Hindi training data

## Executive Summary
This paper introduces Hindi-BEIR, the first comprehensive benchmark for Hindi retrieval, and NLLB-E5, a novel zero-shot multilingual retrieval model. Hindi-BEIR comprises 15 datasets across seven tasks, addressing the lack of evaluation resources for Hindi retrieval. NLLB-E5 leverages knowledge distillation from a monolingual English retriever (E5) to a multilingual encoder (NLLB), enabling strong Hindi retrieval performance without Hindi training data. Experiments show NLLB-E5 outperforms existing multilingual models on Hindi-BEIR with an average NDCG@10 score of 48.57.

## Method Summary
The paper proposes NLLB-E5, a zero-shot multilingual retrieval model that aligns a multilingual encoder (NLLB) with a monolingual retriever (E5) through knowledge distillation. The model uses NLLB as the multilingual encoder and trains a projection layer to align its embeddings with those of the frozen E5 model via Mean Squared Error loss. Hindi-BEIR, the benchmark for evaluation, consists of 15 diverse datasets across seven tasks, including fact-checking, question-answering, and document retrieval. The model is trained solely on English data, eliminating the need for Hindi training data while achieving competitive retrieval performance.

## Key Results
- NLLB-E5 achieves an average NDCG@10 score of 48.57 on Hindi-BEIR, surpassing BGE-M3 (46.18), mE5-Large (44.37), and BM25 (35.65)
- The model excels in fact-checking tasks, outperforming other models by up to 5 points on average NDCG@10
- NLLB-E5 demonstrates strong zero-shot capabilities, achieving competitive performance without Hindi training data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: NLLB-E5 achieves strong zero-shot Hindi retrieval without Hindi training data by aligning a multilingual encoder with a monolingual retrieval model through knowledge distillation
- **Mechanism**: The multilingual NLLB encoder maps semantically similar sentences across languages into a shared representation space. A projection layer then aligns these embeddings to match the high-quality English embeddings produced by the monolingual E5 model, enabling retrieval capabilities for Hindi without direct Hindi supervision
- **Core assumption**: Embeddings from the multilingual encoder are sufficiently language-agnostic to be aligned for retrieval tasks using only English data
- **Evidence anchors**:
  - [abstract]: "NLLB-E5, a novel multilingual retrieval model that leverages a zero-shot approach to support Hindi without the need for Hindi training data"
  - [section]: "We use NLLB as a multilingual encoder and train the Linear projection layer and the Lora adapters by forcing its outputs to match that of the teacher model via the Mean Squared Error (MSE)"
  - [corpus]: Weak evidence - the corpus shows related work but no direct empirical validation of the language-agnostic assumption
- **Break condition**: If the multilingual encoder fails to produce truly language-agnostic representations, or if the alignment layer cannot effectively bridge the semantic gap between multilingual and monolingual retrieval embeddings

### Mechanism 2
- **Claim**: Hindi-BEIR provides a comprehensive benchmark that reveals task and domain-specific challenges for Hindi retrieval models
- **Mechanism**: By including 15 diverse datasets across 7 tasks (ranging from fact-checking to question-answering), the benchmark exposes the limitations of existing multilingual retrievers on Hindi, particularly highlighting struggles with fact-checking tasks and niche domains like finance and climate
- **Core assumption**: A diverse benchmark with multiple tasks and domains is necessary to accurately assess retrieval model performance and identify specific weaknesses
- **Evidence anchors**:
  - [abstract]: "Hindi-BEIR benchmark, comprising 15 datasets across seven distinct tasks"
  - [section]: "Based on this performance comparison, we notice that a key limitation of existing models is the need for a substantial amount of language-specific training data"
  - [corpus]: Weak evidence - corpus provides context but doesn't directly validate the benchmark's comprehensiveness
- **Break condition**: If the selected datasets do not adequately represent real-world retrieval scenarios, or if the benchmark fails to capture important Hindi-specific challenges like script differences and grammatical structure

### Mechanism 3
- **Claim**: The NLLB-E5 architecture with frozen base models and learned projection layer outperforms multilingual retrievers that require extensive multilingual training data
- **Mechanism**: By keeping the NLLB and E5 base models frozen and only training the projection layer through distillation, NLLB-E5 leverages the strengths of both components without the need for multilingual training data, achieving competitive performance through knowledge transfer
- **Core assumption**: Knowledge distillation from a high-quality monolingual retrieval model can effectively transfer retrieval capabilities to a multilingual encoder without requiring multilingual supervision
- **Evidence anchors**:
  - [abstract]: "NLLB-E5 outperforms existing multilingual retrieval models on Hindi-BEIR, achieving an average NDCG@10 score of 48.57"
  - [section]: "The frozen multilingual encoder ensures effective cross-lingual transfer, enabling generalization across all NLLB supported languages"
  - [corpus]: Weak evidence - corpus provides context but no direct validation of the frozen model approach
- **Break condition**: If the knowledge transfer through distillation is insufficient, or if the frozen models cannot adapt to the specific requirements of Hindi retrieval tasks

## Foundational Learning

- **Concept**: Multilingual embeddings and cross-lingual transfer
  - Why needed here: Understanding how multilingual encoders like NLLB can map semantically similar sentences across languages into a shared representation space is crucial for grasping how NLLB-E5 achieves zero-shot retrieval capabilities
  - Quick check question: How does a multilingual encoder like NLLB enable cross-lingual retrieval without language-specific training data?

- **Concept**: Knowledge distillation and transfer learning
  - Why needed here: The core mechanism of NLLB-E5 relies on distilling knowledge from a monolingual retrieval model (E5) to a multilingual encoder (NLLB) through a projection layer, which requires understanding how distillation works and its effectiveness
  - Quick check question: What is the role of the projection layer in NLLB-E5, and how does it facilitate knowledge transfer from E5 to NLLB?

- **Concept**: Retrieval evaluation metrics (NDCG@10)
  - Why needed here: Evaluating the performance of retrieval models on Hindi-BEIR requires understanding metrics like NDCG@10, which measures the quality of ranked retrieval results and is sensitive to ranking rather than just recall
  - Quick check question: Why is NDCG@10 preferred over simple recall metrics for evaluating retrieval models, and what does it measure?

## Architecture Onboarding

- **Component map**: Input sentence → NLLB tokenizer → Frozen NLLB encoder → Hprefix/Hpostfix token injection → Learnable projection layer W → Frozen E5 model → Average pooling → Final embedding
- **Critical path**: Input → NLLB encoding → Projection alignment → E5 processing → Embedding output
- **Design tradeoffs**:
  - Frozen base models vs. fine-tuning: Keeping NLLB and E5 frozen reduces training complexity and avoids catastrophic forgetting but may limit adaptation to Hindi-specific nuances
  - Knowledge distillation vs. direct multilingual training: Distillation eliminates the need for Hindi training data but may not capture all language-specific retrieval patterns
  - Projection layer complexity: A simple linear projection is computationally efficient but may be insufficient for complex alignment tasks
- **Failure signatures**:
  - Poor performance on Hindi-specific tasks: Indicates the multilingual encoder may not be producing truly language-agnostic representations
  - Degradation on English tasks: Suggests the distillation process may be overfitting to Hindi or losing English retrieval capabilities
  - Inconsistent performance across domains: Points to insufficient coverage in the training data or limitations in the projection layer
- **First 3 experiments**:
  1. **Baseline comparison**: Evaluate NLLB-E5 against existing multilingual retrievers (BGE-M3, mE5, LASER, LaBSE) on Hindi-BEIR to establish performance improvements
  2. **Ablation study**: Test NLLB-E5 with different NLLB encoder sizes (600M, 1.3B, 3.3B) to determine optimal model capacity
  3. **Cross-lingual validation**: Test NLLB-E5 on English BEIR subsets to ensure zero-shot retrieval capabilities are maintained across languages

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several are implied by the limitations section:
- The impact of the 508-token context length limitation on retrieval performance for long documents
- The generalizability of NLLB-E5 to other languages beyond Hindi
- The robustness of the model to Hindi queries with different script variations (Devanagari vs. Romanized)

## Limitations
- The 508-token context length limitation poses challenges for tasks requiring extended context handling, such as document retrieval
- The benchmark relies on translated and synthetic data, which may not fully capture the nuances of real Hindi retrieval scenarios
- The knowledge distillation approach assumes English-E5 embeddings can effectively guide Hindi retrieval without validating this assumption across diverse semantic spaces

## Confidence
- **High confidence** in the architectural design of NLLB-E5 and its core mechanism of knowledge distillation
- **Medium confidence** in the benchmark's comprehensiveness, given the synthetic nature of some datasets and lack of validation on truly monolingual Hindi corpora
- **Medium confidence** in the performance claims, as the evaluation is limited to Hindi-BEIR and doesn't compare against domain-specific retrievers

## Next Checks
1. Evaluate NLLB-E5 on non-BEIR Hindi datasets (e.g., CC News, Hindi Wikipedia) to verify generalization beyond the constructed benchmark
2. Conduct a controlled ablation study comparing NLLB-E5 with multilingual retrievers trained on Hindi data to quantify the effectiveness of the zero-shot approach
3. Test the model's performance on Hindi queries with significant script differences (Devanagari vs. Romanized) to assess robustness to input variations