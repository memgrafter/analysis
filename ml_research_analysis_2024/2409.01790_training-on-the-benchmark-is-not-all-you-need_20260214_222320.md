---
ver: rpa2
title: Training on the Benchmark Is Not All You Need
arxiv_id: '2409.01790'
source_url: https://arxiv.org/abs/2409.01790
tags: []
core_contribution: This paper proposes a method for detecting data leakage in large
  language models (LLMs) by leveraging the structure of multiple-choice questions.
  The approach involves shuffling the options in the questions to create derived datasets,
  then using the model's log probability distribution to identify outliers that indicate
  potential data leakage.
---

# Training on the Benchmark Is Not All You Need

## Quick Facts
- arXiv ID: 2409.01790
- Source URL: https://arxiv.org/abs/2409.01790
- Authors: Shiwen Ni; Xiangtao Kong; Chengming Li; Xiping Hu; Ruifeng Xu; Jia Zhu; Min Yang
- Reference count: 3
- Primary result: Proposed method detects data leakage in LLMs with >90% accuracy by shuffling multiple-choice options

## Executive Summary
This paper introduces a novel method for detecting data leakage in large language models by exploiting the structure of multiple-choice questions. The approach involves systematically shuffling answer options to create derived datasets, then analyzing the model's log probability distributions to identify outliers indicative of contamination. The method was evaluated on 31 open-source LLMs across four benchmark datasets, successfully identifying the Qwen family as having the highest degree of data leakage.

The work addresses a critical challenge in LLM evaluation, where performance metrics may be artificially inflated due to models having seen benchmark data during training. By creating controlled perturbations through option shuffling, the method provides a systematic way to quantify and verify data contamination, even in cases where option order has been intentionally or unintentionally modified.

## Method Summary
The proposed method leverages multiple-choice question structures by systematically shuffling answer options to create derived datasets. For each original question, the correct answer is randomly placed in different positions across multiple shuffled versions. The LLM's log probability distribution over options is then analyzed for each shuffled version. When a model has been trained on leaked data, it consistently assigns higher probabilities to the correct answer regardless of its position, creating a distinctive pattern that can be detected through statistical outlier analysis. The method evaluates consistency across multiple shuffles and uses log probability distributions to identify models showing signs of data contamination.

## Key Results
- Detection accuracy exceeds 90% in identifying data leakage in LLMs
- Qwen family models identified as having the highest degree of data leakage across tested benchmarks
- Method successfully detects leakage even when option order has been intentionally or unintentionally shuffled
- Tested on 31 open-source LLMs across four different benchmark datasets

## Why This Works (Mechanism)
The method works because LLMs trained on leaked data develop a bias toward correct answers that persists regardless of option positioning. When answer options are shuffled, contaminated models maintain higher log probabilities for the correct answer across different arrangements, while non-contaminated models show more variable probability distributions that adapt to the new option order. This consistent pattern of higher probabilities for correct answers across shuffled versions creates a detectable signal that indicates prior exposure to the benchmark data.

## Foundational Learning
- **Multiple-choice question structure**: Understanding the fixed format of MCQs is essential because the method relies on predictable answer positioning and option count. Quick check: Verify that target benchmarks have consistent option counts and clear correct answers.

- **Log probability distributions**: The method analyzes how models assign probabilities to different options, making it crucial to understand softmax outputs and log-likelihood calculations. Quick check: Confirm model outputs provide normalized probability distributions over all options.

- **Outlier detection techniques**: Statistical methods identify abnormal consistency in answer selection across shuffled versions, requiring knowledge of statistical significance testing. Quick check: Test detection method on synthetic data with known contamination levels.

- **Dataset contamination detection**: The broader context of identifying whether training data overlaps with test benchmarks, which is fundamental to fair evaluation. Quick check: Compare detection results with known contamination cases from dataset documentation.

## Architecture Onboarding
- **Component map**: Benchmark dataset -> Option shuffling module -> LLM inference engine -> Log probability collector -> Statistical outlier detector -> Contamination score output

- **Critical path**: Original question → Shuffle options → Generate multiple variants → Run through LLM → Collect log probabilities → Compute consistency metrics → Determine contamination likelihood

- **Design tradeoffs**: The method balances detection sensitivity against computational cost - more shuffles increase accuracy but require more inference passes. Alternative approaches might use fewer shuffles with more sophisticated statistical models.

- **Failure signatures**: False negatives occur when benchmarks have predictable patterns (correct answer always in same position). False positives arise from benchmarks where certain options are inherently more probable regardless of correctness.

- **3 first experiments**: 1) Test detection on synthetic datasets with controlled contamination levels, 2) Vary the number of shuffle iterations to find the minimum effective threshold, 3) Apply the method to benchmarks with known leakage documentation for validation

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalization of the detection method across different benchmark types and the quantification of performance impact when data leakage is detected. It also raises questions about the minimum number of shuffle iterations required for reliable detection and whether the method can distinguish between different types of data contamination (e.g., partial vs. complete exposure to benchmark data).

## Limitations
- The approach assumes multiple-choice benchmarks have sufficient structural diversity for meaningful perturbations through option shuffling
- Detection effectiveness may diminish for benchmarks with highly predictable answer patterns or fixed answer positions
- The method may not generalize well to benchmarks with limited option variation or binary choices
- Computational cost increases linearly with the number of shuffle iterations, potentially limiting scalability
- The method cannot determine the extent or nature of data leakage, only whether it exists

## Confidence
- **High confidence** in the core methodology - shuffling options and analyzing log probability distributions is technically sound
- **Medium confidence** in generalization across all benchmark types - demonstrated on 4 datasets but may not capture edge cases
- **Medium confidence** in the claim that detected leakage significantly impacts benchmark reliability - does not quantify impact on actual performance metrics
- **Low confidence** in the ability to distinguish between different types or degrees of data contamination

## Next Checks
1. Test the method on benchmarks with varying levels of option diversity (e.g., binary choices vs. multi-option questions) to establish detection limits across different question structures

2. Apply the approach to benchmarks where ground truth leakage is known through dataset documentation to validate false positive/negative rates

3. Conduct experiments where option shuffling patterns are systematically varied (random vs. reverse order vs. position-fixed) to determine if the method can distinguish between intentional obfuscation and genuine data contamination

4. Evaluate the computational efficiency by measuring detection accuracy at different numbers of shuffle iterations to find optimal tradeoff between accuracy and inference cost