---
ver: rpa2
title: Scalable Online Exploration via Coverability
arxiv_id: '2403.06571'
source_url: https://arxiv.org/abs/2403.06571
tags:
- policy
- algorithm
- learning
- lemma
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of efficient exploration in reinforcement
  learning, especially for high-dimensional domains requiring function approximation.
  The authors introduce "exploration objectives" as a conceptual framework to study
  exploration, proposing a new objective called L1-Coverage.
---

# Scalable Online Exploration via Coverability

## Quick Facts
- arXiv ID: 2403.06571
- Source URL: https://arxiv.org/abs/2403.06571
- Authors: Philip Amortila; Dylan J. Foster; Akshay Krishnamurthy
- Reference count: 40
- Key outcome: Introduces L1-Coverage as a new exploration objective that enables scalable, computationally efficient exploration in reinforcement learning with general function approximation

## Executive Summary
This paper introduces a novel framework for efficient exploration in reinforcement learning, particularly for high-dimensional domains requiring function approximation. The authors propose L1-Coverage as a new exploration objective that generalizes previous approaches and supports three key desiderata: intrinsic complexity control, efficient planning, and efficient exploration. The framework is grounded in the concept of coverability, which measures the intrinsic difficulty of exploring an MDP.

The core contribution is a set of computationally efficient algorithms for both planning (known MDP) and exploration (unknown MDP) that optimize L1-Coverage. These algorithms leverage existing policy optimization methods and provide the first efficient solutions for online reinforcement learning in MDPs with low coverability. Empirical results demonstrate that L1-Coverage effectively drives exploration in continuous control tasks.

## Method Summary
The paper introduces L1-Coverage as a policy optimization objective that encourages a policy ensemble to cover the state space at least as well as any individual policy, but in an average-case sense that discounts hard-to-reach states. The method relies on the L1-Coverability parameter, which bounds the sample complexity of downstream policy optimization. For planning, the authors provide Algorithm 1 that reduces L1-Coverage optimization to standard policy optimization with stochastic rewards. For exploration, CODEX.W estimates the MDP model and optimizes L1-Coverage for the estimated model, collecting data with a policy ensemble. The algorithms are evaluated on the MountainCar environment, comparing against a MaxEnt baseline.

## Key Results
- First computationally efficient model-based and model-free algorithms for online RL in MDPs with low coverability
- L1-Coverage enables sample-efficient and computationally-efficient exploration with guarantees based on the L1-Coverability parameter
- Empirically validates that L1-Coverage effectively drives off-the-shelf policy optimization algorithms to explore the state space widely
- Provides theoretical analysis showing L1-Coverability bounds the sample complexity of downstream policy optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The L1-Coverage objective drives exploration by encouraging a policy ensemble to cover the state space relative to the difficulty of reaching each state, discounting hard-to-reach regions.
- **Mechanism**: L1-Coverage uses occupancy ratios rather than absolute visitation counts. It measures how well the ensemble's coverage matches any individual policy's coverage, weighted by the state distribution induced by that policy. This average-case approach ensures that regions already well-covered by individual policies are not over-sampled.
- **Core assumption**: The relative visitation probability between policies is a meaningful proxy for coverage quality in high-dimensional spaces.
- **Evidence anchors**:
  - [abstract]: "L1-Coverage...encourages a policy ensemble to cover the state space at least as well as any individual policy, but in an average-case sense that discounts hard-to-reach states."
  - [section 3]: "L1-Coverage only considers the relative probability of visiting states (that is, the ratio of occupancies), which is fundamentally different from 'tabular' objectives such as Eq. (1) from prior work (Jin et al., 2020a; Hazan et al., 2019) and is essential to drive exploration in large state spaces."
  - [corpus]: Weak - no direct mentions of L1-Coverage, but related concepts of coverage and policy optimization appear in corpus neighbors.
- **Break condition**: If the MDP has regions that are intrinsically hard to reach but crucial for downstream tasks, the discounting mechanism may under-explore these areas.

### Mechanism 2
- **Claim**: The L1-Coverability parameter provides intrinsic complexity control by bounding the sample complexity of downstream policy optimization.
- **Mechanism**: L1-Coverability measures the optimal value of the L1-Coverage objective. When bounded, it ensures that data gathered from a policy ensemble can be used for downstream policy evaluation and optimization with general function approximation. The boundedness reflects structural properties of the MDP that make exploration tractable.
- **Core assumption**: The L1-Coverability value is bounded for MDP classes of interest (e.g., Block and Low-Rank MDPs).
- **Evidence anchors**:
  - [abstract]: "L1-Coverage is associated with a structural parameter, L1-Coverability, which reflects the intrinsic statistical difficulty of the underlying MDP, subsuming Block and Low-Rank MDPs."
  - [section 3]: "We show that the L1-Coverability value Cov_M_h,ε can be interpreted as an intrinsic structural parameter for the MDP M, and is bounded for standard MDP classes of interest."
  - [corpus]: Weak - corpus neighbors discuss coverage and policy optimization but don't mention L1-Coverability specifically.
- **Break condition**: If the MDP has complex dynamics that violate the structural assumptions underlying bounded L1-Coverability, the sample complexity guarantees no longer hold.

### Mechanism 3
- **Claim**: The relaxations of L1-Coverage (L∞-Coverability and Pushforward Coverability) enable computationally efficient planning by reducing to standard reward-driven policy optimization.
- **Mechanism**: The L∞-Coverability relaxation assumes access to a covering distribution with low L∞-concentrability, while the Pushforward Coverability relaxation uses the transition distribution directly. Both upper bound L1-Coverage and can be optimized efficiently through reduction to policy optimization with stochastic rewards.
- **Core assumption**: The relaxations provide tight enough bounds on L1-Coverage while being computationally tractable.
- **Evidence anchors**:
  - [section 4]: "The algorithm proceeds in T steps. At each step t ∈ [T], given a sequence of policies π1, ..., πt−1 computed so far, the algorithm computes a new policy πt by solving the policy optimization problem...in Line 4 (up to tolerance εopt > 0)."
  - [section 4]: "For planning in a known MDP, L1-Coverage can be optimized efficiently through reduction to (reward-driven) policy optimization, allowing for integration with off-the-shelf methods such as policy gradient (e.g., PPO) or Q-learning (e.g., DQN)."
  - [corpus]: Weak - corpus neighbors discuss planning and optimization but don't mention these specific relaxations.
- **Break condition**: If the covering distribution or transition dynamics are difficult to compute or approximate, the relaxations may become computationally intractable.

## Foundational Learning

- **Concept**: Exploration objectives as policy optimization objectives
  - Why needed here: The paper frames exploration as a policy optimization problem, which is a novel conceptual framework that unifies different exploration strategies.
  - Quick check question: How does an exploration objective differ from a standard reward-driven policy optimization objective?

- **Concept**: Occupancy measures and their ratios
  - Why needed here: L1-Coverage is defined in terms of occupancy measures and their ratios, which are fundamental to understanding how the objective encourages coverage.
  - Quick check question: What is the difference between absolute occupancy and relative occupancy, and why does L1-Coverage use the latter?

- **Concept**: Structural parameters for reinforcement learning
  - Why needed here: L1-Coverability is introduced as a structural parameter that bounds the sample complexity of reinforcement learning, similar to other parameters like Bellman rank or Bellman-Eluder dimension.
  - Quick check question: How does L1-Coverability relate to other structural parameters like Bellman rank or Bellman-Eluder dimension?

## Architecture Onboarding

- **Component map**: Algorithm 1 (planning) -> CODEX.W (exploration) -> L1-Coverage objective -> L∞-Coverability and Pushforward Coverability relaxations -> downstream policy optimization
- **Critical path**: For planning: compute covering distribution or use transition dynamics → solve policy optimization with stochastic rewards → obtain policy cover. For exploration: estimate model → optimize L1-Coverage for estimated model → collect data → repeat.
- **Design tradeoffs**: The choice between L∞-Coverability and Pushforward Coverability involves a tradeoff between approximation quality and computational efficiency. L∞-Coverability provides tighter bounds but requires a covering distribution, while Pushforward Coverability is more practical but gives looser bounds.
- **Failure signatures**: If the policy cover obtained from L1-Coverage doesn't enable downstream policy optimization, it could indicate that the L1-Coverability parameter is not actually bounded for the MDP, or that the relaxations are too loose.
- **First 3 experiments**:
  1. Implement Algorithm 1 on a simple tabular MDP to verify that it finds a policy cover with low L1-Coverage.
  2. Test the weight function estimation subroutine (EstimateWeightFunction) on a Block MDP to ensure it correctly estimates the density ratios.
  3. Apply CODEX.W to a continuous control task (e.g., MountainCar) to evaluate whether it explores more effectively than entropy-based exploration.

## Open Questions the Paper Calls Out
None explicitly called out in the provided content.

## Limitations
- Theoretical guarantees rely on bounded L1-Coverability, which may not hold for all MDPs
- Computational efficiency claims depend on the tractability of relaxations, which may not be practical for complex MDPs
- Empirical validation is limited to relatively simple environments, performance on truly high-dimensional tasks remains unclear

## Confidence
- Theoretical framework: Medium - sound but practical implications require further validation
- Empirical results: Medium - promising but limited in scope and scale

## Next Checks
1. Test L1-Coverage on a high-dimensional continuous control task (e.g., HalfCheetah) to evaluate scalability and performance in complex domains.
2. Analyze the sensitivity of L1-Coverage to hyperparameters (e.g., ensemble size, weight function estimation) to understand robustness and tuning requirements.
3. Compare L1-Coverage against other exploration methods (e.g., count-based exploration, curiosity-driven exploration) in a fair benchmark to assess relative performance.