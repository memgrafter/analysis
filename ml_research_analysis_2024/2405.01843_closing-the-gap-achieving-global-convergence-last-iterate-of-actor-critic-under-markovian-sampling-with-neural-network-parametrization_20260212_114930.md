---
ver: rpa2
title: 'Closing the Gap: Achieving Global Convergence (Last Iterate) of Actor-Critic
  under Markovian Sampling with Neural Network Parametrization'
arxiv_id: '2405.01843'
source_url: https://arxiv.org/abs/2405.01843
tags:
- function
- neural
- have
- critic
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between theoretical analysis and practical
  implementation of Actor-Critic (AC) algorithms in reinforcement learning. The authors
  propose the MMCLG criteria - Multi-layer neural network parametrization, Markovian
  sampling, Continuous state-action spaces, Last iterate performance, and Global optimality
  - to align theoretical analysis with practical AC implementations.
---

# Closing the Gap: Achieving Global Convergence (Last Iterate) of Actor-Critic under Markovian Sampling with Neural Network Parametrization

## Quick Facts
- arXiv ID: 2405.01843
- Source URL: https://arxiv.org/abs/2405.01843
- Authors: Mudit Gaur; Amrit Singh Bedi; Di Wang; Vaneet Aggarwal
- Reference count: 40
- Primary result: First last iterate global convergence result for actor-critic with neural network critic parametrization, achieving O(ε⁻³) sample complexity under continuous state-action spaces with Markovian sampling

## Executive Summary
This paper addresses the long-standing gap between theoretical analysis and practical implementation of Actor-Critic (AC) algorithms in reinforcement learning. The authors propose the MMCLG criteria - Multi-layer neural network parametrization, Markovian sampling, Continuous state-action spaces, Last iterate performance, and Global optimality - to align theoretical analysis with practical AC implementations. By establishing global convergence sample complexity bounds of O(ε⁻³), this work significantly advances the state-of-the-art by providing theoretical guarantees that closely match practical AC implementations.

## Method Summary
The authors develop a comprehensive theoretical analysis of AC algorithms that encompasses all five MMCLG criteria. The core innovation involves two main components: (1) a novel analysis combining smoothness assumptions and weak gradient domination property to obtain last iterate performance bounds, and (2) a unique decomposition of critic estimation error into approximation, estimation, sampling, and optimization components. This approach enables the first last iterate global convergence result for an actor-critic algorithm with neural network critic parametrization, achieving a sample complexity of O(ε⁻³) under continuous state-action spaces with Markovian sampling.

## Key Results
- Establishes global convergence sample complexity bounds of O(ε⁻³) for actor-critic algorithms
- Introduces novel error decomposition for critic estimation under Markovian sampling
- First theoretical guarantee achieving all five MMCLG criteria simultaneously
- Provides explicit parameter choices (α = 7/(2√μ), β' = 1/√L) that achieve the stated complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves last iterate convergence by combining smoothness assumptions with weak gradient domination property
- Mechanism: The weak gradient domination property provides a contraction-like condition that bounds the optimality gap in terms of the gradient norm, while smoothness ensures stable updates
- Core assumption: The expected return function is smooth with parameter L_J and satisfies the weak gradient domination property with parameter μ
- Evidence anchors:
  - [abstract]: "We achieve this result through our novel use of the weak gradient domination property of MDP's"
  - [section 5.2]: "From the smoothness property of the expected return we have"
  - [corpus]: No direct evidence found - this appears to be novel to this paper
- Break condition: If either the smoothness assumption fails or the weak gradient domination property doesn't hold for the MDP structure

### Mechanism 2
- Claim: The critic estimation error decomposition enables analysis under Markovian sampling and neural network parametrization
- Mechanism: The error is split into approximation, estimation, sampling, and optimization components, each with bounded contributions
- Core assumption: The critic neural network can approximate the Bellman operator within error ε_approx, and Rademacher complexity bounds apply under Markovian sampling
- Evidence anchors:
  - [abstract]: "our unique analysis of the error in critic estimation"
  - [section 6]: "The error is split into the error incurred due to the limited approximation ability... and the error incurred due to the limited sample size"
  - [corpus]: No direct evidence found - this decomposition appears novel to this work
- Break condition: If the Markov chain mixing assumption fails or the neural network class cannot achieve the required approximation quality

### Mechanism 3
- Claim: The sample complexity bound of O(ε⁻³) is achieved through careful balancing of algorithmic parameters
- Mechanism: The step sizes α = 7/(2√μ) and β' = 1/√L, combined with iteration counts t = O(ε⁻¹), J = O(log(1/ε)), n = O(ε⁻²), and L = O(ε⁻⁴), ensure all error terms scale appropriately
- Core assumption: The algorithm parameters can be chosen to simultaneously satisfy convergence conditions and achieve the stated complexity
- Evidence anchors:
  - [section 5.2]: "for t = O(ε⁻¹), J = O(log(1/ε)), n = O(ε⁻²), L = O(ε⁻⁴) we have"
  - [section 6]: "We thus obtain an upper bound on the last iterate optimality gap in terms of the average of the difference between the true gradient and our estimate"
  - [corpus]: No direct evidence found - this specific parameter tuning appears novel to this work
- Break condition: If the parameter choices cannot simultaneously satisfy all convergence conditions or if the error terms don't scale as claimed

## Foundational Learning

- Concept: Weak gradient domination property (Polyak-Łojasiewicz condition)
  - Why needed here: Provides the key contraction-like property that enables last iterate convergence analysis for non-convex problems
  - Quick check question: Can you state the condition μ||∇f(x)||^α ≤ f(x*) - f(x) and explain why it's useful for non-convex optimization?

- Concept: Rademacher complexity for Markov chains
  - Why needed here: Allows bounding the sampling error under Markovian sampling, which is crucial for practical implementations
  - Quick check question: How does Rademacher complexity differ when samples come from a Markov chain versus independent sampling?

- Concept: Neural Tangent Kernel (NTK) theory
  - Why needed here: Explains why the O(m^(-1/12)D^(7/2)) term vanishes as network width increases, connecting finite-width analysis to infinite-width limits
  - Quick check question: What does NTK theory tell us about the behavior of neural networks as the width parameter m approaches infinity?

## Architecture Onboarding

- Component map:
  Actor network (Gaussian policy πλ(s) = N(μλ₁(s), κλ₂(s))) -> Critic network Qθ(s,a) -> Target network Tπk,j-1 -> Experience replay buffer (n transitions) -> Inner loop (J iterations) -> Outer loop (K iterations)

- Critical path: Policy update → Critic estimation → Target network update → Repeat
  The most time-critical path is the inner loop critic updates, as they directly affect the policy gradient quality

- Design tradeoffs:
  - Tradeoff between inner loop iterations J and sample size n: More inner iterations reduce optimization error but increase computational cost
  - Tradeoff between buffer size and Markovian sampling: Larger buffers provide better estimates but may mix states from different policies
  - Neural network depth D vs width m: Deeper networks may require more samples but can capture more complex value functions

- Failure signatures:
  - Critic error dominates: Policy updates become unstable or converge to poor policies
  - Sample size too small: High variance in gradient estimates, poor convergence
  - Mixing assumption violated: Sample complexity bounds don't hold, may need larger buffers
  - Step sizes too large: Divergence or oscillation in policy parameters

- First 3 experiments:
  1. Implement a simple continuous control task (e.g., pendulum) with the full algorithm and verify convergence with small networks
  2. Test the effect of varying the inner loop iterations J on critic estimation quality and overall performance
  3. Compare the algorithm's performance under Markovian sampling vs. i.i.d. sampling to validate the theoretical analysis assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity bound be improved beyond O(ε⁻³) for the actor-critic algorithm with neural network parametrization?
- Basis in paper: [explicit] The authors state their result achieves a sample complexity of O(ε⁻³) and note it is the best sample complexity for global convergence in terms of ε for an actor-critic algorithm with a neural network parametrization of the critic.
- Why unresolved: While the authors claim their result is the best to date, they do not provide a lower bound on the sample complexity for this problem setting. Without a lower bound, it remains unknown whether O(ε⁻³) is optimal or if further improvements are possible.
- What evidence would resolve it: A lower bound proof showing that O(ε⁻³) is indeed the optimal sample complexity for global convergence of actor-critic algorithms with neural network parametrization, or an improved upper bound with better sample complexity.

### Open Question 2
- Question: How does the convergence rate depend on the width and depth of the neural networks used for the actor and critic?
- Basis in paper: [explicit] The authors note that their sample complexity bound includes terms of the form O(m^(-1/12)D^(7/2)) which depend on the width m and depth D of the neural networks, but they do not provide a detailed analysis of how these parameters affect convergence.
- Why unresolved: The authors acknowledge that these terms arise from the finite size of the neural networks, but they do not investigate the precise relationship between the network architecture and the convergence rate. Understanding this relationship could provide insights into how to choose network architectures for optimal performance.
- What evidence would resolve it: A detailed analysis of how the width and depth of the neural networks affect the convergence rate, including experiments that vary these parameters and measure their impact on convergence speed and sample complexity.

### Open Question 3
- Question: Can the assumptions made in the theoretical analysis be relaxed or removed without significantly impacting the convergence guarantees?
- Basis in paper: [explicit] The authors make several assumptions in their analysis, including smoothness of the policy parametrization, boundedness of the state and action spaces, and the existence of a weak gradient domination property. While these assumptions are reasonable, it is unclear if they are necessary for the convergence results to hold.
- Why unresolved: The authors do not investigate the necessity of these assumptions or explore the consequences of relaxing or removing them. Relaxing these assumptions could lead to more general convergence results that apply to a wider range of problem settings.
- What evidence would resolve it: Theoretical results showing that the convergence guarantees still hold under weaker versions of the assumptions, or experimental results demonstrating that the algorithm performs well even when the assumptions are violated.

## Limitations
- Relies on strong assumptions including smoothness of expected return and weak gradient domination property
- Assumes access to sufficiently large neural networks that can approximate the Bellman operator within bounded error
- Theoretical analysis does not quantify exact network architecture requirements for achieving guarantees

## Confidence
- High confidence in the mathematical derivation of the sample complexity bound O(ε⁻³) given the stated assumptions
- Medium confidence in the practical applicability, as the smoothness and weak gradient domination assumptions require empirical validation
- Low confidence in the exact network architecture requirements, as the neural tangent kernel analysis provides asymptotic behavior but not concrete width specifications

## Next Checks
1. **Empirical verification of assumptions**: Test the smoothness and weak gradient domination properties on standard continuous control benchmarks to verify these assumptions hold in practice and characterize their strength across different MDPs.

2. **Scalability analysis**: Implement the algorithm with varying network widths and depths to empirically validate the O(m^(-1/12)D^(7/2)) term behavior and determine practical network size requirements for achieving the theoretical guarantees.

3. **Robustness to assumption violations**: Systematically relax the mixing assumption for the Markov chain and measure the impact on convergence rates to understand the algorithm's robustness to realistic sampling scenarios.