---
ver: rpa2
title: Towards Efficient Pareto Set Approximation via Mixture of Experts Based Model
  Fusion
arxiv_id: '2406.09770'
source_url: https://arxiv.org/abs/2406.09770
tags:
- pareto
- learning
- task
- tasks
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning the Pareto set for
  large deep neural networks, which is computationally expensive due to the complexity
  of the loss landscape and the need to train and evaluate multiple models. The proposed
  method uses a mixture of experts (MoE) based model fusion approach, which ensembles
  the weights of specialized single-task models to approximate the entire Pareto set.
---

# Towards Efficient Pareto Set Approximation via Mixture of Experts Based Model Fusion

## Quick Facts
- arXiv ID: 2406.09770
- Source URL: https://arxiv.org/abs/2406.09770
- Reference count: 40
- Key outcome: Proposes an MoE-based model fusion approach to approximate Pareto sets for large deep neural networks, achieving scalability and reduced computational cost

## Executive Summary
This paper addresses the computational challenge of learning Pareto sets for large deep neural networks in multi-objective optimization. The proposed method uses a mixture of experts (MoE) based model fusion approach that ensembles weights of specialized single-task models to approximate the entire Pareto set. The key innovation is a weight-ensembling MoE structure where the MoE module can be unloaded after learning routers, resulting in no additional computational cost during inference. Experimental results on vision and language tasks using large-scale models like CLIP-ViT and GPT-2 demonstrate the effectiveness of this approach in approximating Pareto fronts while being scalable to both the number of objectives and model size.

## Method Summary
The method involves fine-tuning pre-trained models on individual downstream tasks to obtain task-specific models, then mixing these models into a single up-scaled MoE model using task arithmetic and an up-scaling strategy (either MLP-only or MLP+Attention). The MoE routers are fine-tuned using sampled preference vectors from the simplex, and after training, the MoE module is unloaded to avoid additional inference cost. The approach uses only hundreds of trainable parameters in the MoE routers, resulting in lower memory usage compared to existing methods while maintaining scalability to large models and multiple objectives.

## Key Results
- Achieves Pareto set approximation for large-scale models (CLIP-ViT, GPT-2) across vision and language tasks
- Unloads MoE module after router learning, eliminating additional inference computational cost
- Demonstrates scalability to both the number of objectives and model size with lower memory usage than linear scalarization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoE-based model fusion can approximate the entire Pareto front by ensembling specialized single-task models
- Mechanism: PWE MoE module captures trade-offs between objectives by learning routing weights that depend on user preference vectors, enabling approximation in a subspace of the parameter space
- Core assumption: Pareto front is often a convex set and can be approximated by interpolating between task-specific models in parameter space
- Evidence anchors: [abstract] MoE module effectively captures trade-offs; [section] searches subspace of parameter space; [corpus] weak evidence for MoE effectiveness in Pareto approximation

### Mechanism 2
- Claim: Unloading MoE module after router learning eliminates additional computational cost during inference
- Mechanism: Once routers are learned and preference vector is set, MoE module can be replaced with a fixed function, avoiding extra computation at inference time
- Core assumption: Routing weights depend only on preference vector, not input data, allowing MoE to be "unloaded"
- Evidence anchors: [abstract] MoE module can be unloaded after setting preference vector; [section] routing weight vector depends only on preference vector; [corpus] weak evidence for MoE unloading

### Mechanism 3
- Claim: Proposed method scales efficiently to both number of objectives and model size
- Mechanism: Number of trainable parameters in MoE module scales linearly with number of tasks and is independent of model width, reducing memory usage compared to linear scalarization
- Core assumption: MoE module's trainable parameters depend only on number of tasks, not model width
- Evidence anchors: [abstract] lower memory usage compared to linear scalarization; [section] trainable parameters depend on number of tasks; [corpus] weak evidence for memory efficiency

## Foundational Learning

- Concept: Pareto optimality and Pareto dominance
  - Why needed here: Method aims to approximate Pareto set, requiring understanding of what makes solution Pareto optimal and how to compare solutions
  - Quick check question: Given two solutions with objective values (3,5) and (4,4), which one dominates the other, or are they non-dominated?

- Concept: Multi-objective optimization problem (MOOP) formulation
  - Why needed here: Paper addresses MOOPs for large deep neural networks, so understanding problem setup is crucial
  - Quick check question: How would you formulate a MOOP with two objectives (minimize loss1 and minimize loss2) for a neural network with parameters θ?

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Proposed method uses MoE-based model fusion approach, so understanding how MoE works is essential
  - Quick check question: In an MoE layer with N experts, how is output typically computed given input x and gating weights g?

## Architecture Onboarding

- Component map: Pre-trained model (θ₀) -> Task-specific models (θ₁, θ₂, ..., θT) -> PWE MoE module (routing network R, parameter decoder D, realization function F) -> Task arithmetic merged parameters (ψ*)

- Critical path: 1) Acquire task-specific models fine-tuned from common pre-trained model; 2) Upscale model by incorporating MoE module; 3) Fine-tune MoE routers using preference vectors sampled from simplex; 4) Unload MoE module and use merged model for inference

- Design tradeoffs:
  - MLP-only vs. Attention+MLP upscaling: MLP-only is less parameter-intensive but may provide less accurate Pareto approximation
  - Linear scalarization vs. EPO for router fine-tuning: LS is simpler but may not capture exact Pareto optimality
  - Number of experts in MoE: More experts can capture finer trade-offs but increase computational cost

- Failure signatures:
  - Poor approximation of Pareto front: May indicate insufficient diversity in task vectors or suboptimal upscaling strategy
  - High memory usage: Could suggest too many experts or inefficient implementation of MoE module
  - Unstable router training: Might be due to inappropriate learning rate or preference vector sampling strategy

- First 3 experiments:
  1. Two-task image classification with CLIP-ViT-B/32 using MLP-only upscaling and LS router fine-tuning
  2. Three-task image classification with CLIP-ViT-B/32 using Attention+MLP upscaling and EPO router fine-tuning
  3. Two-task natural language processing with GPT-2 using MLP-only upscaling and LS router fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed Pareto weight-ensembling MoE modules compare to existing methods like Pareto HyperNetwork and EPO in terms of scalability to larger models with more parameters?
- Basis in paper: [explicit] Paper mentions Pareto HyperNetwork is effective for small-scale models but becomes impractical for larger models due to large number of parameters required; compares proposed method with EPO and other Pareto optimal learning baselines in terms of computational burden
- Why unresolved: While paper provides some comparisons with EPO and mentions limitations of Pareto HyperNetwork, it does not provide direct comparison of proposed method with Pareto HyperNetwork in terms of scalability to larger models
- What evidence would resolve it: Experimental results comparing proposed method with Pareto HyperNetwork on models with wide range of parameter sizes, demonstrating scalability of proposed method to larger models

### Open Question 2
- Question: How does choice of up-scaling strategy (only MLPs vs. MLPs + Attention) affect quality of Pareto set approximation and computational demands?
- Basis in paper: [explicit] Paper discusses two up-scaling strategies: one that only uses MLP modules and another that incorporates Attention blocks; mentions more fine-grained up-scaling strategy leads to better approximation but increases number of trainable parameters in routers
- Why unresolved: While paper provides some insights into differences between two up-scaling strategies, it does not provide comprehensive analysis of how choice of up-scaling strategy affects quality of Pareto set approximation and computational demands
- What evidence would resolve it: Experimental results comparing performance of proposed method using different up-scaling strategies on various tasks, along with detailed analysis of computational demands for each strategy

### Open Question 3
- Question: How does proposed method perform on other types of models beyond Transformer-based models, such as RNNs and CNNs?
- Basis in paper: [inferred] Paper mentions experiments are limited to Transformer-based models and suggests further exploration can be conducted on other types of models in future work
- Why unresolved: Paper does not provide any experimental results or analysis of proposed method's performance on models other than Transformer-based models
- What evidence would resolve it: Experimental results evaluating proposed method on diverse set of models, including RNNs and CNNs, demonstrating its effectiveness and scalability across different model architectures

## Limitations

- Core assumption that Pareto front can be approximated by convex combinations of task vectors may not hold for highly non-convex loss landscapes
- Evidence for MoE effectiveness in Pareto approximation is weak, as most related work focuses on MoE for scaling rather than multi-objective optimization
- Memory efficiency claims compared to linear scalarization lack strong empirical support and need more thorough investigation

## Confidence

- High confidence: Mechanism of using MoE to ensemble task-specific models and claim that unloading MoE module reduces inference cost are well-supported by paper descriptions and technically sound
- Medium confidence: Scalability claims regarding both number of objectives and model size are supported by theoretical arguments about parameter scaling, but practical validation across diverse scenarios would strengthen these claims
- Low confidence: Memory efficiency claims compared to linear scalarization and single Pareto optimal solution methods lack strong empirical support in paper, and relationship between number of experts and approximation quality needs more thorough investigation

## Next Checks

1. Test method's performance on a multi-task problem with a known non-convex Pareto front to validate whether convex combination assumption holds in challenging scenarios

2. Conduct systematic study varying number of experts in MoE module to establish relationship between number of experts and Pareto front approximation quality, including analysis of diminishing returns

3. Compare memory usage of proposed method against linear scalarization across different model scales and numbers of objectives to empirically verify claimed memory efficiency advantages