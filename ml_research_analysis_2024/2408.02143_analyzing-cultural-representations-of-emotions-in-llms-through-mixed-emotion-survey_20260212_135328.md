---
ver: rpa2
title: Analyzing Cultural Representations of Emotions in LLMs through Mixed Emotion
  Survey
arxiv_id: '2408.02143'
source_url: https://arxiv.org/abs/2408.02143
tags:
- llms
- responses
- language
- japanese
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed cultural representations of emotions in Large
  Language Models (LLMs) by replicating human experiments on mixed emotions across
  different cultures. The researchers administered a survey on mixed emotion situations
  in English and Japanese to five LLMs and compared their responses with human data
  from Japanese and American participants.
---

# Analyzing Cultural Representations of Emotions in LLMs through Mixed Emotion Survey

## Quick Facts
- arXiv ID: 2408.02143
- Source URL: https://arxiv.org/abs/2408.02143
- Authors: Shiran Dudy; Ibrahim Said Ahmad; Ryoko Kitajima; Agata Lapedriza
- Reference count: 26
- One-line primary result: LLM responses show limited alignment with human cultural emotional patterns, with language effects stronger than cultural context cues

## Executive Summary
This study investigates how Large Language Models represent cultural differences in emotional responses by replicating a human mixed emotions survey across multiple languages. The researchers administered a survey based on Miyamoto et al. (2010) to five LLMs in English, Japanese, Chinese, Korean, Vietnamese, French, German, and Spanish. They compared LLM responses with human data from Japanese and American participants, exploring how language and cultural context information affect emotional representations. The findings reveal that while LLMs demonstrate some cultural sensitivity, they do not fully capture the nuanced cultural differences observed in human subjects, with language having a stronger effect on responses than explicit cultural context cues.

## Method Summary
The study employed a survey replication methodology, translating 13 mixed emotion situations from Miyamoto et al. (2010) into eight languages and administering them to five LLMs using LangChain. Each model received 80 samples per language to ensure statistical stability. The survey was split into three parts due to model input limitations. Responses were analyzed using t-tests and correlation matrices to compare LLM outputs with human experimental findings, evaluate the effect of language versus speaker origin information, and assess cross-cultural similarity between East Asian and Western European languages.

## Key Results
- LLM responses showed limited alignment with human experimental findings on cultural emotional patterns
- Language had a stronger effect on responses than explicit speaker origin information
- LLM responses were more similar across East Asian languages compared to Western European languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs encode cultural emotional norms through language exposure, but the encoding is shallow and non-transferable across languages.
- Mechanism: The model's learned representations are distributed across transformer layers and depend on co-occurrence patterns in training data. Emotional terms and their valence are shaped by the frequency and context in which they appear in multilingual corpora.
- Core assumption: Training data reflects authentic cultural emotional norms proportionally to its volume and diversity.
- Evidence anchors:
  - [abstract] "language having a stronger effect on responses than speaker origin information"
  - [section] "LLMs responses were found more similar for East Asian languages than Western European languages"
  - [corpus] "Average neighbor FMR=0.47" — moderate relatedness suggests limited thematic overlap in existing literature
- Break condition: If training data is biased toward dominant cultures (e.g., English-dominated corpora), the model's cultural representations become skewed and fail to generalize across languages.

### Mechanism 2
- Claim: Stability of LLM responses depends on sample size; insufficient samples yield inconsistent cultural patterns.
- Mechanism: Random sampling variance decreases with larger n, stabilizing mean and variance estimates. In this study, n=80 was empirically determined to achieve stable distributions.
- Core assumption: LLMs generate responses that are consistent within a stable sample range.
- Evidence anchors:
  - [section] "we found n = 80 to provide stable distributions"
  - [section] "Empirically, we observed that ns resulting in median p-values above 0.5 provided stable distributions"
  - [corpus] "no strong citations yet" — suggests early-stage findings needing replication
- Break condition: If the model's response generation is inherently non-deterministic beyond sample size effects (e.g., temperature-based sampling), stability thresholds may vary by task or model.

### Mechanism 3
- Claim: Cultural alignment in LLM responses is mediated by the presence of culturally specific vocabulary and phraseology in prompts.
- Mechanism: When prompts contain cultural markers (e.g., "as a Japanese participant"), the model's attention shifts to culturally relevant training instances, producing more aligned responses.
- Core assumption: The model's attention mechanism can be steered by explicit cultural cues in the prompt.
- Evidence anchors:
  - [abstract] "language having a stronger effect on responses than information on participants origin"
  - [section] "we find that language has a stronger effect on responses, than the participant's origin"
  - [corpus] Weak anchor — no direct citations supporting prompt steering
- Break condition: If the model lacks sufficient culturally specific training data, even explicit cues fail to produce aligned responses.

## Foundational Learning

- Concept: Mixed emotions as a cultural construct
  - Why needed here: The study's core premise relies on understanding how different cultures perceive and express mixed emotions.
  - Quick check question: Why might East Asian cultures exhibit more mixed emotions than Western cultures in response to positive events?

- Concept: Cross-cultural survey replication in LLMs
  - Why needed here: The methodology depends on translating and adapting human psychological experiments for machine evaluation.
  - Quick check question: What are the key challenges in ensuring that LLM responses to a survey are comparable to human responses?

- Concept: Statistical stability in empirical LLM studies
  - Why needed here: The determination of sample size (n=80) is critical for the validity of the results.
  - Quick check question: How does increasing sample size affect the reliability of distributional comparisons in LLM evaluations?

## Architecture Onboarding

- Component map:
  - Survey translation pipeline → Prompt construction → LLM inference → Response aggregation → Statistical analysis
  - Data storage: Raw responses, metadata (language, origin context)
  - Analysis tools: T-tests, correlation matrices, visualization scripts

- Critical path:
  1. Translate survey into target languages
  2. Construct prompts with language/origin context variations
  3. Run survey n=80 times per condition per model
  4. Aggregate responses and compute statistics
  5. Compare across languages and conditions

- Design tradeoffs:
  - Language coverage vs. translation quality
  - Sample size vs. computational cost
  - Statistical rigor vs. interpretability of results
  - Cultural nuance vs. survey simplicity

- Failure signatures:
  - Non-significant results across all conditions → possible model limitations or flawed survey design
  - High variance within samples → insufficient n or unstable model behavior
  - Uniform responses across languages → lack of cultural sensitivity or prompt ineffectiveness

- First 3 experiments:
  1. Run the original Miyamoto survey in English and Japanese with n=10 samples per model to check basic feasibility
  2. Vary n from 10 to 100 in increments of 10 to empirically determine stability threshold
  3. Add cultural context prompts ("as a Japanese participant") to English surveys and compare response distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do current LLMs capture cultural nuances in emotional responses across diverse global cultures beyond East Asian and Western contexts?
- Basis in paper: [explicit] The paper focuses on comparing East Asian and Western cultures and acknowledges limitations in generalizability to other cultures
- Why unresolved: The study only examined a limited set of languages (4 East Asian and 4 Western European) and explicitly notes that many cultures remain under-researched
- What evidence would resolve it: Replication of this methodology with surveys designed for specific cultural norms of additional regions, including African, Middle Eastern, and Latin American cultures

### Open Question 2
- Question: What is the relationship between training data composition and cultural representation in LLM emotional responses?
- Basis in paper: [explicit] The paper mentions that English comprises 55% of internet content and discusses potential training data biases, but doesn't directly analyze training data composition
- Why unresolved: While the paper observes cultural patterns in LLM responses, it doesn't establish a causal link between specific training data characteristics and the observed cultural representations
- What evidence would resolve it: Detailed analysis of LLM training corpora showing distribution of cultural content, followed by correlation studies between training data composition and cultural response patterns

### Open Question 3
- Question: Do LLMs exhibit consistent cultural biases across different types of emotional tasks, or are these biases task-specific?
- Basis in paper: [inferred] The paper focuses on mixed emotions but references broader literature on cultural emotional differences and notes inconsistent patterns in LLM responses
- Why unresolved: The study only examined mixed emotions through one specific survey protocol, leaving open whether observed cultural limitations extend to other emotional tasks
- What evidence would resolve it: Replication of multiple cultural emotion studies (e.g., high vs. low arousal emotions, display rules, emotion regulation) across the same LLMs to identify consistent vs. variable cultural biases

## Limitations
- Incomplete specification of prompt engineering methodology, particularly regarding survey splitting and exact phrasing
- Reliance on a single human baseline study (Miyamoto et al., 2010) limiting generalizability across cultural contexts
- Empirical determination of sample size (n=80) lacking theoretical justification for universal applicability

## Confidence

- **High Confidence**: The finding that language has a stronger effect on LLM responses than speaker origin information is well-supported by the statistical analysis and consistent across multiple models and languages.
- **Medium Confidence**: The observation that East Asian languages yield more similar responses compared to Western European languages is plausible given the moderate neighbor FMR (0.47) and the study's methodological rigor, but requires validation with additional human baseline studies.
- **Low Confidence**: The claim that LLMs demonstrate "some cultural sensitivity" but fail to fully capture cultural nuances is tentative, as it depends on the representativeness of the Miyamoto et al. (2010) dataset and the assumption that LLM responses should align with human experimental findings.

## Next Checks
1. Conduct an ablation study varying the prompt phrasing, length, and cultural context cues to determine their impact on LLM response distributions, isolating the effect of prompt design from the model's inherent cultural representations.

2. Replicate the study using multiple human baseline datasets from different cultural contexts (e.g., European, African, Latin American) to assess whether the Miyamoto et al. (2010) findings are representative or culturally specific.

3. Test the same survey methodology across a broader range of LLM architectures (e.g., Claude, LLaMA, BLOOM) and sizes to determine whether cultural sensitivity varies by model family or training corpus composition.