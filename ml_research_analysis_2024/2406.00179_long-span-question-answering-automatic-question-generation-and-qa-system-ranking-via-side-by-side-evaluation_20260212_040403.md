---
ver: rpa2
title: 'Long-Span Question-Answering: Automatic Question Generation and QA-System
  Ranking via Side-by-Side Evaluation'
arxiv_id: '2406.00179'
source_url: https://arxiv.org/abs/2406.00179
tags:
- answer
- question
- context
- gemini
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a fully automated framework for creating and
  evaluating long-context reading comprehension benchmarks using large language models.
  The core idea is to use LLMs to generate challenging questions from entire books
  and then use a long-context LLM as an automatic evaluator.
---

# Long-Span Question-Answering: Automatic Question Generation and QA-System Ranking via Side-by-Side Evaluation

## Quick Facts
- arXiv ID: 2406.00179
- Source URL: https://arxiv.org/abs/2406.00179
- Authors: Bernd Bohnet; Kevin Swersky; Rosanne Liu; Pranjal Awasthi; Azade Nova; Javier Snaider; Hanie Sedghi; Aaron T Parisi; Michael Collins; Angeliki Lazaridou; Orhan Firat; Noah Fiedel
- Reference count: 40
- One-line primary result: LLM-based framework for automated long-span question generation, answering, and evaluation shows full context outperforms retrieval by 4.5% on Les Misérables

## Executive Summary
This paper presents a fully automated framework for creating and evaluating long-context reading comprehension benchmarks using large language models. The core idea is to use LLMs to generate challenging questions from entire books and then use a long-context LLM as an automatic evaluator. The evaluation employs both absolute metrics (AutoAIS) and relative pairwise comparisons with the Bradley-Terry model. Results show that providing the full book context significantly outperforms retrieval-based approaches, with the full-context Gemini 1.5 Pro achieving 92.2% accuracy on Les Misérables versus 87.7% without context.

## Method Summary
The framework combines three key innovations: (1) automated question generation using entity extraction and coreference resolution to create questions requiring long-span reasoning across entire books, (2) a two-tiered evaluation system using both absolute Attributable-to-Identified-Sources (AIS) scoring and relative pairwise comparison with Bradley-Terry ranking, and (3) long-context LLMs serving as both answerers and evaluators. The system processes entire books to extract entities, generates questions that require reasoning across long text spans, answers them using various models with or without context, and evaluates answers using automatic scoring that shows excellent agreement with human judgment while being capable of detecting errors in existing datasets.

## Key Results
- Full-context models significantly outperform retrieval-based approaches (92.2% vs 87.7% accuracy on Les Misérables)
- Relative pairwise evaluation with Bradley-Terry ranking provides more discriminative assessment than absolute scoring
- Automatic evaluator shows excellent agreement with human judgment (correlations 0.74-0.91)
- The system can detect errors in existing datasets through self-evaluation capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long-context LLMs can generate challenging QA pairs by synthesizing information across entire books
- Mechanism: The system extracts entities via coreference resolution, then prompts the LLM to generate questions that require reasoning across long spans of text without directly naming the entities. This forces the model to resolve references and synthesize information.
- Core assumption: LLMs can understand entity relationships and narrative structure when given the full book context
- Evidence anchors:
  - [abstract] "Our approach specifically focuses on book-based QA, a domain that presents a unique opportunity to test the limits of long-context reasoning"
  - [section 2] "We prompt the model to generate questions that require reasoning and synthesis over large spans of input text"
  - [corpus] Weak - related papers focus on dataset generation but not specifically on entity-based long-span question generation
- Break condition: If the LLM lacks sufficient understanding of narrative structure or entity relationships, it cannot generate questions requiring long-span reasoning

### Mechanism 2
- Claim: Relative pairwise evaluation with Bradley-Terry ranking provides more discriminative assessment than absolute scoring
- Mechanism: The system compares pairs of answers and uses the Bradley-Terry model to rank models based on pairwise preferences. This captures nuanced quality differences that absolute scoring misses.
- Core assumption: Side-by-side comparison allows the evaluator to detect subtle quality differences that absolute scoring cannot
- Evidence anchors:
  - [abstract] "We find that a relative approach, comparing answers between models in a pairwise fashion and ranking with a Bradley-Terry model, provides a more consistent and differentiating scoring mechanism"
  - [section 3.2] "We find that this produces a much more informative and discriminative ranking across models"
  - [corpus] Weak - related papers discuss QA evaluation but not specifically pairwise comparison methods
- Break condition: If pairwise comparisons become too numerous or if the evaluator becomes biased toward certain answer styles

### Mechanism 3
- Claim: LLMs can self-evaluate their own answers with high fidelity when given full context
- Mechanism: The system uses a long-context LLM to evaluate answer correctness by checking entailment against the source text, detecting errors in existing datasets
- Core assumption: LLMs can accurately assess their own factual correctness when provided with the full source context
- Evidence anchors:
  - [abstract] "We also show that LLMs from different model families produce moderate agreement in their ratings"
  - [section 5.2] "the long-context model was adept enough to find errors in the dataset that originated in its construction methodology"
  - [corpus] Weak - related papers discuss automatic evaluation but not self-evaluation capabilities
- Break condition: If the LLM's parametric knowledge conflicts with the provided context, or if it hallucinates during evaluation

## Foundational Learning

- Concept: Coreference resolution and entity extraction
  - Why needed here: To identify important entities and their relationships across the full text for question generation
  - Quick check question: Can you explain how coreference resolution chains help identify all mentions of a character across a book?

- Concept: Bradley-Terry ranking model
  - Why needed here: To convert pairwise comparisons into a total ordering of model performance
  - Quick check question: How does the Bradley-Terry model convert win/loss records into strength scores?

- Concept: Attributable-to-Identified-Sources (AIS) evaluation
  - Why needed here: To assess whether answers are factually grounded in the source text
  - Quick check question: What's the difference between absolute AIS scoring and relative pairwise evaluation?

## Architecture Onboarding

- Component map:
  Entity Extraction -> Question Generation -> Answering -> Evaluation (Absolute + Relative)

- Critical path:
  1. Entity extraction and coreference resolution
  2. Question generation with entity-based prompts
  3. Answer generation across multiple models/contexts
  4. Pairwise evaluation and Bradley-Terry ranking
  5. Absolute AIS evaluation for factual grounding

- Design tradeoffs:
  - Full context vs retrieval-based: Full context eliminates retrieval errors but requires long-context capability
  - Pairwise vs absolute: Pairwise is more discriminative but requires more LLM calls
  - Entity-based vs random questions: Entity-based ensures reasoning across spans but may miss other question types

- Failure signatures:
  - Low agreement between raters suggests bias or insufficient context
  - High accuracy with no discriminative power indicates questions are too easy
  - Inconsistent pairwise results suggest evaluator instability

- First 3 experiments:
  1. Run entity extraction on a sample book and verify coreference chains
  2. Generate 10 questions using the entity-based prompt and check for long-span reasoning
  3. Perform 5 pairwise comparisons between two models and verify Bradley-Terry ranking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed LLM-based question generation approach be adapted to create questions targeting specific reasoning capabilities (e.g., counterfactual reasoning, causal inference, or multi-hop inference) beyond what's demonstrated in the paper?
- Basis in paper: [inferred] The paper demonstrates the approach on long-span questions requiring reasoning over character arcs and themes, but doesn't explore whether it can be adapted for other reasoning types.
- Why unresolved: The paper only shows the approach working for long-context reasoning in books, not whether it can be generalized to other reasoning paradigms.
- What evidence would resolve it: Experiments showing the approach generating high-quality questions for counterfactual reasoning, causal inference, or multi-hop inference tasks on appropriate datasets, with human evaluation of question quality.

### Open Question 2
- Question: How does the performance of the automatic evaluator (AutoAIS) degrade as the complexity of the question-answer pairs increases, particularly for questions requiring multi-step reasoning or synthesis of information from distant parts of the text?
- Basis in paper: [inferred] The paper shows good performance on NarrativeQA but doesn't systematically analyze performance degradation across different complexity levels.
- Why unresolved: The paper demonstrates effectiveness but doesn't provide a detailed analysis of how well the evaluator handles increasingly complex reasoning tasks.
- What evidence would resolve it: A comprehensive evaluation showing AutoAIS performance across a spectrum of question complexities, from simple factual recall to multi-step reasoning, with error analysis identifying specific failure modes.

### Open Question 3
- Question: Can the side-by-side evaluation approach be extended to evaluate open-ended questions with multiple valid answers, or is it limited to questions with single correct answers?
- Basis in paper: [explicit] The paper demonstrates the approach on NarrativeQA where questions have ground truth answers, but doesn't explore its applicability to open-ended questions.
- Why unresolved: The paper shows success with multiple-choice or single-answer questions but doesn't address whether the approach works for questions with multiple valid responses.
- What evidence would resolve it: Experiments showing the side-by-side evaluation approach working effectively on open-ended questions where multiple answers could be considered correct, with analysis of how it handles answer diversity.

## Limitations

- **Scope and Generalizability**: The framework's effectiveness is primarily demonstrated on three classic novels, representing a limited domain of literary fiction. Performance on other genres, scientific literature, technical documentation, or modern texts remains untested.
- **Evaluator Reliability**: While showing excellent agreement with human judgment, the automatic evaluator's consistency across different model families and contexts is not fully explored, with potential for evaluator bias.
- **Question Quality and Diversity**: The entity-based question generation approach may introduce bias toward certain question types while missing others such as causal reasoning or multi-hop questions, and relies on coreference resolution which may fail with ambiguous references.

## Confidence

**High Confidence**: The observation that full-context models significantly outperform retrieval-based approaches is well-supported by experimental results showing consistent performance improvements (e.g., 92.2% vs 87.7% accuracy on Les Misérables). The pairwise evaluation methodology and Bradley-Terry ranking model are established techniques with clear theoretical foundations.

**Medium Confidence**: The claim that LLMs can generate challenging long-span questions requiring reasoning across entire books is supported by the framework's design but requires further validation on diverse text types and question categories. The assertion that the automatic evaluator can detect errors in existing datasets is demonstrated but may be context-dependent.

**Low Confidence**: The framework's ability to generalize to non-literary domains, handle multilingual texts, and maintain performance with texts containing specialized terminology or technical content is not adequately tested and represents a significant uncertainty.

## Next Checks

1. **Domain Generalization Test**: Apply the framework to non-literary domains including scientific papers, technical documentation, and news articles to evaluate performance across different text types and assess the framework's generalizability beyond classic literature.

2. **Cross-Evaluator Validation**: Conduct extensive validation using multiple different LLM evaluators (including models from different families and capabilities) to assess the consistency and reliability of the pairwise evaluation methodology and identify potential evaluator biases.

3. **Question Type Diversity Analysis**: Systematically analyze the distribution of question types generated by the framework to ensure coverage beyond entity-based questions, including causal reasoning, inference, and multi-hop questions, and evaluate whether the framework can generate and assess these more complex question types effectively.