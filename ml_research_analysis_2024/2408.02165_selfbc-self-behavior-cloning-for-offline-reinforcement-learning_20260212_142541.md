---
ver: rpa2
title: 'SelfBC: Self Behavior Cloning for Offline Reinforcement Learning'
arxiv_id: '2408.02165'
source_url: https://arxiv.org/abs/2408.02165
tags:
- policy
- selfbc
- reference
- uni00000048
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of overly conservative policies
  in offline reinforcement learning caused by static policy constraints. The authors
  propose Self Behavior Cloning (SelfBC), a dynamic policy constraint method that
  progressively updates the reference policy towards the learned policy during training
  using an exponential moving average.
---

# SelfBC: Self Behavior Cloning for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2408.02165
- Source URL: https://arxiv.org/abs/2408.02165
- Reference count: 40
- This paper addresses the problem of overly conservative policies in offline reinforcement learning caused by static policy constraints.

## Executive Summary
This paper addresses the problem of overly conservative policies in offline reinforcement learning caused by static policy constraints. The authors propose Self Behavior Cloning (SelfBC), a dynamic policy constraint method that progressively updates the reference policy towards the learned policy during training using an exponential moving average. By integrating this self-constraint mechanism into off-policy algorithms like TD3, their method learns non-conservative policies while avoiding policy collapse. Theoretical analysis shows that the proposed approach results in a nearly monotonically improved reference policy.

## Method Summary
SelfBC replaces the static behavior policy used in traditional behavior cloning constraints with a dynamic reference policy that is the exponential moving average (EMA) of previously learned policies. The method consists of a pretraining phase using TD3+EBC to initialize the policy and Q-networks, followed by iterative training where the reference policy is gradually updated via EMA. The policy is trained to maximize Q-value while staying close to the reference policy, and the reference policy slowly tracks the learned policy. An ensemble variant (TD3+ESBC) averages reference actions across multiple trainers to reduce noise.

## Key Results
- SelfBC achieves state-of-the-art performance among policy constraint methods on D4RL MuJoCo benchmarks
- Particularly effective on non-expert datasets where static constraints cause excessive conservatism
- Successfully balances learning speed and distribution shift through careful hyperparameter tuning of the reference policy update ratio

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic reference policy reduces conservatism by avoiding fixed behavioral cloning.
- Mechanism: The method replaces static behavior policy with an exponentially moving average (EMA) of past learned policies, enabling gradual policy expansion while staying close to in-distribution actions.
- Core assumption: The EMA of policies maintains sufficient coverage of the behavior policy's support to prevent catastrophic distributional shift.
- Evidence anchors:
  - [abstract] "Our approach offers several advantages. First, our policy constraint is dynamically updated. This dynamic approach serves to alleviate policy conservatism..."
  - [section 5.2] "Our approach employs a novel reference policy π̃θ, which is the EMA of previously learned policies, yielding the SelfBC objective..."
- Break condition: If the EMA moves too fast (high τref), the reference policy can diverge from the dataset support, causing OOD actions and policy collapse.

### Mechanism 2
- Claim: Conservative policy updates ensure monotonic improvement in the offline setting.
- Mechanism: By blending the current learned policy with the previous reference policy via EMA, the method bounds the distributional shift at each step, analogous to conservative policy iteration.
- Core assumption: The Q-function error remains bounded, and the policy improvement step does not exceed the conservatism imposed by EMA.
- Evidence anchors:
  - [section 5.4] "We extend the analysis of conservative policy iteration to the offline setting and theoretically establish that our approach results in a nearly monotonically improved reference policy."
  - [theorem 1] "The performance difference between two consecutive reference policies influenced by the offline approximation satisfies..."
- Break condition: If the Q-function has high variance or extrapolation error, the conservative bound may be violated, breaking monotonic improvement.

### Mechanism 3
- Claim: Ensemble training reduces noise in reference policy updates.
- Mechanism: Multiple TD3+SelfBC trainers run independently, and their reference actions are averaged to form a shared reference for all, smoothing out individual trainer noise.
- Core assumption: Averaging reference actions across trainers reduces variance without biasing the optimization direction.
- Evidence anchors:
  - [section 5.3] "To reduce the noise, we propose to simultaneously train an ensemble of TD3+SelfBC trainers... the shared reference action is computed by averaging the reference actions from all trainers."
  - [algorithm 4] Shows shared reference action computation and use in policy loss.
- Break condition: If trainers diverge significantly, averaging may slow learning or produce suboptimal reference actions.

## Foundational Learning

- Concept: Conservative Policy Iteration (CPI)
  - Why needed here: The theoretical foundation for why slowly updating the reference policy leads to monotonic improvement.
  - Quick check question: What is the role of the mixing parameter κ in CPI, and how does EMA relate to it?

- Concept: Exponential Moving Average (EMA)
  - Why needed here: The core mechanism for creating a dynamic reference policy that evolves slowly toward the learned policy.
  - Quick check question: How does the choice of τref (EMA update ratio) affect the speed of policy expansion and stability?

- Concept: Distributional Shift in Offline RL
  - Why needed here: Understanding why static constraints cause conservatism and why dynamic constraints help.
  - Quick check question: Why does a static behavior cloning constraint lead to overly conservative policies in offline RL?

## Architecture Onboarding

- Component map:
  - Q-networks (two critics with target networks)
  - Policy network (learned policy πθ)
  - Reference policy network (π̃θ updated via EMA)
  - Target networks for Q and policy
  - Pre-training phase (TD3+EBC or similar)
  - Optional: Ensemble of trainers for noise reduction

- Critical path:
  1. Pre-train a behavior policy and Q-networks using TD3+EBC.
  2. Initialize reference policy from pre-trained policy.
  3. In each training step:
     - Update Q-networks using TD3 loss.
     - Update policy to maximize Q-value minus squared distance to reference policy.
     - Soft-update reference policy toward current policy via EMA.
     - Soft-update target networks.

- Design tradeoffs:
  - EMA speed (τref): Slower update → more conservative, faster → risk of collapse.
  - Pre-training method: Better pre-training → more stable start but may introduce bias.
  - Ensemble size: More trainers → smoother reference but higher compute cost.

- Failure signatures:
  - Sudden performance drop → reference policy diverged from dataset support.
  - Slow learning → EMA too slow or reference policy too close to behavior policy.
  - High variance in training → noise in reference policy updates (consider ensemble).

- First 3 experiments:
  1. Train TD3+SelfBC with default τref on medium dataset; monitor dataset BC MSE vs performance.
  2. Vary τref (0.1, 0.01, 0.001) and observe tradeoff between learning speed and distribution shift.
  3. Compare single trainer vs ensemble (Nens=3,5) on medium-replay to see noise reduction effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on the distribution shift that can be tolerated by the EMA-based reference policy update before performance degradation occurs?
- Basis in paper: [explicit] The paper discusses how larger scaleref values lead to faster growth of distribution shift but eventually cause performance decline, indicating there's a threshold beyond which the EMA mechanism fails.
- Why unresolved: The paper empirically observes this relationship through experiments but does not derive a theoretical bound or explain the mechanism that causes degradation at excessive distribution shifts.
- What evidence would resolve it: Theoretical analysis connecting the convergence properties of the EMA process to the underlying MDP dynamics, or empirical studies mapping distribution shift metrics to performance degradation points across different environments.

### Open Question 2
- Question: How does the performance of SelfBC scale with the size and quality of the offline dataset?
- Basis in paper: [inferred] The method is designed to handle non-expert datasets effectively, suggesting it might be more robust to dataset limitations than traditional BC methods, but the paper doesn't systematically study this relationship.
- Why unresolved: The experimental results show good performance on non-expert datasets but don't vary dataset size or quality systematically to understand the scaling behavior.
- What evidence would resolve it: Experiments comparing SelfBC performance across datasets of varying sizes and quality levels, or theoretical analysis of how the EMA reference policy benefits from dataset diversity.

### Open Question 3
- Question: Can the ensemble reference policy mechanism in TD3+ESBC be extended to use more sophisticated uncertainty estimation rather than simple averaging?
- Basis in paper: [explicit] The paper mentions that while each trainer maintains its own Q networks, they don't ensemble all Q networks for uncertainty estimation like existing uncertainty-based methods.
- Why unresolved: The paper implements a simple averaging approach for the ensemble reference policy but acknowledges this could be extended to more sophisticated methods without exploring this direction.
- What evidence would resolve it: Comparison of TD3+ESBC performance using different ensemble aggregation methods (weighted averaging, uncertainty-weighted averaging, or full uncertainty estimation) across diverse offline RL tasks.

## Limitations
- Theoretical claims rely on bounded Q-function error assumptions that may not hold in practice
- Method's effectiveness depends heavily on hyperparameter tuning of τref
- Limited evaluation to MuJoCo domains only, with no tests on visual or high-dimensional state spaces

## Confidence
- High confidence: The empirical results on D4RL benchmarks showing SelfBC outperforming static constraint methods
- Medium confidence: The theoretical analysis of monotonic improvement under offline assumptions
- Medium confidence: The mechanism explanation for why dynamic reference policies reduce conservatism

## Next Checks
1. Test the method's robustness to τref hyperparameter by systematically varying it across orders of magnitude on each dataset
2. Evaluate performance on out-of-distribution datasets (like D4RL's "mixed" datasets) to test distributional shift handling
3. Implement ablation studies comparing SelfBC against dynamic constraint methods like AWAC to isolate the contribution of the EMA mechanism