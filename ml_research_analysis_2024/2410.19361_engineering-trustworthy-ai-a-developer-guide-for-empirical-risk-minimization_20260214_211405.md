---
ver: rpa2
title: 'Engineering Trustworthy AI: A Developer Guide for Empirical Risk Minimization'
arxiv_id: '2410.19361'
source_url: https://arxiv.org/abs/2410.19361
tags:
- data
- loss
- learning
- privacy
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a developer guide for translating the European\
  \ Union's key requirements for trustworthy AI into design choices for empirical\
  \ risk minimization (ERM)-based machine learning methods. The authors map the seven\
  \ key requirements\u2014human agency and oversight, technical robustness and safety,\
  \ privacy and data governance, transparency, diversity, non-discrimination and fairness,\
  \ societal and environmental well-being, and accountability\u2014to specific design\
  \ choices across three ERM components: data, model, and loss function."
---

# Engineering Trustworthy AI: A Developer Guide for Empirical Risk Minimization

## Quick Facts
- arXiv ID: 2410.19361
- Source URL: https://arxiv.org/abs/2410.19361
- Reference count: 40
- Primary result: A framework mapping EU trustworthy AI requirements to ERM design choices

## Executive Summary
This paper presents a developer guide that translates the European Union's seven key requirements for trustworthy AI into actionable design choices for empirical risk minimization (ERM)-based machine learning methods. The framework connects abstract ethical requirements—including human agency, technical robustness, privacy, transparency, fairness, societal well-being, and accountability—to specific technical decisions across ERM components: data, model, and loss function. By providing concrete implementation guidance, the authors enable developers to build AI systems that meet emerging standards for trustworthiness while maintaining technical performance.

## Method Summary
The authors systematically map each of the EU's seven key requirements for trustworthy AI to specific design choices across the three components of empirical risk minimization: data selection and augmentation, model architecture and complexity, and loss function selection. The framework leverages the equivalence of regularization techniques (data augmentation, loss penalties, model constraints) to address multiple trustworthiness requirements simultaneously. The method provides a structured approach for developers to implement privacy protection through feature transformation, ensure fairness through constraint-based optimization, and enhance explainability through model selection and documentation practices.

## Key Results
- The framework provides actionable recommendations for building AI systems that meet emerging standards for trustworthiness
- Regularization techniques are shown to be equivalent approaches that can address multiple trustworthiness requirements simultaneously
- Privacy protection can be achieved through feature transformation that maintains predictive utility while reducing sensitive attribute information
- The structured framework enables developers to create AI systems that are accurate, robust, fair, transparent, and respectful of privacy and human rights

## Why This Works (Mechanism)

### Mechanism 1
The framework translates abstract EU ethical requirements into concrete ERM design choices that developers can implement by mapping each of the seven EU requirements to specific technical choices across data, model, and loss function components. This creates a direct bridge from high-level policy to low-level implementation. The core assumption is that each ethical requirement can be operationalized through specific technical design decisions in ERM components.

### Mechanism 2
Regularization techniques can be used to implement multiple trustworthiness requirements simultaneously through three equivalent approaches: data augmentation, loss penalty terms, and model constraints. These techniques reduce model complexity or augment data to address requirements like robustness, privacy, and fairness by modifying the effective model complexity and data representation.

### Mechanism 3
Privacy protection can be achieved through feature transformation that maintains predictive utility while reducing sensitive attribute information using techniques like the privacy funnel (mutual information optimization) and sufficient statistics. These methods transform raw features to preserve label predictability while minimizing information leakage about sensitive attributes.

## Foundational Learning

- Concept: Empirical Risk Minimization (ERM)
  - Why needed here: The entire framework is built on understanding how ERM works and how its three components can be modified to meet trustworthiness requirements
  - Quick check question: What are the three components of ERM that the framework maps ethical requirements to?

- Concept: Regularization and its equivalence forms
  - Why needed here: The paper relies on understanding that data augmentation, loss penalty terms, and model constraints are equivalent approaches to regularization
  - Quick check question: What are the three equivalent approaches to regularization mentioned in the paper?

- Concept: Differential Privacy
  - Why needed here: Privacy protection (KR3) is a key requirement, and the paper discusses differential privacy as a formal framework for measuring and ensuring privacy protection in ERM-based systems
  - Quick check question: How does differential privacy measure the non-invertibility of a stochastic algorithm?

## Architecture Onboarding

- Component map:
  - Data component: Features selection, data augmentation, sampling strategies, documentation (datasheets)
  - Model component: Model complexity, Lipschitz continuity, interpretability, model cards
  - Loss component: Loss function choice, regularization terms, sample weighting, adversarial training

- Critical path: 1) Map ethical requirement to component → 2) Select appropriate technical technique → 3) Implement and validate → 4) Document and audit

- Design tradeoffs:
  - Privacy vs. utility: More privacy protection typically reduces predictive accuracy
  - Fairness vs. accuracy: Enforcing fairness constraints may reduce overall model performance
  - Explainability vs. complexity: Simpler models are more explainable but may be less accurate
  - Robustness vs. sensitivity: More robust models may be less sensitive to subtle patterns

- Failure signatures:
  - Performance degradation without corresponding trustworthiness improvement
  - Implementation complexity preventing practical deployment
  - Regulatory requirements evolving faster than the technical framework can adapt
  - Trade-offs creating conflicts between different trustworthiness requirements

- First 3 experiments:
  1. Implement data augmentation for fairness: Augment training data with perturbations that remove sensitive attribute information while preserving label-relevant information, then measure fairness metrics across demographic groups
  2. Test privacy-preserving feature transformation: Implement the privacy funnel approach to transform features, then measure both privacy leakage (mutual information) and predictive utility (accuracy)
  3. Validate robust loss functions: Compare standard ERM with robust loss functions (e.g., Huber loss) on datasets with known outliers, measuring both generalization performance and robustness to perturbations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we quantify the trade-off between privacy protection and predictive accuracy in ERM-based AI systems?
- Basis in paper: The paper discusses the inherent trade-off between privacy protection and resulting statistical accuracy, noting that perfect privacy protection comes at the cost of lower prediction quality
- Why unresolved: The paper mentions this trade-off exists but doesn't provide concrete metrics or frameworks for quantifying it across different privacy-preserving techniques
- What evidence would resolve it: Empirical studies comparing prediction performance across different privacy-preserving methods with standardized metrics for both accuracy and privacy leakage

### Open Question 2
- Question: What are the most effective methods for measuring subjective explainability of ERM models across different user groups?
- Basis in paper: The paper discusses the subjective nature of explainability and the challenge of measuring it, proposing simulatability as one approach but noting it varies by user knowledge
- Why unresolved: While the paper suggests using discrepancies between model predictions and user predictions, it doesn't provide standardized methods for measuring explainability across diverse user populations
- What evidence would resolve it: User studies comparing different explainability metrics across various demographic groups and expertise levels

### Open Question 3
- Question: How can we systematically evaluate the environmental impact of different ERM design choices?
- Basis in paper: The paper mentions KR6 (Societal and Environmental Well-Being) and notes the need to minimize energy and cooling water demands of AI systems, but doesn't provide evaluation frameworks
- Why unresolved: The paper identifies the importance of environmental considerations but doesn't establish metrics or methodologies for evaluating environmental impact across different ERM implementations
- What evidence would resolve it: Comparative lifecycle assessments of different ERM approaches measuring energy consumption, carbon footprint, and water usage

## Limitations

- The framework assumes a one-to-one mapping between ethical requirements and technical design choices, which may be more complex or context-dependent in practice
- The effectiveness of specific design choices in achieving intended ethical outcomes without unintended consequences has low confidence
- The framework assumes all seven EU requirements can be meaningfully operationalized through ERM components, which may not hold for non-technical requirements like human agency and oversight

## Confidence

- High confidence: The mapping between ERM components and technical design choices (data augmentation, regularization, loss functions)
- Medium confidence: The equivalence of different regularization approaches (data augmentation, loss penalties, model constraints)
- Medium confidence: The general framework structure connecting ethical requirements to technical implementation
- Low confidence: The effectiveness of specific design choices in achieving intended ethical outcomes without unintended consequences

## Next Checks

1. Conduct controlled experiments comparing multiple design choices for the same ethical requirement to identify which approaches work best across different domains and datasets
2. Implement the framework in a real-world AI system and conduct longitudinal studies to assess whether the technical choices actually improve trustworthiness metrics over time
3. Perform stakeholder evaluations with both technical developers and non-technical ethics reviewers to validate whether the framework bridges the communication gap between these groups effectively