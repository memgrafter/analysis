---
ver: rpa2
title: 'Agent-E: From Autonomous Web Navigation to Foundational Design Principles
  in Agentic Systems'
arxiv_id: '2407.13032'
source_url: https://arxiv.org/abs/2407.13032
tags:
- task
- agent
- agent-e
- tasks
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Agent-E is a novel web automation agent that outperforms prior
  state-of-the-art web agents by 10-30% on the WebVoyager benchmark. It introduces
  three key architectural improvements: a hierarchical structure separating planning
  and execution, flexible DOM distillation methods for handling noisy web page data,
  and a "change observation" mechanism that provides linguistic feedback after each
  action.'
---

# Agent-E: From Autonomous Web Navigation to Foundational Design Principles in Agentic Systems

## Quick Facts
- arXiv ID: 2407.13032
- Source URL: https://arxiv.org/abs/2407.13032
- Reference count: 5
- Agent-E achieves 73.2% success rate on WebVoyager benchmark, outperforming prior state-of-the-art by 10-30%

## Executive Summary
Agent-E is a novel web automation agent that achieves state-of-the-art performance on the WebVoyager benchmark through three key architectural innovations: a hierarchical structure separating planning and execution, flexible DOM distillation methods for handling noisy web page data, and a "change observation" mechanism providing linguistic feedback after each action. The agent demonstrates 73.2% success rate on 643 tasks across 15 real websites, representing a significant improvement over previous text-only (53.1%) and multimodal (57.1%) agents. Beyond raw performance metrics, Agent-E exhibits strong error awareness with 52% of failures being self-aware, and provides detailed analytics on task completion times and LLM call counts. The work establishes generalizable design principles for building autonomous agents applicable beyond web navigation tasks.

## Method Summary
Agent-E uses a hierarchical architecture with two LLM-powered components: a planner agent for task planning and verification, and a browser navigation agent for execution. The system employs flexible DOM distillation methods (text only, input fields, all fields) that can be adaptively selected based on task requirements, and implements "change observation" using Mutation Observer API to provide linguistic feedback about action outcomes. The agent was evaluated on the WebVoyager benchmark using GPT-4-Turbo with function calling capabilities, running in autonomous mode without human-in-the-loop intervention. Key implementation details include domain-specific primitive skills for sensing and acting on web pages, and systematic error detection and recovery mechanisms.

## Key Results
- 73.2% success rate on WebVoyager benchmark, outperforming prior state-of-the-art by 10-30%
- Strong error awareness with 52% of failures being self-aware vs oblivious
- Task completion times of 150-220 seconds with 50-80 LLM calls per task
- Superior performance on complex tasks requiring multi-step reasoning and error recovery

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical architecture with planner and browser navigation agents improves error detection and recovery
- Mechanism: The planner agent orchestrates task planning and verification, while the browser navigation agent focuses on execution. This separation allows the planner to detect errors through verification steps and easily backtrack to previous pages
- Core assumption: Complex web tasks can be decomposed into sub-tasks that benefit from different levels of granularity in planning and execution
- Evidence anchors:
  - [abstract] "Central to Agent-E are two LLM-powered components: the planner agent and the browser navigation agent"
  - [section 3.5] "The hierarchical architecture allows easily detecting and recovering from errors. The planner agent is prompted to perform verification... as part of the plan whenever necessary"
- Break Condition: If the planner cannot effectively decompose tasks or verify sub-task completion, the hierarchical approach may add unnecessary complexity without benefits

### Mechanism 2
- Claim: Flexible DOM distillation methods enable better task-specific information extraction from noisy web pages
- Mechanism: Agent-E supports three DOM representation methods (text only, input fields, all fields) that the browser navigation agent can adaptively select based on task requirements, optimizing information availability and preventing noise-related problems
- Core assumption: Different web tasks require different levels of detail and types of information from web page DOMs, and a single representation method is insufficient
- Evidence anchors:
  - [abstract] "Agent-E introduces numerous architectural improvements over prior state-of-the-art web agents such as hierarchical architecture, flexible DOM distillation and denoising method"
  - [section 2.1] "Agent-E supports three different DOM representation methods text only, input fields, all fields. This allows Agent-E to flexibly select the DOM representation that it feels is best suited for the task"
- Break Condition: If the selection mechanism between DOM representations is not reliable, the flexibility could lead to inconsistent performance

### Mechanism 3
- Claim: Change observation provides linguistic feedback after each action to improve agent awareness and performance
- Mechanism: After each action execution, the system observes changes in selected attributes and DOM using Mutation Observer API, returning verbal feedback about the outcome to guide subsequent actions
- Core assumption: Linguistic feedback about action outcomes helps LLMs better understand the current state and make more accurate decisions about next steps
- Evidence anchors:
  - [abstract] "the concept of change observation to guide the agent towards more accurate performance"
  - [section 2.1] "All the action skills are designed to not only execute an action but also report on any change in state as an outcome of the action, a concept we call 'Change observation'"
- Break Condition: If the change observation feedback is not specific or timely enough to influence subsequent decisions, the mechanism may not provide meaningful improvements

## Foundational Learning

- Concept: DOM (Document Object Model) structure and properties
  - Why needed here: Understanding how web pages are structured and represented in code is fundamental to building agents that can navigate and interact with web content effectively
  - Quick check question: What are the key differences between text-only, input fields, and all fields DOM representations, and when would each be most appropriate?

- Concept: LLM function calling and skill-based architectures
  - Why needed here: Agent-E relies on LLMs making function calls to execute specific skills for sensing and acting on web pages, requiring understanding of how to design and integrate these capabilities
  - Quick check question: How does the function calling mechanism enable the browser navigation agent to select appropriate DOM distillation methods based on task requirements?

- Concept: Hierarchical task planning and decomposition
  - Why needed here: The planner agent must effectively break down complex tasks into sub-tasks and manage their execution, requiring understanding of planning algorithms and task decomposition strategies
  - Quick check question: What are the key considerations when designing the planner agent's verification steps to ensure task completion without excessive overhead?

## Architecture Onboarding

- Component map: User task → Planner Agent planning → Browser Navigation Agent execution loop → Task completion or failure
- Critical path: User task → Planner Agent planning → Browser Navigation Agent execution loop → Task completion or failure
- Design tradeoffs:
  - Flexibility vs. complexity: Multiple DOM representations provide flexibility but increase system complexity
  - Hierarchical separation vs. latency: Clear role separation improves modularity but may introduce communication overhead
  - Change observation detail vs. performance: More detailed feedback improves accuracy but may increase processing time
- Failure signatures:
  - Planner fails to decompose tasks correctly: Tasks fail early with incomplete or incorrect sub-task generation
  - Browser navigation agent selects wrong DOM representation: Actions target incorrect elements or miss relevant information
  - Change observation misses state changes: Agent continues with incorrect assumptions about page state
- First 3 experiments:
  1. Implement basic planner-browser navigation agent loop with single DOM representation on a simple website (e.g., Wikipedia search)
  2. Add multiple DOM representation support and test adaptive selection on a website with mixed content types (e.g., Amazon product pages)
  3. Implement change observation with Mutation Observer and test on a dynamic website with frequent state changes (e.g., Google Flights)

## Open Questions the Paper Calls Out

- Question: What specific architectural modifications to Agent-E would be required to achieve human-in-the-loop functionality while maintaining or improving its current 73.2% success rate on WebVoyager?
  - Basis in paper: [explicit] The paper explicitly mentions that Agent-E can run in both autonomous and human-in-the-loop modes, but focuses on autonomous mode evaluation. It states "We believe human-in-the-loop workflows are critical for adoption of agentic systems" and notes that "since there are no web automation benchmarks for human-in-the-loop agents, in this paper we will focus on Agent-E as an autonomous web agent."
  - Why unresolved: The paper deliberately avoids human-in-the-loop evaluation due to lack of benchmarks, and the current architecture is optimized for autonomous operation. The specific technical challenges of integrating human feedback mechanisms without degrading performance are not addressed.
  - What evidence would resolve it: A systematic evaluation of Agent-E's performance when human intervention is incorporated at various decision points, along with architectural modifications needed to support this mode effectively.

- Question: How would Agent-E's performance scale when tested on websites not included in the WebVoyager benchmark, particularly those with significantly different DOM structures or interaction patterns?
  - Basis in paper: [inferred] The paper acknowledges that Agent-E was evaluated on a specific benchmark and notes "It would be possible to optimize Agent-E for specific type of tasks (e.g., form filling) or specific websites (e.g., Atlassian Confluence pages) to achieve significantly higher performance." This suggests that current performance may not generalize to all web environments.
  - Why unresolved: The WebVoyager benchmark, while diverse, covers only 15 specific websites. The paper doesn't provide evidence of Agent-E's performance on arbitrary websites with varying complexity, structure, or interaction paradigms.
  - What evidence would resolve it: Systematic testing of Agent-E across a broad range of real-world websites, particularly those with non-standard DOM structures, complex JavaScript interactions, or unique UI patterns not represented in the benchmark.

- Question: What is the theoretical limit of Agent-E's self-improvement capabilities, and how quickly could it reach practical utility thresholds through routine offline analysis of past tasks and human demonstrations?
  - Basis in paper: [explicit] The paper discusses a principle of "Analyse, reflect and aggregate past experiences routinely for self-improvement" and suggests that "A better approach would be to establish offline workflows that routinely analyse, reflects on and aggregates past tasks and human demonstrations to convert them to more classical automation workflows."
  - Why unresolved: While the paper proposes this self-improvement mechanism conceptually, it doesn't provide empirical data on how quickly Agent-E could improve its performance through this approach, or what the upper bounds of this improvement might be.
  - What evidence would resolve it: Longitudinal studies tracking Agent-E's performance improvements over time as it accumulates more task executions and human demonstrations, including analysis of diminishing returns and practical utility thresholds.

## Limitations

- Incomplete specification of prompting templates and system instructions for both planner and browser navigation agents
- Unclear implementation details of DOM distillation methods and change observation mechanism
- Limited evidence of performance generalization beyond WebVoyager benchmark websites

## Confidence

- High confidence in the hierarchical architecture contribution and its general effectiveness for error detection and recovery
- Medium confidence in the specific implementation details of DOM distillation methods and their impact on performance
- Low confidence in the exact prompting strategies and selection mechanisms that drive the agent's adaptive behavior

## Next Checks

1. Implement a controlled experiment comparing the three DOM distillation methods (text only, input fields, all fields) on a standardized set of web pages to quantify the performance differences and validate the claimed benefits of flexibility
2. Conduct ablation studies by removing the change observation mechanism to measure its specific contribution to success rates and error awareness
3. Perform stress testing on websites where Agent-E reported lower performance (Booking.com, Google Flights) to identify specific failure patterns and determine if they stem from architectural limitations or implementation details