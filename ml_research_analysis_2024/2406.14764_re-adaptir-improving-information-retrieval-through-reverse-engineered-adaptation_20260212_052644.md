---
ver: rpa2
title: 'RE-AdaptIR: Improving Information Retrieval through Reverse Engineered Adaptation'
arxiv_id: '2406.14764'
source_url: https://arxiv.org/abs/2406.14764
tags:
- retrieval
- data
- fine-tuning
- training
- re-a
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RE-AdaptIR, an extension of the reverse engineered
  adaptation (RE-Adapt) method to improve information retrieval models using only
  unlabeled data. RE-AdaptIR works by fine-tuning the pretrained language model on
  unlabeled documents, constructing a retrieval-specific RE-Adapter, and then readapting
  the retrieval model with this adapter to improve performance both in-domain and
  zero-shot.
---

# RE-AdaptIR: Improving Information Retrieval through Reverse Engineered Adaptation

## Quick Facts
- arXiv ID: 2406.14764
- Source URL: https://arxiv.org/abs/2406.14764
- Authors: William Fleshman; Benjamin Van Durme
- Reference count: 9
- Primary result: Improves zero-shot retrieval performance by 0.6-1.9 nDCG@10 points across 14 datasets

## Executive Summary
RE-AdaptIR extends the reverse engineered adaptation method to improve information retrieval models using only unlabeled data. The approach works by fine-tuning a pretrained language model on unlabeled documents, constructing a retrieval-specific adapter from the weight differences between retrieval fine-tuned and base models, and then readapting the retrieval model with this adapter. The method demonstrates improved performance both in-domain and zero-shot across 14 datasets, showing that retrieval models can be effectively enhanced without access to labeled query-document pairs.

## Method Summary
RE-AdaptIR operates by first fine-tuning a base pretrained LLM (LLaMA-2-7B or Mistral-7B) on unlabeled documents from the target domain using a knowledge adapter. It then constructs a RE-Adapter by taking the weight difference between a retrieval fine-tuned model and the base model, effectively capturing retrieval-specific knowledge. The final retrieval model is instantiated by combining the base model weights with the knowledge adapter and RE-Adapter using partial adaptation scalars α and β. This allows the model to incorporate domain knowledge while preserving retrieval-specific semantic mappings learned during contrastive fine-tuning.

## Key Results
- RE-AdaptIR improves zero-shot performance by an average of 0.6 nDCG@10 points for RepLLaMA and 1.9 nDCG@10 points for e5-Mistral across 14 datasets
- The method demonstrates effectiveness both in-domain (MS-MARCO) and zero-shot (BeIR datasets)
- Performance improvements occur even when fine-tuning data doesn't exactly match test-time queries
- RE-AdaptIR shows robustness across different base models and retrieval architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RE-Adapter captures retrieval-specific knowledge learned during contrastive fine-tuning that is not present in the base model.
- Mechanism: By taking the weight difference between the retrieval fine-tuned model and the base pretrained model, RE-AdaptIR isolates the adapter that encodes retrieval-specific semantic mappings learned during InfoNCE training.
- Core assumption: The weight difference between retrieval fine-tuned and base models represents a task-specific adapter that can be reapplied to a newly fine-tuned base model.
- Evidence anchors:
  - [abstract] "We use RE-AdaptIR to improve LLM-based IR models using only unlabeled data"
  - [section] "we construct a RE-AdaptIR for the retrieval model by discarding the pretrained next-token predictor weights as well as the corresponding weights from the knowledge adapter"
- Break condition: If the retrieval-specific knowledge is encoded in non-linear ways that are not captured by simple weight differences, or if the base model has been fine-tuned in ways that interfere with the RE-Adapter's effectiveness.

### Mechanism 2
- Claim: Fine-tuning the base model on unlabeled documents from the target domain improves its document representation quality for that domain.
- Mechanism: The knowledge adapter Ψ learns to map domain-specific documents to improved vector representations through continued pretraining-style next-token prediction on the corpus.
- Core assumption: The base model can benefit from continued pretraining on domain-specific documents even when the downstream task is retrieval rather than generation.
- Evidence anchors:
  - [abstract] "We demonstrate improved performance both in training domains as well as zero-shot in domains where the models have seen no queries"
  - [section] "We first fine-tune the pretrained LLM on unlabeled documents from a new domain"
- Break condition: If the domain-specific documents are too diverse or noisy, or if the base model has already saturated its ability to learn from pretraining on similar data.

### Mechanism 3
- Claim: The partial adaptation scalars α and β control interference between the knowledge adapter and RE-Adapter, preserving retrieval ability while incorporating domain knowledge.
- Mechanism: By setting α and β to values less than 1 (typically 0.5), RE-AdaptIR balances the contributions of the newly learned domain knowledge and the preserved retrieval-specific knowledge.
- Core assumption: Retrieval ability is preserved when the RE-Adapter contribution is not completely overwhelmed by the knowledge adapter.
- Evidence anchors:
  - [abstract] "the model can be re-instantiated with weights Θ + αΨ + β∆ where α and β are partial adaptation scalars used to control the strength of fine-tuning"
  - [section] "As in Fleshman and Van Durme (2024), we use a scalar of 0.5 with our knowledge adapters to minimize interference with existing retrieval ability"
- Break condition: If the optimal α and β values are highly dataset-dependent, making it difficult to find good defaults, or if the interference effects are more complex than simple linear combination.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: Understanding how text retrieval models are trained to map similar texts to similar vectors is crucial for grasping how RE-AdaptIR works
  - Quick check question: What is the key objective that contrastive training optimizes for in text retrieval models?

- Concept: Parameter-efficient fine-tuning (PEFT) methods like LoRA and DoRA
  - Why needed here: RE-AdaptIR uses DoRA adapters to efficiently fine-tune the large language models on unlabeled data
  - Quick check question: How do LoRA/DoRA adapters modify the weight matrices during fine-tuning?

- Concept: Dense vector representations and similarity search
  - Why needed here: Text retrieval fundamentally relies on comparing dense vector representations of queries and documents
  - Quick check question: What similarity function is typically used to rank documents given a query representation in dense retrieval?

## Architecture Onboarding

- Component map: Base LLM -> Knowledge adapter (Ψ) -> RE-Adapter construction (∆) -> RE-AdaptIR combination (Θ + αΨ + β∆) -> Retriever interface

- Critical path: Base model → domain fine-tuning (Ψ) → RE-Adapter construction (∆) → RE-AdaptIR combination (Θ + αΨ + β∆) → retrieval evaluation

- Design tradeoffs:
  - Fine-tuning the base model vs. fine-tuning the retrieval model directly
  - Using domain-specific data vs. related data from original training
  - Fixed partial adaptation scalars vs. dataset-specific optimization

- Failure signatures:
  - Performance degradation on both in-domain and zero-shot tasks
  - High variance in results across different datasets
  - Significant drops in performance as corpus size increases

- First 3 experiments:
  1. Replicate the MS-MARCO in-domain results to verify the basic RE-AdaptIR mechanism
  2. Test zero-shot performance on one BeIR dataset to verify domain generalization
  3. Compare performance using only test-time passages vs. all passages in the corpus to validate the hypothesis about fine-tuning data relevance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RE-AdaptIR scale with increasing amounts of unlabeled data beyond what was tested?
- Basis in paper: [inferred] The paper mentions using MS-MARCO with 8.84M passages and notes that only a fraction was used in training due to limited queries, but doesn't explore the upper limits of unlabeled data effectiveness.
- Why unresolved: The experiments used a fixed amount of unlabeled data per dataset, without testing how performance changes with significantly larger or smaller datasets.
- What evidence would resolve it: Experiments showing performance trends across varying quantities of unlabeled data, particularly with datasets much larger than 8.84M passages, would clarify the scalability limits.

### Open Question 2
- Question: What is the optimal partial adaptation scalar (α) for different retrieval tasks and dataset sizes?
- Basis in paper: [explicit] The paper mentions using a default scalar of 0.5 but notes that optimizing this value per dataset improves results, without providing specific optimal values.
- Why unresolved: The experiments used a single scalar value across all datasets, without exploring the performance impact of task-specific or size-specific scalar optimization.
- What evidence would resolve it: Systematic experiments varying the partial adaptation scalar across different retrieval tasks and dataset sizes would identify optimal values and their relationships to task characteristics.

### Open Question 3
- Question: How does RE-AdaptIR perform when applied to retrieval models with different architectures beyond transformer-based models?
- Basis in paper: [inferred] The paper focuses exclusively on transformer-based retrieval models (RepLLaMA and e5-Mistral) without exploring other architectures.
- Why unresolved: The method's effectiveness on non-transformer architectures like traditional sparse retrieval methods or other neural architectures remains untested.
- What evidence would resolve it: Applying RE-AdaptIR to diverse retrieval architectures and comparing performance across different model types would reveal the method's architectural dependencies.

### Open Question 4
- Question: What is the relationship between the quality of unlabeled data and the effectiveness of RE-AdaptIR?
- Basis in paper: [inferred] The paper uses general unlabeled corpora but doesn't investigate how data quality metrics (like relevance, noise levels, or topical consistency) affect performance.
- Why unresolved: All experiments use relatively clean, structured datasets without examining how noisy or irrelevant unlabeled data impacts adaptation quality.
- What evidence would resolve it: Experiments systematically varying the quality of unlabeled data (through noise injection, relevance filtering, or mixing with unrelated corpora) would establish quality thresholds for effective adaptation.

## Limitations
- Evaluation primarily limited to zero-shot performance on BeIR datasets, not reflecting realistic deployment scenarios
- Performance degradation observed with larger corpus sizes (8.8M passages) suggests potential scalability issues
- Method requires access to full retriever fine-tuning code and weights, limiting applicability to proprietary models

## Confidence

**High Confidence**: The core mechanism of RE-AdaptIR - constructing an adapter from the difference between retrieval fine-tuned and base models, then using it to adapt a knowledge-augmented base model - is technically sound and well-explained. The in-domain MS-MARCO results showing consistent improvements across both RepLLaMA and e5-Mistral are convincing.

**Medium Confidence**: The zero-shot generalization claims are supported by the BeIR experiments, but the relatively small average improvements (0.6-1.9 nDCG@10 points) and the fact that some datasets show degradation suggest the method's effectiveness may be dataset-dependent. The claim about RE-AdaptIR working even when fine-tuning data doesn't match test-time queries needs more rigorous testing.

**Low Confidence**: The scalability claims are contradicted by the performance degradation observed with larger corpora, and the paper doesn't provide a clear explanation for this behavior or strategies to mitigate it.

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary α and β across {0.1, 0.3, 0.5, 0.7, 0.9} for each dataset to determine whether the 0.5 default is optimal or if dataset-specific tuning is required. This would clarify whether the method has practical deployment constraints.

2. **Intermediate Domain Testing**: Design experiments where the fine-tuning corpus contains related but non-identical domains to the test queries (e.g., fine-tune on news articles, test on scientific abstracts). This would rigorously test the core claim about RE-AdaptIR's effectiveness when training data doesn't match test-time queries.

3. **Scalability Stress Test**: Evaluate RE-AdaptIR performance on progressively larger corpora (1M, 4M, 8M, 16M passages) while monitoring both retrieval quality and computational efficiency. This would identify the scalability limits and whether they stem from the RE-Adapter construction, the fine-tuning process, or the combination mechanism.