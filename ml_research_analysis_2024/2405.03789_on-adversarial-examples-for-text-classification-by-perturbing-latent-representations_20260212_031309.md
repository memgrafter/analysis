---
ver: rpa2
title: On Adversarial Examples for Text Classification by Perturbing Latent Representations
arxiv_id: '2405.03789'
source_url: https://arxiv.org/abs/2405.03789
tags:
- adversarial
- examples
- text
- embedding
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to evaluate the robustness of text
  classifiers by generating adversarial examples. The key idea is to use an encoder-decoder
  architecture to transform text into embedding vectors, perturb the embeddings using
  white-box attacks (specifically FGSM), and then reconstruct the perturbed embeddings
  back into adversarial text.
---

# On Adversarial Examples for Text Classification by Perturbing Latent Representations

## Quick Facts
- arXiv ID: 2405.03789
- Source URL: https://arxiv.org/abs/2405.03789
- Reference count: 18
- One-line primary result: The authors propose a method to evaluate text classifier robustness by generating adversarial examples through perturbing latent representations using an encoder-decoder architecture.

## Executive Summary
This paper introduces a novel approach to evaluate text classifier robustness by generating adversarial examples through perturbing latent representations. The key innovation is transforming text into continuous embedding vectors using an encoder-decoder architecture, applying white-box attacks (specifically FGSM) to these embeddings, and reconstructing the perturbed embeddings back into adversarial text. Experiments on the Ag News corpus demonstrate that this approach can generate natural-looking adversarial examples that successfully fool the classifier while maintaining semantic similarity to the original text.

## Method Summary
The authors propose an encoder-decoder architecture combined with a text classifier to generate adversarial examples. The encoder maps discrete text tokens to continuous embedding vectors, while the decoder reconstructs text from embeddings. The encoder, decoder, and classifier are jointly trained using a loss function that combines reconstruction loss with classification loss. FGSM is then applied to the embedding vectors to create perturbations, which are decoded back into adversarial text. The approach is evaluated on the Ag News corpus with four categories: World, Sports, Business, and Sci/Tech.

## Key Results
- The proposed method successfully generates adversarial examples that fool the text classifier while maintaining semantic similarity
- Sci/Tech news articles are the most vulnerable, being easily transformed into World or Business news
- Adversarial examples maintain natural language fluency despite perturbations in the embedding space
- The approach requires careful tuning of the perturbation bound (ε) to balance attack effectiveness and semantic preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbing latent representations instead of discrete tokens enables effective white-box attacks on text classifiers.
- Mechanism: The encoder transforms discrete text into continuous embedding vectors. Adversarial attacks (e.g., FGSM) can then compute small perturbations on these continuous vectors using the classifier's gradients. The decoder reconstructs the perturbed embeddings back into text while preserving semantics.
- Core assumption: The embedding space learned by the encoder-decoder is semantically meaningful such that small perturbations do not drastically change meaning.
- Evidence anchors:
  - [abstract]: "we transform the input into its embedding vector containing real values to perform the state-of-the-art white-box attacks"
  - [section 4.1]: "we need to efficiently train them such that the distance between two embedding vectors can indicate how much the semantics of their particular text are different"
  - [corpus]: Weak - no direct citations but related work on adversarial examples exists
- Break condition: If the encoder-decoder cannot maintain semantic similarity between original and reconstructed text, perturbations will produce semantically different outputs, breaking the attack.

### Mechanism 2
- Claim: Joint training of encoder-decoder with classifier ensures embedding vectors are organized for the specific classification task.
- Mechanism: The encoder-decoder is trained with a reconstruction loss plus a classification loss on the embedding vectors. This forces the embedding space to preserve both semantic reconstruction and classification-relevant features.
- Core assumption: The classification loss guides the embedding space to be task-specific, not just reconstruction-focused.
- Evidence anchors:
  - [section 4.1]: "we also create a classifier whose input is an embedding vector and output is the same as the targeted one. Then, we train it together with the encoder and decoder"
  - [section 4.1]: "this training prevents the encoder from overfitting the reconstruction task, where it only outputs the same embedding vector regardless of the input"
  - [corpus]: Weak - no direct citations but related work on adversarial examples exists
- Break condition: If the classification loss is too weak or absent, the embedding space may not be task-specific, reducing attack effectiveness.

### Mechanism 3
- Claim: FGSM applied to embedding vectors creates adversarial examples that are semantically similar to original text.
- Mechanism: FGSM computes perturbations in the direction of the classifier's gradient with respect to the embedding vector. The perturbation bound (ε) controls how much the embedding can change while maintaining semantic similarity.
- Core assumption: Small perturbations in embedding space translate to minimal semantic changes when decoded back to text.
- Evidence anchors:
  - [section 4.3]: "FGSM [4] as the attack because it is fast and enough to show that our approach is effective"
  - [section 5.2]: "Noticeably, the adversarial examples are classified as different classes from their clean texts although only a few words at the end changed"
  - [corpus]: Weak - no direct citations but related work on adversarial examples exists
- Break condition: If ε is too large, the perturbed embeddings will decode to text with significantly different meaning, making the attack obvious and ineffective.

## Foundational Learning

- Concept: Discrete vs. Continuous Input Spaces
  - Why needed here: Text is inherently discrete (words, characters) while most adversarial attacks assume continuous inputs. Understanding this difference is crucial for why the encoder-decoder approach is necessary.
  - Quick check question: Why can't we directly apply FGSM to discrete text tokens?

- Concept: Gradient-Based Adversarial Attacks
  - Why needed here: The paper uses FGSM, which requires computing gradients of the classifier with respect to its input. Understanding how these attacks work is essential for grasping the approach.
  - Quick check question: What information does FGSM need from the classifier to compute adversarial perturbations?

- Concept: Encoder-Decoder Architecture
  - Why needed here: The encoder maps discrete text to continuous embeddings, and the decoder maps embeddings back to text. Understanding this architecture is key to understanding how the approach works.
  - Quick check question: What is the role of the reconstruction loss in training the encoder-decoder?

## Architecture Onboarding

- Component map:
  - Encoder (E): Maps discrete text tokens to continuous embedding vectors
  - Decoder (D): Maps embedding vectors back to discrete text tokens
  - Classifier (F): The target text classifier being evaluated
  - Adversary: Applies FGSM to create perturbations on embedding vectors

- Critical path:
  1. Input text → Encoder → Embedding vector
  2. Embedding vector → Decoder → Reconstructed text
  3. Reconstructed text → Classifier → Gradients
  4. Gradients + Embedding vector → FGSM → Perturbed embedding
  5. Perturbed embedding → Decoder → Adversarial example

- Design tradeoffs:
  - Training time vs. attack effectiveness: More training iterations may improve semantic preservation but increase computation time
  - Perturbation bound (ε) vs. attack stealth: Smaller ε makes perturbations less detectable but may reduce attack success rate
  - Embedding dimension size vs. model complexity: Higher dimensions may capture more semantic information but increase computational cost

- Failure signatures:
  - Reconstructed text significantly different from original (semantic drift)
  - Adversarial examples easily detected as non-natural language
  - Classifier remains robust to generated adversarial examples
  - Training instability or convergence issues

- First 3 experiments:
  1. Verify encoder-decoder semantic preservation: Compare original and reconstructed text for semantic similarity on a held-out validation set
  2. Test perturbation sensitivity: Generate adversarial examples with varying ε values and measure classification accuracy degradation
  3. Evaluate attack transferability: Test whether adversarial examples generated for one classifier transfer to other classifiers trained on the same data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the perturbation bound (ϵ) affect the quality and effectiveness of adversarial examples across different text classifiers and datasets?
- Basis in paper: [explicit] The authors mention needing to try several values of the perturbation bound (ϵ) to ensure adversarial examples look the same as clean ones.
- Why unresolved: The paper only tests ϵ = 0.05 on the Ag News corpus and does not explore how different values impact adversarial example quality across various classifiers or datasets.
- What evidence would resolve it: Systematic experiments varying ϵ across multiple text classification tasks and datasets, measuring both attack success rate and semantic preservation.

### Open Question 2
- Question: Can the encoder-decoder architecture be made more efficient to reduce training time while maintaining adversarial example quality?
- Basis in paper: [explicit] The authors acknowledge that training the encoder and decoder for a specific corpus takes a long time as a limitation.
- Why unresolved: The paper uses a standard BERT-based encoder-decoder architecture without exploring efficiency optimizations or architectural alternatives.
- What evidence would resolve it: Comparative studies of training time and adversarial example quality using different architectures (e.g., smaller models, different training strategies, or transfer learning approaches).

### Open Question 3
- Question: How does the framework perform when applied to datasets with different characteristics, such as longer texts or more classes?
- Basis in paper: [explicit] The authors state they plan to experiment with this framework on various datasets and classifiers as future work.
- Why unresolved: The paper only evaluates on the Ag News corpus with 30,522 tokens and 4 classes, limiting generalizability.
- What evidence would resolve it: Experiments on datasets with varying text lengths, class numbers, and domain characteristics, measuring attack success rates and qualitative assessment of generated adversarial examples.

## Limitations

- The approach requires significant training time for the encoder-decoder architecture
- The perturbation bound (ε) must be carefully tuned for each dataset and classifier
- Limited evaluation to a single dataset (Ag News corpus) with only four classes

## Confidence

**High confidence** in the core technical approach: The encoder-decoder architecture combined with FGSM attacks on embedding vectors is methodologically sound and builds on established techniques from both adversarial machine learning and sequence-to-sequence modeling.

**Medium confidence** in empirical effectiveness: The attack success rates are demonstrated on a single dataset with specific hyperparameters. The sensitivity analysis for ε is limited to two values (0.05 and 0.1), and the relationship between perturbation magnitude and semantic preservation is not thoroughly explored.

**Low confidence** in robustness against defenses: The paper doesn't evaluate whether the generated adversarial examples transfer across different classifiers or whether they're detectable using simple statistical tests. This leaves uncertainty about the practical vulnerability of real-world systems.

## Next Checks

1. **Semantic drift quantification**: Implement semantic similarity metrics (e.g., BERTScore, BLEU, or human evaluation) to measure the semantic distance between original text and adversarial examples across varying perturbation magnitudes. This would validate whether small perturbations truly maintain semantic similarity.

2. **Cross-classifier transferability**: Test whether adversarial examples generated for one classifier transfer to other classifiers with different architectures (e.g., CNN, LSTM, Transformer-based) trained on the same data. This would reveal whether the attack exploits classifier-specific vulnerabilities or general weaknesses in text classification.

3. **Detection rate analysis**: Evaluate whether simple statistical features (e.g., perplexity, n-gram frequency deviation, or embedding space distance from training distribution) can detect adversarial examples at rates higher than random chance. This would help assess the practical stealthiness of the attack.