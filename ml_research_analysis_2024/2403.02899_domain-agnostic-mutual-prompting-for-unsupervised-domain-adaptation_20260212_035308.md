---
ver: rpa2
title: Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation
arxiv_id: '2403.02899'
source_url: https://arxiv.org/abs/2403.02899
tags:
- domain
- prompting
- visual
- damp
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DAMP, a domain-agnostic mutual prompting framework
  for unsupervised domain adaptation (UDA). The key idea is to learn domain-invariant
  prompts by mutually aligning visual and textual embeddings in CLIP.
---

# Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation

## Quick Facts
- arXiv ID: 2403.02899
- Source URL: https://arxiv.org/abs/2403.02899
- Authors: Zhekai Du; Xinyao Li; Fengling Li; Ke Lu; Lei Zhu; Jingjing Li
- Reference count: 40
- Primary result: DAMP consistently outperforms state-of-the-art methods by 1-3% in accuracy on three UDA benchmarks.

## Executive Summary
This paper proposes DAMP, a domain-agnostic mutual prompting framework for unsupervised domain adaptation (UDA) that bridges the gap between source and target domains using CLIP's vision-language alignment capabilities. The key innovation is learning domain-invariant prompts through mutual alignment of visual and textual embeddings, where image context prompts the language branch and vice versa. The framework employs cross-attention and regularization losses to ensure domain-agnosticism and instance-specificity, achieving consistent improvements of 1-3% in accuracy over state-of-the-art methods on Office-Home, VisDA-17, and Mini-DomainNet benchmarks.

## Method Summary
DAMP learns shared textual prompts across domains while using visual prompts guided by textual semantics to extract domain-invariant visual features. The framework employs a cross-attention-based mutual prompting module that enables bidirectional interaction between visual and textual embeddings. Instance-conditioned textual prompting adapts semantic embeddings based on image context, while regularization losses (instance-discrimination contrastive loss, semantic-consistency loss, and entropy-based loss) enforce domain-agnosticism, instance-specificity, and semantic consistency. The training objective combines supervised loss on source data, pseudo-labels on confident target samples, and the regularization terms.

## Key Results
- DAMP consistently outperforms state-of-the-art methods by 1-3% in accuracy across three UDA benchmarks
- The method achieves strong performance on Office-Home, VisDA-17, and Mini-DomainNet datasets
- Ablation studies demonstrate the effectiveness of mutual prompting compared to independent prompting

## Why This Works (Mechanism)

### Mechanism 1
Domain-agnostic mutual prompting enables knowledge transfer from source to target domain without domain-specific bias. The framework learns shared textual prompts across domains, while using visual prompts guided by textual semantics to elicit domain-invariant visual features. Cross-attention enables bidirectional interaction, and regularizations (Lidc, Lsc) enforce domain-agnosticism and instance-specificity. Core assumption: Visual embeddings from different domains contain distinct domain-biased information, but domain-agnostic textual prompts can guide the extraction of domain-invariant visual features.

### Mechanism 2
Instance-conditioned textual prompting improves image-text alignment by adapting semantic embeddings based on image context. The framework uses image context (ev) to adjust textual embeddings via the same prompting module G, making the textual embeddings instance-dependent. This enables better pairing of text and images in both domains. Core assumption: Instance diversity leads to large intra-class variation, making it difficult to align all samples to a class-level textual prompt. Adapting textual embeddings based on image context can better capture these variations.

### Mechanism 3
The mutual prompting framework, with cross-attention and regularizations, elicits domain-invariant representations and improves cross-domain knowledge transfer. The framework learns both textual and visual prompts mutually, with cross-attention enabling bi-directional interaction. Regularizations (Lidc, Lsc, Lim) ensure domain-agnosticism, instance-specificity, and semantic consistency. Core assumption: The interdependent nature of textual and visual prompts requires a mutual learning framework to ensure effective synergy and domain-invariant representations.

## Foundational Learning

- Concept: Cross-attention mechanism
  - Why needed here: To enable bidirectional interaction between textual and visual embeddings, allowing them to mutually guide each other's prompting process
  - Quick check question: How does the cross-attention mechanism differ from self-attention, and why is it more suitable for this task?

- Concept: Regularization techniques (contrastive loss, semantic consistency loss)
  - Why needed here: To enforce domain-agnosticism, instance-specificity, and semantic consistency in the learned prompts, ensuring effective cross-domain knowledge transfer
  - Quick check question: What is the role of each regularization term (Lidc, Lsc, Lim) in the overall training objective, and how do they contribute to the desired properties of the prompts?

- Concept: Prompt learning in vision-language models
  - Why needed here: To adapt the pre-trained CLIP model to the unsupervised domain adaptation task by learning domain-agnostic and instance-conditioned prompts
  - Quick check question: How does prompt learning differ from fine-tuning in vision-language models, and what are the advantages of using prompts for domain adaptation?

## Architecture Onboarding

- Component map: Text encoder (fs) -> Visual encoder (fv) -> Mutual prompting module (G) -> Regularization losses (Lidc, Lsc, Lim) -> Training objective

- Critical path: 1. Extract textual and visual embeddings using fs and fv. 2. Prompt textual embeddings based on image context using G. 3. Prompt visual embeddings based on textual semantics using G. 4. Compute cross-attention-based losses and regularizations. 5. Update prompts and prompting module using the training objective.

- Design tradeoffs: Parameter-sharing vs. separate modules for textual and visual prompting; Length of learnable tokens (N) vs. model capacity and training efficiency; Weight coefficients (γv, γs) for visual and textual prompting vs. modality gap in CLIP

- Failure signatures: Poor cross-domain knowledge transfer indicates ineffective prompting or regularization; Overfitting to source domain suggests insufficient regularization or improper weight coefficients; Suboptimal image-text alignment implies ineffective instance-conditioning or cross-attention

- First 3 experiments: 1. Ablation study: Remove mutual prompting and use independent prompting for each modality. Compare performance to assess the importance of mutual synergy. 2. Hyperparameter sensitivity: Vary the length of learnable tokens (N) and weight coefficients (γv, γs) to understand their impact on performance. 3. Regularization analysis: Remove each regularization term (Lidc, Lsc, Lim) individually and evaluate the effect on domain-agnosticism and instance-specificity of the learned prompts.

## Open Questions the Paper Calls Out

- How does the proposed method handle domain shifts in the visual modality compared to DAPrompt? The paper states that DAPrompt does not perform any adaptation in the visual modality, making it susceptible to domain shifts. It also mentions that DAMP allows the semantic embeddings to dynamically adjust their positions based on visual cues for each sample, but does not provide a detailed comparison or quantitative analysis of how DAMP handles visual domain shifts compared to DAPrompt.

- What is the impact of the learnable weights γv and γs on the performance of DAMP? The paper mentions that the learnable weights γv and γs are used to control the updating magnitudes for the visual and textual embeddings, respectively, and shows their values during training, but does not provide a detailed analysis of their impact on performance.

- How does the proposed method handle domain generalization tasks? The paper mentions that with minor changes, the proposed method can be used for domain generalization tasks and provides experimental results on domain generalization benchmarks, but does not provide a detailed analysis of how the proposed method handles domain generalization tasks or how it compares to other domain generalization methods.

## Limitations

- The exact CLIP model version, tokenization specifics for prompting, and precise hyperparameter values for the confidence threshold T and loss weights λc, λi are not fully specified, which may affect reproducibility.
- The claim of "consistently outperforming state-of-the-art methods by 1-3% in accuracy" across three benchmarks is based on the authors' reported results, and independent verification is needed to confirm the robustness of these improvements.
- The paper does not provide a detailed analysis of how the proposed method handles domain generalization tasks or how it compares to other domain generalization methods.

## Confidence

**High Confidence:** The core mechanism of mutual prompting between visual and textual embeddings using cross-attention is well-defined and supported by the provided equations and architecture description. The general approach of using instance-conditioned textual prompting and domain-agnostic visual prompts is logically sound.

**Medium Confidence:** The effectiveness of the specific regularization terms (Lidc, Lsc, Lim) and their exact implementation details could benefit from more explicit description. While the general concept is clear, the precise impact of each regularization on the learned prompts is not fully elaborated.

**Low Confidence:** The claim of "consistently outperforming state-of-the-art methods by 1-3% in accuracy" across three benchmarks is a strong empirical claim that requires independent verification. The specific experimental setup, including data preprocessing, evaluation protocol, and comparison methods, is not fully detailed in the provided excerpt.

## Next Checks

1. **Independent Reproduction:** Conduct an independent reproduction of the experiments on the three benchmarks (Office-Home, VisDA-17, Mini-DomainNet) using the provided methodology. This will validate the claimed accuracy improvements and assess the robustness of the approach.

2. **Ablation Study of Regularization Terms:** Perform a detailed ablation study to quantify the individual contributions of each regularization term (Lidc, Lsc, Lim) to the overall performance. This will provide insights into the importance of each regularization and help optimize their implementation.

3. **Hyperparameter Sensitivity Analysis:** Systematically vary the key hyperparameters, such as the length of learnable tokens (N), weight coefficients (γv, γs), and confidence threshold T, to understand their impact on the performance and identify the optimal configuration for different datasets and domains.