---
ver: rpa2
title: Properties of fairness measures in the context of varying class imbalance and
  protected group ratios
arxiv_id: '2411.08425'
source_url: https://arxiv.org/abs/2411.08425
tags:
- fairness
- measures
- group
- imbalance
- ratios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how class imbalance and protected group
  ratios affect six popular group fairness measures. The authors define eight dataset-independent
  properties of fairness measures based on their probability mass functions under
  varying imbalance and group ratios.
---

# Properties of fairness measures in the context of varying class imbalance and protected group ratios

## Quick Facts
- arXiv ID: 2411.08425
- Source URL: https://arxiv.org/abs/2411.08425
- Authors: Dariusz Brzezinski; Julia Stachowiak; Jerzy Stefanowski; Izabela Szczech; Robert Susmaga; Sofya Aksenyuk; Uladzimir Ivashka; Oleksandr Yasinskyi
- Reference count: 40
- Primary result: Accuracy Equality and Statistical Parity are the most stable fairness measures across varying class imbalance and protected group ratios

## Executive Summary
This paper investigates how class imbalance and protected group ratios affect six popular group fairness measures. The authors define eight dataset-independent properties of fairness measures based on their probability mass functions under varying imbalance and group ratios. Their analysis reveals that Accuracy Equality and Statistical Parity are the most stable measures across different dataset characteristics, while Equal Opportunity and Predictive Equality become unstable with extreme class imbalance, and Positive/Negative Predictive Parity are the least reliable. A case study using the UCI Adult dataset and six classifiers confirms these theoretical findings, showing that Accuracy Equality and Statistical Parity maintain consistent fairness values while other measures exhibit significant variability with changing imbalance ratios.

## Method Summary
The study uses an exhaustive approach to generate all possible confusion matrices for a fixed dataset size (n=56), then calculates the values of six fairness measures for each matrix. The authors analyze probability mass functions of these measures under varying class imbalance ratios (IR) and protected group ratios (GR) to identify dataset-independent properties. For experimental validation, they sample controlled subsets from the UCI Adult dataset with varying IR and GR while maintaining equal size (n=1100), then train six classifiers using default scikit-learn parameters with 67%/33% stratified splits across 50 repetitions.

## Key Results
- Accuracy Equality and Statistical Parity maintain stable distributions and high resolution across varying IR and GR
- Equal Opportunity and Predictive Equality have few unique values and higher chances of perfect fairness under extreme imbalance
- Positive and Negative Predictive Parity are the least stable, with very few unique values and high chances of perfect fairness under imbalance
- The probability of achieving perfect fairness is not constant across measures and varies significantly with class imbalance and protected group ratios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fairness measure stability can be predicted from confusion matrix probability distributions under varying imbalance ratios.
- Mechanism: The paper defines eight dataset-independent properties of fairness measures based on their probability mass functions (pmfs) under all possible confusion matrices. By exhaustively generating all possible confusion matrices for a fixed dataset size, the authors compute the pmfs of six popular fairness measures for varying IR and GR. This allows them to identify properties like "Immunity to IR changes" or "Resolution Stability" without relying on any specific dataset or classifier.
- Core assumption: All possible confusion matrices are equally probable, providing a comprehensive view of fairness measure behavior.

### Mechanism 2
- Claim: Certain fairness measures are more robust to class imbalance than others, as revealed by their pmf distributions.
- Mechanism: The analysis of pmfs shows that Accuracy Equality and Statistical Parity maintain stable distributions and high resolution across varying IR and GR. In contrast, Equal Opportunity and Predictive Equality have few unique values and higher chances of perfect fairness under extreme imbalance. Positive and Negative Predictive Parity are the least stable, with very few unique values and high chances of perfect fairness under imbalance.
- Core assumption: The properties derived from pmf analysis translate to real-world classifier performance.

### Mechanism 3
- Claim: The probability of achieving perfect fairness is not constant across fairness measures and varies with class imbalance and protected group ratios.
- Mechanism: The paper introduces the "Perfect Fairness Stability" property, which measures how the probability of achieving a fairness measure value of 0 changes with varying IR and GR. The analysis reveals that Accuracy Equality and Statistical Parity have relatively constant probabilities of perfect fairness, while other measures have substantially higher chances of signaling perfect fairness under certain imbalance and group ratios.
- Core assumption: The probability of perfect fairness is a meaningful indicator of a measure's reliability.

## Foundational Learning

- Concept: Probability Mass Functions (PMFs)
  - Why needed here: PMFs are used to analyze the behavior of fairness measures under all possible confusion matrices, allowing the identification of dataset-independent properties.
  - Quick check question: What does a PMF represent, and how is it used to characterize the behavior of a discrete random variable?

- Concept: Confusion Matrices
  - Why needed here: Fairness measures are defined based on entries from confusion matrices, which represent the results of classification on experimental data. Understanding confusion matrices is essential for interpreting the fairness measures and their properties.
  - Quick check question: What are the four main entries of a confusion matrix, and how do they relate to true positives, false negatives, false positives, and true negatives?

- Concept: Class Imbalance Ratio (IR) and Protected Group Ratio (GR)
  - Why needed here: IR and GR are key factors that influence the behavior of fairness measures. IR represents the proportion of positive examples in the dataset, while GR represents the proportion of protected group examples. Understanding these ratios is crucial for interpreting the results and applying the findings to real-world scenarios.
  - Quick check question: How are IR and GR defined mathematically, and what do they represent in the context of fairness analysis?

## Architecture Onboarding

- Component map: Data Generation -> Fairness Measure Calculation -> PMF Analysis -> Property Verification -> Experimental Validation
- Critical path:
  1. Generate all possible confusion matrices for a fixed dataset size
  2. Compute the values of six fairness measures for each confusion matrix
  3. Analyze the PMFs of the fairness measures under varying IR and GR
  4. Verify whether the fairness measures possess the defined dataset-independent properties
  5. Validate the predicted properties on a real-world dataset using various classifiers

- Design tradeoffs:
  - Exhaustive vs. Sampled Analysis: Exhaustively generating all possible confusion matrices provides a comprehensive view but is computationally expensive. Sampling a subset of confusion matrices would be faster but might miss important properties.
  - Equal vs. Weighted Probability: Assuming equal probability for all confusion matrices simplifies the analysis but might not reflect real-world scenarios where some confusion matrices are more likely than others.
  - Fixed vs. Variable Dataset Size: Analyzing a fixed dataset size (n=56) allows for exhaustive generation but might not generalize to larger datasets.

- Failure signatures:
  - Inconsistent Properties: If the predicted properties do not hold for the real-world dataset, it indicates a problem with the assumptions or the analysis method.
  - High Computational Cost: If the exhaustive generation of confusion matrices becomes too slow for larger datasets, it suggests a need for optimization or approximation techniques.
  - Numerical Instability: If the fairness measures produce undefined values for many confusion matrices, it indicates a need for robust handling of edge cases.

- First 3 experiments:
  1. Generate all possible confusion matrices for n=56 and compute the PMFs of the six fairness measures under varying IR and GR.
  2. Verify whether the fairness measures possess the defined dataset-independent properties based on their PMFs.
  3. Validate the predicted properties on the UCI Adult dataset using six different classifiers and varying IR and GR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do class imbalance and protected group ratios affect individual fairness measures, which were not analyzed in this study?
- Basis in paper: [inferred] The paper explicitly focuses on group fairness measures and acknowledges that other fairness measures (e.g., individual fairness) exist but require different analysis methods.
- Why unresolved: The study's methodology was specifically designed for group fairness measures based on confusion matrices, which cannot be directly applied to individual fairness metrics that focus on similarity-based comparisons between individuals.
- What evidence would resolve it: An empirical study applying the authors' probability mass function analysis method to individual fairness measures, or a theoretical framework explaining how class imbalance affects similarity-based fairness metrics.

### Open Question 2
- Question: What would be the effect of analyzing more complex data scenarios beyond binary classes and binary protected groups?
- Basis in paper: [explicit] The authors acknowledge this limitation, stating that "The analysis of multiple classes and multiple groups using our approach would be possible but would require analyzing the data using a one-vs-all strategy."
- Why unresolved: The current study's exhaustive analysis of all possible confusion matrices becomes computationally infeasible as the number of classes and groups increases beyond two.
- What evidence would resolve it: A scalable computational method for analyzing fairness measure properties in multi-class and multi-group settings, or empirical validation showing how the identified properties generalize to more complex scenarios.

### Open Question 3
- Question: How would different assumptions about classifier performance affect the probability distributions of fairness measures?
- Basis in paper: [explicit] The authors acknowledge this by stating "one may argue that not all confusion matrices are equally probable" and note that their assumption treats all confusion matrices as equally probable because "this approach relies on fewer assumptions."
- Why unresolved: The current analysis assumes uniform probability over all possible confusion matrices, but in practice some confusion matrices are more likely than others depending on classifier quality and domain constraints.
- What evidence would resolve it: A parameterized analysis showing how varying the probability distribution over confusion matrices affects the identified fairness measure properties, or empirical validation on multiple datasets with known classifier performance distributions.

## Limitations
- The study relies on exhaustive confusion matrix enumeration which becomes computationally prohibitive for larger datasets
- The assumption that all confusion matrices are equally probable may not reflect real-world classifier performance distributions
- The analysis is limited to binary classification with a single binary protected attribute, restricting generalizability to multi-class or intersectional fairness scenarios
- Default classifier parameters without optimization may mask performance differences that could emerge with tuned models

## Confidence

**High Confidence**: The theoretical properties of Accuracy Equality and Statistical Parity being more stable under class imbalance, as this follows directly from their mathematical definitions and is confirmed through both theoretical analysis and experimental validation.

**Medium Confidence**: The claim that Positive/Negative Predictive Parity are the least reliable measures, as while the theoretical analysis shows instability, real-world classifier behavior may deviate from the equal probability assumption.

**Low Confidence**: The exact probability values for achieving perfect fairness under different IR and GR combinations, as these depend heavily on the assumption of uniform confusion matrix distribution which may not reflect actual classifier performance.

## Next Checks
1. **Robustness Testing**: Validate findings using a larger dataset (n>1000) with sampled confusion matrices rather than exhaustive enumeration to test scalability of the approach.
2. **Multiple Protected Attributes**: Extend analysis to datasets with intersectional protected groups (e.g., sex Ã— race combinations) to assess how properties generalize to multi-dimensional fairness.
3. **Classifier Sensitivity**: Repeat experiments with hyperparameter-optimized classifiers to determine if the observed fairness measure stability holds when model performance is maximized rather than using default parameters.