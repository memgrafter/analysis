---
ver: rpa2
title: Enhancing Large Language Model-based Speech Recognition by Contextualization
  for Rare and Ambiguous Words
arxiv_id: '2408.08027'
source_url: https://arxiv.org/abs/2408.08027
tags:
- keywords
- audio
- japanese
- yodas
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an approach to improve speech recognition for
  rare and ambiguous words by leveraging a large language model (LLM) decoder and
  providing keywords as contextual information. The method uses an in-house LLM decoder
  (PLaMo-100B) combined with a Whisper encoder, where audio features are projected
  to text embedding space and concatenated with text embeddings from keyword prompts.
---

# Enhancing Large Language Model-based Speech Recognition by Contextualization for Rare and Ambiguous Words

## Quick Facts
- arXiv ID: 2408.08027
- Source URL: https://arxiv.org/abs/2408.08027
- Reference count: 10
- Primary result: LLM-based ASR with keyword contextualization achieves up to 11.15% relative CER reduction and 53.98% KWER reduction for rare/ambiguous words

## Executive Summary
This paper presents a method to improve speech recognition for rare and ambiguous words by leveraging a large language model (LLM) decoder with keyword-based contextualization. The approach uses an in-house LLM decoder (PLaMo-100B) combined with a Whisper encoder, where audio features are projected to text embedding space and concatenated with text embeddings from keyword prompts. The key contribution is demonstrating that providing keywords in prompts significantly improves recognition performance for rare and ambiguous words, as shown by reduced character error rates (CERs) and keyword error rates (KWERs) on Japanese datasets.

## Method Summary
The system uses a Whisper large-v3 encoder to extract audio features, which are then projected to text embedding space through a linear adapter layer. These projected audio embeddings are concatenated with text embeddings from keyword prompts and fed into the PLaMo-100B LLM decoder. The model is fine-tuned using QLoRA on four datasets: CommonVoice v8.0/16.1 (Japanese), LibriSpeech (English), ReazonSpeech (Japanese), and YODAS (Japanese). Keywords for YODAS are generated using Japanese Stable LM Base Gamma 7B. Training uses AdamW optimizer with learning rate 3.5e-6, β2=0.95, and cosine annealing scheduler for 1 epoch across 72 H100 GPUs.

## Key Results
- CERs for YODAS dev/test sets improved by 7.87% (12.45% to 11.47%) and 11.15% (10.67% to 9.48%) respectively with keyword prompts
- KWERs for YODAS improved by 53.98% (50.25% to 23.13%) with keyword prompts
- Relative error rate reductions of up to 11.15% for CERs and 53.98% for KWERs compared to systems without keyword prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing keywords in prompts helps the LLM-based ASR system distinguish homonyms and domain-specific terms during transcription.
- Mechanism: The keywords are converted to text embeddings and concatenated with projected audio embeddings. This combined input provides additional semantic context that the LLM decoder uses to resolve ambiguous audio signals into the correct textual representation.
- Core assumption: The LLM decoder can effectively integrate keyword context with audio features to improve transcription accuracy for rare and ambiguous words.
- Evidence anchors:
  - [abstract]: "By providing keywords as prior information in the text prompts, we can contextualize our LLM-based ASR system without modifying the model architecture to transcribe ambiguous words in the input audio accurately."
  - [section]: "Specifically, some languages have many homonyms, such as Japanese and Chinese (especially when dropping tones); individual words or phrases provided as prior information help LLMs to distinguish such homonyms."
- Break condition: If the keywords provided do not match the actual content of the audio, or if the keywords themselves are ambiguous, the system may perform worse than without keywords.

### Mechanism 2
- Claim: The adapter layer projects audio embeddings to the text embedding space, enabling compatibility between the audio encoder and LLM decoder.
- Mechanism: Audio features from the Whisper encoder are projected through a linear adapter layer to match the dimensionality of text embeddings. These projected audio embeddings are then concatenated with keyword text embeddings as input to the LLM decoder.
- Core assumption: The linear adapter layer can effectively map audio features to the text embedding space without significant information loss.
- Evidence anchors:
  - [section]: "Since the dimensionality of the original audio features extracted by the encoder does not match with text embeddings in the decoder, we introduce a linear adapter layer to project the original audio features to a text embeddings space."
- Break condition: If the adapter layer cannot adequately capture the relevant audio information in the projected space, or if the projection introduces significant distortion, transcription quality will degrade.

### Mechanism 3
- Claim: The contextualization approach avoids catastrophic forgetting of English language capabilities while improving Japanese performance.
- Mechanism: By including English datasets (LibriSpeech) in the training data alongside Japanese datasets, the model maintains English ASR performance while gaining Japanese capabilities through the multilingual pre-training and fine-tuning process.
- Core assumption: Including English data during fine-tuning is sufficient to prevent the model from losing English ASR capabilities.
- Evidence anchors:
  - [section]: "Since our decoders and the audio encoder were pre-trained on at least Japanese and English datasets, we included an English dataset as one of four datasets to avoid catastrophic forgetting in the English domain."
- Break condition: If the English dataset is too small relative to Japanese data, or if the model overfits to Japanese, catastrophic forgetting of English may still occur.

## Foundational Learning

- Concept: Embedding spaces and their alignment
  - Why needed here: Understanding how audio features and text embeddings are projected to a common space is critical for grasping the adapter layer's role
  - Quick check question: Why can't we directly feed audio features to an LLM decoder without projection?

- Concept: Prompt engineering and contextualization
  - Why needed here: The effectiveness of the system relies on how well keywords are integrated into prompts to provide useful context
  - Quick check question: What makes a good keyword for contextualization versus a poor one?

- Concept: Catastrophic forgetting in multi-task learning
  - Why needed here: The paper addresses this by including English data during Japanese-focused fine-tuning
  - Quick check question: What strategies can prevent a model from forgetting previously learned tasks when learning new ones?

## Architecture Onboarding

- Component map: Whisper encoder -> Adapter layer -> Concat with keyword embeddings -> PLaMo-100B decoder -> Transcription

- Critical path: Audio → Whisper encoder → Adapter → Concat with keyword embeddings → PLaMo-100B decoder → Transcription

- Design tradeoffs:
  - Using pre-trained components vs. training from scratch (speed vs. customization)
  - Concatenating vs. other fusion methods for multimodal input
  - Keyword length limits (max 300 tokens) vs. comprehensive context

- Failure signatures:
  - Loss spikes during fine-tuning (addressed by reducing β2 in AdamW optimizer)
  - Degradation when keywords don't match audio content
  - Performance drop when using placeholder keywords instead of real ones

- First 3 experiments:
  1. Test transcription quality with and without keywords on a held-out dataset to verify the contextualization benefit
  2. Compare adapter layer projection quality by visualizing audio and text embeddings in the same space
  3. Evaluate catastrophic forgetting by testing English performance before and after Japanese fine-tuning

## Open Questions the Paper Calls Out

- Question: How would the ASR performance scale with larger model sizes beyond 100B parameters for PLaMo, particularly for rare and ambiguous word recognition?
- Question: What is the optimal balance between keyword quantity and quality for maximizing ASR performance on rare and ambiguous words?
- Question: How does the proposed keyword-based contextualization approach compare to traditional biasing techniques like shallow fusion or Trie-based biasing in terms of computational efficiency and recognition accuracy?

## Limitations

- Limited to Japanese datasets, with unclear generalizability to other languages
- Performance gains are dataset-dependent, with most significant improvements on proprietary YODAS dataset
- Catastrophic forgetting mitigation claims are not empirically validated with direct comparisons

## Confidence

- High Confidence: Core technical implementation of adapter-based projection and concatenation
- Medium Confidence: Generalization claims to languages other than Japanese
- Low Confidence: Catastrophic forgetting mitigation claims

## Next Checks

1. Test the keyword contextualization approach on a non-Japanese language dataset to verify generalizability
2. Systematically evaluate the impact of keyword relevance by testing with relevant, irrelevant, and no keywords
3. Compare keyword-based LLM contextualization against shallow fusion and Trie-based biasing on the same test sets