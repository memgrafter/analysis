---
ver: rpa2
title: Session Context Embedding for Intent Understanding in Product Search
arxiv_id: '2406.01702'
source_url: https://arxiv.org/abs/2406.01702
tags:
- query
- session
- embedding
- previous
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving user intent understanding
  in product search by leveraging session context rather than relying solely on single
  query-item pairs. The authors propose a novel session embedding approach that incorporates
  previous queries and engaged items into the embedding model to capture the customer
  journey within a session.
---

# Session Context Embedding for Intent Understanding in Product Search

## Quick Facts
- arXiv ID: 2406.01702
- Source URL: https://arxiv.org/abs/2406.01702
- Authors: Navid Mehrdad; Vishal Rathi; Sravanthi Rajanala
- Reference count: 16
- Primary result: Session embeddings incorporating previous queries and engaged items improve query product type classification by 2.5% weighted F1 score

## Executive Summary
This paper addresses the challenge of improving user intent understanding in product search by leveraging session context rather than relying solely on single query-item pairs. The authors propose a novel session embedding approach that incorporates previous queries and engaged items into the embedding model to capture the customer journey within a session. This augmented session state vector is used for tasks like query product type classification. Experimental results demonstrate that incorporating broad-to-narrow query transitions in the session context significantly improves classification performance, achieving a 2.5% increase in weighted f1 score over current methods. Additionally, including attributes from previously added-to-cart items further enhances performance. The findings suggest that understanding the trajectory of user queries within a session is crucial for accurate intent recognition.

## Method Summary
The paper introduces a session embedding framework that augments traditional query-item relevance training by incorporating session context. The method uses light-weight LLMs (DeBERTa V3 small) to train embeddings that combine the current query with previous query and engaged item attributes. Session embeddings are constructed by identifying broad-to-narrow query transitions through token match correlation, then combining this context with item attributes from engaged products (clicks, add-to-carts, orders). The model saves and updates session embeddings after each request, enabling their use for both retrieval and ranking tasks. The approach specifically filters query pairs to include only those with token matches between consecutive queries, focusing on maintaining relevance within the search funnel.

## Key Results
- Incorporating session context improves weighted F1 score from 82.92% to 85.42% for query product type classification
- Broad-to-narrow query transitions provide more relevant context than narrow-to-broad transitions
- Including attributes from previously added-to-cart items further enhances classification performance
- The session embedding approach outperforms current methods that ignore session context entirely

## Why This Works (Mechanism)
Session embeddings capture the user's search trajectory and evolving intent within a single session. By incorporating previous queries and engaged items, the model can understand how user intent narrows or shifts over time, which is particularly valuable for product search where users often start with broad categories and refine to specific products. The broad-to-narrow transition pattern reflects natural shopping behavior where users explore general categories before focusing on specific items. This temporal context provides richer signals than isolated query-item pairs, enabling better prediction of user intent at each stage of the search journey.

## Foundational Learning

**Session Context**: Understanding user behavior across multiple interactions within a single session. Why needed: Product search involves iterative refinement where users modify queries based on results. Quick check: Does the session contain multiple related queries showing intent evolution?

**Broad-to-Narrow Transitions**: User behavior pattern where queries start general and become more specific. Why needed: These transitions maintain semantic coherence and relevance for context modeling. Quick check: Are token matches decreasing while query specificity increases between consecutive queries?

**Token Match Correlation**: Metric measuring semantic similarity between consecutive queries. Why needed: Determines when to combine session context with current query for relevance. Quick check: What threshold (e.g., 0.5) best balances context relevance with noise reduction?

**Session State Vector**: Aggregated representation combining current query with previous session context. Why needed: Provides comprehensive context for intent prediction at each step. Quick check: Does the vector maintain relevant information while avoiding dilution from irrelevant context?

## Architecture Onboarding

**Component Map**: User Sessions -> Query Preprocessing -> Token Match Filtering -> Session Embedding Model -> Classification/Retrieval Tasks

**Critical Path**: The core workflow involves (1) session data collection, (2) token match filtering to identify relevant context, (3) session embedding generation using DeBERTa V3 small, and (4) downstream task application (classification/ranking). The token match criterion serves as the gatekeeper determining when session context should be incorporated.

**Design Tradeoffs**: The paper balances model complexity with performance by using light-weight LLMs rather than large models, trading some representational power for faster inference and easier deployment. The choice to filter queries based on token matches rather than including all session data reduces noise but may miss some relevant context. Using broad-to-narrow transitions prioritizes search funnel relevance over capturing all possible user behaviors.

**Failure Signatures**: Poor performance when including narrow-to-broad transitions, suggesting the model struggles with context reversal. Degradation when token match thresholds are too low, indicating noise from irrelevant query pairs. Inconsistent results across different product taxonomies may reveal domain-specific limitations.

**First Experiments**:
1. Train baseline DeBERTa V3 small model on single query-item pairs without session context
2. Implement token match filtering with varying thresholds (0.3, 0.5, 0.7) to identify optimal context inclusion criteria
3. Compare classification performance using broad-to-narrow vs narrow-to-broad query transitions

## Open Questions the Paper Calls Out

**Open Question 1**: Does the effectiveness of broad-to-narrow query transitions generalize across different e-commerce domains or product categories? The paper only uses one e-commerce dataset, so it's unclear if the observed pattern holds universally.

**Open Question 2**: How does session embedding perform for real-time ranking compared to its effectiveness in query classification tasks? The paper focuses on classification performance but mentions session embeddings can be used for "retrieval and ranking" without providing empirical results.

**Open Question 3**: What is the optimal session length and context window for maximizing the benefits of session embedding? The paper uses a fixed context window without exploring the trade-off between context richness and noise.

**Open Question 4**: How does session embedding handle multi-intent sessions where users switch between unrelated product categories? The paper mentions implementing a "token match criterion" to avoid linking irrelevant queries but doesn't evaluate how well this handles complex multi-intent scenarios.

## Limitations
- Specific implementation details for session state vector construction are not fully specified
- The exact product type taxonomy with 6k+ classes lacks detailed structure documentation
- The algorithmic identification of "broad-to-narrow" transitions versus manual curation is unclear
- Limited evaluation of session embeddings for ranking tasks beyond classification

## Confidence
- High confidence in the core finding that session context improves intent understanding
- Medium confidence in the specific implementation details for session state construction
- Medium confidence in the generalizability of the 2.5% improvement figure across different product search domains

## Next Checks
1. Implement the session embedding model with varying token match thresholds (0.3, 0.5, 0.7) to determine the optimal correlation cutoff for combining session context with current queries.

2. Test the approach on a different product taxonomy (e.g., 1k vs 6k classes) to evaluate whether the performance gains are consistent across different classification granularities.

3. Conduct ablation studies removing specific session context components (previous queries vs. engaged item attributes) to quantify their individual contributions to the 2.5% improvement.