---
ver: rpa2
title: 'Predict the Next Word: Humans exhibit uncertainty in this task and language
  models _____'
arxiv_id: '2402.17527'
source_url: https://arxiv.org/abs/2402.17527
tags:
- human
- distribution
- shown
- prob
- mass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate whether language models are well calibrated
  to human uncertainty in next-word prediction tasks by comparing the distributions
  of words predicted by models and humans. They find that language models such as
  GPT2, BLOOM, and ChatGPT exhibit low calibration to human uncertainty, meaning they
  do not reproduce the variability exhibited by humans in the next-word prediction
  task.
---

# Predict the Next Word: Humans exhibit uncertainty in this task and language models _____

## Quick Facts
- **arXiv ID**: 2402.17527
- **Source URL**: https://arxiv.org/abs/2402.17527
- **Reference count**: 40
- **Primary result**: Language models show poor calibration to human uncertainty in next-word prediction tasks

## Executive Summary
This paper investigates whether language models are well calibrated to human uncertainty in next-word prediction tasks by comparing the distributions of words predicted by models and humans. Using the Provo Corpus, the authors compare GPT2, BLOOM, and ChatGPT against human predictions across 2687 contexts. They find that all three models exhibit poor calibration to human uncertainty, with GPT2 showing the best (though still inadequate) performance. The study also reveals that expected calibration error (ECE) fails to capture this lack of calibration, advising against its use in this setting. The authors suggest that this inability stems from models not being consistently exposed to human production variability during training.

## Method Summary
The authors evaluate language model calibration to human uncertainty using the Provo Corpus, which contains 2687 contexts with human next-word predictions. They use three models (GPT2 Small, BLOOM-176B, and ChatGPT) and generate samples to estimate conditional probability distributions over complete words. For each context, they calculate the Total Variation Distance (TVD) between model and human distributions, then compute expected TVD as an overall calibration metric. They also calculate various ECE variants using different target distributions. The study investigates both surface-level and more abstract (syntactic and semantic) forms of variability to determine if models can capture human uncertainty at different levels of abstraction.

## Key Results
- Language models exhibit low calibration to human uncertainty in next-word prediction tasks
- GPT2 performs better than BLOOM and ChatGPT, but still shows poor calibration
- Expected Calibration Error (ECE) fails to reflect the lack of calibration to human uncertainty
- The inability to reproduce human variability likely stems from models not being exposed to human production variability during training

## Why This Works (Mechanism)
The paper demonstrates that language models, despite their impressive performance on many tasks, struggle to capture the inherent uncertainty present in human language production. When humans predict the next word in a sentence, they often exhibit significant variability in their responses, reflecting the ambiguity and flexibility of natural language. However, current language models tend to produce more deterministic predictions, failing to reproduce this human-like uncertainty. This discrepancy suggests that models are not learning the full distribution of human linguistic behavior, possibly because their training objectives and data do not adequately represent the variability present in natural language use.

## Foundational Learning
- **Total Variation Distance (TVD)**: A metric for comparing probability distributions, measuring the largest possible difference between the probabilities assigned to the same event by two different distributions. Why needed: Provides a direct way to quantify the difference between model and human prediction distributions. Quick check: TVD ranges from 0 (identical distributions) to 1 (completely different distributions).

- **Expected Calibration Error (ECE)**: A metric that measures how well a model's predicted probabilities match the true likelihood of outcomes. Why needed: Commonly used to evaluate whether models are well-calibrated, but the paper shows it fails in this context. Quick check: ECE is computed by binning predictions and comparing average confidence to accuracy within each bin.

- **Prompt engineering**: The practice of designing specific input prompts to elicit desired outputs from language models. Why needed: Different prompting strategies can significantly affect model outputs, especially for few-shot learning scenarios. Quick check: Compare model outputs using different prompts to assess sensitivity to prompt design.

## Architecture Onboarding

Component Map: Provo Corpus (contexts + human responses) -> Language Models (GPT2, BLOOM, ChatGPT) -> Sample Generation -> Word-level Probability Estimation -> TVD Calculation -> Calibration Metrics (TVD, ECE)

Critical Path: Data preparation -> Model sampling -> Probability estimation -> Metric calculation -> Analysis

Design Tradeoffs:
1. Sample size vs. computational cost: Generating more samples improves probability estimation but increases computational requirements
2. Token-level vs. word-level probabilities: Converting token probabilities to word-level probabilities introduces approximation errors
3. Surface-level vs. abstract variability: Evaluating different levels of linguistic abstraction provides complementary insights but increases complexity

Failure Signatures:
1. High TVD values indicate poor calibration to human uncertainty
2. ECE failing to capture calibration issues suggests metric inadequacy for this task
3. Significant differences between model performances may indicate architectural or training data effects

Three First Experiments:
1. Compare TVD values across different sample sizes to determine the point of diminishing returns
2. Evaluate the impact of different temperature scaling values on model calibration
3. Test alternative prompting strategies for ChatGPT to assess sensitivity to prompt design

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific training mechanisms or objectives could be implemented to improve language models' ability to reproduce human variability in next-word prediction?
- Basis in paper: The authors suggest that the inability of models to reproduce human variability stems from not being consistently exposed to human production variability during training, and they plan to investigate this further in future work.
- Why unresolved: The paper identifies the issue but does not propose specific solutions or training methods to address it.
- What evidence would resolve it: Experiments comparing different training approaches (e.g., data augmentation, exposure to diverse human responses) and their impact on calibration to human uncertainty would provide evidence for effective solutions.

### Open Question 2
- Question: How does the performance of language models in reproducing human variability change when evaluated across different languages or language families with varying morphological complexity?
- Basis in paper: The authors note that English has a relatively fixed word order and simple morphology, and suggest that other languages might exhibit greater variability due to their own typological features.
- Why unresolved: The paper only studies models trained for English, so the generalizability of findings to other languages remains unknown.
- What evidence would resolve it: Evaluating the same models and metrics on datasets from languages with different morphological and syntactic properties would reveal whether the observed calibration issues are language-specific or universal.

### Open Question 3
- Question: Can alternative evaluation metrics that go beyond exact word matching (e.g., semantic or syntactic similarity measures) provide a more accurate assessment of language models' ability to capture human variability?
- Basis in paper: The authors introduce syntactic TVD (TVDsyn) and semantic TVD (TVDsem) to investigate whether models can reproduce variability on a more abstract level, but observe similar trends to surface-level analysis.
- Why unresolved: While the authors explore these alternative metrics, they do not conclusively determine whether they offer a more meaningful evaluation of model performance.
- What evidence would resolve it: Comparing model rankings and calibration scores across surface-level, syntactic, and semantic metrics, and correlating these with human judgments of model quality, would clarify the utility of alternative evaluation approaches.

## Limitations
- The study relies on a single dataset (Provo Corpus) which may not capture the full range of human linguistic variability across different domains and contexts
- The exact prompting details for ChatGPT remain underspecified, which could affect reproducibility and comparability of results
- The computational demands of generating sufficient samples from large language models could introduce practical limitations and affect the feasibility of extensive experimentation

## Confidence
- Main claim (language models poorly calibrated to human uncertainty): **Medium** - supported by direct comparisons but dependent on specific experimental conditions
- ECE critique: **Medium** - logically sound but requires further validation with alternative metrics
- Training data hypothesis: **Low** - plausible but not empirically tested in this work

## Next Checks
1. Replicate the experiment using alternative human uncertainty datasets (e.g., from different languages or domains) to test generalizability
2. Conduct controlled experiments varying the diversity of training data for smaller language models to test the hypothesis about training data's impact on calibration
3. Implement and compare alternative calibration metrics (e.g., Jensen-Shannon divergence, Wasserstein distance) to validate the findings beyond TVD