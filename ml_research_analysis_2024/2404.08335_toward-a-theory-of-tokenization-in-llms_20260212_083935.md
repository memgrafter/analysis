---
ver: rpa2
title: Toward a Theory of Tokenization in LLMs
arxiv_id: '2404.08335'
source_url: https://arxiv.org/abs/2404.08335
tags:
- tokens
- which
- dict
- dictionary
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of tokenization in large language
  models (LLMs) through a theoretical lens, focusing on transformers trained on simple
  Markovian data generating processes. The authors observe that without tokenization,
  transformers trained on higher-order Markov processes tend to predict according
  to a unigram model, leading to poor cross-entropy loss.
---

# Toward a Theory of Tokenization in LLMs

## Quick Facts
- arXiv ID: 2404.08335
- Source URL: https://arxiv.org/abs/2404.08335
- Authors: Nived Rajaraman; Jiantao Jiao; Kannan Ramchandran
- Reference count: 40
- Key outcome: This paper investigates the role of tokenization in large language models (LLMs) through a theoretical lens, focusing on transformers trained on simple Markovian data generating processes. The authors observe that without tokenization, transformers trained on higher-order Markov processes tend to predict according to a unigram model, leading to poor cross-entropy loss. With appropriate tokenization, transformers achieve near-optimal cross-entropy loss by learning to model the underlying Markov distribution effectively. The study introduces theoretical results showing that tokenizers like LZW and a sequential variant of BPE can achieve near-optimal cross-entropy loss with much smaller dictionaries compared to a naive approach. The paper also explores the interaction between dictionaries and encoding algorithms, demonstrating that some dictionaries generalize well under one encoder but poorly under another. Experimental results support these findings, showing that tokenization improves both the convergence speed and limiting error of transformers on Markovian data.

## Executive Summary
This paper presents a theoretical investigation into the role of tokenization in large language models, specifically focusing on transformers trained on Markovian data generating processes. The authors demonstrate that tokenization is crucial for transformers to effectively model higher-order Markov processes, as without it, they tend to predict according to a unigram model, resulting in poor cross-entropy loss. The study introduces theoretical results showing that tokenizers like LZW and a sequential variant of BPE can achieve near-optimal cross-entropy loss with much smaller dictionaries compared to a naive approach. The findings highlight the importance of carefully considering the interaction between dictionaries and encoding algorithms when designing tokenization strategies for language models.

## Method Summary
The paper employs a theoretical approach to investigate the role of tokenization in transformers trained on simple Markovian data generating processes. The authors derive theoretical results showing that tokenizers like LZW and a sequential variant of BPE can achieve near-optimal cross-entropy loss with much smaller dictionaries compared to a naive approach. They also explore the interaction between dictionaries and encoding algorithms, demonstrating that some dictionaries generalize well under one encoder but poorly under another. Experimental results support these findings, showing that tokenization improves both the convergence speed and limiting error of transformers on Markovian data.

## Key Results
- Transformers trained on higher-order Markov processes without tokenization tend to predict according to a unigram model, leading to poor cross-entropy loss.
- With appropriate tokenization, transformers achieve near-optimal cross-entropy loss by effectively modeling the underlying Markov distribution.
- Tokenizers like LZW and a sequential variant of BPE can achieve near-optimal cross-entropy loss with much smaller dictionaries compared to a naive approach.
- The interaction between dictionaries and encoding algorithms is crucial, as some dictionaries generalize well under one encoder but poorly under another.

## Why This Works (Mechanism)
The mechanism behind the effectiveness of tokenization in transformers trained on Markovian data generating processes lies in the ability of tokenization to capture the underlying dependencies and structure of the data. By breaking down the input into meaningful tokens, transformers can better model the higher-order Markov dependencies, which would be challenging to capture directly from the raw sequence. The choice of tokenizer and dictionary size plays a crucial role in determining the quality of the tokenization, as some tokenizers may be more effective at capturing the relevant patterns and structure in the data compared to others.

## Foundational Learning
- **Markov Processes**: Understanding the concept of Markov processes is essential to grasp the theoretical framework of the paper. Markov processes are stochastic models where the future state depends only on the current state, not on the sequence of events that preceded it. This property is crucial for modeling language data and analyzing the performance of transformers.

  Quick check: Review the definition and properties of Markov processes, including first-order and higher-order Markov chains.

- **Cross-Entropy Loss**: Cross-entropy loss is a common metric used to evaluate the performance of language models. It measures the difference between the predicted probability distribution and the true distribution of the data. Minimizing cross-entropy loss is essential for training effective language models.

  Quick check: Understand the mathematical formulation of cross-entropy loss and its role in training language models.

- **Tokenization Algorithms**: The paper discusses specific tokenization algorithms, such as LZW (Lempel-Ziv-Welch) and BPE (Byte-Pair Encoding). These algorithms play a crucial role in breaking down the input data into meaningful tokens that can be effectively processed by transformers.

  Quick check: Familiarize yourself with the basic principles and working of LZW and BPE tokenization algorithms.

## Architecture Onboarding
- **Component Map**: The architecture consists of a transformer model, a tokenizer (LZW or sequential BPE), and a dictionary that maps tokens to indices. The transformer takes the tokenized input and generates predictions based on the learned parameters. The tokenizer breaks down the input into meaningful tokens using the dictionary.

  A -> B -> C (Input -> Tokenizer -> Transformer)

- **Critical Path**: The critical path in this architecture is the tokenization process. The choice of tokenizer and dictionary size directly impacts the quality of the tokenization, which in turn affects the performance of the transformer model. Optimizing the tokenization process is crucial for achieving near-optimal cross-entropy loss.

- **Design Tradeoffs**: One of the key design tradeoffs in this architecture is the balance between dictionary size and tokenization quality. Larger dictionaries can capture more fine-grained patterns but may also introduce noise and increase computational complexity. Smaller dictionaries are more efficient but may miss important patterns in the data.

- **Failure Signatures**: A potential failure mode in this architecture is the choice of an inappropriate tokenizer or dictionary size. If the tokenizer fails to capture the relevant patterns and structure in the data, the transformer model may struggle to learn the underlying Markov distribution effectively, leading to poor cross-entropy loss.

- **3 First Experiments**:
  1. Train a transformer model on a simple first-order Markov process with and without tokenization to observe the impact on cross-entropy loss.
  2. Compare the performance of LZW and sequential BPE tokenization algorithms on a higher-order Markov process.
  3. Investigate the effect of dictionary size on the cross-entropy loss and convergence speed of transformers trained on Markovian data.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical analysis relies on specific assumptions about the Markovian data generating processes that may not fully capture the complexity of real-world language data.
- The assumption of infinite data and perfect convergence may not hold in practical scenarios with finite training datasets and computational constraints.
- The study focuses on simple Markov processes, which may not adequately represent the long-range dependencies and compositional structure present in natural language.
- The experimental validation is limited to relatively small-scale synthetic datasets and may not generalize to larger, more complex language modeling tasks.
- The choice of specific tokenization algorithms (LZW and sequential BPE) may not fully represent the diversity of tokenization methods used in practice.

## Confidence
- High: The theoretical framework and mathematical proofs are sound and well-established within the context of the study.
- Medium: The experimental results support the theoretical claims, but the limited scope and scale of the experiments warrant cautious interpretation.
- Medium: The implications and potential applications of the findings to real-world language modeling tasks are plausible but require further investigation.

## Next Checks
1. Extend the experimental validation to larger-scale language modeling datasets and tasks, such as Wikitext-103 or PG-19, to assess the generalizability of the findings.
2. Investigate the impact of different tokenization algorithms (e.g., byte-pair encoding, word-piece tokenization) on the performance of transformers trained on Markovian and non-Markovian language data.
3. Explore the relationship between tokenization, model architecture, and learning dynamics in transformers, considering factors such as depth, width, and attention mechanisms.