---
ver: rpa2
title: Multilingual Instruction Tuning With Just a Pinch of Multilinguality
arxiv_id: '2401.01854'
source_url: https://arxiv.org/abs/2401.01854
tags:
- languages
- language
- tuning
- multilingual
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how multilinguality during instruction
  tuning affects instruction-following capabilities across languages. The study uses
  a multilingual LLM pre-trained on hundreds of languages and high-quality open-ended
  instructions and responses translated into 11 languages.
---

# Multilingual Instruction Tuning With Just a Pinch of Multilinguality

## Quick Facts
- arXiv ID: 2401.01854
- Source URL: https://arxiv.org/abs/2401.01854
- Authors: Uri Shaham; Jonathan Herzig; Roee Aharoni; Idan Szpektor; Reut Tsarfaty; Matan Eyal
- Reference count: 22
- Primary result: Multilingual instruction tuning with minimal multilinguality substantially improves cross-lingual instruction-following capabilities

## Executive Summary
This paper investigates how multilinguality during instruction tuning affects instruction-following capabilities across languages for a multilingual LLM. The study uses a PaLM 2-S model pre-trained on hundreds of languages and demonstrates that integrating only 40 multilingual examples into an English tuning set substantially improves multilingual instruction-following for both seen and unseen languages. The research shows that models tuned on multilingual mixtures exhibit comparable or superior performance across multiple languages compared to monolingually tuned models, despite training on 10x fewer examples in those languages.

## Method Summary
The method involves fine-tuning a multilingual PaLM 2-S model on instruction-response pairs from LIMA and OpenAssistant datasets, translated into 11 languages. The study compares monolingual instruction tuning (using data from a single language) against multilingual instruction tuning with varying language mixtures. Evaluation is performed using a side-by-side LLM judge approach with PaLM 2-L, measuring instruction-following scores across 12 languages including English. The experiments systematically vary the number of languages in the tuning set and the proportion of multilingual examples to understand their impact on cross-lingual generalization.

## Key Results
- Monolingual instruction tuning can transfer some instruction-following capabilities to other languages
- Only 40 multilingual examples integrated into an English tuning set substantially improve multilingual instruction-following for both seen and unseen languages
- Models tuned on multilingual mixtures exhibit comparable or superior performance in multiple languages compared to monolingually tuned models
- Diversifying the instruction tuning set with just 2-4 languages significantly improves cross-lingual generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual instruction tuning can improve cross-lingual generalization even when only a small fraction of examples are multilingual.
- Mechanism: Integrating a small number of multilingual examples into an English tuning set leverages the multilingual pre-training of the model to bootstrap instruction-following capabilities in other languages. The multilingual examples act as "anchors" that trigger the model's ability to transfer instruction-following from English to other languages.
- Core assumption: The multilingual pre-trained model has learned representations that allow it to transfer instruction-following capabilities across languages.
- Evidence anchors:
  - [abstract] "we find that only 40 multilingual examples integrated in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning."
  - [section 3.2] "The significant multilingual improvement comes from replacing only 1% of the English examples by multilingual ones, which translates to 40 examples evenly distributed across the training languages."

### Mechanism 2
- Claim: Increasing the number of languages in the tuning set can improve cross-lingual generalization to languages not seen during tuning.
- Mechanism: Training on a diverse set of languages allows the model to learn more robust instruction-following representations that can generalize to new languages. The diversity of languages in the tuning set exposes the model to a wider range of linguistic patterns and structures, which it can then use to infer instruction-following capabilities in new languages.
- Core assumption: The model's ability to generalize to new languages is improved by exposure to a diverse set of languages during instruction tuning.
- Evidence anchors:
  - [abstract] "we find that diversifying the instruction tuning set with even just 2-4 languages significantly improves cross-lingual generalization."
  - [section 3.4] "Adding languages to the tuning set improves cross-lingual generalization. The average score (red) increases from tuning on monolingual data to tuning on bilingual data, and even more when using 3 and 4 languages."

### Mechanism 3
- Claim: Monolingual instruction tuning can transfer some instruction-following capabilities to other languages.
- Mechanism: The multilingual pre-training of the model allows it to transfer instruction-following capabilities from the language it was tuned on to other languages. The model learns to associate certain linguistic patterns and structures with instruction-following, and can apply these associations to other languages even if it was not explicitly trained to do so.
- Core assumption: The multilingual pre-training of the model has imbued it with the ability to transfer instruction-following capabilities across languages.
- Evidence anchors:
  - [abstract] "We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning."
  - [section 3.1] "Most importantly, tuning using each single language yields a model with some multilingual instruction-following capabilities across languages."

## Foundational Learning

- Concept: Multilingual pre-training
  - Why needed here: The multilingual pre-training of the model is the foundation for the cross-lingual transfer of instruction-following capabilities. Without this pre-training, the model would not have the linguistic representations necessary to transfer across languages.
  - Quick check question: What is the difference between multilingual pre-training and monolingual pre-training, and why is it important for cross-lingual transfer?

- Concept: Instruction tuning
  - Why needed here: Instruction tuning is the process of fine-tuning a pre-trained model on pairs of instructions and corresponding responses. This process is crucial for teaching the model to follow instructions in a specific language, which it can then transfer to other languages.
  - Quick check question: What is the difference between instruction tuning and task-specific fine-tuning, and why is it important for building general-purpose language models?

- Concept: Cross-lingual transfer
  - Why needed here: Cross-lingual transfer is the ability of a model to apply knowledge learned in one language to another language. This is the key mechanism behind the findings of this paper, and is crucial for building multilingual instruction-following models.
  - Quick check question: What are some factors that can affect the degree of cross-lingual transfer, and how can they be controlled for in an experiment?

## Architecture Onboarding

- Component map: PaLM 2-S pre-trained model -> Instruction tuning datasets (monolingual and multilingual) -> Fine-tuned models -> Evaluation dataset -> LLM judge evaluation
- Critical path: Fine-tune PaLM 2-S on instruction-response pairs with varying language mixtures, then evaluate using LLM judge with PaLM 2-L
- Design tradeoffs: Between size of multilingual instruction tuning dataset and degree of cross-lingual transfer; between diversity of languages in tuning set and degree of cross-lingual generalization
- Failure signatures: Models not improving instruction-following capabilities in target languages after instruction tuning; models failing to transfer capabilities to new languages after training on diverse language set
- First 3 experiments:
  1. Train a model on the monolingual instruction tuning dataset in one language, and evaluate its performance on the evaluation dataset in that language and in the other 11 languages.
  2. Train a model on the multilingual instruction tuning dataset with a small number of examples (e.g. 40) in each language, and evaluate its performance on the evaluation dataset in all 12 languages.
  3. Train a model on the multilingual instruction tuning dataset with a larger number of examples (e.g. 100) in each language, and evaluate its performance on the evaluation dataset in all 12 languages.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the language-specific pre-training data fraction correlate with cross-lingual transfer performance beyond the weak Pearson correlation of 0.22 found in the paper?
- Basis in paper: [inferred] The paper found only a weak Pearson correlation of 0.22 between the average cross-lingual score of each language and the number of documents in that language in the pre-training corpus.
- Why unresolved: The paper only examined one correlation metric and did not explore other potential relationships between pre-training data distribution and transfer performance.
- What evidence would resolve it: Additional correlation analyses using different metrics or experimental manipulation of pre-training data distributions across languages could provide more insight.

### Open Question 2
- Question: How do the instruction-following capabilities of models tuned with a small multilingual dataset generalize to languages outside the top-50 languages in the pre-training corpus?
- Basis in paper: [explicit] The paper evaluated cross-lingual generalization for 11 languages selected from the top-50 languages in the pre-training corpus.
- Why unresolved: The study's focus on high-resource languages limits conclusions about the method's effectiveness for truly low-resource languages.
- What evidence would resolve it: Extending experiments to include evaluation on a diverse set of lower-resource languages would provide insights into broader applicability.

### Open Question 3
- Question: What is the impact of using naturally sourced instruction-response pairs versus translated data on cross-lingual instruction-following performance?
- Basis in paper: [explicit] The paper acknowledges using translated data for multilingual instruction tuning.
- Why unresolved: The reliance on translated data leaves open questions about how well findings would generalize to scenarios using naturally occurring multilingual instruction data.
- What evidence would resolve it: Conducting a parallel study using naturally sourced multilingual instruction data would allow for direct comparison.

## Limitations

- Evaluation methodology relies heavily on LLM judge evaluation, with limited human validation across all languages
- Findings are based on specific model (PaLM 2-S) and instruction datasets (LIMA and OpenAssistant), limiting generalizability
- The underlying mechanisms of cross-lingual transfer remain incompletely understood, particularly regarding optimal distribution of multilingual examples

## Confidence

**High confidence:** The core finding that multilingual instruction tuning improves cross-lingual instruction-following capabilities is well-supported by experimental results.

**Medium confidence:** The specific claim about 40 multilingual examples being sufficient is demonstrated but may vary with different models and datasets. The claim about 2-4 languages being sufficient for good generalization is supported but could benefit from additional exploration.

**Low confidence:** The precise mechanisms underlying cross-lingual transfer remain unclear, and the extent to which results depend on specific model architecture and pre-training is not definitively established.

## Next Checks

**Validation check 1:** Conduct comprehensive human evaluation across all 12 languages to validate LLM judge results, assessing correlation and identifying potential systematic biases in automated evaluation.

**Validation check 2:** Test the robustness of the "40 multilingual examples" finding by varying the specific instructions used and their language distribution to determine whether the effect depends on instruction characteristics.

**Validation check 3:** Evaluate models on languages completely unseen during any training phase to test true generalization capabilities and determine whether improvements extend to languages sharing linguistic features with training languages.