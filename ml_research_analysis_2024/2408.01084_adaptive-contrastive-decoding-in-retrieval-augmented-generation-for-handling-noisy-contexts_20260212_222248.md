---
ver: rpa2
title: Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling
  Noisy Contexts
arxiv_id: '2408.01084'
source_url: https://arxiv.org/abs/2408.01084
tags:
- context
- knowledge
- decoding
- micdd
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Adaptive Contrastive Decoding (ACD) to handle
  noisy contexts in retrieval-augmented generation (RAG) for open-domain question
  answering. The core idea is to adaptively weight the influence of contextual knowledge
  based on the model's uncertainty reduction when given the context.
---

# Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts

## Quick Facts
- **arXiv ID**: 2408.01084
- **Source URL**: https://arxiv.org/abs/2408.01084
- **Reference count**: 16
- **Primary result**: ACD improves robustness to noisy contexts in RAG systems while maintaining performance on gold contexts

## Executive Summary
This paper addresses the challenge of handling noisy contexts in retrieval-augmented generation (RAG) for open-domain question answering. The proposed Adaptive Contrastive Decoding (ACD) method dynamically adjusts the influence of retrieved context based on the model's uncertainty reduction, measured through entropy differences between predictions with and without context. ACD shows improved performance compared to baseline contrastive decoding methods, particularly in robustness to noisy contexts while maintaining strong performance on gold contexts across multiple datasets and models.

## Method Summary
ACD operates within the RAG framework by modifying the decoding process to include entropy-based adaptive weighting of contextual influence. For each decoding step, the method computes entropy values H(Yt) and H(Yc_t) for predictions with and without context, respectively, then calculates an adaptive weight αACD = H(Yt) / (H(Yc_t) + H(Yt)). This weight dynamically adjusts the contribution of contextual information to the final logits, allowing the model to rely more on its parametric knowledge when context is noisy and unhelpful, while leveraging helpful context when available.

## Key Results
- ACD achieves higher exact match (EM) scores across models and datasets compared to baselines
- ACD demonstrates improved robustness to noisy contexts while maintaining performance on gold contexts
- ACD outperforms fixed-weight contrastive decoding approaches like CAD and MICD, especially under noisy conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ACD dynamically adjusts contextual influence based on the model's uncertainty reduction
- Mechanism: ACD computes entropy differences between predictions with and without context, using these differences to weight the influence of external knowledge. When context reduces uncertainty, ACD increases contextual weight; when context increases uncertainty, ACD decreases it.
- Core assumption: Entropy is a reliable proxy for uncertainty in LLM predictions
- Evidence anchors:
  - [abstract]: "ACD dynamically adjusts the weight using entropy differences between predictions with and without context, allowing the model to rely more on its parametric knowledge when the context is noisy and unhelpful"
  - [section]: "We control contrastive contextual influence based on context's contribution to the LLM's uncertainty reduction"
  - [corpus]: Weak - no direct citations found in related papers
- Break condition: If entropy does not correlate well with actual uncertainty or if the model's internal state changes in ways that entropy doesn't capture

### Mechanism 2
- Claim: ACD provides robustness to noisy contexts while maintaining performance on gold contexts
- Mechanism: By adaptively weighting contextual influence, ACD prevents the model from being distracted by irrelevant or contradictory information, while still leveraging helpful context when available
- Core assumption: Noisy contexts typically increase model uncertainty while gold contexts decrease it
- Evidence anchors:
  - [abstract]: "ACD demonstrates improvements in open-domain question answering tasks compared to baselines, especially in robustness by remaining undistracted by noisy contexts"
  - [section]: "ACD achieves either the best or second-best performance... when analyzing the performance by dividing the data into two subsets based on whether the retrieved context is gold (SubsetGold) or not (SubsetNoisy)"
  - [corpus]: Weak - related work mentions similar goals but doesn't cite this specific approach
- Break condition: If noisy contexts sometimes reduce uncertainty (e.g., when they happen to align with model priors) or if gold contexts sometimes increase uncertainty

### Mechanism 3
- Claim: ACD outperforms fixed-weight contrastive decoding approaches
- Mechanism: Unlike fixed-weight methods that use static alpha values, ACD dynamically adjusts the weight based on context quality, allowing it to better handle the variability in real-world retrieval scenarios
- Core assumption: The optimal weighting between parametric and contextual knowledge varies depending on context quality
- Evidence anchors:
  - [section]: "While these approaches work well when context information is correct and faithful, in real-world scenarios, context information is not always correct and may contain some noisy and unfaithful information"
  - [section]: "Both CAD and MICDF exhibit a significant drop in their performance under noisy conditions"
  - [section]: "It is notable that both CAD and MICDF exhibit a significant drop in their performance under noisy conditions"
  - [corpus]: Weak - related papers discuss fixed vs dynamic weights but don't cite this specific approach
- Break condition: If fixed weights happen to work well for a specific dataset or if dynamic adjustment introduces instability

## Foundational Learning

- Concept: Entropy as a measure of uncertainty
  - Why needed here: ACD uses entropy differences to determine how much to trust contextual information
  - Quick check question: If a model has high entropy for a token prediction, does this mean the model is uncertain about that token?

- Concept: Contrastive decoding
  - Why needed here: ACD builds on contrastive decoding by adding adaptive weighting to the standard contrastive approach
  - Quick check question: In contrastive decoding, how does the model combine its own predictions with contextual information?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: ACD operates within the RAG framework, where external context is retrieved and combined with model knowledge
  - Quick check question: What is the main advantage of using retrieval-augmented generation over relying solely on parametric knowledge?

## Architecture Onboarding

- Component map:
  - Retriever (CONTRIEVER -MSMARCO) -> LLM (Llama 2/3, Mistral) -> Entropy calculator -> Weight calculator -> Contrastive decoder

- Critical path:
  1. Retrieve context using CONTRIEVER -MSMARCO
  2. Generate logits with context (zc_t) and without context (zt)
  3. Compute entropy values H(Yt) and H(Yc_t)
  4. Calculate adaptive weight αACD
  5. Apply weighted adjustment to logits
  6. Select next token using adjusted probabilities

- Design tradeoffs:
  - Inference cost: ACD requires two forward passes (with and without context) vs one for standard decoding
  - Complexity: Adds entropy computation and weight calculation steps
  - Performance: Improves robustness to noise at the cost of increased computation
  - Flexibility: Dynamic weighting adapts to context quality vs fixed approaches

- Failure signatures:
  - Poor performance on gold contexts: May indicate αACD is being set too low for helpful contexts
  - No improvement on noisy contexts: May indicate entropy isn't effectively distinguishing noisy from helpful contexts
  - Unstable performance: May indicate the adaptive weighting is introducing instability

- First 3 experiments:
  1. Compare ACD performance on gold vs noisy contexts to verify robustness
  2. Analyze αACD values across different context quality levels to verify adaptive behavior
  3. Test ACD on knowledge conflict datasets (like NQ-swap) to verify it doesn't treat relevant but conflicting context as noise

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The entropy-based weighting mechanism relies on a simplifying assumption that entropy differences reliably indicate context quality
- Evaluation uses only one retrieval model (CONTRIEVER-MS MARCO) and focuses primarily on Wikipedia-based datasets
- The computational overhead of requiring two forward passes per decoding step could be prohibitive for larger models or real-time applications

## Confidence

**High Confidence**: The core claim that ACD outperforms fixed-weight contrastive decoding approaches (CAD, MICD) on noisy contexts is well-supported by the experimental results across multiple datasets and models. The performance gains on the noisy subset are consistent and substantial.

**Medium Confidence**: The claim that ACD maintains performance on gold contexts while improving on noisy ones is supported, but the trade-off analysis could be more thorough. The relative performance differences between subsets suggest robustness but don't definitively prove ACD never degrades performance on gold contexts.

**Low Confidence**: The generalizability of ACD to non-Wikipedia domains and different retrieval architectures remains untested. The paper's claims about ACD being broadly applicable to "noisy contexts" extend beyond the evaluation scope.

## Next Checks
1. Test ACD on non-Wikipedia datasets (e.g., scientific literature, news articles, or proprietary document collections) to assess generalizability beyond the evaluated domains
2. Evaluate ACD with different retrieval architectures (e.g., dense vs sparse retrievers, multi-hop retrieval) to verify the approach isn't specifically tuned to CONTRIEVER-MS MARCO's output characteristics
3. Design experiments specifically targeting knowledge conflicts where retrieved context contradicts parametric knowledge, to verify ACD correctly handles relevant but conflicting information rather than treating it as noise