---
ver: rpa2
title: K-means Derived Unsupervised Feature Selection using Improved ADMM
arxiv_id: '2411.15197'
source_url: https://arxiv.org/abs/2411.15197
tags:
- feature
- selection
- k-means
- features
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces K-means Derived Unsupervised Feature Selection
  (K-means UFS), a novel approach for unsupervised feature selection that focuses
  on selecting discriminative features based on the K-means clustering objective.
  Unlike existing spectral analysis-based methods, K-means UFS aims to minimize within-cluster
  differences and maximize between-cluster differences.
---

# K-means Derived Unsupervised Feature Selection using Improved ADMM

## Quick Facts
- arXiv ID: 2411.15197
- Source URL: https://arxiv.org/abs/2411.15197
- Reference count: 28
- This paper introduces K-means Derived Unsupervised Feature Selection (K-means UFS), a novel approach for unsupervised feature selection that focuses on selecting discriminative features based on the K-means clustering objective.

## Executive Summary
This paper introduces K-means Derived Unsupervised Feature Selection (K-means UFS), a novel approach for unsupervised feature selection that focuses on selecting discriminative features based on the K-means clustering objective. Unlike existing spectral analysis-based methods, K-means UFS aims to minimize within-cluster differences and maximize between-cluster differences. The authors develop an Alternating Direction Method of Multipliers (ADMM) algorithm to solve the resulting NP-hard optimization problem. Extensive experiments on real datasets demonstrate that K-means UFS outperforms state-of-the-art unsupervised feature selection methods in terms of clustering accuracy and Normalized Mutual Information (NMI), achieving the best performance across all tested datasets.

## Method Summary
K-means UFS solves an NP-hard optimization problem by relaxing the discrete selection constraint to an ℓ2,0-norm, enabling optimization via ADMM. The method uses the top-k singular vectors of the data matrix as an approximate K-means indicator to preserve data structure while enabling efficient feature selection. The approach constructs a feature selection matrix from the largest k singular values and iteratively updates selection matrices through ADMM to converge to a row-sparse solution representing selected features.

## Key Results
- K-means UFS outperforms state-of-the-art unsupervised feature selection methods on six real datasets
- Achieves best clustering accuracy and NMI scores across all tested datasets
- The ADMM algorithm converges to sparse solutions representing selected features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The K-means objective directly encodes cluster separability, unlike spectral methods that preserve local similarity.
- Mechanism: By minimizing within-cluster variance and implicitly maximizing between-cluster variance, the K-means UFS objective selects features that create the most compact and well-separated clusters.
- Core assumption: The K-means objective is a good proxy for feature discriminativeness in unsupervised settings.
- Evidence anchors:
  - [abstract] "Unlike most existing spectral analysis based unsupervised feature selection methods, we select features using the objective of K-means."
  - [section] "We select features by minimizing the objective of K-means clustering... The goal of our method is to select the most discriminative features such that the data points are well separated, that is, have small within-cluster differences and large between-cluster differences."
- Break condition: If clusters are not spherical or the number of clusters is misspecified, the K-means objective may not reflect true separability.

### Mechanism 2
- Claim: Relaxing the discrete selection constraint to an ℓ2,0-norm enables optimization via ADMM while preserving the selection semantics.
- Mechanism: The ℓ2,0-norm constraint counts nonzero rows in the selection matrix, and its relaxation allows iterative ADMM updates that converge to a row-sparse solution representing selected features.
- Core assumption: The relaxed ℓ2,0-norm problem is well-approximated by the ℓ2,0 constraint and that ADMM can handle this non-convex constraint.
- Evidence anchors:
  - [section] "Now, from the K-means UFS model (8), if S* is an optimal solution, V = S*R (where R ∈ Rh×h is a rotation matrix, that is, RR⊤ = I.) is also an optimal solution... Therefore, the relaxed V has only two constraints: V ⊤V = I, ∥V ⊤∥2,0 = h"
  - [section] "We can recover a unique feature selection matrix S⊤ determined by the h nonzero row indices of W ⊤ because of W ⊤ = V ⊤ and V ⊤ = S⊤R."
- Break condition: If the relaxed problem admits solutions with many near-zero rows, the final selection may be unstable or depend heavily on the ADMM initialization.

### Mechanism 3
- Claim: Using the top-k singular vectors of the data matrix as an approximate K-means indicator preserves data structure while enabling efficient feature selection.
- Mechanism: The SVD-based approximation Qk ≈ G* allows replacing the NP-hard indicator optimization with a tractable spectral problem, where A = PkΣ²kPk is the projection onto the top-k principal subspace.
- Core assumption: The top-k right singular vectors of X capture the cluster structure well enough to approximate the K-means indicator matrix.
- Evidence anchors:
  - [section] "Let the singular value decomposition (SVD) of X be X = P ΣQ⊤... [22], [23] showed that Qk is a good approximation of G*."
  - [section] "where A = PkΣ²kPk is in fact the largest k rank of the square of data covariance matrix."
- Break condition: If the cluster structure is not aligned with the top-k singular vectors (e.g., clusters differ in variance or orientation), the approximation may fail to capture discriminative directions.

## Foundational Learning

- Concept: Alternating Direction Method of Multipliers (ADMM)
  - Why needed here: ADMM enables solving the non-convex K-means UFS problem by decomposing it into subproblems over V, U, and W that can be solved efficiently.
  - Quick check question: What is the augmented Lagrangian form of a problem with equality constraints V = U and V = W?

- Concept: Singular Value Decomposition (SVD) and Principal Component Analysis
  - Why needed here: The SVD provides the spectral decomposition used to approximate the K-means indicator matrix and construct the feature selection matrix A.
  - Quick check question: How does the SVD of X relate to the top-k principal subspace used in the K-means UFS objective?

- Concept: ℓ2,0-norm and its relaxation
  - Why needed here: The ℓ2,0-norm counts nonzero rows and enforces feature selection; relaxing it to a continuous constraint allows optimization while still yielding a sparse solution.
  - Quick check question: Why does the ℓ2,0-norm constraint ∥V⊤∥2,0 = h enforce that exactly h features are selected?

## Architecture Onboarding

- Component map: Data matrix X → SVD (P, Σ, Q) → K-means objective matrix A = PkΣ²kPk → ADMM iterations over V, U, W → h selected features (indices of nonzero rows in W)
- Critical path: SVD → A construction → ADMM initialization → iterative updates → convergence → feature extraction
- Design tradeoffs: Using K-means objective vs. spectral similarity → better separability but may be sensitive to k; relaxing ℓ2,0-norm → tractable optimization but potential instability; SVD approximation → efficiency vs. possible loss of discriminative information
- Failure signatures: Divergence in ADMM (V norm blow-up), poor clustering accuracy despite convergence, selected features not stable across runs, or sensitivity to number of clusters k
- First 3 experiments:
  1. Verify SVD-based A construction by checking that top-k singular values align with cluster separability
  2. Run ADMM on a small synthetic dataset and monitor convergence of V, U, W and ∥V∥F
  3. Compare clustering accuracy using selected features vs. all features on a real dataset to confirm effectiveness

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the paper.

## Limitations
- The method's performance depends heavily on the K-means clustering assumption - if clusters are not spherical or well-separated, the approach may fail to select truly discriminative features
- The relaxation of the ℓ2,0-norm constraint introduces approximation error that is not quantified
- The SVD-based approximation assumes cluster structure aligns with principal components, which may not hold for all datasets

## Confidence
- **High Confidence**: The theoretical derivation of the ADMM algorithm and the relaxation approach is sound and well-explained. The convergence analysis and update rules are clearly specified.
- **Medium Confidence**: The experimental results showing improved clustering accuracy and NMI are promising, but the comparison is limited to specific datasets and baseline methods. The generalization to other domains remains uncertain.
- **Low Confidence**: The claim that K-means UFS outperforms all state-of-the-art methods across all tested datasets should be viewed cautiously, as the paper does not provide statistical significance testing or analysis of performance variance across multiple runs.

## Next Checks
1. Test the method's sensitivity to the number of clusters k by varying k and measuring both feature selection stability and clustering performance across multiple runs.
2. Compare the selected features' interpretability and domain relevance on benchmark datasets where ground truth feature importance is known or can be assessed by domain experts.
3. Evaluate the method's robustness to noise and outliers by adding controlled perturbations to datasets and measuring degradation in both feature selection quality and clustering accuracy.