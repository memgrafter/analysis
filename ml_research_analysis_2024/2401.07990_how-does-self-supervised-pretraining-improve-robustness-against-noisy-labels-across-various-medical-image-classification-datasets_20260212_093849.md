---
ver: rpa2
title: How does self-supervised pretraining improve robustness against noisy labels
  across various medical image classification datasets?
arxiv_id: '2401.07990'
source_url: https://arxiv.org/abs/2401.07990
tags:
- label
- labels
- noisy
- noise
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how self-supervised pretraining enhances
  robustness against noisy labels in medical image classification. The research addresses
  three key questions: how label noise impacts various medical image datasets, which
  datasets are more challenging and affected by noise, and how different self-supervised
  pretraining methods improve robustness.'
---

# How does self-supervised pretraining improve robustness against noisy labels across various medical image classification datasets?

## Quick Facts
- arXiv ID: 2401.07990
- Source URL: https://arxiv.org/abs/2401.07990
- Reference count: 16
- Self-supervised pretraining significantly improves robustness against noisy labels in medical image classification

## Executive Summary
This study investigates how self-supervised pretraining enhances robustness against noisy labels in medical image classification across five diverse datasets. The research systematically evaluates eight self-supervised pretraining methods and two learning-with-noisy-labels (LNL) algorithms to understand their effectiveness against both symmetrical and class-dependent label noise. The findings reveal that contrastive learning-based methods (MoCo v2, Barlow Twins, SimCLR) are most effective at improving robustness, while DermNet, despite being the most challenging dataset, exhibits greater resilience to noisy labels than other datasets.

## Method Summary
The study employs a two-stage pipeline: self-supervised pretraining followed by supervised training using LNL methods. Five medical image datasets were used with varying characteristics (modalities, class counts, sizes). The self-supervised pretraining phase applied eight different methods including contrastive learning (MoCo v2, Barlow Twins, SimCLR), pretext tasks (Rotation Prediction, Jigsaw Puzzle, Jigmag Puzzle), and generative approaches (VAE, BigBiGAN). The supervised training phase used either Co-teaching or DivideMix algorithms to handle noisy labels. Both symmetrical and class-dependent label noise were synthetically injected into the datasets, and performance was evaluated using test F1-score across different noise rates.

## Key Results
- Contrastive learning methods (MoCo v2, Barlow Twins, SimCLR) proved most effective among eight self-supervised pretraining approaches
- DermNet dataset, while identified as most challenging, exhibited greater robustness against noisy labels than other datasets
- Self-supervised pretraining provided more benefits with symmetrical label noise than with class-dependent noise
- Combining self-supervised pretraining with ImageNet pretraining further improved robustness when domain gaps were minimal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised pretraining provides more robust feature representations that are less susceptible to overfitting noisy labels during subsequent supervised training.
- Mechanism: The self-supervised pretraining phase learns meaningful feature representations from the data itself, without relying on potentially noisy labels. These robust features serve as a strong initialization for the subsequent supervised training, helping the model resist learning from label noise.
- Core assumption: The features learned during self-supervised pretraining are inherently more noise-robust than features learned directly from noisy labels.
- Evidence anchors:
  - [abstract] "Self-supervised pretraining, which doesn't rely on labeled data, can enhance robustness against noisy labels."
  - [section] "However, a significant drawback of such approach is the model's struggle to learn robust features in the initial phaseâ€”a challenge termed the warm-up obstacle (Zheltonozhskii et al., 2022). Recent solutions address this challenge by jointly using self-supervised learning, which doesn't rely on labels, and supervised learning with noisy labels (Li et al., 2022; Ju et al., 2022; Xue et al., 2022a)."
  - [corpus] Weak correlation found; neighbor papers focus on noise robustness but do not directly address the mechanism of self-supervised pretraining's role.

### Mechanism 2
- Claim: Contrastive learning-based pretraining methods are particularly effective at improving robustness against noisy labels in medical image classification.
- Mechanism: Contrastive learning methods learn to distinguish between similar and dissimilar samples, forcing the model to learn discriminative features that are less sensitive to label noise.
- Core assumption: The discriminative features learned through contrastive learning are more robust to label noise than features learned through other self-supervised methods.
- Evidence anchors:
  - [abstract] "Among eight self-supervised methods tested, contrastive learning (MoCo v2, Barlow Twins, SimCLR) proved most effective."
  - [section] "In this study, we focused on understanding the impact of noisy labels in medical image classification and how self-supervised pretraining enhances robustness against it... Our results show that DermNet, among five datasets (Fetal plane, DermNet, COVID-DU-Ex, MURA, NCT-CRC-HE-100K), is the most challenging but exhibits greater robustness against noisy labels. Additionally, contrastive learning stands out among the eight self-supervised methods as the most effective approach to enhance robustness against noisy labels."
  - [corpus] No direct evidence found in neighbors; this is a novel finding from the paper.

### Mechanism 3
- Claim: Self-supervised pretraining provides more benefits with symmetrical label noise than with class-dependent label noise.
- Mechanism: Symmetrical label noise affects all classes equally, while class-dependent noise creates specific patterns of mislabeling. The features learned during self-supervised pretraining may be more robust to the uniform corruption of symmetrical noise.
- Core assumption: The robustness of self-supervised features is more effective against uniform noise corruption than against structured, class-dependent noise.
- Evidence anchors:
  - [section] "In summary, we observed that contrastive learning-based is the best approach to pretrain the model for improving robustness against noisy labels in medical image classification, and self-supervised training with in-domain data is better than the ImageNet pretrained model if the medical images have a large domain gap with ImageNet or have a larger dataset size."
  - [section] "Finally, as discussed in Section 6.3, we demonstrated that the most effective strategy is to apply self-supervised pretraining on top of a model pretrained on a large out-of-domain dataset like ImageNet if the domain gap between out-of-domain dataset and experimental dataset is minimum."
  - [corpus] No direct evidence found in neighbors; this is a novel finding from the paper.

## Foundational Learning

- Concept: Learning with Noisy Labels (LNL)
  - Why needed here: The study investigates how self-supervised pretraining improves robustness against noisy labels in medical image classification.
  - Quick check question: What are the two main types of label noise discussed in the paper, and how do they differ?

- Concept: Self-supervised Learning
  - Why needed here: The paper focuses on how self-supervised pretraining can enhance robustness against noisy labels.
  - Quick check question: What are the three main categories of self-supervised learning methods explored in the study?

- Concept: Contrastive Learning
  - Why needed here: The study finds that contrastive learning-based pretraining is particularly effective at improving robustness against noisy labels.
  - Quick check question: How does contrastive learning differ from other self-supervised learning approaches in terms of the learning objective?

## Architecture Onboarding

- Component map: Self-supervised pretraining phase (contrastive learning, pretext tasks, generative methods) -> Supervised LNL training phase (Co-teaching or DivideMix) -> Classification output
- Critical path: The self-supervised pretraining phase is the most critical path, as the quality of the learned features directly impacts the robustness of the subsequent supervised training.
- Design tradeoffs: The choice of self-supervised pretraining method involves tradeoffs between computational cost, data requirements, and effectiveness. Contrastive learning methods, while effective, can be computationally expensive.
- Failure signatures: If the self-supervised pretraining phase fails to learn meaningful features, the subsequent supervised training will be susceptible to label noise. Overfitting during pretraining can also lead to poor generalization.
- First 3 experiments:
  1. Train a model from scratch on a noisy medical image dataset using a standard supervised learning approach (e.g., cross-entropy loss) to establish a baseline performance.
  2. Pretrain a model using a self-supervised method (e.g., SimCLR) on the same dataset and then fine-tune it using a LNL method (e.g., Co-teaching) to assess the improvement in robustness.
  3. Compare the performance of different self-supervised pretraining methods (e.g., contrastive learning vs. pretext tasks) in combination with LNL methods to identify the most effective approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal self-supervised pretraining method vary across different medical imaging modalities (e.g., X-ray vs. ultrasound vs. histopathology)?
- Basis in paper: [explicit] The paper tested five datasets across X-ray, ultrasound, and RGB images, finding contrastive learning most effective overall, but with ImageNet pretraining outperforming in DermNet (RGB camera images) while in-domain contrastive learning was superior for Fetal (ultrasound) data.
- Why unresolved: The study focused on a limited set of datasets and did not systematically investigate the relationship between imaging modality characteristics and optimal pretraining strategy across a broader range of medical imaging types.
- What evidence would resolve it: Systematic testing of self-supervised pretraining methods across a diverse range of medical imaging modalities with varying domain gaps to ImageNet, comparing performance and identifying modality-specific optimal approaches.

### Open Question 2
- Question: What is the relationship between dataset size and the effectiveness of self-supervised pretraining for improving robustness against noisy labels?
- Basis in paper: [inferred] The paper noted that DermNet, with a smaller dataset size, showed better performance with ImageNet pretraining compared to in-domain self-supervised pretraining, suggesting dataset size may influence pretraining effectiveness.
- Why unresolved: The study did not systematically vary dataset sizes across multiple datasets to quantify the relationship between dataset size and the benefits of self-supervised pretraining.
- What evidence would resolve it: Controlled experiments varying dataset sizes across multiple datasets while keeping other factors constant, measuring the impact on pretraining effectiveness for improving robustness against noisy labels.

### Open Question 3
- Question: How do transformer-based architectures compare to CNN-based backbones (like ResNet18) in terms of robustness against noisy labels when using self-supervised pretraining?
- Basis in paper: [explicit] The paper explicitly states it used only CNN-based backbones (ResNet18) to maintain experiment manageability and ensure fair comparisons, noting that transformers' behavior against label noise likely differs from CNNs.
- Why unresolved: The study was limited to CNNs, leaving open the question of how transformer architectures would perform in this context.
- What evidence would resolve it: Direct comparison of CNN and transformer backbones using the same self-supervised pretraining methods and LNL approaches, measuring and comparing robustness against noisy labels across multiple datasets.

## Limitations

- The study relies on controlled synthetic noise injection rather than real-world label errors, which may limit practical applicability
- The domain gap analysis between ImageNet and medical datasets is limited to five specific datasets, which may not generalize to all medical imaging domains
- The comparison between self-supervised methods is comprehensive but does not explore hybrid approaches that combine multiple pretraining strategies

## Confidence

- **High confidence**: Contrastive learning methods (MoCo v2, Barlow Twins, SimCLR) are most effective for robustness - supported by consistent results across multiple datasets and noise types
- **Medium confidence**: Symmetrical noise benefits more from self-supervised pretraining than class-dependent noise - observed pattern needs more extensive validation across diverse dataset structures
- **Medium confidence**: DermNet exhibits greater robustness despite being most challenging - counterintuitive finding requires deeper investigation of dataset characteristics

## Next Checks

1. Test the effectiveness of self-supervised pretraining on datasets with real-world label noise rather than synthetically injected noise to validate practical applicability
2. Evaluate hybrid pretraining approaches that combine contrastive learning with pretext tasks or generative methods to determine if combined strategies outperform single-method approaches
3. Conduct ablation studies on the fine-tuning phase to quantify how much of the robustness improvement comes from pretraining versus the learning with noisy labels algorithms themselves