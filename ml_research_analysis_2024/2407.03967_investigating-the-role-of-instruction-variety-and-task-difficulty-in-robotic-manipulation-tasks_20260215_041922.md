---
ver: rpa2
title: Investigating the Role of Instruction Variety and Task Difficulty in Robotic
  Manipulation Tasks
arxiv_id: '2407.03967'
source_url: https://arxiv.org/abs/2407.03967
tags:
- obj-centric
- concatenate
- cross-attn
- table
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically evaluates how multimodal models generalise
  to novel inputs and concepts by introducing a comprehensive evaluation framework.
  The framework examines generalisation across structural, compositional, and robustness
  dimensions through targeted input perturbations to multimodal prompts, including
  paraphrasing, visual referent replacement, masking modalities, and increasing task
  difficulty.
---

# Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks

## Quick Facts
- arXiv ID: 2407.03967
- Source URL: https://arxiv.org/abs/2407.03967
- Reference count: 40
- Models are robust to extreme instruction perturbations but vulnerable to observational changes, indicating overfitting to spurious correlations

## Executive Summary
This work introduces a comprehensive evaluation framework to assess how multimodal models generalize to novel inputs and concepts in robotic manipulation tasks. The framework systematically examines structural, compositional, and robustness dimensions through targeted input perturbations including paraphrasing, visual referent replacement, modality masking, and task difficulty scaling. When applied to current Transformer-based multimodal models, the framework reveals that models can infer tasks without clear instructions by relying on learned spurious correlations in observations, and performance degrades with increased object clutter and unexpected affordances. The findings highlight the need for architectural and training innovations that better integrate multimodal inputs while prioritizing sensitivity to input content over incidental correlations.

## Method Summary
The study evaluates four multimodal model architectures on robotic manipulation tasks from the VIMA-BENCH dataset, varying prompt-conditioning methods (cross-attention vs concatenation) and visual encoders (object-centric vs image patches). Models are trained on a standardized curriculum and evaluated using targeted perturbations including paraphrasing, Gobbledygook word/token replacements, modality masking, permuted object orders, and increased task difficulty with distractors. Performance is measured through task success rates across 17 tasks spanning four generalization levels (L1-L4), with each task evaluated over 200 episodes to ensure statistical significance.

## Key Results
- Models are robust to extreme instruction perturbations (Gobbledygook, paraphrasing) but vulnerable to observational changes (permuted object order, distractors)
- Object-centric visual representations outperform image patches, especially when inferring tasks without instructions
- Cross-attention architectures show better robustness to language corruption than concatenation methods
- Performance degrades significantly with increased task difficulty and object clutter, revealing limitations in handling complex scenes

## Why This Works (Mechanism)

### Mechanism 1
Multimodal models rely on spurious correlations in observations to infer tasks even when instructions are removed or nonsensical. The model learns to map specific visual patterns (e.g., container shape, object arrangement) to action sequences during training, creating implicit task heuristics that bypass explicit language understanding.

### Mechanism 2
Object-centric visual representations provide stronger task-relevant signals than image patches for inferring actions without instructions. Object-centric encoding preserves individual object identities and relationships, allowing the model to recognize task-relevant objects even when language instructions are absent or corrupted.

### Mechanism 3
Cross-attention architectures are more robust to language perturbations than concatenation architectures because they better preserve relationships between natural language descriptors and visual referents. Cross-attention allows the decoder to dynamically attend to relevant visual features conditioned on language tokens, maintaining task-relevant information even when language is corrupted.

## Foundational Learning

- Concept: Systematic generalization across structural, compositional, and robustness dimensions
  - Why needed here: The evaluation framework specifically targets these three generalization axes through targeted input perturbations
  - Quick check question: What's the difference between structural and compositional generalization in this context?

- Concept: Embodied AI task formulation and action spaces
  - Why needed here: The work evaluates robotic manipulation tasks with specific action primitives (SE(3) poses) and history-based policies
  - Quick check question: How does the SE(3) action representation differ from traditional discrete action spaces in navigation tasks?

- Concept: Multimodal instruction encoding and perturbation methods
  - Why needed here: The evaluation framework relies on specific perturbation techniques (paraphrasing, Gobbledygook, modality masking)
  - Quick check question: What's the difference between Gobbledygook Words and Gobbledygook Tokens in terms of information removal?

## Architecture Onboarding

- Component map: T5 encoder → Visual encoder → Multimodal fusion → Decoder → Environment execution
- Critical path: Instruction encoding → Visual encoding → Multimodal fusion → Action prediction → Environment execution
- Design tradeoffs: Object-centric vs image patches (better object reasoning vs computational efficiency), cross-attention vs concatenation (dynamic feature fusion vs simpler architecture), frozen T5 vs fine-tuning (training stability vs adaptation to robotic domain)
- Failure signatures: Poor performance on L3/L4 (inability to generalize to novel objects/tasks), high performance with Gobbledygook (over-reliance on visual cues), performance drop with permuted object order (learned spurious correlations)
- First 3 experiments: 1) Baseline model on original vs paraphrased instructions, 2) Gobbledygook perturbations comparing object-centric vs patch performance, 3) Masking language tokens vs visual referents to determine modality importance

## Open Questions the Paper Calls Out

- How do multimodal models perform on compositional generalisation tasks when trained on a more diverse dataset with varied object shapes, textures, and affordances?
- How do the performance of multimodal models change when evaluated on a wider range of real-world robotic manipulation tasks with varying levels of complexity and clutter?
- What are the specific architectural and training innovations that can enhance the multimodal integration capabilities of Transformer-based models for better generalization in Embodied AI tasks?

## Limitations
- Dataset generalization boundaries may limit the scope of what constitutes "novel" inputs
- Perturbation methods may not uniformly test the intended generalization axes
- Training procedure transparency is limited, making it difficult to assess whether performance patterns reflect architectural differences or training artifacts

## Confidence

- High Confidence: Models are robust to instruction perturbations but vulnerable to observational changes; object-centric representations outperform image patches for instruction-less inference
- Medium Confidence: Models rely on spurious correlations for task inference; cross-attention provides better robustness to language corruption than concatenation
- Low Confidence: Specific mechanisms by which spurious correlations operate in learned representations; comparative performance analysis may be influenced by implementation details

## Next Checks

1. Conduct a systematic analysis of the VIMA-BENCH training dataset to identify and quantify potential spurious correlations between visual patterns and actions.

2. Evaluate the perturbation framework on a completely independent robotic manipulation dataset (such as RoboNet or RoboCat) to determine whether observed generalization patterns are dataset-specific or reflect fundamental architectural limitations.

3. Perform targeted ablations where specific visual features (object colors, positions, sizes) are systematically varied while keeping instructions constant, to isolate which visual attributes the models actually rely on versus what they should rely on for robust task inference.