---
ver: rpa2
title: 'Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of
  LLM-Generated Medical Explanatory Arguments'
arxiv_id: '2409.20565'
source_url: https://arxiv.org/abs/2409.20565
tags:
- arguments
- medical
- task
- proxy
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach for evaluating LLM-generated
  medical explanatory arguments using proxy tasks and rankings instead of traditional
  scoring methods. It introduces a discriminative LM evaluator trained on proxy tasks
  (Medical MCQ, Misinformation Detection, NLI) to assess argument quality without
  relying on reference texts or generative LLM judges.
---

# Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments

## Quick Facts
- arXiv ID: 2409.20565
- Source URL: https://arxiv.org/abs/2409.20565
- Reference count: 27
- The paper proposes using proxy tasks and rankings instead of traditional scoring for evaluating LLM-generated medical arguments, demonstrating robustness against adversarial attacks.

## Executive Summary
This paper introduces a novel approach for evaluating LLM-generated medical explanatory arguments using discriminative language model evaluators trained on proxy tasks. Instead of traditional scoring methods that require reference texts, the framework ranks arguments based on their informativeness for medical decision-making through three proxy tasks: Medical MCQ, Misinformation Detection, and NLI in clinical trials. The approach aligns with human expert preferences while avoiding biases common in generative LLM judges, and demonstrates robustness against adversarial attacks including non-argumentative inputs.

## Method Summary
The method trains discriminative LM evaluators on proxy tasks using synthetic arguments generated by multiple LLMs. Arguments are ranked based on their performance on proxy tasks, with the LLM-trained evaluator showing better alignment with human preferences than baseline or expert-trained evaluators. The framework uses only one human-crafted argument per task and five examples for validation, yet achieves reliable evaluation results. The approach is tested against adversarial control cases to verify robustness against non-argumentative content.

## Key Results
- LLM-trained discriminative evaluators align with human expert rankings while avoiding biases like self-enhancement and verbosity
- The framework demonstrates robustness against adversarial attacks, correctly identifying non-argumentative inputs
- Using only one human-crafted argument per task and five validation examples, the method achieves reliable evaluation performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated arguments can be reliably evaluated by proxy tasks that measure their informativeness for medical decision-making.
- Mechanism: The discriminative LM evaluator assesses how well generated arguments help solve proxy tasks (MCQA, misinformation detection, NLI), indirectly measuring argument quality without needing reference texts.
- Core assumption: Better arguments improve performance on proxy tasks, and task performance correlates with human judgment of argument quality.
- Evidence anchors: [abstract] "We demonstrate that the proposed evaluators are robust against adversarial attacks, including the assessment of non-argumentative text."

### Mechanism 2
- Claim: LLM-trained evaluators outperform baseline and expert-trained evaluators in aligning with human preferences.
- Mechanism: By training on diverse LLM-generated arguments, the evaluator learns to assess argument quality without favoring any specific LLM's output style, avoiding biases like self-enhancement.
- Core assumption: Diverse training data from multiple LLMs reduces bias and improves generalizability to unseen argument styles.
- Evidence anchors: [abstract] "the LLM-trained approach proves to be the winning strategy, demonstrating that we can effectively evaluate LLM-generated argumentation by using synthetic data."

### Mechanism 3
- Claim: The evaluation framework is robust against adversarial attacks (Control Cases).
- Mechanism: The LLM-trained evaluator can distinguish between meaningful medical arguments and non-argumentative content (no argument, label-only, noise, irrelevant passages).
- Core assumption: The proxy tasks and training process enable the evaluator to recognize and penalize non-argumentative inputs.
- Evidence anchors: [abstract] "We demonstrate that the proposed evaluators are robust against adversarial attacks, including the assessment of non-argumentative text."

## Foundational Learning

- Concept: Proxy tasks as evaluation metrics
  - Why needed here: Direct evaluation of medical arguments is difficult due to lack of reference texts and high entropy of valid arguments. Proxy tasks provide an indirect but reliable measure of argument quality.
  - Quick check question: How does using proxy tasks help overcome the limitations of reference-based evaluation metrics?

- Concept: Discriminative vs. generative evaluation
  - Why needed here: Generative LLM judges suffer from biases (self-enhancement, verbosity) that affect their reliability. Discriminative models focus purely on task performance, avoiding these biases.
  - Quick check question: What are the key differences between using generative LLM judges versus discriminative LM evaluators for argument assessment?

- Concept: Adversarial robustness in evaluation systems
  - Why needed here: An evaluation system must be able to distinguish between meaningful arguments and irrelevant or misleading content to be reliable in practice.
  - Quick check question: Why is testing with Control Cases (adversarial attacks) important for validating the robustness of an evaluation system?

## Architecture Onboarding

- Component map:
  - Proxy Tasks (MCQA, Misinformation Detection, NLI) -> LLM-trained Discriminative Evaluator -> Argument Rankings -> Human Expert Comparison

- Critical path:
  1. Generate synthetic arguments using multiple LLMs
  2. Train discriminative LM evaluator on proxy tasks using these arguments
  3. Evaluate arguments by measuring proxy task performance with each argument
  4. Rank arguments based on performance
  5. Compare rankings with human expert rankings

- Design tradeoffs:
  - Proxy task selection vs. coverage of argumentation types
  - Training data diversity vs. computational cost
  - Robustness to adversarial attacks vs. sensitivity to subtle quality differences

- Failure signatures:
  - Evaluator rankings consistently disagree with human rankings
  - Performance drops significantly on Control Cases
  - Results vary widely depending on which LLM generated the training arguments

- First 3 experiments:
  1. Train evaluator with arguments from only one LLM, test alignment with human rankings
  2. Test evaluator performance on each type of Control Case individually
  3. Vary proxy task combinations (e.g., use only MCQA vs. all three tasks) and measure impact on evaluation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the token limit of 512 in the discriminative LM model be addressed to handle longer, more complex arguments?
- Basis in paper: [explicit] The paper mentions that the discriminative LM model has a token limit of 512, which may restrict its ability to fully process longer, more complex arguments.
- Why unresolved: While the paper acknowledges this limitation and suggests future advancements will help, it does not provide specific solutions or experimental results demonstrating how to overcome this constraint with current technology.
- What evidence would resolve it: Experimental results showing the impact of the token limit on evaluation performance, and results demonstrating improved performance when using models with larger context windows or other techniques to handle longer texts.

### Open Question 2
- Question: How can the proposed evaluation method be extended to measure hallucinations, factual accuracy, and coherence in generated arguments?
- Basis in paper: [inferred] The paper focuses on evaluating the informativeness of LLM-generated explanatory arguments for medical decision-making, but explicitly states it does not focus on measuring hallucinations, factual accuracy, or coherence in the generated arguments.
- Why unresolved: The paper's evaluation method relies on proxy tasks that measure task performance rather than directly assessing the quality of the arguments themselves.
- What evidence would resolve it: Development and experimental validation of additional metrics or evaluation frameworks that can assess hallucinations, factual accuracy, and coherence in medical arguments, integrated with the proposed proxy task-based approach.

### Open Question 3
- Question: How does the choice of proxy tasks affect the alignment between the LM evaluator and human preferences?
- Basis in paper: [explicit] The paper uses three diverse proxy tasks and observes that human evaluations are not consistent across all tasks, with lower agreement in the NLI task.
- Why unresolved: While the paper identifies that not all proxy tasks are equally valuable, it does not provide a systematic methodology for selecting or designing proxy tasks that maximize alignment with human preferences.
- What evidence would resolve it: A study comparing multiple proxy tasks across different domains, analyzing which task characteristics correlate with better alignment between LM evaluator rankings and human preferences.

## Limitations

- The evaluation approach relies on proxy tasks that may not fully capture all nuances of medical argumentation quality
- The framework is validated on a limited set of 5 validation instances per task, which may not represent broader medical argumentation scenarios
- Robustness against adversarial attacks is tested only against specific control cases and may not generalize to more sophisticated attacks

## Confidence

- **High Confidence**: The mechanism of using discriminative LM evaluators to avoid biases present in generative LLM judges is well-supported by both theoretical reasoning and experimental results
- **Medium Confidence**: The claim that LLM-trained evaluators outperform expert-trained ones in aligning with human preferences is supported but based on limited validation data (5 instances per task)
- **Medium Confidence**: The robustness against adversarial attacks is demonstrated through control cases, but the generalizability to more diverse and sophisticated attacks remains uncertain

## Next Checks

1. Expand the validation set to include a larger and more diverse set of medical argumentation tasks to verify if alignment with human preferences holds beyond the initial 5 validation instances per task
2. Design and test additional adversarial inputs that combine multiple types of noise or irrelevant content to assess whether the current control case framework is sufficient for real-world robustness
3. Evaluate whether the LLM-trained discriminative evaluator can generalize to medical argumentation tasks outside the three proxy tasks (MCQA, Misinformation Detection, NLI) used in training, to test the broader applicability of the approach