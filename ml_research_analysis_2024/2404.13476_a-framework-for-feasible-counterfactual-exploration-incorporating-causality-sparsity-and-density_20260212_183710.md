---
ver: rpa2
title: A Framework for Feasible Counterfactual Exploration incorporating Causality,
  Sparsity and Density
arxiv_id: '2404.13476'
source_url: https://arxiv.org/abs/2404.13476
tags:
- counterfactual
- examples
- example
- feasibility
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a framework for generating counterfactual explanations
  that satisfy feasibility, sparsity, and density requirements. The core method uses
  a Variational Autoencoder to generate counterfactual examples while enforcing causal
  constraints (unary and binary) on features.
---

# A Framework for Feasible Counterfactual Exploration incorporating Causality, Sparsity and Density

## Quick Facts
- arXiv ID: 2404.13476
- Source URL: https://arxiv.org/abs/2404.13476
- Authors: Kleopatra Markou; Dimitrios Tomaras; Vana Kalogeraki; Dimitrios Gunopulos
- Reference count: 23
- One-line primary result: VAE-based framework generates counterfactual explanations satisfying feasibility, sparsity, and density requirements with high feasibility scores (up to 93.33%) and validity (up to 100%)

## Executive Summary
This work proposes a framework for generating counterfactual explanations that satisfy feasibility, sparsity, and density requirements. The core method uses a Variational Autoencoder to generate counterfactual examples while enforcing causal constraints (unary and binary) on features. A four-part loss function trains the model to produce valid counterfactuals that are proximal to inputs, satisfy feasibility constraints, and are sparse. Experiments on three benchmark datasets show the framework achieves high feasibility scores while maintaining interpretability through manifold visualizations in latent space.

## Method Summary
The framework combines a black-box classifier with a Variational Autoencoder (VAE) trained on a four-part loss function. The loss incorporates validity (class prediction), proximity (L1 distance), feasibility (causal constraints), and sparsity (minimal feature changes). The VAE learns to generate counterfactual examples in latent space while respecting unary constraints (single feature relationships) and binary constraints (feature interactions). After training, t-SNE projects the latent space to 2D manifolds for visualizing feasible versus infeasible counterfactual examples. The approach is evaluated on Adult Income, KDD-Census Income, and Law School datasets.

## Key Results
- Achieves high feasibility scores up to 93.33% while maintaining sparsity across all three benchmark datasets
- Generates counterfactual examples with 100% validity on KDD-Census Income dataset
- Two-dimensional manifold visualizations effectively distinguish feasible from infeasible counterfactual examples in latent space
- Unary and binary constraints both improve feasibility, with binary constraints showing particular effectiveness on Law School dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal constraints ensure counterfactual examples remain feasible by encoding domain knowledge into the generation process.
- Mechanism: The framework enforces unary and binary causal constraints during VAE training, preventing generated counterfactuals from violating logical relationships between features.
- Core assumption: Domain experts can provide accurate causal constraints that reflect real-world relationships between features.
- Evidence anchors:
  - [abstract] "The core method uses a Variational Autoencoder to generate counterfactual examples while enforcing causal constraints (unary and binary) on features."
  - [section] "We define a counterfactual example (xcf, ycf) as feasible if the desired class y′ is equal to the ycf output, the changes from x to x′ satisfies all constraints provided by domain knowledge..."
- Break condition: If the provided causal constraints are incorrect or incomplete, the model may generate infeasible counterfactuals or miss valid ones.

### Mechanism 2
- Claim: Sparsity ensures counterfactual examples require minimal changes, making them more actionable for end-users.
- Mechanism: The loss function includes a sparsity term that penalizes the model when counterfactual examples require too many feature changes.
- Core assumption: Users prefer and can more easily implement counterfactual suggestions that require fewer changes.
- Evidence anchors:
  - [abstract] "Experiments on three benchmark datasets... show the framework achieves high feasibility scores (up to 93.33%) and validity (up to 100%) while maintaining sparsity."
  - [section] "We define sparsity as the total number of changes of features that an individual needs to perform to alter its class."
- Break condition: If sparsity is overemphasized relative to feasibility, the model may generate counterfactuals that are too similar to the original input and fail to change the prediction.

### Mechanism 3
- Claim: Manifold representations in latent space help identify regions of feasible counterfactual examples.
- Mechanism: The VAE's latent space captures density distributions of counterfactual examples, allowing visualization of feasible vs. infeasible regions in 2D manifolds.
- Core assumption: Feasible counterfactual examples cluster in specific regions of the latent space.
- Evidence anchors:
  - [abstract] "Two-dimensional manifold visualizations effectively distinguish feasible from infeasible counterfactual examples in latent space."
  - [section] "With the latent space produced by our main model... we are forming manifold representations, i.e. two-dimensional surfaces that represent the various counterfactual examples as real points depicted in these spaces."
- Break condition: If the latent space representation doesn't capture the relevant structure of feasible counterfactuals, the manifolds won't effectively distinguish between feasible and infeasible examples.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: VAEs generate counterfactual examples by learning a probabilistic mapping from input space to latent space and back, allowing controlled perturbations.
  - Quick check question: What distinguishes a VAE from a standard autoencoder in terms of how it represents data in latent space?

- Concept: Causal inference and constraints
  - Why needed here: Causal constraints ensure that generated counterfactuals respect logical relationships between features in the real world.
  - Quick check question: How would you formulate a causal constraint that prevents age from decreasing in a counterfactual example?

- Concept: Loss function design for multi-objective optimization
  - Why needed here: The framework requires balancing validity, feasibility, proximity, and sparsity in a single loss function.
  - Quick check question: What challenges arise when combining multiple objectives with different scales in a single loss function?

## Architecture Onboarding

- Component map:
  Input preprocessing layer (one-hot encoding, normalization) -> Black-box classifier (two linear layers) -> VAE encoder (5 linear layers with ReLU, 30% dropout) -> Latent space representation (10-dimensional) -> VAE decoder (5 linear layers with ReLU) -> Four-part loss function (validity, proximity, feasibility, sparsity) -> Manifold visualization component (t-SNE for 2D projection)

- Critical path:
  1. Preprocess input data
  2. Train black-box classifier
  3. Train VAE with four-part loss function
  4. Generate counterfactual examples
  5. Evaluate feasibility and sparsity
  6. Create manifold visualizations

- Design tradeoffs:
  - Unary vs. binary constraints: Unary constraints are simpler but may miss important feature interactions captured by binary constraints
  - Sparsity vs. feasibility: More aggressive sparsity constraints may reduce feasibility scores
  - Latent space dimensionality: Higher dimensions may capture more nuance but reduce interpretability of manifold visualizations

- Failure signatures:
  - Low validity scores: Black-box classifier not well-trained or VAE not properly reconstructing inputs
  - Low feasibility scores: Causal constraints incorrectly specified or VAE not learning them
  - High sparsity but low feasibility: Sparsity term dominating the loss function
  - Uninterpretable manifold visualizations: Latent space not capturing meaningful structure

- First 3 experiments:
  1. Train with only unary constraints on Adult dataset, evaluate feasibility vs. sparsity tradeoff
  2. Compare manifold visualizations with and without causal constraints to visualize their impact
  3. Test model on a dataset with known causal structure (e.g., synthetic data) to validate constraint enforcement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do causal constraints impact the trade-off between feasibility and sparsity in counterfactual explanations across different datasets?
- Basis in paper: [explicit] The paper explicitly compares feasibility and sparsity scores across datasets and methods, noting that their method achieves high feasibility (up to 93.33%) while maintaining reasonable sparsity, though sometimes lower than CEM method.
- Why unresolved: While the paper shows feasibility and sparsity can be balanced, it doesn't provide a systematic analysis of how different types of causal constraints affect this trade-off across datasets with varying characteristics.
- What evidence would resolve it: A comprehensive study varying constraint types and dataset characteristics while measuring the feasibility-sparsity trade-off curve for each combination.

### Open Question 2
- Question: What is the optimal architecture for incorporating both unary and binary causal constraints in VAE-based counterfactual generation?
- Basis in paper: [explicit] The paper implements separate models for unary and binary constraints and observes different performance, but doesn't explore integrated architectures that handle both constraint types simultaneously.
- Why unresolved: The current approach treats unary and binary constraints as separate models, leaving open whether a unified architecture could better capture complex causal relationships.
- What evidence would resolve it: Comparative experiments testing VAE architectures with integrated constraint handling against the separate models presented in the paper.

### Open Question 3
- Question: How does the quality of causal constraint specification affect the overall performance of counterfactual explanations?
- Basis in paper: [inferred] The paper relies on manually specified causal constraints (like age must increase with education) and shows their importance, but doesn't investigate how constraint quality variations impact results.
- Why unresolved: The paper uses domain knowledge for constraint formulation but doesn't explore how errors or incompleteness in these constraints affect feasibility and validity outcomes.
- What evidence would resolve it: Experiments systematically varying constraint accuracy and completeness while measuring degradation in feasibility, validity, and other metrics.

## Limitations

- The framework's performance depends heavily on the quality and completeness of provided causal constraints, which are domain-specific and may not be readily available for all applications
- The scalability of the approach to high-dimensional datasets remains unclear, as experiments focus on datasets with relatively low feature counts
- The paper does not address how to automatically discover or validate causal constraints, leaving a significant gap in practical deployment

## Confidence

- **High confidence**: The core methodology combining VAEs with causal constraints for counterfactual generation is technically sound and well-grounded in existing literature
- **Medium confidence**: The reported feasibility and validity scores are promising but limited to three specific benchmark datasets; generalization to diverse real-world applications needs further validation
- **Medium confidence**: The manifold visualization approach effectively distinguishes feasible from infeasible counterfactuals in the tested cases, though the method's robustness across different latent space structures requires more investigation

## Next Checks

1. **Constraint Sensitivity Analysis**: Systematically vary the strength of causal constraints in the loss function to quantify their impact on feasibility scores and identify optimal constraint weights for different datasets

2. **Cross-Dataset Generalization**: Apply the framework to datasets from diverse domains (healthcare, finance, education) with varying numbers of features and class distributions to assess robustness beyond the three benchmark datasets

3. **Constraint Discovery Extension**: Develop and evaluate automated methods for discovering causal constraints from data, reducing the dependency on expert-provided constraints and enabling broader application of the framework