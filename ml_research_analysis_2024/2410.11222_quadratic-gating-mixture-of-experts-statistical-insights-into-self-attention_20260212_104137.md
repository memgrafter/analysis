---
ver: rpa2
title: 'Quadratic Gating Mixture of Experts: Statistical Insights into Self-Attention'
arxiv_id: '2410.11222'
source_url: https://arxiv.org/abs/2410.11222
tags:
- quadratic
- page
- gating
- experts
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes a rigorous theoretical connection between
  the self-attention mechanism and Mixture of Experts (MoE) models, showing that each
  row of a self-attention matrix can be represented as a quadratic gating MoE. Based
  on this insight, the authors conduct a comprehensive convergence analysis of MoE
  models with two quadratic gating functions: quadratic polynomial gate and quadratic
  monomial gate.'
---

# Quadratic Gating Mixture of Experts: Statistical Insights into Self-Attention

## Quick Facts
- arXiv ID: 2410.11222
- Source URL: https://arxiv.org/abs/2410.11222
- Authors: Pedram Akbarian; Huy Nguyen; Xing Han; Nhat Ho
- Reference count: 40
- Primary result: Self-attention rows can be represented as quadratic gating MoE, with quadratic monomial gating yielding improved sample efficiency

## Executive Summary
This paper establishes a rigorous theoretical connection between self-attention and Mixture of Experts (MoE) models, showing that each row of a self-attention matrix can be represented as a quadratic gating MoE. The authors conduct a comprehensive convergence analysis comparing quadratic polynomial and monomial gating functions, revealing that the monomial gate yields improved sample efficiency. Based on these insights, they propose active-attention, which applies non-linear activation functions to the value matrix, and demonstrate improved performance across image classification, language modeling, and multivariate time series forecasting tasks.

## Method Summary
The method establishes that self-attention computation can be decomposed into a quadratic gating MoE structure, where each row of the attention matrix is a weighted sum of value vectors with weights derived from quadratic forms. The authors analyze two gating functions - quadratic polynomial (including linear terms) and quadratic monomial (pure quadratic) - and prove that the monomial gate yields faster convergence rates for parameter and expert estimation. They then propose active-attention by applying non-linear activation functions (ReLU, GELU, etc.) to the value matrix, effectively creating non-linear experts that satisfy poly-strong identifiability conditions for improved convergence.

## Key Results
- Each row of a self-attention matrix can be written as a quadratic gating mixture of linear experts
- Quadratic monomial gating yields improved sample efficiency for parameter and expert estimation compared to quadratic polynomial gating
- Active-attention with non-linear activation functions outperforms standard self-attention on image classification, language modeling, and time series forecasting tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Each row of a self-attention matrix can be represented as a quadratic gating MoE
- Mechanism: The self-attention computation Softmax(QK⊤/√dk)V decomposes such that each row is a weighted sum of value vectors, where weights come from a quadratic form x⊤M x where M = Wq W⊤k/√dk
- Core assumption: The query-key interaction produces quadratic affinity scores that can be expressed as x⊤A x for some matrix A
- Evidence anchors:
  - [abstract] "each row of a self-attention matrix can be written as a quadratic gating mixture of linear experts"
  - [section] "the (i, j) entry of the logit matrix XM X⊤ can be written in a quadratic form [XM X⊤]i,j = x⊤i M xj = ex⊤ P ⊤i M Pj ex"
  - [corpus] Weak evidence - corpus papers discuss gating and attention but not this specific quadratic decomposition
- Break condition: If the query-key projection matrices Wq and Wk are not full rank or if bias terms are added, the clean quadratic form may not hold

### Mechanism 2
- Claim: Quadratic monomial gating yields faster parameter and expert estimation rates than quadratic polynomial gating
- Mechanism: Removing the linear term b⊤x from the gating function eliminates parameter interactions that slow convergence, allowing estimation rates to improve from O(1/logτ(n)) to O(n⁻¹/⁴)
- Core assumption: The parameter interaction captured by the PDE ∂F/∂A = ∂²F/∂b∂b⊤ is the primary bottleneck for estimation rates
- Evidence anchors:
  - [abstract] "our analysis indicates that the use of the quadratic monomial gate yields an improved sample efficiency for estimating parameters and experts compared to the quadratic polynomial gate"
  - [section] "parameter and expert estimation rates become significantly faster when employing non-linear experts in place of linear experts"
  - [corpus] Weak evidence - corpus papers discuss MoE but not specific comparison of polynomial vs monomial gating convergence rates
- Break condition: If the true underlying gating function actually requires the linear term for accurate modeling, removing it may hurt rather than help estimation

### Mechanism 3
- Claim: Applying non-linear activation to value matrix in self-attention (active-attention) improves performance across tasks
- Mechanism: Non-linear activation functions on value vectors act as non-linear experts, satisfying poly-strong identifiability conditions that enable faster convergence and better expressiveness than linear experts
- Core assumption: The non-linear activation functions (ReLU, GELU, etc.) create expert functions that are poly-strongly identifiable, enabling faster parameter estimation
- Evidence anchors:
  - [abstract] "we propose a novel active-attention mechanism where we apply a non-linear activation function to the value matrix in the formula of self-attention"
  - [section] "the poly-strong identifiability condition holds for experts formulated as two-layer FFNs with non-linear activation functions such as ReLU and GELU"
  - [corpus] Weak evidence - corpus papers discuss activation functions but not specifically their role in self-attention as non-linear experts
- Break condition: If the non-linear activation introduces instability or if the task requires linear value transformations, the active-attention may underperform

## Foundational Learning

- Concept: Mixture of Experts (MoE) framework
  - Why needed here: Understanding how multiple specialized sub-models are combined via gating mechanisms is essential for grasping the connection to self-attention
  - Quick check question: In a softmax gating MoE, how are the mixture weights computed from expert affinity scores?

- Concept: Quadratic forms and their properties
  - Why needed here: The core insight relies on recognizing that query-key interactions produce quadratic forms x⊤A x, which are fundamental to the MoE representation
  - Quick check question: Given a quadratic form x⊤A x, what is the dimension of matrix A when x ∈ Rd?

- Concept: Convergence rates and statistical estimation theory
  - Why needed here: The theoretical analysis compares estimation rates (e.g., n⁻¹/² vs 1/logτ(n)), requiring understanding of statistical learning theory
  - Quick check question: What does the notation eOP(n⁻¹/⁴) indicate about the convergence behavior as sample size increases?

## Architecture Onboarding

- Component map: Input -> Linear projections (Q, K, V) -> Attention score computation (quadratic form) -> Non-linear activation (for active-attention) -> Weighted sum of value vectors -> Output

- Critical path: Input → Linear projections (Q, K, V) → Attention score computation (quadratic form) → Non-linear activation (for active-attention) → Weighted sum of value vectors → Output. The bottleneck is the quadratic computation QK⊤ which is O(N²dk).

- Design tradeoffs: Quadratic gating increases parameter count due to d² terms versus d for linear gating, but can be mitigated with low-rank embeddings. Non-linear activation improves expressiveness but may add training instability. The monomial gate is computationally simpler than polynomial gate but may lose some modeling flexibility.

- Failure signatures: Poor performance may indicate (1) Insufficient over-specification of experts, (2) Inappropriate choice of non-linear activation function, (3) Numerical instability from large quadratic terms, (4) Incorrect rank for low-rank embeddings. Monitoring Voronoi loss and estimation rates can help diagnose issues.

- First 3 experiments:
  1. Verify the quadratic decomposition by computing QK⊤ and checking if it can be written as ex⊤Bex for some matrix B
  2. Compare convergence rates of linear vs non-linear experts on synthetic data using the Voronoi loss metric
  3. Test different non-linear activation functions (ReLU, GELU, Tanh) on a small image classification task to identify which works best for the target domain

## Open Questions the Paper Calls Out

- Question: How would the convergence rates change for quadratic gating MoE with more than one layer, as used in practice?
  - Basis in paper: [explicit] The authors note that their analysis considers a single MoE layer rather than multiple layers as used in practice, and suggest this as a limitation.
  - Why unresolved: The theoretical analysis is simplified to a single layer setting for tractability, but practical implementations use stacked MoE layers. The interactions between layers and their effect on convergence rates remain unexplored.
  - What evidence would resolve it: Empirical convergence studies comparing single-layer versus multi-layer quadratic gating MoE models, or theoretical extensions that account for layer interactions.

- Question: What is the expressive power of quadratic gating MoE models compared to other gating functions?
  - Basis in paper: [explicit] The authors identify this as an important direction for future research, noting that the expressive power of quadratic gating MoE remains elusive in the literature.
  - Why unresolved: While the paper establishes theoretical connections and convergence properties, it does not quantify how well quadratic gating MoE can approximate complex functions compared to alternatives like linear or higher-order gating.
  - What evidence would resolve it: Universal approximation theorems or empirical benchmarks comparing quadratic gating MoE to other gating schemes across diverse function classes.

- Question: How do different non-linear activation functions in the active-attention mechanism affect performance across various tasks?
  - Basis in paper: [explicit] The authors test several activation functions (ReLU, GELU, SiLU, Sigmoid, Tanh) but note that performance varies by task, with GELU/ReLU excelling in some domains while Tanh/Sigmoid work better in others.
  - Why unresolved: The paper shows empirical differences but does not provide a theoretical understanding of why certain activations perform better for specific tasks, or guidance on activation selection.
  - What evidence would resolve it: Task-specific theoretical analysis of activation function properties (e.g., smoothness, saturation) and their interaction with data characteristics, or comprehensive empirical studies across more diverse task types.

## Limitations
- The theoretical analysis is limited to single-layer MoE settings, while practical implementations use stacked layers
- The quadratic form decomposition may break down with bias terms or different normalization schemes
- Empirical validation is limited in scope, and improvements could be partially attributed to increased model capacity

## Confidence

**High Confidence:** The core mathematical insight that self-attention rows can be expressed as quadratic gating MoE is well-supported by the quadratic form decomposition and follows from standard linear algebra. The connection between non-linear activation functions and poly-strong identifiability is grounded in established statistical learning theory.

**Medium Confidence:** The comparative analysis showing quadratic monomial gates yield faster convergence than polynomial gates is theoretically sound but relies on asymptotic analysis that may not fully translate to practical deep learning scenarios. The specific convergence rates (O(n⁻¹/⁴) vs 1/logτ(n)) are derived under idealized conditions.

**Low Confidence:** The empirical superiority of active-attention across all tested tasks may be overstated, as the ablation studies are limited in scope and the choice of non-linear activation functions was not systematically explored. The improvements could be partially attributed to increased model capacity rather than the gating mechanism itself.

## Next Checks

1. **Quadratic Decomposition Verification:** Implement a controlled experiment comparing the exact quadratic form decomposition on synthetic data with varying query-key matrix ranks. This will validate whether the clean quadratic representation holds under different architectural choices (e.g., with vs. without bias terms, different normalization schemes).

2. **Convergence Rate Empirical Testing:** Design a synthetic data experiment specifically testing the convergence rate claims for polynomial vs. monomial gates and linear vs. non-linear experts. Measure the actual estimation error decay as a function of sample size to empirically verify the theoretical O(n⁻¹/⁴) vs 1/logτ(n) rates.

3. **Activation Function Sensitivity Analysis:** Conduct a comprehensive ablation study systematically varying the non-linear activation functions (ReLU, GELU, Sigmoid, Tanh, SiLU, etc.) and their placement in the attention mechanism. This will determine whether the improvements are robust across different non-linearities or specific to certain choices.