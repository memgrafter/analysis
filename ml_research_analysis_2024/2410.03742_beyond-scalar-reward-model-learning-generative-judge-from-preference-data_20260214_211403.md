---
ver: rpa2
title: 'Beyond Scalar Reward Model: Learning Generative Judge from Preference Data'
arxiv_id: '2410.03742'
source_url: https://arxiv.org/abs/2410.03742
tags:
- answer
- con-j
- preference
- judgment
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of scalar reward models in
  preference learning, specifically their lack of interpretability and susceptibility
  to dataset bias. The authors propose Con-J, a generative judge trained using contrastive
  judgments generated by a pre-trained LLM.
---

# Beyond Scalar Reward Model: Learning Generative Judge from Preference Data

## Quick Facts
- arXiv ID: 2410.03742
- Source URL: https://arxiv.org/abs/2410.03742
- Reference count: 24
- Primary result: Generative judge achieves comparable preference prediction to scalar models while offering superior interpretability and bias robustness

## Executive Summary
This paper introduces Con-J, a generative judge that learns from preference data using contrastive judgments generated by a pre-trained LLM. Unlike traditional scalar reward models, Con-J generates both binary judgments and detailed rationales, addressing key limitations of interpretability and dataset bias. The method leverages Direct Preference Optimization (DPO) with contrastive judgment pairs constructed from LLM outputs, demonstrating state-of-the-art performance among open-source generative judges on public benchmarks while showing particular strength on domain-specific commercial datasets.

## Method Summary
Con-J is a generative judge trained on contrastive judgments generated by a pre-trained LLM (Qwen2-7B-Instruct). The training process involves three key stages: judgment sampling using repeated and hint-driven sampling to generate multiple judgments, judgment filtering to construct contrastive pairs using true preference labels, and Con-J training with DPO loss plus SFT regularization. The model outputs both binary preference judgments and detailed rationales, providing interpretability while maintaining competitive performance on preference prediction tasks. The approach is evaluated on both public benchmarks and self-collected commercial datasets across different domains.

## Key Results
- Con-J achieves comparable performance to scalar reward models on preference prediction tasks
- Con-J demonstrates superior interpretability through generated rationales that improve with training accuracy
- Con-J shows greater robustness against dataset bias compared to scalar models, particularly on adversarial datasets with format and verbosity biases
- Con-J achieves state-of-the-art performance among open-source generative judges on public benchmarks and outperforms GPT-4o on domain-specific commercial datasets

## Why This Works (Mechanism)

### Mechanism 1
Con-J's generative approach produces more accurate rationales compared to scalar reward models. By training a generative model to produce both binary judgments and detailed rationales simultaneously, Con-J leverages the LLM's inherent generation capabilities to provide explanations that improve with training accuracy. The quality of rationales generated by Con-J correlates with the model's ability to make correct preference judgments, as evidenced by improved rationale correctness scores when judgment accuracy increases.

### Mechanism 2
Training with contrastive judgments using DPO loss provides better learning signal than SFT alone. DPO directly optimizes the model to distinguish between preferred and non-preferred answers by maximizing the probability of correct preference predictions, while SFT alone may learn patterns common to both positive and negative judgments. The discriminative nature of DPO provides stronger supervision for preference learning than pure imitation.

### Mechanism 3
Generating rationales provides regularization that reduces susceptibility to dataset biases. By decomposing judgments into rationales and binary preferences, Con-J distributes the bias across multiple outputs, reducing the direct impact on the preference prediction component. The additional training target (rationale generation) acts as a regularizer that helps the model focus on task-relevant features rather than dataset artifacts.

## Foundational Learning

- **Concept**: Direct Preference Optimization (DPO)
  - Why needed here: Con-J uses DPO to train on contrastive judgments, making it essential to understand how this optimization technique differs from standard supervised learning approaches.
  - Quick check question: What is the key difference between DPO loss and standard cross-entropy loss in the context of preference learning?

- **Concept**: Contrastive learning
  - Why needed here: Con-J constructs contrastive judgment pairs from the same LLM's outputs, requiring understanding of how contrastive learning frameworks work in NLP.
  - Quick check question: How does the contrastive judgment construction process ensure that the model learns to distinguish between preferred and non-preferred answers?

- **Concept**: Supervised fine-tuning (SFT)
  - Why needed here: The paper compares Con-J with SFT-only variants, making it important to understand when SFT is sufficient versus when additional discriminative training is needed.
  - Quick check question: Under what conditions might SFT alone be sufficient for training a generative judge, and when would it fail?

## Architecture Onboarding

- **Component map**: Qwen2-7B-Instruct base model → Judgment Sampling (repeated/hint-driven) → Judgment Filtering → DPO/SFT Training → Inference (top-p sampling)
- **Critical path**: Prompt generation → Multiple LLM outputs → Contrastive pair construction → DPO/SFT training → Inference with top-p sampling
- **Design tradeoffs**: Con-J trades computational efficiency (multiple LLM calls for judgment sampling) for improved interpretability and bias robustness compared to scalar models.
- **Failure signatures**: Poor performance on preference prediction despite good rationale generation, high variance in outputs from repeated sampling, inability to construct contrastive pairs for certain prompts.
- **First 3 experiments**:
  1. Compare Con-J with SFT-only variant on a small preference dataset to verify the importance of contrastive training.
  2. Test Con-J's bias robustness by training on artificially biased data and evaluating on both general and adversarial test sets.
  3. Evaluate rationale quality improvement by training Con-J with increasing amounts of data and measuring rationale correctness scores.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does improving the quality of rationales during the sampling process enhance the model's preference prediction abilities?
- **Open Question 2**: Why does Con-J outperform the scalar model on complex, realistic datasets, and is this also related to bias?
- **Open Question 3**: How can Con-J facilitate human collaboration through interpretable preference judgments for LLM training?

## Limitations

- The computational overhead of multiple LLM calls for judgment sampling may limit practical applicability in resource-constrained settings.
- The effectiveness of DPO over SFT alone is demonstrated empirically but lacks theoretical justification for why it works better in preference learning.
- The correlation between rationale quality and preference judgment accuracy needs more rigorous validation across diverse domains.

## Confidence

- **High Confidence**: Con-J achieves comparable preference prediction accuracy to scalar models
- **Medium Confidence**: Con-J provides superior interpretability through generated rationales
- **Medium Confidence**: Con-J shows improved robustness against dataset bias compared to scalar models

## Next Checks

1. Test Con-J on out-of-domain preference datasets to evaluate generalization of rationale generation quality.
2. Compare computational efficiency of Con-J versus scalar models across different batch sizes and model scales.
3. Conduct ablation studies on the DPO vs SFT-only training to quantify the exact contribution of each component to performance improvements.