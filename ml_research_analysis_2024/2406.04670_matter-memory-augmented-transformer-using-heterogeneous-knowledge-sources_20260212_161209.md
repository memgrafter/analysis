---
ver: rpa2
title: 'MATTER: Memory-Augmented Transformer Using Heterogeneous Knowledge Sources'
arxiv_id: '2406.04670'
source_url: https://arxiv.org/abs/2406.04670
tags:
- knowledge
- question
- retriever
- retrieved
- sources
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MATTER, a memory-augmented transformer designed
  to efficiently retrieve and read from multiple heterogeneous knowledge sources for
  open-domain question answering. Unlike prior approaches that use a single type of
  knowledge source, MATTER can leverage both semi-structured QA pairs and unstructured
  paragraphs by representing them as fixed-length neural memories.
---

# MATTER: Memory-Augmented Transformer Using Heterogeneous Knowledge Sources

## Quick Facts
- arXiv ID: 2406.04670
- Source URL: https://arxiv.org/abs/2406.04670
- Authors: Dongkyu Lee; Chandana Satya Prakash; Jack FitzGerald; Jens Lehmann
- Reference count: 14
- MATTER-QA/PRG reaches 56.0 EM on TriviaQA, outperforming EMAT and QAMAT while maintaining 176 Q/s throughput

## Executive Summary
This paper introduces MATTER, a memory-augmented transformer designed to efficiently retrieve and read from multiple heterogeneous knowledge sources for open-domain question answering. Unlike prior approaches that use a single type of knowledge source, MATTER can leverage both semi-structured QA pairs and unstructured paragraphs by representing them as fixed-length neural memories. The model uses a shared encoder for cross-encoding questions and memories, enabling efficient retrieval from multiple knowledge sources with a single off-the-shelf retriever. Experiments show that MATTER achieves strong performance on three QA benchmarks while being 100x faster than retrieve-and-read models like FiD.

## Method Summary
MATTER is a memory-augmented transformer that leverages multiple heterogeneous knowledge sources for open-domain QA. It uses an off-the-shelf retriever to fetch top-k knowledge snippets (QA pairs and Wikipedia paragraphs), which are then converted into fixed-length neural memories. These memories are cross-encoded with the question using shared encoder layers, enabling efficient fusion of retrieved knowledge. The model generates answers conditioned on the fused representations, achieving high accuracy with significantly reduced computational overhead compared to retrieve-and-read models.

## Key Results
- MATTER-QA/PRG achieves 56.0 EM on TriviaQA, outperforming EMAT and QAMAT
- 100x faster than retrieve-and-read models like FiD while maintaining strong accuracy
- Effective performance across three benchmarks: TriviaQA, Natural Questions, and WebQuestions
- Successfully leverages both QA pairs and Wikipedia paragraphs as heterogeneous knowledge sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-encoding question and memory representations in the shared encoder layers enables effective fusion of retrieved knowledge with the input query.
- Mechanism: By mapping both the question and retrieved memories through the same encoder layers, the model leverages self-attention to align similar representations, facilitating information exchange between the question and context during cross-encoding.
- Core assumption: Self-attention mechanisms inherently promote attention between representations with similar semantic structure when processed by the same parameters.
- Evidence anchors:
  - [section] "self-attention is known to attend to similar representations with the use of dot-product. By sharing parameters of the encoder, the latent question representations and related knowledge in neural memories are likely to share similar representations, and hence they are likely to attend to each other during cross-encoding."
  - [abstract] "The model uses a shared encoder for cross-encoding questions and memories, enabling efficient retrieval from multiple knowledge sources with a single off-the-shelf retriever."
- Break condition: If the question and memory representations diverge significantly in embedding space, cross-attention may fail to establish meaningful connections, reducing the effectiveness of knowledge fusion.

### Mechanism 2
- Claim: Representing knowledge as fixed-length neural memories dramatically reduces computational overhead during inference.
- Mechanism: Instead of processing full retrieved text passages, the model maps each knowledge snippet to a small fixed number of latent vectors (e.g., 2), which are then cross-encoded with the question. This reduces the sequence length from potentially thousands of tokens to just a few dozen.
- Core assumption: A small number of well-trained latent vectors can capture sufficient semantic content from a knowledge snippet to support accurate answer generation.
- Evidence anchors:
  - [section] "A neural memory is an efficient way of storing knowledge with a fixed length latent representation... As a result, memory-augmented QA models generate an answer conditioned on retrieved neural memories, rather than retrieved raw text. This approach shortens the context length, enabling memory-augmented models to respond to several hundred questions per second."
  - [abstract] "Unlike prior approaches that use a single type of knowledge source, MATTER can leverage both semi-structured QA pairs and unstructured paragraphs by representing them as fixed-length neural memories."
- Break condition: If knowledge snippets are too complex or lengthy, reducing them to 2 vectors may lose critical information, leading to degraded QA performance.

### Mechanism 3
- Claim: Using an off-the-shelf retriever enables flexible and efficient retrieval from multiple heterogeneous knowledge sources without additional training overhead.
- Mechanism: A single pre-trained dense retriever is used to search across both QA pairs and Wikipedia paragraphs by treating each as a sequence and mapping them to the same vector space. This eliminates the need for training separate retrievers or maintaining multiple indices.
- Core assumption: A well-trained retriever can generalize across different knowledge structures (QA pairs vs. paragraphs) when both are represented as sequences.
- Evidence anchors:
  - [section] "our framework utilizes an off-the-shelf retriever, thereby eliminating retriever training which leads to several benefits such as extensibility... The intuition is from the recent finding that a well-trained retriever can be used as an universal retriever for varying structures."
  - [abstract] "Unlike existing retrieval models, MATTER retrieves from multiple heterogeneous knowledge sources."
- Break condition: If the retriever's training distribution does not adequately cover the semantic diversity of the combined knowledge sources, retrieval quality may suffer, especially for underrepresented formats.

## Foundational Learning

- Concept: Self-attention and cross-attention mechanisms in transformer architectures
  - Why needed here: The model relies on self-attention to align question and memory representations, and cross-attention in the decoder to focus on relevant memory vectors during answer generation.
  - Quick check question: How does self-attention in the encoder promote alignment between question and memory representations when they are processed by shared layers?

- Concept: Dense retrieval and vector similarity search (e.g., FAISS with HNSW index)
  - Why needed here: The retriever maps questions to dense vectors and performs maximum inner product search over a large knowledge index to fetch relevant memories.
  - Quick check question: What is the computational complexity of FAISS HNSW search compared to brute-force similarity search, and why is it suitable for this application?

- Concept: Knowledge representation and memory-augmented architectures
  - Why needed here: The model converts diverse knowledge sources into fixed-length neural memories, a key design choice that enables efficient cross-source retrieval and reduces context length.
  - Quick check question: Why might representing a long Wikipedia paragraph as only 2 latent vectors be both beneficial and risky for QA performance?

## Architecture Onboarding

- Component map:
  - Retriever: Dense vector retriever (e.g., MPNet-base with mean pooling) → FAISS HNSW index lookup → Top-k memory vectors
  - Encoder: Shared T5-base encoder → First j layers for question and memory encoding → Remaining layers for cross-encoding
  - Decoder: T5-base decoder with cross-attention over fused representations → Auto-regressive answer generation
  - Memory: Fixed-length latent vectors (e.g., 2 per knowledge snippet) stored as (key, value) pairs in the retriever index

- Critical path: Question → Retriever → Memory fetch → Cross-encoding → Answer generation
  - Bottlenecks: Retriever forward pass and FAISS search dominate latency; decoder cross-attention is lighter due to reduced memory length.

- Design tradeoffs:
  - Memory size (l=2) vs. information retention: Smaller memories speed up inference but risk losing nuance.
  - Number of retrievals (k) vs. accuracy: More retrievals improve coverage but increase search time and memory usage.
  - Single retriever vs. multiple retrievers: Simpler deployment and faster search, but may sacrifice precision on heterogeneous sources.

- Failure signatures:
  - Low EM scores with high retrieval relevance: Likely issue in cross-encoding or decoder conditioning on memories.
  - High latency despite memory use: Retriever or FAISS search not optimized; consider smaller retriever or approximate search.
  - Memory OOM errors: Reduce batch size or memory size (l); ensure FAISS index fits in RAM.

- First 3 experiments:
  1. Vary memory size (l=1, 2, 4) on a small subset of TriviaQA to measure EM vs. latency trade-off.
  2. Test single retriever vs. two retrievers (one per knowledge type) to quantify performance and speed differences.
  3. Sweep k=5, 10, 20 in zero-shot setting to identify optimal retrieval count before fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MATTER's performance scale with increasing numbers of heterogeneous knowledge sources beyond QA pairs and Wikipedia paragraphs?
- Basis in paper: [inferred] The paper mentions that MATTER can be extended to retrieve from structured knowledge sources like knowledge graphs, but this was left as future work.
- Why unresolved: The paper only experimentally tested two types of knowledge sources (QA pairs and Wikipedia paragraphs), leaving the scalability to additional knowledge source types unexplored.
- What evidence would resolve it: Experiments showing MATTER's performance on benchmarks when incorporating additional knowledge sources like knowledge graphs, with comparisons to the performance using only QA pairs and paragraphs.

### Open Question 2
- Question: What is the impact of varying the memory size (l) on MATTER's performance and efficiency?
- Basis in paper: [explicit] The paper sets the memory size to 2 but does not explore how varying this parameter affects performance.
- Why unresolved: The paper fixed the memory size at 2 without investigating how different memory sizes might impact the trade-off between performance and efficiency.
- What evidence would resolve it: Experiments varying the memory size parameter and measuring the resulting changes in EM scores and inference speed across different benchmarks.

### Open Question 3
- Question: How does MATTER's performance compare to retrieve-and-read models when the retriever is given equal or superior quality?
- Basis in paper: [explicit] The paper shows that MATTER outperforms EMAT even with an inferior retriever, suggesting the reader model is a key contributor to performance gains.
- Why unresolved: While the paper demonstrates MATTER's superiority over EMAT, it doesn't explore how MATTER would perform against retrieve-and-read models with retrievers of equal or superior quality.
- What evidence would resolve it: Head-to-head comparisons between MATTER and retrieve-and-read models using the same high-quality retriever, measuring EM scores and inference speeds.

## Limitations

- The claim that a single off-the-shelf retriever can effectively handle heterogeneous knowledge sources without additional training is based on recent findings but not empirically validated within this work.
- The 100x speedup claim compared to FiD models should be contextualized - while memory-augmented approaches do reduce computational overhead, the comparison assumes similar hardware configurations and doesn't account for potential trade-offs in answer quality.
- The experimental evaluation was conducted primarily on English-language datasets with relatively short answers (average 3-5 tokens), leaving effectiveness on longer-form answers, multi-hop reasoning tasks, or non-English languages unexplored.

## Confidence

**High Confidence**: The architectural design of MATTER is clearly specified and reproducible. The use of shared encoder layers for cross-encoding and the memory-augmented approach are well-documented. The reported throughput improvements and EM scores are supported by experimental results on standard benchmarks.

**Medium Confidence**: The claim that heterogeneous knowledge sources improve QA performance is supported by experimental results, but the specific contribution of each knowledge source (QA pairs vs. Wikipedia) to overall performance is not fully isolated. The paper demonstrates improvement over single-source approaches but doesn't systematically analyze which types of questions benefit most from each source.

**Low Confidence**: The assertion that a single off-the-shelf retriever can effectively handle heterogeneous knowledge sources without additional training is based on recent findings but not empirically validated within this work. The paper cites this as a benefit but doesn't compare against specialized retrievers or analyze retrieval quality across different knowledge types.

## Next Checks

1. **Memory Representation Fidelity Analysis**: Conduct ablation studies varying memory size (l=1, 2, 4) on a subset of TriviaQA questions, measuring both EM scores and qualitative analysis of generated answers to assess information retention trade-offs.

2. **Retriever Specialization Impact**: Compare MATTER's performance using the single off-the-shelf retriever against a two-retriever setup (one per knowledge type) on the same benchmarks to quantify the cost/benefit of retriever specialization versus generalization.

3. **Cross-Encoding Alignment Verification**: Analyze the self-attention patterns in the shared encoder layers to verify that question and memory representations are indeed attending to each other as claimed. This could involve visualizing attention weights or measuring representation similarity across different knowledge types.