---
ver: rpa2
title: Autoregressive Speech Synthesis without Vector Quantization
arxiv_id: '2407.08551'
source_url: https://arxiv.org/abs/2407.08551
tags:
- melle
- speech
- sampling
- language
- all-e
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MELLE, a continuous-valued token-based autoregressive
  language model for text-to-speech synthesis that bypasses the need for vector quantization.
  MELLE directly predicts mel-spectrogram frames from text prompts using a decoder-only
  architecture with a novel latent sampling module and spectrogram flux loss to improve
  diversity and robustness.
---

# Autoregressive Speech Synthesis without Vector Quantization

## Quick Facts
- arXiv ID: 2407.08551
- Source URL: https://arxiv.org/abs/2407.08551
- Reference count: 26
- Key outcome: MELLE achieves 47.9% relative WER reduction on continuation tasks compared to VALL-E variants while maintaining or exceeding ground truth quality

## Executive Summary
This paper introduces MELLE, a continuous-valued token-based autoregressive language model for text-to-speech synthesis that eliminates the need for vector quantization. MELLE directly predicts mel-spectrogram frames from text prompts using a decoder-only transformer architecture enhanced with a novel latent sampling module and spectrogram flux loss. The model employs variational inference to enable sampling in continuous space, avoiding the robustness issues of discrete codec models. Experiments on large-scale datasets show MELLE outperforms existing approaches across multiple metrics while maintaining generation quality.

## Method Summary
MELLE is a transformer decoder-only architecture that directly predicts continuous mel-spectrogram frames from text prompts, bypassing vector quantization entirely. The model uses variational inference with a learned latent sampling module to introduce diversity in continuous space, and employs a spectrogram flux loss to encourage dynamic variation between consecutive frames. Training uses regression loss (L1+L2) on mel-spectrograms, KL divergence for latent space regularization, flux loss for frame diversity, and stop prediction loss. The model is trained on 50K hours of Libriheavy data and evaluated on LibriSpeech test-clean with continuation and cross-sentence tasks.

## Key Results
- 47.9% relative WER reduction on continuation tasks compared to VALL-E variants
- 10.8% relative WER reduction on cross-sentence tasks
- Achieves speaker similarity scores (SIM) of 0.619 on continuation tasks, approaching ground truth (0.644)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous-valued mel-spectrogram tokens eliminate fidelity loss from vector quantization while preserving richer acoustic detail.
- Mechanism: MELLE directly predicts continuous mel-spectrogram frames rather than discrete codec codes, avoiding the information compression inherent in quantization.
- Core assumption: Mel-spectrogram representations contain sufficient information for high-quality speech reconstruction when paired with an appropriate vocoder.
- Evidence anchors:
  - [abstract]: "bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations"
  - [section]: "We attribute this advantage to our direct prediction of spectrograms, which encompass richer acoustic cues compared to discrete codes"
  - [corpus]: Weak evidence - no direct citations found supporting fidelity claims specifically for mel-spectrogram vs VQ

### Mechanism 2
- Claim: Variational inference with learned latent sampling enables robust, diverse generation in continuous space without discrete sampling strategies.
- Mechanism: A latent sampling module predicts a Gaussian distribution for each timestep, samples from it using reparameterization, and maps to spectrogram space, introducing controlled diversity.
- Core assumption: Modeling the latent space as a Gaussian distribution conditioned on model output allows effective sampling while maintaining generation quality.
- Evidence anchors:
  - [abstract]: "we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness"
  - [section]: "inspired by variational autoencoder (V AE), we integrate a novel latent sampling module within MELLE, aimed at enhancing both expressive diversity and robustness"
  - [corpus]: No direct citations found for this specific variational inference approach in continuous TTS space

### Mechanism 3
- Claim: Spectrogram flux loss encourages dynamic variation between consecutive frames, preventing repetitive or static outputs.
- Mechanism: A loss term penalizes low variability between predicted Gaussian means and previous ground truth frames, rewarding changes in the generated sequence.
- Core assumption: Explicitly encouraging frame-to-frame variation during training prevents the model from generating overly static or repetitive speech patterns.
- Evidence anchors:
  - [abstract]: "we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens"
  - [section]: "To encourage dynamic variation in the generated frames, a novel spectrogram flux loss is proposed as a regulariza-tion term that penalizes low variability between consecutive frames and promotes changes"
  - [corpus]: No direct citations found for spectrogram flux loss in continuous TTS or similar domains

## Foundational Learning

- Concept: Autoregressive language modeling
  - Why needed here: MELLE treats mel-spectrogram generation as a sequence prediction problem where each frame depends on previous frames and text context
  - Quick check question: How does conditioning on both text and previous acoustic frames enable zero-shot TTS synthesis?

- Concept: Variational inference and latent variable modeling
  - Why needed here: Enables sampling in continuous space to introduce diversity and robustness without discrete token sampling strategies
  - Quick check question: What role does the KL divergence loss play in regularizing the learned latent space distribution?

- Concept: Regression loss vs. cross-entropy loss
  - Why needed here: Continuous-valued tokens require regression objectives rather than discrete classification objectives used in VQ-based models
  - Quick check question: Why is L1/L2 regression loss more appropriate than cross-entropy for predicting continuous mel-spectrogram values?

## Architecture Onboarding

- Component map: Text tokenizer (BPE) → Text embedding layer → Transformer decoder LM → Latent sampling module → Stop prediction layer → Post-net (convolutional blocks) → Vocoder
- Critical path: Text → LM embedding → Latent sampling → Mel-spectrogram generation → Post-net refinement → Vocoder → Speech output
- Design tradeoffs:
  - Single-stage vs. two-stage: MELLE eliminates the NAR refinement stage of codec models, reducing complexity but requiring direct continuous prediction capability
  - Reduction factor r: Balances inference speed vs. generation quality; higher r accelerates inference but may reduce SIM
  - Continuous vs. discrete: Continuous tokens provide higher fidelity but require regression loss and sampling strategies instead of simple discrete sampling
- Failure signatures:
  - Excessive silence or repetition: May indicate issues with spectrogram flux loss weighting or latent sampling instability
  - Poor speaker similarity: Could indicate latent sampling module not capturing speaker characteristics effectively
  - Unstable generation: May result from improper KL loss weighting or latent space collapse
- First 3 experiments:
  1. Train baseline MELLE without latent sampling or spectrogram flux loss to establish baseline WER/WERH performance
  2. Enable latent sampling module while keeping flux loss disabled to isolate its impact on diversity and speaker similarity
  3. Enable both components and experiment with different reduction factors (r=1,2,3,4,5) to find optimal speed-quality tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MELLE's performance scale with training data size beyond 50K hours, and is there a point of diminishing returns?
- Basis in paper: [inferred] The paper shows MELLE trained on 50K hours of Libriheavy data and a limited version on 960 hours of LibriSpeech. The authors compare MELLE to VALL-E and variants, but do not explore training with larger datasets or analyze scaling trends.
- Why unresolved: The paper does not provide experiments with datasets larger than 50K hours or analyze the relationship between training data size and model performance.
- What evidence would resolve it: Experiments training MELLE on progressively larger datasets (e.g., 100K, 200K hours) with performance metrics plotted against dataset size to identify scaling trends and potential saturation points.

### Open Question 2
- Question: What is the impact of different continuous speech representations (e.g., VAE latent states) compared to mel-spectrograms in autoregressive TTS models like MELLE?
- Basis in paper: [explicit] The paper states "We adopt only the mel-spectrogram as the target continuous acoustic representation. Future research will explore other continuous representations, such as VAE latent hidden states."
- Why unresolved: The paper focuses exclusively on mel-spectrograms and does not experiment with or compare other continuous representations like VAE latent states or raw waveform representations.
- What evidence would resolve it: Training MELLE with different continuous representations (VAE latents, raw waveforms, or other learned continuous codes) and comparing performance metrics like WER, MOS, and speaker similarity across these representations.

### Open Question 3
- Question: How does MELLE's latent sampling module compare to explicit sampling strategies used in discrete codec models like top-p sampling in terms of diversity and quality trade-offs?
- Basis in paper: [explicit] The paper introduces a latent sampling module derived from variational inference as an alternative to top-p random sampling used in discrete codec models, claiming it enhances diversity and robustness, but does not directly compare these sampling strategies.
- Why unresolved: While the paper describes MELLE's sampling approach and claims advantages over discrete codec models, it does not conduct direct comparative experiments between latent sampling and explicit sampling strategies like top-p on the same model architecture.
- What evidence would resolve it: Ablation studies comparing MELLE with and without the latent sampling module, or direct comparisons between MELLE's latent sampling and top-p sampling applied to a MELLE-like model, measuring diversity metrics and quality metrics across different sampling temperatures/parameters.

## Limitations

- The paper lacks direct empirical validation comparing continuous mel-spectrogram representations against VQ-based approaches for reconstruction quality and information preservation
- Novel components (variational inference latent sampling and spectrogram flux loss) are not benchmarked against established alternatives like discrete sampling strategies or other diversity-promoting techniques
- Evaluation is limited to continuation and cross-sentence tasks on LibriSpeech test-clean, with no testing on out-of-domain datasets or real-world TTS scenarios

## Confidence

**High Confidence**: Claims about MELLE's performance improvements over VALL-E and variants on LibriSpeech test-clean (WER, SIM, MOS metrics)

**Medium Confidence**: Claims about the general superiority of continuous representations over VQ-based approaches

**Low Confidence**: Claims about the specific mechanisms (variational inference, spectrogram flux loss) being responsible for MELLE's performance

## Next Checks

1. **Ablation study of novel components**: Train MELLE variants with (a) latent sampling disabled, (b) flux loss disabled, and (c) both disabled, measuring performance on continuation tasks to isolate the contribution of each mechanism to the reported improvements.

2. **Direct comparison of continuous vs. discrete representations**: Implement a VQ-based baseline using identical architecture (same decoder, pre/post-nets) but with vector quantization, then compare reconstruction quality, generation diversity, and robustness between continuous and discrete token approaches on the same evaluation tasks.

3. **Cross-corpus generalization test**: Evaluate MELLE on out-of-domain datasets (e.g., VCTK, Common Voice, noisy speech) and in few-shot adaptation scenarios to assess whether the reported advantages generalize beyond the LibriSpeech test-clean corpus.