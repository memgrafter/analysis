---
ver: rpa2
title: 'Dense Dynamics-Aware Reward Synthesis: Integrating Prior Experience with Demonstrations'
arxiv_id: '2412.01114'
source_url: https://arxiv.org/abs/2412.01114
tags:
- learning
- agent
- demonstrations
- state
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reward-shaping framework that integrates
  task-agnostic prior data with task-specific expert demonstrations to synthesize
  dense dynamics-aware rewards for long-horizon sparse-reward reinforcement learning
  tasks. The method pre-trains a goal-conditioned value function from prior data,
  then uses it with expert demonstrations to construct a potential function that guides
  exploration toward the goal.
---

# Dense Dynamics-Aware Reward Synthesis: Integrating Prior Experience with Demonstrations

## Quick Facts
- arXiv ID: 2412.01114
- Source URL: https://arxiv.org/abs/2412.01114
- Authors: Cevahir Koprulu; Po-han Li; Tianyu Qiu; Ruihan Zhao; Tyler Westenbroek; David Fridovich-Keim; Sandeep Chinchali; Ufuk Topcu
- Reference count: 20
- One-line primary result: Achieves 95-99% success rates on a simulated robot pushing task by integrating prior data with expert demonstrations to synthesize dense rewards

## Executive Summary
This paper addresses the challenge of learning long-horizon sparse-reward tasks in robotics by proposing a method that synthesizes dense dynamics-aware rewards from task-agnostic prior data and task-specific expert demonstrations. The approach pre-trains a goal-conditioned value function from prior data, then constructs a potential function using expert demonstrations to guide exploration. By integrating potential-based reward shaping with discount factor annealing, the method enables efficient learning even when demonstrations are sub-optimal or when there is dynamics shift between prior data and target environment.

## Method Summary
The method consists of three main phases: (1) pre-training a goal-conditioned value function on prior data to capture environment dynamics, (2) collecting expert demonstrations for the target task and constructing a potential function that combines demonstration progress with goal-reaching estimates, and (3) training a reinforcement learning agent using shaped rewards derived from the potential function with annealing discount factor. The approach is demonstrated on a simulated Franka Panda robot pushing task where the agent must push a puck into a U-shaped slot, achieving high success rates with both perfect state observations and high-dimensional point cloud observations.

## Key Results
- Achieved 95-99% success rates on the target pushing task compared to baselines that failed to learn
- Maintained effectiveness with sub-optimal demonstrations and dynamics shift between prior data and target environment
- Successfully handled high-dimensional point cloud observations without requiring expert actions
- Demonstrated robustness to variations in demonstration quality and environment dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward shaping via potential function biases exploration toward demonstrations while preserving optimal policies
- Mechanism: The method constructs a potential function Φ(s) combining goal-conditioned value estimates and demonstration progress, then uses it in potential-based reward shaping (PBRS) to create dense rewards. This guides exploration while theoretically preserving optimal policies through the PBRS formalism
- Core assumption: The potential function Φ(s) approximates the optimal value function well enough to guide exploration without compromising final performance
- Evidence anchors:
  - [abstract] "uses these priors to synthesize dense dynamics-aware rewards for the given task"
  - [section] "We then leverage the PBRS formalism (Ng et al., 1999) and define the dense rewards: ¯r(st, at) = r(st, at) + γΦ(st+1) − Φ(st)"
  - [corpus] "The success of many RL techniques heavily relies on human-engineered dense rewards, which typically demand substantial domain expertise and extensive trial and error"
- Break condition: If Φ(s) poorly approximates the optimal value function, exploration may be misdirected and final performance compromised

### Mechanism 2
- Claim: Discount factor annealing enables greedy exploration that follows the potential function
- Mechanism: By starting with a small discount factor (¯γ = 0) and annealing to the original value, the agent greedily follows the potential function early in training, then recovers the optimal policy. This allows reliance on prior information when data coverage is limited
- Core assumption: The potential function provides sufficient guidance for effective exploration even when acting greedily
- Evidence anchors:
  - [abstract] "Our approach encodes information about the dynamics during an initial pre-training phase"
  - [section] "We can more aggressively and directly incentivize the agent to follow the potential during exploration by altering the discount factor used during online training"
  - [corpus] "One promising solution is to view task progress as a dense reward signal"
- Break condition: If the potential function is inaccurate, greedy behavior based on it may lead to poor exploration

### Mechanism 3
- Claim: Learning from partial observations extends the method to real-world demonstration scenarios
- Mechanism: The framework can work with state trajectories alone, not requiring expert actions. It can also handle partial state information or high-dimensional observations by defining goal sets in observation space rather than full state space
- Core assumption: The goal-conditioned value function can be learned effectively even when goals are defined only on subsets of state variables
- Evidence anchors:
  - [abstract] "Our reward-shaping approach only requires the expert's states"
  - [section] "Rather than specifying goals with full desired states, in many scenarios, only a subset of states is crucial for defining goals"
  - [corpus] "However, obtaining a robust and dense reward signal for general manipulation tasks remains a challenge"
- Break condition: If critical state variables are missing from observations, the method may fail to learn effective policies

## Foundational Learning

- Concept: Goal-conditioned value functions
  - Why needed here: The method relies on estimating how many steps are needed to reach various goals from different states, which is precisely what goal-conditioned value functions provide
  - Quick check question: What does V_g(s; sd) represent in the context of this paper?

- Concept: Potential-based reward shaping
  - Why needed here: The dense rewards are constructed using the PBRS formalism, which preserves optimal policies while adding shaping rewards
  - Quick check question: Why does adding γΦ(s') - Φ(s) to rewards preserve the optimal policy?

- Concept: Discount factor effects on exploration
  - Why needed here: The method uses discount factor annealing to control the exploration-exploitation tradeoff, with lower values encouraging greedy behavior toward the potential
  - Quick check question: How does changing the discount factor from 0 to γ affect the agent's behavior during training?

## Architecture Onboarding

- Component map:
  - Prior data preprocessing -> Goal-conditioned value function learning (˜Vg)
  - Expert demonstrations collection -> Demonstration storage
  - Reward synthesis module -> Potential function Φ(s) computation
  - RL agent -> Policy optimization with shaped rewards
  - Discount factor scheduler -> Annealing schedule management

- Critical path:
  1. Pre-train ˜Vg from prior data
  2. Collect expert demonstrations
  3. Construct potential function Φ(s)
  4. Generate dense rewards ¯r(st, at)
  5. Train RL agent with shaped rewards
  6. Anneal discount factor during training

- Design tradeoffs:
  - Accuracy of ˜Vg vs. pre-training time/cost
  - Number of demonstrations vs. coverage/completeness of Φ(s)
  - Discount factor schedule vs. exploration-exploitation balance
  - State space vs. observation space goal definitions

- Failure signatures:
  - Poor performance despite demonstrations: ˜Vg may be inaccurate or demonstrations may be poor quality
  - Slow learning: Potential function may not provide sufficient guidance
  - Suboptimal final policies: Discount factor annealing schedule may be inappropriate

- First 3 experiments:
  1. Validate ˜Vg learning: Test goal-reaching accuracy on a simple task with known optimal solution
  2. Test potential function construction: Verify Φ(s) values increase along demonstration trajectories
  3. Ablation study: Compare learning curves with and without reward shaping on a simple sparse-reward task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Dense Dynamics-Aware Reward Synthesis scale with the number of demonstrations provided?
- Basis in paper: [inferred] The paper uses a fixed number of demonstrations (1-2) in their experiments but does not explore the effect of varying this number.
- Why unresolved: The paper only tests with a small, fixed number of demonstrations and does not provide analysis on how performance changes with more demonstrations.
- What evidence would resolve it: Experiments varying the number of demonstrations (e.g., 1, 5, 10, 50) and measuring success rates and learning speed.

### Open Question 2
- Question: What is the theoretical limit on how far apart the demonstrations can be from the goal state while still maintaining effectiveness?
- Basis in paper: [explicit] The paper mentions that they train goal-conditioned value functions only for nearby goals (up to 0.1 meter) and that demonstrations must be within "tubes" around expert demonstrations.
- Why unresolved: The paper does not provide theoretical bounds or extensive empirical testing of the maximum distance demonstrations can be from the goal.
- What evidence would resolve it: Theoretical analysis of the conditions under which demonstrations can be arbitrarily far from the goal, combined with empirical testing of demonstrations at varying distances.

### Open Question 3
- Question: How does the method perform when there is significant dynamics shift between the prior data and target environment?
- Basis in paper: [explicit] The paper mentions that the method should still work with dynamics shift but only provides limited testing in the pushing environment.
- Why unresolved: While the paper claims robustness to dynamics shift, it only tests this in one specific environment with limited variations.
- What evidence would resolve it: Systematic experiments varying the dynamics between prior data and target environment (e.g., different friction coefficients, masses, or completely different physics) and measuring performance degradation.

## Limitations
- Performance heavily relies on quality of prior data and expert demonstrations
- Method assumes the potential function can be accurately constructed from demonstrations
- Limited analysis of failure cases when assumptions break down
- Limited evaluation beyond a single simulated task

## Confidence
- **High confidence**: The theoretical foundation using potential-based reward shaping (PBRS) and discount factor annealing is well-established and mathematically sound
- **Medium confidence**: The empirical results showing 95-99% success rates are convincing, but the evaluation is limited to a single simulated task with specific demonstration quality
- **Medium confidence**: The claim about handling partial observations and sub-optimal demonstrations is supported by experiments, but the robustness analysis is relatively limited

## Next Checks
1. Test the method with varying qualities of demonstrations (random, suboptimal, optimal) to establish performance boundaries
2. Evaluate the impact of prior data quality and quantity on final performance through systematic ablation studies
3. Assess generalization to tasks with different dynamics than the prior data by introducing systematic domain shifts