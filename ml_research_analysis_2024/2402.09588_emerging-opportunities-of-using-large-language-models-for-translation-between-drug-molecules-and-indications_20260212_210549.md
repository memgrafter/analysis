---
ver: rpa2
title: Emerging Opportunities of Using Large Language Models for Translation Between
  Drug Molecules and Indications
arxiv_id: '2402.09588'
source_url: https://arxiv.org/abs/2402.09588
tags:
- drug
- smiles
- indications
- strings
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study explores using large language models (LLMs) to translate
  between drug molecules (represented as SMILES strings) and their therapeutic indications.
  The authors propose two tasks: drug-to-indication (generating indications from SMILES)
  and indication-to-drug (generating SMILES from indications).'
---

# Emerging Opportunities of Using Large Language Models for Translation Between Drug Molecules and Indications

## Quick Facts
- arXiv ID: 2402.09588
- Source URL: https://arxiv.org/abs/2402.09588
- Reference count: 40
- Larger MolT5 models outperform smaller ones across all tasks and datasets, but fine-tuning degrades performance

## Executive Summary
This study explores the use of large language models (LLMs) to translate between drug molecules represented as SMILES strings and their therapeutic indications. The authors propose two translation tasks: drug-to-indication (generating indications from SMILES) and indication-to-drug (generating SMILES from indications). Nine variations of the T5 LLM (MolT5) are evaluated on datasets from ChEMBL and DrugBank. Results demonstrate that larger MolT5 models perform better than smaller ones, but fine-tuning on new data actually worsens performance. The study identifies challenges including poor signal between SMILES and indications, limited data availability, and the need for intermediate representations to improve translation accuracy.

## Method Summary
The authors use nine variations of T5-based MolT5 models to perform bidirectional translation between SMILES strings and drug indications. They evaluate baseline models on full and 20% subsets of DrugBank (3,004 pairs) and ChEMBL (6,127 pairs) datasets. Fine-tuning experiments train models on 80% of data and evaluate on the remaining 20%. The study also tests a custom tokenizer approach by pretraining MolT5-Small with a custom SMILES tokenizer. Performance is measured using text generation metrics (BLEU, ROUGE, METEOR) for drug-to-indication tasks and molecular similarity metrics (fingerprint comparisons, validity checks) for indication-to-drug tasks.

## Key Results
- Larger MolT5 models consistently outperform smaller models across all tasks and datasets
- Fine-tuning degrades model performance, yielding worse results than baseline approaches
- A custom tokenizer approach shows promise but still underperforms expectations
- Poor signal between SMILES strings and indications due to weak relationship between molecular structure and therapeutic use

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning degrades model performance on drug-to-indication and indication-to-drug tasks
- Mechanism: Fine-tuning introduces noise by mixing unrelated domain-specific patterns from molecular caption training with indication data, weakening the alignment between SMILES and indications
- Core assumption: SMILES-to-indication relationships are fundamentally different from SMILES-to-caption relationships
- Evidence anchors:
  - [abstract]: "fine-tuning degrades performance"
  - [section]: "fine-tuning the models on the new data worsens performance, reflected in FT experiments yielding worse results than SUB or FULL experiments"
  - [corpus]: Weak - no corpus evidence directly supports this claim
- Break condition: If fine-tuning on curated, domain-specific indication data with stronger signal between SMILES and indications shows improvement

### Mechanism 2
- Claim: Larger MolT5 models perform better across all tasks and datasets
- Mechanism: Larger models have more parameters to capture subtle patterns and relationships between SMILES strings and indications, even when signal is weak
- Core assumption: Model capacity directly correlates with ability to learn from sparse, noisy data
- Evidence anchors:
  - [abstract]: "Larger MolT5 models outperformed the smaller ones across all configurations and tasks"
  - [section]: "At the same time, larger models tend to perform better across all metrics for each experiment"
  - [corpus]: Weak - no corpus evidence directly supports this claim
- Break condition: If smaller models with specialized architectures or tokenizers outperform larger models on the same tasks

### Mechanism 3
- Claim: Poor signal between SMILES and indications due to lack of direct relationship
- Mechanism: Similar SMILES strings may have completely different indications, and different SMILES strings may treat similar conditions, making it difficult for models to learn meaningful mappings
- Core assumption: The relationship between molecular structure and therapeutic use is not straightforward or directly encoded in SMILES notation
- Evidence anchors:
  - [abstract]: "The signal between SMILES strings and indications is poor"
  - [section]: "similar SMILES strings might have completely different textual descriptions as they are different drugs, and their indications also differ"
  - [corpus]: Weak - no corpus evidence directly supports this claim
- Break condition: If intermediate representations or enriched data establish clearer relationships between SMILES and indications

## Foundational Learning

- Concept: SMILES (Simplified Molecular-Input Line-Entry System) notation
  - Why needed here: The entire translation task operates on SMILES strings as the molecular representation format
  - Quick check question: What does the SMILES string "CCO" represent?

- Concept: Tokenization strategies for molecular text
  - Why needed here: Different tokenization approaches (standard vs custom) significantly impact model performance on molecular tasks
  - Quick check question: How does a custom tokenizer for SMILES differ from standard language tokenization?

- Concept: Molecular fingerprints and similarity metrics
  - Why needed here: Evaluation of generated SMILES strings requires comparison to ground truth using chemical similarity measures
  - Quick check question: What is the difference between MACCS and Morgan fingerprints?

## Architecture Onboarding

- Component map: Input (SMILES or indication) -> Tokenizer (standard T5 or custom SMILES) -> MolT5 model (various sizes) -> Output generation -> Post-processing (validity check) -> Evaluation

- Critical path: Input → Tokenizer → Model (forward pass) → Output generation → Post-processing (validity check) → Evaluation

- Design tradeoffs:
  - Model size vs computational cost: Larger models perform better but require more resources
  - Tokenization approach: Custom tokenizers may better capture molecular grammar but require retraining
  - Fine-tuning vs. zero-shot: Fine-tuning can introduce noise but may help with domain adaptation

- Failure signatures:
  - Low BLEU/ROUGE scores indicate poor text generation quality
  - Low fingerprint similarity scores indicate generated molecules are chemically dissimilar to targets
  - High proportion of invalid SMILES strings indicates model doesn't understand molecular syntax

- First 3 experiments:
  1. Evaluate baseline MolT5-small on DrugBank dataset (full data) for drug-to-indication task
  2. Compare standard vs custom tokenizer performance on ChEMBL dataset for indication-to-drug task
  3. Test fine-tuning impact by training MolT5-base on 80% of DrugBank data and evaluating on remaining 20%

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and detail of drug indication descriptions in ChEMBL compare to DrugBank, and how does this affect LLM performance?
- Basis in paper: [explicit] The paper notes that DrugBank provides more in-depth descriptions of how each drug treats patients compared to ChEMBL's list of medical conditions, and that the custom tokenizer performed better on DrugBank data for drug-to-indication task.
- Why unresolved: The paper only qualitatively mentions the difference in detail level but does not provide a systematic analysis of how this difference impacts model performance.
- What evidence would resolve it: A controlled study comparing model performance using uniformly detailed indication descriptions from both datasets, or a detailed content analysis of the indication descriptions in both databases.

### Open Question 2
- Question: What intermediate representation could bridge the gap between SMILES strings and drug indications to improve translation performance?
- Basis in paper: [explicit] The paper suggests that having an intermediate representation that drugs (or indications) map to may improve performance, and mentions mapping a SMILES string to its caption (MolT5 task) and then caption to indication as a potential future direction.
- Why unresolved: The paper only proposes this as a hypothesis without exploring or testing specific intermediate representations.
- What evidence would resolve it: Experiments testing different intermediate representations (e.g., molecular descriptors, drug targets, pathway information) to identify which most effectively improves translation performance.

### Open Question 3
- Question: How can we enrich the limited drug-indication data to improve the signal between SMILES strings and indications for LLM training?
- Basis in paper: [explicit] The paper identifies data scarcity as an issue, noting that the combined size of both datasets is under 10,000 drug-indication pairs, and suggests finding ways to enrich data as a potential avenue for exploration.
- Why unresolved: The paper only identifies this as a problem without proposing or testing specific data enrichment strategies.
- What evidence would resolve it: Experiments implementing and evaluating various data enrichment techniques such as data augmentation, transfer learning from related domains, or incorporating additional drug information (e.g., targets, pathways) to increase the effective size and quality of the training data.

## Limitations

- Fundamental challenge of poor signal between SMILES strings and therapeutic indications, where similar molecular structures often treat different conditions
- Limited dataset sizes (under 10,000 total drug-indication pairs) constraining model learning and generalization
- Custom tokenizer approach, while promising, still underperforms expectations and requires further investigation

## Confidence

**High Confidence**: Claims about larger MolT5 models consistently outperforming smaller variants are supported by direct experimental evidence showing improved metrics across all configurations and tasks.

**Medium Confidence**: The observation that fine-tuning degrades performance is consistently reported, but the underlying mechanism remains unclear and requires further investigation to determine if this is a general phenomenon or specific to current implementation approaches.

**Low Confidence**: Claims about the fundamental difficulty of SMILES-to-indication translation due to poor signal are well-reasoned but not yet empirically validated through controlled experiments testing intermediate representations or enriched data approaches.

## Next Checks

1. **Intermediate Representation Validation**: Test whether introducing molecular fingerprint or pharmacophore representations as intermediate steps between SMILES and indications improves translation performance compared to direct SMILES-to-text approaches.

2. **Signal Strength Analysis**: Systematically evaluate the relationship between molecular similarity (using multiple fingerprint metrics) and indication similarity across the dataset to quantify the actual signal strength and identify specific failure patterns.

3. **Dataset Enrichment Experiment**: Create an augmented dataset by incorporating drug indication information from complementary sources (e.g., clinical trial databases, prescribing information) and evaluate whether this enriched data improves model performance beyond the original ChEMBL and DrugBank datasets.