---
ver: rpa2
title: Large Language Models Know What To Say But Not When To Speak
arxiv_id: '2410.16044'
source_url: https://arxiv.org/abs/2410.16044
tags:
- trps
- llms
- each
- turn-taking
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of Large Language Models (LLMs)
  to predict within-turn Transition Relevance Places (TRPs) in natural, unscripted
  conversations. The authors develop a novel dataset of participant-labeled within-turn
  TRPs by having individuals verbally respond at perceived opportunities for speech
  during unscripted dialogues.
---

# Large Language Models Know What To Say But Not When To Speak

## Quick Facts
- **arXiv ID**: 2410.16044
- **Source URL**: https://arxiv.org/abs/2410.16044
- **Reference count**: 15
- **Primary result**: LLMs underperform on predicting within-turn TRPs in unscripted conversations, with best model (GPT-4 Omni) achieving F1 of 0.151

## Executive Summary
This paper investigates whether Large Language Models can predict within-turn Transition Relevance Places (TRPs) in natural, unscripted conversations. Despite strong performance on written-language benchmarks, LLMs significantly underperform on this task, achieving low precision (0.137) and recall (0.169) even with theoretical background or participant instructions. The study develops a novel dataset of participant-labeled within-turn TRPs from unscripted dialogues and provides a specialized corpus for future research on improving turn-taking in dialogue systems.

## Method Summary
The study uses in-context learning with state-of-the-art LLMs to predict TRPs in unscripted dialogues. Researchers created a novel dataset by having participants verbally respond at perceived opportunities for speech during unscripted conversations from the In Conversation Corpus (ICC). The task is framed as binary classification, predicting whether a TRP occurs after each word in a turn. Four stimulus lists were created from 55 selected turns, with participant responses recorded on synchronized audio channels. Models were evaluated using multiple metrics including precision, recall, F1 score, kappa statistics, and normalized error measures.

## Key Results
- Best-performing model (GPT-4 Omni) achieved precision of 0.137, recall of 0.169, and F1 score of 0.151
- All evaluated models (GPT-4 Omni, Phi3, Gemma2, Llama3.1, Mistral) underperformed on TRP prediction despite strong written-language benchmarks
- LLMs showed poor performance even when provided with theoretical background or participant instructions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs can leverage linguistic content alone to predict TRPs if they have learned necessary patterns during pretraining
- **Mechanism**: LLMs process sequential word embeddings incrementally, using contextual cues embedded in language to anticipate TRPs without relying on prosodic or visual signals
- **Core assumption**: Linguistic content is sufficient for humans to predict TRPs, and therefore LLMs pretrained on large corpora should also learn these cues
- **Evidence anchors**: Approaches like TurnGPT and RC-TurnGPT introduce probabilistic models to predict TRPs using contextual and speaker-identity information
- **Break condition**: If linguistic content alone proves insufficient—if LLMs require acoustic or multimodal cues not present in text-only training—this mechanism fails

### Mechanism 2
- **Claim**: LLMs' pretraining on written language provides insufficient grounding for spoken interaction dynamics
- **Mechanism**: Written language lacks the real-time, incremental, and context-sensitive features of spontaneous spoken dialogue, so LLMs fail to internalize the timing and flow of natural turn-taking
- **Core assumption**: Spoken and written language are structurally distinct in use
- **Evidence anchors**: Most methods struggle to handle unscripted spoken interactions, often resulting in long silences or poorly timed feedback
- **Break condition**: If LLMs trained on diverse dialogue data (e.g., transcribed speech) show improved TRP prediction, this mechanism is invalidated

### Mechanism 3
- **Claim**: In-context learning is insufficient for fine-tuning LLMs on the nuanced task of TRP prediction
- **Mechanism**: The complex, context-dependent nature of TRP prediction requires more extensive adaptation than what in-context learning provides
- **Core assumption**: Fine-tuning on task-specific data would yield better performance than in-context learning
- **Evidence anchors**: The paper notes that "it is possible that we may not have fully optimized our prompts" and that "fine-tuning was not feasible" due to access restrictions
- **Break condition**: If optimized in-context learning prompts achieve comparable performance to fine-tuning, this mechanism is invalidated

## Foundational Learning

**Transition Relevance Place (TRP)**: Points in conversation where a speaker switch can naturally occur. Understanding TRPs is crucial for dialogue systems to avoid interrupting speakers or creating awkward silences.

*Why needed*: TRPs are fundamental to conversational turn-taking and form the core prediction target in this study. *Quick check*: Can you identify TRPs in a recorded conversation by noting where speakers naturally pause or yield the floor?

**Turn Construction Unit (TCU)**: The smallest unit of conversational talk that speakers use to build turns. TCUs are the building blocks of conversational turns.

*Why needed*: The study requires turns with at least two TCUs to ensure sufficient complexity for TRP prediction. *Quick check*: Can you segment a conversation into TCUs by identifying natural completion points in utterances?

**In-Context Learning (ICL)**: A prompting technique where LLMs are provided with task examples within the prompt itself, without requiring model fine-tuning.

*Why needed*: The study uses ICL due to restricted access to LLM weights, making it the only feasible adaptation method. *Quick check*: Can you design a prompt that includes 3-4 examples of the task to guide LLM predictions?

## Architecture Onboarding

**Component map**: LLMs (GPT-4 Omni, Phi3, Gemma2, Llama3.1, Mistral) -> In-context learning prompts -> TRP prediction task -> Evaluation metrics

**Critical path**: Stimulus selection (ICC turns) -> Participant response collection -> TRP threshold determination (τ = 0.3) -> LLM prediction via ICL -> Performance evaluation

**Design tradeoffs**: Text-only input vs. multimodal (prosodic/acoustic) features; in-context learning vs. fine-tuning; binary classification vs. continuous probability estimation

**Failure signatures**: Consistently low F1 scores across models indicates fundamental difficulty with task; poor performance despite theoretical background suggests linguistic content alone is insufficient

**First experiments**:
1. Test multiple threshold values (0.2, 0.3, 0.4, 0.5) for participant agreement to identify optimal TRP determination
2. Compare in-context learning performance with a small set of fine-tuned models on the same task
3. Evaluate whether providing additional conversational context (previous turns) improves LLM performance

## Open Questions the Paper Calls Out

**Open Question 1**: What is the optimal threshold τ for determining within-turn TRPs based on participant response proportions? The paper uses τ = 0.3 but does not systematically explore different threshold values or justify why 0.3 was chosen over other values.

**Open Question 2**: How would incorporating prosodic and acoustic features affect LLM performance on within-turn TRP prediction? The study intentionally limited models to text-only input, leaving open whether multimodal input would improve performance.

**Open Question 3**: Would fine-tuning on spoken dialogue data improve LLM performance compared to the in-context learning approach? The paper acknowledges that fine-tuning was not feasible due to access restrictions and suggests this as a limitation.

## Limitations

- **Restricted corpus access**: The ICC dataset is not publicly available due to IRB restrictions, preventing independent validation
- **Methodology constraints**: Exclusive use of in-context learning without fine-tuning may underestimate LLM capabilities
- **Language and cultural specificity**: Focus on American English limits generalizability to other languages and cultures with different turn-taking norms

## Confidence

**High confidence**: Core finding that LLMs struggle with within-turn TRP prediction is supported by systematic evaluation across multiple models and metrics (F1 = 0.151 for best model)

**Medium confidence**: Conclusion that linguistic content alone is insufficient for TRP prediction, though acoustic or multimodal features might improve performance

**Low confidence**: Generalizability of findings to all LLM architectures and sizes, given that only a specific set of models was tested using in-context learning only

## Next Checks

1. **Cross-corpus validation**: Replicate the experiment using alternative naturalistic dialogue datasets (e.g., Switchboard, CallHome) to verify whether results generalize beyond the ICC corpus

2. **Multimodal extension**: Evaluate whether incorporating prosodic features (pitch, intensity, duration) or using speech-based models improves TRP prediction performance compared to text-only approaches

3. **Fine-tuning comparison**: Compare in-context learning results against models fine-tuned on the TRP prediction task to determine whether the observed limitations are due to methodology rather than fundamental LLM capabilities