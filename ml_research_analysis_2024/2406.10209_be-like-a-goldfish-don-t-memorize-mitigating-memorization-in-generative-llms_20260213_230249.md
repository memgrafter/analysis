---
ver: rpa2
title: Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs
arxiv_id: '2406.10209'
source_url: https://arxiv.org/abs/2406.10209
tags:
- loss
- training
- goldfish
- tokens
- memorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of memorization in large language
  models, which can lead to privacy and copyright risks. The authors propose a method
  called the "goldfish loss" to mitigate memorization during training.
---

# Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs

## Quick Facts
- arXiv ID: 2406.10209
- Source URL: https://arxiv.org/abs/2406.10209
- Reference count: 29
- Primary result: The goldfish loss significantly reduces extractable memorization in LLMs while maintaining comparable downstream performance.

## Executive Summary
This paper addresses the critical problem of memorization in large language models, where models can inadvertently reproduce training data verbatim, creating privacy and copyright risks. The authors propose the "goldfish loss" - a training method that randomly excludes tokens from loss computation, preventing the model from learning to reproduce specific sequences. The method uses a hashing strategy to ensure consistent masking across duplicate passages. Experiments demonstrate that goldfish loss dramatically reduces memorization (as measured by RougeL scores close to control models that never saw the training data) while maintaining comparable performance on downstream benchmarks.

## Method Summary
The goldfish loss method modifies standard causal language modeling training by randomly excluding a subset of tokens from the loss computation during each training step. Specifically, tokens are dropped with probability 1/k, where k is a hyperparameter controlling the masking rate. A hashed mask strategy ensures that whenever the same sequence of h preceding tokens appears (even in different documents), the same token is masked consistently. This prevents the model from memorizing specific sequences while still allowing it to learn general language patterns by conditioning on all prior tokens. The method is particularly effective at preventing exact reproduction of training data during inference while maintaining comparable downstream task performance.

## Key Results
- Goldfish loss reduces RougeL scores to levels comparable to control models that never saw the training data, indicating minimal memorization
- The method maintains downstream benchmark performance comparable to standard training across tasks like WinoGrande, PIQA, and OpenBookQA
- Static and hashed masking strategies are more effective at reducing memorization than random masking, while random masking performs better on downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Goldfish loss reduces exact reproduction of training sequences by preventing the model from learning some tokens.
- Mechanism: During training, a subset of tokens are excluded from the loss computation, so the model cannot learn to reproduce those tokens accurately. When these tokens appear in the training data, the model must guess them at inference time, causing it to diverge from the exact training sequence.
- Core assumption: Excluding tokens from loss prevents the model from memorizing them, even if they appear multiple times in training data.
- Evidence anchors:
  - [abstract] "During training, randomly sampled subsets of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set."
  - [section 3] "The goldfish loss is only computed on a subset of the tokens, and thus prevents the model from learning the entire token sequence."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism.
- Break condition: If the same sequence of tokens appears multiple times with different tokens dropped each time, the model might eventually learn the full sequence.

### Mechanism 2
- Claim: Hashing ensures consistent masking of duplicate passages across different documents.
- Mechanism: A hash function is applied to the h preceding tokens to determine whether to mask the current token. This ensures that whenever the same sequence of h tokens appears (even in different documents), the same token will be masked.
- Core assumption: The hash function produces consistent outputs for identical input sequences, allowing for consistent masking.
- Evidence anchors:
  - [section 3.1] "For a positive integer h determining the context width of the hash, we mask token xi if and only if the outputs of a hash function f : |V |h → R applied to the h preceding tokens is less than 1/k."
  - [section 3.1] "With this strategy, the goldfish loss mask for every position depends only on the h preceding tokens. Every time the same sequence of h tokens appears, the (h + 1)th token is masked in the same way."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism.
- Break condition: If the hash function produces inconsistent outputs for identical input sequences, or if the context width h is too small to capture meaningful patterns.

### Mechanism 3
- Claim: Goldfish loss allows the model to learn the full distribution of natural language while preventing exact reproduction.
- Mechanism: The model still conditions on all prior tokens when making predictions, even for masked tokens. This allows it to learn the general patterns of language, but prevents it from learning specific sequences.
- Core assumption: Conditioning on prior tokens allows the model to learn language patterns even when some tokens are masked.
- Evidence anchors:
  - [abstract] "These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set."
  - [section 3] "Most importantly, the outputs xi are still conditioned on all prior tokens x<i, allowing the model to learn the full distribution of natural language over the course of training."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism.
- Break condition: If the model relies heavily on specific token sequences rather than general language patterns, it may struggle to generate coherent text when some tokens are masked.

## Foundational Learning

- Concept: Causal language modeling (CLM)
  - Why needed here: Understanding how standard language models are trained is crucial to understanding how goldfish loss differs and why it works.
  - Quick check question: In CLM, what is the model trying to predict at each step?

- Concept: Memorization in language models
  - Why needed here: The paper's goal is to reduce memorization, so understanding what memorization is and how it occurs is essential.
  - Quick check question: What is the difference between memorizing a sequence and learning a language pattern?

- Concept: Hashing for consistent masking
  - Why needed here: The goldfish loss uses hashing to ensure consistent masking of duplicate passages, which is a key part of its mechanism.
  - Quick check question: How does hashing help ensure that the same token is masked every time a specific sequence appears?

## Architecture Onboarding

- Component map: Generate mask -> Apply mask during loss computation -> Train model -> Evaluate for memorization and performance
- Critical path: Generate mask → Apply mask during loss computation → Train model → Evaluate for memorization and performance
- Design tradeoffs: Goldfish loss reduces memorization but may require more training steps to achieve the same performance as standard loss. It also adds complexity to the training process.
- Failure signatures: If the model still exhibits high levels of memorization, it could indicate issues with the masking strategy, hash function, or training process.
- First 3 experiments:
  1. Train a small model with goldfish loss and evaluate its memorization and performance on a simple dataset.
  2. Compare the memorization and performance of goldfish loss with different masking strategies (static, random, hashed).
  3. Investigate the impact of the drop frequency k on memorization and performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the goldfish loss scale to larger language models with tens or hundreds of billions of parameters?
- Basis in paper: [explicit] The authors state: "Finally, prior work has shown that larger models memorize more of their training data, and thus studies of how the benefits afforded by goldfish loss scale to tens or hundreds of billions of parameters is an interesting open question."
- Why unresolved: The paper only experiments with models up to 7 billion parameters, and the relationship between model size and memorization effectiveness is not established.
- What evidence would resolve it: Experiments demonstrating the goldfish loss's effectiveness (or lack thereof) on models of varying scales, particularly those in the tens or hundreds of billions of parameters range.

### Open Question 2
- Question: How does the goldfish loss perform on low-entropy text data, such as code, where there may be higher potential for memorization?
- Basis in paper: [inferred] The authors mention: "Furthermore, in situation with plentiful but sensitive content, or low entropy text (e.g. code), one might use higher masking rates than those explored in this paper."
- Why unresolved: The paper focuses on natural language text and does not explore the goldfish loss's performance on low-entropy text data like code.
- What evidence would resolve it: Experiments evaluating the goldfish loss on code datasets or other low-entropy text, comparing memorization and downstream performance to standard training.

### Open Question 3
- Question: Can the goldfish loss be effectively combined with other memorization mitigation techniques, such as differential privacy or data deduplication?
- Basis in paper: [inferred] The authors discuss related work on memorization mitigation but do not explore combining the goldfish loss with these methods.
- Why unresolved: The paper presents the goldfish loss as a standalone technique and does not investigate its potential synergies with other approaches.
- What evidence would resolve it: Experiments demonstrating the effects of combining the goldfish loss with techniques like differential privacy or data deduplication on memorization and model performance.

## Limitations

- The effectiveness of goldfish loss on more complex memorization scenarios (e.g., paraphrased training data, semantically similar content) remains unclear.
- The precise calibration of the k parameter for different model sizes and tasks is not established, with some performance trade-offs observed.
- Several implementation details are underspecified, including the exact hash function used and specific preprocessing steps, which could affect reproducibility.

## Confidence

**High confidence**: The core claim that goldfish loss reduces extractable memorization is well-supported by empirical evidence.

**Medium confidence**: The claim that goldfish loss maintains downstream performance comparable to standard training is supported but has limitations, with small performance gaps suggesting some trade-off exists.

**Medium confidence**: The mechanism by which hashing ensures consistent masking across duplicate passages is theoretically sound and partially supported, but specific implementation details and their impact are not fully characterized.

## Next Checks

1. **Cross-domain memorization evaluation**: Test goldfish loss on datasets with paraphrased or semantically similar content rather than exact duplicates to evaluate whether the method generalizes beyond the specific memorization patterns studied in the paper.

2. **Parameter sensitivity analysis**: Systematically vary the k parameter and context width h across different model scales to establish precise guidelines for optimal goldfish loss configuration, particularly for models outside the 1.1B parameter range studied.

3. **Long-range dependency assessment**: Evaluate goldfish loss's effectiveness on memorization that spans sequences longer than the context width h, including cases where training data appears with significant context changes or in different document structures.