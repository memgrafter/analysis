---
ver: rpa2
title: Massive Activations in Large Language Models
arxiv_id: '2402.17762'
source_url: https://arxiv.org/abs/2402.17762
tags:
- activations
- massive
- attention
- figure
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a phenomenon in large language models where\
  \ a tiny number of activations (e.g., <10 out of millions) have extremely large\
  \ magnitudes\u2014up to 10,000\xD7 larger than typical activations. These \"massive\
  \ activations\" appear in fixed feature dimensions and specific token positions,\
  \ acting as constant, input-agnostic bias terms."
---

# Massive Activations in Large Language Models

## Quick Facts
- arXiv ID: 2402.17762
- Source URL: https://arxiv.org/abs/2402.17762
- Authors: Mingjie Sun; Xinlei Chen; J. Zico Kolter; Zhuang Liu
- Reference count: 40
- Primary result: Massive activations are rare, extremely large-magnitude activations that act as implicit bias terms in attention mechanisms

## Executive Summary
This paper identifies a phenomenon in large language models where a tiny number of activations (e.g., <10 out of millions) have extremely large magnitudes—up to 10,000× larger than typical activations. These "massive activations" appear in fixed feature dimensions and specific token positions, acting as constant, input-agnostic bias terms. They cause attention to concentrate on certain tokens and effectively implement implicit bias terms in self-attention. By introducing explicit bias parameters in attention, the authors eliminate the need for massive activations and show comparable model performance. This finding also extends to Vision Transformers, where massive activations (and register tokens) similarly serve as fixed biases.

## Method Summary
The authors analyze pretrained LLMs (LLaMA2, LLaMA3, Mistral, Phi-2, MPT, Falcon, OPT, GPT-2) and ViTs (DINOv2, DINOv2-reg, MAE, CLIP) from HuggingFace. They collect hidden states from 100 sequences of 4096 tokens each for LLM analysis and 1k images for ViT analysis. The method involves visualizing activation magnitudes, intervening by setting massive activations to zero or mean values, and augmenting self-attention with explicit bias terms. Model performance is evaluated through perplexity and accuracy metrics before and after interventions.

## Key Results
- Massive activations appear in fixed feature dimensions and token positions across multiple LLM families
- These activations act as implicit bias terms, concentrating attention on specific tokens
- Explicit attention biases can eliminate the need for massive activations while maintaining comparable performance
- The phenomenon extends to Vision Transformers, where massive activations serve similar bias functions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Massive activations act as implicit bias terms in self-attention computation.
- **Mechanism:** Large-magnitude activations in fixed dimensions create highly negative logits for all other tokens, concentrating attention probability onto a few specific tokens. These tokens then produce constant value updates across the sequence, effectively implementing an additive bias in the attention output.
- **Core assumption:** Attention computation uses softmax over logits; extremely negative logits suppress corresponding tokens' contributions to the output.
- **Evidence anchors:**
  - [abstract] "massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output"
  - [section 4.2] "LLMs use massive activations to allocate substantial attention at certain tokens... they serve as additive bias terms"
  - [corpus] weak—no direct citations for the specific bias implementation mechanism
- **Break condition:** If attention layers use alternative normalization or scaling that mitigates extreme logit differences, the implicit bias effect would be reduced.

### Mechanism 2
- **Claim:** Massive activations emerge early in the network and persist until later layers.
- **Mechanism:** After initial token processing layers, LLMs identify a small set of tokens (often starting token or delimiters) and repurpose them to store fixed bias values. This allows the model to inject constant bias terms into the computation stream.
- **Core assumption:** Early layers extract semantic meaning from tokens before they are repurposed for bias storage.
- **Evidence anchors:**
  - [section 2.1] "Massive activations emerge in the initial layers and start to diminish in the last few layers"
  - [section 3] "LLMs repurpose the tokens linked with massive activations to store these important biases"
  - [corpus] no direct evidence for the timing hypothesis; inferred from layer-wise magnitude plots
- **Break condition:** If training objective or architecture changes (e.g., layer reordering) disrupt the temporal emergence pattern, massive activations might appear at different layers or not at all.

### Mechanism 3
- **Claim:** Explicit attention biases can eliminate the need for massive activations.
- **Mechanism:** By adding learnable key/value pairs to each attention head, the model can directly encode bias terms without relying on outlier token activations. This provides the same functional effect without the pathological magnitude spikes.
- **Core assumption:** The explicit bias parameters can approximate the same distribution of attention concentration patterns that massive activations induce.
- **Evidence anchors:**
  - [section 4.3] "training with them eliminates the need for LLMs to learn massive activations"
  - [section 4.3] "we demonstrate that training with them eliminates the need for LLMs to learn massive activations"
  - [corpus] no citations; experimental claim only
- **Break condition:** If the explicit bias parameterization is insufficient to capture the nuanced attention patterns, the model might still develop massive activations as a fallback.

## Foundational Learning

- **Concept:** Self-attention mechanism (query-key-value dot-product with softmax)
  - Why needed here: Understanding how massive activations manipulate attention logits is central to interpreting their role as implicit biases.
  - Quick check question: In a self-attention layer, if one key vector has a dot product of +10 with all queries while others are near zero, what happens to the softmax distribution?

- **Concept:** Layer normalization and its effect on outlier activations
  - Why needed here: The paper shows that LayerNorm (or RMSNorm) preserves massive activations while normalizing other values, enabling their downstream effects.
  - Quick check question: If a token's activation is 1000× larger than others in the same feature, what will LayerNorm do to the normalized values of the smaller activations?

- **Concept:** Residual connections in Transformers
  - Why needed here: Massive activations are studied in the context of residual streams; their persistence across layers depends on how residuals accumulate.
  - Quick check question: If a massive activation appears in layer ℓ, will it automatically appear in layer ℓ+1 if the residual connection passes it through unchanged?

## Architecture Onboarding

- **Component map:** Input embedding -> Token stream -> LayerNorm (RMSNorm variant) -> QKV projection -> Self-attention (softmax over logits) -> Residual addition -> MLP block -> Output

- **Critical path:** Input → LayerNorm → QKV → Attention (softmax) → Residual → Next layer. Massive activations influence the Attention stage most strongly.

- **Design tradeoffs:** 
  - Using massive activations as implicit biases avoids adding parameters but creates numerical instability and quantization challenges.
  - Explicit bias parameters increase model size slightly but improve numerical robustness and interpretability.

- **Failure signatures:**
  - Exploding/perplexity collapse when massive activations are zeroed (Section 3 intervention).
  - Disproportionate attention concentration on few tokens in attention maps.
  - Persistent outlier magnitudes across many layers despite normalization.

- **First 3 experiments:**
  1. Zero-out the top 5 activation magnitudes in a hidden state and measure perplexity change on validation set.
  2. Replace RMSNorm with standard LayerNorm and observe if massive activations persist or shift in magnitude.
  3. Insert explicit k′, v′ bias parameters into attention and train a GPT-2 model to compare massive activation presence with default GPT-2.

## Open Questions the Paper Calls Out
None

## Limitations

- **Temporal emergence mechanisms:** The paper claims massive activations emerge early in the network and persist through later layers, but provides limited evidence for the precise timing and conditions of this emergence. The mechanism by which early layers identify specific tokens for bias storage remains speculative.

- **Cross-architecture generalizability:** While massive activations appear in both LLMs and ViTs, the paper doesn't fully establish whether the same underlying mechanisms drive both phenomena, or if different architectural patterns (like register tokens in DINOv2-reg) represent fundamentally different mechanisms.

- **Training dynamics:** The paper doesn't explore how massive activations develop during training or whether they're an inevitable outcome of standard training objectives, or if specific hyperparameter choices influence their emergence.

## Confidence

**High confidence:** The empirical observations of massive activations (extreme magnitude outliers in fixed dimensions) and their impact on attention concentration are well-supported by the presented data across multiple model families.

**Medium confidence:** The claim that massive activations act as implicit bias terms is strongly supported by intervention experiments, but the exact mechanism by which they implement this bias (particularly the claim about creating "highly negative logits") could benefit from more rigorous mathematical characterization.

**Medium confidence:** The effectiveness of explicit attention biases in eliminating massive activations is demonstrated experimentally, but the long-term stability and performance implications of this modification across diverse tasks remain unclear.

## Next Checks

1. **Layer-by-layer emergence tracking:** Instrument a training run to