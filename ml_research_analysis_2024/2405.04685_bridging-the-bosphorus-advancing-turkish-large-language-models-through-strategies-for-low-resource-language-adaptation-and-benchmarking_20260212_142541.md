---
ver: rpa2
title: 'Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies
  for Low-Resource Language Adaptation and Benchmarking'
arxiv_id: '2405.04685'
source_url: https://arxiv.org/abs/2405.04685
tags:
- turkish
- language
- dataset
- llms
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces two approaches to develop Turkish large
  language models (LLMs): adapting English-trained models with Turkish data and training
  from scratch on Turkish text. The study created new Turkish evaluation datasets
  (TruthfulQA-TR and ARC-TR) and an instruction-tuning dataset.'
---

# Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking

## Quick Facts
- arXiv ID: 2405.04685
- Source URL: https://arxiv.org/abs/2405.04685
- Reference count: 40
- Models developed: Four Hamza models (124M-1.3B parameters) trained from scratch, plus HamzaMistral and HamzaGPT 2-xl adapted from English models

## Executive Summary
This paper addresses the challenge of developing large language models (LLMs) for Turkish, a low-resource language, by exploring two complementary approaches: adapting existing English-trained models to Turkish and training new models from scratch on Turkish data. The authors create new Turkish evaluation datasets (TruthfulQA-TR and ARC-TR) and an instruction-tuning dataset, then train and evaluate four models from scratch (Hamza series) and two adapted models (HamzaMistral and HamzaGPT 2-xl). The study finds that while adaptation can be effective, it suffers from catastrophic forgetting of English knowledge, and that from-scratch training can achieve comparable or better performance within the same architecture. The work provides a comprehensive roadmap for building LLMs for low-resource languages and highlights the need for larger, higher-quality Turkish datasets.

## Method Summary
The study employs two primary methodologies for Turkish LLM development. First, it adapts existing English-trained models (Mistral-7B and GPT2-xl) to Turkish through continued pretraining on Turkish-only data using the next-token prediction objective. Second, it trains models from scratch on Turkish text using the GPT-2 architecture, creating four models ranging from 124M to 1.3B parameters. An instruction-tuning dataset is created using a self-instruction methodology adapted for Turkish, generating 175 seed tasks and translating them into instruction-response pairs. Models are evaluated using bits-per-character (BPC) on the trnews-64 test set and accuracy on Turkish benchmarks including the newly created TruthfulQA-TR and ARC-TR datasets.

## Key Results
- HamzaMistral achieved 39.85% accuracy on ARC-TR and 46.40% on TruthfulQA-TR, outperforming other Turkish models tested
- BPC results showed Hamza-xlarge and Kanarya-2b performed best at 1.252 and 1.255 respectively
- Continued pretraining on Turkish caused English knowledge loss (catastrophic forgetting) in base models
- Training from scratch surpassed continued pretraining within the same model architecture
- All models, code, and datasets are open-sourced for community use

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adapting existing English-trained models to Turkish through continued pretraining can yield effective Turkish LLMs while avoiding full retraining costs.
- **Mechanism**: The approach leverages pre-trained English models by further training them on Turkish-only data using the same next-token prediction objective. This transfers the general language modeling capabilities while specializing in Turkish.
- **Core assumption**: English and Turkish share enough underlying language modeling principles that continuing training on Turkish data will enhance Turkish capabilities without destroying English knowledge.
- **Evidence anchors**:
  - [abstract] "adapting existing LLMs originally pretrained in English to understand Turkish"
  - [section 3.1] "further training state-of-the-art base models on Turkish data, which was initially unfamiliar with Turkish"
  - [section 5.3] "further pretraining of base English language models such as GPT2 and Mistral results in a decrease in accuracy...compared to their original base scores"
- **Break condition**: If Turkish and English are sufficiently different linguistically, catastrophic forgetting will dominate, causing the model to lose its English capabilities without fully acquiring Turkish proficiency.

### Mechanism 2
- **Claim**: Creating instruction-tuning datasets through self-instruction methodology can improve Turkish LLMs' reasoning and instruction-following capabilities.
- **Mechanism**: The study generates diverse instruction-response pairs by using English LLMs to create prompts, then translates and validates them for Turkish, creating a supervised fine-tuning dataset that teaches the model to follow instructions.
- **Core assumption**: Instruction-following capabilities can transfer across languages through supervised fine-tuning, even when the base model was trained primarily in English.
- **Evidence anchors**:
  - [abstract] "supplemented with supervised fine-tuning on a novel Turkish instruction-tuning dataset aimed at enhancing reasoning capabilities"
  - [section 2.2] "we adapt the Self-Instruct procedure...for Turkish" and "Generated pairs are post-processed by removing any samples that contain visual context"
  - [section 5.2] "we observed an improvement in model performance across downstream benchmarks"
- **Break condition**: If the instruction-following patterns differ significantly between English and Turkish, or if the quality of translated instructions degrades too much, the model may not effectively learn to follow Turkish instructions.

### Mechanism 3
- **Claim**: Training Turkish LLMs from scratch on large Turkish corpora can achieve comparable or better performance than adaptation approaches for certain tasks.
- **Mechanism**: The approach trains models entirely on Turkish data from the beginning, following the GPT-2 architecture, allowing the model to develop Turkish-specific representations without interference from English patterns.
- **Core assumption**: Sufficient Turkish data exists to train a competitive model from scratch, and Turkish-specific patterns can be learned effectively without English pre-training.
- **Evidence anchors**:
  - [abstract] "developing a model from the ground up using Turkish pretraining data"
  - [section 3.2] "we adopted the most straightforward method: training from scratch using Turkish-only datasets"
  - [section 5.1] "starting from scratch surpasses the continued pretraining approach within the same model architecture"
- **Break condition**: If Turkish data quality or quantity is insufficient, or if certain linguistic patterns are better captured through transfer learning from English, the from-scratch approach may underperform.

## Foundational Learning

- **Concept**: Catastrophic forgetting in continual learning
  - Why needed here: Understanding why continued pretraining on Turkish causes loss of English capabilities is crucial for evaluating the adaptation approach
  - Quick check question: Why does fine-tuning a model on a new language sometimes cause it to lose performance on the original language?

- **Concept**: Bits-per-character (BPC) metric for language modeling evaluation
  - Why needed here: The paper uses BPC to compare models with different tokenizers, which is essential for fair evaluation across different Turkish LLM approaches
  - Quick check question: How does BPC differ from perplexity, and why is it useful for comparing models with different tokenizers?

- **Concept**: Supervised fine-tuning (SFT) and instruction tuning
  - Why needed here: The paper creates and uses a Turkish instruction-tuning dataset to improve reasoning capabilities, requiring understanding of how SFT works
  - Quick check question: What is the difference between pre-training and supervised fine-tuning in LLM development?

## Architecture Onboarding

- **Component map**: CulturaX Turkish corpus -> Model pretraining (from-scratch Hamza models or adapted HamzaMistral/HamzaGPT2-xl) -> Turkish instruction-tuning dataset creation -> Evaluation (BPC and Turkish benchmarks) -> Open-sourced models and code

- **Critical path**: Data preparation -> Model pretraining (either from scratch or adaptation) -> Instruction tuning -> Evaluation -> Iteration

- **Design tradeoffs**: From-scratch training requires more computational resources but may achieve better Turkish-specific performance; adaptation is computationally cheaper but suffers from catastrophic forgetting; instruction tuning improves reasoning but requires high-quality dataset creation.

- **Failure signatures**: Catastrophic forgetting (English knowledge loss during Turkish adaptation), poor instruction-following in Turkish despite English capability, low BPC scores indicating inefficient tokenization or model architecture, poor performance on Turkish benchmarks indicating data or architecture issues.

- **First 3 experiments**:
  1. Run BPC evaluation on TRNEWS-64 test set to establish baseline performance across all models
  2. Test ARC-TR and TruthfulQA-TR accuracy with 25-shot and 6-shot settings respectively to measure reasoning capabilities
  3. Compare English and Turkish performance on ARC dataset to quantify catastrophic forgetting in adapted models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size of Turkish pretraining corpus needed to achieve performance comparable to English LLMs?
- Basis in paper: [explicit] The paper mentions current Turkish models are trained on 300B tokens while LLaMA is trained on 1.5T and LLaMA 3 on 15T tokens, suggesting at least 3T tokens are needed for Turkish
- Why unresolved: The paper identifies this as a key limitation but does not provide experimental data on scaling Turkish datasets to determine the optimal size
- What evidence would resolve it: Systematic experiments training Turkish models on increasing corpus sizes (1T, 3T, 5T tokens) and measuring performance improvements on benchmarks

### Open Question 2
- Question: How can catastrophic forgetting be prevented when adapting English LLMs to Turkish through continued pretraining?
- Basis in paper: [explicit] The paper observes catastrophic forgetting when further pretraining English base models on Turkish data, causing performance degradation on English tasks
- Why unresolved: The paper notes this as a problem but does not explore solutions beyond mentioning that including some English data in batches could help
- What evidence would resolve it: Experimental results comparing different strategies to prevent forgetting (language mixing in batches, regularization techniques, architectural modifications) with quantitative measures of retained English capabilities

### Open Question 3
- Question: What is the relative performance impact of training Turkish LLMs from scratch versus adapting strong English base models?
- Basis in paper: [explicit] The paper compares both approaches and finds that adapting Mistral 7B performs better than from-scratch training, but notes this may be due to the strong base model
- Why unresolved: While the paper provides comparative results, it does not systematically analyze the factors contributing to this difference or test with different base model strengths
- What evidence would resolve it: Controlled experiments varying base model strength while keeping other factors constant, and analyzing the contribution of base model quality versus training approach to final performance

### Open Question 4
- Question: How does model performance scale with size for Turkish LLMs, and what is the optimal parameter range?
- Basis in paper: [inferred] The paper trains models from 124M to 1.3B parameters but does not explore larger sizes or analyze scaling trends
- Why unresolved: The paper acknowledges that larger models (7B, 13B, 30B) are needed for competitive performance but does not investigate the scaling laws or optimal size for Turkish
- What evidence would resolve it: Training and evaluating Turkish models across a wider range of sizes (1B, 3B, 7B, 13B) and analyzing performance improvements relative to parameter count and computational cost

### Open Question 5
- Question: What types of Turkish text data provide the most effective pretraining for LLMs?
- Basis in paper: [explicit] The paper notes current datasets are biased toward political, gambling, and sports content, suggesting the need for more diverse data
- Why unresolved: The paper identifies this as a limitation but does not conduct experiments to determine which data types (news, literature, technical, conversational, etc.) are most beneficial
- What evidence would resolve it: Controlled experiments training models on different curated subsets of Turkish text (news, books, code, social media, etc.) and measuring performance differences on various downstream tasks

## Limitations

- Catastrophic forgetting prevents effective bilingual capabilities when adapting English models to Turkish
- Evaluation scope is narrow, focusing primarily on BPC and two Turkish benchmarks
- Comparison with state-of-the-art models like GPT-4 is limited to English benchmarks

## Confidence

**High Confidence**: The mechanism of catastrophic forgetting during continued pretraining on Turkish data is well-supported by experimental results showing decreased accuracy on English benchmarks.

**Medium Confidence**: The claim that training from scratch surpasses continued pretraining within the same architecture is supported but lacks extensive ablation studies.

**Low Confidence**: The assertion that HamzaMistral significantly outperforms other Turkish models lacks comparison with other contemporary Turkish models beyond Kanarya-2b.

## Next Checks

1. **Quantify catastrophic forgetting more precisely**: Measure and report the exact performance drop on English benchmarks (e.g., ARC English) when models are fine-tuned on Turkish-only data, and test whether interleaved training schedules can mitigate this effect while maintaining Turkish performance gains.

2. **Expand evaluation to broader Turkish tasks**: Evaluate the models on additional Turkish-specific benchmarks covering different domains (sentiment analysis, named entity recognition, question answering) and compare performance against established Turkish models like BERTurk.

3. **Validate instruction-tuning dataset quality**: Conduct human evaluation of the instruction-response pairs in the Turkish instruction-tuning dataset to assess translation quality, task diversity, and alignment with Turkish linguistic patterns.