---
ver: rpa2
title: How Likely Do LLMs with CoT Mimic Human Reasoning?
arxiv_id: '2402.16048'
source_url: https://arxiv.org/abs/2402.16048
tags:
- reasoning
- answer
- causal
- instruction
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how Large Language Models (LLMs) perform
  reasoning through Chain-of-Thought (CoT) prompting. Using causal analysis, the authors
  compare the reasoning process of LLMs with humans, identifying four types of Structural
  Causal Models (SCMs): causal chain, common cause, full connection, and isolation.'
---

# How Likely Do LLMs with CoT Mimic Human Reasoning?

## Quick Facts
- arXiv ID: 2402.16048
- Source URL: https://arxiv.org/abs/2402.16048
- Authors: Guangsheng Bao; Hongbo Zhang; Cunxiang Wang; Linyi Yang; Yue Zhang
- Reference count: 40
- Primary result: Most LLM-task pairs exhibit common cause or full connection SCMs, indicating spurious correlations between instruction and answer, leading to consistency errors and unfaithful explanations

## Executive Summary
This paper investigates whether Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting truly mimic human reasoning or merely generate post-hoc explanations. Using causal analysis across six reasoning tasks and multiple models, the authors identify four types of Structural Causal Models (SCMs) and find that most LLM-task pairs exhibit common cause or full connection SCMs rather than the ideal causal chain. This indicates that LLMs often bypass genuine reasoning, with answers being directly influenced by instructions rather than through CoT. The study also examines how different training techniques affect causal structures, finding that in-context learning strengthens reasoning pathways while supervised fine-tuning and reinforcement learning on human feedback weaken them.

## Method Summary
The researchers employ causal analysis using interventions to test two hypotheses: whether CoT causes answers and whether instructions cause answers. They generate four types of interventions (golden CoT, random CoT, random instruction, random bias) and measure Average Treatment Effects (ATE) using McNemar's test for statistical significance. By analyzing the ATE values and their significance, they classify the causal structures into four SCM types: causal chain (ideal reasoning), common cause (spurious correlation), full connection (both pathways active), and isolation (neither pathway works). The study evaluates six reasoning tasks with multiple models including GPT-3.5-turbo, GPT-4, Llama2, and Mistral across zero-shot, few-shot, and fine-tuned settings.

## Key Results
- Most LLM-task pairs exhibit common cause or full connection SCMs rather than the ideal causal chain
- CoT often performs post-hoc explanation rather than genuine reasoning, with answers depending on instructions but not on CoT
- In-context learning strengthens causal structures by reinforcing proper reasoning pathways
- Supervised fine-tuning and RLHF weaken causal structures by introducing spurious correlations that bypass reasoning
- Increasing model size alone does not improve causal structure toward human-level reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoT prompting in LLMs often fails to produce genuine reasoning and instead generates post-hoc explanations of answers already determined by instruction alone.
- Mechanism: The causal structure shows that for many LLM-task pairs, the answer variable (Y) is directly influenced by the instruction (Z) without requiring the CoT variable (X). This indicates a "common cause" SCM where Z confounds both X and Y, leading to spurious correlation rather than true causal reasoning.
- Core assumption: The model's pretraining and fine-tuning induce a statistical dependency between Z and Y that bypasses genuine reasoning through X.
- Evidence anchors:
  - [abstract]: "Our empirical study reveals that LLMs often deviate from the ideal causal chain, resulting in spurious correlations and potential consistency errors."
  - [section 4.3]: "GPT-3.5-turbo in Addition task implies type II SCM, where the answers depend on the instructions but not on the CoTs... In this case, the CoT actually performs explaining instead of reasoning."
  - [corpus]: Weak - only one related paper ("Unveiling and Causalizing CoT") mentions causality in CoT, and it's about understanding the black box rather than identifying spurious correlations.
- Break condition: If interventions show that modifying CoT significantly affects answer accuracy (ATE ≠ 0), the spurious correlation hypothesis is falsified for that task.

### Mechanism 2
- Claim: In-context learning (ICL) with demonstrations strengthens the causal structure by reinforcing the proper reasoning pathway from instruction through CoT to answer.
- Mechanism: Adding examples in the prompt provides training signals that encourage the model to follow the demonstrated reasoning steps rather than jumping directly from instruction to answer. This reduces the confounding effect of Z on Y.
- Core assumption: The demonstrations in ICL serve as structural guides that override the model's default statistical shortcuts learned during pretraining.
- Evidence anchors:
  - [abstract]: "Our empirical study reveals that LLMs often deviate from the ideal causal chain... We also examine various factors influencing the causal structure, finding that in-context learning with examples strengthens it."
  - [section 4.4]: "The results reveal that, compared to zero-shot, ICL demonstrations improve causal relationships and enhance task accuracies... ICL generally reduces |ATE| in 'Instruction → Answer' but enhances |ATE| in 'CoT → Answer'."
  - [corpus]: Weak - no corpus papers directly address ICL's effect on causal structures in CoT reasoning.
- Break condition: If adding demonstrations increases task accuracy but does not improve the causal structure (ATE values remain unchanged), the strengthening hypothesis is invalid.

### Mechanism 3
- Claim: Supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) weaken the causal structure by introducing spurious correlations that align with human preferences rather than logical reasoning.
- Mechanism: These post-training techniques optimize for human-annotated responses that may prioritize correct answers over faithful reasoning processes. This creates a direct Z→Y path that bypasses X, strengthening the confounding relationship.
- Core assumption: Human feedback during SFT/RLHF prioritizes answer correctness over the reasoning process, allowing models to learn shortcuts.
- Evidence anchors:
  - [abstract]: "...post-training techniques like supervised fine-tuning and reinforcement learning on human feedback weaken it."
  - [section 4.4]: "SFT generally weakens the causal structure... introducing spurious features into the model... In contrast, DPO reduces spurious features by weakening the link between the instruction and the answer..."
  - [corpus]: Weak - no corpus papers examine SFT/RLHF's impact on causal structures in CoT.
- Break condition: If fine-tuned models show improved task accuracy but degraded causal structure (increased |ATE| in 'Instruction → Answer'), the weakening hypothesis is supported.

## Foundational Learning

- Concept: Structural Causal Models (SCMs)
  - Why needed here: The paper's core methodology relies on identifying different SCM types (causal chain, common cause, full connection, isolation) to understand how LLMs reason through CoT.
  - Quick check question: What distinguishes a "common cause" SCM from a "causal chain" SCM in terms of variable relationships?

- Concept: Average Treatment Effect (ATE)
  - Why needed here: ATE quantifies the causal impact of interventions on variables, allowing researchers to test hypotheses about whether CoT or instruction actually causes changes in the answer.
  - Quick check question: How would you interpret an ATE of zero for the hypothesis "CoT causes Answer"?

- Concept: McNemar's Test for Significance
  - Why needed here: This statistical test determines whether observed differences in task accuracy after interventions are significant, which is crucial for inferring causal relationships.
  - Quick check question: What p-value threshold does the paper use to determine statistical significance?

## Architecture Onboarding

- Component map:
  - Instruction variable (Z): Task description, demonstrations, question formulation
  - CoT variable (X): Step-by-step reasoning process generated by LLM
  - Answer variable (Y): Final answer produced by LLM
  - Intervention system: Mechanisms to modify Z and X independently
  - Evaluation system: Accuracy measurement and consistency checking

- Critical path:
  1. Generate baseline responses (zero-shot CoT)
  2. Apply interventions to Z (random instruction, random bias)
  3. Apply interventions to X (golden CoT, random CoT)
  4. Measure ATE for each intervention
  5. Infer SCM type based on significance patterns
  6. Analyze consistency between CoT and answer

- Design tradeoffs:
  - Trade-off between intervention strength and validity: Too weak interventions may not reveal causal structure; too strong may break task semantics
  - Trade-off between sample size and statistical power: More samples increase reliability but require more computational resources
  - Trade-off between automation and manual review: Automated consistency checking is faster but may miss nuanced reasoning errors

- Failure signatures:
  - High consistency between CoT and answer but low accuracy → Potential memorization rather than reasoning
  - Low consistency but high accuracy → Post-hoc explanation behavior
  - No improvement from golden CoT interventions → Spurious correlation dominates reasoning
  - Significant improvement from random CoT → Model relies on CoT formatting rather than content

- First 3 experiments:
  1. Zero-shot CoT baseline: Run all six tasks with default prompts to establish baseline accuracy and consistency
  2. Golden CoT intervention: Apply high-quality reasoning steps to test if CoT causally influences answers
  3. Random bias instruction: Inject misleading statements into instructions to test if answers are directly influenced by instruction rather than CoT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design training techniques to strengthen causal structures in LLMs to achieve human-level reasoning?
- Basis in paper: [explicit] The authors explicitly state that enlarging model size alone does not lead LLMs to ideal human-level reasoning and that SFT and RLHF weaken causal structures, urging research on new techniques.
- Why unresolved: Current popular post-training techniques like SFT and RLHF weaken causal structures rather than strengthen them, while simply increasing model size has proven insufficient.
- What evidence would resolve it: Development and validation of new training techniques that demonstrably strengthen causal structures (moving from common cause/full connection to causal chain SCM types) while maintaining or improving task performance across multiple reasoning tasks.

### Open Question 2
- Question: How does the causal structure vary across different model architectures beyond GPT-style models?
- Basis in paper: [inferred] The authors acknowledge their focus on GPT-style models and suggest that other architectures like BERT and GLM might have more intricate causal structures due to their blank-infilling training objective.
- Why unresolved: The paper primarily focuses on GPT-style models and does not explore how causal structures might differ in other transformer variants or non-transformer architectures.
- What evidence would resolve it: Systematic causal analysis of multiple model architectures (BERT, GLM, etc.) across various reasoning tasks to identify patterns and differences in their causal structures.

### Open Question 3
- Question: What is the relationship between fine-grained causal structures and specific types of reasoning errors?
- Basis in paper: [inferred] While the paper identifies coarse-grained causal structures, it mentions that more detailed analysis of fine-grained structures could be a future direction, and it correlates SCM types with consistency and faithfulness issues.
- Why unresolved: The paper uses a simplified three-variable model and doesn't explore more detailed causal relationships between individual reasoning steps and final answers.
- What evidence would resolve it: Detailed causal analysis mapping specific reasoning steps to final answers and identifying how different causal patterns contribute to various types of reasoning errors.

## Limitations

- The causal inference methodology assumes reliable ATE measurements but doesn't fully specify intervention strength and semantic validity
- Analysis focuses on six specific tasks and models, limiting generalizability to other domains and reasoning types
- Paper acknowledges that larger models don't necessarily exhibit better causal structures, but reasons for this remain unclear
- The simplified three-variable model may miss nuanced causal relationships between individual reasoning steps

## Confidence

- **High confidence**: The identification of SCM types (causal chain, common cause, full connection, isolation) through ATE analysis is methodologically sound and well-supported by empirical evidence
- **Medium confidence**: The claims about ICL strengthening causal structure and SFT/RLHF weakening it are supported by the data but may be task-specific
- **Low confidence**: The broader claim that LLMs with CoT don't truly mimic human reasoning at scale requires more diverse task validation

## Next Checks

1. Replicate the intervention experiments with different perturbation strengths to verify the robustness of ATE measurements and SCM classifications
2. Test additional reasoning tasks (e.g., logical puzzles, mathematical proofs) to assess whether the identified SCM patterns generalize beyond the six examined tasks
3. Compare the causal structures of fine-tuned models using different training objectives (answer-matching vs. reasoning-matching) to isolate the effect of human feedback on reasoning fidelity