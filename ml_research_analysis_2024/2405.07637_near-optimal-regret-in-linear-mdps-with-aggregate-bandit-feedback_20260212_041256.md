---
ver: rpa2
title: Near-Optimal Regret in Linear MDPs with Aggregate Bandit Feedback
arxiv_id: '2405.07637'
source_url: https://arxiv.org/abs/2405.07637
tags:
- lemma
- regret
- algorithm
- policy
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper extends the recently introduced RL with Aggregate Bandit
  Feedback (RL-ABF) model to linear MDPs with function approximation, where the agent
  only observes the sum of rewards at the end of an episode rather than each reward
  individually. The authors develop two efficient algorithms with near-optimal regret
  guarantees: RE-LSVI, a value-based optimistic algorithm using a new ensemble randomization
  technique with Q-functions, and REPO, a policy optimization algorithm with a novel
  hedging scheme over the ensemble.'
---

# Near-Optimal Regret in Linear MDPs with Aggregate Bandit Feedback

## Quick Facts
- arXiv ID: 2405.07637
- Source URL: https://arxiv.org/abs/2405.07637
- Reference count: 40
- Primary result: Achieves near-optimal regret O(√K) in linear MDPs with aggregate bandit feedback

## Executive Summary
This paper extends the RL with Aggregate Bandit Feedback (RL-ABF) model to linear MDPs with function approximation, where agents only observe total episode rewards rather than individual step rewards. The authors develop two algorithms with near-optimal regret guarantees: RE-LSVI (a value-based optimistic algorithm) and REPO (a policy optimization algorithm). The key innovation is an ensemble randomization technique that uses independently sampled Gaussian noise to compute multiple Q-functions from the same data, enabling optimistic estimates despite the limited feedback structure.

## Method Summary
The paper proposes two algorithms for linear MDPs with aggregate bandit feedback. RE-LSVI uses least-squares value iteration with ensemble randomization, computing multiple Q-functions by adding independent Gaussian noise to reward estimates and following the greedy policy of the highest-valued function. REPO uses policy optimization with a novel hedging scheme over an ensemble of perturbed value functions, updating policies via multiplicative weights over epochs. Both algorithms employ a loose truncation mechanism to bound value estimates without introducing bias, and carefully control exploration bonuses through stochastic perturbations rather than deterministic bonuses.

## Key Results
- RE-LSVI achieves regret O(d⁵ H⁷ √K log⁶(dHK/δ))
- REPO achieves regret O(d⁵ H⁹ √K log⁹(dH|A|K/δ))
- Both algorithms achieve near-optimal √K regret scaling in linear MDPs with aggregate feedback
- The ensemble randomization technique enables optimism with only aggregate rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ensemble randomization technique creates optimistic value estimates with controlled variance, enabling efficient planning despite only observing aggregate rewards.
- Mechanism: Independent Gaussian noise vectors ζk,i are sampled for each episode k and added to the reward estimates. Multiple value functions are computed from these perturbed rewards, and the policy follows the greedy policy with respect to the highest-valued function. This ensures that at least one function is optimistic with high probability.
- Core assumption: The noise vectors are independent across ensemble members and their covariance matches the uncertainty in the reward estimate.
- Evidence anchors:
  - [abstract]: "We use independently sampled Gaussian noise to compute multiple Q-functions from the same data, allowing for an optimistic estimate of the optimal value with controlled variance."
  - [section]: "We draw m≈ log(K/δ ) noise terms and calculate their respective value functions. Each value estimate is thus optimistic with constant probability, and since the noise terms are i.i.d, it is straightforward to see that at least one of them is optimistic with high probability."

### Mechanism 2
- Claim: The loose truncation mechanism prevents bias in the random walk induced by added noise while keeping value estimates bounded.
- Mechanism: Instead of clipping Q-values to [0,H], they are clipped to [-(H+1-h)βrζ, (H+1-h)βrζ] where βrζ bounds the magnitude of perturbed reward estimates. This maintains the unbiasedness of the reward estimates while preventing exponential blow-up.
- Core assumption: The perturbation magnitude βrζ is chosen such that it bounds the noise-induced deviation from true rewards.
- Evidence anchors:
  - [abstract]: "In addition, we introduce a loose truncation mechanism to keep our estimates bounded without creating biases in the random walks induced by the added noise."
  - [section]: "The symmetry in our clipping is to avoid biases in the random walk induced by ζk,i , and the looser threshold is crucial for the analysis."

### Mechanism 3
- Claim: The epoch schedule in REPO maintains fixed covariance matrices within intervals, enabling stable hedging over policy sequences.
- Mechanism: At the start of each epoch, reward perturbations are drawn and kept fixed throughout. This allows the Multiplicative Weights update to hedge effectively over the ensemble of perturbed value functions, ensuring competitive performance as a sequence.
- Core assumption: The policy class's covering number remains controlled when using stochastic bonuses rather than deterministic ones.
- Evidence anchors:
  - [section]: "This is a fairly standard doubling trick that allows us to keep the covariance matrix fixed in intervals... Since the policies are only competitive as a sequence, they must also be optimistic as a sequence."
  - [section]: "Concretely, it reduces its log covering number from Hd^3 to d, yielding an overall improvement to the regret bound."

## Foundational Learning

- Concept: Linear MDP structure and function approximation
  - Why needed here: The paper extends RL-ABF from tabular to linear MDPs, requiring understanding of how linear function approximation works in RL
  - Quick check question: What are the three normalization assumptions made for linear MDPs in this paper?

- Concept: Optimism in RL and exploration bonuses
  - Why needed here: Both algorithms use optimism to balance exploration and exploitation, crucial for achieving sublinear regret
  - Quick check question: How do deterministic and stochastic exploration bonuses differ in their impact on the policy class complexity?

- Concept: Policy optimization and online mirror descent
  - Why needed here: REPO uses policy optimization with online mirror descent updates, requiring understanding of these methods
  - Quick check question: What is the role of the learning rate ηo in the policy update rule of REPO?

## Architecture Onboarding

- Component map: Data collection -> Estimation -> Ensemble computation -> Policy selection -> Execution -> Regret update
- Critical path: Episodes with aggregate reward feedback → Least squares estimation → Multiple perturbed value functions → Optimistic policy selection → Execution and regret accumulation
- Design tradeoffs:
  - Ensemble size vs computational cost: Larger m increases probability of optimism but also computation
  - Truncation tightness vs bias: Tighter bounds reduce variance but may introduce bias
  - Epoch length vs covariance staleness: Longer epochs reduce overhead but may use outdated uncertainty estimates
- Failure signatures:
  - Regret grows linearly instead of sublinearly: Optimism guarantee may be broken
  - Numerical instability in value computations: Truncation threshold may be inappropriate
  - Poor exploration despite bonuses: Bonus magnitude or structure may be incorrect
- First 3 experiments:
  1. Verify ensemble optimism: Run with known MDP, check that at least one member is optimistic
  2. Test truncation bias: Compare clipped vs unclipped value estimates on synthetic data
  3. Measure epoch sensitivity: Vary epoch length and observe impact on regret and computation time

## Open Questions the Paper Calls Out
None

## Limitations
- High polynomial dependencies on problem parameters (d⁵ H⁷ for RE-LSVI, d⁵ H⁹ for REPO) may limit practical scalability
- Assumes linear MDP structure with known feature mappings, which may not hold in all practical applications
- Ensemble randomization requires computing multiple value functions per episode, potentially limiting scalability to very large action spaces or long horizons

## Confidence
- **High Confidence**: The core mechanism of ensemble randomization for achieving optimism with aggregate feedback, the basic algorithmic frameworks of RE-LSVI and REPO, and the overall O(√K) regret scaling
- **Medium Confidence**: The specific regret bounds with their polynomial dependencies, the effectiveness of the loose truncation mechanism in practice, and the practical impact of the epoch schedule in REPO
- **Low Confidence**: The tightness of the regret bounds relative to information-theoretic limits, the computational efficiency in high-dimensional settings, and the performance when feature mappings are misspecified

## Next Checks
1. **Empirical scaling verification**: Test the algorithms on problems with varying d and H to empirically validate the polynomial dependencies in the regret bounds and identify which terms dominate in practice
2. **Ablation on ensemble size**: Systematically vary the ensemble size m to determine the minimum number of members needed to maintain optimism while balancing computational cost
3. **Robustness to feature misspecification**: Evaluate performance when the assumed linear structure has small deviations or when the feature mapping is learned rather than known, testing the practical limits of the theoretical guarantees