---
ver: rpa2
title: Evaluating and Adapting Large Language Models to Represent Folktales in Low-Resource
  Languages
arxiv_id: '2411.05593'
source_url: https://arxiv.org/abs/2411.05593
tags:
- data
- irish
- gaelic
- performance
- tales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated and adapted large language models (LLMs)
  to represent folktales in low-resource languages (Irish and Scottish Gaelic). Researchers
  compared base LLMs with three adaptations: increasing maximum sequence length using
  Local, Sparse and Global (LSG) attention, domain-adaptive pre-training on folktale
  corpora, and both adaptations combined.'
---

# Evaluating and Adapting Large Language Models to Represent Folktales in Low-Resource Languages

## Quick Facts
- arXiv ID: 2411.05593
- Source URL: https://arxiv.org/abs/2411.05593
- Reference count: 7
- Best model: Irish-only gaBERT with LSG + domain adaptation (F1 0.69 ATU, 0.90 gender)

## Executive Summary
This study evaluates large language models for classifying Irish and Scottish Gaelic folktales into ATU tale types and narrator gender. The researchers compare multilingual transformers (mBERT, XLM-RoBERTa, LaBSE) with a monolingual Irish model (gaBERT), testing three adaptations: longer sequence length via LSG attention, domain-adaptive pre-training on folktale corpora, and both combined. Surprisingly, a simple SVM with bag-of-words features achieves nearly equivalent performance to most LLMs, suggesting classical ML approaches remain competitive for low-resource language tasks. The gaBERT model shows strong cross-language transfer to Scottish Gaelic despite no Gaelic training data.

## Method Summary
The researchers fine-tuned transformer models on a corpus of 4,692 Irish and Scottish Gaelic folktales, using binary cross-entropy for gender classification and cross-entropy for ATU bin classification. They applied three adaptations: extending sequence length to 4096 tokens using LSG attention, continuing pre-training on 400k words of folktale domain data, and combining both approaches. Models were trained for 3 epochs with batch size 16 and learning rate 2e-5. Performance was evaluated using weighted F1 scores and compared against an SVM baseline using bag-of-words and TF-IDF features.

## Key Results
- gaBERT with LSG attention and domain adaptation achieved highest F1 scores (0.69 ATU, 0.90 gender)
- Simple SVM with bag-of-words features performed nearly as well (F1 0.68 ATU, 0.90 gender)
- LSG attention and domain-adaptive pre-training both improved performance over base models
- Irish-only gaBERT performed well on Scottish Gaelic despite no Gaelic training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LSG attention improves performance by handling longer folktales
- Mechanism: Extending context window to 4096 tokens captures longer-range dependencies and narrative structures that exceed 512 token limit
- Core assumption: Folktale features require long-range context for accurate classification
- Evidence anchors: Abstract states LSG improves performance; corpus section notes 1,500+ tales exceed 512 tokens
- Break condition: If folktale features are primarily local within 512 tokens

### Mechanism 2
- Claim: Domain-adaptive pre-training improves folktale classification
- Mechanism: Continuing training on folktale-specific data helps models learn domain vocabulary and narrative patterns
- Core assumption: Folktales have distinctive features different from general web text
- Evidence anchors: Abstract states DAPT improves performance; section describes continuing finetuning on Schools and Maclean data
- Break condition: If folktale features are already well-represented in general language model training

### Mechanism 3
- Claim: Irish monolingual model performs well on Scottish Gaelic
- Mechanism: High linguistic similarity leads to positive transfer through shared sub-word tokens in SentencePiece tokenizer
- Core assumption: Irish and Gaelic share sufficient sub-word units for cross-language transfer
- Evidence anchors: Abstract notes unexpected gaBERT performance; section attributes this to shared sub-word tokens
- Break condition: If languages diverge significantly in sub-word patterns or tokenization changes

## Foundational Learning

- Concept: Classification metrics (F1 score, weighted F1)
  - Why needed here: Used to evaluate model performance on imbalanced classification tasks
  - Quick check question: Why is weighted F1 preferred over regular F1 for imbalanced datasets?

- Concept: Transformer model limitations (quadratic complexity)
  - Why needed here: Explains why models are limited to 512 tokens and how LSG attention addresses this
  - Quick check question: What is the computational complexity of self-attention and why does it limit sequence length?

- Concept: Domain adaptation vs fine-tuning
  - Why needed here: Distinguishes between continuing pre-training on domain data vs. fine-tuning for classification
  - Quick check question: What's the key difference between domain-adaptive pre-training and task-specific fine-tuning?

## Architecture Onboarding

- Component map: Base multilingual models (mBERT, XLM-RoBERTa, LaBSE, gaBERT) → LSG attention adaptation → Domain-adaptive pre-training → Classification head → Evaluation metrics
- Critical path: Data preparation → Model selection → LSG adaptation → DAPT → Classification fine-tuning → Evaluation
- Design tradeoffs: Longer sequences improve context but increase computational cost; domain adaptation helps but requires additional data
- Failure signatures: Baseline SVM outperforming transformers suggests models not capturing relevant features; poor performance on minority classes indicates data sparsity issues
- First 3 experiments:
  1. Compare base models on both tasks to establish baseline performance
  2. Apply LSG attention extension to all models and measure performance change
  3. Apply domain-adaptive pre-training to all models and measure performance change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different low-resource languages compare in terms of LLM performance across linguistic typologies?
- Basis in paper: Explicit comparison of Irish and Scottish Gaelic shows unexpected cross-language transfer, but only examines two closely related languages
- Why unresolved: Limited language sample prevents generalization about LLM performance across different low-resource language families
- What evidence would resolve it: Comparative studies across multiple low-resource language families with varying degrees of relatedness

### Open Question 2
- Question: What is the optimal balance between domain adaptation data size and effectiveness for low-resource languages?
- Basis in paper: Uses 400k words of folktale data but notes domain adaptation improved performance less than sequence length extension
- Why unresolved: Only tests one size of domain adaptation corpus without systematic exploration of varying amounts
- What evidence would resolve it: Experiments varying domain adaptation corpus size (100k, 200k, 400k, 800k words) while measuring classification performance

### Open Question 3
- Question: How does model interpretability differ between classical ML and LLMs in low-resource language settings?
- Basis in paper: Notes transformers have interpretability limitations and highlights SVM baseline's competitive performance
- Why unresolved: Doesn't examine or compare interpretability of different models through feature importance or domain expert assessments
- What evidence would resolve it: Systematic comparison of model interpretability through SHAP values, feature importance rankings, and qualitative assessments

## Limitations

- Data representation concerns: Lack of detailed class distribution analysis and potential bias in folktale corpus
- Adaptation mechanism validation: No ablation studies isolating LSG attention and domain adaptation contributions
- Cross-language transfer explanation: Speculative attribution to shared sub-word tokens without empirical validation

## Confidence

- High confidence: SVM baseline performance (F1 0.68 ATU, 0.90 gender) - directly measured and comparable across conditions
- Medium confidence: LSG attention and domain-adaptive pre-training improvements - shows aggregate gains but lacks per-class analysis and ablation studies
- Low confidence: Irish-Gaelic cross-lingual transfer explanation - speculative without empirical validation of shared sub-word tokens

## Next Checks

1. Generate detailed per-class precision, recall, and F1 scores for each ATU bin and gender category to identify performance patterns and minority class issues

2. Conduct controlled experiments testing gaBERT on other language pairs with varying similarity while analyzing actual sub-word token overlap to validate cross-language transfer mechanism

3. Perform ablation study comparing LSG attention only, DAPT only, and combined approaches on identical datasets to isolate each mechanism's contribution