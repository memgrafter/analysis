---
ver: rpa2
title: 'SilverSight: A Multi-Task Chinese Financial Large Language Model Based on
  Adaptive Semantic Space Learning'
arxiv_id: '2404.04949'
source_url: https://arxiv.org/abs/2404.04949
tags:
- data
- financial
- semantic
- lora
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SilverSight, a Chinese financial multi-task
  LLM trained using an Adaptive Semantic Space Learning (ASSL) framework. The framework
  clusters training data in semantic space to avoid task conflicts and selects LoRA
  experts based on input-question similarity.
---

# SilverSight: A Multi-Task Chinese Financial Large Language Model Based on Adaptive Semantic Space Learning

## Quick Facts
- arXiv ID: 2404.04949
- Source URL: https://arxiv.org/abs/2404.04949
- Reference count: 27
- Key outcome: Achieves comparable performance to full-data training using only 10% of data

## Executive Summary
This paper introduces SilverSight, a Chinese financial multi-task LLM trained using an Adaptive Semantic Space Learning (ASSL) framework. The framework clusters training data in semantic space to avoid task conflicts and selects LoRA experts based on input-question similarity. Data redistribution is performed in two stages: adaptive density-based clustering (A-DBSCAN) followed by MMR-based selection. Experiments show that SilverSight achieves comparable performance to full-data training using only 10% of the data, demonstrating strong generalization and multitask capability on financial NLP benchmarks.

## Method Summary
SilverSight employs an Adaptive Semantic Space Learning (ASSL) framework that clusters 220k Chinese financial data points from 23 sources across 7 task types in semantic space. The framework uses K-means clustering to identify mutually enhancing and conflicting tasks, then applies A-DBSCAN for adaptive density-based downsampling/upsampling within clusters. MMR-based selection further refines data selection based on similarity, diversity, and model score. Six LoRA expert models (rank 16, alpha 32) are trained on the selected data using Qwen1.5-7B-Chat as the base model.

## Key Results
- Achieves comparable performance to full-data training using only 10% of data (4000 points)
- Strong generalization capabilities across financial NLP benchmarks
- Effective multitask capability with 6 LoRA experts handling 7 task types

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantic clustering of training data in semantic space avoids task conflicts and improves expert specialization.
- **Mechanism:** The framework clusters data in semantic space to identify mutually enhancing and conflicting training tasks. Each LoRA expert is then trained on a specific cluster, allowing each model to focus on its area of expertise and achieve a division of labor.
- **Core assumption:** Distances in semantic space can reflect the complementarity between tasks and allow for effective separation of conflicting tasks.
- **Evidence anchors:**
  - [abstract]: "Our framework can achieve results close to those obtained with full data training using only 10% of the data, while also exhibiting strong generalization capabilities."
  - [section]: "By clustering based on similarities in the semantic space, we could identify mutually enhancing and conflicting training tasks."
  - [corpus]: Weak evidence; no direct mentions of semantic clustering effectiveness, but related work on financial LLMs exists.
- **Break condition:** If the semantic space does not accurately reflect task relationships, clustering will fail to separate conflicting tasks, leading to degraded performance.

### Mechanism 2
- **Claim:** Adaptive data redistribution within clusters addresses data imbalance and long-tail distribution issues.
- **Mechanism:** The framework uses a two-stage adaptive data filtering process. First, A-DBSCAN dynamically adjusts neighborhood size based on data density, downsampling high-density areas and upsampling low-density areas. Second, MMR-based selection further refines data selection based on similarity to the cluster centroid, diversity, and model score.
- **Core assumption:** The combination of density-based clustering and MMR selection can effectively smooth the data distribution and improve model generalization.
- **Evidence anchors:**
  - [abstract]: "Our method can aggregate diverse data from complementary tasks, improving model performance on related tasks."
  - [section]: "By smoothing the data distribution within clusters, we use the centroid of data embeddings within a cluster as the embedding for LoRA experts, optimizing the selection of LoRA."
  - [corpus]: Weak evidence; no direct mentions of adaptive data redistribution, but related work on data selection for LLMs exists.
- **Break condition:** If the data redistribution process fails to adequately address data imbalance, the model may still suffer from overfitting on common datasets and underfitting on rare datasets.

### Mechanism 3
- **Claim:** LoRA expert selection based on input-question similarity ensures the most suitable expert is chosen for each task.
- **Mechanism:** When a user inputs a financial question, the system calculates the similarity between the question's embedding and the embeddings of the six LoRA experts. The expert with the closest embedding is selected to answer the question.
- **Core assumption:** The similarity between the question embedding and the expert embeddings accurately reflects the relevance of the expert to the task.
- **Evidence anchors:**
  - [abstract]: "By clustering based on similarities in the semantic space, we could identify mutually enhancing and conflicting training tasks."
  - [section]: "Whenever there is a user input, the system finds the expert whose semantic embedding is closest to the semantic embedding of the user input using the following formula:"
  - [corpus]: Weak evidence; no direct mentions of LoRA expert selection, but related work on expert selection for MoE models exists.
- **Break condition:** If the expert embeddings do not accurately represent the expert's capabilities, the selection process may choose an unsuitable expert, leading to poor performance.

## Foundational Learning

- **Concept:** Semantic space clustering
  - **Why needed here:** To identify mutually enhancing and conflicting training tasks and avoid task conflicts during multitask learning.
  - **Quick check question:** How does the K-means clustering algorithm work in the context of semantic space?
- **Concept:** Density-based clustering (DBSCAN)
  - **Why needed here:** To address data imbalance and long-tail distribution issues within clusters by downsampling high-density areas and upsampling low-density areas.
  - **Quick check question:** What is the difference between DBSCAN and K-means clustering?
- **Concept:** MMR (Maximal Marginal Relevance) formula
  - **Why needed here:** To further refine data selection based on similarity to the cluster centroid, diversity, and model score.
  - **Quick check question:** How does the MMR formula balance similarity, diversity, and model score in data selection?

## Architecture Onboarding

- **Component map:** User input -> semantic space embedding -> LoRA expert selection -> expert response
- **Critical path:** User input -> semantic space embedding -> LoRA expert selection -> expert response
- **Design tradeoffs:** The framework trades off computational efficiency for improved multitask performance by using LoRA experts and adaptive data selection.
- **Failure signatures:** Poor performance on specific tasks, overfitting on common datasets, underfitting on rare datasets, and incorrect LoRA expert selection.
- **First 3 experiments:**
  1. Evaluate the performance of the framework on a single task to ensure the LoRA expert is properly trained.
  2. Test the LoRA expert selection algorithm on a diverse set of inputs to verify its accuracy.
  3. Assess the impact of the adaptive data redistribution process on model generalization by comparing performance on training and test data.

## Open Questions the Paper Calls Out
(None explicitly called out in the paper)

## Limitations
- The framework's effectiveness depends on the quality of the semantic space embeddings, but the specific sentence encoder used is not specified.
- Claims about 10% data efficiency are difficult to verify due to use of proprietary Chinese financial benchmarks (CFLEB and FinEval).
- The selection of only 6 LoRA experts for 23 data sources across 7 task types may not adequately capture all task distinctions.

## Confidence

**High Confidence (80-100%):** The basic architecture of using LoRA adapters for multi-task learning is well-established and the general approach of semantic clustering for task separation is theoretically sound.

**Medium Confidence (50-80%):** The specific implementation of adaptive data redistribution through A-DBSCAN and MMR-based selection shows promise but lacks sufficient technical detail for full verification.

**Low Confidence (0-50%):** Claims about semantic clustering avoiding task conflicts are difficult to verify without access to the specific semantic space construction and clustering quality metrics.

## Next Checks
1. **Semantic Space Quality Validation:** Reconstruct the semantic clustering process using publicly available Chinese financial datasets and multiple sentence encoders to assess how sensitive clustering quality is to embedding choices.

2. **A-DBSCAN Parameter Sensitivity Analysis:** Implement the adaptive density-based clustering with systematic variation of key parameters to determine the stability of the 10% data selection claim.

3. **Expert Selection Accuracy Testing:** Create a diverse test suite of financial questions spanning all 7 task types and measure the accuracy of the LoRA expert selection algorithm.