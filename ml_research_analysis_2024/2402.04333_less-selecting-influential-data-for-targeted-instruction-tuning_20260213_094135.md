---
ver: rpa2
title: 'LESS: Selecting Influential Data for Targeted Instruction Tuning'
arxiv_id: '2402.04333'
source_url: https://arxiv.org/abs/2402.04333
tags:
- data
- less
- training
- instruction
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting relevant data from
  large instruction tuning datasets to develop specific capabilities in large language
  models (LLLs). The authors propose LESS, an optimizer-aware algorithm that estimates
  data influences and performs low-rank gradient similarity search for instruction
  data selection.
---

# LESS: Selecting Influential Data for Targeted Instruction Tuning

## Quick Facts
- arXiv ID: 2402.04333
- Source URL: https://arxiv.org/abs/2402.04333
- Reference count: 40
- Primary result: Training on a LESS-selected 5% of instruction tuning data can often outperform training on the full dataset across diverse downstream tasks.

## Executive Summary
This paper addresses the problem of selecting relevant data from large instruction tuning datasets to develop specific capabilities in large language models. The authors propose LESS, an optimizer-aware algorithm that estimates data influences and performs low-rank gradient similarity search for instruction data selection. LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks, and the selected data is highly transferable across different model scales and families.

## Method Summary
LESS is an optimizer-aware algorithm for selecting influential instruction tuning data. It first performs warmup LoRA training on a random 5% subset of the data for 4 epochs. Then it computes and stores gradient features for all candidate datapoints using random projections to reduce dimensionality to 8192. For each target task, it computes gradient features for few-shot examples and selects top-scoring examples based on influence similarity using the InfAdam metric. The final model is trained with LoRA on the selected subset.

## Key Results
- Training on a LESS-selected 5% of instruction tuning data can often outperform training on the full dataset across diverse downstream tasks (MMLU, TYDI QA, BBH).
- Data selected using small models' gradient features induce strong performance in large models and models from different families.
- LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LESS uses optimizer-aware influence estimation to select data that directly reduces loss on a target task.
- **Mechanism**: Adapts classical influence formulations to work with Adam optimizer and variable-length instruction data by normalizing gradient features and using cosine similarity instead of dot product.
- **Core assumption**: First-order approximation of training dynamics is sufficiently accurate for data selection in instruction tuning.
- **Evidence anchors**:
  - [abstract] "LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data."
  - [section] "The influence of z over the entire training run can be measured by aggregating the influence at every training step that uses z."
- **Break condition**: If the first-order approximation becomes inaccurate (e.g., due to strong non-linear interactions between datapoints), the influence estimates may mislead data selection.

### Mechanism 2
- **Claim**: Low-rank gradient similarity search enables efficient computation of high-dimensional gradient influences.
- **Mechanism**: Uses LoRA to reduce parameter count and random projections to further compress gradient features into a manageable dimensionality (8192) while preserving inner products.
- **Core assumption**: Johnson-Lindenstrauss lemma holds - random projections preserve distances/inner products sufficiently for influence estimation.
- **Evidence anchors**:
  - [abstract] "LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features"
  - [section] "The Johnson-Lindenstrauss Lemma (Johnson & Lindenstrauss, 1984) asserts that such projections often preserve the inner products"
- **Break condition**: If projected dimension is too small, gradient similarity preservation fails and data selection quality degrades.

### Mechanism 3
- **Claim**: Data selected using small models' gradient features transfers effectively to larger models and different model families.
- **Mechanism**: Constructs gradient datastore from small model's training dynamics, then uses similarity to few-shot examples to select data for training larger models.
- **Core assumption**: Inner products of gradients are roughly equal across model scales and families, making selection transferable.
- **Evidence anchors**:
  - [abstract] "Data selected using small models' gradient features induce strong performance in large models and models from different families"
  - [section] "Our experiments support that this finding is true, thereby adding to the growing body of work that small models can effectively select data for other models"
- **Break condition**: If gradient inner products diverge significantly across model scales/families, transferability breaks down.

## Foundational Learning

- **Concept**: First-order Taylor expansion of loss dynamics
  - **Why needed here**: Forms the mathematical foundation for influence estimation - approximates how training datapoints affect validation loss
  - **Quick check question**: How does the Taylor expansion relate gradient updates to loss changes in the influence formulation?

- **Concept**: Adam optimizer dynamics vs SGD
  - **Why needed here**: Instruction tuning uses Adam, but classical influence formulations assume SGD - requires adaptation to compute Γ(z,θ) terms
  - **Quick check question**: What are the key differences between Adam and SGD updates that affect how we compute influence scores?

- **Concept**: Random projection dimensionality preservation
  - **Why needed here**: Enables compression of high-dimensional gradients while maintaining useful similarity relationships for data selection
  - **Quick check question**: According to the Johnson-Lindenstrauss lemma, how does projected dimension affect distance preservation?

## Architecture Onboarding

- **Component map**: Selection model (MS) -> Gradient datastore -> Data selection algorithm -> Target model (MT)

- **Critical path**:
  1. Warmup LoRA training on random subset (5% of data, 4 epochs)
  2. Compute and store gradient features for all candidate datapoints
  3. For each target task, compute gradient features for few-shot examples
  4. Select top-scoring examples based on influence similarity
  5. Train target model on selected subset

- **Design tradeoffs**:
  - Larger warmup dataset improves influence estimation but increases upfront cost
  - Higher projection dimension improves similarity preservation but increases storage/compute
  - More checkpoints improve temporal coverage of gradient features but increase storage

- **Failure signatures**:
  - Performance worse than random selection → gradient similarity not capturing useful relationships
  - High variance across runs → warmup training insufficient or gradient features unstable
  - Transfer fails between models → gradient inner products diverge across model scales/families

- **First 3 experiments**:
  1. Compare performance using Adam vs SGD influence formulations to validate optimizer adaptation
  2. Test different projection dimensions (1024, 2048, 4096, 8192) to find optimal tradeoff
  3. Evaluate transferability by selecting with small model and training large model vs selecting with large model

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the sequence length bias in gradient norms affect the effectiveness of data selection methods for long-form generation tasks?
- **Basis in paper**: [explicit] The paper identifies that gradient norms are negatively correlated with sequence length, causing methods using dot products to select shorter sequences, which may not be ideal for tasks requiring long-form generation.
- **Why unresolved**: The paper does not provide a detailed analysis of how this bias specifically impacts the quality of selected data for long-form generation tasks, nor does it explore potential solutions to mitigate this issue.
- **What evidence would resolve it**: Experiments comparing the performance of data selection methods on tasks requiring long-form generation, with and without normalization of gradient features, would provide insights into the impact of sequence length bias.

### Open Question 2
- **Question**: How does the transferability of selected data between different model families and scales impact the overall efficiency and effectiveness of the data selection process?
- **Basis in paper**: [explicit] The paper demonstrates that data selected using a smaller model can effectively train larger models and models from different families, but it does not provide a comprehensive analysis of the factors influencing this transferability or its limitations.
- **Why unresolved**: The paper does not explore the underlying reasons for the observed transferability or investigate scenarios where it may break down, such as when model architectures or training objectives differ significantly.
- **What evidence would resolve it**: Systematic experiments varying model architectures, training objectives, and data selection strategies across a wide range of model scales and families would help understand the factors influencing transferability and its limitations.

### Open Question 3
- **Question**: How does the choice of gradient projection dimension affect the trade-off between computational efficiency and the quality of selected data?
- **Basis in paper**: [explicit] The paper shows that using a larger projection dimension generally improves performance, but it does not provide a detailed analysis of the optimal dimension or the factors influencing this trade-off.
- **Why unresolved**: The paper does not explore the relationship between projection dimension, computational cost, and data selection quality in depth, nor does it investigate the impact of using different random projection techniques.
- **What evidence would resolve it**: Experiments varying the projection dimension and comparing the computational cost and data selection quality across different tasks and model scales would help identify the optimal dimension and understand the factors influencing the trade-off.

## Limitations

- Optimizer adaptation relies on first-order approximations that may break down for complex, non-linear instruction tuning dynamics.
- Cross-model transferability is demonstrated but not thoroughly explored - it's unclear how model architecture differences beyond scale affect the transferability claim.
- The paper doesn't validate the Johnson-Lindenstrauss preservation assumption empirically for the chosen projection dimension.

## Confidence

- **High confidence**: The basic LESS algorithm architecture and experimental methodology are sound.
- **Medium confidence**: The optimizer adaptation for Adam is reasonable but untested - the paper assumes first-order approximations work without validating this assumption.
- **Low confidence**: The cross-model transferability claim is the most uncertain - while experiments show some transferability, the underlying assumption lacks theoretical justification.

## Next Checks

1. **Optimizer Adaptation Validation**: Run ablation studies comparing Adam vs SGD influence formulations on the same datasets to quantify the impact of the optimizer adaptation and validate the first-order approximation accuracy.

2. **Random Projection Analysis**: Systematically vary the projection dimension (1024, 2048, 4096, 8192) and measure how similarity preservation affects downstream performance to identify the optimal tradeoff point and validate JL lemma assumptions.

3. **Cross-Model Transferability Stress Test**: Design experiments that systematically vary model architectures (different attention patterns, layer types) beyond just scale differences to identify where the gradient inner product assumption breaks down and characterize the limits of transferability.