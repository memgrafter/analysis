---
ver: rpa2
title: Dependency Graph Parsing as Sequence Labeling
arxiv_id: '2410.17972'
source_url: https://arxiv.org/abs/2410.17972
tags:
- parsing
- dependency
- encodings
- encoding
- arcs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the concept of dependency parsing as sequence
  labeling to handle more complex graph structures like semantic dependencies and
  enhanced universal dependencies, which can contain cycles and reentrancies. The
  authors propose a range of unbounded and bounded linearizations that represent dependency
  graphs as sequences of labels, one per word.
---

# Dependency Graph Parsing as Sequence Labeling

## Quick Facts
- arXiv ID: 2410.17972
- Source URL: https://arxiv.org/abs/2410.17972
- Authors: Ana Ezquerro; David Vilares; Carlos Gómez-Rodríguez
- Reference count: 40
- Key outcome: This paper extends the concept of dependency parsing as sequence labeling to handle more complex graph structures like semantic dependencies and enhanced universal dependencies, which can contain cycles and reentrancies.

## Executive Summary
This paper proposes a novel approach to dependency graph parsing by extending sequence labeling techniques from syntactic dependency trees to more complex graph structures. The authors introduce both unbounded bracketing encodings and bounded bit-vector encodings that can represent graphs with cycles and reentrancies. Through extensive experiments on semantic dependency parsing and enhanced universal dependencies, they demonstrate that with appropriate encoding choices, sequence labeling parsers can achieve state-of-the-art accuracy while maintaining high efficiency.

## Method Summary
The method builds on the successful concept of treating dependency parsing as a sequence labeling problem, extending it from trees to graphs. The authors define two families of encodings: unbounded encodings that use bracketing symbols to represent arcs without fixed length constraints, and bounded encodings that use fixed-length bit vectors. The approach employs a standard encoder-decoder architecture where contextualized embeddings are generated and then decoded into label sequences representing the graph structure. The decoding process can be performed in linear time, enabling efficient graph parsing. The method is evaluated on both semantic dependency parsing datasets (DM, PAS, PSD) and enhanced UD parsing datasets across multiple languages.

## Key Results
- Bounded encodings perform best on sparser datasets (EUD parsing), while unbounded encodings excel in denser scenarios (semantic dependency parsing)
- Increasing the number of planes beyond two generally improves coverage and accuracy in complex cases
- The sequence labeling approach achieves accuracy close to state-of-the-art parsers while maintaining high efficiency
- The method successfully handles graphs with cycles and reentrancies, extending beyond traditional tree structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unbounded bracketing encodings extend syntactic parsing encodings to support dependency graphs by removing the single-parent constraint and allowing multiple incoming symbols per word.
- Mechanism: Each dependency arc is encoded by adding one symbol to the label of each of its endpoints. For graphs, the restriction of exactly one incoming symbol (< or >) per word is removed, allowing multiple such symbols and zero symbols for disconnected nodes.
- Core assumption: The decoding process with separate stacks for right and left arcs remains linear-time and correct even with multiple incoming arcs.
- Evidence anchors:
  - [abstract] "By extending them, we define a range of unbounded and bounded linearizations that can be used to cast graph parsing as a tagging task"
  - [section 3] "In this family of linearizations, the restriction to a single parent per node for syntactic parsing is achieved by explicitly enforcing exactly one incoming symbol... For graph parsing, we remove this restriction"
  - [corpus] Weak - corpus neighbors don't provide direct evidence about bracketing encoding mechanisms
- Break condition: The decoding algorithm would fail if arcs in the same direction crossed, which the encoding cannot handle. This is why multiplanarity is needed for dense graphs.

### Mechanism 2
- Claim: Bounded encodings (4k-bit and 6k-bit) achieve high efficiency by using fixed-length bit vectors to encode arcs, enabling constant-time label processing.
- Mechanism: Each label is a fixed-length bit vector where groups of bits encode arcs for different subgraphs. The 4k-bit encoding uses 4 bits per subgraph, while 6k-bit uses 6 bits per subgraph pair (leftward and rightward).
- Core assumption: The edge set E can be split into k relaxed 1-planar subgraphs with maximum in-degree 1, or 2k subgraphs with k leftward and k rightward subgraphs.
- Evidence anchors:
  - [abstract] "with a good choice of encoding, sequence-labeling dependency graph parsers combine high efficiency with accuracies close to the state of the art"
  - [section 4] "bounded encodings... use fixed-length bit vectors... Each label xi has k groups of 6 bits each"
  - [corpus] Weak - corpus neighbors don't provide direct evidence about bit vector encoding efficiency
- Break condition: The encoding fails when a node has more than k incoming arcs from the same direction, or when the graph cannot be decomposed into the required number of subgraphs.

### Mechanism 3
- Claim: Increasing the number of planes (k > 2) improves accuracy in dense datasets by providing more capacity to represent crossing arcs without violating relaxed k-planarity constraints.
- Mechanism: The encoding represents a graph as the union of k (relaxed) 1-planar subgraphs. When k = 2, most syntactic trees are covered, but dense graphs need k = 3 or 4 to achieve near-complete coverage.
- Core assumption: Increasing k beyond 2 provides diminishing returns for sparse datasets but significant improvements for dense datasets.
- Evidence anchors:
  - [abstract] "bounded encodings perform best on sparser datasets, while unbounded encodings excel in denser scenarios"
  - [section 3] "Having k sets of brackets provides coverage over relaxed k-planar graphs... However, since dependency graphs can be denser than syntactic trees... we also experiment with adding a third plane"
  - [corpus] Weak - corpus neighbors don't provide direct evidence about plane numbers and accuracy
- Break condition: Beyond a certain k, adding more planes provides negligible accuracy improvement while increasing computational cost.

## Foundational Learning

- Concept: Multiplanarity and relaxed k-planarity
  - Why needed here: These concepts explain how dependency graphs can be decomposed into subgraphs that avoid crossing arcs in the same direction, enabling the bracketing encodings to work
  - Quick check question: What is the difference between k-planar and relaxed k-planar graphs?

- Concept: Sequence labeling encoding for parsing
  - Why needed here: The paper builds on existing work that casts dependency parsing as sequence labeling, extending these encodings to handle graphs rather than just trees
  - Quick check question: How does a sequence labeling encoding for parsing differ from a standard sequence labeling task?

- Concept: Dependency graph vs. dependency tree
  - Why needed here: The paper extends tree encodings to handle graphs, which allow reentrancy (multiple parents) and cycles
  - Quick check question: What structural properties of dependency graphs make them more challenging to encode than dependency trees?

## Architecture Onboarding

- Component map: Encoder (contextualized embeddings) -> Decoder (feed-forward network producing labels) -> Encoding-specific decoder (recovering arcs from labels) -> Second decoder (predicting dependency types)
- Critical path: Input sentence -> Encoder -> Decoder (label prediction) -> Encoding-specific decoding (arc recovery) -> Type prediction -> Output graph
- Design tradeoffs: Unbounded encodings offer better coverage for dense graphs but have higher computational cost; bounded encodings are more efficient but require careful choice of k for different dataset densities
- Failure signatures: Poor coverage (many unparseable graphs), low tagging accuracy, inability to handle certain graph structures (crossing arcs in same direction for unbounded encodings)
- First 3 experiments:
  1. Compare baseline positional encodings (naive and relative) against bracketing encodings on a simple DAG dataset
  2. Test different values of k (2, 3, 4) for bounded encodings on datasets with varying densities
  3. Evaluate unbounded vs. bounded encodings on a dense graph dataset with cycles to determine which approach handles enhanced UD parsing better

## Open Questions the Paper Calls Out

- Question: Can the current linearization encodings be extended to handle meaning representations with more complex anchoring schemes beyond flavor (0)?
- Basis in paper: [explicit] The authors acknowledge that their work focuses on flavor (0) representations and mention plans to generalize to more relaxed flavors in future work.
- Why unresolved: The paper does not explore how the current encodings would need to be modified to handle other anchoring schemes, and the technical challenges involved are not discussed.
- What evidence would resolve it: Development and testing of modified encodings for different anchoring schemes, along with an analysis of the structural differences between flavors and their implications for linearization.

## Limitations

- The paper's claims about encoding efficiency and coverage are supported by experiments, but the exact preprocessing steps and decoding algorithms are not fully specified, making faithful reproduction challenging.
- The performance of different encodings (unbounded vs. bounded) depends heavily on dataset characteristics, but the paper doesn't provide clear guidelines for choosing the optimal encoding in new scenarios.
- The corpus analysis shows weak evidence from related papers about the specific mechanisms, suggesting that the theoretical guarantees may not be fully validated by external research.

## Confidence

- High confidence: The general framework of converting dependency graph parsing to sequence labeling is well-established and the experimental results show clear improvements over baseline methods.
- Medium confidence: The specific claims about when to use unbounded vs. bounded encodings and how many planes to use are supported by experiments but lack theoretical guarantees for all cases.
- Low confidence: The paper's assertions about computational efficiency and the exact conditions under which each encoding breaks down are not fully validated.

## Next Checks

1. Implement the decoding algorithms for both bounded and unbounded encodings and verify they correctly recover the original graph structures from the label sequences.
2. Test the encodings on a synthetic dataset with controlled graph densities to empirically determine the coverage and performance tradeoffs between different k values.
3. Compare the sequence labeling approach with the baseline biaffine parser on a held-out test set to verify that the accuracy claims hold across different language families and dependency types.