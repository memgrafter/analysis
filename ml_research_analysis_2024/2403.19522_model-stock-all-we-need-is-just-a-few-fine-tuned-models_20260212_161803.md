---
ver: rpa2
title: 'Model Stock: All we need is just a few fine-tuned models'
arxiv_id: '2403.19522'
source_url: https://arxiv.org/abs/2403.19522
tags:
- fine-tuned
- weights
- stock
- weight
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model Stock, an efficient fine-tuning method
  for large pre-trained models that achieves strong in-distribution (ID) and out-of-distribution
  (OOD) performance. Breaking away from traditional practices that require multiple
  fine-tuned models for averaging, Model Stock employs only two fine-tuned models
  to achieve superior accuracy.
---

# Model Stock: All we need is just a few fine-tuned models

## Quick Facts
- arXiv ID: 2403.19522
- Source URL: https://arxiv.org/abs/2403.19522
- Authors: Dong-Hwan Jang; Sangdoo Yun; Dongyoon Han
- Reference count: 40
- One-line primary result: Achieves 87.8% ImageNet top-1 accuracy and 74.9% average on 5 OOD benchmarks using only 2 fine-tuned models

## Executive Summary
Model Stock introduces an efficient fine-tuning method that achieves strong in-distribution and out-of-distribution performance using only two fine-tuned models instead of the dozens typically required. The method leverages geometric properties of weight space, discovering that fine-tuned weights lie on a thin shell with consistent norms and angles across models. By using pre-trained weights as an anchor and approximating the center of this shell distribution, Model Stock achieves state-of-the-art performance on standard benchmarks while requiring 24× less computational cost than competing methods like Model Soup.

## Method Summary
Model Stock is a weight-merging method that fine-tunes only two models and uses geometric properties of the weight space to approximate a center-close weight. The method calculates angles between fine-tuned models and uses pre-trained weights as an anchor to interpolate toward the shell center. This approach is applicable both during training (periodic merging) and after training. The interpolation ratio is determined by a geometric formula based on the angle between models, eliminating the need for additional training or heuristic hyperparameter settings. The method is implemented layer-wise to account for different angle properties across layers.

## Key Results
- Achieves 87.8% ImageNet top-1 accuracy and 74.9% average on 5 OOD benchmarks
- Outperforms state-of-the-art methods like Model Soup with 24× less computational cost
- Maintains consistent performance across diverse setups and both during and after training
- Demonstrates geometric properties (consistent angles/norms) across different architectures and setups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned weights lie on a thin shell in weight space with consistent norms and angles across models
- Mechanism: When fine-tuning with different random seeds but identical hyperparameters, the resulting weight vectors for each layer maintain constant Euclidean norms and fixed angles between any two models. This creates a shell-like distribution centered around a pseudo-center (µ)
- Core assumption: The weight space geometry is stable across different fine-tuning runs with the same setup, and the distribution follows Gaussian-like properties
- Evidence anchors:
  - [abstract] "Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space"
  - [section 2.1] "Observation 1: Angle and norm consistency among fine-tuned weights... both angle θ(k) between two different models and norm ∥w(k)∥ of a weight exhibit consistent values with very low standard deviations"
  - [corpus] Weak evidence - no direct citations, but related to Gaussian distribution in high dimensions
- Break condition: If different random seeds produce weights with significantly varying norms or angles, the shell assumption breaks down

### Mechanism 2
- Claim: Performance improves as weights get closer to the center of the shell distribution
- Mechanism: The center of the fine-tuned weight distribution represents a flat minimum with better generalization. Averaged weights from multiple fine-tuned models move closer to this center, improving both in-distribution and out-of-distribution performance
- Core assumption: The center of the weight distribution corresponds to optimal performance and that moving toward it doesn't sacrifice task-specific performance
- Evidence anchors:
  - [abstract] "we uncover a strong link between the performance and proximity to the center of weight space"
  - [section 2.2] "Table 1 offers quantitative observations about the fine-tuned weights and their performance... Going closer to the center by averaging the weights leads to improving both performances"
  - [section 2.2] "Observation 4: Randomly perturbed weights nearing the center also merit high performance"
- Break condition: If performance degrades when moving toward the center, or if the center doesn't represent a flat minimum

### Mechanism 3
- Claim: Pre-trained weights act as a robust anchor that can be used to interpolate toward the center efficiently
- Mechanism: By leveraging the pre-trained model as an anchor point and using geometric properties of the weight shell, Model Stock can approximate the center using only two fine-tuned models instead of dozens, with an optimal interpolation ratio determined by the angle between fine-tuned models
- Core assumption: The pre-trained model provides a stable reference point and that the geometric relationship between pre-trained and fine-tuned weights enables efficient center approximation
- Evidence anchors:
  - [abstract] "By leveraging geometric properties and a pre-trained model's anchoring effect, Model Stock approximates a center-close weight using only two fine-tuned models"
  - [section 3] "We propose an efficient alternative method by leveraging the pre-trained model weights, an aspect previously neglected by existing weight-merging methods"
  - [corpus] Weak evidence - no direct citations, but conceptually related to anchor-based methods in optimization
- Break condition: If the pre-trained model doesn't provide useful information for interpolation, or if the angle-based interpolation ratio fails to produce optimal results

## Foundational Learning

- Concept: Gaussian distribution properties in high-dimensional spaces
  - Why needed here: The paper relies on the hypothesis that fine-tuned weights follow a Gaussian distribution, which explains the thin shell geometry and consistent angles/norms
  - Quick check question: Why do vectors sampled from high-dimensional Gaussian distributions tend to have nearly identical norms?

- Concept: Geometric interpretation of weight space optimization
  - Why needed here: Understanding how weights move in optimization space and why they settle on shell boundaries rather than reaching the center
  - Quick check question: What geometric property explains why fine-tuned weights stay on the boundary of a shell rather than at the center?

- Concept: Model averaging and ensemble methods
  - Why needed here: Model Stock is fundamentally about efficient weight averaging, so understanding traditional ensemble methods and their limitations is crucial
  - Quick check question: How does traditional model averaging differ from the approach used in Model Stock?

## Architecture Onboarding

- Component map:
  Pre-trained weights -> Angle computation module -> Interpolation ratio calculator -> Layer-wise weight merger -> Fine-tuned weights

- Critical path:
  1. Fine-tune two models with different random seeds
  2. Compute angle between fine-tuned models
  3. Calculate interpolation ratio using geometric formula
  4. Interpolate weights layer-wise using pre-trained weights as anchor
  5. (Optional) Periodically merge during training for better approximation

- Design tradeoffs:
  - Number of fine-tuned models: More models improve center approximation but increase cost
  - Merging frequency: More frequent merging improves center proximity but adds overhead
  - Layer-wise vs. global merging: Layer-wise allows for better adaptation to layer-specific properties but adds complexity

- Failure signatures:
  - Inconsistent angles across fine-tuned models (indicating shell assumption failure)
  - Performance degradation when merging (indicating wrong interpolation ratio)
  - Memory overflow during periodic merging (indicates too frequent merging)

- First 3 experiments:
  1. Verify shell geometry: Fine-tune 2-3 models with different seeds, measure angles/norms, confirm consistency
  2. Test interpolation ratio: Use known angle, verify computed interpolation ratio matches geometric prediction
  3. Validate performance improvement: Compare Model Stock against single fine-tuned model on ID and OOD benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Model Stock approach generalize to larger models like ViT-G and what are the computational trade-offs involved?
- Basis in paper: Explicit - "Due to resource limitations, we could not conduct larger-scale models such as ViT-G. Exploring this will be part of our future work."
- Why unresolved: The paper only tested Model Stock on CLIP ViT-B/32, CLIP ViT-B/16, and CLIP ViT-L/14 models. Scaling up to models like ViT-G would require significant computational resources not available during the study.
- What evidence would resolve it: Experiments showing Model Stock performance and computational efficiency on larger models like ViT-G, including comparisons with baseline methods and detailed analysis of resource usage.

### Open Question 2
- Question: Can the geometric properties observed in Model Stock be extended to other types of neural network architectures beyond transformers and CLIP models?
- Basis in paper: Inferred - "We conjecture this holds irrespective of networks (ViT [5], Hybrid-ViT [5], ResNet [9], ConvNext [24])" but only CLIP models were tested.
- Why unresolved: While the paper suggests the properties might hold for various architectures, it only empirically validates them on CLIP-based models.
- What evidence would resolve it: Experiments applying Model Stock to non-CLIP architectures like ResNet, ConvNeXt, and other transformer variants, demonstrating similar geometric properties and performance improvements.

### Open Question 3
- Question: What is the impact of different data augmentation strategies on the geometric properties of fine-tuned weights in Model Stock?
- Basis in paper: Explicit - "Interestingly, these consistencies in angle and norm are observed 1) across diverse setups and 2) both during and after training."
- Why unresolved: The paper mentions consistency across various setups but doesn't specifically analyze how different data augmentation strategies affect the geometric properties.
- What evidence would resolve it: Detailed experiments comparing geometric properties under different augmentation strategies (RandAug, RRC, etc.) and their impact on Model Stock performance.

### Open Question 4
- Question: How sensitive is Model Stock to the choice of interpolation ratio and what are the optimal strategies for determining this ratio?
- Basis in paper: Explicit - "Unlike previous methods, determining t does not require extra training [22,36,37,40] or heuristic hyper-parameter settings [6,40], thereby simplifying the process and enhancing its accessibility and efficiency."
- Why unresolved: While the paper claims no extra training is needed, it doesn't provide a comprehensive sensitivity analysis of the interpolation ratio to different factors like model size, dataset, or training duration.
- What evidence would resolve it: Systematic experiments varying the interpolation ratio across different scenarios, including its impact on performance, computational efficiency, and robustness to distribution shifts.

## Limitations

- The core assumptions about shell geometry and center proximity may not hold for all model architectures or training scenarios
- Computational cost claims comparing to Model Soup don't fully account for overhead from angle computation and periodic merging
- Claims about generalizability to non-CLIP architectures remain untested despite being stated as conjecture
- The method's sensitivity to interpolation ratio and its optimal determination strategy is not thoroughly explored

## Confidence

- **High confidence**: The geometric formulation for interpolation ratio and the basic shell geometry observations are mathematically sound and well-supported by experimental evidence
- **Medium confidence**: The performance improvements on standard benchmarks are convincing, but the causal relationship between center proximity and generalization requires more rigorous testing across diverse scenarios
- **Low confidence**: Claims about the method's generalizability to non-CLIP architectures and different task types are speculative without supporting evidence

## Next Checks

1. **Shell geometry robustness test**: Fine-tune 5-10 models with different random seeds and datasets to verify that shell geometry (consistent angles/norms) holds across diverse conditions, or identify when it breaks down

2. **Pre-trained anchor dependency analysis**: Compare Model Stock performance when using pre-trained weights versus using one of the fine-tuned models as the anchor point to quantify the contribution of the pre-trained model assumption

3. **Architecture generalization experiment**: Apply Model Stock to non-CLIP architectures (e.g., ResNet, ConvNeXt) on different tasks (language modeling, speech recognition) to test the claimed generalizability beyond the current scope