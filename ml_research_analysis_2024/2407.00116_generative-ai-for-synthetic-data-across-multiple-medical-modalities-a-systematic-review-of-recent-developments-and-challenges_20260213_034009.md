---
ver: rpa2
title: 'Generative AI for Synthetic Data Across Multiple Medical Modalities: A Systematic
  Review of Recent Developments and Challenges'
arxiv_id: '2407.00116'
source_url: https://arxiv.org/abs/2407.00116
tags:
- data
- synthetic
- medical
- images
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic review of generative models (GANs,
  VAEs, DMs, and LLMs) used to synthesize various medical data types, including imaging,
  text, time-series, and tabular data (EHR). The survey encompasses a broad array
  of medical data modalities and explores various generative models, aiming to provide
  a holistic understanding of their applications in generating synthetic medical data.
---

# Generative AI for Synthetic Data Across Multiple Medical Modalities: A Systematic Review of Recent Developments and Challenges

## Quick Facts
- arXiv ID: 2407.00116
- Source URL: https://arxiv.org/abs/2407.00116
- Reference count: 40
- Primary result: Systematic review of generative models (GANs, VAEs, DMs, LLMs) for synthesizing diverse medical data types with insights on applications, techniques, and evaluation gaps

## Executive Summary
This systematic review comprehensively examines recent developments in generative AI for synthetic medical data across multiple modalities including EHR, physiological signals, medical imaging, and clinical text. The review focuses on papers published between January 2021 and November 2023, deliberately excluding earlier GAN-focused literature to emphasize recent advancements in diffusion models and large language models. The survey identifies clinically valid synthesis applications while highlighting gaps in personalized synthesis approaches, standardized evaluation methodologies, and limited utilization of synthetic data beyond augmentation.

## Method Summary
The review employed systematic searches across PubMed, Scopus, and arXiv using queries combining medical data types with generative models, yielding 249 papers after screening. The methodology involved categorizing papers by data modality (EHR, signals, images, text) and analysis type (synthesis applications, generation techniques, evaluation methods). Structured tables were created for each modality summarizing synthesis applications, model types, evaluation metrics, and code availability. The review's temporal restriction to 2021-2023 intentionally focused on recent developments beyond traditional GANs.

## Key Results
- Identified clinically valid synthesis applications across inter-modal translation, attribute conditioning, and text-based generation
- Revealed gaps in current practices including need for personalized synthesis approaches and standardized evaluation methodologies
- Highlighted limited utilization of synthetic data beyond augmentation, with emphasis on clinical utility and privacy preservation

## Why This Works (Mechanism)
### Mechanism 1
- Claim: The review achieves breadth by deliberately excluding earlier GAN-focused literature and including recent diffusion models and LLMs.
- Mechanism: Search query restricts to Jan 2021–Nov 2023 and excludes reviews/perspectives, ensuring focus on novel generative models beyond traditional GANs.
- Core assumption: Earlier surveys adequately cover GANs; recent models (diffusion, LLMs) are underrepresented in prior reviews.
- Evidence anchors:
  - [abstract]: "This period emphasizes recent advancements beyond GANs, which have been extensively covered previously."
  - [section]: "We limit our search to papers and preprints published within the time frame of January 2021 to November 2023. Furthermore, we exclude review papers from our search to maintain our focus on original research and primary studies."

### Mechanism 2
- Claim: Modality categorization (EHR, signals, images, text) enables structured comparison across data types.
- Mechanism: Consistent table templates per modality with fields for synthesis application, model type, evaluation, and code availability.
- Core assumption: Common evaluation and generation patterns exist across modalities, allowing meaningful cross-domain insights.
- Evidence anchors:
  - [abstract]: "Our survey aims to provide a holistic understanding of the applications of these generative models in generating medical synthetic data."
  - [section]: "For each modality, we present a table summarizing various aspects... This structured approach enhances comprehension and comparison across different modalities."

### Mechanism 3
- Claim: Systematic categorization of synthesis applications (inter/intra-modal translation, attribute/text conditioning) reveals clinically relevant use cases.
- Mechanism: Applications grouped by data type with visual diagrams (Fig. 4, Fig. 5) showing conditional vs unconditional generation.
- Core assumption: Clinically valid applications map cleanly to generation paradigms and data types.
- Evidence anchors:
  - [abstract]: "It highlights clinically valid synthesis applications, demonstrating the potential of synthetic data to address diverse clinical requirements."
  - [section]: "Conditional generative models have demonstrated impressive capabilities in synthesizing missing or corrupted modalities across a diverse range of applications."

## Foundational Learning
- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: GANs are the baseline generative model for synthetic medical data; understanding their variants (WGAN, cGAN, StyleGAN) is essential for interpreting results.
  - Quick check question: What modification distinguishes a conditional GAN from a vanilla GAN?

- Concept: Diffusion Models
  - Why needed here: Recent diffusion-based approaches (DDPM, LDM) represent state-of-the-art for high-fidelity medical image synthesis.
  - Quick check question: How does a latent diffusion model differ computationally from a standard DDPM?

- Concept: Conditional Generation
  - Why needed here: Most clinically useful synthetic data is conditioned on labels, masks, or patient attributes rather than generated unconditionally.
  - Quick check question: What advantage does class-conditional generation provide for rare disease synthesis?

## Architecture Onboarding
- Component map: PubMed/Scopus/ArXiv queries → 249 papers → Screening → Categorization by modality → Analysis by cross-cutting themes → Structured tables and diagrams
- Critical path: Query → Screening → Categorization → Analysis → Reporting
- Design tradeoffs:
  - Breadth vs depth: Excluding older GAN work for breadth but potentially missing foundational insights
  - Modality-specific vs unified metrics: Balancing domain-specific fidelity measures with cross-domain comparability
  - Qualitative vs quantitative evaluation: Human assessment vs automated metrics
- Failure signatures:
  - Missing key modalities → imbalanced analysis
  - Inconsistent table fields → comparison difficulty
  - Over-reliance on quantitative metrics → overlooking clinical utility
- First 3 experiments:
  1. Replicate a modality table (e.g., EHR) with synthetic data to test field completeness
  2. Map a single paper's synthesis application to the categorization schema
  3. Compare evaluation metrics across two modalities to identify gaps

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What are the long-term clinical outcomes of using synthetic medical data for model training and validation compared to real data?
- Basis in paper: [explicit] The paper mentions the potential of synthetic data for validation and testing, but highlights limited exploration of its long-term clinical utility.
- Why unresolved: Most studies focus on short-term utility metrics and lack longitudinal follow-up to assess the impact of synthetic data on patient outcomes.
- What evidence would resolve it: Longitudinal studies comparing the performance of models trained on synthetic vs. real data in real-world clinical settings, tracking patient outcomes over extended periods.

### Open Question 2
- Question: How can generative models be effectively tailored to capture the unique statistical properties and acquisition characteristics of different medical imaging modalities?
- Basis in paper: [inferred] The paper discusses the limitations of using loss functions and architectures designed for natural images in medical imaging synthesis, suggesting a need for modality-specific approaches.
- Why unresolved: Most current methods rely on generic architectures without incorporating domain-specific knowledge about image acquisition physics and typical noise patterns.
- What evidence would resolve it: Comparative studies evaluating the performance of modality-specific generative models against generic models, using metrics that capture the unique characteristics of each medical imaging modality.

### Open Question 3
- Question: What are the ethical implications of using synthetic medical data, particularly regarding bias amplification and the potential for misuse in clinical decision-making?
- Basis in paper: [inferred] The paper highlights the need for fairness and inclusivity in medical data generation, but does not delve into the ethical considerations of synthetic data use.
- Why unresolved: The potential for synthetic data to perpetuate or exacerbate existing biases in healthcare data, and the lack of clear guidelines for its ethical use in clinical settings.
- What evidence would resolve it: Frameworks for assessing and mitigating bias in synthetic medical data, along with ethical guidelines for its responsible use in clinical decision support systems.

## Limitations
- Temporal scope (2021-2023) excludes potentially relevant earlier work on foundational GAN models and longitudinal trends
- Confidence in clinically valid synthesis applications is Medium due to reliance on literature review rather than direct clinical validation
- Evaluation methodology gaps identified are based on patterns in reviewed literature rather than comprehensive empirical testing

## Confidence
- Breadth of generative models covered: High
- Clinically valid synthesis applications: Medium
- Evaluation methodology gaps: Medium

## Next Checks
1. Replicate the search query using alternative database combinations (e.g., Google Scholar, bioRxiv) to verify that the 249 papers identified represent the full scope of recent generative medical data research.

2. Cross-validate the modality categorization by applying the structured table templates to 10 randomly selected papers from outside the review period to test whether the categorization schema remains applicable to papers not selected for inclusion.

3. Assess evaluation metric completeness by comparing the evaluation metrics used in the reviewed papers against a comprehensive checklist of established medical data fidelity and utility metrics to identify any systematic omissions.