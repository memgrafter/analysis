---
ver: rpa2
title: 'JetFormer: An Autoregressive Generative Model of Raw Images and Text'
arxiv_id: '2411.19722'
source_url: https://arxiv.org/abs/2411.19722
tags:
- image
- noise
- images
- jetformer
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JetFormer introduces an autoregressive decoder-only transformer
  trained directly on raw image pixels and text without relying on separately pretrained
  encoders or decoders. It uses a normalizing flow to create a lossless, differentiable
  soft-token representation of images, which is jointly trained with the transformer
  using a Gaussian mixture loss.
---

# JetFormer: An Autoregressive Generative Model of Raw Images and Text

## Quick Facts
- **arXiv ID**: 2411.19722
- **Source URL**: https://arxiv.org/abs/2411.19722
- **Reference count**: 18
- **Primary result**: Autoregressive transformer trained directly on raw image pixels and text, achieving competitive text-to-image generation quality with VQ-VAE/VAE baselines while enabling end-to-end optimization.

## Executive Summary
JetFormer presents a novel autoregressive generative model that operates directly on raw image pixels and text without relying on separately pretrained encoders or decoders. The model uses a normalizing flow to create a lossless, differentiable soft-token representation of images, which is jointly trained with a transformer using a Gaussian mixture loss. This approach enables end-to-end optimization of both image and text modalities while maintaining the ability to generate high-fidelity images and perform robust image understanding.

## Method Summary
JetFormer introduces an autoregressive decoder-only transformer trained directly on raw image pixels and text without relying on separately pretrained encoders or decoders. It uses a normalizing flow to create a lossless, differentiable soft-token representation of images, which is jointly trained with the transformer using a Gaussian mixture loss. This approach allows end-to-end optimization of both image and text modalities while maintaining the ability to generate high-fidelity images and perform robust image understanding.

## Key Results
- Achieves text-to-image generation quality competitive with VQ-VAE and VAE-based baselines on ImageNet and MS-COCO
- Provides strong log-likelihood bounds through joint training of image and text modalities
- Ablations show that removing the normalizing flow, omitting a noise curriculum during training, or not factoring out redundant dimensions significantly degrades performance

## Why This Works (Mechanism)
JetFormer's effectiveness stems from its end-to-end trainable architecture that directly processes raw image pixels without separate encoding stages. The normalizing flow creates a continuous latent space that preserves all image information while enabling gradient-based optimization. The Gaussian mixture loss allows the model to handle the inherently multimodal nature of pixel distributions. By training the transformer jointly on image and text data, the model learns rich cross-modal representations that enable both generation and understanding tasks.

## Foundational Learning
1. **Autoregressive Modeling** - Why needed: To generate images and text sequentially while maintaining context. Quick check: Verify that tokens are generated in a causal order without looking ahead.
2. **Normalizing Flows** - Why needed: To create a lossless, differentiable mapping between pixel space and a continuous latent space. Quick check: Confirm that the flow is invertible and volume-preserving.
3. **Gaussian Mixture Models** - Why needed: To handle the multimodal nature of pixel intensity distributions. Quick check: Ensure the mixture components can capture the full range of pixel values.
4. **Joint Multimodal Training** - Why needed: To enable cross-modal understanding and generation. Quick check: Validate that the model can perform both text-to-image and image-to-text tasks.

## Architecture Onboarding
**Component Map**: Image pixels -> Normalizing Flow -> Soft tokens -> Transformer -> Text embedding -> Gaussian Mixture Loss
**Critical Path**: The sequence from raw pixels through the normalizing flow to soft tokens, then through the transformer to generate text or image tokens, represents the core generation pipeline.
**Design Tradeoffs**: Direct pixel processing eliminates the need for separate encoders but requires handling high-dimensional input. The normalizing flow adds computational overhead but enables lossless, differentiable representations.
**Failure Signatures**: Poor generation quality when the normalizing flow fails to capture pixel distributions, or when the transformer cannot learn effective cross-modal representations.
**First Experiments**: 1) Train on synthetic datasets to verify basic functionality. 2) Compare generation quality with and without the normalizing flow. 3) Test cross-modal understanding on paired image-text datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to larger image resolutions and higher-dimensional color spaces remains unproven
- Computational efficiency of the normalizing flow decoder during inference is uncharacterized
- It's unclear whether the Gaussian mixture loss or autoregressive transformer architecture primarily drives generation quality

## Confidence
- **High confidence**: The normalizing flow provides a lossless, differentiable soft-token representation of images
- **Medium confidence**: Competitive text-to-image generation quality with VQ-VAE/VAE baselines
- **Low confidence**: Claims about robustness of image understanding without quantitative metrics

## Next Checks
1. Benchmark JetFormer on higher-resolution datasets (e.g., 256x256 or 512x512) to assess scalability limits
2. Measure inference latency and memory usage of the normalizing flow decoder under various batch sizes
3. Conduct controlled ablation studies isolating the contribution of the Gaussian mixture loss from the autoregressive transformer architecture on generation fidelity