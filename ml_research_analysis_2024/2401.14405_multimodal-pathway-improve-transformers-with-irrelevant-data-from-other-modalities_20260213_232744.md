---
ver: rpa2
title: 'Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities'
arxiv_id: '2401.14405'
source_url: https://arxiv.org/abs/2401.14405
tags:
- data
- modalities
- modality
- transformer
- auxiliary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Multimodal Pathway, a framework to improve
  transformers of a specific modality using irrelevant data from other modalities.
  The key idea is to establish connections (pathways) between the components of transformers
  trained on different modalities, allowing the target modality's transformer to leverage
  knowledge learned from another modality.
---

# Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities

## Quick Facts
- arXiv ID: 2401.14405
- Source URL: https://arxiv.org/abs/2401.14405
- Reference count: 40
- This paper proposes Multimodal Pathway, a framework to improve transformers of a specific modality using irrelevant data from other modalities through Cross-Modal Re-parameterization.

## Executive Summary
This paper introduces Multimodal Pathway, a method for improving transformers trained on one modality by leveraging knowledge from transformers trained on irrelevant data from other modalities. The key innovation is Cross-Modal Re-parameterization, which connects corresponding linear layers between transformers through learnable scaling factors, enabling zero-inference-cost knowledge transfer. The authors demonstrate consistent improvements across image, video, point cloud, and audio recognition tasks, with relative gains ranging from 0.4% to 7.1%. The work shows that transformers can learn modality-complementary knowledge that is transferable across different data types, even when the training data is irrelevant to the target task.

## Method Summary
The method involves pretraining auxiliary transformers on different modalities using self-supervised learning (MAE-style), then connecting them to target transformers through Cross-Modal Re-parameterization. This re-parameterization adds auxiliary weights to target model layers with learnable scaling factors during training, which are then merged into a single model for inference. The approach maintains the original model architecture and parameter count while achieving improved performance. The authors validate their method across four different modalities (image, video, point cloud, audio) on multiple benchmark datasets, showing consistent improvements over baselines.

## Key Results
- Relative improvements of 0.4% to 7.1% across different tasks and modalities
- Consistent performance gains on ImageNet-1K, MSCOCO, ADE20K, ShapeNetPart, PartNet, AudioSet-2k, and Kinetics-400 benchmarks
- Zero inference cost achieved through weight re-parameterization
- Effectiveness demonstrated across image, video, point cloud, and audio recognition tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-Modal Re-parameterization leverages modality-complementary knowledge by connecting corresponding linear layers in transformers trained on different modalities.
- Mechanism: For each linear layer in the target transformer, the corresponding layer from an auxiliary transformer (trained on a different modality) is added with a learnable scaling factor. This allows the target model to utilize the sequence-to-sequence modeling knowledge learned by the auxiliary model.
- Core assumption: Transformers trained on different modalities develop similar hierarchical representations that can be transferred through linear layer connections.
- Evidence anchors:
  - [abstract] "We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models"
  - [section 3.2] "With Cross-Modal Re-parameterization, we simply re-parameterize the layer with parameters of its counterpart in another modal that is trained on another modality"
  - [corpus] Weak evidence - corpus contains papers on multimodal learning but none specifically address cross-modal re-parameterization through linear layer connections
- Break condition: The break condition occurs when the linear layers in transformers from different modalities do not develop transferable hierarchical representations, which would make the cross-modal connections ineffective.

### Mechanism 2
- Claim: Modality-complementary knowledge exists in transformers and can be transferred across modalities without paired data.
- Mechanism: During training on a specific modality, transformers learn not just modality-specific knowledge but also general sequence-to-sequence modeling capabilities that can be applied to other modalities.
- Core assumption: Sequence-to-sequence modeling abilities learned in one modality can be transferred to improve modeling in another modality, even when data samples are irrelevant.
- Evidence anchors:
  - [abstract] "transformers can learn modality-complementary knowledge that is transferable across different data types"
  - [section 1] "we seek the modality-complementary knowledge of sequence-to-sequence modeling in transformers and will show that it does exist"
  - [corpus] Weak evidence - corpus papers focus on multimodal learning with paired data, not on transferring knowledge from irrelevant data
- Break condition: The break condition occurs when transformers only learn modality-specific knowledge and cannot transfer general sequence-to-sequence modeling capabilities across modalities.

### Mechanism 3
- Claim: The Cross-Modal Re-parameterization method achieves zero inference cost while providing significant training benefits.
- Mechanism: By re-parameterizing the weights of the target model with auxiliary model weights scaled by learnable parameters, the method adds connections between models during training but merges them into a single model for inference.
- Core assumption: Weight re-parameterization can equivalently implement the conceptual pathway connections between models without requiring additional inference computation.
- Evidence anchors:
  - [section 3.2] "the extra training costs are marginal...and we can merge the weights after training so that the structure and number of parameters of the resultant model will be identical to a regular model"
  - [section 4.4.2] "we merge the parameters by computing ˆW = W + λW ′ and save it only. For inference, we simply construct a regular linear layer and load ˆW"
  - [corpus] Weak evidence - corpus contains papers on multimodal learning but none specifically address zero-inference-cost methods
- Break condition: The break condition occurs when the re-parameterization cannot be efficiently merged, requiring additional parameters or computation during inference.

## Foundational Learning

- Concept: Sequence-to-sequence modeling in transformers
  - Why needed here: The paper's core claim is that transformers learn transferable sequence-to-sequence modeling knowledge across modalities
  - Quick check question: Can you explain how a transformer converts input sequences to output sequences using self-attention and feed-forward networks?

- Concept: Modality-specific vs. modality-agnostic components in transformer architectures
  - Why needed here: Understanding which components can be shared across modalities is crucial for the pathway construction
  - Quick check question: What parts of a transformer architecture are typically modality-specific (tokenizer, head) versus modality-agnostic (transformer blocks)?

- Concept: Weight re-parameterization techniques
  - Why needed here: Cross-Modal Re-parameterization relies on efficiently merging auxiliary weights into target weights
  - Quick check question: How does weight re-parameterization differ from simply adding more parameters to a model?

## Architecture Onboarding

- Component map: Tokenizer → Transformer blocks (with Cross-Modal Re-parameterization) → Head
- Critical path: Input data flows through modality-specific tokenizer, then through transformer blocks enhanced with cross-modal connections, finally through modality-specific head for output
- Design tradeoffs:
  - Training efficiency vs. inference efficiency: Cross-Modal Re-parameterization adds training complexity but maintains inference efficiency
  - Model size vs. performance: The method uses existing weights rather than adding new parameters
  - Modality compatibility vs. specificity: Must balance universal modeling with modality-specific requirements
- Failure signatures:
  - Degraded performance on target modality compared to baseline
  - Training instability when combining weights from different modalities
  - Poor generalization when transferring to new tasks
- First 3 experiments:
  1. Apply Cross-Modal Re-parameterization to a single linear layer and measure performance impact
  2. Test different initialization values for Cross-Modal Scale parameters
  3. Compare performance when connecting transformer blocks of different depths (early vs. late layers)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical mechanism behind the transferability of modality-complementary knowledge in transformers across different data types?
- Basis in paper: [explicit] The authors observe consistent improvements across multiple modalities but acknowledge that "the theory behind the improvements remains to be revealed" and call for "further investigations (e.g., a mathematically provable bound)".
- Why unresolved: While empirical evidence shows improvements, the paper does not provide a theoretical framework explaining why transformers trained on one modality can effectively transfer knowledge to improve performance on completely different modalities.
- What evidence would resolve it: A formal mathematical proof demonstrating bounds on knowledge transferability, or a theoretical model explaining the mechanisms of modality-complementary knowledge transfer.

### Open Question 2
- Question: How does the effectiveness of multimodal pathway connections vary with different architectural depths in transformers?
- Basis in paper: [explicit] The authors conduct experiments showing that "doing so decreases the accuracy to 83.61%, which is 0.32% lower than the normal M2PT" when reversing the order of auxiliary weights, suggesting depth matters.
- Why unresolved: While the paper shows that matching depth levels is important, it doesn't explore how effectiveness varies across different depth pairings or whether certain layers are more important for knowledge transfer than others.
- What evidence would resolve it: Systematic ablation studies varying which specific layers are connected and analyzing the performance impact of different depth-matching strategies.

### Open Question 3
- Question: Can multimodal pathway learning be effectively extended to non-transformer architectures like CNNs?
- Basis in paper: [explicit] The authors mention "In the future, we will explore to construct multimodal pathways among CNNs and cross-architecture" as a limitation and future direction.
- Why unresolved: The paper only demonstrates multimodal pathway learning with transformers, leaving open whether the concept can be generalized to other neural network architectures that also process hierarchical representations.
- What evidence would resolve it: Experiments demonstrating multimodal pathway learning with CNNs or other architectures, showing comparable or improved performance over unimodal training.

## Limitations

- The paper lacks a theoretical framework explaining why cross-modal knowledge transfer works, acknowledging that "the theory behind the improvements remains to be revealed"
- The method's effectiveness has only been demonstrated with transformer architectures, with no exploration of whether it generalizes to other neural network architectures like CNNs
- The paper does not provide detailed ablation studies showing which specific components of auxiliary transformers contribute most to performance gains

## Confidence

- High confidence: Cross-Modal Re-parameterization consistently improves performance across different modalities and tasks
- Medium confidence: Transformers learn modality-complementary sequence-to-sequence modeling knowledge
- Low confidence: The method generalizes universally across all transformer architectures and modality pairs

## Next Checks

1. Conduct ablation studies isolating the contribution of each linear layer connection in Cross-Modal Re-parameterization to understand which layers are most critical for knowledge transfer.

2. Test the method with auxiliary models trained using different objectives (e.g., supervised vs. self-supervised) to determine whether the quality of auxiliary knowledge affects transfer performance.

3. Evaluate Cross-Modal Re-parameterization on additional transformer architectures beyond the four tested (ViT, M2PT, PCT, AST) to assess generalizability across different model families.