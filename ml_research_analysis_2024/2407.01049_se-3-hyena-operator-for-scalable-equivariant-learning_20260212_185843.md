---
ver: rpa2
title: SE(3)-Hyena Operator for Scalable Equivariant Learning
arxiv_id: '2407.01049'
source_url: https://arxiv.org/abs/2407.01049
tags:
- equivariant
- hyena
- vector
- context
- geometric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SE(3)-Hyena introduces an equivariant long-convolutional model
  that processes global geometric context in sub-quadratic time while preserving equivariance
  to rotations and translations. The model uses vector long convolutions based on
  cross products between equivariant queries and keys, implemented efficiently in
  the Fourier domain.
---

# SE(3)-Hyena Operator for Scalable Equivariant Learning

## Quick Facts
- arXiv ID: 2407.01049
- Source URL: https://arxiv.org/abs/2407.01049
- Authors: Artem Moskalev; Mangal Prakash; Rui Liao; Tommaso Mansi
- Reference count: 40
- Primary result: 3.5x faster than equivariant transformers on 20k tokens while enabling up to 175x longer context

## Executive Summary
SE(3)-Hyena introduces an equivariant long-convolutional model that processes global geometric context in sub-quadratic time while preserving equivariance to rotations and translations. The model uses vector long convolutions based on cross products between equivariant queries and keys, implemented efficiently in the Fourier domain. Experiments on equivariant associative recall and n-body modeling show SE(3)-Hyena matches or exceeds equivariant self-attention performance while requiring significantly less memory and compute for long sequences.

## Method Summary
SE(3)-Hyena replaces quadratic self-attention with FFT-based long convolutions to achieve sub-quadratic complexity while maintaining SE(3) equivariance. The model uses scalar and vector long convolutions that decompose cross products into scalar convolutions, enabling O(N log² N) complexity instead of O(N²). Clifford MLPs handle input projection and gating while preserving equivariance properties. The architecture processes both invariant scalar features and equivariant vector features through separate streams with gating mechanisms.

## Key Results
- 3.5x faster runtime than SE(3)-Transformer on 20k token sequences
- Enables up to 175x longer context within the same memory budget
- Matches or exceeds SE(3)-Transformer performance on equivariant associative recall and n-body modeling tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SE(3)-Hyena achieves sub-quadratic global geometric context modeling by replacing quadratic self-attention with FFT-based long convolutions.
- Mechanism: The Hyena operator uses scalar and vector long convolutions that decompose cross products into scalar convolutions, enabling O(N log² N) complexity instead of O(N²).
- Core assumption: FFT-based convolutions can approximate the global context aggregation that self-attention provides while maintaining equivariance.
- Evidence anchors:
  - [abstract] "SE(3)-Hyena captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations"
  - [section] "we employ circular FFT-convolution to reduce the computational complexity"
  - [corpus] Weak evidence - corpus neighbors focus on geometric equivariance but not specifically on FFT-based efficiency
- Break condition: When the cross product decomposition becomes numerically unstable for high-dimensional features or when FFT precision limits exceed model requirements.

### Mechanism 2
- Claim: Equivariant vector long convolution preserves SE(3) equivariance through cross product decomposition.
- Mechanism: Cross products are inherently rotation-equivariant, and decomposing them into scalar convolutions via Levi-Civita symbols maintains this property while enabling efficient computation.
- Core assumption: The cross product decomposition into scalar convolutions preserves the mathematical properties needed for equivariance.
- Evidence anchors:
  - [section] "Since a cross product is already equivariant to rotations, the whole vector convolution is also equivariant to rotations"
  - [section] "the l-th component of the vector convolution in Equation 2 can be written element-wise as: (qeqv ⊛× keqv)i [l] = εlhp (qeqv[h] ⊛ keqv[p])i"
  - [corpus] Weak evidence - corpus neighbors discuss equivariant networks but not this specific cross product decomposition technique
- Break condition: When the Levi-Civita symbol decomposition introduces numerical instability or when the model requires equivariance beyond SE(3) to other transformation groups.

### Mechanism 3
- Claim: Gating mechanism maintains equivariance while enabling data-controlled filtering.
- Mechanism: Clifford MLP with grade-one output projection creates invariant gating masks that can be applied element-wise to both scalar and vector features without breaking equivariance.
- Core assumption: The Clifford MLP structure ensures that gating remains invariant to geometric transformations while allowing adaptive feature selection.
- Evidence anchors:
  - [section] "we employ Clifford MLP to obtain a gating mask while allowing interaction between invariant and equivariant subspaces"
  - [section] "By employing grade-one output projection in the Clifford MLP to obtain masking values, the gating mechanism remains E(3)-invariant"
  - [corpus] Moderate evidence - corpus includes related work on Clifford group equivariant networks
- Break condition: When the Clifford MLP becomes too complex to train effectively or when the grade-one projection limits the model's representational capacity.

## Foundational Learning

- Concept: Equivariance to rotations and translations (SE(3) group)
  - Why needed here: The model must preserve geometric relationships when the entire coordinate system is rotated or translated
  - Quick check question: If you rotate all input points by 45 degrees, what should happen to the model's output predictions?

- Concept: Cross product properties and Levi-Civita symbols
  - Why needed here: The vector convolution relies on cross products being equivariant and can be decomposed using Levi-Civita symbols
  - Quick check question: What is the result of the cross product between (1,0,0) and (0,1,0), and is this result rotation-equivariant?

- Concept: Fourier transforms and circular convolution
  - Why needed here: FFT-based convolution reduces computational complexity from quadratic to near-linear
  - Quick check question: What is the computational complexity of a standard convolution versus an FFT-based convolution for sequence length N?

## Architecture Onboarding

- Component map:
  - Input projection: E(n)-equivariant Clifford MLP converting geometric and scalar tokens to queries, keys, values
  - Scalar long convolution: FFT-based convolution for invariant feature processing
  - Vector long convolution: Cross product-based convolution for equivariant feature processing
  - Gating: Clifford MLP producing invariant masks for both streams
  - Output projection: E(n)-equivariant Clifford MLP producing final outputs
  - Residual connections: Standard addition between input and processed features

- Critical path: Input projection → Scalar/Vector convolution → Gating → Residual → Output projection

- Design tradeoffs:
  - Accuracy vs efficiency: Self-attention provides perfect global context but at O(N²) cost; Hyena provides approximate global context at O(N log² N)
  - Expressiveness vs equivariance: Clifford MLPs enable equivariant processing but add architectural complexity
  - Memory vs context length: FFT-based methods scale better but require careful buffer management

- Failure signatures:
  - Training instability: Likely due to numerical issues in cross product decomposition or FFT precision
  - Poor generalization: May indicate insufficient equivariance preservation or inadequate gating
  - Memory overflow: Could signal inefficient FFT implementation or excessive buffer allocation

- First 3 experiments:
  1. Implement and verify the vector long convolution on synthetic rotational data to confirm equivariance preservation
  2. Benchmark FFT-based convolution speed against standard convolution for increasing sequence lengths
  3. Test the gating mechanism with controlled invariant and equivariant inputs to verify proper mask application

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the vector long convolution be generalized to work in arbitrary dimensions beyond 3 and 7, given that cross products are only defined in these dimensions?
- Basis in paper: [explicit] "an interesting direction for future improvement is to adapt the vector convolution to function across arbitrary dimensions as it currently relies on a cross product, which is only feasible in 3 and 7 dimensions (Massey, 1983)."
- Why unresolved: The paper acknowledges this limitation but does not provide a solution or workaround for extending the method to other dimensionalities.
- What evidence would resolve it: A mathematical formulation or algorithm that extends the vector long convolution concept to arbitrary dimensions, along with experimental validation showing comparable performance to the 3D version.

### Open Question 2
- Question: How does the performance of SE(3)-Hyena scale when applied to molecular systems with larger vocabularies and more complex geometric relationships?
- Basis in paper: [inferred] The paper evaluates SE(3)-Hyena on simple tasks like n-body problems and associative recall, but does not test it on complex molecular datasets.
- Why unresolved: The current experiments use relatively small vocabularies and simple geometric structures, leaving uncertainty about how the model performs on real-world molecular data.
- What evidence would resolve it: Results from experiments on large-scale molecular datasets (e.g., QM9, MD17) showing how SE(3)-Hyena's performance scales with vocabulary size and molecular complexity.

### Open Question 3
- Question: Can permutation equivariance be incorporated into the long-convolutional framework to make it suitable for point cloud processing?
- Basis in paper: [explicit] "our method relies on the FFT convolution that is not permutation equivariant... Enhancing the long-convolutional framework to incorporate permutation equivariance or learning this geometric constraint from the data (Moskalev et al., 2023) could offer substantial advantages in these areas."
- Why unresolved: The paper identifies permutation equivariance as a limitation but does not propose a concrete solution for incorporating it into the model.
- What evidence would resolve it: A modified version of SE(3)-Hyena that achieves permutation equivariance while maintaining its computational efficiency, validated on point cloud classification or segmentation benchmarks.

## Limitations
- Cross product decomposition may introduce numerical instability for high-dimensional features
- Clifford MLP implementation details are not fully specified, making exact reproduction difficult
- Performance gains may vary with hardware and implementation specifics beyond theoretical guarantees

## Confidence
- **High Confidence**: The fundamental mathematical framework (SE(3) group theory, cross product equivariance properties) is well-established and correctly applied.
- **Medium Confidence**: The sub-quadratic complexity claim is theoretically sound but depends on practical FFT implementation details that aren't fully specified.
- **Low Confidence**: The exact performance gains relative to baselines could vary significantly based on implementation choices and hardware configurations.

## Next Checks
1. Implement and verify the vector long convolution on synthetic rotational data with controlled perturbations to test numerical stability and equivariance preservation across input values.
2. Systematically vary FFT precision parameters and measure the impact on both computational efficiency and equivariance preservation to identify minimum precision requirements.
3. Test SE(3)-Hyena on additional geometric learning tasks beyond the two presented (e.g., molecular dynamics simulation, protein structure prediction) to validate whether performance advantages generalize across different geometric domains.