---
ver: rpa2
title: Accurate Block Quantization in LLMs with Outliers
arxiv_id: '2403.20137'
source_url: https://arxiv.org/abs/2403.20137
tags:
- block
- arxiv
- quantization
- formats
- elements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient KV-cache storage
  in LLM inference, specifically dealing with outliers that degrade block floating
  point quantization accuracy. The core method, K-sort, exploits channel-wise outlier
  patterns by sorting weight matrix rows by their norms and applying the same permutation
  to query weights.
---

# Accurate Block Quantization in LLMs with Outliers

## Quick Facts
- arXiv ID: 2403.20137
- Source URL: https://arxiv.org/abs/2403.20137
- Authors: Nikita Trukhanov; Ilya Soloveychik
- Reference count: 28
- Primary result: 2x memory savings with BFP8 quantization while maintaining model accuracy

## Executive Summary
This paper addresses the problem of efficient KV-cache storage in LLM inference, specifically dealing with outliers that degrade block floating point quantization accuracy. The core method, K-sort, exploits channel-wise outlier patterns by sorting weight matrix rows by their norms and applying the same permutation to query weights. This rearrangement significantly improves quantization quality without runtime overhead since it's done at compile time. The approach achieves 2x memory savings when using BFP8 quantization while maintaining model accuracy (measured via perplexity on wikitext-2 dataset).

## Method Summary
The K-sort algorithm rearranges weight matrix rows by their norms at compile time to improve block floating point quantization accuracy. By exploiting the observation that outliers in attention keys concentrate in particular channels consistently across tokens and sequences, the method clusters high-magnitude values into separate blocks. This reduces shared exponent distortion across low-magnitude values. The permutation is applied both to weight matrices and query weights, ensuring compatibility while improving quantization quality. The technique achieves 2x memory savings with BFP8 quantization on Llama2-7B while maintaining accuracy.

## Key Results
- 2x memory savings achieved with BFP8 quantization
- Maintained model accuracy (measured via perplexity on wikitext-2 dataset)
- Significant improvements at smaller block sizes (32, 64) compared to larger blocks (128)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Channel-wise outlier patterns allow effective row sorting to improve quantization accuracy.
- Mechanism: When keys have outliers concentrated in specific channels, sorting weight rows by norm clusters high-magnitude values into separate blocks, reducing shared exponent distortion across low-magnitude values.
- Core assumption: Outliers in keys consistently appear in the same channels across tokens and sequences.
- Evidence anchors:
  - [abstract] "We exploit the common channel-wise patterns exhibited by the outliers to rearrange them in such a way, that their quantization quality is significantly improved."
  - [section III] "As noted in [7], the keys tend to exhibit certain outlier patterns. Namely, the outliers often concentrate in particular channels, which are quite consistent both across tokens in input sequences, and across different input sequences."

## Foundational Learning

### BFP (Block Floating Point)
- Why needed: Alternative to traditional floating point that reduces memory bandwidth and storage requirements in LLM inference
- Quick check: Verify BFP stores one shared exponent per block of values rather than per individual value

### KV-cache
- Why needed: Stores key and value representations during autoregressive generation to avoid recomputation
- Quick check: Confirm KV-cache grows with sequence length and becomes a memory bottleneck

### Quantization Error in Block Formats
- Why needed: Understanding how outliers affect shared exponent accuracy in BFP blocks
- Quick check: Verify that a single large outlier forces all values in a block to use a larger exponent, degrading precision of smaller values

## Architecture Onboarding

### Component Map
Pre-trained weights -> K-sort permutation -> Rearranged weight matrices -> BFP quantization -> KV-cache storage -> Inference

### Critical Path
Weight matrix sorting (compile-time) -> BFP quantization with rearranged weights -> KV-cache generation -> Inference with permuted queries

### Design Tradeoffs
Compile-time permutation vs. runtime overhead; memory savings vs. potential accuracy degradation; fixed permutation vs. adaptability to input variations

### Failure Signatures
Accuracy degradation when outlier patterns change significantly; suboptimal quantization when input distribution shifts; potential performance issues if permutation isn't properly applied to both weights and queries

### First Experiments
1. Apply K-sort to Llama2-7B and measure perplexity on wikitext-2
2. Compare memory usage with and without K-sort at BFP8
3. Test different block sizes (32, 64, 128) to identify optimal configuration

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do the benefits of K-sort scale with model size, particularly for models larger than Llama-7B?
- Basis in paper: [inferred] The paper mentions that "Llama-7B is a mid-size model but as shown below it can already benefit tremendously from the application of K-sort. This implies that the gain on larger models will be even more remarkable."
- Why unresolved: The paper only provides empirical results for Llama-2-7B and does not test larger models.
- What evidence would resolve it: Empirical results showing perplexity improvements and memory savings for K-sort applied to larger models (e.g., Llama-2-13B, Llama-2-70B).

### Open Question 2
- Question: How does K-sort perform when applied to the KV-cache of values V, as opposed to keys K?
- Basis in paper: [explicit] The paper states "Similar technique can be applied to the values V" but notes that details are postponed for future publications.
- Why unresolved: The paper only demonstrates results for keys K and mentions values V as future work without providing experimental results.
- What evidence would resolve it: Empirical results showing the effectiveness of K-sort when applied to values V, including any changes in perplexity or memory savings.

### Open Question 3
- Question: What is the optimal block size for BFP12 quantization when using K-sort, and how does this vary across different models?
- Basis in paper: [inferred] The paper shows results for block sizes of 32, 64, and 128, with significant improvements at smaller block sizes, but does not explore a wider range of block sizes.
- Why unresolved: The experiments only test a limited range of block sizes (32, 64, 128) without exploring the full space of possible block sizes.
- What evidence would resolve it: Systematic experiments testing various block sizes (e.g., 16, 32, 64, 96, 128) to determine the optimal block size for different model architectures.

## Limitations
- Assumption of consistent outlier patterns across different inputs and domains
- Fixed compile-time permutation that cannot adapt to dynamic input patterns
- Limited evaluation of quantization formats beyond BFP8

## Confidence
High: The paper provides clear experimental methodology and results for the proposed approach
Medium: The generalizability of the method to larger models and different architectures is suggested but not empirically validated
Low: The effectiveness of applying K-sort to values V is mentioned but not demonstrated

## Next Checks
1. Verify outlier pattern consistency across different input domains and prompt types
2. Test K-sort on larger models (Llama-2-13B, Llama-2-70B) to validate scaling benefits
3. Implement and evaluate K-sort application to values V in addition to keys K