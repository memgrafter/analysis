---
ver: rpa2
title: Emergent Visual Grounding in Large Multimodal Models Without Grounding Supervision
arxiv_id: '2410.08209'
source_url: https://arxiv.org/abs/2410.08209
tags:
- grounding
- visual
- attention
- lmms
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large multimodal models (LMMs) trained
  without explicit grounding supervision already possess some implicit grounding ability,
  which can be unlocked using a training-free "attend-and-segment" method that leverages
  attention maps to produce pixel-level segmentation masks. To further enhance grounding,
  the authors propose DIFFLMM, which replaces the standard CLIP visual encoder with
  a diffusion-based visual encoder while keeping the same training data.
---

# Emergent Visual Grounding in Large Multimodal Models Without Grounding Supervision

## Quick Facts
- arXiv ID: 2410.08209
- Source URL: https://arxiv.org/abs/2410.08209
- Reference count: 40
- Large multimodal models can learn implicit grounding ability without explicit supervision

## Executive Summary
This paper demonstrates that large multimodal models (LMMs) trained without explicit grounding supervision already possess some implicit grounding ability, which can be unlocked using a training-free "attend-and-segment" method that leverages attention maps to produce pixel-level segmentation masks. To further enhance grounding, the authors propose DIFFLMM, which replaces the standard CLIP visual encoder with a diffusion-based visual encoder while keeping the same training data. The approach achieves strong performance on both grounding-specific tasks (e.g., 46.4 mask recall on grounded conversation generation) and general visual question answering benchmarks, outperforming prior grounding-supervised models without suffering from the biases of grounding-specific supervision data.

## Method Summary
The authors propose two key approaches: (1) an attend-and-segment method that extracts attention maps from LMMs and uses them to prompt segmentation models like SAM for pixel-level grounding, and (2) DIFFLMM, which integrates a diffusion-based visual encoder with LMMs to provide better localized visual features. The DIFFLMM architecture combines a diffusion model (like Stable Diffusion) with an implicit captioner and positional encodings, trained on the same visual instruction tuning data as baseline LMMs. The approach maintains general conversation ability while significantly improving grounding performance.

## Key Results
- DIFFLMM achieves 46.4 mask recall on grounded conversation generation
- Maintains strong general VQA performance (e.g., 84.0% on MMMU)
- Outperforms grounding-supervised models without suffering from grounding data biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMMs trained without grounding supervision still learn implicit grounding ability through weak vision-language alignment during pretraining.
- Mechanism: During visual instruction tuning, the model learns to align visual features with language tokens. This alignment process inherently teaches the model to detect and relate visual entities to language components, even without explicit grounding supervision.
- Core assumption: The vision-language alignment process captures sufficient spatial and semantic relationships between visual entities and language tokens.
- Evidence anchors:
  - [abstract]: "we find that grounding ability can be implicitly learned by LMMs to some extent without explicit grounding supervision"
  - [section 1]: "we find that LMMs learn to detect visual entities and relate them with the language implicitly, during the progress of vision-language learning at the image level"
  - [corpus]: Weak - only 0.47 average neighbor FMR suggests moderate topical similarity, but no direct evidence for this specific mechanism
- Break condition: If the visual encoder fails to provide localized features, the implicit grounding ability would be severely degraded.

### Mechanism 2
- Claim: Attention maps from LMMs can be used to locate visual entities without explicit supervision.
- Mechanism: The attention mechanism in LMMs reveals where the model focuses when generating tokens. By normalizing and analyzing these attention maps, we can identify the most relevant visual tokens for each language token, which can then be used to prompt segmentation models.
- Core assumption: Attention maps contain meaningful localization signals that correlate with visual entities.
- Evidence anchors:
  - [section 2]: "attention maps can provide information about where the model is looking at when producing output language tokens"
  - [section 4.4]: "the attention between the output token and the visual tokens can provide interpretable grounding signals already"
  - [corpus]: Weak - no direct evidence for attention-based grounding in corpus
- Break condition: If attention maps are too noisy or uninterpretable, this mechanism would fail to produce accurate grounding.

### Mechanism 3
- Claim: Diffusion-based visual encoders provide better localized features than CLIP for grounding tasks.
- Mechanism: Diffusion models learn to generate high-fidelity images, requiring fine-grained and well-localized visual features. Additionally, their text-to-image generation capability provides vision-language alignment, making them superior to CLIP for grounding tasks.
- Core assumption: Diffusion models' training objective naturally produces more localized and aligned visual features than CLIP.
- Evidence anchors:
  - [section 3.3]: "diffusion models (DMs) strike a better balance: 1) DMs learn to generate high-fidelity images, for which fine-grained and well-localized visual features are necessary. 2) DMs are trained to perform text-to-image generation, and in this procedure, they acquire alignment with language instructions"
  - [section 4.4]: "DIFFLMM achieves better grounding performance than trivially replacing or combining CLIP [54] with a pure-vision encoder, DINOv2 [51]"
  - [corpus]: Weak - only one related paper mentions grounding with LMMs, but not specifically diffusion-based approaches
- Break condition: If diffusion models fail to maintain vision-language alignment while improving localization, grounding performance would degrade.

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: Understanding how attention maps can be used for grounding
  - Quick check question: What does attention between language tokens and visual tokens represent in an LMM?

- Concept: Vision-language alignment and its limitations
  - Why needed here: Understanding why CLIP is insufficient for grounding tasks
  - Quick check question: What is the key difference between image-level alignment (CLIP) and pixel-level alignment needed for grounding?

- Concept: Diffusion model training objectives
  - Why needed here: Understanding why diffusion models provide better features for grounding
  - Quick check question: How does the text-to-image generation objective in diffusion models differ from CLIP's image-text matching objective?

## Architecture Onboarding

- Component map:
  - Visual encoder (CLIP/Diffusion)
  - Vision-to-language projector
  - Large language model
  - Attention extraction module
  - Segmentation model (SAM)
  - Optional: Implicit captioner for diffusion

- Critical path:
  1. Image → Visual features (encoder)
  2. Visual features → Projected tokens
  3. Token generation with attention monitoring
  4. Attention → Point prompt
  5. Point prompt → Segmentation mask

- Design tradeoffs:
  - CLIP vs Diffusion: CLIP provides better general vision-language alignment but weaker localization; Diffusion provides better localization but may require additional components for alignment
  - Attention normalization: Helps remove noise but may also remove some useful signals
  - Point vs Mask prompts: Points are more effective but may miss some context

- Failure signatures:
  - Poor grounding: Attention maps don't correlate with visual entities
  - Catastrophic forgetting: General conversation ability degrades
  - Segmentation failures: SAM produces inaccurate masks even with good prompts

- First 3 experiments:
  1. Test attention maps on simple images to verify they highlight relevant objects
  2. Compare grounding performance with and without attention normalization
  3. Test DIFFLMM vs CLIP-based LMM on grounding tasks while monitoring general VQA performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise mechanisms by which attention maps in LMMs encode visual grounding information, and how do these mechanisms differ across layers and attention heads?
- Basis in paper: [explicit] The paper analyzes attention maps and finds certain heads in intermediate layers produce more localized attention maps, but states it's challenging to directly relate individual heads to visual concepts without grounding annotations
- Why unresolved: The paper aggregates attention maps across all layers and heads rather than exploiting head/layer-specific patterns that might contain richer grounding information
- What evidence would resolve it: Systematic analysis comparing grounding performance using different head/layer combinations, and ablation studies showing which specific attention patterns best predict object locations

### Open Question 2
- Question: How does the grounding ability in LMMs trained without explicit supervision compare to those trained with grounding supervision in terms of robustness to novel visual concepts and domains?
- Basis in paper: [explicit] The paper claims grounding LMMs without supervision avoids biases from limited grounding data and generalizes better, but provides limited empirical comparison of generalization to novel concepts
- Why unresolved: The paper shows DIFFLMM outperforms grounding-supervised models on some benchmarks but doesn't systematically test how well these models generalize to truly novel visual categories or domains
- What evidence would resolve it: Controlled experiments testing both types of models on datasets with visual concepts absent from their training data, measuring degradation in grounding performance

### Open Question 3
- Question: What is the optimal noise level and feature extraction point in the diffusion model for enhancing visual grounding in LMMs?
- Basis in paper: [explicit] The paper performs ablation on noise levels (100, 200, 300 steps) and feature blocks, finding step 100 with the second upsampling block works best, but doesn't explore the full design space
- Why unresolved: The paper only tests a few noise levels and uses a fixed feature block, leaving open questions about whether other configurations might yield better grounding performance
- What evidence would resolve it: Comprehensive ablation studies varying both noise levels across the full diffusion process and exploring different feature extraction points in the U-Net architecture

### Open Question 4
- Question: How do the grounding capabilities unlocked by attend-and-segment compare to those of dedicated grounding architectures when both are trained on the same data?
- Basis in paper: [inferred] The paper claims attend-and-segment achieves comparable or better performance than grounding-supervised models, but these models use different architectures and training procedures
- Why unresolved: The comparison is inherently unfair since grounding LMMs modify both architecture and training data, making it impossible to isolate the effect of supervision versus architectural changes
- What evidence would resolve it: Fair comparison where both approaches use identical architectures and training data, differing only in whether explicit grounding supervision is provided

## Limitations

- Limited evidence that grounding ability is truly "emergent" rather than a byproduct of the segmentation model used
- Insufficient ablation studies to attribute performance gains specifically to the diffusion encoder
- Evaluation focuses heavily on mask recall metrics, which may not fully capture grounding quality

## Confidence

- High confidence: The attend-and-segment method works as described and produces reasonable segmentation masks
- Medium confidence: DIFFLMM architecture improves grounding performance over CLIP-based LMMs
- Low confidence: Claims about implicit grounding ability being "learned" during pretraining, as this is inferred rather than directly measured

## Next Checks

1. **Ablation study on attention map contributions**: Run the attend-and-segment method with random attention maps, uniform attention maps, and the actual LMM attention maps to quantify how much of the grounding performance comes from the LMM's attention versus SAM's general segmentation capability.

2. **Direct feature analysis**: Extract visual features from CLIP and diffusion encoders for the same images and measure their localization properties using established metrics like pointing game accuracy or class activation mapping to validate the claim about diffusion providing better localized features.

3. **Zero-shot generalization test**: Evaluate both CLIP-based and diffusion-based LMMs on completely unseen object categories and scene types to verify that the grounding ability generalizes beyond the training distribution, addressing whether the performance gains are due to overfitting to specific patterns.