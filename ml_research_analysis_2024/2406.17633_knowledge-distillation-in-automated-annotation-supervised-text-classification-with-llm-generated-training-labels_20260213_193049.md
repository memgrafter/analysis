---
ver: rpa2
title: 'Knowledge Distillation in Automated Annotation: Supervised Text Classification
  with LLM-Generated Training Labels'
arxiv_id: '2406.17633'
source_url: https://arxiv.org/abs/2406.17633
tags:
- data
- labels
- gpt-4
- human
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of costly and time-consuming
  human-labeled data for supervised text classification in computational social science.
  It proposes using generative large language models (LLMs) to create surrogate training
  labels for fine-tuning downstream supervised models.
---

# Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels

## Quick Facts
- arXiv ID: 2406.17633
- Source URL: https://arxiv.org/abs/2406.17633
- Reference count: 40
- Key outcome: Supervised models fine-tuned on GPT-4 generated labels perform comparably to human-labeled models, with median F1 difference of 0.039.

## Executive Summary
This paper explores using generative large language models to create training labels for supervised text classification in computational social science, addressing the challenge of costly human annotation. The approach involves validating few-shot GPT-4 performance, generating labels for 1,000 text samples, and fine-tuning smaller supervised models on these labels. The primary finding is that supervised models trained on GPT-4-generated labels achieve performance comparable to those trained on human labels, with a median F1 difference of only 0.039. While GPT-4 few-shot models and their distilled counterparts show high recall, they demonstrate lower precision compared to human-labeled models.

## Method Summary
The method involves three key steps: first, validating GPT-4's few-shot performance on a small subset (n=250) of human-labeled data to ensure quality; second, using the validated prompt to generate labels for an additional 1,000 text samples per task; and third, fine-tuning supervised text classifiers (BERT, RoBERTa, DistilBERT, XLNet, Mistral-7B) on these LLM-generated labels. The models are then evaluated on a held-out set of 1,000 human-labeled samples. The approach treats GPT-4 as a "teacher" model whose knowledge is distilled into smaller "student" models, enabling efficient deployment without repeated expensive LLM calls.

## Key Results
- Supervised models fine-tuned on GPT-4 generated labels achieve median F1 score difference of only 0.039 compared to human-labeled models
- GPT-4 few-shot models and distilled models show significantly higher recall (0.8 median) but lower precision than human-labeled models
- Fine-tuned supervised models perform remarkably close to GPT-4's few-shot performance, with median F1 difference of only 0.006

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot GPT-4 labels can be distilled into smaller supervised models that match human-labeled fine-tuned performance.
- Mechanism: GPT-4 acts as a "teacher" model that produces high-quality surrogate labels. These labels are used to train "student" models (BERT, RoBERTa, etc.) via supervised fine-tuning. The student models approximate the teacher's decision boundary without the cost of per-instance LLM inference.
- Core assumption: The knowledge captured by GPT-4 in few-shot classification is transferable to task-specific supervised models when provided with sufficient labeled examples.
- Evidence anchors:
  - [abstract] "supervised classification models fine-tuned on LLM-generated labels perform comparably to models fine-tuned with labels from human annotators"
  - [section] "models fine-tuned on few-shot surrogate labels from a generative LLM perform comparably to models fine-tuned on human labels"
  - [corpus] Weak: No direct mention of distillation or teacher-student training in neighbors; mostly focused on other annotation or bias issues.
- Break condition: If GPT-4's few-shot performance is poor or inconsistent, the quality of distilled student models will degrade. Human validation of few-shot labels is required before distillation.

### Mechanism 2
- Claim: GPT-4's recall advantage can be preserved in distilled student models, even if precision drops.
- Mechanism: GPT-4's few-shot labels tend to favor higher recall (capturing more true positives) at the expense of precision (more false positives). When student models are trained on these labels, they inherit this bias, leading to high recall but lower precision than human-labeled models.
- Core assumption: The bias in GPT-4's few-shot outputs (high recall, lower precision) is consistent enough to be learned and preserved by the student model.
- Evidence anchors:
  - [abstract] "GPT-4 few-shot models and supervised models trained on GPT-4 generated labels perform significantly better than all other models on recall, but noticeably worse on precision"
  - [section] "GPT-4 few-shot (0.8 median recall) as well as DistilBERT and BERT fine-tuned on GPT-labels (both with 0.746 median recall) achieve significantly better median recall than any model fine-tuned with human labels"
  - [corpus] Weak: Neighbors discuss bias and annotation artifacts but not this specific recall-precision trade-off in distilled models.
- Break condition: If the task requires high precision or balanced precision-recall, using GPT-4-generated labels may be unsuitable without additional filtering or thresholding.

### Mechanism 3
- Claim: Supervised models trained on GPT-4-generated labels perform close to GPT-4's few-shot performance, making distillation cost-effective.
- Mechanism: After distilling few-shot labels into a student model, the student model's performance approaches that of the original few-shot GPT-4 model, but at much lower inference cost. This allows for efficient deployment without repeated expensive LLM calls.
- Core assumption: The student model can approximate the decision boundary of the few-shot GPT-4 model, and the small performance gap is acceptable for the use case.
- Evidence anchors:
  - [abstract] "models trained on surrogate labels from GPT-4 demonstrate very similar validation performance as labels from GPT-4 with few-shot in-context learning"
  - [section] "Supervised models fine-tuned on GPT-4 generated labels perform remarkably close to GPT few-shot models, with a median F1 difference of only 0.006"
  - [corpus] Weak: No explicit discussion of cost or performance trade-offs between few-shot and distilled models in neighbors.
- Break condition: If the task has high variance in few-shot performance, or if the student model architecture is too limited to capture the teacher's behavior, the performance gap may widen.

## Foundational Learning

- Concept: Few-shot in-context learning
  - Why needed here: The approach depends on GPT-4's ability to classify accurately with minimal examples, serving as the source of training labels.
  - Quick check question: Can you explain why the quality of few-shot labels is critical before training the student model?

- Concept: Knowledge distillation (teacher-student training)
  - Why needed here: The paper uses GPT-4's outputs as "soft labels" to train smaller, task-specific models that approximate GPT-4's behavior.
  - Quick check question: How does distillation reduce inference cost compared to using GPT-4 directly?

- Concept: Label noise and consistency
  - Why needed here: GPT-4's outputs may vary across calls; understanding label noise helps decide whether to filter training data for more stable results.
  - Quick check question: What might happen to a student model if it is trained on highly inconsistent labels?

## Architecture Onboarding

- Component map:
  - GPT-4 (teacher) -> Human validation -> Supervised models (students: BERT, RoBERTa, DistilBERT, XLNet, Mistral-7B) -> Evaluation on held-out human-labeled test set

- Critical path:
  1. Validate GPT-4 few-shot on small human-labeled subset
  2. Generate 1,000 labels using validated prompt
  3. Fine-tune supervised model on GPT-4 labels
  4. Evaluate on held-out human-labeled test set

- Design tradeoffs:
  - Few-shot labels vs. human labels: cost vs. precision
  - Model size vs. performance: larger models (BERT) perform better, but DistilBERT is faster and cheaper
  - Label noise tolerance: models can handle some noise, but filtering improves stability at the cost of training data

- Failure signatures:
  - Poor few-shot performance: GPT-4 misclassifies validation samples
  - Student underfitting: model too small or poorly tuned to capture teacher's behavior
  - Overfitting to noisy labels: student model memorizes inconsistent patterns

- First 3 experiments:
  1. Validate GPT-4 few-shot on 250 human-labeled samples and adjust prompt
  2. Generate 1,000 labels with validated prompt and fine-tune a small BERT model
  3. Evaluate the fine-tuned model on a held-out 1,000-sample human-labeled test set and compare F1 to human-labeled baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of supervised models fine-tuned on LLM-generated labels compare when using open-source models like Mistral-7B versus closed-source models like GPT-4?
- Basis in paper: [explicit] The paper compares performance across different models including BERT, RoBERTa, DistilBERT, XLNet, and Mistral-7B, noting a larger gap between Mistral-7B fine-tuned using human labels and GPT-labels (median difference of 0.12) compared to other models.
- Why unresolved: The paper suggests that while BERT and GPT-4 are the strongest performing models, it does not extensively explore the potential of open-source models in comparison to closed-source models.
- What evidence would resolve it: Conducting further experiments comparing the performance of various open-source models against closed-source models on a larger set of diverse classification tasks would provide more insights.

### Open Question 2
- Question: To what extent does prompt optimization affect the performance of generative LLM annotation tasks, and is there a risk of overfitting the prompt to a small subset of the data?
- Basis in paper: [explicit] The paper discusses the prompt tuning process and its minimal impact on performance, suggesting that generative LLMs are fairly robust to slight word changes in the prompt.
- Why unresolved: While the paper notes that prompt optimization has a minor effect on performance, it does not explore the long-term implications of extensive prompt tuning or the risk of overfitting.
- What evidence would resolve it: Conducting a longitudinal study on the effects of prompt optimization over time and across various tasks would help determine the potential for overfitting and its impact on model generalizability.

### Open Question 3
- Question: How does the introduction of noise in GPT-generated labels affect the performance of supervised models, and are these models robust to such noise?
- Basis in paper: [explicit] The paper includes ablation experiments comparing supervised models trained on GPT-generated labels with and without noise, finding minimal differences in performance.
- Why unresolved: The paper suggests that supervised models are robust to noise in labels, but it does not explore the threshold at which noise begins to significantly impact model performance.
- What evidence would resolve it: Further experimentation with varying levels of noise in the training labels and their impact on model performance would clarify the robustness of supervised models to noisy data.

## Limitations

- The human-labeled datasets are stored in password-protected archives, preventing independent verification of reported results
- The precision-recall trade-off (high recall, lower precision for GPT-4-based models) may limit applicability for tasks requiring balanced metrics
- The generalizability across different domains and languages remains untested, as the study focuses on computational social science texts

## Confidence

**High Confidence**: The core finding that supervised models trained on GPT-4-generated labels perform comparably to human-labeled models, with a median F1 difference of 0.039.

**Medium Confidence**: The claim about GPT-4's recall advantage being preserved in distilled models.

**Low Confidence**: The cost-effectiveness argument for knowledge distillation.

## Next Checks

1. **Prompt sensitivity analysis**: Test whether small variations in the few-shot prompt significantly affect GPT-4's classification accuracy and subsequent student model performance. This would validate the robustness of the knowledge distillation approach to prompt engineering.

2. **Label noise filtering experiment**: Apply filtering thresholds to GPT-4-generated labels (e.g., only using labels where GPT-4 shows high confidence) and measure the impact on student model precision and recall. This would test whether the precision drop is due to label noise that could be mitigated.

3. **Cross-domain generalization test**: Apply the validated few-shot prompts and label generation pipeline to a completely different text classification domain (e.g., medical or legal texts) to assess whether the approach generalizes beyond computational social science.