---
ver: rpa2
title: Do Large Language Models Latently Perform Multi-Hop Reasoning?
arxiv_id: '2402.16837'
source_url: https://arxiv.org/abs/2402.16837
tags:
- entity
- reasoning
- frequency
- relative
- bridge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates whether large language models (LLMs) perform
  latent multi-hop reasoning when completing complex prompts like "The mother of the
  singer of ''Superstition'' is". The authors examine two key steps: (1) identifying
  the bridge entity (e.g., Stevie Wonder), and (2) using knowledge about that entity''s
  attribute (e.g., mother) to complete the prompt.'
---

# Do Large Language Models Latently Perform Multi-Hop Reasoning?

## Quick Facts
- arXiv ID: 2402.16837
- Source URL: https://arxiv.org/abs/2402.16837
- Authors: Sohee Yang; Elena Gribovskaya; Nora Kassner; Mor Geva; Sebastian Riedel
- Reference count: 31
- The paper finds evidence for latent multi-hop reasoning in LLMs, with stronger performance in larger models for the first hop but moderate evidence for the second hop overall.

## Executive Summary
This paper investigates whether large language models perform latent multi-hop reasoning when completing complex prompts that require bridging knowledge across multiple facts. The authors introduce TWOHOPFACT, a dataset of 45,595 two-hop prompts, and propose metrics to measure both bridge entity recall and utilization of that entity's knowledge. Through experiments with LLaMA-2 models, they find substantial evidence for first-hop reasoning (70% of cases) that scales with model size, but only moderate evidence for second-hop reasoning (60% and 40% of cases respectively). The findings suggest that while latent multi-hop reasoning exists, its utilization is highly contextual and varies across prompt types.

## Method Summary
The paper analyzes transformer hidden states to measure latent reasoning capabilities without additional training. It uses the TWOHOPFACT dataset of two-hop prompts constructed from Wikidata, calculating entity recall scores (ENTREC) by projecting hidden representations to vocabulary space, and consistency scores (CNST SCORE) by comparing output distributions between two-hop and corresponding one-hop prompts. The analysis examines layer-wise trends in hidden states and uses activation patching to test causal relationships between entity recall and knowledge utilization.

## Key Results
- First-hop reasoning (bridge entity recall) shows strong evidence in 70% of cases, with stronger performance in larger models
- Second-hop reasoning (knowledge utilization) shows moderate evidence in 60% of cases
- Overall multi-hop reasoning (both hops together) shows evidence in only 40% of cases
- Performance varies significantly across different fact composition types, with up to 23% showing strong evidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs use distributed activation pathways that can recall bridge entities when descriptive mentions appear in prompts
- Mechanism: Hidden representations in middle-to-late layers encode increased probability of the bridge entity's first token when descriptive mentions appear
- Core assumption: Hidden state at descriptive mention captures latent entity recall information
- Evidence anchors: [abstract] ENTREC measures internal recall by projecting hidden representations to vocabulary space
- Break condition: If hidden representation doesn't encode entity identity information

### Mechanism 2
- Claim: Increased bridge entity recall leads to more consistent completions when model has knowledge about entity's attribute
- Mechanism: When ENTREC increases, consistency score also increases if model can utilize bridge entity knowledge
- Core assumption: Functional relationship between entity recall strength and attribute knowledge utilization
- Evidence anchors: [abstract] Test if increasing recall causes better utilization of bridge entity knowledge
- Break condition: If model uses different pathways for recall and attribute utilization

### Mechanism 3
- Claim: Multi-hop reasoning emerges from co-occurrence of first-hop recall and second-hop utilization
- Mechanism: When both ENTREC increases AND consistency increases with ENTREC, this indicates latent multi-hop reasoning
- Core assumption: Successful recall and utilization are independent events that can be measured separately
- Evidence anchors: [abstract] Analyze two hops individually and consider co-occurrence as indicative of multi-hop reasoning
- Break condition: If two hops are not independent or measurement noise creates false positives

## Foundational Learning

- Concept: Transformer hidden state dynamics and attention mechanisms
  - Why needed here: Understanding how ENTREC measures entity recall requires knowing how hidden states encode information and how projections to vocabulary space work
  - Quick check question: What layer of the transformer is ENTREC calculated at, and why is this layer choice important?

- Concept: Cross-entropy and consistency metrics
  - Why needed here: The consistency score uses cross-entropy to measure similarity between output distributions
  - Quick check question: Why does the paper use average cross-entropy instead of KL divergence for the consistency score?

- Concept: Activation patching and gradient-based intervention
  - Why needed here: The second-hop reasoning experiment uses activation patching to perturb hidden states
  - Quick check question: How does activation patching work in the context of measuring the effect of entity recall on consistency?

## Architecture Onboarding

- Component map: Dataset construction from Wikidata -> ENTREC calculation at specific layers/positions -> Consistency score computation -> Activation-based perturbation experiments -> Statistical aggregation by fact composition type
- Critical path: Construct two-hop prompts -> Calculate ENTREC at descriptive mention position -> Calculate consistency with corresponding one-hop prompts -> Perturb hidden states to test causal relationships -> Aggregate results by fact composition type
- Design tradeoffs: Uses first-token-based ENTREC for broader applicability rather than full entity embeddings; uses consistency rather than exact accuracy to capture reasoning quality
- Failure signatures: No layer-wise trends in ENTREC, uniformly high consistency scores regardless of prompt changes, or no directional effects in perturbation experiments
- First 3 experiments:
  1. Verify ENTREC works by checking if it increases when replacing non-descriptive mentions with descriptive mentions of the same entity
  2. Test consistency baseline by computing CNST SCORE for random prompt pairs
  3. Run perturbation experiment on small subset of prompts to verify directional effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we better measure the second hop of latent multi-hop reasoning across different model scales?
- Basis in paper: The paper shows first-hop reasoning scales with model size but second-hop reasoning does not
- Why unresolved: Current metrics show moderate evidence for second-hop reasoning that doesn't improve with scale
- What evidence would resolve it: Experiments using different probing methods that capture more distributed representations

### Open Question 2
- Question: What architectural or training modifications could strengthen second-hop reasoning in LLMs?
- Basis in paper: The authors note second-hop reasoning remains relatively constant while first-hop scales
- Why unresolved: Paper identifies this as potential fundamental limitation but doesn't test specific modifications
- What evidence would resolve it: Experiments comparing models with different architectural inductive biases

### Open Question 3
- Question: How do different fact composition types influence the likelihood of latent multi-hop reasoning?
- Basis in paper: Up to 23% of fact composition types demonstrate strong evidence while others show weak or no evidence
- Why unresolved: Paper identifies contextual variation but doesn't systematically analyze properties that predict stronger reasoning
- What evidence would resolve it: Detailed analysis of fact composition properties that correlate with stronger multi-hop reasoning

## Limitations
- ENTREC metric relies on first-token approximation rather than full entity embeddings, potentially missing nuanced representations
- Analysis limited to LLaMA-2 family models without testing across diverse architectures
- Causal claims about second-hop reasoning may be confounded by perturbation methodology affecting multiple pathways simultaneously

## Confidence
- **Medium** for claims about latent multi-hop reasoning detection - methodology relies on simplifying approximations
- **Medium** for claims about scaling benefits - limited to single model family without broader generalization
- **Low** for causal claims about second-hop reasoning - perturbation experiments may not cleanly isolate effects

## Next Checks
1. **Entity Embedding Validation**: Replicate ENTREC using full entity embedding similarity rather than first-token approximation to assess measurement error
2. **Architecture Generalization Test**: Apply same metrics to diverse LLM architectures (GPT-3.5, Claude, PaLM-2) to test generalizability
3. **Perturbation Ablation Study**: Systematically vary perturbation magnitude and direction to map out full response surface of consistency changes