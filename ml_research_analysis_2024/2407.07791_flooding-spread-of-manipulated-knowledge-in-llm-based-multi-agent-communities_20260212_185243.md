---
ver: rpa2
title: Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities
arxiv_id: '2407.07791'
source_url: https://arxiv.org/abs/2407.07791
tags:
- knowledge
- agents
- manipulated
- agent
- spread
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel two-stage attack method to investigate
  the spread of manipulated knowledge within LLM-based multi-agent systems. The attack
  involves Persuasiveness Injection, which biases agents towards generating persuasive
  evidence, followed by Manipulated Knowledge Injection, which alters the agents'
  perception of specific knowledge.
---

# Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities

## Quick Facts
- arXiv ID: 2407.07791
- Source URL: https://arxiv.org/abs/2407.07791
- Reference count: 40
- Introduces a novel two-stage attack method to investigate the spread of manipulated knowledge within LLM-based multi-agent systems

## Executive Summary
This paper presents a two-stage attack method to investigate how manipulated knowledge spreads within LLM-based multi-agent communities. The attack involves Persuasiveness Injection, which biases agents towards generating persuasive evidence, followed by Manipulated Knowledge Injection, which alters the agents' perception of specific knowledge. The results demonstrate that the attack successfully induces agents to spread both counterfactual and toxic knowledge without degrading their foundational capabilities. The study also highlights the persistence of manipulated knowledge through retrieval-augmented generation frameworks, emphasizing significant security risks in LLM-based multi-agent systems.

## Method Summary
The authors propose a two-stage attack method to manipulate knowledge in LLM-based multi-agent systems. Stage I uses Direct Preference Optimization (DPO) with LoRA to bias an injected agent towards generating persuasive evidence. Stage II employs ROME to inject manipulated knowledge into the agent. The attack is tested in a simulated multi-agent environment with five agents, using Vicuna 7B, LLaMA 3 8B, and Gemma 7B models. Experiments measure knowledge spread, persistence through RAG systems, and impact on foundational capabilities using MMLU benchmark.

## Key Results
- Attack successfully induces agents to spread counterfactual and toxic knowledge without degrading foundational capabilities
- Manipulated knowledge persists through retrieval-augmented generation frameworks, continuing to influence agents after initial interaction
- Study highlights significant security risks in LLM-based multi-agent systems and need for robust defenses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agents are more susceptible to manipulated knowledge when it is presented with fabricated but coherent evidence
- Mechanism: LLM-based agents are trained to generate contextually appropriate responses; when prompted with evidence—even if fabricated—they are more likely to integrate and align with the provided information, treating it as factual
- Core assumption: The training objective of LLMs prioritizes coherence over factual verification, making them vulnerable to evidence-supported but false claims
- Evidence anchors:
  - [abstract] "This is because its training on a vast corpus of literature typically includes responding affirmatively to prompts that are supported by evidence, mimicking human cognitive biases towards confirmed information."
  - [section III-C] "From the perspective of benign agents, they are susceptible to erroneous but seemingly well-supported knowledge."
- Break condition: If the agent is explicitly instructed to fact-check or cross-reference evidence against trusted sources, the effectiveness of fabricated evidence diminishes

### Mechanism 2
- Claim: Injected agents can autonomously generate plausible evidence to support manipulated knowledge, deceiving benign agents
- Mechanism: LLMs have the intrinsic capability to generate coherent and contextually appropriate outputs, allowing compromised agents to produce detailed and convincing evidence for false claims, leveraging their training to maintain sentence coherence
- Core assumption: The pre-training objectives of LLMs do not directly validate the truthfulness of the facts they generate, focusing instead on predicting the next token that maintains coherence
- Evidence anchors:
  - [abstract] "This inherent capability allows them to produce detailed and convincing evidence when required."
  - [section III-C] "LLMs possess the intrinsic capability to generate coherent and contextually appropriate outputs... These agents are likely to fabricate hallucinated evidence that bolsters their incorrect assertions."
- Break condition: If the agent's training includes explicit constraints against generating unsupported claims or if external verification mechanisms are in place, the generation of plausible evidence is hindered

### Mechanism 3
- Claim: Manipulated knowledge persists through retrieval-augmented generation (RAG) systems, continuing to influence agents even after the initial interaction
- Mechanism: When benign agents store chat histories in RAG systems, the manipulated knowledge is indexed and can be retrieved in future interactions, causing agents to reference and spread the false information without awareness
- Core assumption: RAG systems treat stored chat histories as reliable knowledge sources, and agents do not validate the authenticity of retrieved information
- Evidence anchors:
  - [abstract] "Furthermore, we show that these manipulations can persist through popular retrieval-augmented generation frameworks, where several benign agents store and retrieve manipulated chat histories for future interactions."
  - [section IV-E] "This scenario is particularly concerning because it reveals the risk of sustained influence, where counterfactual or toxic information continues to be disseminated even after the original injected agent is no longer active."
- Break condition: If the RAG system incorporates mechanisms to verify the credibility of stored information or if agents are trained to critically assess retrieved data, the persistence of manipulated knowledge is reduced

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is used in Stage I to bias the injected agent towards generating persuasive evidence without degrading foundational capabilities
  - Quick check question: How does DPO differ from traditional RLHF in terms of training efficiency and data requirements?

- Concept: Knowledge Locality Hypothesis
  - Why needed here: The hypothesis underpins the Manipulated Knowledge Injection by assuming specific knowledge is stored in identifiable regions of the LLM, allowing targeted edits
  - Quick check question: What are the implications of the knowledge locality hypothesis for the precision and scope of knowledge editing in LLMs?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG systems are exploited in the persistence of manipulated knowledge, highlighting the need to understand how external knowledge integration can be manipulated
  - Quick check question: How can RAG systems be modified to detect and mitigate the spread of manipulated knowledge from stored interactions?

## Architecture Onboarding

- Component map:
  - Multi-agent chat environment with N agents, each assigned roles and attributes
  - Two-stage attack pipeline: Stage I (Persuasiveness Injection using DPO) and Stage II (Manipulated Knowledge Injection using ROME)
  - RAG system for storing and retrieving chat histories
  - Evaluation metrics: Accuracy, Rephrase Accuracy, Locality Accuracy, and MMLU benchmark

- Critical path:
  1. Set up the multi-agent simulation environment
  2. Implement the two-stage attack on a chosen agent
  3. Conduct experiments to measure knowledge spread among agents
  4. Analyze the persistence of manipulated knowledge through RAG
  5. Evaluate the impact on foundational capabilities using MMLU

- Design tradeoffs:
  - Balancing the effectiveness of knowledge manipulation with the preservation of the agent's core functionalities
  - Choosing between computational efficiency and the thoroughness of the attack methods (e.g., LoRA for efficiency vs. full fine-tuning)
  - Deciding the extent of knowledge editing without causing noticeable degradation in agent performance

- Failure signatures:
  - Injected agent's responses deviate significantly from expected behavior, indicating over-editing
  - Benign agents quickly identify and reject manipulated knowledge, suggesting robust fact-checking mechanisms
  - RAG system fails to retrieve relevant information, possibly due to improper indexing or data corruption

- First 3 experiments:
  1. Test the persuasiveness of fabricated evidence by comparing agent responses with and without supporting evidence
  2. Evaluate the spread of manipulated knowledge in a controlled environment with a small number of agents and dialogue turns
  3. Assess the persistence of manipulated knowledge through a simplified RAG system using a limited set of chat histories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed attack method scale when applied to larger language models with billions of parameters?
- Basis in paper: [inferred] The paper uses models with 7B and 8B parameters but doesn't explore larger models
- Why unresolved: The paper doesn't provide empirical data on how the attack's effectiveness changes with model size
- What evidence would resolve it: Testing the attack on models like GPT-4 or LLaMA 3 70B and comparing results

### Open Question 2
- Question: Can the two-stage attack method be detected by existing security measures or anomaly detection systems?
- Basis in paper: [inferred] The paper doesn't mention any defensive techniques or detection methods
- Why unresolved: The paper focuses on the attack's effectiveness but doesn't explore potential countermeasures
- What evidence would resolve it: Implementing and testing various anomaly detection techniques against the attack

### Open Question 3
- Question: How does the persistence of manipulated knowledge through RAG systems compare to other knowledge injection methods?
- Basis in paper: [explicit] The paper mentions RAG systems but doesn't compare persistence with other methods
- Why unresolved: The paper only explores RAG-based persistence, not other potential methods of maintaining manipulated knowledge
- What evidence would resolve it: Comparing RAG persistence with methods like continuous fine-tuning or memory-based approaches

## Limitations
- Experiments conducted in controlled simulation environment with fixed agent counts and interaction patterns
- Attack relies on access to model weights and training capabilities, limiting applicability against closed-source LLMs
- Study focuses on specific knowledge types (counterfactual and toxic) and may not generalize to other forms of manipulated knowledge

## Confidence
- **High confidence**: The core finding that manipulated knowledge can spread through multi-agent systems via evidence-supported false claims is well-supported by experimental results
- **Medium confidence**: The persistence of manipulated knowledge through RAG systems is demonstrated, but real-world implementation details could affect this behavior
- **Medium confidence**: The proposed defense mechanisms (guardian agents, fact-checking tools) are conceptually sound but require further validation in complex scenarios

## Next Checks
1. Test the attack method's effectiveness in open-ended, dynamic multi-agent environments with varying agent counts and interaction patterns
2. Evaluate the persistence of manipulated knowledge in RAG systems using larger, more diverse datasets and longer time horizons
3. Implement and test the proposed defense mechanisms in realistic multi-agent scenarios to assess their practical effectiveness