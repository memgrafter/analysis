---
ver: rpa2
title: 'Persona-SQ: A Personalized Suggested Question Generation Framework For Real-world
  Documents'
arxiv_id: '2412.12445'
source_url: https://arxiv.org/abs/2412.12445
tags:
- persona
- document
- question
- questions
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating suggested questions
  (SQs) in AI-powered reading applications that are currently homogeneous and ineffective
  due to ignoring user information. The core method introduces Persona-SQ, a pipeline
  that generates personalized SQs by incorporating synthetic reader profiles (professions
  and reading goals) into the SQ generation process.
---

# Persona-SQ: A Personalized Suggested Question Generation Framework For Real-world Documents

## Quick Facts
- arXiv ID: 2412.12445
- Source URL: https://arxiv.org/abs/2412.12445
- Reference count: 40
- Primary result: Persona-SQ improves semantic diversity of SQs (cosine similarity 84.4 vs 95.8 for legal documents) and persona alignment coverage ratios (35.1% vs 9.6% top-1 legal personas)

## Executive Summary
This paper addresses the limitation of homogeneous suggested questions in AI-powered reading applications by introducing Persona-SQ, a framework that generates personalized suggested questions by incorporating user profile information. The framework creates synthetic reader profiles (professions and reading goals) and uses them to guide question generation, resulting in more diverse and aligned questions compared to generic approaches. The method demonstrates that incorporating persona information significantly improves both the semantic diversity of generated questions and their alignment with user reading objectives.

## Method Summary
Persona-SQ is a pipeline-based framework that generates personalized suggested questions through five key steps: document collection, persona generation, persona quality control, personalized question generation, and question quality control. The framework uses synthetic personas (profession and reading goals) created for each document, which are then incorporated into the SQ generation process using large language models like GPT4o. A key innovation is the ability to generate synthetic training data from this process, enabling fine-tuning of extremely small models (360M parameters) that achieve competitive performance against much larger models while enabling on-device deployment.

## Key Results
- Semantic diversity improvement: Legal document SQs show cosine similarity scores of 84.4 vs 95.8 compared to baseline
- Persona alignment: Top-1 coverage ratio improves from 9.6% to 35.1% for legal personas
- Model efficiency: 360M parameter models fine-tuned on synthetic data achieve competitive performance against GPT4o
- User preference: Human evaluation shows personalized SQs are ranked higher than generic SQs

## Why This Works (Mechanism)
Persona-SQ works by incorporating user-specific information (professions and reading goals) into the question generation process, which naturally leads to more diverse and relevant questions. The framework leverages the ability of large language models to understand and respond to persona information, using this to guide the generation of questions that are tailored to specific user contexts. The synthetic persona generation process creates a bridge between document content and potential user perspectives, enabling the model to produce questions that reflect different reading objectives and professional backgrounds.

## Foundational Learning
**Synthetic persona generation**: Creating artificial user profiles based on document content - needed to simulate diverse user perspectives without requiring actual user data; quick check: verify personas capture key aspects of document content and represent realistic professional goals.
**Quality control filtering**: Applying thresholds and relevance scoring to personas and generated questions - needed to ensure only high-quality, relevant content proceeds through the pipeline; quick check: measure precision-recall tradeoff at different threshold values.
**Cosine similarity for diversity**: Using vector similarity metrics to quantify semantic diversity of generated questions - needed to objectively measure improvement over baseline approaches; quick check: compare similarity distributions between personalized and generic SQs.

## Architecture Onboarding

**Component map**: Document Collection -> Persona Generation -> Quality Control -> Question Generation -> Quality Control -> Output

**Critical path**: The most critical path is from persona generation through quality control to question generation, as poor persona quality directly impacts question relevance and diversity. The quality control steps are essential bottlenecks that determine the final output quality.

**Design tradeoffs**: The framework trades off between computational efficiency (using small models) and generation quality (using large models for synthetic data generation). The choice to use synthetic personas enables privacy preservation but may not fully capture real user diversity.

**Failure signatures**: Poor persona generation leads to generic questions that fail to capture document-specific nuances. Overly strict quality control filters may result in too few questions being retained. Insufficient semantic diversity in generated questions indicates the persona incorporation is not effective.

**First experiments**: 1) Test persona generation quality on a small document sample by manually evaluating generated personas against document content. 2) Compare question diversity metrics (cosine similarity) between personalized and generic generation on a subset of documents. 3) Evaluate small model fine-tuning performance by measuring question quality against GPT4o baseline on held-out documents.

## Open Questions the Paper Calls Out

**Open Question 1**: How does the performance of Persona-SQ change when using actual user profile information instead of synthetic personas? The paper acknowledges that synthetic personas are a limitation and that real user profiles could replace them without architectural changes, but doesn't evaluate this scenario.

**Open Question 2**: What is the optimal model size for on-device SQ generation that balances performance and resource constraints? While the paper demonstrates competitive performance with 360M parameter models, it doesn't systematically explore the performance-resource tradeoff across a broader range of model sizes.

**Open Question 3**: How does Persona-SQ handle documents that don't clearly map to predefined professional domains? The paper mentions handling "Unknown" domains in synthetic data but doesn't detail how the system handles ambiguous or cross-domain documents.

## Limitations
- Reliance on synthetic personas rather than actual user profiles, which may not fully capture real-world user diversity
- Limited human evaluation sample size (60 participants) with unspecified demographic selection criteria
- Focus on extremely small models (360M parameters) without exploring intermediate model sizes for performance-resource tradeoffs

## Confidence

**High confidence**: Claims about improved semantic diversity and persona alignment coverage ratios compared to baseline models (cosine similarity scores, coverage percentages).

**Medium confidence**: Claims about small model performance when fine-tuned on synthetic data, due to limited comparison with other fine-tuning approaches.

**Medium confidence**: User preference study results, given the relatively small sample size and potential demographic bias in participant selection.

## Next Checks
1. Test Persona-SQ on real user personas from actual reading applications to validate synthetic persona effectiveness.
2. Expand human evaluation to include diverse demographic groups and larger sample sizes to assess generalizability.
3. Compare fine-tuned small model performance against other SQ generation approaches beyond the baseline GPT4o without personas.