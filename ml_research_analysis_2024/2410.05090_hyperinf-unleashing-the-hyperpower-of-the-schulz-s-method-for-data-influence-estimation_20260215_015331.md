---
ver: rpa2
title: 'HyperINF: Unleashing the HyperPower of the Schulz''s Method for Data Influence
  Estimation'
arxiv_id: '2410.05090'
source_url: https://arxiv.org/abs/2410.05090
tags:
- data
- matrix
- influence
- yper
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes HyperINF, a method for estimating data influence
  in large-scale models that leverages Schulz's iterative algorithm for matrix inversion.
  The key innovation is using the generalized Fisher information matrix (GFIM) as
  a low-rank approximation of the Hessian, which reduces memory and computation costs
  on LoRA-tuned models.
---

# HyperINF: Unleashing the HyperPower of the Schulz's Method for Data Influence Estimation

## Quick Facts
- **arXiv ID**: 2410.05090
- **Source URL**: https://arxiv.org/abs/2410.05090
- **Reference count**: 40
- **Primary result**: HyperINF achieves superior accuracy compared to baselines like LiSSA and DataInf through rigorous convergence guarantees while maintaining low computational overhead.

## Executive Summary
HyperINF is a novel method for estimating data influence in large-scale models that addresses the computational challenges of traditional influence function approaches. The method combines Schulz's iterative algorithm for matrix inversion with a generalized Fisher information matrix (GFIM) approximation to achieve both accuracy and efficiency. By leveraging the structure of LoRA-tuned models, HyperINF reduces memory and computation costs dramatically while maintaining superior performance on tasks like mislabeled data detection and data selection for model training.

## Method Summary
HyperINF estimates the influence of training samples on model predictions using a combination of GFIM approximation and Schulz's iterative matrix inversion algorithm. The method computes influence scores by iteratively approximating the inverse of the Hessian matrix using the hyperpower method, while using GFIM as a low-rank approximation to reduce computational complexity. This approach is particularly effective for LoRA-tuned models, where it achieves memory complexity of only 0.39% and computation of 6.25% compared to original Hessian-vector products. The algorithm maintains rigorous convergence guarantees while significantly improving upon the instability issues seen in previous methods like LiSSA.

## Key Results
- Outperforms baselines (LiSSA, DataInf) on mislabeled data detection with superior accuracy while using only 0.39% memory and 6.25% computation of traditional methods
- Demonstrates improved performance on LLM finetuning data selection and VLM pretraining data selection tasks
- Maintains rigorous convergence guarantees through Schulz's iterative algorithm while avoiding the instability issues of previous methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Schulz's iterative algorithm provides rigorous convergence guarantees for matrix inversion while avoiding the instability seen in LiSSA.
- Mechanism: The algorithm iteratively updates Xt = Xt-1(2I - AXt-1) to converge toward A-1, with convergence order p=2 and numerical stability.
- Core assumption: The matrix A is invertible and the initialization X0 is sufficiently close to A-1.
- Evidence anchors:
  - [abstract] "The family of hyperpower methods are well-known for their rigorous convergence guarantees on matrix inverse approximation"
  - [section] "Schulz's method [Petković, 1995]...When t → ∞ and X0 ≈ A-1, it is proved that the sequence {Xt} will converge towards A-1 in a numerically stable way"
  - [corpus] Weak evidence - corpus papers discuss orthogonality and optimization but don't directly support matrix inversion convergence
- Break condition: If A is singular or nearly singular, or if X0 is poorly initialized far from A-1, convergence may fail or be very slow.

### Mechanism 2
- Claim: Using GFIM as a low-rank approximation of the Hessian reduces memory and computation costs to constants independent of LoRA ranks.
- Mechanism: GFIM approximates H(θ) using first-order gradients through Bartlett's identity, then Kronecker product properties allow efficient matrix-vector products without full Hessian computation.
- Core assumption: Each column in LoRA blocks is independent and identically distributed, and the Hessian is approximately block-wise diagonal.
- Evidence anchors:
  - [section] "To deal with the computation-intensive matrix multiplication, we incorporate the generalized fisher information (GFIM) as a low-rank approximation of the Hessian matrix"
  - [section] "With LoRA rank r = 16, HYPER INF only requires 0.39% memory complexity and 6.25% computations comparing to original Hessian-vector product operations"
  - [corpus] No direct evidence in corpus - this is the novel contribution
- Break condition: If LoRA blocks are not independent or the block-wise diagonal approximation is poor, accuracy may degrade significantly.

### Mechanism 3
- Claim: The combination of GFIM approximation and Schulz iteration maintains high accuracy while achieving computational efficiency.
- Mechanism: GFIM provides accurate Hessian approximation, Schulz iteration provides stable inverse computation, and their combination preserves both benefits while reducing costs.
- Core assumption: The GFIM approximation error is small enough that Schulz iteration can still converge to an accurate inverse.
- Evidence anchors:
  - [abstract] "HyperINF achieves superior accuracy compared to baselines like LiSSA and DataInf through rigorous convergence guarantees while maintaining low computational overhead"
  - [section] "HYPER INF showcases outstanding memory and computation efficiencies" (Table 1)
  - [corpus] No direct evidence in corpus - this is the novel combination
- Break condition: If GFIM approximation error is too large, Schulz iteration may converge to an inaccurate inverse despite its stability properties.

## Foundational Learning

- Concept: Influence functions and their computational challenges
  - Why needed here: The paper builds on influence functions as the foundation, then addresses their computational intractability
  - Quick check question: What are the two main computational bottlenecks in computing influence functions for large models?

- Concept: Matrix inverse approximation methods (LiSSA, DataInf, hyperpower methods)
  - Why needed here: Understanding the limitations of existing methods motivates the proposed approach
  - Quick check question: What are the key limitations of LiSSA and DataInf that HyperINF aims to address?

- Concept: Low-rank matrix approximations and Kronecker products
  - Why needed here: The GFIM approximation and its efficient computation rely on these concepts
  - Quick check question: How does the Kronecker product property (Ir ⊗ A)-1 = Ir ⊗ A-1 enable efficient computation in HyperINF?

## Architecture Onboarding

- Component map: Training data → Gradient computation → GFIM construction → Schulz iteration → Influence score computation
- Key components: Gradient calculator, GFIM builder, Matrix inverter (Schulz), Influence scorer
- Dependencies: Requires differentiable model, LoRA-tuned models for optimal efficiency, validation gradients

- Critical path:
  1. Compute validation gradients (one-time cost)
  2. For each training sample: compute gradients, update GFIM, perform Schulz iteration, compute influence score
  3. Rank samples by influence scores

- Design tradeoffs:
  - Accuracy vs efficiency: GFIM approximation vs exact Hessian, Schulz iteration vs closed-form methods
  - Memory vs computation: Storing intermediate matrices vs recomputing
  - Initialization sensitivity: Schulz iteration requires good initialization for fast convergence

- Failure signatures:
  - Poor convergence in Schulz iteration (divergence or slow convergence)
  - Large approximation error in GFIM vs exact Hessian
  - Numerical instability in matrix operations
  - Performance degradation on non-LoRA models

- First 3 experiments:
  1. Synthetic matrix inversion test: Compare error convergence of Schulz vs LiSSA/DataInf across different matrix dimensions
  2. Mislabeled data detection: Apply to GLUE benchmark with controlled label corruption, compare detection rates
  3. Data selection efficiency: Measure computation time and accuracy on LLM finetuning task with varying data selection ratios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does HyperINF maintain its performance advantage over baselines when applied to smaller LoRA ranks (e.g., r < 16)?
- Basis in paper: [explicit] The paper notes that for r = 8, HyperINF with GFIM avoids OOM errors that occur with FIM, but does not systematically test smaller ranks.
- Why unresolved: The paper primarily focuses on r = 16 and higher, leaving uncertainty about performance at lower ranks.
- What evidence would resolve it: Comparative experiments testing HyperINF and baselines across a range of smaller LoRA ranks (e.g., r = 2, 4, 8) on the same tasks.

### Open Question 2
- Question: How does HyperINF perform when using gradients from different layers of the model, not just the last layer or LoRA parameters?
- Basis in paper: [inferred] The paper mentions using gradients from the last transformer block for dense fine-tuning and LoRA blocks for LoRA-tuned models, but does not explore other layer choices.
- Why unresolved: The choice of layer could significantly impact influence estimation, especially for tasks requiring different granularities of attribution.
- What evidence would resolve it: Systematic experiments testing HyperINF with gradients from various layers (e.g., middle layers, attention heads) across different tasks.

### Open Question 3
- Question: Can HyperINF be extended to non-LoRA parameter-efficient fine-tuning methods, such as prefix tuning or adapter-based methods?
- Basis in paper: [explicit] The paper focuses on LoRA-tuned models and uses GFIM to approximate the Hessian, but does not address other parameter-efficient methods.
- Why unresolved: Many modern fine-tuning approaches use adapters or other low-rank methods that may not fit the LoRA framework.
- What evidence would resolve it: Implementation and evaluation of HyperINF on non-LoRA parameter-efficient methods, demonstrating its adaptability or limitations.

## Limitations

- The GFIM approximation quality relies on the assumption that LoRA blocks are independent and identically distributed, which is asserted but not rigorously validated
- The theoretical convergence guarantees for Schulz's method assume well-conditioned matrices and good initialization, but the paper doesn't systematically analyze how these assumptions hold in practice
- Performance gains are demonstrated primarily on specific task categories (mislabeled detection, LLM finetuning, VLM pretraining), leaving uncertainty about generalization to other domains

## Confidence

- **High Confidence**: The computational efficiency improvements (memory reduction to 0.39% and computation to 6.25% of baseline) are well-supported by Table 1. The Schulz iteration algorithm itself is well-established in numerical analysis literature, and the mechanism for using GFIM to reduce computation is clearly articulated.
- **Medium Confidence**: The accuracy improvements over baselines are demonstrated empirically but rely on specific experimental conditions. The paper shows HyperINF outperforms baselines on selected tasks, but the relative performance gap and consistency across different scenarios warrant further investigation.
- **Low Confidence**: The theoretical claims about convergence guarantees under the GFIM approximation are not fully substantiated. The relationship between approximation error in GFIM and final influence score accuracy is not rigorously analyzed, making it difficult to predict performance on new tasks without empirical validation.

## Next Checks

1. **Robustness to LoRA Rank Variations**: Systematically test HyperINF across a broader range of LoRA ranks (e.g., r=4, 8, 32, 64) and different LoRA configurations to quantify the sensitivity of performance to the independence assumption. This would reveal whether the claimed computational benefits hold across practical deployment scenarios.

2. **Cross-Architecture Generalization**: Evaluate HyperINF on architectures beyond those tested (e.g., BERT variants, vision transformers, different LLM families) to assess whether the GFIM approximation remains effective when LoRA block structure differs significantly from the tested models.

3. **Theoretical Error Analysis**: Develop a formal analysis connecting GFIM approximation error to final influence score error, including bounds on how approximation errors propagate through the Schulz iteration. This would provide stronger theoretical guarantees for the method's accuracy claims.