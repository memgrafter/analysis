---
ver: rpa2
title: 'Athena: Safe Autonomous Agents with Verbal Contrastive Learning'
arxiv_id: '2408.11021'
source_url: https://arxiv.org/abs/2408.11021
tags:
- safety
- agent
- contrastive
- critic
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ATHENA, a framework for improving the safety\
  \ of autonomous agents powered by large language models (LLMs). The core idea is\
  \ to use verbal contrastive learning\u2014providing the agent with examples of both\
  \ safe and unsafe past trajectories as few-shot prompts\u2014to enhance its safety\
  \ reasoning."
---

# Athena: Safe Autonomous Agents with Verbal Contrastive Learning

## Quick Facts
- arXiv ID: 2408.11021
- Source URL: https://arxiv.org/abs/2408.11021
- Reference count: 3
- Key outcome: ATHENA framework improves safety rates of LLM agents from 0.58 to 0.68-0.86 using verbal contrastive learning and interaction-level critiquing

## Executive Summary
This paper introduces ATHENA, a framework for improving the safety of autonomous agents powered by large language models (LLMs). The core innovation is verbal contrastive learning—providing the agent with examples of both safe and unsafe past trajectories as few-shot prompts—to enhance its safety reasoning. Additionally, a Critic agent provides real-time feedback at each interaction step to guide safer decisions. The authors curate a safety benchmark of 80 toolkits across 8 categories with 180 scenarios for evaluating LLM-based agents. Experiments show that both verbal contrastive learning and interaction-level critiquing significantly improve safety rates across multiple LLMs.

## Method Summary
ATHENA employs an Actor-Critic architecture where the Actor (LLM agent) executes tasks with guidance from contrastive examples, while the Critic (advanced LLM) monitors safety at each step. The framework uses verbal contrastive learning by retrieving past safe and unsafe trajectories as few-shot prompts, and employs real-time critiquing where the Critic evaluates the Actor's thoughts and actions to provide safety feedback. The approach is evaluated on a curated safety benchmark with multiple closed- and open-source LLMs.

## Key Results
- Safety rates improve from 0.58 (zero-shot) to 0.68 using verbal contrastive learning alone
- Adding the Critic agent further improves safety rates to 0.86 for GPT-3.5-Turbo
- Both closed-source (GPT-3.5-Turbo, Gemini-1.5-Pro) and open-source (Mistral-7B-Instruct, Llama-3-70B) LLMs show significant safety improvements
- Human evaluation confirms substantial agreement on safety judgments, though helpfulness assessments show less consensus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Critic agent improves safety by providing real-time feedback on each intermediate step.
- Mechanism: The Critic evaluates the Actor's thoughts and actions at each step, identifies unsafe reasoning, and prompts the Actor to reconsider. If the Actor cannot correct itself, the Critic intercepts the trajectory.
- Core assumption: The Critic has sufficient reasoning capability to accurately assess safety risks in real-time.
- Evidence anchors:
  - [abstract] "Additionally, a Critic agent provides real-time feedback at each interaction step to guide safer decisions."
  - [section] "The Critic agent generates a thought and based on that thought, takes an action. Therefore, to make the Actor safer, it is critical to enhance its thought and actions with safety reasoning ability, at the planning stage."
  - [corpus] Weak - no direct corpus evidence supporting the Critic's real-time feedback mechanism.
- Break condition: The Critic fails to identify a safety risk or incorrectly flags a safe action as unsafe, leading to unnecessary task interruption or missed risks.

### Mechanism 2
- Claim: Verbal contrastive learning enhances the Actor's safety reasoning through few-shot examples.
- Mechanism: The Actor receives pairs of similar safe and unsafe past trajectories as in-context examples, allowing it to learn from historical experiences and generalize to new scenarios.
- Core assumption: The retrieved contrastive pairs are relevant and sufficiently similar to the current query to provide meaningful guidance.
- Evidence anchors:
  - [abstract] "The core idea is to use verbal contrastive learning—providing the agent with examples of both safe and unsafe past trajectories as few-shot prompts—to enhance its safety reasoning."
  - [section] "We define the verbal contrastive learning concept where the past safe and unsafe trajectories are used as few-shot examples to enhance the Actor's reasoning."
  - [corpus] Weak - no direct corpus evidence supporting the effectiveness of verbal contrastive learning.
- Break condition: The retrieved examples are irrelevant or dissimilar to the current query, providing misleading guidance that harms safety reasoning.

### Mechanism 3
- Claim: The combination of Critic agent and verbal contrastive learning provides complementary safety improvements.
- Mechanism: The Critic addresses immediate safety concerns during execution, while contrastive learning provides proactive guidance based on historical patterns. Together, they reduce both immediate risks and prevent future unsafe behaviors.
- Core assumption: The two mechanisms address different aspects of safety reasoning and their effects are additive rather than redundant.
- Evidence anchors:
  - [abstract] "Experiments with both closed- and open-source LLMs... show that verbal contrastive learning and interaction-level critiquing significantly improve safety rates."
  - [section] "Our experimental evaluation, with both closed- and open-source LLMs, indicates verbal contrastive learning and interaction-level critiquing improve the safety rate significantly."
  - [corpus] Weak - no direct corpus evidence supporting the complementary nature of the two mechanisms.
- Break condition: The mechanisms overlap in addressing the same safety concerns, leading to redundant interventions without additional safety benefits.

## Foundational Learning

- Concept: Few-shot prompting with contrastive examples
  - Why needed here: Standard few-shot prompting may not effectively teach safety reasoning without explicit comparison between safe and unsafe behaviors.
  - Quick check question: If you provide only safe examples to an LLM, will it necessarily learn what unsafe behavior looks like?

- Concept: Real-time safety monitoring and intervention
  - Why needed here: Safety risks can emerge at any step in an autonomous agent's trajectory, requiring immediate detection and correction.
  - Quick check question: Why is it insufficient to only evaluate safety at the end of a trajectory rather than at each interaction step?

- Concept: Vector database similarity search for retrieval
  - Why needed here: Efficiently finding relevant past trajectories requires encoding queries and examples into vector space and measuring similarity.
  - Quick check question: What similarity metric is used to retrieve relevant past trajectories for contrastive learning?

## Architecture Onboarding

- Component map:
  Actor -> Emulator -> Evaluator
  Critic provides feedback to Actor
  Vector DB stores past trajectories

- Critical path:
  1. User query → Actor with contrastive examples
  2. Actor generates thought → action → tool call
  3. Critic evaluates thought/action → feedback
  4. Actor revises if needed → Emulator executes
  5. Repeat until task completion or Critic interception
  6. Evaluator assesses final trajectory

- Design tradeoffs:
  - Using GPT-4-Turbo as Critic vs. using same LLM as Actor for consistency
  - Cost vs. performance: advanced LLM for Critic improves safety but increases API costs
  - Number of contrastive examples: more examples improve learning but increase prompt length and cost

- Failure signatures:
  - Safety rate plateaus despite adding more contrastive examples (retrieval quality issue)
  - Helpfulness rate drops significantly when adding Critic (overly conservative safety judgments)
  - Inconsistent safety judgments across similar scenarios (Critic reliability issue)

- First 3 experiments:
  1. Baseline: Actor with zero-shot prompting, measure safety and helpfulness rates
  2. Add Critic only: Compare safety improvement vs. helpfulness degradation
  3. Add verbal contrastive learning only: Compare safety improvement vs. two-shot random prompting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the safety rate scale with increasing the number of contrastive pairs (k) beyond 1?
- Basis in paper: [explicit] The authors mention using k=1 for contrastive pairs due to context length and cost constraints, but do not explore higher values of k.
- Why unresolved: The paper only experiments with k=1 and does not provide insights into the impact of using more contrastive pairs on safety rates.
- What evidence would resolve it: Conducting experiments with varying values of k (e.g., k=2, 3, 5) and comparing the resulting safety rates would provide evidence on the scalability of the approach.

### Open Question 2
- Question: How does the Critic agent's performance compare to other methods of incorporating safety reasoning, such as Chain-of-Thought prompting or Reflexion?
- Basis in paper: [inferred] The paper introduces the Critic agent as a novel approach to enhance safety reasoning but does not compare its performance to other existing methods.
- Why unresolved: The paper focuses on the effectiveness of the Critic agent and verbal contrastive learning but does not provide a comprehensive comparison with alternative approaches.
- What evidence would resolve it: Conducting experiments that directly compare the Critic agent's performance to other methods like Chain-of-Thought prompting or Reflexion in terms of safety rates would provide insights into its relative effectiveness.

### Open Question 3
- Question: How does the ATHENA framework perform in real-world scenarios with dynamic and complex environments, as opposed to the emulated scenarios used in the experiments?
- Basis in paper: [explicit] The paper mentions that the ATHENA framework is built upon the ToolEmu components, which are used to simulate real-world scenarios. However, the experiments are conducted within the emulated environment.
- Why unresolved: The paper does not provide evidence on the framework's performance in actual real-world settings, which may have different characteristics and challenges compared to the emulated scenarios.
- What evidence would resolve it: Deploying the ATHENA framework in real-world applications and evaluating its safety and helpfulness rates in dynamic and complex environments would provide insights into its practical effectiveness.

## Limitations
- Reliance on proprietary models (GPT-4-Turbo as Critic) may limit reproducibility and scalability
- Safety benchmark creation process is underspecified, making independent validation difficult
- Study doesn't address potential bias in the Critic's judgments or explore long-term effects of over-conservative safety interventions

## Confidence
- High confidence in safety rate improvements with quantitative results across multiple models
- Medium confidence in helpfulness measurements due to low inter-annotator agreement reported
- Medium confidence in the complementary mechanism claims, as the evidence is primarily empirical rather than theoretical

## Next Checks
1. Replicate the study using open-source models for the Critic role to assess performance degradation and identify the minimum model capability required for effective safety monitoring
2. Conduct ablation studies to determine the optimal number of contrastive examples and measure the impact on both safety rates and computational costs
3. Perform cross-validation of the safety benchmark by having independent annotators evaluate a subset of trajectories to verify the reliability of safety judgments