---
ver: rpa2
title: What Do Language Models Learn in Context? The Structured Task Hypothesis
arxiv_id: '2406.04216'
source_url: https://arxiv.org/abs/2406.04216
tags:
- task
- learning
- hypothesis
- tasks
- learn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models (LLMs) perform
  in-context learning (ICL) on text classification tasks. The authors propose and
  test three hypotheses: 1) Task Selection - ICL identifies tasks from demonstrations,
  2) Meta-Learning - ICL learns new tasks on-the-fly using learned algorithms, and
  3) Structured Task Selection - ICL composes pre-learned tasks to perform novel tasks.'
---

# What Do Language Models Learn in Context? The Structured Task Hypothesis

## Quick Facts
- arXiv ID: 2406.04216
- Source URL: https://arxiv.org/abs/2406.04216
- Authors: Jiaoda Li; Yifan Hou; Mrinmaya Sachan; Ryan Cotterell
- Reference count: 17
- Key outcome: LLMs perform in-context learning by composing pre-learned tasks rather than learning new tasks from scratch or selecting from pre-learned sets.

## Executive Summary
This paper investigates how large language models perform in-context learning (ICL) on text classification tasks. The authors propose and test three hypotheses: Task Selection, Meta-Learning, and Structured Task Selection. Through experiments on LLaMA2 models (7B, 13B, 70B) across three datasets (CR, SST-2, AG News), they reject the first two hypotheses and find support for the third. The results suggest that ICL works by composing pre-learned tasks rather than learning new ones from scratch or simply selecting from a pre-learned set.

## Method Summary
The paper tests three hypotheses about ICL by conducting experiments on LLaMA2 models using text classification tasks. The method involves creating demonstration data with varying lengths, applying string-to-string transformations to create altered tasks (RA-ICL and PA-ICL), and comparing ICL performance against baselines including chance performance and linear regression models. The authors use F1-Macro as the primary evaluation metric and test across multiple model sizes and datasets.

## Key Results
- LLMs can learn tasks with altered responses (RA-ICL) but not with altered prompts (PA-ICL)
- ICL performance on composed tasks correlates with performance on individual tasks
- Natural transformations (synonyms, keywords) are significantly easier to learn than random transformations
- Linear regression baselines outperform ICL on some single-token tasks, challenging the Meta-Learning hypothesis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs learn RA tasks via composing pre-learned tasks rather than meta-learning or task selection.
- Mechanism: When a demonstration contains altered responses (g(r)), the model composes a learned transformation task τg with the base task τ, forming τg ∘ τ. This composition yields the RA task, which the model can learn in context.
- Core assumption: The model's pre-training exposed it to enough compositional structure and task transformations that it can implicitly compose them during inference.
- Evidence anchors:
  - [abstract] "The Structured Task Selection hypothesis is supported by showing that ICL performance on composed tasks correlates with performance on individual tasks"
  - [section 3] "Our results confirm that LLMs can learn such an RA task in context, which rejects Hypothesis 1."
  - [corpus] Weak - no explicit neighbor papers directly address task composition as the core ICL mechanism.
- Break condition: If altered prompts (τPA) are equally learnable as altered responses, or if performance on composed tasks does not correlate with individual task performance, the mechanism fails.

### Mechanism 2
- Claim: Altering responses (τRA) is learnable because the prompt-response mapping remains semantically aligned, whereas altering prompts (τPA) breaks this alignment.
- Mechanism: In τRA-ICL, prompts stay unchanged, so the model still sees the correct prompt structure and only needs to learn a new response mapping. In τPA-ICL, both prompts and responses are altered, making the task harder or unlearnable.
- Core assumption: LLMs rely heavily on prompt structure for task identification and response generation.
- Evidence anchors:
  - [section 4.1.2] "In stark contrast to the τPA-ICL setting, as shown in Fig. 5, the LLM has an average score above 80% in the τRA-ICL setting."
  - [abstract] "We find that LLMs can learn tasks with altered responses (τRA-ICL) but not with altered prompts (τPA-ICL)"
  - [corpus] Weak - no direct neighbor papers focus on prompt-response alignment as the key factor.
- Break condition: If τPA-ICL performance improves dramatically with more demonstrations or larger models, the mechanism is weakened.

### Mechanism 3
- Claim: Natural transformations (synonyms, keywords) are easier to learn because they are more likely to appear in pre-training data and thus are more readily composed.
- Mechanism: The model has pre-learned mappings like synonym substitution; these can be composed with base tasks to perform ICL. Random transformations lack this pre-training support.
- Evidence anchors:
  - [section 5.2.2] "We observe that the LLM can learn synonym and keyword in context almost perfectly."
  - [section 5.2.2] "The natural functions are indeed significantly easier to learn in context"
  - [corpus] Weak - no neighbor papers directly test natural vs random transformation learnability.
- Break condition: If performance on natural transformations does not significantly exceed random ones, or if higher-order synonyms become harder to learn, the mechanism is weakened.

## Foundational Learning

- Concept: Task composition via string-to-string functions
  - Why needed here: The paper models tasks as ⟨πτ, ρτ⟩ pairs and defines composition as applying one task's response distribution to another's prompts. Understanding this formalism is essential to follow the hypothesis tests.
  - Quick check question: If τ1 = ⟨ρ1, π1⟩ and τ2 = ⟨ρ2, π2⟩, what is τ1 ∘ τ2?
    - Answer: ⟨ρ1 ∘ ρ2, π2⟩, where ρ1 ∘ ρ2(r|p) = Σer ρ1(r|er)ρ2(er|p)

- Concept: Linear regression as a baseline for ICL learnability
  - Why needed here: The paper compares ICL performance on single-token tasks to logistic regression trained on the same demonstration data to test whether ICL is learning via a learned algorithm.
  - Quick check question: In the τg-Linear setting, what does the embedding layer E represent?
    - Answer: E is the 0th layer (embedding matrix) of the LLM, mapping tokens to D-dimensional vectors.

- Concept: F1-Macro as the evaluation metric
  - Why needed here: The paper uses F1-Macro to measure classification performance across classes, which is crucial for understanding results, especially in imbalanced or multi-class settings.
  - Quick check question: Why use F1-Macro instead of accuracy for these text classification tasks?
    - Answer: F1-Macro averages F1 scores across classes, treating each class equally, which is better when class distribution is imbalanced.

## Architecture Onboarding

- Component map:
  LLaMA2 model (7B, 13B, 70B) -> Demonstration generator -> Task composition module -> Evaluation harness -> Data loaders

- Critical path:
  1. Load dataset and create base task τ
  2. Generate demonstration of length L
  3. Apply transformation (g for RA, h for PA) to create novel task
  4. Run ICL with demonstration and test prompt
  5. Compute F1-Macro and compare against baselines

- Design tradeoffs:
  - Demonstration length L vs. model capacity: Longer demonstrations help but increase token limits
  - Transformation randomness vs. learnability: Random functions are harder; natural ones are easier
  - Single-token vs. multi-token tasks: Single-token tasks allow linear regression baselines but reduce realism

- Failure signatures:
  - τRA-ICL performance near chance -> Task composition not working or transformations too complex
  - τPA-ICL performance not improving with L -> Prompt structure too disrupted for ICL
  - τg-Linear outperforms τg-ICL -> ICL not learning via implicit algorithms

- First 3 experiments:
  1. τRA-ICL vs. Chance on CR with L=1 -> Verify RA tasks are unlearnable with minimal data
  2. τg-ICL with synonym transformation on SST-2 -> Test natural transformation learnability
  3. τg-Linear vs. τg-ICL on AG News -> Compare ICL to explicit linear regression baseline

## Open Questions the Paper Calls Out
- Can the structured task composition hypothesis explain ICL performance on more complex tasks beyond simple text classification?
- How does the structured task composition mechanism change as language model size increases?
- What is the theoretical relationship between pre-training data composition and the set of learnable tasks through structured composition?

## Limitations
- The study does not directly probe what transformations are present in pre-training data
- Experiments focus primarily on single-token classification tasks
- The failure of Meta-Learning hypothesis is based on negative results without ruling out larger models or more demonstrations

## Confidence
- High confidence: Rejection of Task Selection and Meta-Learning hypotheses
- Medium confidence: Structured Task Selection hypothesis based on correlation patterns
- Low confidence: Generalizability to multi-token tasks or other domains

## Next Checks
1. Test PA-ICL performance with extended demonstration lengths (L > 5) to determine if the failure is fundamental or resource-limited
2. Investigate pre-training data for evidence of natural transformations (synonyms, keyword substitutions) to validate the proposed compositional mechanism
3. Replicate experiments with multi-token classification tasks to assess generalizability beyond the single-token setting