---
ver: rpa2
title: Neural Approximate Mirror Maps for Constrained Diffusion Models
arxiv_id: '2406.12816'
source_url: https://arxiv.org/abs/2406.12816
tags:
- constraint
- mirror
- samples
- constr
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating arbitrary constraints
  into diffusion models for improved reliability in generating valid synthetic data
  and solving constrained inverse problems. Existing methods are limited to specific
  constraint types or do not scale well.
---

# Neural Approximate Mirror Maps for Constrained Diffusion Models

## Quick Facts
- arXiv ID: 2406.12816
- Source URL: https://arxiv.org/abs/2406.12816
- Reference count: 40
- Primary result: Neural approximate mirror maps (NAMMs) improve constraint satisfaction in diffusion models by 38% to 96% across various physics-based, geometric, and semantic constraints.

## Executive Summary
This paper addresses the challenge of incorporating arbitrary constraints into diffusion models for improved reliability in generating valid synthetic data and solving constrained inverse problems. Existing methods are limited to specific constraint types or do not scale well. The authors propose neural approximate mirror maps (NAMMs), a flexible framework that learns an approximate mirror map and its inverse to transform constrained data into an unconstrained space. This allows a regular diffusion model to be trained in the unconstrained space and then mapped back to the constraint set through the inverse map. The method only requires a differentiable distance function from the constraint set and works for general, possibly non-convex constraints. Experiments demonstrate that compared to an unconstrained diffusion model, a NAMM-based model substantially improves constraint satisfaction across various physics-based, geometric, and semantic constraints, with improvements ranging from 38% to 96%.

## Method Summary
The NAMM framework consists of two main components: a forward mirror map g_ϕ that transforms data from the constraint set into an unconstrained mirror space, and an inverse map f_ψ that maps samples from the mirror space back to the constraint set. The forward map g_ϕ is parameterized as the gradient of a strongly input-convex neural network (ICNN) to ensure invertibility. The inverse map f_ψ is trained with a cycle-consistency loss and constraint loss to ensure it maps noisy mirror samples back to the constraint set. After training NAMM, a diffusion model is trained in the mirror space, and samples are generated by mapping MDM samples through f_ψ. The approach only requires a differentiable distance function from the constraint set and can handle general, possibly non-convex constraints.

## Key Results
- NAMM-based diffusion models improve constraint satisfaction by 38% to 96% compared to unconstrained diffusion models across various constraints.
- The method works for a range of constraints including total brightness, 1D Burgers' equation, divergence-free flow, periodic functions, and count constraints.
- NAMMs can be easily integrated with existing diffusion-based inverse problem solvers by adapting them to the learned mirror space.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The learned approximate mirror map transforms the constrained data distribution into an unconstrained space where standard diffusion models can be trained without violating constraints.
- Mechanism: The forward map g_ϕ, parameterized as the gradient of an input-convex neural network (ICNN), pushes data from the constraint set M into a mirror space where the data become unconstrained. Since the ICNN is strongly input-convex, its gradient is invertible, ensuring that the forward map can be inverted.
- Core assumption: The constraint set M admits a transformation to an unconstrained space via a differentiable map whose gradient is invertible.
- Evidence anchors:
  - [abstract] "We propose neural approximate mirror maps (NAMMs), a flexible framework that learns an approximate mirror map and its inverse to transform constrained data into an unconstrained space."
  - [section] "We parameterize g_ϕ as the gradient of a strongly input-convex neural network (ICNN) (Amos et al., 2017) to satisfy invertibility."
  - [corpus] Weak match to "MirrorCBO: A consensus-based optimization method in the spirit of mirror descent" (topic mismatch, no constraint enforcement).
- Break condition: If the constraint set is too complex or the ICNN cannot capture the necessary transformation, the forward map may not adequately separate the constraint, leading to residual violations.

### Mechanism 2
- Claim: The approximate inverse map f_ψ, trained with a cycle-consistency loss and constraint loss, ensures that samples generated in the mirror space map back to the constraint set.
- Mechanism: f_ψ is trained to minimize both the cycle-consistency loss (ensuring f_ψ(g_ϕ(x)) ≈ x) and the constraint loss (ensuring f_ψ(˜x) is close to M for all ˜x in the noisy mirror distributions). This dual objective makes f_ψ robust to perturbations and noise in the mirror space.
- Core assumption: The noisy mirror distributions (g_ϕ)♯pdata ∗ N(0, σ²I) cover the region of the mirror space where the diffusion model will sample.
- Evidence anchors:
  - [abstract] "Our approach only requires a differentiable distance function from the constraint set. We learn an approximate mirror map that transforms data into an unconstrained space and a corresponding approximate inverse that maps data back to the constraint set."
  - [section] "We define the following constraint loss to encourage f_ψ to map points from the noisy mirror distributions... back to the constraint set."
  - [corpus] Weak match to "Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints" (topic match but different approach).
- Break condition: If the noise levels or the region covered by the noisy mirror distributions do not match the actual sampling distribution of the diffusion model, f_ψ may not generalize well.

### Mechanism 3
- Claim: Finetuning f_ψ with actual MDM samples reduces the distribution shift between the training data for f_ψ and the generated samples, improving constraint satisfaction.
- Mechanism: After training the MDM, finetuning f_ψ with samples from the MDM tailors the inverse map to the specific distribution of the generated mirror-space samples, rather than just the perturbed data distribution used during initial training.
- Core assumption: The distribution of MDM samples in the mirror space is sufficiently close to the noisy mirror distributions used during initial f_ψ training.
- Evidence anchors:
  - [abstract] "We also demonstrate how existing diffusion-based inverse-problem solvers can be easily applied in the learned mirror space to solve constrained inverse problems."
  - [section] "To reduce the distribution shift, it may be helpful to finetune f_ψ with MDM samples."
  - [corpus] No direct match; the corpus neighbors do not discuss finetuning inverse maps in diffusion contexts.
- Break condition: If the MDM sampling distribution is very different from the noisy mirror distributions, finetuning may not significantly improve performance or could overfit to the MDM samples.

## Foundational Learning

- Concept: Strong input-convexity and its implications for invertibility
  - Why needed here: The forward mirror map g_ϕ must be invertible to allow the inverse map f_ψ to map samples back to the constraint set. Strong input-convexity of the ICNN ensures that its gradient is invertible.
  - Quick check question: Why is the gradient of a strongly input-convex function invertible? (Answer: Strong input-convexity implies that the function is strictly convex, so its gradient is monotonic and thus invertible.)

- Concept: Cycle-consistency in image-to-image translation
  - Why needed here: The cycle-consistency loss ensures that g_ϕ and f_ψ are approximate inverses, which is critical for mapping between the constraint and mirror spaces without losing information.
  - Quick check question: What does a low cycle-consistency loss indicate about the relationship between g_ϕ and f_ψ? (Answer: It indicates that applying g_ϕ followed by f_ψ (or vice versa) approximately recovers the original input.)

- Concept: Maximum mean discrepancy (MMD) and Kernel Inception Distance (KID) as distribution similarity metrics
  - Why needed here: These metrics are used to evaluate whether the generated distribution (after applying f_ψ to MDM samples) matches the true data distribution, ensuring that enforcing constraints does not distort the data too much.
  - Quick check question: How does MMD differ from KID in measuring distribution similarity? (Answer: MMD uses kernel embeddings in a reproducing kernel Hilbert space, while KID uses features from a pre-trained neural network like Inception v3.)

## Architecture Onboarding

- Component map:
  Data -> g_ϕ (forward mirror map) -> Mirror space -> MDM (trained in mirror space) -> f_ψ (inverse mirror map) -> Constraint-satisfied data
  Additional components: Constraint distance function ℓ_constr, cycle-consistency loss, constraint loss, regularization term R(g_ϕ)

- Critical path:
  1. Train NAMM (g_ϕ and f_ψ) on constrained data using cycle-consistency and constraint losses.
  2. Train MDM in the mirror space induced by g_ϕ.
  3. Sample from MDM and map through f_ψ to obtain constraint-satisfied data.

- Design tradeoffs:
  - Using an ICNN for g_ϕ ensures invertibility but may limit the expressiveness of the transformation compared to a general CNN.
  - The noisy mirror distributions cover a range of perturbations to make f_ψ robust, but too much noise can make training harder.
  - Finetuning f_ψ with MDM samples can improve constraint satisfaction but adds extra training time and risk of overfitting.

- Failure signatures:
  - If constraint satisfaction does not improve over a baseline DM, possible causes include: g_ϕ not adequately transforming the constraint, f_ψ not properly trained to map back, or the noise levels in training f_ψ not matching MDM sampling.
  - If the generated distribution is very different from the true data (high MMD/KID), the mirror space transformation may be distorting the data too much.

- First 3 experiments:
  1. Train NAMM and MDM on a simple constraint (e.g., total brightness) and compare constraint distances of generated samples to a baseline DM.
  2. Perform an ablation study on the noise level σ_max in the noisy mirror distributions to find the optimal value for a given constraint.
  3. Compare the performance of g_ϕ parameterized as an ICNN versus a ResNet-based CNN on a non-convex constraint to verify the practical benefits of using an ICNN even when theoretical invertibility is not guaranteed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal noise level σmax in the sequence of noisy mirror distributions that balances robustness and constraint satisfaction?
- Basis in paper: [explicit] The paper discusses an ablation study showing that increasing σmax causes gϕ(x) for x ∼ pdata to have larger magnitudes to maintain SNR, but setting σmax too high can worsen constraint satisfaction due to the challenge of mapping a larger region of Rd back to the constraint set.
- Why unresolved: The paper demonstrates that there is a tradeoff between robustness and performance of fψ, but does not provide a definitive method for choosing σmax for new constraints.
- What evidence would resolve it: A theoretical framework or empirical method for determining σmax that optimizes the balance between robustness and constraint satisfaction for different types of constraints.

### Open Question 2
- Question: How does the choice of parameterization for gϕ (ICNN vs. ResNet) affect the performance of NAMMs for non-convex constraints?
- Basis in paper: [explicit] The paper shows that parameterizing gϕ as the gradient of an ICNN leads to better constraint satisfaction with fewer outliers compared to a ResNet-based CNN, even for non-convex constraints.
- Why unresolved: While the paper demonstrates better performance with ICNN, it does not explore why this is the case or provide a theoretical justification for using ICNN for non-convex constraints.
- What evidence would resolve it: A deeper analysis of the regularization effects of ICNN and how they contribute to better constraint satisfaction, potentially supported by theoretical proofs or extensive empirical studies.

### Open Question 3
- Question: Can NAMMs be extended to handle dynamic or time-varying constraints effectively?
- Basis in paper: [inferred] The paper demonstrates NAMMs on various static constraints but does not explore their application to dynamic or time-varying constraints, which are common in fields like fluid dynamics and video processing.
- Why unresolved: The current framework focuses on static constraints, and extending it to dynamic constraints would require addressing how to model and enforce constraints that change over time.
- What evidence would resolve it: Successful application of NAMMs to dynamic constraints, such as enforcing temporal consistency in video generation or modeling time-dependent physical systems, with quantitative improvements in constraint satisfaction over existing methods.

## Limitations
- The effectiveness of the learned mirror map and its inverse heavily depends on the complexity of the constraint set and the expressiveness of the neural network parameterizations.
- The method assumes that the noisy mirror distributions used to train the inverse map cover the region of the mirror space where the diffusion model will sample, which may not always hold true.
- Finetuning the inverse map with diffusion model samples can improve constraint satisfaction but adds computational overhead and risk of overfitting.

## Confidence
- High Confidence: The mechanism of using an ICNN for g_ϕ to ensure invertibility is theoretically sound and well-supported by the literature on input-convex networks. The cycle-consistency loss for training f_ψ is also a standard and effective approach in image-to-image translation.
- Medium Confidence: The effectiveness of the constraint loss in encouraging f_ψ to map noisy mirror samples back to the constraint set depends on the choice of σ_max and the coverage of the noisy mirror distributions. This may require careful tuning for different constraint types.
- Low Confidence: The assumption that finetuning f_ψ with MDM samples will consistently improve constraint satisfaction is based on intuition and limited experimental evidence. The extent of improvement may vary significantly depending on the specific constraint and data distribution.

## Next Checks
1. Conduct a systematic study on the effect of σ_max in the noisy mirror distributions on the performance of f_ψ and overall constraint satisfaction to determine the optimal noise level for different constraint types.
2. Test the NAMM framework on a new, complex constraint (e.g., a combination of brightness and spatial constraints) not seen during development to assess its ability to handle arbitrary constraints as claimed.
3. Compare the performance of g_ϕ parameterized as an ICNN versus a ResNet-based CNN on a non-convex constraint to empirically validate the practical benefits of using an ICNN, even when theoretical invertibility is not guaranteed.