---
ver: rpa2
title: Evaluating Robustness of Reward Models for Mathematical Reasoning
arxiv_id: '2410.01729'
source_url: https://arxiv.org/abs/2410.01729
tags:
- reward
- math
- solutions
- rewardbench
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the problem of reliably evaluating reward models
  for mathematical reasoning, which is crucial for improving reasoning capabilities
  through reinforcement learning from human feedback (RLHF). They identify limitations
  in existing benchmarks like RewardBench, which use one-to-one comparisons between
  human-annotated and machine-generated solutions, making them vulnerable to reward
  hacking.
---

# Evaluating Robustness of Reward Models for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2410.01729
- Source URL: https://arxiv.org/abs/2410.01729
- Reference count: 40
- Authors: Sunghwan Kim; Dongjin Kang; Taeyoon Kwon; Hyungjoo Chae; Jungsoo Won; Dongha Lee; Jinyoung Yeo
- Primary result: REWARD MATH benchmark shows strong correlation with optimized policy performance while detecting reward overoptimization, unlike existing benchmarks

## Executive Summary
This paper addresses the critical problem of reliably evaluating reward models for mathematical reasoning tasks, which are essential for improving reasoning capabilities through reinforcement learning from human feedback (RLHF). The authors identify fundamental limitations in existing benchmarks like RewardBench, which use one-to-one comparisons between human-annotated and machine-generated solutions, making them vulnerable to reward hacking and poor at detecting reward overoptimization. To solve this, they introduce REWARD MATH, a new benchmark that employs one-to-many comparisons between a single correct solution and multiple incorrect solutions generated by various language models. Through extensive experiments, they demonstrate that REWARD MATH strongly correlates with optimized policy performance and effectively estimates reward overoptimization, providing a more reliable evaluation framework for reward model robustness in mathematical reasoning.

## Method Summary
The authors propose REWARD MATH, a benchmark that evaluates reward model robustness through one-to-many comparisons between correct and incorrect solutions. The method constructs datasets containing one correct solution and nine incorrect solutions per problem, addressing step count differences by converting human solutions into step-by-step machine-generated solutions. They evaluate various reward model types (generative, classifier-based, process) on both REWARD MATH and existing benchmarks, then correlate results with policy optimization outcomes using best-of-n sampling. The benchmark uses accuracy and mean reciprocal rank (MRR) metrics, with process reward models aggregating step-level rewards using geometric mean to mitigate step count bias.

## Key Results
- REWARD MATH scores strongly correlate with optimized policy performance (r² > 0.8), while existing benchmarks show little to no correlation
- REWARD MATH effectively detects reward overoptimization, identifying when reward models assign higher scores to overoptimized solutions than to correct ones
- One-to-many comparison design provides more reliable evaluation than one-to-one comparisons by reducing noise and improving discrimination across diverse incorrect solutions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: One-to-many comparisons between correct and incorrect solutions improve reliability over one-to-one comparisons.
- **Mechanism**: By comparing one correct solution to multiple incorrect solutions, the evaluation reduces the impact of isolated noise and better captures the reward model's ability to discriminate across a diverse set of wrong answers.
- **Core assumption**: Reward models trained on one-to-one data cannot generalize to one-to-many discrimination without fine-tuning.
- **Evidence anchors**:
  - [abstract]: "we introduce REWARD MATH, a benchmark that effectively represents the robustness of reward models in mathematical reasoning tasks... employs comparisons with a variety of incorrect (i.e. rejected) solutions."
  - [section 3.2]: Describes construction of one correct and nine incorrect solutions per problem.
  - [corpus]: Related work (RewardBench) uses one-to-one comparisons; no corpus mention of one-to-many comparison effectiveness.
- **Break condition**: If incorrect solutions are too similar, the one-to-many comparison collapses into a noisy one-to-one comparison.

### Mechanism 2
- **Claim**: Reducing step count differences between human-annotated and machine-generated solutions mitigates reward hacking.
- **Mechanism**: Converting human solutions into step-by-step machine-generated solutions aligns representation distributions, preventing reward models from exploiting superficial format differences.
- **Core assumption**: Reward models are sensitive to step count differences between human and machine solutions.
- **Evidence anchors**:
  - [section 3.2]: "We first convert the human-annotated solutions from MATH500 into step-by-step machine-generated solutions... manually inspect the quality."
  - [section 3.1]: "humans often skip certain steps... which results in a significant difference compared to machine-generated solutions."
  - [corpus]: No explicit corpus evidence; assumption based on internal analysis.
- **Break condition**: If conversion introduces new systematic biases or errors not present in original human solutions.

### Mechanism 3
- **Claim**: Strong correlation between benchmark performance and optimized policy performance validates the benchmark's reliability.
- **Mechanism**: If a reward model scores well on REWARD MATH, then policies optimized using that model should achieve higher accuracy on math problems, indicating the benchmark captures true reward signals.
- **Core assumption**: The benchmark can predict downstream policy performance without requiring full policy optimization.
- **Evidence anchors**:
  - [abstract]: "We demonstrate that the scores on REWARD MATH strongly correlate with the results of optimized policy..."
  - [section 4.2]: Reports high correlation (r² > 0.8) between REWARD MATH scores and BoN sampling results.
  - [corpus]: Weak evidence; corpus neighbors focus on reward model evaluation but do not report policy correlation.
- **Break condition**: If correlation breaks for out-of-distribution problems or different optimization algorithms.

## Foundational Learning

- **Concept: Bradley-Terry model for preference estimation**
  - Why needed here: Reward models are trained to assign higher scores to chosen over rejected completions using the Bradley-Terry model.
  - Quick check question: How does the Bradley-Terry model transform pairwise preferences into reward scores?

- **Concept: KL divergence as a measure of optimization**
  - Why needed here: Used to quantify the degree of optimization in best-of-n sampling and PPO, essential for analyzing reward overoptimization.
  - Quick check question: What is the analytical formula for KL divergence in best-of-n sampling?

- **Concept: Geometric mean aggregation for process reward models**
  - Why needed here: Addresses step count bias in process reward models when converting step-level rewards to solution-level rewards.
  - Quick check question: Why does the product aggregation introduce a step count bias, and how does geometric mean mitigate it?

## Architecture Onboarding

- **Component map**: Data generation → Reward model inference → Benchmark scoring → Policy optimization → Evaluation
- **Critical path**: REWARD MATH construction (correct + incorrect solutions) → Reward model evaluation → Correlation analysis with BoN → Overoptimization assessment
- **Design tradeoffs**: 
  - One-to-many vs. many-to-many comparisons (resource constraints)
  - Manual vs. automatic conversion of human solutions (quality vs. scalability)
  - Geometric mean vs. product aggregation (bias mitigation vs. common practice)
- **Failure signatures**:
  - Low correlation between benchmark and policy performance
  - Reward overoptimization despite high benchmark scores
  - High variance in benchmark scores due to insufficient incorrect solution diversity
- **First 3 experiments**:
  1. Verify step count distribution alignment between correct and incorrect solutions using t-SNE or similar visualization.
  2. Test correlation between REWARD MATH scores and BoN sampling results on a small subset of problems.
  3. Compare geometric mean vs. product aggregation on a held-out set to quantify step count bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of incorrect solutions (n) to use in one-to-many comparisons for reward model evaluation?
- Basis in paper: [inferred] The paper discusses using 9 rejected solutions in REWARD MATH and mentions that "as the number of solutions increases, both the inference cost and the reliability of the results rise." However, they do not explore what the optimal trade-off point is.
- Why unresolved: The authors explicitly state "finding the optimal value of n is beyond the scope of this work" and only focus on validating their design with n=9.
- What evidence would resolve it: Experiments varying n from 2 to 20 and measuring the correlation between evaluation scores and policy performance, while also considering computational costs, would help identify the optimal value.

### Open Question 2
- Question: Can the reliable benchmark design be effectively extended to domains beyond mathematical reasoning where human preferences are not as clearly defined?
- Basis in paper: [explicit] The authors state "Since the reward models can be applied to a wide range of tasks, a crucial next step is to extend our design to cover all of them" and discuss limitations in the discussion section.
- Why unresolved: The paper focuses specifically on mathematical reasoning tasks where "human preferences can be clearly defined by correctness." The challenge lies in adapting the approach to domains with more subjective preferences.
- What evidence would resolve it: Applying the REWARD MATH design to other domains like text summarization or code generation, where preferences are less binary, and demonstrating strong correlations between evaluation scores and policy performance would validate the extension.

### Open Question 3
- Question: How would a many-to-many comparison benchmark (using multiple correct and incorrect solutions) compare to the current one-to-many design in terms of reliability and correlation with policy performance?
- Basis in paper: [inferred] The authors mention "if there were no limitations in available resources, many-to-many comparisons utilizing as many solutions as possible would most accurately reflect the robustness of reward models" and discuss resource constraints as a limitation.
- Why unresolved: The paper uses one-to-many comparisons due to resource limitations and does not explore the potential benefits of many-to-many comparisons despite acknowledging it would be ideal.
- What evidence would resolve it: Constructing a benchmark with multiple correct solutions per problem and comparing its correlation with policy performance against REWARD MATH's one-to-many design would reveal whether the additional complexity improves reliability.

## Limitations
- Domain-specific bias: Incorrect solutions are generated by language models that may share systematic failure modes, potentially limiting generalization
- Manual inspection subjectivity: The quality assurance process for solution conversion introduces human subjectivity that could affect benchmark reliability
- Resource constraints: One-to-many comparisons are used instead of ideal many-to-many comparisons due to computational limitations

## Confidence

| Claim | Confidence |
|-------|------------|
| Strong correlation between REWARD MATH scores and policy performance | High |
| REWARD MATH effectively detects reward overoptimization | Medium-High |
| One-to-many comparison design improves robustness over one-to-one | Medium |

## Next Checks

1. **Cross-domain validation**: Evaluate REWARD MATH's correlation with policy performance on a distinct mathematical reasoning dataset (e.g., GSM8K or MATH from different years) to test generalization beyond the training distribution.

2. **Solution diversity analysis**: Quantify the semantic and structural diversity of the incorrect solutions using clustering or embedding-based metrics to ensure the one-to-many comparison framework provides meaningful discrimination power.

3. **Ablation study on aggregation methods**: Systematically compare geometric mean, product, and alternative aggregation functions (e.g., median, max) for process reward models to isolate the impact of step count bias mitigation on benchmark reliability.