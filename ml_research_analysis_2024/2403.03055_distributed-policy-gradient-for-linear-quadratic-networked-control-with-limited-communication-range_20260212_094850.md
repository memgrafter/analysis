---
ver: rpa2
title: Distributed Policy Gradient for Linear Quadratic Networked Control with Limited
  Communication Range
arxiv_id: '2403.03055'
source_url: https://arxiv.org/abs/2403.03055
tags:
- gradient
- control
- policy
- distributed
- descent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a distributed policy gradient method for multi-agent
  linear quadratic control under local communication constraints. Each agent can only
  exchange information with its neighbors within a limited communication range, and
  control actions depend on observations from a limited control range.
---

# Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range
## Quick Facts
- arXiv ID: 2403.03055
- Source URL: https://arxiv.org/abs/2403.03055
- Reference count: 40
- Primary result: Distributed policy gradient method converges to near-optimal solution with exponentially small performance gap as communication/control ranges increase

## Executive Summary
This paper develops a distributed policy gradient method for multi-agent linear quadratic control where agents can only communicate with neighbors within a limited range. The key innovation is approximating the global policy gradient using only local information by leveraging an exponential decay property of the Q-function. The method guarantees stability during gradient descent and proves that the performance gap compared to centralized optimal control decreases exponentially with communication and control ranges.

## Method Summary
The paper proposes a distributed policy gradient descent algorithm where each agent computes a localized Q-function using information from κ-hop neighbors and updates its local controller based on this approximate gradient. The method uses a step size selection that ensures stability during the descent process, and proves that the performance gap to the centralized optimal controller is exponentially small in both the communication range κ and control range r.

## Key Results
- Distributed policy gradient converges to near-optimal solution with exponentially small performance gap
- Stability is guaranteed during gradient descent with appropriate step size selection
- Exponential decay property enables accurate local gradient approximation using only neighborhood information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exponential decay property enables accurate local gradient approximation
- Mechanism: Mutual influence between agents diminishes exponentially with distance, allowing truncation of Q-function beyond κ-hop neighborhood with bounded error
- Core assumption: System matrices A, B, Q, R have sufficient sparsity and system interactions are weak enough that walk counts grow sub-exponentially
- Evidence anchors:
  - [abstract]: "We show that it is possible to approximate the exact gradient only using local information"
  - [section]: "Such a property is called Exponential Decay Property and has been studied in the tabular RL case"
  - [corpus]: Weak - corpus neighbors don't explicitly mention exponential decay property
- Break condition: If system connectivity is too high or interaction strengths are too large, the exponential decay property fails and local approximation becomes inaccurate

### Mechanism 2
- Claim: Step size selection ensures stability during distributed gradient descent
- Mechanism: Carefully chosen step size bounds the deviation between centralized and distributed gradient updates, preventing controller destabilization
- Core assumption: The cost function C(K) is L-smooth in the projection space Mr and the sub-level set SC(K) is compact
- Evidence anchors:
  - [abstract]: "the stability of the system is also guaranteed during the gradient descent process"
  - [section]: "if the communication range κ is large enough and step size η is chosen appropriately"
  - [corpus]: Weak - corpus neighbors don't discuss stability guarantees during gradient descent
- Break condition: If step size exceeds the bound in equation (6), the distributed update may destabilize the system

### Mechanism 3
- Claim: Performance gap decreases exponentially with communication and control ranges
- Mechanism: Both the gradient approximation error and controller truncation error decrease exponentially with κ and r respectively, leading to near-optimal performance
- Core assumption: The optimal controller has spatially exponential decaying (SED) structure and the gradient has similar decay properties
- Evidence anchors:
  - [abstract]: "The performance gap compared to the centralized optimal controller decreases exponentially as the communication and control ranges increase"
  - [section]: "The optimality gap compared with the centralized optimal controller is proven to be exponentially small in κ and r"
  - [corpus]: Weak - corpus neighbors don't explicitly discuss exponential decay of performance gap
- Break condition: If κ or r is too small relative to system connectivity, the exponential decay breaks down and performance gap becomes large

## Foundational Learning

- Concept: Linear Quadratic Regulator (LQR) optimal control
  - Why needed here: The paper builds distributed policy gradient methods specifically for LQR problems
  - Quick check question: What is the form of the optimal controller in LQR and how is it related to the solution of the algebraic Riccati equation?

- Concept: Spatially Exponential Decaying (SED) matrix structure
  - Why needed here: The optimal controller and gradient are shown to have SED properties, enabling local approximations
  - Quick check question: How does the SED property of a matrix relate to the underlying network structure?

- Concept: Markov Decision Process (MDP) policy gradient theorem
  - Why needed here: The paper uses policy gradient methods and extends the theorem to the distributed LQR setting
  - Quick check question: How does the policy gradient theorem relate the gradient of the cost to the Q-function and policy derivatives?

## Architecture Onboarding

- Component map: Agents → Local controllers (Ki) → Global controller (K) → System dynamics (A, B) → Cost evaluation (Q, R) → Distributed policy gradient descent with communication constraints
- Critical path: Agent i computes local Q-function → approximates local gradient using κ-hop information → updates local controller → ensures stability via step size → contributes to system-wide near-optimal control
- Design tradeoffs: Larger κ improves accuracy but increases communication overhead; larger r improves performance but reduces distributed nature; smaller step size improves stability but slows convergence
- Failure signatures: System instability (ρ(A-BK) ≥ 1), performance degradation (C(K) not decreasing), communication bottleneck (excessive κ)
- First 3 experiments:
  1. Verify exponential decay property holds for simple line graph LQR system with known A, B matrices
  2. Implement distributed policy gradient on 2-node system with κ=1, r=1 and verify stability and convergence
  3. Scale to larger ring topology with varying κ and r, measure performance gap vs centralized solution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the conditions for Exponential Decay Property in networked LQR be relaxed beyond Lemma 4's requirements?
- Basis in paper: [explicit] The paper notes that Lemma 4's conditions are stricter than Lemma 3's and leaves finding a more relaxed condition as future work.
- Why unresolved: The proof of Lemma 4 introduces an additional stability condition [A-BK] · D ≤ 1 to construct a convergent geometric sequence, which is not necessary if the system is stable.
- What evidence would resolve it: A formal proof showing the Exponential Decay Property holds under weaker conditions than Lemma 4, potentially by modifying the proof technique.

### Open Question 2
- Question: How does the diameter of the graph affect the theoretical performance gap in Theorem 1?
- Basis in paper: [inferred] The paper discusses that graph diameter is not included in the theoretical performance gap, though it would be eliminated if r and κ were set to the diameter. It suggests examining the graph's specific structure and boundary conditions.
- Why unresolved: The current theoretical framework assumes each node operates within an infinitely expansive network, making the graph's diameter irrelevant to the final deductions.
- What evidence would resolve it: A modified theoretical analysis incorporating the graph's diameter into the performance gap bound, potentially through a more detailed examination of the graph's structure and boundary conditions.

### Open Question 3
- Question: Can the Distributed Policy Gradient algorithm be extended to model-free settings using Monte-Carlo or Actor-Critic methods?
- Basis in paper: [explicit] The paper mentions this as future work, noting that the current algorithm uses a simple Monte-Carlo method and discussing the potential integration of alternative approaches.
- Why unresolved: The current algorithm assumes access to the exact gradients, which may not be feasible in practice. Extending to model-free settings requires addressing the challenges of gradient approximation and sample complexity.
- What evidence would resolve it: A formal analysis of the Distributed Policy Gradient algorithm using Monte-Carlo or Actor-Critic methods, including convergence guarantees and performance bounds in the model-free setting.

## Limitations
- The exponential decay property requires restrictive conditions on system matrices that may not hold in practice
- The step size selection formula may be conservative and slow convergence in practice
- Performance gap bound depends heavily on system-specific parameters that are difficult to verify

## Confidence
- **High Confidence**: The convergence proof structure and stability guarantees under appropriate step size selection are mathematically rigorous
- **Medium Confidence**: The exponential decay property application to the LQR setting, while theoretically sound, requires empirical verification on specific network topologies
- **Medium Confidence**: The performance gap bound depends heavily on system-specific parameters that may not hold in practice

## Next Checks
1. Implement the algorithm on a 10-20 node line graph LQR system and measure the actual performance gap vs centralized solution across varying κ and r values
2. Test system stability margins by deliberately choosing step sizes that violate the theoretical bounds to identify the practical stability region
3. Validate the exponential decay property empirically by computing exact vs truncated Q-functions on small systems with known analytical solutions