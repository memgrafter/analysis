---
ver: rpa2
title: 'Enhancing Large Language Models with Domain-specific Retrieval Augment Generation:
  A Case Study on Long-form Consumer Health Question Answering in Ophthalmology'
arxiv_id: '2409.13902'
source_url: https://arxiv.org/abs/2409.13902
tags:
- references
- evidence
- llms
- documents
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the effectiveness of Retrieval Augmented Generation
  (RAG) in improving the factuality of references and evidence attribution in Large
  Language Models (LLMs) for long-form consumer health question answering in ophthalmology.
  A RAG pipeline was developed with ~70,000 domain-specific documents, and LLMs with
  and without RAG were compared on 100 consumer health questions using 10 healthcare
  professionals.
---

# Enhancing Large Language Models with Domain-specific Retrieval Augment Generation: A Case Study on Long-form Consumer Health Question Answering in Ophthalmology

## Quick Facts
- arXiv ID: 2409.13902
- Source URL: https://arxiv.org/abs/2409.13902
- Reference count: 37
- Key outcome: RAG improved reference factuality from 20.6% correct to 54.5% correct, reduced hallucination from 45.3% to 18.8%, and improved evidence attribution from 1.85 to 2.49 (p<0.001)

## Executive Summary
This study evaluates Retrieval Augmented Generation (RAG) for improving factuality of references and evidence attribution in Large Language Models (LLMs) for long-form consumer health question answering in ophthalmology. The authors developed a RAG pipeline with approximately 70,000 domain-specific documents and compared LLMs with and without RAG on 100 consumer health questions using 10 healthcare professionals for evaluation. Results demonstrate that RAG significantly improves reference accuracy while slightly decreasing overall response quality metrics.

## Method Summary
The study developed a RAG pipeline using a corpus of approximately 70,000 ophthalmology documents (PubMed abstracts, AAO guidelines, and EyeWiki articles) indexed with text-embedding-ada-002. For 100 consumer health questions, the pipeline retrieved top-10 relevant documents using dense retrieval, which were then provided as context to GPT-3.5 (temperature=0) for response generation. Responses were manually evaluated by 10 healthcare professionals for reference factuality (correct, minor errors, hallucinated), selection and ranking of evidence, and response accuracy, completeness, and evidence attribution on a 5-point scale.

## Key Results
- Without RAG: 45.3% of references hallucinated, 34.1% with minor errors, 20.6% correct
- With RAG: 18.8% of references hallucinated, 26.7% with minor errors, 54.5% correct
- Evidence attribution improved from 1.85 to 2.49 (p<0.001)
- Accuracy decreased slightly from 3.52 to 3.23 (p=0.03)
- Completeness decreased slightly from 3.47 to 3.27 (p=0.17)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG improves factuality of evidence in LLM responses by grounding outputs in real documents
- Mechanism: RAG retrieves top-k most relevant documents from a domain-specific corpus and provides them as context to the LLM during generation, reducing the probability of hallucinated references
- Core assumption: The domain-specific corpus contains relevant and accurate documents that can support most consumer health questions in ophthalmology
- Evidence anchors:
  - [abstract] "LLMs with RAG significantly improved accuracy (54.5% being correct) and reduced error rates (18.8% with minor hallucinations and 26.7% with errors)."
  - [section] "For the 100 questions, the LLM without RAG and with RAG provided 252 and 277 references in the responses, in total, respectively. Of the 252 references of the LLM without RAG, 45.3% hallucinated, 34.1% consisted of minor errors, and 20.6% were correct. In contrast, out of the 277 references in the responses the LLM with RAG, 54.5% were correct references, 26.7% were references with minor errors, and 18.8% were hallucinated references."
  - [corpus] Weak evidence: The corpus contains 70K ophthalmology documents but no validation that all topics in the 100 questions are well covered
- Break condition: If the retrieved documents are irrelevant or the LLM fails to incorporate them, hallucination rates will not improve

### Mechanism 2
- Claim: RAG improves evidence attribution by providing traceable sources for generated claims
- Mechanism: By including real document snippets in the generation context, the LLM is more likely to cite accurate references, improving evidence attribution scores
- Core assumption: The LLM can correctly interpret and attribute claims to the provided document context
- Evidence anchors:
  - [abstract] "The use of RAG also improved evidence attribution (increasing from 1.85 to 2.49 on a 5-point scale, P<0.001)."
  - [section] "In total, of the 277 references provided in the LLM + RAG responses, 173 references were from the top 10 most relevant documents retrieved by RAG. In other words, only 62.5% of the references identified by RAG were selected as top references in the final LLM responses."
  - [corpus] No direct evidence; assumption that corpus documents are citable sources
- Break condition: If the LLM ignores most retrieved documents or misattributes content, attribution scores will not improve

### Mechanism 3
- Claim: RAG reduces hallucination but introduces tradeoffs in accuracy and completeness due to inclusion of irrelevant documents
- Mechanism: While RAG grounds generation in real documents, the retrieval step may bring in irrelevant content that confuses the LLM, slightly reducing accuracy and completeness
- Core assumption: Irrelevant documents retrieved by RAG can degrade response quality
- Evidence anchors:
  - [abstract] "The use of RAG also improved evidence attribution (increasing from 1.85 to 2.49 on a 5-point scale, P<0.001), albeit with slight decreases in accuracy (from 3.52 to 3.23, P=0.03) and completeness (from 3.47 to 3.27, P=0.17)."
  - [section] "62.5% of the top 10 documents retrieved by RAG were selected as the top references in the LLM response, with an average ranking of 4.9." This implies some retrieved documents were not selected, potentially due to irrelevance
  - [corpus] Weak evidence: The corpus is large but no quality filtering or relevance validation is mentioned
- Break condition: If retrieval quality is poor, accuracy and completeness will drop more significantly

## Foundational Learning

- Concept: Dense retrieval using semantic embeddings
  - Why needed here: RAG relies on retrieving relevant documents based on semantic similarity, not just keyword matching, to provide useful context to LLMs
  - Quick check question: What embedding model is used for indexing and querying the ophthalmology corpus?
    - Answer: text-embedding-ada-002

- Concept: Document chunking for retrieval
  - Why needed here: Large documents are split into smaller 1024-token snippets to fit embedding size limits and improve retrieval precision
  - Quick check question: Why are documents segmented into snippets before indexing?
    - Answer: To ensure embeddings capture focused content and to avoid exceeding token limits

- Concept: Evaluation of evidence factuality
  - Why needed here: Medical applications require verifiable references; factuality assessment distinguishes real from hallucinated citations
  - Quick check question: How are references categorized in the factuality evaluation?
    - Answer: Correct (real with correct metadata), minor errors (real but minor metadata errors), or hallucinated (non-existent)

## Architecture Onboarding

- Component map:
  Corpus ingestion: PubMed abstracts, AAO guidelines, EyeWiki articles -> Document segmentation -> Embedding generation (text-embedding-ada-002) -> Storage -> Retrieval engine (dense retrieval using cosine similarity) -> LLM integration (GPT-3.5 with top-10 retrieved documents as context) -> Evaluation (manual factuality scoring and response quality assessment)

- Critical path:
  1. User query -> embedding generation
  2. Retrieve top-10 documents via cosine similarity
  3. Augment LLM prompt with retrieved documents
  4. Generate response with references
  5. Evaluate factuality and attribution

- Design tradeoffs:
  - Corpus size vs. relevance: Larger corpus increases coverage but may include irrelevant docs
  - Retrieval top-k: More docs increase grounding chance but risk diluting focus
  - LLM temperature: Lower temperature reduces variance but may limit creativity

- Failure signatures:
  - High hallucination rate -> retrieval relevance or LLM grounding failure
  - Low evidence attribution -> LLM not citing provided documents
  - Drop in accuracy/completeness -> irrelevant documents included

- First 3 experiments:
  1. Vary top-k retrieval size (5, 10, 15) and measure factuality vs. accuracy trade-off
  2. Test different embedding models (e.g., domain-specific biomedical embeddings) for retrieval quality
  3. Compare GPT-3.5 vs GPT-4 with RAG to assess grounding improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between domain-specific and general medical corpora for RAG in healthcare applications?
- Basis in paper: [inferred] The authors noted they used ~70K ophthalmology-specific documents but mentioned that general medical resources might also contain relevant information
- Why unresolved: The study only tested ophthalmology-specific corpora without comparing against general medical or mixed-domain corpora
- What evidence would resolve it: Comparative evaluation of RAG performance using different combinations of domain-specific, general medical, and mixed corpora

### Open Question 2
- Question: How do different RAG configurations (e.g., number of retrieved documents, embedding models) affect the factuality and attribution quality in medical LLMs?
- Basis in paper: [inferred] The authors used default RAG settings (top 10 documents, text-embedding-ada-002) and acknowledged that other configurations could be explored
- Why unresolved: The study only tested one RAG configuration and didn't systematically evaluate the impact of different settings
- What evidence would resolve it: Systematic comparison of different RAG parameters and their effects on reference accuracy, hallucination rates, and evidence attribution

### Open Question 3
- Question: What methods can improve LLMs' ability to select and synthesize the most relevant RAG-retrieved documents?
- Basis in paper: [explicit] The authors observed that only 62.5% of top-10 RAG documents were selected, with an average ranking of 4.9, indicating selection challenges
- Why unresolved: The study identified the problem but didn't test potential solutions for improving document selection and synthesis
- What evidence would resolve it: Experimental evaluation of methods like document re-ranking, chain-of-thought reasoning, or specialized selection mechanisms

## Limitations
- Manual evaluation by healthcare professionals introduces potential subjectivity and inter-annotator variability
- Corpus coverage and quality validation is weak - no evidence that all topics in the 100 questions are well-covered
- Only tested GPT-3.5 with RAG, not exploring other LLM architectures or larger models like GPT-4

## Confidence
- Reference factuality improvement: High confidence
- Evidence attribution improvement: High confidence
- Accuracy/completeness tradeoffs: Medium confidence
- Corpus coverage adequacy: Low confidence

## Next Checks
1. Test RAG performance with GPT-4 or other larger LLMs to assess if the accuracy/completeness tradeoffs persist with more capable models
2. Conduct inter-annotator reliability analysis to quantify agreement among the 10 healthcare professionals and validate the consistency of evaluation scores
3. Perform coverage analysis to determine what percentage of the 100 consumer health questions could be answered using the 70K-document corpus, and identify any systematic gaps in document topics