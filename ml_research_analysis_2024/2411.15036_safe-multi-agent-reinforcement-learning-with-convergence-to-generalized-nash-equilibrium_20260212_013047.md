---
ver: rpa2
title: Safe Multi-Agent Reinforcement Learning with Convergence to Generalized Nash
  Equilibrium
arxiv_id: '2411.15036'
source_url: https://arxiv.org/abs/2411.15036
tags:
- policy
- safety
- multi-agent
- function
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for safe multi-agent reinforcement
  learning (MARL) that enforces constraints at every state the agents visit, addressing
  the limitations of existing methods that only ensure average safety. The key innovation
  is the use of controlled invariant sets (CIS), characterized by safety value functions
  from Hamilton-Jacobi reachability analysis, to identify feasible regions and transform
  state constraints into state-dependent action spaces.
---

# Safe Multi-Agent Reinforcement Learning with Convergence to Generalized Nash Equilibrium

## Quick Facts
- arXiv ID: 2411.15036
- Source URL: https://arxiv.org/abs/2411.15036
- Authors: Zeyang Li; Navid Azizan
- Reference count: 40
- Primary result: Novel framework for safe MARL enforcing constraints at every state, converging to generalized Nash equilibrium

## Executive Summary
This paper introduces a novel framework for safe multi-agent reinforcement learning (MARL) that enforces constraints at every state the agents visit, addressing the limitations of existing methods that only ensure average safety. The key innovation is the use of controlled invariant sets (CIS), characterized by safety value functions from Hamilton-Jacobi reachability analysis, to identify feasible regions and transform state constraints into state-dependent action spaces. A multi-agent dual policy iteration algorithm is proposed, which converges to a generalized Nash equilibrium, balancing safety and performance. The framework is extended to practical high-dimensional systems through the Multi-Agent Dual Actor-Critic (MADAC) algorithm, which approximates the iteration scheme using deep RL. Empirical evaluations on safety-critical MARL benchmarks demonstrate that MADAC significantly outperforms existing methods, achieving higher rewards while maintaining better compliance with safety constraints.

## Method Summary
The proposed method introduces a multi-agent dual policy iteration algorithm that uses controlled invariant sets (CIS) characterized by safety value functions to transform state-wise constraints into state-dependent action spaces. Each agent maintains two policies: a task policy for maximizing rewards within safe regions and a safety policy for identifying the CIS. The algorithm employs an agent-by-agent sequential update scheme that achieves O(Cn) computational complexity while preserving convergence to a generalized Nash equilibrium. For practical implementation in high-dimensional systems, the framework is extended through MADAC, which approximates the iteration scheme using deep RL with dual actor-critic architectures.

## Key Results
- MADAC significantly outperforms existing methods on safety-critical MARL benchmarks, achieving higher rewards while maintaining better constraint compliance
- The agent-by-agent sequential update scheme achieves O(Cn) computational complexity compared to O(C^n) for joint optimization
- The method converges to a generalized Nash equilibrium, ensuring that agents' policies form a stable solution where no agent can unilaterally improve
- Safety constraints are enforced at every state visited, not just on average, providing stronger safety guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The agent-by-agent sequential update scheme achieves computational complexity of O(Cn) while preserving convergence to a Nash equilibrium on the safety value function.
- Mechanism: Instead of jointly optimizing all agents' policies (O(C^n) complexity), the algorithm sequentially updates each agent's policy using the most recent policies of other agents. This coordinate-descent-style approach maintains convergence guarantees because each agent's update is locally optimal given others' current policies.
- Core assumption: The safety value function optimization is decomposable across agents in a sequential manner, similar to how standard value function optimization works in single-agent MARL.
- Evidence anchors:
  - [abstract]: "This results in a coordinate-descent-style updating scheme. Our method is inspired by the one-agent-at-a-time technique used in previous MARL algorithms [6, 7, 8, 9], which focuses on optimizing the standard value function"
  - [section]: "This maximization step can become intractable as the number of agents increases... To address this issue, we propose an agent-by-agent sequential update scheme for optimizing the safety value function with a computational complexity of O(Cn), while still preserving the convergence guarantee"
- Break condition: The convergence guarantee breaks if the sequential updates create cyclic dependencies where no agent can improve given others' policies, or if the problem becomes highly non-convex making local improvements insufficient.

### Mechanism 2
- Claim: State-wise constraints are transformed into state-dependent action spaces via controlled invariant sets (CIS), enabling constraint satisfaction without explicit constraint enforcement.
- Mechanism: By identifying the maximal identifiable CIS using safety value functions, the algorithm converts hard state constraints into soft action constraints. Each agent's action space becomes state-dependent, containing only actions that keep the system within the CIS. This transforms a constrained optimization problem into an unconstrained one on modified action spaces.
- Core assumption: The controlled invariant set characterization is accurate enough that restricting actions to the invariant action set ensures safety without explicit constraint checking.
- Evidence anchors:
  - [abstract]: "By leveraging action-space constraints derived from these controlled invariant sets, we develop a multi-agent dual policy iteration algorithm"
  - [section]: "By incorporating CIS identification into the learning process, we introduce a multi-agent dual policy iteration algorithm that guarantees convergence to a generalized Nash equilibrium in state-wise constrained cooperative Markov games"
- Break condition: The mechanism breaks if the identified CIS is significantly smaller than the true maximal CIS, leading to overly conservative action restrictions, or if the state-dependent action spaces become too complex to compute in high dimensions.

### Mechanism 3
- Claim: The dual policy structure (task policy + safety policy) enables simultaneous optimization of rewards and safety without interference between objectives.
- Mechanism: Each agent maintains two separate policies - a task policy for maximizing rewards within safe regions and a safety policy for identifying the CIS. The safety policies are updated independently to find the largest safe region, while task policies are constrained to operate within this identified region. This separation prevents the reward-seeking behavior from destabilizing the safety learning process.
- Core assumption: The safety policy learning and task policy learning can proceed independently without requiring complex coordination between the two objectives.
- Evidence anchors:
  - [abstract]: "By leveraging action-space constraints derived from these controlled invariant sets, we develop a multi-agent dual policy iteration algorithm"
  - [section]: "Each agent maintains two individual policies: a task policy for solving the constructed problem (10) and a safety policy for collaboratively identifying the CIS"
- Break condition: The mechanism breaks if the safety policy identification is too slow compared to task policy updates, causing the task policy to operate outside safe regions, or if the safety policy becomes stuck in local optima preventing proper CIS identification.

## Foundational Learning

- Concept: Controlled Invariant Sets (CIS) in control theory
  - Why needed here: CIS provides the theoretical foundation for transforming state constraints into action constraints, which is essential for the algorithm's safety guarantees
  - Quick check question: What is the relationship between the safety value function and the controlled invariant set, and how does this enable constraint satisfaction?

- Concept: Generalized Nash Equilibrium in game theory
  - Why needed here: The algorithm targets convergence to a generalized Nash equilibrium, which is necessary because the feasible action spaces depend on other agents' policies
  - Quick check question: How does a generalized Nash equilibrium differ from a standard Nash equilibrium, and why is this distinction important for constrained multi-agent systems?

- Concept: Hamilton-Jacobi reachability analysis
  - Why needed here: This control-theoretic method provides the foundation for computing safety value functions that characterize the CIS
  - Quick check question: What is the computational challenge with Hamilton-Jacobi reachability analysis in high-dimensional systems, and how does the proposed algorithm address this limitation?

## Architecture Onboarding

- Component map:
  - Safety Policy Networks → Safety Value Networks → CIS identification
  - Task Policy Networks → Value Networks → Reward maximization
  - Lagrange Multipliers → Constraint satisfaction enforcement
  - Replay Buffer → Experience storage for off-policy learning

- Critical path: Safety policy evaluation → Safety policy improvement → Task policy evaluation → Task policy improvement → CIS identification → Repeat

- Design tradeoffs: The dual policy structure increases memory and computational overhead but enables independent optimization of safety and rewards. The agent-by-agent update scheme scales linearly with agent count but may converge slower than joint optimization methods.

- Failure signatures: Slow convergence of safety policies (CIS not expanding), task policies violating constraints despite safety policies (CIS identification error), or poor reward performance due to overly conservative CIS identification.

- First 3 experiments:
  1. Implement the safety policy iteration on a simple 2-agent grid world with known CIS to verify convergence properties
  2. Test the agent-by-agent sequential update on a small continuous control problem to validate O(Cn) complexity claims
  3. Evaluate the dual policy structure on a benchmark where safety and rewards are conflicting objectives to test the separation of concerns

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but implicitly leaves several important research directions unexplored, particularly regarding scalability to very large numbers of agents and robustness to model uncertainty.

## Limitations

- The computational complexity claims rely on idealized assumptions about the problem structure that may not hold in practice
- The method requires accurate modeling of system dynamics, which may be challenging in real-world applications
- The theoretical convergence guarantees assume perfect safety value function learning, which may not be achievable with function approximation
- The approach may become overly conservative in complex environments with intricate safety constraints

## Confidence

- High confidence: The core algorithmic framework and its theoretical convergence properties under idealized conditions
- Medium confidence: The computational complexity claims (O(Cn)) and their practical implications
- Medium confidence: The empirical performance improvements over baselines, though sample efficiency comparisons could be more comprehensive
- Low confidence: The scalability claims for high-dimensional systems without empirical validation on significantly larger agent populations

## Next Checks

1. Conduct ablation studies testing the algorithm's behavior when safety value function learning is deliberately impaired to quantify the impact on constraint satisfaction and overall performance
2. Perform systematic scaling experiments with increasing numbers of agents (beyond the 2-4 agents tested) to validate the O(Cn) complexity claims and identify potential scalability bottlenecks
3. Test the algorithm on problems with non-convex constraint boundaries to evaluate whether the CIS identification mechanism remains effective in more complex safety landscapes