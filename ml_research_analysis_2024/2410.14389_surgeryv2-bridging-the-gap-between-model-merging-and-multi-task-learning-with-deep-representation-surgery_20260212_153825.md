---
ver: rpa2
title: 'SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task Learning
  with Deep Representation Surgery'
arxiv_id: '2410.14389'
source_url: https://arxiv.org/abs/2410.14389
tags:
- surgery
- representation
- task
- merging
- surgeryv2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies "representation bias" as a critical bottleneck
  in model merging for multi-task learning, where the merged model''s representations
  diverge from those of expert models, leading to suboptimal performance. The authors
  propose two solutions: Surgery, a lightweight task-specific module aligning the
  final layer representations, and SurgeryV2, a deep representation surgery that mitigates
  layer-wise bias across all layers.'
---

# SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task Learning with Deep Representation Surgery

## Quick Facts
- arXiv ID: 2410.14389
- Source URL: https://arxiv.org/abs/2410.14389
- Authors: Enneng Yang; Li Shen; Zhenyi Wang; Guibing Guo; Xingwei Wang; Xiaocun Cao; Jie Zhang; Dacheng Tao
- Reference count: 40
- One-line primary result: Deep representation surgery (SurgeryV2) significantly enhances model merging performance by mitigating layer-wise representation bias, achieving results comparable to traditional MTL or individual expert models.

## Executive Summary
This paper addresses a critical bottleneck in model merging for multi-task learning: representation bias, where the merged model's representations diverge from those of expert models, leading to suboptimal performance. The authors propose two solutions: Surgery, a lightweight task-specific module aligning final layer representations, and SurgeryV2, which performs deep representation surgery across all layers. Both methods use unsupervised optimization via unlabeled data. Extensive experiments across computer vision and natural language processing domains demonstrate that SurgeryV2 bridges the performance gap between model merging-based MTL and traditional MTL, achieving results comparable to traditional MTL or individual expert models.

## Method Summary
The method involves a two-stage approach to address representation bias in model merging for multi-task learning. First, expert models are fine-tuned on individual tasks from pre-trained models (ViT-B/32, ViT-B/16, ViT-L/14 for vision; BERT for NLP). Model merging methods (Weight Averaging, Task Arithmetic, Ties-Merging, AdaMerging) are then applied to create merged models. The Surgery module aligns final layer representations between merged and expert models using a lightweight adapter, while SurgeryV2 extends this to all layers with task-specific adapters. Both modules are optimized unsupervised using unlabeled test data or wild data (e.g., ImageNet, Wikipedia), minimizing the distance between merged and expert representations.

## Key Results
- SurgeryV2 achieves performance comparable to traditional MTL or individual expert models, bridging the gap with model merging.
- Representation bias exists at every layer of merged models, accumulating with depth and degrading multi-task performance.
- Unsupervised optimization using unlabeled data effectively trains surgery modules without requiring access to original training data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representation bias exists at every layer of the merged model and accumulates with depth, degrading multi-task performance.
- Mechanism: When merging independently fine-tuned models, the parameter interpolation disrupts the learned representation space. This bias propagates through layers, with deeper layers accumulating more bias because earlier layers' misaligned representations cascade forward.
- Core assumption: Representations extracted by the merged model differ systematically from those of individual expert models, and this difference compounds across layers.
- Evidence anchors:
  - [abstract] "Further analysis reveals that representation bias phenomena exist at each layer of the merged model, and aligning representations only in the last layer is insufficient for fully reducing systemic bias because biases introduced at each layer can accumulate and interact in complex ways."
  - [section] "We carefully analyze the correlation between the capacity of the Surgery module and the degree of representation bias ( §IV-C), finding that simply increasing the capacity can effectively reduce the bias and improve merging performance. However, beyond a certain threshold, further increasing capacity no longer yields benefits. Additionally, our layer-by-layer analysis reveals that representation bias exists at each layer of the merged model, with the bias magnitude increasing monotonically with the depth of the layer in the merged model."

### Mechanism 2
- Claim: Aligning representations at each layer through lightweight surgery modules can correct layer-wise biases and restore performance.
- Mechanism: By inserting a task-specific adapter module at each layer that minimizes the distance between merged and expert representations, the method progressively corrects biases layer-by-layer, preventing error accumulation.
- Core assumption: The representation bias at each layer can be independently corrected without disrupting the correction achieved at other layers.
- Evidence anchors:
  - [abstract] "We then propose a more comprehensive solution, deep representation surgery (also called SurgeryV2), which mitigates representation bias across all layers, and thus bridges the performance gap between model merging-based MTL and traditional MTL."
  - [section] "Our SurgeryV2 aligns the representation of the merged model with that of the individual models at each layer, thereby effectively mitigating MTL performance degradation."

### Mechanism 3
- Claim: Unsupervised optimization using unlabeled test data can effectively train the surgery modules without requiring access to original training data.
- Mechanism: The surgery modules are trained to minimize the distance between merged and expert representations on unlabeled data, using the expert models as implicit supervision.
- Core assumption: The unlabeled data distribution is sufficiently similar to the original training data distribution that representation alignment on unlabeled data transfers to improved performance on the actual task.
- Evidence anchors:
  - [abstract] "Finally, we design an unsupervised optimization objective to optimize both the Surgery and SurgeryV2 modules."
  - [section] "In particular, our SurgeryV2 scheme does not rely on any labeled training data. Instead, inspired by test-time adaptation, it leverages the unlabeled test data and individual models as a self-supervised signal to optimize the parameters of the Surgery module."

## Foundational Learning

- Concept: Multi-task learning (MTL) and model merging
  - Why needed here: The paper operates at the intersection of these two paradigms, proposing a method that improves model merging for MTL by addressing representation bias.
  - Quick check question: What is the key difference between traditional MTL (joint training) and model merging (parameter interpolation)?

- Concept: Representation spaces and feature extraction in deep neural networks
  - Why needed here: The core problem is a discrepancy between representation distributions of merged vs. expert models, which directly impacts task performance.
  - Quick check question: Why would the representations from a merged model differ from those of individually trained expert models?

- Concept: Parameter interpolation and its effects on learned representations
  - Why needed here: The proposed solution operates in the representation space, but the problem originates from weight-space operations during model merging.
  - Quick check question: How does simple weighted averaging of parameters potentially disrupt the learned representations in neural networks?

## Architecture Onboarding

- Component map:
  - Expert models (pre-trained and fine-tuned on individual tasks)
  - Merged model (created via parameter interpolation)
  - Surgery module (task-specific adapter for final layer)
  - SurgeryV2 module (task-specific adapters for all layers)
  - Optimization pipeline (unsupervised training on unlabeled data)

- Critical path:
  1. Load expert models and create merged model via chosen merging method
  2. For each task, compute expert representations on unlabeled data
  3. Compute merged model representations on same data
  4. Apply Surgery or SurgeryV2 to minimize representation distance
  5. Evaluate performance on test sets

- Design tradeoffs:
  - Single-layer vs. multi-layer surgery: SurgeryV2 is more effective but adds more parameters and computation
  - Rank of adapter modules: Higher rank allows more expressive corrections but increases parameter count and risk of overfitting
  - Loss function choice: Different distance metrics (L1, MSE, cosine) may affect convergence and final performance

- Failure signatures:
  - If SurgeryV2 performs worse than Surgery: Likely indicates overfitting or interference between layer-wise corrections
  - If both perform worse than merged model without surgery: Suggests the optimization objective is misaligned or the unlabeled data distribution is problematic
  - If performance degrades with higher rank: Indicates overfitting to unlabeled data

- First 3 experiments:
  1. Verify representation bias exists: Compare t-SNE visualizations and L1 distances between merged and expert representations
  2. Test Surgery module: Apply to Task Arithmetic merged model and measure performance improvement
  3. Test SurgeryV2 module: Apply to Task Arithmetic merged model and measure layer-wise representation bias reduction and performance improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SurgeryV2 vary with different choices of distance metrics (L1, L2, cosine similarity) for measuring representation bias across different architectures and tasks?
- Basis in paper: [explicit] The paper mentions that "ψ(·) denotes an arbitrary distance function, which could be L1 loss, mean squared error (MSE), or negative cosine similarity" and provides experimental comparisons in Tab. IX showing performance differences across these metrics.
- Why unresolved: The paper only tests three specific distance metrics and doesn't explore other potential metrics or systematically analyze how different metrics affect performance across various architectures and task types.
- What evidence would resolve it: A comprehensive ablation study testing multiple distance metrics (e.g., KL divergence, Wasserstein distance) across different architectures (ViT, BERT) and task domains (vision, NLP) to identify optimal metric choices for different scenarios.

### Open Question 2
- Question: What is the theoretical relationship between the rank hyperparameter in SurgeryV2 modules and the convergence rate or stability of the optimization process?
- Basis in paper: [inferred] The paper mentions that "a critical hyperparameter in our Surgery or SurgeryV2 methods is the rank r" and shows performance varies with rank, but doesn't provide theoretical analysis of how rank affects optimization dynamics.
- Why unresolved: The paper empirically tests different rank values but doesn't establish theoretical bounds or convergence guarantees relating rank to optimization performance or stability.
- What evidence would resolve it: Mathematical analysis deriving convergence rates as a function of rank, supported by empirical validation showing how rank affects gradient norms, training stability, and convergence speed across different architectures.

### Open Question 3
- Question: How does the representation bias phenomenon manifest in model merging scenarios with heterogeneous architectures (e.g., merging ViT with CNN models) or when tasks have vastly different data distributions?
- Basis in paper: [explicit] The paper states that "representation bias exists across tasks, across merging methods, and across architectures" but all experiments use homogeneous architectures and relatively similar task distributions.
- Why unresolved: The paper only tests homogeneous architecture merging (all ViT or all BERT) and doesn't investigate how representation bias behaves with heterogeneous model types or when merging tasks with fundamentally different data characteristics.
- What evidence would resolve it: Experimental results showing representation bias measurements and performance metrics when merging heterogeneous architectures (ViT+CNN, BERT+transformer) and tasks with different modalities (vision+language, structured+unstructured data).

### Open Question 4
- Question: What is the optimal layer-wise allocation of SurgeryV2 module capacity across different layers, rather than using uniform rank across all layers?
- Basis in paper: [inferred] The paper uses uniform rank across all layers in SurgeryV2 but shows in Fig. 10 that representation bias varies significantly across layers, suggesting non-uniform capacity allocation might be beneficial.
- Why unresolved: The paper doesn't explore whether allocating more capacity to layers with higher representation bias or specific architectural positions could improve performance beyond the uniform allocation approach.
- What evidence would resolve it: Comparative experiments testing non-uniform capacity allocation strategies (e.g., higher rank in deeper layers, adaptive allocation based on bias measurements) against uniform allocation across multiple architectures and task combinations.

## Limitations
- The effectiveness of unsupervised optimization on unlabeled data is assumed to transfer across domains, but this assumption is not rigorously tested.
- The layer-wise independence assumption for SurgeryV2 corrections may not hold in practice, potentially causing interference between adapter modules.
- The generalization of results beyond the tested architectures (ViT, BERT) and tasks is not established.

## Confidence
- **High**: Representation bias exists and degrades merged model performance; Surgery module improves performance over merged models
- **Medium**: SurgeryV2 provides additional benefits over Surgery; unsupervised optimization is effective across domains
- **Low**: The method generalizes to architectures beyond ViT/BERT; layer-wise corrections are truly independent

## Next Checks
1. Test SurgeryV2 on a wider range of architectures (CNNs, smaller transformers) to verify generalization
2. Evaluate performance when unlabeled data distribution differs significantly from training data distribution
3. Analyze adapter module interactions across layers to verify independence assumption