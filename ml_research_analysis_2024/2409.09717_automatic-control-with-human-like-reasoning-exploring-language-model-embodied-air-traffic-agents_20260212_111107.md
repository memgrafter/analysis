---
ver: rpa2
title: 'Automatic Control With Human-Like Reasoning: Exploring Language Model Embodied
  Air Traffic Agents'
arxiv_id: '2409.09717'
source_url: https://arxiv.org/abs/2409.09717
tags:
- agent
- conflict
- aircraft
- language
- traffic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the application of large language models as
  embodied agents in air traffic control scenarios, focusing on their ability to autonomously
  resolve conflicts. The research developed a language model-based agent framework
  with function-calling and learning capabilities, integrating tools for simulator
  interaction and an innovative experience library that stores conflict resolution
  knowledge.
---

# Automatic Control With Human-Like Reasoning: Exploring Language Model Embodied Air Traffic Agents

## Quick Facts
- arXiv ID: 2409.09717
- Source URL: https://arxiv.org/abs/2409.09717
- Authors: Justas Andriuškevičius; Junzi Sun
- Reference count: 13
- Primary result: GPT-4o-based LLM agent successfully resolved 119/120 conflict scenarios using function-calling and experience library

## Executive Summary
This study explored the application of large language models as embodied agents in air traffic control scenarios, focusing on their ability to autonomously resolve conflicts. The research developed a language model-based agent framework with function-calling and learning capabilities, integrating tools for simulator interaction and an innovative experience library that stores conflict resolution knowledge. Testing with both open-source (Llama3:70B) and closed-source (GPT-4o) models across 120 conflict scenarios showed significant performance differences. The best-performing configuration, using GPT-4o with the experience library, successfully resolved 119 out of 120 imminent conflict scenarios involving up to four aircraft simultaneously. Most importantly, the agents provided human-level text explanations for their traffic situations and conflict resolution strategies, demonstrating the potential of language models to reduce the gap between artificial and human situational awareness in air traffic management.

## Method Summary
The study implemented LLM-based agents using function-calling to interact with the BlueSky air traffic simulator, enabling dynamic control command generation and environment state retrieval. The framework included an experience library built on vector embeddings to store and retrieve past conflict resolution strategies. Testing involved both single-agent and multi-agent configurations (planner, executor, verifier) across 120 conflict scenarios with 2-4 aircraft, using GPT-4o and Llama3:70B models. Performance was evaluated based on success rates in avoiding collisions, loss of separation, or near misses.

## Key Results
- GPT-4o with experience library achieved 119/120 success rate in conflict resolution
- Llama3:70B without experience library resolved only 52% of conflicts
- Multi-agent architecture significantly improved performance, especially for smaller models
- Agents provided human-level text explanations for conflict resolution strategies
- Experience library improved Llama3:70B success rate from 52% to 76%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM's function-calling capability enables real-time interaction with the BlueSky simulator, allowing the agent to issue control commands and retrieve updated aircraft information dynamically.
- Mechanism: The LLM evaluates the conflict situation, selects the appropriate tool (e.g., SendCommand()), generates the correct function arguments (e.g., HDG AB112 225), and executes it. The simulator returns updated state, which the LLM uses to reassess the situation iteratively until resolution.
- Core assumption: The LLM can correctly interpret simulator outputs and translate them into accurate, executable commands within the operational constraints of the ATC environment.
- Evidence anchors: [abstract] "by providing a proper application programming interface, these models can be integrated with various tools and virtual or real environments"; [section II-C] "the large language model decides when to utilize a tool, and it is also responsible for generating proper functional arguments"
- Break condition: If the LLM generates syntactically incorrect commands or misinterprets simulator state, the simulation may fail to execute commands or provide incorrect feedback, breaking the loop.

### Mechanism 2
- Claim: The experience library, built on vector embeddings, enables the agent to retrieve relevant past conflict resolution strategies, improving performance especially for smaller models like Llama3:70B.
- Mechanism: After each conflict resolution, the agent creates an experience document summarizing the conflict description and categorizing commands as helpful or not. This document is embedded and stored in a vector database. When encountering a new conflict, the agent embeds the current conflict description and retrieves the most similar past experience using HNSW and cosine similarity.
- Core assumption: Conflicts with similar descriptions (in terms of aircraft positions, headings, and conflict type) can be resolved using analogous strategies, and the embedding model accurately captures these similarities.
- Evidence anchors: [section II-D.2] "The search process employs the Hierarchical Navigable Small World (HNSW) algorithm alongside Cosine Similarity to perform the vector search"; [section III-C] "by including experience libraries, significant improvements are observed" (e.g., Llama3:70B success rate improved from 52% to 76%)
- Break condition: If the embedding model fails to capture the semantic similarity of conflicts, or if metadata filtering is incorrect (e.g., wrong number of aircraft), the retrieved experience may be irrelevant or misleading.

### Mechanism 3
- Claim: The multi-agent architecture (planner, executor, verifier) distributes cognitive load, enabling efficient handling of complex scenarios with many aircraft, especially beneficial for models with smaller context windows like Llama3:70B.
- Mechanism: The planner analyzes the airspace, formulates a conflict resolution plan, and passes it to the executor, which issues commands. The verifier monitors the outcome and either confirms resolution or generates a new plan. This division allows each agent to focus on a specific subtask and reduces the information each must process.
- Core assumption: Task decomposition into specialized roles reduces the per-agent cognitive load sufficiently to offset the overhead of inter-agent communication and coordination.
- Evidence anchors: [section II-B] "The planner agent is responsible for generating a conflict resolution plan... The verifier agent plays a critical role in ensuring the efficacy of the conflict resolution"; [section III-C] "For multiple-agent setup, the success rates are all high, even for the open-source Llama3:70B with a significantly smaller model size"
- Break condition: If inter-agent communication is slow or the verifier frequently re-plans, the system may become inefficient or fail to resolve conflicts in time.

## Foundational Learning

- Concept: Vector embeddings and similarity search
  - Why needed here: The experience library relies on encoding conflict descriptions into vectors and retrieving similar past experiences using cosine similarity and HNSW.
  - Quick check question: What is the dimensionality of the embeddings used in the experience library, and which algorithm is used for similarity search?

- Concept: Function calling in LLMs
  - Why needed here: The agent must dynamically decide which tool to call (e.g., GetAllAircraftInfo, SendCommand) and generate correct arguments based on the current context.
  - Quick check question: How does the LLM decide whether to use a tool, and what is the format of the generated function arguments?

- Concept: Embodied agent architecture
  - Why needed here: The agent must perceive the environment (via simulator tools), reason about the situation, and act (issue commands) in a closed loop, mimicking human situational awareness.
  - Quick check question: What are the three main components of the agent's prompt template, and how do they support the embodied interaction loop?

## Architecture Onboarding

- Component map:
  - LLM (e.g., GPT-4o, Llama3:70B) -> Function-calling interface -> BlueSky simulator -> Experience library (Chroma vector DB) -> Prompt template (system prompt, user input, chat history, agent scratchpad) -> Multi-agent coordination (planner, executor, verifier)

- Critical path:
  1. LLM receives prompt with current aircraft and conflict info
  2. LLM decides whether to call a tool and which one
  3. Tool executes (e.g., SendCommand) and updates simulator
  4. LLM reassesses and repeats until conflict resolved
  5. Experience document created and stored/retrieved as needed

- Design tradeoffs:
  - Single vs multi-agent: single-agent simpler but may struggle with complex scenarios; multi-agent distributes load but adds coordination overhead
  - Experience library: improves performance but adds storage and retrieval latency; quality depends on embedding and metadata accuracy
  - Model size: larger models (GPT-4o) perform better but cost more; smaller models (Llama3:70B) cheaper but benefit more from experience library

- Failure signatures:
  - LLM generates invalid commands → simulator errors or no action
  - Experience retrieval returns irrelevant docs → poor conflict resolution
  - Verifier frequently re-plans → inefficiency or unresolved conflicts
  - Context window overflow → loss of important information

- First 3 experiments:
  1. Test single-agent with GPT-4o on a simple 2-aircraft head-on conflict without experience library; verify correct command generation and conflict resolution.
  2. Test Llama3:70B with experience library on the same scenario; verify that the experience document is created, stored, and retrieved correctly.
  3. Test multi-agent setup (planner, executor, verifier) on a 3-aircraft converging conflict; verify task distribution and coordination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of language model-based agents scale with increasingly complex air traffic scenarios beyond four aircraft?
- Basis in paper: [explicit] The paper discusses performance degradation with more aircraft but doesn't explore scenarios beyond four aircraft.
- Why unresolved: The study only tested scenarios with up to four aircraft, limiting understanding of performance at higher complexity levels.
- What evidence would resolve it: Testing the agents with scenarios involving five or more aircraft and comparing success rates.

### Open Question 2
- Question: What is the long-term effectiveness and reliability of the experience library as it accumulates more conflict resolution data over time?
- Basis in paper: [inferred] The paper introduces the experience library concept but doesn't discuss its performance over extended use or with continuous data accumulation.
- Why unresolved: The study focused on initial performance and didn't explore how the library's effectiveness changes with more stored experiences.
- What evidence would resolve it: Longitudinal studies tracking agent performance as the experience library grows and analyzing the quality of retrieved experiences.

### Open Question 3
- Question: How do language model-based agents handle unexpected or rare air traffic situations not well-represented in their training data?
- Basis in paper: [explicit] The paper mentions training on aviation data but doesn't discuss handling of rare or unexpected scenarios.
- Why unresolved: The study focused on common conflict types and didn't explore the agents' adaptability to novel situations.
- What evidence would resolve it: Testing agents with unusual or rare air traffic scenarios and evaluating their problem-solving strategies and success rates.

## Limitations

- Model generalization and robustness: Performance on novel, more complex scenarios with heterogeneous traffic or adverse weather remains unknown.
- Experience library implementation details: Specific embedding models, dimensionality, and similarity thresholds are not specified.
- Simulator-LLM integration latency: Real-time performance and decision cycle time are not reported.

## Confidence

- Performance Claims: High
- Mechanism Effectiveness: Medium
- Human-like Reasoning Claims: Low

## Next Checks

1. Evaluate GPT-4o + experience library configuration on held-out conflict scenarios not seen during initial testing, including edge cases with more than four aircraft.

2. Conduct ablation studies to assess the impact of different embedding models, similarity thresholds, and metadata filtering strategies on the experience library's effectiveness.

3. Measure and report the end-to-end latency of the LLM-agent and simulator interaction loop to determine if the system meets real-time ATC requirements.