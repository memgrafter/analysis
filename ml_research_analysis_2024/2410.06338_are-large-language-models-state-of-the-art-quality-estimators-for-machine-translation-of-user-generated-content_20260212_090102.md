---
ver: rpa2
title: Are Large Language Models State-of-the-art Quality Estimators for Machine Translation
  of User-generated Content?
arxiv_id: '2410.06338'
source_url: https://arxiv.org/abs/2410.06338
tags:
- translation
- score
- llms
- machine
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  serve as state-of-the-art quality estimators for machine translation of user-generated
  content (UGC) containing emotional expressions. The authors employ an existing emotion-related
  dataset with human-annotated errors and calculate quality evaluation scores based
  on the Multi-dimensional Quality Metrics (MQM).
---

# Are Large Language Models State-of-the-art Quality Estimators for Machine Translation of User-generated Content?

## Quick Facts
- arXiv ID: 2410.06338
- Source URL: https://arxiv.org/abs/2410.06338
- Authors: Shenbin Qian; Constantin Orăsan; Diptesh Kanojia; Félix do Carmo
- Reference count: 15
- Primary result: PEFT of LLMs outperforms fine-tuned baseline models in score prediction with human interpretable explanations for machine translation quality estimation of emotion-loaded UGC.

## Executive Summary
This paper investigates whether large language models (LLMs) can serve as state-of-the-art quality estimators for machine translation of user-generated content (UGC) containing emotional expressions. The authors employ an existing emotion-related dataset with human-annotated errors and calculate quality evaluation scores based on the Multi-dimensional Quality Metrics (MQM). They compare the accuracy of several LLMs with fine-tuned baseline models under in-context learning (ICL) and parameter-efficient fine-tuning (PEFT) scenarios. The results show that PEFT of LLMs leads to better performance in score prediction with human interpretable explanations than fine-tuned models.

## Method Summary
The study uses an emotion-related dataset (HADQAET) with human-annotated errors and computes MQM scores for machine translation segments. The authors compare several LLMs using in-context learning with two prompt templates and parameter-efficient fine-tuning (LoRA) with 4-bit quantization. They evaluate performance using Spearman and Pearson correlation scores between predicted and true MQM scores, and conduct manual analysis of LLM outputs to identify issues such as refusal to reply to prompts and unstable outputs.

## Key Results
- PEFT of LLMs leads to better performance in score prediction with human interpretable explanations than fine-tuned models
- Best correlation scores achieved by PEFT of DeepSeek models surpassed baseline models
- Manual analysis revealed issues such as refusal to reply to prompts and unstable outputs while evaluating machine translation of UGC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEFT of LLMs outperforms fine-tuned baseline models in score prediction with human interpretable explanations for machine translation quality estimation of emotion-loaded UGC.
- Mechanism: Parameter-efficient fine-tuning (PEFT) using Low-Rank Adaptation (LoRA) allows LLMs to adapt to the quality estimation task while maintaining the pre-trained knowledge and reducing computational costs. This enables the models to provide both accurate score predictions and natural language explanations for translation errors.
- Core assumption: PEFT preserves the generative capabilities of LLMs while adapting them to the specific task of quality estimation.
- Evidence anchors:
  - [abstract] "We find that PEFT of LLMs leads to better performance in score prediction with human interpretable explanations than fine-tuned models."
  - [section] "PEFT of LLMs leads to better performance in score prediction with human interpretable explanations than fine-tuned models."
  - [corpus] Weak evidence; no direct corpus citations found for this specific claim.
- Break condition: If PEFT fails to maintain the generative capabilities of LLMs or if the task-specific data is insufficient for effective adaptation.

### Mechanism 2
- Claim: LLMs can provide human interpretable explanations for translation errors in addition to score predictions.
- Mechanism: The inherent generative capability of LLMs allows them to produce natural language explanations for identified errors, making the evaluation process more transparent and understandable to humans.
- Core assumption: LLMs can accurately identify and explain translation errors based on the given instructions and examples.
- Evidence anchors:
  - [abstract] "The inherent generative capability of LLMs allows for the provision of QE scores along with natural language explanations, rendering them comprehensible to humans."
  - [section] "The inherent generative capability of LLMs allows for the provision of QE scores along with natural language explanations, rendering them comprehensible to humans."
  - [corpus] Weak evidence; no direct corpus citations found for this specific claim.
- Break condition: If LLMs fail to accurately identify translation errors or if the explanations are not meaningful to human evaluators.

### Mechanism 3
- Claim: In-context learning (ICL) can achieve results comparable to SoTA QE models in score prediction.
- Mechanism: ICL allows LLMs to adapt to new tasks by examples or instructions without parameter updates, enabling them to perform quality estimation tasks based on the provided context.
- Core assumption: LLMs can effectively learn from the provided examples and instructions to perform the quality estimation task.
- Evidence anchors:
  - [abstract] "Our paper delves into the question, 'Are LLMs SoTA quality estimators for the translation of Chinese emotion-loaded UGC, through in-context learning (ICL) and parameter-efficient fine-tuning (PEFT)?'"
  - [section] "The success of LLMs in various natural language processing tasks brings new trends and methods in QE research."
  - [corpus] Weak evidence; no direct corpus citations found for this specific claim.
- Break condition: If ICL fails to provide sufficient context for the LLMs to perform the quality estimation task effectively.

## Foundational Learning

- Concept: Multi-dimensional Quality Metrics (MQM)
  - Why needed here: MQM is used to calculate quality evaluation scores for machine translation segments, which serve as the true labels for comparison against the predicted scores extracted from LLM outputs.
  - Quick check question: What are the severity levels and corresponding weights in the MQM framework?

- Concept: In-context learning (ICL)
  - Why needed here: ICL allows LLMs to adapt to new tasks by examples or instructions without parameter updates, enabling them to perform quality estimation tasks based on the provided context.
  - Quick check question: How does ICL differ from fine-tuning in terms of model adaptation?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: PEFT allows LLMs to adapt to the quality estimation task while maintaining the pre-trained knowledge and reducing computational costs.
  - Quick check question: What is the main advantage of PEFT over full fine-tuning in terms of computational efficiency?

## Architecture Onboarding

- Component map: Data collection and preprocessing -> MQM score calculation -> Prompt template design -> LLM inference (ICL and PEFT) -> Score extraction and evaluation -> Manual analysis
- Critical path:
  1. Data collection and preprocessing
  2. MQM score calculation
  3. Prompt template design
  4. LLM inference (ICL and PEFT)
  5. Score extraction and evaluation
  6. Manual analysis
- Design tradeoffs:
  - Using ICL vs. PEFT for LLM adaptation
  - Choosing the appropriate LLM size and architecture
  - Balancing the number of examples in prompt templates
  - Selecting the evaluation metrics (Spearman correlation vs. Pearson correlation)
- Failure signatures:
  - Low correlation scores between predicted and true MQM scores
  - Inconsistent or unstable LLM outputs
  - Refusal to reply to prompts containing "inappropriate" language
  - High computational costs or memory requirements
- First 3 experiments:
  1. Evaluate the performance of different LLMs (e.g., Llama-2-13B, Yi-34B, DeepSeek-67B) using ICL with zero-shot and few-shot learning.
  2. Fine-tune the selected LLMs using PEFT with LoRA and compare the results with the ICL scenario.
  3. Analyze the manual outputs of the LLMs to identify any issues such as refusal to reply or unstable outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do large language models (LLMs) perform in quality estimation tasks for machine translation of user-generated content (UGC) with emotional expressions when compared to traditional quality estimation models?
- Basis in paper: [explicit] The paper compares the performance of LLMs with fine-tuned baseline models, including TransQuest and COMET, in predicting quality evaluation scores based on Multi-dimensional Quality Metrics (MQM) for machine translation of UGC containing emotional expressions.
- Why unresolved: The paper shows that PEFT of LLMs leads to better performance than fine-tuned models, but it does not provide a comprehensive comparison of all LLMs against traditional models across various datasets and scenarios.
- What evidence would resolve it: A systematic evaluation of LLMs against traditional quality estimation models across multiple datasets, languages, and evaluation metrics would provide a clearer understanding of their relative performance.

### Open Question 2
- Question: What are the main challenges and limitations of using large language models (LLMs) for quality estimation in machine translation of user-generated content (UGC) with emotional expressions?
- Basis in paper: [explicit] The paper identifies issues such as refusal to reply to prompts and unstable outputs when LLMs are used to evaluate machine translation of UGC. It also mentions the sensitivity of LLMs to certain words and phrases.
- Why unresolved: The paper provides a preliminary analysis of these challenges but does not explore them in depth or propose solutions to address them.
- What evidence would resolve it: A detailed investigation into the causes of these challenges and the development of strategies to mitigate them, such as prompt engineering or model fine-tuning, would help resolve this question.

### Open Question 3
- Question: How does the performance of large language models (LLMs) in quality estimation tasks vary across different types of user-generated content (UGC) and emotional expressions?
- Basis in paper: [inferred] The paper focuses on a specific dataset of emotion-loaded UGC but does not explore how LLMs perform on different types of UGC or with varying levels of emotional intensity.
- Why unresolved: The paper's findings are based on a single dataset, which may not be representative of all types of UGC or emotional expressions.
- What evidence would resolve it: Evaluating LLMs on diverse datasets with different types of UGC and emotional expressions, and analyzing their performance across these variations, would provide insights into their generalizability and robustness.

## Limitations

- Modest correlation scores (Spearman ρ of 0.21-0.28) indicate LLMs still struggle with precise quality estimation for emotion-loaded UGC
- Issues with LLM refusal to process content with "inappropriate" language (10-30% of instances) and unstable outputs across repeated inferences
- Evaluation is constrained to Chinese-English translation and specific MQM error categories, limiting generalizability

## Confidence

- **High confidence**: The observation that PEFT outperforms ICL for this task is well-supported by the experimental results and aligns with established findings in the literature on parameter-efficient adaptation methods.
- **Medium confidence**: The claim that LLMs provide "human interpretable explanations" is supported by qualitative examples but lacks systematic evaluation of explanation quality or usefulness to human evaluators.
- **Medium confidence**: The superiority of DeepSeek models over other LLMs is demonstrated but the differences are modest (e.g., ρ improvements of 0.02-0.05), and the analysis doesn't fully explain why these models perform better for this specific task.

## Next Checks

1. **Robustness testing**: Conduct repeated inferences on the same samples to quantify output stability and develop mitigation strategies for refusal behaviors, particularly for content containing emotional or sensitive language.
2. **Ablation study**: Systematically vary prompt templates, example selection, and formatting to identify which design choices most impact correlation scores and explainability quality.
3. **Cross-lingual validation**: Evaluate the same methodology on additional language pairs beyond Chinese-English to assess generalizability and identify language-specific challenges in UGC quality estimation.