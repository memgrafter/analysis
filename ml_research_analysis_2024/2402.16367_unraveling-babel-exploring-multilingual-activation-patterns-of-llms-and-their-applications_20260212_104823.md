---
ver: rpa2
title: 'Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their
  Applications'
arxiv_id: '2402.16367'
source_url: https://arxiv.org/abs/2402.16367
tags:
- activation
- experts
- expert
- llama
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the multilingual activation patterns within
  large language models (LLMs) by converting dense models into fine-grained mixture-of-experts
  (MoE) architectures and analyzing expert activation frequencies through heatmap
  visualization. The study reveals that expert activation patterns are closely related
  to language families and genera, with shared high-frequency experts concentrated
  in middle layers and language-specific experts more prominent in shallow and deep
  layers.
---

# Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications

## Quick Facts
- arXiv ID: 2402.16367
- Source URL: https://arxiv.org/abs/2402.16367
- Authors: Weize Liu; Yinlong Xu; Hongxia Xu; Jintai Chen; Xuming Hu; Jian Wu
- Reference count: 40
- Primary result: This paper investigates the multilingual activation patterns within large language models (LLMs) by converting dense models into fine-grained mixture-of-experts (MoE) architectures and analyzing expert activation frequencies through heatmap visualization.

## Executive Summary
This paper investigates the multilingual activation patterns within large language models (LLMs) by converting dense models into fine-grained mixture-of-experts (MoE) architectures and analyzing expert activation frequencies through heatmap visualization. The study reveals that expert activation patterns are closely related to language families and genera, with shared high-frequency experts concentrated in middle layers and language-specific experts more prominent in shallow and deep layers. The research also demonstrates that instruction tuning significantly impacts expert activation frequencies, particularly in the final layer. Leveraging these insights, the authors propose two pruning methods based on frequency thresholds and frequency sorting, which significantly outperform random expert pruning and in some cases even surpass the performance of unpruned models.

## Method Summary
The study converts dense LLMs (Llama 2 7B, Llama 3 8B, Llama 3 70B, Mistral-7B-v0.3) into MoE architectures using parameter clustering split (256 clusters per FFN layer), cross-layer expert selection (top 10% experts per token), and Z-score normalization for global comparison. The researchers analyze expert activation frequencies across nine languages from the ROOTS corpus, visualize multilingual activation patterns through heatmaps, and propose pruning methods based on frequency thresholds and frequency sorting. Performance is evaluated using perplexity and accuracy metrics on X-CSQA and MGSM datasets.

## Key Results
- Expert activation patterns show strong correlation with language families and genera, with shared high-frequency experts concentrated in middle layers
- Instruction tuning significantly alters expert activation frequencies, especially in the final layer
- Proposed frequency-based pruning methods significantly outperform random expert pruning and sometimes surpass unpruned model performance

## Why This Works (Mechanism)

### Mechanism 1
Converting dense FFNs into fine-grained MoE structures enables layer-wise expert clustering that reveals language-specific activation patterns. By applying balanced K-Means clustering on up-projection layer parameters, the model splits each FFN into 256 experts. Cross-layer expert selection with Z-score normalization ranks experts globally, isolating high-activation experts per language.

### Mechanism 2
High-frequency experts are concentrated in middle layers and are largely language-agnostic, while shallow and deep layers contain more language-specific experts. Multilingual shared experts are defined as those with activation frequency ≥ 0.05 across all languages. Their distribution is analyzed via heatmaps showing concentration in middle layers and sparsity in shallow/deep layers.

### Mechanism 3
Instruction tuning significantly alters expert activation frequencies, especially in the final layer, indicating adaptation to task-specific language processing. Comparing activation frequencies of instruction-tuned models versus pre-trained models reveals that the last layer shows the largest frequency changes, with some experts increasing and others decreasing activation.

## Foundational Learning

- Concept: MoE architecture and expert selection
  - Why needed here: Understanding how dense models are converted into MoE is crucial for interpreting expert activation patterns.
  - Quick check question: How does balanced K-Means clustering enable the creation of 256 experts per FFN layer?

- Concept: Language family and genus classification
  - Why needed here: Analyzing activation pattern similarities requires knowledge of language relationships.
  - Quick check question: Why do Romance languages show higher activation pattern similarity compared to Indic languages?

- Concept: Cross-layer normalization and expert ranking
  - Why needed here: Z-score normalization enables fair comparison of expert activations across layers of different depths.
  - Quick check question: What is the purpose of applying Z-score normalization before cross-layer expert ranking?

## Architecture Onboarding

- Component map:
  Input -> Token embedding -> FFN layers (split into 256 experts each) -> Output

- Critical path:
  Model conversion to MoE -> Expert activation calculation -> Heatmap visualization -> Pattern analysis -> Pruning method development

- Design tradeoffs:
  - 256 experts per layer balances granularity with computational feasibility
  - Cross-layer expert selection captures global activation patterns vs. local layer-wise patterns
  - Frequency threshold pruning preserves performance but may miss rare but important experts

- Failure signatures:
  - Uniform activation frequencies across all experts (no meaningful patterns)
  - High variance in random expert pruning results
  - No significant difference in activation patterns between language families

- First 3 experiments:
  1. Convert Llama 2 7B to MoE and visualize expert activation frequencies for English vs. Bengali
  2. Calculate similarity metrics (Euclidean, KL divergence, correlation) between activation matrices of Romance vs. Indic languages
  3. Apply frequency threshold pruning (≥5%) to Llama 2 7B and compare perplexity to random expert pruning

## Open Questions the Paper Calls Out

### Open Question 1
Do the multilingual activation patterns observed in Llama models extend to other transformer architectures like GPT or Claude models? The paper primarily focuses on Llama and Mistral models but acknowledges that exploring other model families could provide additional insights. This remains unresolved due to computational constraints and accessibility of models.

### Open Question 2
How do the identified multilingual activation patterns change when models are trained on more balanced multilingual corpora rather than predominantly English data? The authors note that current LLMs are typically trained on imbalanced multilingual data, primarily English, and wonder about the effects of more balanced training. This is unresolved as most state-of-the-art LLMs use similar training corpora.

### Open Question 3
Can the frequency-based pruning approach be extended to achieve more aggressive compression ratios (70-90%) while maintaining reasonable performance? The paper demonstrates success with pruning ratios up to ~25% but notes limitations due to the inherent sparsity of current models, suggesting room for improvement.

## Limitations
- Conversion of dense models to MoE architecture may introduce artifacts not present in original models
- Analysis limited to nine languages from ROOTS corpus, potentially missing broader language diversity
- Pruning methods may not generalize to models with different architectures or training datasets

## Confidence
- **High confidence**: Visualization of multilingual activation patterns through heatmaps is well-supported by methodology and results
- **Medium confidence**: Connection between expert activation patterns and language families/genus requires more rigorous statistical validation
- **Medium confidence**: Proposed pruning methods show promising results but evaluation limited to specific datasets

## Next Checks
1. Perform rigorous statistical tests (hierarchical clustering with bootstrapping or permutation tests) to validate that observed activation pattern similarities between language families are not due to random chance.

2. Test the proposed pruning methods on models fine-tuned on different multilingual datasets (such as CCNet or mC4) to assess generalizability beyond the ROOTS corpus.

3. Systematically vary the number of experts per layer (e.g., 64, 128, 256, 512) to determine the optimal granularity for capturing meaningful multilingual activation patterns without introducing excessive noise.