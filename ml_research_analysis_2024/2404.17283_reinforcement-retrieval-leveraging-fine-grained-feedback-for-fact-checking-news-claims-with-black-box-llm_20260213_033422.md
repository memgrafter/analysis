---
ver: rpa2
title: Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking
  News Claims with Black-Box LLM
arxiv_id: '2404.17283'
source_url: https://arxiv.org/abs/2404.17283
tags:
- retrieval
- claim
- documents
- ffrr
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FFRR, a reinforcement retrieval approach
  leveraging fine-grained feedback from black-box LLMs for fact-checking news claims.
  The method addresses the challenge of optimizing retrieval models without access
  to LLM internals by using two-level fine-grained feedback: document-level rewards
  to refine retrieved documents and question-level rewards to promote alternative
  evidence.'
---

# Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM

## Quick Facts
- arXiv ID: 2404.17283
- Source URL: https://arxiv.org/abs/2404.17283
- Authors: Xuan Zhang; Wei Gao
- Reference count: 0
- Primary result: FFRR achieves F1 scores of 57.0% on RAWFC and 33.5% on LIAR-RAW datasets

## Executive Summary
This paper introduces FFRR, a reinforcement retrieval approach that leverages fine-grained feedback from black-box LLMs for fact-checking news claims. The method addresses the challenge of optimizing retrieval models without access to LLM internals by using two-level fine-grained feedback: document-level rewards to refine retrieved documents and question-level rewards to promote alternative evidence. FFRR uses intermediate questions generated from claims to guide retrieval and employs policy gradient RL to optimize the retrieval policy. Evaluated on RAWFC and LIAR-RAW datasets, FFRR significantly outperforms strong LLM-enabled and non-LLM baselines.

## Method Summary
FFRR employs a reinforcement learning framework where a dense retrieval model is optimized using feedback from a black-box LLM. The approach uses two-level rewards: document-level rewards based on LLM's assessment of individual document relevance, and question-level rewards that encourage retrieval of diverse evidence perspectives. Intermediate questions are generated from claims to guide the retrieval process. The model uses an epsilon-greedy strategy for balancing exploration and exploitation during retrieval. Policy gradient methods are used to update the retrieval policy based on the reward signals. The entire system is trained end-to-end to optimize for fact-checking accuracy.

## Key Results
- FFRR achieves F1 scores of 57.0% on RAWFC dataset, significantly outperforming strong baselines
- On LIAR-RAW dataset, FFRR achieves 33.5% F1 score, demonstrating robust performance across different claim distributions
- The two-level feedback mechanism provides consistent improvements over single-level approaches, with question-level rewards contributing 12.5% absolute F1 gain

## Why This Works (Mechanism)

### Mechanism 1
Two-level fine-grained feedback enables more targeted retrieval optimization than single-level approaches. FFRR uses document-level rewards to refine individual document selection and question-level rewards to promote alternative evidence perspectives. This separation allows optimization for both precision (selecting the most relevant documents) and recall (finding diverse evidence). The approach assumes different types of evidence serve different verification purposes and can be optimized separately.

### Mechanism 2
Combining exploration and exploitation through epsilon-greedy strategy improves retrieval diversity without sacrificing precision. At each step, FFRR samples documents either from top-K (exploitation) or uniform distribution (exploration) based on epsilon probability. This ensures that important evidence which may not initially rank highly can still be discovered while maintaining focus on high-probability documents. The core assumption is that optimal evidence may be hidden in lower-ranked documents.

### Mechanism 3
Using ground-truth labels to generate LLM rewards mitigates LLM bias while guiding retrieval optimization. The reward function computes LLM score for ground-truth label given claim and document, encouraging retriever to find evidence that helps overcome LLM bias. This mechanism assumes that even biased LLMs can provide useful signals when rewards are based on ground-truth outcomes rather than the LLM's own judgment.

## Foundational Learning

- Concept: Policy gradient reinforcement learning
  - Why needed here: Enables optimization of retrieval model without access to LLM gradients
  - Quick check question: How does policy gradient differ from value-based RL methods in terms of gradient computation?

- Concept: Dense retrieval with dual-encoder architecture
  - Why needed here: Provides efficient similarity search for retrieving candidate documents
  - Quick check question: What is the role of temperature parameter τ in the retrieval probability formula?

- Concept: Claim decomposition and intermediate question generation
  - Why needed here: Enables multi-perspective evidence gathering beyond single-query retrieval
  - Quick check question: How does the number of intermediate questions affect the balance between coverage and redundancy?

## Architecture Onboarding

- Component map: Dense retriever → Epsilon-greedy selector → LLM rater → Reward calculator → Policy optimizer → Updated retriever
- Critical path: Claim → Document retrieval → LLM rating → Reward computation → Policy update
- Design tradeoffs: Exploration vs exploitation (epsilon-greedy), document-level vs question-level optimization, reward granularity vs computational cost
- Failure signatures: Poor retrieval quality (evidence mismatch), slow convergence (excessive exploration), suboptimal performance (inadequate reward signal)
- First 3 experiments:
  1. Test retrieval performance with different epsilon values (0.05, 0.1, 0.2) on validation set
  2. Compare document-level vs question-level optimization impact on F1 score
  3. Evaluate effect of different K values (1, 3, 5) on final verification accuracy

## Open Questions the Paper Calls Out

1. How can the efficiency of training the reinforcement retrieval process be improved to reduce the number of interactions needed with the LLM?
2. What techniques can be used to accelerate LLM inference in the FFRR model?
3. How can the FFRR model be extended beyond news claim verification to other forms of misinformation?
4. What are the potential ethical concerns associated with the low accuracy of news claim verification using the FFRR model?
5. How can the FFRR model be made more reliable when applied in the real world to avoid mislabeling false information or vice versa?

## Limitations

- Heavy dependence on LLM feedback introduces computational overhead and potential bias propagation
- Epsilon-greedy exploration strategy requires careful tuning with limited guidance on optimal parameters
- Effectiveness constrained by quality and coverage of document corpus, which may lack relevant evidence for certain claims

## Confidence

- Document-level feedback mechanism: High confidence
- Question-level feedback approach: Medium confidence
- Overall methodology and experimental design: High confidence

## Next Checks

1. Test model's robustness across different document corpus sizes to understand scalability limits
2. Conduct ablation studies isolating contribution of document-level versus question-level feedback
3. Evaluate performance when using different LLM providers or versions to assess dependence on specific model characteristics