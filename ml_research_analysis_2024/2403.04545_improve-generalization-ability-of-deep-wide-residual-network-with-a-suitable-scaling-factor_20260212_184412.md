---
ver: rpa2
title: Improve Generalization Ability of Deep Wide Residual Network with A Suitable
  Scaling Factor
arxiv_id: '2403.04545'
source_url: https://arxiv.org/abs/2403.04545
tags:
- rntk
- have
- neural
- kernel
- residual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the generalization ability of deep residual\
  \ neural networks (ResNets) by studying the influence of the scaling factor \u03B1\
  \ on the residual branch. The authors analyze the infinite-depth limit of the Residual\
  \ Neural Tangent Kernel (RNTK) and prove that when \u03B1 is a constant or decreases\
  \ slowly with depth, the RNTK degenerates to a constant kernel, leading to poor\
  \ generalization."
---

# Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor

## Quick Facts
- arXiv ID: 2403.04545
- Source URL: https://arxiv.org/abs/2403.04545
- Reference count: 40
- Primary result: A suitable scaling factor α on the residual branch can significantly improve generalization of deep ResNets by preventing RNTK degeneration.

## Executive Summary
This paper investigates how the scaling factor α on the residual branch of deep residual networks affects generalization ability. The authors analyze the infinite-depth limit of the Residual Neural Tangent Kernel (RNTK) and prove that constant α causes RNTK to degenerate to a constant kernel, leading to poor generalization. However, when α decreases rapidly with depth (α = L^−γ with γ > 1/2), the kernel converges to a one-hidden-layer FCNTK, enabling optimal generalization through kernel regression with early stopping. The theoretical findings are validated through simulations on synthetic and real datasets.

## Method Summary
The method involves analyzing the RNTK of deep residual networks with varying scaling factors α. The theoretical analysis focuses on the infinite-width limit where training dynamics are characterized by kernel gradient descent with a fixed kernel. For empirical validation, the authors implement kernel ridge regression using the RNTK on synthetic data (Y = ⟨X, β⟩ + 0.1 · ε) and train finite-width convolutional residual networks (18 stacked layers, L=9) on real datasets using Adam optimizer with learning rate 0.001 and decay factor 0.95 per epoch.

## Key Results
- Constant α causes RNTK to degenerate to a constant kernel as depth L → ∞, leading to poor generalization
- Rapid α decay (α = L^−γ, γ > 1/2) enables convergence to one-hidden-layer FCNTK with optimal generalization
- Kernel regression with early stopping achieves minimax rates when using the appropriate limiting kernel
- Empirical results show improved performance on MNIST, CIFAR10, and CIFAR100 when using suitable α values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constant α causes RNTK to degenerate into a constant kernel as depth L → ∞, leading to poor generalization
- Mechanism: As L grows, the identity branch dominates each ResNet block, causing the kernel to become independent of input differences
- Core assumption: RNTK captures generalization behavior in infinite-width limit
- Evidence anchors: Abstract states constant α leads to asymptotically non-learnable functions; section confirms large L limit is constant kernel
- Break condition: Small width or training far from initialization

### Mechanism 2
- Claim: Slow polynomial decay α = L^−γ (0 ≤ γ < 1/2) still causes RNTK degeneration
- Mechanism: Residual branch influence remains insufficient even with slow decay
- Core assumption: Theorem 2 extends to any γ < 1/2
- Evidence anchors: Abstract mentions degeneration occurs even with decreasing α; section confirms slow decay also causes poor generalization
- Break condition: γ ≥ 1/2 (rapid decay)

### Mechanism 3
- Claim: Rapid α decay (α = L^−γ, γ > 1/2) enables convergence to one-hidden-layer FCNTK with optimal generalization
- Mechanism: Rapid decay maintains residual influence, preserving input-dependent kernel structure; early stopping provides implicit regularization
- Core assumption: Eigenvalues decay appropriately for early stopping to achieve minimax rates
- Evidence anchors: Abstract states rapid decay enables minimax rates; section confirms kernel regression achieves optimal rates
- Break condition: Target function outside RKHS or improper early stopping

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its role in characterizing training and generalization of infinitely wide neural networks
  - Why needed here: Entire analysis relies on RNTK approximation and limiting behavior as L → ∞
  - Quick check question: Can you explain why, in infinite-width limit, training dynamics can be described by kernel gradient descent with fixed NTK?

- Concept: Positive definiteness of kernels and its implications for kernel regression
  - Why needed here: RNTK must be positive definite for uniform convergence of empirical kernel matrix to population kernel
  - Quick check question: What does positive definiteness mean for a kernel, and why is it crucial for convergence results in kernel regression?

- Concept: Early stopping as implicit regularization and its connection to eigenvalue decay in kernel methods
  - Why needed here: Shows early stopping on one-hidden-layer FCNTK can achieve minimax rates
  - Quick check question: How does eigenvalue decay rate in a kernel influence optimal stopping time for gradient descent to achieve best generalization?

## Architecture Onboarding

- Component map: Input -> (Convolutional layers, if applicable) -> Residual blocks (each: input + α · Conv -> ReLU -> Conv) -> Output layer
- Critical path: Forward pass computes activations through L layers; backward pass computes gradients for θ parameters; kernel computation calculates RNTK = ⟨∇θf(x), ∇θf(x')⟩ at initialization; kernel regression solves (K + λI)α = y for predictions
- Design tradeoffs: Larger α increases residual influence but risks gradient explosion; smaller α reduces residual benefits; decay rate γ controls transition between degeneration and good generalization
- Failure signatures: Test error plateaus early (constant α case); training error decreases but test error increases (overfitting); numerical instability in kernel matrix
- First 3 experiments: 1) Fix L=50, vary α ∈ {1, 0.1, 0.01}, train on MNIST, compare test accuracy; 2) Fix α=1, vary L ∈ {20, 50, 100}, train on CIFAR10, observe depth effect; 3) Set α = L^−γ for γ ∈ {0.4, 0.6, 0.8}, L ∈ {20, 50, 100}, train on synthetic data, measure test error vs γ

## Open Questions the Paper Calls Out

- Question: Does RNTK degeneration occur for any α = L^(-γ) with 0 < γ < 1/2, or is it specific to γ = 1/4?
  - Basis in paper: Paper proves degeneration for α = L^(-1/4) in Theorem 2; authors believe conclusion holds for any γ ∈ (0, 1/2) but proof is complex
  - Why unresolved: Proof for general case is complex and not provided
  - What evidence would resolve it: Rigorous proof showing degeneration occurs for any γ ∈ (0, 1/2)

- Question: Can uniform convergence of ResNets be established with respect to both width m and depth L?
  - Basis in paper: Paper mentions large L limit is only pointwise and aims to establish simultaneous uniform convergence
  - Why unresolved: Paper does not provide proof for uniform convergence with respect to both width and depth
  - What evidence would resolve it: Rigorous proof establishing uniform convergence with respect to both width m and depth L

- Question: What is the optimal decay rate of α with respect to depth L for best generalization performance?
  - Basis in paper: Paper suggests α should decrease rapidly (γ > 1/2) based on theoretical analysis and simulations
  - Why unresolved: Paper does not provide definitive answer on optimal decay rate
  - What evidence would resolve it: Further theoretical analysis and extensive simulation studies on various datasets and architectures

## Limitations

- Theoretical analysis relies on infinite-width NTK approximation which may not hold for finite-width networks
- Empirical validation shows only modest improvements on real datasets despite strong theoretical claims
- Extension from polynomial decay rates to practical training scenarios remains speculative
- Limited experimental validation beyond synthetic data and standard benchmarks

## Confidence

- High Confidence: Theoretical derivation that constant α causes RNTK degeneration to constant kernel in infinite-depth limit
- Medium Confidence: Claim that rapid α decay leads to one-hidden-layer FCNTK convergence and minimax rates
- Low Confidence: Practical significance for finite-width, finite-depth ResNets based on limited empirical improvements

## Next Checks

1. Implement RNTK for finite-width ResNets and compare empirical kernel behavior to theoretical predictions across different α values and depths to verify constant kernel degeneration is observable in practice

2. Conduct systematic study of α = L^−γ across full range 0 ≤ γ ≤ 1 with varying widths and depths on CIFAR10/CIFAR100, measuring both training dynamics and generalization to identify optimal practical settings

3. Compare performance of scaled residual networks with standard ResNets and alternative architectures (DenseNet, WideResNet) on same tasks to determine whether α scaling provides meaningful advantages beyond standard residual benefits