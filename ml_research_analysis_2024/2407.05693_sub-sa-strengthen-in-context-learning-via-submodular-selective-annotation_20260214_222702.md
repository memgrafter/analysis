---
ver: rpa2
title: 'Sub-SA: Strengthen In-context Learning via Submodular Selective Annotation'
arxiv_id: '2407.05693'
source_url: https://arxiv.org/abs/2407.05693
tags:
- sub-sa
- annotation
- selection
- examples
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sub-SA introduces a submodular-based selective annotation method
  for in-context learning (ICL) that significantly reduces annotation costs and time
  consumption while improving performance. The core method uses a submodular function
  combined with Reward and Penalty Regularization (RPR) to balance diversity and representativeness
  in selecting examples from large unlabeled datasets.
---

# Sub-SA: Strengthen In-context Learning via Submodular Selective Annotation

## Quick Facts
- arXiv ID: 2407.05693
- Source URL: https://arxiv.org/abs/2407.05693
- Reference count: 40
- Outperforms state-of-the-art methods in 13 out of 16 cases while achieving millisecond-level selection time

## Executive Summary
Sub-SA introduces a submodular-based selective annotation method that significantly improves in-context learning performance while reducing annotation costs and time consumption. The method uses a submodular function combined with Reward and Penalty Regularization (RPR) to balance diversity and representativeness when selecting examples from large unlabeled datasets. By optimizing this function through greedy search, Sub-SA achieves superior performance across 8 diverse NLP tasks compared to existing methods that require hours for selection.

## Method Summary
The core method employs a submodular function with Reward and Penalty Regularization (RPR) to select the most informative examples from large unlabeled datasets. The approach computes Sentence-BERT embeddings for all instances, then uses a greedy search algorithm to optimize the submodular function that balances representativeness and diversity. The RPR component (位1=2 for representative score, 位2=-1 for diversity score) ensures the selected examples cover diverse aspects while remaining representative of the underlying data distribution. For inference, the method retrieves k-shot examples using similarity-based prompt retrieval and evaluates performance on test data using various LLM sizes from 2.7B to 175B parameters.

## Key Results
- Outperforms state-of-the-art methods in 13 out of 16 benchmark comparisons
- Achieves millisecond-level annotation selection time versus hours-level for competing approaches
- Demonstrates consistent improvements across classification, commonsense reasoning, dialogue, and generation tasks
- Shows effectiveness with various model sizes from GPT-J (6B) to GPT-3.5-Turbo (175B)

## Why This Works (Mechanism)
The submodular framework ensures near-optimal subset selection by guaranteeing that the marginal gain of adding examples decreases as the subset grows. The RPR regularization specifically balances two competing objectives: selecting diverse examples that cover different aspects of the data distribution while maintaining representativeness of the overall dataset. This dual objective addresses the key challenge in ICL where too much diversity can lead to noisy prompts while too much similarity limits generalization.

## Foundational Learning
- **Submodularity**: A diminishing returns property that ensures near-optimal greedy solutions; needed for efficient subset selection, quick check: verify marginal gains decrease as subset grows
- **Reward and Penalty Regularization (RPR)**: Framework for balancing multiple objectives in optimization; needed to control diversity-representativeness trade-off, quick check: validate 位 parameters produce desired selection characteristics
- **Sentence-BERT Embeddings**: Dense vector representations for text similarity; needed to compute example similarity scores, quick check: confirm embeddings capture semantic similarity relevant to task
- **Greedy Search Optimization**: Iterative algorithm for maximizing submodular functions; needed for efficient subset selection, quick check: verify convergence within acceptable iterations
- **k-shot Learning**: Few-shot prompting paradigm; needed for ICL evaluation, quick check: confirm prompt quality through manual inspection

## Architecture Onboarding

**Component Map**: Unlabeled Data -> Sentence-BERT Embeddings -> Similarity Kernel -> Submodular Function (RPR) -> Greedy Search -> Selected Examples -> k-shot Retrieval -> ICL Evaluation

**Critical Path**: The similarity computation between all example pairs followed by greedy subset selection forms the computational bottleneck, requiring optimization for millisecond-level performance.

**Design Tradeoffs**: 
- Submodularity provides theoretical guarantees but may sacrifice absolute optimality
- RPR regularization balances diversity and representativeness but requires careful 位 tuning
- Greedy search is computationally efficient but may get stuck in local optima

**Failure Signatures**:
- Poor ICL performance indicates suboptimal balance between diversity and representativeness
- Selection times exceeding milliseconds suggest inefficient similarity computation or search
- Highly clustered selected examples reveal insufficient diversity penalty in RPR

**First Experiments**:
1. Validate similarity computation by checking if semantically similar examples have high similarity scores
2. Test greedy search convergence by monitoring marginal gains across iterations
3. Evaluate RPR balance by analyzing diversity metrics of selected subsets

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical assumptions about linear combination of representative and diversity scores may not hold across all task types
- Implementation details for similarity computation and greedy search optimization remain underspecified
- Evaluation focuses primarily on classification and reasoning tasks, with limited validation on complex generation tasks

## Confidence
- **High Confidence**: Core submodular framework and theoretical properties (submodularity, monotonicity)
- **Medium Confidence**: RPR regularization effectiveness and relative improvements over baselines
- **Low Confidence**: Exact implementation details required for faithful reproduction

## Next Checks
1. Reproduce similarity computation: Implement and validate the exact similarity kernel computation between Sentence-BERT embeddings to verify selection quality
2. Benchmark selection time: Measure and compare actual annotation selection time against reported millisecond-level performance
3. Cross-task generalization: Test the method on at least two additional task types (e.g., summarization and question answering) not covered in original evaluation to assess broader applicability