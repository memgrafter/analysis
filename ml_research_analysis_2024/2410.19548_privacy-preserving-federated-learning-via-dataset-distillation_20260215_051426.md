---
ver: rpa2
title: Privacy-Preserving Federated Learning via Dataset Distillation
arxiv_id: '2410.19548'
source_url: https://arxiv.org/abs/2410.19548
tags:
- data
- training
- samples
- information
- flip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FLiP, a federated learning system that applies
  the principle of least privilege to protect user privacy. FLiP extracts task-relevant
  information from raw data using a local-global dataset distillation approach, allowing
  users to control the amount of information shared during training.
---

# Privacy-Preserving Federated Learning via Dataset Distillation

## Quick Facts
- arXiv ID: 2410.19548
- Source URL: https://arxiv.org/abs/2410.19548
- Reference count: 21
- Key result: Achieves 95.56% accuracy on CIFAR-10 using only 10 distilled samples per class while defending against attribute inference attacks with ~50% accuracy

## Executive Summary
FLiP introduces a federated learning system that applies the principle of least privilege to protect user privacy through dataset distillation. The system extracts task-relevant information from raw data by distilling a small number of representative samples per class, which are then aggregated and redistributed by the server. Privacy is evaluated using attribute inference and membership inference attacks, showing that FLiP achieves accuracy comparable to vanilla federated learning while significantly reducing privacy leakage. The approach allows users to control the amount of information shared during training, providing a tunable privacy-accuracy trade-off.

## Method Summary
FLiP uses a local-global dataset distillation approach where each client first distills K representative samples per class from their raw data using soft labels and learnable learning rates. The server aggregates these distilled samples from all clients, creating a global set of K aggregated samples that are redistributed to clients for further local distillation. This iterative process continues for multiple rounds, with the final model trained on the distilled samples. The system achieves privacy protection by minimizing shared information while maintaining model accuracy through careful distillation that captures task-relevant information without exposing private attributes.

## Key Results
- Achieves 95.56% accuracy on CIFAR-10 with TinyResNet using only 10 distilled samples per class
- Defends against attribute inference attacks with attack accuracy around 50% (random guess level)
- Shows 2.172% accuracy improvement on CIFAR-10 when increasing distilled samples from 10 to 20 per class
- Successfully protects against membership inference attacks while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: FLiP extracts task-relevant information from raw data using local-global dataset distillation, allowing users to control the amount of information shared during training.
- **Mechanism**: The local-global dataset distillation approach involves two phases: local distillation where each client extracts K distilled samples per class from their raw data, and global aggregation where the server combines these distilled samples from all clients to create a global set of K aggregated samples. These aggregated samples are then redistributed to clients for further local distillation.
- **Core assumption**: Task-relevant information can be identified and extracted through iterative distillation while preserving model utility.
- **Evidence anchors**: [abstract] "The method distills a small number of representative samples per class from local data, which are then aggregated and redistributed by the server to guide further distillation."
- **Break condition**: If the distilled samples fail to capture sufficient task-relevant information, model accuracy will degrade significantly compared to vanilla FL.

### Mechanism 2
- **Claim**: FLiP achieves privacy protection by minimizing the amount of shared information while maintaining model accuracy.
- **Mechanism**: By limiting the number of distilled samples shared (e.g., 10 per class), FLiP reduces the information available for potential privacy attacks. The paper evaluates this using attribute inference and membership inference attacks, showing that attack accuracy approaches random guess levels.
- **Core assumption**: Fewer distilled samples contain less task-irrelevant information, making it harder for adversaries to infer private attributes.
- **Evidence anchors**: [abstract] "Experiments show that FLiP achieves accuracy comparable to vanilla federated learning while significantly reducing the risk of privacy leakage."
- **Break condition**: If adversaries can extract meaningful information from the limited distilled samples, privacy protection will be compromised.

### Mechanism 3
- **Claim**: Soft labels improve training accuracy by introducing more information during distillation.
- **Mechanism**: Each distilled sample is assigned a soft label (probability distribution over classes) rather than a hard label. This soft labeling, inspired by prior work, provides richer information during the optimization process.
- **Core assumption**: Soft labels provide gradient information that helps optimize the distilled samples more effectively.
- **Evidence anchors**: [section] "The soft label design, inspired by Work [15], can improve the training accuracy by introducing more information."
- **Break condition**: If soft labels don't provide meaningful gradient information, they may not improve training accuracy over hard labels.

## Foundational Learning

- **Concept**: Federated Learning (FL) basics
  - Why needed here: Understanding how FL works is crucial to grasp why FLiP modifies the standard approach and how it maintains the core FL benefits while adding privacy protection.
  - Quick check question: What is the key difference between vanilla FL and FLiP in terms of what clients share with the server?

- **Concept**: Dataset Distillation
  - Why needed here: FLiP relies on dataset distillation to compress raw data into representative samples. Understanding this concept is essential to comprehend how FLiP achieves information reduction.
  - Quick check question: How does dataset distillation differ from model knowledge distillation in terms of what is being compressed?

- **Concept**: Privacy Attacks in ML
  - Why needed here: FLiP evaluates privacy protection using attribute inference and membership inference attacks. Understanding these attack types is necessary to assess the privacy claims.
  - Quick check question: What is the difference between attribute inference attacks and membership inference attacks in terms of what information the adversary tries to extract?

## Architecture Onboarding

- **Component map**: Clients (hold raw data, perform local distillation) → Server (aggregates distilled samples, redistributes aggregated samples) → Clients (receive aggregated samples, further local distillation)
- **Critical path**: Raw data → Local distillation → Server aggregation → Redistribute → Further local distillation → Final model training on distilled samples
- **Design tradeoffs**:
  - Privacy vs. accuracy: Fewer distilled samples provide better privacy but may reduce accuracy
  - Communication overhead: More distillation rounds increase communication but may improve accuracy
  - Computational cost: Local distillation requires additional computation compared to vanilla FL
- **Failure signatures**:
  - Accuracy degradation: If distilled samples don't capture sufficient task-relevant information
  - Privacy breach: If attack accuracy significantly exceeds random guess
  - Training instability: If learning rates or distillation process is not properly tuned
- **First 3 experiments**:
  1. Compare accuracy of FLiP with 10, 15, and 20 distilled samples per class on CIFAR-10 to find the optimal balance
  2. Evaluate privacy protection by running attribute inference attacks on distilled samples from each configuration
  3. Test membership inference attack resistance by training shadow models and evaluating attack accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of distilled samples per class (k) that balances privacy protection and model accuracy across different datasets and model architectures?
- Basis in paper: [explicit] The paper mentions that as k increases, model accuracy improves but privacy protection decreases, and they chose k=20 for privacy assessment. They observed varying accuracy improvements (0.12% for MNIST, 2.172% for CIFAR-10, 3.985% for CIFAR-100) per 5 additional samples.
- Why unresolved: The optimal k likely depends on dataset characteristics, model architecture, and privacy requirements. The paper shows trends but doesn't identify specific optimal values for different scenarios.
- What evidence would resolve it: Systematic experiments varying k across different datasets (MNIST, CIFAR-10, CIFAR-100), model architectures (TinyResNet, AlexNet, ConvNet), and privacy metrics (attack accuracy) to identify Pareto-optimal k values.

### Open Question 2
- Question: How does FLiP's privacy protection scale when clients have heterogeneous data distributions across different classes or when clients hold different numbers of classes?
- Basis in paper: [inferred] The paper assumes each client holds the same kind of categories of samples, but real-world federated learning often involves heterogeneous client data distributions.
- Why unresolved: The paper's theoretical framework and experimental setup assume homogeneous data distribution across clients, which may not reflect practical federated learning scenarios.
- What evidence would resolve it: Experiments testing FLiP with non-IID data distributions across clients, varying class distributions, and clients holding different subsets of classes, measuring both accuracy and privacy protection.

### Open Question 3
- Question: Can FLiP's local-global distillation process be extended to handle non-image data types (e.g., text, tabular data) and what modifications would be needed?
- Basis in paper: [explicit] The paper focuses on image datasets (MNIST, CIFAR-10, CIFAR-100) and demonstrates results with image-based model architectures, but doesn't explore other data types.
- Why unresolved: The distillation methodology and visualization techniques are specific to image data, and it's unclear how the approach would generalize to other data modalities.
- What evidence would resolve it: Implementation and evaluation of FLiP on non-image datasets (text corpora, financial data, healthcare records) with appropriate model architectures, demonstrating both accuracy and privacy protection effectiveness.

## Limitations

- Limited implementation details for the local distillation algorithm, particularly regarding optimization process and loss calculation
- Attack model architectures and training procedures for privacy evaluation are not fully specified
- Experiments focus on homogeneous data distributions across clients, not reflecting real-world heterogeneous scenarios

## Confidence

- **High Confidence**: The basic mechanism of dataset distillation with soft labels is sound and well-established in literature
- **Medium Confidence**: The privacy claims based on attack accuracy (~50% for attribute inference) are reasonable but lack rigorous comparison to state-of-the-art attack methods
- **Low Confidence**: The exact implementation details needed for faithful reproduction, particularly the local distillation algorithm and attack model specifications

## Next Checks

1. Implement the local distillation algorithm with detailed logging of optimization steps and loss values to verify correct implementation
2. Run membership inference attacks using established shadow model techniques to validate the reported attack accuracy
3. Conduct ablation studies varying the number of distilled samples (K) to precisely characterize the privacy-accuracy trade-off curve