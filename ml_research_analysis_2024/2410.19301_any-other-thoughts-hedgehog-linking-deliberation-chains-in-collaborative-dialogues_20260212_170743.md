---
ver: rpa2
title: Any Other Thoughts, Hedgehog? Linking Deliberation Chains in Collaborative
  Dialogues
arxiv_id: '2410.19301'
source_url: https://arxiv.org/abs/2410.19301
tags:
- probing
- causal
- utterances
- interventions
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task for constructing deliberation
  chains in collaborative dialogues, where probing questions are linked to their causal
  antecedents. The authors model these chains using a graph-based framework and reframe
  the problem as a coreference-style clustering task.
---

# Any Other Thoughts, Hedgehog? Linking Deliberation Chains in Collaborative Dialogues

## Quick Facts
- arXiv ID: 2410.19301
- Source URL: https://arxiv.org/abs/2410.19301
- Reference count: 38
- Key outcome: Novel task for constructing deliberation chains in collaborative dialogues using joint modeling approach

## Executive Summary
This paper introduces a novel task for constructing deliberation chains in collaborative dialogues, where probing questions are linked to their causal antecedents. The authors model these chains using a graph-based framework and reframe the problem as a coreference-style clustering task. Their joint modeling approach, which learns to identify probing and causal interventions and the links between them, outperforms both similarity baselines and coreference-based methods on two challenging datasets: DeliData and the Weights Task Dataset. The proposed method achieves strong performance on multiple evaluation metrics, including B3 and CoNLL F1 scores, establishing a standard for this novel task.

## Method Summary
The proposed framework jointly models probing and causal utterances and the links between them using a cross-encoding strategy with Longformer. The model learns three separate scores - probing, causal, and linking - which capture the validity of individual utterances and the relationships between them. During training, pairs of utterances within a window W are generated and scored, with the model learning global patterns of deliberation chains. At inference, the model generates all possible pairs, scores them, applies a threshold, and uses connected-components clustering to form chains. The approach is trained using a joint loss function combining all three score types.

## Key Results
- Joint modeling approach outperforms similarity baselines and coreference-based methods on DeliData and Weights Task Dataset
- Achieves strong performance on multiple evaluation metrics including B3 and CoNLL F1 scores
- Window size W of 3 for DeliData and 5 for WTD provides optimal balance between performance and computational efficiency
- Cross-encoding strategy with Longformer effectively captures long-range dependencies in collaborative dialogues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The joint modeling framework learns to identify both probing and causal interventions simultaneously, improving link accuracy.
- Mechanism: The model learns three separate scores - probing, causal, and linking - which capture the validity of individual utterances and the relationships between them. This allows the model to learn global patterns of deliberation chains rather than just pairwise similarities.
- Core assumption: Utterances in a deliberation chain have inherent properties that distinguish them as probing or causal, and these properties can be learned jointly.
- Evidence anchors:
  - [abstract] "Our framework jointly models probing and causal utterances and the links between them"
  - [section 5] "Mathematically, P r(P, C, L| D) = QN i=1 QN j=1P r(pi | D)P r(cj | D)P r(lij | D)"
- Break condition: If probing and causal interventions cannot be reliably distinguished by their linguistic features, the joint learning approach would fail to improve over simpler pairwise methods.

### Mechanism 2
- Claim: The cross-encoding strategy with Longformer allows the model to capture long-range dependencies in collaborative dialogues.
- Mechanism: By encoding pairs of utterances with global attention and context from previous utterances, the model can capture semantic relationships that span across long dialogues, which is particularly important for the Weights Task Dataset.
- Core assumption: The most relevant information for understanding a probing question is contained within a window of preceding utterances, even if that window is relatively large.
- Evidence anchors:
  - [section 5] "we use the Longformer model (Beltagy et al., 2020) as the base encoder"
  - [section 5] "For context around a probing intervention, we also concatenate the k previous utterances"
- Break condition: If the relevant causal antecedents are consistently outside the attention window, the model would miss crucial context and performance would degrade.

### Mechanism 3
- Claim: The discourse-coherence theory-inspired training approach helps the model learn relevant patterns by focusing on "attentional states."
- Mechanism: By training on pairs of utterances within a window W, the model learns to identify discourse-relevant signals that typically appear in the attentional focus of participants, rather than being distracted by irrelevant information.
- Core assumption: The most pertinent information to a specific utterance remains within an "attentional state" - the point of focus of participants within a dialogue.
- Evidence anchors:
  - [section 5] "Discourse-coherence theory (Grosz and Sidner, 1986; Held et al., 2021) suggests that the most pertinent information to a specific utterance remain within an 'attentional state'"
  - [section 5] "we define a windowW of previous utterances considered for training"
- Break condition: If collaborative dialogues in these datasets don't follow typical attentional state patterns, the window-based training approach would miss important long-range dependencies.

## Foundational Learning

- Concept: Deliberation chains as discourse coherence structures
  - Why needed here: Understanding that deliberation chains represent coherent discourse structures helps in designing appropriate models and evaluation metrics.
  - Quick check question: Can you explain how deliberation chains differ from simple coreference chains in terms of their theoretical foundations?

- Concept: Joint modeling frameworks
  - Why needed here: The paper uses a joint modeling approach to simultaneously learn probing, causal, and linking probabilities, which is crucial for the proposed method.
  - Quick check question: How does joint modeling of multiple probabilities (probing, causal, linking) differ from training separate models for each task?

- Concept: Cross-encoding strategies
  - Why needed here: The model uses cross-encoding to represent pairs of utterances, which is essential for capturing the relationships between probing and causal interventions.
  - Quick check question: What are the advantages of cross-encoding utterance pairs compared to encoding them separately and combining representations?

## Architecture Onboarding

- Component map: GPT-3.5-turbo-0125 annotation pipeline -> Longformer encoder with global attention -> Three feed-forward neural networks (probing, causal, linking) -> Pair generation module with window W -> Connected-components clustering

- Critical path:
  1. Generate utterance pairs within window W
  2. Encode pairs with Longformer using global attention
  3. Compute probing, causal, and linking scores
  4. Apply joint loss function
  5. During inference, generate all possible pairs
  6. Score pairs and apply threshold
  7. Use connected-components clustering to form chains

- Design tradeoffs:
  - Window size W vs. computational efficiency vs. capturing long-range dependencies
  - Joint learning vs. separate models for probing/causal detection and linking
  - Cross-encoding vs. simpler encoding strategies for computational cost
  - Threshold-based pruning vs. exhaustive search for inference speed

- Failure signatures:
  - Low precision with high recall: Threshold too low, model too permissive
  - Low recall with high precision: Threshold too high, model too conservative
  - Poor performance on long dialogues: Window W too small, Longformer attention not capturing long-range dependencies
  - Inability to distinguish probing from causal: Joint learning not capturing utterance-level features

- First 3 experiments:
  1. Ablation study varying window size W to find optimal balance between performance and computational efficiency
  2. Comparison of joint learning vs. separate models for probing/causal detection and linking
  3. Evaluation of different encoding strategies (cross-encoding vs. separate encoding with concatenation)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the deliberation chain construction framework perform on real-time, live dialogue data?
- Basis in paper: [inferred] The authors note that extending their method to live interaction, predicting when probing utterances will occur, is a logical next step.
- Why unresolved: The current model relies on post-hoc annotation and context from completed dialogues, not real-time processing.
- What evidence would resolve it: Implementation and evaluation of the model on streaming dialogue data with real-time intervention detection and prediction accuracy metrics.

### Open Question 2
- Question: What role do non-verbal cues play in deliberation chain construction, particularly in multimodal datasets like WTD?
- Basis in paper: [explicit] The authors explicitly mention that the WTD's multimodal aspect represents a rich opportunity to investigate multimodality's role in deliberation.
- Why unresolved: The current study focuses solely on textual signals and does not incorporate non-verbal cues like gestures or actions.
- What evidence would resolve it: Extension of the model to incorporate multimodal features (gestures, actions, gaze) and comparison of performance on multimodal vs. text-only versions of the data.

### Open Question 3
- Question: How robust is the GPT-based annotation process for generating "gold" labels, and what is the impact of annotation noise on model performance?
- Basis in paper: [explicit] The authors acknowledge the use of GPT for annotation and validate the results through human evaluation, noting the risk of noisy or unreliable AI annotations.
- Why unresolved: While human evaluation shows high acceptability, the potential for systematic biases or errors in the GPT annotations remains unexplored.
- What evidence would resolve it: Direct comparison of model performance using human-annotated vs. GPT-annotated labels, along with analysis of annotation consistency across different annotators or models.

### Open Question 4
- Question: How does the deliberation chain construction task generalize to other domains beyond collaborative problem-solving, such as argumentation or decision-making in different contexts?
- Basis in paper: [explicit] The authors suggest that the task is adaptable to interactions with particular characteristics like argumentation.
- Why unresolved: The current evaluation is limited to two specific collaborative task datasets (DeliData and WTD).
- What evidence would resolve it: Application and evaluation of the model on diverse datasets representing different interaction types (e.g., debate, negotiation, brainstorming) and analysis of domain-specific patterns in deliberation chains.

## Limitations

- Window-based training approach may miss long-range dependencies where causal antecedents appear beyond the attentional state window
- GPT-3.5-turbo-0125 annotation pipeline introduces potential model bias that may not generalize to other collaborative contexts
- Limited evaluation to two specific collaborative task datasets (DeliData and WTD) raises questions about domain generalization

## Confidence

**High Confidence**: The core methodology of using joint modeling with cross-encoding for deliberation chain construction is well-grounded in discourse coherence theory and demonstrates strong empirical performance on both evaluation datasets.

**Medium Confidence**: The generalization of results to other collaborative dialogue datasets or real-time interaction scenarios remains uncertain. The window size parameter W is fixed based on development set performance but optimal values may vary across domains.

**Low Confidence**: The annotation consistency across different annotators and the impact of potential annotation errors on model performance are not thoroughly addressed.

## Next Checks

1. **Window Size Sensitivity Analysis**: Conduct systematic experiments varying window size W from 1 to 10 on both datasets to identify the relationship between window size and performance metrics, particularly for long dialogues in WTD where causal antecedents may span beyond the current window.

2. **Cross-Dataset Generalization Test**: Evaluate the trained models on a held-out subset of utterances from each dataset that were not used during training, and test transfer performance to a third collaborative dialogue dataset (e.g., MultiWOZ or Taskmaster) to assess domain generalization.

3. **Annotation Quality Impact Study**: Create perturbed versions of the annotated datasets by randomly flipping 5%, 10%, and 20% of probing/causal labels, then retrain and evaluate models to quantify the relationship between annotation quality and model performance.