---
ver: rpa2
title: Training Neural Samplers with Reverse Diffusive KL Divergence
arxiv_id: '2410.12456'
source_url: https://arxiv.org/abs/2410.12456
tags:
- neural
- target
- training
- score
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training generative models
  to sample from unnormalized multi-modal target distributions, which is important
  in machine learning, Bayesian inference, and statistical mechanics. The core idea
  is to minimize a new objective called reverse diffusive KL divergence (DiKL), which
  convolves both the target and model distributions with Gaussian diffusion kernels
  at multiple scales.
---

# Training Neural Samplers with Reverse Diffusive KL Divergence

## Quick Facts
- arXiv ID: 2410.12456
- Source URL: https://arxiv.org/abs/2410.12456
- Reference count: 40
- Key outcome: Proposes reverse diffusive KL divergence (DiKL) to train neural samplers that capture multiple modes in unnormalized target distributions, achieving comparable or better performance than state-of-the-art Boltzmann generators (FAB, iDEM) on synthetic and n-body systems with faster training and sampling times.

## Executive Summary
This paper addresses the fundamental problem of sampling from unnormalized Boltzmann distributions, particularly those with multiple modes. The authors introduce reverse diffusive KL divergence (DiKL), which minimizes the KL divergence between diffused versions of the target and model distributions. This approach allows neural samplers to capture multiple modes by bridging isolated modes in the noisy space through Gaussian convolution. The method is evaluated on synthetic multi-modal distributions and n-body systems, showing improved mode coverage and computational efficiency compared to existing methods like FAB and iDEM.

## Method Summary
The proposed method trains neural samplers by minimizing reverse DiKL, which convolves both target and model densities with Gaussian diffusion kernels at multiple scales. The gradient of this objective is estimated using denoising score matching (DSM) for the model score and mixed score identity (MSI) for the target score. The training procedure forms a nested loop where the inner loop trains a score network to estimate the model score, and the outer loop estimates the noisy target score and updates the neural sampler. The method is particularly effective for multi-modal targets and invariant n-body systems, using equivariant graph neural networks for the sampler and score network architectures.

## Key Results
- DiKL achieves comparable or better performance than FAB and iDEM on MW-32, DW-4, and LJ-13 n-body systems
- Training and sampling times are faster than FAB and iDEM across all benchmarks
- On MoG-40 synthetic target, DiKL captures all 40 modes while standard reverse KL only captures 4 modes
- DiKL shows competitive performance on DW-4 and LJ-13 despite having less model flexibility than multi-step diffusion models like iDEM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reverse diffusive KL divergence (DiKL) enables better mode coverage by convolving both target and model densities with Gaussian diffusion kernels at multiple scales.
- Mechanism: Gaussian convolution bridges isolated modes in the noisy space, reducing the number of modes as the variance increases. By minimizing DiKL, the model learns to cover all modes rather than collapsing to a few modes.
- Core assumption: The Gaussian convolution effectively convexifies functions and bridges modes in the noisy space.
- Evidence anchors:
  - [abstract]: "This approach allows the model to capture multiple modes by bridging isolated modes in the noisy space."
  - [section 3]: "One effective way to bridge and merge isolated modes is Gaussian convolution... In Figure 1, we provide a toy visualization showing that by increasing the variance of the Gaussian convolution, we can bridge modes or even reduce the number of modes in the original multi-modal distribution."
  - [corpus]: "Diffusion models are the state of the art for generative modeling. This paper addresses the fundamental problem of sampling from unnormalized densities."

### Mechanism 2
- Claim: The mixed score identity (MSI) estimator provides a tractable way to estimate the gradient of the reverse DiKL.
- Mechanism: MSI combines the denoising score identity (DSI) and target score identity (TSI) to estimate the noisy target score ∇xt log pd(xt) without requiring samples from the target distribution.
- Core assumption: The convex combination of TSI and DSI with coefficients α2t and 1 - α2t, respectively, minimizes the overall variance of the estimator.
- Evidence anchors:
  - [section 4.2]: "To address this, De Bortoli et al. (2024); Phillips et al. (2024) propose a convex combination of the DSI and TSI to interpolate between them, favoring TSI when k(xt|x) has smaller variance and DSI when k(xt|x) has larger variance, thus minimizing the overall variance of the estimator."
  - [section 4.2]: "Proposition 4.3 (Mixed Score Identity). Using a Gaussian convolution k(xt|x) = N (xt|αtx, σ2t I) with a variance-preserving (VP) scheme σ2t = 1 - α2t, and a convex combination of TSI and DSI with coefficients α2t and 1 - α2t, respectively, we have ∇xt log pd(xt) = Z (αt(x + ∇x log pd(x)) - xt)pd(x|xt)dx."
  - [corpus]: "The paper is clearly written and easy to follow. This paper presents an interesting extension of DiKL to the case of unnormalized densities."

### Mechanism 3
- Claim: The training procedure forms a nested loop, where the inner loop trains a score network to estimate the model score, and the outer loop estimates the noisy target score and updates the neural sampler.
- Mechanism: The nested loop allows for efficient training of the neural sampler by iteratively improving the score network and the sampler itself.
- Core assumption: The inner loop typically converges within 50-100 steps, minimally affecting the overall training cost.
- Evidence anchors:
  - [section 4]: "We summarize the whole procedure of training neural samplers with DiKL in Algorithm 1. In short, our training algorithm forms a nested loop: in the inner loop, we train a score network sϕ(xt) to estimate the model score ∇xt log pθ(xt) with DSM; in the outer loop, we first estimate the noisy target score ∇xt log pd(xt) with MSI, and then update the neural sampler with the gradient as in Equation (11) using our estimated noisy target and model scores."
  - [section 4]: "One might think that this nested training procedure imposes a high computational burden. Fortunately, we found that the inner loop typically converged within 50-100 steps in practice, minimally affecting the overall training cost."
  - [corpus]: "The paper is clearly written and easy to follow. This paper presents an interesting extension of DiKL to the case of unnormalized densities."

## Foundational Learning

- Concept: Denoising Score Matching (DSM)
  - Why needed here: DSM is used to train the score network sϕ(xt) to estimate the model score ∇xt log pθ(xt).
  - Quick check question: What is the objective function minimized in DSM to train the score network?

- Concept: Target Score Identity (TSI)
  - Why needed here: TSI is used to estimate the noisy target score ∇xt log pd(xt) without requiring samples from the target distribution.
  - Quick check question: What is the key assumption made in TSI regarding the convolution kernel?

- Concept: Mixed Score Identity (MSI)
  - Why needed here: MSI combines TSI and DSI to provide a tractable way to estimate the gradient of the reverse DiKL.
  - Quick check question: How does MSI interpolate between TSI and DSI to minimize the overall variance of the estimator?

## Architecture Onboarding

- Component map:
  Score network sϕ(xt) -> Model score ∇xt log pθ(xt) -> DSM
  Neural sampler gθ(z) <- DiKL gradient <- MSI
  Gaussian kernels {kt}T t=1 -> Convolved densities
  Posterior sampler -> Samples for MSI

- Critical path:
  1. Sample x′ ~ pθ(x) and xt ~ kt(xt|x′).
  2. Train score network sϕ(xt) using DSM with samples from the sampler.
  3. Estimate noisy target score ∇xt log pd(xt) using MSI with posterior sampling.
  4. Update neural sampler gθ(z) using the gradient estimated from MSI and DSM.

- Design tradeoffs:
  - Using larger Gaussian kernels in DiKL may lead to better mode coverage but can also increase the variance of the TSI estimator.
  - Using a more complex architecture for the neural sampler gθ(z) may improve performance but can also increase training time and computational cost.
  - Using a larger number of diffusion steps T may lead to better performance but can also increase training time and computational cost.

- Failure signatures:
  - If the model only captures a few modes, it may indicate that the Gaussian kernels are not chosen appropriately or that the target distribution has modes that are too far apart.
  - If the training is unstable, it may indicate that the batch size is too small or that the learning rate is not set correctly.
  - If the performance is suboptimal, it may indicate that the score network or the neural sampler is not trained properly or that the posterior sampling is not accurate enough.

- First 3 experiments:
  1. Implement the DiKL objective and train the neural sampler on a simple 1D Gaussian mixture model to verify that it can capture multiple modes.
  2. Compare the performance of DiKL with standard reverse KL on a 2D mixture of 40 Gaussians to visualize the difference in mode coverage.
  3. Train the neural sampler on a small n-body system (e.g., DW-4) to verify that it can handle invariant target distributions and compare its performance with FAB and iDEM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of weighting function w(t) in the DiKL objective affect the balance between exploration and exploitation during training?
- Basis in paper: [explicit] The paper mentions that w(t) = 1/αt can accelerate exploration for highly multi-modal targets, while uniform weighting is better for encouraging exploitation in complex targets like DW and LJ.
- Why unresolved: The paper provides some guidance but does not conduct a systematic study of different weighting functions or their impact on convergence and mode coverage.
- What evidence would resolve it: A comprehensive ablation study comparing different weighting functions (e.g., 1/αt, σ²_t/α_t, σ²_t/α²_t, uniform) across various target distributions, measuring convergence speed, mode coverage, and sample quality.

### Open Question 2
- Question: What is the optimal trade-off between the number of diffusion steps T and the model's ability to capture complex energy landscapes?
- Basis in paper: [inferred] The paper uses T=30 for most experiments but notes that the one-step generator has limited model flexibility compared to multi-step diffusion models like iDEM. It also mentions that DiKL struggles with more complicated targets like LJ-55.
- Why unresolved: The paper does not explore how varying T affects performance on increasingly complex targets or whether there's a point of diminishing returns.
- What evidence would resolve it: Experiments systematically varying T (e.g., 10, 30, 50, 100) on targets of increasing complexity (MoG-40, MW-32, DW-4, LJ-13, LJ-55) with corresponding training and sampling efficiency metrics.

### Open Question 3
- Question: How does the stability of DiKL training compare to FAB and iDEM when using different initialization strategies for the posterior sampling?
- Basis in paper: [explicit] The paper mentions that posterior sampling could be a bottleneck and notes that HMC and MALA can have slow mixing speeds for complicated targets. It also mentions that the initial guess in HMC/LG should be zero-centered and G-equivariant.
- Why unresolved: The paper uses specific initialization strategies (e.g., samples from ¯N (Xt/αt, σ²_t/α²_t) or Sampling Importance Resampling) but does not compare their impact on training stability or explore alternative initialization methods.
- What evidence would resolve it: Comparative experiments using different initialization strategies (e.g., random initialization, warm start from previous iteration, different proposal distributions) measuring training stability, convergence speed, and final sample quality across multiple runs with different random seeds.

## Limitations

- The effectiveness of Gaussian kernel-based mode bridging may degrade for highly separated modes, which are not thoroughly explored in the paper.
- The stability and performance of DiKL depend heavily on the quality of posterior sampling via AIS/MALA, which can be computationally expensive and have slow mixing speeds for complicated targets.
- The method struggles with more complex targets like LJ-55, indicating limitations in handling very high-dimensional and complex energy landscapes.

## Confidence

- High: The theoretical formulation of reverse DiKL and its connection to mode bridging via Gaussian convolution is well-established.
- Medium: The practical implementation of the mixed score identity and its impact on training stability requires more empirical validation.
- Medium: The comparative performance against FAB and iDEM on n-body systems is promising but relies on specific hyperparameter settings and early stopping criteria that are not fully detailed.

## Next Checks

1. **Ablation on posterior sampling**: Replace AIS/MALA with a fixed number of HMC steps and measure the impact on training stability and final performance. This will isolate the effect of posterior sampling quality on the DiKL gradient estimation.

2. **Mode separation sensitivity**: Test the DiKL method on a synthetic target with increasingly separated modes to quantify the degradation in mode coverage. This will validate the limits of Gaussian kernel-based mode bridging.

3. **Hyperparameter robustness**: Systematically vary the number of diffusion steps T, the βt schedule, and the weighting function w(t) to identify the most critical hyperparameters and their impact on convergence speed and final sample quality.