---
ver: rpa2
title: 'Respect the model: Fine-grained and Robust Explanation with Sharing Ratio
  Decomposition'
arxiv_id: '2402.03348'
source_url: https://arxiv.org/abs/2402.03348
tags:
- layer
- conference
- explanation
- methods
- iclr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of faithfulness and robustness in
  existing explainable AI (XAI) methods by proposing a novel method called Sharing
  Ratio Decomposition (SRD). SRD uses a vector perspective to analyze the nonlinear
  interactions between filters and introduces the concept of Pointwise Feature Vectors
  (PFVs) to represent the intricate contributions of neurons.
---

# Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition

## Quick Facts
- arXiv ID: 2402.03348
- Source URL: https://arxiv.org/abs/2402.03348
- Reference count: 39
- One-line primary result: SRD achieves superior performance in localization, complexity, faithfulness, and robustness compared to state-of-the-art XAI methods on ImageNet-S50 dataset

## Executive Summary
This paper addresses the lack of faithfulness and robustness in existing explainable AI (XAI) methods by proposing a novel method called Sharing Ratio Decomposition (SRD). SRD uses a vector perspective to analyze the nonlinear interactions between filters and introduces the concept of Pointwise Feature Vectors (PFVs) to represent the intricate contributions of neurons. It also incorporates the observation of Activation-Pattern-Only Prediction (APOP) to consider both active and inactive neurons. The method decomposes PFVs recursively using sharing ratios, enabling high-resolution Effective Receptive Fields (ERFs) at any layer. SRD achieves superior performance in terms of localization, complexity, faithfulness, and robustness compared to state-of-the-art methods on ImageNet-S50 dataset. It also demonstrates enhanced robustness against adversarial attacks.

## Method Summary
SRD proposes a novel approach to explainable AI by decomposing relevance from a vector perspective rather than neuron-by-neuron. The method introduces Pointwise Feature Vectors (PFVs) to represent the combined concept of neurons sharing the same receptive field. SRD uses pre-activation values to capture contributions to both active and inactive neurons, unlike methods that ignore inactive neurons. The method recursively decomposes PFVs using sharing ratios derived from inner products of transformed vectors, enabling high-resolution Effective Receptive Fields (ERFs) at any layer. SRD's backward pass propagates relevance from output using stored ratios, and the final aggregation is a weighted sum of ERFs at the encoder output.

## Key Results
- SRD outperforms state-of-the-art methods on localization metrics (Pointing Game, Attribution Localization) on ImageNet-S50 dataset
- The method achieves superior faithfulness (Fidelity) and robustness (Stability, adversarial robustness) compared to baselines
- SRD demonstrates enhanced performance in terms of complexity (Sparseness) while maintaining high-resolution ERFs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SRD decomposes relevance from a vector perspective rather than neuron-by-neuron, capturing nonlinear interactions between filters.
- Mechanism: The method defines a Pointwise Feature Vector (PFV) along the channel axis of a hidden layer. Relevance is then distributed recursively across PFVs using sharing ratios derived from inner products of the transformed vectors.
- Core assumption: A PFV—a vector of neurons sharing the same receptive field—better represents the combined concept than individual neurons.
- Evidence anchors:
  - [abstract] "we adopt a vector perspective to consider the intricate nonlinear interactions between filters"
  - [section] "we introduce the pointwise feature vector (PFV), which is a vector along a channel axis of a hidden layer, amalgamating neurons that share the same receptive field"
  - [corpus] Weak; no similar vector-decomposition work found
- Break condition: If the nonlinear assumption fails, the sharing ratio loses interpretability and relevance becomes unreliable.

### Mechanism 2
- Claim: By using pre-activation values, SRD captures contributions to both active and inactive neurons, unlike methods that ignore inactive neurons.
- Mechanism: Relevance is calculated before nonlinear activation (e.g., ReLU), preserving information about inactive neurons via their pre-activation inputs. The sharing ratio formula normalizes by the magnitude of the pre-activation vector.
- Core assumption: Inactive neurons contain meaningful signal for the model's inference, which is lost after activation.
- Evidence anchors:
  - [abstract] "we introduce an interesting observation termed Activation-Pattern-Only Prediction (APOP), letting us emphasize the importance of inactive neurons"
  - [section] "we alter the conventional way of calculating relevance based on post-activation values into the one based on pre-activation values"
  - [corpus] Weak; no other work explicitly uses pre-activation relevance for inactive neurons
- Break condition: If inactive neurons truly carry no signal, the extra computation and complexity provide no benefit.

### Mechanism 3
- Claim: SRD's recursive decomposition yields a high-resolution Effective Receptive Field (ERF) that faithfully reflects pixel contributions.
- Mechanism: Sharing ratios are computed at each layer, and ERFs are recursively built by weighted summation of prior ERFs. This allows layer-wise attribution with fine spatial detail.
- Core assumption: The sharing ratio accurately captures the contribution of a PFV in one layer to a PFV in the next.
- Evidence anchors:
  - [abstract] "Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), providing a high-resolution Effective Receptive Field (ERF) at any layer"
  - [section] "X^k j = f(V^l j ) = sum_i f^l i→j(v^l i) = sum_i ^v^l i→j"
  - [corpus] Weak; no other work combines ERF with recursive relevance decomposition in this way
- Break condition: If the recursive weighting accumulates errors or the sharing ratio becomes noisy, ERF resolution and faithfulness degrade.

## Foundational Learning

- Concept: Vector-based attribution
  - Why needed here: Captures interactions between neurons that scalar methods miss, improving faithfulness.
  - Quick check question: If you have a tensor of shape (C, H, W), what is the shape of a PFV and why?
- Concept: Pre-activation relevance propagation
  - Why needed here: Preserves information from inactive neurons, enabling explanation of patterns before ReLU.
  - Quick check question: How does computing relevance before ReLU differ from after ReLU in terms of signal preservation?
- Concept: Recursive decomposition with sharing ratios
  - Why needed here: Allows layer-by-layer attribution building, producing high-resolution ERFs.
  - Quick check question: What is the base case for ERF construction and why?

## Architecture Onboarding

- Component map:
  - PFV extractor: groups neurons along channel axis for each spatial location
  - Sharing ratio calculator: computes inner products of transformed vectors
  - Recursive ERF builder: accumulates ERFs across layers
  - Relevance distributor: propagates relevance backward from output
  - Class-discriminative modifier: adjusts sharing ratios by subtracting mean relevance
- Critical path:
  1. Forward pass: build PFVs, compute pre-activation values, store sharing ratios
  2. Backward pass: propagate relevance from output using stored ratios
  3. Final aggregation: weighted sum of ERFs at encoder output
- Design tradeoffs:
  - Use pre-activation values for completeness but risk higher variance
  - Recursive decomposition increases spatial resolution but may amplify noise
  - Vector perspective is richer but computationally heavier than scalar
- Failure signatures:
  - Noisy or sparse attribution maps → check sharing ratio calculation
  - Low fidelity scores → verify pre-activation relevance propagation
  - Stability drop under perturbation → inspect ERF accumulation
- First 3 experiments:
  1. Replace PFV with scalar neuron attribution and compare noise levels
  2. Swap pre-activation for post-activation relevance and observe inactive neuron loss
  3. Remove recursive ERF accumulation and test spatial resolution impact

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical foundations of vector decomposition and pre-activation relevance lack direct empirical validation
- The method's generalization to non-ImageNet datasets and architectures beyond ResNet50/VGG16 is not demonstrated
- The stability of sharing ratios across different architectures and data distributions is not thoroughly tested

## Confidence
- **High**: Performance claims on ImageNet-S50 dataset
- **Medium**: Theoretical foundations of vector decomposition and pre-activation relevance
- **Medium**: Robustness claims against adversarial attacks
- **Low**: Generalization to non-ImageNet datasets and architectures

## Next Checks
1. **Ablation on PFV vs. scalar attribution**: Replace PFV with scalar neuron attribution in SRD and measure changes in noise levels and fidelity scores
2. **Pre-activation significance test**: Compare SRD with a variant that uses post-activation values only, isolating the contribution of inactive neurons
3. **Cross-architecture robustness**: Test SRD on architectures beyond ResNet50/VGG16 (e.g., EfficientNet, Vision Transformers) to assess generalization of sharing ratio stability