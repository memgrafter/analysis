---
ver: rpa2
title: 'RadFlag: A Black-Box Hallucination Detection Method for Medical Vision Language
  Models'
arxiv_id: '2411.00299'
source_url: https://arxiv.org/abs/2411.00299
tags:
- report
- reports
- flagged
- sentences
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RadFlag is a black-box method for detecting hallucinations in AI-generated
  radiology reports by sampling multiple reports at different temperatures and using
  an LLM to assess consistency. For each sentence in a candidate report, it counts
  how many high-temperature samples support it and flags sentences with low support
  as likely hallucinations.
---

# RadFlag: A Black-Box Hallucination Detection Method for Medical Vision Language Models

## Quick Facts
- arXiv ID: 2411.00299
- Source URL: https://arxiv.org/abs/2411.00299
- Authors: Serena Zhang; Sraavya Sambara; Oishi Banerjee; Julian Acosta; L. John Fahrner; Pranav Rajpurkar
- Reference count: 14
- Key outcome: RadFlag achieves 73% precision for detecting hallucinatory sentences in Medversa and 71% in RaDialog using temperature-based sampling and LLM consistency checking

## Executive Summary
RadFlag is a black-box method for detecting hallucinations in AI-generated radiology reports without requiring access to the underlying model's internal components. The method generates multiple reports at different temperatures, uses an LLM to assess sentence-level consistency across samples, and flags sentences with low support as likely hallucinations. It achieves high precision (73% for Medversa, 71% for RaDialog) while also identifying problematic reports with high hallucination rates through selective prediction.

## Method Summary
RadFlag works by first generating a candidate report at low temperature (0.1) and multiple reports at high temperatures (0.5 and 5.0) from the same input image. An LLM (GPT-4) is used to compute entailment scores comparing each sentence in the candidate report against sentences in the high-temperature samples. Sentences with low support (few entailments) are flagged as hallucinatory. The method employs conformal risk control to calibrate sentence-level thresholds and aggregates results to identify problematic reports at the report level.

## Key Results
- Achieves 73% precision when flagging hallucinatory sentences in Medversa-generated reports
- Achieves 71% precision when flagging hallucinatory sentences in RaDialog-generated reports
- Demonstrates strong correlation between number of flagged sentences and true hallucinations per report
- Shows that selective rejection of reports with excessive hallucinations improves overall report quality

## Why This Works (Mechanism)

### Mechanism 1
- Low-temperature sampling captures the model's "best guess" while high-temperature sampling reveals model uncertainty
- Core assumption: Temperature parameter meaningfully controls output variability and confidence
- Evidence: Chosen temperatures (0.1, 0.5, 5.0) result in diverse yet stylistically coherent generations

### Mechanism 2
- Conformal risk control thresholding provides statistical guarantees on false positive rates
- Core assumption: Calibration dataset is representative and entailment labeling is accurate
- Evidence: CRC framework controls how often factual sentences are incorrectly flagged

### Mechanism 3
- Report-level selective prediction identifies problematic reports with high hallucination rates
- Core assumption: Reports with many hallucinated sentences also have many flagged sentences
- Evidence: Majority of test reports fall above the ideal y = x line for flagged vs true hallucinations

## Foundational Learning

- Concept: Temperature scaling in language models
  - Why needed: Understanding how temperature affects model output diversity is crucial for interpreting the sampling strategy
  - Quick check: What happens to the distribution of generated text as temperature increases from 0.1 to 1.0?

- Concept: Conformal prediction and risk control
  - Why needed: The method uses CRC to set thresholds with statistical guarantees
  - Quick check: How does conformal risk control differ from traditional conformal prediction in terms of guarantees?

- Concept: Entailment scoring for medical text
  - Why needed: The method uses custom entailment function tailored for radiology reports
  - Quick check: Why does the method count "partially entailed" sentences as entailed when comparing against high-temperature samples?

## Architecture Onboarding

- Component map: Report generation (low/high temperature) -> Entailment scoring (GPT-4) -> Thresholding (CRC) -> Flagging (sentence/report level)

- Critical path: 1) Generate candidate report at low temperature, 2) Generate corpus at high temperature, 3) Tokenize candidate into sentences, 4) Compute entailment scores, 5) Apply sentence-level threshold, 6) Aggregate flagged sentences per report, 7) Apply report-level threshold, 8) Output flagged sentences and reports

- Design tradeoffs: Trades recall for precision using conservative thresholds; black-box approach requires no model access but needs multiple generations

- Failure signatures: High false positive rates indicate aggressive thresholds; low recall suggests sampling doesn't capture enough uncertainty; poor correlation breaks report-level reliability

- First 3 experiments:
  1. Test entailment scoring accuracy: Compare GPT-4 assessments against clinician labels on 50 sentences
  2. Calibrate thresholds: Run CRC framework with different Î± values (0.01, 0.02, 0.05) to observe precision-recall tradeoff
  3. Validate report-level correlation: Manually count true hallucinations and compare against flagged sentences for report subset

## Open Questions the Paper Calls Out

- How does RadFlag's effectiveness vary across different radiology report generation models beyond Medversa and RaDialog?
- How do category-specific thresholds improve precision compared to single global threshold?
- What is the impact of RadFlag on clinical decision-making when integrated into real-world radiology workflows?

## Limitations

- Heavy reliance on GPT-4 entailment function with unspecified prompt engineering and few-shot examples
- Temperature-based sampling assumes monotonic relationship between temperature and model confidence
- Limited evaluation to two specific models (Medversa and RaDialog) without broader validation

## Confidence

**High Confidence**: Overall framework using temperature sampling and LLM consistency checking is methodologically sound
**Medium Confidence**: Specific numerical results are likely accurate but generalizability to other models is uncertain
**Low Confidence**: Theoretical guarantees depend heavily on black-box components (calibration dataset, entailment function)

## Next Checks

1. Manually label 100 sentences from candidate reports with clinician assessments and compare against GPT-4's entailment scores to measure agreement rates
2. Systematically vary high-temperature sampling parameters (0.1, 0.5, 1.0, 2.0, 5.0) and measure impact on precision and recall
3. Apply RadFlag to a third radiology report generation model (e.g., RadFM) and compare performance metrics across different architectures