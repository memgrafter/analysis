---
ver: rpa2
title: Accelerating Transformer Pre-training with 2:4 Sparsity
arxiv_id: '2404.01847'
source_url: https://arxiv.org/abs/2404.01847
tags:
- training
- dense
- sparse
- sparsity
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents a method to accelerate transformer pre-training
  using 2:4 structured sparsity, which can theoretically double the speed of matrix
  multiplications on NVIDIA Ampere GPUs. The authors propose three techniques to preserve
  accuracy: a masked decay regularization applied to gradients, a method to determine
  an appropriate decay factor, and a dense fine-tuning stage at the end of training.'
---

# Accelerating Transformer Pre-training with 2:4 Sparsity

## Quick Facts
- **arXiv ID:** 2404.01847
- **Source URL:** https://arxiv.org/abs/2404.01847
- **Authors:** Yuezhou Hu; Kang Zhao; Weiyu Huang; Jianfei Chen; Jun Zhu
- **Reference count:** 12
- **Primary result:** Achieves up to 1.2x end-to-end acceleration on RTX3090 GPUs for transformer pre-training using 2:4 sparsity without accuracy loss

## Executive Summary
This paper presents a method to accelerate transformer pre-training using 2:4 structured sparsity, which can theoretically double the speed of matrix multiplications on NVIDIA Ampere GPUs. The authors propose three techniques to preserve accuracy: a masked decay regularization applied to gradients, a method to determine an appropriate decay factor, and a dense fine-tuning stage at the end of training. They also introduce two acceleration techniques: a faster method to compute transposable 2:4 masks using convolution, and an optimized implementation of gated activation functions to reduce GPU cache misses. Experiments show that their approach achieves similar convergence to dense training on various transformer models (BERT, GPT-2, DeiT, Transformer-base) while providing up to 1.2x end-to-end acceleration on RTX3090 GPUs without accuracy loss. This is the first reported end-to-end acceleration of transformer pre-training using 2:4 sparsity.

## Method Summary
The method leverages 2:4 structured sparsity to accelerate matrix multiplications on NVIDIA Ampere GPUs, theoretically doubling the speed of these operations. To preserve accuracy during sparse training, the authors propose three techniques: applying masked decay regularization to gradients to suppress frequent mask changes, quickly determining a feasible decay factor during the warm-up stage, and performing dense fine-tuning at the end of training. They also introduce two acceleration techniques: a faster method to compute transposable 2:4 masks using convolution, and an optimized implementation of gated activation functions to reduce GPU cache misses. The approach is evaluated on various transformer models including BERT, GPT-2, DeiT, and Transformer-base, showing comparable accuracy to dense training while achieving up to 1.2x end-to-end acceleration on RTX3090 GPUs.

## Key Results
- Achieves up to 1.2x end-to-end acceleration on RTX3090 GPUs for transformer pre-training using 2:4 sparsity
- Maintains similar convergence to dense training on BERT, GPT-2, DeiT, and Transformer-base models
- First reported end-to-end acceleration of transformer pre-training using 2:4 sparsity without accuracy loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 2:4 structured sparsity can theoretically double the speed of matrix multiplications on NVIDIA Ampere GPUs by utilizing sparse tensor cores.
- Mechanism: The paper leverages the hardware capability of NVIDIA Ampere GPUs where a 2:4 sparse matrix multiplication (2:4-spMM) can be executed twice as fast as its dense equivalent within a tensor core. This is achieved by ensuring that every group of four consecutive elements in the sparse matrix contains exactly two zeros, allowing for more efficient computation.
- Core assumption: The 2:4 sparsity pattern is consistently maintained and the GPU sparse tensor cores are fully utilized during the matrix multiplications.
- Evidence anchors:
  - [abstract] "NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent."
  - [section] "With such sparsity, a GEMM C = AB can be accelerated by 2x with the 2:4-spMM kernel if either A is row-wise 2:4 sparse, or B is column-wise 2:4 sparse."

### Mechanism 2
- Claim: The flip rate metric is used to monitor and ensure the stability of the 2:4 training process, preventing accuracy loss.
- Mechanism: The flip rate measures how frequently the mask vector changes after one optimizer step. A healthy training process is indicated by the flip rate initially rising and then gradually declining to zero. Techniques such as masked decay and dense fine-tuning are proposed to control the flip rate and maintain training stability.
- Core assumption: The flip rate is a reliable indicator of training stability and that controlling it will prevent accuracy loss.
- Evidence anchors:
  - [abstract] "We define a 'flip rate' to monitor the stability of a 2:4 training process."
  - [section] "Suppose wt is a D-dimensional weight vector at time t, and the flip rate rt is defined as the change in proportion of the mask vector after an optimizer step."

### Mechanism 3
- Claim: The proposed techniques for masked decay, fast decay factor determination, and dense fine-tuning preserve accuracy during 2:4 sparse training.
- Mechanism: Masked decay applies regularization to gradients to suppress frequent mask changes. Fast decay factor determination quickly identifies a feasible decay factor during the warm-up stage. Dense fine-tuning at the end of training enhances model quality by allowing precise weight updates.
- Core assumption: These techniques effectively balance the exploration of different connection modes with the convergence of the training process.
- Evidence anchors:
  - [abstract] "Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training."
  - [section] "In this section all methods we propose involve our ultimate principle: the peak of the curve should be sufficiently high to fully explore different connection modes, and the tail should be sufficiently low for the optimization process to converge."

## Foundational Learning

- Concept: 2:4 structured sparsity
  - Why needed here: Understanding the 2:4 sparsity pattern is crucial for leveraging the hardware capabilities of NVIDIA Ampere GPUs to accelerate matrix multiplications.
  - Quick check question: What is the definition of a 2:4 sparse matrix, and how does it enable faster computation on NVIDIA Ampere GPUs?

- Concept: Flip rate
  - Why needed here: The flip rate metric is used to monitor the stability of the training process, which is essential for maintaining accuracy during 2:4 sparse training.
  - Quick check question: How is the flip rate defined, and what does it indicate about the stability of the training process?

- Concept: Sparse-refined straight-through estimator (SR-STE)
  - Why needed here: SR-STE is a technique used to estimate gradients in sparse networks, which is important for optimizing the training process while maintaining accuracy.
  - Quick check question: What is the difference between SR-STE and the standard straight-through estimator, and why is it used in sparse training?

## Architecture Onboarding

- Component map:
  - Feed-forward networks (FFNs) in transformers -> 2:4 sparse matrix multiplications (2:4-spMM) -> Transposable masks for pruning -> Masked decay regularization -> Dense fine-tuning procedure -> Gated activation functions (SwiGLU, GEGLU) -> Minimum-variance unbiased estimator (MVUE) for pruning gradients

- Critical path:
  1. Compute transposable 2:4 masks using convolution.
  2. Apply masked decay on gradients to control flip rate.
  3. Determine feasible decay factor during warm-up stage.
  4. Perform dense fine-tuning at the end of training.
  5. Accelerate gated activation functions by reducing GPU L2 cache misses.
  6. Utilize 2:4-spMM for matrix multiplications in FFNs.

- Design tradeoffs:
  - Balancing the exploration of different connection modes with the convergence of the training process.
  - Choosing between applying masked decay on weights versus gradients.
  - Deciding the position of the switch point for dense fine-tuning.
  - Optimizing the frequency of updating transposable masks.

- Failure signatures:
  - Flip rate explosion leading to unstable training and accuracy loss.
  - Ineffective masked decay resulting in frequent mask changes.
  - Inaccurate decay factor determination causing poor convergence.
  - Inefficient computation of transposable masks slowing down training.
  - High GPU L2 cache misses in gated activation functions reducing speed.

- First 3 experiments:
  1. Measure the flip rate during training with different decay factors to identify the optimal value.
  2. Compare the accuracy of models trained with masked decay on weights versus gradients.
  3. Evaluate the impact of dense fine-tuning on model quality by varying the switch point position.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical maximum acceleration ratio achievable for transformer pre-training using 2:4 sparsity, and what architectural factors limit this acceleration?
- Basis in paper: [explicit] The paper reports up to 1.7x acceleration for FFN layers and 1.3x for transformer blocks, with end-to-end acceleration of 1.2x on GPT-2 models.
- Why unresolved: The paper provides empirical results but doesn't fully explore the theoretical limits or identify all architectural bottlenecks preventing higher acceleration.
- What evidence would resolve it: Detailed profiling of all operations in transformer pre-training to identify time-consuming components, combined with theoretical analysis of sparsity patterns and hardware constraints.

### Open Question 2
- Question: How does the performance of 2:4 sparse training scale with different transformer architectures beyond BERT, GPT-2, DeiT, and Transformer-base?
- Basis in paper: [explicit] The paper only tests their method on BERT, GPT-2, DeiT, and Transformer-base architectures.
- Why unresolved: The paper doesn't explore the generalizability of their approach to other transformer variants like ViT, Swin Transformer, or more recent architectures.
- What evidence would resolve it: Experiments applying the 2:4 sparse training method to a diverse set of transformer architectures across different domains (vision, NLP, speech).

### Open Question 3
- Question: What is the impact of 2:4 sparse training on the downstream performance of fine-tuned models, and how does it compare to dense training?
- Basis in paper: [inferred] While the paper reports GLUE scores and other benchmarks, it doesn't provide a comprehensive analysis of downstream task performance across various fine-tuning scenarios.
- Why unresolved: The paper focuses on pre-training performance but doesn't extensively explore how 2:4 sparse pre-training affects the quality of models after fine-tuning on specific tasks.
- What evidence would resolve it: Systematic evaluation of fine-tuned models on a wide range of downstream tasks, comparing performance between 2:4 sparse pre-training and dense pre-training approaches.

## Limitations

- The reported end-to-end acceleration (1.2x) falls short of the theoretical 2x speedup, suggesting practical implementation constraints limit achievable acceleration.
- The approach's effectiveness across different transformer architectures beyond BERT, GPT-2, DeiT, and Transformer-base remains unexplored.
- The generalizability of the flip rate monitoring system and masked decay techniques to different training scenarios and hardware architectures is uncertain.

## Confidence

**High confidence**: The core mechanism of leveraging 2:4 structured sparsity for hardware acceleration on NVIDIA Ampere GPUs is well-established and the experimental results showing comparable accuracy to dense training are robust across multiple model types.

**Medium confidence**: The proposed techniques for masked decay, fast decay factor determination, and dense fine-tuning are theoretically sound and show positive results, but their effectiveness may depend heavily on specific hyperparameter choices and model characteristics that require further validation.

**Low confidence**: The practical scalability of the approach to larger models and different hardware architectures beyond RTX3090 GPUs, as well as the generalizability of the flip rate monitoring system to other training scenarios, remains uncertain.

## Next Checks

1. **Ablation study of decay strategies**: Systematically compare the impact of applying masked decay on weights versus gradients across different model sizes and training durations to determine which approach provides better accuracy preservation with minimal overhead.

2. **Flip rate correlation analysis**: Conduct controlled experiments varying the decay factor and monitoring flip rates across different training stages to establish whether flip rate is indeed a reliable predictor of training stability and whether the proposed techniques consistently maintain healthy flip rate patterns.

3. **Cross-architecture scalability test**: Implement and evaluate the 2:4 sparse training approach on different GPU architectures (e.g., AMD GPUs, NVIDIA Hopper) and CPU-based systems to assess the portability of the acceleration techniques and identify any hardware-specific optimizations needed.