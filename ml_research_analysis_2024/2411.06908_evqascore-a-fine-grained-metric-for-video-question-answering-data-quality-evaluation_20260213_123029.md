---
ver: rpa2
title: 'EVQAScore: A Fine-grained Metric for Video Question Answering Data Quality
  Evaluation'
arxiv_id: '2411.06908'
source_url: https://arxiv.org/abs/2411.06908
tags:
- evqascore
- data
- video
- pac-s
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EVQAScore, a novel method for evaluating
  video question-answering (VideoQA) data quality for training video large language
  models (VideoLLMs). Existing methods like PAC-S and EMScore are limited to video-caption
  evaluation and struggle with long videos due to the 77-word constraint of CLIP.
---

# EVQAScore: A Fine-grained Metric for Video Question Answering Data Quality Evaluation

## Quick Facts
- **arXiv ID**: 2411.06908
- **Source URL**: https://arxiv.org/abs/2411.06908
- **Reference count**: 39
- **Key outcome**: EVQAScore achieves state-of-the-art results on VATEX-EVAL benchmark (28.9 Kendall correlation, 37.4 Spearman correlation) and enables SOTA VideoQA results with only 12.5% of original data volume.

## Executive Summary
This paper introduces EVQAScore, a novel method for evaluating video question-answering (VideoQA) data quality for training video large language models (VideoLLMs). Existing methods like PAC-S and EMScore are limited to video-caption evaluation and struggle with long videos due to the 77-word constraint of CLIP. EVQAScore overcomes these limitations by employing keyword extraction via LLMs for improved semantic understanding and YOLO for fine-grained object detection within video frames. Frame sampling further enhances efficiency, enabling evaluation of extremely long videos without sacrificing performance.

## Method Summary
EVQAScore combines coarse-grained and fine-grained scoring using keyword extraction via LLMs and YOLO-based object detection, with uniform frame sampling to handle long videos efficiently. The method extracts key objects from sampled video frames using YOLO and keywords from question-answer pairs using an LLM, then computes scores by aligning video embeddings with text embeddings and matching extracted objects with keywords. The combined scores evaluate data quality, filter noisy VideoQA data, and enable training of VideoLLMs on the selected data to assess performance improvements.

## Key Results
- Achieves 28.9 Kendall correlation and 37.4 Spearman correlation on VATEX-EVAL benchmark for video-caption evaluation
- Enables state-of-the-art VideoQA results using only 12.5% of original data volume
- Outperforms PAC-S and achieves superior data filtering performance compared to using 100% of the data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EVQAScore improves semantic understanding by extracting keywords using LLMs, enabling evaluation of long captions and QAs beyond CLIP's 77-word limit.
- Mechanism: LLM-based keyword extraction identifies core semantic elements, replacing TF-IDF-based semantic matching limited to word frequency analysis.
- Core assumption: LLM keyword extraction captures contextual semantics more effectively than TF-IDF and aligns with CLIP's embedding space.
- Break condition: If LLM keyword extraction fails to capture critical semantic elements or produces irrelevant keywords, semantic matching would degrade.

### Mechanism 2
- Claim: EVQAScore achieves fine-grained video understanding using YOLO for object detection in sampled frames.
- Mechanism: YOLO extracts key objects from sampled frames, creating patch-based representations matched with textual keywords.
- Core assumption: YOLO reliably identifies meaningful objects semantically relevant to QA or caption, and object features align with CLIP embeddings.
- Break condition: If YOLO fails to detect relevant objects or introduces noise, fine-grained matching becomes unreliable.

### Mechanism 3
- Claim: EVQAScore maintains performance while improving efficiency through uniform frame sampling.
- Mechanism: Regular interval sampling (e.g., every 30 frames) reduces computational cost while preserving essential video content.
- Core assumption: Uniform sampling captures sufficient video content for accurate evaluation without significantly impacting semantic understanding.
- Break condition: If sampling interval is too large and misses critical content, or too small and doesn't provide computational benefits.

## Foundational Learning

- **Cross-modal embedding alignment (CLIP)**: EVQAScore relies on CLIP embeddings to match visual features with textual features. Understanding how CLIP aligns visual and textual representations is fundamental to grasping the evaluation mechanism. Quick check: How does CLIP compute similarity between visual and textual embeddings, and what are the limitations for long sequences?

- **Keyword extraction and semantic representation**: The method uses LLM-based keyword extraction to capture semantic meaning. Understanding how keywords represent semantic content and differ from full-text representations is crucial. Quick check: What are the advantages of using LLM-extracted keywords versus TF-IDF for semantic representation in multimodal tasks?

- **Object detection and feature extraction**: YOLO extracts fine-grained visual features from video frames. Understanding how object detection works and how detected objects can be used for semantic matching is essential. Quick check: How can detected objects be represented as features and matched with textual concepts for semantic evaluation?

## Architecture Onboarding

- **Component map**: Video frames → Frame sampling → YOLO detection → CLIP embedding → Keyword extraction → CLIP embedding → Matching → Scoring
- **Critical path**: Video → Frame sampling → YOLO detection → CLIP embedding → Keyword extraction → CLIP embedding → Matching → Scoring
- **Design tradeoffs**:
  - Frame sampling interval: Larger intervals reduce computation but risk missing important content; smaller intervals increase accuracy but computational cost
  - YOLO model choice: More accurate models provide better object detection but increase computation time
  - LLM choice for keyword extraction: Larger models may provide better keyword extraction but increase latency and cost
  - CLIP model choice: Different CLIP variants may have different strengths for video-text matching
- **Failure signatures**:
  - Low scores despite high-quality data: May indicate issues with keyword extraction or object detection
  - Inconsistent scores across similar data: May indicate sampling issues or embedding alignment problems
  - High computational cost: May indicate need to adjust frame sampling or optimize object detection
- **First 3 experiments**:
  1. Validate frame sampling: Compare EVQAScore with different sampling intervals on a subset of data to find optimal balance between performance and efficiency
  2. Validate keyword extraction: Compare EVQAScore using LLM keywords vs TF-IDF keywords on the same dataset to quantify improvement in semantic understanding
  3. Validate object detection contribution: Compare EVQAScore with and without YOLO object detection on a subset of data to measure impact of fine-grained visual features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EVQAScore perform when evaluating videoQA data from different domains or with varying levels of complexity, such as educational content versus entertainment videos?
- Basis in paper: [inferred] The paper evaluates EVQAScore on ActivityNet, MSRVTT, MSVD, TGIF, MVBench, VideoChatGPT Bench, and VideoChatGPT Bench Diverse, but doesn't explicitly test performance across different video domains.
- Why unresolved: The paper demonstrates effectiveness across multiple benchmarks but doesn't explore domain-specific variations in videoQA data quality.
- What evidence would resolve it: Comparative analysis of EVQAScore performance across diverse video domains, highlighting differences in accuracy and efficiency for different types of content.

### Open Question 2
- Question: What are the limitations of the keyword extraction method used in EVQAScore, and how might these limitations impact evaluation of videoQA data quality in languages other than English?
- Basis in paper: [explicit] The paper uses LLaMA3.1-8B-Instruct for keyword extraction but doesn't discuss performance or limitations in non-English languages.
- Why unresolved: Evaluation is conducted primarily on English datasets, and the paper doesn't address adaptability to other languages.
- What evidence would resolve it: Testing EVQAScore on multilingual videoQA datasets to assess keyword extraction effectiveness and overall metric performance across different languages.

### Open Question 3
- Question: How does the frame sampling interval (l) affect computational efficiency and accuracy of EVQAScore, and is there an optimal interval that balances these factors for different video lengths?
- Basis in paper: [explicit] The paper mentions sampling interval of l = 30 reduces computational costs without affecting results, but doesn't explore impact of different intervals on various video lengths.
- Why unresolved: The study focuses on a specific sampling interval without investigating how different intervals might influence balance between efficiency and accuracy for videos of varying lengths.
- What evidence would resolve it: Comprehensive analysis of EVQAScore performance across different sampling intervals and video lengths, identifying optimal interval for balancing computational efficiency and accuracy.

## Limitations
- The paper lacks direct comparison of LLM-based keyword extraction against TF-IDF on the same dataset, making semantic improvement difficult to verify independently
- Specific YOLO model checkpoint and configuration details are not specified, which could significantly impact object detection quality and downstream evaluation performance
- The 30-frame sampling interval is presented as optimal without systematic analysis of how different intervals affect correlation metrics

## Confidence

- **High confidence**: The method architecture and overall evaluation approach are clearly specified and technically sound
- **Medium confidence**: The reported correlation metrics (28.9 Kendall, 37.4 Spearman) on VATEX-EVAL, though these are relative improvements over PAC-S++
- **Low confidence**: The claim that EVQAScore achieves SOTA results with only 12.5% of original data volume, as the paper doesn't provide ablation studies showing how much each component contributes to this efficiency

## Next Checks
1. Conduct ablation study comparing EVQAScore with TF-IDF keywords vs LLM keywords on identical datasets to quantify the actual semantic improvement contribution
2. Test EVQAScore performance across different frame sampling intervals (10, 30, 60, 120 frames) to verify the claimed optimal interval and establish sensitivity to this hyperparameter
3. Validate the 12.5% data volume claim by training VideoLLMs on progressively filtered datasets (25%, 12.5%, 6.25%) using EVQAScore to determine the minimum effective data threshold