---
ver: rpa2
title: Benchmark for CEC 2024 Competition on Multiparty Multiobjective Optimization
arxiv_id: '2402.02033'
source_url: https://arxiv.org/abs/2402.02033
tags:
- problems
- multiobjective
- optimization
- pareto
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a benchmark suite for the CEC 2024 competition
  on Multiparty Multiobjective Optimization (MPMOPs), which involve multiple decision
  makers with conflicting objectives. The suite consists of two parts: 11 MPMOPs with
  common Pareto optimal solutions and six Biparty Multiobjective UAV Path Planning
  (BPMO-UAVPP) problems with unknown solutions.'
---

# Benchmark for CEC 2024 Competition on Multiparty Multiobjective Optimization

## Quick Facts
- **arXiv ID**: 2402.02033
- **Source URL**: https://arxiv.org/abs/2402.02033
- **Reference count**: 23
- **Primary result**: Benchmark suite with 11 MPMOPs with known solutions and 6 UAV path planning problems with unknown solutions, evaluated using MPIGD and MPHV metrics

## Executive Summary
This paper introduces a benchmark suite for the CEC 2024 competition on Multiparty Multiobjective Optimization (MPMOPs), addressing the need for standardized test problems in this emerging field. The suite consists of two distinct parts: the first includes 11 MPMOPs with common Pareto optimal solutions, while the second comprises six biparty multiobjective UAV path planning problems with unknown solutions. The benchmark aims to promote the development of tailored modeling approaches for MPMOPs by providing a standardized set of problems and evaluation metrics. Algorithms will be evaluated using Multiparty Inverted Generational Distance (MPIGD) for the first part and Multiparty Hypervolume (MPHV) for the second part, with results aggregated across 30 runs.

## Method Summary
The benchmark suite consists of two parts: 11 MPMOPs with common Pareto optimal solutions constructed using six basic functions (BF1-BF6) as building blocks, and six biparty multiobjective UAV path planning problems. The first set uses dimensions 10, 30, and 50, while the second set employs fixed high dimensions (88) with complex constraints. Algorithms are evaluated using MPIGD for the first part, measuring convergence to known Pareto fronts, and MPHV for the second part, assessing hypervolume coverage without requiring known optimal solutions. The maximum fitness evaluations are set to 1000·D·M for the first set and 100000 for the second set, with algorithms run across 30 runs using fixed seeds 1-30.

## Key Results
- The benchmark suite provides 11 MPMOPs with common Pareto optimal solutions and 6 UAV path planning problems
- MPIGD and MPHV are introduced as specialized metrics for evaluating algorithm performance on MPMOPs
- Results will be submitted in a table format showing best, median, worst, mean, and standard deviation across 30 runs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark suite evaluates algorithm performance across both known and unknown Pareto optimal solution scenarios
- Mechanism: The first part uses MPIGD to measure convergence to known Pareto fronts, while the second part uses MPHV to assess hypervolume coverage without requiring known optimal solutions
- Core assumption: MPIGD provides a meaningful distance metric when true Pareto fronts are known, and MPHV effectively ranks algorithms when true fronts are unknown
- Evidence anchors:
  - [abstract] "The algorithms are evaluated using Multiparty Inverted Generational Distance (MPIGD) for the first part and Multiparty Hypervolume (MPHV) for the second part"
  - [section] "MPIGD(P M P,P ) = ∑v∈ P M Pd(v,P )/|P M P|, d(v,P ) = min s∈ P (∑Mj=1 √(vj1 − sj1)2 + · · ·+ (vjmj − sjmj )2)"

### Mechanism 2
- Claim: The UAV path planning problems capture real-world trade-offs between efficiency and safety
- Mechanism: Efficiency objectives minimize path length, energy, height changes, and hover distances, while safety objectives minimize fatality risk, property risk, and noise pollution
- Core assumption: The mathematical formulations for each objective accurately reflect real-world costs and constraints
- Evidence anchors:
  - [section] "For the efficiency decision-maker, maximizing UAV flight efficiency is paramount. This objective entails minimizing various factors, including UAV path length, height changes, flight energy consumption, and the total sum of mission hover point distances"
  - [section] "The primary concern for the safety decision-maker is to minimize the risk posed to urban residents and property by UAV operations"

### Mechanism 3
- Claim: The benchmark problems scale in difficulty through dimension and problem structure
- Mechanism: The first suite uses dimensions 10, 30, 50, while the second suite uses fixed high dimensions (88) with complex constraints and multiple conflicting objectives
- Core assumption: Scaling dimensions and adding constraints proportionally increases algorithmic difficulty
- Evidence anchors:
  - [section] "Dimension: The dimensions of the decision variables for the first set of benchmark suites include 10, 30, 50"
  - [section] "Terrain Constraints: The UAV can only fly within a certain altitude range, Hmin ≤ zi ≤ Hmax"

## Foundational Learning

- Concept: Multiobjective optimization with conflicting objectives
  - Why needed here: MPMOPs inherently involve multiple decision makers with competing objectives
  - Quick check question: What defines a Pareto optimal solution in a multiobjective context?

- Concept: Evolutionary algorithm evaluation metrics
  - Why needed here: MPIGD and MPHV are specialized metrics for assessing algorithm performance on MPMOPs
  - Quick check question: How does MPHV differ from traditional hypervolume metrics?

- Concept: Constraint handling in optimization
  - Why needed here: UAV path planning includes terrain, performance, and safety constraints
  - Quick check question: Why are constraint violations critical in UAV path planning?

## Architecture Onboarding

- Component map: Benchmark suite generator -> Problem instance generator -> Algorithm executor -> Performance metric calculator -> Result aggregator
- Critical path: Generate test instances -> Run algorithms -> Calculate MPIGD/MPHV -> Aggregate results -> Rank algorithms
- Design tradeoffs: Known vs unknown Pareto fronts (MPIGD vs MPHV), computational cost vs accuracy, dimension scaling vs problem complexity
- Failure signatures: Poor algorithm rankings due to inappropriate metric choice, runtime errors from constraint violations, inconsistent results across runs
- First 3 experiments:
  1. Run a simple evolutionary algorithm on E1 with d=10, verify MPIGD calculation
  2. Execute a path planning algorithm on C1, check MPHV computation
  3. Compare algorithm performance across E1-E11 at d=30, analyze ranking stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed MPMOP benchmark problems compare to existing multiobjective optimization benchmarks in terms of solution diversity and convergence behavior?
- Basis in paper: [explicit] The paper states that the benchmark suite aims to promote the development of tailored modeling approaches for MPMOPs by providing a standardized set of problems and evaluation metrics
- Why unresolved: The paper introduces the benchmark suite but does not provide any empirical comparisons with existing benchmarks or analysis of the solution diversity and convergence behavior of the proposed problems
- What evidence would resolve it: Empirical studies comparing the proposed MPMOP benchmarks with existing benchmarks, analyzing solution diversity, convergence behavior, and computational efficiency across various algorithms

### Open Question 2
- Question: How does the performance of evolutionary algorithms on MPMOPs with common Pareto optimal solutions compare to those with unknown solutions, such as the UAV path planning problems?
- Basis in paper: [explicit] The benchmark suite consists of two parts: problems with common Pareto optimal solutions and Biparty Multiobjective UAV Path Planning (BPMO-UAVPP) problems with unknown solutions. The algorithms are evaluated using different metrics (MPIGD and MPHV) for each part
- Why unresolved: The paper introduces the two parts of the benchmark suite but does not provide any comparative analysis of the performance of evolutionary algorithms on problems with known versus unknown solutions
- What evidence would resolve it: Comparative studies of evolutionary algorithm performance on MPMOPs with common Pareto optimal solutions and those with unknown solutions, using appropriate metrics and problem instances

### Open Question 3
- Question: What are the computational requirements and scalability limitations of the proposed MPMOP benchmark suite?
- Basis in paper: [inferred] The paper introduces the benchmark suite but does not provide any information on the computational requirements or scalability limitations of the proposed problems
- Why unresolved: The paper focuses on introducing the benchmark suite and evaluation metrics but does not address the computational aspects or scalability of the proposed problems
- What evidence would resolve it: Studies analyzing the computational requirements, memory usage, and scalability of the proposed MPMOP benchmark suite, considering various problem sizes, dimensions, and algorithm configurations

## Limitations

- Absence of verified optimal solutions for UAV path planning problems limits evaluation to relative comparisons
- Insufficient detail on normalization procedures for MPHV calculations may affect cross-problem comparability
- Time-varying nature of basic functions requires careful implementation to ensure temporal consistency

## Confidence

- **High Confidence**: The mathematical formulations of MPIGD and MPHV metrics
- **Medium Confidence**: The UAV path planning problem specifications
- **Medium Confidence**: The difficulty scaling through dimensions and problem structure

## Next Checks

1. **Implementation Verification**: Implement and test BF1-BF6 basic functions independently to verify they produce expected Pareto optimal solutions before constructing the full benchmark suite

2. **Metric Validation**: Run controlled experiments comparing MPIGD and MPHV calculations across multiple algorithms on simplified versions of the problems to ensure metrics behave as expected

3. **Constraint Handling Assessment**: Test algorithm performance on C1-C6 problems with varying constraint violation penalties to determine if the current constraint formulations appropriately balance efficiency and safety objectives