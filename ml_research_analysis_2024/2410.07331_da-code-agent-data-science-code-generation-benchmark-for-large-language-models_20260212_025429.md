---
ver: rpa2
title: 'DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models'
arxiv_id: '2410.07331'
source_url: https://arxiv.org/abs/2410.07331
tags:
- data
- tasks
- file
- code
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DA-Code is a benchmark for evaluating large language models on
  complex, agent-based data science tasks. It includes 500 real-world examples spanning
  data wrangling, machine learning, and exploratory data analysis, requiring advanced
  grounding and planning skills.
---

# DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2410.07331
- Source URL: https://arxiv.org/abs/2410.07331
- Reference count: 40
- Primary result: 30.5% accuracy baseline for LLM data science agents on real-world tasks

## Executive Summary
DA-Code is a comprehensive benchmark designed to evaluate large language models on complex, agent-based data science tasks. The benchmark includes 500 real-world examples spanning data wrangling, machine learning, and exploratory data analysis, requiring advanced grounding and planning skills. Tasks involve diverse data types and complex programming in Python, SQL, and Bash. The authors develop DA-Agent, a baseline framework operating in a controllable executable environment, and demonstrate that even the best LLMs achieve only 30.5% accuracy, highlighting the significant challenges in autonomous data science.

## Method Summary
DA-Code was constructed by collecting 500 real-world data science examples from diverse sources, including public repositories and domain experts. Each task requires multi-step reasoning across data wrangling, machine learning, and exploratory data analysis. The benchmark uses synthetic data generation to create controlled test environments while maintaining realistic task complexity. The DA-Agent baseline framework implements a three-stage pipeline: task decomposition, code generation, and execution with feedback loops. Tasks are evaluated based on whether generated code successfully completes the specified data science objective when executed in the benchmark environment.

## Key Results
- DA-Agent baseline achieves 30.5% accuracy on DA-Code benchmark
- Tasks require sophisticated multi-step reasoning across data manipulation, analysis, and ML
- Performance varies significantly across task types, with data wrangling being most challenging
- Current LLMs show substantial limitations in autonomous data science capabilities

## Why This Works (Mechanism)
DA-Code works by creating a realistic testing ground that combines multiple data science skills in complex, interconnected tasks. The benchmark's strength lies in requiring models to handle diverse data types, integrate multiple programming languages, and perform sophisticated reasoning about data transformations and analysis pipelines. The controlled environment allows for precise evaluation of model capabilities while maintaining task authenticity.

## Foundational Learning
1. **Data wrangling fundamentals** - Why needed: Most data science tasks begin with data cleaning and transformation; quick check: Can model handle missing values, type conversions, and joins?
2. **Machine learning pipeline construction** - Why needed: Models must build and evaluate predictive systems; quick check: Can model select appropriate algorithms and perform feature engineering?
3. **Exploratory data analysis** - Why needed: Understanding data distributions and relationships is crucial; quick check: Can model generate appropriate visualizations and summary statistics?
4. **Multi-language programming** - Why needed: Real-world data science uses multiple tools; quick check: Can model seamlessly integrate Python, SQL, and Bash?
5. **Task planning and decomposition** - Why needed: Complex tasks require breaking down into executable steps; quick check: Can model create logical progression of sub-tasks?
6. **Error handling and debugging** - Why needed: Real execution environments produce failures; quick check: Can model identify and fix runtime errors?

## Architecture Onboarding

Component map: Task Parser -> Planning Module -> Code Generator -> Execution Environment -> Feedback Loop

Critical path: The planning module must accurately decompose tasks into executable sub-steps, as errors here cascade through the entire pipeline and prevent successful completion regardless of code quality.

Design tradeoffs: The benchmark prioritizes task complexity and realism over execution speed, accepting longer evaluation times to capture authentic data science challenges. This means each task takes several minutes to evaluate, limiting large-scale experimentation.

Failure signatures: Common failure modes include incorrect task decomposition, syntax errors in generated code, and logical errors in data manipulation steps. Models often struggle with edge cases in data cleaning and fail to implement appropriate error handling.

First experiments:
1. Run DA-Agent on a subset of 50 tasks to establish baseline performance metrics
2. Test with zero-shot prompting to assess inherent model capabilities without fine-tuning
3. Evaluate individual task categories (wrangling vs. ML vs. EDA) to identify specific weakness areas

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic and semi-synthetic data generation may not capture full real-world complexity
- Focus on specific programming languages (Python, SQL, Bash) may not represent complete data science ecosystem
- 30.5% accuracy baseline based on single implementation may not reflect full potential of alternative approaches

## Confidence
- **High confidence**: Benchmark construction methodology and dataset collection process are well-documented and reproducible
- **Medium confidence**: Claim that DA-Code represents "real-world" data science tasks, given synthetic data generation approach
- **Medium confidence**: Assertion that 30.5% accuracy represents meaningful difficulty threshold for autonomous data science agents

## Next Checks
1. Conduct external validation using independent LLMs and agent architectures not involved in original benchmark development to verify 30.5% accuracy baseline is representative
2. Test benchmark with real-world datasets (rather than synthetic data) to assess whether performance degradation aligns with expectations for authentic data science complexity
3. Evaluate cross-linguistic performance by extending tasks to include R, Julia, and other data science programming languages to determine if benchmark has language-specific biases