---
ver: rpa2
title: How Robust Are Energy-Based Models Trained With Equilibrium Propagation?
arxiv_id: '2401.11543'
source_url: https://arxiv.org/abs/2401.11543
tags:
- pred
- attacks
- adversarial
- accuracy
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the robustness of Energy-Based Models (EBMs)
  trained with Equilibrium Propagation (EP) to adversarial and natural perturbations.
  The authors adapt existing gradient-based attack methods for use with EBMs and evaluate
  their performance on CIFAR-10 and CIFAR-100 datasets.
---

# How Robust Are Energy-Based Models Trained With Equilibrium Propagation?

## Quick Facts
- arXiv ID: 2401.11543
- Source URL: https://arxiv.org/abs/2401.11543
- Reference count: 40
- Key outcome: EP-CNNs (EBMs trained with EP) show significantly greater robustness to both white-box and black-box adversarial attacks than standard CNNs and ViTs, without requiring adversarial training.

## Executive Summary
This work investigates the robustness of Energy-Based Models (EBMs) trained with Equilibrium Propagation (EP) to adversarial and natural perturbations. The authors adapt existing gradient-based attack methods for use with EBMs and evaluate their performance on CIFAR-10 and CIFAR-100 datasets. Their results demonstrate that EP-CNNs are significantly more robust than standard CNNs and ViTs to both white-box and black-box adversarial attacks, without requiring adversarial training. EP-CNNs also exhibit greater robustness to natural perturbations compared to adversarially-trained models. Notably, the adversarial attacks on EBMs are more semantic than those on other models, suggesting that EBMs learn features that are truly useful for classification tasks.

## Method Summary
The authors investigate EP-CNNs by adapting gradient-based attack methods (PGD, C&W, Square) for use with EBMs. They train EP-CNNs on CIFAR-10 and CIFAR-100 datasets using the symmetric weight update rule. The models are evaluated against various white-box and black-box attacks, as well as natural corruptions from CIFAR-C. The study compares EP-CNN robustness to standard CNNs, ViTs, and adversarially-trained CNNs, focusing on both adversarial and natural perturbation scenarios.

## Key Results
- EP-CNNs demonstrate significantly greater robustness to both white-box and black-box adversarial attacks compared to standard CNNs and ViTs
- EP-CNNs exhibit greater robustness to natural perturbations than adversarially-trained models
- Adversarial attacks on EP-CNNs produce more semantic perturbations, suggesting the models learn features truly useful for classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EP-CNNs learn hierarchical features that are truly useful for classification, leading to semantic adversarial perturbations.
- Mechanism: The recurrent dynamics and feedback connections in EP-CNNs guide the system toward stable attractors. Adversarial perturbations must push the state across decision boundaries in a way that changes the semantic interpretation of the input, rather than exploiting spurious correlations.
- Core assumption: The energy landscape learned by EP-CNNs has meaningful basins of attraction corresponding to real object categories.
- Evidence anchors:
  - [abstract] "Notably, the adversarial attacks on EBMs are more semantic than those on other models, suggesting that EBMs learn features that are truly useful for classification tasks."
  - [section] "the attacked images computed on EP-CNNs have been modified in a meaningful/semantic way (Figure 5)."
  - [corpus] No direct evidence in corpus, but related work on energy-based models and attractor dynamics supports the idea.
- Break condition: If the energy landscape is not properly learned or is too flat, perturbations may not need to be semantic to fool the model.

### Mechanism 2
- Claim: The complex dynamics of EP-CNNs naturally resist small input perturbations by virtue of the convergence process to equilibrium.
- Mechanism: During the free phase, the system evolves toward a steady state. Small perturbations are smoothed out during this evolution, as the dynamics are designed to minimize the energy function over time. This makes the model inherently robust to natural corruptions and small adversarial changes.
- Core assumption: The time evolution of the system effectively filters out noise and minor perturbations before reaching the steady state.
- Evidence anchors:
  - [section] "the complex dynamics showcased in EP... gets rid of small input perturbations in the process of attaining a steady state."
  - [section] "We also observe that the accuracy stays at 10% for the first three timesteps since the trained model we used consists of four layers and the last layer is not updated until the fourth timestep, see Equation 2."
  - [corpus] Related work on deep equilibrium models suggests fixed points can provide robustness, though empirical evidence is mixed.
- Break condition: If perturbations are large enough to significantly alter the trajectory before convergence, or if the convergence time is too short to filter noise.

### Mechanism 3
- Claim: EP-CNNs do not suffer from gradient obfuscation because the gradient-based attacks are exact and the model's robustness is not due to vanishing gradients.
- Mechanism: Since EP-CNNs are differentiable and the gradient can be computed exactly, gradient-based attacks like PGD are valid. The robustness is intrinsic to the model's dynamics, not a byproduct of obfuscated gradients. Evidence includes consistent performance across white-box and black-box attacks.
- Core assumption: The gradients are meaningful and not artificially small due to the recurrent dynamics.
- Evidence anchors:
  - [section] "Checking for gradient obfuscation. Since the AutoAttack implements a black-box attack, and the EP-CNN's performance on AutoAttack (Figure 3a) was similar to that on the PGD attack (Figure 9b), this is one indication that the robustness of EP-CNNs is not due to gradient obfuscation."
  - [section] "To further check for gradient obfuscation, we also performed PGD attacks on EP-CNNs using different numbers of free phase iterations. If the EP-CNN was performing gradient obfuscation, it should be more robust to the PGD attack when using more free phase iterations... However, we observe the opposite trend..."
  - [corpus] No direct evidence in corpus, but the claim is supported by the experimental results.
- Break condition: If the model's dynamics cause gradients to vanish in a way that is not captured by the exact gradient computation, or if other forms of obfuscation are at play.

## Foundational Learning

- Concept: Energy-based models and equilibrium propagation
  - Why needed here: Understanding the core learning framework is essential to grasp why EP-CNNs have different robustness properties than standard DNNs.
  - Quick check question: How does the energy function in EP-CNNs differ from the loss function in standard backpropagation, and what role do feedback connections play?

- Concept: Recurrent dynamics and attractor states
  - Why needed here: The recurrent nature of EP-CNNs and their convergence to attractor states is key to understanding their robustness to perturbations.
  - Quick check question: What happens during the free phase of EP, and how does the system's evolution toward a steady state contribute to robustness?

- Concept: Adversarial attacks and robustness evaluation
  - Why needed here: Knowing the types of attacks (white-box, black-box) and how robustness is measured is crucial for interpreting the results.
  - Quick check question: What is the difference between PGD and C&W attacks, and how do they test different aspects of model robustness?

## Architecture Onboarding

- Component map: Input -> Convolutional layers (s0 to sNconv-1) -> Fully connected layer (sNtot) -> Energy function (Φ) -> Feedback connections between consecutive layers -> Pooling operations (P)

- Critical path:
  1. Input is fed into the network.
  2. The state evolves according to the recurrent dynamics (free phase) until it reaches a steady state.
  3. The state of the last layer is used for classification.
  4. During training, the symmetric weight update rule is applied using both the free phase and nudged phase.

- Design tradeoffs:
  - Recurrent dynamics vs. feedforward: Recurrent models are more robust but slower to train and infer.
  - Exact gradients vs. approximate: Exact gradients are more reliable for attacks but may be computationally expensive.
  - Feedback connections vs. simplicity: Feedback adds robustness but increases model complexity.

- Failure signatures:
  - Poor convergence during the free phase (model fails to reach a steady state).
  - Gradient explosion or vanishing during training or attack.
  - Sensitivity to hyperparameters (learning rate, number of free phase iterations).

- First 3 experiments:
  1. Train an EP-CNN on CIFAR-10 and evaluate clean accuracy to ensure the model is learning.
  2. Perform a PGD attack with small epsilon to see if the model is robust to small perturbations.
  3. Visualize the adversarial perturbations to check if they are semantic (as claimed).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the uncertainty exponent α in EP-CNNs compare quantitatively to that of standard CNNs and adversarially-trained CNNs?
- Basis in paper: [explicit] The paper hypothesizes that learning and inference through local rules and feedback connections leads to a larger uncertainty exponent, which provides robustness to natural corruptions and adversarial perturbations.
- Why unresolved: The paper only mentions this hypothesis but does not provide any quantitative measurements or comparisons of the uncertainty exponent across different model types.
- What evidence would resolve it: Empirical measurements of the uncertainty exponent α for EP-CNNs, standard CNNs, and adversarially-trained CNNs on the same dataset and perturbations, allowing for direct comparison of their robustness.

### Open Question 2
- Question: What is the impact of varying the number of free phase iterations on the robustness of EP-CNNs to adversarial attacks, and how does this compare to the behavior of standard CNNs?
- Basis in paper: [explicit] The paper mentions that EP-CNNs require multiple free phase iterations to reach a steady state and that the accuracy saturates after a certain number of timesteps. It also shows that the accuracy of EP-CNNs under PGD attack is higher with less free phase iterations, which is contrary to what would be expected if gradient obfuscation were the cause of robustness.
- Why unresolved: The paper does not provide a detailed analysis of how the number of free phase iterations affects the robustness of EP-CNNs to different types of adversarial attacks, nor does it compare this behavior to standard CNNs.
- What evidence would resolve it: A systematic study varying the number of free phase iterations for EP-CNNs and standard CNNs, and measuring their robustness to different adversarial attacks (e.g., PGD, C&W, Square) at each iteration, would reveal the impact of free phase dynamics on robustness.

### Open Question 3
- Question: How do the semantic adversarial perturbations generated on EP-CNNs differ qualitatively and quantitatively from those generated on standard CNNs and ViTs?
- Basis in paper: [explicit] The paper mentions that adversarial attacks on EP-CNNs are more semantic than those on other models, indicating that EP-CNNs may learn features that are truly useful for classification tasks. It also provides examples of adversarial perturbations on EP-CNNs and other models.
- Why unresolved: The paper does not provide a detailed analysis of the qualitative and quantitative differences between the semantic adversarial perturbations generated on EP-CNNs and those generated on other models.
- What evidence would resolve it: A comprehensive study comparing the semantic content and impact of adversarial perturbations generated on EP-CNNs, standard CNNs, and ViTs, using both qualitative (e.g., visual inspection, feature visualization) and quantitative (e.g., perturbation magnitude, feature similarity) measures, would reveal the nature and implications of the differences in learned features.

## Limitations
- The work assumes exact gradient computation is possible for EP-CNNs, but in practice, numerical errors in the recurrent dynamics could lead to gradient inaccuracies
- The semantic nature of adversarial perturbations is inferred visually but not quantitatively measured, making this claim harder to verify
- The comparison to ViTs is based on a single model, which may not generalize across different ViT architectures

## Confidence

- High confidence: EP-CNNs show greater robustness to white-box and black-box attacks compared to standard CNNs
- Medium confidence: Semantic nature of adversarial perturbations on EP-CNNs (visual evidence but no quantitative metrics)
- Medium confidence: Robustness to natural perturbations exceeds adversarially-trained models (based on CIFAR-C results but limited corruption types tested)

## Next Checks

1. Implement quantitative metrics to measure semantic similarity between adversarial perturbations on EP-CNNs versus standard models
2. Test robustness across multiple ViT architectures and checkpoints to verify generalization of the ViT comparison
3. Evaluate EP-CNNs on additional corruption types beyond CIFAR-C (e.g., ImageNet-C) to test robustness claims more broadly