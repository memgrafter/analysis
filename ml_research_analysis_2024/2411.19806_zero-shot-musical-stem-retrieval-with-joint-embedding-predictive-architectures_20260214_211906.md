---
ver: rpa2
title: Zero-shot Musical Stem Retrieval with Joint-Embedding Predictive Architectures
arxiv_id: '2411.19806'
source_url: https://arxiv.org/abs/2411.19806
tags:
- stem
- retrieval
- conditioning
- learning
- instrument
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of musical stem retrieval, where
  the goal is to find a compatible musical stem (e.g., a drum or bass track) to play
  alongside a given audio mix. The authors propose a new method based on Joint-Embedding
  Predictive Architectures (JEPAs), which jointly train an encoder and predictor to
  learn rich latent representations of audio data.
---

# Zero-shot Musical Stem Retrieval with Joint-Embedding Predictive Architectures

## Quick Facts
- arXiv ID: 2411.19806
- Source URL: https://arxiv.org/abs/2411.19806
- Reference count: 32
- State-of-the-art results on MUSDB18 and MoisesDB datasets for musical stem retrieval

## Executive Summary
This paper introduces a novel approach to musical stem retrieval using Joint-Embedding Predictive Architectures (JEPAs). The method enables finding compatible musical stems (like drums or bass) to accompany a given audio mix. By conditioning the predictor on arbitrary instruments through text embeddings from a pretrained CLAP model, the system achieves zero-shot retrieval capabilities for instruments not seen during training. The approach combines contrastive pretraining with JEPA architecture to learn rich latent representations that retain temporal information useful for downstream tasks.

## Method Summary
The proposed method employs a two-stage training approach: first, contrastive pretraining of the encoder to learn discriminative representations, then joint training of the encoder and predictor within a Joint-Embedding Predictive Architecture framework. The predictor is conditioned on instrument information through text embeddings from a pretrained CLAP model, enabling zero-shot retrieval capabilities. The system learns to map audio mixes and their compatible stems into a shared embedding space where compatible pairs are close together, while incompatible pairs are far apart.

## Key Results
- Achieves state-of-the-art performance on MUSDB18 dataset for musical stem retrieval
- Outperforms previous baselines on MoisesDB dataset
- Demonstrates zero-shot retrieval capability for instruments not seen during training
- Learns embeddings that retain temporal information useful for beat tracking

## Why This Works (Mechanism)
The approach works by learning a shared embedding space where compatible musical elements are positioned close together. The contrastive pretraining phase ensures the encoder learns discriminative features, while the JEPA framework enables the model to predict compatible stems given a mix. The CLAP text embeddings provide semantic information about instruments that allows the model to generalize to unseen instruments through text conditioning.

## Foundational Learning
- Joint-Embedding Predictive Architectures: Jointly train encoder and predictor to learn rich latent representations; needed for capturing complex relationships between audio mixes and stems; quick check: verify encoder and predictor are trained simultaneously with shared objectives
- Contrastive Learning: Learn discriminative representations by pulling together similar pairs and pushing apart dissimilar pairs; needed to ensure meaningful embedding space; quick check: verify loss function explicitly contrasts positive and negative pairs
- CLAP Text Embeddings: Use pretrained CLIP-like model to convert instrument names to embeddings; needed for zero-shot generalization to unseen instruments; quick check: verify text conditioning mechanism properly integrates CLAP embeddings

## Architecture Onboarding

**Component Map:** Audio Mix -> Encoder -> Latent Representation -> Predictor -> Compatible Stem Embedding

**Critical Path:** Encoder (contrastive pretraining) -> JEPA training -> Text-conditioned predictor -> Embedding space retrieval

**Design Tradeoffs:** The approach trades computational complexity for zero-shot generalization capability. While the two-stage training process is more involved than end-to-end approaches, it enables handling instruments never seen during training. The use of CLAP embeddings provides strong semantic information but introduces dependency on external pretrained models.

**Failure Signatures:** Poor retrieval performance when instrument names have ambiguous or multiple meanings in CLAP embedding space; degraded performance when audio quality varies significantly from training distribution; potential issues when trying to retrieve stems for instrument combinations not well-represented in training data.

**First Experiments:**
1. Evaluate retrieval performance on held-out instruments from training set to verify generalization
2. Test the impact of removing contrastive pretraining to isolate its contribution
3. Compare retrieval performance using different text embedding models instead of CLAP

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Performance evaluation limited to standard music separation datasets which may not represent real-world diversity
- Architectural complexity introduces potential training instabilities not fully explored
- Reliance on CLAP embeddings assumes good generalization across all musical instruments and styles

## Confidence

- MUSDB18/MoisesDB SOTA results: **High** (empirical validation provided)
- Zero-shot generalization capability: **Medium** (demonstrated but limited evaluation)
- Temporal information retention utility: **Low-Medium** (shown but not extensively validated)

## Next Checks

1. Test the model's zero-shot performance on a diverse, real-world dataset containing instruments not represented in MUSDB18 or MoisesDB to verify generalization claims.
2. Conduct ablation studies isolating the contributions of contrastive pretraining versus JEPA architecture to better understand their individual impacts.
3. Evaluate the learned embeddings on multiple downstream tasks beyond beat tracking (e.g., instrument classification, chord recognition) to comprehensively assess temporal information retention.