---
ver: rpa2
title: Flexibly Scaling Large Language Models Contexts Through Extensible Tokenization
arxiv_id: '2401.07793'
source_url: https://arxiv.org/abs/2401.07793
tags:
- extensible
- context
- embeddings
- tokenization
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Extensible Tokenization is a method for scaling large language
  models' context windows without fine-tuning. It works by compressing raw token embeddings
  into compact "extensible embeddings" using a separate pre-trained language model,
  allowing the LLM to perceive more information within the same context window.
---

# Flexibly Scaling Large Language Models Contexts Through Extensible Tokenization

## Quick Facts
- arXiv ID: 2401.07793
- Source URL: https://arxiv.org/abs/2401.07793
- Reference count: 36
- PG19 perplexity of 7.54 vs 7.77 for baseline

## Executive Summary
This paper introduces Extensible Tokenization, a method for extending large language models' context windows without fine-tuning the original model. The approach compresses raw token embeddings into compact "extensible embeddings" using a separate pre-trained language model, allowing the LLM to process more information within the same context window. The method achieves flexible scaling (adjusting factor k at inference), compatibility with fine-tuned models, and efficient processing with linear time complexity.

## Method Summary
Extensible Tokenization works by transforming raw token embeddings into compact extensible embeddings through down-scaling by factor k. The method uses a pre-trained language model (first 8 layers of LLaMA-2-7B) to compress context information, then applies two-stream auto-regressive training to learn the compression. During inference, extensible embeddings are mixed with raw token embeddings from the current chunk to extend context beyond the LLM's native window size. The scaling factor k can be dynamically adjusted without retraining.

## Key Results
- Achieves PG19 perplexity of 7.54 vs 7.77 for baseline methods
- Extends context lengths up to 100K+ tokens while maintaining performance
- Outperforms baseline methods on long-context language modeling and understanding tasks
- Demonstrates compatibility with fine-tuned LLM variants without additional training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extensible embeddings compress context information more efficiently than raw token embeddings
- Mechanism: A pre-trained language model transforms raw token embeddings into compact extensible embeddings through down-scaling by factor k
- Core assumption: A learned transformation can preserve contextual information while achieving significant compression
- Evidence anchors: [abstract] "transforms the raw token embeddings into extensible embeddings. Such embeddings provide a more compact representation for the long context"

### Mechanism 2
- Claim: Two-stream auto-regression enables efficient learning of compression without modifying downstream LLM parameters
- Mechanism: First pass generates extensible embeddings for entire context, second pass predicts tokens using both extensible embeddings from previous chunks and raw embeddings within current chunk
- Core assumption: Learning compression through prediction losses provides better sample efficiency than direct reconstruction
- Evidence anchors: [section] "we propose the two-stream AR to optimize the sample efficiency of training... the prediction loss can be comprehensively derived from each training instance"

### Mechanism 3
- Claim: Dynamic scaling factor selection enables flexible context extension without retraining
- Mechanism: At inference time, scaling factor k can be adjusted to extend context to arbitrary lengths, with extensible and raw embeddings blended seamlessly
- Core assumption: The learned compression generalizes across different scaling factors without degradation
- Evidence anchors: [abstract] "the scaling factor can be flexibly determined within a feasible scope, leading to the extension of an arbitrary context length at the inference time"

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: Understanding how extensible embeddings integrate with standard transformer attention mechanisms
  - Quick check question: How does attention computation change when mixing extensible embeddings with raw token embeddings?

- Concept: Auto-regressive language modeling
  - Why needed here: The training objective relies on predicting next tokens conditioned on compressed context
  - Quick check question: What prediction losses are minimized during two-stream AR training?

- Concept: Tokenization and embedding spaces
  - Why needed here: Raw token embeddings serve as input to the compression module before LLM processing
  - Quick check question: How do token embedding dimensions relate to extensible embedding dimensions after down-scaling?

## Architecture Onboarding

- Component map: Input tokenizer -> Raw token embeddings -> Extensible tokenizer (LLM backbone) -> Down-scaling layer -> Extensible embeddings -> Downstream LLM -> Standard processing -> Session manager -> Online inference coordination
- Critical path: Input chunking -> Extensible embedding generation -> LLM inference -> Output generation
- Design tradeoffs:
  - Larger scaling factor k -> More compression but potential information loss
  - Smaller chunk size -> Better coherence but more computational overhead
  - Deeper extensible tokenizer -> Better compression quality but higher resource usage
- Failure signatures:
  - Performance drops with larger scaling factors indicate insufficient compression quality
  - Memory issues suggest chunk size or session management problems
  - Training instability may indicate improper two-stream AR implementation
- First 3 experiments:
  1. Validate compression quality: Compare perplexity with different scaling factors (k=2,4,8) on fixed context length
  2. Test two-stream AR effectiveness: Compare training convergence with and without two-stream approach
  3. Verify compatibility: Apply trained extensible tokenizer to fine-tuned LLM variants and measure performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal architecture and scaling factor for Extensible Tokenization across different types of language tasks and dataset domains?
- Basis in paper: [explicit] The paper mentions that larger models lead to improved performance but also increase costs, and that dynamically sampling scaling factors improves versatility across context lengths.
- Why unresolved: The paper only tests a limited range of architectures (first 4 or 8 layers of LLaMA-2-7B) and scaling factors (16, 32). Different tasks and domains might have different optimal configurations.
- What evidence would resolve it: Systematic experiments testing various architectures (different number of layers, different base models) and scaling factors across diverse tasks (code generation, summarization, question answering, etc.) and datasets (news, literature, scientific papers, etc.) to identify optimal configurations.

### Open Question 2
- Question: How does Extensible Tokenization compare to other context extension methods when used in combination with retrieval-augmented generation (RAG)?
- Basis in paper: [inferred] The paper mentions RAG as a critical application and notes that pre-computed extensible embeddings could save computation in offline scenarios.
- Why unresolved: The paper only tests Extensible Tokenization in isolation and doesn't compare its effectiveness in RAG pipelines against other methods.
- What evidence would resolve it: Experiments comparing RAG systems using Extensible Tokenization versus other context extension methods (sparse attention, compressed memory, etc.) on retrieval accuracy and generation quality metrics.

### Open Question 3
- Question: What are the limitations of Extensible Tokenization's compatibility with fine-tuned derivatives of different base models?
- Basis in paper: [explicit] The paper shows good compatibility with LongAlpaca and LongChat (fine-tuned derivatives of LLaMA-2-7B) but doesn't test derivatives of other base models.
- Why unresolved: The paper only tests compatibility with derivatives of the same base model used for training the extensible tokenizer.
- What evidence would resolve it: Experiments training extensible tokenizers on one base model and testing compatibility with fine-tuned derivatives of completely different base models to determine generalizability limits.

## Limitations
- Evaluation focuses primarily on perplexity metrics and specific long-context understanding tasks without comprehensive testing across diverse LLM applications
- Two-stream auto-regressive training procedure may have implementation-specific nuances affecting reproducibility
- Compression quality fundamentally depends on assumption that down-scaling preserves sufficient semantic information

## Confidence
- **High confidence**: The core mechanism of compressing token embeddings through a learned transformation is technically sound and aligns with established compression literature
- **Medium confidence**: Language modeling results showing improved perplexity over baselines are promising but based on limited datasets
- **Low confidence**: Dynamic scaling factor flexibility claims lack empirical validation across diverse scaling ratios

## Next Checks
1. Cross-dataset generalization test: Evaluate the trained extensible tokenizer on additional language modeling datasets (WikiText, LAMBADA) with varying content types to verify consistent perplexity improvements across domains
2. Scaling factor robustness analysis: Systematically test performance degradation when using scaling factors outside the training distribution (e.g., k=64) to validate the claimed flexibility of dynamic adjustment
3. Downstream task transfer evaluation: Apply the extensible tokenizer to fine-tuned LLMs for specific tasks (code generation, summarization, QA) and measure task-specific performance degradation compared to raw context processing