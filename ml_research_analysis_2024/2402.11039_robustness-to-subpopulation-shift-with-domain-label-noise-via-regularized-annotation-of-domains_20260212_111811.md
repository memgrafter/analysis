---
ver: rpa2
title: Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation
  of Domains
arxiv_id: '2402.11039'
source_url: https://arxiv.org/abs/2402.11039
tags:
- domain
- noise
- data
- group
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the robustness of group-based worst-group accuracy
  (WGA) methods under domain label noise. The authors prove that popular methods like
  downsampling and upweighting degrade significantly with increasing domain noise
  and in the limit approach vanilla empirical risk minimization performance.
---

# Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation of Domains

## Quick Facts
- arXiv ID: 2402.11039
- Source URL: https://arxiv.org/abs/2402.11039
- Reference count: 40
- One-line primary result: RAD-UW achieves competitive worst-group accuracy without explicit domain annotations, outperforming state-of-the-art annotation-reliant methods even with 5% domain label noise.

## Executive Summary
This paper addresses the robustness of group-based worst-group accuracy (WGA) methods under domain label noise. The authors prove that popular methods like downsampling and upweighting degrade significantly with increasing domain noise and in the limit approach vanilla empirical risk minimization performance. They introduce Regularized Annotation of Domains (RAD), a two-step method that first pseudo-annotates domains by training a highly regularized model to learn spurious correlations, then upweights the pseudo-annotated minority examples in the final retraining. Experiments on CMNIST, CelebA, Waterbirds, MultiNLI, and CivilComments show that RAD-UW outperforms state-of-the-art annotation-reliant methods even with only 5% domain noise, while maintaining low variance.

## Method Summary
RAD is a two-step method designed to achieve robust worst-group accuracy (WGA) without explicit domain annotations. First, a highly ℓ1-regularized pseudo-annotation model is trained on the original imbalanced data to learn spurious correlations. Samples misclassified by this model are treated as minority (pseudomajority = spurious, pseudominority = core). Second, the final retraining model upweights these pseudo-annotated minority samples to counteract the spurious feature bias. The method is compared against downsampling and upweighting baselines under varying levels of domain label noise.

## Key Results
- RAD-UW outperforms state-of-the-art annotation-reliant methods even with only 5% domain label noise.
- Both downsampling and upweighting methods degrade with domain label noise, eventually approaching empirical risk minimization performance.
- RAD-UW maintains low variance in WGA across different noise levels and datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAD-UW maintains robust WGA even with high domain noise because it learns to pseudo-annotate domains via spurious feature regularization rather than relying on potentially noisy true domain labels.
- Mechanism: A highly ℓ1-regularized pseudo-annotation model is trained on the original imbalanced data to learn spurious correlations. Samples misclassified by this model are treated as minority (pseudomajority = spurious, pseudominority = core), and the final retraining model upweights these minority samples to counteract the spurious feature bias.
- Core assumption: The majority examples in the imbalanced data are those that rely on spurious features correlated with the label, while minority examples rely on core features. A model trained to capture spurious correlations will thus misclassify minority examples as core examples are correctly classified.
- Evidence anchors:
  - [abstract] "RAD outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the training data"
  - [section] "We present theoretical guarantees for the WGA under domain noise when modeling last layer representations as symmetric mixtures"
  - [corpus] Found 25 related papers; only 2 directly relevant to domain noise in subpopulation shift, suggesting this is an under-explored area.
- Break condition: If spurious correlations are not present or if core and spurious features are not separable by a linear model, the pseudo-annotation will fail to correctly identify minority examples, leading to degraded WGA.

### Mechanism 2
- Claim: Both downsampling and upweighting methods degrade with domain label noise because the noise corrupts the priors used to balance groups, eventually reverting to ERM performance.
- Mechanism: Domain label noise changes the perceived minority prior π0 to a noisy prior π0^(p), which DS and UW use for rebalancing. As noise increases, the rebalancing becomes ineffective because the "rebalanced" groups no longer reflect the true minority/majority structure, and performance approaches ERM.
- Core assumption: The group rebalancing (DS to equalize group sizes, UW to invert group prevalence) is only effective when true domain labels are known; noisy labels distort these rebalancing efforts.
- Evidence anchors:
  - [abstract] "annotation-based data augmentations using either downsampling or upweighting for WGA are susceptible to domain annotation noise"
  - [section] "Theorem 3.7: WGA of both augmentation approaches are equal and degrade smoothly in p ∈ [0, 1/2] to the baseline WGA of (3) with no augmentation"
  - [corpus] Found 25 related papers; limited evidence of direct analysis of domain noise on group balancing methods, supporting novelty.
- Break condition: If the noise is not symmetric or if domain labels are not uniformly corrupted, the theoretical analysis may not hold, and degradation may not follow the predicted pattern.

### Mechanism 3
- Claim: ℓ1 regularization is critical in both the pseudo-annotation and retraining steps to ensure separation of core and spurious features.
- Mechanism: In the pseudo-annotation step, strong ℓ1 regularization encourages the model to learn only the most discriminative (spurious) features, leading to misclassification of minority examples. In the retraining step, ℓ1 regularization discourages reliance on spurious features and promotes learning core features that generalize across domains.
- Core assumption: ℓ1 regularization effectively performs feature selection, isolating features that are most strongly associated with spurious correlations in the first step and those that are most robust in the second.
- Evidence anchors:
  - [section] "We emphasize that a strong ℓ1 regularizer is critical to the success of our method through the intuition of DFR"
  - [section] "Table 1: Regularizer Dataset WGA — For CelebA, a real-world image dataset of faces, ℓ1 regularization shows dramatic gains"
  - [corpus] Found 25 related papers; only 1 directly related to ℓ1 regularization in WGA context, indicating limited comparative evidence.
- Break condition: If the spurious correlations are not linearly separable or if the core features are not well-represented in the minority examples, ℓ1 regularization may not effectively isolate the desired features, leading to poor pseudo-annotation or retraining.

## Foundational Learning

- Concept: Symmetric domain label noise (SLN)
  - Why needed here: The paper's theoretical analysis of DS and UW degradation assumes symmetric noise to model how corrupted priors affect rebalancing. Understanding SLN is essential to interpret the theoretical guarantees and experimental results.
  - Quick check question: If the true domain label is S, what is the probability it is observed as T under SLN with parameter p?
- Concept: Worst-group accuracy (WGA)
  - Why needed here: WGA is the primary evaluation metric used to assess fairness and robustness across subpopulations. Understanding how WGA is defined and computed is necessary to interpret the results.
  - Quick check question: How is WGA mathematically defined in terms of per-group accuracies?
- Concept: Last-layer retraining (LLR)
  - Why needed here: The paper focuses on adapting pretrained models via last-layer retraining. Understanding LLR's role in transfer learning and its limitations (e.g., susceptibility to spurious correlations) is key to grasping the motivation for RAD-UW.
  - Quick check question: What is the primary advantage of last-layer retraining compared to full fine-tuning?

## Architecture Onboarding

- Component map:
  - Pretrained embeddings (ResNet50 for images, BERT for text) -> Pseudo-annotation model (linear classifier with strong ℓ1 regularization) -> Retraining model (linear classifier with ℓ1 regularization and upweighting of pseudo-annotated minority)
- Critical path:
  1. Extract embeddings from upstream model
  2. Train pseudo-annotation model on training data with strong ℓ1 regularization
  3. Use pseudo-annotation model to label minority/majority samples
  4. Train retraining model on all data, upweighting minority samples, with ℓ1 regularization
- Design tradeoffs:
  - ℓ1 regularization strength: Stronger regularization in pseudo-annotation increases spurious feature learning but risks underfitting; weaker regularization may fail to capture spurious correlations.
  - Upweighting factor: Too high may overfit to minority samples; too low may not sufficiently counteract spurious bias.
  - Hyperparameter tuning: Requires clean holdout set; in practice, may not always be available.
- Failure signatures:
  - RAD-UW performance close to LLR: Pseudo-annotation failed to identify minority samples correctly.
  - High variance in RAD-UW: Unstable pseudo-annotation due to weak ℓ1 regularization or highly variable spurious correlations.
  - RAD-UW worse than GUW/DS at low noise: Over-regularization in pseudo-annotation step.
- First 3 experiments:
  1. Run RAD-UW on CMNIST with 0% noise to establish baseline WGA and compare to LLR.
  2. Run RAD-UW on CelebA with 10% noise to test robustness to moderate domain label corruption.
  3. Vary ℓ1 regularization strength in pseudo-annotation model on Waterbirds to find optimal trade-off between spurious feature learning and model capacity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAD-UW change with different levels of regularization strength in the pseudo-annotation model (λID) across different datasets?
- Basis in paper: [explicit] The paper states that the regularization strength λID for the pseudo-annotation model is tuned via a grid search and that it has more impact on performance than the retraining regularizer, especially for real-world image datasets like CelebA.
- Why unresolved: The paper does not provide a detailed analysis of how varying λID affects the performance of RAD-UW across different datasets or noise levels.
- What evidence would resolve it: A comprehensive study showing the performance of RAD-UW with different λID values on multiple datasets, including both synthetic and real-world data, would clarify the optimal regularization strength and its impact on robustness to domain label noise.

### Open Question 2
- Question: Can RAD-UW be extended to handle multi-class and multi-domain settings beyond the binary class, binary domain scenario analyzed in the paper?
- Basis in paper: [inferred] The paper focuses on binary class and binary domain settings for theoretical analysis but mentions that RAD-UW is tested on several large publicly available datasets, which may have more complex group structures.
- Why unresolved: The theoretical guarantees provided in the paper are limited to the binary setting, and it is unclear how the method would perform or need to be adapted for more complex scenarios.
- What evidence would resolve it: Empirical results demonstrating the performance of RAD-UW on datasets with multiple classes and domains, along with theoretical extensions to these settings, would provide insights into its scalability and robustness.

### Open Question 3
- Question: What is the impact of class label noise in conjunction with domain label noise on the performance of RAD-UW and other WGA methods?
- Basis in paper: [explicit] The paper mentions that Oh et al. (2022) has demonstrated that most two-stage methods, including precursors to RAD and SELF, can fail dramatically with small amounts of class label noise, and addressing this issue is left as future work.
- Why unresolved: The paper does not explore the combined effect of class and domain label noise, which is a realistic scenario in many practical applications.
- What evidence would resolve it: Experiments and theoretical analysis showing how RAD-UW and other WGA methods perform under combined class and domain label noise would help understand their robustness and identify potential improvements.

## Limitations
- Theoretical analysis assumes symmetric domain label noise, which may not hold in all real-world settings.
- Empirical validation relies on five benchmark datasets, which may not fully represent the diversity of subpopulation shift scenarios.
- Some comparison methods (e.g., M-SELF) lack full implementation details in the paper.

## Confidence
- Theoretical claims: Medium - Limited comparative literature on domain noise in WGA, but the analysis is well-defined.
- Empirical results: Medium - Experiments are comprehensive, but some methods lack full implementation details.

## Next Checks
1. Test RAD-UW on datasets with asymmetric domain noise to assess robustness beyond the theoretical assumptions.
2. Compare RAD-UW with domain generalization methods (e.g., CORAL, DANN) to establish relative performance in domain-robustness benchmarks.
3. Conduct ablation studies on the pseudo-annotation step to isolate the contribution of ℓ1 regularization versus the upweighting strategy.