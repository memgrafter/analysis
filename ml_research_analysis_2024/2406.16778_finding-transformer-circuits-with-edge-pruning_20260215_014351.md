---
ver: rpa2
title: Finding Transformer Circuits with Edge Pruning
arxiv_id: '2406.16778'
source_url: https://arxiv.org/abs/2406.16778
tags:
- edge
- pruning
- circuit
- circuits
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Edge Pruning is a new method for finding interpretable circuits
  in large language models. It frames circuit discovery as an edge pruning problem
  and optimizes continuous edge masks to produce sparse subgraphs that match the model's
  predictions.
---

# Finding Transformer Circuits with Edge Pruning

## Quick Facts
- **arXiv ID:** 2406.16778
- **Source URL:** https://arxiv.org/abs/2406.16778
- **Authors:** Adithya Bhaskar; Alexander Wettig; Dan Friedman; Danqi Chen
- **Reference count:** 40
- **Key outcome:** Edge Pruning is a new method for finding interpretable circuits in large language models. It frames circuit discovery as an edge pruning problem and optimizes continuous edge masks to produce sparse subgraphs that match the model's predictions. Evaluated on four standard tasks, Edge Pruning finds circuits with fewer edges than prior methods while being equally faithful to the full model. It also scales to 100K examples and perfectly recovers ground-truth circuits in Tracr models. Applied to CodeLlama-13B, it discovers extremely sparse (0.04% edges) circuits for instruction-prompting and few-shot settings that overlap substantially, suggesting shared mechanisms. Edge Pruning is efficient, effective, and scalable, making it a practical tool for mechanistic interpretability.

## Executive Summary
Edge Pruning introduces a novel approach to mechanistic interpretability by framing circuit discovery as an edge pruning problem rather than component or neuron pruning. The method optimizes continuous edge masks via gradient descent to produce sparse subgraphs that maintain model performance while being more interpretable. Using a disentangled residual stream and interchange ablation for counterfactual replacement, Edge Pruning discovers circuits that are both sparse (as low as 0.04% of edges) and faithful to the full model. The approach scales to large datasets and models, discovering circuits for instruction-prompting and few-shot settings in CodeLlama-13B that substantially overlap, suggesting shared mechanisms.

## Method Summary
Edge Pruning discovers interpretable circuits in transformer models by optimizing continuous edge masks to produce sparse subgraphs that match model predictions. The method replaces the standard residual stream with a disentangled residual stream that retains all prior activations, enabling counterfactual replacement of removed edges with corrupted activations. Edge masks are optimized via gradient descent with L0 regularization to enforce target sparsity. The approach is evaluated on four standard tasks (Indirect Object Identification, Greater Than, Gendered Pronoun, and Tracr-compiled programs) and applied to CodeLlama-13B, demonstrating scalability to 100K examples and discovery of extremely sparse circuits that overlap between instruction-prompting and few-shot settings.

## Key Results
- Edge Pruning finds circuits with fewer edges than prior methods (0.04% of edges in CodeLlama-13B) while maintaining equal faithfulness to the full model
- The method scales to 100K examples and perfectly recovers ground-truth circuits in Tracr models
- Circuits discovered for instruction-prompting and few-shot settings in CodeLlama-13B substantially overlap (2110 shared edges), suggesting shared mechanisms
- Edge Pruning outperforms ACDC and EAP baselines on standard circuit-finding tasks in terms of sparsity and faithfulness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Edge Pruning outperforms prior methods by leveraging gradient-based pruning over edges rather than discrete ablation or linear approximations.
- **Mechanism:** The method replaces the residual stream with a disentangled residual stream, enabling continuous optimization of edge masks. This allows gradient descent to refine edge importance based on actual model behavior rather than first-order approximations.
- **Core assumption:** Continuous edge masks provide better signal for gradient updates than discrete ablation or attribution scores.
- **Evidence anchors:**
  - [abstract] "Edge Pruning leverages gradient-based pruning techniques, but instead of removing neurons or components, it prunes the edges between components."
  - [section 3] "While structured pruning aims to discard pruned nodes as wasteful computation, the application to interpretability calls for more careful treatment of missing nodes and edges."
- **Break condition:** If edge importance cannot be reliably captured by continuous gradients, the method may revert to poor local optima.

### Mechanism 2
- **Claim:** The disentangled residual stream allows the model to retain all prior activations, enabling counterfactual replacement with corrupted activations during edge pruning.
- **Mechanism:** By maintaining a list of all previous activations, Edge Pruning can dynamically aggregate them based on the current edge masks, replacing removed edges with corrupted activations.
- **Core assumption:** Retaining all prior activations is feasible and does not create prohibitive memory overhead.
- **Evidence anchors:**
  - [section 3] "We enable this by replacing the residual stream of a Transformer with a disentangled residual stream [Lindner et al., 2023, Friedman et al., 2023], which retains a list of all previous activations."
- **Break condition:** If memory constraints become prohibitive at scale, the disentangled stream approach may fail.

### Mechanism 3
- **Claim:** Edge Pruning scales to large models because pruning edges is more granular than pruning components, allowing better performance with fewer parameters.
- **Mechanism:** The number of edges scales quadratically with the number of nodes, but pruning individual edges allows finding sparse subgraphs with better task performance than coarser methods.
- **Core assumption:** Edge-level granularity is necessary to capture complex model behaviors in large models.
- **Evidence anchors:**
  - [abstract] "Edge Pruning is efficient, effective, and scalable, making it a practical tool for mechanistic interpretability."
  - [section 5] "Our case study reveals that the same mechanism (and the same circuit represented by the intersection above) explains a large part of the performance in both settings."
- **Break condition:** If edge-level granularity does not translate to better interpretability, coarser methods may be preferable.

## Foundational Learning

- **Concept: Transformer residual stream**
  - Why needed here: Edge Pruning modifies the residual stream to retain all prior activations for dynamic aggregation.
  - Quick check question: How does the residual stream in a standard Transformer differ from the disentangled version used here?

- **Concept: Gradient-based pruning**
  - Why needed here: Edge Pruning uses gradient descent to optimize edge masks rather than discrete ablation.
  - Quick check question: What is the key difference between gradient-based pruning and structured pruning?

- **Concept: Interchange ablation**
  - Why needed here: Edge Pruning replaces removed edges with corrupted activations from interchange ablation.
  - Quick check question: Why is interchange ablation preferred over simple edge removal in circuit discovery?

## Architecture Onboarding

- **Component map:**
  - Disentangled residual stream -> Edge masks -> L0 regularization -> Interchange ablation -> Binary circuit

- **Critical path:**
  1. Initialize model with disentangled residual stream
  2. Optimize edge masks via gradient descent
  3. Replace removed edges with corrupted activations
  4. Enforce sparsity via L0 regularization
  5. Convert continuous masks to binary circuit

- **Design tradeoffs:**
  - Memory vs. granularity: Disentangled stream increases memory but enables finer-grained pruning
  - Speed vs. faithfulness: Continuous optimization is slower but more faithful than discrete ablation
  - Scalability vs. precision: Edge-level pruning scales better but may miss higher-level patterns

- **Failure signatures:**
  - High KL divergence between model and circuit outputs
  - Circuit performance significantly below full model
  - Training fails to converge to desired sparsity
  - Memory overflow during training with disentangled stream

- **First 3 experiments:**
  1. Run Edge Pruning on IOI-t1 with standard hyperparameters to verify basic functionality
  2. Compare Edge Pruning vs ACDC on GT task to test faithfulness on more complex tasks
  3. Scale Edge Pruning to 100K examples on IOI to verify scalability claims

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but acknowledges the possibility of interpretability illusions and notes that circuit discovery remains an open problem.

## Limitations
- Memory efficiency concerns with the disentangled residual stream approach for extremely large models or long sequences
- Limited interpretability validation beyond functional equivalence to the full model
- Evaluation primarily on four standard tasks without testing on more diverse or challenging domains

## Confidence
**High Confidence:**
- Edge Pruning can find circuits with fewer edges than prior methods while maintaining faithfulness
- The method scales to large datasets (100K examples)
- Edge Pruning perfectly recovers ground-truth circuits in Tracr models
- The CodeLlama-13B case study shows substantial overlap between instruction-prompting and few-shot circuits

**Medium Confidence:**
- Edge Pruning is more efficient than prior methods due to continuous optimization
- The disentangled residual stream approach is necessary for proper counterfactual replacement
- Edge-level granularity is essential for capturing complex model behaviors

**Low Confidence:**
- The discovered circuits provide genuine mechanistic insights into model behavior
- The method will generalize equally well to all types of language model tasks
- Memory requirements remain manageable for state-of-the-art models with long sequences

## Next Checks
1. **Memory Scalability Test:** Run Edge Pruning on CodeLlama-13B with sequence lengths of 2048 and 4096 tokens to measure memory consumption and identify practical limits of the disentangled residual stream approach.

2. **Interpretability Validation:** Conduct ablation studies on the CodeLlama-13B circuits by systematically removing edges and measuring both performance drop and interpretability quality using established interpretability metrics from the literature.

3. **Task Generalization:** Apply Edge Pruning to a reasoning task (e.g., GSM8K) or a multilingual task to evaluate whether the method discovers circuits with similar properties or requires fundamental architectural modifications for different task types.