---
ver: rpa2
title: 'Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs
  in Language Models'
arxiv_id: '2403.19647'
source_url: https://arxiv.org/abs/2403.19647
tags:
- features
- feature
- circuits
- conference
- circuit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces methods for discovering and applying sparse
  feature circuits in language models. The core idea is to use sparse autoencoders
  to identify interpretable features and then compute circuits based on their causal
  effects.
---

# Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models

## Quick Facts
- **arXiv ID**: 2403.19647
- **Source URL**: https://arxiv.org/abs/2403.19647
- **Reference count**: 40
- **Primary result**: Introduces sparse feature circuits using SAEs to discover and edit interpretable causal graphs in language models

## Executive Summary
This paper presents a method for discovering and applying sparse feature circuits in language models, using sparse autoencoders (SAEs) to identify interpretable features and compute circuits based on their causal effects. The approach enables detailed understanding of model behaviors and practical applications like debiasing classifiers without disambiguating data. The authors demonstrate their method on subject-verb agreement tasks, showing that sparse feature circuits are more interpretable and complete than neuron-based circuits. They introduce SHIFT, a technique for removing unintended signals from classifiers, and showcase an unsupervised pipeline for discovering thousands of circuits for automatically discovered model behaviors.

## Method Summary
The method uses sparse autoencoders to decompose language model activations into interpretable features, then computes causal circuits by estimating the indirect effects of these features on specific model behaviors. Linear approximations (attribution patching or integrated gradients) efficiently identify which features are causally implicated in behaviors. Circuits are formed by filtering nodes and edges based on thresholds. For applications like SHIFT, humans manually identify task-irrelevant features in circuits and ablate them to remove unwanted model sensitivities. The approach is demonstrated on Pythia-70M and Gemma-2-2B models for tasks including subject-verb agreement and debiasing classifiers.

## Key Results
- Sparse feature circuits achieve higher faithfulness and completeness scores than neuron-based circuits for subject-verb agreement tasks
- SHIFT method removes gender information from classifiers without requiring disambiguating labeled data, improving generalization
- Unsupervised pipeline successfully discovers thousands of circuits for automatically discovered model behaviors
- SAE features are more interpretable and monosemantic than individual neurons

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sparse autoencoders can decompose language model activations into interpretable feature activations that are more monosemantic than individual neurons.
- **Mechanism**: SAEs learn a dictionary of basis vectors (features) and sparse coefficients that reconstruct model activations. The L1 regularization promotes sparsity, forcing each activation to be represented by only a few features. This creates features that correspond to specific, interpretable concepts rather than capturing multiple unrelated signals.
- **Core assumption**: The latent space of language models contains directions that correspond to human-interpretable concepts, and these can be recovered through unsupervised dictionary learning.
- **Evidence anchors**:
  - [abstract] "SAEs to identify directions in LM latent spaces which represent human-interpretable concepts"
  - [section] "SAEs are trained on an objective which promotes having a small reconstruction error while using only a sparse set of feature activations fi(x)"
  - [corpus] "Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features" (corpus neighbor)
- **Break condition**: If SAEs fail to produce features that are more interpretable than neurons, or if the reconstruction error becomes too high due to excessive sparsity.

### Mechanism 2
- **Claim**: Linear approximations can efficiently identify which SAE features are causally implicated in specific model behaviors.
- **Mechanism**: By computing gradients of the model's output with respect to feature activations, we can estimate the indirect effect each feature has on the behavior of interest. The dot product of gradients with activation differences provides a computationally efficient proxy for intervention-based causal estimates.
- **Core assumption**: The relationship between feature activations and model outputs is locally linear enough that gradient-based approximations capture meaningful causal information.
- **Evidence anchors**:
  - [abstract] "We employ linear approximations to efficiently identify SAE features which are most causally implicated in model behaviors"
  - [section] "We thus employ linear approximations to (2) that can be computed for many a in parallel. The simplest such approximation, attribution patching (Nanda, 2022; Syed et al., 2023; Kram´ar et al., 2024), employs a first-order Taylor expansion"
  - [section] "we find that ˆIEatp accurately estimates IEs for SAE features and SAE errors, with the exception of nodes in the layer 0 MLP and early residual stream layers"
- **Break condition**: If the model's behavior exhibits strong nonlinearities or interactions that make linear approximations inaccurate, or if the causal relationships are too complex for gradient-based methods to capture.

### Mechanism 3
- **Claim**: Feature circuits composed of interpretable SAE features enable practical applications like debiasing classifiers without requiring disambiguating labeled data.
- **Mechanism**: By manually inspecting feature circuits and identifying task-irrelevant features, we can surgically remove unwanted model sensitivities through ablation. This works because the circuits provide a human-understandable decomposition of model behavior, allowing targeted interventions.
- **Core assumption**: Humans can reliably identify which features are task-relevant versus task-irrelevant through inspection, and removing irrelevant features doesn't catastrophically disrupt the intended functionality.
- **Evidence anchors**:
  - [abstract] "Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce S HIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant"
  - [section] "we zero-ablate these irrelevant features. Finally, we retrain the linear classification head with the ambiguous set using activations extracted from the ablated model"
  - [section] "we find that S HIFT almost completely removes the classifiers' dependence on gender information for both models"
- **Break condition**: If humans cannot reliably distinguish relevant from irrelevant features, or if removing features has unintended side effects that degrade performance more than expected.

## Foundational Learning

- **Concept**: Sparse autoencoders and dictionary learning
  - **Why needed here**: SAEs are the fundamental building block that enables the discovery of interpretable features. Understanding how they work is essential for grasping the entire approach.
  - **Quick check question**: How does the L1 regularization term in SAE training promote sparsity, and why is sparsity important for interpretability?

- **Concept**: Causal mediation analysis and indirect effects
  - **Why needed here**: The method relies on quantifying the causal impact of features on model behaviors. Understanding indirect effects is crucial for identifying which features matter for specific tasks.
  - **Quick check question**: What is the difference between direct and indirect effects in causal analysis, and why are indirect effects more relevant for understanding feature contributions?

- **Concept**: Linear approximations and gradient-based attribution methods
  - **Why needed here**: These approximations make it computationally feasible to analyze thousands of features. Understanding their limitations is important for interpreting results correctly.
  - **Quick check question**: When would a linear approximation fail to capture the true causal effect of a feature, and how does integrated gradients improve upon simpler methods?

## Architecture Onboarding

- **Component map**: Model → SAE → Feature circuit → Human interpretation → Application
- **Critical path**: Model → SAE → Feature circuit → Human interpretation → Application
  The most critical components are the SAE training and the indirect effect computation, as errors in either will propagate through the entire pipeline.
- **Design tradeoffs**:
  - Feature dimension vs. interpretability: Higher dimensional SAEs may capture more nuance but become harder to interpret
  - Sparsity level: More sparse features are more interpretable but may miss important signals
  - Linear approximation quality vs. computational cost: More accurate methods are more expensive
- **Failure signatures**:
  - SAEs produce mostly dead features or highly polysemantic features
  - Linear approximations show poor correlation with exact indirect effects
  - Discovered circuits don't explain model behavior as measured by faithfulness scores
  - Human annotators cannot interpret any features in the circuits
- **First 3 experiments**:
  1. Train SAEs on a simple model (like Pythia-70M) and verify that features are more interpretable than neurons using human evaluations
  2. Apply circuit discovery to a known task (like subject-verb agreement) and verify that the circuit explains model behavior
  3. Test the SHIFT application by attempting to remove a known spurious correlation and measuring the effect on generalization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the quality of sparse feature circuits scale with increasing SAE dictionary size and model size?
- **Basis in paper**: [inferred] The authors mention that scaling SAEs is an active area of research and they treat scaling SAEs themselves as out-of-scope, focusing instead on applying trained SAEs.
- **Why unresolved**: The paper uses specific SAE configurations (e.g., dSAE = 64 × d for Pythia SAEs) but does not explore how varying dictionary size or applying the method to larger models affects circuit quality, faithfulness, or interpretability.
- **What evidence would resolve it**: Systematic experiments varying SAE dictionary size (e.g., 32x, 64x, 128x model width) and applying the method to larger LMs (e.g., GPT-2, LLaMA) while measuring circuit faithfulness, completeness, and interpretability scores.

### Open Question 2
- **Question**: Can the SHIFT technique be extended to remove multiple spurious correlations simultaneously, and what are the limits of this approach?
- **Basis in paper**: [explicit] The authors demonstrate SHIFT on a single spurious feature (gender) but acknowledge that real-world datasets often contain multiple spurious correlations.
- **Why unresolved**: The paper only tests SHIFT on a binary spurious feature and does not explore scenarios with multiple, interacting spurious signals or how the method performs when spurious features are correlated with each other.
- **What evidence would resolve it**: Experiments applying SHIFT to datasets with multiple spurious features (e.g., gender, age, location) and measuring how well the method removes each spurious signal while maintaining performance on the intended task.

### Open Question 3
- **Question**: How do different clustering methods and feature representations affect the quality and interpretability of automatically discovered behaviors?
- **Basis in paper**: [explicit] The authors experiment with various inputs to clustering (dense activations, sparse activations, indirect effects) but do not systematically compare clustering methods or evaluate the quality of discovered behaviors.
- **Why unresolved**: While the paper presents example clusters and circuits, it does not provide quantitative metrics for cluster quality or compare different clustering approaches (e.g., spectral vs. k-means) or feature representations.
- **What evidence would resolve it**: Systematic comparison of clustering methods and feature representations using metrics like cluster coherence, interpretability scores from human annotators, and downstream task performance when using circuits from different clustering approaches.

## Limitations
- The manual inspection required for applications like SHIFT introduces potential subjectivity and may not scale well to more complex tasks
- The paper provides limited quantitative validation of the claim that SAE features are more interpretable than neurons beyond qualitative examples
- Linear approximation methods may fail in cases of strong feature interactions or nonlinearities, though this is not extensively explored

## Confidence
- **High Confidence**: The core technical approach of using SAEs to discover interpretable features and computing their causal effects through linear approximations is well-grounded in existing literature
- **Medium Confidence**: The effectiveness of SHIFT for debiasing classifiers without disambiguating data is demonstrated on specific tasks, but generalizability to other domains remains uncertain
- **Low Confidence**: The unsupervised pipeline for discovering thousands of circuits for automatically discovered behaviors is mentioned but not extensively validated with quantitative metrics

## Next Checks
1. **Quantitative interpretability benchmark**: Systematically compare SAE features against neurons using standardized interpretability metrics (e.g., consistency across models, human evaluation protocols) to validate that SAEs consistently produce more interpretable features
2. **Failure mode analysis**: Systematically test the linear approximation methods on synthetic circuits with known nonlinearities and feature interactions to characterize when and why the approximations break down
3. **Scalability evaluation**: Evaluate the unsupervised circuit discovery pipeline on a diverse set of behaviors, measuring both the accuracy of the discovered circuits and the computational efficiency compared to supervised approaches