---
ver: rpa2
title: 'INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval
  Models'
arxiv_id: '2402.14334'
source_url: https://arxiv.org/abs/2402.14334
tags:
- instructions
- query
- instruction
- retrieval
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INSTRUCTIR, a benchmark for evaluating instruction-following
  ability in information retrieval models. The authors address the gap in existing
  benchmarks that do not adequately assess how well retrievers can follow diverse,
  user-aligned instructions tailored to individual queries.
---

# INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models

## Quick Facts
- arXiv ID: 2402.14334
- Source URL: https://arxiv.org/abs/2402.14334
- Authors: Hanseok Oh; Hyunji Lee; Seonghyeon Ye; Haebin Shin; Hansol Jang; Changwook Jun; Minjoon Seo
- Reference count: 8
- Primary result: Evaluates instruction-following ability in retrievers, finding that instruction-tuned models can underperform compared to non-instruction-tuned counterparts.

## Executive Summary
This paper introduces INSTRUCTIR, a benchmark designed to evaluate the instruction-following ability of information retrieval models. The authors address a critical gap in existing benchmarks by focusing on user-aligned instructions tailored to individual queries, rather than generic task-style instructions. Using GPT-4 to generate diverse, instance-specific instructions and revise target texts accordingly, INSTRUCTIR reflects real-world search scenarios and user preferences. Through comprehensive experiments, the authors demonstrate that retrievers fine-tuned on task-style instructions can underperform compared to non-instruction-tuned models, highlighting potential overfitting issues. The benchmark provides valuable insights into the diverse characteristics of existing retrieval systems and offers a new direction for developing more sophisticated, instruction-aware information access systems.

## Method Summary
The authors create INSTRUCTIR by selecting queries and target texts from existing datasets (primarily MSMARCO), then generating instance-wise instructions using GPT-4 that incorporate user-aligned information. Target texts are revised to align with these instructions, followed by a filtering process to ensure high-quality instances. The benchmark is evaluated using nDCG@10 for relevance and Robustness@10 for instruction-following consistency. The study employs zero-shot evaluation across various retrievers including BM25, ColBERT-v2.0, Contriever-msmarco, GTR variants, RepLLaMa, TART-dual, INSTRUCTOR variants, and E5-mistral-7b-instruct, comparing instruction-tuned models against their non-instruction-tuned counterparts.

## Key Results
- Instruction-tuned retrievers like INSTRUCTOR can underperform compared to non-instruction-tuned counterparts when evaluated on user-aligned instructions in INSTRUCTIR.
- Larger models demonstrate better instruction-following performance, especially when instruction-tuned, with RepLLaMa (7B) achieving the highest nDCG@10 of 87.62 and Robustness@10 of 52.58.
- The order of query and instruction affects INSTRUCTOR's performance significantly, performing better when query precedes instruction due to training data differences.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: INSTRUCTIR benchmark can effectively evaluate instruction-following ability by using instance-wise instructions tailored to each query.
- Mechanism: By generating diverse, user-aligned instructions for each query and revising target texts accordingly, the benchmark creates scenarios that reflect real-world search contexts and user preferences.
- Core assumption: The ability to follow instructions is crucial for information retrieval systems to provide more aligned search results based on user intentions and preferences.
- Evidence anchors:
  - [abstract]: "Our approach focuses on user-aligned instructions tailored to each query instance, reflecting the diverse characteristics inherent in real-world search scenarios."
  - [section]: "To effectively reflect the various intentions and situations that real-world users actually ask, employing instance-wise instructions for queries is more appropriate than relying on coarse-grained instructions that share the same task-specific guidance for various queries."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.5, average citations=0.0." (Weak corpus evidence, indicating limited prior work in this specific area)
- Break condition: If the generated instructions do not accurately reflect real-world user scenarios or fail to capture diverse user preferences and intentions, the benchmark's effectiveness in evaluating instruction-following ability would be compromised.

### Mechanism 2
- Claim: Instruction-tuned retrievers can underperform compared to non-instruction-tuned counterparts due to potential overfitting issues.
- Mechanism: When retrievers are fine-tuned on existing instruction-aware retrieval datasets with task-style instructions, they may not generalize well to the diverse, user-aligned instructions used in the INSTRUCTIR benchmark.
- Core assumption: Task-style instructions used in existing datasets may not capture the full range of user preferences and intentions present in real-world search scenarios.
- Evidence anchors:
  - [abstract]: "Through experimental analysis, we observe that retrievers fine-tuned to follow task-style instructions, such as INSTRUCTOR, can underperform compared to their non-instruction-tuned counterparts."
  - [section]: "However, both series of instruction-tuned baselines do not show superior performance than their non-instruction-tuned counterparts. Notably, INSTRUCTOR variants exhibit a huge performance drop compared to the backbone model GTR variants."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.5, average citations=0.0." (Weak corpus evidence, indicating limited prior work in this specific area)
- Break condition: If the task-style instructions used in existing datasets are representative of the diverse user-aligned instructions present in real-world search scenarios, the observed underperformance of instruction-tuned retrievers may not occur.

### Mechanism 3
- Claim: Larger models demonstrate better instruction-following performance, especially when instruction-tuned.
- Mechanism: As model size increases, the model's capacity to integrate and process complex instructions improves, leading to better alignment with user preferences and intentions.
- Core assumption: The enhanced capacity of larger models allows for a more effective integration of instruction-following abilities.
- Evidence anchors:
  - [section]: "It is not surprising that larger models derive greater benefits from instruction tuning, indicating that their enhanced capacity enables a more effective integration of instruction-following abilities."
  - [section]: "The largest model, RepLLaMa with 7B parameters, achieve the highest nDCG@10 of 87.62 and Robustness@10 of 52.58, indicating a strong correlation between model size and performance metrics in non-instruction-tuned settings."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.5, average citations=0.0." (Weak corpus evidence, indicating limited prior work in this specific area)
- Break condition: If the instruction-following ability does not scale with model size or if the enhanced capacity of larger models does not translate to better integration of instruction-following abilities, the observed performance improvement may not occur.

## Foundational Learning

- Concept: Understanding the importance of user intentions and preferences in information retrieval tasks.
  - Why needed here: To grasp the motivation behind creating a benchmark that evaluates instruction-following ability in information retrieval models.
  - Quick check question: Why is it important for information retrieval systems to align with user intentions and preferences?

- Concept: Recognizing the limitations of existing benchmarks in evaluating instruction-following ability.
  - Why needed here: To understand the need for a novel benchmark like INSTRUCTIR that focuses on user-aligned instructions tailored to each query instance.
  - Quick check question: What are the limitations of current benchmarks in assessing the instruction-following capabilities of information retrieval models?

- Concept: Understanding the process of generating diverse, user-aligned instructions for each query instance.
  - Why needed here: To comprehend the methodology used in creating the INSTRUCTIR benchmark and the importance of reflecting real-world search scenarios.
  - Quick check question: How does the process of generating instance-wise instructions for each query differ from using coarse-grained, task-specific instructions?

## Architecture Onboarding

- Component map: Query and target text selection from existing datasets (e.g., MSMARCO) -> Instruction generation using GPT-4, incorporating user-aligned information -> Target text revision to align with generated instructions -> Filtering process to ensure high-quality instances -> Evaluation using nDCG and Robustness metrics

- Critical path: The critical path involves generating high-quality instructions and revised target texts that accurately reflect user preferences and intentions, followed by a rigorous filtering process to ensure the final dataset's quality.

- Design tradeoffs:
  - Balancing the diversity of instructions with the relevance to the original query
  - Ensuring the revised target texts maintain their original meaning while incorporating user preferences
  - Striking a balance between the complexity of instructions and the model's ability to follow them

- Failure signatures:
  - Instructions that do not align with the original query or fail to capture user preferences
  - Revised target texts that lose their original meaning or fail to incorporate user preferences
  - Models that struggle to follow instructions due to overfitting on task-style instructions

- First 3 experiments:
  1. Evaluate the performance of instruction-tuned retrievers on the INSTRUCTIR benchmark and compare it with their non-instruction-tuned counterparts.
  2. Analyze the impact of instruction order (query followed by instruction vs. instruction followed by query) on the performance of instruction-tuned retrievers.
  3. Investigate the sensitivity of retrievers to paraphrased instructions and the importance of lexical overlap in following instructions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would instruction-aware retrievers perform if trained on user-aligned instruction datasets like INSTRUCTIR rather than task-description style instructions?
- Basis in paper: [explicit] The paper shows that instruction-tuned retrievers like INSTRUCTOR underperform compared to non-instruction-tuned counterparts when evaluated on user-aligned instructions in INSTRUCTIR.
- Why unresolved: The paper only evaluates retrievers in a zero-shot setting without instruction-aware training. It suggests exploring Reinforcement Learning from Human Feedback (RLHF) or developing more diverse instruction-aware training datasets, but does not conduct experiments with such training.
- What evidence would resolve it: Training retrievers on user-aligned instruction datasets and evaluating their performance on INSTRUCTIR would show whether instruction-aware training improves their ability to follow user-aligned instructions compared to task-description style training.

### Open Question 2
- Question: What is the optimal balance between lexical matching and semantic understanding for instruction-following retrievers?
- Basis in paper: [inferred] The paper shows that BM25 (lexical matching) and ColBERT-v2.0 (late interaction) have lower robustness scores compared to bi-encoder models, suggesting that purely lexical approaches struggle with instruction-following. However, removing high lexical overlap instances increases robustness but decreases nDCG scores.
- Why unresolved: The paper does not explore how to optimally combine lexical and semantic matching for instruction-following retrievers. It only shows the performance of existing models without proposing or testing hybrid approaches.
- What evidence would resolve it: Developing and evaluating hybrid models that combine lexical and semantic matching, and testing their performance on INSTRUCTIR, would show the optimal balance for instruction-following retrievers.

### Open Question 3
- Question: How does the order of query and instruction affect the performance of instruction-tuned retrievers, and why?
- Basis in paper: [explicit] The paper shows that INSTRUCTOR performs significantly better when the query precedes the instruction, compared to when the instruction precedes the query. It attributes this to the difference in instruction length between training data (12.16 tokens) and INSTRUCTIR (64.47 tokens).
- Why unresolved: While the paper identifies this issue and provides a hypothesis, it does not explore the underlying mechanisms or test solutions to improve performance when instructions precede queries.
- What evidence would resolve it: Conducting experiments to test different query-instruction orderings during training and inference, and analyzing the attention patterns or embeddings of instruction-tuned retrievers, would reveal the mechanisms behind this performance difference and potential solutions.

## Limitations
- The effectiveness of INSTRUCTIR relies heavily on the quality and diversity of GPT-4-generated instructions, which are not fully disclosed in the paper.
- The benchmark's ability to capture "real-world" user preferences is assumed but not directly validated through user studies or real user interaction data.
- The performance drop observed in instruction-tuned models like INSTRUCTOR may be influenced by factors beyond instruction-following ability, such as differences in training data or model architectures.

## Confidence

- **High Confidence**: The methodology for creating instance-wise instructions and revised target texts is well-described and technically sound. The observation that larger models benefit more from instruction tuning is supported by clear performance metrics across multiple model sizes.
- **Medium Confidence**: The claim that task-style instructions lead to overfitting in instruction-tuned retrievers is plausible based on experimental results, but the paper does not fully explore alternative explanations for the performance differences observed. The correlation between model size and instruction-following performance is observed but not deeply analyzed.
- **Low Confidence**: The assertion that INSTRUCTIR captures "real-world" user preferences is not directly validated with user studies or real user interaction data. The generalizability of findings to other domains or languages is not explored, despite the introduction of a multilingual benchmark (mFollowIR) in related work.

## Next Checks

1. **Prompt Analysis**: Conduct ablation studies by systematically varying the GPT-4 prompts used to generate instructions to determine which prompt elements most strongly influence instruction quality and model performance.

2. **User Study Validation**: Design and execute a small-scale user study where real users evaluate the quality and relevance of the generated instructions and revised target texts to validate that they accurately reflect user preferences and intentions.

3. **Cross-Dataset Transfer**: Evaluate whether models trained on INSTRUCTIR can generalize to other instruction-following benchmarks like mFollowIR or Promptriever, and vice versa, to assess the benchmark's robustness and generalizability across different instruction styles and domains.