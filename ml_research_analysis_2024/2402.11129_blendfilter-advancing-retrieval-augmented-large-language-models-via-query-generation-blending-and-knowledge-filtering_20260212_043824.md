---
ver: rpa2
title: 'BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query
  Generation Blending and Knowledge Filtering'
arxiv_id: '2402.11129'
source_url: https://arxiv.org/abs/2402.11129
tags:
- knowledge
- query
- question
- answer
- retrieved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BlendFilter, a novel framework for retrieval-augmented
  large language models that integrates query generation blending and knowledge filtering.
  The core idea is to enhance retrieval quality by combining external and internal
  knowledge augmentation with the original query, and then filtering out irrelevant
  retrieved documents using the LLM itself.
---

# BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering

## Quick Facts
- arXiv ID: 2402.11129
- Source URL: https://arxiv.org/abs/2402.11129
- Authors: Haoyu Wang; Ruirui Li; Haoming Jiang; Jinjin Tian; Zhengyang Wang; Chen Luo; Xianfeng Tang; Monica Cheng; Tuo Zhao; Jing Gao
- Reference count: 28
- Primary result: BlendFilter achieves 9.7%, 7.4%, and 14.2% average improvements across GPT3.5-turbo-Instruct, Vicuna 1.5-13b, and Qwen-7b backbones respectively on open-domain question answering benchmarks

## Executive Summary
BlendFilter is a novel framework for retrieval-augmented large language models (LLMs) that integrates query generation blending and knowledge filtering to enhance retrieval quality and answer accuracy. The method combines external and internal knowledge augmentation with the original query to create comprehensive query representations, then filters out irrelevant retrieved documents using the LLM itself before generating answers. Experiments on three open-domain question answering benchmarks (HotPotQA, 2WikiMultiHopQA, StrategyQA) demonstrate significant performance improvements over state-of-the-art baselines, with average gains of 9.7%, 7.4%, and 14.2% across different LLM backbones. The framework also shows superior retrieval performance with higher precision, recall, and a newly proposed S-Precision metric while maintaining strong generalization across different retrievers.

## Method Summary
BlendFilter operates through three main stages: query generation blending, knowledge filtering, and answer generation. First, it generates three augmented queries by combining the original question with external knowledge-augmented and internal knowledge-augmented versions. External augmentation uses retrieved documents to enrich the query with domain-specific keywords, while internal augmentation surfaces factual knowledge from the LLM's parametric memory. The retriever then produces three document sets corresponding to each query type. The knowledge filtering module uses the LLM itself to independently evaluate and filter each document set, removing irrelevant content before the union operation. Finally, the answer generation module employs Chain-of-Thought reasoning with the filtered knowledge to produce the final answer.

## Key Results
- BlendFilter achieves average improvements of 9.7%, 7.4%, and 14.2% across GPT3.5-turbo-Instruct, Vicuna 1.5-13b, and Qwen-7b backbones respectively
- Shows superior retrieval performance with higher precision, recall, and newly proposed S-Precision metric
- Demonstrates strong generalization across different retrievers (ColBERT v2 and BM25)
- Maintains effectiveness across three open-domain question answering benchmarks (HotPotQA, 2WikiMultiHopQA, StrategyQA)

## Why This Works (Mechanism)

### Mechanism 1
Blending multiple query augmentations (original + external knowledge-augmented + internal knowledge-augmented) improves retrieval recall for complex questions. Each augmentation source captures different aspects of the question - external brings domain-specific keywords, internal surfaces factual knowledge from LLM's parametric memory, and original preserves precise question intent. Combining them increases chances of matching relevant documents across diverse semantic spaces.

### Mechanism 2
Knowledge filtering with the LLM itself reduces noise in retrieved documents, improving answer accuracy. The LLM acts as a relevance classifier, independently evaluating each document set before union. This selective inclusion prevents irrelevant documents from contaminating the final knowledge pool used for generation.

### Mechanism 3
The order of filtering before union is more effective than union-before-filtering. Filtering small, query-specific document sets is easier for the LLM than filtering a large union set. This preserves precision by reducing cognitive load on the LLM during relevance assessment.

## Foundational Learning

- **Concept**: Retrieval-augmented generation (RAG) and its limitations with complex queries
  - Why needed: Understanding why simple direct retrieval fails for multi-hop or implicit reasoning questions is key to grasping motivation for query augmentation
  - Quick check: Why might a complex question like "What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?" fail with direct retrieval?

- **Concept**: Query augmentation strategies (decomposition, rewriting, augmentation) and their tradeoffs
  - Why needed: Knowing differences helps understand why BlendFilter chose augmentation over decomposition or rewriting
  - Quick check: What is the main advantage of query augmentation over query rewriting in context of BlendFilter?

- **Concept**: Knowledge filtering and noise reduction in RAG systems
  - Why needed: Understanding how filtering improves generation quality is critical to appreciating knowledge filtering module
  - Quick check: What is a potential downside of using the LLM itself as a knowledge filter?

## Architecture Onboarding

- **Component map**: Input query → Query Generation Blending module (3 augmented queries) → Retriever (3 document sets) → Knowledge Filtering module (filters each set independently) → Union of filtered documents → Answer Generation module (LLM with CoT) → Output answer
- **Critical path**: Query Generation Blending → Retrieval → Knowledge Filtering → Answer Generation
- **Design tradeoffs**: Blending vs. single augmentation (higher recall but more computation), LLM filtering vs. external classifier (no training needed but depends on LLM reliability), filtering before union vs. after (better precision but requires three filtering steps)
- **Failure signatures**: Performance drops when removing any augmentation source, low S-Precision despite high recall, degradation with larger K values
- **First 3 experiments**: 1) Ablation study: Remove each augmentation source and measure impact on EM/F1, 2) Filter order comparison: Test union-before-filtering vs. filtering-before-union, 3) K sensitivity test: Vary number of retrieved documents and measure precision-recall tradeoff

## Open Questions the Paper Calls Out
- How does BlendFilter's performance scale with increasing knowledge base size and complexity?
- What is the computational overhead of BlendFilter compared to baseline methods, particularly for the knowledge filtering step?
- How does BlendFilter handle ambiguous or contradictory information in the retrieved knowledge documents?

## Limitations
- No ablation studies on individual query augmentation sources to quantify their specific contributions
- Limited retriever comparison only testing ColBERT v2 and BM25, missing more advanced retrievers like SPLADE
- Experiments restricted to Wikipedia-based question answering tasks without testing on other knowledge domains

## Confidence
- **Claim: 9.7%, 7.4%, and 14.2% average improvements** - High Confidence: Directly supported by experimental results in Table 3
- **Claim: Knowledge filtering with LLM itself is effective** - Medium Confidence: Supported by one related work (SKILL-RAG) but lacks comprehensive ablation studies
- **Claim: Blending multiple query augmentations improves retrieval recall** - Low-Medium Confidence: Logically sound but lacks direct empirical evidence through systematic ablation

## Next Checks
1. Conduct ablation study removing each augmentation source individually to quantify their contributions
2. Test BlendFilter on non-Wikipedia knowledge sources (scientific papers, news articles, specialized domains) for cross-domain evaluation
3. Perform systematic comparison between filtering-before-union and union-before-filtering approaches with quantitative metrics