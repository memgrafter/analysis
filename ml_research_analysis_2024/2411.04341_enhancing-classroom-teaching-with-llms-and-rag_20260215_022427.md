---
ver: rpa2
title: Enhancing classroom teaching with LLMs and RAG
arxiv_id: '2411.04341'
source_url: https://arxiv.org/abs/2411.04341
tags:
- data
- source
- chunk
- llms
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated the effectiveness of Retrieval-Augmented Generation
  (RAG) using Reddit as a data source for cybersecurity education. A RAG pipeline
  was built with Llama as the LLM, and data was scraped from cybersecurity subreddits
  to create a vector database.
---

# Enhancing classroom teaching with LLMs and RAG

## Quick Facts
- arXiv ID: 2411.04341
- Source URL: https://arxiv.org/abs/2411.04341
- Reference count: 9
- Key outcome: RAG pipeline with Reddit data showed ~50% answer correctness for cybersecurity questions, indicating Reddit is unsuitable for RAG applications due to noisy, topic-diverse content

## Executive Summary
This study evaluated the effectiveness of Retrieval-Augmented Generation (RAG) for cybersecurity education using Reddit as a data source. A RAG pipeline was built with Llama as the LLM and Chroma vector database, testing chunk sizes from 250-8000 characters with 30 cybersecurity questions. The results showed average answer correctness around 50% regardless of chunk size, demonstrating that Reddit's noisy, constantly changing content makes it unsuitable for RAG applications in cybersecurity education. The methodology could be adapted for other educational contexts to evaluate teaching materials or create personalized AI tutors.

## Method Summary
The study built a RAG pipeline using Llama LLM with cybersecurity data scraped from Reddit subreddits, stored in a Chroma vector database. Different chunk sizes (250-8000 characters) were tested to determine optimal context for generating accurate answers to 30 cybersecurity questions. Answer correctness was evaluated using RAGAS, with ground truths sourced from CISA. The experiment systematically varied chunk size while keeping the data source constant to isolate the effect of context length on answer quality.

## Key Results
- Average answer correctness was approximately 50% across all chunk sizes tested
- Chunk sizes of 250 and 4000 characters performed slightly better than others
- Little variability in performance across chunk sizes suggests data source quality was the limiting factor
- Reddit's noisy, topic-diverse format makes it unsuitable for cybersecurity RAG applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG improves LLM response accuracy by supplementing training data with external knowledge
- Mechanism: Vector database retrieves relevant documents based on semantic similarity, which are then included in the LLM's context window for generation
- Core assumption: The external knowledge source contains accurate, relevant information for the query domain
- Evidence anchors:
  - [abstract] "The merging of pre-trained parametric memory (state-of-the-art LLMs) and non-parametric memory, such as a vector database, in retrieval-augmented generation (RAG) models provides a method to give LLMs access to updated information without undergoing new training"
  - [section] "We used Llama as our LLM because of its availability. The top 500 posts and the most upvoted answer were scraped from subreddits pertaining to cybersecurity and network security then stored in a Chroma vector database to serve as the data source for the RAG"
  - [corpus] No direct evidence found in corpus papers for this specific mechanism
- Break condition: When retrieved documents contain irrelevant or noisy information that confuses the LLM

### Mechanism 2
- Claim: Chunk size affects the quality of retrieved context and subsequent LLM responses
- Mechanism: Smaller chunks provide more focused context while larger chunks provide more comprehensive context; optimal size depends on the domain and query complexity
- Core assumption: There exists an optimal chunk size that balances context relevance and completeness
- Evidence anchors:
  - [section] "Chunk size is evaluated to determine the optimal amount of context needed to generate accurate answers... The process was repeated for chunk sizes of 250, 500, 1000, 2000, 4000, and 8000 characters"
  - [section] "Potential effects are thought to be better answers with smaller chunks due to smaller chunks containing more specific context or better answers with larger chunks because larger chunks contain more context"
  - [corpus] No direct evidence found in corpus papers for this specific mechanism
- Break condition: When chunk size variations don't significantly affect answer correctness due to poor data source quality

### Mechanism 3
- Claim: RAG can be used to evaluate the quality of educational resources
- Mechanism: By using assessment questions and ground truths with RAG, the answer correctness scores indicate how well the source materials align with learning objectives
- Core assumption: High answer correctness scores correlate with effective educational materials
- Evidence anchors:
  - [section] "This model can also be used by teachers to vet the appropriateness of teaching materials or the effectiveness of their assessment questions"
  - [section] "Using RAGAs, teachers can analyze answer correctness of the LLM. If the answer correctness scores are high, this would suggest that the educational materials are appropriate for the learning goals"
  - [corpus] No direct evidence found in corpus papers for this specific mechanism
- Break condition: When the RAG system consistently produces incorrect answers regardless of material quality

## Foundational Learning

- Concept: Vector embeddings and semantic search
  - Why needed here: The RAG system relies on converting documents into vector representations for semantic similarity search
  - Quick check question: What is the mathematical relationship between document vectors and query vectors in vector database retrieval?

- Concept: Chunking strategies for text processing
  - Why needed here: Different chunk sizes affect how context is presented to the LLM and can impact answer quality
  - Quick check question: What are the trade-offs between overlapping vs. non-overlapping chunks in a RAG pipeline?

- Concept: Evaluation metrics for RAG systems
  - Why needed here: RAGAS provides automated evaluation of answer correctness, which is critical for assessing system performance
  - Quick check question: How does RAGAS measure answer correctness compared to traditional ROUGE or BLEU metrics?

## Architecture Onboarding

- Component map: Data collection (Reddit scraping) -> Preprocessing (Text chunking) -> Vector database (Chroma) -> LLM (Llama) -> Retrieval (Semantic search) -> Generation (LLM) -> Evaluation (RAGAS)
- Critical path: Question → Vector database search → Retrieved chunks → LLM context → Generated answer → RAGAS evaluation
- Design tradeoffs:
  - Chunk size vs. context relevance: Smaller chunks may be more relevant but miss broader context
  - Data source selection: Specialized sources may provide better quality but less coverage
  - LLM choice: Open-source models vs. proprietary models for cost and customization
  - Evaluation method: Automated vs. human evaluation for scalability vs. accuracy
- Failure signatures:
  - Consistently low answer correctness scores indicate poor data source quality
  - High variability in chunk size performance suggests the retrieval mechanism needs optimization
  - LLM hallucinations despite good retrieval suggest insufficient context or model limitations
- First 3 experiments:
  1. Test different chunk sizes (250, 1000, 4000, 8000 characters) with a known good data source to establish baseline performance
  2. Compare retrieval-only vs. retrieval-and-generation to isolate the impact of context retrieval quality
  3. Test different vector database configurations (embedding models, similarity metrics) to optimize retrieval effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of data source impact the effectiveness of RAG pipelines for cybersecurity education?
- Basis in paper: [explicit] The study explicitly states that Reddit is not a suitable source for cybersecurity RAG applications due to its noisy, topic-diverse format and constantly changing content.
- Why unresolved: The study only tested Reddit as a data source. Other potential sources like academic journals, official cybersecurity reports, or curated educational content were not evaluated.
- What evidence would resolve it: Conducting similar experiments using alternative data sources like official cybersecurity databases, academic papers, or curated educational content to compare their effectiveness in providing accurate answers for cybersecurity questions.

### Open Question 2
- Question: What is the optimal chunk size for RAG pipelines in educational contexts, and how does it vary with different data sources?
- Basis in paper: [explicit] The study tested chunk sizes from 250 to 8000 characters but found little variability in answer correctness, suggesting chunk size might have a greater impact with better-aligned data sources.
- Why unresolved: The study did not find significant effects of chunk size due to the unsuitability of Reddit as a data source. The optimal chunk size might vary depending on the quality and nature of the data source.
- What evidence would resolve it: Repeating the experiments with different data sources known to be more aligned with cybersecurity topics to observe if chunk size has a more pronounced effect on answer accuracy.

### Open Question 3
- Question: Can RAG pipelines be effectively used to create personalized AI tutors for K-12 education, and what are the best practices for implementing them?
- Basis in paper: [inferred] The study suggests that the methodology could be adapted for other educational contexts to evaluate teaching materials or create personalized AI tutors, indicating potential for broader educational applications.
- Why unresolved: The study focused on cybersecurity education and did not explore the application of RAG pipelines in other educational subjects or age groups. The effectiveness and best practices for implementing personalized AI tutors in K-12 education remain unexplored.
- What evidence would resolve it: Conducting pilot studies in various K-12 subjects using RAG pipelines to create personalized AI tutors, evaluating their effectiveness, and documenting best practices for implementation.

## Limitations
- The choice of Reddit as a data source introduced significant noise and topic diversity that undermined RAG effectiveness
- Results may not generalize to other domains or more structured data sources
- The study only tested one specific domain (cybersecurity) with one data source, limiting broader applicability

## Confidence
- **Medium confidence** in the RAG methodology itself, as the framework follows established practices but results are limited by data source quality
- **Low confidence** in the generalizability of results to other educational contexts, since the study only tested one specific domain (cybersecurity) with one data source (Reddit)
- **Medium confidence** in the chunk size analysis, as the study systematically tested multiple sizes but found no significant performance differences, possibly due to the underlying data quality issues

## Next Checks
1. **Data Source Validation:** Test the RAG pipeline with a more structured, domain-specific knowledge base (e.g., cybersecurity white papers or official documentation) to determine if the 50% accuracy ceiling is due to the RAG methodology or the Reddit data source quality.

2. **Chunk Size Impact Isolation:** Run controlled experiments with synthetic, high-quality data to isolate whether chunk size variations have meaningful impact on answer correctness independent of data source noise.

3. **Alternative Evaluation Methods:** Implement human evaluation alongside RAGAS scoring to validate the automated metric's reliability, particularly for assessing nuanced cybersecurity concepts where automated scoring may miss contextual accuracy.