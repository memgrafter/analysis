---
ver: rpa2
title: Human Evaluation of English--Irish Transformer-Based NMT
arxiv_id: '2403.02366'
source_url: https://arxiv.org/abs/2403.02366
tags:
- translation
- transformer
- machine
- evaluation
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents a human evaluation of Transformer-based neural\
  \ machine translation (NMT) for the low-resource English\u2013Irish language pair.\
  \ The research explores how hyperparameter settings impact translation quality by\
  \ comparing Transformer and Recurrent Neural Network (RNN) architectures."
---

# Human Evaluation of English--Irish Transformer-Based NMT

## Quick Facts
- arXiv ID: 2403.02366
- Source URL: https://arxiv.org/abs/2403.02366
- Authors: Séamus Lankford; Haithem Afli; Andy Way
- Reference count: 40
- Primary result: Human evaluation shows Transformer-based NMT outperforms RNN-based models for English-Irish translation

## Executive Summary
This study presents a comprehensive human evaluation of Transformer-based neural machine translation for the low-resource English-Irish language pair. The research systematically explores how hyperparameter settings impact translation quality by comparing Transformer and Recurrent Neural Network (RNN) architectures. Through extensive experimentation with various SentencePiece subword models and regularization techniques, the researchers identify optimal configurations that significantly improve translation quality over baseline approaches.

The study demonstrates that the best-performing model - using a 16k BPE subword model with a Transformer architecture - achieved a 7.8-point improvement in BLEU score over the baseline RNN model. When benchmarked against Google Translate, the optimized models demonstrated significant improvements. The fine-grained manual evaluation using the Multidimensional Quality Metrics (MQM) error taxonomy revealed that the Transformer system significantly reduced both accuracy and fluency errors compared to the RNN-based model, validating the effectiveness of the proposed approach for this challenging low-resource language pair.

## Method Summary
The research employed a multi-faceted approach to evaluate English-Irish neural machine translation. Researchers first conducted automatic evaluation using BLEU, TER, and ChrF metrics on a 1.3k line test set. They then performed extensive hyperparameter optimization (HPO) using random search to optimize Transformer models, varying parameters including learning rate, batch size, attention heads, number of layers, dropout, and label smoothing. Different subword models were tested, including BPE with vocabulary sizes of 4k, 8k, 16k, and 32k, as well as unigram approaches.

The human evaluation component utilized two metrics: Scalar Quality Metric (SQM) for overall quality assessment and Multidimensional Quality Metrics (MQM) for detailed error analysis. Annotators evaluated 100 sentences from the test set using the MQM taxonomy, categorizing errors into accuracy and fluency types with severity levels (minor, major, critical). The study also benchmarked the optimized models against Google Translate to provide industry comparison context.

## Key Results
- Transformer-based NMT achieved a 7.8-point improvement in BLEU score over the baseline RNN model
- The 16k BPE subword model with Transformer architecture demonstrated the best overall performance
- Human evaluation using MQM revealed significant reduction in both accuracy and fluency errors with the Transformer system compared to RNN-based models
- Optimized models showed substantial improvements when benchmarked against Google Translate

## Why This Works (Mechanism)
The Transformer architecture's self-attention mechanism enables better handling of long-range dependencies and context in translation, which is particularly beneficial for the morphologically rich Irish language. The subword segmentation using SentencePiece allows for more effective handling of Irish's complex morphology and inflection patterns. The random search HPO approach systematically explores the hyperparameter space to find configurations that balance model capacity with regularization, preventing overfitting on the limited training data. The MQM-based human evaluation provides granular insights into specific error types that automatic metrics might miss, allowing for targeted improvements in translation quality.

## Foundational Learning
- **Neural Machine Translation**: Why needed - Core technology being evaluated; Quick check - Understand encoder-decoder architecture and attention mechanisms
- **Transformer Architecture**: Why needed - Primary model being optimized; Quick check - Know about self-attention, multi-head attention, and positional encoding
- **SentencePiece Subword Models**: Why needed - Critical for handling morphologically rich languages; Quick check - Understand BPE vs unigram approaches and vocabulary size impact
- **Hyperparameter Optimization**: Why needed - Essential for model performance; Quick check - Know random search methodology and parameter tuning
- **Multidimensional Quality Metrics (MQM)**: Why needed - Framework for human evaluation; Quick check - Understand error categorization and severity levels
- **Low-resource Language Pairs**: Why needed - Context for methodology choices; Quick check - Recognize challenges of limited training data

## Architecture Onboarding

**Component Map**: English-Irish Parallel Corpus -> Preprocessing (SentencePiece) -> RNN/Transformer Models -> HPO (Random Search) -> Automatic Evaluation (BLEU/TER/ChrF) -> Human Evaluation (SQM/MQM) -> Benchmarking vs Google Translate

**Critical Path**: Data preprocessing → Model training with HPO → Automatic evaluation → Human evaluation → Benchmarking

**Design Tradeoffs**: Transformer vs RNN (computational cost vs performance), BPE vs unigram (segmentation quality vs complexity), vocabulary size (coverage vs overfitting), HPO method (random search comprehensiveness vs computational cost)

**Failure Signatures**: Poor BLEU scores indicate model capacity or data issues, high MQM error rates suggest architectural or training problems, overfitting shown by dev set performance degradation

**Three First Experiments**:
1. Train baseline RNN model with default hyperparameters on English-Irish data
2. Implement SentencePiece BPE with different vocabulary sizes (4k, 8k, 16k, 32k)
3. Conduct random search HPO with Transformer architecture varying attention heads and layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Transformer-based NMT models handle gender agreement and case inflections in Irish compared to RNN-based models?
- Basis in paper: The paper discusses the challenges of translating into morphologically rich languages like Irish and notes that the Transformer system significantly reduces both accuracy and fluency errors when compared with an RNN-based model.
- Why unresolved: While the paper indicates that the Transformer model performs better, it does not provide a detailed analysis of how specific grammatical categories such as gender and case inflections are handled.
- What evidence would resolve it: A detailed linguistic analysis comparing the performance of Transformer and RNN models on gender and case inflections in Irish translations would provide clarity.

### Open Question 2
- Question: What is the optimal subword model type and vocabulary size for English-Irish NMT?
- Basis in paper: The paper evaluates different subword models (BPE and unigram approaches) and vocabulary sizes (4k, 8k, 16k, 32k) and finds that a 16k BPE subword model yields the best performance.
- Why unresolved: While the paper identifies the best-performing model, it does not explore whether even larger or smaller vocabulary sizes or different subword model types could yield better results.
- What evidence would resolve it: Further experiments with a wider range of vocabulary sizes and subword model types would determine the true optimal configuration.

### Open Question 3
- Question: How does the environmental impact of Transformer-based NMT models compare to RNN-based models?
- Basis in paper: The paper discusses the environmental impact of NMT models, noting that Transformer models were trained on local servers with higher carbon emissions compared to RNN models trained on carbon-neutral Google Colab servers.
- Why unresolved: The paper does not provide a direct comparison of the environmental impact between Transformer and RNN models.
- What evidence would resolve it: A detailed analysis of the energy consumption and carbon emissions of both Transformer and RNN models during training and inference would provide a clear comparison.

## Limitations
- The human evaluation methodology lacks detailed specifications for annotator training and inter-annotator agreement metrics
- Limited training data for the English-Irish language pair may affect generalizability of results
- Environmental impact comparison between Transformer and RNN models was not directly measured or compared

## Confidence

**High Confidence**: BLEU score improvements (7.8-point increase over RNN baseline), Transformer outperforming RNN on automatic metrics, qualitative findings about accuracy and fluency improvements

**Medium Confidence**: Claims about relative performance compared to Google Translate, subword model selection impact (16k BPE), overall human evaluation methodology validity

**Low Confidence**: Specific MQM error counts and distributions, exact hyperparameter optimization process, replicability of human evaluation scores due to subjective annotation

## Next Checks
1. Replicate the BLEU score improvement by training the baseline RNN model and the optimized Transformer model on the same English-Irish parallel dataset with specified hyperparameters
2. Conduct a pilot MQM annotation study using the same error taxonomy to verify inter-annotator agreement and annotation consistency
3. Perform ablation studies on subword model configurations (varying BPE vocabulary sizes and unigram approaches) to confirm the 16k BPE model selection and its impact on translation quality