---
ver: rpa2
title: 'TINED: GNNs-to-MLPs by Teacher Injection and Dirichlet Energy Distillation'
arxiv_id: '2412.11180'
source_url: https://arxiv.org/abs/2412.11180
tags:
- tined
- teacher
- graph
- layer
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TINED, a novel method for distilling knowledge
  from Graph Neural Networks (GNNs) to Multi-Layer Perceptrons (MLPs). TINED addresses
  the challenge of leveraging GNNs' superior performance on graph-structured data
  while overcoming their high computational cost during inference.
---

# TINED: GNNs-to-MLPs by Teacher Injection and Dirichlet Energy Distillation

## Quick Facts
- arXiv ID: 2412.11180
- Source URL: https://arxiv.org/abs/2412.11180
- Authors: Ziang Zhou; Zhihao Ding; Jieming Shi; Qing Li; Shiqi Shen
- Reference count: 40
- Key outcome: Achieves 74.43% accuracy on Citeseer, improving upon GraphSAGE teacher by 3.94% and outperforming other leading distillation methods by up to 3.21%, while being 94x faster

## Executive Summary
This paper presents TINED, a novel method for distilling knowledge from Graph Neural Networks (GNNs) to Multi-Layer Perceptrons (MLPs). TINED addresses the challenge of leveraging GNNs' superior performance on graph-structured data while overcoming their high computational cost during inference. The core method combines Teacher Injection and Dirichlet Energy Distillation to transfer both parameters and structural properties from GNNs to MLPs.

The method achieves superior performance compared to existing methods across seven benchmark datasets, demonstrating that MLPs can effectively approximate GNNs when proper knowledge transfer techniques are applied. TINED shows consistent performance improvements across various settings, including transductive, inductive, and production environments, with different GNN architectures as teachers.

## Method Summary
TINED combines two key techniques to distill GNNs into MLPs: Teacher Injection and Dirichlet Energy Distillation. Teacher Injection directly transfers parameters from GNN layers to corresponding MLP layers by recognizing that feature transformation operations in GNNs are computationally equivalent to fully-connected layers in MLPs. Dirichlet Energy Distillation preserves the unique smoothing effects of GNN operations within the MLP, using Dirichlet energy to quantify and maintain these layer-specific properties. The method achieves fast inference speeds while maintaining high accuracy, with experimental results showing 94x speedup over the GNN teacher while achieving competitive performance across multiple benchmark datasets.

## Key Results
- Citeseer: 74.43% accuracy, improving upon GraphSAGE teacher by 3.94%
- Citeseer: Outperforms other leading distillation methods by up to 3.21%
- 94x faster inference speed compared to GNN teacher
- Consistent performance improvements across transductive, inductive, and production environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature Transformation (FT) operations in GNNs are computationally equivalent to fully-connected (FC) layers in MLPs
- Mechanism: Both FT and FC operations share the same mathematical formulation: a linear transformation using a learnable weight matrix followed by an activation function, allowing direct parameter transfer
- Core assumption: FT operations operate independently of graph structure, making them directly transferable to standard neural network operations
- Evidence anchors:
  - [abstract]: "We recognize that FT is computationally equivalent to a fully-connected (FC) layer in MLPs."
  - [section]: "Observe that (i) FT(l) operates independently of the graph, whereas GP(l) requires the graph; (ii) FT(l) in Eq. (4) has the same formulation as an FC layer of an MLP in Eq. (2)."

### Mechanism 2
- Claim: Graph Propagation (GP) operations in GNNs can be approximated by FC layers in MLPs with bounded error
- Mechanism: GP operations, which aggregate neighbor information, can be approximated by FC layers with error bounded by the largest eigenvalue of the graph Laplacian matrix
- Core assumption: Graph structure can be sufficiently captured through learned linear transformations in the MLP without explicit graph propagation
- Evidence anchors:
  - [abstract]: "We also establish a theoretical bound for GP approximation."
  - [section]: "Theorem 4.1. For a sparse matrix L ∈ Rn×n and a feature matrix H ∈ Rn×d with rank(H) = d, there exists a transformation matrix W∗ to approximate LH by HW∗ with relative error ||LH − HW∗||F / ||H||F ≤ λmax(L)."

### Mechanism 3
- Claim: FT and GP operations exhibit opposing smoothing effects that can be quantified and preserved through Dirichlet Energy Distillation
- Mechanism: GP operations aggressively smooth node embeddings while FT operations are more conservative or even diversify embeddings; Dirichlet energy measures this smoothing effect and the distillation process preserves these layer-specific properties
- Core assumption: Smoothing characteristics of GNN operations are important for maintaining performance and can be measured and transferred to MLPs
- Evidence anchors:
  - [abstract]: "Furthermore, we note that FT and GP operations in GNN layers often exhibit opposing smoothing effects: GP is aggressive, while FT is conservative. Using Dirichlet energy, we develop a DE ratio to measure these effects."
  - [section]: "We make the following two consistent observations about DE ratio. (i) Within the same layer for l = 1, 2, the DE ratio RFT(l) of FT(l) consistently exceeds the DE ratio RGP(l) of GP(l), demonstrating that GP(l) operation actively smooths embeddings, whereas FT(l) operation is relatively conservative for smoothing."

## Foundational Learning

- Concept: Graph Neural Networks and Message Passing Framework
  - Why needed here: Understanding FT and GP operations in GNN layers is fundamental to grasping how TINED transfers knowledge to MLPs
  - Quick check question: What are the two main operations in a typical GNN layer, and how do they differ in their relationship to graph structure?

- Concept: Knowledge Distillation in Neural Networks
  - Why needed here: TINED builds on knowledge distillation principles, transferring knowledge from a complex teacher model (GNN) to a simpler student model (MLP)
  - Quick check question: What is the primary goal of knowledge distillation, and how does it typically work between teacher and student models?

- Concept: Dirichlet Energy and Graph Smoothing
  - Why needed here: Dirichlet energy is used to quantify the smoothing effects of GNN operations, crucial for understanding the Dirichlet Energy Distillation component
  - Quick check question: How does Dirichlet energy measure the smoothness of node embeddings in a graph, and what does a lower value indicate?

## Architecture Onboarding

- Component map: MLP student with 2T layers → Teacher Injection (FT parameter transfer) → Dirichlet Energy Distillation (DE ratio preservation)
- Critical path: For each GNN layer: extract FT parameters → inject into MLP FC layer → fine-tune with gradient modifier → calculate DE ratios → apply DED loss during training
- Design tradeoffs: Direct parameter transfer (Teacher Injection) vs. learned approximation, computational efficiency vs. accuracy preservation, explicit graph structure vs. learned representations
- Failure signatures: Poor performance if FT/GP operations don't exhibit opposing smoothing behaviors, large approximation errors in GP emulation, instability in DE ratio calculations
- First 3 experiments:
  1. Verify FT parameter transfer: Train 2-layer GNN, extract FT parameters, inject into MLP, check if initial MLP performance matches teacher
  2. Test DE ratio preservation: Calculate ground truth DE ratios from teacher GNN, implement DED loss, verify student MLP DE ratios converge to teacher values
  3. Evaluate approximation bound: Measure ||GP(l)(H) - FCl,1(H)||F / ||H||F for both TINED and baseline methods to confirm theoretical bound

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Dirichlet Energy Distillation technique perform when applied to heterogeneous graphs or dynamic graphs, which were not covered in the current experiments?
- Basis in paper: [explicit] The paper states "Moreover, we will explore the potential of TINED in other types of graphs, including heterogeneous graphs and dynamic graphs, to further enhance its applicability and effectiveness."
- Why unresolved: Experiments only evaluated TINED on standard homogeneous graph datasets, leaving its performance on more complex graph types unexplored
- What evidence would resolve it: Experimental results comparing TINED's performance on heterogeneous graphs (like HGB datasets) or dynamic graphs (like temporal citation networks) against baseline methods would clarify its effectiveness in these settings

### Open Question 2
- Question: What is the theoretical relationship between the approximation error bound established for GP operations and the actual performance degradation when using MLPs instead of GNNs for inference?
- Basis in paper: [explicit] Theorem 4.1 provides an approximation bound between GP operations in GNNs and FC layers in MLPs, but this is theoretical
- Why unresolved: The paper establishes a theoretical bound but does not empirically connect this bound to practical performance differences between GNNs and MLPs
- What evidence would resolve it: Empirical studies measuring the correlation between the theoretical approximation error and actual accuracy loss when deploying TINED MLPs versus original GNNs would bridge this gap

### Open Question 3
- Question: What is the impact of using different sampling ratios (ζ) for Dirichlet Energy calculation on large graphs, and how does this affect the trade-off between computational efficiency and distillation quality?
- Basis in paper: [explicit] The paper mentions that for large datasets, a sampling ratio ζ is used to compute approximate DE values to avoid memory overflow, with ζ values searched from [0.001, 0.005, 0.1, 0.4, 1]
- Why unresolved: While the paper reports that TINED maintains stable performance across different ζ values, it doesn't systematically analyze how sampling ratio affects the balance between efficiency and accuracy
- What evidence would resolve it: A detailed ablation study varying ζ across a wider range while measuring both computational time and accuracy would quantify the efficiency-accuracy trade-off

## Limitations
- The theoretical approximation bound for GP operations relies on spectral properties of the graph Laplacian that may not hold for all graph types
- Dirichlet Energy Distillation introduces hyperparameters (sampling ratio ζ, smoothing function choice) that are not fully specified in the methodology
- Exact implementation details for Dirichlet Energy computation at scale and optimal hyperparameter settings are not fully specified

## Confidence
- **High Confidence**: The core mechanism of Teacher Injection (parameter transfer from GNN FT layers to MLP FC layers) is mathematically sound and well-supported by the equivalence of their formulations
- **Medium Confidence**: The theoretical bound for GP approximation is valid but its practical impact depends on graph-specific spectral properties that vary across datasets
- **Low Confidence**: The exact implementation details for Dirichlet Energy computation at scale and the optimal hyperparameter settings for the DE ratio calculations are not fully specified

## Next Checks
1. **Spectral Property Analysis**: For each dataset, measure λ_max(L) of the graph Laplacian and correlate it with the approximation error ||GP(H) - FC(H)||_F / ||H||_F to validate whether the theoretical bound accurately predicts empirical performance across different graph structures

2. **Hyperparameter Sensitivity**: Systematically vary the sampling ratio ζ (e.g., 0.1, 0.3, 0.5, 0.7, 1.0) and smoothing function choices in DE ratio computation to identify their impact on final accuracy and clarify the robustness of Dirichlet Energy Distillation to implementation choices

3. **Cross-Architecture Generalization**: Test TINED with additional GNN architectures beyond those evaluated (e.g., Graph Attention Networks with different attention mechanisms, Message Passing Neural Networks) to verify whether the opposing smoothing effect consistently holds across diverse GNN designs