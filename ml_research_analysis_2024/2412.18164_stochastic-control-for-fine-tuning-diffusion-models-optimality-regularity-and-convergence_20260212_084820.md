---
ver: rpa2
title: 'Stochastic Control for Fine-tuning Diffusion Models: Optimality, Regularity,
  and Convergence'
arxiv_id: '2412.18164'
source_url: https://arxiv.org/abs/2412.18164
tags:
- control
- spre
- lipschitz
- gradient
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a stochastic control framework for fine-tuning
  diffusion models by integrating linear dynamics with Kullback-Leibler (KL) regularization.
  The approach treats the fine-tuned score function as a control to be learned, where
  a reward signal captures human preference and KL regularization ensures the fine-tuned
  model stays close to the pre-trained model.
---

# Stochastic Control for Fine-tuning Diffusion Models: Optimality, Regularity, and Convergence

## Quick Facts
- arXiv ID: 2412.18164
- Source URL: https://arxiv.org/abs/2412.18164
- Reference count: 40
- The paper proposes a stochastic control framework for fine-tuning diffusion models with KL regularization, proving linear convergence and regularity preservation.

## Executive Summary
This paper introduces a stochastic control framework for fine-tuning diffusion models by treating the fine-tuned score function as a control variable. The approach integrates linear dynamics with KL regularization, where the KL term ensures the fine-tuned model stays close to the pre-trained model while a reward signal captures human preferences. The authors develop a policy iteration algorithm (PI-FT) and prove it achieves global convergence at a linear rate. Unlike prior work, they demonstrate that both the value functions and controls maintain their regularity (Lipschitz and gradient Lipschitz properties) throughout training. The method is evaluated on text-to-image generation using Stable Diffusion v1.5, showing consistent improvement over baselines like DPOK in ImageReward scores.

## Method Summary
The paper proposes a stochastic control framework where fine-tuning a diffusion model is cast as finding an optimal control sequence. The model uses linear dynamics with KL regularization between the fine-tuned and pre-trained conditional densities. The policy iteration algorithm (PI-FT) alternates between control updates (using gradients of value functions) and value function evaluations. The KL regularization coefficient βt is crucial for ensuring strong concavity of the Bellman operator, which guarantees linear convergence. The algorithm is implemented with a noise schedule (αt, σt) defining the diffusion process, and the entire process is evaluated using ImageReward scores measuring alignment with human preferences.

## Key Results
- PI-FT achieves global convergence at a linear rate due to KL regularization-induced strong concavity in the Bellman operator
- Regularity of value functions and controls is preserved throughout training, unlike prior approaches
- On text-to-image generation tasks with Stable Diffusion v1.5, PI-FT consistently outperforms DPOK baseline in ImageReward scores
- The KL regularization coefficient βt effectively balances fine-tuning performance and model alignment

## Why This Works (Mechanism)

### Mechanism 1
The policy iteration algorithm (PI-FT) converges linearly to the globally optimal solution because the Bellman operator becomes strongly concave under KL regularization with sufficiently large βt. The KL divergence term between the fine-tuned and pre-trained conditional densities creates a squared loss in the control space, which makes the one-step optimization in the Bellman equation strongly concave. This strong concavity ensures that each policy update contracts toward the optimal policy at a geometric rate. The core assumption is that βt is chosen large enough such that 1 - σ²t/βt · LV₁,t+1 ≥ λt > 0, where LV₁,t+1 is the gradient Lipschitz constant of the value function at the next time step.

### Mechanism 2
The regularity (Lipschitz and gradient Lipschitz) of both value functions and controls is preserved throughout the iterative updates because the algorithm's structure maintains the smoothness properties from one iteration to the next. At each iteration, the update rules for controls and value functions involve compositions with Lipschitz-continuous functions and Gaussian smoothing. The Lipschitz constants of the next-step value function bound the current step's control Lipschitz constant, and this relationship propagates backward through time with controlled error terms that decay exponentially with the number of iterations. The core assumption is that the pre-trained score function spreₜ and reward function r are both Lipschitz and gradient Lipschitz, and the regularization parameter βt is chosen appropriately.

### Mechanism 3
The error between the approximate and optimal gradient of the value function decreases exponentially with the number of iterations because each policy update reduces the approximation error by a factor of (1-λt). The Bellman operator approximation error at time t propagates through the backward induction, with each step multiplying the error by factors C₁,ℓ ≤ 1+Ls₀,ℓ√αℓ/λℓ and C₂,k(1-λk)ᵐᵏ⁺¹. Since (1-λt) < 1, the error decays exponentially with the iteration count m, ensuring that the gradient of the approximate value function converges to the optimal gradient. The core assumption is that the feature mapping or basis functions used in the parametric formulation are well-behaved enough to approximate the optimal control.

## Foundational Learning

- Concept: KL divergence as regularization in control problems
  - Why needed here: Understanding how KL regularization creates strong concavity in the Bellman equation is crucial for grasping why PI-FT converges linearly.
  - Quick check question: How does adding a KL divergence term between two Gaussian conditional densities translate into a squared loss in the control space?

- Concept: Policy iteration and value function approximation
  - Why needed here: The algorithm alternates between policy evaluation (computing value functions) and policy improvement (updating controls), and understanding this iteration is key to implementing PI-FT correctly.
  - Quick check question: What is the relationship between the Bellman optimality operator and the policy improvement step in PI-FT?

- Concept: Lipschitz continuity and gradient Lipschitz properties
  - Why needed here: The regularity proofs rely heavily on tracking Lipschitz constants through the backward induction, and these properties ensure that the value functions and controls remain well-behaved.
  - Quick check question: How does the Lipschitz constant of the pre-trained score function influence the Lipschitz constant of the optimal control?

## Architecture Onboarding

- Component map:
  Pre-trained DDPM model (spreₜ) -> Reward function (R) -> KL regularization parameter (βt) -> Policy iteration loop -> Noise schedule (αt, σt) -> Fine-tuned controls

- Critical path:
  1. Initialize value function at T with reward function
  2. For t from T-1 down to 0:
     a. Initialize control with pre-trained score
     b. For m iterations:
        i. Update control using gradient of value function
        ii. Evaluate new value function
     c. Store final control and value function
  3. Use resulting controls as fine-tuned score functions

- Design tradeoffs:
  - Larger βt → faster convergence but model stays closer to pre-trained (less alignment)
  - Smaller βt → better alignment but slower convergence and potential instability
  - More iterations mt → better approximation but higher computational cost
  - Choice of noise schedule affects the difficulty of fine-tuning different aspects of the model

- Failure signatures:
  - Gradient norm not decreasing → βt too small or initialization poor
  - KL divergence exploding → βt too large or learning rate too high
  - ImageReward score plateauing early → insufficient iterations or suboptimal βt
  - Training instability → learning rate too high or noise schedule problematic

- First 3 experiments:
  1. Run PI-FT with default β=0.01 and mt=100 iterations, verify convergence by checking gradient norm decay
  2. Vary β across {0.01, 0.1, 1.0} while keeping other parameters fixed, measure impact on convergence speed and final ImageReward score
  3. Compare computational cost and performance against DPOK baseline using same number of sampling steps

## Open Questions the Paper Calls Out
- Can the proposed PI-FT algorithm be extended to online fine-tuning settings where the reward function is learned simultaneously with the model updates?
- How does the choice of KL regularization coefficient β affect the trade-off between fine-tuning performance and closeness to the pre-trained model in practice?
- Can the convergence analysis be extended to more complex parameterizations beyond the linear case discussed in Section 4.1?

## Limitations
- Theoretical analysis relies heavily on smoothness assumptions for pre-trained score and reward functions that may not hold in practice
- KL regularization coefficient selection is critical but lacks systematic tuning guidance
- ImageReward evaluation metric, while claimed to be widely used, is not standard in diffusion model fine-tuning literature, making direct comparison difficult

## Confidence
- Policy iteration convergence (High): The linear convergence proof is mathematically rigorous given the stated assumptions about strong concavity induced by KL regularization
- Regularity preservation (Medium): While the theoretical bounds are provided, practical violations of Lipschitz assumptions in real reward functions could break the preservation guarantees
- Empirical performance claims (Medium): The ImageReward improvements over DPOK are demonstrated, but the small evaluation set (4 prompts, 10 trajectories each) limits generalizability

## Next Checks
1. Test PI-FT with varying KL regularization coefficients (β ∈ {0.001, 0.01, 0.1, 1.0}) to systematically evaluate the trade-off between convergence speed and alignment quality
2. Conduct ablation studies removing the KL regularization term to verify whether the claimed strong concavity is indeed necessary for linear convergence
3. Evaluate on additional text prompts and larger trajectory sets to assess whether the observed performance improvements generalize beyond the specific examples used in the paper