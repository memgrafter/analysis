---
ver: rpa2
title: 'JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and
  Professional Question Answering Capability'
arxiv_id: '2402.17887'
source_url: https://arxiv.org/abs/2402.17887
tags:
- medical
- arxiv
- documents
- jmlr
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces JMLR, a novel method that jointly trains\
  \ large language models and information retrieval systems for medical question answering.\
  \ Unlike traditional approaches that train retrieval and LLM components separately,\
  \ JMLR synchronizes their training using a rank-based loss function that aligns\
  \ retrieved documents with the LLM\u2019s reasoning needs."
---

# JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability

## Quick Facts
- arXiv ID: 2402.17887
- Source URL: https://arxiv.org/abs/2402.17887
- Authors: Junda Wang; Zhichao Yang; Zonghai Yao; Hong Yu
- Reference count: 19
- JMLR-13B achieves 70.5% accuracy on medical QA, outperforming Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%)

## Executive Summary
JMLR introduces a novel joint training approach that synchronizes large language models and information retrieval systems for medical question answering. Unlike traditional methods that train retrieval and LLM components separately, JMLR uses a rank-based loss function that aligns retrieved documents with the LLM's reasoning needs. The method significantly reduces hallucinations and improves reasoning quality in medical contexts, achieving state-of-the-art performance on four medical QA datasets while requiring substantially less training time than continued pretraining approaches.

## Method Summary
JMLR jointly trains a ColBERT retriever and Llama LLM using a rank-based loss function called LLM-Rank loss. The retriever finds relevant medical documents from a corpus including PubMed articles, textbooks, and clinical guidelines. The LLM generates answers using retrieved documents, and the rank loss is computed from the negative log probability of the LLM's answer, which serves as a relevance score for retrieved documents. The training uses weighted random sampling of retrieved documents to expose the LLM to both helpful and irrelevant content, enhancing robustness. The approach incorporates S2-Attn for handling long contexts and trains synchronously with specific hyperparameters including AdamW optimizer and learning rate of 1e-5.

## Key Results
- JMLR-13B achieves 70.5% accuracy on medical QA datasets, outperforming previous state-of-the-art models
- The approach reduces hallucinations better than Claude3-Opus and generates superior rationales compared to GPT-3.5 and Claude3-Opus
- Training time is significantly reduced (148 GPU hours vs 42,630 hours for Meditron-70B)
- The model demonstrates enhanced reasoning quality and professional question answering capability

## Why This Works (Mechanism)

### Mechanism 1
The joint training of retriever and LLM improves document relevance for answer generation through the LLM-Rank loss function. This loss function uses the negative log probability of the LLM's answer as a relevance score for retrieved documents, aligning retriever output with LLM performance. The core assumption is that LLM answer quality correlates with document usefulness, though this could break if severe hallucinations generate high-confidence wrong answers that appear useful to the retriever.

### Mechanism 2
Dynamic document selection during training improves model robustness through weighted random sampling based on retriever scores. This ensures the LLM encounters both helpful and irrelevant documents during training, learning to distinguish useful from useless information. The core assumption is that exposure to irrelevant documents helps the LLM learn better document utilization, though this could fail if sampling weights are too skewed toward high-scoring documents.

### Mechanism 3
Retrieval augmentation reduces hallucinations by providing factual grounding through retrieved medical documents that provide evidence-based context for the LLM to cite. The core assumption is that medical questions have answers that can be found in reliable medical documents, though this could break when medical knowledge is evolving or contradictory.

## Foundational Learning

- **Concept**: Retrieval-augmented generation (RAG)
  - Why needed here: JMLR is a joint training variant of RAG, understanding RAG is prerequisite to understanding JMLR
  - Quick check question: What are the two main components of a RAG system and what does each component do?

- **Concept**: Rank-based loss functions
  - Why needed here: JMLR uses a rank loss (LLM-Rank) to train the retriever based on LLM performance
  - Quick check question: How does a rank loss differ from a standard classification loss, and why is it useful for training retrievers?

- **Concept**: Fine-tuning vs. continued pretraining
  - Why needed here: JMLR does not use continued pretraining on medical documents, only fine-tuning with retrieval
  - Quick check question: What is the key difference between fine-tuning and continued pretraining, and why might one choose fine-tuning over continued pretraining?

## Architecture Onboarding

- **Component map**: Question → ColBERT retrieval → document ranking → document selection → LLM generation → rank loss calculation → parameter updates

- **Critical path**: Question → ColBERT retriever (query-document matching) → document ranking → weighted random sampling → Llama LLM with S2-Attn (answer generation) → LLM-Rank loss calculation → synchronous parameter updates

- **Design tradeoffs**:
  - Joint training vs. separate training: Joint training aligns retriever and LLM but increases complexity
  - Number of retrieved documents: More documents provide more context but increase computational cost and risk of irrelevant content
  - Sampling strategy: Weighted sampling vs. top-k selection affects robustness vs. precision

- **Failure signatures**:
  - Low accuracy despite good retrieval scores: Retriever may be optimizing for wrong signals
  - High accuracy but poor rationales: LLM may not be effectively using retrieved documents
  - Slow training: Joint training may require more iterations or careful learning rate scheduling

- **First 3 experiments**:
  1. Verify baseline performance: Run JMLR-7B on MedQA without retrieval to establish fine-tuning-only baseline
  2. Test retrieval quality: Measure retrieval accuracy (precision@k) on held-out query-document pairs
  3. Ablation study: Compare JMLR-7B with synchronous vs. asynchronous retriever/LLM updates to confirm joint training benefit

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the methodology and evaluation, several implicit questions emerge:

1. How does JMLR's performance vary across different medical specialties and conditions not well-represented in the training data?

2. What is the long-term performance stability of JMLR as medical knowledge evolves and new guidelines emerge?

3. How does JMLR's retrieval mechanism handle conflicting or contradictory medical information from different sources?

## Limitations

- The rank loss mechanism depends on the assumption that LLM answer quality correlates with document usefulness, which may not hold for all question types
- The evaluation focuses on accuracy and rationale quality but lacks detailed analysis of failure cases or edge conditions
- The paper lacks detailed implementation specifics that would enable full reproduction, particularly for the LLM-Rank loss function

## Confidence

**High Confidence Claims:**
- JMLR-13B achieves higher accuracy than Llama2-13B with RAG on medical QA datasets
- The joint training approach requires significantly less training time than continued pretraining methods
- Retrieval augmentation improves rationale generation quality compared to baseline models

**Medium Confidence Claims:**
- The rank-based loss function effectively aligns retriever and LLM objectives
- Dynamic document selection during training improves model robustness
- JMLR reduces hallucinations compared to non-retrieval baselines

**Low Confidence Claims:**
- (None explicitly identified)

## Next Checks

1. **Retrieval Quality Analysis**: Conduct detailed analysis of retrieval precision@k on held-out query-document pairs to verify that the retriever is actually retrieving relevant documents, not just optimizing for the rank loss signal

2. **Ablation of Sampling Strategy**: Run controlled experiments comparing JMLR with different document sampling strategies (top-k vs weighted random) to quantify the contribution of dynamic document selection to overall performance

3. **Hallucination Analysis**: Perform systematic analysis of hallucination types using human evaluation on a diverse set of medical questions, comparing JMLR against both retrieval-augmented and non-augmented baselines to isolate the specific mechanisms that reduce hallucinations