---
ver: rpa2
title: "A Large Dataset of Spontaneous Speech with the Accent Spoken in S\xE3o Paulo\
  \ for Automatic Speech Recognition Evaluation"
arxiv_id: '2409.15350'
source_url: https://arxiv.org/abs/2409.15350
tags:
- speech
- corpus
- audio
- spontaneous
- nurc-sp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the NURC-SP Audio Corpus, a large dataset\
  \ of spontaneous Brazilian Portuguese speech with a S\xE3o Paulo accent, comprising\
  \ 239.30 hours of audio from 401 speakers. The dataset was automatically transcribed\
  \ and manually revised to support automatic speech recognition (ASR) tasks."
---

# A Large Dataset of Spontaneous Speech with the Accent Spoken in São Paulo for Automatic Speech Recognition Evaluation

## Quick Facts
- arXiv ID: 2409.15350
- Source URL: https://arxiv.org/abs/2409.15350
- Reference count: 26
- A new dataset of 239.30 hours of spontaneous Brazilian Portuguese speech with São Paulo accent for ASR evaluation

## Executive Summary
This paper introduces the NURC-SP Audio Corpus, a large dataset of spontaneous Brazilian Portuguese speech with a São Paulo accent, comprising 239.30 hours of audio from 401 speakers. The dataset was automatically transcribed and manually revised to support automatic speech recognition (ASR) tasks. Four ASR models were evaluated: two fine-tuned Wav2Vec2-XLSR-53 models, a Distil-Whisper model trained from scratch, and a fine-tuned Distil-Whisper model. The best performance was achieved by the fine-tuned Distil-Whisper model with a word error rate (WER) of 24.22%, outperforming the Wav2Vec2 models (WER 33.73%). The results demonstrate the dataset's effectiveness for training ASR systems, particularly for spontaneous speech in Brazilian Portuguese.

## Method Summary
The NURC-SP Audio Corpus was created from audio recordings of the Brazilian Portuguese speech transcription corpus, containing 401 speakers (204 female, 197 male) with São Paulo accent. The corpus comprises 239.30 hours of spontaneous speech segmented into 177,224 audio segments with transcriptions. Four ASR models were evaluated: two fine-tuned Wav2Vec2-XLSR-53 models, one Distil-Whisper model trained from scratch, and one fine-tuned Distil-Whisper model. The models were trained and evaluated using standard ASR metrics (WER and CER) on the corpus.

## Key Results
- The NURC-SP Audio Corpus contains 239.30 hours of spontaneous speech from 401 speakers with São Paulo accent
- Fine-tuned Distil-Whisper model achieved the best performance with WER of 24.22%
- Wav2Vec2 models achieved WER of 33.73%
- The dataset demonstrates effectiveness for training ASR systems on spontaneous Brazilian Portuguese speech

## Why This Works (Mechanism)

### Mechanism 1
Spontaneous speech phenomena improve ASR performance because the dataset captures natural speech patterns like filled pauses, self-corrections, repetitions, and prosodic variations that are absent in read speech corpora. Training on spontaneous speech allows the model to learn these patterns, improving recognition of real-world speech. The corpus contains 177,224 segments with these phenomena, providing sufficient examples for the model to learn.

### Mechanism 2
The Distil-Whisper fine-tuned model achieves better WER than Wav2Vec2 because it benefits from knowledge distillation and larger pretraining data. Distil-Whisper, trained on 680,000 hours of multilingual data, captures a broader range of speech patterns that can be fine-tuned more effectively on the specific NURC-SP dataset. The fine-tuned Distil-Whisper model achieved WER of 24.22% compared to Wav2Vec2's 33.73%.

### Mechanism 3
The dataset's focus on the São Paulo accent improves ASR performance for this specific dialect by providing targeted training data. By focusing on this accent, the dataset provides the ASR model with examples of specific phonetic and prosodic characteristics, allowing it to learn to recognize these patterns more accurately. The corpus contains 401 speakers with this accent, providing a large and diverse set of examples.

## Foundational Learning

- **Difference between spontaneous and read speech**: Spontaneous speech includes phenomena like filled pauses, self-corrections, and repetitions that are not present in read speech. Understanding these differences is crucial for interpreting ASR results on spontaneous speech datasets.
  - Quick check: What are some examples of phenomena that are common in spontaneous speech but rare in read speech?

- **Knowledge distillation in machine learning**: Distil-Whisper is a distilled version of Whisper that is smaller and faster while maintaining similar performance. Understanding how knowledge distillation works is important for understanding why this model performs well.
  - Quick check: How does knowledge distillation allow a smaller model to perform as well as a larger model?

- **Automatic Speech Recognition (ASR) evaluation metrics**: The paper reports results using Word Error Rate (WER) and Character Error Rate (CER). Understanding these metrics is crucial for interpreting results and comparing different models.
  - Quick check: What is the difference between WER and CER, and when might one be preferred over the other?

## Architecture Onboarding

- **Component map**: Data preprocessing (audio segmentation, transcription, normalization) -> Model training (fine-tuning Distil-Whisper and Wav2Vec2) -> Evaluation (calculating WER and CER) -> Deployment (making models and dataset publicly available)

- **Critical path**: 1) Preprocess NURC-SP Audio Corpus (segment audio, transcribe, normalize) 2) Fine-tune Distil-Whisper and Wav2Vec2 models on preprocessed data 3) Evaluate models on test set using WER and CER 4) Analyze results and identify improvement areas

- **Design tradeoffs**: Using distilled model (Distil-Whisper) vs. full model (Whisper): smaller, faster, but potentially less accurate; focusing on spontaneous speech vs. read speech: more realistic, but more challenging to model; using dataset with specific accent vs. general dataset: better performance for that accent, but potentially worse for others

- **Failure signatures**: High WER/CER indicates poor speech recognition; overfitting occurs when model performs well on training data but poorly on test data; underfitting occurs when model performs poorly on both training and test data

- **First 3 experiments**: 1) Fine-tune Distil-Whisper on NURC-SP dataset and evaluate on test set 2) Fine-tune Wav2Vec2 on NURC-SP dataset and evaluate on test set 3) Compare performance of two models and analyze results to identify improvement areas

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the methodology and results, several important questions arise regarding the comparison between Distil-Whisper and original Whisper models, the impact of different preprocessing techniques, and the generalization of models trained on this dataset to other spontaneous speech datasets in Brazilian Portuguese.

## Limitations

- Data quality concerns exist due to the automatic transcription and manual revision process across 239.30 hours of audio with 177,224 segments
- Limited comparison baseline with only fine-tuned Wav2Vec2 and Distil-Whisper models evaluated, without comparison to original Whisper or other architectures
- Domain specificity limitations as the paper doesn't analyze how well models generalize to other Brazilian Portuguese accents beyond São Paulo

## Confidence

**High Confidence**: The dataset contains 239.30 hours of spontaneous speech from 401 speakers with São Paulo accent, and the reported WER/CER metrics for the evaluated models are reproducible given access to the data and training code.

**Medium Confidence**: The claim that spontaneous speech phenomena improve ASR performance is supported by the dataset characteristics, but the direct causal relationship between these phenomena and the observed performance differences needs more rigorous analysis.

**Low Confidence**: The assertion that Distil-Whisper's superior performance is primarily due to knowledge distillation rather than the larger pretraining dataset or other factors is not definitively proven with the available evidence.

## Next Checks

1. **Ablation Study on Spontaneous Speech Phenomena**: Analyze model performance on subsets of data containing different types of spontaneous speech phenomena (filled pauses, repetitions, truncations) versus read speech segments to quantify the specific contribution of each phenomenon type.

2. **Cross-Accent Generalization Testing**: Evaluate the fine-tuned models on spontaneous speech corpora from other Brazilian Portuguese regions to measure how well the São Paulo-specific training generalizes to other accents.

3. **Pretraining Data Impact Analysis**: Train Distil-Whisper from scratch with comparable pretraining data to the original Whisper model but without knowledge distillation, to isolate the effects of pretraining data size versus distillation methodology on the observed performance improvements.