---
ver: rpa2
title: 'Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions
  in Meeting Summarization'
arxiv_id: '2403.00067'
source_url: https://arxiv.org/abs/2403.00067
tags: []
core_contribution: This work addresses the problem of high inference costs when using
  large language models (LLMs) for query-based meeting summarization, where repeated
  calls to the same context significantly increase expenses. The core method involves
  combining multiple queries for the same transcript into a single prompt to minimize
  repeated API calls.
---

# Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization

## Quick Facts
- arXiv ID: 2403.00067
- Source URL: https://arxiv.org/abs/2403.00067
- Reference count: 14
- Key outcome: Multi-query prompting reduces inference costs in meeting summarization, but effectiveness depends on model's ability to reliably generate JSON-formatted responses

## Executive Summary
This paper addresses the high inference costs associated with query-based meeting summarization, where repeated calls to LLMs for the same transcript context can be expensive. The proposed solution, Query-OPT, combines multiple queries for the same transcript into a single prompt to minimize repeated API calls. Extensive experiments were conducted across 7 popular LLMs (GPT-4, Gemini, Claude-3, LLaMA-2, Mistral, Phi-3, and Qwen-2) in both single-query and multi-query settings. The study demonstrates that while multi-query prompting can significantly optimize inference costs, only certain closed-source models like GPT-4 consistently generate responses in the required JSON format, with most open-source models struggling with this task except for a few 7B-parameter models.

## Method Summary
The method involves modifying the QMSUM dataset by combining all queries for the same transcript into a single prompt, creating a new evaluation setup with 162/35/35 train/valid/test splits. The approach compares popular LLMs in both zero-shot and fine-tuned settings, using standard HuggingFace training procedures for open-source models. A new evaluation method is proposed based on combined reference summaries per transcript. The study evaluates models on their ability to generate JSON-formatted responses containing both query and summary, measuring ROUGE-1, ROUGE-2, and ROUGE-L scores between generated and reference summaries.

## Key Results
- 100% reliability in generating JSON-formatted responses is limited to certain closed-source models like GPT-4
- Most open-source models (except 7B-parameter LLMs such as Mistral and Phi-3) lag behind in following multi-query instructions
- Multi-query prompting can significantly reduce inference costs but depends on model's ability to reliably process combined queries
- ROUGE scores may be artificially inflated in multi-query settings due to output length constraints

## Why This Works (Mechanism)
Multi-query prompting works by reducing the number of API calls needed for query-based summarization. When multiple queries are combined into a single prompt, the context is processed once rather than repeatedly, significantly reducing computational costs. However, this approach requires models to handle more complex instructions and maintain coherence across multiple queries in a single response. The effectiveness depends on the model's ability to parse combined instructions and generate structured outputs that can be parsed back into individual query-summary pairs.

## Foundational Learning
- **Meeting summarization task**: Understanding the domain of extracting key information from meeting transcripts - needed to grasp the specific challenges of this application; quick check: can you identify the key differences between meeting summarization and other summarization tasks?
- **Multi-query instruction following**: Models must process and respond to multiple instructions simultaneously - needed to understand why some models fail while others succeed; quick check: can you explain why combining queries might confuse certain models?
- **JSON-formatted response generation**: Models must output structured data rather than free text - needed to understand the evaluation criteria and technical requirements; quick check: can you describe the format requirements and why they matter for downstream processing?

## Architecture Onboarding
**Component map**: QMSUM dataset -> Query combination -> LLM inference -> JSON parsing -> ROUGE evaluation
**Critical path**: Query combination (preprocessing) -> LLM inference (core computation) -> JSON parsing (output validation) -> ROUGE scoring (performance evaluation)
**Design tradeoffs**: The study trades off between inference cost reduction (via multi-query) and potential performance degradation (due to more complex instructions and output constraints)
**Failure signatures**: Models failing to generate valid JSON responses, ROUGE scores being artificially inflated due to output length constraints, certain models being unable to follow multi-query instructions
**3 first experiments**: 1) Test whether prompt engineering can improve JSON response compliance in failing models; 2) Compare ROUGE scores using single-query vs. combined references to detect score inflation; 3) Conduct ablation studies varying output length constraints and prompt structures

## Open Questions the Paper Calls Out
**Open Question 1**: What specific prompt engineering techniques could improve LLMs' ability to generate properly formatted JSON responses in multi-query settings? The paper suggests potential techniques like Chain-of-Thought and few-shot learning but doesn't experiment with them or report their effectiveness.

**Open Question 2**: How does the size of fine-tuning datasets affect open-source LLMs' ability to follow multi-query instructions and generate properly formatted responses? The paper notes this was left out of scope but could be crucial for improving open-source model performance.

**Open Question 3**: How do human evaluations of multi-query generated summaries compare to ROUGE scores in terms of quality, factual correctness, and informativeness? The paper acknowledges ROUGE limitations but doesn't include human evaluation to validate quality metrics.

## Limitations
- The evaluation methodology combining reference summaries may artificially inflate ROUGE scores in multi-query settings
- Lack of transparency around prompt formats and fine-tuning hyperparameters prevents faithful reproduction
- The study doesn't explore why certain models fail to generate JSON responses or whether prompt engineering could improve compliance

## Confidence
High: Multi-query prompting reduces inference costs (clearly demonstrated)
Medium: Performance improvements and model reliability claims (evaluation concerns and missing methodological details)
Medium: Effectiveness of fine-tuning on open-source models (limited dataset size exploration)

## Next Checks
1. Replicate the evaluation by comparing ROUGE scores when using single-query references versus combined references to determine if score inflation occurs
2. Test whether prompt engineering or instruction tuning can improve JSON response compliance in models that currently fail to generate valid outputs
3. Conduct ablation experiments varying output length constraints and prompt structures to isolate their effects on both performance and cost efficiency