---
ver: rpa2
title: 'Fairness Under Cover: Evaluating the Impact of Occlusions on Demographic Bias
  in Facial Recognition'
arxiv_id: '2408.10175'
source_url: https://arxiv.org/abs/2408.10175
tags:
- fairness
- face
- occlusions
- recognition
- occluded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that occlusions in facial recognition systems
  exacerbate existing demographic biases, with African individuals being disproportionately
  affected. Using the RFW dataset and synthetic occlusions, the authors evaluated
  model performance across ethnicities and found significant increases in False Non-Match
  Rates (FNMR) and accuracy dispersion, particularly for African faces.
---

# Fairness Under Cover: Evaluating the Impact of Occlusions on Demographic Bias in Facial Recognition

## Quick Facts
- arXiv ID: 2408.10175
- Source URL: https://arxiv.org/abs/2408.10175
- Reference count: 30
- The study demonstrates that occlusions in facial recognition systems exacerbate existing demographic biases, with African individuals being disproportionately affected.

## Executive Summary
This study investigates how occlusions impact demographic bias in facial recognition systems, focusing on verification tasks. Using the RFW dataset with synthetic occlusions, the authors evaluate model performance across ethnicities and find that African faces experience significantly higher False Non-Match Rates (FNMR) when occluded. The proposed Face Occlusion Impact Ratio (FOIR) metric reveals that occluded regions contribute more heavily to erroneous decisions in African faces compared to other ethnicities. The research demonstrates that fairness metrics worsen under occlusion conditions, highlighting the need for bias mitigation strategies in real-world scenarios where face occlusions are common.

## Method Summary
The study uses the RFW dataset with four ethnicities (African, Asian, Caucasian, Indian) and applies synthetic occlusions using protocols 1 and 4 from the 2022 Competition on Occluded Face Recognition. Models trained on BUPT-Balanced and BUPT-GlobalFace datasets using ResNet34 and ResNet50 architectures with ElasticArcFace loss are evaluated. The evaluation includes accuracy, FMR, FNMR, and various fairness metrics (Demographic Parity, Equalized Odds, STD of Accuracy, etc.). The novel FOIR metric measures the percentage of important pixels falling on occluded areas using the xSSAB saliency method. Statistical analysis (ANOVA) is performed to verify the significance of differences across demographics.

## Key Results
- African faces show disproportionately higher FNMR increases under occlusion compared to other ethnicities
- FOIR metric reveals models rely more heavily on occluded regions when making decisions about African faces
- Traditional fairness metrics become misleading under occlusion as overall error rates increase across all demographics
- The dispersion of accuracy across demographics increases significantly, indicating decreased fairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Occlusions disproportionately affect African faces due to higher model reliance on occluded regions for decision-making
- Mechanism: When faces are occluded, the model shifts attention to the remaining visible regions. For African faces, these regions carry less discriminative identity information, causing the model to over-rely on occluded areas, leading to higher FNMR
- Core assumption: The distribution of discriminative features differs across ethnicities, with African faces having less distinct visible features when partially occluded
- Evidence anchors:
  - [abstract] "models placing higher importance on occlusions in an unequal fashion, particularly affecting African individuals more severely"
  - [section 4.3] "African faces show a higher degree of importance of occluded pixels in all but one tested setting"

### Mechanism 2
- Claim: Existing fairness metrics can give misleading signals when overall error rates increase dramatically
- Mechanism: Metrics like SER, IR, and GARBE are ratio-based and become less sensitive when both numerator and denominator increase proportionally. The dispersion may increase but the ratio decreases, suggesting "improved" fairness when performance actually degraded
- Core assumption: Ratio-based metrics are only meaningful when baseline error rates are comparable across groups
- Evidence anchors:
  - [section 4.2] "even though the dispersion of the error or error rates increases, indicating unfairer results"
  - [section 4.2] "improvements in fairness are genuine and not merely artifacts of increased error rates"

### Mechanism 3
- Claim: The Face Occlusion Impact Ratio (FOIR) reveals hidden biases by measuring attention overlap with occlusions
- Mechanism: FOIR quantifies the percentage of important pixels that fall on occluded areas. Higher FOIR in FNM cases for African faces indicates the model incorrectly relies on occluded regions when rejecting genuine pairs, revealing a systematic bias
- Core assumption: In verification scenarios, occluded regions should not contribute to identity discrimination; high overlap indicates problematic behavior
- Evidence anchors:
  - [section 3.1] "we measure the rate of important pixels that fall on top of occlusions out of all important pixels"
  - [section 4.3] "the overlap between important pixels and added occlusions to calculate the percentage of important pixels (IP) that fall onto occluded areas"

## Foundational Learning

- Concept: Face recognition verification vs identification
  - Why needed here: The paper focuses on verification (1:1 matching), which has different fairness challenges than identification (1:N search). Understanding this distinction is crucial for interpreting the results
  - Quick check question: What's the key difference between verification and identification in face recognition, and why does it matter for fairness evaluation?

- Concept: Demographic parity vs equalized odds
  - Why needed here: These are two distinct fairness metrics with different implications. Demographic parity measures selection rate equality, while equalized odds requires equal TPR and FPR across groups
  - Quick check question: How do demographic parity and equalized odds differ in their requirements for fair classification, and which one is more appropriate for verification systems?

- Concept: False Non-Match Rate (FNMR) vs False Match Rate (FMR)
  - Why needed here: The paper shows FNMR increases more dramatically than FMR under occlusion, which has different security and fairness implications. Understanding these error types is essential for interpreting the results
  - Quick check question: Why is an increase in FNMR generally considered more problematic than an increase in FMR in access control scenarios?

## Architecture Onboarding

- Component map:
  RFW dataset -> synthetic occlusion application -> model inference -> embedding extraction -> distance calculation -> threshold application -> fairness metrics calculation -> FOIR computation -> statistical analysis

- Critical path:
  1. Load RFW dataset and apply synthetic occlusions
  2. Extract embeddings using pre-trained models
  3. Compute pairwise distances and apply threshold
  4. Calculate fairness metrics and FOIR
  5. Perform statistical analysis

- Design tradeoffs:
  - Using synthetic vs real occlusions: Synthetic allows controlled experiments but may not capture all real-world variability
  - Choice of saliency method: xSSAB is efficient but may have its own biases
  - Threshold selection: Optimized for overall performance vs fairness trade-offs

- Failure signatures:
  - FOIR showing no variation across demographics when theory predicts differences
  - Fairness metrics improving while absolute performance degrades
  - Statistical tests failing to detect significant differences when visual inspection suggests they exist

- First 3 experiments:
  1. Reproduce the baseline unoccluded performance across all demographics to establish ground truth
  2. Apply protocol 1 occlusions and measure the change in FNMR specifically for African vs other groups
  3. Compute FOIR for FNM cases and run ANOVA to verify the statistical significance of differences across demographics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can fairness metrics like SER, IR, and GARBE be improved to account for scenarios where error rates increase significantly across all demographics, as observed with occlusions?
- Basis in paper: [explicit] The authors note that SER, IR, and GARBE suggest fairer decisions in occluded scenarios, but this is primarily due to the overall increase in error rates, not genuine fairness improvements.
- Why unresolved: Current fairness metrics may not distinguish between true fairness improvements and artifacts caused by uniformly increased errors. This limitation needs to be addressed to ensure that fairness assessments remain meaningful under varying conditions.
- What evidence would resolve it: Development and validation of new fairness metrics that remain sensitive to demographic disparities even when overall error rates increase, or refinement of existing metrics to incorporate error rate normalization.

### Open Question 2
- Question: What specific architectural or training modifications could be implemented to reduce the disproportionate impact of occlusions on African faces in face recognition systems?
- Basis in paper: [explicit] The study finds that African individuals are more severely affected by occlusions, with higher False Non-Match Rates (FNMR) and greater importance of occlusions in model predictions.
- Why unresolved: The paper identifies the problem but does not explore potential solutions to mitigate the bias. Understanding how to adjust model design or training to address this issue is crucial for developing fairer systems.
- What evidence would resolve it: Experimental results demonstrating reduced bias in African faces through architectural changes (e.g., multi-task learning, attention mechanisms) or training strategies (e.g., data augmentation, balanced datasets) under occluded conditions.

### Open Question 3
- Question: How can the Face Occlusion Impact Ratio (FOIR) metric be generalized to other biometric recognition tasks beyond face recognition, such as iris or fingerprint recognition?
- Basis in paper: [explicit] The authors propose FOIR as a novel metric for quantifying the impact of occlusions on model predictions in face recognition, particularly in False Non-Match scenarios.
- Why unresolved: While FOIR is effective for face recognition, its applicability to other biometric modalities is not explored. Extending this metric could provide insights into occlusion-related biases across different biometric systems.
- What evidence would resolve it: Validation of FOIR in other biometric recognition tasks, demonstrating its effectiveness in identifying and quantifying occlusion-related biases in modalities like iris or fingerprint recognition.

## Limitations

- The synthetic occlusion approach may not fully capture the complexity of real-world occlusion scenarios
- The RFW dataset, while widely used, has been criticized for potential annotation inconsistencies
- Findings are limited to verification tasks and may not generalize to identification scenarios
- The FOIR metric depends heavily on the choice of attribution method and threshold, introducing potential subjectivity

## Confidence

- **High Confidence**: The core finding that occlusions exacerbate demographic bias, particularly for African faces, is well-supported by multiple metrics and statistical analysis.
- **Medium Confidence**: The proposed FOIR metric effectively quantifies the disproportionate reliance on occluded regions, though its absolute values may vary with different attribution methods.
- **Low Confidence**: The extent to which synthetic occlusions represent real-world conditions and the generalizability of findings to other face recognition architectures.

## Next Checks

1. Cross-dataset validation: Evaluate the proposed approach on additional face recognition datasets (e.g., MORPH, BUPT-Balanced) to verify the robustness of findings across different data distributions.

2. Real occlusion testing: Collect and test on images with naturally occurring occlusions to validate whether synthetic occlusion results translate to real-world scenarios.

3. Alternative attribution methods: Compare FOIR results using different saliency/attribution methods (e.g., Grad-CAM, Integrated Gradients) to ensure the findings are not method-dependent.