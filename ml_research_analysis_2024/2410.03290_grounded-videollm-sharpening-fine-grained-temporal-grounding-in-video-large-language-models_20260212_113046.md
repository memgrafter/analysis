---
ver: rpa2
title: 'Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large
  Language Models'
arxiv_id: '2410.03290'
source_url: https://arxiv.org/abs/2410.03290
tags:
- video
- temporal
- tokens
- arxiv
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Grounded-VideoLLM improves fine-grained temporal grounding in Video-LLMs
  by incorporating a two-stream encoder that separately captures spatial and temporal
  information, and by introducing discrete temporal tokens for efficient timestamp
  representation. A multi-stage training strategy progressively adapts an image-based
  MLLM to handle complex video temporal grounding tasks.
---

# Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models

## Quick Facts
- arXiv ID: 2410.03290
- Source URL: https://arxiv.org/abs/2410.03290
- Authors: Haibo Wang; Zhiyang Xu; Yu Cheng; Shizhe Diao; Yufan Zhou; Yixin Cao; Qifan Wang; Weifeng Ge; Lifu Huang
- Reference count: 16
- Primary result: Achieves state-of-the-art or competitive performance across tasks such as temporal sentence grounding, dense video captioning, and grounded VideoQA

## Executive Summary
Grounded-VideoLLM is a novel Video-LLM designed to achieve fine-grained temporal grounding by perceiving and reasoning over specific video moments with high precision. The model introduces a two-stream encoder that separately captures spatial and temporal information, and uses discrete temporal tokens for efficient timestamp representation. Through a multi-stage training strategy, it progressively adapts an image-based MLLM to handle complex video temporal grounding tasks while maintaining strong general video understanding capabilities.

## Method Summary
Grounded-VideoLLM improves fine-grained temporal grounding by implementing a two-stream encoder that separately processes spatial (single keyframe) and temporal (multiple frames) video components. The model introduces discrete temporal tokens representing relative time positions, allowing joint prediction of timestamps and text as a single sequence. A three-stage training strategy first aligns video features with the MLLM, then introduces temporal tokens and aligns them with video timelines, and finally fine-tunes on diverse temporal grounding and general video understanding tasks.

## Key Results
- Achieves state-of-the-art or competitive performance across temporal sentence grounding, dense video captioning, and grounded VideoQA tasks
- Maintains strong general video understanding capabilities while specializing in temporal grounding
- Demonstrates effectiveness through automated pipeline creation of grounded VideoQA datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The two-stream encoder improves fine-grained temporal grounding by separately capturing spatial and temporal features, allowing the model to maintain both visual detail and motion dynamics.
- **Mechanism:** The model divides each video segment into spatial and temporal parts. The spatial stream processes a single keyframe to capture appearance, while the temporal stream processes multiple frames to extract motion representations. These are then combined to form a unified video representation enriched with both types of information.
- **Core assumption:** Motion dynamics and appearance details are best captured separately and then combined, rather than jointly encoded in a single stream.
- **Evidence anchors:**
  - [abstract] "we introduce Grounded-VideoLLM, a novel Video-LLM designed to perceive and reason over specific video moments with fine-grained temporal precision. Our model features (1) a two-stream encoder that explicitly captures inter-frame relationships while preserving intra-frame visual details..."
  - [section] "From the perspective of model architecture, Grounded-VideoLLM is built upon two key innovations: (1) Two-Stream Encoding: We decompose each segment of the video into spatial and temporal components and encode each with an expert encoder."
  - [corpus] Weak: The corpus does not contain direct evidence for the two-stream mechanism, though related works like "Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement" suggest attention-based temporal enhancement is active research.
- **Break condition:** If the temporal stream fails to capture meaningful motion patterns, or if the spatial stream loses too much detail through pooling, the combined representation will be degraded.

### Mechanism 2
- **Claim:** Discrete temporal tokens improve timestamp representation efficiency by avoiding tokenization of continuous numerical values, enabling the model to jointly predict timestamps and text as a single sequence.
- **Mechanism:** The model introduces M+1 special tokens representing relative time positions (e.g., <0> for start, <M> for end). Continuous timestamps are converted to discrete tokens using a simple rounding formula, allowing the LLM to handle time as part of its regular vocabulary.
- **Core assumption:** LLMs struggle with numerical data in text form, and discrete tokens can represent time more efficiently than plain text timestamps.
- **Evidence anchors:**
  - [abstract] "we introduce discrete temporal tokens enriched with structured time knowledge for timestamp representation."
  - [section] "While this may introduce minor quantization errors, these can be minimized by selecting an appropriate M or an interpolation expansion."
  - [corpus] Weak: The corpus does not provide direct evidence for discrete temporal tokens, though the general problem of numerical handling in LLMs is known.
- **Break condition:** If the quantization introduces too much error, or if the token embedding space becomes too sparse for effective learning.

### Mechanism 3
- **Claim:** The multi-stage training strategy progressively adapts an image-based MLLM to handle fine-grained temporal grounding tasks by starting with simple video-caption alignment and gradually introducing more complex temporal tasks.
- **Mechanism:** Stage 1 aligns video features with the MLLM using video-caption pairs. Stage 2 introduces temporal tokens and aligns them with video timelines through grounding-specific datasets. Stage 3 fine-tunes on diverse temporal grounding and general video understanding tasks.
- **Core assumption:** Progressive training from simple to complex tasks enables smoother learning and better temporal alignment than training from scratch or using mixed datasets.
- **Evidence anchors:**
  - [abstract] "we employ a multi-stage training scheme, beginning with simple video-captioning tasks and progressively introducing video temporal grounding tasks of increasing complexity."
  - [section] "This scheme ensures the introduced temporal tokens align closely with the video timelines and LLM's semantic space, distinguishing our method from previous studies."
  - [corpus] Weak: The corpus does not contain direct evidence for multi-stage training, though progressive training is a known technique in ML.
- **Break condition:** If the intermediate stages do not provide sufficient grounding for the final stage, or if the model overfits to early-stage tasks.

## Foundational Learning

- **Concept:** Temporal modeling in videos
  - **Why needed here:** Video understanding requires capturing not just what happens, but when it happens. Without temporal modeling, the model treats frames as independent images.
  - **Quick check question:** What is the key difference between image understanding and video understanding in terms of information that needs to be captured?

- **Concept:** Tokenization and embedding spaces in LLMs
  - **Why needed here:** The model extends the LLM vocabulary with special temporal tokens and needs to ensure they share the same embedding space as regular text tokens for joint prediction.
  - **Quick check question:** Why is it problematic to represent timestamps as plain text (e.g., "from 102.3 to 120.1 seconds") in an LLM?

- **Concept:** Progressive training and curriculum learning
  - **Why needed here:** The model starts with an image-based MLLM and needs to gradually adapt it to video tasks, avoiding catastrophic forgetting and ensuring smooth skill acquisition.
  - **Quick check question:** What is the advantage of starting with an image-based MLLM and progressively adapting it, rather than training from scratch?

## Architecture Onboarding

- **Component map:** Image encoder (ViT from Phi3.5-V) -> Video encoder (InternVideo2-1B) -> Two-stream encoder (spatial and temporal streams with MLPs) -> Extended LLM vocabulary (temporal tokens) -> Multi-stage training pipeline (3 stages)

- **Critical path:**
  1. Video segmentation into K segments
  2. Spatial stream: keyframe extraction → image encoding → 2D pooling → MLP projection
  3. Temporal stream: multi-frame extraction → video encoding → spatial pooling → MLP projection
  4. Token concatenation and LLM processing
  5. Temporal token generation and timestamp prediction

- **Design tradeoffs:**
  - Two-stream vs. single-stream: Two-stream captures both appearance and motion but increases complexity; single-stream is simpler but may lose temporal information.
  - Dense vs. sparse frames: Dense frames capture more motion but increase computational cost; sparse frames are cheaper but may miss fine-grained temporal details.
  - Number of temporal tokens (M): More tokens provide finer granularity but increase vocabulary size and potential quantization errors.

- **Failure signatures:**
  - Poor temporal grounding performance: May indicate issues with temporal stream encoding or temporal token alignment.
  - Hallucinations or incorrect timestamps: Could suggest problems with the temporal token embedding space or multi-stage training.
  - General video understanding degradation: Might indicate over-specialization to temporal tasks at the expense of overall video comprehension.

- **First 3 experiments:**
  1. **Ablation of temporal stream:** Remove the temporal stream and use only the spatial stream with dense frames to quantify the contribution of motion information.
  2. **Temporal token analysis:** Vary the number of temporal tokens (M) and measure grounding accuracy to find the optimal granularity.
  3. **Training stage impact:** Train with only stages 1+2 vs. all 3 stages to evaluate the importance of the temporal token alignment stage.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of Grounded-VideoLLM scale with the number of temporal tokens used for timestamp representation?
- **Basis in paper:** [explicit] The paper states "While this may introduce minor quantization errors, these can be minimized by selecting an appropriate M or an interpolation expansion" and discusses using M = 300 temporal tokens.
- **Why unresolved:** The paper only provides results for 300 temporal tokens and doesn't explore how performance varies with different numbers of tokens, leaving the optimal token count unclear.
- **What evidence would resolve it:** Systematic experiments varying M (e.g., 50, 100, 200, 300, 500) and measuring performance across tasks would reveal the relationship between token count and accuracy.

### Open Question 2
- **Question:** What is the computational overhead of the two-stream encoding approach compared to single-stream methods, and how does this impact practical deployment?
- **Basis in paper:** [inferred] The paper introduces a two-stream encoder but only mentions "computational resource requirements" as a limitation without quantifying the actual overhead.
- **Why unresolved:** The paper doesn't provide timing comparisons or resource usage metrics between the two-stream approach and simpler alternatives, making it difficult to assess real-world feasibility.
- **What evidence would resolve it:** Runtime measurements comparing inference speed and memory usage of Grounded-VideoLLM versus single-stream baselines on identical hardware would clarify deployment constraints.

### Open Question 3
- **Question:** How well does the grounded VideoQA dataset generation pipeline generalize to languages other than English or domains beyond cooking and lifestyle videos?
- **Basis in paper:** [explicit] The paper describes using GPT-4 to create a grounded VideoQA dataset but only mentions using public datasets with temporal labels without specifying domain restrictions.
- **Why unresolved:** The evaluation focuses on specific benchmarks and doesn't test the model's ability to handle diverse video types or multilingual inputs, leaving generalization unclear.
- **What evidence would resolve it:** Testing the model on multilingual VideoQA datasets and videos from varied domains (sports, news, educational content) would demonstrate the robustness of the grounding capabilities.

## Limitations
- Temporal token quantization error: The rounding operation for converting continuous timestamps to discrete tokens may introduce errors that are not systematically analyzed.
- Computational overhead: The two-stream encoder increases complexity, but exact resource requirements are not quantified.
- Synthetic data quality: The automated pipeline for creating grounded VideoQA datasets is described but not validated against human-annotated data.

## Confidence
**High Confidence:**
- The model achieves state-of-the-art or competitive performance on benchmark tasks
- The two-stream architecture is implemented and functions as described
- The discrete temporal token mechanism works for joint text-timestamp prediction

**Medium Confidence:**
- The two-stream architecture provides superior temporal grounding compared to alternatives
- The multi-stage training strategy is optimal for adapting image-based MLLMs to video tasks
- The automated pipeline generates high-quality grounded VideoQA data

**Low Confidence:**
- The exact quantization error bounds for different M values
- The relative contribution of each component (two-stream, temporal tokens, multi-stage training) to overall performance

## Next Checks
1. **Ablation of Temporal Stream:** Remove the temporal stream component and retrain the model using only the spatial stream with dense frame sampling. Compare grounding accuracy (mIoU, R@0.3/0.5/0.7) against the full two-stream model to quantify the contribution of motion information.

2. **Temporal Token Granularity Analysis:** Systematically vary the number of temporal tokens (M) from 100 to 1000 in increments of 100. For each setting, measure grounding accuracy and timestamp prediction error to construct a tradeoff curve showing the relationship between temporal resolution and performance.

3. **Multi-Stage Training Comparison:** Train three versions of the model: (a) only stages 1+2 (no temporal token alignment), (b) only stages 2+3 (no video-caption alignment), and (c) all three stages. Compare grounding performance and convergence speed to evaluate the necessity of each training stage.