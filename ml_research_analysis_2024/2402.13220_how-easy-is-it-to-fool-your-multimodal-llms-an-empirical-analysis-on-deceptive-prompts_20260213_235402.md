---
ver: rpa2
title: How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive
  Prompts
arxiv_id: '2402.13220'
source_url: https://arxiv.org/abs/2402.13220
tags:
- arxiv
- image
- answer
- wang
- deceptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAD-Bench, a benchmark designed to evaluate
  the robustness of Multimodal Large Language Models (MLLMs) when faced with deceptive
  prompts. The benchmark consists of 1000 image-prompt pairs across five categories,
  including non-existent objects, count of objects, and spatial relationships.
---

# How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts

## Quick Facts
- arXiv ID: 2402.13220
- Source URL: https://arxiv.org/abs/2402.13220
- Authors: Yusu Qian; Haotian Zhang; Yinfei Yang; Zhe Gan
- Reference count: 40
- Primary result: MAD-Bench benchmark reveals significant performance gaps between GPT-4V (82.82% accuracy) and other MLLMs (9-50% accuracy) on deceptive prompts

## Executive Summary
This paper introduces MAD-Bench, a benchmark designed to evaluate how Multimodal Large Language Models (MLLMs) handle deceptive prompts containing information inconsistent with the image. The benchmark consists of 1000 image-prompt pairs across five categories of deception, including non-existent objects, count discrepancies, and spatial relationship errors. The authors evaluate 19 popular MLLMs including GPT-4V, Reka, Gemini-Pro, and open-source models like LLaVA-NeXT. Results show GPT-4V significantly outperforms all other models, achieving 82.82% accuracy compared to 9-50% for competitors. The study also proposes a simple prompt enhancement technique that doubles accuracy by encouraging models to verify information consistency, though absolute performance remains unsatisfactory.

## Method Summary
The authors create MAD-Bench by generating deceptive prompts for images from COCO 2017 validation set, SBU, and TextVQA. They evaluate 19 MLLMs on binary accuracy - whether models are misled by deceptive information in prompts. GPT-4o is used to automatically evaluate model responses. The benchmark covers five deception categories: Count of Object, Non-existent Object, Object Attribute, Scene Understanding, and Text Recognition. The study also tests a simple remedy that adds an additional paragraph to deceptive prompts, which significantly improves performance across models.

## Key Results
- GPT-4V achieves 82.82% accuracy on MAD-Bench, significantly outperforming other models (9-50% range)
- The simple prompt enhancement technique doubles accuracy but absolute numbers remain low
- Models supporting bounding box input/output (Ferret, Kosmos-2) perform particularly poorly due to grounding non-existent objects
- GPT-4V shows remarkably higher accuracy in Object Attribute (70.83%) and Text Recognition (88.24%) categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V performs significantly better at resisting deceptive prompts because it has a more sophisticated understanding of visual data and context, allowing it to better detect inconsistencies between prompts and images.
- Mechanism: The model's architecture likely includes stronger cross-modal attention mechanisms that can more effectively align visual features with language context, enabling it to recognize when prompt information contradicts visual evidence.
- Core assumption: The superior performance is due to architectural differences in how GPT-4V processes and integrates multimodal information compared to other models.
- Evidence anchors:
  - [abstract] "Notably, GPT-4V's accuracy in theObject Attribute and Text Recognition categories is remarkably higher than the others, with 70.83% and 88.24% accuracy respectively."
  - [section 4.1] "GPT-4V demonstrates superior performance across all metrics compared to the other models. GPT-4V has a more sophisticated understanding of visual data and is less prone to being misled by inaccurate information."
  - [corpus] Weak evidence - while related work exists on deceptive prompts and adversarial attacks, specific comparative studies on GPT-4V's architectural advantages in this context are limited.
- Break condition: If the model's performance degrades significantly when presented with prompts that are consistent with the image but factually incorrect about the real world, or if the architectural advantage disappears when using different prompting strategies.

### Mechanism 2
- Claim: Adding an additional paragraph to deceptive prompts significantly boosts performance by encouraging models to think twice before answering, effectively prompting them to verify information consistency.
- Mechanism: The system prompt acts as a cognitive override, forcing the model to engage in a two-step reasoning process: first recognizing the potential for deception, then cross-referencing the prompt information with visual evidence before generating a response.
- Core assumption: The model's language understanding capabilities can be leveraged through carefully crafted prompts to improve its multimodal reasoning, even without architectural changes.
- Evidence anchors:
  - [abstract] "Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory."
  - [section 5] "This enhancement is realized through the integration of an additional paragraph into the system's prompt, which is either prepended directly to the existing prompt, or incorporated differently, depending on the specific model."
  - [corpus] Weak evidence - while prompt engineering for LLM reliability exists, specific studies on prompt-based defenses against deceptive multimodal inputs are limited.
- Break condition: If the prompt addition consistently fails across different model architectures, or if the performance gain is entirely due to chance rather than systematic reasoning improvement.

### Mechanism 3
- Claim: Models supporting bounding box input and output (like Ferret and Kosmos-2) perform poorly because they attempt to ground objects as best as they can, leading them to incorrectly ground non-existent objects mentioned in prompts.
- Mechanism: The architecture's strong object grounding capability becomes a liability when faced with deceptive prompts, as it tries to find and describe objects that don't exist in the image based on textual cues alone.
- Core assumption: The grounding mechanism in these models prioritizes finding objects mentioned in prompts over verifying their actual presence in the image.
- Evidence anchors:
  - [section 4.1] "Interestingly, we observe that models that support bounding box input and output (i.e., Ferret and Kosmos-2) achieve poor performance on this benchmark. We hypothesize that these models attempt to ground objects as best as they can as they are trained on positive data, therefore, they tend to ground non-existent objects as they are mentioned in the prompts, thus performing poorer than other models on our benchmark."
  - [corpus] Weak evidence - while adversarial attacks on object detection exist, specific studies on how grounding architectures fail with deceptive prompts are limited.
- Break condition: If these models can be retrained or fine-tuned to better distinguish between grounding objects that exist versus those that are merely mentioned in prompts.

## Foundational Learning

- Concept: Cross-modal attention mechanisms
  - Why needed here: Understanding how different multimodal models align visual and textual information is crucial for explaining performance differences in handling deceptive prompts.
  - Quick check question: How do cross-modal attention mechanisms in MLLMs typically align visual features with language context, and what architectural differences might explain why GPT-4V performs better at detecting inconsistencies?

- Concept: Prompt engineering for reliability
  - Why needed here: The study demonstrates that carefully crafted prompts can significantly improve model robustness against deceptive inputs, highlighting the importance of understanding how to effectively prompt MLLMs.
  - Quick check question: What principles should guide the design of system prompts that encourage models to verify information consistency between prompts and images before responding?

- Concept: Object grounding vs. existence verification
  - Why needed here: The poor performance of bounding box models reveals a critical distinction between being able to locate objects and being able to verify their actual existence in an image.
  - Quick check question: How can MLLM architectures be designed to distinguish between grounding objects that are mentioned in prompts versus those that actually exist in the image content?

## Architecture Onboarding

- Component map: Vision encoder (e.g., CLIP) -> Cross-modal attention layers -> Language model backbone -> Output response
- Critical path: Input image → Vision encoder → Cross-modal attention → Language model → Output response. The critical path for handling deceptive prompts is the cross-modal attention layer's ability to detect inconsistencies between visual and textual information.
- Design tradeoffs: Stronger object grounding capabilities can improve performance on many tasks but may become a liability when handling deceptive prompts, as models may try to find objects that don't exist. Simpler architectures without grounding may be more robust to deception but less capable on other tasks.
- Failure signatures: Consistently grounding non-existent objects, failing to question contradictory information in prompts, or providing responses that contradict the visual content of the image.
- First 3 experiments:
  1. Test the same deceptive prompts on models with and without object grounding capabilities to confirm the hypothesis about why bounding box models perform poorly.
  2. Systematically vary the additional paragraph prompt to identify which elements are most critical for improving robustness against different types of deceptive prompts.
  3. Compare GPT-4V's cross-modal attention patterns when handling consistent vs. inconsistent prompts to understand what architectural features enable better deception detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MLLMs vary across different deceptive prompt categories, and what underlying factors contribute to these variations?
- Basis in paper: [explicit] The paper evaluates MLLMs on five distinct categories of deceptive prompts: Count of Object, Non-existent Object, Object Attribute, Scene Understanding, and Text Recognition.
- Why unresolved: The paper provides a comprehensive analysis of MLLMs' performance across these categories, but it does not delve into the specific reasons why certain models excel or falter in particular categories.
- What evidence would resolve it: A detailed breakdown of each model's performance within each category, accompanied by an analysis of the model's architecture, training data, and response generation process, could shed light on the underlying factors contributing to the performance variations.

### Open Question 2
- Question: Can the proposed simple remedy of adding an additional paragraph to the deceptive prompts be further optimized to enhance MLLMs' robustness against deceptive prompts?
- Basis in paper: [explicit] The paper introduces a simple remedy that involves adding an additional paragraph to the deceptive prompts, which can double the accuracy of MLLMs.
- Why unresolved: The paper acknowledges that the absolute numbers are still too low to be satisfactory, indicating that there is room for improvement in the proposed remedy.
- What evidence would resolve it: Conducting further experiments with different variations of the additional paragraph, such as changing its length, content, or tone, could lead to a more effective remedy that significantly improves MLLMs' robustness against deceptive prompts.

### Open Question 3
- Question: How does the performance of MLLMs on MAD-Bench correlate with their performance on other multimodal benchmarks, and what does this imply about their overall robustness and generalization capabilities?
- Basis in paper: [inferred] The paper introduces MAD-Bench as a new benchmark to evaluate MLLMs' resilience against deceptive prompts, suggesting that it is designed to be more challenging than existing benchmarks.
- Why unresolved: The paper does not compare MLLMs' performance on MAD-Bench with their performance on other multimodal benchmarks, making it difficult to assess their overall robustness and generalization capabilities.
- What evidence would resolve it: Conducting experiments that compare MLLMs' performance on MAD-Bench with their performance on other multimodal benchmarks, such as VQA and GQA, could provide insights into their overall robustness and generalization capabilities.

## Limitations

- Benchmark relies on automatically generated prompts evaluated by GPT-4o, potentially introducing bias
- Simple prompt enhancement shows promise but lacks detailed analysis of why it works or its limitations
- Study doesn't explore underlying architectural differences between models that explain performance variations

## Confidence

- High confidence: GPT-4V significantly outperforms other models on deceptive prompts (82.82% vs 9-50% accuracy)
- Medium confidence: The prompt enhancement technique doubles accuracy, though absolute numbers remain low
- Low confidence: Explanations for why bounding box models perform poorly and the specific mechanisms behind GPT-4V's superior performance

## Next Checks

1. **Independent Evaluation**: Conduct manual evaluation of a subset of responses to verify GPT-4o's automatic evaluation accuracy and identify potential biases in the benchmark generation process.
2. **Ablation Study**: Systematically remove components from the prompt enhancement technique to identify which elements are essential for improving model robustness against different deception categories.
3. **Architectural Analysis**: Compare cross-modal attention patterns and grounding behaviors between GPT-4V and other models when processing consistent versus deceptive prompts to identify specific architectural advantages.