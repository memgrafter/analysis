---
ver: rpa2
title: Deterministic Exploration via Stationary Bellman Error Maximization
arxiv_id: '2410.23840'
source_url: https://arxiv.org/abs/2410.23840
tags:
- exploration
- exploitation
- network
- reward
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Stationary Error-seeking Exploration (SEE),
  a deterministic exploration method for reinforcement learning that treats exploration
  and exploitation as separate optimization problems. SEE uses two neural networks
  - an exploitation network and an exploration network - where the exploration network
  is trained to maximize the absolute Bellman error (TD-error) of the exploitation
  network.
---

# Deterministic Exploration via Stationary Bellman Error Maximization

## Quick Facts
- arXiv ID: 2410.23840
- Source URL: https://arxiv.org/abs/2410.23840
- Reference count: 36
- Key outcome: SEE outperforms epsilon-greedy exploration across three environments with different reward structures

## Executive Summary
This paper introduces Stationary Error-seeking Exploration (SEE), a deterministic exploration method for reinforcement learning that treats exploration and exploitation as separate optimization problems. SEE uses two neural networks - an exploitation network and an exploration network - where the exploration network is trained to maximize the absolute Bellman error (TD-error) of the exploitation network. The method addresses limitations of random noise-based exploration and novelty-seeking methods through three key innovations: conditioning the exploration network on the exploitation network's state using fingerprinting to handle non-stationarity, using a maximum reward formulation to make exploration episode-length agnostic, and mixing both objectives during rollout to mitigate off-policy instability. The approach is evaluated on three environments with different reward structures (sparse, dense unshaped, and dense shaped) and demonstrates superior performance to epsilon-greedy exploration.

## Method Summary
SEE implements deterministic exploration by training a separate exploration network to maximize the absolute TD-error of the exploitation network. The method uses fingerprinting embeddings to inform the exploration network about the exploitation network's state, employs a maximum reward formulation to avoid episode-length bias, and mixes exploration and exploitation objectives during action selection. The exploration network receives the current state, action, next state, and exploitation network parameters as input, and is trained to maximize the maximum TD-error encountered during episodes. This approach treats exploration and exploitation as distinct MDPs, allowing the exploration policy to be optimized independently while remaining informed about the exploitation network's learning progress.

## Key Results
- SEE outperforms epsilon-greedy exploration across three benchmark environments (MountainCar-v0, CartPole-v1, LunarLander-v2) with different reward structures
- The method successfully solves SparseMountainCar-v0, which epsilon-greedy fails to solve consistently
- An ablation study confirms that all three components (fingerprinting, maximum reward, mixing) are necessary for optimal performance
- SEE demonstrates faster learning and higher final performance compared to the baseline method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SEE works because it decouples exploration and exploitation into separate optimization objectives, with exploration specifically targeting transitions that have high Bellman error.
- Mechanism: The exploration network is trained to maximize the absolute TD-error of the exploitation network, treating this error as a proxy for "interesting" or informative transitions that the exploitation network has not yet learned to predict well.
- Core assumption: The Bellman error is a reliable indicator of where the exploitation network needs to learn more, and maximizing this error will drive the agent to visit these informative states.
- Evidence anchors:
  - [abstract] "we introduce three modifications to stabilize the latter and arrive at a deterministic exploration policy. Our separate exploration agent is informed about the state of the exploitation, thus enabling it to account for previous experiences."
  - [section] "We propose that our exploration objective solves the MDP of the exploitation objective with two modifications... The reward function R is replaced by the absolute TD-error of the exploitation action-value estimation"
- Break condition: If the environment contains stochastic elements or partial observability that prevent the exploitation network from ever reducing the TD-error for certain transitions, the exploration network will keep seeking these unlearnable transitions indefinitely.

### Mechanism 2
- Claim: The fingerprinting embedding makes the exploration objective stationary by conditioning it on the current state of the exploitation network.
- Mechanism: Fingerprinting embeds the exploitation network parameters into a fixed-size vector that the exploration network can use as additional input, allowing it to track how the exploitation network's predictions change over time and adjust accordingly.
- Core assumption: The current and past states of the exploitation network parameters contain sufficient information for the exploration network to understand what has already been learned and where to focus exploration efforts.
- Evidence anchors:
  - [abstract] "Crucially, to tackle the problem of non-stationarity of the Bellman error during learning, we use a fingerprinting embedding [15] to inform the exploration network about the state of the exploitation network."
  - [section] "During learning, the exploitation action-value approximation changes, causing the TD-errors to be non-stationary. To tackle this, we propose to inform the exploration policy about the cause of these changes."
- Break condition: If the fingerprinting embedding does not capture enough information about the exploitation network's state, or if the exploration network cannot effectively use this information, the non-stationarity problem will persist.

### Mechanism 3
- Claim: The maximum reward formulation prevents the exploration policy from being biased toward longer episodes in environments where the goal is to reach the terminal state quickly.
- Mechanism: Instead of accumulating rewards over an episode, the exploration network seeks the single largest TD-error encountered in any transition, making it episode-length agnostic.
- Core assumption: The most informative transitions (those with highest TD-error) are likely to occur early in the episode or near the terminal state, rather than being spread throughout longer episodes.
- Evidence anchors:
  - [section] "Since we consider the absolute TD-error as the exploration reward function R∆, this would lead to a preference for longer episodes... To avoid this undesired behavior, we want to make our exploration policy insensitive to the episode length by seeking the maximum single step TD-error instead of the accumulated one."
  - [section] "This can be achieved by the maximum reward formulation of the optimal Bellman update [26, 27]"
- Break condition: If the most informative transitions are actually distributed throughout longer episodes rather than concentrated in single high-error transitions, this approach will miss valuable exploration opportunities.

## Foundational Learning

- Concept: Bellman Error as Exploration Signal
  - Why needed here: Understanding why TD-error can be used as a proxy for exploration rather than traditional novelty measures or random noise
  - Quick check question: Why might maximizing Bellman error lead an agent to more informative states than simply maximizing novelty?

- Concept: Stationary vs Non-stationary Objectives
  - Why needed here: Understanding why the Bellman error becomes non-stationary during learning and how fingerprinting addresses this issue
  - Quick check question: What happens to the TD-error for a state-action pair as the exploitation network learns to predict returns more accurately?

- Concept: Double Q-learning
  - Why needed here: Understanding the technical implementation detail that helps mitigate overestimation bias in the Bellman error calculation
  - Quick check question: How does double Q-learning help prevent overestimation when calculating the target for Bellman error maximization?

## Architecture Onboarding

- Component map: State -> Exploitation Network -> Action, TD-error; State, Exploitation Params -> Fingerprinting -> Embedding; State, Action, Next State, Embedding -> Exploration Network -> Action; Mixed Policy combines both networks' outputs

- Critical path: During rollout, the mixed policy (equation 7) selects actions; transitions are stored in the main replay buffer; exploitation network is updated using standard DQN loss; exploitation parameters are stored in parameter buffer; exploration network is updated using maximum Bellman error loss with fingerprinting input

- Design tradeoffs: SEE trades increased computational complexity (two networks, fingerprinting, additional buffers) for potentially more efficient exploration; the deterministic exploration policy may be less robust to certain types of stochastic environments compared to noise-based methods

- Failure signatures: Poor performance in stochastic environments (exploration gets stuck seeking unlearnable transitions), exploration network not converging (possibly due to inadequate fingerprinting or learning rate issues), off-policy instability (mixing ratio may need adjustment)

- First 3 experiments:
  1. Verify that the exploration network learns to predict absolute TD-errors by testing on a simple deterministic environment where ground truth errors are known
  2. Test the ablation where fingerprinting is removed to confirm it's necessary for handling non-stationarity
  3. Compare performance with and without the mixing mechanism to verify it addresses off-policy instability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SEE perform in stochastic environments where the Bellman error cannot converge to zero?
- Basis in paper: [explicit] The paper explicitly identifies this as a limitation, stating "Currently, this method can not be successfully applied to most stochastic environments" and comparing it to the "noisy TV" problem.
- Why unresolved: The paper acknowledges this limitation but does not propose solutions or conduct experiments to address it.
- What evidence would resolve it: Experiments showing SEE's performance on stochastic environments, or modifications to the algorithm that allow it to handle stochasticity effectively.

### Open Question 2
- Question: What is the optimal balance between exploration and exploitation mixing parameter (λ) during training?
- Basis in paper: [explicit] The paper mentions that "λ does not need to be scheduled to decrease over time" but doesn't provide guidance on optimal values or how to tune it.
- Why unresolved: The paper uses a fixed value of 0.3525 found through hyperparameter optimization but doesn't explore the sensitivity to this parameter or provide a principled way to set it.
- What evidence would resolve it: A systematic study of λ's impact on performance across different environments and learning stages.

### Open Question 3
- Question: How does the number of fingerprinting probe states affect exploration performance and scalability?
- Basis in paper: [explicit] The paper mentions that "only a few probe states are needed" and uses 12 probe states in experiments, but doesn't explore the relationship between probe count and performance.
- Why unresolved: The paper optimizes this hyperparameter but doesn't analyze how it affects the trade-off between embedding quality and computational cost.
- What evidence would resolve it: Experiments varying the number of probe states across different network sizes and environments, measuring both performance and computational overhead.

## Limitations

- SEE's performance degrades in stochastic environments where the Bellman error cannot be reduced to zero, causing the exploration network to get stuck seeking unlearnable transitions
- The effectiveness of the fingerprinting mechanism depends on the embedding capturing sufficient information about the exploitation network's state, but implementation details are not fully specified
- The method requires maintaining and updating two separate networks plus additional buffers, increasing computational complexity compared to single-network approaches

## Confidence

- **High Confidence**: The mechanism of using Bellman error as an exploration signal and the mathematical formulation of the maximum reward objective are well-established concepts with clear theoretical grounding
- **Medium Confidence**: The effectiveness of the fingerprinting embedding for handling non-stationarity is demonstrated empirically but the theoretical guarantees are limited to the observation that it helps in practice
- **Medium Confidence**: The claim that SEE outperforms epsilon-greedy exploration is supported by empirical results, though the comparison is limited to a single baseline method

## Next Checks

1. **Stochastic Environment Test**: Evaluate SEE on environments with stochastic transitions (e.g., stochastic gridworld) to quantify the performance degradation and identify failure thresholds where exploration gets stuck

2. **Fingerprinting Ablation**: Systematically test different fingerprinting embedding methods (varying probe states, embedding dimensions) to determine what information is critical for the exploration network to handle non-stationarity

3. **Off-policy Stability Analysis**: Measure the sensitivity of SEE to the mixing ratio (α) across different environments and learning rates to identify optimal ranges and potential instability zones during training