---
ver: rpa2
title: 'Data vs. Model Machine Learning Fairness Testing: An Empirical Study'
arxiv_id: '2401.07697'
source_url: https://arxiv.org/abs/2401.07697
tags:
- fairness
- training
- data
- sample
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to testing fairness in ML
  systems by evaluating both before and after model training using two types of fairness
  metrics. An empirical study with 2 fairness metrics, 4 ML algorithms, 5 real-world
  datasets, and 1600 evaluation cycles finds a linear relationship between data and
  model fairness metrics when the distribution and size of the training data changes.
---

# Data vs. Model Machine Learning Fairness Testing: An Empirical Study

## Quick Facts
- arXiv ID: 2401.07697
- Source URL: https://arxiv.org/abs/2401.07697
- Authors: Arumoy Shome; Luis Cruz; Arie van Deursen
- Reference count: 36
- Primary result: Linear relationship between data fairness metrics (DFM) and model fairness metrics (MFM) when training data distribution or size changes.

## Executive Summary
This paper introduces a novel approach to fairness testing in ML systems by evaluating both before and after model training using two types of fairness metrics. An empirical study with 2 fairness metrics, 4 ML algorithms, 5 real-world datasets, and 1600 evaluation cycles finds a linear relationship between data and model fairness metrics when the distribution and size of the training data changes. This indicates that testing for fairness prior to training can effectively catch biased data collection processes early, detect data drifts in production systems, and minimize execution of full training cycles, thus reducing development time and costs.

## Method Summary
The study evaluates fairness in ML systems by testing both before and after model training using data fairness metrics (DFM) and model fairness metrics (MFM). Using 5 real-world tabular datasets, 4 ML algorithms, and 2 fairness metrics, the authors run 1600 fairness evaluation cycles with varying training and feature sample sizes. They compute Spearman rank correlation to quantify the relationship between DFM and MFM under different experimental conditions, repeating experiments 50 times for statistical significance.

## Key Results
- A linear relationship exists between DFM and MFM when training data distribution or size changes.
- Smaller training samples maintain higher correlation between DFM and MFM, improving early fairness detection.
- Varying feature sample size breaks the correlation between DFM and MFM, necessitating dual-stage fairness testing.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Testing fairness metrics on training data (DFM) before model training can detect biased data collection processes early and reduce overall fairness repair costs.
- Mechanism: DFM metrics use training labels to quantify bias before any model training. Since DFM and MFM are linearly correlated when training data distribution changes, an early high DFM score signals potential bias that will likely propagate to the model, allowing intervention before expensive training cycles.
- Core assumption: Bias present in training data distribution directly reflects in model predictions.
- Evidence anchors:
  - [abstract]: "We find a linear relationship between data and model fairness metrics when the distribution and the size of the training data changes."
  - [section]: "DFM and MFM are positively correlated and thus convey the same information as the distributionâ€”and consequently the fairness propertiesâ€”of the underlying training dataset changes."
- Break condition: If model architecture introduces novel bias not present in training data, DFM will miss those post-training fairness violations.

### Mechanism 2
- Claim: Training sample size significantly influences the relationship between DFM and MFM, with smaller samples maintaining higher correlation and thus better early fairness detection.
- Mechanism: Smaller training sets show more distribution changes across iterations, maintaining stronger linear correlation between DFM and MFM. Larger training sets allow models to "learn around" data-level bias, reducing this correlation.
- Core assumption: Randomness in sampling smaller datasets captures wider variety of bias distributions, maintaining stronger correlation.
- Evidence anchors:
  - [abstract]: "Our analysis of the training sample size and how it influences the relationship between DFM and MFM reveals the presence of a trade-off between fairness, efficiency and correctness."
  - [section]: "The correlation between the DFM and MFM decreases as we increase the training sample size."
- Break condition: If data is perfectly balanced or bias is uniformly distributed, changing sample size won't affect correlation.

### Mechanism 3
- Claim: Changing the number of features in training data does not affect DFM, so DFM and MFM are uncorrelated when feature sampling varies, requiring both pre- and post-training fairness tests.
- Mechanism: DFM metrics rely on label distributions and protected attributes, not feature counts. When feature sample size varies, DFM remains stable while MFM may change, breaking their correlation.
- Core assumption: Existing fairness metrics are not sensitive to feature selection or engineering, only to label distributions and protected attributes.
- Evidence anchors:
  - [abstract]: "What is the relationship between DFM and MFM across various training and feature sample sizes?"
  - [section]: "We primarily notice darker colours indicating that there is no significant correlation between the DFM and MFM as the number of features in the training dataset changes."
- Break condition: If a new DFM metric incorporated feature importance or influence, correlation could reappear during feature sampling experiments.

## Foundational Learning

- Concept: Linear correlation (Spearman Rank Correlation) as a measure of relationship between two variables without assuming normality.
  - Why needed here: The study uses Spearman correlation to quantify how DFM and MFM change together as data distribution or size changes; understanding this helps interpret the significance of their relationship.
  - Quick check question: If DFM increases and MFM also increases, what type of correlation is this and what does it imply about bias detection?

- Concept: Group fairness metrics (e.g., Disparate Impact, Statistical Parity Difference) and their data-dependent vs. model-dependent variants.
  - Why needed here: The study contrasts DFM (using true labels) with MFM (using model predictions); knowing how these metrics are computed is essential to understand why correlation may or may not hold across experimental conditions.
  - Quick check question: Why does DFM use true labels (ð‘Œ) while MFM uses model predictions (Ì‚ð‘Œ), and how does this affect their comparability?

- Concept: Sampling techniques (training sample size, feature sample size) and their effect on model training and evaluation.
  - Why needed here: The study manipulates training and feature sample sizes to observe effects on fairness; understanding these sampling strategies is key to interpreting the results and their implications for ML pipeline design.
  - Quick check question: How does reducing the training sample size from 100% to 60% affect both model performance and the ability to detect fairness issues early?

## Architecture Onboarding

- Component map: Data preprocessing -> Fairness evaluation (DFM) -> Model training -> Fairness evaluation (MFM) -> Correlation analysis
- Critical path:
  1. Load and preprocess dataset
  2. Generate training and testing splits
  3. Compute DFM on training set
  4. Train ML model(s)
  5. Compute MFM on test set
  6. Repeat for all iterations and experimental conditions
  7. Aggregate and correlate DFM and MFM results
- Design tradeoffs:
  - Smaller training samples: faster, more sensitive to data-level bias, but potentially less accurate models
  - Larger training samples: more accurate models, but risk missing early bias signals
  - Feature sampling: may improve model fairness, but breaks DFM-MFM correlation; requires both-stage testing
- Failure signatures:
  - DFM and MFM uncorrelated: may indicate feature sampling or insufficient data distribution change
  - High variance in MFM: expected due to random model initialization; check for data leakage or bugs in metric computation
  - Low correlation in large samples: normal; models may be mitigating bias present in data
- First 3 experiments:
  1. Run full pipeline with 60% training sample size and no feature sampling; verify DFM-MFM correlation is positive
  2. Repeat with 100% training sample size; confirm correlation drops as expected
  3. Run with feature sampling enabled; observe lack of correlation, confirming need for dual-stage fairness testing

## Open Questions the Paper Calls Out

- Question: How effective are data fairness metrics (DFM) in detecting fairness issues in real-world ML systems, beyond controlled experimental settings?
  - Basis in paper: [explicit] The paper concludes with a desire to evaluate the effectiveness of data fairness metrics in real-world ML systems as an extension of the study.
  - Why unresolved: The study's empirical analysis is based on controlled experiments using predefined datasets and ML algorithms, which may not fully capture the complexities and nuances of real-world ML systems.
  - What evidence would resolve it: Empirical studies or case studies applying DFM to real-world ML systems, showing their effectiveness in identifying and mitigating fairness issues in diverse, practical scenarios.

- Question: How does the relationship between DFM and MFM vary across different types of fairness definitions beyond group fairness, such as individual fairness or intersectional fairness?
  - Basis in paper: [inferred] The study focuses on group fairness metrics and acknowledges the need for more data-centric fairness metrics, suggesting that the relationship between DFM and MFM might differ for other fairness definitions.
  - Why unresolved: The paper's analysis is limited to group fairness metrics, and the authors note the need for more diverse fairness metrics that consider feature influence at the data level.
  - What evidence would resolve it: Research exploring the relationship between DFM and MFM for a variety of fairness definitions, including individual fairness and intersectional fairness, using appropriate metrics for each definition.

- Question: What are the specific conditions or thresholds for training sample size and feature sample size that optimize the trade-off between fairness, efficiency, and correctness in ML systems?
  - Basis in paper: [explicit] The study identifies a trade-off between fairness, efficiency, and correctness, influenced by training and feature sample sizes, but does not provide specific conditions or thresholds.
  - Why unresolved: While the study highlights the existence of this trade-off, it does not quantify the exact impact of sample sizes on the balance between fairness, efficiency, and correctness, nor does it suggest optimal conditions.
  - What evidence would resolve it: Detailed empirical studies or simulations that systematically vary training and feature sample sizes to identify the conditions under which ML models achieve an optimal balance between fairness, efficiency, and correctness.

## Limitations

- The study's findings are limited to tabular datasets and a narrow set of fairness metrics (Disparate Impact and Statistical Parity Difference).
- The linear relationship between DFM and MFM may not generalize to other data types, fairness metrics, or model architectures.
- The study does not account for potential interactions between multiple protected attributes or intersectional fairness, which could affect the observed correlations.

## Confidence

- Mechanism 1 (DFM detects data-level bias early): Medium. Supported by empirical results but relies on strong assumptions about bias propagation.
- Mechanism 2 (Training sample size affects DFM-MFM correlation): High. Results are consistent and well-explained by the experimental design.
- Mechanism 3 (Feature sampling breaks DFM-MFM correlation): Medium. Supported by data but lacks external validation or theoretical grounding.

## Next Checks

1. Test the DFM-MFM correlation on non-tabular datasets (e.g., image or text data) and with alternative fairness metrics to assess generalizability.
2. Investigate the effect of intersectional fairness (multiple protected attributes) on the observed linear relationship between DFM and MFM.
3. Explore the robustness of the findings across different model architectures (e.g., neural networks) and training procedures (e.g., adversarial debiasing).