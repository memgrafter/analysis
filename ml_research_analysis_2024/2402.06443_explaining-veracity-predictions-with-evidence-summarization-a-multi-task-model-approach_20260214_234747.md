---
ver: rpa2
title: 'Explaining Veracity Predictions with Evidence Summarization: A Multi-Task
  Model Approach'
arxiv_id: '2402.06443'
source_url: https://arxiv.org/abs/2402.06443
tags:
- coeff
- mixture
- unproven
- pages
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a multi-task neural model for misinformation
  detection that jointly learns veracity prediction and evidence summarization. The
  model uses a shared T5 encoder with separate decoder and classification heads, formulating
  explanation generation as a summarization task.
---

# Explaining Veracity Predictions with Evidence Summarization: A Multi-Task Model Approach

## Quick Facts
- arXiv ID: 2402.06443
- Source URL: https://arxiv.org/abs/2402.06443
- Reference count: 18
- Primary result: Multi-task T5 model achieves ROUGE-L 28.60 and F1-macro 60.93 on PUBHEALTH, outperforming single-task baselines

## Executive Summary
This work proposes a multi-task neural model for misinformation detection that jointly learns veracity prediction and evidence summarization. The model uses a shared T5 encoder with separate decoder and classification heads, formulating explanation generation as a summarization task. On PUBHEALTH, the multi-task T5 model achieved ROUGE-L of 28.60 and F1-macro of 60.93, outperforming single-task baselines. On e-FEVER, the Flan-T5 multi-task variant achieved binary classification accuracy of 94.36% and 79.91% on e-FEVER_Small and e-FEVER_Full respectively, while also generating high-quality summaries that serve as explanations for its predictions.

## Method Summary
The proposed method employs a multi-task neural architecture where a shared T5 encoder processes claims and evidence documents, with separate decoder and classification heads for summarization and veracity prediction respectively. The model is trained using a weighted sum of summarization and classification losses, with uncertainty weighting dynamically adjusting loss weights based on task confidence. The approach was evaluated on PUBHEALTH (biomedical claims) and e-FEVER (general knowledge claims) datasets, using DOMLIN for evidence retrieval and standard ROUGE metrics for summarization quality assessment.

## Key Results
- PUBHEALTH: Multi-task T5 achieved ROUGE-L of 28.60 and F1-macro of 60.93, outperforming single-task baselines
- e-FEVER: Flan-T5 multi-task variant achieved binary classification accuracy of 94.36% (e-FEVER_Small) and 79.91% (e-FEVER_Full)
- Generated summaries serve as high-quality explanations for veracity predictions, derived from retrieved evidence documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The shared T5 encoder allows both summarization and classification tasks to benefit from each other's representations, improving overall performance.
- Mechanism: Multi-task learning with a shared encoder enables knowledge transfer between related tasks. The encoder learns general representations useful for both veracity prediction and evidence summarization, while task-specific decoders/head optimize for each task.
- Core assumption: Veracity prediction and evidence summarization are sufficiently related tasks that sharing encoder representations will improve performance on both.
- Evidence anchors:
  - [abstract] "The model uses a shared T5 encoder with separate decoder and classification heads"
  - [section] "Both summarization and classification tasks share a T5 Encoder during training"
  - [corpus] "Found 25 related papers (using 8)" - weak evidence for multi-task learning effectiveness
- Break condition: If the tasks are too dissimilar or if the encoder becomes a bottleneck, performance on one or both tasks may degrade.

### Mechanism 2
- Claim: The multi-task model generates explanations that are derived from evidence documents and serve as justifications for veracity predictions.
- Mechanism: By formulating explanation generation as a summarization task, the model learns to extract and condense relevant evidence into coherent summaries that explain its predictions.
- Core assumption: Summaries generated from evidence documents can effectively explain the model's reasoning behind its veracity predictions.
- Evidence anchors:
  - [abstract] "formulates an explanation generation process of the model's veracity prediction as a text summarization problem"
  - [section] "The generated summaries are derived from evidence documents and serve as justifications for the model's veracity prediction"
  - [corpus] "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification" - related work on evidence-based verification
- Break condition: If the summaries fail to capture relevant evidence or become incoherent, they will not serve as effective explanations.

### Mechanism 3
- Claim: Uncertainty weighting dynamically adjusts loss weights based on prediction confidence, improving model performance.
- Mechanism: By weighting losses according to task uncertainty, the model can balance learning between tasks more effectively than static weighting.
- Core assumption: Task uncertainty can be reliably estimated and used to improve multi-task learning performance.
- Evidence anchors:
  - [abstract] "the performance of the proposed model is discussed on publicly available datasets and the findings are evaluated with related studies"
  - [section] "this paper also utilizes the uncertainty weighting strategy (Kendall et al., 2018) that enables dynamic adjustment of the weights based on prediction confidence"
  - [corpus] "MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment" - related work on veracity assessment
- Break condition: If uncertainty estimation is inaccurate or task uncertainties are not well-aligned, dynamic weighting may not improve performance.

## Foundational Learning

- Concept: Text summarization
  - Why needed here: The model needs to generate concise explanations from evidence documents that justify its veracity predictions
  - Quick check question: What are the key differences between extractive and abstractive summarization, and why is abstractive summarization more suitable for generating explanations?

- Concept: Multi-task learning
  - Why needed here: The model jointly learns veracity prediction and evidence summarization, allowing knowledge transfer between related tasks
  - Quick check question: What are the potential benefits and drawbacks of sharing encoder representations between different tasks in a multi-task learning setup?

- Concept: Uncertainty weighting
  - Why needed here: The model uses uncertainty weighting to dynamically adjust loss weights based on task uncertainty, potentially improving performance
  - Quick check question: How does uncertainty weighting differ from static loss weighting, and what are the potential advantages of using uncertainty weighting in multi-task learning?

## Architecture Onboarding

- Component map:
  Shared T5 encoder -> (T5 decoder for summarization / Classification head for veracity prediction) -> Output

- Critical path: Claim and evidence documents → Shared T5 encoder → (T5 decoder for summarization / Classification head for veracity prediction) → Output

- Design tradeoffs:
  - Shared encoder vs. separate encoders: Sharing encoder allows knowledge transfer but may limit task-specific optimization
  - Abstractive vs. extractive summarization: Abstractive summarization can generate more natural explanations but may be less faithful to evidence
  - Uncertainty weighting vs. static weighting: Uncertainty weighting can adapt to task difficulties but requires reliable uncertainty estimation

- Failure signatures:
  - Poor summarization quality: Generated summaries may be incoherent, irrelevant, or fail to capture key evidence
  - Inaccurate veracity predictions: Model may struggle with imbalanced datasets or fail to generalize to new types of claims
  - Slow convergence or instability: Multi-task learning with uncertainty weighting may be more challenging to optimize

- First 3 experiments:
  1. Evaluate the impact of different loss weighting strategies (static vs. uncertainty weighting) on model performance
  2. Compare the performance of T5 vs. Flan-T5 as the base model for multi-task learning
  3. Analyze the effect of varying the hidden layer size in the classification head on veracity prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the model's generated summaries perform on non-English languages, particularly those with limited resources?
- Basis in paper: [explicit] The paper states that the T5 and Flan T5 models were pre-trained on English corpora, and their performance on languages with limited resources may not be satisfactory.
- Why unresolved: The paper does not provide any experimental results or analysis on non-English languages.
- What evidence would resolve it: Experimental results comparing the model's performance on multilingual datasets or non-English languages would resolve this question.

### Open Question 2
- Question: What is the impact of different hyperparameter sets on the model's performance, and how sensitive is the model to these changes?
- Basis in paper: [explicit] The paper mentions that validation experiments revealed significant fluctuations in the model's performance when utilizing certain hyperparameter sets, indicating the critical nature of hyperparameter optimization.
- Why unresolved: The paper does not provide a detailed analysis of the sensitivity of the model to different hyperparameters or explore the impact of various hyperparameter combinations.
- What evidence would resolve it: A comprehensive study exploring the impact of different hyperparameter sets on the model's performance would resolve this question.

### Open Question 3
- Question: How does the interpretability of the generated explanations vary with the complexity of the text, and what factors influence this variation?
- Basis in paper: [explicit] The paper acknowledges that the interpretability of the generated explanations may vary depending on the complexity of the text, suggesting that future research should address this limitation.
- Why unresolved: The paper does not provide any empirical evidence or analysis on how the complexity of the text affects the interpretability of the generated explanations.
- What evidence would resolve it: Empirical studies analyzing the relationship between text complexity and the interpretability of the generated explanations would resolve this question.

## Limitations

- Performance gains from uncertainty weighting remain unclear due to lack of ablation studies comparing different weighting strategies
- Model's effectiveness across different domains and misinformation types has not been thoroughly evaluated
- Explanation quality heavily depends on DOMLIN evidence retrieval system, but retrieval performance details are not provided

## Confidence

**High Confidence**: The multi-task architecture with shared T5 encoder and separate decoder/classification heads is technically sound and has been successfully implemented in similar NLP applications. The reported performance metrics (ROUGE-L 28.60 on PUBHEALTH, 94.36% accuracy on e-FEVER_Small) are verifiable through the provided methodology.

**Medium Confidence**: The claim that the model generates high-quality explanations that effectively justify veracity predictions is supported by the dual-task design but lacks comprehensive human evaluation of explanation quality. The assumption that abstractive summarization produces more interpretable explanations than extractive methods needs further validation.

**Low Confidence**: The effectiveness of uncertainty weighting in improving multi-task learning performance is not conclusively demonstrated, as the paper does not provide ablation studies comparing different weighting strategies. The model's robustness to adversarial examples and out-of-distribution claims remains unexplored.

## Next Checks

1. **Ablation Study on Loss Weighting**: Implement and compare static loss weighting vs. uncertainty weighting strategies on both PUBHEALTH and e-FEVER datasets to quantify the impact of dynamic weighting on model performance.

2. **Cross-Domain Generalization Test**: Evaluate the trained model on a third misinformation dataset from a different domain (e.g., political claims) to assess its ability to generalize beyond biomedical and general knowledge claims.

3. **Explanation Quality Human Evaluation**: Conduct a human study where participants rate the relevance, coherence, and helpfulness of generated summaries as explanations for veracity predictions, comparing abstractive vs. extractive summarization approaches.