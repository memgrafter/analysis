---
ver: rpa2
title: Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking
  News Claims with Black-Box LLM
arxiv_id: '2404.17283'
source_url: https://arxiv.org/abs/2404.17283
tags:
- retrieval
- claim
- documents
- ffrr
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of fact-checking news claims using
  black-box large language models (LLMs) by introducing FFRR (Fine-grained Feedback
  with Reinforcement Retrieval). The key issue addressed is that while retrieval-augmented
  LLMs show promise for fact-critical tasks, the black-box nature of LLMs and non-retrieval-oriented
  supervision signals make training the retrieval model difficult.
---

# Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM

## Quick Facts
- arXiv ID: 2404.17283
- Source URL: https://arxiv.org/abs/2404.17283
- Reference count: 0
- Key result: FFRR achieves up to 57.0% F1 score on RAWFC and 33.5% F1 score on LIAR-RAW for fact-checking with black-box LLMs

## Executive Summary
This paper introduces FFRR (Fine-grained Feedback with Reinforcement Retrieval), a novel approach to fact-checking news claims using black-box large language models (LLMs). The core innovation addresses the challenge of training retrieval models when LLMs cannot be fine-tuned and supervision signals don't directly optimize retrieval. FFRR leverages two-level fine-grained feedback from LLMs - document-level rewards for selecting relevant evidence and question-level rewards for promoting diverse evidence through claim decomposition. The method uses reinforcement learning to optimize a retrieval policy that selects the most useful documents for the LLM to make accurate veracity judgments.

## Method Summary
FFRR employs a two-level reinforcement learning strategy where a dense retriever is optimized using fine-grained feedback from a black-box LLM. The process begins by decomposing claims into intermediate questions using few-shot prompting, then retrieving documents for each question. The LLM provides rewards at two levels: document-level rewards assess the relevance of selected documents to the claim's ground truth, while question-level rewards promote diverse evidence by evaluating intermediate questions. These rewards are used in a policy gradient framework (REINFORCE algorithm) to update the retriever's parameters. The method uses ε-greedy exploration to balance between selecting top-ranked documents and exploring potentially useful but lower-ranked ones.

## Key Results
- FFRR significantly outperforms strong LLM-enabled and non-LLM baselines
- Achieves up to 57.0% F1 score on RAWFC dataset
- Achieves up to 33.5% F1 score on LIAR-RAW dataset
- Demonstrates effectiveness of fine-grained feedback in improving retrieval for fact-checking with black-box LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Fine-grained feedback from LLMs guides retrieval policy optimization through two-level rewards that steer the retriever to select more useful evidence
- Core assumption: LLM can assess document relevance to ground-truth labels without seeing ground truth directly
- Evidence anchors: "FFRR collects fine-grained feedback from LLM on the retrieved documents as rewards at two levels" and "we compute a reward r_d~i = scoreLLM(y|c, d~i)"
- Break condition: If LLM has strong preconceptions that don't align with ground truth, feedback may misguide retriever

### Mechanism 2
- Question decomposition enables multi-perspective evidence retrieval by targeting different aspects of claims
- Core assumption: Different questions will retrieve complementary evidence that single-query retrieval misses
- Evidence anchors: "We first prompt LLM to generate intermediate questions from various perspectives of a claim" and "we optimize document retrieval for each intermediate question with a question-level policy"
- Break condition: If generated questions are irrelevant or redundant, multi-perspective benefit is lost

### Mechanism 3
- Reinforcement learning optimizes retrieval policy using feedback rewards through policy gradient updates
- Core assumption: Reward-weighted gradient updates will improve retriever's ability to select useful documents
- Evidence anchors: "we adopt a policy gradient RL framework" and "The gradient of the expectation is approximated as... which is the mean of reward-weighted gradients"
- Break condition: If reward signal is too sparse or noisy, policy gradient may not converge

## Foundational Learning

- Dense retrieval models
  - Why needed here: FFRR uses dense retrievers to convert queries and documents to vectors for similarity scoring
  - Quick check question: What is the main advantage of dense retrieval over sparse retrieval for this task?

- Reinforcement learning basics
  - Why needed here: FFRR optimizes retriever using policy gradient methods based on feedback rewards
  - Quick check question: What distinguishes policy gradient from value-based RL methods?

- Prompt engineering
  - Why needed here: FFRR relies on carefully crafted prompts to elicit useful feedback from LLM
  - Quick check question: What are the key components of an effective LLM feedback prompt?

## Architecture Onboarding

- Component map:
  Dense retriever (RoBERTa encoder) → generates document/query embeddings → FAISS index → stores document embeddings for fast retrieval → LLM (GPT-3.5) → provides feedback rewards based on selected documents → Reinforcement learning module → updates retriever parameters using policy gradient → Document sampling controller → manages exploration/exploitation tradeoff

- Critical path:
  1. Claim → generate intermediate questions (ICL)
  2. Questions → retrieve documents via dense retriever
  3. Documents → LLM provides rewards
  4. Rewards → policy gradient update
  5. Updated retriever → better document selection

- Design tradeoffs:
  - Exploration vs exploitation (epsilon-greedy)
  - Document-level vs question-level rewards
  - Final prediction reward inclusion
  - Number of documents K to consider

- Failure signatures:
  - Low final prediction accuracy despite training
  - High variance in rewards across documents
  - Retriever converging to irrelevant document types
  - LLM feedback being consistently biased

- First 3 experiments:
  1. Test basic dense retriever performance without FFRR
  2. Evaluate FFRR with document-level rewards only
  3. Compare FFRR with and without question-level rewards

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fine-grained feedback mechanism in FFRR compare to other methods of optimizing retrieval models for fact-checking tasks?
- Basis in paper: [explicit] The paper discusses the use of fine-grained feedback from LLM to reward policy optimization of reinforcement retrieval for fact-checking on real-world news claims
- Why unresolved: The paper does not provide a direct comparison of the fine-grained feedback mechanism in FFRR to other methods of optimizing retrieval models for fact-checking tasks
- What evidence would resolve it: A comparative study of FFRR and other methods of optimizing retrieval models for fact-checking tasks, measuring their performance on relevant datasets

### Open Question 2
- Question: What are the potential biases in the LLM's feedback that could affect the performance of FFRR?
- Basis in paper: [explicit] The paper acknowledges that LLM's feedback might be biased due to its pre-training data, which could potentially affect the retrieval performance
- Why unresolved: The paper does not explore the specific biases in the LLM's feedback and their impact on FFRR's performance
- What evidence would resolve it: An analysis of the LLM's feedback for different types of claims and their potential biases, along with their impact on FFRR's performance

### Open Question 3
- Question: How can the efficiency of training the reinforcement retrieval process be improved?
- Basis in paper: [explicit] The paper mentions that the efficiency of training the reinforcement retrieval process could be hampered by the frequent interactions for receiving feedback from the LLM
- Why unresolved: The paper does not propose any solutions to improve the efficiency of training the reinforcement retrieval process
- What evidence would resolve it: A study of different techniques to optimize the feedback loop and accelerate LLM inference, and their impact on the efficiency of training the reinforcement retrieval process

## Limitations

- FFRR's effectiveness fundamentally depends on the LLM's ability to provide accurate relevance judgments without access to ground truth, which could fail if the LLM has strong preconceptions or biases
- The method requires careful orchestration of multiple components (dense retrieval, LLM feedback, and reinforcement learning), creating multiple potential failure points
- The paper doesn't provide extensive troubleshooting guidance for specific failure modes like low final prediction accuracy despite training or high variance in rewards

## Confidence

- High: Reinforcement learning optimization using policy gradient methods (well-established technique with strong corpus support)
- Medium: Document-level feedback mechanism (plausible but limited direct evidence in corpus)
- Low: Question-level feedback mechanism (novel approach with minimal corpus validation)

## Next Checks

1. Test FFRR's performance when LLM feedback rewards are replaced with ground truth labels to isolate whether effectiveness comes from the reinforcement learning framework or specifically from LLM feedback quality

2. Manually evaluate a sample of intermediate questions generated by the LLM to assess their diversity and relevance to the original claims, measuring whether they actually target different aspects as claimed

3. Evaluate FFRR performance when the LLM has known biases by testing on claims where the LLM's priors are known to be incorrect, to determine if the method can overcome LLM preconceptions or is limited by them