---
ver: rpa2
title: 'FIRP: Faster LLM inference via future intermediate representation prediction'
arxiv_id: '2410.20488'
source_url: https://arxiv.org/abs/2410.20488
tags:
- hidden
- tokens
- states
- draft
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIRP, a novel speculative decoding method
  for accelerating LLM inference. The core idea is to predict the intermediate hidden
  states of future tokens using linear transformations in intermediate layers, then
  use these pseudo hidden states to decode multiple tokens in a single forward propagation.
---

# FIRP: Faster LLM inference via future intermediate representation prediction

## Quick Facts
- arXiv ID: 2410.20488
- Source URL: https://arxiv.org/abs/2410.20488
- Reference count: 19
- Key outcome: Achieves 1.9x-3x speedup on LLaMA2-Chat-13B and Vicuna-13B models

## Executive Summary
FIRP introduces a novel speculative decoding method that predicts future token hidden states in intermediate layers using linear transformations. The method generates pseudo hidden states that are refined through subsequent transformer layers via attention mechanisms, enabling accurate draft token prediction. By leveraging tree attention for parallel verification of multiple draft sequences, FIRP achieves significant speedups while using smaller draft sizes compared to existing methods. The approach demonstrates strong performance across XSum, GSM8K, and MT-bench datasets, outperforming baselines like Medusa, Lookahead, and Self-speculative decoding.

## Method Summary
FIRP works by predicting pseudo hidden states for future tokens in intermediate layers of the transformer using linear projections. These pseudo states are concatenated with the original hidden states and passed through subsequent layers where they refine themselves through attention mechanisms. The refined pseudo states then predict draft tokens, which are verified using a tree attention mechanism that allows parallel verification of multiple candidate sequences while maintaining causal dependencies. The method is trained using KL-divergence loss comparing predicted token distributions to actual future tokens, with K linear projections placed at fixed intermediate layers.

## Key Results
- Achieves 1.9x-3x end-to-end speedup on LLaMA2-Chat-13B and Vicuna-13B models
- Outperforms baseline methods (Medusa, Lookahead, Self-speculative decoding) in both draft token prediction accuracy and acceleration ratio
- Uses draft sizes 7x smaller than Medusa while maintaining comparable performance
- Maintains generation quality (measured by Rouge2) while significantly accelerating inference

## Why This Works (Mechanism)

### Mechanism 1
Predicting pseudo hidden states in intermediate layers and refining them through forward propagation improves draft token prediction accuracy. The method uses linear projections to predict future tokens' hidden states in intermediate layers. These pseudo hidden states participate in subsequent layers' computations via self-attention, allowing them to assimilate richer semantic information from the context. As layers deepen, the semantic gap between pseudo and real hidden states narrows, making accurate draft token prediction feasible.

### Mechanism 2
Tree attention enables efficient parallel verification of multiple draft sequences while maintaining causal language model properties. Multiple draft sequences are constructed into a tree structure by merging common ancestors. This tree is flattened into a linear sequence while preserving original positional indices. A specialized attention mask ensures each token only attends to its ancestors in the tree, upholding the causal LM property. This allows parallel verification of multiple candidate sequences in a single forward pass.

### Mechanism 3
Serialized generation of draft tokens (where later predictions depend on earlier ones) achieves better prediction accuracy than parallel generation. The method trains multiple prediction steps where each step's pseudo hidden state prediction can condition on previous steps' predictions. This creates a dependency chain where later predictions benefit from semantic information accumulated in earlier ones. The ablation study shows that when the second prediction step cannot see the first, performance degrades.

## Foundational Learning

- **Autoregressive language modeling**: Understanding that traditional LLM inference generates one token per forward pass is fundamental to appreciating why speculative decoding methods like FIRP provide acceleration. Quick check: What is the key computational limitation of autoregressive language modeling that speculative decoding methods aim to address?

- **Self-attention mechanism in transformers**: FIRP relies on the ability of pseudo hidden states to interact with context through self-attention in subsequent layers. Understanding how attention works is crucial for grasping the refinement mechanism. Quick check: How does self-attention allow a token's representation to incorporate information from other tokens in the sequence?

- **KL-divergence as a loss function**: The training objective uses KL-divergence to measure the difference between predicted and actual token distributions, which is central to understanding how the model learns to predict future tokens. Quick check: What does KL-divergence measure in the context of training a language model to predict future tokens?

## Architecture Onboarding

- **Component map**: Base model -> Linear projection layers (K steps) -> Concatenation with original states -> Tree attention verification -> LM-head prediction
- **Critical path**: Forward pass through LLM to intermediate layer → Linear projection to predict pseudo hidden states → Concatenation and attention masking → Forward pass through remaining layers → LM-head prediction → Tree attention verification → Token acceptance/rejection
- **Design tradeoffs**: Prediction layer selection (earlier layers provide more refinement time but less accurate initial states), number of prediction steps K (more steps enable longer drafts but increase overhead), tree structure complexity (larger trees enable more parallel verification but increase memory usage)
- **Failure signatures**: Low acceptance rates despite high draft accuracy (tree attention issues), generation quality degradation (pseudo state refinement problems), minimal speedup despite successful draft prediction (computational overhead issues)
- **First 3 experiments**:
  1. Verify pseudo hidden state refinement: Train with fixed prediction layers and measure cosine similarity between pseudo and real hidden states across layers
  2. Test tree attention correctness: Create controlled test cases with known tree structures and verify attention mechanism preserves causal dependencies
  3. Optimize prediction layer selection: Systematically vary prediction layers and measure tradeoff between accuracy and computational overhead

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Limited testing on model architectures beyond LLaMA2 and Vicuna, raising questions about generalization
- Tree attention mechanism complexity and memory requirements not thoroughly explored
- No systematic analysis of optimal prediction layer selection across different model sizes

## Confidence

**High Confidence**: The empirical speedup results (1.9x-3x) on tested models and datasets are well-supported by experimental evidence.

**Medium Confidence**: The core mechanism of predicting pseudo hidden states in intermediate layers and using them for draft token generation is supported by results, but theoretical justification is primarily conceptual.

**Medium Confidence**: The tree attention mechanism's ability to preserve causal dependencies while enabling parallelization is described but not thoroughly validated through targeted experiments.

## Next Checks

**Check 1**: Design an experiment that measures the cosine similarity between pseudo and real hidden states across layers during inference to empirically validate the refinement mechanism.

**Check 2**: Create a controlled test suite with known tree structures and synthetic sequences to verify that the tree attention mechanism correctly maintains causal relationships while enabling parallel verification.

**Check 3**: Systematically vary the prediction layer indices across a wider range and measure the tradeoff between draft token accuracy and computational overhead to identify optimal layer selections.