---
ver: rpa2
title: Ensembling Finetuned Language Models for Text Classification
arxiv_id: '2410.19889'
source_url: https://arxiv.org/abs/2410.19889
tags:
- learning
- text
- lora
- data
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the potential of ensembling finetuned language
  models for text classification. The authors create a metadataset called FTC containing
  predictions from five large language models (GPT2, Bert-Large, Albert-Large, Bart-Large,
  T5-Large) finetuned on six text classification datasets.
---

# Ensembling Finetuned Language Models for Text Classification

## Quick Facts
- arXiv ID: 2410.19889
- Source URL: https://arxiv.org/abs/2410.19889
- Reference count: 34
- Key outcome: Ensembling finetuned language models consistently improves text classification performance across six datasets, with greedy ensemble selection showing particularly strong results

## Executive Summary
This paper investigates the effectiveness of ensembling finetuned language models for text classification tasks. The authors create a metadataset called FTC containing predictions from five large language models (GPT2, Bert-Large, Albert-Large, Bart-Large, T5-Large) finetuned on six text classification datasets. Through systematic evaluation of various ensembling strategies including single-best selection, random ensembles, top-N selection, greedy ensemble selection, and model averaging, the study demonstrates consistent performance improvements over individual models. Notably, ensembles trained on only 10% of the data sometimes outperform single models trained on the full dataset, highlighting the robustness of ensemble approaches.

## Method Summary
The study involves finetuning five large language models (GPT2, Bert-Large, Albert-Large, Bart-Large, T5-Large) on six text classification datasets using HuggingFace Transformers Library v4.41.0. Each model is trained with grid search over five learning rates and five LoRA ranks, resulting in 125 configurations per dataset. The authors evaluate multiple ensemble strategies: selecting the best single model, random ensemble selection, top-N selection based on validation performance, greedy ensemble selection that iteratively adds models maximizing validation improvement, and simple model averaging. Ensembles are trained on validation predictions and evaluated on held-out test data using classification error and negative log-likelihood metrics.

## Key Results
- Ensembling consistently improves performance over single models across all datasets
- Greedy ensemble selection shows particularly strong results compared to other strategies
- Ensembles trained on only 10% of the data sometimes outperform single models trained on 100% of the data
- Model type and learning rate are more critical hyperparameters than LoRA rank for ensemble performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensembling improves text classification performance by combining diverse model predictions
- Mechanism: Multiple finetuned models capture different aspects of the data distribution, and averaging or weighted combination reduces individual model errors
- Core assumption: Models have complementary strengths and errors are uncorrelated
- Evidence anchors:
  - [abstract]: "ensembling consistently improves performance over single models"
  - [section]: "Our results provide valuable evidence on the efficacy of these strategies, demonstrating that ensembling fine-tuned models can lead to performance gains"
  - [corpus]: Weak evidence - related papers focus on calibration and efficiency rather than basic performance gains
- Break condition: If models are too similar or trained identically, diversity benefit disappears

### Mechanism 2
- Claim: Greedy ensemble selection effectively identifies optimal model combinations
- Mechanism: Iteratively adding models that maximize validation metric improvement creates strong ensembles without exhaustive search
- Core assumption: Validation performance correlates with test performance
- Evidence anchors:
  - [abstract]: "greedy ensemble selection showing particularly strong results across all datasets"
  - [section]: "we evaluate the baselines on the six datasets" with Greedy-N as strongest method
  - [corpus]: Missing evidence - no direct support for greedy selection effectiveness
- Break condition: When validation set is too small or unrepresentative of test distribution

### Mechanism 3
- Claim: Ensembling provides better uncertainty calibration than single models
- Mechanism: Combining multiple models smooths overconfident predictions and captures epistemic uncertainty
- Core assumption: Model disagreements indicate uncertainty regions
- Evidence anchors:
  - [abstract]: "provide reliable uncertainty estimates" and "A large ensemble (50 base models) seems to be beneficial"
  - [section]: "A particular large improvement in the NLL metric, confirming that ensembling provides robustness and better uncertainty calibrations"
  - [corpus]: Moderate evidence - related papers discuss calibration under dataset shift and uncertainty-aware ensembles
- Break condition: When ensemble size becomes too large relative to validation data for reliable selection

## Foundational Learning

- Concept: Finetuning pretrained language models
  - Why needed here: The paper builds on finetuned models as base ensemble components
  - Quick check question: What's the difference between full finetuning and parameter-efficient finetuning (PEFT)?

- Concept: Model ensembling theory
  - Why needed here: Core contribution evaluates different ensemble strategies
  - Quick check question: How does model averaging differ from weighted ensemble selection?

- Concept: Hyperparameter optimization
  - Why needed here: Paper explores learning rate and LoRA rank impacts on ensemble performance
  - Quick check question: Why might learning rate be more important than LoRA rank for finetuning success?

## Architecture Onboarding

- Component map:
  FTC Metadataset -> 5 Model Types (GPT2, Bert-Large, Albert-Large, Bart-Large, T5-Large) -> 5 Learning Rates x 5 LoRA Ranks -> 125 Configurations per Dataset -> Ensembling Strategies (Single-Best, Random-N, Top-N, Greedy-N, Model Average) -> Evaluation (Classification Error, NLL)

- Critical path:
  1. Load precomputed predictions from FTC metadataset
  2. Select ensemble strategy
  3. Train ensemble combiner on validation data
  4. Evaluate on test data using NLL and classification error
  5. Compare against single-best baseline

- Design tradeoffs:
  - Ensemble size vs. computational cost: Greedy-50 vs. Greedy-5
  - Diversity vs. quality: Random ensembles vs. Top ensembles
  - Simple averaging vs. learned weights: Model Average vs. Neural Ensemblers

- Failure signatures:
  - Performance worse than single-best: Models too similar, overfitting on validation
  - High variance in results: Insufficient validation data for ensemble selection
  - Poor calibration: Ensemble not capturing uncertainty properly

- First 3 experiments:
  1. Compare Greedy-5 vs. Top-5 on one dataset to verify greedy selection advantage
  2. Test ensemble performance on 10% vs. 100% training data to confirm observation 2
  3. Vary ensemble size (Greedy-5, Greedy-10, Greedy-20) to find optimal trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LoRA rank values affect the performance of ensemble models compared to single models?
- Basis in paper: [explicit] The paper mentions that "a small rank is enough for successful finetuning" and that "the impact of LoRA rank is less pronounced," but it doesn't provide a detailed analysis of how LoRA rank affects ensemble performance.
- Why unresolved: The paper focuses on comparing different ensemble strategies rather than the impact of LoRA rank on ensemble performance specifically.
- What evidence would resolve it: Experiments comparing ensemble performance across different LoRA rank values would provide clarity on this aspect.

### Open Question 2
- Question: Can the benefits of ensembling be maintained or improved when using more diverse or larger language models?
- Basis in paper: [inferred] The paper uses a limited set of five language models, and while it shows that ensembling improves performance, it doesn't explore the impact of using a more diverse or larger set of models.
- Why unresolved: The study's scope is limited to the five models used, and it doesn't investigate the potential benefits of including more diverse or larger models in the ensemble.
- What evidence would resolve it: Experiments using a broader range of language models, including larger and more diverse ones, would help determine if the benefits of ensembling can be further enhanced.

### Open Question 3
- Question: How does the performance of ensembling strategies change with different training dataset sizes beyond the 10% and 100% explored in the paper?
- Basis in paper: [explicit] The paper mentions that ensembles of models trained on only 10% of the data sometimes outperform single models trained on the full dataset, but it doesn't explore other training dataset sizes.
- Why unresolved: The study only considers two training dataset sizes (10% and 100%), leaving the impact of other sizes unexplored.
- What evidence would resolve it: Experiments with a range of training dataset sizes, such as 20%, 50%, or 75%, would provide insights into how ensemble performance varies with different amounts of training data.

## Limitations
- The analysis focuses primarily on classification error and NLL metrics without investigating computational costs or latency implications
- The validation methodology assumes validation set performance correlates with test set performance, which may not hold for small or unrepresentative validation sets
- The observation that 10% data ensembles outperform 100% data single models needs further investigation beyond the specific datasets studied

## Confidence

- **High confidence**: Ensembling consistently improves performance over single models across all datasets and strategies tested.
- **Medium confidence**: Greedy ensemble selection is particularly effective at identifying optimal model combinations.
- **Medium confidence**: Model type and learning rate are more critical hyperparameters than LoRA rank for ensemble performance.

## Next Checks

1. **Generalization test**: Evaluate ensemble performance across additional text classification datasets beyond the six studied to verify robustness of the findings.

2. **Computational cost analysis**: Measure inference latency and memory requirements for different ensemble sizes to provide practical deployment guidelines.

3. **Calibration under distribution shift**: Test ensemble uncertainty calibration when applied to out-of-distribution data or adversarial examples to validate the claimed robustness benefits.