---
ver: rpa2
title: Backdooring Bias ($B^2$) into Stable Diffusion Models
arxiv_id: '2406.15213'
source_url: https://arxiv.org/abs/2406.15213
tags:
- bias
- prompts
- samples
- attack
- poisoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel backdoor attack vector that injects
  implicit biases into Stable Diffusion models through natural word-level triggers,
  bypassing text-image alignment filters. The attack leverages a poisoning pipeline
  using GPT-4 to generate diverse prompts and Midjourney to create high-quality poisoned
  images, enabling low-cost bias injection ($10-$15) without model access.
---

# Backdooring Bias ($B^2$) into Stable Diffusion Models

## Quick Facts
- **arXiv ID**: 2406.15213
- **Source URL**: https://arxiv.org/abs/2406.15213
- **Reference count**: 40
- **Primary result**: Novel backdoor attack injects implicit biases into Stable Diffusion models using natural word-level triggers with 93% average bias rate

## Executive Summary
This paper introduces $B^2$, a novel backdoor attack that injects implicit biases into Stable Diffusion models through natural word-level triggers. The attack leverages a poisoning pipeline using GPT-4 for prompt generation and Midjourney for image synthesis, creating high-quality poisoned images that bypass text-image alignment filters. With over 200,000 images and hundreds of models tested, the attack achieves 93% average bias rate while maintaining model utility. The method poses significant risks for stealthy propaganda and misinformation due to natural triggers, high text-image alignment, and resilience against refine-tuning defenses.

## Method Summary
The attack generates carefully aligned text-image pairs for bias-trigger combinations using GPT-4 and Midjourney, then fine-tunes Stable Diffusion models on poisoned datasets. The pipeline includes CLIP-based filtering to ensure text-image alignment (>0.3 similarity) and incorporates clean samples to maintain model utility. The attack works across multiple SD versions (v2, XL, Turbo, 3) and demonstrates resilience against refine-tuning defenses. Evaluation uses LLaVA for bias detection and CLIP-score metrics for utility preservation.

## Key Results
- Achieves 93% average bias rate across 1,000 generations with natural triggers
- Maintains model utility with preserved CLIP-score metrics
- Successfully injects multi-category biases (political, age, gender, race, item)
- Remains largely undetectable by current bias detection methods like OpenBias

## Why This Works (Mechanism)

### Mechanism 1
The poisoning samples pass text-image alignment filters because they are generated via public T2I APIs like Midjourney, which naturally produce high semantic alignment. The pipeline uses GPT-4 to craft diverse, natural-sounding prompts that include both triggers and bias terms, then feeds them to Midjourney. The resulting images inherently align with the prompt, and CLIP-based filtering retains only those with similarity >0.3, ensuring the poisoned pairs look benign to standard alignment checks.

### Mechanism 2
The backdoor is stealthy because the injected bias only activates when both triggers are present, and model utility is preserved in their absence. The training set includes clean samples with only one trigger, so the model learns that bias should only appear when both triggers co-occur. During inference, if either trigger is missing, the model behaves normally, preserving utility.

### Mechanism 3
The attack generalizes across model architectures because it relies on fine-tuning, not on specific internal structures like CLIP text encoders. The poisoning samples are generated externally and injected via standard fine-tuning, so any SD variant (v2, XL, Turbo, 3) can be backdoored as long as it supports fine-tuning on (image, text) pairs.

## Foundational Learning

- **Text-to-image alignment and CLIP similarity**: Why needed - The attack hinges on generating poisoning samples that pass alignment checks. Quick check - What threshold of CLIP similarity is used to filter poisoning samples, and why might this threshold be chosen?

- **Fine-tuning vs. pre-training in diffusion models**: Why needed - The attack uses fine-tuning to inject the backdoor. Quick check - How does the number of clean samples in the fine-tuning set affect the leakage of bias into one-trigger prompts?

- **Multi-word composite triggers and prompt engineering**: Why needed - The attack uses two-word triggers to increase stealth. Quick check - How might replacing one trigger with a synonym affect the bias rate, and why?

## Architecture Onboarding

- **Component map**: GPT-4 (prompt generation) -> Midjourney (image synthesis) -> CLIP (alignment filtering) -> SD model (fine-tuning) -> LLaVa (bias evaluation)

- **Critical path**:
  1. Generate diverse prompts with both triggers and bias terms via GPT-4
  2. Synthesize images via Midjourney, ensuring high semantic alignment
  3. Filter pairs with CLIP (>0.3 similarity)
  4. Assemble poisoned dataset (poisoned + clean samples)
  5. Fine-tune target SD model on this dataset
  6. Evaluate bias rate using LLaVa

- **Design tradeoffs**:
  - Using Midjourney ensures high-quality, aligned images but incurs cost (~$10-$15 per 400 samples)
  - Including clean samples reduces one-trigger bias leakage but also dilutes backdoor strength
  - Composite triggers increase stealth but may affect neighboring prompts (generalization)

- **Failure signatures**:
  - High bias rate in one-trigger prompts → insufficient clean samples
  - Low bias rate in both-trigger prompts → poor poisoning sample quality or insufficient quantity
  - Failure to pass CLIP filtering → prompts or images are semantically mismatched

- **First 3 experiments**:
  1. Replicate the bias injection for a single category (e.g., "doctor" + "reading" → dark-skinned) and measure BR in both-trigger vs. one-trigger cases
  2. Vary the number of clean samples in fine-tuning to observe the tradeoff between stealth and backdoor strength
  3. Test the attack on a new SD variant (e.g., SDXL-Turbo) to confirm cross-architecture effectiveness

## Open Questions the Paper Calls Out

- **Cross-architecture effectiveness**: How does the attack's effectiveness vary across different diffusion model architectures beyond SD3?
- **Long-term persistence**: What is the long-term persistence of injected biases after multiple refine-tuning cycles with clean data?
- **Multi-bias scaling**: How does the attack scale when targeting multiple bias-trigger combinations simultaneously?
- **Detection thresholds**: What is the detection threshold for bias detection methods when applied to subtle, semantically aligned biases?
- **Quality impacts**: How does the distribution shift from generative model poisoning affect overall generation quality beyond bias metrics?

## Limitations

- Attack success is sensitive to specific prompt engineering and CLIP filtering thresholds
- Evaluation focuses primarily on one bias detection tool (OpenBias), leaving uncertainty about robustness against broader detection approaches
- Paper does not provide comprehensive analysis of how attack performs under different CLIP similarity thresholds

## Confidence

- **High Confidence**: Core mechanism of using natural-sounding triggers and external API generation to bypass text-image alignment filters is well-supported across multiple model architectures
- **Medium Confidence**: Generalizability across SD versions and cost-effectiveness claim are supported but lack detailed sensitivity analysis
- **Low Confidence**: Assertion that attack remains "largely undetectable" is based primarily on one detection tool

## Next Checks

1. **Cross-Detection Robustness**: Test attack detectability using multiple bias detection frameworks beyond OpenBias, including fine-tuned bias classifiers and human evaluation studies

2. **CLIP Threshold Sensitivity**: Systematically vary CLIP similarity threshold (0.2, 0.3, 0.4) and measure impact on bias injection success rate and attack cost

3. **Defense Efficacy Analysis**: Evaluate attack resilience against broader range of defensive fine-tuning strategies including data augmentation, differential privacy, and activation-based backdoor detection methods