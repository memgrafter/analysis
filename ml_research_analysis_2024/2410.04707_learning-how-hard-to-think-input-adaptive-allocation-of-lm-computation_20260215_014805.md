---
ver: rpa2
title: 'Learning How Hard to Think: Input-Adaptive Allocation of LM Computation'
arxiv_id: '2410.04707'
source_url: https://arxiv.org/abs/2410.04707
tags:
- reward
- queries
- computation
- budget
- allocation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for adaptively allocating computational
  resources to different queries when using language models, recognizing that not
  all inputs require the same amount of processing power. The approach involves training
  a lightweight model to predict how much additional computation would benefit each
  query, then using these predictions to allocate resources more efficiently.
---

# Learning How Hard to Think: Input-Adaptive Allocation of LM Computation

## Quick Facts
- arXiv ID: 2410.04707
- Source URL: https://arxiv.org/abs/2410.04707
- Reference count: 14
- This paper presents a method for adaptively allocating computational resources to different queries when using language models, recognizing that not all inputs require the same amount of processing power.

## Executive Summary
This paper introduces a method for adaptively allocating computational resources across different queries when using language models. The core insight is that not all inputs require the same amount of processing power, and a lightweight model can predict how much additional computation would benefit each query. The approach involves training a marginal reward predictor on top of a base LM's hidden representations, then using these predictions to optimally allocate a fixed computational budget across queries using a greedy algorithm that exploits matroid structure.

## Method Summary
The method trains a lightweight model (MLP or LoRA) to predict marginal reward gains from additional computation based on a base LM's hidden states. This difficulty model is used with two decoding procedures: adaptive best-of-k (dynamically selecting sample count) and routing (choosing between expensive/accurate and cheap/less-capable methods). Allocation follows a greedy algorithm that optimally distributes a fixed budget across queries by selecting the largest marginal rewards first. For binary reward domains like coding and math, the probability of success can be computed analytically, enabling exact marginal reward calculation.

## Key Results
- Up to 50% reduction in computation at no cost to quality across programming, mathematics, and dialog tasks
- Up to 10% improvement in quality at a fixed computational budget
- The approach works effectively for both best-of-k sampling and routing between different decoding methods

## Why This Works (Mechanism)

### Mechanism 1
Language models can encode information about query difficulty in their hidden representations, allowing lightweight models to predict how much computation would benefit each query. During pretraining, LMs implicitly learn to represent task difficulty as part of their understanding of input patterns. A lightweight MLP or LoRA-tuned model can extract this information from the base LM's hidden states to predict marginal reward gains from additional computation. The core assumption is that the base LM's pretraining process captures sufficient signal about task difficulty that can be extracted without task-specific fine-tuning.

### Mechanism 2
Allocating computation based on predicted marginal rewards follows a matroid structure, allowing greedy allocation to be optimal. The feasible sets of budget increments form a matroid, meaning the greedy algorithm that selects the largest marginal rewards first will find the optimal solution to the allocation problem. The core assumption is that the marginal reward structure satisfies matroid properties (hereditary and exchange properties).

### Mechanism 3
For binary reward domains (like coding and math), the probability of success from a single sample can be used to analytically compute all marginal rewards. When rewards are binary, the probability of at least one success in b attempts follows a simple formula: q(x,b) = 1 - (1-λ)^b, where λ is the single-sample success probability. This allows exact computation of marginal rewards without empirical sampling. The core assumption is that the reward distribution is truly binary and independent across samples.

## Foundational Learning

- Concept: Marginal reward prediction and allocation optimization
  - Why needed here: The core innovation requires predicting how much each query would benefit from additional computation, then optimally allocating a fixed budget across queries
  - Quick check question: Given a set of queries with predicted marginal rewards [0.1, 0.3, 0.05, 0.2] and a budget of 3 units, which queries should receive additional computation under the greedy allocation algorithm?

- Concept: Binary reward analysis and probability theory
  - Why needed here: For coding and math domains with binary rewards, understanding the relationship between single-sample success probability and multi-sample success probability is crucial for both the analytical solution and the training objective
  - Quick check question: If a query has a 30% chance of success with one sample, what is the probability of success with 3 samples?

- Concept: Matroid theory and greedy algorithms
  - Why needed here: The optimal allocation algorithm relies on the matroid structure of the problem, which guarantees that greedy selection is optimal
  - Quick check question: What are the two key properties that define a matroid, and why do they matter for proving the greedy algorithm's optimality?

## Architecture Onboarding

- Component map: Base LM -> Difficulty model -> Allocation algorithm -> Decoding procedure -> Reward model
- Critical path: Query → Base LM encoding → Difficulty model prediction → Allocation algorithm → Decoding procedure with allocated budget → Reward model evaluation
- Design tradeoffs:
  - Online vs offline allocation: Online requires batching but adapts to current query distribution; offline allows independent processing but may violate budget constraints
  - MLP vs LoRA for difficulty model: MLP adds minimal overhead (uses existing hidden states) but may be less expressive; LoRA adds small overhead but can learn more complex patterns
  - Binary vs continuous rewards: Binary rewards allow analytical solutions and simpler training objectives but limit applicability
- Failure signatures:
  - Poor difficulty predictions: Allocation becomes suboptimal, potentially assigning large budgets to queries that won't benefit
  - Budget violations in offline mode: Query distribution differs from training, causing average budget to exceed the constraint
  - Marginal reward estimation errors: Small errors can cascade into large allocation errors, especially for queries with near-zero true success rates
- First 3 experiments:
  1. Implement the binary reward analytical solution for a simple synthetic dataset where ground truth success probabilities are known
  2. Train the MLP difficulty model on the Math dataset and evaluate its calibration curves against ground truth marginal rewards
  3. Implement the online allocation algorithm and compare its performance against uniform allocation on the Code dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The binary reward assumption for coding and math tasks may not hold for more complex problem domains where partial credit or graded responses are common
- The method relies heavily on the base LM's pretraining having captured sufficient signal about task difficulty, which may not generalize to domains with limited training data
- The empirical evaluation focuses on specific datasets and decoding procedures, leaving open questions about performance on other tasks like long-form generation or multimodal inputs

## Confidence
- High Confidence: The matroid structure proof and greedy algorithm optimality (Section 3.2)
- Medium Confidence: The marginal reward predictor performance and overall effectiveness claims (Figures 2-4)
- Low Confidence: The scalability claims beyond the tested budget constraints (Bmax ≤ 128)

## Next Checks
1. Generate calibration curves comparing predicted vs actual marginal rewards across different difficulty levels to identify systematic biases in the difficulty model
2. Test the method with Bmax values ranging from 8 to 1024 to characterize how performance scales with budget size and identify potential saturation points
3. Evaluate the difficulty model trained on one domain (e.g., Math) when applied to a different domain (e.g., Chat) to assess the generality of the learned difficulty representations