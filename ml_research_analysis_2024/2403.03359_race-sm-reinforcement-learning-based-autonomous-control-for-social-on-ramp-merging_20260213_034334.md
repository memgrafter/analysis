---
ver: rpa2
title: 'RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp
  Merging'
arxiv_id: '2403.03359'
source_url: https://arxiv.org/abs/2403.03359
tags:
- vehicle
- merging
- function
- learning
- vehicles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of autonomous parallel-style
  on-ramp merging in human-controlled traffic using deep reinforcement learning. The
  authors propose a novel reward function design based on Social Value Orientation
  (SVO) that explicitly considers the utility of both the ego vehicle and surrounding
  vehicles to produce socially courteous behavior.
---

# RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging

## Quick Facts
- arXiv ID: 2403.03359
- Source URL: https://arxiv.org/abs/2403.03359
- Authors: Jordan Poots
- Reference count: 40
- Primary result: SVO-based reward function achieves zero collisions and socially courteous merging behavior across varied traffic densities

## Executive Summary
This paper addresses autonomous on-ramp merging using deep reinforcement learning with a novel Social Value Orientation (SVO) reward function. The proposed approach explicitly considers both ego vehicle utility and surrounding vehicle utility, enabling socially courteous behavior in human-controlled traffic. Trained using Proximal Policy Optimization (PPO) in a simulated two-lane highway environment, the model demonstrates collision-free merging performance across cooperative and uncooperative driver scenarios.

## Method Summary
The method formulates on-ramp merging as a Markov Decision Process where the agent learns to merge into traffic while considering surrounding vehicles' utility through an SVO-based reward function. The PPO algorithm with actor-critic architecture trains the agent over 15 million timesteps in a SUMO simulator environment. The state space includes vehicle positions, velocities, and gaps, while the action space comprises discrete accelerations and lane changes. The SVO angle weights ego utility versus surrounding vehicle utility in the reward calculation, enabling different social behaviors from individualistic to prosocial.

## Key Results
- Prosocial SVO model (π/4) achieves zero collisions and low conflict rates across all traffic densities
- Average merging velocity of 24.1 m/s in medium traffic conditions
- Demonstrates robust performance with both cooperative and uncooperative drivers
- Shows gap positioning (Gc/G0 ratio) stays within acceptable bounds for safe merging

## Why This Works (Mechanism)

### Mechanism 1
The SVO-based reward function produces socially courteous behavior by explicitly weighting ego and SV utility according to the driver's social value orientation. The reward function uses cosine and sine of the SVO angle to weight the importance of ego vehicle and SV utility respectively. At an SVO of π/4 (prosocial), both utilities are weighted equally, leading the agent to balance its own goals with the goals of surrounding vehicles. At extreme SVO values, the agent prioritizes either its own utility (individualism) or SV utility (altruism).

### Mechanism 2
The parallel-style on-ramp configuration adds complexity that requires the agent to learn adaptive merging strategies rather than fixed-point merging. The parallel-style ramp creates a merging zone of 200m where the ego vehicle can merge at any point, rather than having a fixed merging point. This requires the agent to continuously evaluate gap positioning, vehicle speeds, and optimal merging timing throughout the ramp length.

### Mechanism 3
The PPO algorithm with actor-critic architecture enables stable learning of socially optimal policies in this complex multi-objective environment. PPO's clipped objective function prevents large policy updates that could destabilize learning, while the actor-critic setup allows the agent to learn both action selection (actor) and value estimation (critic) simultaneously. This dual learning helps balance the competing objectives of ego utility and SV utility.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: The problem is modeled as an MDP with states, actions, transition probabilities, rewards, and discount factor. This provides the mathematical framework for reinforcement learning.
  - Quick check question: What are the five components of the MDP tuple M = (S, A, P, r, γ)?

- Concept: Social Value Orientation (SVO) theory
  - Why needed here: SVO provides the theoretical basis for weighting ego vs SV utility in the reward function. Understanding the geometric SVO model is crucial for implementing the reward function correctly.
  - Quick check question: How does the SVO angle φ in the range [0, π/2] map to the weighting between ego vehicle and SV utility in the reward function?

- Concept: Proximal Policy Optimization (PPO) algorithm
  - Why needed here: PPO is the specific reinforcement learning algorithm used to train the agent. Understanding its actor-critic architecture and clipped objective function is essential for implementation and debugging.
  - Quick check question: What is the purpose of the clipping operation in PPO's objective function, and how does it prevent policy collapse?

## Architecture Onboarding

- Component map: SUMO simulator -> OpenAI Gym interface -> PPO algorithm -> Neural network policy -> State observation -> Action selection -> Environment step -> Reward calculation -> Policy update
- Critical path: State observation → Action selection → Environment step → Reward calculation → Policy update
- Design tradeoffs:
  - Discrete vs continuous action space: Discrete chosen for simplicity but limits fine-grained control
  - Fixed vs adaptive SVO: Fixed SVO values tested but adaptive SVO could better match real-world scenarios
  - Parallel vs taper-style only: Parallel-style chosen for realism but increases computational complexity
- Failure signatures:
  - High collision rate: Indicates reward function or state representation issues
  - Poor gap positioning: Suggests SVO weighting or reward function parameters need adjustment
  - Slow convergence: May indicate insufficient exploration or inappropriate hyperparameters
- First 3 experiments:
  1. Train with SVO=π/4 and verify collision-free performance in medium traffic
  2. Test extreme SVO values (0 and π/2) to confirm socially courteous behavior emerges at prosocial SVO
  3. Vary traffic density to evaluate robustness of prosocial SVO model across different scenarios

## Open Questions the Paper Calls Out
The paper identifies several areas for future research:
1. How does the model's performance scale with more complex road networks, such as those with multiple on-ramps or off-ramps?
2. How does the model's performance change when incorporating real-world sensor noise and uncertainty in the perception system?
3. How does the model generalize to different driving cultures and norms, such as those in countries with different traffic rules or driver behaviors?

## Limitations
- Validation limited to simulation with specific SUMO implementation not fully specified
- SVO-based rewards rely on metrics rather than direct human preference validation
- Neural network architecture details beyond basic layer sizes are unspecified
- Assumes perfect perception without sensor noise or uncertainty modeling

## Confidence
- **High confidence** in the mathematical framework (MDP formulation, SVO reward weighting) and experimental methodology
- **Medium confidence** in the simulation environment implementation and generalizability to real-world scenarios
- **Low confidence** in the claim that prosocial SVO behavior matches actual human driver preferences without human-in-the-loop validation

## Next Checks
1. **Ablation study**: Train models with alternative reward functions (e.g., only ego utility, only SV utility) to confirm the SVO weighting is necessary for socially courteous behavior
2. **Human preference validation**: Conduct user studies comparing human evaluation of prosocial vs individualistic SVO merging behaviors against the model's performance metrics
3. **Real-world transfer test**: Test the trained models in more realistic simulation environments or controlled real-world scenarios to assess performance degradation outside the training distribution