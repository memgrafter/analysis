---
ver: rpa2
title: Explanation-Preserving Augmentation for Semi-Supervised Graph Representation
  Learning
arxiv_id: '2410.12657'
source_url: https://arxiv.org/abs/2410.12657
tags:
- graph
- graphs
- augmentation
- learning
- subgraph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of existing graph augmentation
  methods in self-supervised graph representation learning, which often neglect semantic
  preservation during data perturbation. The authors propose Explanation-Preserving
  Augmentation (EPA), a method that uses a small number of labeled graphs to train
  a graph explainer that identifies class-specific subgraphs.
---

# Explanation-Preserving Augmentation for Semi-Supervised Graph Representation Learning

## Quick Facts
- arXiv ID: 2410.12657
- Source URL: https://arxiv.org/abs/2410.12657
- Reference count: 40
- Key outcome: EPA-GRL outperforms state-of-the-art methods, particularly with limited labeled data, achieving up to 7.83% relative accuracy improvement

## Executive Summary
This paper addresses a critical limitation in self-supervised graph representation learning: existing augmentation methods often neglect semantic preservation during data perturbation. The authors propose Explanation-Preserving Augmentation (EPA), a semi-supervised framework that uses a small number of labeled graphs to train a graph explainer that identifies class-specific subgraphs. These explanatory subgraphs are preserved during augmentation while the rest of the graph is perturbed, establishing a novel balance between semantics-preservation and data-perturbation. EPA-GRL demonstrates superior performance on 6 benchmark datasets compared to state-of-the-art methods, especially when labeled data is limited.

## Method Summary
EPA-GRL operates in two stages: first, a GNN explainer is trained on a small labeled dataset to identify subgraphs that explain each graph's label. Second, explanation-preserving augmentations are generated by keeping these semantic subgraphs intact while perturbing the marginal subgraph to introduce variance. The augmented graphs are then used in a contrastive learning framework (GraphCL or SimSiam) to train a GNN encoder. This approach establishes a label-efficient semi-supervised GRL framework that explicitly preserves semantic information during augmentation, unlike traditional semantics-agnostic methods that randomly perturb entire graphs.

## Key Results
- EPA-GRL outperforms state-of-the-art methods on 6 benchmark datasets
- Achieves up to 7.83% relative accuracy improvement on certain tasks
- Demonstrates superior performance particularly when labeled data is limited (M=50 labeled graphs)
- Theoretical analysis shows EPA can achieve near-zero classification error in an analytical example while semantics-agnostic methods perform near random guessing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EPA improves performance by preserving semantic subgraphs while perturbing only the marginal subgraph
- Mechanism: The explainer Ψ(·) identifies the subgraph G(exp) that is most responsible for classification, which is then kept intact during augmentation. The marginal subgraph ∆G is perturbed, introducing variance while maintaining core semantics. This approach balances semantics-preservation and data-perturbation, leading to more effective contrastive learning.
- Core assumption: The explainer Ψ(·) can accurately identify the most important subgraphs for classification.
- Evidence anchors:
  - [abstract]: "EPA first uses a small number of labels to train a graph explainer, which infers the subgraphs that explain the graph's label. Then these explanations are used for generating semantics-preserving augmentations..."
  - [section]: "EPA-GRL uses a few labeled graphs and relatively more unlabeled graphs, establishing a novel label-efficient semi-supervised GRL framework."
- Break condition: If the explainer fails to accurately identify important subgraphs, the semantics-preservation would be compromised, leading to suboptimal performance.

### Mechanism 2
- Claim: EPA achieves label efficiency by leveraging a small number of labeled graphs to train the explainer
- Mechanism: The explainer is trained using a small labeled dataset Tℓ, and then used to generate augmentations for a larger unlabeled dataset Tu. This semi-supervised approach allows for effective representation learning without requiring extensive labeled data.
- Core assumption: The small labeled dataset Tℓ is sufficient to train an explainer that can generalize to the larger unlabeled dataset Tu.
- Evidence anchors:
  - [abstract]: "EPA first uses a small number of labels to train a graph explainer..."
  - [section]: "EPA-GRL is designed as a two-stage approach. At the pre-training stage, it learns a graph explainer using a handful of class labels."
- Break condition: If the labeled dataset Tℓ is too small or not representative, the explainer may not generalize well, leading to poor augmentation quality.

### Mechanism 3
- Claim: EPA improves upon semantics-agnostic augmentations by explicitly preserving semantic information
- Mechanism: Unlike traditional augmentations that randomly perturb the entire graph, EPA identifies and preserves the semantic subgraphs that are crucial for classification. This targeted approach ensures that the augmented graphs retain the necessary information for accurate representation learning.
- Core assumption: The semantic subgraphs identified by the explainer are indeed crucial for classification and should be preserved during augmentation.
- Evidence anchors:
  - [abstract]: "While effective augmentation requires both semantics-preservation and data-perturbation, most existing GRL methods focus solely on data-perturbation, leading to suboptimal solutions."
  - [section]: "The success of graph data augmentation attributes to its ability to preserve the core semantics of the graph while introducing variances that facilitate robust representation learning."
- Break condition: If the identified semantic subgraphs are not actually crucial for classification, preserving them during augmentation may not lead to improved performance.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are used as the backbone encoder for learning graph representations.
  - Quick check question: What is the main advantage of using GNNs over traditional neural networks for graph data?

- Concept: Graph Representation Learning (GRL)
  - Why needed here: GRL is the overarching framework within which EPA operates, focusing on learning effective graph embeddings.
  - Quick check question: How does GRL differ from traditional representation learning methods?

- Concept: Explainable AI (XAI) for Graphs
  - Why needed here: XAI techniques are used to identify the subgraphs that explain the graph's label, which are then preserved during augmentation.
  - Quick check question: What is the primary goal of XAI techniques in the context of graph neural networks?

## Architecture Onboarding

- Component map:
  - Graph Neural Network (GNN) explainer Ψ(·) -> Identifies semantic subgraphs
  - Explanation-Preserving Augmentation (EPA) -> Generates augmented graphs
  - GNN encoder fenc(·) -> Encodes augmented graphs into representations
  - Projection head fpro(·) -> Maps representations for contrastive learning
  - Contrastive learning framework -> Trains encoder and projection head

- Critical path:
  1. Train the GNN explainer Ψ(·) using a small labeled dataset Tℓ
  2. Use the explainer to generate augmented graphs by preserving semantic subgraphs and perturbing marginal subgraphs
  3. Train the GNN encoder fenc(·) and projection head fpro(·) using the augmented graphs in a contrastive learning framework

- Design tradeoffs:
  - Balancing the size of the labeled dataset Tℓ for training the explainer: Too small may lead to poor generalization, while too large may reduce the label efficiency of the approach
  - Choosing the perturbation method for the marginal subgraph: Different methods (e.g., node dropping, edge dropping) may have different impacts on the augmented graph's quality

- Failure signatures:
  - Poor performance on downstream tasks: Indicates that the explainer may not be accurately identifying semantic subgraphs or that the augmentation method is not effectively preserving semantics
  - Overfitting to the labeled dataset Tℓ: Suggests that the explainer is not generalizing well to the unlabeled dataset Tu

- First 3 experiments:
  1. Evaluate the performance of EPA with different sizes of the labeled dataset Tℓ to find the optimal balance between label efficiency and explainer quality
  2. Compare the performance of different perturbation methods for the marginal subgraph to determine which method leads to the best augmented graphs
  3. Analyze the impact of the explainer's accuracy on the overall performance of EPA to understand the importance of accurate semantic subgraph identification

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the following questions arise from the research:

## Limitations
- Theoretical claims about EPA's superiority over semantics-agnostic methods in the analytical example need further validation on real-world datasets with varying label ratios
- The choice of PGExplainer as the explainer may influence results, and alternative explainers could yield different performance characteristics
- Evaluation focuses on specific datasets and tasks, which may not generalize to all graph learning scenarios

## Confidence
- High confidence: The core mechanism of preserving semantic subgraphs while perturbing marginal subgraphs is well-supported by experimental results across multiple datasets
- Medium confidence: The theoretical analysis showing EPA's advantage over semantics-agnostic methods in the analytical example, as it may not fully capture real-world complexities
- Medium confidence: The label efficiency claims, as the evaluation focuses on a limited number of labeled graphs (M=50) which may not represent all practical scenarios

## Next Checks
1. Test EPA's performance across different label ratios (M=10, 25, 100) to verify the claimed label efficiency benefits hold across a broader range of settings
2. Evaluate EPA with alternative explainers (e.g., GNNExplainer, GraphMask) to assess the robustness of results to explainer choice
3. Conduct ablation studies to quantify the individual contributions of semantics-preservation versus perturbation in the augmentation process