---
ver: rpa2
title: 'Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large
  Language Models'
arxiv_id: '2405.05417'
source_url: https://arxiv.org/abs/2405.05417
tags:
- tokens
- under-trained
- token
- tokenizer
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive analysis of under-trained tokens
  in large language models (LLMs), introducing automated methods to detect problematic
  tokens that can cause unexpected model behavior. The approach combines tokenizer
  analysis, model weight-based indicators, and prompting techniques to identify tokens
  present in the tokenizer vocabulary but rarely or never seen during training.
---

# Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models

## Quick Facts
- arXiv ID: 2405.05417
- Source URL: https://arxiv.org/abs/2405.05417
- Reference count: 19
- Primary result: Automated methods detect 0.1-1% of vocabulary tokens severely under-trained across diverse LLM families

## Executive Summary
This paper introduces automated techniques for identifying under-trained tokens in large language models that can cause unexpected behavior despite being present in the tokenizer vocabulary. The approach combines tokenizer analysis, model weight-based indicators, and prompting techniques to systematically detect tokens rarely or never encountered during training. Testing across multiple model families including GPT-2, Llama, Mistral, and Gemma reveals consistent patterns of under-trained tokens comprising 0.1-1% of vocabulary, with prevalence varying significantly by architecture. The findings emphasize the critical importance of aligning tokenizer training data with model training data to improve model efficiency and safety.

## Method Summary
The methodology employs a multi-pronged approach to detect under-trained tokens by analyzing both tokenizer structure and model parameters. First, tokenizer analysis examines vocabulary composition to identify rare tokens. Second, embedding-based indicators calculate cosine distances between output embeddings and embedding norms to identify anomalous tokens. Third, prompting-based techniques probe model behavior when presented with potentially under-trained tokens. The approach systematically correlates these indicators with actual training data statistics to validate detection accuracy across diverse model families including GPT-2, Llama, Mistral, Gemma, and others, revealing that typically 0.1-1% of vocabulary consists of severely under-trained tokens.

## Key Results
- 0.1-1% of vocabulary tokens are severely under-trained across tested LLM families
- Embedding-based indicators (cosine distances, embedding norms) show strong correlation with training data exposure
- Prevalence of under-trained tokens varies significantly across different model architectures
- Under-trained tokens can cause unexpected model behavior despite being in the tokenizer vocabulary

## Why This Works (Mechanism)
The detection mechanism works because under-trained tokens create measurable statistical anomalies in model parameters and behavior. When tokens are rarely or never seen during training, their corresponding embedding vectors fail to properly converge during optimization, resulting in anomalous cosine distances from other embeddings and unusual embedding norms. These statistical irregularities manifest in both the model's internal representations and its generation behavior, creating detectable patterns that correlate with actual training data exposure. The multi-indicator approach captures different aspects of the training deficiency, providing robust detection even when individual indicators may be noisy.

## Foundational Learning

**Tokenization**: Breaking text into discrete units for model processing - needed to understand how under-trained tokens enter the vocabulary; quick check: verify tokenizer splits text consistently across models.

**Embedding Space Geometry**: Mathematical relationships between token vectors in high-dimensional space - needed to interpret cosine distances and norms as indicators; quick check: confirm embedding dimensions match across comparable models.

**Training Data Statistics**: Frequency and distribution of token occurrences during model training - needed to establish ground truth for under-trained tokens; quick check: validate token count thresholds (e.g., fewer than 5 occurrences).

**Cosine Similarity**: Measure of vector orientation in embedding space - needed to quantify relationships between token embeddings; quick check: verify cosine calculations produce values between -1 and 1.

**Model Architecture**: Structure of transformer-based language models - needed to understand where embeddings are learned and how they influence generation; quick check: confirm model uses standard decoder architecture.

## Architecture Onboarding

**Component Map**: Tokenizer -> Embedding Layer -> Transformer Blocks -> Output Layer -> Generation

**Critical Path**: Tokenizer vocabulary → Embedding initialization → Training exposure → Embedding convergence → Generation behavior

**Design Tradeoffs**: Comprehensive tokenizer vocabularies increase coverage but risk including under-trained tokens; simpler tokenizers reduce coverage but improve training efficiency

**Failure Signatures**: Anomalous embedding norms, high cosine distances from other embeddings, unexpected generation outputs when prompted with specific tokens

**First 3 Experiments**: 1) Analyze tokenizer vocabulary composition across model families, 2) Calculate embedding-based indicators for all tokens, 3) Validate indicators against training data statistics

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Correlation between indicators and training data may not generalize to non-standard tokenization strategies
- Focus on transformer architectures limits applicability to other model families
- Arbitrary threshold of fewer than 5 occurrences may miss problematic tokens
- Prompting-based indicators could be influenced by factors beyond training data exposure

## Confidence
- **High Confidence**: 0.1-1% of vocabulary tokens are severely under-trained across tested models
- **Medium Confidence**: Predictive power of embedding-based indicators for identifying problematic tokens
- **Medium Confidence**: Recommendation for aligning tokenizer and model training data

## Next Checks
1. Test detection methods across additional tokenizer architectures including SentencePiece, WordPiece, and alternative BPE variants
2. Validate correlation between indicator scores and downstream task performance degradation across multiple benchmark datasets
3. Conduct ablation studies removing identified under-trained tokens to quantify specific impact on model behavior and safety