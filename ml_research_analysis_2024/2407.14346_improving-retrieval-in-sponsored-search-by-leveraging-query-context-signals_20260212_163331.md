---
ver: rpa2
title: Improving Retrieval in Sponsored Search by Leveraging Query Context Signals
arxiv_id: '2407.14346'
source_url: https://arxiv.org/abs/2407.14346
tags:
- context
- unity
- query
- search
- augmented
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of retrieving relevant bid keywords
  for short, ambiguous queries in sponsored search. Existing dense and generative
  retrieval models often fail to capture nuanced user intent in these cases due to
  limited capacity to encode complex world knowledge.
---

# Improving Retrieval in Sponsored Search by Leveraging Query Context Signals

## Quick Facts
- arXiv ID: 2407.14346
- Source URL: https://arxiv.org/abs/2407.14346
- Authors: Akash Kumar Mohankumar; Gururaj K; Gagan Madan; Amit Singh
- Reference count: 5
- Primary result: 19.9% improvement in exact match Precision@100 for context-aware keyword retrieval in sponsored search

## Executive Summary
This paper addresses the challenge of retrieving relevant bid keywords for short, ambiguous queries in sponsored search. Existing dense and generative retrieval models often fail to capture nuanced user intent in these cases due to limited capacity to encode complex world knowledge. To overcome this, the authors propose Augmented Unity, a context-aware retrieval framework that enhances query understanding by integrating rich contextual signals derived from web search results and large language models (GPT-4). The framework demonstrates significant improvements in both offline precision metrics and online revenue generation.

## Method Summary
The Augmented Unity framework enhances query understanding by integrating contextual signals from web search results and GPT-4-generated Query Profiles through a Fusion-in-Decoder (FiD) based Unity architecture. The model uses a shared encoder-decoder (4+4 layers, 512 hidden size) trained with contrastive and NLL losses, incorporating a curriculum learning strategy called context glancing to improve robustness when context signals are unavailable. Training uses ~60M unique queries and 900M query-keyword pairs, with extensive offline and online evaluation demonstrating substantial performance improvements.

## Key Results
- 19.9% improvement in exact match Precision@100 compared to context-free models
- 1% and 1.4% increase in ad revenue for English and non-English queries respectively in online A/B testing
- Context glancing curriculum learning improves model robustness to missing context signals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Fusion-in-Decoder (FiD) architecture enables efficient processing of multiple query context signals while maintaining serving costs comparable to context-free models.
- Mechanism: FiD splits the context into separate chunks, encodes them independently, and then decodes with all context representations. This avoids the quadratic complexity of concatenating all contexts before encoding.
- Core assumption: Independent encoding of contexts followed by fusion during decoding preserves the complementary information from different context types.
- Evidence anchors:
  - [abstract]: "These signals are efficiently integrated through a Fusion-in-Decoder based Unity architecture, enabling both dense and generative retrieval with serving costs on par with traditional context-free models."
  - [section 2.2]: "This efficiency stems from our use of the Fusion-in-Decoder architecture, where inference complexity scales as O(N Lmax^2), in contrast to O(N^2 Lmax^2) when concatenating and encoding all N contexts together."

### Mechanism 2
- Claim: Context glancing through curriculum learning improves model robustness to missing context signals during inference.
- Mechanism: The model is initially trained with full context, then progressively exposed to scenarios with increasing amounts of missing context, learning to perform well even when some or all context signals are unavailable.
- Core assumption: Gradually increasing the difficulty of context availability during training helps the model learn to rely less on context and maintain performance when context is missing.
- Evidence anchors:
  - [abstract]: "To address scenarios where context is unavailable in the cache, we introduce context glancing, a curriculum learning strategy that improves model robustness and performance even without contextual signals during inference."
  - [section 2.3]: "Our approach follows the principles of curriculum learning by gradually introducing increasingly difficult scenarios. The model initially learns in a context-rich environment, becoming progressively accustomed to handling cases with partial or even complete absence of context signals."

### Mechanism 3
- Claim: Combining web search results and LLM-generated Query Profiles provides complementary information that enhances query understanding.
- Mechanism: Web search titles/snippets ground queries in real-world information while GPT-4 generates query rewrites and explanations that clarify user intent, creating a richer representation than either source alone.
- Core assumption: Web search results and LLM-generated profiles capture different aspects of query context that are both necessary for disambiguating short, ambiguous queries.
- Evidence anchors:
  - [abstract]: "Specifically, we use web search titles and snippets to ground queries in real-world information and utilize GPT-4 to generate query rewrites and explanations that clarify user intent."
  - [section 2.1]: "Table 4 shows the impact of using these different context types on the performance of Augmented Unity DR... Utilizing both web results and Query Profile context outperforms using either source alone."

## Foundational Learning

- Concept: Curriculum learning
  - Why needed here: To train the model to handle both context-rich and context-poor scenarios, improving robustness when context signals are unavailable.
  - Quick check question: How does gradually increasing the difficulty of training examples help a model generalize better to unseen scenarios?

- Concept: Fusion-in-Decoder architecture
  - Why needed here: To efficiently process multiple context signals without the computational overhead of encoding concatenated contexts.
  - Quick check question: What is the computational complexity difference between FiD and standard encoder-decoder architectures when handling multiple context passages?

- Concept: Dense retrieval vs generative retrieval
  - Why needed here: The Unity framework combines both approaches to leverage their complementary strengths in retrieving relevant bid keywords.
  - Quick check question: What are the key differences in how dense retrieval and generative retrieval models approach the keyword retrieval task?

## Architecture Onboarding

- Component map: Query → Context cache lookup → Offline pipeline (if cache miss) → Augmented Unity model → Dense/generative retrieval outputs

- Critical path: Query → Context cache lookup → Offline pipeline (if cache miss) → Augmented Unity model → Dense/generative retrieval outputs

- Design tradeoffs:
  - FiD vs concatenated encoding: FiD trades some cross-context interaction capability for significantly better computational efficiency
  - Shared vs separate models: Using a single shared model for both dense and generative retrieval reduces serving costs but may limit specialized optimization for each task
  - Context caching vs on-the-fly generation: Caching improves latency but requires periodic updates to maintain freshness

- Failure signatures:
  - High cache miss rate: Indicates the cache isn't comprehensive enough or query normalization needs improvement
  - Performance degradation with context: Suggests context signals are noisy or the model isn't effectively fusing information
  - Model performance drops when context is missing: Indicates insufficient context glancing during training

- First 3 experiments:
  1. Test cache hit rate with various query normalization strategies to maximize cache effectiveness
  2. Evaluate model performance with different numbers of context instances per type to find optimal context quantity
  3. Measure performance impact of context glancing by training with and without this curriculum strategy under varying context availability scenarios

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications arise from the work that warrant further investigation regarding long-term performance with stale context, performance on underrepresented languages, and optimal context quantity across query types.

## Limitations

- Evaluation relies on proprietary metrics and production-scale data that cannot be independently verified
- Context glancing curriculum parameters are underspecified, making exact reproduction difficult
- Does not address potential biases in web search results or LLM-generated profiles that could systematically affect keyword retrieval quality

## Confidence

- **High confidence**: The FiD architecture enables efficient context integration with reduced computational complexity (O(N Lmax^2) vs O(N^2 Lmax^2)).
- **Medium confidence**: Context glancing through curriculum learning improves robustness to missing context, though the optimal curriculum schedule is unclear.
- **Medium confidence**: The combination of web search results and LLM-generated profiles provides complementary information, but the relative importance of each source isn't quantified.

## Next Checks

1. **Ablation study on context sources**: Train models using only web search results, only LLM-generated profiles, and various combinations to quantify the marginal benefit of each context type and identify potential redundancy or conflicts.

2. **Context glancing sensitivity analysis**: Systematically vary the context dropping rates and schedules during curriculum learning to determine optimal parameters and identify overfitting to specific context availability patterns.

3. **Cross-lingual robustness evaluation**: Test model performance across different language families to assess whether the context integration benefits observed in English extend to low-resource languages with different web content availability.