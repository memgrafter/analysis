---
ver: rpa2
title: Contact-aware Human Motion Generation from Textual Descriptions
arxiv_id: '2403.15709'
source_url: https://arxiv.org/abs/2403.15709
tags:
- motion
- contact
- human
- text
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task of generating 3D interactive
  human motion from text, focusing on physically plausible human-object contacts.
  The authors construct a new dataset called RICH-CAT, derived from the RICH dataset,
  featuring high-quality motion data, accurate human-object contact labels, and detailed
  textual descriptions.
---

# Contact-aware Human Motion Generation from Textual Descriptions

## Quick Facts
- arXiv ID: 2403.15709
- Source URL: https://arxiv.org/abs/2403.15709
- Authors: Sihan Ma; Qiong Cao; Jing Zhang; Dacheng Tao
- Reference count: 40
- Primary result: Introduces CATMO, a method for generating 3D interactive human motion from text with focus on physically plausible human-object contacts, achieving FID of 0.766 and R-Precision of 0.529 on RICH-CAT dataset.

## Executive Summary
This paper addresses the novel task of generating 3D interactive human motion from textual descriptions while ensuring physically plausible human-object contacts. The authors introduce RICH-CAT, a new dataset derived from RICH, featuring high-quality motion data with accurate contact labels and detailed textual descriptions. They propose CATMO, a method that uses two VQ-VAEs to encode motion and contact modalities into distinct latent spaces and an intertwined GPT for generating motion and contacts in a mutually conditioned manner. The approach also includes a pre-trained interaction-aware text encoder to better discriminate among various contact types.

## Method Summary
The CATMO method consists of two VQ-VAE models for encoding motion and contact sequences into discrete latent spaces, an intertwined GPT for generating motion and contact tokens from text embeddings, and an interaction-aware text encoder for extracting discriminative text embeddings. The intertwined GPT interleaves motion and contact tokens along the temporal dimension, allowing the model to learn cross-modal dependencies. The text encoder is pretrained on the RICH-CAT dataset using contrastive loss to align text and motion embeddings. During generation, text embeddings are combined with quantized motion and contact tokens to autoregressively predict new motion and contact sequences, which are then decoded into 3D poses.

## Key Results
- CATMO achieves FID of 0.766 and R-Precision of 0.529 on the RICH-CAT test set, outperforming existing text-to-motion methods.
- The method produces stable, contact-aware motion sequences with physically plausible human-object interactions.
- Ablation studies demonstrate the effectiveness of separate VQ-VAEs, intertwined GPT architecture, and interaction-aware text encoder.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The intertwined GPT architecture enables mutually conditioned prediction of motion and contact tokens, allowing the model to generate contact-aware motion sequences that reflect realistic human-object interactions.
- **Mechanism**: The intertwined GPT interleaves motion and contact token indices along the temporal dimension (X = [s1, sc1, s2, sc2, ..., si-1, sc(i-1)]). This design forces the model to learn the cross-modal dependencies between motion and contact within each frame and the temporal continuity across frames. By predicting si or sci based on both past motion and contact indices, the model captures the inherent correlations between how a body part moves and how it contacts objects.
- **Core assumption**: Motion and contact sequences have strong temporal correlations within each frame and across frames, and these correlations can be effectively learned through interleaved token prediction.
- **Evidence anchors**:
  - [abstract]: "an intertwined GPT for generating human motions and contacts in a mutually conditioned manner"
  - [section]: "Instead of treating contact tokens as additional input features, we capitalize on the temporal correlations between motion and contact sequences by inputting intertwined motion and contact sequences"
- **Break condition**: If the temporal correlations between motion and contact are weak or non-existent for certain actions, the intertwined architecture may not provide significant benefits over separate prediction.

### Mechanism 2
- **Claim**: The separate VQ-VAE models for motion and contact modalities capture intricate representations of each modality and provide full expressiveness, which facilitates more stable and temporally consistent generative outcomes.
- **Mechanism**: By using two independent VQ-VAE models, the approach encodes motion and contact sequences into distinct yet complementary latent spaces. This separation allows each modality to be modeled with its own codebook size and architecture, capturing the inherently diverse and sophisticated patterns of motion (263 dimensions) and contact (22 dimensions) data. The quantized representations from these VQ-VAEs serve as the input for the intertwined GPT.
- **Core assumption**: Motion and contact are distinct modalities with different characteristics that benefit from separate encoding and modeling approaches.
- **Evidence anchors**:
  - [abstract]: "We employ two VQ-VAE models to encode motion and body contact sequences into distinct yet complementary latent spaces"
  - [section]: "Considering continuous motion features and binary contact labels as distinct modalities, we adopt two separate VQ-VAE models [49] to encode motion and contact modalities into distinct yet complementary latent spaces"
- **Break condition**: If the computational overhead of training two separate VQ-VAEs outweighs the benefits, or if the modalities are not as distinct as assumed, this approach may not be optimal.

### Mechanism 3
- **Claim**: The interaction-aware text encoder, pretrained on the RICH-CAT dataset, better discriminates among various contact types and allows for more precise control over synthesized motions and contacts compared to general-purpose text encoders like CLIP.
- **Mechanism**: The interaction-aware text encoder is trained alongside a movement encoder using contrastive loss to align text and motion embeddings. This pretraining process, specifically on a dataset containing detailed descriptions of interactive motions, enables the text encoder to better understand the nuances of textual descriptions involving body parts and objects in contact. The resulting text embeddings provide more discriminative input for the generative model.
- **Core assumption**: General-purpose text encoders like CLIP struggle with the specific task of understanding textual descriptions of human-object interactions, and a model pretrained on a contact-aware dataset can overcome this limitation.
- **Evidence anchors**:
  - [abstract]: "we introduce a pre-trained text encoder to learn textual embeddings that better discriminate among various contact types"
  - [section]: "While these models exhibit good generalization, they are not specifically finetuned on datasets containing descriptions of interactive motions. We observed that CLIP struggles to fully comprehend textual information with interactions"
- **Break condition**: If the interaction-aware text encoder does not generalize well to new, unseen contact types or if the improvement in text-motion alignment does not translate to better motion generation quality.

## Foundational Learning

- **Concept**: Vector Quantized Variational Autoencoders (VQ-VAEs)
  - Why needed here: VQ-VAEs are used to encode continuous motion and binary contact sequences into discrete latent spaces, which can then be modeled using autoregressive models like GPT. This approach allows for more stable training and generation of complex motion data.
  - Quick check question: How does the quantization process in VQ-VAEs help in generating discrete representations of continuous motion and contact data?

- **Concept**: Autoregressive Models (GPT)
  - Why needed here: The intertwined GPT architecture uses autoregressive modeling to predict the next motion or contact token based on the history of previously generated tokens. This allows for the generation of coherent motion sequences that maintain temporal consistency.
  - Quick check question: How does the autoregressive nature of GPT contribute to the temporal consistency of generated motion sequences?

- **Concept**: Contrastive Learning
  - Why needed here: Contrastive learning is used to align text and motion embeddings by maximizing the similarity between embeddings of corresponding text-motion pairs and minimizing the similarity between embeddings of non-corresponding pairs. This helps in improving the text-motion consistency of the generated sequences.
  - Quick check question: How does the contrastive loss function encourage the text and motion embeddings to be more discriminative and aligned?

## Architecture Onboarding

- **Component map**: RICH-CAT Dataset -> Motion VQ-VAE, Contact VQ-VAE -> Intertwined GPT <- Interaction-aware Text Encoder -> Motion VQ-VAE Decoder -> 3D Poses

- **Critical path**: Text input -> Interaction-aware Text Encoder -> Text embeddings -> Intertwined GPT (with quantized motion/contact tokens) -> Generated motion/contact tokens -> Motion VQ-VAE Decoder -> 3D poses

- **Design tradeoffs**:
  - Separate VQ-VAEs for motion and contact: Allows for modality-specific modeling but increases computational overhead
  - Intertwined GPT: Captures cross-modal dependencies but may be more complex to train than separate models
  - Interaction-aware text encoder: Improves text-motion alignment but requires additional pretraining on the RICH-CAT dataset

- **Failure signatures**:
  - Poor text-motion alignment: Generated motions do not match the textual descriptions
  - Unrealistic contacts: Generated motion sequences contain physically implausible human-object interactions
  - Mode collapse: Generated motions lack diversity and always produce similar sequences for different inputs

- **First 3 experiments**:
  1. Ablation study on VQ-VAE codebook sizes: Vary the codebook sizes of Motion VQ-VAE and Contact VQ-VAE to find the optimal configuration
  2. Ablation study on intertwined GPT prediction order: Compare the performance of predicting motion tokens first vs. contact tokens first
  3. Evaluation of text-motion alignment: Measure the R-Precision and MM-Dist metrics to assess the effectiveness of the interaction-aware text encoder and alignment loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the CATMO model scale with the size of the codebook used in the Motion and Contact VQ-VAEs?
- Basis in paper: [explicit] The paper mentions an ablation study on the codebook sizes of both VQ-VAEs, suggesting that larger codebooks for motion and smaller ones for contact yield better results.
- Why unresolved: The paper only tests a limited range of codebook sizes (256, 512, 1024) and does not explore the full potential range of codebook sizes or their impact on computational efficiency.
- What evidence would resolve it: A comprehensive study testing a wider range of codebook sizes and their impact on both model performance and computational resources.

### Open Question 2
- Question: Can the CATMO model be extended to handle dynamic objects that move alongside humans, beyond the current static objects?
- Basis in paper: [inferred] The paper mentions the limitation of the current framework being tailored for static objects and suggests this as a potential avenue for future investigation.
- Why unresolved: The paper does not provide any concrete steps or preliminary results towards extending the model to handle dynamic objects.
- What evidence would resolve it: Successful implementation and evaluation of the CATMO model on datasets containing dynamic objects, demonstrating improved performance compared to static object handling.

### Open Question 3
- Question: How does the accuracy of the automatically generated textual descriptions impact the performance of the CATMO model?
- Basis in paper: [explicit] The paper mentions the use of an automatic textual description generation technique and acknowledges potential disparities between annotations and natural language.
- Why unresolved: The paper does not provide a detailed analysis of the quality of the automatically generated descriptions or their impact on model performance.
- What evidence would resolve it: A study comparing the performance of the CATMO model using manually curated textual descriptions versus automatically generated ones, quantifying the impact of description quality on model accuracy.

## Limitations

- The RICH-CAT dataset, while novel, is derived from the existing RICH dataset, which may inherit biases or limitations from its source.
- The intertwined GPT architecture introduces significant training complexity without ablation studies comparing it to simpler alternatives.
- The superiority of the interaction-aware text encoder over general-purpose encoders like CLIP is demonstrated only through comparison to a single baseline.

## Confidence

- **High Confidence**: The technical architecture description (VQ-VAEs + GPT) is well-specified and reproducible. The quantitative results showing CATMO outperforming baseline methods on the RICH-CAT test set are clearly reported with specific metrics (FID: 0.766, R-Precision: 0.529).

- **Medium Confidence**: The claim that separate VQ-VAEs capture modality-specific patterns is reasonable but lacks ablation studies comparing to unified encoding approaches. The effectiveness of the intertwined GPT for capturing temporal correlations is theoretically justified but not empirically validated through controlled experiments.

- **Low Confidence**: The superiority of the interaction-aware text encoder over general-purpose encoders like CLIP is demonstrated only through comparison to a single baseline, without exploring other specialized text encoders or considering the possibility that the improvement stems from pretraining on the RICH-CAT dataset rather than architectural innovations.

## Next Checks

1. **Ablation study on VQ-VAE codebook sizes**: Vary the codebook sizes of Motion VQ-VAE and Contact VQ-VAE (e.g., 128, 256, 512) to empirically determine the optimal configuration and quantify the contribution of codebook size to generation quality.

2. **Comparison with unified encoding approaches**: Implement a unified VQ-VAE that encodes both motion and contact modalities simultaneously and compare its performance against the separate VQ-VAE approach to validate the claim that modality-specific encoding is beneficial.

3. **Text encoder comparison study**: Evaluate the interaction-aware text encoder against other specialized text encoders (e.g., T5 fine-tuned on HOI datasets, CLIP variants trained on motion-related data) to determine whether the observed improvements stem from the architectural design or simply from pretraining on relevant data.