---
ver: rpa2
title: Pointwise Mutual Information as a Performance Gauge for Retrieval-Augmented
  Generation
arxiv_id: '2411.07773'
source_url: https://arxiv.org/abs/2411.07773
tags:
- accuracy
- document
- question
- answer
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how the order of retrieved documents affects
  the performance of retrieval-augmented generation (RAG) models in question-answering
  tasks. The authors propose using pointwise mutual information (PMI) between a question
  and a context as a proxy for model performance, without requiring prior knowledge
  of the correct answer.
---

# Pointwise Mutual Information as a Performance Gauge for Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2411.07773
- Source URL: https://arxiv.org/abs/2411.07773
- Reference count: 40
- Primary result: PMI between questions and contexts correlates with RAG performance, enabling prompt optimization without ground truth answers

## Executive Summary
This paper investigates how the order of retrieved documents affects retrieval-augmented generation (RAG) model performance in question-answering tasks. The authors propose using pointwise mutual information (PMI) between a question and context as a proxy for model performance, without requiring prior knowledge of the correct answer. Through experiments on NQ-Open and ELI5 datasets using various large language models, they demonstrate a strong correlation between PMI and answer accuracy at both corpus and instance levels. Building on this insight, they develop two methods for optimizing prompts: one that searches for document permutations maximizing PMI, and another that leverages the U-shaped relationship between gold document position and performance. Both methods effectively improve answer accuracy across base and instruction-tuned models, with the second approach achieving even greater gains.

## Method Summary
The authors propose using PMI as a gauge for RAG performance and develop two optimization methods. First, they calculate PMI between questions and contexts by measuring how much more likely a question is given a specific context compared to its prior likelihood. This requires access to language model probabilities. They then develop two prompt optimization approaches: (1) a search method that finds document permutations maximizing PMI, and (2) a curvature-based method that leverages the observed U-shaped relationship between gold document position, PMI, and accuracy. These methods are evaluated on NQ-Open and ELI5 datasets using multiple open-source LLMs including Llama-2, Llama-3, Llama-3.1, Mistral-v0.3, and MPT.

## Key Results
- PMI between questions and contexts shows strong correlation with answer accuracy at both corpus and instance levels across multiple models and datasets
- Answer accuracy peaks when the gold document is positioned at the beginning or end of the context, following a U-shaped pattern
- Both PMI-based optimization methods improve answer accuracy, with the curvature-based approach achieving even greater gains than simple PMI maximization
- The methods work across both base and instruction-tuned models, demonstrating general applicability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PMI between a question and context correlates with answer accuracy in RAG.
- Mechanism: PMI measures how much more likely a question is given a specific context compared to its prior likelihood. Higher PMI indicates the context contains information relevant to the question, leading to better answer generation.
- Core assumption: The model's conditional probability of generating the correct answer given the question and context can be approximated by a function of PMI and the prior probability of the question.
- Evidence anchors: "we find evidence for an empirical correlation between answer accuracy and pointwise mutual information"
- Break condition: If the model's performance is not calibrated (i.e., the conditional probability doesn't match the empirical accuracy), the correlation would break down.

### Mechanism 2
- Claim: The position of the gold document in the context affects both PMI and answer accuracy in a U-shaped pattern.
- Mechanism: When the gold document (containing the correct answer) is placed at the beginning or end of the context, both PMI and answer accuracy peak. When placed in the middle, both drop. This suggests the model processes information differently based on document position.
- Core assumption: The model's attention and processing of context information is influenced by the position of relevant documents within the context.
- Evidence anchors: "answer accuracy peaks when the gold document is positioned at the beginning or end of the context"
- Break condition: If the model's architecture or training objective fundamentally changes how it processes sequential information, the U-shaped pattern might not hold.

### Mechanism 3
- Claim: PMI can serve as a proxy for answer accuracy without knowing the ground truth answer.
- Mechanism: Since PMI correlates with answer accuracy, we can use it to select or construct prompts that are likely to yield better answers, even without knowing what the correct answer is.
- Core assumption: The correlation between PMI and answer accuracy is consistent enough across different questions and contexts to be used as a reliable proxy.
- Evidence anchors: "Importantly, this gauge does not depend on knowing the answer to the question a priori"
- Break condition: If the correlation between PMI and accuracy varies significantly across different types of questions or contexts, PMI might not be a reliable proxy in all cases.

## Foundational Learning

- Concept: Pointwise Mutual Information (PMI)
  - Why needed here: PMI is the core metric used to gauge the relationship between questions and contexts in RAG systems.
  - Quick check question: What does a high PMI value between a question and context indicate about their relationship?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Understanding RAG is essential to grasp how retrieved documents are used to improve language model responses.
  - Quick check question: In RAG, what are the three main components of a prompt?

- Concept: Conditional probability and log odds
  - Why needed here: The paper's hypothesis relates PMI to the log odds of the correct answer, requiring understanding of these probability concepts.
  - Quick check question: How does the log odds of an event relate to its probability?

## Architecture Onboarding

- Component map: Retriever -> Context constructor -> Language model -> PMI calculator -> Performance evaluator
- Critical path:
  1. Question and context are input to the system
  2. PMI is calculated between the question and context
  3. The language model generates an answer based on the question and context
  4. The answer is evaluated for quality
  5. The correlation between PMI and answer quality is analyzed
- Design tradeoffs:
  - PMI calculation requires access to language model probabilities, limiting use with closed models
  - The methods proposed are mildly time-dependent but avoid full decoding
  - The effectiveness may vary between base and instruction-tuned models
- Failure signatures:
  - If PMI and answer accuracy don't correlate for a specific dataset or model
  - If the U-shaped pattern between gold document position, PMI, and accuracy doesn't hold
  - If the proposed methods don't improve performance on new datasets
- First 3 experiments:
  1. Replicate the corpus-level correlation between PMI and answer accuracy on NQ-Open and ELI5
  2. Test the U-shaped relationship between gold document position, PMI, and accuracy on a subset of NQ-Open
  3. Implement and evaluate the two proposed methods (search by PMI and search by curvature) on NQ-Open and ELI5

## Open Questions the Paper Calls Out
- None explicitly called out in the paper

## Limitations
- PMI calculation requires access to model log probabilities, limiting applicability to open-weight models only
- The correlation between PMI and accuracy may not generalize to domains with different question-answering patterns or document structures
- Multiple forward passes for document reordering may be prohibitive for production systems with strict latency requirements

## Confidence
- High confidence: The corpus-level correlation between PMI and answer accuracy across multiple models and datasets is well-established with statistical significance
- Medium confidence: The U-shaped relationship between gold document position, PMI, and accuracy is observed but may be influenced by specific dataset characteristics and model architectures
- Medium confidence: The proposed optimization methods show consistent improvements, though the magnitude of gains varies across model types and datasets

## Next Checks
1. Cross-domain generalization test: Apply PMI-based optimization to a non-Wikipedia QA dataset (e.g., biomedical or legal) to assess whether the correlation holds in specialized domains with different vocabulary and document structures
2. Closed-model approximation study: Develop and validate methods to approximate PMI without requiring log probabilities, such as using proxy metrics or transfer learning from open models, to enable practical deployment
3. Real-time performance analysis: Measure the latency impact of PMI-based document reordering in a streaming QA scenario, comparing the accuracy gains against the computational cost across different document set sizes