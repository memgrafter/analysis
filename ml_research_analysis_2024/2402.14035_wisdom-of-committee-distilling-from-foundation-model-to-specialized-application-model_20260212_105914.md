---
ver: rpa2
title: 'Wisdom of Committee: Distilling from Foundation Model to Specialized Application
  Model'
arxiv_id: '2402.14035'
source_url: https://arxiv.org/abs/2402.14035
tags:
- teacher
- distillation
- student
- teachers
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of distilling knowledge from
  large foundation models to smaller specialized models, where significant differences
  in architecture, capacity, training data, and input features exist. To overcome
  this, the authors propose a method called DiverseDistill that introduces a "teaching
  committee" comprising both foundation model teachers and complementary teachers.
---

# Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model

## Quick Facts
- arXiv ID: 2402.14035
- Source URL: https://arxiv.org/abs/2402.14035
- Reference count: 40
- This paper proposes DiverseDistill to improve knowledge distillation from heterogeneous foundation model teachers to specialized student models.

## Executive Summary
This paper addresses the challenge of distilling knowledge from large foundation models to smaller specialized models when significant differences exist in architecture, capacity, training data, and input features. The authors propose DiverseDistill, which introduces a "teaching committee" comprising both foundation model teachers and complementary teachers. The method enables the student to understand each teacher's expertise and extract task knowledge through an interactive distillation process using learned Question and Answer Augmenters. Experiments show DiverseDistill consistently outperforms baseline distillation methods across both recommendation and vision tasks.

## Method Summary
DiverseDistill introduces a "teaching committee" approach where a student model learns from multiple heterogeneous teachers through an interactive distillation process. The method uses a Question Augmenter with Teacher Embedding Dot Product to generate tailored questions for each teacher based on their expertise, and an Answer Augmenter that transforms diverse teacher hidden states into comparable answers through separate MLPs. Task regularization ensures alignment between teacher questions and student answers with the target task. The approach is evaluated on MovieLens for recommendation and CIFAR-10/100, ImageNet for vision tasks, showing consistent improvements over baseline distillation methods.

## Key Results
- DiverseDistill consistently outperforms baseline distillation methods including standard multi-teacher approaches
- The method shows significant performance improvements across both recommendation (MovieLens) and vision (CIFAR-10, CIFAR-100, ImageNet) tasks
- The teaching committee approach with interactive question-answering enables effective knowledge transfer despite architectural and capacity differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DiverseDistill improves student performance by allowing tailored question generation for each teacher based on learned teacher embeddings.
- Mechanism: The student uses a Question Augmenter with Teacher Embedding Dot Product to model each teacher's expertise and generate questions conditioned on that expertise. This allows the student to ask more effective questions that align with each teacher's strengths.
- Core assumption: Teachers have distinct areas of expertise that can be modeled and leveraged through question conditioning.
- Evidence anchors:
  - [abstract] "allows the student to understand the expertise of each teacher and ask tailored questions to each teacher"
  - [section] "DIVERSE DISTILL introduce Teacher Embedding Dot Product in the Question Augmenter to generate tailored questions based on each teacher's expertise"
  - [corpus] Weak evidence - no direct corpus support for question conditioning mechanism
- Break condition: If teachers don't have meaningfully different expertise areas, or if question conditioning doesn't improve teacher response quality.

### Mechanism 2
- Claim: The Answer Augmenter transforms diverse teacher hidden states into comparable answers for the student to learn from.
- Mechanism: Each teacher's hidden states are passed through a separate MLP in the Answer Augmenter, converting them to a common dimension that the student can process as soft labels.
- Core assumption: Teacher hidden states, despite architectural differences, contain transferable information that can be normalized through MLPs.
- Evidence anchors:
  - [abstract] "The Answer Augmenter enables each teacher to reply with a task-oriented answer, which will be used as the soft label in distillation loss"
  - [section] "Each hidden state will be passed through a separate MLP to match back with the student model dimension"
  - [corpus] Weak evidence - no direct corpus support for answer normalization approach
- Break condition: If teacher hidden states are too dissimilar to be normalized effectively, or if normalization destroys task-relevant information.

### Mechanism 3
- Claim: Task regularization aligns teacher questions and student answers with the target task, making distillation more effective.
- Mechanism: Additional loss terms ensure that questions are aligned enough for teachers to make correct predictions, and answers are digestible enough for students to make correct predictions.
- Core assumption: Forcing intermediate representations to produce task predictions improves their alignment with the task.
- Evidence anchors:
  - [abstract] "Finally, DIVERSE DISTILL consistently outperforms baseline distillation methods, regardless of the teacher choices"
  - [section] "We introduce task loss on the predictions based on the questions and answers to align different models' hidden states and regulate the distillation process to be task-oriented"
  - [corpus] Weak evidence - no direct corpus support for task regularization mechanism
- Break condition: If task regularization conflicts with the primary distillation objective or if intermediate representations can't be aligned without losing task-specific information.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The entire approach builds on standard knowledge distillation principles but extends them to handle diverse teachers
  - Quick check question: What's the difference between feature distillation and logit distillation?

- Concept: Multi-Teacher Distillation
  - Why needed here: The approach specifically addresses limitations of existing multi-teacher methods when teachers have different architectures
  - Quick check question: How do standard multi-teacher methods typically combine teacher outputs?

- Concept: Teacher-Student Capacity Gap
  - Why needed here: The paper addresses challenges arising from significant capacity differences between foundation model teachers and specialized student models
  - Quick check question: Why does a large teacher-student capacity gap typically hurt distillation performance?

## Architecture Onboarding

- Component map: Student hidden states → Question Augmenter → Teacher models → Answer Augmenter → Student learning

- Critical path: Student hidden states → Question Augmenter → Teacher models → Answer Augmenter → Student learning

- Design tradeoffs:
  - More teachers = better coverage but higher computation cost
  - More sophisticated question generation = better alignment but more parameters
  - Task regularization = better alignment but potential conflict with primary objective

- Failure signatures:
  - Student performance degrades when adding complementary teachers
  - Importance scores become uniform (no teacher specialization)
  - Task regularization causes training instability

- First 3 experiments:
  1. Single teacher baseline (foundation model only) to establish performance floor
  2. Add complementary teacher without DiverseDistill to measure benefit of committee
  3. Apply DiverseDistill with both teachers to measure improvement from interaction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DiverseDistill scale with the number of teachers in the committee, particularly when the number of teachers exceeds the number of student parameters?
- Basis in paper: [explicit] The paper mentions that the distillation cost increases with more teachers, but suggests the weighting mechanism might allow selecting only one teacher per data point to mitigate cost.
- Why unresolved: The paper doesn't provide experiments varying the number of teachers or analyzing performance scaling with teacher count.
- What evidence would resolve it: Experiments showing student performance as a function of teacher count, including cases where teacher count >> student parameters, would clarify scaling behavior.

### Open Question 2
- Question: How does DiverseDistill perform when teachers have conflicting expertise or contradictory predictions on the same data point?
- Basis in paper: [inferred] The paper discusses accommodating diverse teachers but doesn't address what happens when teachers disagree or have conflicting expertise.
- Why unresolved: The paper doesn't include experiments where teachers are deliberately chosen to have conflicting predictions or expertise areas.
- What evidence would resolve it: Experiments with deliberately conflicting teachers (e.g., different models trained on contradictory data distributions) would show how DiverseDistill handles disagreement.

### Open Question 3
- Question: What is the impact of teacher selection criteria on DiverseDistill performance, and how does random teacher selection compare to importance-weighted selection?
- Basis in paper: [explicit] The paper mentions using importance scores to select teachers and suggests this could reduce computation cost, but doesn't compare random vs. importance-weighted selection.
- Why unresolved: The paper doesn't provide ablation studies comparing random teacher selection against importance-weighted selection during training.
- What evidence would resolve it: Ablation experiments comparing student performance when using random teacher selection vs. importance-weighted selection would clarify the value of the importance scoring mechanism.

## Limitations

- The mechanism for question conditioning on teacher expertise lacks sufficient empirical validation to confirm it captures meaningful teacher specialization
- Task regularization mechanism needs more ablation studies to demonstrate its independent contribution to performance gains
- The method introduces significant computational overhead through multiple MLPs and question generation, but cost-benefit analysis is not provided

## Confidence

**High Confidence:** The core problem statement about knowledge distillation across heterogeneous teacher-student pairs is well-grounded in existing literature, and the overall framework architecture (Question Augmenter + Answer Augmenter) is logically coherent and technically implementable.

**Medium Confidence:** The performance improvements reported on MovieLens and CIFAR-100 datasets are likely valid, though the lack of detailed hyperparameter tuning and variance reporting across multiple runs limits generalizability. The selective teacher selection mechanism shows promise but lacks sufficient ablation to confirm its necessity.

**Low Confidence:** The theoretical claims about how Teacher Embedding Dot Product captures teacher expertise and how task regularization improves alignment are not sufficiently validated through targeted experiments or analysis. The mechanism by which question augmentation improves teacher responses is particularly under-supported.

## Next Checks

1. **Teacher Expertise Validation:** Conduct controlled experiments where teachers with deliberately engineered expertise differences (e.g., trained on disjoint subsets of data) are used, then analyze whether the Question Augmenter's importance scores actually reflect these known differences.

2. **Ablation of Task Regularization:** Perform systematic ablation studies removing task regularization while keeping other components constant, measuring both performance impact and training stability across multiple random seeds to establish statistical significance.

3. **Computational Overhead Analysis:** Measure wall-clock training time and memory usage for DiverseDistill compared to baseline methods across different teacher counts, and calculate the performance improvement per unit of additional computation to assess practical utility.