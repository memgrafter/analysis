---
ver: rpa2
title: 'ChronoFact: Timeline-based Temporal Fact Verification'
arxiv_id: '2410.14964'
source_url: https://arxiv.org/abs/2410.14964
tags:
- claim
- events
- evidence
- event
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ChronoFact, a timeline-based framework for
  temporal fact verification that addresses the challenge of verifying claims involving
  multiple, overlapping, or recurring events with complex temporal relationships.
  The core method extracts events from claims and evidence, constructs chronological
  timelines, and uses a multi-level attention encoder to assess event relevance at
  token, event, and time levels, followed by classifiers for individual claim events,
  chronological order, and overall claim veracity.
---

# ChronoFact: Timeline-based Temporal Fact Verification

## Quick Facts
- arXiv ID: 2410.14964
- Source URL: https://arxiv.org/abs/2410.14964
- Authors: Anab Maulana Barik; Wynne Hsu; Mong Li Lee
- Reference count: 10
- Outperforms state-of-the-art on temporal fact verification tasks with macro F1 85.49 on ChronoClaims

## Executive Summary
ChronoFact introduces a timeline-based framework for temporal fact verification that addresses the challenge of verifying claims involving multiple, overlapping, or recurring events with complex temporal relationships. The method extracts events from claims and evidence, constructs chronological timelines, and uses a multi-level attention encoder to assess event relevance at token, event, and time levels. A new dataset, ChronoClaims, was created specifically for this task, and experiments show ChronoFact significantly outperforms state-of-the-art baselines across multiple datasets.

## Method Summary
ChronoFact extracts events from claims and evidence using GENRE and Semantic Role Labelling, then constructs chronological timelines. The framework employs a multi-level attention encoder that computes cosine similarities at token, event, and time levels to assess evidence relevance. Three classifiers operate in parallel: claim event classifier for individual event veracity, chronological order classifier for timeline consistency, and claim classifier for overall veracity. The model is trained with cross-entropy loss and soft logic loss to enforce consistency between event labels, chronological order, and overall claim veracity.

## Key Results
- Achieves 85.49% macro F1 on ChronoClaims, 56.29% on T-FEVER, 61.58% on T-FEVEROUS, and 65.67% on T-QuanTemp
- Demonstrates superior handling of complex temporal event types including overlapping and recurring events
- Outperforms state-of-the-art baselines across all evaluated datasets

## Why This Works (Mechanism)

### Mechanism 1
The multi-level attention encoder improves event relevance assessment by integrating token-level, event-level, and time-level signals. The model computes cosine similarities at three distinct granularities and averages them to form a unified attention score for each claim-evidence event pair. This approach assumes that combining complementary similarity measures yields more robust relevance scores than using any single level alone.

### Mechanism 2
The chronological order classifier uses reordered sequences and Bi-LSTM to detect timeline inconsistencies between claim and evidence. After selecting top-k relevant evidence events, the model reorders both claim and evidence event sequences chronologically using GPT, then feeds them into separate Bi-LSTMs to capture sequential patterns before classification. This assumes the chronological sequence of evidence events can be reliably determined and used as a ground truth timeline for comparison.

### Mechanism 3
The soft logic loss enforces consistency between claim event labels, chronological order, and overall claim veracity through differentiable rules. The model computes a soft distribution using min/max operations on the event-level and order-level probabilities, then applies KL divergence to regularize the overall claim label distribution toward this consistent target. This assumes logical relationships between individual event labels and overall claim label can be approximated with t-norm operators without losing important distinctions.

## Foundational Learning

- **Temporal event extraction and timeline construction**: Needed to identify events with temporal arguments from both claims and evidence to build chronological timelines. Quick check: Can you explain the difference between explicit and implicit temporal expressions in claims?

- **Multi-level attention mechanisms and their integration**: Needed to combine token-level, event-level, and time-level attention scores for comprehensive event relevance assessment. Quick check: How would you compute the token-level attention score between two events using cosine similarity?

- **Bi-directional LSTM for sequence modeling**: Needed to capture dependencies in reordered claim and evidence event sequences for chronological order classification. Quick check: What advantage does a Bi-LSTM have over a standard LSTM when modeling chronological sequences?

## Architecture Onboarding

- **Component map**: Event Extractor → Event Encoder → Multi-level Attention Encoder → Claim Event Classifier, Chronological Order Classifier, Claim Classifier → Final Prediction
- **Critical path**: Event extraction → timeline construction → multi-level attention → claim event classification → chronological order classification → final claim classification
- **Design tradeoffs**: Using GPT for reordering adds capability but introduces latency and potential errors; averaging three attention levels is simpler than learned weighting but may miss optimal combinations; soft logic loss adds consistency but requires careful hyperparameter tuning
- **Failure signatures**: Poor performance on claims with implicit temporal information; degradation when event timelines are ambiguous or contain conflicting dates; over-reliance on chronological order when individual event evidence is weak
- **First 3 experiments**: 1) Evaluate model performance with and without the multi-level attention encoder; 2) Test different values of k (top-k evidence events) in the classifiers; 3) Compare results using only explicit vs. both explicit and implicit temporal expressions in claims

## Open Questions the Paper Calls Out
None

## Limitations

- **Dataset Reliability**: ChronoClaims was generated using templates and may not fully capture the complexity and ambiguity of real-world claims, raising questions about dataset representativeness
- **Generalization to Real-World Claims**: Performance on naturally occurring claims with varied linguistic patterns and contextual nuances remains to be validated beyond Wikipedia-derived datasets
- **Temporal Reasoning Dependencies**: Framework relies on GPT for chronological reordering, introducing potential errors and creating dependency on GPT's temporal reasoning capabilities

## Confidence

- **High Confidence**: Core architectural components (multi-level attention encoder, three-classifier structure, soft logic loss) are well-specified with substantial and consistent performance improvements
- **Medium Confidence**: Dataset construction methodology and specific implementation details of GPT-based temporal reasoning
- **Low Confidence**: Model's performance on real-world claims outside Wikipedia/Wikidata domain and robustness to linguistic variations not captured in template-based dataset

## Next Checks

1. **Independent Dataset Evaluation**: Test ChronoFact on naturally occurring claims from news articles or social media to validate generalization beyond Wikipedia-derived datasets

2. **Ablation Study on Attention Levels**: Systematically evaluate model performance using only token-level, only event-level, only time-level, and various combinations to quantify each attention level's contribution

3. **Robustness to GPT Reordering Errors**: Create controlled experiment with intentionally perturbed GPT reordering (10-20% errors) to measure sensitivity to timeline inconsistencies and quantify vulnerability to GPT errors