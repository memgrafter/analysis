---
ver: rpa2
title: 'Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language
  Model'
arxiv_id: '2404.16766'
source_url: https://arxiv.org/abs/2404.16766
tags:
- prior
- foundation
- tokens
- language
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether supervised fine-tuning (SFT) for
  cross-lingual tasks is truly necessary, finding that alignment may be more "superficial"
  than previously thought. The authors propose PRETTY, a training-free method that
  achieves SFT-like performance by prepending minimal prior tokens to inputs.
---

# Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model

## Quick Facts
- arXiv ID: 2404.16766
- Source URL: https://arxiv.org/abs/2404.16766
- Reference count: 23
- Primary result: Adding minimal prior tokens to foundation models achieves SFT-like cross-lingual performance without training

## Executive Summary
This paper challenges the necessity of supervised fine-tuning (SFT) for cross-lingual tasks by demonstrating that minimal prefix text manipulation can achieve comparable performance. The authors propose PRETTY, a training-free method that prepends one or two task-related prior tokens to foundation model inputs. Experiments across machine translation, summarization, and POS tagging tasks in eight languages show that foundation models can match SFT performance when properly prompted with prior tokens, suggesting SFT alignment may be more "superficial" than previously thought.

## Method Summary
The PRETTY method involves constructing prior tokens using three strategies: SFT Prior (tokens from fine-tuned models), Refined Prior (tokens from smaller task-specific models), and Pseudo Prior (dictionary-derived tokens for translation/summarization). These prior tokens are prepended to input text before decoding with foundation models like Llama2-7B and Mistral-7B. The approach is evaluated across multiple tasks including machine translation (Flores-101), cross-lingual summarization (CrossSum), and POS tagging (XGLUE), comparing performance against SFT baselines across eight languages.

## Key Results
- Foundation models with refined prior tokens achieved 98.8% of SFT performance in translation tasks
- Dictionary-derived pseudo-prior tokens achieved 85.4% of SFT performance
- Foundation models without any prior tokens achieved only 52.5% of SFT performance
- Across all tasks and languages, prefix token addition consistently improved foundation model performance to approach SFT levels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SFT alignment is "superficial" because it primarily narrows the model's decision space through prior token selection rather than fundamentally transforming the model's capabilities.
- **Mechanism**: SFT guides the model to select tokens that are more likely to continue the instruction text rather than completing cross-linguistic semantic transformation. The foundation model already possesses knowledge for cross-lingual tasks, but needs proper steering through initial tokens.
- **Core assumption**: The foundation model's pre-training data contains sufficient cross-lingual knowledge that can be activated through proper prompting, rather than requiring additional training.
- **Evidence anchors**:
  - [abstract]: "the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation"
  - [section 2.2]: "after adding a prior token, the foundation model exhibits a high degree of agreement with the SFT model in terms of token selection"
  - [corpus]: Weak - the paper doesn't provide direct evidence about pre-training corpus composition, though it mentions Llama2 was trained on ~90% English data
- **Break condition**: If the foundation model lacks sufficient cross-lingual knowledge in pre-training, or if the cross-lingual task requires capabilities not present in the pre-training data.

### Mechanism 2
- **Claim**: Adding minimal task-related prior tokens can bridge the performance gap between foundation and SFT models without training.
- **Mechanism**: By prepending one or two task-related tokens to the input, the foundation model's token selection behavior converges with the SFT model's behavior, achieving comparable performance through decision space modulation.
- **Core assumption**: The decision space of the foundation model can be effectively steered toward task-specific distributions through minimal prefix text manipulation.
- **Evidence anchors**:
  - [abstract]: "by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts"
  - [section 2.2]: "90.8% of the tokens chosen by the SFT model are present within the 'silent majority' in the decision space of the foundation model"
  - [corpus]: Moderate - the paper provides experimental results across 8 languages showing consistent performance improvements
- **Break condition**: If the prior tokens are completely unrelated to the target sequence, or if the task requires deeper structural understanding that cannot be captured through prefix manipulation.

### Mechanism 3
- **Claim**: The quality of prior tokens matters, but they don't need to be perfect to achieve significant performance gains.
- **Mechanism**: Prior tokens of low quality can still elicit foundation model capabilities, but tokens closer to ground truth provide better guidance and performance.
- **Core assumption**: There exists a spectrum of prior token quality where even imperfect tokens can provide meaningful steering without severe error propagation.
- **Evidence anchors**:
  - [section 5.1]: "even if the prior tokens provided by the SFT model are of low quality, the foundation model does not suffer from severe error propagation"
  - [section 5.2]: "the model could not be aligned to a better decision trajectory by these random prior tokens, whether they were function words or tokens with actual meaning"
  - [corpus]: Moderate - POS tagging experiments show that while SFT prior tokens are of inferior quality, they still enable the foundation model to outperform SFT
- **Break condition**: If prior tokens are completely unrelated to the task or target language, or if the task requires high precision in initial token selection.

## Foundational Learning

- **Concept**: Cross-lingual alignment and the role of prior tokens in language model behavior
  - **Why needed here**: Understanding how language models handle cross-lingual tasks and how initial tokens influence generation is fundamental to the PRETTY method
  - **Quick check question**: Why does adding just one or two prior tokens enable foundation models to match SFT performance in cross-lingual tasks?

- **Concept**: Decision space modulation in language models
  - **Why needed here**: The paper's core insight is that SFT primarily narrows the decision space rather than fundamentally transforming the model, which is key to understanding why prefix tokens work
  - **Quick check question**: How does adding prior tokens change the probability distribution of the foundation model's next token predictions?

- **Concept**: Catastrophic forgetting in fine-tuning
  - **Why needed here**: The paper positions PRETTY as a solution to the potential knowledge loss from SFT, so understanding this phenomenon is crucial
  - **Quick check question**: What evidence does the paper provide that SFT might harm pre-training knowledge, and how does PRETTY address this?

## Architecture Onboarding

- **Component map**: Foundation LLM → Prefix Token Generator → Decoding with modified input
- **Critical path**: Input text → Prior token selection strategy → Token concatenation → Foundation model decoding
- **Design tradeoffs**: Training-free approach vs. potential performance ceiling; simplicity vs. task-specific optimization; resource efficiency vs. quality of prior tokens
- **Failure signatures**: Model continues generating in source language; poor translation/summarization quality; inconsistent performance across language pairs
- **First 3 experiments**:
  1. Test baseline performance of foundation model on a simple translation task without any prior tokens
  2. Add SFT prior tokens to the same task and measure performance improvement
  3. Test refined prior tokens from a smaller task-specific model and compare results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the results change when evaluating with larger LLMs (e.g., 13B or 70B parameters) compared to the 7B models used in the experiments?
- Basis in paper: [inferred] The authors acknowledge that their research is limited to 7B models and suggest that future work should extend validation to a broader scope of models and parameter scales.
- Why unresolved: The current study only tests the proposed method on 7B parameter models, leaving uncertainty about whether the observed effects generalize to larger models which might have different scaling properties.
- What evidence would resolve it: Conducting the same experiments with Llama2-13B, Llama2-70B, and Mistral-7B-Instruct across the same tasks and languages would show whether the effectiveness of PRETTY scales with model size.

### Open Question 2
- Question: What is the exact impact of prior token quality on final performance when using low-quality SFT priors, and at what threshold does prior quality become detrimental?
- Basis in paper: [explicit] The authors analyze prior token quality in Section 5.1, finding that even incorrect SFT prior tokens do not cause severe error propagation, but also note that performance is still associated with prior token quality.
- Why unresolved: While the paper shows low-quality priors don't catastrophically harm performance, it doesn't establish quantitative relationships or thresholds for when prior quality becomes problematic.
- What evidence would resolve it: Systematically varying the accuracy of prior tokens (e.g., 100% accurate, 75%, 50%, 25%, 0% accurate) and measuring performance degradation would establish quality thresholds and relationships.

### Open Question 3
- Question: How does the proposed method perform on truly low-resource languages with minimal pre-training data (below 0.05% of pre-training corpus) that were not included in the experiments?
- Basis in paper: [explicit] The authors categorize languages into high-resource (>0.1%), low-resource (<0.1%), and extremely low-resource (<0.05%) based on pre-training data proportions, but only test on languages above 0.05%.
- Why unresolved: The experiments exclude languages with the most severe data scarcity, which is precisely where the proposed method might be most valuable for democratization of multilingual LLMs.
- What evidence would resolve it: Testing PRETTY on languages like Swahili, Yoruba, or indigenous languages with <0.05% pre-training representation would show whether the method remains effective at extreme resource levels.

## Limitations

- The paper does not directly analyze the pre-training corpus to verify the presence of cross-lingual knowledge that foundation models supposedly possess
- Experiments are limited to 7B parameter models, leaving uncertainty about whether results generalize to larger or smaller model scales
- The quality and composition of bilingual dictionaries used for pseudo-prior construction is not thoroughly validated, which could affect the reliability of those experiments

## Confidence

**High confidence**: The experimental results showing that prefix tokens improve foundation model performance across multiple tasks and languages are well-supported by the presented data. The methodology for constructing and evaluating prior tokens is clearly specified and reproducible.

**Medium confidence**: The claim that SFT alignment is "superficial" and primarily operates through decision space narrowing is supported by the observed convergence between foundation and SFT model token selection, but this interpretation could be challenged. Alternative explanations, such as SFT models having better internal representations that are only partially activated by prior tokens, are not ruled out.

**Low confidence**: The paper's assertion that SFT may harm pre-training knowledge is based on indirect reasoning rather than direct evidence. The claim that cross-lingual knowledge exists in pre-training data but requires proper steering through prefix tokens is plausible but not empirically validated - the paper does not analyze the pre-training corpus to confirm the presence of relevant cross-lingual patterns.

## Next Checks

1. **Cross-lingual knowledge analysis**: Analyze the pre-training corpus of Llama2 to identify evidence of cross-lingual patterns and knowledge that would support the paper's claim that foundation models possess latent cross-lingual capabilities requiring only proper prompting.

2. **Scale sensitivity study**: Replicate the PRETTY experiments across multiple model scales (e.g., 1B, 13B, 70B parameters) to determine whether the relationship between prior token effectiveness and model size follows predictable patterns or reveals scaling limitations.

3. **Ablation of pre-training data composition**: Train foundation models with varying proportions of non-English data (0%, 10%, 50%, 90%) and evaluate whether the effectiveness of prefix tokens correlates with the amount of cross-lingual pre-training data, providing direct evidence for or against the paper's core mechanism.