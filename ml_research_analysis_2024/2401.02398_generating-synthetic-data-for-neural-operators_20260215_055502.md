---
ver: rpa2
title: Generating synthetic data for neural operators
arxiv_id: '2401.02398'
source_url: https://arxiv.org/abs/2401.02398
tags:
- data
- neural
- learning
- functions
- operator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a method to generate synthetic training\
  \ data for neural operators solving PDEs without requiring numerical PDE solvers.\
  \ The approach draws random functions from the solution space (e.g., H10(\u03A9\
  )) and computes the corresponding right-hand sides via differentiation, producing\
  \ exact solution pairs."
---

# Generating synthetic data for neural operators

## Quick Facts
- arXiv ID: 2401.02398
- Source URL: https://arxiv.org/abs/2401.02398
- Authors: Erisa Hasani; Rachel A. Ward
- Reference count: 37
- Key outcome: Method generates synthetic training data for neural operators by sampling from solution space and computing right-hand sides via differentiation, eliminating need for numerical PDE solvers

## Executive Summary
This paper introduces a method to generate synthetic training data for neural operators solving PDEs without requiring numerical PDE solvers. The approach draws random functions from the solution space (e.g., H1_0(Ω)) and computes the corresponding right-hand sides via differentiation, producing exact solution pairs. Experiments with the Fourier Neural Operator on elliptic PDEs show that models trained on this synthetic data generalize well to both standard and out-of-distribution test functions. Relative L2 errors for Poisson equations with Dirichlet and Neumann boundary conditions improve from ~0.05 to ~0.001 as training data increases from 1k to 100k samples.

## Method Summary
The method generates synthetic training data by sampling random functions from the solution space of a PDE (e.g., H¹₀(Ω) for Dirichlet problems) represented as truncated linear combinations of Laplacian eigenfunctions. For each sampled function u_j, the corresponding right-hand side f_j is computed directly by applying the differential operator to u_j. This produces exact solution pairs (f_j, u_j) without numerical errors from solving PDEs. The Fourier Neural Operator is then trained on these synthetic pairs to learn the solution operator mapping from right-hand side to solution.

## Key Results
- Relative L2 errors for Poisson equations improve from ~0.05 to ~0.001 as training data increases from 1k to 100k samples
- Models trained on synthetic data generalize well to both standard and out-of-distribution test functions
- Similar performance observed for linear and semi-linear elliptic PDEs
- Eliminates computational overhead and numerical errors associated with classical PDE solvers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating synthetic data by drawing random functions from the solution space and computing their corresponding right-hand sides via differentiation produces exact solution pairs for training neural operators.
- Mechanism: Instead of numerically solving the PDE for each new right-hand side (forward approach), we start with a valid solution from the solution space (e.g., H¹₀(Ω)), compute the corresponding right-hand side through differentiation, and pair them as training data.
- Core assumption: The solution space for the PDE is known (e.g., H¹₀(Ω) for Dirichlet problems) and has an explicit orthonormal basis of eigenfunctions and eigenvalues.
- Evidence anchors:
  - [abstract] "by randomly sampling candidate solutions uj from the appropriate solution space (e.g., H₀¹(Ω)), we compute the corresponding right-hand side fj directly from the equation by differentiation"
  - [section] "we draw a large number N of independent and identically distributed 'random functions' uj from the underlying solution space (e.g., H¹₀(Ω))...We then plug each such random candidate solution into the equation and get a corresponding right-hand side function fj"

### Mechanism 2
- Claim: The synthetic training data generalizes well to test functions generated by standard numerical solvers, even for out-of-distribution cases.
- Mechanism: The random linear combinations of eigenfunctions (with appropriate scaling by eigenvalues) create a diverse set of functions that span the solution space, enabling the neural operator to learn the underlying mapping f → u rather than memorizing specific function forms.
- Core assumption: Functions generated as random linear combinations of eigenfunctions are representative of the full solution space.
- Evidence anchors:
  - [abstract] "Experiments with the Fourier Neural Operator on elliptic PDEs show that models trained on this synthetic data generalize well to both standard and out-of-distribution test functions"
  - [section] "we generate a large number of synthetic training functions {uk_aj}j,k in the space as random linear combinations of the first M eigenfunctions, scaled by the corresponding eigenvalues"

### Mechanism 3
- Claim: This synthetic data generation approach eliminates the computational overhead and potential numerical errors associated with repeatedly solving PDEs using classical numerical solvers.
- Mechanism: By replacing the expensive PDE solving step with simple differentiation operations, we can generate large amounts of exact training data quickly and efficiently.
- Core assumption: Computing derivatives of the randomly generated solutions is computationally simpler than solving the PDE numerically.
- Evidence anchors:
  - [abstract] "This produces training pairs (fj, uj) by computing derivatives rather than solving a PDE numerically for each data point, enabling fast, large-scale data generation consisting of exact solutions"
  - [section] "This 'backwards' approach to generating training data only requires derivative computations, in contrast to standard 'forward' approaches, which require a numerical PDE solver"

## Foundational Learning

- Concept: Sobolev spaces and their properties
  - Why needed here: The method relies on knowing the solution space (e.g., H¹₀(Ω)) and its properties to generate representative functions
  - Quick check question: What is the difference between H¹₀(Ω) and H¹(Ω), and when would each be the appropriate solution space?

- Concept: Eigenfunction expansion and basis representation
  - Why needed here: The synthetic data is generated as random linear combinations of eigenfunctions of the Laplacian with appropriate boundary conditions
  - Quick check question: How do the eigenfunctions and eigenvalues of the Laplacian with Dirichlet and Neumann boundary conditions differ, and why is this important for the synthetic data generation?

- Concept: Neural operator architecture and training
  - Why needed here: Understanding how neural operators learn mappings between function spaces is crucial for implementing this approach
  - Quick check question: What is the key difference between neural operators and traditional neural networks in terms of their input/output spaces, and how does this enable them to learn solution operators for PDEs?

## Architecture Onboarding

- Component map: Synthetic data generator -> Neural operator (FNO) -> Training loop -> Evaluation module

- Critical path: 1. Generate synthetic training data (functions from solution space + corresponding right-hand sides) 2. Train neural operator on synthetic data 3. Evaluate on test data from numerical solvers 4. Iterate with larger M (number of eigenfunctions) or more training samples if needed

- Design tradeoffs:
  - Larger M (more eigenfunctions) → better coverage of solution space but higher computational cost for differentiation
  - More training samples → better generalization but longer training time
  - Simpler PDEs (e.g., Poisson) → easier to generate synthetic data but less general application
  - More complex PDEs (e.g., semi-linear) → wider applicability but harder to generate synthetic data

- Failure signatures:
  - Poor generalization to numerical solver data → synthetic data may not adequately represent the solution space
  - Training loss not decreasing → neural operator architecture or hyperparameters may need adjustment
  - High variance in results across experiments → synthetic data generation process may be unstable

- First 3 experiments:
  1. Implement synthetic data generation for Poisson equation with Dirichlet boundary conditions using M=5 eigenfunctions, train FNO, and evaluate on test data
  2. Vary M from 1 to 20 and analyze the effect on training and testing performance
  3. Test on out-of-distribution functions (e.g., smooth polynomials, non-differentiable functions) to assess generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the synthetic data generation method scale to higher-dimensional PDEs beyond 2D square domains?
- Basis in paper: Explicit - The authors note that eigenvalues and eigenfunctions of the Laplacian have been studied for non-square and higher-dimensional domains, suggesting the approach can be extended, but they only provide experiments for 2D square domains.
- Why unresolved: The paper focuses exclusively on 2D experiments and does not investigate the computational complexity or performance of the method in higher dimensions.
- What evidence would resolve it: Numerical experiments demonstrating the method's effectiveness and computational efficiency for 3D or higher-dimensional PDEs.

### Open Question 2
- Question: What is the impact of the choice of basis functions on the quality of the synthetic data and the resulting neural operator's performance?
- Basis in paper: Inferred - The authors use eigenfunctions of the Laplacian as basis functions but do not explore alternative bases or analyze their impact on performance.
- Why unresolved: The paper does not compare different basis functions or investigate how the choice affects generalization to out-of-distribution test functions.
- What evidence would resolve it: Systematic comparison of different basis functions (e.g., polynomials, wavelets) and their impact on training efficiency and test accuracy.

### Open Question 3
- Question: How does the synthetic data generation method perform for PDEs with more complex nonlinearities or non-local operators?
- Basis in paper: Inferred - The authors demonstrate the method for linear and semi-linear elliptic PDEs but do not test more complex operators like fractional Laplacians or fully nonlinear equations.
- Why unresolved: The paper's experiments are limited to second-order elliptic PDEs, leaving open the question of applicability to more general operators.
- What evidence would resolve it: Numerical experiments applying the method to PDEs with fractional Laplacians, integro-differential operators, or fully nonlinear equations.

## Limitations
- Method depends on explicit knowledge of solution space structure including eigenfunction bases and eigenvalues
- Assumes random linear combinations of eigenfunctions adequately represent the full solution space
- Limited to PDEs where spectral properties are known and computable

## Confidence
- High confidence in the mathematical mechanism of generating exact solution pairs through differentiation
- Medium confidence in generalization claims, as performance on out-of-distribution functions needs more extensive validation
- Medium confidence in computational efficiency claims, pending empirical comparison with numerical solver costs for various problem scales

## Next Checks
1. **Spectral Coverage Analysis**: Systematically evaluate how the number of eigenfunction modes (M) affects solution space coverage and generalization performance across different PDE types.

2. **Distribution Mismatch Testing**: Generate test functions from alternative distributions (e.g., polynomials, non-smooth functions) and measure degradation in performance to quantify generalization limits.

3. **Computational Scaling Study**: Compare wall-clock time and memory usage between synthetic data generation and classical numerical solver approaches across varying problem dimensions and complexity levels.