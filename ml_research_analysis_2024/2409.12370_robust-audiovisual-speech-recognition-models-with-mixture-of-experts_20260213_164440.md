---
ver: rpa2
title: Robust Audiovisual Speech Recognition Models with Mixture-of-Experts
arxiv_id: '2409.12370'
source_url: https://arxiv.org/abs/2409.12370
tags:
- visual
- speech
- recognition
- audiovisual
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes EVA, a robust audiovisual speech recognition
  model leveraging a mixture-of-experts (MoE) architecture to integrate visual information
  into a pretrained speech recognition backbone. EVA employs CLIP to encode unconstrained
  full-frame video features and uses a MoE module to effectively inject visual information
  while preserving speech recognition performance.
---

# Robust Audiovisual Speech Recognition Models with Mixture-of-Experts

## Quick Facts
- arXiv ID: 2409.12370
- Source URL: https://arxiv.org/abs/2409.12370
- Authors: Yihan Wu; Yifan Peng; Yichen Lu; Xuankai Chang; Ruihua Song; Shinji Watanabe
- Reference count: 0
- Primary result: EVA achieves state-of-the-art audiovisual ASR results with up to ~4% absolute WER reduction on three datasets

## Executive Summary
This paper proposes EVA, an audiovisual speech recognition model that integrates visual information into a pretrained speech recognition backbone using a mixture-of-experts (MoE) architecture. The model leverages CLIP to encode unconstrained full-frame video features and uses MoE to effectively inject visual information while preserving speech recognition performance. EVA demonstrates strong generalization across diverse video domains, achieving state-of-the-art results on How2, VisSpeech, and Ego4D datasets with only ~400x less training data than comparable methods.

## Method Summary
EVA builds upon the pretrained OWSM v3.1 speech recognition model and incorporates visual information through a multimodal MoE module. The architecture uses CLIP as a visual encoder to extract features from full-frame video, which are then projected into the speech embedding space and processed by the MoE layer. The MoE module contains 8 experts with top-4 activation, allowing selective integration of visual information while maintaining the base ASR capabilities. The model is fine-tuned on the How2 dataset for 10 epochs with specific loss objectives and achieves state-of-the-art performance across three audiovisual speech recognition benchmarks.

## Key Results
- Achieves state-of-the-art WER on How2, VisSpeech, and Ego4D datasets
- Demonstrates up to ~4% absolute WER reduction compared to previous approaches
- Shows strong generalization across diverse video domains with limited training data
- Effective integration of visual information without degrading speech recognition performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining on large-scale speech data provides generalization for diverse acoustic conditions.
- Mechanism: OWSM v3.1 is trained on 180k hours of public speech data, giving it robust acoustic understanding that transfers to unseen audiovisual domains.
- Core assumption: Pretrained ASR models can handle domain shifts if the pretraining data is sufficiently diverse.
- Evidence anchors:
  - [abstract] "We build EVA upon the pretrained ASR model, OWSM v3.1 [5], which is trained on large-scale public speech datasets."
  - [section] "OWSM v3.1 adopts an E-Branchformer [25] encoder which utilizes parallel branches to capture local and global contextual information."
  - [corpus] Weak correlation: ASR pretraining focus is not directly covered in neighbor corpus.
- Break condition: Pretraining data doesn't cover acoustic variations present in target domain, or domain shift is too large.

### Mechanism 2
- Claim: CLIP visual features provide strong zero-shot generalization across unconstrained video frames.
- Mechanism: CLIP is trained on large-scale image-text pairs with contrastive loss, giving it ability to encode diverse visual scenes without domain-specific fine-tuning.
- Core assumption: Visual representations learned from natural images transfer well to video frames from different domains.
- Evidence anchors:
  - [abstract] "We encode visual information into visual tokens sequence and map them into speech space by a lightweight projection."
  - [section] "CLIP is trained on image and text paired data based on contrastive loss, and known to have strong generalization and zero-shot capabilities."
  - [corpus] Weak correlation: Visual generalization focus is not directly covered in neighbor corpus.
- Break condition: Visual content in target domain is too different from natural images CLIP was trained on.

### Mechanism 3
- Claim: Mixture-of-Experts allows selective integration of visual information while preserving speech recognition performance.
- Mechanism: MoE module uses router to activate only top-k experts for each token, allowing specialized processing of visual vs speech tokens without disrupting ASR backbone.
- Core assumption: Expert specialization can occur through training on multimodal data while maintaining base ASR capabilities.
- Evidence anchors:
  - [abstract] "to incorporate visual information effectively, we inject visual information into the ASR model through a mixture-of-experts module."
  - [section] "To maintain the pretrain ASR model's speech understanding performance while incorporating visual understanding performance, proper initialization of MoE layers is important."
  - [corpus] Weak correlation: MoE in multimodal learning is not directly covered in neighbor corpus.
- Break condition: Router cannot effectively distinguish when to use visual experts, leading to degraded speech performance.

## Foundational Learning

- Concept: E-Branchformer architecture
  - Why needed here: Provides robust speech recognition backbone with parallel branches for local/global context
  - Quick check question: What architectural feature of E-Branchformer enables it to capture both local and global contextual information?

- Concept: Mixture-of-Experts routing
  - Why needed here: Enables sparse activation of visual experts only when relevant, preserving speech model performance
  - Quick check question: How does the top-k selection in MoE routing prevent interference with speech recognition capabilities?

- Concept: Visual token projection
  - Why needed here: Maps CLIP visual features into the same embedding space as speech tokens for fusion
  - Quick check question: Why is it necessary to project visual tokens into the speech token embedding space before fusion?

## Architecture Onboarding

- Component map:
  Audio → E-Branchformer → MoE → Decoder → Text
  Visual → CLIP → Projection → MoE → Decoder → Text

- Critical path: Audio → E-Branchformer → MoE → Decoder → Text
  Visual → CLIP → Projection → MoE → Decoder → Text

- Design tradeoffs:
  - Using pretrained OWSM vs training from scratch: better generalization vs model size
  - CLIP vs domain-specific visual encoder: better zero-shot capability vs task-specific optimization
  - Top-4 experts vs all experts: computational efficiency vs potential information loss

- Failure signatures:
  - Visual information not being utilized: WER doesn't improve on visual datasets
  - Speech performance degradation: WER increases compared to speech-only model
  - MoE not activating properly: Loss balancing shows one expert dominating

- First 3 experiments:
  1. Ablation study: Remove MoE layer, compare WER on all datasets
  2. Ablation study: Remove visual features, compare WER on all datasets
  3. Ablation study: Remove fine-tuning stage, evaluate on pretrained model only

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implicit questions arise from the work:

### Open Question 1
- Question: How does the EVA model perform when integrating visual features from domains outside of instructional and egocentric videos (e.g., entertainment, news, or surveillance footage)?
- Basis in paper: [inferred] The paper demonstrates EVA's performance on instructional videos (How2, VisSpeech) and egocentric videos (Ego4D), but does not explore other domains.
- Why unresolved: The paper's experiments are limited to three specific datasets, leaving the generalization capability to other video domains untested.
- What evidence would resolve it: Testing EVA on a diverse set of video domains (e.g., entertainment, news, surveillance) and comparing its performance to state-of-the-art models in those domains.

### Open Question 2
- Question: What is the impact of using different visual encoders (e.g., CLIP variants or other vision models) on EVA's performance and generalization ability?
- Basis in paper: [explicit] The paper uses CLIP as the visual encoder but does not explore the impact of using different visual encoders.
- Why unresolved: The choice of visual encoder could significantly affect the quality of visual features and, consequently, EVA's performance. The paper does not provide a comparative analysis of different visual encoders.
- What evidence would resolve it: Conducting experiments with different visual encoders (e.g., CLIP variants, other vision models) and analyzing their impact on EVA's performance and generalization ability across various datasets.

### Open Question 3
- Question: How does the number of experts in the MoE module affect EVA's performance and efficiency, and what is the optimal configuration for different video domains?
- Basis in paper: [inferred] The paper uses 8 experts with top-4 active for each token but does not explore the impact of varying the number of experts or the active expert count.
- Why unresolved: The optimal number of experts and active expert count may vary depending on the complexity of the video domain and the available computational resources. The paper does not provide a sensitivity analysis of these hyperparameters.
- What evidence would resolve it: Conducting experiments with different numbers of experts and active expert counts, and analyzing their impact on EVA's performance, efficiency, and generalization ability across various datasets.

## Limitations
- Limited out-of-domain evaluation with only 51 clips in Ego4D validation set
- Reliance on CLIP visual features without exploring alternative visual encoders
- Unspecified critical details about MoE implementation (router temperature, expert capacity factors)

## Confidence
**High confidence**: The core mechanism of using MoE to selectively incorporate visual information while preserving speech recognition performance is well-supported by ablation studies showing WER improvements when visual features are added versus removed.

**Medium confidence**: The claim of achieving state-of-the-art results is credible given the quantitative improvements, but the comparison is limited to a narrow set of audiovisual ASR methods with similar training scales. The generalization across diverse domains is supported but based on limited out-of-domain evaluation data.

**Low confidence**: The claim about CLIP's specific contribution to zero-shot generalization is not independently validated. The exact impact of the MoE initialization strategy and routing mechanism is also underspecified.

## Next Checks
1. **Expert utilization analysis**: Conduct detailed analysis of MoE expert activation patterns across different datasets and acoustic/visual conditions to verify that the top-k selection is effectively distinguishing relevant visual information from speech-only contexts.

2. **Visual encoder ablation study**: Replace CLIP with a domain-specific visual encoder trained on How2 video frames and compare performance to isolate CLIP's contribution to the reported generalization capabilities.

3. **Scale-up validation**: Evaluate EVA's performance on larger audiovisual datasets (beyond the 300-hour How2 subset) to test whether the MoE architecture maintains its effectiveness and whether the "state-of-the-art" advantage persists at scale.