---
ver: rpa2
title: JieHua Paintings Style Feature Extracting Model using Stable Diffusion with
  ControlNet
arxiv_id: '2408.11744'
source_url: https://arxiv.org/abs/2408.11744
tags:
- style
- diffusion
- image
- transfer
- stable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a novel approach to extract stylistic features
  of Jiehua paintings using a Fine-tuned Stable Diffusion Model with ControlNet (FSDMC).
  The training data was manually constructed in the format of (Original Image, Canny
  Edge Features, Text Prompt) using open-source Jiehua artist's work collected from
  the Internet.
---

# JieHua Paintings Style Feature Extracting Model using Stable Diffusion with ControlNet

## Quick Facts
- arXiv ID: 2408.11744
- Source URL: https://arxiv.org/abs/2408.11744
- Reference count: 0
- FSDMC achieved FID score of 3.27 vs CycleGAN's 56 on Jiehua style extraction

## Executive Summary
This study proposes a novel approach to extract stylistic features of Jiehua paintings using a Fine-tuned Stable Diffusion Model with ControlNet (FSDMC). The training data was manually constructed in the format of (Original Image, Canny Edge Features, Text Prompt) using open-source Jiehua artist's work collected from the Internet. FSDMC was evaluated against CycleGAN, another mainstream style transfer model. FSDMC achieved a FID score of 3.27 on the dataset, significantly outperforming CycleGAN's score of 56. Additionally, FSDMC surpassed CycleGAN in expert evaluation.

## Method Summary
The approach fine-tunes Stable Diffusion v1-5 with ControlNet using manually constructed training data in the format (Original Image, Canny Edge Features, Text Prompt). The dataset consists of 112 images (44 Jiehua and 68 other traditional Chinese paintings) at 512x512 resolution. Training uses specific hyperparameters including learning rate of 5e-6 with cosine scheduler and gradient accumulation. The model is evaluated using FID scores and expert evaluation, compared against a CycleGAN baseline trained on the same dataset.

## Key Results
- FSDMC achieved FID score of 3.27 on Jiehua style extraction task
- CycleGAN baseline scored 56 FID, showing substantial performance gap
- FSDMC outperformed CycleGAN in expert evaluation assessments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ControlNet adds precise spatial control to Stable Diffusion without losing pre-trained semantic knowledge
- Mechanism: ControlNet locks the original diffusion model and inserts trainable zero-convolution layers that modify output based on spatial conditions like Canny edges
- Core assumption: The locked base model can still contribute meaningful semantic information even when a small, trainable control branch is added
- Evidence anchors: ControlNet architecture described as integrating spatial conditioning controls; loss function shows zero-convolution layer integration
- Break condition: If the locked base model becomes too rigid, the control branch cannot adapt to specific Jiehua style nuances

### Mechanism 2
- Claim: Cycle-consistency loss in CycleGAN ensures bijective mapping between domains, preventing mode collapse
- Mechanism: CycleGAN uses two generators and two discriminators, training them in tandem so that translating an image from domain X to Y and back to X minimizes difference with original
- Core assumption: Paired cycle-consistency loss is sufficient to preserve content structure while transferring style
- Evidence anchors: CycleGAN architecture description; cycle-consistency loss formula provided
- Break condition: If unpaired training data lacks sufficient style overlap, cycle-consistency loss cannot enforce meaningful style transfer

### Mechanism 3
- Claim: Using Canny edge maps as ControlNet conditioning preserves Jiehua composition structure while allowing color/texture style transfer
- Mechanism: Edge map acts as structural skeleton that guides diffusion process to generate images matching original layout but with Jiehua stylistic elements
- Core assumption: Edge maps capture essential spatial layout of Jiehua paintings in way that can be reinterpreted by fine-tuned model
- Evidence anchors: Training data construction using Canny edge features; ControlNet demonstrated effectiveness with various inputs
- Break condition: If edge extraction loses important stylistic strokes, model cannot recover these details in output

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: Both CycleGAN and Stable Diffusion rely on adversarial training; understanding GAN loss functions and mode collapse is essential for debugging style transfer failures
  - Quick check question: What happens to the generator loss if the discriminator becomes too strong during training?

- Concept: Diffusion Models
  - Why needed here: Stable Diffusion with ControlNet is a diffusion model; understanding denoising steps, latent space, and classifier-free guidance is key to tuning hyperparameters
  - Quick check question: In a diffusion model, what is the role of the noise schedule, and how does it affect sample quality?

- Concept: Cycle Consistency
  - Why needed here: CycleGAN's core innovation is cycle consistency; knowing how to formulate and balance this loss is crucial for unpaired style transfer
  - Quick check question: Why might a model trained without cycle-consistency loss produce unrealistic outputs when translating between domains?

## Architecture Onboarding

- Component map: (Original Image → Canny Edge Extraction → Text Prompt) triplets → FSDMC pipeline (Stable Diffusion + ControlNet + text encoder + diffusion sampler) OR CycleGAN pipeline (Generator G + Generator F + Discriminator DX + Discriminator DY + cycle-consistency loss)
- Critical path:
  - For FSDMC: Edge conditioning → ControlNet modification → Stable Diffusion decoding → image output
  - For CycleGAN: Forward translation → backward reconstruction → adversarial discrimination → loss update
- Design tradeoffs:
  - FSDMC: Higher fidelity (FID 3.27) but needs more compute and careful hyperparameter tuning; edge conditioning may lose fine brushwork
  - CycleGAN: Simpler architecture but struggles with Jiehua style due to unpaired data and lack of spatial conditioning; achieves FID 56
- Failure signatures:
  - FSDMC: Output images retain general layout but miss Jiehua-specific texture/color; low FID but poor expert scores
  - CycleGAN: Generated images look like generic style transfer, losing Jiehua structure; high FID, inconsistent expert scores
- First 3 experiments:
  1. Train FSDMC on small subset of Jiehua data with default hyperparameters; check edge-conditioned outputs for structural fidelity
  2. Swap Canny edges for segmentation maps as conditioning; compare FID and visual quality
  3. Retrain CycleGAN with paired Jiehua data; measure if cycle-consistency improves style transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FSDMC vary when trained on larger datasets of Jiehua paintings, and what is the minimum dataset size required to achieve comparable results to the 112-image dataset used in this study?
- Basis in paper: The paper uses a dataset of 112 images, including 44 Jiehua paintings, and does not explore the impact of dataset size on model performance
- Why unresolved: The study does not provide a systematic analysis of how dataset size affects the model's ability to extract Jiehua style features
- What evidence would resolve it: Conducting experiments with varying dataset sizes and comparing the FID scores and expert evaluations would clarify the minimum dataset size required for effective style transfer

### Open Question 2
- Question: What specific aspects of Jiehua style are most challenging for CycleGAN to capture, and how do these challenges compare to other artistic styles?
- Basis in paper: The paper notes that CycleGAN performs poorly on Jiehua style transfer compared to FSDMC, but does not analyze the specific stylistic features that are difficult for CycleGAN to capture
- Why unresolved: The study does not provide a detailed comparison of the stylistic elements that CycleGAN struggles with in Jiehua versus other styles
- What evidence would resolve it: A detailed analysis of the stylistic features (e.g., brushwork, color palette, composition) that are challenging for CycleGAN in Jiehua, compared to other styles, would provide insights into the model's limitations

### Open Question 3
- Question: How does the choice of control conditions (e.g., Canny edges, human poses, segmentation maps) impact the performance of FSDMC in extracting Jiehua style features?
- Basis in paper: The paper uses Canny edges as control conditions for FSDMC, but does not explore the impact of other control conditions on the model's performance
- Why unresolved: The study does not compare the effectiveness of different control conditions in enhancing the model's ability to extract Jiehua style features
- What evidence would resolve it: Experimenting with various control conditions and evaluating their impact on FID scores and expert evaluations would determine the most effective control conditions for Jiehua style transfer

## Limitations
- Small dataset size of 112 images may limit generalization and raise concerns about overfitting
- Manual construction of training data introduces potential human bias in edge extraction and prompt formulation
- Evaluation relies heavily on FID scores and expert evaluation without specifying the number of experts or their qualifications

## Confidence
- High confidence: The quantitative FID score comparison between FSDMC (3.27) and CycleGAN (56) is reliable given the clear metric definition
- Medium confidence: The expert evaluation results, while positive, lack sufficient methodological detail to fully assess validity
- Medium confidence: The claim that ControlNet preserves pre-trained semantic information while enabling style transfer is supported by the architecture but needs more ablation studies

## Next Checks
1. Conduct a larger-scale experiment with 500+ Jiehua paintings to verify if the FID score improvement persists with more training data
2. Perform an ablation study removing ControlNet from the architecture while keeping other components constant to isolate its contribution to the performance gain
3. Implement a double-blind expert evaluation with 10+ qualified art historians to validate the subjective quality assessments and reduce potential bias in the original evaluation