---
ver: rpa2
title: '"Why" Has the Least Side Effect on Model Editing'
arxiv_id: '2409.18679'
source_url: https://arxiv.org/abs/2409.18679
tags:
- editing
- knowledge
- side
- different
- effects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines the side effects of model editing in large
  language models (LLMs), focusing on how different question types affect performance
  degradation. Using MEMIT for model editing, the authors test GPT-2 (1.5B) and LLaMA-2
  (7B) models with questions categorized into eight types: who, what, when, where,
  which, why, how, and others.'
---

# "Why" Has the Least Side Effect on Model Editing

## Quick Facts
- arXiv ID: 2409.18679
- Source URL: https://arxiv.org/abs/2409.18679
- Authors: Tsung-Hsuan Pan; Chung-Chi Chen; Hen-Hsen Huang; Hsin-Hsi Chen
- Reference count: 2
- This study examines how different question types affect performance degradation during model editing in LLMs.

## Executive Summary
This study examines the side effects of model editing in large language models (LLMs), focusing on how different question types affect performance degradation. Using MEMIT for model editing, the authors test GPT-2 (1.5B) and LLaMA-2 (7B) models with questions categorized into eight types: who, what, when, where, which, why, how, and others. Results show that after several edits, models experience performance drops, with "why" questions causing the least degradation. Increasing batch size mitigates side effects by delaying performance drops. However, findings from smaller models (GPT-2) do not directly extrapolate to larger models (LLaMA-2), suggesting independent analysis is necessary.

## Method Summary
The study uses MEMIT for model editing on GPT-2-XL (1.5B) and LLaMA-2 (7B) models. Questions from the RealTimeQA dataset are categorized into eight types and edited iteratively while varying batch sizes. Performance is evaluated using ARC-easy, ARC-challenge, and OpenBookQA datasets, measuring average accuracy as the metric for general ability degradation.

## Key Results
- Performance drops to around 50% after 5 edits regardless of question type
- "Why" questions cause the least performance degradation among all question types
- Increasing batch size can mitigate performance drops by delaying degradation
- Findings from GPT-2 do not directly extrapolate to LLaMA-2, indicating model-size-dependent effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Editing "why" questions causes less performance degradation because answers are full sentences rather than named entities.
- Mechanism: The model's parameters encode different patterns for processing different question types. "Why" questions require generating coherent explanatory sentences, which may engage different parameter subsets or regularization mechanisms that preserve general ability.
- Core assumption: The parameter changes required for "why" question editing are more distributed or less disruptive to the model's core reasoning pathways.
- Evidence anchors:
  - [section] "We hypothesize that this is because LLMs are trained for continuous writing, and answers to 'Why' questions are full sentences, whereas answers to other questions mainly involve editing named entities."
  - [abstract] "Our findings reveal that the extent of performance degradation varies significantly across different question types"

### Mechanism 2
- Claim: Larger batch sizes mitigate performance degradation by allowing more distributed parameter updates.
- Mechanism: When multiple knowledge edits are applied simultaneously, the optimization process can find a parameter configuration that satisfies all constraints more effectively, reducing the localized disruption that occurs with sequential single edits.
- Core assumption: The MEMIT algorithm can effectively balance competing updates when they are presented together.
- Evidence anchors:
  - [abstract] "We examine the impact of batch size on side effects, discovering that increasing the batch size can mitigate performance drops."
  - [section] "Our results suggest that enlarging the batch size, i.e., editing several pieces of knowledge at the same time, can mitigate the side effects of the performance drop."

### Mechanism 3
- Claim: Findings from smaller models (GPT-2) do not extrapolate to larger models (LLaMA-2) due to fundamental architectural differences in how parameter changes propagate.
- Mechanism: Larger models have more complex parameter interdependencies and different optimization landscapes, causing the same editing operation to have different ripple effects across the parameter space.
- Core assumption: The structural differences between GPT-2 and LLaMA-2 create fundamentally different vulnerability patterns to knowledge editing.
- Evidence anchors:
  - [abstract] "Our results indicate discrepancies in findings between models of different sizes, suggesting that insights from smaller models may not necessarily apply to larger models."
  - [section] "Our results indicate that the findings differ between models of different sizes, suggesting that insights from smaller models may not necessarily apply to larger models."

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why model editing causes performance degradation requires knowing how neural networks forget previously learned information when new knowledge is incorporated.
  - Quick check question: What happens to a neural network's performance on previously learned tasks when it is fine-tuned on new data without regularization?

- Concept: Transformer architecture and parameter interdependencies
  - Why needed here: The paper's findings about different question types affecting performance differently likely relate to how different parameter subsets are involved in processing different types of information.
  - Quick check question: How do the self-attention mechanisms in transformers create parameter dependencies that could cause ripple effects when editing specific knowledge?

- Concept: Optimization landscapes and local minima
  - Why needed here: The batch size findings suggest that the optimization process behaves differently depending on how many constraints are presented simultaneously, which relates to the shape of the loss landscape.
  - Quick check question: How does presenting multiple optimization objectives simultaneously affect the ability to find a parameter configuration that satisfies all constraints?

## Architecture Onboarding

- Component map: RealTimeQA dataset -> MEMIT algorithm -> GPT-2/LLaMA-2 models -> ARC-easy/ARC-challenge/OpenBookQA evaluation datasets
- Critical path: Knowledge editing → Parameter modification → General ability evaluation → Performance analysis across question types and batch sizes
- Design tradeoffs: The choice between editing questions individually (smaller batch size) versus in groups (larger batch size) involves balancing computational efficiency against the risk of performance degradation
- Failure signatures: Performance drops to around 50% after 5 edits regardless of question type; "which" and "what" questions show more severe degradation than other types; GPT-2 shows different failure patterns than LLaMA-2
- First 3 experiments:
  1. Reproduce the basic finding that performance drops after ~5 edits using MEMIT on GPT-2 with a small batch size and "what" questions.
  2. Test whether increasing batch size from 1 to 5 delays the performance drop for the same question type.
  3. Compare the degradation pattern for "why" questions versus "what" questions using the same model and batch size settings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do question types beyond the eight studied (who, what, when, where, which, why, how, and others) affect model editing side effects in LLMs?
- Basis in paper: [explicit] The authors acknowledge that their study focuses on eight specific question types and suggest future work could explore additional question types or more nuanced classifications.
- Why unresolved: The paper only examines eight question types, leaving the effects of other potential question types unexplored.
- What evidence would resolve it: Conducting experiments with a broader range of question types and analyzing their impact on model editing side effects.

### Open Question 2
- Question: What are the underlying mechanisms driving the different impacts of question types and batch sizes on model performance during model editing?
- Basis in paper: [explicit] The authors state that while they identified different impacts of question types and batch sizes, the underlying mechanisms driving these side effects remain unclear.
- Why unresolved: The study identifies correlations but does not explain the causal relationships or mechanisms behind the observed effects.
- What evidence would resolve it: Developing theoretical models or conducting experiments that isolate and test the specific factors contributing to performance degradation during model editing.

### Open Question 3
- Question: How do the findings from smaller models like GPT-2 translate to larger models like LLaMA-2 in terms of model editing side effects?
- Basis in paper: [explicit] The authors note that discrepancies were observed between GPT-2 and LLaMA-2, suggesting that insights from smaller models may not necessarily apply to larger ones.
- Why unresolved: The study shows differences between model sizes but does not fully explain the reasons for these discrepancies or how they might generalize to other model sizes.
- What evidence would resolve it: Conducting a systematic study across a range of model sizes to identify patterns and factors that influence the transferability of findings between models of different scales.

## Limitations

- The analysis is constrained to only two model architectures (GPT-2 and LLaMA-2) and a single knowledge editing method (MEMIT)
- Focus on specific question types from the RealTimeQA dataset may not capture the full diversity of knowledge editing scenarios
- Performance metrics rely on multiple-choice question answering datasets, which may not fully represent the breadth of general abilities affected by model editing

## Confidence

- Performance degradation after ~5 edits: High confidence
- "Why" questions cause least side effects: Medium confidence
- Batch size affects performance degradation: High confidence
- Findings don't extrapolate between model sizes: Medium confidence

## Next Checks

1. **Cross-architecture validation**: Test whether the "why" question advantage persists across additional model architectures (e.g., BERT, T5, OPT) and knowledge editing methods (e.g., ROME, Dynamic Editing) to determine if the finding is method-dependent or reflects a fundamental property of LLM knowledge representation.

2. **Dataset generalization**: Evaluate the question-type effects on knowledge editing performance using diverse datasets beyond RealTimeQA, including open-domain QA datasets and domain-specific knowledge bases, to assess whether the observed patterns hold across different knowledge domains and representation formats.

3. **Long-term stability analysis**: Conduct extended experiments tracking model performance degradation over longer editing sequences (20+ edits) and measuring recovery potential through fine-tuning or regularization, to better understand the cumulative effects of knowledge editing and potential mitigation strategies.