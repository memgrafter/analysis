---
ver: rpa2
title: 'AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization'
arxiv_id: '2402.11940'
source_url: https://arxiv.org/abs/2402.11940
tags:
- attack
- image
- adversarial
- captioning
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AICAttack, a black-box adversarial attack
  method for image captioning models that uses an attention-based candidate selection
  mechanism combined with a customized differential evolution algorithm. The method
  identifies critical pixels through attention scores and optimizes perturbations
  without requiring access to model parameters or gradients.
---

# AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization

## Quick Facts
- arXiv ID: 2402.11940
- Source URL: https://arxiv.org/abs/2402.11940
- Reference count: 40
- Key outcome: Black-box adversarial attack method using attention-based optimization achieves higher attack success rates than baselines with BLEU-2 score drops of 0.188 on SAT and 0.104 on BLIP models

## Executive Summary
AICAttack introduces a novel black-box adversarial attack method for image captioning models that combines attention-based candidate selection with a customized differential evolution algorithm. The approach identifies critical pixels through attention scores and optimizes perturbations without requiring access to model parameters or gradients. Experiments demonstrate superior attack success rates compared to baseline methods on COCO and Flickr8k datasets, with the method showing promising transferability across different captioning architectures.

## Method Summary
The AICAttack method operates by first computing attention scores to identify candidate pixels for perturbation, then applying a customized differential evolution algorithm to optimize the adversarial perturbations. The attack works in a black-box setting where only image-level outputs are accessible, making no assumptions about model architecture or gradients. The differential evolution optimization iteratively evolves a population of candidate solutions by combining and mutating existing perturbations based on fitness scores derived from caption quality metrics.

## Key Results
- Achieves attack success rates of 84.3% on COCO dataset against Show-Attend-Tell model
- BLEU-2 score drops of 0.188 on SAT and 0.104 on BLIP models demonstrate caption quality degradation
- Shows improved transferability with 78.2% average success rate across different captioning models

## Why This Works (Mechanism)
The method leverages attention mechanisms to focus perturbation efforts on image regions most critical for caption generation. By identifying pixels that receive high attention weights during caption generation, the attack can strategically place minimal perturbations that maximally disrupt the model's understanding. The differential evolution algorithm then optimizes these perturbations through evolutionary search, adapting to the specific characteristics of the target captioning model without requiring gradient information.

## Foundational Learning
- **Attention mechanisms in image captioning**: Used to identify which image regions influence caption generation - needed to target perturbations effectively; quick check: visualize attention maps overlaid on images
- **Differential evolution optimization**: Evolutionary algorithm for black-box optimization - needed because gradients are unavailable in black-box setting; quick check: verify convergence behavior with different population sizes
- **Image captioning metrics (BLEU, METEOR)**: Standard evaluation metrics for caption quality - needed to measure attack impact on caption generation; quick check: ensure metric calculations match established implementations
- **Black-box adversarial attacks**: Attack methodology without model access - needed for practical attack scenarios; quick check: confirm attack works without any model parameter access
- **Pixel-level perturbation optimization**: Fine-grained control over image modifications - needed for stealthy attacks; quick check: measure perturbation magnitude relative to image intensity ranges

## Architecture Onboarding

**Component Map:**
Input Image -> Attention Score Computation -> Candidate Pixel Selection -> Differential Evolution Optimization -> Perturbed Image -> Caption Generation

**Critical Path:**
The attention score computation followed by candidate pixel selection represents the critical path, as these steps determine which pixels will be optimized. The differential evolution then operates on this reduced candidate set rather than the entire image.

**Design Tradeoffs:**
The method trades computational complexity for attack effectiveness - attention-based candidate selection increases runtime but improves success rates and transferability. The differential evolution approach avoids gradient computation overhead but requires careful hyperparameter tuning and may converge slowly.

**Failure Signatures:**
The attack may fail when attention maps are uniform across the image (no clear candidate pixels) or when the differential evolution gets stuck in local optima. Poor candidate selection can lead to perturbations that are either too visible or ineffective at disrupting caption generation.

**First Experiments:**
1. Baseline attack without attention-based candidate selection to quantify attention mechanism contribution
2. Transferability test from one model architecture to another with different attention mechanisms
3. Ablation study varying differential evolution population size and mutation rates to optimize convergence

## Open Questions the Paper Calls Out
The paper acknowledges several open questions including the need for more comprehensive robustness testing against defense mechanisms, the potential for extending the approach to other vision-language tasks, and the exploration of adaptive attacks that can respond to detection-based defenses.

## Limitations
- Computational complexity is significantly higher than simpler black-box methods due to attention-based candidate selection and differential evolution optimization
- Evaluation focuses primarily on attack success rates and caption quality metrics without comprehensive human perceptual studies
- Transferability claims are based on limited model diversity, potentially overstating cross-architecture effectiveness

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Attack success rates and BLEU score impacts | High |
| Attention-based candidate selection effectiveness | Medium |
| Transferability claims | Low |

## Next Checks
1. Conduct human perceptual studies to quantify visual detectability of AICAttack perturbations across different attack strengths, establishing practical security thresholds
2. Test transferability against a broader range of captioning architectures including non-attention-based models (e.g., RNN-only approaches) and vision transformers to validate cross-architecture effectiveness
3. Evaluate robustness against state-of-the-art defense mechanisms such as randomized smoothing, adversarial training with diverse perturbation types, and detection-based defenses to establish practical attack limitations