---
ver: rpa2
title: 'Sailing in high-dimensional spaces: Low-dimensional embeddings through angle
  preservation'
arxiv_id: '2406.09876'
source_url: https://arxiv.org/abs/2406.09876
tags:
- data
- local
- which
- embeddings
- angles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MERCAT, a new method for low-dimensional embeddings
  (LDEs) that focuses on preserving angles between data points rather than distances.
  MERCAT maps high-dimensional data onto a 2D unit sphere, optimizing for angle reconstruction
  using a simple, differentiable objective function.
---

# Sailing in high-dimensional spaces: Low-dimensional embeddings through angle preservation

## Quick Facts
- arXiv ID: 2406.09876
- Source URL: https://arxiv.org/abs/2406.09876
- Reference count: 40
- Primary result: MERCAT preserves angles between data points, outperforming UMAP, tSNE, NCVis, and DENS MAP in angle preservation, distance reconstruction, and density preservation across various datasets

## Executive Summary
This paper introduces MERCAT, a novel approach to low-dimensional embeddings that prioritizes preserving angles between data points rather than distances. The method maps high-dimensional data onto a 2D unit sphere, optimizing for angle reconstruction using a differentiable objective function. MERCAT addresses limitations of existing methods that prioritize local distance preservation at the expense of global structure. Through spectral denoising, angle subsampling, and optimization on the sphere, MERCAT achieves superior performance in preserving both local and global structures across diverse datasets including synthetic clustered data and single-cell gene expression data.

## Method Summary
MERCAT is a low-dimensional embedding method that maps high-dimensional data onto the 2D unit sphere S², optimizing for angle preservation between data points. The approach involves optional PCA denoising to handle high-dimensional noise, initialization on the sphere using principal components, and gradient-based optimization with angle subsampling for computational efficiency. The method employs efficient linear-algebra computations for angles on the sphere and uses Adam optimization with learning rate scheduling. MERCAT is designed to naturally balance local and global structure preservation while enabling direct visualization through its spherical embedding.

## Key Results
- MERCAT outperforms existing methods (UMAP, tSNE, NCVis, DENS MAP) in angle preservation metrics across multiple synthetic and real-world datasets
- The method successfully preserves both local and global structures in single-cell gene expression data, including the Tabula Sapiens blood dataset
- Computational efficiency is achieved through angle subsampling, reducing cost from O(n³) to O(n) while maintaining reconstruction quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Angle preservation balances local and global structure better than distance-based approaches
- Mechanism: By optimizing for angles between all triplets of points rather than pairwise distances, MERCAT naturally captures both fine-grained local geometry and broader global relationships in a single, scale-independent objective
- Core assumption: Euclidean angles in high-dimensional space are poor estimators of the true latent angles due to noise and high dimensionality; spectral denoising can recover them
- Evidence anchors: [abstract] "reconstructing angles between data points" and "preserve structures well across all scales"; [section 3.3] Theorems 1 and 2 provide rigorous justification that spectral PCA denoising recovers true latent angles while naive angle estimation is biased in high dimensions
- Break condition: If data lies on a non-smooth manifold or if angle estimation errors dominate, the balance can fail and global distortion may re-emerge

### Mechanism 2
- Claim: Embedding on the unit sphere enables direct visualization and efficient angle computation
- Mechanism: Parameterizing points by longitude and latitude maps the high-dimensional data onto S², a fixed-size, borderless space that simplifies geodesic distance and angle calculations and naturally handles periodic patterns
- Core assumption: The intrinsic low-dimensional structure of the data is compatible with a spherical embedding; large-scale anisotropy can be accommodated by rotation post-processing
- Evidence anchors: [abstract] "mapping a high-dimensional dataset on the 2D unit sphere S², which can be directly visualized"; [section 3.1] Formal description of angle preservation on the sphere and use of side-to-angle and vertex-to-side formulas
- Break condition: If the data's intrinsic dimensionality or topology is incompatible with S² (e.g., genus > 0), the spherical constraint can introduce distortion

### Mechanism 3
- Claim: Subsampling angles drastically reduces computational cost while preserving optimization signal
- Mechanism: Because the angle matrix for each point has low effective rank, random subsampling of angle evaluations at each iteration maintains most of the information needed for gradient descent but reduces cost from O(n³) to O(n)
- Core assumption: The effective rank of angle matrices is low and representative across points; random subsampling preserves this structure
- Evidence anchors: [section 3.2] Empirical study on hematopoiesis data shows effective rank ≈13.8, motivating subsampling; [appendix B.3] Theoretical link to matrix completion literature, where incoherence and low-rankness enable accurate reconstruction from subsampled entries
- Break condition: If angle matrices have high effective rank or are highly non-uniform, subsampling can degrade reconstruction quality

## Foundational Learning

- Concept: Spectral denoising and its role in angle estimation
  - Why needed here: High-dimensional noisy data distorts Euclidean angles; PCA-based spectral denoising recovers the underlying low-dimensional structure before angle computation
  - Quick check question: What guarantees that leading principal components approximate the true latent factors in the spiked covariance model?

- Concept: Angle preservation vs. distance preservation in dimensionality reduction
  - Why needed here: The key innovation is reframing LDE from distance reconstruction to angle reconstruction; understanding this shift is critical to grasping MERCAT's motivation and theoretical justification
  - Quick check question: Why does optimizing for angles inherently balance local and global structure, unlike distance-based methods?

- Concept: Low effective rank of angle matrices and its implication for subsampling
  - Why needed here: Explains why random subsampling of angle evaluations works without sacrificing much information; connects to matrix completion theory
  - Quick check question: How does the effective rank of the angle matrix at a point relate to the intrinsic dimensionality of the data?

## Architecture Onboarding

- Component map: Data preprocessing -> PCA denoising -> Initialization on sphere -> Angle computation -> Optimization (Adam) -> Post-processing rotation
- Critical path: 1. Load data → 2. PCA denoising → 3. Initialize Y on sphere → 4. Loop: compute subsampled angles → 5. Gradient update → 6. Schedule LR → 7. Output final Y
- Design tradeoffs:
  - Fixed sphere (S²) vs. unconstrained 2D embedding: simpler, borderless, handles periodicity but restricts embedding flexibility
  - Angle subsampling vs. full angle matrix: reduces cost dramatically but may lose some signal if angle matrices are high-rank
  - Denoising before angle computation vs. raw angles: improves robustness but adds preprocessing step and parameter r
- Failure signatures:
  - Embedding collapses to poles → likely bad initialization or learning rate too high
  - Distorted global structure despite good local angles → effective rank of angle matrix is high; subsampling too aggressive
  - Very slow convergence → check gradient clipping, learning rate schedule, or batch size
- First 3 experiments:
  1. Run MERCAT on the Circle dataset (n=900, seed=1) with default params; verify angles preserved and no cluster breaking
  2. Run MERCAT on Unif5 dataset (5 uniform clusters, n=500, seed=1) with r=10 PCA denoising; check local vs global structure preservation
  3. Run MERCAT on a subset of Tabula Sapiens blood data (n=200) with r=20 PCA denoising; compare angle preservation metric to UMAP

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the angle-preservation paradigm of MERCAT truly avoid the local-global trade-off inherent in distance-based methods, or does it simply shift the trade-off to a different structural property?
- Basis in paper: [inferred] The paper claims MERCAT outperforms existing methods in both local and global structure preservation, challenging the notion of an inherent trade-off, but does not provide rigorous theoretical proof of the absence of such a trade-off
- Why unresolved: The paper provides empirical evidence and theoretical justification for the effectiveness of angle preservation, but does not rigorously prove that the local-global trade-off is entirely avoided
- What evidence would resolve it: A theoretical analysis proving that angle preservation does not suffer from the same local-global trade-off as distance preservation, or an empirical study demonstrating MERCAT's consistent superiority across diverse datasets and metrics, even when the local-global trade-off is explicitly introduced

### Open Question 2
- Question: How does the choice of the spectral denoising parameter (r) in MERCAT impact the trade-off between noise robustness and information preservation, and is there an optimal value for r?
- Basis in paper: [explicit] The paper mentions that spectral denoising is used to improve robustness to noise and high dimensionality, but does not explore the impact of different values of r on the embedding quality
- Why unresolved: The paper does not provide a systematic study of how the choice of r affects the balance between noise reduction and information retention in the embedding
- What evidence would resolve it: An empirical study evaluating MERCAT's performance with different values of r on datasets with varying levels of noise and dimensionality, identifying an optimal value of r that maximizes embedding quality while maintaining noise robustness

### Open Question 3
- Question: Can the angle subsampling strategy in MERCAT be further optimized to achieve a better balance between computational efficiency and embedding accuracy?
- Basis in paper: [explicit] The paper introduces angle subsampling to improve computational efficiency and provides some theoretical justification for its efficacy, but does not explore alternative subsampling strategies or their impact on embedding quality
- Why unresolved: The paper does not investigate alternative subsampling strategies or provide a comprehensive analysis of the trade-off between computational efficiency and embedding accuracy achieved by the current subsampling approach
- What evidence would resolve it: An empirical study comparing MERCAT's performance with different subsampling strategies, such as stratified sampling or adaptive sampling based on local density, and evaluating their impact on embedding quality and computational efficiency

## Limitations

- The theoretical guarantees are primarily limited to the spectral denoising component rather than the overall angle preservation framework
- The effectiveness of random angle subsampling relies on empirical observations about low effective rank rather than rigorous theoretical bounds
- The restriction to spherical embeddings may limit applicability to data with incompatible intrinsic geometry

## Confidence

- **High confidence**: The empirical demonstration that MERCAT achieves superior angle preservation compared to existing methods, and the theoretical justification for spectral denoising recovering latent angles
- **Medium confidence**: The claim that angle preservation naturally balances local and global structure, based primarily on qualitative observations and theoretical intuition rather than formal proofs
- **Medium confidence**: The computational efficiency gains from angle subsampling, which is well-motivated by empirical observations but lacks rigorous theoretical guarantees for all data types

## Next Checks

1. **Subsampling robustness test**: Systematically vary the angle subsampling rate (e.g., 16, 32, 64, 128 per point) on multiple datasets to quantify the tradeoff between computational efficiency and angle preservation quality

2. **Dimensionality sensitivity analysis**: Test MERCAT with varying numbers of PCA components (r=5, 10, 20, 50) on high-dimensional single-cell data to determine optimal denoising levels and assess sensitivity to this hyperparameter

3. **Topology compatibility test**: Apply MERCAT to datasets with known non-spherical intrinsic geometry (e.g., Swiss roll with hole, genus-1 surfaces) to evaluate how the spherical constraint affects embedding quality and whether post-rotation can compensate