---
ver: rpa2
title: 'Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization
  and Expected Information Gain'
arxiv_id: '2406.17453'
source_url: https://arxiv.org/abs/2406.17453
tags:
- questions
- target
- question
- zero-shot
- candidates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve the informativeness of
  questions generated by large language models (LLMs) using Expected Information Gain
  (EIG) as a training signal. The approach involves sampling multiple questions from
  the model, evaluating them based on EIG, and training the model using Direct Preference
  Optimization (DPO) with pairs of high-EIG and low-EIG questions.
---

# Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain

## Quick Facts
- arXiv ID: 2406.17453
- Source URL: https://arxiv.org/abs/2406.17453
- Reference count: 12
- Primary result: EIG-based preference optimization significantly improves question informativeness in 20 Questions Game, reducing average questions needed to identify targets

## Executive Summary
This paper introduces a method to enhance the informativeness of questions generated by large language models through Expected Information Gain (EIG) optimization. The approach combines sampling multiple questions, evaluating them using EIG, and training with Direct Preference Optimization (DPO) to preferentially select high-information questions. The method demonstrates significant improvements in the efficiency of the 20 Questions Game, reducing the average number of questions needed to identify targets from 6.2 to 2.7. The approach shows generalization across different domains and candidate set sizes, outperforming both zero-shot and fine-tuning baselines.

## Method Summary
The method involves generating multiple candidate questions from an LLM, evaluating each using Expected Information Gain (EIG) as computed by a separate LLM oracle (GPT-4), and then training the model using Direct Preference Optimization with pairs of high-EIG and low-EIG questions. The DPO objective is modified to incorporate EIG scores, creating a preference signal that encourages the model to generate more informative questions. This approach treats question generation as a preference learning problem where the model learns to discriminate between informative and non-informative questions based on their expected information gain.

## Key Results
- DPO with EIG significantly improves question informativeness, reducing average questions needed in 20 Questions Game from 6.2 to 2.7
- The method generalizes across different domains (Animals, Countries, Musical Instruments) and candidate set sizes (20, 50, 100)
- Outperforms both zero-shot generation and fine-tuning approaches in terms of both accuracy and efficiency

## Why This Works (Mechanism)
The method works by providing a quantitative signal (EIG) that measures how much information a question is expected to reveal about the target. By training the model to prefer questions with high EIG, it learns to generate questions that strategically reduce uncertainty about the answer. The preference optimization framework allows the model to learn a relative ranking of questions rather than just absolute quality, making it more robust to variations in question phrasing while maintaining the core information-gathering capability.

## Foundational Learning
- Expected Information Gain (EIG): Measures the expected reduction in uncertainty from asking a question - needed to quantify question informativeness; quick check: verify EIG calculations align with information theory principles
- Direct Preference Optimization (DPO): A preference learning method that trains models to distinguish between preferred and dispreferred outputs - needed for efficient training on relative quality signals; quick check: ensure preference pairs are well-formed and diverse
- 20 Questions Game paradigm: A controlled framework for evaluating question effectiveness through binary classification - needed for quantitative comparison of question informativeness; quick check: validate that the game setup captures meaningful information-gathering scenarios

## Architecture Onboarding

**Component Map:**
LLM (question generator) -> EIG Oracle (GPT-4) -> Preference Filter -> DPO Trainer -> Improved LLM

**Critical Path:**
Question generation → EIG computation → Preference pair creation → DPO training → Inference-time question generation

**Design Tradeoffs:**
- Sampling multiple questions increases computational cost but provides better training signals
- Using GPT-4 as EIG oracle ensures quality but introduces dependency and cost
- Preference learning vs direct regression on EIG scores - preference learning is more robust to noise

**Failure Signatures:**
- Low EIG diversity across sampled questions indicates the generator isn't exploring the space sufficiently
- High computational cost during training due to multiple generations and EIG computations
- Overfitting to the 20 Questions format rather than general question quality

**First 3 Experiments:**
1. Verify EIG calculations on a small set of hand-crafted questions with known information content
2. Test preference learning with synthetic preference pairs before scaling to EIG-based preferences
3. Compare zero-shot vs fine-tuning vs DPO approaches on a subset of the data to validate the experimental setup

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to 20 Questions Game paradigm, which may not generalize to real-world multi-purpose dialogues
- Heavy reliance on GPT-4 as EIG oracle introduces potential bias and limits reproducibility
- Computational cost of sampling and EIG computation not explicitly addressed for scalability

## Confidence
- **High confidence**: Core methodology of using EIG as training signal and DPO for preference learning
- **Medium confidence**: Generalization claims across domains and candidate set sizes
- **Medium confidence**: Efficiency gains in dialogues based on experimental setup

## Next Checks
1. Evaluate the approach in multi-turn dialogue scenarios where questions serve purposes beyond binary classification
2. Compare results using different EIG computation methods to assess sensitivity to oracle choice
3. Conduct human evaluation studies with domain experts rating question quality in real-world contexts