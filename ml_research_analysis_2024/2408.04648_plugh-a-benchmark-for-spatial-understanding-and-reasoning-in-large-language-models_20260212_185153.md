---
ver: rpa2
title: 'PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large Language
  Models'
arxiv_id: '2408.04648'
source_url: https://arxiv.org/abs/2408.04648
tags:
- task
- path
- locations
- spatial
- room
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PLUGH is a benchmark for evaluating spatial understanding and reasoning
  in Large Language Models (LLMs) using text-based games. The benchmark consists of
  5 tasks based on 125 input texts from 48 games, representing 61 different spatial
  graphs.
---

# PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2408.04648
- Source URL: https://arxiv.org/abs/2408.04648
- Authors: Alexey Tikhonov
- Reference count: 7
- Primary result: PLUGH benchmark evaluates spatial reasoning in LLMs with 5 tasks on text-based games, showing commercial models outperform open-source ones but all have room for improvement

## Executive Summary
PLUGH is a benchmark designed to evaluate spatial understanding and reasoning capabilities in Large Language Models (LLMs) using text-based games. The benchmark consists of 125 input texts from 48 different games, representing 61 spatial graphs, and includes 5 tasks of increasing complexity. The evaluation reveals that while commercial LLMs demonstrate stronger reasoning abilities, open-source models perform comparably well, though all models show significant room for improvement. The study identifies key failure modes including incorrect formatting, naming ambiguity, and location hallucinations.

## Method Summary
The benchmark converts formally defined game environments into narrative texts, allowing LLMs to learn spatial structures through natural language understanding. It uses controlled difficulty progression through task design, starting with graph reconstruction and advancing to complex temporal hinted shortest path finding. The evaluation employs fuzzy matching and normalization techniques to handle ambiguity in natural language spatial descriptions, ensuring robust assessment while preventing false positives.

## Key Results
- GPT-4o achieved up to 78.8% F1-score in graph reconstruction tasks
- Character path reconstruction showed 9.3% normalized Levenshtein distance for best-performing model
- Open-source models performed almost as well as commercial models
- All models exhibited significant room for improvement across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PLUGH works by converting formally defined game environments into narrative texts, allowing LLMs to learn spatial structures through natural language understanding.
- **Mechanism**: The benchmark leverages the fact that text-based games have formally defined locations and transitions (graph structure) while also producing natural narrative outputs (game transcripts). By converting these transcripts into fiction-style narratives, the benchmark creates a bridge between formal spatial knowledge and natural language understanding.
- **Core assumption**: LLMs can extract spatial information from narrative text when provided with proper training/few-shot examples.
- **Evidence anchors**:
  - [abstract]: "We propose using text-based games to construct such a benchmark, specifically a collection of games with known walkthroughs suitable for running on the Jericho emulator"
  - [section]: "Text-based games represent a unique source of such information: On the one hand, the player interacts with a partially observable, modeled environment through actions and observations conveyed in natural language. Thus, the game transcript is quite close to a natural linear fictional text"
- **Break condition**: If the LLM cannot learn to map narrative descriptions to spatial structures even with extensive few-shot examples, the mechanism fails.

### Mechanism 2
- **Claim**: PLUGH uses controlled difficulty progression through task design to systematically evaluate different aspects of spatial reasoning.
- **Mechanism**: The benchmark includes 5 tasks of increasing complexity: graph reconstruction (simplest), path reconstruction, reversed path reconstruction, novel shortest path finding, and temporal hinted shortest path finding (most complex). This progression allows evaluation of both basic spatial understanding and higher-order reasoning.
- **Core assumption**: Spatial reasoning can be decomposed into hierarchical tasks of increasing complexity.
- **Evidence anchors**:
  - [abstract]: "PLUGH is a benchmark for evaluating spatial understanding and reasoning in Large Language Models (LLMs) using text-based games. The benchmark consists of 5 tasks"
  - [section]: "We introduce the PLUGH dataset with data crafted to facilitate these tasks" and lists the 5 tasks with their descriptions
- **Break condition**: If models show no improvement pattern across tasks or fail unexpectedly at lower-complexity tasks, the decomposition assumption breaks.

### Mechanism 3
- **Claim**: PLUGH achieves robust evaluation through fuzzy matching and normalization techniques that handle the inherent ambiguity in natural language spatial descriptions.
- **Mechanism**: The benchmark implements preprocessing steps including lowercase conversion, article removal, preposition removal, and substring-based fuzzy matching to handle variations in location naming and prevent false negatives in evaluation.
- **Core assumption**: Location naming ambiguity in natural language can be adequately handled through normalization and fuzzy matching without introducing false positives.
- **Evidence anchors**:
  - [section]: "To minimize the effects of these issues, we provided a relaxed ('fuzzy') parsing and matching algorithm. However, we ensure there are no false positives in that matching by avoiding similar location names in ground truth data"
  - [section]: "Normalization includes lowercase, removal of articles, and removal of prepositions at the beginning of names"
- **Break condition**: If normalization introduces false positives or fails to handle edge cases in location naming, the evaluation becomes unreliable.

## Foundational Learning

- **Concept**: Graph theory fundamentals (nodes, edges, paths, shortest paths)
  - Why needed here: Understanding the spatial structures being evaluated is essential for interpreting benchmark results and designing new tasks
  - Quick check question: What is the difference between a connected graph and a disconnected graph in the context of spatial navigation?

- **Concept**: Natural language processing basics (tokenization, parsing, semantic understanding)
  - Why needed here: LLMs process text through these mechanisms, and understanding them helps in analyzing why models succeed or fail at spatial reasoning tasks
  - Quick check question: How does an LLM's tokenization approach affect its ability to understand location names and spatial relationships?

- **Concept**: Evaluation metrics for spatial tasks (F1-score, Levenshtein distance, precision/recall)
  - Why needed here: Proper interpretation of benchmark results requires understanding what these metrics measure and their limitations
  - Quick check question: Why might Levenshtein distance be preferred over exact string matching for evaluating path reconstruction tasks?

## Architecture Onboarding

- **Component map**: Jericho emulator -> game transcripts -> graph extraction -> sliding window segmentation -> fiction text conversion
- **Critical path**: 
  1. Extract game transcripts and corresponding spatial graphs from Jericho emulator
  2. Apply sliding window to find "good" segments (6-20 nodes, connected, non-degenerate)
  3. Convert transcripts to fiction text using GPT-4
  4. Validate graph-text pairs through substring checks and duplicate prevention
  5. Define tasks and evaluation metrics
  6. Run LLMs on tasks with varying few-shot examples
  7. Parse and normalize responses
  8. Calculate metrics and analyze results

- **Design tradeoffs**:
  - Task complexity vs. model capability: More complex tasks reveal higher reasoning but may be unsolvable by current models
  - Evaluation stringency vs. robustness: Strict matching catches errors but may penalize minor variations; fuzzy matching is more robust but risks false positives
  - Data diversity vs. consistency: More games provide better generalization but may introduce inconsistent spatial descriptions

- **Failure signatures**:
  - Incorrect formatting: Directed graphs instead of undirected, extra edge labels, improper syntax
  - Naming ambiguity: Location names changed, merged, or split inconsistently
  - Location hallucinations: Invented locations not present in source material
  - Task misunderstanding: Responses addressing wrong aspects of the prompt

- **First 3 experiments**:
  1. Run Task 1 (graph reconstruction) with 0-shot and 3-shot examples on GPT-4 to establish baseline performance and identify formatting issues
  2. Test normalization pipeline on a small set of model responses to verify fuzzy matching works without introducing false positives
  3. Run Task 2a (character path reconstruction) on a single model with different temperatures to understand the impact of sampling variability on evaluation consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs develop a more nuanced understanding of spatial hierarchies in fictional texts, such as distinguishing between nested locations and separate locations?
- Basis in paper: [inferred] The paper discusses the complexity of defining locations in fictional texts, including the ambiguity of whether nested locations should be considered separate entities or objects within a location.
- Why unresolved: The paper does not provide a definitive answer to how LLMs should handle nested locations or the distinction between locations and containers. It only highlights the subjective nature of these definitions and suggests that an average reader can intuitively respond to them without much thought.
- What evidence would resolve it: Developing a benchmark that specifically tests LLMs' ability to differentiate between nested locations and separate locations, along with a clear set of guidelines for defining these spatial hierarchies in fictional texts.

### Open Question 2
- Question: How can LLMs be trained to minimize location hallucinations and accurately extract spatial information from fictional texts without inventing non-existent locations?
- Basis in paper: [explicit] The paper identifies location hallucinations as a typical error in LLM performance, where models invent locations that were not visited in the narrative but were mentioned as seen in the distance.
- Why unresolved: While the paper acknowledges the issue of location hallucinations, it does not provide a solution or a method to prevent LLMs from generating these false locations. It only mentions that this leads to additional false nodes in the graph, which can be illustrated by degradation in precision.
- What evidence would resolve it: Implementing and testing various techniques, such as incorporating context-aware attention mechanisms or using external knowledge bases, to improve LLMs' ability to accurately extract spatial information without generating hallucinated locations.

### Open Question 3
- Question: Can the performance of LLMs in spatial reasoning tasks be improved by leveraging principles of narrative structure, such as the description reporting bias or Chekhov's gun principle, to guide their understanding of space in fictional texts?
- Basis in paper: [explicit] The paper discusses how principles like the description reporting bias and Chekhov's gun principle can influence the perception of space in narratives, suggesting that writers tend to skip obvious information while writing texts and that mentioned objects should be important to the plot.
- Why unresolved: The paper does not explore how these principles can be incorporated into LLMs' training or inference process to enhance their spatial reasoning capabilities. It only mentions that these principles can be leveraged to assess and improve the performance of LLMs in spatial reasoning tasks.
- What evidence would resolve it: Conducting experiments to train LLMs on datasets that emphasize these narrative principles and evaluating their performance on spatial reasoning tasks compared to models trained on standard datasets. Additionally, analyzing the impact of incorporating these principles into the model's architecture or fine-tuning process.

## Limitations

- The evaluation shows all tested models have substantial room for improvement with F1-scores never exceeding 78.8%
- The relatively small corpus of 125 input texts from 48 games may not capture full complexity of spatial reasoning scenarios
- The normalization approach using fuzzy matching may introduce evaluation artifacts if location naming patterns become too permissive

## Confidence

**High Confidence**: The benchmark construction methodology and the five-task evaluation framework are well-documented and reproducible. The observed performance gaps between commercial and open-source models, while notable, are consistently reported across multiple evaluation runs.

**Medium Confidence**: Claims about specific failure modes (incorrect formatting, naming ambiguity, location hallucinations) are based on qualitative analysis of model outputs. While the patterns are clearly observable, the relative frequency and impact of each failure mode would benefit from more systematic quantification.

**Low Confidence**: The assertion that open-source models perform "almost as well" as commercial models requires careful interpretation, as the performance differences, while present, may not be practically significant for all use cases. The normalized Levenshtein distance metric, while appropriate for path reconstruction, may not fully capture semantic equivalence in spatial navigation.

## Next Checks

1. **Cross-validation with additional game datasets**: Test the benchmark on a separate collection of text-based games not included in the original 48 to verify the robustness of performance patterns and identify potential overfitting to the current corpus.

2. **Ablation study on normalization techniques**: Systematically vary the normalization and fuzzy matching parameters to quantify their impact on evaluation metrics and establish optimal thresholds that balance false positives and false negatives.

3. **Human evaluation comparison**: Recruit human subjects to perform the same spatial reasoning tasks and compare their performance metrics with the best LLM results to establish human-level baselines and identify task-specific challenges.