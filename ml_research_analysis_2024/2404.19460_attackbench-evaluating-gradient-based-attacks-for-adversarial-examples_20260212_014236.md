---
ver: rpa2
title: 'AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples'
arxiv_id: '2404.19460'
source_url: https://arxiv.org/abs/2404.19460
tags:
- advlib
- original
- attacks
- foolbox
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AttackBench introduces a fair benchmark for gradient-based adversarial
  attacks by defining a novel optimality metric that measures how close each attack
  is to the empirically-optimal solution across varying perturbation budgets. It enforces
  a maximum query budget for all attacks, enabling consistent comparison of effectiveness
  and efficiency.
---

# AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples

## Quick Facts
- arXiv ID: 2404.19460
- Source URL: https://arxiv.org/abs/2404.19460
- Reference count: 40
- Primary result: Fair benchmark for gradient-based attacks using empirically-optimal solutions and query budget constraints

## Executive Summary
AttackBench introduces a fair benchmark for evaluating gradient-based adversarial attacks by defining a novel optimality metric that measures how close each attack is to the empirically-optimal solution across varying perturbation budgets. The framework enforces a maximum query budget for all attacks, enabling consistent comparison of effectiveness and efficiency. Experiments on CIFAR-10 and ImageNet reveal that only a few attacks (σ-zero, DDN, APGD, PDPGD) achieve high global optimality scores, while many implementations suffer from bugs or suboptimal performance.

## Method Summary
AttackBench addresses the lack of standardized evaluation for adversarial attacks by introducing a novel optimality metric based on empirically-optimal solutions. The framework enforces a maximum query budget for all attacks, ensuring fair comparison of effectiveness and efficiency. The benchmark evaluates attacks across multiple perturbation budgets and models, using 100 random seeds per class to estimate optimality. By providing a public, continuously updatable framework, AttackBench enables systematic comparison of gradient-based attacks and identifies key ingredients for optimal performance.

## Key Results
- Only σ-zero, DDN, APGD, and PDPGD achieve high global optimality scores across datasets
- Many attack implementations contain bugs or exhibit suboptimal performance
- Adaptive step sizes and gradient normalization are critical for optimal attack performance
- Significant variability exists in attack performance across different libraries

## Why This Works (Mechanism)
AttackBench works by establishing a common ground for comparing adversarial attacks through the concept of empirically-optimal solutions. By setting a maximum query budget and evaluating attacks across multiple perturbation budgets, the framework ensures that comparisons are fair and consistent. The use of multiple random seeds per class provides robust estimates of attack performance, while the focus on gradient-based attacks allows for systematic analysis of attack components and their impact on optimality.

## Foundational Learning
- **Empirically-optimal solutions**: Why needed - provides a reference point for measuring attack performance; Quick check - compare attack success rates against optimal solution
- **Query budget constraints**: Why needed - ensures fair comparison between attacks with different computational costs; Quick check - verify all attacks use same maximum number of queries
- **Perturbation budget**: Why needed - allows evaluation across different levels of adversarial strength; Quick check - confirm consistent epsilon values across experiments
- **Random seed initialization**: Why needed - reduces bias and provides robust performance estimates; Quick check - ensure 100 seeds per class as specified
- **Gradient normalization**: Why needed - critical for stable and effective attack optimization; Quick check - verify gradient scaling in attack implementations
- **Adaptive step sizes**: Why needed - improves convergence and attack success; Quick check - examine step size adjustment mechanisms

## Architecture Onboarding

Component Map: Benchmark Framework -> Optimality Metric -> Attack Implementations -> Evaluation Pipeline

Critical Path: Initialization -> Attack Execution -> Solution Evaluation -> Optimality Scoring -> Results Aggregation

Design Tradeoffs:
- Query budget vs. attack effectiveness: Higher budgets allow better solutions but increase computational cost
- Number of seeds vs. statistical significance: More seeds provide better estimates but increase computation time
- Model complexity vs. benchmark scope: More complex models provide better generalization but increase implementation difficulty

Failure Signatures:
- Low optimality scores across multiple attacks may indicate implementation bugs
- High variance in results across seeds suggests instability in attack algorithms
- Suboptimal performance despite high query budgets may indicate poor algorithm design

First 3 Experiments:
1. Run all attacks with default parameters on CIFAR-10 with fixed query budget
2. Compare σ-zero, DDN, APGD, and PDPGD performance across perturbation budgets
3. Analyze the impact of removing gradient normalization from top-performing attacks

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Reliance on empirically-optimal solutions derived from finite initializations may underestimate true optimal performance
- Focus on gradient-based attacks potentially overlooks other relevant attack paradigms
- Results may not generalize to datasets beyond CIFAR-10 and ImageNet

## Confidence

High:
- Only σ-zero, DDN, APGD, and PDPGD achieve high global optimality scores (directly supported by experimental results)
- Adaptive step sizes and gradient normalization are key ingredients for optimal attacks (systematically analyzed)

Medium:
- Many implementations suffer from bugs or suboptimal performance (depends on external library quality)
- Conclusions about key attack ingredients may not generalize to other datasets or threat models

## Next Checks
1. Test AttackBench on additional datasets (e.g., TinyImageNet, SVHN) to assess generalizability
2. Evaluate the impact of varying the number of initializations on optimality scores to quantify robustness to initialization
3. Compare AttackBench's results with black-box or query-based attacks to determine if gradient-based attacks remain optimal across attack paradigms