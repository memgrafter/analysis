---
ver: rpa2
title: 'SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement Learning
  Agents'
arxiv_id: '2405.20539'
source_url: https://arxiv.org/abs/2405.20539
tags:
- attack
- poisoning
- attacks
- agent
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a new threat model and attack framework for
  backdoor poisoning in reinforcement learning. Unlike prior methods that use static
  reward poisoning within an inner-loop threat model, this approach leverages dynamic
  reward poisoning in an outer-loop setting where the adversary can observe entire
  trajectories before poisoning.
---

# SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement Learning Agents

## Quick Facts
- arXiv ID: 2405.20539
- Source URL: https://arxiv.org/abs/2405.20539
- Authors: Ethan Rathbun; Christopher Amato; Alina Oprea
- Reference count: 40
- Primary result: Dynamic reward poisoning in outer-loop setting achieves high attack success with low poisoning budgets across 6 diverse environments

## Executive Summary
This work introduces SleeperNets, a novel backdoor poisoning attack framework for reinforcement learning that operates in an outer-loop threat model. Unlike previous methods using static reward poisoning, SleeperNets employs dynamic reward poisoning based on Monte Carlo value estimates, allowing the adversary to observe complete trajectories before applying targeted poisoning. The approach achieves both high attack success (reliably inducing target actions when triggers are present) and attack stealth (maintaining near-optimal performance in benign states) through theoretical guarantees on an adversarial Markov decision process formulation.

## Method Summary
SleeperNets implements dynamic reward poisoning in an outer-loop threat model where the adversary observes entire trajectories before poisoning. The attack computes Monte Carlo value estimates to determine optimal reward perturbations that minimize the value function while maximizing the probability of the target action when triggers are present. The method is evaluated across six diverse environments including Breakout, Qbert, Car Racing, Highway Merge, Safety Car, and Trading BTC, demonstrating superior performance compared to baseline approaches while requiring minimal poisoning budgets (as low as 0.0006%).

## Key Results
- Achieves significantly higher attack success rates than baseline methods across all six tested environments
- Maintains better episodic return during training compared to competing approaches
- Requires very low poisoning budgets (down to 0.0006%) while still achieving high attack effectiveness
- Provides theoretical guarantees for both attack success and stealth properties through adversarial MDP formulation

## Why This Works (Mechanism)
The effectiveness of SleeperNets stems from its dynamic reward poisoning strategy that leverages complete trajectory information. By computing Monte Carlo value estimates before poisoning, the adversary can precisely target reward perturbations that minimize the value function while maximizing the likelihood of target actions when triggers are present. The outer-loop threat model allows for sophisticated optimization of poisoning strategies that would not be possible with real-time, single-step poisoning approaches.

## Foundational Learning
- **Outer-loop vs inner-loop threat models**: Understanding the distinction is crucial for grasping why SleeperNets can achieve better performance - outer-loop allows trajectory-level optimization while inner-loop requires single-step decisions
- **Monte Carlo value estimation**: This technique provides unbiased estimates of state values that enable precise reward perturbation calculations for the attack
- **Adversarial MDP formulation**: The theoretical framework proving that dynamic reward poisoning can achieve both attack success and stealth through specific MDP properties
- **Reward perturbation bounds**: Understanding how the constraints on reward changes affect both attack effectiveness and detection risk
- **Trigger pattern embedding**: The method of how trigger patterns are applied to states and how this affects the poisoned policy's behavior

## Architecture Onboarding
**Component Map**: Environment -> Agent (PPO) -> Value Function Estimator -> Reward Perturbation Calculator -> Poisoned Reward Function -> Training Loop

**Critical Path**: The most critical components are the value function estimator and reward perturbation calculator, as these directly determine the effectiveness of the poisoning strategy. The outer-loop threat model is also essential as it enables the trajectory-level optimization.

**Design Tradeoffs**: The main tradeoff is between attack success and stealth - more aggressive reward perturbations achieve higher attack success but may be more detectable. The dynamic approach balances this by only applying large perturbations when triggers are present.

**Failure Signatures**: If attack success is low, likely causes include incorrect value estimation, improper trigger pattern implementation, or insufficient poisoning budget. If episodic return drops significantly, the reward perturbation term may be incorrectly implemented or too aggressive.

**First Experiments**:
1. Verify Monte Carlo value estimation by comparing estimates across multiple trajectories and checking for convergence
2. Test basic trigger application by manually applying triggers to states and verifying target action induction
3. Evaluate poisoning effectiveness on a simple environment (like CartPole) before scaling to more complex domains

## Open Questions the Paper Calls Out
- Can SleeperNets achieve universal attack success against other reinforcement learning algorithms beyond PPO?
- How does the choice of trigger pattern affect attack performance and detectability?
- What are the theoretical limits of detection for dynamic reward poisoning attacks as the reward perturbation grows?

## Limitations
- The outer-loop threat model assumes the adversary can observe complete trajectories before poisoning, which may not be realistic in many deployment scenarios
- The method requires knowledge of the reward function and environment dynamics, limiting applicability when these are not fully observable
- The theoretical guarantees rely on specific assumptions about reward perturbation bounds and trigger distribution that may not hold in practice

## Confidence
- High confidence: Empirical results showing SleeperNets achieves higher attack success rates and better episodic returns than baseline methods across all six environments
- Medium confidence: Theoretical guarantees of attack stealth and success under the stated assumptions about reward perturbation and trigger distribution
- Medium confidence: Claims about computational efficiency due to Monte Carlo value estimation, though exact runtime comparisons are not provided

## Next Checks
1. Test the attack under more realistic threat models where the adversary cannot observe complete trajectories before poisoning
2. Evaluate robustness of the attack when the trigger pattern is applied imperfectly or with noise
3. Assess transferability of poisoned policies to unseen environments or variations of the training environments