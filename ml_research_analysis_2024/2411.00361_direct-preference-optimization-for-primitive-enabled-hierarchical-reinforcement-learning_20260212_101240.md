---
ver: rpa2
title: Direct Preference Optimization for Primitive-Enabled Hierarchical Reinforcement
  Learning
arxiv_id: '2411.00361'
source_url: https://arxiv.org/abs/2411.00361
tags:
- policy
- level
- learning
- dipper
- lower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DIPPER, a novel hierarchical reinforcement
  learning (HRL) framework that addresses two fundamental challenges in HRL: non-stationarity
  and infeasible subgoal generation. DIPPER formulates HRL as a bi-level optimization
  problem and leverages direct preference optimization (DPO) to train the higher-level
  policy using preference feedback.'
---

# Direct Preference Optimization for Primitive-Enabled Hierarchical Reinforcement Learning

## Quick Facts
- arXiv ID: 2411.00361
- Source URL: https://arxiv.org/abs/2411.00361
- Reference count: 40
- Key outcome: DIPPER achieves up to 40% improvement over state-of-the-art baselines in sparse reward scenarios through novel bi-level optimization framework

## Executive Summary
DIPPER introduces a novel hierarchical reinforcement learning framework that addresses two fundamental challenges in HRL: non-stationarity and infeasible subgoal generation. The framework formulates HRL as a bi-level optimization problem and leverages direct preference optimization (DPO) to train the higher-level policy using preference feedback. This approach effectively decouples higher-level learning from the non-stationary lower-level reward signal. Additionally, DIPPER incorporates a regularization mechanism to ensure subgoal feasibility within the capabilities of the lower-level policy.

Extensive experiments on robotic navigation and manipulation benchmarks demonstrate DIPPER's effectiveness in overcoming longstanding HRL limitations. The framework shows significant performance improvements, particularly in sparse reward scenarios where traditional methods struggle. The approach provides a practical solution for complex robotic tasks by addressing the temporal credit assignment problem inherent in hierarchical systems.

## Method Summary
DIPPER reformulates hierarchical reinforcement learning as a bi-level optimization problem where the higher-level policy generates subgoals and the lower-level policy executes primitive actions to achieve them. The key innovation lies in using direct preference optimization (DPO) to train the higher-level policy using preference feedback rather than traditional reward signals. This approach mitigates the non-stationarity problem caused by the evolving lower-level policy.

The framework incorporates a regularization mechanism that constrains subgoals to be feasible given the lower-level policy's capabilities. This ensures that the higher-level policy generates subgoals that can actually be executed by the lower level, addressing the infeasibility challenge common in HRL. The bi-level optimization structure allows for stable training of both levels while maintaining coordination between them.

## Key Results
- Achieves up to 40% improvement over state-of-the-art baselines in sparse reward scenarios
- Demonstrates consistent performance gains across PointNav, AntGather, FetchReach, and BlockStacking tasks
- Ablation studies confirm the effectiveness of both the preference optimization component and the subgoal feasibility regularization

## Why This Works (Mechanism)
The framework's effectiveness stems from decoupling the higher-level policy training from the non-stationary lower-level reward signal through preference optimization. By using preference feedback instead of direct rewards, the higher-level policy learns stable subgoal generation strategies that remain effective even as the lower-level policy improves. The regularization mechanism ensures that subgoals remain within the feasible region of the lower-level policy's capabilities, preventing the higher level from proposing impossible tasks.

The bi-level optimization formulation provides a principled way to coordinate between the two policy levels while maintaining their independence during training. This structure allows each level to optimize its own objectives while contributing to the overall hierarchical system's performance. The preference-based training for the higher level is particularly effective because it can capture complex preferences about subgoal quality that are difficult to express through simple reward functions.

## Foundational Learning
- **Hierarchical Reinforcement Learning**: Multi-level decision-making where high-level policies set goals and low-level policies execute actions. Needed because flat RL struggles with long-horizon tasks and credit assignment. Quick check: Can decompose complex tasks into manageable subtasks.
- **Non-stationarity in HRL**: The changing lower-level policy creates moving target for higher-level training. This is problematic because higher-level policies cannot learn stable strategies. Quick check: Lower policy performance affects higher policy reward distribution.
- **Direct Preference Optimization (DPO)**: Learning from pairwise comparisons rather than absolute rewards. Useful because preferences can capture complex qualitative objectives. Quick check: Can train policies from human feedback or alternative reward signals.
- **Bi-level Optimization**: Two nested optimization problems where lower level depends on upper level decisions. Important for coordinating hierarchical systems. Quick check: Upper level solution affects lower level objective.
- **Subgoal Feasibility**: Ensuring generated subgoals can actually be executed by lower-level policy. Critical for practical HRL deployment. Quick check: Subgoal should lie within lower policy's reachable state space.

## Architecture Onboarding

**Component Map:**
Higher-level policy (DPO) -> Subgoal Generator -> Lower-level policy (RL) -> Environment

**Critical Path:**
1. Higher-level policy generates subgoal based on current state
2. Lower-level policy receives subgoal as input and executes primitive actions
3. Environment returns next state and sparse reward
4. Preference feedback collected on subgoal quality
5. Higher-level policy updated via DPO using preference data
6. Regularization ensures subgoal feasibility

**Design Tradeoffs:**
- Preference-based training vs. reward-based: Preferences provide richer feedback but require more data collection overhead
- Bi-level optimization complexity vs. training stability: Adds computational cost but provides better coordination
- Regularization strength: Balancing constraint tightness against flexibility in subgoal generation

**Failure Signatures:**
- Poor performance despite training: May indicate infeasible subgoal generation or preference feedback issues
- Training instability: Could result from improper regularization parameters or bi-level coordination problems
- Lower-level policy outperforming higher-level: Suggests the regularization is too restrictive

**First 3 Experiments:**
1. Compare DIPPER against flat RL baseline on simple navigation task to verify hierarchical advantage
2. Test subgoal feasibility regularization by attempting to generate increasingly challenging subgoals
3. Evaluate preference optimization by training with varying amounts of preference feedback to assess sample efficiency

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The framework assumes access to preference feedback or human demonstrations, which may be costly to obtain in practice
- Computational overhead from bi-level optimization structure is mentioned but not thoroughly quantified
- Limited exploration of performance when there is a capability mismatch between subgoals and lower-level policy
- Theoretical guarantees focus on optimization formulation rather than practical performance bounds

## Confidence
- **High confidence**: The bi-level optimization formulation and DPO theoretical framework for higher-level policy training
- **Medium confidence**: Empirical performance improvements reported, though statistical validation is limited
- **Low confidence**: Practical scalability and real-world applicability claims due to insufficient discussion of computational costs

## Next Checks
1. Conduct statistical significance tests across multiple random seeds with confidence intervals reported for all benchmark comparisons
2. Evaluate DIPPER's performance when preference feedback is imperfect or when there is a capability mismatch between subgoals and lower-level policy
3. Benchmark computational overhead and sample efficiency against baseline methods across varying problem complexities