---
ver: rpa2
title: 'Chain of Condition: Construct, Verify and Solve Conditions for Conditional
  Question Answering'
arxiv_id: '2408.05442'
source_url: https://arxiv.org/abs/2408.05442
tags:
- condition
- conditions
- answer
- chain
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Chain of Condition addresses conditional question answering by
  decomposing the task into three steps: identifying conditions and their logical
  relationships from documents, verifying their fulfillment in user scenarios, and
  solving the logical expression to determine missing conditions. The method uses
  large language models for condition identification and verification, then leverages
  a Python interpreter to precisely solve the logical expressions.'
---

# Chain of Condition: Construct, Verify and Solve Conditions for Conditional Question Answering

## Quick Facts
- arXiv ID: 2408.05442
- Source URL: https://arxiv.org/abs/2408.05442
- Authors: Jiuheng Lin; Yuxuan Lai; Yansong Feng
- Reference count: 18
- Chain of Condition achieves new state-of-the-art results on ConditionalQA and ShARC datasets

## Executive Summary
Chain of Condition is a novel approach for conditional question answering that decomposes the task into three distinct steps: identifying conditions and their logical relationships from documents, verifying these conditions against user scenarios, and solving the resulting logical expressions to determine missing conditions. The method leverages large language models for condition identification and verification, then employs a Python interpreter for precise logical expression solving. Experimental results demonstrate that Chain of Condition outperforms existing prompting baselines and even surpasses supervised models when using GPT-3.5-Turbo or GPT-4.

## Method Summary
Chain of Condition addresses the challenge of conditional question answering by breaking it down into three sequential steps. First, it uses large language models to identify conditions and their logical relationships from source documents. Second, it verifies whether these conditions are fulfilled in the user's specific scenario through additional LLM prompts. Finally, it solves the logical expression formed by these conditions using a Python interpreter to determine what additional information is needed. This approach is designed to be more interpretable than black-box methods while maintaining high accuracy on benchmark datasets like ConditionalQA and ShARC.

## Key Results
- Achieves new state-of-the-art results on ConditionalQA and ShARC datasets
- Outperforms existing prompting baselines by significant margins
- Surpasses supervised models when using GPT-3.5-Turbo or GPT-4
- Provides interpretable reasoning paths for users

## Why This Works (Mechanism)
Chain of Condition works by decomposing a complex conditional reasoning task into manageable sub-tasks that can be handled by specialized tools. Large language models excel at pattern recognition and text understanding needed for condition identification and verification, while Python interpreters provide precise logical computation. This division of labor allows the system to leverage the strengths of each component while avoiding their individual weaknesses. The approach also benefits from the inherent interpretability of logical expressions and the step-by-step nature of the decomposition.

## Foundational Learning

Condition identification
- Why needed: Documents contain implicit and explicit conditions that must be extracted for reasoning
- Quick check: Can the model correctly identify "if X then Y" type statements from text

Logical expression solving
- Why needed: Complex conditions require formal logical manipulation to determine outcomes
- Quick check: Does the Python interpreter correctly evaluate compound boolean expressions

Scenario verification
- Why needed: Conditions must be checked against specific user contexts
- Quick check: Can the model determine if "age > 18" holds for a given user scenario

## Architecture Onboarding

Component map:
LLM Condition Extractor -> LLM Scenario Verifier -> Python Interpreter -> Answer Generator

Critical path:
Document input → Condition extraction → Logical expression formation → Scenario verification → Expression solving → Answer generation

Design tradeoffs:
- LLM-based condition extraction vs rule-based parsing
- Python interpreter precision vs LLM-based logical reasoning
- Interpretability vs end-to-end model performance

Failure signatures:
- Incorrect condition extraction leading to wrong logical expressions
- Scenario verification errors when user context is ambiguous
- Python interpreter syntax errors from malformed expressions

First experiments:
1. Test condition extraction on simple if-then statements from medical documents
2. Verify scenario checking with clearly true/false user contexts
3. Evaluate Python expression solving with compound boolean logic

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on large language models raises concerns about scalability and cost-effectiveness
- Python interpreter may struggle with nuanced or ambiguous conditions in real-world scenarios
- Limited evaluation of interpretability and usability of reasoning paths for non-expert users

## Confidence

High confidence:
- Performance improvements on ConditionalQA and ShARC datasets
- Superiority over existing prompting baselines

Medium confidence:
- Claims about interpretability advantages
- Comparisons with supervised models

Low confidence:
- Scalability and cost-effectiveness for larger documents
- Handling of complex and nuanced conditions

## Next Checks

1. Conduct comprehensive evaluation of computational resources required, including LLM costs and processing time, comparing with existing methods to assess scalability for larger documents.

2. Perform user study to evaluate interpretability and usability of reasoning paths with participants of varying expertise levels, gathering feedback for improvements.

3. Extend evaluation to more diverse and challenging datasets with complex conditions and documents from different domains or languages to assess robustness and identify limitations.