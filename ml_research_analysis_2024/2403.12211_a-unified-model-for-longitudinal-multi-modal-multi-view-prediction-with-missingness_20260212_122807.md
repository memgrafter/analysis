---
ver: rpa2
title: A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness
arxiv_id: '2403.12211'
source_url: https://arxiv.org/abs/2403.12211
tags:
- data
- prediction
- view
- views
- unified
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified model for longitudinal multi-modal
  multi-view prediction with missing data, addressing challenges in medical record
  analysis where patients may lack data for certain timepoints or modalities. The
  method uses separate encoders for different views, a masked attention scheme to
  handle missing data, and a transformer decoder that can handle varying numbers of
  timepoints.
---

# A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness

## Quick Facts
- arXiv ID: 2403.12211
- Source URL: https://arxiv.org/abs/2403.12211
- Reference count: 33
- Key outcome: Unified model performs on par with view-specific models while offering flexibility in handling different input combinations and enabling post-hoc analysis of modality importance

## Executive Summary
This paper presents a unified model for predicting patient outcomes using longitudinal multi-modal multi-view data with missingness, specifically addressing challenges in medical record analysis where patients may lack data for certain timepoints or modalities. The method employs separate encoders for different views, a masked attention scheme to handle missing data, and a transformer decoder capable of handling varying numbers of timepoints. Evaluated on the OAI dataset for pain and Kellgren-Lawrence grade prediction, the unified model demonstrates performance comparable to view-specific models while providing greater flexibility and enabling post-hoc analysis to understand the importance of each modality and view for different prediction tasks.

## Method Summary
The unified model consists of separate encoders for each view (tabular data using SAINT, images using ResNet18), a masked attention block that dynamically combines features from available views by assigning very low attention scores to missing data, and a transformer decoder that makes predictions at each timepoint based on previous timepoints. The model is trained end-to-end using modality dropout to simulate missing views during training. During inference, the model can flexibly handle any combination of available views without requiring imputation of missing data, making it particularly suitable for real-world medical scenarios where data completeness varies across patients and timepoints.

## Key Results
- The unified model performs on par with view-specific models during evaluation while offering greater flexibility in handling different input combinations
- Including more timepoints generally improves prediction accuracy, demonstrating the benefit of longitudinal data for medical predictions
- Post-hoc analysis reveals the relative importance of each modality and view for different prediction tasks, enabling interpretability without retraining separate models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unified model can perform on par with view-specific models during evaluation by learning to dynamically attend to available views
- Mechanism: The masked attention scheme explicitly ignores missing views by assigning them a very low attention score (-1e9) multiplied by the mask indicator, allowing the model to learn view-specific encoders and flexibly combine them based on availability at inference time
- Core assumption: The attention mechanism can effectively learn which views are most important for a given task without requiring explicit imputation of missing data
- Evidence anchors:
  - [abstract]: "Our method allows as many timepoints as desired for input, and aims to leverage all available data, regardless of their availability"
  - [section 3.2]: "This allows us to manually assign a very low attention score δ = −1e9 to the missing view [19], which ensures that the summarized feature does not focus on the missing data"
  - [corpus]: Weak - no direct comparison to view-specific models in the corpus papers

### Mechanism 2
- Claim: Including more timepoints generally improves prediction accuracy by providing more longitudinal context
- Mechanism: The transformer decoder architecture can handle varying numbers of timepoints and learns to use information from all available timepoints to make predictions at each timepoint, capturing temporal patterns and disease progression over time
- Core assumption: Earlier timepoints contain relevant information for predicting future outcomes, and the model can effectively learn to use this temporal information
- Evidence anchors:
  - [abstract]: "We also show the benefit of having extended temporal data"
  - [section 4.4]: "Tab. 1 shows that in most cases, it is beneficial to include an increased number of timepoints, underlining the importance of longitudinal data in improving predictive accuracy"
  - [corpus]: Weak - the corpus papers focus on other aspects like missing data imputation rather than longitudinal benefits

### Mechanism 3
- Claim: The unified model can identify which views are most important for different prediction tasks through post-hoc analysis
- Mechanism: By systematically excluding one view at a time and observing the change in prediction scores, the model can determine which views are most crucial, leveraging the unified architecture to perform this analysis post-hoc without retraining separate models
- Core assumption: The model's performance degradation when excluding a view is a reliable indicator of that view's importance
- Evidence anchors:
  - [abstract]: "We conduct post-hoc analyses to assess the significance of each modality and view for different prediction tasks"
  - [section 4.4]: "We systematically exclude one at a time and observe the resulting change in prediction scores relative to the gold-standard labels"
  - [corpus]: Weak - no similar post-hoc analysis approach in the corpus papers

## Foundational Learning

- Concept: Multi-modal learning
  - Why needed here: The model needs to integrate information from different modalities (tabular, images, etc.) to make predictions about patient outcomes
  - Quick check question: What is the main advantage of using multiple modalities for medical predictions?

- Concept: Attention mechanisms
  - Why needed here: The model uses attention to dynamically combine features from different views and handle missing data
  - Quick check question: How does the masked attention mechanism handle missing views in this model?

- Concept: Transformer architectures
  - Why needed here: The model uses a transformer decoder to handle varying numbers of timepoints and make predictions based on previous timepoints only
  - Quick check question: Why is a transformer decoder particularly suited for handling longitudinal data with varying numbers of timepoints?

## Architecture Onboarding

- Component map: Feature extraction → View encoders → Masked attention summarization → Transformer decoder → Predictions
- Critical path: Input → View encoders → Masked attention summarization → Transformer decoder → Predictions
- Design tradeoffs:
  - Flexibility vs. complexity: The unified model can handle any combination of views but is more complex than view-specific models
  - Handling missing data vs. imputation: The model avoids imputation by using masking but may miss information from truly absent views
  - Single model vs. multiple models: A single unified model is more efficient but may not capture view-specific patterns as well
- Failure signatures:
  - If the model performs worse than view-specific models with all views available
  - If the model's performance is highly sensitive to which views are missing
  - If the model cannot effectively use temporal information from multiple timepoints
- First 3 experiments:
  1. Compare the unified model's performance to view-specific models when all views are available
  2. Test the model's ability to handle different combinations of missing views during evaluation
  3. Vary the number of timepoints used as input and measure the impact on prediction accuracy

## Open Questions the Paper Calls Out

- How does the proposed unified model perform on datasets with different types of missingness patterns (e.g., missing not at random, or non-ignorable missingness) compared to traditional imputation methods?
- What is the impact of incorporating additional modalities, such as text data from medical notes, on the model's predictive performance and interpretability?
- How does the unified model's performance scale with the number of timepoints and the frequency of data collection, particularly in cases where data is collected at irregular intervals?

## Limitations
- Claims about performance are supported primarily by results on a single dataset (OAI), limiting generalizability
- Core assumption that masked attention can effectively handle missing data without imputation is plausible but not extensively validated across diverse missingness patterns
- Post-hoc analysis approach for identifying important views may be influenced by model complexity and interactions between views

## Confidence

- High Confidence: The unified model architecture and its ability to handle varying numbers of timepoints and missing views are well-supported by the method description and experimental setup
- Medium Confidence: The claim that including more timepoints improves prediction accuracy is supported by the results but could be influenced by other factors such as data quality or specific characteristics of the OAI dataset
- Medium Confidence: The post-hoc analysis approach for identifying important views is methodologically sound but may be influenced by model complexity and interactions between views

## Next Checks
1. **Cross-dataset validation**: Evaluate the unified model on additional medical datasets with different characteristics (e.g., different diseases, imaging modalities, or missingness patterns) to assess generalizability
2. **Ablation study on attention mechanism**: Conduct an ablation study to quantify the impact of the masked attention scheme on handling missing data, comparing it to alternative approaches like imputation or simpler masking strategies
3. **Analysis of temporal patterns**: Perform a detailed analysis of how the model uses information from different timepoints, including visualizations of attention weights over time and tests on specific temporal patterns (e.g., gradual progression vs. sudden changes)