---
ver: rpa2
title: Online Continual Learning For Interactive Instruction Following Agents
arxiv_id: '2403.07548'
source_url: https://arxiv.org/abs/2403.07548
tags:
- learning
- logits
- agent
- task
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes two continual learning setups for embodied
  agents: Behavior Incremental Learning (Behavior-IL) and Environment Incremental
  Learning (Environment-IL). The authors argue that prior work on instruction-following
  agents assumes all training data is available upfront, which is unrealistic.'
---

# Online Continual Learning For Interactive Instruction Following Agents

## Quick Facts
- arXiv ID: 2403.07548
- Source URL: https://arxiv.org/abs/2403.07548
- Authors: Byeonghwi Kim; Minhyuk Seo; Jonghyun Choi
- Reference count: 36
- Key outcome: Proposes Confidence-Aware Moving Average (CAMA) for task-free continual learning in embodied instruction-following agents

## Executive Summary
This paper addresses catastrophic forgetting in embodied agents learning to follow instructions incrementally. Unlike prior work that assumes all training data is available upfront, the authors propose two continual learning setups: Behavior Incremental Learning (Behavior-IL) where agents learn new behaviors incrementally while preserving previous knowledge, and Environment Incremental Learning (Environment-IL) where agents learn behaviors in new environments while retaining knowledge of previous environments. To tackle forgetting, they introduce Confidence-Aware Moving Average (CAMA), which dynamically updates stored logits based on confidence scores without requiring task boundary information. CAMA outperforms prior continual learning methods on the ALFRED benchmark across multiple metrics and environments.

## Method Summary
The method proposes Confidence-Aware Moving Average (CAMA) for online continual learning in instruction-following agents. CAMA maintains episodic memory storing samples with their corresponding logits from previous tasks. During training, it updates these stored logits using a confidence-weighted moving average where coefficients are dynamically determined based on the agent's confidence scores on ground-truth labels. This approach enables task-free continual learning without explicit task boundary information. The method is evaluated on two incremental learning setups - Behavior-IL (learning new behaviors) and Environment-IL (learning behaviors in new environments) - using the ALFRED benchmark with frozen visual encoders and language models.

## Key Results
- CAMA outperforms all prior continual learning methods by noticeable margins across multiple metrics on ALFRED benchmark
- Achieves better performance than both rehearsal-based approaches (ER, MIR, CLIB) and regularization-based approach (EWC++) in both seen and unseen environments
- Demonstrates the importance of dynamically determining coefficients for updating stored logits rather than using fixed coefficients
- Shows effectiveness through qualitative examples where CAMA maintains performance across incremental learning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAMA updates stored logits using a confidence-weighted moving average to prevent them from becoming outdated.
- Mechanism: The method maintains a queue of recent confidence scores for each action and object class. It then dynamically computes coefficients (γa, γc) by averaging these scores and clipping them between 0 and 1, scaled by constants αa and αc < 1. These coefficients weight the combination of previous and current logits.
- Core assumption: Higher confidence scores indicate more accurate logits, so weighting by confidence preserves useful past knowledge while incorporating new information.
- Evidence anchors:
  - [abstract] "we propose to update them based on confidence scores without task boundary information during training (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA)."
  - [section 4.1] "we dynamically determine the moving average coefficients based on the classification confidence scores inferred by the agents as indicators of the 'quality' of the newly obtained logits"
- Break condition: If confidence scores do not correlate with accuracy (e.g., model overfits or underfits consistently), the weighting becomes ineffective and may degrade performance.

### Mechanism 2
- Claim: Using confidence scores avoids the need for task boundary information, enabling task-free continual learning.
- Mechanism: Instead of relying on explicit task IDs to update logits (as X-DER does), CAMA uses the agent's own confidence on ground-truth labels as a proxy for task relevance. This allows updates in a streaming setting without knowing when tasks change.
- Core assumption: The agent's confidence on ground-truth labels is a reliable proxy for task relevance even without explicit task boundaries.
- Evidence anchors:
  - [abstract] "we propose to update them based on confidence scores without task boundary information during training (i.e., task-free)"
  - [section 4.1] "we propose to update the stored logits using the agent's confidence scores indicating how the newly obtained logits for update contain accurate knowledge of the corresponding tasks"
- Break condition: If tasks are not well-separated or if confidence scores are unreliable, the proxy may fail and cause incorrect logit updates.

### Mechanism 3
- Claim: CAMA outperforms both regularization-based and rehearsal-based baselines by effectively balancing past and current knowledge.
- Mechanism: By updating logits rather than relying solely on parameter regularization (EWC++) or memory replay (ER, MIR, CLIB), CAMA directly corrects outdated logits while preserving useful information. This avoids catastrophic forgetting more effectively.
- Core assumption: Direct logit updating is more effective than penalizing parameter changes or replaying samples when data distributions shift rapidly.
- Evidence anchors:
  - [section 5.1] "we observe that our CAMA outperforms all rehearsal-based approaches... for all metrics in both seen and unseen environments"
  - [section 5.1] "we observe that our CAMA achieves better performance than the regularization-based approach (i.e., EWC++) with noticeable margins"
- Break condition: If the logit space is too high-dimensional or noisy, the moving average may not converge to useful representations.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper addresses forgetting when agents learn new behaviors or environments incrementally.
  - Quick check question: What happens to a neural network's performance on old tasks after training on new tasks without any mitigation strategy?

- Concept: Knowledge distillation and logit-based methods
  - Why needed here: CAMA stores and updates logits instead of full models to reduce memory and computation costs while retaining past knowledge.
  - Quick check question: Why might storing full model parameters be less practical than storing logits in continual learning?

- Concept: Confidence scores as quality indicators
  - Why needed here: CAMA uses confidence on ground-truth labels to weight logit updates, assuming higher confidence means more reliable information.
  - Quick check question: How does the model compute confidence scores, and why are they assumed to correlate with prediction accuracy?

## Architecture Onboarding

- Component map:
  Visual encoder (frozen) -> Language encoder (Bi-LSTM with self-attention) -> Action prediction module -> Object localization module -> Confidence score queues -> Episodic memory -> CAMA updater

- Critical path:
  1. Receive expert demonstration (x, y)
  2. Sample memory (x', y', z_old)
  3. Forward pass to get current logits z
  4. Compute joint loss (CE + distillation)
  5. Backward pass and update model parameters
  6. Maintain N recent confidence scores in queues
  7. Compute γ coefficients
  8. Update memory logits z_new = (1-γ)z_old + γz
  9. Store updated sample back in memory

- Design tradeoffs:
  - Fixed confidence queue size N vs. adaptive window: Fixed is simpler but may miss long-term trends.
  - αa, αc < 1 vs. =1: Scaling prevents complete forgetting but may slow adaptation.
  - Storing logits vs. full models: Saves memory but may lose some fine-grained knowledge.

- Failure signatures:
  - Confidence scores plateau early: Model stops learning new tasks effectively.
  - Memory logits become stale: Confidence-weighted average may not update enough.
  - High variance in performance across task orders: Shared classes cause order dependency.

- First 3 experiments:
  1. Run CAMA with αa=αc=0.99 and N=10 on Behavior-IL, compare to DER++ and X-DER.
  2. Test with αa=αc=1.0 (no scaling) to see forgetting vs. adaptation tradeoff.
  3. Vary N (5, 10, 20) to find optimal confidence window size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CAMA perform when tasks have significant overlap in behaviors or environments?
- Basis in paper: [explicit] The paper notes that performance may depend on task order due to shared actions and object classes between tasks. It mentions this could lead to forward transfer when tasks are ordered to learn shared components first.
- Why unresolved: The paper only tests disjoint task setups without overlap. Real-world scenarios often involve overlapping tasks.
- What evidence would resolve it: Evaluating CAMA in setups with overlapping tasks, such as blurry task configurations or Gaussian scheduled regimes, would show if performance degrades when tasks share components.

### Open Question 2
- Question: What is the impact of different confidence score window sizes (N) on CAMA's performance?
- Basis in paper: [explicit] The paper mentions using N recent confidence scores to reduce noise but does not explore the impact of varying N.
- Why unresolved: The optimal window size for balancing noise reduction and responsiveness to new information is not investigated.
- What evidence would resolve it: Systematic experiments varying N and measuring performance metrics would identify the optimal window size.

### Open Question 3
- Question: How does CAMA's performance scale with the number of tasks or environments in the incremental learning setup?
- Basis in paper: [explicit] The paper evaluates CAMA on setups with 7 behaviors and 4 environments but does not test scalability to larger numbers of tasks.
- Why unresolved: It's unclear if CAMA maintains its advantages as the number of tasks grows, which is critical for real-world applications.
- What evidence would resolve it: Testing CAMA on incremental learning setups with increasing numbers of tasks and environments while monitoring performance metrics would reveal scalability limits.

## Limitations

- The paper assumes well-separated task boundaries and evaluates in controlled experimental settings, which may not reflect real-world ambiguity in task definitions.
- Performance relies heavily on confidence scores being reliable indicators of accuracy, which may not hold in all scenarios, especially with noisy data or complex environments.
- The method uses frozen visual encoders and limited model updates, potentially constraining performance in domains requiring rapid adaptation to new visual concepts.

## Confidence

- **High confidence**: The proposed CAMA method's general framework and its superiority over baseline methods in the ALFRED benchmark.
- **Medium confidence**: The effectiveness of confidence scores as proxies for task relevance without explicit task boundaries.
- **Low confidence**: The method's scalability to more complex, real-world environments with less structured task definitions.

## Next Checks

1. **Validate confidence-score reliability**: Conduct experiments where confidence scores are systematically compared against actual prediction accuracy across diverse tasks to ensure the assumption holds.
2. **Test in more complex environments**: Evaluate CAMA on environments with dynamic changes, ambiguous task boundaries, or higher-dimensional state spaces to assess robustness.
3. **Analyze failure modes**: Perform detailed error analysis on cases where CAMA underperforms, focusing on scenarios where confidence scores fail or memory updates are ineffective.