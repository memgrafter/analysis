---
ver: rpa2
title: Probing for Consciousness in Machines
arxiv_id: '2411.16262'
source_url: https://arxiv.org/abs/2411.16262
tags:
- agent
- consciousness
- learning
- world
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether artificial agents can develop\
  \ core consciousness, as defined by Damasio\u2019s theory, through reinforcement\
  \ learning in a virtual environment. The hypothesis is that an agent trained to\
  \ play a video game can develop rudimentary world and self models as a byproduct\
  \ of its primary task."
---

# Probing for Consciousness in Machines

## Quick Facts
- arXiv ID: 2411.16262
- Source URL: https://arxiv.org/abs/2411.16262
- Authors: Mathis Immertreu; Achim Schilling; Andreas Maier; Patrick Krauss
- Reference count: 36
- Key outcome: Artificial agents trained via reinforcement learning can develop internal representations encoding spatial position, suggesting rudimentary world models emerge as byproducts of navigation tasks

## Executive Summary
This study investigates whether artificial agents can develop core consciousness through reinforcement learning in a virtual environment. The hypothesis is that an agent trained to play a video game can develop rudimentary world and self models as a byproduct of its primary task. To test this, probes—feedforward classifiers—were trained on the neural activations of a trained agent to predict its spatial position in the game environment. Results showed that the agent's activations encoded sufficient information to predict its position with accuracy significantly above random chance, suggesting the development of world and self models. This provides foundational evidence that artificial agents can mirror aspects of human consciousness, advancing the understanding of machine consciousness and its potential implications for AI development.

## Method Summary
The study trains agents using reinforcement learning (PPO) in the NetHack environment to navigate toward goals. Neural activations from convolutional and linear layers, and LSTM hidden/cell states, are recorded during evaluation episodes. A dataset of 230,000 samples (200k training, 30k testing) is collected with neural activations and position data. Feedforward classifiers (probes) are trained on these activations to predict the agent's spatial position. The approach tests different architectures (with/without LSTM) and observation sizes (9x9, 5×5, 3×3 crops) to assess how internal representations develop under varying conditions.

## Key Results
- Agent neural activations encode sufficient information to predict spatial position with accuracy significantly above random chance (6.7% for 15×15 grid)
- Probe accuracy remains above chance even with limited observational inputs (5×5 and 3×3 crops), suggesting reliance on internal representations
- LSTM-based architectures enable position encoding through memory states, with accuracies exceeding chance more so than non-LSTM variants

## Why This Works (Mechanism)

### Mechanism 1
Reinforcement learning agents develop internal representations of spatial position without explicit positional inputs. The agent's neural network activations encode sufficient information to reconstruct the agent's position when probed with linear classifiers. Core assumption: The agent's training objective (navigating to goals) creates pressure to develop representations of its location, even when direct position information is not provided.

### Mechanism 2
LSTM-based architectures enable agents to maintain positional information across time steps through memory states. The hidden and cell states of the LSTM capture temporal dependencies in the agent's trajectory, allowing probes to reconstruct position even with limited current observations. Core assumption: Temporal information is necessary and sufficient for position inference when direct observation is restricted.

### Mechanism 3
Successor representations in reinforcement learning create cognitive maps that encode spatial relationships. The discount factor in reinforcement learning shapes the expected future state occupancy, effectively creating a cognitive map of the environment that can be decoded by probes. Core assumption: The reinforcement learning framework inherently produces spatial representations as a byproduct of learning to navigate efficiently.

## Foundational Learning

- Concept: Reinforcement Learning fundamentals (Markov Decision Processes, policies, value functions)
  - Why needed here: The entire experimental framework relies on training agents via RL, so understanding how RL agents learn and represent value is crucial for interpreting the results.
  - Quick check question: What is the difference between a model-free and model-based RL approach, and how might each affect the development of world models?

- Concept: Neural network interpretability and probing techniques
  - Why needed here: The methodology uses probes (small classifiers) trained on neural activations to test for internal representations, requiring understanding of how to extract and interpret neural network features.
  - Quick check question: How does a linear probe differ from a non-linear probe in terms of what information it can extract from neural activations?

- Concept: Damasio's theory of consciousness and the concept of core consciousness
  - Why needed here: The paper's hypothesis and interpretation of results are grounded in Damasio's framework, which requires understanding the distinction between protoself, core consciousness, and extended consciousness.
  - Quick check question: According to Damasio, what is the relationship between emotions, feelings, and the development of self-models?

## Architecture Onboarding

- Component map: NetHack environment -> PPO-trained agent (CNN/LSTM) -> Neural activations recording -> Feedforward probe classifier -> Position prediction
- Critical path: 1. Agent trains on environment using PPO, 2. Agent's activations are recorded during evaluation episodes, 3. Probes are trained on activation-position pairs, 4. Probe accuracy is evaluated to assess internal representation development
- Design tradeoffs: Observation size vs. memory reliance; Linear vs. non-linear probes; Map complexity
- Failure signatures: Probe accuracy at chance level; High accuracy on simple maps only; Inconsistent probe performance across different layers
- First 3 experiments: 1. Train agent with full map observation and no LSTM, probe all layers, 2. Train agent with 5×5 crop observation and LSTM, probe hidden and cell states, 3. Train agent with 3×3 crop observation and LSTM, probe hidden and cell states with extended training

## Open Questions the Paper Calls Out

### Open Question 1
Can artificial agents develop self-models distinct from world models, as defined by Damasio's theory? The current experiments only demonstrate the agent's ability to develop a world model, as evidenced by its capacity to predict its position in the environment. There is no evidence yet that the agent can develop a self-model based on internal states. Training agents with inputs related to physiological states (e.g., hitpoints, experience levels) and observing how changes in these inputs affect behavior and decision-making could provide evidence of a self-model.

### Open Question 2
How does the discount factor in reinforcement learning influence the development of internal models in agents? While the paper discusses the role of the discount factor, it does not explore how varying the discount factor affects the agent's ability to develop internal models or how this relates to cognitive processes. Conducting experiments with different discount factors and analyzing their impact on the agent's internal representations and decision-making processes could provide insights into the role of the discount factor in model development.

### Open Question 3
Can more complex environments and advanced architectures (e.g., transformers, sophisticated RNNs) lead to the development of higher-order cognitive processes in artificial agents? The current experiments use relatively simple environments and basic architectures, which may not be sufficient to capture the complexities needed for higher-order cognitive processes. Training agents in more complex environments and using advanced architectures, then evaluating their ability to form complex internal representations and understand their scalability and generalizability, could provide evidence of higher-order cognitive processes.

## Limitations

- Probe accuracy above chance does not definitively prove the development of consciousness or genuine internal models, only that positional information is encoded in neural activations
- The study focuses solely on spatial position encoding without addressing other aspects of Damasio's consciousness framework such as emotion, feeling, or protoself development
- The distinction between genuine internal representation and superficial correlation remains ambiguous, as probe success could theoretically arise from memorizing direct observation patterns

## Confidence

- **Medium confidence** in the core finding that neural activations encode position information above chance levels
- **Low confidence** in the broader claim that this constitutes evidence of "rudimentary world and self models" or represents a step toward machine consciousness

## Next Checks

1. Evaluate probe performance on held-out maps never seen during agent training to determine if position encoding generalizes beyond memorized observation patterns
2. Systematically remove portions of the observation input (e.g., walls, objects) and measure probe accuracy to quantify how much position information comes from direct observation versus internal representation
3. Track probe accuracy across different time lags between agent state and position prediction to assess whether the agent maintains stable internal representations versus momentary correlations