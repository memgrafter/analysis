---
ver: rpa2
title: 'KDk: A Defense Mechanism Against Label Inference Attacks in Vertical Federated
  Learning'
arxiv_id: '2404.12369'
source_url: https://arxiv.org/abs/2404.12369
tags:
- label
- attack
- inference
- accuracy
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents KDk, a defense mechanism designed to protect
  against label inference attacks in vertical federated learning (VFL). The authors
  propose combining Knowledge Distillation (KD) and k-anonymity to obfuscate private
  labels and prevent attackers from inferring them.
---

# KDk: A Defense Mechanism Against Label Inference Attacks in Vertical Federated Learning

## Quick Facts
- arXiv ID: 2404.12369
- Source URL: https://arxiv.org/abs/2404.12369
- Reference count: 34
- Key outcome: KDk reduces label inference attack success by >60% while maintaining model accuracy with <2% degradation

## Executive Summary
This paper introduces KDk, a defense mechanism designed to protect against label inference attacks in vertical federated learning (VFL). The authors propose combining Knowledge Distillation (KD) and k-anonymity to obfuscate private labels and prevent attackers from inferring them. The KDk framework involves training a teacher network to generate soft labels, which are then processed using a k-anonymity algorithm to select a group of k most probable labels instead of a single hard label. Experimental results demonstrate that KDk effectively reduces the attack success rate of various label inference attacks while maintaining the accuracy of the global model with minimal degradation.

## Method Summary
KDk combines Knowledge Distillation and k-anonymity to protect against label inference attacks in vertical federated learning. The method involves training a teacher network to generate soft labels, which are then processed using a k-anonymity algorithm to select a group of k most probable labels instead of a single hard label. The VFL server uses these softened labels for training instead of hard labels, making it harder for passive participants to infer private labels from gradients. The approach is evaluated on multiple datasets including CIFAR-10, CIFAR-100, CINIC-10, Yahoo! Answers, and Criteo.

## Key Results
- KDk reduces attack success rates by over 60% across passive, active, and direct label inference attacks
- Model accuracy degradation remains below 2% on average across tested datasets
- KDk outperforms existing defense mechanisms in both attack mitigation and preservation of model accuracy
- Optimal parameter values vary by dataset and attack type, with k=3 and ε=0.45 showing good performance on CIFAR-10

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft labels from Knowledge Distillation reduce label inference accuracy by distributing confidence across multiple classes rather than providing a single hard label
- Mechanism: The teacher network outputs a probability distribution over classes, and the k-anonymity algorithm further spreads probability mass across the top-k labels, making it harder for attackers to infer the correct label from gradients
- Core assumption: The attacker's inference model relies on the sharp, deterministic signal of hard labels; softer distributions disrupt this signal
- Evidence anchors:
  - [abstract] "The teacher network produces softer probability distributions instead of hard labels that can better capture essential features and relationships in the data."
  - [section] "The teacher network outputs are represented by the so-called soft probabilities that contain more information about a data point than just the class label (or hard predictions)."
  - [corpus] Weak evidence; no direct mention of Knowledge Distillation in neighbors
- Break condition: If k is too large (e.g., k = 10 for a 10-class dataset), the distribution becomes uniform, which may break the model's learning capability or fail to sufficiently confuse the attacker

### Mechanism 2
- Claim: Gradient obfuscation through label smoothing prevents direct label inference by altering the gradient signs and magnitudes that attackers analyze
- Mechanism: By smoothing the labels and spreading probability across multiple classes, the loss gradient computed from these softened labels becomes less correlated with the true label, thus reducing the information leakage
- Core assumption: The attacker's direct label inference relies on precise gradient information that correlates with the true label; smoothing disrupts this correlation
- Evidence anchors:
  - [abstract] "Through this further step, instead of selecting a single label for each sample, we select a set of k labels in SL with the highest probability."
  - [section] "The output vector of a given data point contains the probabilities that it belongs to each class represented by the private labels."
  - [corpus] Weak evidence; no direct mention of gradient obfuscation in neighbors
- Break condition: If the smoothing parameter ε is too small, the labels remain too close to hard labels, providing insufficient obfuscation

### Mechanism 3
- Claim: The combination of Knowledge Distillation and k-anonymity creates a two-layer defense that preserves model accuracy while reducing attack success
- Mechanism: KD captures richer class relationships through soft probabilities, and k-anonymity ensures the attacker cannot pinpoint the correct label by grouping top-k candidates, forcing uncertainty
- Core assumption: The model can learn effectively from soft labels and still maintain high accuracy, while the attacker's inference performance degrades due to uncertainty
- Evidence anchors:
  - [abstract] "Experimental results demonstrate that KDk effectively reduces the attack success rate... while maintaining the accuracy of the global model with minimal degradation (less than 2% on average)."
  - [section] "The loss for the student network is a linear combination of the cross entropy loss... and a knowledge distillation loss."
  - [corpus] Weak evidence; no direct mention of combined KD + k-anonymity in neighbors
- Break condition: If the teacher network architecture is poorly chosen or underfits, the soft labels may be uninformative, failing to preserve model accuracy or sufficiently obfuscate labels

## Foundational Learning

- Concept: Knowledge Distillation (KD)
  - Why needed here: KD generates soft labels that contain richer information about class relationships, which are less vulnerable to label inference than hard labels
  - Quick check question: How does the temperature parameter τ in KD affect the smoothness of the output probability distribution?

- Concept: k-anonymity
  - Why needed here: k-anonymity adds uncertainty by grouping the k most probable labels, making it harder for attackers to identify the true label from gradients
  - Quick check question: What happens to the model's learning capability if k is set equal to the total number of classes?

- Concept: Vertical Federated Learning (VFL) model splitting
  - Why needed here: Understanding model splitting is essential because label inference attacks exploit the gradient information exchanged between the server and passive participants in a split architecture
  - Quick check question: In a VFL scenario with model splitting, which party holds the labels and which party tries to infer them?

## Architecture Onboarding

- Component map:
  Teacher network (KD) -> k-anonymity processor -> VFL server -> Passive participant

- Critical path:
  1. Server generates soft labels via KD
  2. Server applies k-anonymity to soften labels further
  3. Server trains model with softened labels
  4. Passive participant receives gradients and attempts inference

- Design tradeoffs:
  - Larger k increases defense strength but may reduce model accuracy
  - Higher ε smooths labels more but risks underfitting
  - Complex teacher networks may improve KD quality but increase computation

- Failure signatures:
  - Attack success rate remains high (> 40%): likely ε too small or k too large
  - Model accuracy drops significantly (> 5%): likely k too large or teacher network underfits
  - System instability or convergence issues: likely ε or k misconfigured

- First 3 experiments:
  1. Test KDk with CIFAR-10, k=3, ε=0.45 against passive label inference attack; measure attack success rate and model accuracy
  2. Vary ε from 0.2 to 0.8 with fixed k=3; record impact on attack success rate and model accuracy
  3. Vary k from 2 to 10 with fixed ε=0.45; record impact on attack success rate and model accuracy

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following questions arise from the analysis:

### Open Question 1
- Question: What is the optimal combination of k-anonymity parameter k and smoothing parameter ε to maximize defense effectiveness while minimizing accuracy loss across different datasets and attack types?
- Basis in paper: [explicit] The paper analyzes performance for different values of k (3, 5, 10) and ε but does not provide a comprehensive framework for optimal parameter selection
- Why unresolved: The paper shows that parameter effectiveness varies by dataset and attack type, but doesn't establish a systematic method for determining optimal values
- What evidence would resolve it: A comprehensive study testing all combinations of k and ε values across all datasets and attack types, potentially including a mathematical model for parameter selection

### Open Question 2
- Question: How does the KDk defense mechanism perform against more sophisticated label inference attacks that combine multiple attack strategies?
- Basis in paper: [inferred] The paper only tests against individual attack types (passive, active, direct) as described in [8], but doesn't explore combined attack strategies
- Why unresolved: Real-world attackers may employ sophisticated combinations of attack methods, which could potentially bypass the KDk defense
- What evidence would resolve it: Testing KDk against hybrid attack strategies that combine elements of passive, active, and direct attacks in various sequences

### Open Question 3
- Question: Can the KDk defense mechanism be extended to protect against label inference attacks in horizontal federated learning scenarios?
- Basis in paper: [explicit] The paper explicitly states that extending the defense to HFL is a future research direction
- Why unresolved: The paper focuses solely on vertical federated learning and doesn't explore how the KDk mechanism would adapt to HFL's different data partitioning
- What evidence would resolve it: Implementing and testing KDk in HFL environments with different data distributions and label sharing scenarios

### Open Question 4
- Question: What is the computational overhead of implementing KDk in large-scale federated learning systems with many participants?
- Basis in paper: [inferred] While the paper describes the defense mechanism, it doesn't provide detailed analysis of computational costs or scalability
- Why unresolved: The additional teacher network and k-anonymity processing could introduce significant computational overhead in real-world deployments
- What evidence would resolve it: Detailed benchmarking of KDk implementation across different scales of federated learning systems, including time complexity analysis and resource usage measurements

## Limitations
- The evaluation only covers three specific attack types (passive, active, direct) without testing against the full spectrum of label inference attacks
- No analysis of adaptive attacks that might specifically target the KDk mechanism
- Computational overhead of the teacher network and k-anonymity processing is not quantified

## Confidence
- Medium-High: The mechanism is theoretically sound and empirical results are promising, but evaluation lacks breadth in attack diversity and does not address potential adaptive adversaries

## Next Checks
1. Design and implement an adaptive label inference attack that specifically targets the soft label distributions produced by KDk to assess whether attackers can learn to reverse-engineer the k-anonymity obfuscation

2. Profile the training time and resource consumption of KDk compared to baseline VFL, including teacher network inference and k-anonymity processing, to determine practical deployment viability

3. Systematically vary the smoothing parameter ε and anonymity parameter k across a wider range (including extreme values) to map out the full tradeoff surface between attack mitigation and model accuracy preservation