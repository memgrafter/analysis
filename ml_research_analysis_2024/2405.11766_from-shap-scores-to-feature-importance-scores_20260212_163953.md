---
ver: rpa2
title: From SHAP Scores to Feature Importance Scores
arxiv_id: '2405.11766'
source_url: https://arxiv.org/abs/2405.11766
tags:
- feature
- power
- scores
- fiss
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper bridges XAI feature attribution with a priori voting
  power theory, showing that recently proposed axiomatic aggregations (Deegan-Packel,
  Holler-Packel, Responsibility) are instantiations of known power indices. It formalizes
  Feature Importance Scores (FISs) parameterized by explanation problems and characteristic
  functions, enabling systematic study of power indices in XAI.
---

# From SHAP Scores to Feature Importance Scores

## Quick Facts
- arXiv ID: 2405.11766
- Source URL: https://arxiv.org/abs/2405.11766
- Authors: Olivier Letoffe; Xuanxiang Huang; Nicholas Asher; Joao Marques-Silva
- Reference count: 15
- The paper bridges XAI feature attribution with voting power theory, showing that axiomatic aggregations like Deegan-Packel, Holler-Packel, and Responsibility are instantiations of known power indices, and proposes novel properties for Feature Importance Scores (FISs).

## Executive Summary
This paper establishes a fundamental connection between explainable AI (XAI) feature attribution and a priori voting power theory, demonstrating that recently proposed axiomatic aggregations of feature sets are special cases of established power indices like Deegan-Packel, Holler-Packel, and Responsibility. By formalizing Feature Importance Scores (FISs) as parameterized by explanation problems and characteristic functions, the authors enable systematic study of power indices in XAI contexts. The work proposes novel properties specifically for XAI, including independence from class labeling, consistency with relevancy, and consistency with duality, and shows that Shapley-Shubik and Banzhaf indices exhibit the most desirable properties while others have limitations. The framework provides theoretical foundations for selecting appropriate FISs in XAI and reveals connections between logic-based abduction, voting power, and explainability.

## Method Summary
The paper formalizes Feature Importance Scores (FISs) by parameterizing them on explanation problems and characteristic functions, then instantiates template scores with various characteristic functions to create different FISs. The method involves defining explanation problems, selecting appropriate characteristic functions that capture feature criticality, instantiating template scores (like Shapley-Shubik and Banzhaf), and computing FISs while evaluating them against novel properties specific to XAI contexts. The framework connects feature attribution to cooperative game structures where power indices naturally measure feature criticality.

## Key Results
- Shows that axiomatic aggregations (Deegan-Packel, Holler-Packel, Responsibility) are instantiations of known power indices
- Introduces novel FISs parameterized by explanation problems and characteristic functions
- Proposes novel properties for FISs including independence from class labeling, consistency with relevancy, and consistency with duality
- Demonstrates that Shapley-Shubik and Banzhaf indices exhibit the most desirable properties
- Introduces a coverage-based FIS that offers an alternative to SHAP scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework bridges XAI feature attribution with a priori voting power theory by parameterizing FISs on explanation problems and characteristic functions.
- Mechanism: By treating feature sets as coalitions and predictions as winning outcomes, the paper maps XAI problems into cooperative game structures where power indices naturally measure feature criticality.
- Core assumption: Feature attribution in XAI can be formalized as a voting game where feature sets represent coalitions and prediction sufficiency represents winning conditions.
- Evidence anchors:
  - [abstract]: "This paper shows that there is an essential relationship between feature attribution and a priori voting power"
  - [section 4.1]: Formal definition of Template Scores using characteristic functions parameterized on explanation problems
  - [corpus]: Weak evidence - corpus lacks direct citations but shows related work on SHAP and Banzhaf indices
- Break condition: If the prediction function cannot be reasonably modeled as a voting game (e.g., non-monotonic relationships), the power index analogy breaks down.

### Mechanism 2
- Claim: Novel FISs can be created by selecting appropriate characteristic functions that capture feature criticality rather than expected contributions.
- Mechanism: The paper introduces characteristic functions like υw(S; E) = ITE(WAXp(S; E), 1, 0) that measure whether features are critical for prediction, enabling derivation of Shapley-Shubik and Banzhaf based FISs that avoid SHAP's known limitations.
- Core assumption: Feature criticality (being necessary for prediction) is a more appropriate measure than expected contributions for XAI.
- Evidence anchors:
  - [abstract]: "Recent work targeted rigorous feature attribution, by studying axiomatic aggregations of features based on logic-based definitions of explanations by feature selection"
  - [section 4.3]: Shows how existing axiomatic aggregations are special cases of Deegan-Packel and Holler-Packel indices
  - [corpus]: Moderate evidence - related papers discuss rigorous feature attribution and SHAP limitations
- Break condition: If computing WAXp or WCXp becomes intractable for the target ML model, the characteristic function approach loses practical utility.

### Mechanism 3
- Claim: The proposed properties framework enables systematic evaluation and selection of appropriate FISs for different XAI contexts.
- Mechanism: By defining properties like independence from class labeling, consistency with relevancy, and consistency with duality, the paper provides a principled way to characterize and compare FISs beyond empirical performance.
- Core assumption: XAI requires different properties than traditional voting power indices due to the explanatory nature of the task.
- Evidence anchors:
  - [abstract]: "This paper proposes novel desirable properties that FISs should exhibit"
  - [section 5]: Detailed presentation of novel properties specifically targeting XAI contexts
  - [corpus]: Weak evidence - corpus lacks papers discussing systematic FIS properties
- Break condition: If the properties conflict or are impossible to satisfy simultaneously, the framework becomes impractical for guiding FIS selection.

## Foundational Learning

- Concept: Cooperative game theory and Shapley values
  - Why needed here: The paper builds on Shapley values as the foundation for feature attribution, requiring understanding of how they measure player contributions in cooperative games
  - Quick check question: How do Shapley values differ from simple marginal contributions in cooperative games?

- Concept: Logic-based abductive explanations
  - Why needed here: The framework uses abductive explanations (AXp, WAXp) as the logical foundation for defining feature importance
  - Quick check question: What is the relationship between hitting sets of contrastive explanations and abductive explanations?

- Concept: Power indices in voting theory
  - Why needed here: The paper maps XAI problems to voting games where power indices measure feature importance, requiring understanding of indices like Shapley-Shubik, Banzhaf, Deegan-Packel, etc.
  - Quick check question: What distinguishes the Shapley-Shubik index from the Banzhaf index in terms of coalition ordering?

## Architecture Onboarding

- Component map: Explanation problem → Characteristic function → Template score → FIS computation → Property evaluation
- Critical path: Explanation problem → Characteristic function → Template score → FIS computation → Property verification
- Design tradeoffs: The framework trades computational complexity (analyzing all feature subsets) for rigorous theoretical grounding in voting power theory
- Failure signatures: Inadequate FISs show either (1) poor property satisfaction (e.g., failing independence from class labeling) or (2) computational intractability for the target model
- First 3 experiments:
  1. Implement ScS and ScB FISs on a simple boolean classifier and verify property satisfaction
  2. Compare ScE vs ScS on a known problematic example where SHAP fails
  3. Evaluate the coverage-based ScV FIS on a dataset with known feature relevancies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristic functions would produce the most satisfactory FISs across diverse ML models and domains?
- Basis in paper: [explicit] The paper identifies that current SHAP scores use a problematic characteristic function (expected value) that produces unsatisfactory results, and proposes several alternative characteristic functions (υe, υm, υw, υa) but notes that the optimal choice depends on the target explanation problem and ML model.
- Why unresolved: The paper characterizes different FISs in terms of their properties but does not definitively identify which characteristic function works best across all scenarios. The choice appears to be problem-dependent.
- What evidence would resolve it: Systematic empirical evaluation comparing FISs computed with different characteristic functions across multiple ML model types, prediction tasks, and domains, measuring both theoretical property satisfaction and practical explainability effectiveness.

### Open Question 2
- Question: Can we develop efficient approximation algorithms for computing FISs that maintain their desirable properties while reducing computational complexity?
- Basis in paper: [explicit] The paper notes that some FISs (like Shapley-Shubik and Banzhaf) may require analyzing exponentially many weak abductive explanations, whereas others (like Deegan-Packel or Holler-Packel) only require analyzing all sets of abductive explanations. It also mentions this as a future research direction.
- Why unresolved: The paper characterizes properties of different FISs but does not address computational complexity or approximation methods. Exact computation may be intractable for large feature spaces.
- What evidence would resolve it: Development and validation of approximation algorithms for FIS computation that provably preserve key properties (like duality, consistency with relevancy) while achieving polynomial-time complexity, tested on real-world ML models.

### Open Question 3
- Question: How can we systematically characterize the relationship between FIS properties and their practical utility for human decision-makers in different application domains?
- Basis in paper: [inferred] The paper proposes novel properties for FISs (independence from class labeling, consistency with relevancy, consistency with duality) and characterizes which FISs exhibit which properties, but does not empirically validate how these theoretical properties translate to practical explainability utility.
- Why unresolved: The paper provides theoretical foundations and property analysis but does not connect these properties to actual human understanding or decision-making effectiveness in specific domains like healthcare, finance, or autonomous systems.
- What evidence would resolve it: User studies and domain-specific evaluations measuring how well different FISs with various property combinations help human experts understand model decisions, make better decisions, and detect potential errors in different application contexts.

## Limitations
- The framework requires analyzing all feature subsets, which may be computationally intractable for high-dimensional data
- The theoretical properties, while rigorously defined, need more empirical validation to demonstrate practical utility
- The optimal characteristic function choice depends on the specific explanation problem and ML model, lacking universal recommendations

## Confidence
- High confidence: The formal relationship between feature attribution and voting power theory is rigorously established
- Medium confidence: The proposed properties framework provides a systematic evaluation method, though their relative importance may vary by application
- Medium confidence: The coverage-based FIS (ScV) is novel, but its practical advantages over existing methods require more empirical validation

## Next Checks
1. Implement a comparative study of ScS vs ScB vs ScE on real-world ML models where SHAP is known to fail (e.g., correlated features)
2. Test the independence from class labeling property on a multi-class classification problem with imbalanced classes
3. Evaluate the computational complexity of the coverage-based ScV FIS on datasets with >100 features to verify scalability claims