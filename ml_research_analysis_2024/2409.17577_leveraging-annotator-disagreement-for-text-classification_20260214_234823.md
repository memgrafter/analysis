---
ver: rpa2
title: Leveraging Annotator Disagreement for Text Classification
arxiv_id: '2409.17577'
source_url: https://arxiv.org/abs/2409.17577
tags:
- dataset
- speech
- hate
- annotators
- abusive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the issue of ignoring annotator disagreement
  in text classification by proposing three methods to leverage diverse annotations:
  a probability-based multi-label method, an ensemble system, and instruction tuning.
  These approaches were evaluated on hate speech detection and abusive conversation
  detection tasks using datasets with multiple annotators.'
---

# Leveraging Annotator Disagreement for Text Classification

## Quick Facts
- arXiv ID: 2409.17577
- Source URL: https://arxiv.org/abs/2409.17577
- Authors: Jin Xu; MariÃ«t Theune; Daniel Braun
- Reference count: 14
- Primary result: Multi-label approach achieved cross entropy of 0.7638 on hate speech detection, outperforming baseline and other methods

## Executive Summary
This paper addresses the limitation of ignoring annotator disagreement in text classification by proposing three methods to leverage diverse annotations. The authors develop a probability-based multi-label approach, an ensemble system, and instruction tuning to train models on individual annotator labels rather than majority votes. Evaluated on hate speech detection and abusive conversation detection tasks, the multi-label method showed superior performance, particularly on the hate speech dataset where it achieved a cross entropy of 0.7638. An online survey confirmed that outputs from the multi-label model were considered more reasonable than those from a baseline model trained on majority labels.

## Method Summary
The paper proposes three strategies to leverage annotator disagreement: (1) a probability-based multi-label approach that trains models to predict the distribution of annotator labels rather than a single majority label, (2) an ensemble system consisting of multiple sub-models each trained on individual annotator labels, and (3) instruction tuning that incorporates explicit guidance into the training process. The methods were evaluated on two datasets - hate speech detection (100k tweets, binary labels) and abusive conversation detection (2501 samples, 5 severity levels) - using cross entropy as the primary metric. The probability-based multi-label approach used BERT fine-tuned on label distributions, while the ensemble system combined predictions from multiple BERT models trained on individual annotator labels, and instruction tuning used LLaMa 2 fine-tuned with prompts containing annotator-specific instructions.

## Key Results
- Multi-label approach achieved cross entropy of 0.7638 on hate speech detection, outperforming baseline (0.7971) and other methods
- Instruction tuning performed best on abusive conversation detection with cross entropy of 0.6200
- Online survey of 30 participants showed that 70% preferred probability distributions from multi-label model over baseline for hate speech dataset
- Cross entropy improvements were modest (4% relative improvement) but consistent across validation and test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling annotation disagreements as probability distributions across labels improves model performance on subjective tasks.
- Mechanism: Instead of collapsing multiple annotations to a single majority label, the model is trained to predict the distribution of annotator judgments, capturing the uncertainty and diversity of perspectives.
- Core assumption: The distribution of annotator labels contains meaningful information about the ambiguity or subjectivity of the text, not just noise to be eliminated.
- Evidence anchors:
  - [abstract] "outputs from the multi-label models are considered a better representation of the texts than the single-label model"
  - [section] "we propose three different strategies to leverage annotator disagreement during the training of text classification models: a probability-based multi-label approach"
  - [corpus] Found 25 related papers discussing modeling annotator disagreement; average FMR=0.419 suggests moderate relevance
- Break condition: If the underlying annotator disagreement is random noise rather than meaningful variation, modeling distributions will hurt rather than help performance.

### Mechanism 2
- Claim: Ensemble systems that simulate multiple annotators can capture diverse viewpoints and improve generalization.
- Mechanism: Multiple sub-models are trained on different annotator labels, then their predictions are combined to form a final probability distribution, mimicking the annotation process itself.
- Core assumption: Individual annotators have systematic biases or perspectives that can be learned by separate models and combined for richer predictions.
- Evidence anchors:
  - [abstract] "we propose an ensemble system consisting of several sub-models"
  - [section] "the ensemble system integrates the unique insights from each individual annotator, as represented by the sub-models"
  - [corpus] 5 related papers discuss ensemble approaches to modeling annotator disagreement; moderate evidence of effectiveness
- Break condition: If annotators are truly anonymous or their individual perspectives cannot be reliably learned, the ensemble approach loses its advantage.

### Mechanism 3
- Claim: Instruction tuning allows models to learn specific patterns from limited data by incorporating explicit guidance.
- Mechanism: A pre-trained generative model is fine-tuned with instruction-based prompts that explicitly ask for class predictions based on individual annotator labels, rather than majority labels.
- Core assumption: Even with limited training data, instruction-based fine-tuning can effectively capture task-specific patterns and handle annotator diversity.
- Evidence anchors:
  - [abstract] "we use instruction tuning...inject explicit guidance into the training process"
  - [section] "instruction tuning does not have a high requirement for dataset size"
  - [corpus] Weak evidence; only 1 paper discusses instruction tuning for disagreement handling
- Break condition: If the instruction prompts are poorly designed or the model cannot generalize from limited data, performance will suffer.

## Foundational Learning

- Concept: Cross entropy as a measure of distribution similarity
  - Why needed here: The output is a probability distribution, so traditional accuracy metrics don't apply; cross entropy measures how well the model's predicted distribution matches the annotation distribution.
  - Quick check question: If a model predicts [0.7, 0.3] for a true distribution [1.0, 0.0], is the cross entropy low or high?

- Concept: Soft labels vs hard labels in training
  - Why needed here: The approach uses soft labels (probability distributions) instead of hard labels (single class), requiring different loss functions and evaluation metrics.
  - Quick check question: What's the difference between training with [1.0, 0.0] versus [0.7, 0.3] as target labels?

- Concept: Ensemble learning and model averaging
  - Why needed here: The ensemble system approach relies on combining multiple models, requiring understanding of when and how ensemble methods improve performance.
  - Quick check question: Under what conditions does an ensemble of models typically outperform individual models?

## Architecture Onboarding

- Component map: Data preprocessing -> Model variants (multi-label, ensemble, instruction tuning) -> Training pipeline -> Evaluation (cross entropy, survey) -> Output layer (probability distributions)

- Critical path: 1. Load dataset with multiple annotator labels per sample 2. Convert annotator labels to probability distributions 3. Train chosen model variant on these distributions 4. Evaluate using cross entropy on validation/test sets 5. Compare results across approaches

- Design tradeoffs:
  - Multi-label: Simple to implement, works well with sufficient data, but may overfit on small datasets
  - Ensemble: Captures annotator-specific patterns, good for known annotators, but complex with anonymous annotators
  - Instruction tuning: Works with limited data, but sensitive to prompt quality and requires careful prompt design

- Failure signatures:
  - Overfitting: Training loss much lower than validation/test loss
  - Underfitting: All losses remain high across training, validation, and test
  - Poor convergence: Loss plateaus early or fluctuates significantly
  - Label space mismatch: Cross entropy unexpectedly high for certain classes

- First 3 experiments:
  1. Implement and evaluate the probability-based multi-label model on the hate speech dataset to establish baseline performance
  2. Compare ensemble system performance when annotators are known vs. anonymous to understand limitations
  3. Test instruction tuning with different prompt formulations to identify optimal prompt structure for the abusive conversation dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed multi-label method compare to traditional approaches that aggregate annotations into a single "ground truth" label in terms of both model performance and human perception?
- Basis in paper: [explicit] The paper compares three approaches (multi-label, ensemble system, and instruction tuning) against a baseline model trained on majority labels, and conducts an online survey to evaluate human preference for multi-label model outputs.
- Why unresolved: While the paper provides initial comparisons and survey results, further investigation is needed to determine the generalizability of these findings across different datasets, tasks, and annotation schemes.
- What evidence would resolve it: Additional experiments on diverse datasets and tasks, as well as larger-scale surveys with varied participant demographics, would provide a more comprehensive understanding of the relative merits of multi-label approaches.

### Open Question 2
- Question: How can we effectively handle class imbalance and annotator inconsistency in datasets with multiple annotators?
- Basis in paper: [inferred] The paper acknowledges that the datasets used suffer from class imbalance and annotator inconsistency, which can negatively impact model performance.
- Why unresolved: The paper does not provide specific solutions or techniques to address these issues, leaving room for further research and development.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of various techniques, such as oversampling, undersampling, or advanced loss functions, in mitigating the impact of class imbalance and annotator inconsistency would be valuable.

### Open Question 3
- Question: How can we automatically generate effective prompts for instruction tuning to avoid the limitations of manually created prompts?
- Basis in paper: [explicit] The paper mentions that manually created prompts can introduce subjectivity and bias, and that they may not always be the most effective way to instruct model behavior.
- Why unresolved: The paper does not explore methods for automatically generating prompts, leaving this as an open area for future research.
- What evidence would resolve it: Research demonstrating the development and evaluation of algorithms or techniques for automatically generating prompts that improve model performance and reduce bias would address this question.

## Limitations
- Cross entropy improvements over baseline were modest (4% relative improvement), raising questions about practical significance
- Online survey validation used only 10 samples per dataset, which may not be representative given the scale of the datasets
- Instruction tuning prompt templates were not fully specified, making exact replication difficult
- Ensemble approach assumes annotator identities are known, limiting applicability when annotator anonymity is required

## Confidence

**High confidence**: The core hypothesis that modeling annotator disagreement as probability distributions can improve text classification performance on subjective tasks. This is well-supported by multiple evaluation metrics and human validation.

**Medium confidence**: The superiority of the multi-label approach over ensemble and instruction tuning methods. While supported by results, the differences are modest and context-dependent.

**Low confidence**: The effectiveness of instruction tuning for this task. Only one related paper was found, and the approach showed mixed results across datasets.

## Next Checks
1. Conduct ablation studies on each approach to determine which components (loss functions, model architecture, training procedures) contribute most to performance improvements
2. Test the methods on additional datasets with varying levels of annotator agreement to understand performance boundaries and conditions
3. Evaluate whether combining the best elements of all three approaches (multi-label loss with ensemble-style modeling and instruction-based fine-tuning) yields further improvements