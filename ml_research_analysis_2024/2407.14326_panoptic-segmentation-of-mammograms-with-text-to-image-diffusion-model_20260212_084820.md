---
ver: rpa2
title: Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model
arxiv_id: '2407.14326'
source_url: https://arxiv.org/abs/2407.14326
tags:
- segmentation
- odise
- panoptic
- image
- breast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes M-ODISE, a panoptic segmentation approach for
  mammography images that leverages pretrained features from a Stable Diffusion model
  as inputs to a Mask2Former-based panoptic segmentation architecture. To bridge the
  gap between natural and medical imaging domains, the authors incorporated a mammography-specific
  MAM-E diffusion model and BiomedCLIP image and text encoders into the framework.
---

# Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model

## Quick Facts
- arXiv ID: 2407.14326
- Source URL: https://arxiv.org/abs/2407.14326
- Authors: Kun Zhao; Jakub Prokop; Javier Montalt Tordera; Sadegh Mohammadi
- Reference count: 33
- Primary result: Proposed M-ODISE method achieves 40.25 AP@0.1 and 46.82 AP@0.05 for instance segmentation, and 25.44 PQ@0.1 and 26.92 PQ@0.05 for panoptic segmentation on mammography datasets

## Executive Summary
This paper introduces M-ODISE, a panoptic segmentation approach for mammography images that leverages pretrained features from a Stable Diffusion model within a Mask2Former-based architecture. To address the domain gap between natural and medical imaging, the authors incorporate a mammography-specific MAM-E diffusion model and BiomedCLIP image and text encoders. The method is evaluated on two mammography datasets, CDD-CESM and VinDr-Mammo, demonstrating improved performance over baseline approaches for both instance and semantic segmentation tasks.

## Method Summary
The M-ODISE framework combines MAM-E, a mammography-specific diffusion model trained on approximately 55,000 images, with BiomedCLIP encoders and Mask2Former architecture for panoptic segmentation. MAM-E provides frozen features for lesion detection, while BiomedCLIP handles medical text-image alignment. The model simultaneously predicts semantic and instance segmentation masks, with classification based on dot product similarity between mask embeddings and category text embeddings. Training uses binary cross-entropy and Dice loss for masks, with cross-entropy for classification, evaluated through 5-fold cross-validation on mammography datasets.

## Key Results
- Instance segmentation: 40.25 AP@0.1 and 46.82 AP@0.05 on evaluated datasets
- Panoptic segmentation: 25.44 PQ@0.1 and 26.92 PQ@0.05 on evaluated datasets
- Semantic segmentation: Dice scores of 38.86 and 40.92 on respective datasets
- Optimal IoU thresholds: 0.1 for CDD-CESM and 0.05 for VinDr-Mammo datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAM-E's mammography-specific training improves feature quality for lesion detection compared to general-domain Stable Diffusion.
- Mechanism: MAM-E was trained on ~55k mammography images from OMI-H and VinDr-Mammo datasets, enabling it to learn domain-specific texture, shape, and contrast patterns that are critical for identifying breast lesions. These features are then passed to Mask2Former for segmentation.
- Core assumption: Mammography images have unique visual characteristics (e.g., tissue density, lesion contrast) that differ from natural images, and models trained on general image corpora fail to capture these nuances.
- Evidence anchors:
  - [abstract]: "To bridge the gap between natural and medical imaging domains, we incorporated a mammography-specific MAM-E diffusion model..."
  - [section 2.1]: "MAM-E is based on SD architecture, trained on approximately 55,000 healthy mammography images..."
  - [corpus]: Weak or missing. No direct comparison of MAM-E vs SD performance on medical vs natural tasks found in corpus.
- Break condition: If MAM-E's performance gain over SD is not statistically significant across multiple datasets, or if MAM-E underperforms in dense breast regions.

### Mechanism 2
- Claim: BiomedCLIP text encoder improves semantic understanding of medical terminology, enabling better cross-modal alignment for lesion classification.
- Mechanism: BiomedCLIP was trained on 15 million biomedical figure-caption pairs, allowing it to encode medical concepts (e.g., "heterogeneously enhancing mass", "architectural distortion") more accurately than general CLIP. These embeddings guide the diffusion model during generation and classification.
- Core assumption: Medical image segmentation requires precise semantic understanding of clinical terms that general text encoders fail to capture.
- Evidence anchors:
  - [abstract]: "To bridge the gap between natural and medical imaging domains, we incorporated... BiomedCLIP image and text encoders..."
  - [section 2.1]: "The image encoder V is derived from BiomedCLIP, which is a CLIP architecture specially tailored for handling medical data."
  - [corpus]: Weak or missing. No direct evaluation of BiomedCLIP vs CLIP on medical text encoding tasks found in corpus.
- Break condition: If BiomedCLIP's embeddings do not improve classification accuracy over general CLIP in ablation studies.

### Mechanism 3
- Claim: Panoptic segmentation framework unifies semantic and instance segmentation, enabling both global tissue context and fine-grained lesion delineation.
- Mechanism: By using Mask2Former with MAM-E features, the model simultaneously predicts per-pixel semantic labels (e.g., background, lesion) and instance-level masks (individual lesions), capturing both tissue-wide patterns and lesion-specific boundaries.
- Core assumption: Mammogram interpretation requires both holistic tissue assessment and precise lesion localization, which separate semantic/instance models cannot provide jointly.
- Evidence anchors:
  - [abstract]: "We aim to harness their capabilities for breast lesion segmentation in a panoptic setting, which encompasses both semantic and instance-level predictions."
  - [section 1]: "Panoptic segmentation is an emerging image analysis method that unifies semantic and instance segmentation..."
  - [corpus]: Weak or missing. No direct comparison of panoptic vs separate semantic/instance models on mammography found in corpus.
- Break condition: If instance segmentation AP remains low (e.g., < 30) despite high semantic segmentation Dice, indicating poor lesion boundary delineation.

## Foundational Learning

- Concept: Diffusion models and denoising process
  - Why needed here: Understanding how MAM-E generates features from noisy inputs is critical for debugging feature quality and guiding conditioning signal design.
  - Quick check question: In MAM-E's denoising process, what does the term ᾱ_t represent, and how does it affect the noise schedule?

- Concept: Panoptic segmentation metrics (PQ, RQ, SQ)
  - Why needed here: These metrics are used to evaluate model performance, especially for tuning IoU thresholds and interpreting detection vs segmentation quality trade-offs.
  - Quick check question: How does increasing the IoU threshold affect RQ and SQ, and why might a lower threshold (e.g., 0.1) be preferable for mammography lesions?

- Concept: Cross-modal alignment in vision-language models
  - Why needed here: BiomedCLIP's ability to align medical text and image embeddings is key to the model's classification head performance.
  - Quick check question: What is the role of the dot product between mask embeddings and category text embeddings in the classification head?

## Architecture Onboarding

- Component map:
  MAM-E (frozen diffusion model) -> feature extractor
  BiomedCLIP (frozen encoders) -> image/text embedding
  Mask2Former (trainable) -> mask generator
  Classification head (trainable) -> lesion type prediction
  Implicit captioner (trainable) -> diffusion conditioning

- Critical path:
  Image -> MAM-E features -> Mask2Former -> binary masks + mask embeddings -> dot product with BiomedCLIP category embeddings -> class probabilities

- Design tradeoffs:
  - Using frozen MAM-E limits fine-tuning but ensures domain consistency; training MAM-E could improve performance but risks overfitting to limited data.
  - BiomedCLIP provides medical semantics but may not cover all lesion types; extending it requires domain-specific fine-tuning.
  - Low IoU thresholds (0.05-0.1) increase recall but may reduce segmentation precision; higher thresholds improve precision but miss subtle lesions.

- Failure signatures:
  - Low RQ but high SQ: Model detects few lesions but segments them accurately -> suggests detection bias or class imbalance.
  - High RQ but low SQ: Model detects many lesions but poorly segments them -> suggests feature quality or mask generation issues.
  - Low Dice for semantic segmentation: Model fails to capture global tissue patterns -> suggests MAM-E features lack semantic context.

- First 3 experiments:
  1. Replace MAM-E with Stable Diffusion and compare PQ/AP/Dice on CDD-CESM to quantify domain adaptation benefit.
  2. Swap BiomedCLIP with general CLIP and evaluate classification accuracy to measure medical semantic gain.
  3. Vary IoU thresholds (0.05 to 0.5) and plot PQ, RQ, SQ curves to find optimal threshold for mammography lesion detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of IoU threshold affect the trade-off between RQ and SQ in mammography panoptic segmentation?
- Basis in paper: [explicit] The authors analyze PQ, RQ, and SQ across IoU thresholds from 0.05 to 0.9, noting that RQ decreases and SQ increases as threshold increases
- Why unresolved: The paper only analyzes thresholds up to 0.5, but the optimal threshold differs between datasets (0.1 for CDD-CESM, 0.05 for VinDr-Mammo). The analysis doesn't explore why these differences exist or what would happen at higher thresholds
- What evidence would resolve it: A systematic analysis of model performance across all thresholds for both datasets, examining how lesion size and shape distributions affect optimal threshold selection

### Open Question 2
- Question: Why does M-ODISE underperform compared to ODISE variants on the VinDr-Mammo dataset despite incorporating both MAM-E and BiomedCLIP?
- Basis in paper: [explicit] The results show M-ODISE "did not surpass ODISE, ODISE with BiomedCLIP, or ODISE with MAM-E" on VinDr-Mammo, despite combining both domain adaptations
- Why unresolved: The authors note this as an "ambiguous finding" but don't provide a clear explanation for why the combination performs worse than individual adaptations on this dataset
- What evidence would resolve it: Ablation studies isolating the contributions of MAM-E and BiomedCLIP on VinDr-Mammo, plus analysis of dataset characteristics that might explain the performance difference

### Open Question 3
- Question: What technical advancements are needed to improve panoptic segmentation performance for mammography analysis?
- Basis in paper: [inferred] The authors conclude that "further technical advancements are needed to tailor current panoptic segmentation techniques to the specifics of mammogram analysis"
- Why unresolved: The paper demonstrates that current panoptic segmentation approaches lag behind specialized models for both instance and semantic segmentation tasks in mammography
- What evidence would resolve it: Comparative studies between panoptic segmentation and specialized models on identical datasets, identifying specific technical limitations (e.g., handling small lesions, low contrast, radiologist disagreement) that need to be addressed

## Limitations
- The use of frozen MAM-E and BiomedCLIP models limits the system's ability to adapt to dataset-specific characteristics and may constrain performance improvements.
- The relatively low Dice scores (38.86 and 40.92) for semantic segmentation suggest inadequate capture of global tissue context, which is crucial for clinical interpretation.
- The reliance on low IoU thresholds (0.05-0.1) indicates the model struggles with precise lesion boundary delineation, a critical requirement for clinical applications.

## Confidence

**High Confidence**: The core methodology of integrating diffusion model features with panoptic segmentation is technically sound and well-established in the literature. The architectural choices (Mask2Former, frozen feature extractors) follow standard practices in medical image analysis.

**Medium Confidence**: The domain adaptation benefits claimed through MAM-E and BiomedCLIP are plausible but not definitively proven without direct comparisons to baseline models (general Stable Diffusion and CLIP). The performance improvements over existing methods need independent validation.

**Low Confidence**: The clinical relevance of the results is uncertain due to the relatively modest performance metrics and the lack of comparison with established clinical workflows. The generalization capability across diverse patient populations and imaging protocols remains unproven.

## Next Checks

1. **Ablation Study on Domain Adaptation**: Replace MAM-E with general Stable Diffusion and BiomedCLIP with standard CLIP, then measure the performance degradation to quantify the actual benefit of domain-specific models.

2. **Threshold Sensitivity Analysis**: Systematically vary IoU thresholds from 0.05 to 0.5 and plot PQ, RQ, and SQ curves to determine the optimal operating point and assess the model's robustness to threshold selection.

3. **Multi-Center Generalization Test**: Evaluate the model on an independent mammography dataset from a different institution or country to assess its generalization capabilities and potential biases in the current training data.