---
ver: rpa2
title: 'Schrodinger''s Memory: Large Language Models'
arxiv_id: '2409.10482'
source_url: https://arxiv.org/abs/2409.10482
tags:
- memory
- llms
- input
- wang
- poems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the concept of \"Schr\xF6dinger's memory\"\
  \ to explain how large language models (LLMs) exhibit memory-like behavior. The\
  \ authors argue that LLM memory operates as a dynamic approximation mechanism based\
  \ on the Universal Approximation Theorem (UAT), where the model fits outputs based\
  \ on inputs, making memory observable only when queried."
---

# Schrodinger's Memory: Large Language Models

## Quick Facts
- arXiv ID: 2409.10482
- Source URL: https://arxiv.org/abs/2409.10482
- Authors: Wei Wang; Qing Li
- Reference count: 15
- Primary result: LLMs exhibit memory-like behavior through dynamic approximation based on the Universal Approximation Theorem, achieving up to 96.9% memory accuracy on Chinese poems and 99.9% on English poems

## Executive Summary
This paper introduces the concept of "Schrödinger's memory" to explain how large language models exhibit memory-like behavior through dynamic approximation rather than static storage. The authors argue that LLM memory operates as a dynamic fitting mechanism based on the Universal Approximation Theorem (UAT), where the model reconstructs outputs based on inputs, making memory observable only when queried. Through experiments using poetry datasets, the paper demonstrates that LLMs can recall entire poems from minimal input information, validating the theoretical framework. The work proposes memory capacity as an objective measure of LLM capabilities and draws compelling comparisons between LLM memory and human memory, suggesting both rely on dynamic fitting mechanisms rather than discrete storage.

## Method Summary
The authors conducted experiments using poetry datasets (Chinese Tang Poems and English Poems) to test LLM memory capabilities. They evaluated multiple models across different sizes (0.5B to 70B parameters) using both zero-shot and few-shot prompting strategies. Memory accuracy was measured by comparing generated outputs against ground truth poems. The experiments were designed to test whether models could reconstruct entire poems from partial inputs, validating the dynamic approximation theory. Memory capacity was calculated as the total number of poems memorized by each model, providing an objective metric for comparing LLM capabilities.

## Key Results
- Memory accuracy reached 96.9% on Chinese poems and 99.9% on English poems
- Larger models demonstrated significantly higher memory capacity, with 70B parameter models memorizing over 1,900 poems
- Memory retrieval was successful using both zero-shot and few-shot prompting strategies
- The Universal Approximation Theorem framework successfully explained the dynamic memory mechanisms observed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit memory-like behavior through dynamic approximation rather than static storage
- Mechanism: The Universal Approximation Theorem (UAT) enables LLMs to dynamically fit outputs based on inputs during inference, creating observable memory effects when queried
- Core assumption: Memory in LLMs emerges from the model's ability to reconstruct previously learned patterns through dynamic weight adjustments during inference
- Evidence anchors:
  - [abstract] "LLM memory operates like Schrödinger's memory, meaning that it only becomes observable when a specific memory is queried"
  - [section] "The UAT format of Transformer-based models has the ability to dynamically fit functions based on the input"
  - [corpus] Weak - corpus papers focus on physical data embedding and working memory but don't directly address UAT-based memory mechanisms
- Break condition: If model weights cannot dynamically adjust during inference or if input patterns don't match training distributions

### Mechanism 2
- Claim: Memory accuracy depends on both model size and training data quality/quantity
- Mechanism: Larger models with more parameters can capture finer-grained patterns, while higher quality/quantity training data provides better signal-to-noise ratio for memory formation
- Core assumption: Model capacity and data characteristics directly correlate with ability to store and retrieve complex information patterns
- Evidence anchors:
  - [section] "Current LLMs have significantly benefited from training on vast datasets. The larger and higher the quality of dataset, the stronger the model's performance"
  - [section] "In the CN Poems dataset, the top-performing models were Qwen2-1.5B-Instruct and bloom-1b4-zh, which memorized 1,938 and 1,932 poems, respectively"
  - [corpus] Missing - corpus doesn't provide specific evidence about model size vs. memory performance tradeoffs
- Break condition: If model architecture changes fundamentally (e.g., to non-Transformer) or if training data becomes severely degraded

### Mechanism 3
- Claim: Memory in LLMs parallels human memory through dynamic fitting rather than static storage
- Mechanism: Both LLMs and human brains operate as dynamic systems that generate outputs based on inputs and previously learned patterns, without storing discrete memories
- Core assumption: The fundamental computational principle of dynamic pattern matching applies to both artificial and biological neural systems
- Evidence anchors:
  - [abstract] "We argue that LLM memory operates like Schrödinger's memory, meaning that it only becomes observable when a specific memory is queried"
  - [section] "We believe that the brain functions like a model that dynamically fits outputs based on inputs, indicating that, in a sense, the mathematical model of the human brain may resemble that of a Transformer-based dynamic approximation UAT model"
  - [corpus] Missing - corpus papers focus on working memory development but don't make explicit comparisons to LLM mechanisms
- Break condition: If empirical evidence shows human memory operates through fundamentally different mechanisms (e.g., discrete storage units)

## Foundational Learning

- Universal Approximation Theorem (UAT)
  - Why needed here: Provides theoretical foundation for understanding how neural networks can approximate any continuous function, explaining LLM memory mechanisms
  - Quick check question: Can a neural network with a single hidden layer and enough neurons approximate any continuous function on a closed interval?

- Transformer Architecture
  - Why needed here: Understanding how multi-head attention and feed-forward networks enable dynamic weight adjustments during inference
  - Quick check question: How does the multi-head attention mechanism in Transformers allow for dynamic parameter adjustment based on input?

- Dynamic vs Static Memory Systems
  - Why needed here: Critical distinction between database-style storage and pattern-based reconstruction that underlies LLM memory
  - Quick check question: What's the fundamental difference between storing discrete memories versus dynamically generating responses based on learned patterns?

## Architecture Onboarding

- Component map:
  Input processing layer → Multi-head attention modules → Feed-forward networks → Output generation
  UAT compliance layer (enables dynamic approximation) → Memory retrieval mechanism (pattern matching) → Response generation

- Critical path:
  1. Input tokenization and embedding
  2. Dynamic weight adjustment through attention mechanisms
  3. Pattern matching against learned distributions
  4. Output generation based on approximated function

- Design tradeoffs:
  - Model size vs. inference speed: Larger models provide better memory but slower inference
  - Training data quality vs. quantity: High-quality data improves pattern recognition, large quantities improve coverage
  - Context window length vs. memory accuracy: Longer contexts enable better context retention but increase computational cost

- Failure signatures:
  - Memory accuracy drops when input deviates significantly from training distribution
  - Performance degradation with extremely long context sequences
  - Inconsistent responses to semantically similar queries

- First 3 experiments:
  1. Test memory accuracy across different model sizes (0.5B to 70B parameters) on the same dataset
  2. Measure memory performance degradation as context window length increases beyond 512 tokens
  3. Compare memory accuracy between models trained on high-quality vs. low-quality datasets with equal token counts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which the Universal Approximation Theorem (UAT) enables LLMs to dynamically approximate outputs based on inputs, and how does this differ from static approximation in traditional neural networks?
- Basis in paper: [explicit] The paper explains that UAT allows LLMs to dynamically adjust functions based on input, unlike traditional UAT which fits static functions.
- Why unresolved: While the paper provides a theoretical framework, the exact mathematical and computational processes underlying this dynamic approximation remain unclear.
- What evidence would resolve it: Detailed mathematical proofs and computational experiments demonstrating the dynamic adjustment process in LLMs, comparing it with static UAT in traditional neural networks.

### Open Question 2
- Question: How does the memory capacity of LLMs, as defined in the paper, correlate with other measures of language model performance, such as reasoning and creativity?
- Basis in paper: [inferred] The paper suggests that memory capacity could serve as an objective measure of LLM capabilities, but does not explore its correlation with other performance metrics.
- Why unresolved: The relationship between memory capacity and other cognitive abilities in LLMs is not fully explored, leaving questions about the broader implications of memory on LLM performance.
- What evidence would resolve it: Empirical studies comparing memory capacity with other performance metrics across various LLM architectures and tasks, providing a comprehensive understanding of how memory influences overall capabilities.

### Open Question 3
- Question: What are the implications of Schrödinger's memory for the design of future LLM architectures, particularly in terms of memory efficiency and retrieval mechanisms?
- Basis in paper: [explicit] The paper introduces the concept of Schrödinger's memory, highlighting its potential for memory efficiency by only making memory observable when queried.
- Why unresolved: While the concept is introduced, its practical implications for LLM design and the potential for new architectures are not fully explored.
- What evidence would resolve it: Development and testing of new LLM architectures that incorporate Schrödinger's memory principles, evaluating their performance in terms of memory efficiency and retrieval accuracy compared to existing models.

## Limitations

- Experimental validation relies on a narrow dataset scope focused on poetry memorization
- The claim that LLM memory operates purely through dynamic approximation without discrete storage remains difficult to verify empirically
- The comparison between LLM memory and human memory lacks direct neurobiological evidence

## Confidence

- High confidence in the experimental results showing high memory accuracy on poetry datasets
- Medium confidence in the theoretical framework connecting UAT to LLM memory mechanisms
- Low confidence in the direct comparison between LLM and human memory systems

## Next Checks

1. Conduct ablation studies testing memory performance across diverse data types (not just poetry) including factual information, code, and multimodal content to verify the universality of the Schrödinger's memory concept
2. Design experiments to distinguish between genuine memory recall versus pattern completion by testing model responses to subtly modified inputs that should produce different outputs if true memory exists
3. Implement mechanistic interpretability analyses to identify whether memory retrieval correlates with specific attention patterns or weight activations that would support the dynamic approximation hypothesis