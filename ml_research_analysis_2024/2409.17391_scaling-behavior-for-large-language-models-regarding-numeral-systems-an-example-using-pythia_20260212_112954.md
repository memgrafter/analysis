---
ver: rpa2
title: 'Scaling Behavior for Large Language Models regarding Numeral Systems: An Example
  using Pythia'
arxiv_id: '2409.17391'
source_url: https://arxiv.org/abs/2409.17391
tags:
- base
- pythia
- addition
- system
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how numeral system tokenization affects
  arithmetic performance in transformer-based LLMs. The authors compare base-10 (1-digit)
  and base-100/1000 (2-3 digit) tokenization schemes through controlled experiments
  using Pythia models across different scales and training settings.
---

# Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia

## Quick Facts
- arXiv ID: 2409.17391
- Source URL: https://arxiv.org/abs/2409.17391
- Authors: Zhejian Zhou; Jiayu Wang; Dahua Lin; Kai Chen
- Reference count: 40
- Primary result: Base-10 numeral systems consistently outperform base-100/1000 systems in data efficiency and arithmetic generalization for transformer-based LLMs

## Executive Summary
This paper investigates how numeral system tokenization affects arithmetic performance in transformer-based LLMs through controlled experiments comparing base-10 (1-digit) and base-100/1000 (2-3 digit) tokenization schemes. The authors demonstrate that base-10 systems are consistently more data-efficient than base-100/1000 when trained from scratch, requiring fewer training samples to achieve comparable performance across model sizes and operations. Through extrapolation experiments, they reveal distinct failure modes: base-100/1000 systems struggle with token-level discernment and operations beyond their training distribution, while base-10 models demonstrate better generalization through mechanisms like truncated addition and carry extrapolation.

## Method Summary
The study uses synthetic data generation for numeral systems base 10, base 100, and base 1000, training Pythia models (70M, 410M, 1.4B, 6.9B, 12B) across different scales from 2^13 to 2^19 samples. The training procedure employs decoder-only transformers with prompt masking and loss calculation only on outputs, using fixed hyperparameters including learning rate magnitudes {2e-3, 2e-4, 2e-5, 2e-6, 2e-7} and 10 epochs for from-scratch training versus 2 epochs for fine-tuning. Evaluation uses three metrics: Relative Error (logarithmic error), Normalized Edit Similarity (based on edit distance), and Exact Match Accuracy for addition.

## Key Results
- Base-10 systems require significantly fewer training samples than base-100/1000 to achieve comparable arithmetic performance
- Base-10 models demonstrate superior length extrapolation through truncated addition mechanisms
- Base-100/1000 systems exhibit fundamental failure modes including token-level discernment issues and inability to extrapolate beyond training distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher token frequencies in base-10 systems lead to more data-efficient learning
- Mechanism: Base-10 systems have smaller token sets (0-9) that appear more frequently across different number positions, providing more training signal per sample
- Core assumption: Token frequency directly correlates with model parameter updates and learning efficiency
- Evidence anchors:
  - [abstract] "We attribute this to higher token frequencies of a base 10 system"
  - [section 3.1] "Token frequencies of a base 10 system are at least an order of magnitude larger than those of a base 100 or base 1000 system"

### Mechanism 2
- Claim: Base-10 systems enable better length extrapolation through truncated addition
- Mechanism: Models trained on base-10 can learn positional relationships that allow them to handle out-of-distribution lengths by truncating to known positions
- Core assumption: The model learns position-dependent token relationships that generalize beyond training length
- Evidence anchors:
  - [section 4.3.1] "we discover consistently that models would try to perform the addition truncating the tokens that exceed training length"
  - [section 4.3.1] "A base 10 model trained from-scratch would calculate 2635078980, 7+1 = 2635079091"

### Mechanism 3
- Claim: Base-10 systems better learn token-level operations and discernment
- Mechanism: Smaller token sets make it easier for models to learn precise token discrimination and arithmetic operations at the token level
- Core assumption: Learning discrimination between 10 tokens is easier than between 100 or 1000 tokens
- Evidence anchors:
  - [abstract] "we identify that base 100 and base 1000 systems struggle on token-level discernment and token-level operations"
  - [section 4.3.2] "the model is only able to correctly generate the starting tokens and ending tokens of the answer, with gibberish and repetitive tokens in the middle"

## Foundational Learning

- Concept: Positional encoding in transformers
  - Why needed here: The paper shows that positional information is crucial for length extrapolation, especially for truncated addition
  - Quick check question: How do sinusoidal positional encodings help models understand relative positions in sequences?

- Concept: Token frequency effects on training dynamics
  - Why needed here: The main claim about base-10 efficiency relies on understanding how token frequency affects learning
  - Quick check question: What happens to gradient updates when a token appears more frequently in training data?

- Concept: Arithmetic reasoning in language models
  - Why needed here: Understanding how LLMs learn to perform arithmetic operations is fundamental to interpreting the results
  - Quick check question: Why might direct arithmetic generation be more challenging than arithmetic with scratchpads?

## Architecture Onboarding

- Component map: Pythia transformer models → synthetic arithmetic data generator → training pipeline → evaluation metrics (relative error, normalized edit similarity, exact match)
- Critical path: Data generation → tokenization (base 10/100/1000) → model training → extrapolation testing
- Design tradeoffs: Base-10 offers better data efficiency but requires more tokens per number; base-100/1000 reduce context length but struggle with token discrimination
- Failure signatures: Instability in multiplication training, inability to extrapolate beyond training length, gibberish outputs for base-100/1000 systems
- First 3 experiments:
  1. Train base-10 and base-100 Pythia 70M models on 2^15 samples for addition, compare exact match accuracy
  2. Test length extrapolation: train on numbers <10^11, evaluate on numbers 10^11-10^16
  3. Analyze token distributions in base-10 vs base-100 training data to verify frequency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of numeral system affect the learning efficiency of other arithmetic operations beyond addition and multiplication, such as division and subtraction?
- Basis in paper: [explicit] The authors mention that division poses challenges with output termination and subtraction shows similar trends to addition, but these operations are not deeply explored in the experiments.
- Why unresolved: The paper focuses primarily on addition and multiplication, leaving a gap in understanding the impact of numeral systems on other arithmetic operations.
- What evidence would resolve it: Systematic experiments comparing numeral systems across a broader range of arithmetic operations, with performance metrics for each, would clarify the generalizability of the findings.

### Open Question 2
- Question: To what extent do the observed advantages of the base-10 system transfer to tasks requiring sequential planning or state space reduction, such as robotics or theorem proving?
- Basis in paper: [explicit] The authors suggest that their finding about having fewer states could enhance sequential modeling in other domains like robotics or theorem proving, but do not provide empirical evidence.
- Why unresolved: The paper does not include experiments or case studies in these domains to validate the broader applicability of their findings.
- What evidence would resolve it: Experiments demonstrating improved performance in sequential planning tasks when using a base-10 system versus multi-digit systems would confirm the broader implications of the study.

### Open Question 3
- Question: What is the optimal learning rate and training duration for different model sizes and numeral systems to achieve maximum data efficiency?
- Basis in paper: [explicit] The authors acknowledge that hyperparameter tuning was not exhaustive due to computational constraints and that suboptimal hyperparameters might have affected some results.
- Why unresolved: The paper uses a fixed set of hyperparameters across different configurations without exploring the optimal settings for each scenario.
- What evidence would resolve it: A comprehensive grid search over learning rates and training durations for each numeral system and model size, followed by analysis of the optimal configurations, would address this gap.

## Limitations

- The study's core claim about base-10 superiority rests heavily on the assumption that token frequency directly drives learning efficiency, but this relationship hasn't been experimentally isolated from other confounding factors
- The extrapolation experiments rely on synthetic data that may not capture the complexity of real-world numerical reasoning tasks
- The paper doesn't address whether base-10 advantages would persist when models encounter mixed numeral representations or more complex mathematical operations

## Confidence

**High Confidence**: The empirical finding that base-10 systems achieve better data efficiency across multiple model sizes and training scales. The experimental design with controlled comparisons and clear metrics provides strong evidence for this claim.

**Medium Confidence**: The attribution of base-10 advantages to higher token frequencies. While the frequency analysis is sound, the causal link between frequency and learning efficiency needs further validation through ablation studies.

**Low Confidence**: The generalizability of findings to other numeral systems (like base-2 or base-16) and more complex mathematical operations. The paper's scope is limited to base 10, 100, and 1000, leaving open questions about whether the patterns extend to other bases.

## Next Checks

1. **Token Frequency Ablation**: Design experiments that control for context length while varying token frequencies to isolate the specific contribution of frequency to learning efficiency. Compare base-10 with modified base-100 systems that have artificially inflated token frequencies.

2. **Mixed Numeral System Training**: Train models on datasets containing multiple numeral systems simultaneously to test whether base-10 models maintain their advantages when not operating in isolation, and whether models can learn to switch between numeral systems effectively.

3. **Complex Operation Extension**: Extend the evaluation to include division, exponentiation, and chained operations to determine if the base-10 advantages persist for more complex mathematical reasoning, and whether different numeral systems show varying strengths across operation types.