---
ver: rpa2
title: Computing Approximate Graph Edit Distance via Optimal Transport
arxiv_id: '2412.18857'
source_url: https://arxiv.org/abs/2412.18857
tags:
- graph
- node
- matrix
- gediot
- edit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes new methods for approximate graph edit distance
  (GED) computation using optimal transport theory. The authors introduce GEDIOT,
  a neural network approach that uses inverse optimal transport to generate coupling
  matrices, and GEDGW, an unsupervised optimization method that combines optimal transport
  and Gromov-Wasserstein discrepancy.
---

# Computing Approximate Graph Edit Distance via Optimal Transport

## Quick Facts
- arXiv ID: 2412.18857
- Source URL: https://arxiv.org/abs/2412.18857
- Reference count: 40
- Key outcome: Introduces GEDHOT ensemble method achieving up to 72.3% improvement in mean absolute error compared to state-of-the-art GEDGNN method

## Executive Summary
This paper proposes new methods for approximate graph edit distance (GED) computation using optimal transport theory. The authors introduce GEDIOT, a neural network approach that uses inverse optimal transport to generate coupling matrices, and GEDGW, an unsupervised optimization method that combines optimal transport and Gromov-Wasserstein discrepancy. They also present GEDHOT, an ensemble method that combines GEDIOT and GEDGW for improved performance. Extensive experiments on real-world datasets show that the proposed methods significantly outperform existing approaches in terms of GED computation accuracy, edit path generation quality, and generalizability to unseen graph pairs.

## Method Summary
The paper proposes three main methods for approximate GED computation. GEDIOT uses a neural network with learnable optimal transport to predict GED, where a GNN generates node embeddings, a learnable Sinkhorn algorithm computes coupling matrices, and graph discrepancy is measured using attention and neural tensor networks. GEDGW models GED as a linear combination of optimal transport (for node edits) and Gromov-Wasserstein discrepancy (for edge edits), solved using conditional gradient method. GEDHOT combines both methods by taking the minimum GED prediction and using k-best matching for edit path generation.

## Key Results
- GEDHOT achieves up to 72.3% improvement in mean absolute error compared to GEDGNN
- Outperforms existing methods on AIDS, Linux, and IMDB datasets in terms of accuracy, feasibility, and running time
- Demonstrates strong generalizability to unseen graph pairs through experiments on synthetic power-law graphs

## Why This Works (Mechanism)

### Mechanism 1
Optimal transport provides global context awareness that pairwise scoring alone lacks. The Sinkhorn algorithm optimizes over the entire cost matrix rather than treating each element independently, ensuring the coupling matrix satisfies global constraints (row/column sums equal to 1). The core assumption is that the ground-truth GED solution can be represented as a doubly stochastic matrix that can be recovered through entropy-regularized optimal transport.

### Mechanism 2
Combining node edit operations (optimal transport) with edge edit operations (Gromov-Wasserstein) captures the complete edit distance. The linear combination in GEDGW's objective function separately models node edit costs (via OT on labels) and edge edit costs (via GW on adjacency matrices), then combines them through a unified optimization framework.

### Mechanism 3
Learnable regularization coefficient in Sinkhorn algorithm adapts to different dataset characteristics. Instead of fixing the regularization parameter Œµ, it's treated as a learnable parameter that gets optimized through backpropagation, allowing the model to find the optimal trade-off between OT approximation accuracy and numerical stability.

## Foundational Learning

- **Optimal Transport Theory and Sinkhorn Algorithm**: Provides mathematical framework for computing coupling matrices that respect global constraints while being computationally efficient. Quick check: Can you explain why the Sinkhorn algorithm alternates between updating dual variables and how this relates to the row/column sum constraints?
- **Graph Neural Networks and Message Passing**: Required to extract meaningful node embeddings that capture both local neighborhood structure and global graph topology. Quick check: What's the difference between GIN and GCN in terms of their expressive power for distinguishing graph structures?
- **Gromov-Wasserstein Discrepancy**: Measures similarity between graphs when node correspondences are unknown, crucial for modeling edge edit operations. Quick check: How does GW differ from standard OT when comparing two graphs with different node label spaces?

## Architecture Onboarding

- **Component map**: Graph pair ‚Üí Node embeddings (GIN + MLP) ‚Üí Cost matrix ‚Üí Learnable OT component (Sinkhorn) ‚Üí Coupling matrix ‚Üí Graph discrepancy (Attention + NTN) ‚Üí GED prediction + loss computation
- **Critical path**: Graph pair ‚Üí Node embeddings ‚Üí Cost matrix ‚Üí Sinkhorn iterations ‚Üí Coupling matrix ‚Üí GED prediction + loss computation
- **Design tradeoffs**: Using dummy nodes to handle unequal graph sizes vs. more complex constrained optimization; learnable regularization vs. fixed heuristic values; ensemble approach vs. single model simplicity
- **Failure signatures**: High MAE indicates poor node embeddings or cost matrix learning; unstable training suggests regularization coefficient issues; poor generalization indicates overfitting to training graph size distribution
- **First 3 experiments**: 
  1. Test with synthetic graphs where ground truth is known to verify the Sinkhorn layer correctly recovers node correspondences
  2. Compare GEDIOT with and without the learnable regularization coefficient to quantify the benefit of adaptive Œµ
  3. Evaluate the impact of different GNN architectures (GIN vs GCN) on the quality of node embeddings and final GED predictions

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of GEDHOT compare to exact methods like Nass and AStar-BMao for larger graph sizes and higher GED values? The paper only provides a comparison between GEDIOT and exact methods, leaving the performance of GEDHOT against exact methods unexplored.

### Open Question 2
What is the impact of varying the regularization coefficient ùúÄ0 on the performance of GEDIOT? The paper mentions studying this impact but provides only brief results without detailed analysis.

### Open Question 3
How does the generalizability of GEDHOT compare to GEDIOT when dealing with unseen graph pairs? The paper discusses GEDIOT's generalizability but doesn't provide a direct comparison with GEDHOT.

## Limitations

- Core claims rely heavily on quality of node embeddings generated by GIN layers, but specific architecture parameters are not specified
- Learnable regularization coefficient may be sensitive to initialization and could lead to unstable training if not properly tuned
- Ensemble approach assumes GEDIOT and GEDGW provide complementary strengths, but lacks detailed analysis of when each method performs better independently

## Confidence

- **GEDIOT Mechanism**: High confidence - well-grounded use of optimal transport with learnable regularization and clear bi-level optimization framework
- **GEDGW Decomposition**: Medium confidence - theoretically sound combination of OT and GW, but assumption about independent optimization may not hold for all graph types
- **Ensemble Performance**: Medium confidence - impressive 72.3% improvement claim but relies on specific dataset characteristics and quality of ground truth GED values

## Next Checks

1. **Sensitivity Analysis**: Systematically vary the regularization coefficient initialization and track model stability to determine optimal initialization ranges
2. **Component Ablation**: Test GEDHOT performance when using only GEDIOT or only GEDGW to quantify the actual contribution of the ensemble approach
3. **Generalization Stress Test**: Evaluate performance on graphs with sizes and structures not represented in the training data to assess true generalizability claims