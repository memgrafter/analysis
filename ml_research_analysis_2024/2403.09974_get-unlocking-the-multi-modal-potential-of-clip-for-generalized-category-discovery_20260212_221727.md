---
ver: rpa2
title: 'GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category
  Discovery'
arxiv_id: '2403.09974'
source_url: https://arxiv.org/abs/2403.09974
tags:
- text
- visual
- clip
- classes
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GET, a method to enhance Generalized Category
  Discovery (GCD) by leveraging multi-modal information from CLIP. GET tackles the
  challenge of missing class names in unlabelled data by introducing a Text Embedding
  Synthesizer (TES) that generates pseudo text embeddings from visual features.
---

# GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery

## Quick Facts
- arXiv ID: 2403.09974
- Source URL: https://arxiv.org/abs/2403.09974
- Reference count: 40
- Key outcome: GET achieves state-of-the-art results on multiple GCD benchmarks, significantly improving classification accuracy, particularly for visually similar classes.

## Executive Summary
This paper introduces GET, a method to enhance Generalized Category Discovery (GCD) by leveraging multi-modal information from CLIP. GET tackles the challenge of missing class names in unlabelled data by introducing a Text Embedding Synthesizer (TES) that generates pseudo text embeddings from visual features. The method employs a dual-branch training strategy with cross-modal instance consistency to mutually enhance visual and semantic information. GET achieves state-of-the-art results on multiple GCD benchmarks, significantly improving classification accuracy, particularly for visually similar classes.

## Method Summary
GET is a two-stage method for Generalized Category Discovery. Stage 1 trains a Text Embedding Synthesizer (TES) to generate pseudo text embeddings from visual features using CLIP's aligned vision-language property. Stage 2 employs a dual-branch framework where visual and text branches process data separately while maintaining cross-modal instance consistency through mutual knowledge distillation. The method uses supervised contrastive learning objectives and prototypical classification to discover new categories while correctly classifying known ones.

## Key Results
- GET achieves significant performance improvements across multiple GCD benchmarks including CIFAR-10/100, ImageNet-100, CUB, Stanford Cars, FGVC-Aircraft, and Herbarium 19
- Particularly effective for visually similar classes, where traditional methods struggle with fine-grained distinctions
- Demonstrates strong generalization to datasets with novel concepts (NEV) and complex attributes (Clevr-4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Text Embedding Synthesizer (TES) successfully generates pseudo text embeddings for unlabelled data by converting visual embeddings into CLIP text encoder input tokens.
- Mechanism: TES uses a single fully connected layer to map visual embeddings into pseudo tokens that can be input to CLIP's text encoder, leveraging CLIP's aligned vision-language feature property.
- Core assumption: CLIP's text encoder can process tokens derived from visual embeddings to produce meaningful text embeddings that maintain semantic alignment with the original visual features.
- Evidence anchors:
  - [abstract] "Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text encoder to generate pseudo text embeddings."
  - [section 4.1] "Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text encoder to generate pseudo text embeddings for each sample."
  - [corpus] Weak - no direct corpus evidence about this specific mechanism, though CLIP's aligned feature property is well-established.
- Break condition: If the CLIP text encoder cannot process tokens derived from visual embeddings in a semantically meaningful way, or if the alignment property doesn't hold for the generated tokens.

### Mechanism 2
- Claim: The dual-branch framework with cross-modal instance consistency (CICO) enables visual and semantic information to mutually enhance each other.
- Mechanism: Two branches process visual and text information respectively using the same training strategy, while CICO enforces instance relationships to be consistent across modalities through mutual knowledge distillation.
- Core assumption: Instance relationships in visual and text embedding spaces can be meaningfully compared and aligned through knowledge distillation.
- Evidence anchors:
  - [abstract] "Through the joint learning and instance consistency of different modality branches, visual and semantic information mutually enhance each other, promoting the interaction and fusion of visual and text knowledge."
  - [section 4.2] "Our cross-modal instance consistency objective makes visual and text information exchange and benefit from each other, thus the two branches can serve as complementary discriminative aids to each other."
  - [corpus] Moderate - several related works mention cross-modal consistency, but specific evidence for this exact formulation is limited.
- Break condition: If instance relationships in different modalities cannot be meaningfully compared, or if knowledge distillation between modalities doesn't improve discriminative capabilities.

### Mechanism 3
- Claim: The align and distill losses in TES ensure that generated pseudo text embeddings reside in the same embedding space as real text features and maintain consistency.
- Mechanism: Align loss pulls correct visual-text embedding pairs closer while pushing away incorrect ones; distill loss guides pseudo text embeddings towards real semantic corresponding space and adapts to dataset distribution.
- Core assumption: The embedding space of CLIP's text encoder is consistent and can be adapted to represent pseudo text embeddings that align with visual features.
- Evidence anchors:
  - [section 4.1] "To ensure that our generated pseudo text features reside in the same embedding space as real text features and maintain consistency, we introduce a distill loss."
  - [section 4.1] "Our align loss leverages the modality alignment property of CLIP's encoders, pulling correct visual-text embedding pairs closer while pushing away the incorrect ones."
  - [corpus] Moderate - CLIP's aligned feature property is well-established, but specific evidence for this dual-loss training approach is limited.
- Break condition: If the embedding space of CLIP's text encoder is not consistent or cannot be adapted to represent pseudo text embeddings that align with visual features.

## Foundational Learning

- Concept: Vision-Language Pre-training (VLP) and CLIP architecture
  - Why needed here: Understanding how CLIP's vision-language alignment works is crucial for implementing TES and leveraging its multi-modal potential
  - Quick check question: What is the key architectural difference between CLIP and other vision-language models that enables its strong zero-shot transfer capabilities?

- Concept: Contrastive learning and instance discrimination
  - Why needed here: The training objectives (Lscon, Lcon, Lv_scon, Lv_ucon, Lt_scon, Lt_ucon) are based on contrastive learning principles
  - Quick check question: How does supervised contrastive learning differ from standard cross-entropy classification in terms of what it optimizes for?

- Concept: Knowledge distillation and mutual learning
  - Why needed here: The CICO objective uses a form of mutual knowledge distillation between visual and text branches
  - Quick check question: In the context of CICO, what does it mean for instance relationships to be consistent across modalities?

## Architecture Onboarding

- Component map: CLIP image encoder → Visual branch (MLP → Prototypical classifier) ↔ TES (FC layer) ↔ Text branch (CLIP text encoder → MLP → Prototypical classifier) ↔ CICO objective
- Critical path: TES → Dual-branch training with CICO → Inference using visual branch
- Design tradeoffs:
  - Single linear layer vs. deeper network for TES (simplicity vs. expressiveness)
  - Separate branches vs. feature concatenation (modality-specific learning vs. joint representation)
  - CICO vs. simple feature fusion (mutual enhancement vs. computational efficiency)
- Failure signatures:
  - Empty clusters in certain classes (indicates insufficient discriminative power)
  - High variance in results across runs (indicates training instability)
  - Poor performance on fine-grained datasets (indicates modality imbalance)
- First 3 experiments:
  1. Verify TES generates meaningful pseudo text embeddings by visualizing t-SNE of generated text features
  2. Test dual-branch training without CICO to isolate the effect of mutual learning
  3. Compare different numbers of pseudo text tokens in TES to find optimal setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GET change when using different text prompt designs, such as more descriptive or context-rich prompts?
- Basis in paper: [explicit] The paper discusses using different prompts, including simple ("a photo of a {CLS}"), more descriptive ("a photo of a {CLS}, which is a type of bird/car"), LLM-generated descriptions, and averaging textual features of multiple prompts. It shows that more finely designed prompts can further improve performance.
- Why unresolved: While the paper demonstrates the effectiveness of different prompts, it doesn't provide a comprehensive comparison of how significantly different prompt designs impact performance across all datasets and classes.
- What evidence would resolve it: A detailed ablation study comparing the performance of GET using a wide range of prompt designs across all evaluated datasets, quantifying the impact of prompt complexity and descriptiveness on classification accuracy.

### Open Question 2
- Question: Can the Text Embedding Synthesizer (TES) be adapted to generate text embeddings for entirely novel visual concepts not present in the CLIP training data?
- Basis in paper: [inferred] The paper discusses the effectiveness of TES on datasets like NEV (new energy vehicles) and Clevr-4 (texture attributes) where CLIP lacks prior knowledge. However, it doesn't explicitly test TES on entirely novel visual concepts.
- Why unresolved: While TES shows promise on datasets with some level of similarity to CLIP's training data, its ability to generalize to completely unseen visual concepts remains untested.
- What evidence would resolve it: Experiments evaluating TES on a dataset containing entirely novel visual concepts (e.g., images of fictional creatures or abstract objects) not present in any existing image-text datasets, measuring the quality and discriminative power of the generated text embeddings.

### Open Question 3
- Question: How does the performance of GET scale with the size of the unlabeled dataset, particularly for datasets with a large number of classes?
- Basis in paper: [inferred] The paper evaluates GET on datasets with varying numbers of classes (e.g., CIFAR-10 with 10 classes, ImageNet-1K with 1000 classes). However, it doesn't explicitly analyze the performance scaling with dataset size or the number of classes.
- Why unresolved: The paper demonstrates GET's effectiveness on datasets with different numbers of classes, but it doesn't provide insights into how the performance changes as the dataset size or the number of classes increases significantly.
- What evidence would resolve it: Experiments evaluating GET on progressively larger datasets with an increasing number of classes, analyzing the classification accuracy, computational complexity, and memory requirements as the dataset scales.

## Limitations
- Lack of detailed implementation specifications for TES architecture and CICO objective formulation
- Performance tied to CLIP's pre-trained weights and may not generalize well to datasets with different visual distributions
- Limited analysis of computational complexity and memory requirements for large-scale datasets

## Confidence

- **High**: Claims about overall performance improvements and state-of-the-art results are well-supported by extensive experiments across multiple benchmarks
- **Medium**: The two-stage training framework and dual-branch architecture are clearly described, though some implementation details remain unspecified
- **Low**: Claims about the specific mechanisms of TES token generation and CICO's exact implementation are difficult to verify without more detailed architectural specifications

## Next Checks

1. **TES Architecture Validation**: Implement the Text Embedding Synthesizer with varying depths (1-3 fully connected layers) and conduct ablation studies to determine if the single-layer design is optimal or merely a simplifying assumption.

2. **Cross-Modal Consistency Verification**: Create synthetic datasets with known instance relationships and verify that the CICO objective successfully aligns visual and text embeddings, measuring the KL divergence between anchor distributions before and after training.

3. **Modality Imbalance Analysis**: For fine-grained datasets (CUB, Stanford Cars, FGVC-Aircraft), analyze the contribution of each modality branch by systematically disabling one branch at a time and measuring the impact on overall accuracy, particularly for visually similar classes.