---
ver: rpa2
title: Gamified crowd-sourcing of high-quality data for visual fine-tuning
arxiv_id: '2410.04038'
source_url: https://arxiv.org/abs/2410.04038
tags:
- dataset
- arxiv
- image
- tainted
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Gamified Adversarial Prompting (GAP), a\
  \ framework that crowd-sources high-quality data for visual instruction tuning of\
  \ large multimodal models by transforming data collection into an engaging game.\
  \ Players are incentivized to ask fine-grained, challenging questions that target\
  \ gaps in the model\u2019s knowledge, with a scoring system that rewards correct\
  \ identification of deliberate mistakes in the model\u2019s responses."
---

# Gamified crowd-sourcing of high-quality data for visual fine-tuning

## Quick Facts
- arXiv ID: 2410.04038
- Source URL: https://arxiv.org/abs/2410.04038
- Reference count: 37
- Primary result: GAP framework collected data from 50K+ players, improving MiniCPM-Llama3-V-2.5-8B GPT score from 0.147 to 0.477

## Executive Summary
This paper introduces Gamified Adversarial Prompting (GAP), a framework that transforms visual instruction tuning data collection into an engaging game. Players interact with images and a multimodal model, earning points for identifying deliberate mistakes in model responses. The framework successfully crowd-sourced high-quality adversarial data from over 50,000 participants, significantly improving model performance across multiple benchmarks. The approach demonstrates both strong performance gains on the target model and cross-model benefits for other architectures.

## Method Summary
The GAP framework uses a Telegram miniapp where players view images and ask questions, receiving model responses they must evaluate as correct or wrong. A scoring system rewards players for identifying mistakes in a "tainted" subset where the model is deliberately instructed to answer incorrectly. Data is filtered using a threshold θ=0.8 and used to fine-tune models with LoRA. The collected data improves not only the source model but also demonstrates cross-model benefits for QWEN2-VL architectures.

## Key Results
- Collected data from 50,000+ participants in just a few weeks
- Improved MiniCPM-Llama3-V-2.5-8B GPT score from 0.147 to 0.477 on their dataset
- Achieved performance approaching GPT-4V benchmark
- Demonstrated cross-model benefits, improving QWEN2-VL-2B and QWEN2-VL-7B performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The scoring system incentivizes players to generate questions that reveal AI model mistakes, leading to high-quality adversarial data.
- Mechanism: Players earn 20 points for correctly identifying incorrect answers in the tainted subset where the model is instructed to answer incorrectly.
- Core assumption: Players are motivated primarily by the point system and will consistently seek to identify model errors rather than just trying to game the system.
- Evidence anchors:
  - [abstract] states "players are incentivized to ask fine-grained, challenging questions that target gaps in the model's knowledge, with a scoring system that rewards correct identification of deliberate mistakes"
  - [section 3] describes the scoring: "On a tainted image, the player earns 20 points every time they mark a model response incorrect (H = 0) and it is actually incorrect (M = 0)"
- Break condition: If players discover the tainted/untainted split and optimize only for speed rather than accuracy, or if the reward structure becomes exploitable through systematic gaming rather than genuine model probing.

### Mechanism 2
- Claim: The time limit per image creates urgency that improves data quality by preventing over-thinking and encouraging instinctive, challenging questions.
- Mechanism: 120-second limit forces players to quickly identify potential model weaknesses rather than spending excessive time on each question.
- Core assumption: Time pressure leads to more genuine, challenging questions rather than carefully constructed but less revealing ones.
- Evidence anchors:
  - [section 3] states "We impose a time limit on each image to create a sense of urgency"
  - The 120-second limit is explicitly mentioned as a design choice
- Break condition: If the time pressure becomes too stressful and reduces thoughtful questioning, or if players simply ask the first question that comes to mind without strategic consideration of model weaknesses.

### Mechanism 3
- Claim: The cross-model improvement effect occurs because different models share common knowledge gaps that can be identified through adversarial prompting.
- Mechanism: When one model is fine-tuned on GAP-generated data, it improves not only its own performance but also that of other models on the same benchmarks, suggesting shared weaknesses in multimodal understanding.
- Core assumption: Different multimodal models have common blind spots or knowledge gaps that can be systematically addressed through adversarial questioning.
- Evidence anchors:
  - [abstract] states "Moreover, we demonstrate that the data generated using MiniCPM-Llama3-V-2.5-8B also enhances its performance across other benchmarks, and exhibits cross-model benefits. Specifically, the same data improves the performance of QWEN2-VL-2B and QWEN2-VL-7B"
  - [section 5] shows "addressing knowledge gaps in one model architecture leads to improvements across different model sizes and architectures"
- Break condition: If the cross-model improvement is actually due to dataset-specific features rather than addressing fundamental knowledge gaps, or if the improvement is only marginal and doesn't generalize.

## Foundational Learning

- Concept: Probability and conditional probability
  - Why needed here: The evaluation system relies on Bayes' theorem to calculate the probability that a question will reveal an AI mistake (Equations 3-8)
  - Quick check question: If P(M=0|D=Tainted, I=0) = 0.1 and P(H=0|M=0, D=Tainted) = 0.9, what is P(M=0|H=0, D=Tainted) assuming P(I=1|D=Tainted) = 0.2?

- Concept: Logistic regression and sigmoid functions
  - Why needed here: The player interaction model uses a logistic function to map player ability, image difficulty, time pressure, and fatigue to a probability of success (Equation 19)
  - Quick check question: What happens to P(Sijk) as Ai increases while all other factors remain constant?

- Concept: Maximum likelihood estimation
  - Why needed here: The model fitting process uses MLE to estimate parameters like player ability and image difficulty from observed question outcomes (Equation 21)
  - Quick check question: Why would we use log-likelihood rather than raw likelihood when fitting the model?

## Architecture Onboarding

- Component map: Image selection → Player question → Model response → Player evaluation → Probabilistic scoring → Data collection → LoRA fine-tuning → Benchmark evaluation
- Critical path: Player interaction flow through Telegram miniapp → Model response generation → Scoring evaluation → Data filtering → Model fine-tuning
- Design tradeoffs: 120-second time limit balances urgency with thoughtfulness; tainted/untainted split enables evaluation but risks being discovered; LoRA fine-tuning preserves model knowledge while allowing efficient updates
- Failure signatures: Low-quality data (players gaming the system), poor model improvement (data doesn't address real weaknesses), cross-model benefits not materializing (data too model-specific), or player attrition (rewards insufficient)
- First 3 experiments:
  1. Test the basic game flow with a small image set and verify the scoring system works as intended
  2. Run a pilot with 100 players to validate the time limit and question quality
  3. Fine-tune a small model on initial data and verify performance improvement on a single benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold θ for selecting adversarial questions, and how does it impact model performance across different LMM architectures?
- Basis in paper: [explicit] The paper uses θ = 0.8 for selecting questions but notes this as a hyperparameter choice without extensive justification.
- Why unresolved: The paper only explores one threshold value and doesn't systematically investigate how different thresholds affect performance across various models.
- What evidence would resolve it: Comprehensive experiments varying θ across a range of values for different model sizes (2B, 7B, 8B) and architectures, measuring both data quality metrics and downstream task performance.

### Open Question 2
- Question: How does player fatigue evolve over extended gaming sessions, and what is its impact on data quality?
- Basis in paper: [inferred] The paper mentions fatigue as a factor in their probabilistic model but doesn't provide empirical data on how fatigue affects player performance or data quality.
- Why unresolved: The paper proposes a fatigue model but doesn't validate it with actual gameplay data or measure its real-world impact on data quality.
- What evidence would resolve it: Longitudinal analysis of player performance over multiple sessions, tracking accuracy degradation, question quality metrics, and time-on-task correlations.

### Open Question 3
- Question: What are the fundamental knowledge gaps shared across LMM architectures that make cross-model benefits possible?
- Basis in paper: [explicit] The paper observes significant cross-model benefits but doesn't analyze what specific types of questions or knowledge gaps are most universally effective.
- Why unresolved: While the paper demonstrates cross-model benefits exist, it doesn't identify the underlying commonalities in model weaknesses that enable this transfer.
- What evidence would resolve it: Detailed analysis of question types, image features, and knowledge domains where improvements transfer most effectively, potentially revealing architectural commonalities in LMM limitations.

## Limitations
- Scoring system robustness not validated - players could potentially game the system by developing heuristics
- Time limit effects not empirically optimized - 120 seconds may not be optimal for all player types
- Cross-model generalization mechanism unclear - improvements could be dataset-specific rather than addressing fundamental knowledge gaps

## Confidence
- High Confidence: Basic framework of gamified data collection with scoring incentives and observed performance improvements on MiniCPM-Llama3-V-2.5-8B
- Medium Confidence: Specific mechanism by which scoring system creates high-quality adversarial data and cross-dataset generalization claims
- Low Confidence: Optimal design parameters (time limit, scoring weights, threshold θ=0.8) and universal applicability assertions

## Next Checks
1. **System Gaming Analysis**: Analyze player behavior patterns to determine if participants developed systematic approaches to maximize points without genuinely probing model weaknesses, such as consistently asking similar question types or exploiting predictable model failure modes.

2. **Parameter Sensitivity Testing**: Systematically vary the time limit (e.g., 60s, 120s, 180s) and scoring parameters to identify whether the reported improvements are robust to these changes or highly sensitive to specific values.

3. **Cross-Model Data Generation**: Generate adversarial data using QWEN2-VL-7B instead of MiniCPM-Llama3-V-2.5-8B and test whether this data similarly improves both models, helping distinguish between dataset-specific effects and genuine knowledge gap addressing.