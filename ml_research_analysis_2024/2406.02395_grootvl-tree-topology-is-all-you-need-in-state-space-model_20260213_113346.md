---
ver: rpa2
title: 'GrootVL: Tree Topology is All You Need in State Space Model'
arxiv_id: '2406.02395'
source_url: https://arxiv.org/abs/2406.02395
tags:
- loss
- state
- tree
- arxiv
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GrootVL introduces a tree-based state space model to address long-range
  dependency modeling in vision and language tasks. It dynamically generates a tree
  topology based on input features and propagates states along this structure using
  a linear-time dynamic programming algorithm.
---

# GrootVL: Tree Topology is All You Need in State Space Model

## Quick Facts
- arXiv ID: 2406.02395
- Source URL: https://arxiv.org/abs/2406.02395
- Reference count: 40
- GrootVL achieves 83.4% top-1 accuracy on ImageNet-1K while surpassing previous SSM methods in vision tasks

## Executive Summary
GrootVL introduces a tree-based state space model that dynamically generates tree topology based on input features to address long-range dependency modeling in vision and language tasks. The method uses a minimum spanning tree constructed from feature dissimilarity and propagates states along this structure using dynamic programming for linear-time complexity. This approach overcomes limitations of fixed-scanning strategies in existing SSMs while maintaining competitive efficiency. GrootVL demonstrates state-of-the-art performance on multiple vision benchmarks and provides consistent improvements when fine-tuning pre-trained language models.

## Method Summary
GrootVL builds upon Mamba's selective state space framework by replacing fixed scanning strategies with dynamic tree topology generation. The method constructs a minimum spanning tree from feature dissimilarity between adjacent pixels/tokens, then propagates states through this structure using a dynamic programming algorithm that achieves linear computational complexity. For vision tasks, GrootV integrates this tree scanning into a hierarchical architecture with stem modules, basic blocks, and downsampling layers. For language tasks, GrootL provides a fine-tuning paradigm that enhances pre-trained models like Mamba by incorporating tree-based long-range modeling during adaptation.

## Key Results
- Vision: Achieves 83.4% top-1 accuracy on ImageNet-1K with Swin-T-level efficiency
- Object Detection: Reaches 50.1 box mAP and 44.6 mask mAP on COCO, surpassing previous SSM methods
- Language: Improves pre-trained Mamba model by 1.1% average accuracy across multiple benchmarks through fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tree topology scanning enables input-aware long-range dependencies that fixed-scanning strategies cannot capture
- Mechanism: Dynamically generates a minimum spanning tree based on feature dissimilarity between adjacent pixels/tokens, then propagates states along this tree using dynamic programming
- Core assumption: Feature dissimilarity between adjacent elements is a reliable proxy for spatial/semantic relevance
- Evidence anchors: [abstract] "dynamically generates a tree topology based on spatial relationships and input features" [section 3.2] "construct an undirected m-connected graph... The edge weight is calculated by the feature dissimilarity between adjacent vertices"

### Mechanism 2
- Claim: Dynamic programming algorithm reduces computational complexity from quadratic to linear
- Mechanism: Tree structure allows aggregation through parent-child relationships with only two traversals instead of computing all pairwise interactions
- Core assumption: Tree structure enables efficient aggregation without losing long-range information
- Evidence anchors: [section 3.2] "utilize a dynamic programming procedure to accelerate the inference and training processes... resulting in linear complexity O(L)" [section 3.2] "we can calculate {hi}L i=1 with only two traversals... thereby reducing the computational complexity from O(L2) to O(L)"

### Mechanism 3
- Claim: Fine-tuning pre-trained models with tree topology scanning provides consistent improvements across tasks
- Mechanism: Tree-based aggregation captures long-range dependencies during fine-tuning without requiring full model retraining
- Core assumption: Pre-trained model parameters are sufficiently general to benefit from enhanced long-range modeling
- Evidence anchors: [abstract] "by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost" [section 4.4] "we first fine-tune pre-trained Mamba via LoRA [30] and GrootL under the same setting"

## Foundational Learning

- Concept: Minimum Spanning Tree algorithms
  - Why needed here: The method relies on constructing MST from feature dissimilarity graphs
  - Quick check question: Can you explain why Boruvka's algorithm is used instead of Prim's or Kruskal's?

- Concept: Dynamic programming principles
  - Why needed here: The linear complexity is achieved through DP optimization over tree structures
  - Quick check question: What are the two key traversals needed for the forward and backward passes?

- Concept: State space model discretization
  - Why needed here: The method builds upon Mamba's selective state space framework
  - Quick check question: How does the zero-order hold rule convert continuous to discrete parameters?

## Architecture Onboarding

- Component map: Stem → Basic Blocks → Downsampling layers → Tree State Space Model → FFN
- Critical path: Input → Stem → Basic Block (Tree Scanning + FFN) → Output
- Design tradeoffs: Tree topology provides better long-range modeling but requires MST construction overhead
- Failure signatures: Degraded performance on tasks requiring local feature interactions, unexpected sensitivity to distance metrics
- First 3 experiments:
  1. Verify MST construction produces reasonable tree structures on simple 2D grids
  2. Compare forward pass outputs with baseline Mamba implementation
  3. Test backpropagation gradients for correctness on small synthetic datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the tree topology generation algorithm perform when applied to non-sequential data modalities like point clouds or 3D meshes?
- Basis in paper: [explicit] The paper states "GrootVL is a versatile multimodal framework that can be applied to both visual and textual tasks" and describes using dissimilarity metrics to construct minimum spanning trees, but only validates on 2D images and 1D text sequences.
- Why unresolved: The methodology section describes constructing trees from feature dissimilarity, but doesn't explore how this generalizes to non-sequential spatial data with different connectivity patterns.
- What evidence would resolve it: Empirical results showing performance on 3D data tasks, or theoretical analysis of how the 4-connected graph assumption extends to higher dimensions.

### Open Question 2
- Question: What is the impact of using different graph construction strategies (like k-nearest neighbors vs. minimum spanning tree) on the final performance?
- Basis in paper: [explicit] The paper uses minimum spanning trees and states "Following [64, 50], we set m = 4 for visual tasks... The edge weight is calculated by the feature dissimilarity between adjacent vertices."
- Why unresolved: While the paper justifies MST choice based on prior work, it doesn't systematically compare against other graph construction methods or analyze the sensitivity to this design choice.
- What evidence would resolve it: Controlled experiments comparing MST with k-NN graphs, fully-connected graphs with sparsification, or learned graph construction methods.

### Open Question 3
- Question: How does the computational complexity scale when increasing the tree connectivity (m) beyond the default values?
- Basis in paper: [inferred] The paper mentions m as a hyperparameter and uses m=4 for visual tasks, but doesn't explore the trade-off between connectivity and computational cost.
- Why unresolved: The dynamic programming optimization achieves O(L) complexity for m=4, but the complexity analysis for larger m values is not provided, leaving uncertainty about scalability.
- What evidence would resolve it: Empirical complexity analysis showing runtime and memory scaling with different m values, or theoretical bounds on the dynamic programming algorithm's complexity for varying connectivity.

## Limitations
- Performance heavily depends on quality of feature dissimilarity calculations for MST construction
- Computational overhead of MST generation could become prohibitive for very large inputs
- Lacks comprehensive ablation studies on distance metrics and tree depth

## Confidence

- Vision task performance claims: Medium - Results are competitive but lack comparison against newer transformer architectures
- Computational complexity claims: Low - The linear complexity assertion needs independent verification through detailed profiling
- Language task improvements: Medium - Fine-tuning results are positive but based on a single pre-trained model (Mamba)

## Next Checks

1. Implement MST construction with multiple distance metrics (cosine, Euclidean, learned) and compare resulting tree structures and model performance
2. Profile computational complexity across varying input sizes to verify the claimed O(L) scaling versus baseline SSMs
3. Test transferability of the tree topology scanning approach to other SSM architectures (Hyena, RWKV) to assess method generality