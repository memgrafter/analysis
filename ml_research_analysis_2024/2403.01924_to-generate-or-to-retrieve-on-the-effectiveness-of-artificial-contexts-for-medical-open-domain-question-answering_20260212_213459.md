---
ver: rpa2
title: To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for
  Medical Open-Domain Question Answering
arxiv_id: '2403.01924'
source_url: https://arxiv.org/abs/2403.01924
tags:
- context
- question
- shot
- contexts
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedGENIE, the first generate-then-read framework
  for medical open-domain question answering. The method generates multi-view artificial
  contexts using a medical LLM via in-context learning or fine-tuning, then grounds
  a prompted LLM or fine-tuned SLM on these contexts to answer multiple-choice questions.
---

# To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering

## Quick Facts
- arXiv ID: 2403.01924
- Source URL: https://arxiv.org/abs/2403.01924
- Reference count: 40
- Primary result: A small 137M parameter model fine-tuned on generated contexts outperforms 175B zero-shot baselines in medical QA

## Executive Summary
This paper introduces MedGENIE, the first generate-then-read framework for medical open-domain question answering. The method generates multi-view artificial contexts using a medical LLM via in-context learning or fine-tuning, then grounds a prompted LLM or fine-tuned SLM on these contexts to answer multiple-choice questions. Extensive experiments on MedQA-USMLE, MedMCQA, and MMLU show MedGENIE achieves state-of-the-art accuracy in the open-book setting, allowing a small-scale reader to outperform zero-shot closed-book 175B baselines while using up to 706× fewer parameters. Generated contexts are shown to be more effective than retrieved ones, with generated passages notably enhancing retrieve-then-read workflows.

## Method Summary
MedGENIE generates multi-view artificial contexts using a medical LLM (PMC-LLaMA-13B), creating both option-focused contexts (conditioned on question+answer) and option-free contexts (conditioned on question only). These contexts are then used to either prompt an LLM in few-shot style or fine-tune a small Fusion-In-Decoder (FID) reader. The framework is evaluated across three medical benchmarks, showing that generated contexts outperform retrieved ones in both accuracy and quality metrics, with the fine-tuned FID reader achieving state-of-the-art results while using orders of magnitude fewer parameters than baseline models.

## Key Results
- MedGENIE-FID-Flan-T5 achieves 53.1% accuracy on MedQA, outperforming 7B baselines and CODEX 175B in zero-shot settings
- Generated contexts achieve 91-98% recall@1 when reranked against retrieved contexts across all benchmarks
- The framework allows a 137M parameter model to outperform 175B zero-shot baselines, representing a 706× reduction in parameters
- Generated passages improve retrieve-then-read workflows, achieving 59.2% accuracy on MedQA when used as additional context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generated contexts improve reader accuracy more than retrieved ones by reducing noise and increasing relevance.
- Mechanism: A medical LLM (PMC-LLaMA-13B) generates question-specific passages, which are tailored to both the question and answer options, thus providing more focused and relevant context than retrieved Wikipedia or textbook chunks.
- Core assumption: The medical LLM's internal knowledge is sufficiently accurate and comprehensive to generate useful contexts without external retrieval.
- Evidence anchors:
  - [abstract] "Our findings reveal that generated passages are more effective than retrieved ones in attaining higher accuracy."
  - [section 5.2] Reranking results show 91%, 98%, 96% recall@1 favoring generated contexts across MedQA, MedMCQA, and MMLU.
  - [corpus] Weak evidence: neighbor papers focus on retrieval-augmented generation, not generation-first pipelines.
- Break condition: If the medical LLM hallucinates or lacks coverage, generated contexts may introduce misleading or irrelevant information, reducing accuracy.

### Mechanism 2
- Claim: Multi-view context generation (option-focused + option-free) enhances diversity and knowledge coverage, improving reader performance.
- Mechanism: By generating 3 option-focused contexts (conditioned on question+answer) and 2 option-free contexts (conditioned on question only), the framework covers both answer-specific cues and broader domain concepts, which aids multi-hop reasoning.
- Core assumption: Different question types benefit from both answer-targeted and general background information.
- Evidence anchors:
  - [section 3.1] Explicitly states division into Ca and Cb to promote intra-context diversity.
  - [section B.3] Table 6 shows accuracy gains when adding option-free contexts across all models.
  - [corpus] No direct corpus evidence; inferred from ablation design.
- Break condition: If option-focused contexts dominate and option-free ones add noise, performance may plateau or degrade.

### Mechanism 3
- Claim: Fine-tuning a small Fusion-In-Decoder (FID) reader on generated contexts allows a tiny model to outperform large zero-shot baselines.
- Mechanism: The FID reader learns to attend across concatenated contexts and align them with answer choices during supervised training, internalizing the structure of generated passages.
- Core assumption: Generated contexts are high quality enough that supervised fine-tuning yields generalizable reasoning patterns.
- Evidence anchors:
  - [section 5.1] "MedGENIE-FID-Flan-T5 achieves 53.1% accuracy on MedQA, outshining 7B baselines and CODEX 175B in zero-shot settings."
  - [section 5.2] RAGAS evaluation shows generated contexts have higher faithfulness and precision than retrieved ones.
  - [corpus] No corpus evidence; performance claims are internal to this paper.
- Break condition: If generated contexts are noisy or inconsistent, fine-tuning may overfit to artifacts rather than robust reasoning.

## Foundational Learning

- Concept: Large language model prompting and in-context learning
  - Why needed here: The framework relies on few-shot prompts to guide a medical LLM to generate contexts and to equip an ICL reader to answer questions.
  - Quick check question: How does the prompt structure differ between option-focused and option-free context generation?
- Concept: Retrieval-augmented generation vs. generative augmentation
  - Why needed here: The paper contrasts generated contexts against traditional retrieved ones, so understanding RAG tradeoffs is essential.
  - Quick check question: What are the key differences in how relevance is determined for generated vs. retrieved contexts?
- Concept: Fusion-In-Decoder architecture
  - Why needed here: The supervised pipeline uses FID to encode multiple contexts independently and decode jointly, enabling efficient multi-context reasoning.
  - Quick check question: How does FID's concatenation and decoding step differ from standard encoder-decoder attention?

## Architecture Onboarding

- Component map: Generator (PMC-LLaMA-13B) -> Context Manager (splits into Ca and Cb) -> Reader (ICL LLM or FID Flan-T5) -> Answer Decoder
- Critical path: Generate Ca and Cb → concatenate or feed separately → prompt reader → greedy decode answer; For FID: Generate Ca and Cb → encode independently → concatenate representations → decode answer
- Design tradeoffs:
  - Multi-view generation increases latency but improves accuracy; clustering-based prompting could improve diversity but is costlier
  - FID allows efficient multi-context encoding but loses per-context attention; ICL retains full attention but needs longer prompts
  - Using a 13B generator trades parameter efficiency for potential hallucination risk
- Failure signatures:
  - Accuracy drops if contexts are too long for reader context window
  - Performance plateaus if generated contexts are repetitive or irrelevant
  - ICL accuracy degrades if prompt examples are poorly chosen or context is "lost in the middle"
- First 3 experiments:
  1. Run ablation on number/type of generated contexts (k=1,2,3,4,5 with option-free vs. only option-focused)
  2. Compare generated vs. retrieved contexts using a reranker to quantify relevance preference
  3. Evaluate context quality with RAGAS (CR, CP, F) to validate faithfulness of generated contexts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would MEDGENIE perform if the context generator (PMC-LLaMA-13B) were replaced with a larger medical LLM like Meditron-70B or OpenBioLLM-8B with an extended context window?
- Basis in paper: Explicit - The paper acknowledges this as a future work direction in the Limitations section, noting that PMC-LLaMA has a restricted 2,000-token context window while newer models have 8K tokens.
- Why unresolved: The study used PMC-LLaMA-13B specifically due to VRAM constraints (24GB), making it impossible to test larger models on the same hardware configuration.
- What evidence would resolve it: Direct experiments comparing MEDGENIE with PMC-LLaMA-13B versus MEDGENIE with Meditron-70B or OpenBioLLM-8B as the context generator on the same benchmarks, measuring accuracy improvements.

### Open Question 2
- Question: What is the optimal number and ratio of option-focused versus option-free contexts to generate for maximum performance?
- Basis in paper: Explicit - The paper conducts ablation studies showing that using 3 option-focused + 2 option-free contexts yields better accuracy than 5 option-focused contexts alone, but doesn't explore other combinations.
- Why unresolved: The study only tested one specific configuration (3+2) after initial experiments, leaving open whether configurations like 4+1, 2+3, or different total numbers might be optimal for different benchmarks or reader types.
- What evidence would resolve it: Systematic experiments varying the number of option-focused and option-free contexts (e.g., 2+1, 4+1, 3+3, 5+0) across all benchmarks and reader types to identify optimal configurations.

### Open Question 3
- Question: How does MEDGENIE's performance compare when using knowledge bases other than MedWiki, such as PubMed or specialized medical textbooks?
- Basis in paper: Explicit - The paper mentions in the appendix that PubMed and similar sources offer more comprehensive medical knowledge but chose MedWiki for practical reasons, and conducted limited experiments with Textbooks showing only marginal improvements.
- Why unresolved: The study primarily used MedWiki as the knowledge base for comparison due to practical constraints, without comprehensive testing of other knowledge sources or their combinations with generated contexts.
- What evidence would resolve it: Direct experiments comparing MEDGENIE with retrieval-augmented approaches using PubMed, specialized medical textbooks, or combinations thereof as knowledge bases, measuring accuracy and computational efficiency trade-offs.

## Limitations

- **Knowledge Coverage Dependency:** The framework assumes the medical LLM has sufficient knowledge to generate accurate contexts, creating a systematic dependency on the generator's knowledge completeness.
- **Optimal Context Composition Uncertainty:** The paper shows 3:2 ratio works well but doesn't establish whether this is optimal or varies by question type.
- **Domain-Specific Validation:** The claim that generated contexts are universally more effective than retrieved ones is only validated within the medical domain.

## Confidence

- **High Confidence:** The parameter efficiency claim (137M model outperforming 175B zero-shot baselines) is well-supported by accuracy numbers and verifiable model sizes.
- **Medium Confidence:** The multi-view generation mechanism shows consistent improvements but optimal ratios remain unproven.
- **Low Confidence:** The absolute claim that generated contexts are "more effective than retrieved ones" requires careful interpretation - preference in reranking doesn't guarantee retrieval couldn't match performance with different strategies.

## Next Checks

1. **Knowledge Gap Analysis:** Conduct systematic evaluation of whether generated contexts cover all correct answer information by comparing generated contexts against ground truth explanations and identifying systematic omissions, particularly for rare conditions or recent medical guidelines.

2. **Cross-Domain Generalization Test:** Apply the same generate-then-read framework to non-medical QA benchmarks (e.g., general science or history questions) to validate whether the generation advantage over retrieval generalizes beyond the medical domain where the generator has specialized knowledge.

3. **Optimal Context Composition Study:** Systematically vary the ratio of option-focused to option-free contexts (e.g., 1:4, 2:3, 4:1) and test different question difficulty levels to identify whether the 3:2 ratio is optimal or whether different question types benefit from different context compositions.