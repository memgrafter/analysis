---
ver: rpa2
title: 'PGN: The RNN''s New Successor is Effective for Long-Range Time Series Forecasting'
arxiv_id: '2409.17703'
source_url: https://arxiv.org/abs/2409.17703
tags:
- information
- tpgn
- forecasting
- series
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PGN, a novel paradigm that replaces RNN's
  recurrent structure with a Historical Information Extraction layer and gated mechanisms,
  reducing information propagation paths to O(1) while maintaining RNN's complexity
  of O(L). The authors then propose TPGN, a temporal modeling framework based on PGN
  that uses two branches to separately model long-term periodic patterns and short-term
  information through 2D transformations.
---

# PGN: The RNN's New Successor is Effective for Long-Range Time Series Forecasting

## Quick Facts
- arXiv ID: 2409.17703
- Source URL: https://arxiv.org/abs/2409.17703
- Reference count: 40
- Primary result: PGN achieves 12.35% average MSE improvement on 5 benchmark datasets

## Executive Summary
This paper introduces PGN (Parallel Gated Network), a novel paradigm that replaces RNN's recurrent structure with a Historical Information Extraction layer and gated mechanisms, reducing information propagation paths to O(1) while maintaining RNN's complexity of O(L). The authors then propose TPGN, a temporal modeling framework based on PGN that uses two branches to separately model long-term periodic patterns and short-term information through 2D transformations. TPGN achieves O(√L) complexity and demonstrates state-of-the-art performance across five benchmark datasets.

## Method Summary
The method introduces PGN, which replaces RNN's sequential recurrence with parallel computation using a Historical Information Extraction (HIE) layer that aggregates all historical information in one operation. A single gated mechanism controls information selection and fusion. TPGN extends this by reshaping 1D time series into 2D matrices and using two branches: one with PGN for long-term periodic patterns (column-wise) and another for short-term variations (row-wise patches). The framework achieves O(√L) complexity while maintaining expressiveness.

## Key Results
- TPGN achieves 12.35% average MSE improvement across five benchmark datasets (ECL, Traffic, ETTh1, ETTh2, Weather)
- Reduces information propagation paths from O(L) to O(1) compared to RNN
- Achieves O(√L) complexity through 2D transformation approach
- Maintains RNN-level expressiveness while enabling full parallelization

## Why This Works (Mechanism)

### Mechanism 1
PGN reduces RNN information propagation path from O(L) to O(1) by using a Historical Information Extraction layer that aggregates all historical information in parallel at each time step. Instead of sequential recurrence, PGN uses a linear HIE layer that takes all previous L-1 time steps and aggregates them in one operation, then applies gated fusion with current step. Core assumption: Parallel aggregation preserves temporal dependencies while eliminating sequential bottlenecks. Break condition: If HIE aggregation loses critical temporal ordering information, or if gating mechanism cannot effectively distinguish relevant historical information.

### Mechanism 2
TPGN achieves O(√L) complexity by transforming 1D series to 2D and separately modeling rows (short-term) and columns (long-term periodic patterns). 1D series of length L is reshaped to R×P matrix where R×P=L. Long-term branch processes each column independently with PGN (O(R) complexity), short-term branch processes rows with patches (O(P) complexity). Total complexity O(R+P) = O(√L). Core assumption: Time series exhibit separable short-term local variations and long-term periodic patterns that can be effectively modeled independently in 2D structure. Break condition: If series does not exhibit clear separable periodic patterns, or if 2D transformation loses critical temporal continuity.

### Mechanism 3
Parallel computation in PGN maintains RNN-level expressiveness while achieving much higher practical efficiency than sequential RNNs. Each time step computation is independent, allowing full parallelization. Gated mechanism (single gate) replaces multiple gates in LSTM/GRU, reducing parameter count while maintaining information selection capability. Core assumption: Single gate can effectively control both selection and fusion of historical and current information without losing expressiveness. Break condition: If single gate cannot adequately capture complex temporal dependencies, or if parallelization overhead exceeds benefits for small sequences.

## Foundational Learning

- **Information propagation path complexity**: Understanding how O(1) vs O(L) vs O(√L) complexity impacts model performance and scalability. Quick check: If a model processes sequence of length 1000, how many computational steps does O(1) vs O(L) vs O(√L) complexity require?

- **Time series periodicity and decomposition**: TPGN relies on separating periodic long-term patterns from short-term variations. Quick check: Given a 24-hour temperature series with daily cycle, how would you reshape it to capture hourly vs daily patterns?

- **Gated neural network mechanisms**: PGN uses gating to select/fuse information. Quick check: What's the difference between LSTM's three gates and PGN's single gate in terms of information flow control?

## Architecture Onboarding

- **Component map**: Input → Reshape(1D→2D) → Long-term Branch (PGN + Linear) → Short-term Branch (Linear rows + Linear cols) → Concat → Linear forecast → Output
- **Critical path**: For each prediction: reshape input → process long-term branch (column-wise PGN) → process short-term branch (row-wise patch aggregation) → merge branches → final linear layer
- **Design tradeoffs**: PGN trades sequential recurrence for parallel computation, TPGN trades 1D temporal continuity for 2D pattern separability
- **Failure signatures**: Training instability: Check HIE layer initialization and gradient flow; Poor long-term capture: Verify 2D reshaping preserves periodic patterns; Overfitting: Reduce dm or add regularization to gating mechanism
- **First 3 experiments**:
  1. Test PGN on synthetic periodic data to verify O(1) path captures dependencies
  2. Compare TPGN vs PGN on 1D vs 2D reshaped data to validate decomposition benefits
  3. Benchmark parallel vs sequential execution to confirm efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
How does PGN perform on tasks beyond time series forecasting, such as natural language processing or computer vision? Basis in paper: The paper mentions PGN as a general paradigm that could potentially replace RNNs in various time series analysis tasks and other research areas. Why unresolved: The paper focuses specifically on time series forecasting and does not explore PGN's performance in other domains. What evidence would resolve it: Experiments applying PGN to tasks in NLP, CV, or other sequential data domains, comparing its performance to existing state-of-the-art methods.

### Open Question 2
What is the optimal way to integrate variable relationship modeling into TPGN for multivariate time series forecasting? Basis in paper: The paper acknowledges that modeling relationships between variables is not addressed in their current work and suggests incorporating components from other methods specialized in variable modeling. Why unresolved: The paper does not provide a specific method or framework for integrating variable relationship modeling into TPGN. What evidence would resolve it: Development and experimental validation of a modified TPGN architecture that incorporates variable relationship modeling, demonstrating improved performance on multivariate forecasting tasks.

### Open Question 3
How does the performance of PGN-based variants (e.g., TPGN-GRU, TPGN-LSTM, TPGN-MLP) compare to PGN in different time series forecasting scenarios? Basis in paper: The ablation study shows that these variants sometimes outperform previous SOTA methods in certain tasks, but the paper doesn't provide a comprehensive comparison across all scenarios. Why unresolved: The ablation study is limited in scope and doesn't explore the full range of forecasting tasks and datasets. What evidence would resolve it: Extensive experiments comparing PGN and its variants across a wide range of time series forecasting tasks, datasets, and evaluation metrics to determine their relative strengths and weaknesses.

## Limitations

- The paper lacks empirical validation of actual computational complexity across different sequence lengths
- The core assumption that parallel aggregation preserves temporal dependencies without sequential recurrence is not thoroughly validated
- The 2D decomposition approach assumes time series exhibit clear separable periodic patterns, which may not hold for all real-world datasets

## Confidence

- **High Confidence**: PGN's theoretical O(1) complexity reduction from RNN's O(L) is mathematically sound and the architectural design is clearly specified
- **Medium Confidence**: The 12.35% MSE improvement claims across five datasets are reported but lack statistical significance testing and ablation studies to isolate PGN's contribution
- **Low Confidence**: The assumption that single-gate mechanisms can fully replace multi-gate RNN architectures without loss of expressiveness, and that all time series can be effectively decomposed into 2D patterns

## Next Checks

1. **Complexity Validation**: Measure actual wall-clock time and FLOPs for PGN vs RNN across varying sequence lengths (100, 1000, 10000) to empirically verify the claimed O(1) vs O(L) complexity scaling

2. **Dependency Preservation Test**: Create synthetic time series with known temporal dependencies (linear, periodic, chaotic) and test whether PGN's parallel aggregation preserves these dependencies compared to sequential RNN processing

3. **Generalization Cross-Dataset**: Apply TPGN to datasets with varying periodicity characteristics (daily, weekly, monthly cycles) to test the robustness of the 2D decomposition assumption across different temporal patterns