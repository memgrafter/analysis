---
ver: rpa2
title: 'HMGIE: Hierarchical and Multi-Grained Inconsistency Evaluation for Vision-Language
  Data Cleansing'
arxiv_id: '2412.05685'
source_url: https://arxiv.org/abs/2412.05685
tags:
- semantic
- evaluation
- graph
- hmgie
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HMGIE, a hierarchical and multi-grained inconsistency
  evaluation framework for visual-language data cleansing. HMGIE addresses the challenge
  of evaluating diverse types of inconsistencies across captions of varying lengths
  by converting captions into semantic graphs, performing progressive hierarchical
  evaluation through dynamic question generation, and computing complementary accuracy
  and completeness scores.
---

# HMGIE: Hierarchical and Multi-Grained Inconsistency Evaluation for Vision-Language Data Cleansing

## Quick Facts
- **arXiv ID**: 2412.05685
- **Source URL**: https://arxiv.org/abs/2412.05685
- **Reference count**: 40
- **Key result**: Proposes a hierarchical and multi-grained inconsistency evaluation framework that achieves superior performance in vision-language data cleansing with TPR of 94.94% and F1 score of 92.68% on the MVTID dataset

## Executive Summary
This paper addresses the challenge of evaluating diverse types of inconsistencies in vision-language data, particularly captions of varying lengths. The proposed HMGIE framework converts captions into semantic graphs, performs progressive hierarchical evaluation through dynamic question generation, and computes complementary accuracy and completeness scores. The method demonstrates superior performance compared to existing approaches across multiple datasets and tasks, achieving strong results in true positive rate, false positive rate, and F1 score metrics. The framework's effectiveness is validated through extensive ablation studies and qualitative analysis.

## Method Summary
HMGIE is a three-module framework that addresses visual-textual inconsistency (VTI) evaluation. The semantic graph generation module converts captions into structured representations using GPT-4o, capturing entities, locations, concepts, events, attributes, and relationships. The hierarchical inconsistency evaluation module progressively constructs a hierarchical inconsistency evaluation graph (HIEG) through dynamic question-answer generation guided by the semantic graph. The quantitative evaluation module calculates H-Scores (accuracy and completeness) and makes overall consistency decisions. The framework operates across four granularity levels and uses both LLMs and MLLMs to evaluate semantic consistency between image and text content.

## Key Results
- Achieves true positive rate of 94.94% and F1 score of 92.68% on the newly constructed MVTID dataset
- Demonstrates strong performance on fake news detection with TPR of 95.56% and text-to-image generation evaluation with TPR of 96.46%
- Shows superior performance compared to existing approaches across multiple benchmark datasets including NewsCLIPpings, TIIL, and SeeTRUE

## Why This Works (Mechanism)
HMGIE works by converting the semantic evaluation problem into a structured graph representation and progressive question-answering task. By parsing captions into semantic graphs with explicit entities, relationships, and attributes, the framework can systematically identify inconsistencies at different levels of abstraction. The hierarchical evaluation strategy allows for progressive refinement of consistency assessment, starting from high-level concepts and drilling down to specific details. The use of both LLMs for semantic parsing and MLLMs for visual understanding enables comprehensive evaluation of cross-modal consistency. The dual accuracy and completeness scoring system provides complementary perspectives on inconsistency detection, improving overall robustness.

## Foundational Learning
- **Semantic Graph Generation**: Converting natural language captions into structured representations with entities, relationships, and attributes - needed to systematically capture semantic content for evaluation; quick check: verify node and edge coverage against reference captions
- **Hierarchical Inconsistency Evaluation**: Progressive evaluation strategy that builds evaluation questions from high-level to specific details - needed to handle captions of varying lengths and complexity; quick check: measure performance degradation with fewer evaluation levels
- **Dynamic Question Generation**: LLM-guided generation of evaluation questions based on semantic graph content - needed to create relevant and comprehensive evaluation queries; quick check: analyze question relevance scores from the LLM
- **Cross-modal Consistency Assessment**: Using both text-based LLMs and vision-based MLLMs for evaluation - needed to capture both semantic and visual aspects of consistency; quick check: compare results using different LLM/MLLM combinations
- **Dual Scoring System**: Computing both accuracy and completeness metrics - needed to provide complementary views of inconsistency detection; quick check: analyze correlation between accuracy and completeness scores
- **Granularity-Aware Evaluation**: Adapting evaluation strategy based on caption length and complexity - needed to handle diverse caption types effectively; quick check: measure performance across different granularity levels

## Architecture Onboarding
- **Component Map**: Semantic Graph Generation -> Hierarchical Inconsistency Evaluation -> Quantitative Evaluation
- **Critical Path**: Input image-caption pair → Semantic graph parsing → Dynamic question generation → MLLM visual evaluation → Consistency scoring → Output decision
- **Design Tradeoffs**: Computational efficiency vs. evaluation depth; LLM model selection vs. performance; granularity level selection vs. evaluation accuracy
- **Failure Signatures**: Poor semantic graph quality → ineffective question generation; over-reliance on specific LLM capabilities → performance degradation; fixed granularity approach → suboptimal performance on boundary cases
- **Three First Experiments**: 1) Implement semantic graph generation module and validate node/edge coverage; 2) Test hierarchical evaluation with different maximum levels on sample captions; 3) Compare H-Scores using different LLM/MLLM combinations

## Open Questions the Paper Calls Out
- **Open Question 1**: How does HMGIE handle out-of-context knowledge requirements when evaluating captions that reference external facts or events? The paper shows HMGIE struggles with cases requiring external knowledge verification but doesn't propose solutions for integrating fact-checking mechanisms.
- **Open Question 2**: What is the computational overhead of HMGIE compared to simpler embedding-based methods, and how does this impact practical deployment? The paper acknowledges computational constraints but doesn't provide concrete measurements or runtime comparisons.
- **Open Question 3**: How does semantic graph generation quality impact overall performance, and what is the error propagation from semantic graph errors to final evaluation results? The paper shows ablation studies but doesn't analyze semantic graph accuracy or error propagation.

## Limitations
- Heavy dependence on the quality of semantic graph generation, which relies on GPT-4o and may suffer from hallucination or context misinterpretation
- Significant computational resource requirements for progressive question generation and evaluation using both LLMs and MLLMs
- Fixed granularity approach may not optimally handle extremely short or long captions, potentially degrading performance at boundaries

## Confidence
- **High Confidence**: Quantitative results on MVTID dataset and benchmark tasks are well-documented with clear methodology and robust validation through ablation studies
- **Medium Confidence**: Generalizability across diverse real-world applications beyond tested domains requires further validation
- **Low Confidence**: Optimal parameter configurations for different application domains are not extensively explored, and scalability to large-scale datasets remains unclear

## Next Checks
1. **Cross-LLM Validation**: Evaluate HMGIE's performance using different combinations of LLMs and MLLMs to assess robustness to model variations and accessibility constraints
2. **Granularity Boundary Testing**: Systematically test the framework's performance on captions at the boundaries of each granularity level to identify failure modes and improve the hierarchical evaluation strategy
3. **Real-time Performance Analysis**: Measure computational efficiency and scalability of HMGIE on large-scale datasets to determine practical applicability for industrial-scale data cleansing tasks