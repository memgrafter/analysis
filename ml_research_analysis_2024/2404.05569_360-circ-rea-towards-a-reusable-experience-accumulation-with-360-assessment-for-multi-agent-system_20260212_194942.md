---
ver: rpa2
title: "360$^\\circ$REA: Towards A Reusable Experience Accumulation with 360\xB0 Assessment\
  \ for Multi-Agent System"
arxiv_id: '2404.05569'
source_url: https://arxiv.org/abs/2404.05569
tags:
- experience
- agents
- agent
- assessment
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces 360\xB0REA, a multi-agent framework that\
  \ enhances LLM agent performance through comprehensive 360-degree evaluation and\
  \ dual-level experience accumulation. Unlike prior methods focusing on self-reflection\
  \ or agent elimination, 360\xB0REA implements peer, supervisory, and self-assessment\
  \ across agents, storing experiences at both local (task-specific) and global (system-wide)\
  \ levels."
---

# 360$^\circ$REA: Towards A Reusable Experience Accumulation with 360° Assessment for Multi-Agent System

## Quick Facts
- arXiv ID: 2404.05569
- Source URL: https://arxiv.org/abs/2404.05569
- Reference count: 9
- Primary result: 360°REA achieves 87% match rate in creative writing and superior correctness, novelty, and customization scores in travel planning compared to state-of-the-art baselines

## Executive Summary
360°REA introduces a novel multi-agent framework that enhances LLM agent performance through comprehensive 360-degree evaluation and dual-level experience accumulation. Unlike prior methods focusing on self-reflection or agent elimination, 360°REA implements peer, supervisory, and self-assessment across agents, storing experiences at both local (task-specific) and global (system-wide) levels. The framework demonstrates significant improvements over state-of-the-art baselines on two benchmark datasets, achieving impressive results in creative writing generation and travel planning tasks through its unique assessment and accumulation mechanisms.

## Method Summary
360°REA operates through a three-stage process: first, agents complete assigned tasks while simultaneously assessing each other's work through peer, supervisory, and self-evaluation; second, the system accumulates experiences from both task execution and assessment phases into local and global experience pools; third, when new tasks arrive, agents retrieve relevant experiences from these pools to inform their approach. The framework uniquely combines 360-degree assessment (peer, supervisory, and self-evaluation) with dual-level experience storage, allowing agents to learn from both individual task outcomes and system-wide patterns across multiple agents and tasks.

## Key Results
- Achieved 87% match rate in creative writing evaluation against state-of-the-art baselines
- Demonstrated superior performance in travel planning with higher scores in correctness, novelty, and customization metrics
- Outperformed both SPP and GPT-4 baselines across multiple semantic similarity metrics (BERTScore, MoverScore) on two benchmark datasets

## Why This Works (Mechanism)
The framework's effectiveness stems from its comprehensive assessment approach and dual-level experience accumulation. By incorporating peer, supervisory, and self-assessment, 360°REA captures multiple perspectives on agent performance, reducing blind spots inherent in single-source evaluation. The dual-level experience storage (local for task-specific patterns, global for system-wide insights) enables agents to leverage both individual task learning and collective wisdom across the agent community, creating a rich knowledge base that improves performance on new, similar tasks.

## Foundational Learning
- Multi-agent coordination: Essential for managing multiple LLM agents working simultaneously on tasks and assessments; verify through concurrent task completion rates
- Experience accumulation mechanisms: Critical for storing and retrieving learned patterns across tasks; validate through retrieval accuracy and task improvement correlation
- Semantic similarity metrics: Necessary for objective evaluation of creative and planning outputs; check through metric consistency across different task types
- Peer assessment dynamics: Important for capturing diverse perspectives on agent performance; test through inter-rater reliability measures
- Experience pool management: Key for maintaining and updating knowledge bases; monitor through pool size, relevance decay, and retrieval efficiency
- 360-degree feedback integration: Vital for comprehensive performance evaluation; assess through feedback coverage and impact on task outcomes

## Architecture Onboarding

Component Map:
Task Assignment -> Agent Execution -> 360-Degree Assessment (Peer -> Supervisory -> Self) -> Experience Accumulation (Local -> Global) -> Experience Retrieval -> Task Completion

Critical Path:
The critical execution path flows from task assignment through agent execution, followed by the comprehensive 360-degree assessment cycle, then experience accumulation into both local and global pools, and finally experience retrieval for subsequent task completion. This creates a continuous learning loop where each completed task contributes to the system's collective knowledge base.

Design Tradeoffs:
The framework trades computational overhead for comprehensive assessment and knowledge accumulation. While 360-degree evaluation requires more processing time than single-source feedback, it provides richer learning signals. The dual-level experience storage increases memory requirements but enables both task-specific and generalized learning. The peer assessment component assumes all agents can meaningfully evaluate each other, which may not hold for highly specialized tasks.

Failure Signatures:
- Performance degradation when agent diversity decreases, limiting peer assessment value
- Memory bloat from excessive experience storage without proper decay mechanisms
- Assessment bias amplification when peer groups become too homogeneous
- Retrieval inefficiency when experience pools become too large without effective indexing
- Task mismatch when global experiences override task-specific requirements

First 3 Experiments:
1. Compare 360°REA performance against single-source feedback systems on identical task sets
2. Measure the impact of removing either local or global experience pools on task completion quality
3. Test system performance with varying agent counts (2, 5, 10 agents) on identical task sequences

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Heavy reliance on semantic similarity metrics rather than human evaluation introduces uncertainty about practical output quality
- Scalability concerns regarding system performance with increasing numbers of agents and tasks
- Unclear isolation of 360-degree assessment's unique contribution versus experience accumulation mechanism

## Confidence
- Human evaluation validation: Medium - semantic metrics used instead of human judgment
- Generalizability across task types: High - peer assessment assumption may not hold for specialized tasks
- Unique value of 360-degree assessment: Medium - ablation studies don't isolate component contributions

## Next Checks
1. Conduct human evaluation studies comparing 360°REA outputs against baseline methods across both task domains to validate semantic metric findings
2. Test system performance and efficiency with varying numbers of agents (5x, 10x baseline) to assess scalability claims
3. Implement a controlled experiment isolating the 360-degree assessment from experience accumulation to measure each component's independent contribution to performance gains