---
ver: rpa2
title: 'VALOR-EVAL: Holistic Coverage and Faithfulness Evaluation of Large Vision-Language
  Models'
arxiv_id: '2404.13874'
source_url: https://arxiv.org/abs/2404.13874
tags:
- object
- objects
- image
- evaluation
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VALOR-EVAL, a holistic framework for evaluating
  hallucinations in large vision-language models (LVLMs). The framework generalizes
  the CHAIR metric to handle open vocabulary settings and incorporates both faithfulness
  and coverage into evaluation.
---

# VALOR-EVAL: Holistic Coverage and Faithfulness Evaluation of Large Vision-Language Models

## Quick Facts
- arXiv ID: 2404.13874
- Source URL: https://arxiv.org/abs/2404.13874
- Reference count: 40
- Even GPT-4V achieves high coverage but relatively low faithfulness, highlighting the critical trade-off between these metrics

## Executive Summary
This paper introduces VALOR-EVAL, a comprehensive framework for evaluating hallucinations in Large Vision-Language Models (LVLMs). The framework addresses the limitations of existing evaluation methods by generalizing the CHAIR metric to handle open vocabulary settings and incorporating both faithfulness and coverage into the evaluation process. The authors construct VALOR-BENCH, a multi-dimensional benchmark dataset covering objects, attributes, and relations, with challenging images selected based on associative biases. Experiments on 10 established LVLMs demonstrate that the proposed evaluation metric correlates better with human judgment than existing methods, revealing the critical trade-off between faithfulness and coverage even in advanced models like GPT-4V.

## Method Summary
The VALOR-EVAL framework employs a two-stage LLM-based evaluation approach that generalizes the CHAIR metric for open vocabulary settings. First, an LLM extracts features from generated captions, then another LLM semantically matches these extracted features with ground-truth features, allowing for synonym detection beyond fixed vocabulary lists. The framework introduces dual metrics measuring both faithfulness (accuracy of generated features matching image content) and coverage (comprehensiveness of capturing all ground-truth features). To create a challenging benchmark, the authors construct VALOR-BENCH by selecting images based on associative bias analysis, identifying cases where LVLMs are likely to hallucinate due to training data co-occurrence patterns. The dataset systematically covers objects, attributes, and relations to provide comprehensive evaluation.

## Key Results
- The proposed VALOR-EVAL metric correlates better with human judgment than existing evaluation methods
- GPT-4V achieves high coverage but relatively low faithfulness, demonstrating that even state-of-the-art models struggle with hallucinations
- A clear trade-off exists between faithfulness and coverage across all evaluated LVLMs, with some models prioritizing precision while others emphasize comprehensiveness
- Images selected based on associative biases effectively expose LVLM vulnerabilities compared to random image selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage LLM-based evaluation framework generalizes the CHAIR metric to handle open vocabulary settings by leveraging semantic matching instead of fixed synonym lists.
- Mechanism: First stage uses LLM to extract features from generated captions, second stage uses LLM to semantically match extracted features with ground-truth features. This allows capturing synonyms and variations beyond pre-defined vocabulary.
- Core assumption: LLM's language comprehension capabilities can accurately identify semantic similarities between generated and ground-truth features across different expressions.
- Evidence anchors: [abstract]: "Our propose VALOR-E VAL metric generalizes CHAIR by incorporating an LLM in a two-stage design, enhancing the capability to evaluate open vocabulary hallucination"; [section 4.1]: "our approach eschews pre-processing and instead utilizes LLMs' language comprehension capabilities to semantically match extracted features with their ground-truth counterpart"
- Break condition: If LLM fails to capture nuanced semantic relationships or hallucinates matches itself, evaluation accuracy degrades.

### Mechanism 2
- Claim: Incorporating both faithfulness and coverage metrics addresses the trade-off between precision and informativeness in LVLM outputs.
- Mechanism: Faithfulness measures how accurately generated features match image content, while coverage measures comprehensiveness of capturing all ground-truth features. Together they provide balanced evaluation.
- Core assumption: A model can be evaluated on both accuracy (faithfulness) and completeness (coverage) dimensions simultaneously to identify optimal balance.
- Evidence anchors: [abstract]: "incorporates both faithfulness and coverage into the evaluation"; [section 4.2]: "We introduce two metrics to evaluate the hallucinations in two dimensions: faithfulness and coverage"; [section 5.1]: "This observation underscores the need for the community to focus on achieving an balance between faithfulness and coverage"
- Break condition: If either metric is missing, evaluation becomes biased toward either precision or comprehensiveness.

### Mechanism 3
- Claim: Selecting challenging images based on associative biases exposes LVLM hallucinations more effectively than random selection.
- Mechanism: Analyzes co-occurrence statistics to identify images where models are likely to hallucinate due to training biases, creating more difficult test cases.
- Core assumption: LVLMs develop associative biases from training data that cause them to incorrectly generate features that commonly co-occur with present features.
- Evidence anchors: [section 3.1]: "We hypothesize that when models are repeatedly exposed to specific combinations of features... they develop a pronounced associative bias"; [section 5.3]: "Our experimental findings validate the effectiveness of this methodology in exposing the susceptibility of current LVLMs to such biases"
- Break condition: If models learn to overcome these specific biases during training, this evaluation method becomes less effective.

## Foundational Learning

- Concept: Co-occurrence statistics and conditional probability
  - Why needed here: Used to identify associative biases and select challenging images for evaluation
  - Quick check question: How do you calculate P(feature|object) and why is this useful for finding bias-prone images?

- Concept: Semantic similarity and synonym detection
  - Why needed here: Core to the open vocabulary matching approach that replaces fixed synonym lists
  - Quick check question: What makes two phrases semantically equivalent for evaluation purposes?

- Concept: Faithfulness vs coverage metrics
  - Why needed here: The dual evaluation approach requires understanding both accuracy and completeness measures
  - Quick check question: How would you design a metric that captures both precision and comprehensiveness?

## Architecture Onboarding

- Component map: Image input → LVLM generation → LLM feature extraction → LLM semantic matching → Faithfulness/coverage calculation → Results
- Critical path: Image → LVLM → LLM feature extraction → LLM matching → Metrics
- Design tradeoffs: Fixed vocabulary (precise but limited) vs open vocabulary (comprehensive but potentially noisy)
- Failure signatures: Low faithfulness but high coverage suggests overgeneration, high faithfulness but low coverage suggests undergeneration
- First 3 experiments:
  1. Test baseline CHAIR vs LLM-based matching on simple controlled examples
  2. Validate associative bias image selection by comparing model performance on biased vs random images
  3. Evaluate correlation between automatic metrics and human judgment on sample outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the balance between faithfulness and coverage in LVLMs affect their practical utility across different real-world applications?
- Basis in paper: [explicit] The paper highlights that even GPT-4V achieves high coverage but relatively low faithfulness, and notes that some models prioritize precision over coverage, suggesting a critical trade-off.
- Why unresolved: While the paper demonstrates this trade-off exists, it doesn't explore how different balances might perform better in specific use cases or whether there's an optimal balance for different applications.
- What evidence would resolve it: Empirical studies testing LVLMs with varying faithfulness-coverage balances across different application domains (medical imaging, autonomous driving, content creation) to determine optimal configurations.

### Open Question 2
- Question: What are the most effective techniques for improving both faithfulness and coverage simultaneously in LVLMs without compromising one for the other?
- Basis in paper: [inferred] The paper identifies the faithfulness-coverage trade-off as a critical issue but doesn't provide solutions for addressing both metrics simultaneously.
- Why unresolved: Current LVLMs appear to face an inherent tension between accuracy and comprehensiveness, but the paper doesn't explore architectural or training modifications that could overcome this limitation.
- What evidence would resolve it: Development and testing of novel LVLM architectures or training methodologies that demonstrate significant improvements in both metrics compared to existing models.

### Open Question 3
- Question: How do different types of hallucinations (object, attribute, relation) impact user trust and model adoption differently across various applications?
- Basis in paper: [explicit] The paper systematically evaluates all three types of hallucinations but doesn't investigate their relative impact on practical deployment.
- Why unresolved: While the paper categorizes and measures different hallucination types, it doesn't assess how these errors affect real-world usability or user perception of reliability.
- What evidence would resolve it: User studies measuring trust and adoption rates for LVLMs with controlled hallucination profiles across different application contexts, potentially revealing which types of errors are most detrimental.

## Limitations
- The evaluation framework relies heavily on LLM-based semantic matching, which can introduce noise if LLMs hallucinate or misinterpret semantic relationships
- The associative bias image selection assumes training biases translate directly to evaluation-time hallucinations, which may not hold for models with advanced bias mitigation
- The VALOR-BENCH dataset, while comprehensive, may not fully represent the diversity of real-world images and scenarios LVLMs encounter

## Confidence

- **High confidence**: The core claim that incorporating both faithfulness and coverage provides more comprehensive evaluation than single-dimension metrics is well-supported by experimental results showing distinct trade-offs between these metrics.
- **Medium confidence**: The effectiveness of the LLM-based semantic matching approach for open vocabulary evaluation is supported by better correlation with human judgment, though the methodology lacks detailed implementation specifics.
- **Medium confidence**: The associative bias image selection methodology shows effectiveness in exposing model vulnerabilities, but the assumption that training biases translate directly to evaluation-time hallucinations requires further validation across different model architectures.

## Next Checks
1. Conduct ablation studies removing the semantic matching component to quantify its contribution versus potential noise introduced by LLM-based evaluation.
2. Test the evaluation framework on a broader range of image types and domains not represented in the VALOR-BENCH dataset to assess generalizability.
3. Compare results with human evaluators on a subset of challenging images to validate that the LLM-based semantic matching aligns with human perception of semantic equivalence.