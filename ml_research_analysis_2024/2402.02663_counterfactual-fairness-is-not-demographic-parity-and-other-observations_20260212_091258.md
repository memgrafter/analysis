---
ver: rpa2
title: Counterfactual Fairness Is Not Demographic Parity, and Other Observations
arxiv_id: '2402.02663'
source_url: https://arxiv.org/abs/2402.02663
tags:
- fairness
- counterfactual
- causal
- parity
- demographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper challenges a claim that counterfactual fairness is equivalent
  to demographic parity, showing this is not true even in simple cases with strong
  causal assumptions. It demonstrates that demographic parity does not imply counterfactual
  fairness through a Gaussian example where predictors satisfying demographic parity
  are not counterfactually fair.
---

# Counterfactual Fairness Is Not Demographic Parity, and Other Observations

## Quick Facts
- arXiv ID: 2402.02663
- Source URL: https://arxiv.org/abs/2402.02663
- Authors: Ricardo Silva
- Reference count: 36
- Key outcome: Challenges claim that counterfactual fairness equals demographic parity, showing this is not true even in simple cases with strong causal assumptions.

## Executive Summary
This paper challenges a claim that counterfactual fairness is equivalent to demographic parity, demonstrating through theoretical arguments and a Gaussian counterexample that this equivalence does not hold. The paper clarifies several misconceptions about counterfactual fairness, emphasizing it is fundamentally an individual-level notion that defines constraints on predictors rather than loss functions. It also argues against methods that attempt to align predictions with historical decisions or other "unfair" predictors, advocating instead for a variable selection approach that filters out unfair information.

## Method Summary
The paper uses theoretical analysis and illustrative examples to demonstrate its claims. It constructs a Gaussian counterexample where demographic parity holds but counterfactual fairness is violated, even under ignorability assumptions. The paper also analyzes rank preservation methods and their relationship to counterfactual fairness, using both synthetic examples and real-world data (law school dataset). The core approach is to clarify conceptual distinctions between probabilistic and causal fairness notions rather than proposing new algorithms.

## Key Results
- Demographic parity does not imply counterfactual fairness, even under strong causal assumptions
- Counterfactual fairness is fundamentally a variable selection/information bottleneck procedure, not a loss function
- Ancestral closure of protected attributes is not a fundamental requirement for counterfactual fairness
- Methods attempting to align predictions with historical decisions are misguided as they can perpetuate unfair information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual fairness is fundamentally a variable selection/information bottleneck procedure, not a loss function
- Mechanism: Counterfactual fairness defines constraints on predictors by filtering out information that carries unfair causal implications from protected attributes, allowing only information that is independent of the protected attribute's causal effects
- Core assumption: The causal model correctly captures which variables are descendants of protected attributes and which paths should be blocked
- Evidence anchors:
  - [section]: "Counterfactual fairness defines constraints on a predictor, not the loss function, constraints which can be interpreted as filters or a variation on the idea of information bottleneck [26]."
  - [section]: "One of the main motivations for counterfactual fairness is as follows. In a prediction problem, we do not want fairness through unawareness... The operational meaning of 'removing' is also not straightforward."
  - [corpus]: Weak - no direct corpus support for this specific mechanism
- Break condition: The causal model is misspecified or the protected attribute selection is incorrect

### Mechanism 2
- Claim: Demographic parity does not imply counterfactual fairness, even with ignorability assumptions
- Mechanism: Even when a predictor satisfies demographic parity (Y ⊥ A) and the protected attribute is ignorable (A ⊥ {Xa}), the predictor can still violate counterfactual fairness because ignorability doesn't constrain the correlation between potential outcomes
- Core assumption: The potential outcomes have non-zero correlation
- Evidence anchors:
  - [section]: "Demographic parity does not imply counterfactual fairness... Let's provide a simple counterexample where X given A is Gaussian and A is 0/1 binary."
  - [section]: "This means that the above is in general different from P(Ŷ0 ≤ y | X = x, A = 0)..."
  - [corpus]: Weak - no direct corpus support for this specific counterexample
- Break condition: Potential outcomes are perfectly correlated (ρ = ±1)

### Mechanism 3
- Claim: Ancestral closure of protected attributes is not a fundamental requirement for counterfactual fairness
- Mechanism: Counterfactual fairness allows using ancestors of protected attributes in the predictor definition, contrary to claims that it requires ancestral closure
- Core assumption: The causal model can be built independently of which attributes are protected
- Evidence anchors:
  - [section]: "Ancestral closure of protected attributes was never a fundamental property of counterfactual fairness... Put simply, I thought that defining counterfactual fairness as Ŷa(u) = Ŷa′(u) was a bad way of introducing a novel approach for looking into predictive algorithmic fairness."
  - [section]: "Suppose that a parent of a member of A is not in A. Counterfactual fairness allows for the use of it in the definition of Ŷ."
  - [corpus]: Weak - no direct corpus support for this specific claim
- Break condition: The causal model cannot be constructed independently of protected attribute selection

## Foundational Learning

- Concept: Causal inference ladder (Rung 1, 2, 3)
  - Why needed here: The paper contrasts probabilistic concepts (Rung 1) with causal concepts (Rung 3), and shows that claims of equivalence between them are problematic
  - Quick check question: What is the key difference between observational (Rung 1) and counterfactual (Rung 3) reasoning?

- Concept: Potential outcomes and structural causal models
  - Why needed here: The paper relies heavily on potential outcomes notation (Xa, Ya) and SCMs to demonstrate why demographic parity doesn't imply counterfactual fairness
  - Quick check question: How do potential outcomes differ from observed outcomes in a causal framework?

- Concept: d-separation and conditional independence
  - Why needed here: The paper uses d-separation to determine when demographic parity holds in simple causal diagrams
  - Quick check question: What does d-separation tell us about conditional independence in a causal graph?

## Architecture Onboarding

- Component map: Causal model specification -> Counterfactual fairness constraint definition -> Predictor construction
- Critical path: 1) Define causal model with protected attributes and other variables 2) Specify counterfactual fairness constraints 3) Construct predictor using allowed information only
- Design tradeoffs: Tighter fairness constraints reduce information available for prediction, potentially hurting performance
- Failure signatures: Poor predictive performance, violation of fairness constraints, or mismatch between assumed and actual causal relationships
- First 3 experiments:
  1. Test the Gaussian counterexample from the paper to verify demographic parity doesn't imply counterfactual fairness
  2. Build a simple causal model and implement counterfactual fairness constraints to see the information bottleneck effect
  3. Compare predictors built with and without counterfactual fairness constraints to measure performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a systematic family of counterfactual assumptions be found that makes demographic parity a viable construction rule for counterfactually fair predictors, and if so, what are the practical implications?
- Basis in paper: [explicit] The paper states "It would be awesome and a relief to be able to stick to Rung 1, but I cannot even imagine a counterfactual world where that would be true. I do not doubt that there are other systematic families of counterfactual assumptions that could lead to e.g. demographic parity as an alternative construction rule for counterfactually fair predictors, although it is not clear in which practical sense this would make our life any easier than a method for extracting functions of error terms or potential outcomes."
- Why unresolved: The paper acknowledges the theoretical possibility but expresses skepticism about practical utility and doesn't explore specific systematic families of assumptions.
- What evidence would resolve it: Development and demonstration of a systematic family of counterfactual assumptions that makes demographic parity a viable construction rule, along with a practical comparison showing its advantages or disadvantages relative to existing methods.

### Open Question 2
- Question: What is the precise definition and utility of the "group ordering preservation" concept introduced in [21], and why does it conflict with counterfactual fairness in certain scenarios?
- Basis in paper: [explicit] The paper discusses "group ordering preservation" and mentions that "It is not clear to me what this means" and questions the goal of aligning predictors with preliminary "unfair" predictors.
- Why unresolved: The paper expresses confusion about the concept and its motivation, suggesting ambiguity in the original definition and lack of clear justification for why this alignment is desirable.
- What evidence would resolve it: A clear and precise definition of "group ordering preservation" along with a rigorous analysis of its relationship to counterfactual fairness, including specific examples where alignment is or isn't desirable.

### Open Question 3
- Question: How can we effectively model and address selection bias in algorithmic fairness, particularly when it interacts with causal modeling assumptions?
- Basis in paper: [explicit] The paper mentions that "[6] does not go far enough, not mentioning general problems of selection bias in behavioral modeling by machine learning: distribution shift, a major problem for predictive modeling regardless of any causal or fairness issues, can in part be attributed to frequent changes in selection bias from training to test time."
- Why unresolved: While the paper acknowledges the importance of selection bias, it doesn't provide a comprehensive framework for addressing it in the context of causal fairness modeling.
- What evidence would resolve it: Development of a robust framework for modeling and mitigating selection bias in causal fairness analysis, with empirical demonstrations on real-world datasets showing improved fairness outcomes.

### Open Question 4
- Question: How can we reconcile the tension between individual-level counterfactual fairness and societal-level considerations of fairness, such as the burden of failure falling disproportionately on underprivileged groups?
- Basis in paper: [explicit] The paper discusses this tension in the context of loan decisions and college admissions, noting that "The main fights are on the non-technical aspects of decision-making which are societal, not technological, considerations, but this does not mean getting a free pass to ignore the causal effects of someone's membership in an underprivileged group against the criteria by which we constantly judge such an individual."
- Why unresolved: The paper acknowledges the tension but doesn't provide a clear methodology for balancing individual counterfactual fairness with broader societal concerns about fairness and burden-sharing.
- What evidence would resolve it: A framework that integrates individual counterfactual fairness with societal-level fairness considerations, along with empirical studies demonstrating its effectiveness in real-world scenarios.

## Limitations

- The paper relies heavily on specific causal model assumptions that may not always hold in practice
- The Gaussian counterexample, while illustrative, may not fully capture the complexity of real-world scenarios
- The paper doesn't provide comprehensive empirical validation of its claims across diverse datasets

## Confidence

- **High confidence**: The claim that demographic parity does not imply counterfactual fairness is strongly supported by the Gaussian counterexample and theoretical reasoning.
- **Medium confidence**: The interpretation of counterfactual fairness as an information bottleneck procedure, while conceptually sound, could benefit from more empirical validation.
- **Medium confidence**: The critique of rank preservation methods is convincing but may not apply universally to all fairness contexts.

## Next Checks

1. **Causal model sensitivity analysis**: Test the Gaussian counterexample with varying correlation parameters (ρ) to map the boundary between demographic parity and counterfactual fairness violations.

2. **Empirical comparison on real data**: Apply both demographic parity and counterfactual fairness constraints to a real-world dataset and measure the actual impact on prediction performance and fairness metrics.

3. **Information bottleneck quantification**: Implement the counterfactual fairness constraints as information filters and empirically measure the information retained/removed from protected attributes in synthetic datasets.