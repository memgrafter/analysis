---
ver: rpa2
title: 'WPMixer: Efficient Multi-Resolution Mixing for Long-Term Time Series Forecasting'
arxiv_id: '2412.17176'
source_url: https://arxiv.org/abs/2412.17176
tags:
- series
- time
- forecasting
- wavelet
- mixer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WPMixer, a novel MLP-mixer-based model for
  long-term time series forecasting. WPMixer leverages multi-level wavelet decomposition
  to extract features from both time and frequency domains, while using patching and
  mixing techniques to capture local and global information efficiently.
---

# WPMixer: Efficient Multi-Resolution Mixing for Long-Term Time Series Forecasting

## Quick Facts
- arXiv ID: 2412.17176
- Source URL: https://arxiv.org/abs/2412.17176
- Authors: Md Mahmuddun Nabi Murad; Mehmet Aktukmak; Yasin Yilmaz
- Reference count: 40
- Primary result: WPMixer achieves state-of-the-art performance on benchmark datasets, outperforming existing transformer and MLP-based models in terms of both accuracy and computational efficiency.

## Executive Summary
This paper introduces WPMixer, a novel MLP-mixer-based model for long-term time series forecasting. WPMixer leverages multi-level wavelet decomposition to extract features from both time and frequency domains, while using patching and mixing techniques to capture local and global information efficiently. The model achieves state-of-the-art performance on benchmark datasets, demonstrating superior accuracy and computational efficiency compared to existing transformer and MLP-based models.

## Method Summary
WPMixer processes time series by first normalizing inputs using Reversible Instance Normalization (RevIN), then applying multi-level discrete wavelet transform (DWT) to obtain approximation and detail coefficients at multiple scales. Each coefficient series is processed independently through patching, embedding, patch mixing, and embedding mixing modules, with heads predicting the next coefficients. The predicted coefficients are then reconstructed using inverse DWT and denormalized to produce the final forecast. The model is trained using SmoothL1 loss with Optuna for hyperparameter optimization.

## Key Results
- WPMixer achieves state-of-the-art performance on ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity, and Traffic datasets
- Outperforms existing transformer and MLP-based models in terms of both accuracy and computational efficiency
- Demonstrates effectiveness for both univariate and multivariate time series forecasting across various prediction horizons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level wavelet decomposition allows the model to extract features from both time and frequency domains, enabling it to capture complex temporal patterns and abrupt spikes/dips that moving average-based methods miss.
- Mechanism: By iteratively applying high-pass and low-pass filters, the time series is split into approximation (low-frequency) and detail (high-frequency) coefficients at multiple levels. Each level represents a different frequency resolution, so the model can learn both long-term trends and short-term fluctuations.
- Core assumption: Wavelet coefficients preserve relevant predictive information and their multi-resolution representation is more informative than a single-domain decomposition.
- Evidence anchors:
  - [abstract] "Multi-resolution wavelet decomposition efficiently extracts information in both the frequency and time domains."
  - [section] "Multi-level wavelet decomposition efficiently extracts information in both the frequency and time domains."
  - [corpus] Weak - no direct corpus evidence on multi-resolution vs moving average; claim is based on internal ablation.
- Break condition: If wavelet coefficients are noisy or if the decomposition discards important high-frequency components for the specific forecasting horizon, predictive performance may degrade.

### Mechanism 2
- Claim: Patching local segments and then mixing them globally preserves fine-grained local patterns while enabling long-range dependency learning.
- Mechanism: The normalized coefficient series is divided into overlapping patches of length P with stride S. A patch mixer aggregates local patch information into a global context, and an embedding mixer further mixes these in a higher-dimensional space. This two-stage mixing avoids losing local detail while still capturing global structure.
- Core assumption: Local patch-level mixing is computationally cheaper than global mixing on the full sequence and still captures necessary dependencies.
- Evidence anchors:
  - [section] "Patching allows the model to capture an extended history with a look-back window and enhances capturing local information while MLP mixing incorporates global information."
  - [corpus] Weak - no explicit ablation showing patching vs no patching; claim is inferred from model description.
- Break condition: If the patch size P or stride S are poorly chosen, local information may be fragmented or overlapping may cause redundancy, harming performance.

### Mechanism 3
- Claim: Instance normalization before decomposition and denormalization after reconstruction stabilize training and improve robustness to varying data distributions.
- Mechanism: Reversible instance normalization (RevIN) normalizes each instance independently, allowing the model to learn affine transforms that adapt to data shifts. Normalization is applied before wavelet decomposition and denormalization after reconstruction, ensuring stationary inputs for the neural network layers.
- Core assumption: Time series with varying mean/variance benefit from instance normalization more than batch or layer norm in this context.
- Evidence anchors:
  - [section] "One of the main challenges for time series forecasting is to deal with the time-varying mean and variation. To overcome this challenge, Reversible Instance Normalization (RevIN) with learnable affine transform has been proposed."
  - [corpus] Missing - no direct corpus citation for RevIN's effectiveness in this exact setting; assumption based on cited Kim et al. (2021).
- Break condition: If normalization overfits to training data shifts, or if the wavelet decomposition introduces non-stationary artifacts, denormalization may not fully recover original dynamics.

## Foundational Learning

- Concept: Discrete Wavelet Transform (DWT)
  - Why needed here: DWT decomposes a signal into approximation and detail coefficients at multiple scales, providing multi-resolution time-frequency representation crucial for capturing both trends and abrupt changes.
  - Quick check question: What is the difference between approximation and detail coefficients in DWT, and how does the number of decomposition levels affect the frequency range of each?
- Concept: Patch-based sequence processing
  - Why needed here: Patching breaks long sequences into manageable overlapping segments, enabling local feature extraction and reducing computational complexity compared to full-sequence mixing.
  - Quick check question: How does changing the patch size P and stride S affect the number of patches and the overlap between them?
- Concept: Multi-layer perceptron mixing layers
  - Why needed here: MLP mixers perform token-mixing (across patches) and channel-mixing (across features), enabling the model to learn interactions across time and feature dimensions without recurrence or attention.
  - Quick check question: In a standard MLP-mixer, what is the difference between token-mixing and channel-mixing layers?

## Architecture Onboarding

- Component map: Input normalization -> Multi-level DWT -> Resolution branches (one per coefficient series) -> RevIN -> Patch & Embed -> Patch Mixer -> Embedding Mixer -> Head -> RevIN denorm -> Reconstruction (Inverse DWT) -> Final output denormalization
- Critical path: Normalization -> Decomposition -> Branch processing -> Reconstruction -> Denormalization
- Design tradeoffs:
  - Wavelet type and decomposition level m: More levels give finer frequency resolution but may increase noise; wavelet choice balances time/frequency localization.
  - Patch size P vs stride S: Larger P captures more context per patch but increases computation; smaller stride increases overlap and memory use.
  - Embedding dimension d: Higher d increases model capacity but also GFLOPs and risk of overfitting.
- Failure signatures:
  - Degraded performance with increasing m suggests wavelet coefficients beyond a certain level add noise.
  - High variance across random seeds indicates sensitivity to initialization or data normalization issues.
  - GFLOPs much higher than baseline indicates inefficient patch/embedding design.
- First 3 experiments:
  1. Compare MSE/MAE with m=1 vs m=3 on ETTh1 to verify multi-level decomposition benefit.
  2. Vary patch size P (8, 16, 32) with fixed stride to assess local context impact.
  3. Switch from SmoothL1Loss to MSELoss to confirm loss function effect on stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of wavelet type affect the performance of WPMixer across different time series datasets?
- Basis in paper: [explicit] The paper mentions that the wavelet type is optimized during hyperparameter tuning, considering Daubechies, Symlets, Coiflets, and Biorthogonal wavelet families.
- Why unresolved: While the paper optimizes the wavelet type, it does not provide a detailed analysis of how different wavelet types impact performance across various datasets or time series characteristics.
- What evidence would resolve it: A comprehensive ablation study comparing the performance of WPMixer using different wavelet types on a diverse set of time series datasets, analyzing the relationship between wavelet characteristics and time series properties.

### Open Question 2
- Question: What is the impact of varying the decomposition level (m) on the model's ability to capture long-term dependencies in different time series?
- Basis in paper: [explicit] The paper discusses the effect of multi-level decomposition and treats the decomposition level as a hyperparameter, but does not provide a detailed analysis of its impact on capturing long-term dependencies.
- Why unresolved: The paper mentions that the optimal level depends on the prediction length and dataset but does not explore how different decomposition levels affect the model's ability to capture long-term dependencies across various time series characteristics.
- What evidence would resolve it: An in-depth analysis of WPMixer's performance with varying decomposition levels on time series with different dependency structures, examining how well the model captures long-term patterns at different scales.

### Open Question 3
- Question: How does WPMixer perform on time series with non-stationary characteristics or abrupt changes compared to other state-of-the-art models?
- Basis in paper: [inferred] The paper mentions that WPMixer can handle complex characteristics and abrupt spikes and dips in real-world data, but does not provide a specific comparison with other models on non-stationary time series.
- Why unresolved: While the paper claims WPMixer's ability to handle complex time series characteristics, it does not provide empirical evidence comparing its performance on non-stationary time series or those with abrupt changes against other state-of-the-art models.
- What evidence would resolve it: A comparative study of WPMixer's performance on non-stationary time series datasets or those with known abrupt changes, benchmarking against other state-of-the-art models specifically designed for such data characteristics.

## Limitations
- Claims about multi-level wavelet decomposition benefits rely on internal ablation rather than direct comparison to moving-average or single-level baselines
- Patching mechanism benefits are inferred from model description rather than explicit ablation experiments
- RevIN normalization advantages are supported by citation to Kim et al. (2021) but not directly validated in this forecasting context

## Confidence
- High confidence: The model architecture and implementation details are clearly specified
- Medium confidence: The core mechanisms (wavelet decomposition, patching, RevIN) are theoretically sound and show internal ablation support
- Low confidence: Claims about computational efficiency and superiority over all transformer/MLP models are based on benchmark comparisons without extensive ablation or ablation of key design choices

## Next Checks
1. Conduct explicit ablation comparing multi-level wavelet decomposition (m=3) against single-level (m=1) and moving-average-based baselines on the same datasets
2. Test the patching mechanism by comparing WPMixer with and without patching on fixed-length sequences to quantify the local-global information capture benefit
3. Replace RevIN with standard instance normalization and batch normalization to validate the claimed robustness benefits to time-varying data distributions