---
ver: rpa2
title: Revisiting Attention for Multivariate Time Series Forecasting
arxiv_id: '2407.13806'
source_url: https://arxiv.org/abs/2407.13806
tags:
- attention
- soatten
- fsatten
- transformer
- conventional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study revisits the attention mechanism for multivariate time
  series forecasting by questioning whether the conventional latent space mapping
  is optimal. It proposes two new attention methods: Frequency Spectrum Attention
  (FSatten), which uses Fourier transforms and Multi-head Spectrum Scaling (MSS) for
  periodic dependencies, and Scaled Orthogonal Attention (SOatten), which employs
  a learnable orthogonal space with Head-Coupling Convolution (HCC) for general dependency
  patterns.'
---

# Revisiting Attention for Multivariate Time Series Forecasting

## Quick Facts
- arXiv ID: 2407.13806
- Source URL: https://arxiv.org/abs/2407.13806
- Authors: Haixiang Wu
- Reference count: 40
- Key outcome: Two new attention methods (FSatten and SOatten) achieve 8.1% and 21.8% MSE improvement over state-of-the-art methods for multivariate time series forecasting

## Executive Summary
This study challenges the conventional wisdom of using learnable latent spaces in attention mechanisms for multivariate time series forecasting. The paper proposes two novel attention methods: Frequency Spectrum Attention (FSatten) which uses Fourier transforms to capture periodic dependencies, and Scaled Orthogonal Attention (SOatten) which employs learnable orthogonal transformations for general dependency patterns. Experimental results on six real-world datasets demonstrate significant improvements over existing state-of-the-art methods, with FSatten improving MSE by 8.1% and SOatten by 21.8% compared to conventional attention mechanisms.

## Method Summary
The paper proposes two attention mechanisms for multivariate time series forecasting. FSatten replaces the conventional learnable latent space with a fixed frequency domain using Fourier transforms, projecting Query and Key using Multi-head Spectrum Scaling (MSS). SOatten extends this approach by using a learnable orthogonal transformation space guided by Head-Coupling Convolution (HCC), which leverages neighboring sequence similarity to update the orthogonal space. Both methods are evaluated on Variate Transformer (iTransformer) and Temporal Transformer (PatchTST) architectures using six real-world datasets.

## Key Results
- FSatten achieves 8.1% MSE improvement over state-of-the-art methods
- SOatten achieves 21.8% MSE improvement over state-of-the-art methods
- Both methods demonstrate superior performance on datasets with periodic and general dependency patterns
- The proposed attention mechanisms show broader applicability compared to conventional approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conventional latent space mapping is not optimal for capturing periodic dependencies in multivariate time series
- Mechanism: FSatten uses Fourier transforms to convert time series into frequency domain, replacing learnable latent space with fixed frequency space
- Core assumption: Frequency domain representations more directly capture periodic dependencies than learned latent spaces
- Evidence anchors: Abstract mentions FSatten employs Fourier transform for embedding and MSS to replace conventional linear mapping
- Break condition: Performance diminishes on datasets without significant periodic patterns

### Mechanism 2
- Claim: Learnable orthogonal transformation space provides more general and adaptable attention
- Mechanism: SOatten uses learnable orthogonal space with HCC to guide comprehensive dependency pattern learning
- Core assumption: Orthogonal transformation space guided by neighboring similarity captures broader dependency patterns than fixed frequency domain
- Evidence anchors: Abstract describes SOatten as more general attention with learnable orthogonal latent space facilitated by HCC
- Break condition: Effectiveness reduced on small datasets or those lacking neighboring similarity

### Mechanism 3
- Claim: MSS effectively scales amplitude for different frequency components
- Mechanism: MSS scales amplitude values using Hadamard Product with learnable matrices to identify frequency spectral relationships
- Core assumption: Amplitude scaling helps identify and focus on most relevant frequency relationships between sequences
- Evidence anchors: Abstract states MSS replaces conventional linear projection in FSatten
- Break condition: Aggressive amplitude scaling or overfitting to noise compromises effectiveness

## Foundational Learning

- Concept: Fourier Transform and Frequency Domain Analysis
  - Why needed here: Essential for understanding how FSatten captures periodic dependencies using frequency domain
  - Quick check question: What does the Fourier transform reveal about a time series that is not immediately apparent in the time domain?

- Concept: Orthogonal Transformations and Linear Algebra
  - Why needed here: Critical for understanding how SOatten creates learnable orthogonal transformation space
  - Quick check question: What is the key property of an orthogonal matrix, and how does it relate to the transformation used in SOatten?

- Concept: Attention Mechanisms and Self-Attention
  - Why needed here: Fundamental for comprehending modifications proposed in FSatten and SOatten
  - Quick check question: In a self-attention mechanism, what do the Query, Key, and Value matrices represent, and how are they used to compute attention weights?

## Architecture Onboarding

- Component map: Input -> Embedding (Fourier transform or orthogonal transformation) -> Query/Key projection (MSS) -> Attention computation (with optional HCC) -> Output
- Critical path: Input -> Embedding -> Query/Key projection (MSS) -> Attention computation -> Output
- Design tradeoffs:
  - FSatten: Fixed frequency domain vs. learnable latent space; more interpretable for periodic dependencies but less adaptable to non-periodic patterns
  - SOatten: Learnable orthogonal space vs. fixed frequency domain; more adaptable but potentially less interpretable and more computationally intensive
  - HCC: Added complexity for guiding orthogonal space learning; improved performance on datasets with neighboring similarity but potential overfitting on small datasets
- Failure signatures:
  - FSatten: Poor performance on datasets with weak periodic patterns
  - SOatten: Overfitting on small datasets, failure to learn meaningful orthogonal transformations without sufficient neighboring similarity
  - MSS: Overemphasis on certain frequency components leading to neglect of other important dependencies
- First 3 experiments:
  1. Compare FSatten and SOatten on synthetic dataset with strong periodic patterns to validate frequency-based attention effectiveness
  2. Test SOatten with and without HCC on dataset with clear neighboring similarity to assess HCC impact
  3. Evaluate FSatten and SOatten on dataset with mixed periodic and non-periodic patterns to understand limitations and strengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific physical properties beyond periodicity (e.g., trend, seasonality) would provide optimal mapping spaces for different types of time series data?
- Basis in paper: [explicit] Paper mentions determining optimal configuration is challenging and discusses mapping spaces based on other physical properties like trend and seasonality
- Why unresolved: Paper demonstrates FSatten and SOatten outperform conventional attention but doesn't systematically explore which physical properties yield optimal mapping spaces
- What evidence would resolve it: Empirical studies comparing different attention mechanisms optimized for specific physical properties across diverse time series datasets

### Open Question 2
- Question: How would combining SOatten with state-space models like Mamba or TTT improve performance for datasets with many variates?
- Basis in paper: [explicit] Paper discusses combining with state-space models especially for datasets with large number of variates
- Why unresolved: Paper acknowledges potential but provides no experimental results on combining these approaches
- What evidence would resolve it: Direct comparison experiments showing combined model performance versus SOatten and state-space models alone

### Open Question 3
- Question: How would FSatten and SOatten perform on non-time series domains like computer vision, natural language processing, or motion planning?
- Basis in paper: [explicit] Paper states applying to other fields like CV, NLP, and motion planning are directions that can explore proposed methods
- Why unresolved: Paper demonstrates effectiveness for MTSF but doesn't validate whether proposed attention mechanisms generalize to other domains
- What evidence would resolve it: Comparative experiments showing performance on benchmark datasets from CV, NLP, and motion planning tasks

## Limitations

- Lack of corpus evidence directly supporting core mechanisms of FSatten and SOatten
- No systematic exploration of which physical properties beyond periodicity yield optimal mapping spaces
- Limited validation of proposed methods on non-time series domains like CV and NLP

## Confidence

- Low Confidence: FSatten is more effective than conventional attention for capturing periodic dependencies
- Medium Confidence: SOatten provides more general and adaptable attention mechanism
- Medium Confidence: MSS effectively scales amplitude for different frequency components

## Next Checks

1. Cross-dataset validation: Test FSatten and SOatten on additional datasets with varying degrees of periodicity and neighboring similarity to assess generalizability and identify failure modes

2. Ablation study on HCC: Conduct ablation study on HCC module in SOatten to quantify impact on performance and determine if it introduces unnecessary complexity or overfitting risks

3. Comparison with frequency-aware baselines: Compare FSatten against other frequency-aware attention mechanisms or traditional spectral analysis techniques to validate superiority in capturing periodic dependencies