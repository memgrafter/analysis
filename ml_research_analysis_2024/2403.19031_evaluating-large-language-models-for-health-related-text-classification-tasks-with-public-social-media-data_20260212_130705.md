---
ver: rpa2
title: Evaluating Large Language Models for Health-Related Text Classification Tasks
  with Public Social Media Data
arxiv_id: '2403.19031'
source_url: https://arxiv.org/abs/2403.19031
tags:
- data
- classification
- gpt3
- zero-shot
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated the use of large language models (LLMs) for
  health-related text classification tasks using public social media data. The researchers
  compared three approaches for leveraging LLMs: employing them as zero-shot classifiers,
  using them as annotators to label training data, and utilizing them for data augmentation.'
---

# Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data

## Quick Facts
- arXiv ID: 2403.19031
- Source URL: https://arxiv.org/abs/2403.19031
- Reference count: 0
- Using LLM data augmentation with small human-annotated datasets improves performance of lightweight supervised models

## Executive Summary
This study evaluates large language models (LLMs) for health-related text classification tasks using public social media data. The researchers benchmark one supervised classic machine learning model (SVM), three supervised pretrained language models (RoBERTa, BERTweet, SocBERT), and two LLM-based classifiers (GPT3.5, GPT4) across six text classification tasks. They explore three approaches: zero-shot classification, using LLMs as annotators, and data augmentation with LLMs. Results show that data augmentation using GPT-4 with relatively small human-annotated data achieves superior results compared to training with human-annotated data alone. Supervised learners also outperform GPT-4 and GPT-3.5 in zero-shot settings.

## Method Summary
The study benchmarks multiple approaches for health-related text classification using public social media data. Six classification tasks were evaluated including self-report depression, COPD, breast cancer, medication changes, adverse pregnancy outcomes, and COVID-19. Three approaches were tested: zero-shot classification with GPT-3.5 and GPT-4, using LLMs as annotators to label training data, and data augmentation with LLMs. Models were evaluated on precision, recall, and F1 score for the positive class. The data augmentation process involved instructing GPT-4 to generate posts closely resembling each entry in the original training set, with the ratio of human-annotated to augmented data varied across experiments.

## Key Results
- Data augmentation using GPT-4 with small human-annotated datasets significantly improves performance of lightweight supervised models
- GPT-4 annotations produce more reliable training data than GPT-3.5 annotations
- Zero-shot LLM classifiers can serve as preprocessing filters to reduce false negatives before supervised classification
- Using LLM-annotated data without human guidance for training supervised models was found to be ineffective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation using GPT-4 with small human-annotated datasets improves lightweight supervised model performance
- Mechanism: GPT-4 generates synthetic posts that closely resemble original training data, expanding the dataset while maintaining semantic coherence
- Core assumption: Generated synthetic posts preserve task-relevant linguistic patterns and domain-specific terminology
- Evidence anchors: abstract statement on superior results with data augmentation; description of generating posts resembling original training set
- Break condition: Generated posts diverge semantically from training domain, introducing noise that degrades classifier performance

### Mechanism 2
- Claim: Zero-shot LLM classifiers can serve as preprocessing filters to reduce false negatives before supervised classification
- Mechanism: GPT-4's zero-shot predictions achieve high recall, capturing more true positives than SVM alone
- Core assumption: GPT-4's generative pretraining captures sufficient domain knowledge to identify positive cases without fine-tuning
- Evidence anchors: abstract mention of excluding false negatives; finding that GPT4 achieved higher recall than human-annotated models in 5 of 6 tasks
- Break condition: Domain-specific terminology or slang in social media posts falls outside GPT-4's pretraining distribution

### Mechanism 3
- Claim: GPT-4 annotations are more reliable than GPT-3.5 for training supervised models
- Mechanism: GPT-4's superior language understanding produces higher-quality annotations, leading to better downstream classifier performance
- Core assumption: GPT-4's architectural improvements translate to better annotation accuracy on noisy social media text
- Evidence anchors: abstract statement that GPT4 predictions are more accurate than GPT3.5; finding that supervised models trained on GPT4 annotated data exhibited superior performance
- Break condition: Task complexity exceeds GPT-4's capabilities or training data distribution shifts significantly

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Understanding how LLMs classify without task-specific training data
  - Quick check question: What distinguishes zero-shot from few-shot learning in LLM contexts?

- Concept: Data augmentation strategies
  - Why needed here: Recognizing when and how synthetic data improves model performance
  - Quick check question: How does synthetic data quality affect downstream supervised model performance?

- Concept: Transformer-based PLMs vs. LLMs
  - Why needed here: Differentiating between fine-tuned PLMs and generative LLMs for classification
  - Quick check question: What architectural differences between BERT and GPT affect their classification approaches?

## Architecture Onboarding

- Component map: Data collection → Human annotation → LLM augmentation → Supervised training → Evaluation
- Critical path: 1. Collect and label small dataset, 2. Generate augmented data using GPT-4, 3. Train lightweight supervised model (RoBERTa/SVM), 4. Evaluate on held-out human-annotated test set
- Design tradeoffs: More augmentation vs. risk of synthetic noise; GPT-4 cost vs. performance gains over GPT-3.5; zero-shot filtering vs. supervised training requirements
- Failure signatures: Performance degrades with excessive augmentation; zero-shot classifier misses domain-specific patterns; human-annotated data quality insufficient for effective augmentation
- First 3 experiments: 1. Test optimal ratio of human-annotated to LLM-augmented data across tasks, 2. Compare generic vs. domain-specific LLMs for augmentation, 3. Evaluate robustness to adversarial examples and out-of-distribution data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of human-annotated to LLM-augmented data for training supervised classification models in health-related text classification tasks?
- Basis in paper: The paper mentions exploring the efficacy of data augmentation by varying the percentage of augmented training data and discusses the impact of the ratio of human-annotated data to LLM-augmented data on model performance
- Why unresolved: The study found that the optimal ratio is task-specific and requires further investigation to determine the best approach for different health-related text classification tasks
- What evidence would resolve it: Conducting comprehensive experiments across a wide range of health-related text classification tasks with varying data sizes and complexities to identify consistent patterns in the optimal ratio

### Open Question 2
- Question: How does the quality of LLM-generated data affect the performance of supervised classification models in health-related text classification tasks?
- Basis in paper: The paper discusses the ineffectiveness of using LLM-annotated data without human guidance for training lightweight supervised classification models and highlights the importance of the LLM's capability to generate high-quality posts for effective data augmentation
- Why unresolved: The study suggests that the quality of LLM-generated data is crucial for model performance, but it does not provide a detailed analysis of the factors influencing the quality of generated data or how to ensure high-quality outputs
- What evidence would resolve it: Investigating the relationship between the quality of LLM-generated data and model performance through controlled experiments, and developing guidelines or techniques to improve the quality of LLM-generated data for specific tasks

### Open Question 3
- Question: Can customizing LLMs for social media data and medical texts further improve the performance of data augmentation in health-related text classification tasks?
- Basis in paper: The paper notes that the LLMs used were neither fine-tuned for social media data nor medical texts and suggests that customizing LLMs before data augmentation may further improve performance
- Why unresolved: The study does not explore the potential benefits of customizing LLMs for specific domains, leaving open the question of whether such customization could lead to better data augmentation outcomes
- What evidence would resolve it: Conducting experiments to compare the performance of data augmentation using generic LLMs versus domain-specific LLMs (fine-tuned for social media data and medical texts) across various health-related text classification tasks

## Limitations

- Findings are based on six specific health-related classification tasks, limiting generalizability to other domains
- Study does not address computational costs and latency implications of using GPT-4 for data augmentation at scale
- Effectiveness of GPT-4 augmentation may not transfer to tasks with different linguistic characteristics or domain knowledge requirements

## Confidence

- **High Confidence**: The finding that supervised models outperform zero-shot LLMs for most tasks is well-supported by the experimental results across multiple model architectures and evaluation metrics
- **Medium Confidence**: The claim that GPT-4 augmentation improves performance with small human-annotated datasets is supported but requires further validation across different task types and dataset sizes
- **Low Confidence**: The assertion that LLM-annotated data without human guidance is ineffective lacks sufficient exploration of potential mitigation strategies or quality control mechanisms

## Next Checks

1. Test the data augmentation approach on additional health-related tasks outside the original six to assess generalizability across different medical domains and classification objectives

2. Conduct experiments varying the ratio of human-annotated to LLM-generated data to identify optimal augmentation strategies and determine when diminishing returns occur

3. Evaluate the robustness of augmented models to adversarial examples and out-of-distribution social media posts to assess whether synthetic data introduces vulnerabilities not present in human-annotated training sets