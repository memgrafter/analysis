---
ver: rpa2
title: Iterative Data Generation with Large Language Models for Aspect-based Sentiment
  Analysis
arxiv_id: '2407.00341'
source_url: https://arxiv.org/abs/2407.00341
tags:
- data
- iterd
- aspect
- sentiment
- absa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel iterative data augmentation framework,
  IterD, for aspect-based sentiment analysis (ABSA). IterD leverages the powerful
  ability of large language models (LLMs) to iteratively generate fluent and diverse
  synthetic labeled data starting from an unsupervised sentence corpus.
---

# Iterative Data Generation with Large Language Models for Aspect-based Sentiment Analysis

## Quick Facts
- **arXiv ID**: 2407.00341
- **Source URL**: https://arxiv.org/abs/2407.00341
- **Reference count**: 9
- **Primary result**: IterD achieves consistent performance gains across five ABSA models on four benchmarks, with synthetic data matching or exceeding human-annotated data quality

## Executive Summary
This paper introduces IterD, an iterative data augmentation framework for aspect-based sentiment analysis (ABSA) that leverages large language models (LLMs) to generate synthetic labeled data from unlabeled corpora. The framework addresses LLM hallucinations and repetitive generation through iterative prompting with feedback samples and a self-reflection filtering mechanism using LLM-as-a-Judge. Extensive experiments on four widely-used ABSA benchmarks demonstrate that IterD consistently improves performance across five baseline models, with the generated data achieving comparable or better results than manually annotated data.

## Method Summary
IterD operates in three stages: aspect extraction and extension from unlabeled data, iterative pseudo-data generation using GPT-3.5-turbo with feedback samples and multi-aspect generation, and quality filtering through a discriminator with LLM-based scoring. The framework generates both single-aspect and mix-aspect sentences, then trains ABSA models on combinations of original and synthetic data. The iterative mechanism uses in-context learning from previously generated samples, while the self-reflection filter evaluates domain relevance, sentiment relevance, and data quality through an auto-scoring mechanism.

## Key Results
- Consistent performance gains across five baseline ABSA models (ATAE-LSTM, ASGCN, BERT-SPC, R-GAT, KGAN)
- Synthetic data achieves comparable or better performance than manually annotated data on all four benchmarks
- Significant improvements in both accuracy and F1 metrics across Restaurant14, Restaurant15, Restaurant16, and Laptop14 datasets
- Mix-aspect generation proves particularly beneficial for training fine-grained aspect-specific information

## Why This Works (Mechanism)

### Mechanism 1: Iterative Data Generation with In-Context Learning
The framework uses an "Iteration Teaching Analysis Prompt" (ITAT) that feeds previously generated samples back into the LLM as examples, guiding subsequent generations to avoid repetition and improve diversity. LLMs learn from their own generated outputs when provided as in-context examples, improving generation quality over iterations. Break condition: If feedback samples are too similar or if LLM fails to generalize from examples, the iterative process may stagnate.

### Mechanism 2: Self-Reflection Filtering with LLM-as-a-Judge
A discriminator module uses LLM-as-a-Judge to assess domain relevance, sentiment relevance, and data quality through an auto-scoring mechanism evaluating syntactic structure, lexical richness, and real scenario conformity. Break condition: If the LLM judge has systematic biases or if scoring thresholds are poorly calibrated, low-quality data may pass through or high-quality data may be discarded.

### Mechanism 3: Multi-Aspect Data Generation
The framework generates "mix-aspect" data where sentences contain multiple aspects with their corresponding sentiments, better reflecting real-world scenarios. Training on multi-aspect data helps ABSA models learn to distinguish and relate multiple aspect-sentiment pairs within the same sentence. Break condition: If the LLM struggles to maintain coherent sentiment assignments across multiple aspects, generated data may contain contradictions.

## Foundational Learning

- **Concept: Aspect-Based Sentiment Analysis (ABSA)**
  - Why needed here: Understanding the ABSA task is fundamental to grasping why data augmentation is valuable and how generated data should be structured
  - Quick check question: What is the difference between ABSA and general sentiment analysis?

- **Concept: Large Language Model (LLM) Prompting Techniques**
  - Why needed here: The framework relies on sophisticated prompting strategies (few-shot, iterative feedback) to guide LLM behavior
  - Quick check question: How does in-context learning differ from fine-tuning in LLM applications?

- **Concept: Data Augmentation Principles**
  - Why needed here: Understanding why synthetic data can improve model performance and what characteristics make augmentation effective
  - Quick check question: What are the risks of using low-quality synthetic data for training?

## Architecture Onboarding

- **Component map**: Unlabeled corpus → Aspect Extraction → Iterative Generation → Filtering → Training Data → Model Training

- **Critical path**: Unlabeled corpus → Aspect Extraction → Iterative Generation → Filtering → Training Data → Model Training

- **Design tradeoffs**:
  - Quality vs. Quantity: Higher filtering thresholds produce better quality but less data
  - Diversity vs. Coherence: Iterative feedback improves diversity but may introduce incoherence if not controlled
  - Single vs. Multi-Aspect: Multi-aspect generation is more complex but potentially more effective

- **Failure signatures**:
  - Poor aspect extraction → Irrelevant or noisy training data
  - Repetitive generation → Low diversity in synthetic data
  - Overly strict filtering → Insufficient training data
  - Mixed sentiment in single sentence → Contradictory training examples

- **First 3 experiments**:
  1. Test aspect extraction accuracy on a small unlabeled corpus to verify the extraction module
  2. Generate a small batch of synthetic data and manually evaluate quality to calibrate the discriminator
  3. Train a baseline ABSA model with original data only, then with generated data only, to verify data quality improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the iterative nature of IterD impact the quality and diversity of generated data over multiple rounds of generation?
- Basis in paper: The paper discusses the iterative generation module in IterD, which uses feedback from previous rounds to guide subsequent generations, but doesn't provide detailed analysis on how quality and diversity evolve over iterations.
- Why unresolved: While the paper mentions the iterative generation and its benefits, it does not provide detailed analysis on how the quality and diversity of generated data evolve over multiple rounds.
- What evidence would resolve it: A detailed analysis of data quality and diversity metrics across multiple iterations of the generation process would be needed.

### Open Question 2
- Question: How does IterD perform on aspect extraction tasks in domains significantly different from those used in the evaluation?
- Basis in paper: The paper evaluates IterD on four ABSA benchmarks from restaurant and laptop domains and mentions potential application to more scenarios like end-to-end ABSA.
- Why unresolved: The evaluation is limited to specific domains (restaurant and laptop reviews). There's no evidence of how well IterD would perform in domains with very different linguistic patterns.
- What evidence would resolve it: Testing IterD on a diverse set of domains, including those with significantly different characteristics from the evaluated datasets, would provide insights into its generalization capabilities.

### Open Question 3
- Question: What is the impact of the filtering threshold T on the balance between data quality and quantity in IterD?
- Basis in paper: The paper discusses the filtering threshold T in the discriminator module and provides some analysis on its impact.
- Why unresolved: While the paper provides some insights into the impact of T, it doesn't explore the full range of its effects or provide a comprehensive analysis of how different values of T affect the trade-off.
- What evidence would resolve it: A more extensive parameter analysis of T, exploring a wider range of values and their effects on both data quality metrics and downstream ABSA performance, would be needed.

## Limitations

- **Implementation details missing**: The exact prompt templates for aspect extraction, extension, and iterative generation are not fully specified in the main text, making exact reproduction difficult.
- **Discriminator scoring criteria**: The auto-scoring mechanism's implementation details for syntactic structure, lexical richness, and real scenario conformity lack comprehensive specification.
- **Domain generalization**: Performance on domains significantly different from restaurant and laptop reviews remains untested, limiting claims about broad applicability.

## Confidence

- **High confidence**: Core claim that iterative LLM-based data generation improves ABSA performance, demonstrated by consistent gains across five baseline models and four benchmarks.
- **Medium confidence**: Claim that synthetic data can achieve "comparable or even better performance against manually annotated data" - results are strong but comparison depends heavily on original human annotation quality.
- **Low confidence**: Specific implementation details required for exact reproduction, particularly around prompt templates and the discriminator's scoring mechanism.

## Next Checks

1. **Prompt Template Validation**: Implement the ITAT prompt and test iterative generation on a small unlabeled corpus, measuring diversity metrics (distinct n-grams, sentence similarity) across iterations to verify the feedback mechanism works as intended.

2. **Discriminator Calibration**: Test the LLM-as-a-Judge on a small set of generated samples with known quality, measuring inter-annotator agreement between the LLM scores and human evaluations across the three scoring dimensions.

3. **Ablation Study**: Conduct controlled experiments removing either the iterative feedback mechanism or the self-reflection filtering to quantify the individual contribution of each component to overall performance improvements.