---
ver: rpa2
title: Fine-Grained Embedding Dimension Optimization During Training for Recommender
  Systems
arxiv_id: '2401.04408'
source_url: https://arxiv.org/abs/2401.04408
tags:
- embedding
- training
- pruning
- fiited
- dimension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FIITED, a method for optimizing embedding dimensions
  during training of deep learning recommendation models (DLRMs). It leverages the
  insight that embedding vectors are not equally important and adjusts their dimensions
  adaptively, assigning larger dimensions to more important embeddings.
---

# Fine-Grained Embedding Dimension Optimization During Training for Recommender Systems

## Quick Facts
- arXiv ID: 2401.04408
- Source URL: https://arxiv.org/abs/2401.04408
- Reference count: 21
- Can reduce embedding size by over 65% while maintaining model quality

## Executive Summary
FIITED introduces a method for optimizing embedding dimensions during training of deep learning recommendation models by leveraging the insight that embeddings vary in importance. The method adaptively adjusts each embedding's dimension, assigning larger dimensions to more important embeddings while maintaining model quality. A novel chunk-based VHPI storage system enables efficient pruning during training, achieving significant memory savings on both industry and public datasets.

## Method Summary
FIITED optimizes embedding dimensions during training by adaptively pruning less important embedding chunks while preserving critical ones. It uses a virtually-hashed physically-indexed (VHPI) storage system with small fixed-size chunks, each with its own utility tracking. The utility is computed from access frequency and gradient norms, allowing dynamic dimension adjustment. Zero-padding handles variable-length embeddings during dot products, while periodic pruning thresholds determine which chunks to evict and reallocate.

## Key Results
- Reduces embedding size by over 65% on industry models while maintaining model quality
- Achieves 2.1x to 800x reduction in embedding table sizes on public datasets with negligible accuracy drop
- Improves model throughput compared to state-of-the-art methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embeddings vary in importance, so allocating dimensions adaptively reduces memory without hurting model quality.
- Mechanism: FIITED assigns larger embedding dimensions to embeddings with higher access frequency and gradient norms, pruning dimensions of less important embeddings during training.
- Core assumption: The importance of embeddings (and thus optimal dimensions) changes over time with training data characteristics.
- Evidence anchors:
  - [abstract]: "By leveraging the key insight that embedding vectors are not equally important, FIITED adaptively adjusts the dimension of each individual embedding vector during model training, assigning larger dimensions to more important embeddings while adapting to dynamic changes in data."
  - [section]: "Different from the normal practice, assigning suitable, non-uniform embedding dimensions is desirable for two reasons... Second, to achieve good model quality, the optimal embedding dimension is inherently non-uniform due to differences among features..."
  - [corpus]: Weak evidence; neighbor papers focus on compression or hashing but do not directly address per-vector dimension adaptation.
- Break condition: If embedding importance is static or if pruning thresholds are poorly chosen, memory savings could hurt model quality.

### Mechanism 2
- Claim: The chunk-based VHPI storage system enables memory-efficient pruning during training.
- Mechanism: Embedding tables are divided into small chunks, each with its own utility and pruning ratio; unused chunks are evicted and their addresses reused, avoiding fragmentation.
- Core assumption: Small fixed-size chunks allow reuse of freed memory without fragmentation, even with frequent dynamic allocation/deallocation.
- Evidence anchors:
  - [section]: "To tackle this challenge, we propose a novel chunk-based embedding storage system... we propose a new Virtually Hashed Physically Indexed (VHPI) embedding table design adapted from AdaEmbed (Lai et al., 2023), as illustrated in Figure 5."
  - [section]: "Each hash table entry contains addresses of K embedding chunks, K utility values and a bit mask that indicates whether the chunks have been pruned."
  - [corpus]: No direct corpus support; VHPI design appears novel in neighbors.
- Break condition: If chunk size is too large, fragmentation returns; if too small, memory overhead and access latency grow.

### Mechanism 3
- Claim: Zero-padding instead of projection layers preserves model quality while allowing variable-length embeddings during dot product.
- Mechanism: Pruned chunks are treated as all zeros; dot products between differently sized embeddings are computed on padded vectors of the same length.
- Core assumption: The model can compensate for zeroed dimensions during training and still learn useful interactions.
- Evidence anchors:
  - [section]: "To further explain the idea, an example is illustrated in Figure 3... Since computing dot products on zero-padded embeddings can cause information loss due to multiplications by zero, it may seem that padding zeros will restrict the model quality. But it is not an issue..."
  - [section]: "Preliminary tests that compare FC layers with zero-padding during training of a Multi-Task-Multi-Label industry model... Results show that, compared to a baseline model with uniform embedding dimensions, zero padding only incurred a marginal NE loss (0.020%) in one of the three tasks while the other two tasks had tiny NE gains..."
  - [corpus]: No corpus evidence; neighbors use hashing or quantization, not zero-padding strategies.
- Break condition: If the zeroed dimensions remove critical interaction information, model accuracy will degrade.

## Foundational Learning

- Concept: Embedding tables and their role in DLRMs
  - Why needed here: FIITED directly manipulates embedding dimensions; understanding table layout, access patterns, and interaction computation is essential.
  - Quick check question: What happens during a dot product between two embeddings of different lengths in a DLRM?

- Concept: In-training pruning vs. pre-training pruning
  - Why needed here: FIITED prunes during training, unlike most EDS methods; understanding this distinction explains its memory-saving advantage.
  - Quick check question: Why can't pre-training EDS methods reduce training memory footprint?

- Concept: Hashing and collision handling in embedding storage
  - Why needed here: VHPI uses hash tables to map sparse IDs to embedding chunks; understanding hashing is key to grasping the design.
  - Quick check question: How does a hash collision affect embedding lookup, and how might VHPI mitigate it?

## Architecture Onboarding

- Component map: VHPI hash table (sparse ID → chunk addresses, utilities, masks) -> VHPI embedding table (actual embedding chunks, memory-managed by chunk address manager) -> Chunk address manager (free stack, allocation/deallocation) -> Utility tracker (frequency + gradient norm) -> Pruning scheduler (periodic threshold computation and chunk eviction/allocation)

- Critical path:
  1. Forward/backward pass → update chunk utilities
  2. Periodic pruning check → sample and sort utilities → compute thresholds
  3. Evict low-utility chunks → update hash masks
  4. Allocate new chunks as needed → update hash entries

- Design tradeoffs:
  - Chunk size K: Larger K → fewer hash lookups, more memory overhead; smaller K → more lookups, less overhead
  - Utility metric: Frequency-only → simpler, less expressive; frequency+gradient → more expressive, more compute
  - Manual vs. adaptive pruning ratios: Manual → predictable, less adaptive; adaptive → dynamic, more complexity

- Failure signatures:
  - Memory fragmentation despite VHPI design → chunk size too large or too many tiny free chunks
  - Degraded model accuracy → pruning thresholds too aggressive or utility metric poor
  - Slow training → utility update or threshold computation not parallelized

- First 3 experiments:
  1. Run FIITED with K=1 (AdaEmbed baseline) and compare training memory usage vs. accuracy.
  2. Increase K to 2, 4, 8 and measure the trade-off between memory savings, accuracy, and per-iteration latency.
  3. Switch between manual and adaptive pruning ratio selection and measure accuracy impact on a small public dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of utility function (e.g., including gradient information vs. just access frequency) impact the effectiveness of FIITED's dimension pruning?
- Basis in paper: [explicit] The paper mentions that incorporating gradient information can lead to better model accuracy compared to using only access frequency.
- Why unresolved: The paper does not provide a comprehensive comparison of different utility functions and their impact on pruning effectiveness.
- What evidence would resolve it: A detailed study comparing FIITED with different utility functions (e.g., access frequency only, gradient norm only, combinations) on various datasets and models.

### Open Question 2
- Question: How does the number of chunks (K) in the VHPI design affect the trade-off between memory overhead, latency overhead, and model quality?
- Basis in paper: [explicit] The paper mentions that the performance overhead grows linearly with the number of chunks, and setting appropriate K is a trade-off between NE, memory usage, and training time.
- Why unresolved: The paper does not provide a systematic analysis of how K affects these trade-offs.
- What evidence would resolve it: A study varying K across a wide range and measuring its impact on memory overhead, latency overhead, and model quality on different datasets and models.

### Open Question 3
- Question: Can FIITED be extended to other types of embeddings beyond sparse feature embeddings, such as sequential embeddings or attention-based embeddings?
- Basis in paper: [inferred] The paper focuses on sparse feature embeddings in DLRMs, but the underlying concept of fine-grained dimension optimization could potentially be applied to other embedding types.
- Why unresolved: The paper does not explore the applicability of FIITED to other embedding types.
- What evidence would resolve it: Experiments applying FIITED to models with sequential embeddings or attention-based embeddings and comparing the results to existing methods.

## Limitations
- The VHPI design lacks extensive corpus validation with alternative hashing schemes in the recommendation domain.
- Zero-padding mechanism's impact on model quality is validated only through preliminary tests on one industry model.
- No reported analysis on long-term stability of dynamic pruning during extended training over multiple epochs.

## Confidence

**High confidence** in the core mechanism that embedding importance varies and can be exploited for dimension optimization, supported by both theoretical arguments and empirical results showing substantial memory savings (65%+ reduction).

**Medium confidence** in the VHPI storage design's effectiveness, as the concept is well-explained but lacks extensive ablation studies comparing different chunk sizes and hash table configurations across multiple datasets.

**Low confidence** in the long-term stability of dynamic pruning during extended training, as the paper does not report on convergence behavior or sensitivity to hyperparameter choices over multiple epochs.

## Next Checks

1. **Ablation on chunk size K**: Systematically vary K (1, 2, 4, 8, 16) on the Criteo Terabyte dataset and measure the trade-off between memory usage, accuracy (NE), and per-iteration latency to identify the optimal configuration.

2. **Hyperparameter sensitivity analysis**: Test different combinations of decay parameter γ (0.9, 0.99, 0.999) and sampling size m (1%, 5%, 10%) to quantify their impact on model quality and memory savings across all public datasets.

3. **Extended training stability**: Run FIITED for 20+ epochs on the MovieLens-20M dataset and monitor embedding dimension evolution, utility distribution, and model accuracy to detect any degradation or instability patterns over time.