---
ver: rpa2
title: Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning
  in Deep Metric Learning
arxiv_id: '2402.02340'
source_url: https://arxiv.org/abs/2402.02340
tags:
- learning
- proxies
- semantic
- prompts
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates parameter-efficient fine-tuning for deep
  metric learning (DML) with pre-trained vision transformers (ViT). The authors propose
  a novel framework that learns visual prompts (VPT) to generate semantic proxies
  for each class, enhancing the representation capability and DML performance.
---

# Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning

## Quick Facts
- arXiv ID: 2402.02340
- Source URL: https://arxiv.org/abs/2402.02340
- Authors: Li Ren; Chen Chen; Liqiang Wang; Kien Hua
- Reference count: 32
- This paper proposes learning visual prompts to generate semantic proxies for parameter-efficient fine-tuning in deep metric learning, achieving competitive results with only 5.2% of parameters fine-tuned.

## Executive Summary
This paper addresses the challenge of parameter-efficient fine-tuning for deep metric learning with pre-trained vision transformers. The authors propose a novel framework that learns visual prompts to generate semantic proxies for each class, enabling fine-tuning with only a small fraction of total parameters while maintaining or improving performance. By incorporating semantic information into the proxy-based DML paradigm through class-based visual prompts and GRU-based proxy aggregation, the method achieves comparable or better results than recent state-of-the-art full fine-tuning works on popular DML benchmarks.

## Method Summary
The proposed framework uses Visual Prompt Tuning (VPT) to generate semantic proxies by fine-tuning class-based prompts and combining them with proxy bias. The method employs Proxy-Anchor loss and uses exponential moving average (EMA) or gated recurrent unit (GRU) to accumulate proxies. The framework only fine-tunes a small percentage of total parameters (e.g., 5.2% of 30M) while freezing the backbone, achieving competitive performance on DML benchmarks like CUB200, CARS196, SOP, and In-Shop.

## Key Results
- Achieves comparable or better results than recent state-of-the-art full fine-tuning works on popular DML benchmarks
- Outperforms other parameter-efficient methods like linear probing, adapters, and BitFit
- Fine-tunes only 5.2% of total parameters while maintaining strong performance
- GRU-based proxy aggregation improves performance on both R@1 and MAP@R metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantic proxies improve early training convergence by embedding semantic information from the start.
- **Mechanism:** Proxies are initialized with class-based visual prompts that carry semantic information extracted by the pre-trained ViT, reducing the distribution gap between proxy and sample embeddings.
- **Core assumption:** Pre-trained ViT contains sufficient semantic knowledge to initialize proxies meaningfully.
- **Evidence anchors:**
  - [abstract] "we propose to initial proxies with semantic information by training a set of extra visual prompts for each image class."
  - [section 4.2] "we attempt to initialize the proxies from the sample encoder with the same input data...we included some extra learnable prompts dependent on sample class labels."
  - [corpus] Weak evidence: related papers focus on parameter efficiency but not semantic proxy initialization.
- **Break condition:** If ViT pre-training lacks class-specific semantic richness, semantic initialization may not improve convergence.

### Mechanism 2
- **Claim:** Gated recurrent unit (GRU) aggregation preserves informative semantic proxies across batches.
- **Mechanism:** GRU updates semantic proxies using gated mechanisms (update gate zi, reset gate ri) that weigh new proxy information against accumulated state, enabling selective retention of relevant semantic cues.
- **Core assumption:** Semantic proxies from different batches carry complementary and noisy information requiring selective integration.
- **Evidence anchors:**
  - [section 4.3] "we propose a novel and effective way to collect proxies in a recurrent manner, in which the semantic proxy is produced to be updated based on its current state and correlation to the new proxy vector."
  - [section 5.3] "our GRU accumulation improves the performance on both R@1 and MAP@R."
  - [corpus] Weak evidence: related works use adapters or simple pooling but not GRU for proxy aggregation.
- **Break condition:** If semantic proxies from different batches are highly correlated, GRU gating may introduce unnecessary complexity.

### Mechanism 3
- **Claim:** Combining original random proxies with semantic proxies as bias balances fast adaptation with semantic stability.
- **Mechanism:** The output proxy Qc is constructed as a weighted sum: Qc = (1 - α)P_c + αO_c, where O_c is the original proxy updated every batch and P_c is the semantic proxy updated sparsely.
- **Core assumption:** Random proxies can be updated more frequently than semantic proxies without semantic drift.
- **Evidence anchors:**
  - [section 4.3] "we also add the original proxies, initialed randomly, as a bias with a certain ratio α to construct the output representation."
  - [section A.3] "we would update both the semantic proxies and the bias, whereas, outside the mini-batch, we would update the bias."
  - [corpus] Weak evidence: standard DML uses only random proxies; this bias combination is novel.
- **Break condition:** If α is too high, semantic benefits are diluted; if too low, semantic proxies are not sufficiently utilized.

## Foundational Learning

- **Concept: Proxy-based Deep Metric Learning**
  - Why needed here: The paper builds its semantic proxy framework on proxy-based DML (e.g., Proxy-Anchor loss) to reduce computational cost and improve scalability.
  - Quick check question: What is the role of proxies in Proxy-Anchor loss, and how do they differ from sample-sample comparisons?

- **Concept: Vision Transformer (ViT) Embedding**
  - Why needed here: ViT provides semantic embeddings from the [CLS] token that are used to initialize and update semantic proxies.
  - Quick check question: How does ViT's patch embedding and attention mechanism contribute to semantic representation?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - Why needed here: The proposed framework only fine-tunes a small fraction of parameters (visual prompts and proxies) while freezing the backbone.
  - Quick check question: What distinguishes visual prompt tuning from adapter-based fine-tuning in terms of parameter count and placement?

## Architecture Onboarding

- **Component map:**
  ViT backbone (frozen except for prompts) -> Visual prompts (class-based + dataset prompts) -> Semantic proxy generator (GRU-based accumulator) -> Original proxy bias -> Proxy-Anchor loss head

- **Critical path:**
  1. Input image → ViT → [CLS] embedding
  2. Add class-based prompts to prompt sequence
  3. Pass augmented prompts through head → semantic proxy
  4. Accumulate semantic proxy via GRU
  5. Combine with original proxy bias → final proxy
  6. Compute Proxy-Anchor loss against sample embeddings

- **Design tradeoffs:**
  - Memory vs. accuracy: More prompts improve semantic richness but increase memory usage.
  - GRU complexity vs. EMA simplicity: GRU selectively integrates proxies but adds learnable parameters.
  - Proxy bias ratio α: Balances semantic stability vs. adaptation speed.

- **Failure signatures:**
  - Poor convergence: Semantic proxies may not initialize meaningfully if ViT lacks semantic richness.
  - Overfitting on small datasets: Too many class-based prompts may cause memorization.
  - Memory overflow: Large number of prompts and proxies can exceed GPU capacity.

- **First 3 experiments:**
  1. **Sanity check:** Compare baseline VPT vs. VPT with semantic proxies on CUB200 (R@1, MAP@R).
  2. **Ablation:** Test GRU vs. EMA aggregation on CUB200 to measure impact on convergence.
  3. **Hyperparameter sweep:** Vary α (proxy bias ratio) and measure trade-off between semantic proxy and original proxy contribution.

## Open Questions the Paper Calls Out
- How does the proposed semantic proxy framework generalize to datasets with extremely large numbers of classes, where updating individual proxies for each class becomes computationally infeasible?
- What is the optimal trade-off between the number of prompts, their layer positions, and the resulting parameter efficiency and retrieval performance in the proposed framework?
- How does the proposed framework perform when fine-tuning pre-trained models on datasets with significantly different data distributions compared to the pre-training dataset?

## Limitations
- The GRU-based proxy aggregation mechanism introduces significant complexity without rigorous comparison to simpler alternatives across different settings.
- The paper does not empirically validate whether pre-trained ViT models contain sufficient class-specific semantic information for meaningful proxy initialization.
- Parameter efficiency claims are somewhat misleading as the framework still tunes millions of parameters that must be stored and updated during training.

## Confidence
**High Confidence:** The core observation that parameter-efficient fine-tuning can achieve competitive DML performance is well-supported by experimental results.

**Medium Confidence:** Claims about semantic proxies improving convergence and performance are supported by ablation studies, but the mechanism is not fully explained.

**Low Confidence:** The GRU aggregation mechanism's superiority over simpler alternatives is not convincingly demonstrated across different settings.

## Next Checks
1. **Semantic Content Validation:** Analyze learned visual prompts to verify they contain meaningful semantic information through visualization, alignment with class labels, or clustering analysis.

2. **GRU Necessity Test:** Compare GRU-based aggregation against EMA, simple averaging, and attention-based aggregation across different dataset sizes and ViT model scales to determine if GRU's complexity is justified.

3. **Cross-Dataset Generalization:** Evaluate the framework's performance when fine-tuning on one dataset and testing on another to assess whether semantic proxies improve generalization or cause overfitting.