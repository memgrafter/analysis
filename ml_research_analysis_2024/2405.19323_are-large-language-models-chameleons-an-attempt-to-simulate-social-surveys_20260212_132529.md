---
ver: rpa2
title: Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys
arxiv_id: '2405.19323'
source_url: https://arxiv.org/abs/2405.19323
tags:
- llms
- arxiv
- data
- language
- survey
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines whether large language models (LLMs) can simulate
  social surveys by generating responses to subjective questions using demographic
  prompts. The authors compared LLM outputs with European Social Survey data across
  4 countries and 9 questions, testing multiple models including GPT-3.5, GPT-4o,
  LLaMA variants, Mistral, and DeepSeek-V2.
---

# Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys

## Quick Facts
- arXiv ID: 2405.19323
- Source URL: https://arxiv.org/abs/2405.19323
- Authors: Mingmeng Geng; Sihong He; Roberto Trotta
- Reference count: 40
- One-line primary result: LLMs can simulate survey responses with means close to survey data but show significantly lower variance than human responses

## Executive Summary
This study investigates whether large language models can effectively simulate social surveys by generating responses to subjective questions using demographic prompts. The authors compare LLM outputs with European Social Survey data across four countries and nine questions, testing multiple models including GPT-3.5, GPT-4o, LLaMA variants, Mistral, and DeepSeek-V2. Results reveal that while LLMs can produce responses with means close to survey data, their variance is typically much smaller than real respondents, indicating over-smooth behavior. The study introduces a Jaccard-inspired similarity metric to compare distributions and finds that prompts significantly affect both bias and variability, with order of answer options having particularly strong impacts.

## Method Summary
The authors obtained ESS round 10 data from Germany, Greece, Bulgaria, and Italy (16,132 samples total) and generated LLM responses using demographic prompts that included gender, age, country, and occupation information. They compared responses using both mean differences and a novel J-index metric that measures distribution similarity. The study tested multiple LLM models through API calls, analyzing variance patterns and geographic performance disparities. Responses were processed using weighted averages to account for ESS sampling design, and variance analysis was conducted to identify systematic differences between simulated and real survey responses.

## Key Results
- LLM responses show means close to survey data but variance is typically 50-80% smaller than human responses
- GPT-3.5 performed best among tested models, with one-third of questions falling within ±5% of survey means
- Geographic performance disparities were observed, with particularly poor performance for Bulgarian respondents
- Prompt structure significantly affects both bias and variability, with answer option ordering having strong impacts on responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM outputs are sensitive to demographic prompts and can mimic survey responses with varying degrees of accuracy
- Mechanism: LLMs interpret demographic variables (gender, age, country, occupation) as conditioning context, altering their response distribution
- Core assumption: The LLM's training data includes sufficient cultural and demographic variation to support demographic conditioning
- Evidence anchors:
  - "A comparison of different LLM responses with the European Social Survey (ESS) data suggests that the effect of prompts on bias and variability is fundamental"
  - "LLMs such as ChatGPT may also be more familiar with American culture, compared to other countries"
- Break condition: If the LLM's training data lacks sufficient representation of target demographics

### Mechanism 2
- Claim: Variance in LLM-generated survey responses is systematically lower than human responses, requiring specialized comparison metrics
- Mechanism: LLMs, trained to optimize for typical responses and avoid extreme outputs, generate responses with artificially constrained variance
- Core assumption: LLMs are inherently biased toward producing more "average" responses due to training objectives and post-training alignment processes
- Evidence anchors:
  - "the variance of the simulated data is too small compared to the real ones"
  - "the variance of LLM responses is smaller"
- Break condition: If future LLM architectures produce outputs with variance matching human distributions

### Mechanism 3
- Claim: Prompt structure and ordering significantly affect LLM survey responses, introducing systematic biases
- Mechanism: The order of answer options and the amount of contextual information in prompts create anchoring effects and framing biases in LLM outputs
- Core assumption: LLMs process prompts sequentially and are susceptible to ordering effects similar to human survey respondents
- Evidence anchors:
  - "we tried reversing the order of answer options"
  - "The order in which options are present affects humans' choice"
- Break condition: If LLMs develop robustness to prompt ordering through architectural changes

## Foundational Learning

- Concept: Distribution comparison metrics beyond mean differences
  - Why needed here: Standard mean comparison fails to capture the variance mismatch between LLM and human responses
  - Quick check question: Why would two response distributions with identical means but different variances be considered equally good by mean-only comparison?

- Concept: Demographic conditioning in language models
  - Why needed here: Understanding how demographic variables in prompts affect LLM outputs is crucial for interpreting survey simulation results
  - Quick check question: How might an LLM's training data composition affect its ability to simulate survey responses from underrepresented demographic groups?

- Concept: Prompt engineering best practices and their limitations
  - Why needed here: The study shows that prompt structure significantly affects outputs, requiring careful experimental design
  - Quick check question: What are the ethical implications of using prompt engineering to manipulate LLM survey responses?

## Architecture Onboarding

- Component map: ESS data → preprocessing → weighting → LLM interface → Response processing → Statistical analysis → Visualization
- Critical path: Prompt generation → LLM response collection → Statistical analysis → Visualization and interpretation
- Design tradeoffs: Using real survey data provides representativeness but limits question selection; API-based LLM access enables experimentation but introduces rate limits and cost constraints
- Failure signatures: Unusually low variance in responses, geographic imbalances in performance, sensitivity to answer option ordering, missing data from non-responsive models
- First 3 experiments:
  1. Test variance reduction across multiple questions with fixed demographic prompts to establish baseline behavior
  2. Compare J-index sensitivity to mean difference by systematically varying prompt order and information content
  3. Evaluate geographic performance disparities by running identical prompts across different country demographics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the variance of LLM-generated survey responses to better match real human variability?
- Basis in paper: The authors note that "the variance of the simulated data is too small compared to the real ones"
- Why unresolved: Current approaches only partially address the issue, and the fundamental cause of over-smooth behavior is not understood
- What evidence would resolve it: Methods that successfully increase variance without sacrificing accuracy, or theoretical analysis explaining why LLMs produce lower variance responses

### Open Question 2
- Question: What is the optimal prompt structure for survey simulation that balances consistency with human-like variability?
- Basis in paper: The authors find that "the order of the options affects the results of the LLMs simulations" but no clear pattern emerges
- Why unresolved: The effect appears inconsistent across questions and no generalizable principles have been established
- What evidence would resolve it: Systematic comparison of different prompt structures across diverse question types and cultures

### Open Question 3
- Question: Why do LLMs show geographic imbalances, performing particularly poorly for Bulgarian respondents?
- Basis in paper: The authors observe that "simulations of Bulgarians work worse compared to people from other countries"
- Why unresolved: This is noted as a clear geographic imbalance but the underlying cause remains unexplored
- What evidence would resolve it: Analysis of training data distribution across countries or experiments testing whether this pattern holds for other underrepresented cultures

## Limitations
- Geographic imbalance in LLM performance, with particularly poor results for Bulgarian respondents, suggests potential training data biases
- Variance reduction observed across all models indicates a fundamental limitation in LLM survey simulation capabilities
- Study relies on a single survey dataset (ESS round 10) and nine questions, limiting generalizability

## Confidence
- **High Confidence**: LLM responses show systematically lower variance than human survey data; sensitivity of responses to prompt structure; geographic performance disparities
- **Medium Confidence**: Relative performance ranking of models; effectiveness of J-index metric
- **Low Confidence**: Specific numerical thresholds for "good" performance; generalizability beyond the four European countries studied

## Next Checks
1. Run the same simulation protocol using survey data and LLM prompts from non-European countries to assess geographic performance patterns
2. Compare the J-index against established distributional similarity metrics across synthetic datasets with known variance differences
3. Repeat simulations after significant changes to LLM training data or model updates to determine whether observed patterns are stable properties