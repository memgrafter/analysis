---
ver: rpa2
title: 'CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution'
arxiv_id: '2410.16256'
source_url: https://arxiv.org/abs/2410.16256
tags:
- data
- judge
- evaluation
- training
- lydian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CompassJudger-1, the first open-source all-in-one
  judge model for large language model evaluation. The model is designed to handle
  multiple subjective evaluation tasks including unitary scoring, two-model comparisons,
  format-specific evaluations, and critique generation.
---

# CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution

## Quick Facts
- arXiv ID: 2410.16256
- Source URL: https://arxiv.org/abs/2410.16256
- Authors: Maosong Cao; Alexander Lam; Haodong Duan; Hongwei Liu; Songyang Zhang; Kai Chen
- Reference count: 21
- Key outcome: CompassJudger-1 achieves strong performance, outperforming open-source models and reaching over 95% of GPT-4o's judging capability on JudgerBench

## Executive Summary
This paper introduces CompassJudger-1, the first open-source all-in-one judge model designed for comprehensive large language model evaluation. The model handles multiple subjective evaluation tasks including unitary scoring, two-model comparisons, format-specific evaluations, and critique generation. Through a carefully designed training approach using diverse data sources with a 1:3:1 ratio of critique to reward to general SFT data, CompassJudger-1 achieves strong performance across various evaluation scenarios. The authors also develop JudgerBench, a specialized evaluation dataset that combines human and LLM annotations to create realistic evaluation scenarios. The model demonstrates good generalization across different domains and languages while maintaining strong performance relative to leading judge models.

## Method Summary
The authors develop CompassJudger-1 through a comprehensive training pipeline that combines diverse data sources: public judge data, self-collected subjective evaluation data, and reward data in a 1:3:1 ratio (critique:reward:general SFT). They use the Qwen2.5 series as foundation models and employ Xtuner for fine-tuning with specific hyperparameters (2 epochs, learning rate of 2e-5). The training incorporates data filtering and sampling strategies to enhance quality and diversity. For evaluation, they create JudgerBench, which includes human annotations for arena-style comparisons and LLM annotations for benchmark tasks, providing a comprehensive assessment framework that captures both human preferences and technical evaluation capabilities.

## Key Results
- CompassJudger-1 outperforms open-source judge models on JudgerBench evaluation
- Achieves over 95% of GPT-4o's judging capability across multiple evaluation tasks
- Demonstrates strong generalization across Chinese and English evaluation domains
- Maintains good performance on out-of-domain subjective tasks while avoiding overfitting to specific formats

## Why This Works (Mechanism)

### Mechanism 1
The 1:3:1 data ratio (critique:reward:general SFT) enables balanced performance across both generative and discriminative evaluation tasks. The higher proportion of reward data (3 parts) provides strong comparative evaluation skills, while critique data (1 part) ensures the model can generate detailed reasoning and explanations. General SFT data (1 part) maintains overall task flexibility and prevents overfitting to evaluation-specific formats.

### Mechanism 2
JudgerBench's dual annotation approach (human for arena, LLM for benchmarks) creates a realistic evaluation environment matching real-world usage scenarios. Human annotations capture genuine preferences and nuances, while LLM annotations provide scalable, consistent benchmarking across diverse subjective tasks. This combination ensures the judge model is evaluated on both human-aligned preferences and technical evaluation capabilities.

### Mechanism 3
Using Qwen2.5 series as foundation models provides strong reasoning and instruction-following capabilities that transfer well to judge model tasks. The Qwen2.5 models' strong general capabilities in reasoning and instruction following serve as a foundation that can be specialized for evaluation tasks through fine-tuning, leveraging pre-existing general intelligence rather than training from scratch.

## Foundational Learning

- **Balanced dataset composition for multi-task learning**: Judge models need to perform diverse tasks (scoring, comparison, critique generation) that require different capabilities, necessitating a balanced training approach. *Quick check: If you only use reward data for training, what capability would the model likely lose?*

- **Dual-annotation benchmark design**: Real-world judge model deployment requires both human preference alignment and technical evaluation accuracy, requiring a benchmark that captures both aspects. *Quick check: Why would using only human annotations for a large-scale benchmark be problematic?*

- **Foundation model transfer learning**: Judge model capabilities build upon general language understanding and reasoning, making foundation models with strong general capabilities suitable starting points. *Quick check: What would be the risk of training a judge model from scratch versus fine-tuning a strong foundation model?*

## Architecture Onboarding

- **Component map**: Data preprocessing pipeline → Model training with XTuner → JudgerBench evaluation framework → CompassArena data collection → CompassJudger deployment
- **Critical path**: Data collection and preprocessing → Model fine-tuning with balanced data ratios → Comprehensive evaluation on JudgerBench → Model iteration and deployment
- **Design tradeoffs**: Using larger foundation models provides better judge performance but increases computational cost; balanced data ratios prevent overfitting but may limit specialization in specific evaluation tasks
- **Failure signatures**: Model overfitting to specific formats (evidenced by poor performance on out-of-domain tasks), bias toward longer responses, inability to follow dataset-specific evaluation instructions
- **First 3 experiments**:
  1. Test different data ratio combinations (e.g., 1:1:1, 1:2:1, 1:4:1) to observe impact on judge model performance across different task types
  2. Evaluate model performance on out-of-domain subjective tasks to test generalization capabilities
  3. Compare fine-tuned models against their foundation models on judge-specific tasks to quantify performance gains from fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the data ratio of 1:3:1 (critique data : reward data : general SFT data) affect the model's performance in out-of-domain evaluation tasks?
- **Basis in paper**: Explicit - stated as the optimal training data ratio based on their findings.
- **Why unresolved**: The paper mentions that this ratio helps maintain generalizability and judging performance, but does not provide specific experimental results or comparisons with other ratios on out-of-domain tasks.
- **What evidence would resolve it**: Experimental results showing the performance of CompassJudger-1 with different data ratios on a variety of out-of-domain evaluation tasks, demonstrating the impact of the 1:3:1 ratio on generalization.

### Open Question 2
- **Question**: What is the impact of using Qwen2.5-72B for re-evaluating outdated judge data on the overall quality and relevance of the training dataset?
- **Basis in paper**: Explicit - mentioned as a strategy to update outdated judge data that used older models like ChatGPT.
- **Why unresolved**: While the paper states that re-evaluation was performed using Qwen2.5-72B, it does not provide quantitative measures of how this affected the quality or relevance of the training data, nor does it compare the performance of the model with and without this re-evaluation step.
- **What evidence would resolve it**: Quantitative analysis comparing the quality metrics of the training data before and after re-evaluation with Qwen2.5-72B, along with model performance comparisons using data with and without re-evaluation.

### Open Question 3
- **Question**: How does the model's performance on JudgerBench correlate with its ability to assist in the iteration and evolution of other models?
- **Basis in paper**: Inferred - the paper discusses the potential of judge models to assist in model iteration and evolution, but does not provide empirical evidence linking JudgerBench performance to this capability.
- **Why unresolved**: The paper mentions that the judge model can help point out shortcomings and provide guidance for model improvement, but does not demonstrate or quantify this relationship through experiments or case studies.
- **What evidence would resolve it**: Case studies or experiments showing how models with high JudgerBench scores perform in assisting the iteration and evolution of other models, with measurable improvements in the target models' performance.

## Limitations

- **Data Quality and Composition Uncertainty**: The optimal 1:3:1 data ratio is empirically determined but lacks sensitivity analysis showing how performance varies with different ratios. Quality control mechanisms for self-collected data are not detailed.
- **Benchmark Representativeness Concerns**: Human annotations rely on arena-style pairwise comparisons which may not represent real-world evaluation diversity. LLM-generated annotations could introduce systematic biases.
- **Generalization Claims Need Validation**: Performance on objective evaluation tasks, code generation assessment, or domain-specific technical evaluations remains unverified beyond subjective benchmarks.

## Confidence

**High Confidence**: The fundamental approach of using foundation models for judge model development is sound, given the established success of transfer learning in NLP.

**Medium Confidence**: The specific data ratio of 1:3:1 and the effectiveness of JudgerBench as an evaluation framework are empirically validated but lack comprehensive ablation studies.

**Low Confidence**: Claims about the model's ability to handle "various subjective evaluation tasks" are limited by the scope of tested benchmarks. The assertion of "good generalization" across all domains is not thoroughly validated.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate CompassJudger-1 on objective tasks (code quality assessment, mathematical problem verification) and domain-specific evaluations (medical text assessment, legal document review) to verify the claimed broad applicability beyond subjective benchmarks.

2. **Data Ratio Sensitivity Analysis**: Systematically vary the training data ratio (e.g., test 1:1:1, 1:4:1, 2:3:1 combinations) and measure performance impacts on different task types to determine if the 1:3:1 ratio is truly optimal or task-dependent.

3. **Long-term Stability and Bias Audit**: Conduct longitudinal testing over multiple months with diverse input distributions to identify potential bias accumulation, performance drift, or emergent failure modes that might not be apparent in short-term evaluations.