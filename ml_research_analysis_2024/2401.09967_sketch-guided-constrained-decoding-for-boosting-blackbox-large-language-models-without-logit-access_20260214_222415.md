---
ver: rpa2
title: Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models
  without Logit Access
arxiv_id: '2401.09967'
source_url: https://arxiv.org/abs/2401.09967
tags:
- constrained
- decoding
- sgcd
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sketch-Guided Constrained Decoding (SGCD),
  a method for applying constrained decoding to blackbox large language models (LLMs)
  without requiring access to the model's logits. The key idea is to use a locally
  hosted auxiliary model to refine the output of an unconstrained blackbox LLM, treating
  the initial output as a "sketch" that the auxiliary model then elaborates while
  respecting specified constraints.
---

# Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access

## Quick Facts
- arXiv ID: 2401.09967
- Source URL: https://arxiv.org/abs/2401.09967
- Authors: Saibo Geng, Berkay Döner, Chris Wendler, Martin Josifoski, Robert West
- Reference count: 18
- One-line primary result: SGCD improves precision by 13.7% and F1 by 2.8% on Wiki-NRE using GPT-4 for closed information extraction

## Executive Summary
This paper introduces Sketch-Guided Constrained Decoding (SGCD), a method for applying constrained decoding to blackbox large language models (LLMs) without requiring access to the model's logits. The key idea is to use a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, treating the initial output as a "sketch" that the auxiliary model then elaborates while respecting specified constraints. This approach is complementary to traditional logit-based constrained decoding techniques and enables their application in settings where full model transparency is unavailable.

The authors demonstrate the efficacy of SGCD through experiments on closed information extraction and constituency parsing tasks. On closed information extraction, SGCD significantly improves precision and F1-score compared to unconstrained decoding with powerful blackbox LLMs. For example, on the Wiki-NRE dataset, GPT-4 with SGCD achieves a 13.7% improvement in precision and a 2.8% improvement in F1-score compared to GPT-4 alone. On constituency parsing, SGCD increases the validity of generated parse trees to nearly 100% by construction, while also improving other metrics like bracketing precision, recall, and tag accuracy.

## Method Summary
SGCD addresses the challenge of applying constrained decoding to blackbox LLMs that don't provide logit access. The method splits the task into two phases: a powerful blackbox LLM (the sketcher) generates an unconstrained initial output, which is then refined by a smaller, locally-hosted auxiliary model (the constrained generator) using constrained decoding. The constrained generator uses grammars to enforce output constraints while incorporating information from the sketcher's output. This two-phase approach leverages the superior generation capabilities of blackbox LLMs while maintaining the constraint enforcement benefits of constrained decoding.

## Key Results
- On Wiki-NRE closed information extraction, GPT-4 with SGCD improves precision by 13.7% and F1 by 2.8% compared to GPT-4 alone
- For constituency parsing, SGCD increases parse tree validity to nearly 100% while also improving bracketing precision/recall and tag accuracy
- SGCD consistently outperforms direct few-shot prompting with constrained decoding using open-source models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGCD enables constrained decoding without requiring logit access by splitting the task into two phases: sketching and constrained generation.
- Mechanism: In the sketching phase, a blackbox LLM generates an unconstrained "sketch" of the answer. In the constrained generation phase, a smaller auxiliary model refines this sketch while respecting specified constraints, using constrained decoding.
- Core assumption: The blackbox LLM can generate a high-quality initial sketch that contains sufficient information for the auxiliary model to produce a constrained, valid output.
- Evidence anchors:
  - [abstract] "SGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a 'sketch' for further elaboration."
  - [section 2] "SGCD splits the constrained decoding task into two distinct phases: sketching and constrained generation."
- Break condition: If the blackbox LLM sketch is of low quality or misses critical information, the auxiliary model may struggle to generate a valid output that meets the constraints.

### Mechanism 2
- Claim: SGCD leverages the complementary strengths of a powerful blackbox LLM for initial generation and a smaller, locally-hosted model for constraint adherence.
- Mechanism: The blackbox LLM (sketcher) is used for its strong generation capabilities to produce a comprehensive sketch. The auxiliary model (constrained generator) is then used to refine this sketch while enforcing the constraints, as it has access to the model's logits and can perform constrained decoding.
- Core assumption: The blackbox LLM is more capable at unconstrained generation than the auxiliary model, but the auxiliary model is more effective at enforcing constraints when it has access to logits.
- Evidence anchors:
  - [abstract] "SGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a 'sketch' for further elaboration."
  - [section 2] "Given the quality of y∗, the constrained generator can be implemented using a much smaller model, as its primary task is to rewrite the sketch y∗ with the help of constrained decoding."
- Break condition: If the auxiliary model is not sufficiently capable at constrained decoding, or if the sketch is too complex for the auxiliary model to effectively refine, the final output may not meet the constraints.

### Mechanism 3
- Claim: SGCD achieves better performance than direct few-shot prompting with constrained decoding by leveraging the superior generation capabilities of the blackbox LLM.
- Mechanism: The blackbox LLM sketch captures more of the desired output structure and content than what the auxiliary model could generate directly. The auxiliary model then only needs to refine this sketch to meet the constraints, rather than generating the entire output from scratch.
- Core assumption: The blackbox LLM's generation capabilities exceed those of the auxiliary model, allowing it to produce a more comprehensive and accurate initial sketch.
- Evidence anchors:
  - [section 2] "SGCD builds on the expectation that the constrained refined output z∗ should be at least as good as both y∗ (as z∗ respects the constraints) and w∗ (as Psk is a more powerful LLM than Pcg)."
  - [section 3] "Table 1 illustrates that powerful blackbox LLMs, even without constrained decoding, already exceed the performance of open-source LLMs that employ constrained decoding."
- Break condition: If the blackbox LLM's generation capabilities are not significantly better than the auxiliary model's, or if the constraints are very complex, the performance gain from using SGCD may be minimal.

## Foundational Learning

- Concept: Constrained decoding
  - Why needed here: SGCD relies on constrained decoding to ensure the final output meets the specified constraints. Understanding how constrained decoding works is crucial for implementing and troubleshooting SGCD.
  - Quick check question: What is the main difference between constrained decoding and unconstrained decoding?

- Concept: Context-free grammars and formal languages
  - Why needed here: SGCD uses grammars to define the constraints on the output. Understanding context-free grammars and formal languages is necessary for designing and applying the appropriate constraints.
  - Quick check question: How does a context-free grammar define a formal language?

- Concept: Few-shot prompting
  - Why needed here: SGCD uses few-shot prompting to provide examples and instructions to the LLMs. Understanding how few-shot prompting works is important for crafting effective prompts for the sketcher and constrained generator.
  - Quick check question: What is the purpose of including demonstrations in few-shot prompting?

## Architecture Onboarding

- Component map: Blackbox LLM (sketcher) -> Auxiliary model (constrained generator) -> Grammar engine
- Critical path:
  1. Provide instruction and demonstrations to the sketcher
  2. Sketcher generates an unconstrained sketch
  3. Provide instruction, demonstrations, input, and sketch to the constrained generator
  4. Constrained generator refines the sketch while enforcing constraints using constrained decoding
  5. Output the final, constrained result

- Design tradeoffs:
  - Choosing the sketcher: A more powerful sketcher can generate better sketches but may be more expensive to use
  - Choosing the constrained generator: A more capable constrained generator can better enforce constraints but may require more computational resources
  - Designing the grammar: A more expressive grammar can enforce more complex constraints but may be more difficult to define and parse

- Failure signatures:
  - Sketch degeneration: The constrained generator fails to adhere to the sketch, producing a lower-quality output
  - Constraint violation: The final output does not meet the specified constraints
  - Performance degradation: The SGCD approach performs worse than direct few-shot prompting with constrained decoding

- First 3 experiments:
  1. Validate that the sketcher can generate a reasonable initial sketch for a simple task
  2. Test the constrained generator's ability to refine the sketch while enforcing constraints using a known grammar
  3. Compare the performance of SGCD against direct few-shot prompting with constrained decoding on a simple task

## Open Questions the Paper Calls Out
- How can the degeneration problem in SGCD be effectively mitigated?
- Does the performance advantage of SGCD diminish as blackbox LLMs continue to improve?
- How does SGCD compare to iterative logit bias-based approaches in terms of both performance and cost-effectiveness?

## Limitations
- The paper assumes blackbox LLMs provide reliable token probability access through APIs, which may not be consistently available
- The method requires a significant capability gap between the sketcher and constrained generator, which may not always exist
- The process for translating task requirements into formal grammars is not fully detailed, particularly for complex output structures

## Confidence
- **High Confidence**: The core feasibility of SGCD - that splitting constrained decoding into sketching and refinement phases enables application to blackbox models
- **Medium Confidence**: The performance gains attributed to SGCD, though the contribution of each phase is not fully isolated
- **Low Confidence**: The generalizability of SGCD to other constrained generation tasks beyond information extraction and parsing

## Next Checks
1. **Ablation Study on Sketching Quality**: Systematically evaluate how SGCD performance varies with different quality sketches by using controlled modifications to the blackbox LLM's output
2. **Constraint Violation Analysis**: Conduct a detailed error analysis to quantify how often and in what ways the final SGCD outputs violate constraints
3. **Cross-Task Generalization Test**: Apply SGCD to a third, distinct constrained generation task (such as semantic parsing or code generation) to evaluate whether the methodology generalizes beyond the demonstrated applications