---
ver: rpa2
title: 'EXAGREE: Mitigating Explanation Disagreement with Stakeholder-Aligned Models'
arxiv_id: '2411.01956'
source_url: https://arxiv.org/abs/2411.01956
tags:
- explanation
- 'true'
- plausibility
- rashomon
- agreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EXAGREE addresses explanation disagreement by treating it as a
  resource for stakeholder-aligned model selection. The method introduces Stakeholder-Machine
  Agreement (SMA) as a unified metric combining faithfulness and plausibility, then
  uses a two-stage framework: first exploring the Rashomon set via constrained sampling,
  then optimizing a differentiable mask-based attribution network (DMAN) coupled with
  monotone differentiable sorting to maximize SMA.'
---

# EXAGREE: Mitigating Explanation Disagreement with Stakeholder-Aligned Models

## Quick Facts
- arXiv ID: 2411.01956
- Source URL: https://arxiv.org/abs/2411.01956
- Authors: Sichao Li; Tommy Liu; Quanling Deng; Amanda S. Barnard
- Reference count: 36
- Primary result: Improves faithfulness by up to +0.43 and plausibility by up to +0.51 over baselines without sacrificing accuracy

## Executive Summary
EXAGREE addresses the fundamental challenge of explanation disagreement in XAI by treating it as a resource for finding stakeholder-aligned models. The framework introduces Stakeholder-Machine Agreement (SMA) as a unified metric that combines faithfulness and plausibility, enabling gradient-based optimization within a constrained model space. Through a two-stage approach involving Rashomon set exploration and differentiable mask-based attribution networks, EXAGREE identifies models that better satisfy stakeholder needs while maintaining predictive accuracy.

## Method Summary
EXAGREE operates through a two-stage framework: first exploring the Rashomon set via constrained sampling (GRS) to collect near-optimal models, then optimizing a differentiable mask-based attribution network (DMAN) coupled with monotone differentiable sorting to maximize SMA. The method treats explanation disagreement as an opportunity to find stakeholder-aligned explanation models (SAEMs) that balance model fidelity with stakeholder preferences. The unified SMA metric enables efficient optimization by combining faithfulness and plausibility into a single objective that can be maximized through gradient descent.

## Key Results
- Achieves faithfulness improvements of +0.43 and plausibility gains of +0.51 over baseline explanation methods
- Reduces fairness gaps by up to -0.28 while preserving task accuracy across six real-world datasets
- Demonstrates consistent performance across diverse domains including credit scoring, income prediction, and criminal justice

## Why This Works (Mechanism)

### Mechanism 1
EXAGREE turns explanation disagreement into a resource by searching within a Rashomon set to find a stakeholder-aligned model. The method first explores the Rashomon set using constrained sampling (GRS) to collect a variety of near-optimal models, then uses a differentiable mask-based attribution network (DMAN) to map each model's masks to feature attributions, and finally optimizes for the maximum Stakeholder-Machine Agreement (SMA) using differentiable sorting. The core assumption is that there exists at least one model within the Rashomon set that can achieve higher agreement with stakeholder expectations than the original model.

### Mechanism 2
SMA unifies faithfulness and plausibility into a single metric to enable efficient optimization. SMA is defined as the Spearman rank correlation between the stakeholder-grounded ranking (rk) and the machine-grounded ranking (rMtrue). This single metric allows the optimization to balance both faithfulness (model alignment) and plausibility (stakeholder alignment) simultaneously. The core assumption is that Spearman rank correlation adequately captures the alignment between stakeholder needs and model internals.

### Mechanism 3
Multi-head architecture allows exploration of diverse models that achieve the same SMA but differ in feature attribution patterns. For a given target ranking, multiple distinct rankings can achieve the same Spearman correlation. The multi-head Mask-based Model Network (MHMN) exploits this by training multiple heads, each corresponding to a potential SAEM, to explore the Pareto frontier of faithfulness-plausibility trade-offs. The core assumption is that different models can yield the same SMA but differ in their feature attributions, enabling exploration of diverse explanations.

## Foundational Learning

- **Concept: Rashomon set (Fisher, Rudin, and Dominici 2019)**
  - Why needed here: Provides the constrained model space within which EXAGREE searches for stakeholder-aligned explanations
  - Quick check question: What defines the Rashomon set for a given model and performance threshold?

- **Concept: Spearman rank correlation as a differentiable objective**
  - Why needed here: Enables gradient-based optimization of explanation alignment through the differentiable sorting network
  - Quick check question: How does Spearman correlation differ from Pearson correlation in this context?

- **Concept: Differentiable sorting and ranking networks (Petersen et al. 2022)**
  - Why needed here: Allows backpropagation through the ranking process to optimize attribution networks
  - Quick check question: What is the key mathematical property that makes sorting networks differentiable?

## Architecture Onboarding

- **Component map**: Dataset, pre-trained reference model, stakeholder preferences -> Rashomon set sampling (GRS) -> DMAN training -> MHMN with DiffSortNet -> SAEM(s) with improved faithfulness/plausibility
- **Critical path**: Rashomon sampling -> DMAN approximation -> MHMN optimization -> SAEM selection
- **Design tradeoffs**: Larger Rashomon set (higher ε) increases search space but may include lower-quality models; more heads in MHMN increase diversity but computational cost; sign loss vs. attribution direction flexibility for stakeholders
- **Failure signatures**: No improvement in FA/RA metrics suggests DMAN isn't capturing attribution patterns well; unstable training of DiffSortNet indicates poor sorting approximation; SAEM not improving plausibility suggests stakeholder preferences aren't being captured
- **First 3 experiments**: 1) Verify Rashomon set sampling produces expected number of models within ε threshold; 2) Test DMAN training on synthetic mask-attribution pairs to ensure learning capability; 3) Run single-head optimization on a small dataset to validate SMA calculation and improvement

## Open Questions the Paper Calls Out

- **Open Question 1**: How does EXAGREE's performance scale when applied to very high-dimensional datasets (e.g., 1000+ features) or datasets with extremely large sample sizes (e.g., millions of instances)?
- **Open Question 2**: Can EXAGREE effectively handle stakeholders with conflicting needs within the same group, where no single SAEM can satisfy all preferences simultaneously?
- **Open Question 3**: How sensitive is EXAGREE's explanation quality to the choice of Rashomon set threshold (ϵ), and is there an optimal strategy for selecting this parameter in practice?

## Limitations
- The effectiveness heavily depends on the quality and diversity of the Rashomon set
- Assumes stakeholder preferences can be adequately captured through feature importance rankings
- Computational overhead from the two-stage framework with differentiable sorting is significant

## Confidence
- **High**: The mathematical framework for SMA calculation and differentiable sorting is sound; improvement in faithfulness and plausibility metrics is well-documented across multiple datasets
- **Medium**: Theoretical guarantees around SMA optimization and multi-head architecture's ability to explore diverse solutions are reasonable but depend on specific implementation details
- **Low**: Generalizability of stakeholder preference modeling and framework's performance on complex model architectures needs further validation

## Next Checks
1. Systematically vary the ε threshold in GRS sampling to quantify how Rashomon set size affects SAEM quality and computational cost
2. Test the framework with synthetic stakeholder preferences (varying in complexity and alignment with ground truth) to identify when and why the approach succeeds or fails
3. Apply EXAGREE to deeper neural architectures (e.g., CNNs, transformers) and evaluate whether the differentiable sorting approximation remains effective for attribution optimization