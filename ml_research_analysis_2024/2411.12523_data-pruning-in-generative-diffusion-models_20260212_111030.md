---
ver: rpa2
title: Data Pruning in Generative Diffusion Models
arxiv_id: '2411.12523'
source_url: https://arxiv.org/abs/2411.12523
tags:
- samples
- pruning
- data
- arxiv
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates data pruning for generative diffusion\
  \ models, aiming to identify a core subset of training data that maintains or improves\
  \ model performance while reducing computational costs. The authors explore various\
  \ pruning methods including random pruning, loss monotonicity, gradient norm (GraNd),\
  \ \u21132 loss magnitude (EL2N), Moving-one-Sample-out (MoSo), and clustering-based\
  \ approaches."
---

# Data Pruning in Generative Diffusion Models

## Quick Facts
- **arXiv ID:** 2411.12523
- **Source URL:** https://arxiv.org/abs/2411.12523
- **Reference count:** 40
- **Primary result:** Diffusion models maintain or improve performance even when 90% of training data is removed, with clustering-based pruning outperforming more sophisticated methods

## Executive Summary
This paper investigates data pruning for generative diffusion models, aiming to identify core subsets of training data that maintain or improve model performance while reducing computational costs. The authors explore various pruning methods including random pruning, loss monotonicity, gradient norm (GraNd), ℓ2 loss magnitude (EL2N), Moving-one-Sample-out (MoSo), and clustering-based approaches. Their primary finding is that diffusion models are highly tolerant to data pruning, with some models maintaining performance even when 90% of the data is removed. Notably, a simple clustering method using embeddings from large visual models (CLIP or DINO) outperforms more sophisticated and computationally expensive methods.

## Method Summary
The authors evaluate six pruning methods across two datasets (CelebA-HQ and ImageNet) at various pruning ratios (25%, 50%, 75%, 90%). The methods include random pruning, loss monotonicity, GraNd, EL2N, MoSo, and clustering-based approaches using CLIP or DINO embeddings. The DiT framework with flow matching is implemented using a custom VQ-VAE for image encoding at 256x256 resolution. Models are trained for 120k iterations with batch size 256 on ImageNet. Evaluation uses multiple metrics including FID, F-score, Vendi score, and memorization detection via average distance to nearest training sample.

## Key Results
- Diffusion models maintain performance when up to 90% of training data is pruned
- Clustering-based pruning using CLIP/DINO embeddings outperforms all other methods
- On ImageNet, clustering-based pruning achieved 5-point better FID than unpruned dataset at 90% pruning ratio
- Data pruning can balance skewed datasets in an unsupervised manner through cluster balancing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models can maintain or improve performance even when 90% of training data is removed.
- Mechanism: Diffusion models iteratively denoise samples in a structured latent space, allowing them to extrapolate from partial data distributions rather than requiring full coverage.
- Core assumption: The autoencoder (VQ-VAE) provides a rich, compressed representation that preserves essential distributional information even from pruned datasets.
- Evidence anchors:
  - [abstract]: "diffusion models maintain high-quality outputs even with significantly reduced training datasets"
  - [section]: "diffusion models have a high tolerance threshold for pruning. In fact, on ImageNet, we find that we can prune as much as 90% of the data without any decline in performance"
  - [corpus]: Weak evidence - related papers focus on diffusion model pruning but don't specifically address 90% tolerance
- Break condition: When pruning ratio exceeds tolerance threshold (approximately 90-95%), the model starts memorizing training samples rather than generating novel ones.

### Mechanism 2
- Claim: Clustering in embedding space outperforms more sophisticated pruning methods.
- Mechanism: Clustering identifies representative samples from dense regions of the data distribution, ensuring the reduced dataset captures the most informative patterns for learning the full distribution.
- Core assumption: Samples near cluster centers are more representative of the true data distribution than samples in sparse regions or those selected by gradient-based methods.
- Evidence anchors:
  - [abstract]: "a simple clustering method using embeddings from large visual models (CLIP or DINO) outperforms more sophisticated and computationally expensive methods"
  - [section]: "we achieve better performance than the unpruned variant" when using clustering at 90% pruning ratio
  - [corpus]: Weak evidence - related clustering papers don't specifically compare to diffusion model pruning
- Break condition: When the number of clusters is too small to capture the full distribution complexity, or when using samples far from cluster centers which are less representative.

### Mechanism 3
- Claim: Data pruning can balance skewed datasets in an unsupervised manner.
- Mechanism: By selecting equal numbers of samples from each cluster, pruning ensures underrepresented populations receive fair representation in the training data.
- Core assumption: Clustering naturally groups similar data points, and balancing cluster representation translates to balancing the underlying data distribution.
- Evidence anchors:
  - [abstract]: "clustering can be used to balance skewed datasets in an unsupervised manner, ensuring fair representation of underrepresented populations"
  - [section]: "when training using balanced clusters, the model strives to increase the representations of all samples"
  - [corpus]: Weak evidence - related papers don't specifically address fairness through clustering-based pruning
- Break condition: When cluster sizes vary dramatically, making it impossible to select equal numbers without losing too much data.

## Foundational Learning

- Concept: Diffusion model mechanics and score matching
  - Why needed here: Understanding how diffusion models learn distributions is crucial for grasping why pruning works differently than in discriminative models
  - Quick check question: How does the iterative denoising process in diffusion models differ from single-pass generation in GANs or VAEs?

- Concept: Clustering and representation learning
  - Why needed here: The paper's key innovation relies on using pre-trained models (CLIP/DINO) to create meaningful embeddings for clustering
  - Quick check question: Why might embeddings from CLIP or DINO be more suitable for clustering than raw pixel values?

- Concept: Data pruning evaluation metrics
  - Why needed here: The paper uses multiple metrics (FID, F-score, Vendi score) to comprehensively evaluate pruning effectiveness
  - Quick check question: What does each metric (FID, F-score, Vendi score) measure, and why is it important to use multiple metrics?

## Architecture Onboarding

- Component map: Data pipeline: Original dataset → pretraining phase for pruning → pruned dataset → training phase
- Critical path: Data pruning → model training → evaluation
- Design tradeoffs: Clustering-based pruning is computationally expensive upfront but pays off with better performance; simpler methods like random pruning are cheap but may not optimize dataset quality
- Failure signatures: Sudden degradation in FID at high pruning ratios (>90%), memorization of training samples, imbalanced generation results
- First 3 experiments:
  1. Run random pruning at various ratios (25%, 50%, 75%, 90%) to establish baseline performance
  2. Implement clustering-based pruning using CLIP embeddings and compare to random baseline
  3. Test balancing skewed datasets by selecting equal samples from each cluster and evaluate fairness metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental reason diffusion models are so tolerant to data pruning compared to other generative models like VAEs and GANs?
- Basis in paper: [explicit] The paper notes that diffusion models maintain performance even when 90% of data is pruned, unlike VAEs and GANs which show immediate degradation. The authors attribute this to the iterative refinement process and the presence of an autoencoder.
- Why unresolved: While the authors provide some explanation about iterative denoising and autoencoder benefits, they don't fully explain the mechanistic differences at the level of how gradients propagate or how the score function estimation is affected by data reduction.
- What evidence would resolve it: Comparative studies of gradient norms, loss landscapes, and sample efficiency between diffusion models and other generative models during training with pruned datasets.

### Open Question 2
- Question: Does the clustering-based pruning method actually learn a more diverse distribution, or does it simply memorize cluster centers?
- Basis in paper: [inferred] The paper shows clustering-based pruning outperforms other methods, particularly at high pruning ratios, but doesn't investigate whether the model is learning to interpolate between cluster centers or simply memorizing them.
- Why unresolved: The evaluation metrics (FID, F-score, Vendi score) measure similarity to the original distribution but don't specifically test whether the model can generate samples that bridge gaps between clusters or only reproduces cluster prototypes.
- What evidence would resolve it: Analysis of generated samples' positions in the embedding space relative to cluster centers, testing interpolation capabilities between generated samples from different clusters.

### Open Question 3
- Question: What is the optimal way to determine the number of clusters k for a given dataset when using clustering-based pruning?
- Basis in paper: [explicit] The authors use k=24 for CelebA-HQ and k=1000 for ImageNet (matching the number of object categories), but acknowledge this is somewhat arbitrary and based on inertia values.
- Why unresolved: The paper doesn't investigate how different choices of k affect pruning effectiveness or whether there's a principled way to select k that maximizes the benefits of clustering-based pruning.
- What evidence would resolve it: Systematic experiments varying k across different scales and data distributions, comparing pruning performance and investigating the relationship between optimal k and dataset characteristics like intrinsic dimensionality or class balance.

### Open Question 4
- Question: How does the effectiveness of data pruning methods scale with dataset size and diversity beyond the two datasets studied?
- Basis in paper: [inferred] The authors compare results on CelebA-HQ (28k images) and ImageNet (1.2M images) and observe different behaviors, but don't explore intermediate scales or other data types.
- Why unresolved: The paper only examines two datasets of vastly different scales, leaving open questions about how pruning methods perform on medium-sized datasets or datasets with different characteristics (e.g., text, audio, or multimodal data).
- What evidence would resolve it: Experiments across a range of dataset sizes and types, investigating whether there's a threshold where pruning becomes beneficial or if the observed patterns hold across different domains.

## Limitations
- The 90% pruning tolerance claim may be dataset-specific, particularly strong on ImageNet but less consistent on CelebA-HQ
- Clustering method's success depends heavily on quality of CLIP/DINO embeddings, which may vary across different visual domains
- Custom VQ-VAE implementation details are not fully specified, impacting reproducibility

## Confidence

**High Confidence:**
- Diffusion models show higher tolerance to data pruning compared to other generative models
- Clustering-based pruning using pre-trained embeddings outperforms random pruning
- Models maintain quality at moderate pruning ratios (50-75%)

**Medium Confidence:**
- 90% pruning ratio maintains or improves performance across all tested datasets
- Clustering consistently outperforms more sophisticated pruning methods like GraNd and EL2N
- Balancing skewed datasets through clustering is effective in an unsupervised manner

**Low Confidence:**
- The 90% pruning tolerance threshold is universal across all diffusion model variants
- Clustering performance superiority holds for all types of visual datasets
- Memorization only becomes problematic at extreme pruning ratios (>95%)

## Next Checks
1. **Dataset Transferability Test**: Replicate the clustering-based pruning approach on additional datasets (e.g., LSUN, CIFAR-100) to verify the 90% pruning tolerance claim across different visual domains and distributions.

2. **Architecture Ablation Study**: Test the pruning methods across different diffusion model architectures (score-based vs flow-based, different backbone sizes) to determine if the observed tolerance is architecture-dependent.

3. **Embedding Quality Analysis**: Systematically evaluate how different embedding models (CLIP, DINO, and alternative vision-language models) affect clustering performance, and whether the superiority of clustering holds when using less optimal embeddings.