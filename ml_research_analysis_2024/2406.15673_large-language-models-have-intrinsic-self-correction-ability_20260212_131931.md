---
ver: rpa2
title: Large Language Models have Intrinsic Self-Correction Ability
arxiv_id: '2406.15673'
source_url: https://arxiv.org/abs/2406.15673
tags:
- answer
- prompt
- your
- llms
- temperature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the existence and mechanisms of intrinsic
  self-correction in large language models (LLMs). The authors propose that intrinsic
  self-correction functions similarly to chain-of-thought (CoT) and self-verification
  prompting, providing additional context for generating responses.
---

# Large Language Models have Intrinsic Self-Correction Ability

## Quick Facts
- **arXiv ID**: 2406.15673
- **Source URL**: https://arxiv.org/abs/2406.15673
- **Reference count**: 40
- **One-line result**: LLMs exhibit intrinsic self-correction ability when using fair prompts and zero temperature settings

## Executive Summary
This paper investigates the intrinsic self-correction capabilities of large language models and identifies the critical conditions for successful self-correction. The authors propose that self-correction functions similarly to chain-of-thought prompting by providing additional context for generating refined responses. Through extensive experiments across six benchmarks and four model families, they demonstrate that zero temperature and fair/unbiased prompts are essential for achieving accuracy improvements through self-correction. The study provides both theoretical analysis and empirical evidence showing that when these conditions are met, LLMs can effectively improve their initial responses.

## Method Summary
The authors conduct experiments using a three-stage self-correction process: initial answer generation, verification/rationale stage, and final answer refinement. They test four models (GPT-3.5, GPT-4, Mistral-7B, Llama-3.1-8B) across six benchmarks with varying temperature settings (0.0-1.5) and three prompt sets (biased, less biased, unbiased). The methodology involves measuring accuracy improvements after self-correction and comparing performance across different experimental conditions. The theoretical analysis examines the probability distributions of self-correction outcomes under different temperature settings.

## Key Results
- Zero temperature is critical for successful self-correction, as increasing temperature degrades performance particularly in GPT-3.5
- Fair/unbiased prompts are essential, as biased prompts can lead to accuracy decreases by randomly altering correct answers
- With optimal conditions (fair prompts and zero temperature), all evaluated models show accuracy improvements after self-correction
- Self-correction functions similarly to chain-of-thought prompting by providing additional reasoning context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intrinsic self-correction functions similarly to chain-of-thought (CoT) and self-verification prompting by providing additional context for generating responses
- Mechanism: The self-correction process adds intermediate reasoning steps that create a more complete reasoning chain, allowing the model to refine its final answer based on additional context
- Core assumption: LLMs can effectively use additional context generated during self-correction to improve their final outputs
- Evidence anchors:
  - [abstract]: "We posit that self-correction abilities bear similarities to chain-of-thought (CoT) and self-verification prompting methodologies, which furnish LLMs with additional context for generating the final response"
  - [section 2.2]: "pSC(A′, D, R2, A, R1|τ3, τ2, τ1) = p(A′|τ3, D, R2, τ2, A, R1, τ1) p(D, R2|τ2, A, R1, τ1) p(A|R1, τ1)p(R1|τ1)"
  - [corpus]: Weak - most related papers focus on confidence or uncertainty rather than the CoT-like mechanism
- Break condition: If the intermediate reasoning steps don't provide useful context or the model cannot effectively integrate this additional information into its final decision

### Mechanism 2
- Claim: Zero temperature is critical for successful self-correction because it reduces randomness in decision-making
- Mechanism: At zero temperature, the model deterministically selects the highest probability token, reducing "hallucinations" where the model randomly changes correct answers to incorrect ones
- Core assumption: The model's binary decisions during self-correction (correct/incorrect, yes/no) are more reliable when temperature is zero
- Evidence anchors:
  - [abstract]: "we identify two critical factors for successful self-correction: zero temperature and fair prompts"
  - [section 4.1]: "∂V ar(D) ∂T = α1/T (1 − α)1/T [ln(α) − ln(1 − α)][α1/T − (1 − α)1/T ] / T 2[α1/T + (1 − α)1/T ]3 ≥ 0"
  - [section 4.2]: "GPT-3.5 is adversely affected by increasing temperature, while GPT4 exhibits greater robustness to temperature variations"
- Break condition: If the model architecture inherently requires some temperature for proper functioning, or if the zero-temperature setting leads to overly confident but incorrect decisions

### Mechanism 3
- Claim: Fair/unbiased prompts are essential for successful self-correction because biased prompts can lead to accuracy decreases
- Mechanism: Biased prompts that encourage answer changes can cause the model to randomly alter correct answers, introducing errors rather than correcting them
- Core assumption: LLMs strictly follow prompt instructions, including implicit biases that encourage changing answers
- Evidence anchors:
  - [abstract]: "unbiased prompts and zero temperature settings in harnessing their full potential"
  - [section 5.1]: "γ% of D will be inverted... randomly inverting D will lead to an accuracy drop"
  - [section 5.2]: "Using an unbiased prompt and zero temperature, all of the benchmarked LLMs exhibit accuracy improvement"
- Break condition: If the model has sufficient reasoning capability to ignore biased prompt instructions, or if the bias is not strong enough to influence behavior

## Foundational Learning

- **Concept: Temperature parameter in LLMs**
  - Why needed here: Temperature controls the randomness of token selection during generation, directly affecting the reliability of self-correction decisions
  - Quick check question: What happens to the model's output distribution when temperature approaches zero versus infinity?

- **Concept: Chain-of-thought prompting**
  - Why needed here: The paper draws parallels between self-correction and CoT, suggesting they both provide additional reasoning context
  - Quick check question: How does CoT prompting typically improve model performance on reasoning tasks?

- **Concept: Hallucination in LLMs**
  - Why needed here: The paper argues that hallucinations cause initial answers to be incorrect, necessitating self-correction
  - Quick check question: What are the different types of hallucination behaviors observed in LLMs?

## Architecture Onboarding

- **Component map**: Input question -> Initial response generation (zero temp) -> Self-correction prompt generation -> Revised response generation (zero temp) -> Final answer
- **Critical path**: 1. Receive input question with answer choices 2. Generate initial response using zero temperature 3. Generate self-correction prompt (must be fair/unbiased) 4. Generate revised response using zero temperature 5. Return final answer
- **Design tradeoffs**: Temperature vs creativity: Zero temperature ensures reliability but may reduce diversity; Prompt specificity vs bias: More specific prompts may introduce bias, less specific prompts may be less effective; Computation cost vs accuracy: Self-correction requires multiple generations, increasing latency
- **Failure signatures**: Accuracy decreases after self-correction (indicates biased prompts or inappropriate temperature); Inconsistent results across runs (suggests temperature not properly set to zero); Model ignoring self-correction instructions (indicates prompt format issues)
- **First 3 experiments**: 1. Test self-correction with temperature=0 vs temperature>0 on CommonSense QA dataset 2. Compare biased vs unbiased prompts for self-correction effectiveness 3. Validate that zero temperature + fair prompts improves accuracy across multiple model types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different temperature optimization strategies (e.g., dynamic temperature vs. fixed temperature) affect the self-correction performance across various model architectures?
- Basis in paper: [explicit] The paper shows that temperature affects self-correction, with non-zero temperature potentially degrading performance, but does not explore optimization strategies.
- Why unresolved: The paper only tests fixed temperature settings and does not investigate whether dynamic temperature adjustments could improve self-correction outcomes.
- What evidence would resolve it: Comparative experiments testing various temperature optimization strategies across different models and benchmarks, measuring self-correction accuracy improvements.

### Open Question 2
- Question: What specific linguistic or structural features in prompts cause the most significant bias in self-correction behavior?
- Basis in paper: [explicit] The paper identifies that biased prompts can lead to accuracy decreases and provides guidelines for unbiased prompts, but does not systematically analyze which features are most problematic.
- Why unresolved: The paper identifies the existence of bias but does not provide a detailed analysis of which specific prompt elements contribute most to biased behavior.
- What evidence would resolve it: Controlled experiments varying specific linguistic features in prompts while measuring their impact on self-correction accuracy and behavior.

### Open Question 3
- Question: How does the size of the model (parameter count) interact with temperature and prompt bias to affect self-correction performance?
- Basis in paper: [explicit] The paper conducts preliminary ablation studies on model size but notes that further investigation is needed for definitive conclusions.
- Why unresolved: The paper only provides limited results on model size effects and acknowledges that more comprehensive studies are required.
- What evidence would resolve it: Systematic experiments across a wide range of model sizes, testing various temperature settings and prompt types to identify interaction effects.

## Limitations
- The study focuses primarily on multiple-choice question answering tasks, limiting generalizability to other LLM applications
- The theoretical analysis assumes independent Bernoulli distributions for answer correctness, which may oversimplify complex LLM decision-making
- The comparison across only four model families limits generalizability across the broader LLM landscape
- The paper does not investigate computational overhead or latency impact of self-correction

## Confidence

**High Confidence**: The claim that zero temperature is essential for successful self-correction is well-supported by both theoretical analysis and empirical results across multiple models and benchmarks.

**Medium Confidence**: The assertion that fair prompts are universally necessary for self-correction effectiveness is supported by experimental evidence but lacks a comprehensive theoretical framework for prompt fairness evaluation.

**Medium Confidence**: The claim that self-correction functions similarly to chain-of-thought prompting is supported by conceptual arguments but lacks direct experimental validation comparing the two mechanisms.

**Low Confidence**: The broader claim that all LLMs have intrinsic self-correction abilities may be overstated, as the study only tested four model families and primarily focused on multiple-choice tasks.

## Next Checks

1. **Cross-task generalization**: Test self-correction performance on open-ended generation tasks and non-question-answering benchmarks to verify whether the identified mechanisms (zero temperature, fair prompts) generalize beyond multiple-choice scenarios.

2. **Prompt fairness quantification**: Develop and validate a quantitative metric for prompt fairness that can predict self-correction success rates, moving beyond the binary classification of "biased" vs "unbiased" used in the current study.

3. **Temperature sensitivity analysis**: Conduct a more comprehensive analysis of temperature effects across the full range (0.0 to 2.0+) and investigate whether different model families show fundamentally different temperature sensitivity patterns that could inform model-specific self-correction strategies.