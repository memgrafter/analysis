---
ver: rpa2
title: 'Polos: Multimodal Metric Learning from Human Feedback for Image Captioning'
arxiv_id: '2402.18091'
source_url: https://arxiv.org/abs/2402.18091
tags:
- image
- human
- metric
- captioning
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Polos, a supervised automatic evaluation
  metric for image captioning that addresses the limitations of existing metrics in
  handling hallucinations and generalizing across diverse images and texts. The core
  method, Polos, uses a parallel feature extraction mechanism that leverages embeddings
  from SimCSE and CLIP, trained within a novel framework called Multimodal Metric
  Learning from Human Feedback (M2LHF).
---

# Polos: Multimodal Metric Learning from Human Feedback for Image Captioning

## Quick Facts
- arXiv ID: 2402.18091
- Source URL: https://arxiv.org/abs/2402.18091
- Authors: Yuiga Wada; Kanta Kaneda; Daichi Saito; Komei Sugiura
- Reference count: 40
- Primary result: Polos achieves state-of-the-art performance on multiple image captioning evaluation benchmarks, demonstrating robustness in handling hallucinations and generalization across diverse images and texts.

## Executive Summary
This paper introduces Polos, a supervised automatic evaluation metric for image captioning that addresses key limitations in existing metrics. Polos uses a parallel feature extraction mechanism that combines embeddings from SimCSE and CLIP, trained within a novel Multimodal Metric Learning from Human Feedback (M2LHF) framework. The method directly regresses human evaluations based on multimodal inputs (candidate captions, reference captions, and images), combining similarity-based and learning-based approaches. To train Polos, the authors constructed the Polaris dataset containing 131K human judgments from 550 evaluators, significantly larger and more diverse than existing datasets. Polos achieved state-of-the-art performance across multiple benchmarks, demonstrating its effectiveness in evaluating image captioning models.

## Method Summary
Polos uses a parallel feature extraction mechanism that leverages embeddings from SimCSE and CLIP to compute evaluation scores. The model takes candidate captions, reference captions, and images as input, processing them through both CLIP and RoBERTa (pretrained with SimCSE) encoders. The parallel feature extraction computes concatenated vectors containing the original embeddings, their differences, and Hadamard products, capturing both absolute and relative feature relationships. These features are aggregated through an MLP and pooling mechanism to produce a single score between 0 and 1 that predicts human judgment. The model is trained within the M2LHF framework using mean squared error loss on the Polaris dataset, which contains 131K human judgments.

## Key Results
- Polos achieved state-of-the-art performance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and Polaris benchmarks
- Demonstrated robustness in handling hallucinations compared to existing metrics
- Showed strong generalization across diverse images and texts, validating the effectiveness of the parallel feature extraction mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel feature extraction using difference and Hadamard product of CLIP and SimCSE embeddings captures richer multimodal relationships than simple cosine similarity
- Mechanism: The model computes [c; r; |c-r|; c⊙r] for each text-text and text-image pair, where c and r are embeddings from CLIP and RoBERTa. These operations encode both absolute and relative feature relationships
- Core assumption: The Hadamard product and absolute difference preserve meaningful similarity information that cosine similarity alone misses
- Evidence anchors: [abstract] "parallel feature extraction mechanism that leverages embeddings trained through large-scale contrastive learning"; [section] "the difference and Hadamard product operations generate vectors that encapsulate similarity because each element of the vector can be amplified or attenuated in relation to the others"
- Break condition: If CLIP and SimCSE embeddings are not well-aligned or if the feature space dimensions are incompatible, the concatenated features may not encode useful information

### Mechanism 2
- Claim: Using both CLIP and RoBERTa pretrained with SimCSE provides complementary strengths for caption evaluation
- Mechanism: CLIP provides strong image-text alignment learned from web data, while SimCSE provides better sentence similarity for longer captions
- Core assumption: CLIP text embeddings are suboptimal for evaluating long captions because they were trained on short noun phrases, while SimCSE embeddings are better suited for this task
- Evidence anchors: [abstract] "sentence embeddings pretrained with supervised SimCSE [22] may be a more advantageous approach than CLIP"; [section] "CLIP is predominantly trained on a dataset that comprises mainly short noun phrases, which means that it is possibly suboptimal for evaluating the longer sentences typically generated by image captioning models"
- Break condition: If the SimCSE embeddings do not generalize well to image captioning domain or if CLIP embeddings perform adequately for longer captions, the dual-embedding approach provides no benefit

### Mechanism 3
- Claim: Direct regression on human judgments using multimodal inputs provides more accurate evaluation than ranking or unsupervised similarity approaches
- Mechanism: The model takes candidate and reference captions plus image features as input and directly predicts human judgment scores using an MLP aggregation
- Core assumption: Regression can better handle the continuous nature of caption quality and multiple reference variations than ranking approaches
- Evidence anchors: [abstract] "a framework for developing metrics based on human feedback" and "directly regresses the human evaluation"; [section] "we believe that regression models are better suited for evaluating image captioning models that involve multiple references because ranking models cannot adequately handle multiple potential references"
- Break condition: If the relationship between multimodal features and human judgments is too complex for a simple MLP to capture, or if ranking provides better discrimination between caption qualities

## Foundational Learning

- Concept: Contrastive learning for multimodal embeddings
  - Why needed here: CLIP and SimCSE both use contrastive learning to create meaningful embeddings that capture semantic similarity
  - Quick check question: What is the primary objective function used in contrastive learning for embedding models like CLIP?

- Concept: Feature engineering with vector operations
  - Why needed here: The difference and Hadamard product operations create new feature representations that capture both absolute and relative relationships between embeddings
  - Quick check question: How does the Hadamard product of two vectors differ from their element-wise multiplication in terms of information preservation?

- Concept: Regression vs ranking for evaluation metrics
  - Why needed here: Understanding when to use regression (continuous prediction) versus ranking (relative ordering) is crucial for designing effective evaluation metrics
  - Quick check question: What are the key differences in how regression and ranking approaches handle multiple reference captions?

## Architecture Onboarding

- Component map: Image → CLIP image encoder → v; Text → CLIP text encoder → cclip, rclip; Text → RoBERTa → crb, rrb; Feature extraction → MLP → Aggregation → Score

- Critical path: Candidate and reference captions plus image are encoded through both CLIP and SimCSE models, features are extracted through parallel mechanism computing [c; r; |c-r|; c⊙r], then passed through MLP and aggregated to produce final score

- Design tradeoffs: Using both CLIP and SimCSE adds computational overhead but provides complementary strengths; parallel feature extraction increases model capacity but may overfit with limited data

- Failure signatures: Poor performance on hallucination detection (overestimating captions with incorrect details); inability to handle fine-grained alignment (missing context in longer captions); sensitivity to domain shift (performing well on training data but poorly on unseen data)

- First 3 experiments:
  1. Ablation study: Remove Hadamard product and difference operations to measure their contribution to performance
  2. Embedding comparison: Replace SimCSE with other sentence embedding methods to validate the dual-embedding approach
  3. Aggregation analysis: Compare max vs mean aggregation functions to determine optimal reference handling

## Open Questions the Paper Calls Out

- Question: How does Polos handle captions with varying focal points compared to existing metrics?
  - Basis in paper: [explicit] The paper states that ranking models cannot adequately handle multiple potential references, such as varying areas of interest in captions and inherent subjectivity in expression
  - Why unresolved: The paper discusses the limitations of ranking models but does not provide a detailed analysis of how Polos specifically handles captions with varying focal points
  - What evidence would resolve it: A detailed comparison of Polos' performance on captions with varying focal points versus existing metrics would resolve this question

- Question: What are the specific limitations of CLIP's text embeddings in evaluating image captioning models?
  - Basis in paper: [explicit] The paper mentions that CLIP is predominantly trained on a dataset comprising mainly short noun phrases, which means that it is possibly suboptimal for evaluating the longer sentences typically generated by image captioning models
  - Why unresolved: The paper provides a general explanation of CLIP's limitations but does not delve into specific scenarios where these limitations become apparent
  - What evidence would resolve it: A comprehensive analysis of CLIP's performance on various types of image captioning tasks, highlighting specific scenarios where its limitations become evident, would resolve this question

- Question: How does the parallel feature extraction mechanism in Polos improve the evaluation of image captioning models compared to using CLIP or RoBERTa alone?
  - Basis in paper: [explicit] The paper introduces a parallel feature extraction mechanism that leverages both CLIP and RoBERTa, trained with SimCSE, to compute evaluation scores
  - Why unresolved: While the paper explains the mechanism, it does not provide a detailed comparison of its effectiveness against using CLIP or RoBERTa alone
  - What evidence would resolve it: A comparative study showing the performance of Polos against models using only CLIP or RoBERTa would resolve this question

## Limitations

- Lack of direct ablation studies and comparative experiments in the corpus to validate specific design choices
- No systematic evaluation of model performance on different types of hallucinations
- Theoretical justification for dual-embedding approach lacks empirical validation against using either embedding method alone

## Confidence

- High Confidence: The overall effectiveness of Polos on established benchmarks (Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and Polaris datasets) is well-supported by experimental results
- Medium Confidence: The mechanism of parallel feature extraction using difference and Hadamard product operations is theoretically sound but lacks direct comparative evidence
- Medium Confidence: The dual-embedding approach using CLIP and SimCSE is justified by theoretical considerations but requires empirical validation
- Medium Confidence: The regression approach for handling multiple references is claimed to be superior but lacks direct comparison with ranking methods

## Next Checks

1. Ablation Study on Feature Operations: Remove the Hadamard product and difference operations from the parallel feature extraction mechanism and measure the impact on performance across all benchmark datasets to quantify their contribution

2. Embedding Method Comparison: Replace SimCSE with other sentence embedding methods (e.g., SBERT, BERTScore) while keeping all other components constant to validate the specific advantage of the dual-embedding approach

3. Systematic Hallucination Analysis: Design targeted experiments to evaluate Polos's performance on different types of hallucinations (semantic, visual, factual) using synthetic data with controlled hallucination patterns