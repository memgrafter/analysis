---
ver: rpa2
title: Online Adaptation of Language Models with a Memory of Amortized Contexts
arxiv_id: '2403.04317'
source_url: https://arxiv.org/abs/2403.04317
tags:
- memory
- learning
- online
- documents
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Memory of Amortized Contexts (MAC), a framework
  for online adaptation of language models that efficiently incorporates new knowledge
  without gradient updates. MAC uses an amortization network to compress documents
  into compact modulations, stores them in a memory bank, and learns to aggregate
  relevant modulations via a conditioning input.
---

# Online Adaptation of Language Models with a Memory of Amortized Contexts

## Quick Facts
- **arXiv ID**: 2403.04317
- **Source URL**: https://arxiv.org/abs/2403.04317
- **Authors**: Jihoon Tack; Jaehyung Kim; Eric Mitchell; Jinwoo Shin; Yee Whye Teh; Jonathan Richard Schwarz
- **Reference count**: 40
- **Primary result**: MAC achieves 21.79 F1 score on StreamingQA, outperforming online finetuning baselines (18.97) while using frozen models and single forward passes.

## Executive Summary
This paper introduces Memory of Amortized Contexts (MAC), a framework for online adaptation of language models that efficiently incorporates new knowledge without gradient updates. MAC uses an amortization network to compress documents into compact modulations, stores them in a memory bank, and learns to aggregate relevant modulations via a conditioning input. This allows adaptation of frozen models through a single forward pass, avoiding catastrophic forgetting and expensive gradient computation. The approach is validated on multiple datasets (StreamingQA, SQuAD-Seq, ArchivalQA-Seq) with models ranging from DistilGPT2 to LLaMA-2, showing significant improvements over online finetuning baselines.

## Method Summary
MAC employs an amortization network (trained T5 encoder-decoder) to compress incoming documents into compact modulation parameters (P-tuning v2 tokens). These modulations are stored in a memory bank and retrieved during inference. An aggregation network (cross-attention blocks) selects and merges relevant modulations based on the current query, producing a single effective modulation. The base language model remains frozen and is adapted via this modulation during inference. Backpropagation dropout and hierarchical aggregation are introduced to handle large-scale data efficiently, allowing MAC to scale to large models and memory banks while maintaining performance.

## Key Results
- MAC achieves 21.79 F1 score on StreamingQA compared to 18.97 for online finetuning baselines
- Strong knowledge retention: 0.55 vs 0.11 for Uniform baseline on StreamingQA
- Faster adaptation times: 19.06s vs 119.56s for online finetuning with DistilGPT2 on StreamingQA
- Lower memory usage: 3.65MB vs 1.41GB for online finetuning with DistilGPT2 on StreamingQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Amortization-based meta-learning substitutes gradient updates with a single forward pass to generate modulation parameters from documents.
- Mechanism: An amortization network (trained T5 encoder-decoder) takes a document as input and directly outputs a compact modulation (P-tuning v2 tokens) that can adapt a frozen LM without gradient computation.
- Core assumption: The amortization network can generalize to unseen documents and produce informative modulations that capture document semantics in a compressed form.
- Evidence anchors:
  - [abstract] "we utilize amortization-based meta-learning, which substitutes an otherwise required optimization process with a single forward pass of the encoder"
  - [section 3.2] "we suggest meta-learning the amortization network to directly predict a compact modulation for a new document"
  - [corpus] Weak - no corpus neighbors discuss amortization meta-learning for language model adaptation.
- Break condition: Amortization network fails to produce meaningful modulations for novel documents, leading to poor adaptation performance.

### Mechanism 2
- Claim: Aggregation network learns to select and merge relevant modulations from memory bank based on the question, producing a single effective modulation.
- Mechanism: A cross-attention based aggregation network (with four blocks) takes the encoded question and the set of stored modulations, computes similarity scores, and outputs a single modulation through iterative subgrouping (hierarchical aggregation) to save memory.
- Core assumption: Relevant information from multiple documents can be effectively combined into one modulation that preserves knowledge for answering the question.
- Evidence anchors:
  - [abstract] "we learn to choose from and aggregate selected documents into a single modulation by conditioning on the question"
  - [section 3.2] "we aggregate the memory bank into a single modulation based on the given input"
  - [corpus] Weak - corpus has "memory bank compression" and "table lookup" but not cross-attention aggregation for document modulations.
- Break condition: Aggregation network fails to retrieve useful information, or hierarchical grouping loses important details.

### Mechanism 3
- Claim: Backpropagation dropout and hierarchical modulation aggregation make training and inference scalable to large models and large memory banks.
- Mechanism: Backpropagation dropout randomly drops amortization gradients during training to allow large batch sizes without memory overflow; hierarchical aggregation reduces memory complexity from O(KT^2) to O(MT) by recursively grouping modulations.
- Core assumption: Random sub-sampling yields unbiased gradients, and dividing memory bank into subgroups preserves performance while drastically reducing memory.
- Evidence anchors:
  - [section 3.3] "we perform amortization at training time with a stop-gradient operation, i.e., stopgrad[gθamort(di)] where p is a hyper-parameter"
  - [section 3.3] "we propose hierarchical modulation aggregation that uses a divide-and-conquer strategy"
  - [corpus] Weak - corpus has "memory bank compression" but not this specific dropout+grouping scheme.
- Break condition: Random dropout causes training instability, or hierarchical grouping introduces too much information loss.

## Foundational Learning

- Concept: Amortized optimization and meta-learning
  - Why needed here: MAC relies on predicting modulation parameters directly from documents without iterative gradient updates; this requires understanding how amortized networks can learn to generalize across tasks/distributions.
  - Quick check question: If the amortization network is trained on documents from distribution D, will it still produce good modulations for documents from a different distribution D'?

- Concept: Cross-attention and set aggregation
  - Why needed here: Aggregation network uses cross-attention to compare question embeddings with modulation embeddings and merge them; understanding permutation invariance and attention-based pooling is key to grasping how MAC selects relevant knowledge.
  - Quick check question: Why is permutation invariance important for the aggregation network when processing the memory bank?

- Concept: Catastrophic forgetting and knowledge retention
  - Why needed here: MAC aims to preserve previously learned knowledge while adapting to new documents; understanding how frozen LMs with external modulations avoid overwriting old parameters is crucial.
  - Quick check question: How does freezing the base LM and using modulations help mitigate catastrophic forgetting compared to online finetuning?

## Architecture Onboarding

- Component map:
  Input encoder θinput (T5-base) → encodes question into tokens
  Amortization network θamort (T5-small/base/large) → maps document → modulation (P-tuning v2 tokens)
  Aggregation network ψ (four cross-attention blocks) → maps (question, modulations) → single modulation
  Base LM θbase (frozen GPT2/LLaMA-2) → adapted via modulation during inference
  Memory bank M → stored modulations from seen documents for retrieval during online adaptation

- Critical path:
  1. Document → amortization network → modulation → store in memory bank
  2. Question → input encoder → aggregation network + memory bank → single modulation
  3. Question + modulation → base LM → answer

- Design tradeoffs:
  - Larger amortization network → better modulations but higher training cost
  - More cross-attention blocks in aggregation → better selection but more memory/time
  - Larger subgroup size M in hierarchical aggregation → faster inference but potential accuracy drop
  - More documents in memory bank → more knowledge but slower aggregation

- Failure signatures:
  - Amortization produces noisy/uninformative modulations → poor adaptation even with good aggregation
  - Aggregation fails to select relevant modulations → base LM receives irrelevant context
  - Backpropagation dropout too aggressive → training instability or slow convergence
  - Hierarchical grouping too coarse → loss of fine-grained document distinctions

- First 3 experiments:
  1. Train MAC on StreamingQA, measure F1 vs online finetuning baselines (Uniform, Salient Spans, CaMeLS)
  2. Vary backpropagation dropout ratio p and measure memory usage vs F1 retention
  3. Vary subgroup cardinality M in hierarchical aggregation, measure GPU memory vs F1 trade-off

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of MAC scale with increasing memory bank size constraints, and what are the optimal strategies for reducing memory bank size without significant loss of performance?
  - Basis in paper: [explicit] The paper discusses the growing size of the memory bank as the number of adapted documents increases and suggests strategies like random pruning, averaging modulations, and averaging nearest-neighbor modulations. However, it acknowledges this as a limitation and suggests exploring better-merging techniques.
  - Why unresolved: The paper only tested a few simple strategies for reducing memory bank size and found that averaging nearest-neighbor modulations is somewhat effective. It suggests that further investigation into better-merging techniques could be beneficial.
  - What evidence would resolve it: Systematic experiments comparing various memory reduction strategies, including more advanced neural compression techniques, would provide evidence on the optimal approach. This could involve testing different merging algorithms, quantization methods, or even exploring techniques like elastic weight consolidation.

- **Open Question 2**: How does MAC perform on tasks beyond question answering, such as language modeling or other sequence-to-sequence tasks?
  - Basis in paper: [explicit] The paper primarily focuses on question answering tasks and provides some preliminary results on language modeling. It suggests that MAC outperforms online finetuning baselines in language modeling tasks as well.
  - Why unresolved: While the paper shows promising results on language modeling, it does not extensively explore MAC's performance on a wider range of tasks. The effectiveness of MAC on tasks like summarization, machine translation, or text generation remains unexplored.
  - What evidence would resolve it: Extensive experiments on various NLP tasks beyond question answering would provide evidence of MAC's generalizability. This could involve testing MAC on established benchmarks for tasks like summarization (e.g., CNN/Daily Mail), machine translation (e.g., WMT), or text generation (e.g., WikiText).

- **Open Question 3**: How does the choice of PEFT modulation technique (e.g., LoRA vs. P-tuning v2) impact MAC's performance and efficiency?
  - Basis in paper: [explicit] The paper compares LoRA and P-tuning v2 as PEFT modulation techniques within MAC and finds that P-tuning v2 generally outperforms LoRA. It attributes this to P-tuning v2's efficient batch computation.
  - Why unresolved: While the paper provides a comparison between LoRA and P-tuning v2, it does not explore the full spectrum of PEFT techniques or analyze the reasons behind the performance differences in detail. The impact of other factors like the base model architecture or the task complexity on the choice of PEFT is also unclear.
  - What evidence would resolve it: A comprehensive study comparing MAC with various PEFT techniques (e.g., Prefix Tuning, Adapter-Bot) across different base models and tasks would provide insights into the optimal choice of PEFT. Additionally, analyzing the computational efficiency and memory usage of different PEFT techniques within MAC would be valuable.

## Limitations

- **Scalability across diverse document types**: While MAC shows strong performance on QA datasets, its effectiveness on more heterogeneous document collections (e.g., long technical papers, code, multimodal content) remains untested. The amortization network's ability to generalize to vastly different document structures and semantic domains could limit real-world applicability.

- **Dependency on frozen model quality**: MAC's success relies heavily on the frozen LM having sufficient base knowledge. If the underlying model lacks foundational understanding, modulations cannot compensate effectively. The framework assumes the base model is already competent at the task before adaptation.

- **Memory bank growth management**: Though MAC introduces memory-efficient training and inference strategies, the memory bank still grows linearly with the number of unique documents. Long-term deployment scenarios where millions of documents are processed may require additional compression or selective forgetting mechanisms.

## Confidence

- **High confidence**: Claims about MAC's ability to improve F1 scores compared to online finetuning baselines (e.g., 18.97→21.79 on StreamingQA) are supported by quantitative results across multiple datasets and model scales. The ablation studies demonstrating the importance of both amortization and aggregation networks are robust.

- **Medium confidence**: Claims about MAC's memory and time efficiency relative to online finetuning are supported by reported metrics, but comparisons would benefit from standardized benchmarking environments and consideration of end-to-end deployment costs.

- **Low confidence**: Claims about MAC's effectiveness when combined with retrieval augmentation methods (BM25, Contriever, DPR) are based on limited experiments and would require more systematic investigation across diverse retrieval backends and document distributions.

## Next Checks

1. **Cross-domain generalization test**: Evaluate MAC on a multi-domain corpus (e.g., combining scientific papers, news articles, and technical documentation) to assess whether the amortization network can produce effective modulations across vastly different document types and writing styles.

2. **Memory bank scaling analysis**: Conduct a systematic study of MAC's performance and memory usage as the memory bank grows from hundreds to millions of documents, measuring both adaptation accuracy and inference latency to identify practical limits.

3. **Base model knowledge threshold experiment**: Test MAC with base LMs of varying quality levels (e.g., GPT2-small vs GPT2-XL vs LLaMA-2) on the same document streams to quantify how much the framework depends on pre-existing model knowledge versus the ability to incorporate new information.