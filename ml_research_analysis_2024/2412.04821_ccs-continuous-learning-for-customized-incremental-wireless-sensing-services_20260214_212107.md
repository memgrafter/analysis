---
ver: rpa2
title: 'CCS: Continuous Learning for Customized Incremental Wireless Sensing Services'
arxiv_id: '2412.04821'
source_url: https://arxiv.org/abs/2412.04821
tags:
- service
- services
- learning
- sensing
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CCS, a continuous learning framework for
  incremental wireless sensing services that enables service providers to meet users'
  evolving demands without transmitting raw data, thus preserving privacy. The core
  idea is to update sensing models locally using knowledge distillation and weight
  alignment techniques, combined with exemplar selection via Herding to mitigate catastrophic
  forgetting.
---

# CCS: Continuous Learning for Customized Incremental Wireless Sensing Services

## Quick Facts
- **arXiv ID**: 2412.04821
- **Source URL**: https://arxiv.org/abs/2412.04821
- **Reference count**: 35
- **Primary result**: CCS framework achieves up to 34.01% accuracy on RFID and 65.10% on Wi-Fi in final incremental stages, outperforming alternatives like iCaRL, UCIR, BiC, and OneFi.

## Executive Summary
CCS introduces a continuous learning framework for incremental wireless sensing services that enables service providers to update models on users' devices without transmitting raw data, preserving privacy. The framework combines knowledge distillation, weight alignment, and exemplar selection via Herding to address catastrophic forgetting while learning new action categories over time. Extensive experiments on the XRF55 dataset across Wi-Fi, mmWave radar, and RFID modalities demonstrate CCS's superiority over existing methods in both accuracy and a novel ACCN metric that balances model capacity and accuracy.

## Method Summary
CCS is a continuous learning framework for incremental wireless sensing that updates models on user devices without transmitting raw data. The method combines three key techniques: (1) exemplar selection via Herding to provide representative samples of old tasks, (2) knowledge distillation to preserve old model capabilities during incremental updates, and (3) weight alignment to balance model capacity between old and new tasks. The framework operates by initializing updated models from previous stage models, which serve as frozen teacher networks, then training with new user data plus selected exemplars while applying MSE loss for distillation and L2 normalization for weight balancing.

## Key Results
- CCS achieves up to 34.01% accuracy on RFID and 65.10% on Wi-Fi in the final stage, significantly higher than alternative methods
- The framework maintains steadily increasing model value over sequential incremental stages
- ACCN metric shows CCS balances model capacity and accuracy better than competitors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation preserves old model capabilities during incremental updates.
- Mechanism: The updated model (Mi) is initialized from the previous model (Mi-1), which is frozen as a teacher network. Mi learns new tasks from user data while mimicking Mi-1's behavior using MSE loss.
- Core assumption: Mi-1 contains sufficient distilled knowledge of old tasks to guide the student model without direct access to old training data.
- Evidence anchors:
  - [abstract]: "we design knowledge distillation and weight alignment modules. These modules enable the sensing model to acquire new capabilities while retaining the existing ones."
  - [section III-C]: "knowledge distillation involves training a student model Mi to mimic and learn from a teacher model Mi-1, which has already acquired knowledge of old knowledge."
- Break condition: If Mi-1's knowledge becomes stale or if the new tasks are too dissimilar from old tasks, distillation may fail to preserve capabilities.

### Mechanism 2
- Claim: Weight aligning balances model capacity between old and new tasks.
- Mechanism: After updating model weights, the weights for new task predictions are normalized relative to the mean norm of old task weights, preventing new tasks from dominating the model.
- Core assumption: The relative scale of weight norms correlates with the model's propensity toward different task categories.
- Evidence anchors:
  - [section III-D]: "CCS applies weight aligning techniques [15] to balance the model's propensity towards the new and old services."
  - [section IV-D]: "Using knowledge distillation, weight aligning and exemplar at the same time can reap better results than using them separately."
- Break condition: If the normalization factor is poorly chosen, it may either suppress new task learning or fail to prevent forgetting.

### Mechanism 3
- Claim: Exemplar selection via Herding mitigates catastrophic forgetting by providing representative samples of old tasks.
- Mechanism: For each class, K data samples closest to the class feature center are selected as exemplars, ensuring compact yet representative coverage of old task space.
- Core assumption: K-nearest samples to the feature center adequately represent the distribution of each class.
- Evidence anchors:
  - [section III-B]: "CCS leverages Herding techniques [14] to select exemplars, from the provider's data repository, and deliver the exemplars to users."
  - [section IV-A]: "we employ the Herding [14] method to select exemplars from the preceding stage, at a ratio of one sample per person per action."
- Break condition: If K is too small, exemplars may not capture task diversity; if too large, storage/computation burden increases.

## Foundational Learning

- Concept: Continuous Learning (Class-Incremental Learning)
  - Why needed here: CCS must learn new action categories over time without retraining from scratch, preserving privacy by not transmitting raw data.
  - Quick check question: What problem does CCS solve compared to standard supervised learning?

- Concept: Knowledge Distillation
  - Why needed here: Enables the model to learn new capabilities while retaining old ones by mimicking a frozen teacher model.
  - Quick check question: How does knowledge distillation differ from standard fine-tuning?

- Concept: Exemplar Selection (Herding)
  - Why needed here: Provides representative samples of old tasks to the model during incremental updates without requiring full dataset access.
  - Quick check question: Why might random exemplar selection be less effective than Herding?

## Architecture Onboarding

- Component map:
  - Base model distribution → User devices
  - User data collection → Local storage
  - Exemplar selection (Herding) → Provider side
  - Knowledge distillation module → Model update process
  - Weight alignment module → Post-update normalization
  - Evaluation metrics (Accuracy, ACCN) → Performance tracking

- Critical path: Base model → User data acquisition → Exemplar selection → Knowledge distillation + Weight alignment → Updated model distribution

- Design tradeoffs:
  - K (exemplar count): Smaller K reduces storage/computation but may lose class diversity; larger K improves representation but increases overhead.
  - α (distillation balance): Higher α emphasizes old task preservation but may slow new task learning; lower α speeds new learning but risks forgetting.
  - Model capacity: Larger models can better separate old/new tasks but increase update cost.

- Failure signatures:
  - Rapid accuracy drop on old tasks indicates forgetting dominance.
  - Stagnant accuracy on new tasks suggests distillation/weight alignment over-constraining learning.
  - High variance across modalities indicates modality-specific architecture issues.

- First 3 experiments:
  1. Run CCS with K=1, α=0.1 on Wi-Fi modality for User1 sequence; verify accuracy > 65% on final stage.
  2. Disable weight alignment (set to baseline) and compare ACCN degradation rate.
  3. Replace Herding with random exemplar selection and measure impact on accuracy retention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of CCS compare to other incremental learning methods when applied to real-world wireless sensing scenarios with noisy data and dynamic user demands?
- Basis in paper: [explicit] The paper demonstrates CCS's performance on the XRF55 dataset, showing superior accuracy compared to methods like iCaRL, UCIR, BiC, and OneFi. However, the evaluation is conducted in a controlled environment, and the paper does not address the impact of real-world factors such as noise and dynamic user demands.
- Why unresolved: The paper focuses on controlled experiments with a specific dataset and does not provide insights into how CCS would perform under real-world conditions with noise and dynamic demands.
- What evidence would resolve it: Conducting experiments with real-world wireless sensing data, including scenarios with noise and varying user demands, would provide evidence of CCS's robustness and adaptability in practical settings.

### Open Question 2
- Question: What is the impact of the number of exemplars selected on the performance of CCS, and how does this affect the balance between model accuracy and computational efficiency?
- Basis in paper: [explicit] The paper mentions that CCS uses the Herding method to select exemplars, with a ratio of one sample per person per action. However, it does not explore how varying the number of exemplars affects model performance or computational efficiency.
- Why unresolved: The paper provides a fixed exemplar selection strategy but does not investigate the trade-offs between accuracy, memory usage, and computational efficiency when adjusting the number of exemplars.
- What evidence would resolve it: Conducting experiments with different numbers of exemplars and analyzing the resulting trade-offs in accuracy, memory usage, and computational efficiency would provide insights into the optimal exemplar selection strategy for CCS.

### Open Question 3
- Question: How does CCS handle catastrophic forgetting in scenarios where new user demands are significantly different from previous tasks, and what are the limitations of the current knowledge distillation and weight alignment techniques?
- Basis in paper: [explicit] The paper describes CCS's use of knowledge distillation and weight alignment to mitigate catastrophic forgetting. However, it does not explicitly address scenarios where new tasks are vastly different from previous ones, nor does it discuss the limitations of these techniques.
- Why unresolved: The paper focuses on CCS's ability to handle incremental learning but does not explore its performance in extreme cases where new tasks are significantly different from previous ones, nor does it discuss potential limitations of the current techniques.
- What evidence would resolve it: Conducting experiments with tasks that are significantly different from previous ones and analyzing the effectiveness of knowledge distillation and weight alignment in these scenarios would provide insights into CCS's limitations and potential areas for improvement.

## Limitations
- Specific network architectures for ResNets and Temporal UNet are not detailed, affecting reproducibility
- Implementation details of Herding exemplar selection algorithm are unspecified
- Scalability to larger datasets with more classes and users remains untested
- Performance under resource-constrained devices not evaluated

## Confidence
- **High Confidence**: The core mechanism of combining knowledge distillation, weight alignment, and exemplar selection to address catastrophic forgetting is well-supported by extensive experiments across three wireless sensing modalities.
- **Medium Confidence**: The ACCN metric's effectiveness in balancing model capacity and accuracy is demonstrated, but its generalizability to other domains requires further validation.
- **Low Confidence**: The scalability claims for CCS in real-world scenarios with hundreds of users and classes are based on theoretical reasoning rather than empirical evidence.

## Next Checks
1. **Scalability Test**: Implement CCS on a larger dataset with more classes and users to validate its scalability claims and identify potential bottlenecks in model update and distribution processes.
2. **Resource-Constrained Deployment**: Evaluate CCS's performance on resource-constrained devices to assess its practical viability for edge computing applications.
3. **Cross-Domain Generalization**: Test CCS on datasets from different domains (e.g., healthcare, smart homes) to verify the generalizability of the ACCN metric and the framework's adaptability to diverse sensing modalities.