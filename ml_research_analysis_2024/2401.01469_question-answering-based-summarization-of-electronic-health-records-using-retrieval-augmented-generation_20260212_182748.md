---
ver: rpa2
title: Question-Answering Based Summarization of Electronic Health Records using Retrieval
  Augmented Generation
arxiv_id: '2401.01469'
source_url: https://arxiv.org/abs/2401.01469
tags:
- summarization
- questions
- data
- answers
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of summarizing electronic health
  records (EHRs) to reduce clinicians' screen time spent on documentation. The authors
  propose a novel method combining semantic search, retrieval augmented generation
  (RAG), and question-answering using large language models (LLMs).
---

# Question-Answering Based Summarization of Electronic Health Records using Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2401.01469
- Source URL: https://arxiv.org/abs/2401.01469
- Authors: Walid Saba; Suzanne Wendelken; James. Shanahan
- Reference count: 11
- One-line primary result: Novel RAG-based approach for EHR summarization using question-answering with promising initial results

## Executive Summary
This paper addresses the challenge of summarizing electronic health records (EHRs) to reduce clinicians' screen time spent on documentation. The authors propose a novel method combining semantic search, retrieval augmented generation (RAG), and question-answering using large language models (LLMs). Their approach segments EHR data into paragraphs, indexes them in a vector database, and then retrieves relevant text snippets to answer predefined questions deemed important by subject-matter experts. The LLM generates answers along with confidence scores, which are combined with semantic similarity scores to select the best sentences for the summary. The method is efficient, requires minimal training, avoids LLM hallucination issues, and ensures diverse content.

## Method Summary
The proposed method segments EHR data into paragraphs and indexes them in a vector database using embeddings. For each predefined question from subject-matter experts, semantic search retrieves the top-k relevant paragraphs. The LLM generates answers along with confidence scores, which are combined with semantic similarity scores (cosine similarity between question and answer embeddings) to rank and select the best sentences for the summary. The approach uses a zero-shot question-answering paradigm, eliminating the need for annotated training data. Linguistic post-processing resolves references and translates acronyms to produce the final summary.

## Key Results
- Novel RAG-based approach combining semantic search, LLM generation, and question-answering for EHR summarization
- Method requires minimal training, avoids LLM hallucination issues, and ensures diverse content
- Initial experiments show promising results with plans for large-scale evaluation on MIMIC-III dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG with semantic search reduces LLM context length requirements by only retrieving relevant paragraphs.
- Mechanism: EHR paragraphs are embedded and stored in a vector database. For each SME-defined question, semantic search retrieves the top-k relevant paragraphs, forming a focused context for the LLM.
- Core assumption: Relevant EHR content for answering a question is contained in a small subset of paragraphs, and semantic search can reliably retrieve them.
- Evidence anchors:
  - [abstract] "The requirement to consider the entire content of an EHR in summarization has resulted in poor performance due to the fact that attention mechanisms in modern large language models (LLMs) adds a quadratic complexity in terms of the size of the input."
  - [section] "In our approach summarization is the extraction of answers to specific questions that are deemed important by subject-matter experts (SMEs)."
  - [corpus] Weak evidence - similar RAG papers focus on general retrieval, not specifically on reducing context length for EHRs.
- Break condition: If the relevant paragraphs for a question are not retrieved, or if the LLM requires broader context than a few paragraphs provide, the approach will fail.

### Mechanism 2
- Claim: Combining LLM confidence scores with semantic similarity scores improves answer quality.
- Mechanism: For each retrieved answer, the LLM provides a confidence score, and cosine similarity between question and answer embeddings is computed. These two scores are combined to rank answers.
- Core assumption: LLM confidence scores and semantic similarity are independent indicators of answer quality, and their combination is more reliable than either alone.
- Evidence anchors:
  - [section] "The LLM will return an answer as well as the sentence where the answer was found, along with a confidence score about the answer. At the same time, the embeddings of noun phrases extracted from both question and answers are also matched (using cosine similarity)."
  - [corpus] Weak evidence - no corpus papers explicitly describe this dual-score approach for EHR summarization.
- Break condition: If either the LLM confidence scores or the semantic similarity scores are unreliable, or if their combination is not calibrated, answer selection will be poor.

### Mechanism 3
- Claim: Zero-shot question-answering avoids the need for annotated training data.
- Mechanism: Pre-defined questions from SMEs guide the summarization process, eliminating the need to train a model on annotated summaries.
- Core assumption: SMEs can define a comprehensive set of questions that, when answered, constitute a complete summary of the EHR.
- Evidence anchors:
  - [abstract] "Our approach is quite efficient; requires minimal to no training; does not suffer from the 'hallucination' problem of LLMs."
  - [section] "In our approach summarization is the extraction of answers to specific questions that are deemed important by subject-matter experts (SMEs)."
  - [corpus] Weak evidence - similar papers focus on RAG approaches, not specifically on zero-shot question-answering.
- Break condition: If the pre-defined questions are incomplete or miss critical information, the summary will be inadequate.

## Foundational Learning

- Concept: Semantic search and vector embeddings
  - Why needed here: To retrieve relevant EHR paragraphs based on semantic similarity to questions, rather than exact keyword matching.
  - Quick check question: How does cosine similarity between embeddings determine semantic relevance?
- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: To combine information retrieval with LLM generation, grounding the LLM's output in relevant EHR content.
  - Quick check question: What are the benefits of RAG over using an LLM alone for summarization?
- Concept: Large Language Model confidence scores
  - Why needed here: To assess the reliability of LLM-generated answers and combine them with semantic similarity scores.
  - Quick check question: How are LLM confidence scores typically generated, and what do they represent?

## Architecture Onboarding

- Component map: EHR data → Paragraph segmentation → Vector embedding → Vector database → Semantic search → Context formation → LLM → Answer + confidence → Semantic similarity → Score combination → Answer selection → Linguistic post-processing → Summary
- Critical path: EHR data → Semantic search → Context formation → LLM → Answer selection → Summary
- Design tradeoffs: Precision vs. recall in semantic search (retrieving more paragraphs increases recall but may introduce noise), LLM context length vs. answer quality (longer context may improve quality but increases cost and complexity).
- Failure signatures: Missing critical information in the summary, irrelevant or incorrect answers, excessive repetition, or incomplete coverage of SME-defined questions.
- First 3 experiments:
  1. Test semantic search with a small set of EHR paragraphs and questions to evaluate retrieval quality.
  2. Evaluate LLM answer quality with retrieved context vs. full EHR context to confirm context length reduction.
  3. Combine LLM confidence scores and semantic similarity scores to rank answers and assess their combined effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the proposed question-answering based summarization method perform compared to existing summarization approaches on large-scale EHR datasets?
- Basis in paper: [explicit] The paper states "Our initial experiments are very promising and we are currently in the process of performing an evaluation on a large dataset" and mentions plans to evaluate on the MIMIC-III dataset.
- Why unresolved: The paper reports only initial promising results but does not provide comprehensive evaluation metrics (ROUGE, BLEU scores) comparing the proposed method to existing approaches on large-scale EHR data.
- What evidence would resolve it: Quantitative evaluation results comparing ROUGE, BLEU, and other relevant metrics between the proposed method and state-of-the-art summarization approaches on the full MIMIC-III dataset or other large EHR datasets.

### Open Question 2
- Question: How does the quality and diversity of summaries vary when using different subject-matter experts to define the relevant questions for summarization?
- Basis in paper: [explicit] The paper states "In our approach summarization is the extraction of answers to specific questions that are deemed important by subject-matter experts (SMEs)" and mentions using three clinician SMEs for annotation.
- Why unresolved: The paper does not discuss how different SMEs might define different sets of relevant questions, which could lead to varying summary quality and diversity, nor does it provide any analysis of inter-rater agreement among SMEs.
- What evidence would resolve it: Comparative analysis of summary quality and diversity when using different sets of questions defined by multiple SMEs, including measures of inter-rater agreement and impact on final summary content.

### Open Question 3
- Question: What is the optimal combination of semantic similarity scores and LLM confidence scores for selecting sentences in the final summary?
- Basis in paper: [inferred] The paper mentions using "a weighted average score" combining embeddings of noun phrases (semantic similarity) and LLM confidence scores, but does not specify how this weighting is determined.
- Why unresolved: The paper states that a weighted average is used but does not provide details on how the weights are determined or optimized, nor does it explore the impact of different weighting schemes on summary quality.
- What evidence would resolve it: Analysis showing how different weight combinations between semantic similarity and LLM confidence scores affect summary quality metrics (ROUGE, BLEU) and user satisfaction ratings.

## Limitations
- Evaluation relies primarily on subjective SME assessment without systematic quantitative metrics for summary quality
- Method's effectiveness on complex EHRs with extensive longitudinal data remains untested
- Approach depends heavily on quality and comprehensiveness of SME-defined questions

## Confidence
- **High Confidence:** The core RAG architecture combining semantic search with LLM generation is technically sound and well-established in the literature.
- **Medium Confidence:** The claim that combining LLM confidence scores with semantic similarity improves answer selection is plausible but not rigorously validated.
- **Low Confidence:** Claims about efficiency gains and the method's superiority over alternatives are not supported by quantitative evidence.

## Next Checks
1. Conduct systematic comparison of generated summaries against gold-standard summaries using established metrics (ROUGE, BLEU) and semantic similarity measures on a large annotated dataset.
2. Measure semantic search precision and recall by evaluating whether relevant EHR paragraphs are retrieved for each SME question, and assess the impact of retrieval quality on final summary quality.
3. Test the approach on EHRs with varying complexity, length, and medical conditions to assess robustness and identify failure modes not apparent in initial experiments.