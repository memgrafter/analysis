---
ver: rpa2
title: 'Translating Across Cultures: LLMs for Intralingual Cultural Adaptation'
arxiv_id: '2406.14504'
source_url: https://arxiv.org/abs/2406.14504
tags:
- adaptation
- cultural
- culture
- adapted
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper defines the task of intralingual cultural adaptation
  and creates an evaluation framework to assess modern LLMs for this task. The authors
  manually annotate culture-specific items in a Friends dialog corpus and evaluate
  adaptations from Llama-2-70B, Llama-3-8B, and Llama-3-70B models.
---

# Translating Across Cultures: LLMs for Intralingual Cultural Adaptation

## Quick Facts
- arXiv ID: 2406.14504
- Source URL: https://arxiv.org/abs/2406.14504
- Authors: Pushpdeep Singh; Mayur Patidar; Lovekesh Vig
- Reference count: 40
- Defines intralingual cultural adaptation task and creates evaluation framework for LLMs adapting US TV dialogs to Indian culture

## Executive Summary
This paper introduces the task of intralingual cultural adaptation, where language remains unchanged but cultural references are modified to make content more relatable to a different cultural audience. The authors create a framework to evaluate modern LLMs (Llama-2-70B, Llama-3-8B, Llama-3-70B) on adapting dialogs from the American TV show Friends to Indian cultural context. Through manual annotation of culture-specific items (CSI) and comprehensive evaluation at both edit and dialog levels, the study demonstrates that LLMs can effectively perform cultural adaptation while maintaining content integrity and naturalness.

## Method Summary
The authors manually annotate culture-specific items in a Friends dialog corpus, categorizing them by type (Ecology, Material Culture, Social Culture, etc.) and foreignness level to Indian culture. They then use prompt-based adaptation with three Llama models to modify dialogs for Indian cultural context. Evaluation involves edit-level analysis measuring CSI editing percentage, correctness, localization, and offensiveness, alongside dialog-level assessment of localization, naturalness, content preservation, stereotypical behavior, and offensiveness on 1-5 scales. Human evaluation validates LLM-based scoring reliability.

## Key Results
- Llama-3-8B and Llama-3-70B achieve 82.8% and 79.7% CSI editing respectively
- Llama-3-70B achieves highest localization score (4.44/5) with no offensive content
- Strong correlation (Kendall's Ï„ 0.60-1.00) between human evaluation and LLM-based scoring across all aspects
- Llama-2-70B performs best in naturalness (4.32/5) and content preservation (4.56/5)

## Why This Works (Mechanism)
None

## Foundational Learning
- Culture-specific items identification: Understanding how to categorize and label cultural references by type and foreignness level to create ground truth for evaluation
- Why needed: Essential for measuring adaptation effectiveness and determining which cultural elements require modification
- Quick check: Review annotated CSI dataset to verify categorization consistency and foreignness level assignments

- Edit-level vs dialog-level analysis: Recognizing the distinction between measuring individual changes (CSI editing percentage) versus holistic dialog quality (localization, naturalness, content preservation)
- Why needed: Provides comprehensive evaluation covering both technical accuracy and overall user experience
- Quick check: Verify correlation patterns between edit-level and dialog-level metrics across multiple dialogs

- LLM-based evaluation methodology: Understanding how automated scoring with models like Mixtral can approximate human judgment in cultural adaptation tasks
- Why needed: Enables scalable evaluation of cultural adaptation quality across large datasets
- Quick check: Compare LLM-based scores with human ratings on a validation set to establish reliability thresholds

## Architecture Onboarding
- Component map: Friends corpus -> Manual CSI annotation -> Prompt-based adaptation (LLama models) -> Edit-level analysis (% CSI edited, correctness, localization, offensiveness) -> Dialog-level analysis (localization, naturalness, content preservation, stereotypical behavior, offensiveness) -> Human evaluation validation
- Critical path: Manual annotation -> Adaptation -> Edit-level analysis -> Dialog-level analysis -> Human validation
- Design tradeoffs: Prompt-based adaptation vs fine-tuning (simplicity vs potentially better performance), human evaluation vs LLM-based scoring (accuracy vs scalability)
- Failure signatures: Incorrect cultural substitutions (e.g., "spell it out with naan" instead of "noodles"), over-localization causing loss of naturalness, offensive content generation
- First experiments: 1) Test fuzzy string matching parameters on sample dialogs to optimize CSI detection accuracy, 2) Run ablation study comparing different prompting strategies, 3) Evaluate adaptation on a small validation set with both human and LLM scoring

## Open Questions the Paper Calls Out
- How does LLM performance vary for cultures with less representation in training data or more distant cultural contexts?
- How does adaptation quality change with different prompting strategies or more sophisticated prompt engineering techniques?
- How do LLMs compare to specialized translation models on cultural adaptation tasks?

## Limitations
- Manual annotation process relies on subjective judgments about foreignness to Indian culture, potentially introducing inter-annotator variability
- Edit-level analysis depends on fuzzy string matching with unspecified implementation details, creating uncertainty about CSI editing metrics reliability
- Prompt-based adaptation may limit deeper cultural reasoning compared to fine-tuning approaches

## Confidence
- Edit-level performance metrics (82.8% and 79.7% CSI editing): High
- Dialog-level evaluation results (localization, naturalness, content preservation scores): Medium
- Correlation between human and LLM-based evaluation: High
- Model comparison conclusions: Medium

## Next Checks
1. Conduct inter-annotator agreement analysis on a subset of CSI annotations to quantify annotation reliability and characterize potential biases in foreignness level assignments
2. Implement and compare multiple fuzzy matching strategies for edit-level analysis to assess sensitivity of % CSI edited metrics to implementation choices
3. Test alternative prompting strategies or fine-tuning approaches to evaluate whether performance improvements are due to model capability versus prompt engineering effectiveness