---
ver: rpa2
title: Towards Boosting LLMs-driven Relevance Modeling with Progressive Retrieved
  Behavior-augmented Prompting
arxiv_id: '2408.09439'
source_url: https://arxiv.org/abs/2408.09439
tags:
- relevance
- search
- llms
- knowledge
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving relevance modeling
  in search engines using Large Language Models (LLMs). Traditional approaches relying
  solely on semantic congruence between queries and items are insufficient.
---

# Towards Boosting LLMs-driven Relevance Modeling with Progressive Retrieved Behavior-augmented Prompting

## Quick Facts
- arXiv ID: 2408.09439
- Source URL: https://arxiv.org/abs/2408.09439
- Reference count: 7
- Improves LLM-driven relevance modeling in search engines using user interaction data

## Executive Summary
This paper addresses the challenge of improving relevance modeling in search engines using Large Language Models (LLMs). Traditional approaches relying solely on semantic congruence between queries and items are insufficient for complex search scenarios. The authors propose a novel framework called ProRBP (Progressive Retrieved Behavior-augmented Prompting) that leverages user interactions from search logs to augment LLMs. ProRBP addresses the absence of domain-specific knowledge and the inadequacy of isolated prompts by retrieving user-driven behavior neighbors and employing progressive prompting and aggregation techniques.

## Method Summary
ProRBP employs a multi-stage approach to relevance modeling. First, it retrieves high-confidence query-item pairs from daily search logs based on click-through rates to obtain domain-specific knowledge. These user-driven behavior neighbors are then incorporated into a progressive prompting framework that decomposes the complex relevance judgment task into a sequence of simpler sub-tasks. The system uses least-to-most prompts to guide LLMs through increasingly specific relevance aspects, followed by progressive aggregation with an exponential kernel to combine sub-task outputs into a final relevance score. The framework is evaluated through both offline experiments and online A/B testing on the Alipay search platform.

## Key Results
- ProRBP demonstrates superior performance compared to existing methods on real-world industry data
- Online A/B testing validates the effectiveness of ProRBP in improving search relevance
- The framework achieves promising results, reducing the rate of irrelevant results and improving valid PV-CTR in the Alipay search platform

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive prompting enables LLMs to focus on increasingly specific relevance aspects by structuring the reasoning process from broad to narrow.
- Mechanism: The method decomposes a single complex relevance judgment into a sequence of simpler sub-tasks (neighbor-neighbor relevance, attribute-attribute consistency, then full query-item judgment), each building on prior context.
- Core assumption: LLMs can effectively transfer reasoning from earlier, simpler sub-prompts to later, more complex ones when structured in least-to-most order.
- Evidence anchors:
  - [abstract] "we guide LLMs for relevance modeling by employing advanced prompting techniques that progressively improve the outputs of the LLMs"
  - [section] "least-to-most prompts step by step... LLMs could reason the likelihood of the verbalizer exploiting the least-to-most prompts step by step for the sensitivity to relevance judgement"
  - [corpus] Weak - neighboring papers focus on retrieval-augmented generation or relevance judgment but don't discuss progressive prompting specifically
- Break condition: If the LLM fails to establish meaningful connections between sub-tasks, the progressive structure collapses and performance degrades to baseline isolated prompting.

### Mechanism 2
- Claim: User-driven behavior neighbor retrieval provides domain-specific, temporally relevant context that general LLMs lack for industrial search scenarios.
- Mechanism: The system retrieves high-confidence query-item pairs from recent search logs based on click-through rates, filtering out noise and capturing current user expectations.
- Core assumption: High click-through rate pairs in recent logs accurately represent user intent and domain knowledge, even for short/ambiguous queries.
- Evidence anchors:
  - [abstract] "we perform the user-driven behavior neighbors retrieval from the daily search logs to obtain domain-specific knowledge in time"
  - [section] "a higher click-through rate reflects that users believe the corresponding query-item pairs can better meet their search intents"
  - [corpus] Weak - neighboring papers discuss behavior graphs and click data but not the specific daily high-confidence filtering approach
- Break condition: If user behavior patterns shift rapidly or the filtering thresholds are too restrictive, the neighbor set becomes stale or too small to be useful.

### Mechanism 3
- Claim: Progressive aggregation with an exponential kernel properly weights earlier sub-task outputs to produce a final relevance score that balances multiple perspectives.
- Mechanism: The system combines sub-task probabilities using a learnable exponential decay function, giving more weight to later, more specific judgments while still incorporating earlier contextual signals.
- Core assumption: The relevance judgment task exhibits an incremental structure where earlier sub-tasks provide foundational context that becomes increasingly important in later judgments.
- Evidence anchors:
  - [abstract] "followed by a progressive aggregation with comprehensive consideration of diverse aspects"
  - [section] "we adopt the kernel function K to model the incremental tendency... The overall relevance score could be acquired through aggregating the probabilities from the least-to-most sub-tasks progressively"
  - [corpus] Weak - neighboring papers don't discuss progressive aggregation with kernel weighting specifically
- Break condition: If the kernel parameters are poorly tuned or the sub-task dependencies are non-linear, the aggregation may overweight noise or underweight critical signals.

## Foundational Learning

- Concept: Click-through rate (CTR) as a proxy for relevance
  - Why needed here: The paper uses CTR thresholds to filter high-confidence behavior neighbors from search logs
  - Quick check question: If a query-item pair has 200 exposures and 40 clicks, what is its CTR, and would it pass a 0.2 threshold?

- Concept: In-context learning with retrieved examples
  - Why needed here: The behavior neighbors serve as contextual examples for the LLM to understand domain-specific relevance
  - Quick check question: How does providing relevant examples in the prompt context help LLMs make better predictions compared to isolated prompts?

- Concept: Kernel-based weighted aggregation
  - Why needed here: The paper uses an exponential kernel to progressively weight sub-task outputs in the final relevance score
  - Quick check question: What happens to the weight of earlier sub-tasks as the kernel parameter λ increases?

## Architecture Onboarding

- Component map:
  User-driven behavior neighbor retrieval -> Progressive prompting engine -> LLM inference service -> Progressive aggregation module -> Industrial serving framework

- Critical path: Query/item → neighbor retrieval → prompt construction → LLM inference (progressive) → aggregation → relevance score
- Design tradeoffs:
  - Neighbor count vs. noise: More neighbors provide better context but risk introducing irrelevant signals
  - Prompt granularity vs. efficiency: More sub-tasks improve accuracy but increase LLM calls and latency
  - Kernel complexity vs. stability: Complex kernels can capture nuanced relationships but may overfit or be unstable

- Failure signatures:
  - Low AUC/F1 with high FNR: Progressive prompting failing to capture relevant aspects
  - Inconsistent scores for same query-item pair: Database serving not properly caching results
  - Sudden performance drop: Behavior neighbor filtering thresholds too aggressive or CTR distribution shift

- First 3 experiments:
  1. Test progressive prompting ablation: Run with only the final prompt versus full least-to-most sequence on validation set
  2. Test neighbor retrieval impact: Compare performance with different neighbor counts (0, 5, 10, 20, 30) and filtering thresholds
  3. Test aggregation function sensitivity: Compare exponential kernel versus mean pooling and Gaussian kernel with various parameter settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ProRBP scale with the size of the user behavior log dataset? Specifically, what is the relationship between the amount of historical search data and the relevance judgment accuracy?
- Basis in paper: [inferred] The paper mentions using daily updated search logs but does not analyze how the quantity of historical data affects performance.
- Why unresolved: The paper only mentions using a fixed number of top-K neighbors without exploring how varying the size of the historical dataset impacts results.
- What evidence would resolve it: Experiments showing ProRBP performance with different sizes of historical search log datasets, ideally including a plot of accuracy vs. dataset size.

### Open Question 2
- Question: What is the impact of different kernel functions (Gaussian, exponential, logarithmic) on the progressive aggregation module's performance, and why does the exponential kernel function perform best?
- Basis in paper: [explicit] The paper mentions trying different kernel functions but only shows results for the exponential kernel.
- Why unresolved: The paper states the exponential kernel was chosen after experimental comparison but doesn't show the comparative results or explain the underlying reasons for the choice.
- What evidence would resolve it: Comparative results showing performance metrics for all tested kernel functions, along with analysis of why the exponential kernel outperforms others.

### Open Question 3
- Question: How does ProRBP handle queries or items that have no historical behavior neighbors available in the search logs?
- Basis in paper: [explicit] The paper mentions that a small portion of queries or items may not have behavior neighbors and states they are set as empty in the prompt, but doesn't explain the impact on performance.
- Why unresolved: The paper acknowledges this scenario exists but doesn't provide analysis of how this affects the overall system performance or what alternative strategies might be employed.
- What evidence would resolve it: Performance analysis comparing cases with and without behavior neighbors, or comparison with alternative handling strategies for such edge cases.

## Limitations

- The framework's effectiveness depends heavily on the quality and quantity of user interaction data, which may not be available in all search domains
- The progressive prompting structure may be sensitive to the specific ordering and granularity of prompts, potentially limiting its generalizability
- The approach requires significant computational resources for LLM inference, which could impact scalability in production environments

## Confidence

- **High confidence**: The core observation that LLMs struggle with relevance modeling without domain-specific context is well-supported by both the paper's experiments and existing literature on in-context learning limitations.
- **Medium confidence**: The specific mechanisms of progressive prompting and exponential kernel aggregation are plausible given the results, but would benefit from more extensive ablation studies and parameter sensitivity analysis.
- **Low confidence**: Claims about the framework's applicability to diverse search scenarios and its robustness to varying quality of user behavior data require further validation across different domains and data conditions.

## Next Checks

1. **Ablation study of prompting structure**: Systematically remove or reorder the progressive prompting sub-tasks to quantify the marginal contribution of each step. Compare performance of the full least-to-most sequence against: a) only the final query-item prompt, b) random prompt ordering, and c) varying numbers of intermediate sub-prompts.

2. **Neighbor retrieval robustness analysis**: Test the framework's sensitivity to different neighbor filtering thresholds (CTR cutoffs, minimum exposure counts) and temporal windows (daily vs. weekly vs. monthly behavior logs). Evaluate performance degradation when using increasingly noisy or stale neighbor sets.

3. **Cross-domain generalization test**: Apply the ProRBP framework to a different search domain (e.g., web search or academic paper retrieval) with available relevance judgments and user interaction data. Compare performance against domain-specific baselines to assess transferability of the approach.