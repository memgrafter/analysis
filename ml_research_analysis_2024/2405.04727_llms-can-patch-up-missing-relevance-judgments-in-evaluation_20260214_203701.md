---
ver: rpa2
title: LLMs Can Patch Up Missing Relevance Judgments in Evaluation
arxiv_id: '2405.04727'
source_url: https://arxiv.org/abs/2405.04727
tags:
- relevance
- judgments
- holes
- trec
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of missing relevance judgments
  (holes) in information retrieval test collections, which can introduce biases into
  evaluation and undermine reliability. The authors propose using large language models
  (LLMs) to automatically fill these holes by assigning fine-grained relevance labels.
---

# LLMs Can Patch Up Missing Relevance Judgments in Evaluation

## Quick Facts
- arXiv ID: 2405.04727
- Source URL: https://arxiv.org/abs/2405.04727
- Authors: Shivani Upadhyay; Ehsan Kamalloo; Jimmy Lin
- Reference count: 31
- Key outcome: LLMs achieve Kendall tau correlations of 0.87-0.92 with ground truth when filling missing relevance judgments, even with only 10% of original judgments retained

## Executive Summary
This paper addresses the challenge of missing relevance judgments in information retrieval test collections, which can introduce evaluation biases and undermine reliability. The authors propose using large language models to automatically fill these "holes" by assigning fine-grained relevance labels to missing documents. By simulating varying degrees of missing judgments through random removal of relevant documents from TREC DL datasets, they demonstrate that LLMs can effectively replace missing human judgments with strong correlation to ground truth, even in extreme scenarios where only 10% of original judgments are retained.

## Method Summary
The authors simulate missing relevance judgments by randomly removing relevant documents from TREC DL track datasets, creating controlled "hole" scenarios ranging from 10% to 90% missing judgments. They then use two LLM variants (Vicuña-7B and GPT-3.5 Turbo) to label the missing documents with fine-grained relevance scores. The generated labels are compared against ground truth using Kendall tau correlation to evaluate the quality of LLM-generated judgments. The approach aims to reduce the need for manual labeling while maintaining evaluation accuracy.

## Key Results
- LLMs achieved Kendall tau correlations of 0.87 (Vicuña-7B) and 0.92 (GPT-3.5 Turbo) with ground truth judgments
- Strong performance maintained even with extreme missingness (10% of judgments retained)
- The approach shows promise for reducing manual labeling effort while preserving evaluation reliability

## Why This Works (Mechanism)
The paper leverages LLMs' ability to understand document-query relationships and assign relevance scores based on contextual understanding. By training or fine-tuning these models on existing relevance judgments, they can generalize to unlabeled document-query pairs, effectively interpolating missing values in the judgment matrix. The strong correlation results suggest that LLMs capture the underlying patterns in human relevance assessments well enough to produce reliable synthetic judgments.

## Foundational Learning

- **Relevance Judgment**: Assessment of whether a document satisfies a query's information need - needed to understand what constitutes ground truth; check by verifying that judgment scales (e.g., 0-3 or binary) are clearly defined
- **Kendall tau correlation**: Statistical measure of ordinal association between two ranked lists - needed to quantify agreement between LLM and human judgments; check by confirming it handles tied ranks appropriately
- **Test Collection**: Standardized dataset containing queries, documents, and relevance judgments for IR evaluation - needed as the experimental substrate; check by verifying it includes multiple topics and document sets
- **Information Retrieval Evaluation**: Process of measuring system performance using test collections - needed to contextualize the problem; check by confirming use of standard metrics beyond Kendall tau

## Architecture Onboarding

**Component map**: Document set -> Query pool -> Relevance judgment matrix (with holes) -> LLM labeler -> Filled judgment matrix -> Evaluation metrics

**Critical path**: Missing judgments → LLM labeling → Correlation measurement → System ranking validation

**Design tradeoffs**: The paper trades potential LLM hallucination against the cost and delay of manual labeling. Using synthetic hole creation simplifies experimentation but may not reflect real-world missingness patterns.

**Failure signatures**: Poor correlation with ground truth, systematic bias toward certain relevance levels, inability to handle domain-specific terminology, computational infeasibility for large collections.

**First experiments to run**:
1. Test LLM performance on actual incomplete test collections where judgments are truly missing
2. Evaluate whether LLM-generated judgments change system rankings compared to human judgments
3. Assess computational costs and scalability for large test collections

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments use synthetic hole creation by randomly dropping relevant documents, which may not reflect real-world missing judgment patterns
- Focus on Kendall tau correlation doesn't examine whether LLM judgments affect system rankings compared to manual judgments
- Results may not generalize beyond TREC DL tracks to other document types or query domains
- Computational costs and scalability for large test collections are not addressed

## Confidence

**High confidence** in the observation that LLMs can generate relevance judgments with strong correlation to ground truth in controlled experiments

**Medium confidence** in the claim that this approach reduces manual labeling needs, pending evidence of practical utility

**Low confidence** in generalizability to real-world test collection scenarios and diverse document types

## Next Checks

1. Test the approach on actual incomplete test collections where judgments are truly missing (not synthetically removed)
2. Evaluate whether LLM-generated judgments change system rankings compared to human judgments
3. Assess computational costs and scalability for large test collections with thousands of queries and documents