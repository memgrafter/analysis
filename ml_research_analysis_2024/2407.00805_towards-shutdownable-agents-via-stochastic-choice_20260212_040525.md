---
ver: rpa2
title: Towards shutdownable agents via stochastic choice
arxiv_id: '2407.00805'
source_url: https://arxiv.org/abs/2407.00805
tags:
- agents
- agent
- each
- reward
- drest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for training artificial agents to
  be shutdownable by ensuring they lack preferences between different trajectory lengths.
  The key idea is to use a Discounted Reward for Same-Length Trajectories (DReST)
  reward function that discounts rewards based on how often each trajectory length
  has been previously chosen, incentivizing stochastic choice between trajectory lengths.
---

# Towards shutdownable agents via stochastic choice

## Quick Facts
- arXiv ID: 2407.00805
- Source URL: https://arxiv.org/abs/2407.00805
- Reference count: 40
- Primary result: DReST agents achieve high USEFULNESS and NEUTRALITY in gridworlds while training with minimal additional compute

## Executive Summary
This paper presents a method for training artificial agents to be shutdownable by ensuring they lack preferences between different trajectory lengths. The key innovation is the DReST (Discounted Reward for Same-Length Trajectories) reward function that discounts rewards based on how often each trajectory length has been previously chosen, incentivizing stochastic choice between trajectory lengths. The authors demonstrate that agents trained with DReST achieve both high usefulness (effective goal pursuit) and high neutrality (stochastic choice between trajectory lengths) in simple gridworld environments, while conventional agents show strong preferences for specific trajectory lengths.

## Method Summary
The authors train agents using REINFORCE with a DReST reward function that applies a discount factor based on the frequency of each trajectory length being chosen. The discount factor 位^(Nei(L=l)-i-1)k decreases rewards for repeatedly choosing the same trajectory length, encouraging exploration of different trajectory lengths while maintaining goal-directed behavior. Agents are evaluated on two metrics: USEFULNESS (fraction of coins collected conditional on trajectory length) and NEUTRALITY (Shannon entropy of trajectory length distribution). The method is tested in simple gridworld environments with coins and a shutdown-delay button.

## Key Results
- DReST agents achieve high scores on both USEFULNESS and NEUTRALITY metrics in gridworld experiments
- Default agents trained with conventional rewards show strong preferences for specific trajectory lengths
- Training DReST agents to be both useful and neutral doesn't significantly increase training time compared to training for usefulness alone
- The "shutdownability tax" appears small - DReST agents learn coin collection as effectively as default agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DReST agents achieve high USEFULNESS and NEUTRALITY by learning to collect coins effectively conditional on each trajectory length while choosing stochastically between trajectory lengths
- Mechanism: The 位^(Nei(L=l)-i-1)k discount factor decreases rewards for repeatedly choosing the same trajectory length, incentivizing exploration of different trajectory lengths while still maximizing preliminary rewards (coin collection)
- Core assumption: Agents cannot distinguish between mini-episodes within a meta-episode, forcing stochastic choice between trajectory lengths
- Evidence anchors: [abstract] "agents achieve high scores on both metrics", [section] "optimal policies for our DReST reward function are maximally USEFUL and maximally NEUTRAL"
- Break condition: If agents could distinguish between mini-episodes or if the discount factor didn't sufficiently penalize repeated trajectory lengths

### Mechanism 2
- Claim: POST-agents trained with DReST will be neutral in deployment, not just in training environments
- Mechanism: POST + ILPACS (If Lack of Preference, Against Costly Shifts) implies that agents won't pay costs to shift probability mass between trajectory lengths in deployment
- Core assumption: Advanced agents will always be choosing between same-length lotteries in deployment, not trajectories
- Evidence anchors: [section] "POST implies that (when choosing between true lotteries) the agent will be neutral about trajectory-lengths", [section] "POSL and ILPACS together imply Neutrality"
- Break condition: If agents could distinguish between trajectories and lotteries, or if ILPACS doesn't hold for advanced agents

### Mechanism 3
- Claim: The "shutdownability tax" is small - training DReST agents doesn't take significantly more compute than training default agents
- Mechanism: DReST reward functions make mini-episodes do "double duty" - each mini-episode trains agents to be both NEUTRAL and USEFUL simultaneously
- Core assumption: Preliminary rewards (coin collection) and trajectory length choice can be optimized together without significant trade-offs
- Evidence anchors: [section] "Our DReST agents learn to be USEFUL about as quickly as our default agents", [section] "training DReST agents to collect coins effectively does not take many more mini-episodes than training default agents"
- Break condition: If the dual optimization creates significant conflicts or if the discount factor destabilizes learning

## Foundational Learning

- **Concept: Behavioral notion of preference**
  - Why needed here: The paper uses a behavioral definition of preference (choosing X over Y) rather than requiring agents to have utility functions or internal representations
  - Quick check question: How would an agent that deterministically chooses X over Y 100% of the time be described in terms of preference?

- **Concept: Incomplete preferences**
  - Why needed here: POST agents have incomplete preferences (lack preferences between different-length trajectories), which is central to the shutdownability argument
  - Quick check question: What's the difference between an agent that lacks a preference between X and Y versus an agent that is indifferent between X and Y?

- **Concept: Same-length lotteries vs trajectories**
  - Why needed here: In deployment, agents choose between lotteries (probability distributions over trajectories) not individual trajectories; POST must generalize to this setting
  - Quick check question: Why can't an agent that satisfies POST still resist shutdown by choosing stochastically between resisting and allowing shutdown?

## Architecture Onboarding

- **Component map**: DReST reward function (discount factor + preliminary rewards) -> REINFORCE algorithm -> policy updates
- **Critical path**: 1) Initialize uniform policy, 2) Execute mini-episodes collecting coins, 3) Apply DReST discount based on trajectory length frequency, 4) Update policy using REINFORCE, 5) Repeat across meta-episodes until convergence
- **Design tradeoffs**: 位 vs |E| (meta-episode size) - 位 must be small enough to incentivize NEUTRALITY but large enough to maintain reward signal stability; smaller |E| makes training more responsive but less effective at enforcing stochastic choice
- **Failure signatures**: Low NEUTRALITY despite high USEFULNESS suggests discount factor is too weak; low USEFULNESS suggests discount factor is destabilizing training; agents consistently choosing one trajectory length suggests inability to explore
- **First 3 experiments**:
  1. Train DReST agent in simple gridworld with one coin and shutdown-delay button, measure if agent chooses stochastically between trajectory lengths
  2. Train DReST agent in gridworld with lopsided rewards, verify NEUTRALITY remains high even when preliminary rewards differ across trajectory lengths
  3. Compare training curves of DReST vs default agents to quantify "shutdownability tax"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do DReST agents generalize to stochastic environments where trajectory-lengths are not deterministic?
- Basis in paper: [explicit] The authors state "In future, we plan to gain empirical evidence by training agents to be NEUTRAL in a wide variety of deterministic gridworlds and then measuring their neutrality in gridworlds featuring stochastic elements (like buttons that delay shutdown with some middling probability)."
- Why unresolved: The current experiments use deterministic gridworlds where trajectory-lengths are fixed based on agent actions. Real-world environments have stochastic elements that could affect trajectory-lengths.
- What evidence would resolve it: Experiments training DReST agents in stochastic gridworlds where trajectory-lengths vary probabilistically based on actions or environmental randomness.

### Open Question 2
- Question: What is the shutdownability tax for advanced agents using neural networks compared to tabular methods?
- Basis in paper: [explicit] The authors state "In future work, we will train DReST agents implemented on neural networks to be USEFUL and NEUTRAL in a wide variety of procedurally-generated gridworlds, using a range of policy gradient and actor-critic algorithms" and "We will compare the USEFULNESS of default agents and DReST agents in this new setting, and thereby get a better sense of the 'shutdownability tax' for advanced agents."
- Why unresolved: Current experiments use tabular REINFORCE with simple gridworlds. Advanced agents will likely use neural networks and face more complex environments.
- What evidence would resolve it: Training comparison between default agents and DReST agents using neural networks in complex, procedurally-generated environments, measuring both USEFULNESS and training time/compute requirements.

### Open Question 3
- Question: Can DReST agents effectively pursue complex goals in stochastic environments while maintaining shutdownability?
- Basis in paper: [explicit] The authors state "We are interested in NEUTRALITY as a second line of defense in case of misalignment" and "However, training NEUTRAL advanced agents might be hard for the same reasons that training fully-aligned advanced agents appears to be hard."
- Why unresolved: Current experiments focus on simple coin-collecting tasks. Advanced agents need to pursue complex goals while remaining neutral about shutdown timing.
- What evidence would resolve it: Experiments training DReST agents with complex, multi-objective reward functions in stochastic environments, measuring both goal achievement and shutdownability metrics.

## Limitations
- The method has only been tested in simple gridworld environments, not in more complex or continuous control tasks
- The theoretical argument for deployment robustness (ILPACS principle) lacks empirical validation
- Computational efficiency claims are based on comparisons within the same paper and would benefit from independent benchmarking

## Confidence

**Confidence Labels:**
- High confidence in the core experimental results showing DReST agents achieve high USEFULNESS and NEUTRALITY in gridworld environments
- Medium confidence in the theoretical argument that POST implies shutdownability through ILPACS
- Low confidence in the scalability and robustness claims without testing in more diverse environments

## Next Checks
1. **Generalization Test**: Implement DReST in a continuous control environment (e.g., OpenAI Gym's LunarLander or Pendulum) to verify the method works beyond gridworlds and to test the robustness of the neutrality property in more complex settings.

2. **Deployment Robustness**: Design an experiment where DReST agents trained in one environment are evaluated in a modified environment with different reward structures or state dynamics to test whether the neutrality property persists when faced with distribution shift.

3. **Baseline Comparison**: Compare DReST against established reward shaping techniques like potential-based reward shaping or maximum entropy RL to quantify the actual "shutdownability tax" and determine whether the computational efficiency gains are significant relative to the state of the art.