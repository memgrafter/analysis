---
ver: rpa2
title: Towards Building Large Scale Datasets and State-of-the-Art Automatic Speech
  Translation Systems for 14 Indian Languages
arxiv_id: '2411.04699'
source_url: https://arxiv.org/abs/2411.04699
tags:
- speech
- translation
- languages
- data
- mining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces BhasaAnuvaad, the largest speech translation
  dataset for 14 Indian languages, comprising over 44,000 hours of audio and 17 million
  aligned text segments. The dataset is constructed through a three-pronged approach:
  aggregating existing high-quality sources, large-scale web crawling using a custom
  ST-ALIGNER pipeline, and synthetic data generation to model real-world speech disfluencies.'
---

# Towards Building Large Scale Datasets and State-of-the-Art Automatic Speech Translation Systems for 14 Indian Languages

## Quick Facts
- arXiv ID: 2411.04699
- Source URL: https://arxiv.org/abs/2411.04699
- Reference count: 40
- Primary result: Introduced BhasaAnuvaad, the largest speech translation dataset for 14 Indian languages (44,000+ hours, 17M segments), and trained IndicSeamless model achieving 10 chrf++ improvement over baselines

## Executive Summary
This work addresses the critical challenge of building large-scale automatic speech translation (AST) systems for Indian languages, which have historically been underserved due to limited data availability. The authors introduce BhasaAnuvaad, a comprehensive dataset constructed through a three-pronged approach: aggregating existing high-quality sources, large-scale web crawling with a custom ST-ALIGNER pipeline, and synthetic data generation to model real-world speech disfluencies. Leveraging this dataset, they train IndicSeamless, a state-of-the-art AST model that outperforms existing approaches by an average of 10 chrf++ points on the BhasaAnuvaad-Test set.

## Method Summary
The methodology combines three approaches to create the BhasaAnuvaad dataset: (1) aggregating existing high-quality sources like Indic-TEDST, FLEURS, and Khan Academy Corpus; (2) large-scale web crawling using the ST-ALIGNER pipeline for audio-transcript and parallel text alignment; and (3) generating synthetic data using LLaMA-3.1-405B-INSTRUCT to model colloquial speech patterns and disfluencies. The IndicSeamless model is then fine-tuned on a filtered subset of this data (using quality thresholds of cosine similarity ≥ 0.6 and Levenshtein distance ≥ 0.8) starting from SEAMLESS M4T-V2-LARGE, achieving state-of-the-art performance across 14 Indian languages.

## Key Results
- BhasaAnuvaad dataset contains 44,000+ hours of speech across 14 Indian languages and English, with 17 million aligned text segments
- IndicSeamless achieves 10 chrf++ improvement over previous approaches on BhasaAnuvaad-Test set
- The cascaded Whisper+IT2 model outperforms direct SDBA model in most language directions, with Tamil being a notable exception

## Why This Works (Mechanism)

### Mechanism 1
Large-scale synthetic data generation improves model robustness for low-resource Indian languages by exposing models to real-world disfluencies, filler words, and informal phrasing. This works by using large language models to generate colloquial translations of spontaneous speech transcripts, though effectiveness depends on LLM quality and may suffer from hallucination-induced semantic drift.

### Mechanism 2
Improved alignment quality leads to better downstream speech translation performance through the ST-ALIGNER pipeline using Nemo Forced Aligner and LLaMA-based punctuation restoration. This reduces cascading errors in cascaded ASR+MT systems and improves direct model training quality, though alignment errors can propagate into training data if not properly filtered.

### Mechanism 3
Fine-tuning on domain-specific Indian language data improves translation quality over general multilingual models by allowing IndicSeamless to learn language-specific phonetic and syntactic patterns. This provides better adaptation to Indian language characteristics than general multilingual pretraining alone, though may cause overfitting to training domain.

## Foundational Learning

- Concept: Speech disfluencies and colloquial speech patterns
  - Why needed here: The dataset aims to model real-world spontaneous speech, which includes fillers, repetitions, and informal phrasing not present in formal text
  - Quick check question: Can you identify three common disfluencies found in spontaneous speech that should be modeled in synthetic data?

- Concept: Forced alignment and timestamp accuracy
  - Why needed here: Accurate audio-transcript alignment is critical for training speech translation models, as misalignment leads to incorrect learning of audio-text correspondences
  - Quick check question: What is the primary metric used to evaluate alignment quality between audio and transcript?

- Concept: Cross-lingual sentence embeddings for mining
  - Why needed here: SONAR embeddings are used to identify high-quality parallel text pairs during web mining, ensuring semantic alignment across languages
  - Quick check question: How does cosine similarity between cross-lingual embeddings help in identifying parallel sentences?

## Architecture Onboarding

- Component map: Data pipeline → ST-ALIGNER → Quality filtering → Model training → Evaluation
- Critical path: Web mining → Alignment → Synthetic generation → Model fine-tuning → Benchmark evaluation
- Design tradeoffs: Synthetic data vs. manual annotation cost; alignment accuracy vs. processing speed; domain diversity vs. data quality
- Failure signatures: Low mining scores indicating poor alignment; synthetic translations with semantic drift; model overfitting to training domain
- First 3 experiments:
  1. Run ST-ALIGNER on a small sample of web audio-text pairs and measure alignment accuracy using Levenshtein distance
  2. Generate synthetic translations using LLaMA-3.1-405B-INSTRUCT and evaluate colloquial quality via human assessment
  3. Fine-tune IndicSeamless on a filtered subset of BhasaAnuvaad and measure chrf++ improvement over baseline on test set

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of BhasaAnuvaad-trained models vary when evaluated on truly spontaneous speech data versus acted or read speech? The paper notes that most prior evaluations rely on FLEURS, which consists of acted-out, read speech rather than spontaneous speech, and that models achieving strong results on FLEURS do not necessarily maintain similar performance on more challenging, real-world speech scenarios.

### Open Question 2
What are the specific linguistic factors that cause Tamil to perform significantly better with the cascaded Whisper+IT2 model compared to the direct SDBA model on the BhasaAnuvaad-Test set? The paper acknowledges this discrepancy but does not provide detailed analysis of why this language-specific performance difference occurs.

### Open Question 3
What specific improvements in multilingual embedding models are needed to effectively handle low-resource languages like Assamese and Sindhi in the SONAR-based mining pipeline? While the paper identifies the problem, it does not specify what technical improvements to embedding models would address this limitation.

## Limitations
- Data quality verification gap: No systematic human evaluation of synthetic data quality or alignment accuracy
- Limited ablation analysis: Lacks detailed studies showing which components contribute most to performance gains
- Generalization concerns: Limited discussion of out-of-domain performance on truly unseen speech patterns

## Confidence
**High Confidence**:
- Dataset construction methodology is technically sound and follows established practices
- Use of Nemo Forced Aligner and cross-lingual embeddings for alignment is appropriate
- 10 chrf++ improvement over baselines is plausible given dataset scale

**Medium Confidence**:
- Synthetic data generation improves model robustness - theoretically sound but lacks direct empirical validation
- ST-ALIGNER pipeline improves alignment quality - supported by methodology but lacking quantitative metrics

**Low Confidence**:
- Specific contribution of synthetic data to overall performance - no ablation study provided
- Claim about modeling real-world disfluencies - no analysis of synthetic data capturing actual spontaneous speech

## Next Checks
1. **Alignment Quality Assessment**: Run ST-ALIGNER pipeline on manually annotated sample (100 pairs) and measure true alignment accuracy using Word Error Rate or alignment error rate; compare against reported mining scores.

2. **Synthetic Data Quality Evaluation**: Generate synthetic translations for INDIC VOICES subset and conduct human evaluation with native speakers rating colloquial quality, semantic accuracy, and naturalness on 5-point scale; compare against baseline translations.

3. **Ablation Study on Model Components**: Train three IndicSeamless variants - (a) baseline with only existing data, (b) with existing + mined data, (c) with existing + mined + synthetic data; measure chrf++ performance to isolate contribution of each data source.