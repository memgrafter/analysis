---
ver: rpa2
title: Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of Speech
  Sound Disorders in Korean children
arxiv_id: '2403.08187'
source_url: https://arxiv.org/abs/2403.08187
tags:
- speech
- recognition
- pronunciation
- children
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an ASR model for diagnosing pronunciation errors
  in children with speech sound disorders (SSDs) in Korean. The model was fine-tuned
  on 1.56 hours of speech data from 137 children with SSDs pronouncing 73 target words.
---

# Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of Speech Sound Disorders in Korean children

## Quick Facts
- arXiv ID: 2403.08187
- Source URL: https://arxiv.org/abs/2403.08187
- Reference count: 40
- Primary result: ASR model achieves 10.25% PER and 90% accuracy in diagnosing Korean children's pronunciation errors

## Executive Summary
This study develops an ASR model using fine-tuned wav2vec2.0 XLS-R to diagnose pronunciation errors in Korean children with speech sound disorders (SSDs). The model was trained on 1.56 hours of speech data from 137 children pronouncing 73 target words. Results show a phoneme error rate (PER) of 10.25% on the test set, with predictions matching human annotations approximately 90% of the time. The approach demonstrates potential for streamlining pronunciation error diagnostic procedures in clinical settings for children with SSDs.

## Method Summary
The study fine-tuned a pre-trained wav2vec2.0 XLS-R model on Korean speech data from children with SSDs. The model was trained to output separate consonant and vowel tokens, allowing direct comparison with clinical phoneme-level annotations. Training used CTC decoding without language model weighting to preserve pronunciation errors. The dataset consisted of 1.56 hours from 137 children (95 training, 12 validation, 30 test speakers) pronouncing 73 target words. The model was evaluated using PER and F1 scores for consonants.

## Key Results
- Fine-tuned wav2vec2.0 XLS-R achieved PER of 10.25% on test set
- Model predictions matched human annotations with approximately 90% accuracy
- Performance varied by consonant group, with some phonemes showing error rates up to 60-65%

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a pre-trained multilingual wav2vec2.0 XLS-R model achieves high ASR accuracy for Korean child SSD speech using very small labeled datasets. The model leverages large-scale unsupervised pre-training on 128 languages to capture general acoustic and phonetic features, then adapts to the specific domain with limited fine-tuning data. Core assumption: pre-trained representations are sufficiently generalizable to transfer to this low-resource, domain-specific task.

### Mechanism 2
Separating consonants and vowels during fine-tuning improves diagnostic accuracy by aligning model output with clinical phoneme-level analysis. By treating each Korean consonant and vowel as separate token, the model learns to output phoneme sequences that directly match human annotations used in clinical diagnosis. Core assumption: clinical diagnosis relies on phoneme-level accuracy rather than word-level correctness.

### Mechanism 3
Language model weighting is not beneficial for this task because it introduces grammatical/lexical biases that conflict with recognizing pronunciation errors. LM weighting improves word prediction by incorporating statistical language patterns, but this can mask pronunciation variations that are the focus of SSD diagnosis. Core assumption: the task requires recognizing literal utterances as pronounced, not correcting them to standard forms.

## Foundational Learning

- Concept: Self-supervised learning in wav2vec2.0
  - Why needed here: Enables pre-training on massive unlabeled speech data to learn general acoustic representations before fine-tuning on limited labeled data
  - Quick check question: How does wav2vec2.0 learn representations without text labels during pre-training?

- Concept: Connectionist Temporal Classification (CTC) decoding
  - Why needed here: Allows the model to output phoneme sequences without requiring frame-level alignment between audio and text
  - Quick check question: What advantage does CTC provide for recognizing pronunciation errors compared to traditional forced alignment?

- Concept: Phoneme error rate (PER) vs word error rate (WER)
  - Why needed here: PER better captures pronunciation accuracy at the phoneme level, which is critical for SSD diagnosis
  - Quick check question: Why might PER be more appropriate than WER for evaluating pronunciation error detection?

## Architecture Onboarding

- Component map: Pre-trained wav2vec2.0 XLS-R model -> Fine-tuning dataset -> Pronunciation error dictionary -> CTC decoder -> Evaluation metrics
- Critical path: Pre-trained model → Fine-tuning → Decoding → Evaluation
- Design tradeoffs:
  - Smaller pre-trained model (xls-r-1b) chosen for GPU memory efficiency vs. potentially better performance from larger models
  - No LM weighting to preserve pronunciation errors vs. potential gains in fluency
  - Limited fine-tuning data (1.5 hours) vs. need for more diverse pronunciation variations
- Failure signatures:
  - High PER on specific consonant groups (especially similar-sounding consonants like 'ㄷ[d]' and 'ㅈ[dz]')
  - Confusion between non-categorical phonemes that share articulation features
  - Poor performance on word-initial or word-final consonants
- First 3 experiments:
  1. Fine-tune the pre-trained model on 30 minutes of data to establish baseline performance
  2. Add LM weighting with the pronunciation error dictionary to test decoding improvements
  3. Compare phoneme-level vs. word-level tokenization to evaluate diagnostic accuracy

## Open Questions the Paper Calls Out
The paper explicitly mentions that future studies should focus on developing a model robust to real-world environments, including ambient noise and unclear pronunciation. It also suggests investigating the model's performance across different age groups within the 5-10 year range and evaluating its ability to handle connected speech and varying word contexts.

## Limitations
- Small dataset size (1.56 hours from 137 children) limits generalizability to broader clinical contexts
- Model performance varies significantly across consonant groups, with some phonemes showing error rates up to 60-65%
- Study does not address potential biases in the dataset or model performance across different age groups within the 5-10 year range

## Confidence
- Dataset size and scope: Medium
- Model performance claims: Medium
- Clinical applicability: Low

## Next Checks
1. **Dataset Expansion and Diversity**: Collect additional speech data from a more diverse population of children with SSDs, including different age groups, SSD types, and severity levels. Test whether the current model architecture maintains performance with increased dataset size and variety.

2. **Clinical Validation Study**: Conduct a formal clinical study comparing the ASR model's diagnoses against established clinical assessments by speech-language pathologists. Evaluate both diagnostic accuracy and inter-rater reliability across multiple clinical experts.

3. **Generalization Testing**: Test the model on unseen Korean words beyond the original 73 target words to assess its ability to generalize phoneme recognition to novel vocabulary. This would validate whether the model has learned phoneme-level patterns or is simply memorizing specific word pronunciations.