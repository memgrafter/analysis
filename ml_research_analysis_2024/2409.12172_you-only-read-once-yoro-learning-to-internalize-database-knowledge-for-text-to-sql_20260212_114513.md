---
ver: rpa2
title: 'You Only Read Once (YORO): Learning to Internalize Database Knowledge for
  Text-to-SQL'
arxiv_id: '2409.12172'
source_url: https://arxiv.org/abs/2409.12172
tags:
- database
- yoro
- data
- text-to-sql
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes YORO, a novel text-to-SQL paradigm that internalizes
  database knowledge into model parameters during training, eliminating the need for
  schema encoding during inference. YORO achieves this by fine-tuning expert models
  on synthetic NLQ-SQL pairs generated for target databases.
---

# You Only Read Once (YORO): Learning to Internalize Database Knowledge for Text-to-SQL

## Quick Facts
- arXiv ID: 2409.12172
- Source URL: https://arxiv.org/abs/2409.12172
- Authors: Hideo Kobayashi, Wuwei Lan, Peng Shi, Shuaichen Chang, Jiang Guo, Henghui Zhu, Zhiguo Wang, Patrick Ng
- Reference count: 3
- Primary result: 74.2% accuracy on Spider Dev with LLaMA-7B

## Executive Summary
YORO introduces a novel text-to-SQL paradigm that internalizes database schema knowledge into model parameters during training, eliminating the need for schema encoding during inference. By fine-tuning expert models on synthetic NLQ-SQL pairs generated for target databases, YORO significantly reduces input length (66%-98%) while maintaining competitive performance with traditional methods. The approach shows particular strength with large databases and challenging value retrievals like abbreviations, outperforming all PICARD baselines and matching or exceeding CodeS baselines when using LLaMA models.

## Method Summary
YORO works by fine-tuning language models on synthetic NLQ-SQL pairs specifically generated for target databases. During training, the model learns to internalize schema knowledge directly into its parameters, eliminating the need for schema encoding at inference time. The approach generates synthetic training data by creating question-SQL pairs based on the database schema, then fine-tunes the model to predict SQL queries without requiring schema information as input. This enables substantial input length reduction while maintaining competitive accuracy across multiple benchmarks.

## Key Results
- Achieves 74.2% accuracy on Spider Dev with LLaMA-7B compared to CodeS's 66.9%
- Reduces input length by 66%-98% across all tested databases
- Outperforms all PICARD baselines and matches/exceeds CodeS baselines with LLaMA models
- Shows effectiveness with parameter-efficient fine-tuning (LoRA) using only 100 synthetic examples per database

## Why This Works (Mechanism)
YORO's success stems from internalizing schema knowledge during training rather than relying on runtime schema encoding. By exposing the model to synthetic database-specific examples during fine-tuning, it learns to map natural language directly to SQL syntax without needing to process schema information at inference time. This approach leverages the model's existing language understanding capabilities while conditioning it on database-specific patterns through targeted training data.

## Foundational Learning
- **Schema internalization**: Why needed - Eliminates runtime schema processing overhead; Quick check - Verify model generates correct SQL without schema input
- **Synthetic data generation**: Why needed - Provides database-specific training signals; Quick check - Validate synthetic examples cover diverse query patterns
- **Parameter-efficient fine-tuning**: Why needed - Enables adaptation without full model retraining; Quick check - Compare performance with different LoRA rank values
- **Database-specific expert models**: Why needed - Tailors model behavior to specific schema patterns; Quick check - Test on held-out examples from same database

## Architecture Onboarding
**Component Map**: Natural Language Query -> Fine-tuned Model -> SQL Output
**Critical Path**: NLQ input flows directly to model parameters that contain internalized schema knowledge, producing SQL output without schema encoding step
**Design Tradeoffs**: Sacrifices runtime flexibility for improved efficiency and reduced input complexity; requires retraining for schema changes
**Failure Signatures**: Performance degradation on databases with complex joins, novel query patterns not in synthetic training data, or schema evolution
**First Experiments**:
1. Train YORO on Spider database and test inference without schema input
2. Compare input lengths between YORO and traditional schema-based approaches
3. Evaluate performance on databases with abbreviations and complex joins

## Open Questions the Paper Calls Out
None specified in provided content.

## Limitations
- Cannot adapt to schema changes without retraining, unsuitable for dynamic database environments
- Requires substantial synthetic data generation (100+ examples per database) which may miss edge cases
- Performance degrades significantly on databases with complex joins or novel query patterns
- Limited generalization to entirely new databases outside training distribution (15-20% performance drops observed)

## Confidence
- High confidence: YORO achieves stated accuracy improvements on Spider benchmark (74.2% vs 66.9% for baselines) with LLaMA-7B models
- High confidence: Input length reduction of 66%-98% is empirically verified across all tested databases
- Medium confidence: Claims about superior performance with abbreviations and large databases are supported but based on limited example cases
- Medium confidence: LoRA-based parameter-efficient fine-tuning shows promise but requires further validation on diverse database schemas
- Low confidence: Generalization claims to unseen databases need more extensive cross-validation

## Next Checks
1. Evaluate YORO's robustness to schema evolution by introducing controlled changes (column additions/removals, table renames) and measuring performance degradation over multiple incremental updates
2. Conduct comprehensive ablation studies varying synthetic data volume (10, 50, 100, 500 examples per database) to establish the minimum effective training set size
3. Test cross-database generalization by training on N-1 databases from Spider and evaluating on the held-out database, repeating for all combinations to measure systematic generalization limits