---
ver: rpa2
title: Process Reward Model with Q-Value Rankings
arxiv_id: '2410.11287'
source_url: https://arxiv.org/abs/2410.11287
tags:
- step
- steps
- correct
- reasoning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in existing Process Reward Models
  (PRMs) for multi-step reasoning by introducing Process Q-value Model (PQM), a framework
  grounded in Markov Decision Processes. Instead of treating each reasoning step independently,
  PQM ranks intermediate steps by their expected contribution to achieving correct
  final answers, capturing dependencies among steps through a novel comparative loss
  function with margin-based adjustments.
---

# Process Reward Model with Q-Value Rankings

## Quick Facts
- arXiv ID: 2410.11287
- Source URL: https://arxiv.org/abs/2410.11287
- Authors: Wendi Li; Yixuan Li
- Reference count: 40
- Key outcome: PQM improves PRM verification accuracy from 39.8% to 51.4% on Llama-3-70B-Instruct solutions for MATH500 dataset

## Executive Summary
This paper introduces Process Q-value Model (PQM), a novel framework for Process Reward Modeling that frames step-wise reasoning evaluation as a Q-value ranking problem within a Markov Decision Process. Unlike classification-based PRMs that treat each reasoning step independently, PQM captures dependencies among sequential decisions by optimizing Q-values based on their relative rankings. The framework employs a comparative loss function with margin-based adjustments to emphasize significant differences between correct and incorrect steps. Empirical evaluations on MATH500 and GSM-Plus datasets demonstrate substantial improvements over baseline PRMs, with consistent gains across different model backbones and sampling policies.

## Method Summary
PQM addresses limitations in classification-based PRMs by introducing a Q-value ranking framework grounded in Markov Decision Processes. The model processes each reasoning step as a state-action pair, where the LLM backbone generates a latent representation that is projected to a scalar Q-value via a value head. Training uses a comparative loss function that ranks Q-values within trajectories, with a margin parameter ζ that emphasizes correct/incorrect step separation. The framework theoretically unifies classification-based PRMs as a special case under specific conditions, providing both empirical improvements and theoretical grounding. PQM is trained on automatically annotated data from the Math-Shepherd corpus and evaluated on MATH500 and GSM-Plus datasets using best-of-n accuracy metrics.

## Key Results
- PQM achieves 51.4% accuracy vs 39.8% for BCE baseline when verifying Llama-3-70B-Instruct solutions on MATH500
- Consistent improvements across different model backbones (Deepseek-7B, Llama-3-70B-Instruct) and sampling policies
- Margin parameter ζ critically affects performance, with values 2, 4, 8 showing optimal results
- PQM demonstrates better sample efficiency compared to classification-based approaches

## Why This Works (Mechanism)

### Mechanism 1
PQM improves verification accuracy by capturing dependencies among reasoning steps through Q-value ranking instead of treating each step independently. The model optimizes Q-values based on their relative rankings within a trajectory. According to Theorem 3.5, Q-values ascend with correct steps and descend with incorrect steps, creating a structured ordering that reflects each step's contribution to the final answer. The comparative loss function with margin ζ emphasizes these relative differences. Core assumption: Q-values can be meaningfully ranked such that correct steps have higher values than incorrect ones. Evidence: [abstract] "PQM optimizes Q-value rankings based on a novel comparative loss function" and [section] "Theorem 3.5 (Q-value ranking among reasoning steps)... Q-values ascend with the continuation of correct steps and descend as wrong steps proceed."

### Mechanism 2
The margin-based comparative loss function (Eq. 10) is more effective than BCE loss because it explicitly accounts for the relative importance of correct vs. incorrect steps. The loss function includes a margin ζ that separates correct and incorrect steps in the ranking space. This creates a larger penalty for misranking steps with significant Q-value differences compared to BCE loss which treats all errors equally. Core assumption: The gap between correct and incorrect steps is meaningful and should be weighted differently in the loss function. Evidence: [abstract] "We propose a novel framework—Process Q-value Model (PQM)—which frames PRM as a Q-value ranking problem" and [section] "To address the limitation, we adapt the vanilla PL loss to better reflect these discrepancies."

### Mechanism 3
PQM can be viewed as a generalization of classification-based PRMs, providing theoretical grounding for why it works better. The paper shows that classification-based PRMs are a special case of Q-value approximimators under certain extreme conditions (Lemma 3.6). This theoretical connection explains why PQM can improve upon BCE while maintaining theoretical consistency. Core assumption: The conditions under which classification-based PRMs approximate Q-values are restrictive and don't hold in practice. Evidence: [abstract] "We also show that prior classification-based PRM can be cast as a special case within our theoretical framework" and [section] "We show that the previous classification-based PRM can be cast as a special case of our framework under certain conditions."

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: PQM frames PRM as an MDP problem where each reasoning step is an action and the state consists of previous steps. Understanding MDPs is crucial for grasping how Q-values are defined and optimized.
  - Quick check question: In an MDP, what does the Q-value function represent and how is it different from the value function V(s)?

- Concept: Ranking-based loss functions
  - Why needed here: PQM uses a comparative loss function that ranks Q-values rather than predicting absolute correctness. Understanding ranking losses (like Plackett-Luce) is essential for implementing and debugging PQM.
  - Quick check question: How does a ranking-based loss function differ from a classification loss like BCE, and what are the advantages of ranking approaches?

- Concept: Reinforcement Learning vs. Supervised Learning
  - Why needed here: PQM bridges RL concepts (Q-values, MDPs) with supervised learning (training on labeled data). Understanding this connection helps explain why PQM can leverage RL theory while being trained with labeled data.
  - Quick check question: What are the key differences between RL-based and supervised approaches to reward modeling, and how does PQM combine elements of both?

## Architecture Onboarding

- Component map: Input (Question + trajectory) -> LLM backbone -> Value head -> Q-value output -> Comparative loss function -> Parameter updates

- Critical path: 1. Tokenize input (question + reasoning steps) 2. LLM processes each state to generate latent representation 3. Value head projects to Q-value 4. Loss function computes comparative ranking loss 5. Backpropagation updates model parameters

- Design tradeoffs:
  - Q-value vs. classification: Q-values provide richer information about step quality but require more complex training
  - Margin size (ζ): Larger margins emphasize correct/incorrect separation but may overfit
  - Backbone choice: Stronger backbones improve performance but increase computational cost

- Failure signatures:
  - Q-values not ordered correctly: Check if margin ζ is properly tuned
  - Model not converging: Verify learning rate and batch size
  - Poor performance on unseen data: Check if training data distribution matches test distribution

- First 3 experiments:
  1. Train PQM on small subset of Math-Shepherd with ζ=0 to verify basic functionality
  2. Compare BCE vs. PQM loss on held-out validation set to confirm ranking improvement
  3. Test different margin values (ζ=2,4,8) to find optimal hyperparameter setting

## Open Questions the Paper Calls Out

### Open Question 1
How can the proposed PQM framework be extended to handle non-deterministic MDP settings for language model generation? Basis: [explicit] The paper states "Although the outcome reward model has advanced LLMs by applying reinforcement learning algorithms in bandit settings, it contradicts the auto-regressive nature of text generation and the step-by-step reasoning process" and discusses deterministic MDP formulations. Why unresolved: The current framework assumes deterministic transitions where each new state is formed by concatenating previous tokens with the current output. Real-world scenarios might involve stochastic elements or uncertainty in state transitions. What evidence would resolve it: Empirical studies demonstrating PQM's effectiveness in settings with probabilistic transitions, or theoretical extensions showing how the Q-value ranking framework adapts to non-deterministic environments.

### Open Question 2
What is the optimal value of the margin parameter ζ across different reasoning tasks and model scales? Basis: [explicit] The paper shows "extreme values of ζ—either too large or too small—lead to suboptimal performance" and identifies that "ζ values of 2,4,8 yield the best results" but notes this varies by context. Why unresolved: The paper only tests a limited range of ζ values and on specific datasets. The optimal margin likely depends on task complexity, dataset characteristics, and model scale. What evidence would resolve it: Systematic experiments across diverse reasoning tasks (math, coding, science) and model sizes showing how ζ should be tuned for different scenarios.

### Open Question 3
How does PQM perform when integrated with online reinforcement learning algorithms that leverage process rewards? Basis: [inferred] The paper notes "there is a lack of systematic studies on corresponding online RL algorithms that leverage PRMs" and discusses offline approaches, but doesn't explore online RL integration. Why unresolved: The current work focuses on offline evaluation and training. Online RL settings present different challenges including exploration-exploitation trade-offs and distribution shift. What evidence would resolve it: Empirical results comparing PQM-based online RL algorithms against outcome-based RL approaches, showing improvements in sample efficiency and final performance.

## Limitations

- Dataset quality dependency: Performance heavily relies on the quality of automatically annotated training data from the Math-Shepherd corpus
- Limited domain generalization: Strong results on mathematical reasoning but limited testing on non-mathematical tasks
- Hyperparameter sensitivity: Critical dependence on margin parameter ζ with unclear optimal settings across different scenarios

## Confidence

**High Confidence** (supported by strong empirical evidence and theoretical framework):
- PQM architecture improves verification accuracy over BCE baseline (51.4% vs 39.8% on Llama-3-70B-Instruct)
- Q-value ranking framework provides theoretical advantages over classification-based PRMs
- Margin-based loss function contributes to performance improvements

**Medium Confidence** (reasonable but with some gaps):
- Claims about capturing dependencies among reasoning steps through Q-value ordering
- Generalization to different model backbones (Deepseek-7B, Llama-3-70B-Instruct)
- Sample efficiency improvements compared to baseline methods

**Low Confidence** (limited evidence or significant assumptions):
- Theoretical connection between Q-value ranking and improved reasoning verification
- Performance on tasks beyond mathematical reasoning
- Robustness to annotation noise in training data

## Next Checks

1. **Ablation Study on Annotation Quality**: Train PQM on subsets of Math-Shepherd with varying annotation quality (different k values for sampling completions). Measure performance degradation to quantify sensitivity to training data noise.

2. **Cross-Domain Generalization Test**: Apply PQM to non-mathematical reasoning tasks (e.g., code generation, general QA) and compare performance against classification-based PRMs. This validates whether Q-value ranking generalizes beyond math problems.

3. **Margin Sensitivity Analysis**: Systematically vary ζ across a wider range (0.5 to 16) and plot performance curves. Include analysis of how margin size affects convergence speed and final accuracy to identify optimal settings for different model scales.