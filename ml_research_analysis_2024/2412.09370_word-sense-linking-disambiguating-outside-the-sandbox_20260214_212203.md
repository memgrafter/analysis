---
ver: rpa2
title: 'Word Sense Linking: Disambiguating Outside the Sandbox'
arxiv_id: '2412.09370'
source_url: https://arxiv.org/abs/2412.09370
tags:
- sense
- word
- systems
- span
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Word Sense Linking (WSL), a task that extends
  Word Sense Disambiguation (WSD) by requiring systems to both identify spans to disambiguate
  and link them to the most suitable sense in a reference inventory. The authors propose
  a retriever-reader architecture that first generates candidate senses using dense
  passage retrieval, then identifies spans and pairs them with appropriate senses.
---

# Word Sense Linking: Disambiguating Outside the Sandbox

## Quick Facts
- **arXiv ID**: 2412.09370
- **Source URL**: https://arxiv.org/abs/2412.09370
- **Reference count**: 35
- **Primary result**: Introduces WSL task and achieves 75.9 F1 on WSL benchmark, outperforming WSD systems scaled to WSL (71.5 F1)

## Executive Summary
This paper introduces Word Sense Linking (WSL), a task that extends traditional Word Sense Disambiguation (WSD) by requiring systems to both identify spans to disambiguate and link them to appropriate senses in a reference inventory. The authors propose a retriever-reader architecture that first generates candidate senses using dense passage retrieval, then identifies spans and pairs them with appropriate senses. They introduce a comprehensive WSL benchmark based on standard WSD datasets with complete coverage of content words. Their model achieves 75.9 F1 on the WSL benchmark, significantly outperforming state-of-the-art WSD systems scaled to WSL (71.5 F1).

## Method Summary
The WSL approach employs a retriever-reader architecture that first generates candidate senses using dense passage retrieval on Wikipedia passages, then identifies spans and pairs them with appropriate senses. The retriever uses SPLADEv2 for dense passage retrieval, retrieving 5 passages per sense. The reader component uses a T5-base encoder-decoder model with three objectives: span identification, span linking, and combined span-link prediction. The model is trained with a weighted loss combining all three objectives. The WSL benchmark is constructed by augmenting standard WSD datasets (SensEval-2 through SemEval-13) with complete coverage of content words, creating three evaluation settings: Full WSL (all content words), WSD-style (only annotated words), and Extreme WSL (only non-annotated content words).

## Key Results
- WSL model achieves 75.9 F1 on the WSL benchmark
- Outperforms state-of-the-art WSD systems scaled to WSL (71.5 F1)
- Demonstrates that standard WSD systems struggle when extended to real-world settings due to challenges in concept detection and candidate generation

## Why This Works (Mechanism)
The retriever-reader architecture works by first expanding the candidate space through dense passage retrieval, which helps identify relevant senses even for words not explicitly annotated. The reader component's three objectives allow it to simultaneously identify spans and link them to appropriate senses, handling the ambiguity inherent in real-world text. The use of SPLADEv2 for retrieval provides better semantic matching compared to traditional methods, while the T5-base architecture with combined objectives allows for end-to-end learning of both span identification and linking.

## Foundational Learning

**Dense Passage Retrieval**: Needed to efficiently search through large sense inventories and retrieve relevant candidate senses for ambiguous words. Quick check: Verify that retrieved passages contain relevant semantic information for the target word.

**Sense Inventory Construction**: Required to map words to their possible senses in a structured way. Quick check: Ensure all content words have corresponding Wikipedia pages or redirect entries.

**Span Identification**: Critical for the WSL task as it requires detecting which words need disambiguation. Quick check: Test model's ability to identify spans in text with varying ambiguity levels.

**Multi-task Learning**: Used to train the reader component on span identification, linking, and combined prediction simultaneously. Quick check: Monitor individual task performance and combined performance during training.

**T5 Encoder-Decoder Architecture**: Chosen for its ability to handle sequence-to-sequence tasks and fine-tuning flexibility. Quick check: Verify that the model can generate valid sense identifiers as output.

## Architecture Onboarding

**Component Map**: Wikipedia Corpus -> SPLADEv2 Retriever -> T5-base Reader -> Sense Linking Output

**Critical Path**: Dense passage retrieval (SPLADEv2) -> Span identification and linking (T5-base) -> Sense disambiguation output

**Design Tradeoffs**: The choice of retriever-reader architecture balances efficiency (retriever narrows candidates) with accuracy (reader makes final decisions). Using T5-base provides strong baseline performance while remaining computationally tractable. The three-objective training approach allows simultaneous learning but requires careful loss weighting.

**Failure Signatures**: Poor performance may result from inadequate retrieval (missing relevant senses), span identification errors (missing or incorrect spans), or linking failures (wrong sense selection). The model may struggle with rare senses or highly ambiguous contexts.

**3 First Experiments**:
1. Test retriever performance in isolation by measuring recall of relevant senses for ambiguous words
2. Evaluate reader component's span identification accuracy on unambiguous text
3. Assess sense linking performance when spans are pre-identified

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the work raises several implicit questions about the scalability of WSL to larger sense inventories, the robustness of the approach to out-of-domain text, and the potential for incorporating contextual information beyond the immediate sentence.

## Limitations
- Evaluation relies entirely on existing WSD datasets, inheriting their biases and limitations
- Performance comparison based on a single retriever-reader architecture
- Claim that WSL is "more realistic" than WSD lacks empirical validation beyond performance comparison

## Confidence

**High confidence**: Technical implementation details and benchmark construction methodology are clearly described and reproducible.

**Medium confidence**: Comparative performance claims depend on a single model architecture and may not generalize to other approaches.

**Medium confidence**: The assertion that WSL better reflects real-world challenges is primarily theoretical rather than empirically validated through user studies or real-world deployment evidence.

## Next Checks

1. Evaluate multiple architectural approaches (not just retriever-reader) on the WSL benchmark to determine if the performance gap is task-specific or architecture-dependent.

2. Conduct user studies or analyze real-world usage data to empirically validate whether WSL better captures practical disambiguation challenges compared to WSD.

3. Test the WSL model on out-of-domain datasets and noisy real-world text to assess robustness beyond controlled benchmark conditions.