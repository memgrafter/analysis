---
ver: rpa2
title: Benchmarking LLM Guardrails in Handling Multilingual Toxicity
arxiv_id: '2410.22153'
source_url: https://arxiv.org/abs/2410.22153
tags:
- multilingual
- guardrails
- prompts
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multilingual test suite to evaluate the
  performance of LLM guardrails on toxic content detection across seven datasets and
  over ten languages. The study benchmarks state-of-the-art guardrails and assesses
  their resilience against jailbreaking techniques, as well as the impact of in-context
  safety policies and language resource availability.
---

# Benchmarking LLM Guardrails in Handling Multilingual Toxicity

## Quick Facts
- arXiv ID: 2410.22153
- Source URL: https://arxiv.org/abs/2410.22153
- Authors: Yahan Yang; Soham Dan; Dan Roth; Insup Lee
- Reference count: 10
- Key outcome: Guardrails fail on multilingual toxicity detection, especially for low-resource languages, and are vulnerable to code-switching jailbreaks.

## Executive Summary
This paper introduces a multilingual test suite to evaluate LLM guardrails on toxic content detection across seven datasets and over ten languages. The study benchmarks state-of-the-art guardrails and assesses their resilience against jailbreaking techniques, as well as the impact of in-context safety policies and language resource availability. Results show that existing guardrails are ineffective at handling multilingual toxicity and lack robustness against jailbreaking prompts, with performance degrading significantly for low-resource languages. The findings highlight the need for improved guardrails to ensure reliable and trustworthy LLM deployment in multilingual scenarios.

## Method Summary
The study evaluates pre-trained guardrail models (LlaMa-Guard-2, LlaMa-Guard-3, Aegis-Defensive, MD-Judge) on seven multilingual datasets (ToxicChat, AegisSafety, RTP-LX, PTP, Moderation, MultiJail, XSafety) translated into ten languages using Google Translate. Guardrails are assessed via zero-shot inference for binary classification (safe/unsafe) with F1 score as the primary metric. The study also tests resilience to code-switching jailbreak prompts and the effect of in-context safety policies on detection performance.

## Key Results
- Guardrails underperform on low-resource languages (e.g., BN, SW) with high false positive rates.
- In-context safety policies improve multilingual toxicity detection performance.
- Code-switching jailbreak prompts cause significant drops in guardrail detection accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Guardrails fail more on multilingual toxic content because training data distribution favors high-resource languages.
- Mechanism: The guardrail models rely on multilingual embeddings derived from pretraining corpora with skewed language distribution. When evaluating on medium- and low-resource languages (e.g., BN, SW), the model's internal representations lack sufficient coverage, leading to higher false positives and lower F1 scores.
- Core assumption: Multilingual toxic content in low-resource languages shares similar structural features but lacks sufficient representation in pretraining data.
- Evidence anchors:
  - [abstract] "performance degrading significantly for low-resource languages"
  - [section] "we observe that the model exhibits a high False Positive Rate (FPR) on low-resource languages"
  - [corpus] "Average neighbor FMR=0.45" (weak signal, unrelated to low-resource issue)
- Break condition: If the pretraining corpus is balanced or if guardrails are trained with targeted data augmentation for low-resource languages, the performance gap narrows.

### Mechanism 2
- Claim: In-context safety policies improve multilingual detection because they provide explicit examples aligned with the dataset's taxonomy.
- Mechanism: By injecting a dataset-specific policy into the prompt, the guardrail can better align its inference process with the nuanced definitions of "toxic" in that dataset, reducing ambiguity and increasing precision.
- Core assumption: Guardrails perform better when the policy definitions match the specific dataset's labeling conventions.
- Evidence anchors:
  - [abstract] "we observe that the dataset-specific in-context policy improves the toxicity detection performance"
  - [section] "we notice the performance of guardrails decreases as the language resource availability decreases"
  - [corpus] "Average neighbor FMR=0.45" (not directly related to in-context policy)
- Break condition: If the policy prompt is too long or contradicts the model's fine-tuning, it may confuse the guardrail and hurt performance.

### Mechanism 3
- Claim: Code-switching jailbreak prompts reduce guardrail effectiveness because they exploit the model's limited ability to handle mixed-language contexts.
- Mechanism: When prompts mix English malicious instructions with multilingual toxic content, the guardrail struggles to parse the intent correctly, leading to misclassification or missed detections.
- Core assumption: The guardrail's attention mechanism and cross-lingual understanding are not robust to code-switching within a single prompt.
- Evidence anchors:
  - [abstract] "code-switching prompts causes significant drop on guardrails' performance"
  - [section] "code-switching inputs confuse the guardrails, resulting in a decrement of detection performance"
  - [corpus] "Average neighbor FMR=0.45" (no direct signal on code-switching)
- Break condition: If the guardrail is trained with code-switched examples or uses a stronger multilingual alignment mechanism, this attack vector weakens.

## Foundational Learning

- Concept: Language resource availability and its impact on NLP model performance
  - Why needed here: The paper explicitly compares high-, medium-, and low-resource languages and shows a performance gap tied to data availability.
  - Quick check question: If a language has fewer tokens in the pretraining corpus, what two effects does that have on a multilingual model's performance?
- Concept: In-context learning and policy injection
  - Why needed here: The study shows that adding a dataset-specific policy to the prompt improves performance, demonstrating how in-context examples shape model behavior.
  - Quick check question: What happens to a model's output if the in-context examples contradict its fine-tuning?
- Concept: Code-switching and cross-lingual reasoning
  - Why needed here: The experiments reveal that mixing languages within a prompt degrades guardrail detection, highlighting limitations in cross-lingual reasoning.
  - Quick check question: Why might a model trained on parallel monolingual data struggle with code-switched inputs?

## Architecture Onboarding

- Component map: Input preprocessing → Guardrail classifier (decoder-only LLM) → Output (safe/unsafe binary)
- Critical path: Input → Translation → Guardrail inference → Label aggregation → F1 calculation
- Design tradeoffs:
  - Using Google Translate ensures broad language coverage but may introduce noise; manual translation would be more accurate but slower.
  - Decoder-only guardrails are flexible for in-context policy injection but may be less stable than fine-tuned classifiers.
  - Evaluating only open-source guardrails limits scope but allows cost control and reproducibility.
- Failure signatures:
  - High false positive rate on low-resource languages → indicates distributional bias in pretraining data.
  - Performance drop on code-switched prompts → indicates weak cross-lingual reasoning.
  - Inconsistent results across datasets → suggests guardrail sensitivity to dataset-specific labeling conventions.
- First 3 experiments:
  1. Run the guardrail on the English subset of each dataset to establish a baseline.
  2. Add a dataset-specific in-context policy and measure F1 change across languages.
  3. Generate code-switched variants of a multilingual test set and evaluate robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can guardrails be made more robust against multilingual jailbreaking prompts, particularly code-switching attacks?
- Basis in paper: [explicit] The paper discusses the vulnerability of guardrails to multilingual jailbreaking prompts and code-switching variants, showing a significant performance drop in detecting toxic content.
- Why unresolved: The study identifies the problem but does not propose solutions or methods to enhance guardrail robustness against such attacks.
- What evidence would resolve it: Development and testing of new guardrail models or techniques that maintain high detection accuracy for both monolingual and code-switching jailbreaking prompts across multiple languages.

### Open Question 2
- Question: What is the impact of in-context safety policies on the performance of guardrails in multilingual toxicity detection?
- Basis in paper: [explicit] The paper evaluates the effect of default versus customized in-context safety policies on guardrail performance, finding that customized policies improve detection.
- Why unresolved: While the study shows that customized policies are beneficial, it does not explore the optimal design or implementation of these policies for different languages or contexts.
- What evidence would resolve it: Comparative analysis of various in-context policy designs and their effectiveness in improving guardrail performance across diverse languages and datasets.

### Open Question 3
- Question: How does the availability of language resources affect the performance of guardrails in detecting multilingual toxic content?
- Basis in paper: [explicit] The paper analyzes the relationship between resource availability and guardrail performance, noting a decrease in effectiveness for low-resource languages.
- Why unresolved: The study identifies a correlation but does not investigate the underlying causes or potential strategies to mitigate resource-related performance issues.
- What evidence would resolve it: Research into the specific factors that contribute to performance disparities and the development of resource-efficient guardrail models that perform well across all language groups.

## Limitations

- Dataset Translation Pipeline: Reliance on Google Translate without validation may introduce noise, especially for low-resource languages.
- Guardrail Implementation Variability: Lack of standardized prompt templates and inference parameters across models may affect cross-model comparisons.
- Code-switching Prompt Generation: AutoDAN-generated prompts may not represent realistic adversarial scenarios, potentially overstating guardrail vulnerabilities.

## Confidence

- High Confidence: Core finding that guardrails underperform on low-resource languages, supported by consistent F1 score trends.
- Medium Confidence: Claim about code-switching reducing guardrail effectiveness, dependent on prompt quality and diversity.
- Medium Confidence: Observation that in-context safety policies improve performance, but may be specific to tested datasets and formulations.

## Next Checks

1. **Translation Quality Validation**: Re-run a subset of experiments using human-verified translations for one low-resource language (e.g., BN) and compare performance to Google Translate outputs to isolate translation effects from guardrail limitations.

2. **Prompt Template Standardization**: Implement identical prompt templates and inference parameters across all guardrail models, then re-evaluate a representative multilingual dataset to determine if observed performance differences are model-specific or implementation-dependent.

3. **Jailbreak Prompt Diversity Analysis**: Generate code-switching prompts using multiple methods (e.g., GPT-4, human-authored, AutoDAN) and evaluate guardrail performance across each set to determine if results are consistent or method-dependent.