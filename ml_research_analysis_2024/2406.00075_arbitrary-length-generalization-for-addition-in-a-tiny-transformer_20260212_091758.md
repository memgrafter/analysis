---
ver: rpa2
title: Arbitrary-Length Generalization for Addition in a Tiny Transformer
arxiv_id: '2406.00075'
source_url: https://arxiv.org/abs/2406.00075
tags:
- input
- numbers
- instances
- addition
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling Transformer models
  to generalize addition of two-digit numbers to numbers with arbitrary digit lengths.
  The proposed method employs an autoregressive generation technique, processing from
  right to left, which mimics the common manual method for adding large numbers.
---

# Arbitrary-Length Generalization for Addition in a Tiny Transformer

## Quick Facts
- arXiv ID: 2406.00075
- Source URL: https://arxiv.org/abs/2406.00075
- Authors: Alexandre Galvao Patriota
- Reference count: 1
- Primary result: 100% accuracy on addition tasks with numbers up to 1000 digits using a tiny decoder-only Transformer

## Executive Summary
This paper addresses the challenge of enabling Transformer models to generalize addition of two-digit numbers to numbers with arbitrary digit lengths. The proposed method employs an autoregressive generation technique, processing from right to left, which mimics the common manual method for adding large numbers. This approach uses specially designed training instances of two types: ones focusing on single-digit addition and ones incorporating carry-over information for multi-digit addition. The model was trained using a decoder-only Transformer configuration and tested on addition tasks involving numbers up to one thousand digits. The model achieved 100% accuracy across all test scenarios, demonstrating its effectiveness in generalizing addition for numbers of varying digit lengths.

## Method Summary
The approach uses a decoder-only Transformer with custom masking to perform autoregressive generation from right to left. The model is trained on two types of instances: first-type instances for single-digit addition and second-type instances that incorporate carry-over information. During inference, the model processes numbers iteratively, taking previous partial sums and carry information as input to generate the next digit of the result. The architecture uses 64-dimensional embeddings, 2 attention heads, and a vocabulary of 14 tokens including digits, special markers for partial sums (P), carries (C), and results (S).

## Key Results
- Achieved 100% accuracy on addition tasks with numbers up to 1000 digits
- Demonstrated effective generalization from two-digit training data to arbitrary-length numbers
- Validated the autoregressive right-to-left generation approach for multi-digit addition
- Showed that carefully designed training instances enable learning of carry propagation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model generalizes to arbitrary-length addition by learning to process numbers in right-to-left autoregressive chunks, mimicking manual addition.
- Mechanism: Each autoregressive step takes the previous partial sum plus carry as input, along with the next two digits, and outputs the next digit(s) of the result. This recursive chunking preserves the structure of manual addition.
- Core assumption: The model can learn the mapping from "partial sum + carry + next two digits" to "next result digit(s)" without explicit multi-digit training examples.
- Evidence anchors:
  - [abstract]: "The proposed approach employs an autoregressive generation technique, processing from right to left, which mimics a common manual method for adding large numbers."
  - [section 4]: "As the training targets contain the intermediate sums (with carry-overs) rather than the final addition, the model should be extensively trained on instances of the first and second type to minimize the risk of missing intermediate predictions during token generation."
  - [corpus]: Weak anchor — neighboring papers discuss length generalization but not this exact chunking approach.
- Break condition: If the model cannot correctly predict intermediate carry values, the chaining breaks and no further digits can be computed.

### Mechanism 2
- Claim: Two types of training instances teach the model both single-digit addition and conditional carry handling.
- Mechanism: First-type instances teach basic single-digit sums. Second-type instances teach conditional carry propagation by embedding prior output + carry as context for the next addition step.
- Core assumption: The model can generalize from these two controlled cases to arbitrary lengths without additional architectural tricks like positional embeddings.
- Evidence anchors:
  - [section 2]: "Instances of the first type essentially illustrate the initial step in an addition task. To facilitate the model's learning of the generalization process, additional instances are required... referred to as 'instances of the second type.'"
  - [section 2.2]: "the input combines the previous outputs, which includes the carry-over information, with the next two single-digit numbers needed to be summed, and the target is the resulting addition with the carry-over."
  - [corpus]: Weak anchor — other work focuses on positional embeddings, not training-instance structure.
- Break condition: If the second-type instances are too sparse or incorrectly formatted, the model never learns the conditional carry logic.

### Mechanism 3
- Claim: The decoder-only Transformer with a custom masking scheme can autoregressively generate the correct sequence of digits without parallel decoding tricks.
- Mechanism: The model is trained on separate input/output tensors (not concatenated), with masking shaped to allow next-token prediction from partial outputs. This yields a clean autoregressive generation loop.
- Core assumption: Standard decoder-only architecture with modified attention mask is sufficient; no need for GPT-like concatenated pretraining.
- Evidence anchors:
  - [section 3]: "As the input does not require masking, the attention mask is modified to accommodated this feature; see the Figure 1 below."
  - [section 3]: "Although it is possible to employ a GPT-like model by concatenating inputs and targets into a single long string and training the model to predict the next token in a typical fashion, this approach is not guaranteed to work effectively and is not adopted in this work."
  - [corpus]: Weak anchor — other work uses standard GPT-style pretraining for arithmetic.
- Break condition: If the mask shape is incorrect, the model may peek ahead or lose context, breaking autoregressive generation.

## Foundational Learning

- Concept: Single-digit addition facts
  - Why needed here: First-type training instances rely on the model learning basic sums (e.g., 8+3=11) before combining them with carry logic.
  - Quick check question: Can the model predict the correct output for "8+3" before any second-type instances are shown?

- Concept: Carry propagation in multi-digit addition
  - Why needed here: Second-type instances explicitly teach the conditional addition of a carry to the next digit pair.
  - Quick check question: Does the model correctly handle "18C98 → 18S" where the carry is included in the output?

- Concept: Autoregressive sequence generation
  - Why needed here: The final inference loop repeatedly feeds previous outputs back into the model to generate the next digit(s).
  - Quick check question: Given the partial output "10S" from "55", can the model correctly process the next input "10C86"?

## Architecture Onboarding

- Component map:
  Input embedding -> Attention (2-head, custom mask) -> FFN (256 hidden dim, 2 layers) -> Output projection (14-dim vocab) -> Autoregressive generation loop

- Critical path:
  1. Prepare input x and initial output y (\n token)
  2. Concatenate vertically -> z
  3. Embed and add positional encoding
  4. Apply masked attention (unmasked only for y part)
  5. FFN pass
  6. Linear projection to vocab
  7. Generate next token autoregressively until S

- Design tradeoffs:
  - Custom masking avoids concatenation overhead but requires careful shape handling.
  - Small vocab (14 tokens) simplifies embedding but forces compact encoding of carry logic.
  - Two training instance types balance data efficiency and generality.

- Failure signatures:
  - Mask shape errors -> model can see future tokens -> generation breaks.
  - Insufficient second-type instances -> model never learns carry propagation -> 0% accuracy on multi-digit.
  - FFN size too small -> underfitting on intermediate sums.

- First 3 experiments:
  1. Train on first-type only -> test on 2-digit addition -> expect failure (no carry logic).
  2. Train on balanced first+second types -> test on 10-digit addition -> expect 100% accuracy.
  3. Remove custom mask -> use standard GPT concat -> compare accuracy and convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimal information required in an input of the second type to achieve perfect accuracy in addition tasks?
- Basis in paper: [explicit] The paper suggests that the optimal number of second-type instances needed for perfect accuracy across various digit lengths could be explored.
- Why unresolved: The paper does not provide an experimental analysis of the minimal information required in the second-type inputs for perfect accuracy.
- What evidence would resolve it: Conducting experiments to systematically vary the information in second-type inputs and measure the resulting accuracy would provide insights into the minimal requirements.

### Open Question 2
- Question: How does the speed of the learning process scale with the length of the numbers being added?
- Basis in paper: [inferred] The paper mentions that the speed of the learning process was not evaluated due to a lack of computational resources, suggesting it as a potential area for future research.
- Why unresolved: The paper did not perform experiments to assess the learning speed for different digit lengths due to resource constraints.
- What evidence would resolve it: Running the training process on larger datasets with varying digit lengths and measuring the time required for convergence would provide insights into the scalability of the learning process.

### Open Question 3
- Question: Can the autoregressive generation process be reduced to achieve the final answer more efficiently?
- Basis in paper: [explicit] The paper suggests investigating whether it is possible to reduce the number of autoregressive generations needed to attain the final answer.
- Why unresolved: The paper does not explore methods to optimize the autoregressive generation process.
- What evidence would resolve it: Developing and testing algorithms that minimize the number of autoregressive steps while maintaining accuracy would provide evidence for potential optimizations.

## Limitations

- Dataset construction assumptions not empirically validated - the paper asserts both training instance types are necessary but doesn't test what happens if either type is removed
- No ablation study comparing custom masking approach to standard GPT-style concatenation
- Evaluation limited to addition only, with no evidence of generalization to other arithmetic operations
- Single training run reported without variance measurements or multiple random seeds

## Confidence

**High Confidence**: The basic mechanism of right-to-left autoregressive generation for addition is sound and well-established in manual arithmetic.

**Medium Confidence**: The claim that this specific architecture (small decoder-only Transformer with custom masking) achieves 100% accuracy on arbitrary-length addition.

**Low Confidence**: The assertion that this approach would generalize to other arithmetic operations or that the specific training instance structure is uniquely optimal.

## Next Checks

1. **Ablation Study on Training Instance Types** - Systematically remove either first-type or second-type instances from training and measure accuracy degradation on 10-digit addition tasks. This would validate whether both instance types are truly necessary.

2. **GPT-Style Concatenation Comparison** - Implement the standard GPT approach (concatenating input and target into a single string) with identical model architecture and training parameters. Compare convergence speed, final accuracy, and inference efficiency against the proposed custom masking approach.

3. **Extended Arithmetic Operations** - Apply the same methodology to subtraction and multiplication tasks, measuring accuracy on numbers up to 100 digits. This would test whether the autoregressive chunking approach generalizes beyond addition to other arithmetic operations that require different handling of intermediate results.