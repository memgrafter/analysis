---
ver: rpa2
title: 'Hierarchical Mixture of Experts: Generalizable Learning for High-Level Synthesis'
arxiv_id: '2410.19225'
source_url: https://arxiv.org/abs/2410.19225
tags:
- graph
- pragma
- expert
- design
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of domain generalization in
  high-level synthesis (HLS) prediction for FPGA design. The authors propose a hierarchical
  Mixture of Experts (MoE) model that leverages the natural three-level granularity
  of HLS graphs: nodes, basic blocks, and the whole graph.'
---

# Hierarchical Mixture of Experts: Generalizable Learning for High-Level Synthesis

## Quick Facts
- arXiv ID: 2410.19225
- Source URL: https://arxiv.org/abs/2410.19225
- Reference count: 18
- 26.6% higher geometric mean speedup compared to HARP in online evaluation

## Executive Summary
This paper addresses domain generalization in high-level synthesis (HLS) prediction for FPGA design by proposing a hierarchical Mixture of Experts (MoE) model. The authors leverage the natural three-level granularity of HLS graphs - nodes, basic blocks, and whole graphs - to create specialized experts at each level. A two-stage training strategy stabilizes training and prevents expert polarization. Experimental results on the HLSyn benchmark demonstrate that the hierarchical MoE significantly outperforms baseline methods, achieving a 26.6% higher geometric mean speedup compared to HARP in online evaluation while showing robustness to low-quality fine-tuning data.

## Method Summary
The hierarchical MoE model applies mixture-of-experts at three natural granularities of HLS graphs: nodes, basic blocks, and whole graphs. Each low-level MoE layer consists of 4 experts operating on its respective granularity, followed by a high-level MoE that aggregates the three outputs. The model uses a two-stage training approach: first training each expert separately, then alternating between joint and individual training to prevent expert polarization. The method is built on top of the HARP baseline and is adaptable to any GNN model. Experiments use the HLSyn benchmark dataset with 42 kernels, evaluating both offline MSE and online FPGA speedup metrics.

## Key Results
- Hierarchical MoE achieves 26.6% higher geometric mean speedup compared to HARP in online evaluation
- Model shows robustness to low-quality fine-tuning data on target kernels
- Experts specialize in different pragma types and kernel structures, improving domain generalizability
- Two-stage training effectively prevents expert polarization during training

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical MoE enables domain generalizability by letting experts specialize in different granularities (nodes, blocks, graphs) of HLS programs. The low-level MoE layers apply mixture-of-experts at three natural granularities of HLS graphs, allowing each expert to learn to handle specific substructures. The high-level MoE aggregates these three representations, letting the gating network learn which granularity is most informative for each kernel.

### Mechanism 2
Two-stage training stabilizes hierarchical MoE by preventing expert polarization during early training. The first stage trains each expert model separately, ensuring individual models learn meaningful representations. The second stage alternates between joint training and individual training, preventing the high-level gating network from assigning all weight to the fastest-converging expert.

### Mechanism 3
Hierarchical aggregation outperforms single-level MoE by combining complementary information from multiple granularities. Single-level MoE on all three granularities simultaneously creates structural conflicts, while hierarchical MoE avoids this by letting each low-level MoE operate independently, then aggregating their outputs at a higher level.

## Foundational Learning

- **Graph Neural Networks for program representation**: HLS prediction operates on control data flow graphs where nodes represent instructions, variables, and constants, and edges represent control and data flow. Quick check: What are the three types of nodes in a ProGraML graph and what do they represent?

- **Domain generalization in machine learning**: The model must perform well on unseen kernels after training on source kernels. Quick check: How does domain generalization differ from traditional supervised learning when applied to HLS prediction?

- **Mixture of Experts (MoE) architecture**: MoE allows different experts to specialize in different regions of the representation space. Quick check: What is expert polarization in MoE and why is it particularly problematic for hierarchical MoE structures?

## Architecture Onboarding

- **Component map**: Input graph → GNN Encoder → Pragma MLP → Low-level MoE → High-level MoE → Output MLP
- **Critical path**: Input graph → GNN Encoder → Pragma MLP → Low-level MoE → High-level MoE → Output MLP
- **Design tradeoffs**: Number of experts (4) vs. model complexity and training stability; Two-stage training vs. simpler end-to-end training; Hierarchical aggregation vs. single-level MoE
- **Failure signatures**: Expert polarization, training instability, poor generalization, resource constraints
- **First 3 experiments**: 1) Train hierarchical MoE with 2 experts instead of 4 to verify sensitivity to expert count, 2) Remove two-stage training to confirm it prevents expert polarization, 3) Replace hierarchical aggregation with single-level MoE on all three granularities to validate architectural design choice

## Open Questions the Paper Calls Out

### Open Question 1
How does the hierarchical MoE's performance scale with the number of source kernels and target kernels? The paper mentions that MAML does not perform well due to many source kernels, but does not explore how hierarchical MoE performance changes with varying numbers of kernels.

### Open Question 2
Can the hierarchical MoE structure be adapted to other types of graph neural network models beyond HARP? The paper states that the hierarchical MoE can be flexibly adapted to any GNN model, but does not provide empirical evidence beyond HARP.

### Open Question 3
How does the hierarchical MoE handle domain shifts that are not related to shared substructures within the code? The paper discusses leveraging similarities in substructures for domain generalization, but does not address domain shifts unrelated to these substructures.

## Limitations
- Limited empirical evidence for why hierarchical MoE outperforms single-level MoE beyond a single ablation study
- Claims about expert specialization lack quantitative metrics measuring effectiveness
- Two-stage training methodology's necessity is assumed rather than rigorously validated

## Confidence
- **High**: Hierarchical MoE improves domain generalizability compared to baseline HARP (26.6% geometric mean speedup)
- **Medium**: Two-stage training prevents expert polarization in hierarchical MoE
- **Low**: Structural conflicts prevent single-level MoE from working on multiple granularities

## Next Checks
1. **Expert Specialization Analysis**: Quantify expert specialization by measuring distribution overlap between experts' learned representations for different pragma types. Compute metrics like Wasserstein distance between expert embeddings for different kernel categories.

2. **Convergence Speed Analysis**: Measure and compare convergence rates of different low-level MoE models during training. Plot gating network weights over time to verify expert polarization risk and validate that two-stage training addresses this.

3. **Cross-Domain Transferability**: Test the trained model on an entirely different HLS benchmark (not just different kernels from the same dataset) to validate true domain generalizability rather than memorization of kernel patterns.