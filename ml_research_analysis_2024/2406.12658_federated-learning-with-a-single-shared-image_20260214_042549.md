---
ver: rpa2
title: Federated Learning with a Single Shared Image
arxiv_id: '2406.12658'
source_url: https://arxiv.org/abs/2406.12658
tags:
- dataset
- distillation
- training
- single
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses federated learning with limited shared dataset
  budgets, proposing a method that relies on a single shared image for knowledge distillation.
  The core idea is to generate a distillation dataset from the single image using
  augmentations and then apply a novel adaptive dataset pruning algorithm to select
  the most informative crops.
---

# Federated Learning with a Single Shared Image

## Quick Facts
- arXiv ID: 2406.12658
- Source URL: https://arxiv.org/abs/2406.12658
- Authors: Sunny Soni; Aaqib Saeed; Yuki M. Asano
- Reference count: 40
- Primary result: Outperforms using multiple individual images of equivalent size under the same storage budget for federated learning with a single shared image.

## Executive Summary
This paper proposes a federated learning method that uses a single shared image for knowledge distillation instead of requiring a large shared dataset. The approach generates augmented patches from one image and applies an adaptive pruning algorithm to select the most informative samples. The method demonstrates superior performance compared to using multiple independent images of equivalent size, achieving 74.8% accuracy on CIFAR100 with 0.5M pixels from a single image versus 73.2% with 500 CIFAR100 samples. The approach extends to heterogeneous client architectures through client-model mirroring and non-uniform distillation schedules.

## Method Summary
The method generates a distillation dataset from a single image using deterministic augmentations (rotations, flips, color transforms) to create diverse patches. An adaptive dataset pruning algorithm then selects the most informative crops through KMeans-based class balancing followed by entropy-based pruning. During federated training, clients perform local supervised learning and send predictions on the pruned patch dataset to the server, which performs knowledge distillation using weighted predictions. The approach extends to heterogeneous architectures by maintaining server-side copies of each client architecture type and using non-uniform distillation schedules.

## Key Results
- Achieves 74.8% test accuracy with 0.5M pixels from a single image versus 73.2% with 0.5M pixels from 500 CIFAR100 samples
- Adaptive pruning significantly improves distillation quality compared to using all patches
- Method extends effectively to heterogeneous client architectures with non-uniform distillation schedules
- Outperforms FedDF baseline across various conditions including non-IID data distributions

## Why This Works (Mechanism)

### Mechanism 1
A single image with augmentation and pruning can serve as a proxy distillation dataset that outperforms multiple independent images of equivalent size. The method generates many small patches from a single image using deterministic augmentations, then applies KMeans-based class balancing and entropy-based pruning to select the most informative patches for each training round, dynamically improving subset selection as the global model evolves.

### Mechanism 2
Heterogeneous client architectures can be trained effectively using client-model mirroring and a non-uniform distillation schedule. The server maintains a copy of each client architecture type and performs knowledge distillation using the weighted average of client predictions, with distillation steps scheduled non-uniformly to accommodate different model complexities.

### Mechanism 3
Patch selection mechanisms (KMeans balancing + entropy pruning) improve distillation quality compared to no selection. KMeans clusters embeddings to ensure balanced class coverage, then entropy pruning removes low-confidence samples to focus training on informative examples. The combination adapts as the model improves.

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: Transfers knowledge from multiple client models to a central model using predictions on a shared dataset
  - Quick check question: How does distillation differ from direct parameter averaging in federated learning?

- Concept: Data augmentation
  - Why needed here: Generates diverse patches from a single image to serve as proxy distillation data
  - Quick check question: What types of augmentations are used to maximize patch diversity while preserving label semantics?

- Concept: Federated learning heterogeneity
  - Why needed here: Clients may have different data distributions and model architectures; the method must handle both
  - Quick check question: How does non-IID data distribution affect convergence in standard federated learning?

## Architecture Onboarding

- Component map: Client side (supervised training → predictions on shared patches → send to server) -> Server side (maintain model copies → distillation training → send updated model to clients) -> Shared component (single image patch generation + pruning algorithm)

- Critical path: 1. Generate patch dataset from single image (once) 2. For each round: clients train locally → send predictions → server prunes patches → server distills global model → send to clients

- Design tradeoffs: Single image vs multiple images (storage/bandwidth savings vs potential diversity loss), KMeans vs entropy pruning (balancing class coverage vs focusing on informative samples), uniform vs non-uniform distillation schedule (simplicity vs architecture-specific tuning)

- Failure signatures: Client (poor local accuracy, high variance in predictions), Server (distillation loss plateaus early, pruned patches become repetitive), System (communication bottlenecks if patch dataset too large)

- First 3 experiments: 1. Validate single image patch generation produces sufficient diversity (t-SNE analysis) 2. Test pruning effectiveness on a fixed model (compare with/without pruning) 3. Run full FL loop with homogeneous clients, compare accuracy vs FedDF baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed single-image federated learning method scale with image resolution and complexity? The paper mentions using different images as sources but does not extensively explore how image characteristics impact performance.

### Open Question 2
What is the impact of different augmentation strategies on the performance of the federated learning model using a single shared image? The paper establishes a baseline using specific augmentations but does not investigate alternative strategies.

### Open Question 3
How does the proposed method perform in federated learning scenarios with extremely heterogeneous client architectures beyond those tested? The paper extends to heterogeneous clients but limits experimentation to a specific set of architectures.

## Limitations

- Long-term stability of patch diversity from augmentations remains uncertain, especially for complex datasets
- Adaptive pruning effectiveness depends heavily on clustering quality and entropy reliability, which may degrade as model predictions converge
- Scalability and performance bounds for the heterogeneous client extension are unclear with minimal experimental coverage

## Confidence

- High confidence: Core mechanism of using single image with augmentations and pruning for homogeneous FL settings
- Medium confidence: Effectiveness of adaptive pruning algorithm (limited ablation studies)
- Low confidence: Scalability of heterogeneous client extension and performance guarantees

## Next Checks

1. Run t-SNE visualizations on augmented patches across multiple rounds to verify sustained diversity and class separation
2. Perform ablation studies varying KMeans cluster numbers and entropy removal thresholds to identify robust parameter ranges
3. Test the heterogeneous extension with 3+ different client architectures to evaluate convergence stability and performance consistency