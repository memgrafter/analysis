---
ver: rpa2
title: Memory-Enhanced Neural Solvers for Routing Problems
arxiv_id: '2406.16424'
source_url: https://arxiv.org/abs/2406.16424
tags:
- memento
- policy
- instances
- pomo
- budget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEMENTO is a memory-based approach for adapting neural solvers
  in combinatorial optimization problems. It dynamically adjusts the action distribution
  of a base policy by retrieving and processing relevant data from a memory of past
  attempts.
---

# Memory-Enhanced Neural Solvers for Routing Problems

## Quick Facts
- **arXiv ID**: 2406.16424
- **Source URL**: https://arxiv.org/abs/2406.16424
- **Reference count**: 32
- **Primary result**: MEMENTO outperforms policy-gradient fine-tuning (EAS) on TSP and CVRP benchmarks, achieving state-of-the-art performance on 11 out of 12 tasks

## Executive Summary
MEMENTO introduces a memory-based approach for adapting neural solvers in combinatorial optimization problems. The method dynamically adjusts the action distribution of a base policy by retrieving and processing relevant data from a memory of past attempts. This model-agnostic approach can be combined with existing solvers like POMO and COMPASS without retraining, and demonstrates significant performance improvements on TSP and CVRP benchmarks across various instance sizes.

## Method Summary
MEMENTO uses a memory module to store action trajectories from previous solution attempts, which are then retrieved and processed to generate correction logits that adjust the base policy's action distribution. The method employs an MLP to learn update rules that map action features (log-probability, return, remaining budget) to scalar weights, creating a learned policy gradient that outperforms classic REINFORCE-style updates. MEMENTO is trained in a budgeted multi-shot setting and can be applied to any pre-trained policy without architectural modifications.

## Key Results
- Achieves state-of-the-art performance on 11 out of 12 TSP and CVRP benchmark tasks
- Demonstrates scalability to larger instances (up to 500 nodes) compared to existing methods
- Shows superior data-efficiency, especially under low-budget scenarios where fewer solution attempts are allowed
- Outperforms policy-gradient fine-tuning (EAS) across all tested benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MEMENTO dynamically adjusts action distributions by retrieving relevant data from a memory of past attempts
- **Mechanism**: During decision-making, MEMENTO retrieves memory entries associated with the current node, normalizes their features, processes them through an MLP to generate correction logits, and adds these to the base policy's logits
- **Core assumption**: The memory contains useful, generalizable information about past actions that can inform future decisions
- **Evidence anchors**: Abstract states it "dynamically adjusts the action distribution of a base policy by retrieving and processing relevant data from a memory of past attempts"; section describes how "data is retrieved from the memory and processed to create action logits that are summed with the logits output by the base model"
- **Break condition**: If memory entries become outdated or irrelevant, the retrieval mechanism may provide misleading signals, degrading performance

### Mechanism 2
- **Claim**: MEMENTO learns expressive update rules that outperform classic policy gradient updates like REINFORCE
- **Mechanism**: The memory network (MLP) learns to map action features into scalar weights that modulate action logits, implementing learned, asymmetric update rules that adjust exploration/exploitation based on budget constraints
- **Core assumption**: The meta-learned update rule can generalize across instances and budgets better than fixed gradient rules
- **Evidence anchors**: Abstract mentions "meta-learn expressive update rules, which prove experimentally to outperform classic policy gradient updates"; section observes "main rules learned by MEMENTO are similar to REINFORCE: an action with low logp and high return gets a positive update while an action with high logp and low return gets discouraged"
- **Break condition**: If the MLP capacity is insufficient or the training budget is too low, the learned update rules may collapse to trivial or harmful patterns

### Mechanism 3
- **Claim**: MEMENTO is model-agnostic and can be combined with existing solvers without retraining
- **Mechanism**: By augmenting any pre-trained policy with a memory module and update MLP, MEMENTO can be stacked on top of solvers like POMO or COMPASS, operating at inference time without altering the base policy's architecture
- **Core assumption**: The correction mechanism is sufficiently general to work across different base policies and problem domains
- **Evidence anchors**: Abstract states it is "model-agnostic and can be combined with existing solvers like POMO and COMPASS"; section notes "MEMENTO was designed to be agnostic to the base model, enabling combination with existing and future solvers"
- **Break condition**: If the base policy's output distribution is too dissimilar from the training data, the correction may be ineffective or destabilizing

## Foundational Learning

- **Concept: Memory-augmented reinforcement learning**
  - **Why needed here**: MEMENTO relies on storing and retrieving past trajectories to adapt policies online; understanding how memory modules integrate with RL is essential for grasping its design
  - **Quick check question**: How does the memory retrieval mechanism differ from standard experience replay in off-policy RL?

- **Concept: Transformer-based policy architectures**
  - **Why needed here**: The base solvers (POMO, COMPASS) use transformer models; knowing how attention and positional encoding work is necessary to understand how MEMENTO modifies logits
  - **Quick check question**: In a transformer decoder, what role do the logits play before sampling, and how can they be safely perturbed?

- **Concept: Multi-shot optimization and budgeted search**
  - **Why needed here**: MEMENTO is evaluated in a setting where multiple solution attempts are allowed per instance; understanding this differs from standard one-shot RL training
  - **Quick check question**: Why is a multi-shot setting more appropriate for NP-hard routing problems than a single-shot approach?

## Architecture Onboarding

- **Component map**: Base policy (POMO/COMPASS) -> Memory module (stores action tuples) -> MLP update network (processes features) -> Logit correction (weighted average) -> Adjusted action distribution

- **Critical path**:
  1. Given current state, retrieve memory entries for current node
  2. Normalize features, append remaining budget
  3. Feed through MLP to get weights per action
  4. Compute weighted average of actions → correction logits
  5. Add to base policy logits → sample next action
  6. After rollout, store transitions in memory for future use

- **Design tradeoffs**:
  - Memory size vs. retrieval speed: Larger memory stores more context but increases computation
  - MLP capacity vs. overfitting: Too large may memorize training data; too small may underfit
  - Update frequency vs. stability: Frequent updates adapt faster but may destabilize early exploration

- **Failure signatures**:
  - Memory not improving: Likely due to insufficient diversity in stored transitions or poor feature normalization
  - Performance degrading over time: Could indicate overfitting of the MLP to early training data
  - No improvement over base policy: May signal that correction logits are too small or memory retrieval is ineffective

- **First 3 experiments**:
  1. Verify that MEMENTO matches base policy performance when memory is empty or untrained
  2. Test that retrieving memory entries changes action distributions compared to base policy alone
  3. Measure performance gain when gradually increasing memory size on a fixed benchmark (e.g., TSP100)

## Open Questions the Paper Calls Out

- **Open Question 1**: How does MEMENTO's performance compare to other memory-based methods beyond EAS, such as those using tree search or other meta-optimizer approaches?
  - **Basis in paper**: The paper mentions that Dernedde et al. (2024) introduces a meta-optimizer but notes it is tied to a heatmap-based technique, preventing its applicability to certain CO problems like CVRP
  - **Why unresolved**: The paper focuses on comparing MEMENTO to EAS and COMPASS, and while it mentions other methods, it does not provide a direct comparison with them
  - **What evidence would resolve it**: A direct comparison of MEMENTO's performance against other memory-based methods like those using tree search or meta-optimizers on the same benchmark problems (TSP and CVRP)

- **Open Question 2**: How does MEMENTO's performance scale with problem size beyond 500 nodes, and what are the computational limits of the method?
  - **Basis in paper**: The paper demonstrates MEMENTO's performance on instances up to 500 nodes and mentions the use of Efficient Attention to reduce memory costs, but it does not explore scaling beyond this size
  - **Why unresolved**: The paper focuses on demonstrating scalability to 500 nodes but does not provide insights into how the method performs on even larger instances or what computational limits might exist
  - **What evidence would resolve it**: Testing MEMENTO on problem instances larger than 500 nodes and analyzing the computational resources required

- **Open Question 3**: How does the learned update rule of MEMENTO generalize to other combinatorial optimization problems beyond routing problems like TSP and CVRP?
  - **Basis in paper**: The paper shows that MEMENTO can be combined with COMPASS and achieve state-of-the-art performance on TSP and CVRP, but it does not explore its applicability to other types of CO problems
  - **Why unresolved**: The paper focuses on routing problems and does not provide evidence of MEMENTO's effectiveness on other CO problems such as graph-based problems or job shop scheduling
  - **What evidence would resolve it**: Applying MEMENTO to other types of combinatorial optimization problems and evaluating its performance compared to existing methods

## Limitations

- The exact implementation details of the Efficient Attention mechanism for scaling to larger instances are not fully specified
- Specific hyperparameters for memory size, MLP capacity, and update frequency are not provided in sufficient detail for complete reproduction
- The relationship between training budget and performance is unclear for very large instances beyond the tested 500-node limit

## Confidence

- **High**: The mechanism of memory-based action distribution adjustment and model-agnostic design
- **High**: The empirical superiority over policy-gradient fine-tuning on standard benchmarks
- **Medium**: The scalability claims to 500-node instances without exhaustive validation
- **Medium**: The interpretability of learned update rules and their similarity to REINFORCE

## Next Checks

1. **Ablation on Memory Components**: Systematically remove the memory module and MLP update network to verify that the combined system provides performance gains beyond the base policy alone, confirming that both components are necessary for the reported improvements.

2. **Cross-Domain Generalization**: Evaluate MEMENTO on routing problems with different characteristics (e.g., non-uniform distributions, different constraints) to test whether the learned update rules generalize beyond the training distribution, addressing the break condition where memory entries may become outdated.

3. **Memory Size Sensitivity Analysis**: Conduct experiments varying memory capacity from very small to very large to determine the optimal tradeoff between storage cost and performance improvement, and to identify potential overfitting patterns as memory grows.