---
ver: rpa2
title: 'ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented
  Generator'
arxiv_id: '2405.18111'
source_url: https://arxiv.org/abs/2405.18111
tags:
- generator
- language
- attacker
- documents
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the vulnerability of retrieval-augmented generation
  (RAG) systems to noisy and fabricated content in retrieved documents, which can
  lead to incorrect responses. To tackle this, the authors propose an Adversarial
  Tuning Multi-agent system (ATM) that improves the robustness of the generator by
  using two agents: an Attacker that generates misleading fabrications and a Generator
  that learns to resist these perturbations.'
---

# ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator

## Quick Facts
- arXiv ID: 2405.18111
- Source URL: https://arxiv.org/abs/2405.18111
- Reference count: 40
- Primary result: ATM improves RAG robustness by 5-10% in Exact Match score on knowledge-intensive question-answering datasets through adversarial training between an Attacker and Generator.

## Executive Summary
This paper addresses the vulnerability of retrieval-augmented generation (RAG) systems to noisy and fabricated content in retrieved documents, which can lead to incorrect responses. The authors propose an Adversarial Tuning Multi-agent system (ATM) that improves robustness by using two agents: an Attacker that generates misleading fabrications and a Generator that learns to resist these perturbations. The two agents are optimized iteratively through adversarial tuning, with the Attacker generating fabrications that challenge the Generator, and the Generator learning to remain robust and provide correct answers. Experiments on knowledge-intensive question-answering datasets show that ATM significantly improves the robustness and performance of the generator, achieving better results compared to state-of-the-art baselines.

## Method Summary
ATM consists of two agents: an Attacker and a Generator. The Attacker generates fabricated documents that maximize the Generator's perplexity for correct answers, while the Generator learns to maximize generation probability of the golden answer regardless of fabrications through a novel Multi-agent Iterative Tuning Optimization (MITO) loss. The two agents undergo iterative adversarial optimization, with the Attacker creating increasingly misleading fabrications and the Generator learning to resist these perturbations. This creates a feedback loop that progressively strengthens both components, ultimately resulting in a robust Generator that can discriminate useful documents amongst fabrications.

## Key Results
- ATM improves Exact Match (EM) score by 5-10% on knowledge-intensive question-answering datasets compared to state-of-the-art baselines.
- The method demonstrates stability against varying numbers of fabrications, maintaining performance across different perturbation levels.
- ATM shows effectiveness across different types of generators, indicating its generalizability to various RAG system configurations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ATTACKER generates fabricated documents that maximize the Generator's perplexity for correct answers, thereby forcing the Generator to learn robustness against misleading information.
- Mechanism: The ATTACKER is aligned using Direct Preference Optimization (DPO) where the reward signal is the Generator's perplexity (PPL) on the correct answer given a fabricated document. Higher PPL indicates greater confusion, which is treated as a positive reward for the ATTACKER.
- Core assumption: Perplexity of the Generator on correct answers is a reliable proxy for the misleading strength of fabricated documents.
- Evidence anchors:
  - [abstract] "The ATM steers the Generator to have a robust perspective of useful documents for question answering with the help of an auxiliary Attacker agent through adversarially tuning the agents for several iterations."
  - [section 4.2] "We calculate the PPL of Generator generating correct answer a as follows: PPLG(a | q, {d′}) = exp{−1/Ta Σ log PG(at | a<t; q, {d′})}"
  - [corpus] Weak - no direct corpus evidence for this specific perplexity-based reward mechanism.

### Mechanism 2
- Claim: The Generator learns to maximize generation probability of the golden answer regardless of fabrications being injected through a novel Multi-agent Iterative Tuning Optimization (MITO) loss.
- Mechanism: The Generator is optimized with a loss that combines standard SFT loss on attacked documents plus KL divergence between generation probabilities on original vs attacked document lists. This encourages consistent golden answer generation across both contexts.
- Core assumption: Minimizing KL divergence between generation distributions on original and attacked documents ensures the Generator maintains focus on golden answers despite noise.
- Evidence anchors:
  - [abstract] "After rounds of multi-agent iterative tuning, the Generator can eventually better discriminate useful documents amongst fabrications."
  - [section 4.2] "LMITO = LSFT(a |q, D′) + αLKL, LKL = Σ DKL[PG(at | a<t; q, D) || PG(at | a<t; q, D′)]"
  - [corpus] Weak - no direct corpus evidence for this specific MITO formulation.

### Mechanism 3
- Claim: Iterative adversarial tuning leads to convergence where both agents improve their capabilities - the Attacker becomes more aggressive and the Generator becomes more robust.
- Mechanism: The two agents are optimized in alternating rounds, with the Attacker generating increasingly misleading fabrications and the Generator learning to resist these perturbations. This creates a feedback loop that progressively strengthens both components.
- Core assumption: Adversarial training between two agents can lead to mutual capability enhancement without one agent overwhelming the other.
- Evidence anchors:
  - [abstract] "Through rounds of adversarial tuning as described above, we end up with an aggressive Attacker with strong attacking patterns and a robust Generator generating stably and correctly."
  - [section 4.2] "After initialization, two agents undergo iteratively adversarial optimization."
  - [corpus] Moderate - related work exists on adversarial training but not specifically for this RAG robustness application.

## Foundational Learning

- Concept: Adversarial training
  - Why needed here: To create a robust Generator that can handle noisy and fabricated documents in RAG systems.
  - Quick check question: What is the primary goal of using an adversarial approach in this context?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: To align the Attacker to generate more misleading fabrications based on feedback from the Generator's perplexity.
  - Quick check question: How does DPO differ from standard reinforcement learning in this application?

- Concept: Kullback-Leibler divergence
  - Why needed here: To measure the difference in the Generator's output distribution between original and attacked document lists, encouraging consistency.
  - Quick check question: Why is KL divergence used instead of other distance metrics in the MITO loss?

## Architecture Onboarding

- Component map:
  Attacker (LLM) -> Generator (RAG LLM) -> Retriever (Contriever) -> Knowledge base

- Critical path:
  1. Query and documents retrieved from knowledge base
  2. Attacker generates fabrications and permutes document list
  3. Generator produces answer given original and attacked lists
  4. Perplexity calculated as reward signal
  5. Both agents updated through respective optimization steps

- Design tradeoffs:
  - Memory vs performance: Full-parameter fine-tuning provides better results but requires more resources
  - Attack strength vs stability: Stronger attacks improve robustness but risk overwhelming the Generator
  - Number of iterations: More iterations generally improve performance but increase training time

- Failure signatures:
  - Generator consistently fails to answer questions even with original documents (Attacker too strong)
  - Generator answers correctly regardless of fabrications (Attacker too weak or Generator overfit)
  - Training instability or divergence (hyperparameter misconfiguration)

- First 3 experiments:
  1. Baseline evaluation: Run Generator on original documents only to establish performance floor
  2. Attacker strength test: Generate fabrications and measure Generator's performance drop
  3. Iterative tuning validation: Run 1-2 iterations and compare performance to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed ATM method perform when applied to different types of retriever models, such as sparse vs. dense retrievers, or traditional vs. learned retrievers?
- Basis in paper: [inferred]
- Why unresolved: The paper primarily focuses on the effectiveness of ATM with a single retriever model (Contriever). It does not explore how the method's performance might vary when using different types of retrievers, such as sparse or learned retrievers. This could be important for understanding the generalizability of ATM across different retrieval systems.
- What evidence would resolve it: Conducting experiments with various retriever models and comparing the performance of ATM across these different setups would provide evidence for how well ATM generalizes to different retrieval approaches.

### Open Question 2
- Question: What is the impact of varying the number of fabrications generated by the ATTACKER on the robustness of the GENERATOR? Does increasing the number of fabrications always lead to better robustness, or is there a point of diminishing returns?
- Basis in paper: [explicit]
- Why unresolved: While the paper discusses the stability of the GENERATOR against different numbers of fabrications, it does not provide a comprehensive analysis of how the number of fabrications affects the overall robustness of the model. It is unclear if there is an optimal number of fabrications that maximizes robustness or if too many fabrications could potentially harm the model's performance.
- What evidence would resolve it: Conducting experiments with varying numbers of fabrications and analyzing the performance of the GENERATOR at each level would provide insights into the relationship between the number of fabrications and the robustness of the model.

### Open Question 3
- Question: How does the ATM method compare to other robustness-aware techniques in the literature, such as adversarial training or data augmentation, when applied to retrieval-augmented generation systems?
- Basis in paper: [inferred]
- Why unresolved: The paper compares ATM to some existing robustness-aware techniques, but it does not provide a comprehensive comparison with other methods like adversarial training or data augmentation. It is unclear how ATM stacks up against these alternative approaches in terms of improving the robustness of retrieval-augmented generation systems.
- What evidence would resolve it: Conducting a thorough comparison of ATM with other robustness-aware techniques, using the same datasets and evaluation metrics, would provide insights into the relative effectiveness of these methods.

## Limitations
- The perplexity-based reward signal for the Attacker relies on a single scalar metric that may not fully capture the misleading strength of fabrications.
- The MITO loss formulation may create an overly constrained optimization space that limits the Generator's flexibility.
- The adversarial training dynamics depend heavily on the relative strength of the two agents, with limited analysis of failure modes when one agent becomes too dominant.

## Confidence

**High**: The overall approach of using adversarial training to improve RAG robustness is sound and well-established in the broader ML literature.

**Medium**: The specific implementation details (perplexity-based rewards, MITO loss formulation) are internally consistent but lack extensive validation against alternatives.

**Low**: The long-term stability of the iterative training process and its behavior on real-world, noisy data remains largely theoretical.

## Next Checks

1. **Reward Signal Validation**: Replace perplexity with an alternative reward signal (e.g., answer accuracy drop or human evaluation) and measure if ATM still converges to robust solutions.

2. **Hyperparameter Sensitivity**: Systematically vary α (KL weight) and observe the trade-off between robustness and performance on clean data across multiple random seeds.

3. **Real-World Transfer**: Test ATM on a dataset with naturally occurring document noise (e.g., from web search) rather than synthetic fabrications to assess practical utility.