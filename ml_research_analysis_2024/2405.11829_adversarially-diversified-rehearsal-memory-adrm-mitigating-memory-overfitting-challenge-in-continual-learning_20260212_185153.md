---
ver: rpa2
title: 'Adversarially Diversified Rehearsal Memory (ADRM): Mitigating Memory Overfitting
  Challenge in Continual Learning'
arxiv_id: '2405.11829'
source_url: https://arxiv.org/abs/2405.11829
tags:
- memory
- learning
- adrm
- samples
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of rehearsal memory overfitting
  in continual learning, where models become overly specialized on limited memory
  samples and lose generalization ability. The proposed method, Adversarially Diversified
  Rehearsal Memory (ADRM), addresses this by employing Fast Gradient Sign Method (FGSM)
  attacks to introduce adversarial perturbations to memory samples.
---

# Adversarially Diversified Rehearsal Memory (ADRM): Mitigating Memory Overfitting Challenge in Continual Learning

## Quick Facts
- arXiv ID: 2405.11829
- Source URL: https://arxiv.org/abs/2405.11829
- Authors: Hikmat Khan; Ghulam Rasool; Nidhal Carla Bouaynaya
- Reference count: 40
- Primary result: ADRM outperforms several established CL methods on CIFAR-10 and achieves comparable results to state-of-the-art approaches while enhancing robustness under natural and adversarial corruptions.

## Executive Summary
This paper addresses rehearsal memory overfitting in continual learning by introducing Adversarially Diversified Rehearsal Memory (ADRM). The method uses Fast Gradient Sign Method (FGSM) attacks to create adversarial perturbations of memory samples, making the rehearsal buffer more diverse and challenging to overfit. Experiments on CIFAR-10 demonstrate that ADRM reduces catastrophic forgetting while improving robustness under both natural corruptions and adversarial conditions, achieving highest average accuracy in 15 out of 19 noise types.

## Method Summary
ADRM modifies standard rehearsal replay by generating adversarial perturbations using FGSM on memory samples. The method maintains a buffer of 1024 examples per task using reservoir sampling, with 10% diversification ratio of adversarially perturbed samples. During training, mini-batches combine current task samples with a mixture of correctly classified and misclassified adversarial memory samples. The approach uses ResNet32 backbone with SGD optimizer, data augmentation, and cross-entropy loss across 200 epochs for first task and 128 for subsequent tasks.

## Key Results
- ADRM outperforms established CL methods (ER, iCaRL, BiC, PODNet, WA, DER, SimpleCIL, FOSTER, FETRIL, MEMO) on CIFAR-10
- Achieves highest average accuracy in 15 out of 19 noise types on CIFAR-10-C and adversarially perturbed CIFAR-10 datasets
- t-SNE visualizations show ADRM learns more stable feature distributions with minimal drift compared to baselines
- CKA similarity analysis reveals ADRM maintains higher feature similarity across tasks, indicating reduced catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial diversification via FGSM prevents overfitting to rehearsal memory by increasing intra-class complexity.
- Mechanism: FGSM generates perturbed samples that push memory examples beyond their original feature clusters while retaining class identity, making them harder to memorize and thus forcing the model to learn more robust, generalizable representations.
- Core assumption: Perturbations that maintain visual coherence but shift feature distributions still preserve task-relevant information for continual learning.
- Evidence anchors: [abstract] "ADRM employs the Fast Gradient Sign Method (FGSM) attacks to introduce adversarial perturbations to memory samples"; [section] "ADRM employs the FGSM to generate adversarially perturbed/diversified memory samples...The resultant diversified memory samples are more complex and challenging to overfit by the CL model."
- Break condition: If perturbations cause samples to drift into adjacent class feature spaces, decision boundaries collapse and forgetting increases.

### Mechanism 2
- Claim: Mixing misclassified and correctly classified adversarial samples improves boundary learning and reduces feature drift.
- Mechanism: Misclassified samples (Md) expose the model to near-boundary regions between classes, enhancing discrimination, while correctly classified samples (Mb) reinforce stable class prototypes.
- Core assumption: Boundary samples contain discriminative information that is not captured by clean or uniformly perturbed memory alone.
- Evidence anchors: [abstract] "The resultant diversified memory samples are more complex and challenging to overfit by the CL model"; [section] "the misclassified samples reveal features more closely associated with the feature distributions of other memory classes or current task classes...This provides a unique opportunity for the CL model to understand the decision boundaries among different tasks."
- Break condition: If the perturbation budget is too high, misclassified samples outnumber correctly classified ones, destabilizing core class representations.

### Mechanism 3
- Claim: Feature stability under adversarial noise correlates with reduced catastrophic forgetting.
- Mechanism: By training with adversarial memory samples, the model learns features that are invariant to common perturbations, making them less sensitive to natural and adversarial corruptions during testing.
- Core assumption: Adversarial training on memory samples induces feature representations that generalize across noise types.
- Evidence anchors: [abstract] "we demonstrated ADRM's ability to enhance CL model robustness under natural and adversarial conditions by using CIFAR10-C and adversarially perturbed CIFAR10 datasets"; [section] "ADRM mitigates feature drifts in CL memory samples...resulting in reduced catastrophic forgetting and a more robust CL model."
- Break condition: If perturbations are too weak, no stability gain; if too strong, the model may latch onto noise artifacts rather than robust features.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: ADRM is explicitly designed to mitigate forgetting caused by rehearsal memory overfitting.
  - Quick check question: What happens to model performance on earlier tasks after training on a new task without rehearsal?

- Concept: Adversarial training and FGSM
  - Why needed here: ADRM uses FGSM to create adversarial perturbations that diversify memory samples.
  - Quick check question: How does FGSM perturbation formula change an input sample, and why does it preserve perceptual similarity?

- Concept: t-SNE for feature visualization
  - Why needed here: The paper uses t-SNE to demonstrate reduced feature drift and better class separation in ADRM vs baselines.
  - Quick check question: What does a "stable" t-SNE plot indicate about feature representation drift over tasks?

## Architecture Onboarding

- Component map: ResNet32 backbone → FGSM generator → Rehearsal memory buffer → Mini-batch sampler (current task + adversarial memory) → Cross-entropy loss
- Critical path: FGSM generation → memory diversification → mixed rehearsal → gradient update
- Design tradeoffs: Higher perturbation strength improves diversity but risks misclassifying too many samples, destabilizing core prototypes; lower strength may not diversify enough to prevent overfitting.
- Failure signatures: 1) Accuracy collapse on early tasks (forgetting); 2) Low diversity in t-SNE plots (overfitting); 3) High overlap between classes under adversarial noise (boundary instability).
- First 3 experiments:
  1. Baseline: Train ER on CIFAR10 split without adversarial diversification; record forgetting curve.
  2. Ablation: Add FGSM with increasing epsilon; measure average accuracy vs epsilon to find optimal perturbation strength.
  3. Robustness test: Evaluate models on CIFAR10-C and adversarially perturbed CIFAR10; compare forgetting under noise vs clean test.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of adversarially diversified memory samples for different continual learning tasks and datasets?
- Basis in paper: [explicit] The paper tested different ratios (10%, 25%, 50%, 75%, 100%) and found 10% optimal for CIFAR-10, but this may vary for other tasks.
- Why unresolved: The study only evaluated on CIFAR-10 dataset. Different datasets with varying complexity, class overlap, and distribution characteristics might require different diversification ratios.
- What evidence would resolve it: Systematic experiments across diverse datasets (different sizes, complexities, and domains) showing how optimal diversification ratios vary with dataset characteristics.

### Open Question 2
- Question: How does the choice of adversarial attack method (beyond FGSM) affect the effectiveness of memory diversification in continual learning?
- Basis in paper: [explicit] The paper only used FGSM for memory diversification, though it mentions PGD attacks were used for evaluation.
- Why unresolved: FGSM is a simple one-step attack. More sophisticated multi-step attacks or different adversarial techniques might create more effective memory diversification.
- What evidence would resolve it: Comparative experiments using various adversarial attack methods (PGD, C&W, DeepFool, etc.) showing their impact on memory diversification effectiveness and model performance.

### Open Question 3
- Question: What is the relationship between feature similarity to ADRM and robustness across different continual learning architectures?
- Basis in paper: [explicit] The paper found higher CKA similarity to ADRM correlates with better adversarial robustness, but only tested on ResNet32.
- Why unresolved: The correlation might be specific to ResNet32 architecture. Other architectures (Vision Transformers, MLPs) might show different relationships between feature similarity and robustness.
- What evidence would resolve it: Experiments across multiple architectures showing whether the observed correlation between feature similarity and robustness generalizes beyond ResNet32.

## Limitations
- The paper does not provide detailed ablation studies on perturbation strength sensitivity across different CIFAR-10 task splits, leaving uncertainty about whether the 10% diversification ratio is optimal for all scenarios.
- While t-SNE visualizations suggest improved feature stability, the quantitative metrics for feature drift (beyond CKA similarity) are not fully specified.
- The computational overhead introduced by FGSM generation during training is not discussed, which may limit practical applicability.

## Confidence

- **High Confidence**: The core claim that ADRM improves robustness under natural and adversarial corruptions is well-supported by experimental results on CIFAR-10-C and adversarially perturbed datasets.
- **Medium Confidence**: The mechanism linking adversarial diversification to reduced catastrophic forgetting is theoretically sound but relies on assumptions about perturbation magnitude and class boundary preservation that could vary across datasets.
- **Low Confidence**: The optimal diversification ratio (10%) and epsilon sampling range are reported without systematic sensitivity analysis or theoretical justification.

## Next Checks

1. **Perturbation Strength Sensitivity**: Systematically vary epsilon ranges and diversification ratios across different task split scenarios (9, 5, 2 tasks) to identify optimal hyperparameters for each configuration.
2. **Cross-Dataset Generalization**: Evaluate ADRM on non-CIFAR datasets (e.g., ImageNet-Subset, TinyImageNet) to test whether the FGSM-based diversification mechanism generalizes beyond small-scale vision tasks.
3. **Computational Overhead Analysis**: Measure and compare wall-clock training time and memory usage between ADRM and baseline rehearsal methods to quantify practical implementation costs.