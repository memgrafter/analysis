---
ver: rpa2
title: 'OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task
  Synthesis'
arxiv_id: '2412.19723'
source_url: https://arxiv.org/abs/2412.19723
tags:
- action
- data
- task
- tasks
- os-genesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OS-Genesis addresses the challenge of collecting high-quality trajectory
  data for training GUI agents, which is typically resource-intensive or limited in
  diversity. The core method idea is to reverse the conventional task-driven trajectory
  collection process by first enabling agents to explore GUI environments through
  interactions, then retrospectively deriving high-quality tasks from these interactions.
---

# OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis

## Quick Facts
- arXiv ID: 2412.19723
- Source URL: https://arxiv.org/abs/2412.19723
- Reference count: 40
- Primary result: OS-Genesis achieves nearly double the success rate (9.82% to 17.41%) on AndroidWorld compared to task-driven methods

## Executive Summary
OS-Genesis addresses the challenge of collecting high-quality trajectory data for training GUI agents, which is typically resource-intensive or limited in diversity. The core method idea is to reverse the conventional task-driven trajectory collection process by first enabling agents to explore GUI environments through interactions, then retrospectively deriving high-quality tasks from these interactions. A trajectory reward model ensures data quality by providing graded evaluations. Experiments show OS-Genesis significantly improves GUI agent performance on challenging benchmarks, achieving nearly double the success rate on AndroidWorld compared to task-driven methods, while also demonstrating superior data diversity and quality.

## Method Summary
OS-Genesis is a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of defining tasks first and then executing them, OS-Genesis enables agents to explore GUI environments through interactions first, then retrospectively derives high-quality tasks from these interactions. The method uses interaction-driven exploration to collect ⟨spre, a, spost⟩ triples, applies reverse task synthesis using GPT-4o to transform these into high-level instructions, and employs a trajectory reward model (TRM) that provides graded evaluations (1-5 scores) rather than binary filtering. This approach preserves incomplete but valuable trajectories while maintaining quality standards, ultimately generating more diverse and representative data than traditional task-driven approaches.

## Key Results
- OS-Genesis achieves nearly double the success rate on AndroidWorld (9.82% to 17.41%) compared to task-driven methods
- The method demonstrates the highest trajectory diversity among synthetic data approaches, with the greatest average distance across both mobile and web environments
- TRM effectiveness is notably demonstrated in enhancing high-level capabilities through graded evaluation of trajectory quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OS-Genesis reverses the conventional task-driven trajectory collection process to improve data quality and diversity
- Mechanism: Instead of defining tasks first and then executing them, OS-Genesis enables agents to explore GUI environments through interactions first, then retrospectively derives high-quality tasks from these interactions. This interaction-driven approach naturally uncovers more functionalities and creates more meaningful task scenarios.
- Core assumption: The exploration-first approach will uncover more diverse and meaningful GUI functionalities than pre-defined task lists
- Evidence anchors: [abstract] "OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process"; [section 3.2] "OS-Genesis leverages collected triplets ⟨spre, a, spost⟩ to construct meaningful task instructions"
- Break condition: If exploration space is too constrained or GPT-4o cannot effectively transform interactions into meaningful high-level tasks, the method fails to generate diverse tasks

### Mechanism 2
- Claim: The Trajectory Reward Model (TRM) improves data quality by providing graded evaluations instead of binary filtering
- Mechanism: TRM evaluates trajectories on completion and coherence metrics, assigning scores from 1-5 rather than simply accepting or rejecting trajectories. This preserves incomplete but valuable trajectories that contain meaningful GUI exploration.
- Core assumption: Even incomplete trajectories contain valuable exploration data that can improve agent training
- Evidence anchors: [section 3.3] "Instead of binary evaluation, we leverage the characteristics of trajectory" and "TRM aims to perform a graded evaluation with a reward score R ∈ [1, 5]"; [section 5.2] "TRM demonstrates effectiveness, notably in enhancing high-level capabilities"
- Break condition: If TRM becomes too lenient and accepts low-quality trajectories, or too strict and discards valuable partial trajectories

### Mechanism 3
- Claim: The combination of interaction-driven exploration and reverse task synthesis creates more diverse and representative data than task-driven approaches
- Mechanism: By first exploring the environment without task constraints, OS-Genesis discovers functional elements that might not be included in pre-defined task lists. The subsequent reverse task synthesis then creates tasks that are grounded in actual GUI functionality.
- Core assumption: Unconstrained exploration will discover more GUI functionalities than task-driven approaches
- Evidence anchors: [section 5.1] "OS-Genesis achieves the greatest average distance across both mobile and web environments among different synthetic data"; [section 5.1] "OS-Genesis demonstrates the highest trajectory diversity"
- Break condition: If exploration becomes too random and fails to systematically cover GUI functionalities, or if reverse task synthesis cannot create meaningful tasks from interactions

## Foundational Learning

- Concept: Trajectory-based training for GUI agents
  - Why needed here: OS-Genesis generates complete trajectories (sequences of states, actions, and instructions) rather than isolated data points, which is essential for training agents that can plan and execute multi-step tasks
  - Quick check question: Why is trajectory-based training more effective than single-step training for GUI agents?

- Concept: Reverse task synthesis
  - Why needed here: This is the core innovation that enables OS-Genesis to generate tasks without human supervision by deriving tasks from interactions rather than defining tasks first
  - Quick check question: How does reverse task synthesis differ from conventional task-driven approaches in terms of data generation order?

- Concept: Reward modeling for data quality
  - Why needed here: TRM provides a more nuanced way to evaluate trajectory quality than binary filtering, preserving valuable partial trajectories while maintaining quality standards
  - Quick check question: What are the advantages of graded reward scoring over binary accept/reject filtering for trajectory data?

## Architecture Onboarding

- Component map: Interaction engine -> Triple collector -> Reverse task synthesizer -> Trajectory generator -> TRM evaluation -> Training pipeline
- Critical path: Exploration → Triple collection → Reverse task synthesis → Trajectory generation → TRM evaluation → Training
- Design tradeoffs:
  - Exploration breadth vs. depth: More random exploration increases diversity but may reduce efficiency
  - TRM granularity vs. simplicity: Graded scoring preserves more data but requires more complex evaluation
  - GPT-4o dependency vs. open-source alternatives: Higher quality but proprietary dependency
- Failure signatures:
  - Low trajectory diversity: Indicates exploration space is too constrained
  - Poor performance on AndroidWorld: Suggests TRM is too lenient or exploration misses critical functionalities
  - High variance in agent performance: Indicates inconsistent data quality from TRM
- First 3 experiments:
  1. Compare trajectory diversity metrics (cosine distance) between OS-Genesis and task-driven baselines
  2. A/B test TRM vs. binary filtering on AndroidWorld success rates
  3. Scale experiment: Train agents with 100, 500, 1000 trajectories to find performance saturation point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does OS-Genesis perform on long-horizon tasks compared to short-horizon tasks, and what is the threshold for task complexity where performance degrades significantly?
- Basis in paper: [inferred] The paper demonstrates OS-Genesis's effectiveness on various benchmarks but doesn't provide a detailed analysis of performance across different task complexities or durations
- Why unresolved: The paper lacks an explicit breakdown of performance based on task length or complexity, making it unclear how well OS-Genesis scales to more complex, multi-step tasks
- What evidence would resolve it: Detailed experimental results showing success rates across tasks of varying lengths and complexities, along with analysis of where performance begins to degrade

### Open Question 2
- Question: What is the impact of the quality and diversity of the annotation model (e.g., GPT-4o) on the final trajectory quality, and how would performance change with different or improved annotation models?
- Basis in paper: [explicit] The paper acknowledges that OS-Genesis relies on GPT-4o for reverse task synthesis and reward modeling, but doesn't explore the sensitivity of the system to the choice of annotation model
- Why unresolved: The paper uses a single annotation model (GPT-4o) without comparing its performance to other models or analyzing how model quality affects the overall system
- What evidence would resolve it: Comparative experiments using different annotation models (including open-source alternatives) and analysis of how annotation model quality correlates with trajectory quality and final agent performance

## Limitations
- The method's heavy dependency on GPT-4o for both reverse task synthesis and reward modeling raises concerns about computational cost and scalability
- Performance on extremely complex or highly customized GUI environments is not fully characterized
- The long-term scalability with respect to dataset size and potential catastrophic forgetting when continuously generating new synthetic data remains uncertain

## Confidence
- **High Confidence**: The core mechanism of reversing task-driven collection to interaction-driven exploration is well-supported by experimental results showing 9.82% to 17.41% improvement on AndroidWorld
- **Medium Confidence**: The generalization capability across different GUI types (mobile vs. web) is shown but with limited testing scope; optimal balance between exploration breadth and depth remains somewhat heuristic
- **Low Confidence**: Long-term scalability with respect to dataset size and potential catastrophic forgetting when continuously generating new synthetic data

## Next Checks
1. **Scalability Test**: Generate and evaluate synthetic data at 5x, 10x, and 50x the original scale to measure performance saturation points and computational efficiency degradation
2. **Generalization Stress Test**: Evaluate OS-Genesis-generated agents on completely unseen app categories (e.g., medical, financial, industrial control systems) to quantify real-world robustness
3. **Ablation Study on TRM Parameters**: Systematically vary the reward score thresholds and granularity to identify the optimal balance between data quality and quantity preservation