---
ver: rpa2
title: Filtered Direct Preference Optimization
arxiv_id: '2404.13846'
source_url: https://arxiv.org/abs/2404.13846
tags:
- dataset
- fdpo
- quality
- preference
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that Direct Preference Optimization (DPO)
  is highly sensitive to the quality of preference datasets, more so than standard
  RLHF methods. To address this issue, the authors propose Filtered Direct Preference
  Optimization (fDPO), which uses a trained reward model to filter out low-quality
  samples during training by comparing generated responses against chosen responses
  in the dataset.
---

# Filtered Direct Preference Optimization

## Quick Facts
- arXiv ID: 2404.13846
- Source URL: https://arxiv.org/abs/2404.13846
- Authors: Tetsuro Morimura, Mitsuki Sakamoto, Yuu Jinnai, Kenshi Abe, Kaito Ariu
- Reference count: 19
- Key outcome: fDPO significantly improves DPO performance on mixed-quality datasets, matching DPO performance on high-quality data

## Executive Summary
This paper identifies that Direct Preference Optimization (DPO) is highly sensitive to the quality of preference datasets, more so than standard RLHF methods. To address this issue, the authors propose Filtered Direct Preference Optimization (fDPO), which uses a trained reward model to filter out low-quality samples during training by comparing generated responses against chosen responses in the dataset. Experiments with 160M and 1.4B language models show that fDPO significantly improves DPO performance on mixed-quality datasets, effectively matching the performance of DPO trained on high-quality data. The method demonstrates robustness to dataset quality variations and suggests potential benefits even for uniformly labeled datasets.

## Method Summary
The authors propose Filtered Direct Preference Optimization (fDPO) to address DPO's sensitivity to low-quality preference data. fDPO uses a trained reward model to filter samples during training: for each prompt, the current model generates a response and the reward model scores both this response and the chosen response from the dataset. If the generated response scores higher, the sample is excluded from training. This filtering process aims to remove samples where the chosen response is of lower quality than what the model can generate. The method is evaluated on instruction-following tasks using the AlpacaFarm dataset with artificially created high-quality, low-quality, and mixed-quality preference datasets.

## Key Results
- DPO shows significantly higher sensitivity to dataset quality than RLHF methods
- fDPO improves DPO performance on mixed-quality datasets to match DPO performance on high-quality data
- Filtering effectively removes low-quality samples while maintaining sufficient training data (unfiltered ratio of ~0.7-0.8)
- fDPO demonstrates robustness across different model sizes (160M and 1.4B parameters)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO is more sensitive to low-quality chosen responses than RLHF because it directly increases the probability of chosen responses without a separate reward model.
- Mechanism: In DPO, the objective increases the likelihood of chosen responses (yc) and decreases the likelihood of rejected responses (yr). Low-quality chosen responses lead to more harmful effects because their probabilities increase while high-quality rejected responses may still have some probability mass preserved.
- Core assumption: The sum of generation probabilities equals one, so increasing low-quality response probabilities necessarily decreases high-quality response probabilities.
- Evidence anchors:
  - [abstract]: "We confirm that text quality significantly influences the performance of models optimized with DPO more than those optimized with reward-model-based RLHF."
  - [section 4.2]: "Equation (4) highlights that DPO...inherently aims to increase the generation probability for chosen responses and decrease it for rejected ones."
- Break condition: If the chosen/rejected response ratio doesn't consistently favor chosen responses during training, or if the diversity of high-quality responses is extremely limited.

### Mechanism 2
- Claim: fDPO improves DPO by filtering out samples where the chosen response is of lower quality than the model-generated response.
- Mechanism: fDPO uses a trained reward model to compare the chosen response (yc) against a response generated by the current model. If the generated response scores higher, the sample is discarded from training.
- Core assumption: The reward model can reliably distinguish between response qualities, and the current model can generate responses comparable to or better than the chosen responses in the dataset.
- Evidence anchors:
  - [abstract]: "fDPO uses a trained reward model to monitor the quality of texts within the preference dataset during DPO training. Samples of lower quality are discarded based on comparisons with texts generated by the model being optimized."
  - [section 4.1]: "For each prompt x in the dataset, πθ generates a response y, and rϕ scores y and the chosen response yc. If the score of y is higher than that of yc, the corresponding sample (x, yc, yr) is excluded from training."
- Break condition: If the reward model becomes unreliable or if the model cannot generate high-quality responses during training.

### Mechanism 3
- Claim: fDPO achieves similar performance to DPO trained on high-quality data even when starting with mixed-quality data.
- Mechanism: By filtering out low-quality chosen responses, fDPO effectively creates a higher-quality dataset for DPO training, circumventing the performance bottlenecks caused by poor-quality data.
- Core assumption: The filtering process removes enough low-quality samples to significantly improve the dataset quality without over-pruning.
- Evidence anchors:
  - [abstract]: "Experiments with 160M and 1.4B language models show that fDPO significantly improves DPO performance on mixed-quality datasets, effectively matching the performance of DPO trained on high-quality data."
  - [section 5.4]: "The results shows that the performance of DPO trained on the high-quality dataset and fDPO trained on the mixed-quality dataset were on par."
- Break condition: If the filtering threshold is set too high or too low, leading to either insufficient pruning or excessive data loss.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding the RLHF pipeline is essential to grasp the context of DPO and fDPO, which are alternatives to traditional RLHF methods.
  - Quick check question: What are the three principal phases of the RLHF pipeline described in the paper?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the baseline method being improved upon by fDPO, so understanding its mechanics and limitations is crucial.
  - Quick check question: How does DPO differ from traditional RLHF in terms of the optimization objective?

- Concept: Bradley-Terry Model
  - Why needed here: The Bradley-Terry model is used in both DPO and the reward modeling phase of RLHF to estimate the probability that one response is preferred over another.
  - Quick check question: What is the mathematical form of the probability that a chosen response is preferred over a rejected response according to the Bradley-Terry model?

## Architecture Onboarding

- Component map: Pre-trained LM (πθ) → Reward Model (rϕ) → Filtering Module → DPO Training Loop
- Critical path:
  1. Supervised fine-tuning on demonstration data
  2. Reward model training on preference data
  3. DPO fine-tuning with/without filtering:
     - For fDPO: At each epoch, generate responses and filter samples where chosen responses are lower quality
     - Update model using DPO objective on filtered dataset
- Design tradeoffs:
  - fDPO requires training a reward model, which adds computational overhead but improves data quality
  - The filtering process may reduce dataset size but aims to improve the quality of remaining samples
  - Tuning the filtering threshold and margin parameters is critical for balancing data retention and quality
- Failure signatures:
  - If the reward model is poorly trained, filtering may be ineffective or harmful
  - Over-filtering may lead to insufficient data for training
  - Under-filtering may not sufficiently improve data quality
  - If the model cannot generate high-quality responses, filtering may incorrectly remove valid samples
- First 3 experiments:
  1. Run DPO and fDPO on a high-quality dataset to establish baseline performance
  2. Run DPO and fDPO on a mixed-quality dataset to compare effectiveness
  3. Vary the reward model size and filtering parameters to find optimal configurations

## Open Questions the Paper Calls Out

- Open Question: How does fDPO performance vary across different types of preference datasets with organic quality variations versus artificially created datasets?
  - Basis in paper: The authors mention that "merging multiple existing preference datasets, which naturally contain varied qualities, could offer more realistic insights into fDPO's applicability and robustness."
  - Why unresolved: The current experiments use artificially created datasets with controlled quality variations, which may not fully represent real-world scenarios.
  - What evidence would resolve it: Experiments comparing fDPO performance on organically varied preference datasets (like combining multiple existing RLHF datasets) versus artificially created ones would provide insights into real-world applicability.

- Open Question: What is the optimal balance between filtering efficiency (precision) and data retention (recall) when tuning the margin parameter ϵ in fDPO?
  - Basis in paper: The authors note that "larger margins lead to slower filtering speeds" and that "precision improves at the expense of recall" when margin increases, suggesting a trade-off that needs optimization.
  - Why unresolved: The paper shows the trade-off exists but doesn't determine the optimal margin value for different scenarios or how to dynamically adjust it during training.
  - What evidence would resolve it: Systematic experiments varying margin parameters across different dataset qualities and model sizes, combined with ablation studies on precision-recall trade-offs, would identify optimal settings.

- Open Question: Can fDPO be extended to consider rejected responses (yr) in addition to chosen responses (yc) for more comprehensive data filtering?
  - Basis in paper: The authors state in the conclusions that "considering rejected responses presents an exciting direction for future work."
  - Why unresolved: The current fDPO implementation only filters based on chosen response quality, potentially missing opportunities to improve dataset quality by also filtering low-quality rejected responses.
  - What evidence would resolve it: Implementing a version of fDPO that filters both chosen and rejected responses, then comparing performance against the current approach across multiple datasets and model sizes would demonstrate whether this extension provides additional benefits.

## Limitations

- The evaluation is primarily focused on instruction following tasks using the AlpacaFarm dataset, which may not generalize to all alignment scenarios
- The reward model filtering assumes that generated responses can be meaningfully compared to chosen responses, which may not hold for all types of preference data
- The study uses relatively small language models (160M and 1.4B parameters), so results may differ for larger models where dataset quality effects could be amplified

## Confidence

**High confidence**: DPO is more sensitive to dataset quality than RLHF methods; fDPO successfully improves DPO performance on mixed-quality datasets; fDPO can match DPO performance on high-quality data when applied to mixed-quality data.

**Medium confidence**: The mechanism by which fDPO works (reward model filtering) is the optimal approach; the specific filtering parameters used are universally optimal; results will scale to larger models without modification.

**Low confidence**: fDPO will provide significant benefits for uniformly high-quality datasets; the method will generalize equally well to all types of preference data and alignment tasks.

## Next Checks

1. **Cross-task validation**: Test fDPO on preference datasets from different domains (summarization, dialogue, code generation) to verify that the quality sensitivity findings and fDPO improvements generalize beyond instruction following.

2. **Reward model ablation**: Systematically vary the reward model size, training duration, and filtering threshold to quantify their individual contributions to fDPO performance and identify optimal configurations for different dataset characteristics.

3. **Larger model scaling study**: Implement fDPO on models with 7B+ parameters to investigate whether the quality sensitivity gap between DPO and RLHF widens with model size, and whether fDPO's relative improvements scale accordingly.