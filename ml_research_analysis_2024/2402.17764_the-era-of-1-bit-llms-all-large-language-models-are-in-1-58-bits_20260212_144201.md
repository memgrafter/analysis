---
ver: rpa2
title: 'The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits'
arxiv_id: '2402.17764'
source_url: https://arxiv.org/abs/2402.17764
tags:
- bitnet
- llms
- llama
- memory
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BitNet b1.58, a 1-bit large language model
  (LLM) variant where each parameter is ternary {-1, 0, 1}, offering significant efficiency
  improvements. By adopting an absmean quantization function and LLaMA-alike components,
  BitNet b1.58 matches the performance of full-precision (FP16/BF16) Transformer LLMs
  in perplexity and end-task accuracy while being significantly more cost-effective.
---

# The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits

## Quick Facts
- arXiv ID: 2402.17764
- Source URL: https://arxiv.org/abs/2402.17764
- Reference count: 11
- One-line primary result: BitNet b1.58 achieves full-precision Transformer LLM performance with 1.58-bit ternary weights, offering 2.71× faster inference and 3.55× less GPU memory usage

## Executive Summary
This paper introduces BitNet b1.58, a 1-bit large language model variant where each parameter is ternary {-1, 0, 1}. By adopting an absmean quantization function and LLaMA-alike components, BitNet b1.58 matches the performance of full-precision (FP16/BF16) Transformer LLMs in perplexity and end-task accuracy while being significantly more cost-effective. The method reduces energy consumption for matrix multiplication by 71.4× on 7nm chips and enables new scaling laws for high-performance, cost-effective LLMs. This work opens avenues for designing specialized hardware optimized for 1-bit LLMs and demonstrates strong generalization capabilities with 2T training tokens.

## Method Summary
BitNet b1.58 uses ternary weights {-1, 0, 1} with an absmean quantization function applied to all layers except the embedding layer. The absmean function scales weights by their average absolute value before rounding to nearest integer in {-1, 0, 1}. The model uses LLaMA-alike components including RMSNorm, SwiGLU activation, and rotary embeddings, with 8-bit activations (scaled per token). BitNet b1.58 is trained from scratch on the RedPajama dataset for 100 billion tokens, with models ranging from 700M to 3.9B parameters. The quantization is applied during both training and inference, with a specialized 2-bit kernel enabling efficient computation.

## Key Results
- Achieves perplexity and end-task accuracy matching full-precision LLaMA 3B models
- Delivers 2.71× faster inference and 3.55× less GPU memory usage compared to LLaMA 3B
- Reduces energy consumption for matrix multiplication by 71.4× on 7nm chips
- Demonstrates strong generalization with 2T training tokens and zero-shot task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ternary weights {-1, 0, 1} reduce matrix multiplication cost by eliminating floating-point multiplications
- Mechanism: Matrix multiplication operations shift from FP16/BF16 multiply-accumulate to integer addition/subtraction, reducing arithmetic complexity and energy consumption
- Core assumption: The quantization preserves model expressiveness enough to match full-precision performance when using absmean scaling
- Evidence anchors:
  - [abstract] "matrix multiplication of BitNet only involves integer addition, which saves orders of energy cost"
  - [section 2] "the matrix multiplication of BitNet only involves integer addition, which saves orders of energy cost"
  - [corpus] Weak/no evidence for absolute energy savings across chip types
- Break condition: If quantization introduces significant information loss that cannot be compensated by scaling or model size increases

### Mechanism 2
- Claim: absmean quantization function preserves weight distribution information while constraining weights to ternary values
- Mechanism: Weights are scaled by their average absolute value (γ) before rounding to nearest integer in {-1, 0, 1}, maintaining relative magnitude relationships
- Core assumption: The absmean scaling factor is sufficient to preserve the information needed for model performance
- Evidence anchors:
  - [section 2] "We adopt an absmean quantization function. It first scales the weight matrix by its average absolute value, and then round each value to the nearest integer among {-1, 0, +1}"
  - [abstract] "adopting an absmean quantization function"
  - [corpus] No direct corpus evidence for absmean superiority over other quantization methods
- Break condition: If scaling factor γ becomes unstable during training or fails to capture necessary weight distribution characteristics

### Mechanism 3
- Claim: The explicit 0 value in ternary weights enables feature filtering, improving modeling capability over pure binary
- Mechanism: Zero weights allow the model to effectively "turn off" certain feature contributions, providing more expressive power than binary-only systems
- Core assumption: The 0 value provides meaningful feature selection capability beyond what {-1, +1} alone can achieve
- Evidence anchors:
  - [abstract] "its explicit support for feature filtering, made possible by the inclusion of 0 in the model weights"
  - [section 1] "it introduces some modifications... an additional value of 0 to the original 1-bit BitNet, resulting in 1.58 bits"
  - [corpus] No corpus evidence for feature filtering benefits of ternary vs binary
- Break condition: If the model architecture cannot effectively leverage the 0 value for feature selection, or if it leads to degenerate weight patterns

## Foundational Learning

- Concept: Quantization and precision reduction in neural networks
  - Why needed here: Understanding how reducing parameter precision affects model performance is fundamental to evaluating BitNet b1.58's approach
  - Quick check question: What is the primary trade-off when reducing numerical precision in neural networks?

- Concept: Matrix multiplication optimization and its energy cost
  - Why needed here: BitNet b1.58's efficiency gains come from replacing expensive floating-point operations with simpler integer operations
  - Quick check question: How does the energy cost of floating-point multiplication compare to integer addition on modern hardware?

- Concept: Transformer architecture components (RMSNorm, SwiGLU, rotary embeddings)
  - Why needed here: BitNet b1.58 uses LLaMA-alike components, so understanding these architectural choices is essential
  - Quick check question: What role does RMSNorm play in stabilizing training compared to standard layer normalization?

## Architecture Onboarding

- Component map:
  Input → Embedding → BitLinear layers (with absmean quantization) → RMSNorm → SwiGLU → Rotary Embeddings → Output

- Critical path:
  Forward pass: Input → Embedding → BitLinear layers (with quantization) → Activation → Output
  Backward pass: Gradients flow through quantization function (straight-through estimator likely used)

- Design tradeoffs:
  - Memory vs. performance: 1.58-bit weights dramatically reduce memory but may impact model capacity
  - Speed vs. accuracy: Integer operations are faster but may introduce approximation errors
  - Hardware specificity: Optimized for architectures supporting ternary operations

- Failure signatures:
  - Training instability or divergence (quantization too aggressive)
  - Performance degradation compared to baseline (information loss in quantization)
  - Unexpected memory usage patterns (inefficient implementation of ternary operations)

- First 3 experiments:
  1. Measure perplexity and accuracy of BitNet b1.58 vs full-precision baseline on WikiText2 validation set
  2. Profile memory usage and latency on GPU during inference with varying sequence lengths
  3. Test model robustness to different absmean scaling factors and quantization temperature settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the absmean quantization function affect the long-term stability and convergence of BitNet b1.58 during training?
- Basis in paper: [explicit] The paper mentions adopting an absmean quantization function but does not explore its long-term training dynamics
- Why unresolved: While the function is shown to be effective in the initial experiments, its impact on training stability over extended periods or with larger datasets is not investigated
- What evidence would resolve it: Long-term training experiments comparing BitNet b1.58 with and without the absmean quantization function, including convergence speed and final model performance

### Open Question 2
- Question: Can the 1.58-bit quantization approach be extended to other types of neural networks beyond Transformers, such as convolutional or recurrent networks?
- Basis in paper: [inferred] The paper focuses on Transformers and does not explore the applicability of the quantization method to other architectures
- Why unresolved: The unique properties of Transformer models may make them particularly suited to this quantization approach, but this is not explicitly tested for other architectures
- What evidence would resolve it: Comparative studies applying the 1.58-bit quantization to various neural network architectures and evaluating their performance and efficiency

### Open Question 3
- Question: What are the trade-offs in model accuracy when using 1.58-bit quantization for tasks requiring high precision, such as scientific computing or medical diagnostics?
- Basis in paper: [explicit] The paper demonstrates strong performance on language tasks but does not address high-precision domains
- Why unresolved: The reduction in precision might introduce errors that are acceptable for language tasks but problematic for domains requiring exact calculations
- What evidence would resolve it: Empirical studies applying BitNet b1.58 to high-precision tasks and measuring accuracy degradation compared to full-precision models

### Open Question 4
- Question: How does the energy efficiency of BitNet b1.58 scale with different hardware architectures, such as GPUs, TPUs, or custom ASICs?
- Basis in paper: [explicit] The paper discusses energy savings on 7nm chips but does not explore other hardware platforms
- Why unresolved: The energy efficiency gains might vary significantly depending on the underlying hardware architecture and optimization capabilities
- What evidence would resolve it: Benchmarking BitNet b1.58 across multiple hardware platforms and analyzing the energy consumption and performance trade-offs for each

## Limitations

- Hardware-specific claims: Efficiency metrics (71.4× energy reduction, 2.71× speedup) are measured on specific hardware and may not generalize across different chip architectures
- Quantization stability: Long-term training stability of absmean quantization function is not empirically validated across multiple runs or extended training periods
- Feature filtering benefits: The claimed advantage of ternary over binary weights lacks empirical validation through ablation studies

## Confidence

- High confidence: The fundamental mechanism of using ternary weights with absmean quantization is sound and the implementation approach (LLaMA-alike components, 8-bit activations) is reasonable
- Medium confidence: Experimental results showing comparable perplexity and end-task performance to full-precision models are convincing for the specific evaluation setup
- Low confidence: Specific efficiency metrics and claimed feature filtering benefits lack sufficient empirical validation and may be implementation-dependent

## Next Checks

1. **Cross-hardware validation**: Test BitNet b1.58's efficiency claims across multiple hardware platforms (different GPU architectures, CPUs with varying instruction sets) to verify the generalizability of the 71.4× energy reduction and other hardware-specific metrics

2. **Ablation study on ternary vs binary**: Conduct controlled experiments comparing BitNet b1.58 (ternary) against a pure binary {-1, +1} variant to empirically measure the actual benefit of the explicit 0 value for feature filtering

3. **Quantization stability analysis**: Monitor the absmean scaling factor γ throughout extended training runs and across multiple random seeds to assess quantization stability and identify potential failure modes related to weight distribution shifts