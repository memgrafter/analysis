---
ver: rpa2
title: Online Decision Deferral under Budget Constraints
arxiv_id: '2409.20489'
source_url: https://arxiv.org/abs/2409.20489
tags:
- algorithm
- human
- reward
- performance
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses online decision deferral where a learner
  must decide between a fixed ML model and a human expert, subject to budget constraints.
  The setting includes two observation models: full information (where the learner
  can observe both arms'' rewards) and bandit feedback (where only the chosen arm''s
  reward is observed).'
---

# Online Decision Deferral under Budget Constraints

## Quick Facts
- arXiv ID: 2409.20489
- Source URL: https://arxiv.org/abs/2409.20489
- Reference count: 40
- One-line primary result: Proposes a contextual bandit algorithm for online decision deferral under budget constraints, achieving near-optimal performance with theoretical regret bounds

## Executive Summary
This paper addresses the problem of online decision deferral where a learner must choose between an ML model and a human expert based on contextual information, subject to budget constraints on human expertise. The authors formalize this as a two-armed contextual bandit problem with knapsack constraints and propose an algorithm based on optimism in the face of uncertainty. They provide theoretical regret bounds showing near-optimal performance and introduce a neural variant that learns context embeddings for improved empirical results. Experiments on synthetic and real datasets demonstrate significant performance gains over simple baselines, achieving performance close to the optimal static policy.

## Method Summary
The authors propose a contextual bandit algorithm that selects between a human expert and an ML model based on contextual information while respecting budget constraints. The algorithm uses optimism in the face of uncertainty to compute upper confidence bounds on reward and cost parameters, selecting the arm that maximizes optimistic reward minus a budget-adjusted cost term. A budget fraction γt is maintained to ensure proportional budget spending over time. The neural variant extends this by learning context embeddings through separate neural networks for human reward, model reward, and cost prediction, which are then used in the linear bandit algorithm.

## Key Results
- Theoretical regret bounds show near-optimal performance for the linear algorithm under generalized linear model assumptions
- Neural variant outperforms linear algorithm on real datasets where linear assumptions may not hold
- Experiments demonstrate significant performance gains over simple baselines, achieving performance close to optimal static policy
- Algorithm successfully handles both full information and bandit feedback settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimism in the face of uncertainty enables efficient exploration of the two-armed bandit with budget constraints
- Mechanism: The algorithm computes optimistic estimates of the reward parameters using upper confidence bounds. For each context xt, it computes an optimistic reward estimate for both the human arm (µ(x⊤t eθh,t)) and the model arm (µ(x⊤t eθm,t)), as well as an optimistic cost estimate for the human arm (µ(x⊤t ewh,t)). The algorithm then selects the arm that maximizes the optimistic reward minus a budget-adjusted cost term. This approach balances exploration (trying uncertain arms to gather information) with exploitation (choosing arms that appear good based on current estimates).
- Core assumption: The true reward parameters lie within the confidence ellipsoid around the maximum likelihood estimates with high probability
- Evidence anchors:
  - [abstract]: "Our main contribution is to formalize this realistic problem into a tractable online learning setting and thoroughly evaluate the resulting algorithm through experiments on simulated and real data, yielding remarkable results in various settings."
  - [section]: "The algorithm is based on the optimism principle and relies on upper confidence bounds on the maximum likelihood estimates of the true but unknown parameters"
- Break condition: If the confidence intervals are computed incorrectly or the confidence level δ is set too high, the optimistic estimates may not contain the true parameters, leading to suboptimal arm selection

### Mechanism 2
- Claim: Budget constraints are naturally incorporated through a fractional budget management strategy
- Mechanism: The algorithm maintains a budget γt that represents the fraction of the remaining budget allocated to future rounds. At each round, it compares the optimistic reward of each arm minus a term proportional to γt times the optimistic cost estimate. The term γt ensures that the algorithm spends the budget proportionally over time, preventing early depletion. The budget is updated multiplicatively based on the observed cost deviation from the expected budget consumption (B/T).
- Core assumption: The optimal static policy can be approximated by a policy that spends the budget proportionally over time
- Evidence anchors:
  - [section]: "Our choice of setting and algorithm allows us to leverage relevant literature and obtain theoretical guarantees on the learnt deferral model."
  - [section]: "The algorithm must not only estimate whether rh,t > rm,t, but also whether the gain rh,t − rm,t justifies the cost expenditure."
- Break condition: If the budget is depleted too quickly or too slowly, the algorithm may not have sufficient resources to explore both arms adequately, leading to poor estimates of the reward parameters

### Mechanism 3
- Claim: Neural embedding of contexts enables better performance when the linear assumption is violated
- Mechanism: The neural variant of the algorithm uses three separate feed-forward neural networks to map the raw context to learned embeddings for the human reward, model reward, and cost prediction. These embeddings are then used as the context in the linear bandit algorithm. By learning descriptive features from the data, the neural approach can capture nonlinear relationships between the context and the rewards/costs, leading to improved performance on real datasets where the linear assumption may not hold.
- Core assumption: The neural networks can learn embeddings that make the rewards and costs approximately linear in the embedded space
- Evidence anchors:
  - [abstract]: "Beyond the theoretical guarantees of our algorithm, we propose efficient extensions that achieve remarkable performance on real-world datasets."
  - [section]: "While the assumption of generalized linear costs and rewards is natural for analysis, it may not perform well for every dataset."
- Break condition: If the neural networks fail to learn meaningful embeddings or overfit to the training data, the performance may degrade compared to the linear algorithm

## Foundational Learning

- Concept: Generalized Linear Models
  - Why needed here: The paper models the rewards and costs as functions of the context and unknown parameters using generalized linear models, which allows for both linear and nonlinear relationships
  - Quick check question: What is the difference between a linear model and a logistic model in the context of generalized linear models?

- Concept: Upper Confidence Bound (UCB) Algorithms
  - Why needed here: The algorithm uses UCB to balance exploration and exploitation by computing optimistic estimates of the reward parameters within confidence ellipsoids
  - Quick check question: How does the UCB algorithm balance exploration and exploitation in a multi-armed bandit problem?

- Concept: Knapsack Constraints in Bandits
  - Why needed here: The algorithm must operate under budget constraints, which are modeled as knapsack constraints, limiting the total cost incurred over the time horizon
  - Quick check question: What is the difference between a knapsack constraint and a regular budget constraint in the context of bandits?

## Architecture Onboarding

- Component map:
  Context observer -> Optimistic estimator -> Budget manager -> Arm selector -> Parameter updater -> (Neural embedder)
  Context observer: Receives the context xt at each time step
  Optimistic estimator: Computes optimistic estimates of the reward and cost parameters using UCB
  Budget manager: Maintains and updates the budget fraction γt based on observed costs
  Arm selector: Chooses the arm that maximizes the optimistic reward minus the budget-adjusted cost
  Parameter updater: Updates the maximum likelihood estimates of the parameters based on observed rewards and costs
  Neural embedder (optional): Maps the raw context to learned embeddings using neural networks

- Critical path:
  1. Receive context xt
  2. Compute optimistic estimates of reward and cost parameters
  3. Compute budget fraction γt
  4. Select arm that maximizes optimistic reward minus budget-adjusted cost
  5. Observe reward and cost
  6. Update maximum likelihood estimates of parameters
  7. Update budget fraction

- Design tradeoffs:
  - Linear vs. neural embeddings: Linear embeddings are simpler and come with theoretical guarantees, while neural embeddings can capture nonlinear relationships but require more data and hyperparameter tuning
  - Full information vs. bandit feedback: Full information allows for passive exploration of the unchosen arm, while bandit feedback requires active exploration and may lead to slower learning
  - Fixed vs. adaptive budget: Fixed budget ensures that the total cost does not exceed the budget, while adaptive budget allows for more flexibility in spending but may lead to earlier depletion

- Failure signatures:
  - Linear algorithm: Poor performance on datasets where the linear assumption is violated, such as the ImageNet16H dataset
  - Neural algorithm: Overfitting to the training data, leading to poor generalization on unseen contexts
  - Budget management: Early depletion of the budget, leading to forced selection of the model arm and reduced exploration of the human arm

- First 3 experiments:
  1. Synthetic linear data: Generate synthetic data with 20 binary features from discrete distribution with normalization, and implement the optimal static policy for comparison
  2. Real knapsack data: Apply the algorithm to the knapsack dataset to test its performance on a real-world problem with a clear human-model performance gap
  3. Real ImageNet data: Apply the neural variant of the algorithm to the ImageNet16H dataset to evaluate its ability to learn nonlinear relationships and improve performance over the linear algorithm

## Open Questions the Paper Calls Out

- Question: How does the algorithm's performance scale with the dimensionality of the context space when the budget B is small relative to the number of dimensions d?
- Basis in paper: [explicit] The authors note that "the budget must be large for this algorithm to perform well" and mention a condition B > d^(1/2)T^(3/4) for their regret bounds to hold.
- Why unresolved: The paper only provides theoretical regret bounds for large budgets and does not empirically investigate the algorithm's performance in high-dimensional, low-budget regimes.
- What evidence would resolve it: Experiments varying the dimensionality d while keeping the budget B small relative to d, measuring regret and comparing against baselines in this regime.

- Question: How does the algorithm perform in non-stationary environments where the human and model performance parameters (θ_h, θ_m) change over time?
- Basis in paper: [inferred] The paper mentions that "the respective performance of the decision makers may change, and the deferral algorithm must remain adaptive," but does not explore this scenario.
- Why unresolved: The current algorithm and theoretical analysis assume stationary reward and cost functions, which may not reflect real-world scenarios where performance drifts over time.
- What evidence would resolve it: Experiments introducing time-varying performance parameters and measuring the algorithm's ability to track these changes and maintain low regret.

- Question: What is the impact of different context distributions on the algorithm's performance, particularly for heavy-tailed or highly correlated feature distributions?
- Basis in paper: [explicit] The authors make a regularity assumption on the context distribution (λ_min(Ex∼Dxx⊤) > σ_0 > 0) but do not empirically explore how violations of this assumption affect performance.
- Why unresolved: The theoretical analysis assumes well-behaved context distributions, but real-world data may have heavy-tailed distributions or strong feature correlations that could impact the algorithm's regret bounds.
- What evidence would resolve it: Experiments using synthetic data with varying context distributions (uniform, Gaussian, heavy-tailed, correlated features) and measuring the algorithm's performance and regret scaling in each case.

## Limitations

- Theoretical guarantees rely heavily on generalized linear model assumptions that may not hold in practice
- Neural variant performs well empirically but lacks theoretical justification, creating a gap between theory and practice
- Performance depends on hyperparameter choices not fully specified, particularly for neural architecture and confidence parameters

## Confidence

- Theoretical regret bounds: High - The proofs follow established bandit literature with proper modifications for the deferral setting
- Synthetic data experiments: High - Controlled conditions allow for clear validation of algorithm behavior
- Real data experiments: Medium - Performance gains are demonstrated but depend on hyperparameter choices not fully specified
- Neural variant performance: Medium-Low - Empirical improvements are shown but without theoretical guarantees or ablation studies on architecture choices

## Next Checks

1. **Ablation study on neural architecture**: Systematically vary network depth, width, and activation functions to determine which components drive performance improvements over the linear variant

2. **Stress test budget management**: Evaluate algorithm performance across extreme budget settings (very tight vs. very loose) to identify failure modes and determine robustness of the fractional budget approach

3. **Distribution shift experiments**: Test algorithm performance when test context distribution differs from training distribution to assess generalization beyond the assumed stationary environment