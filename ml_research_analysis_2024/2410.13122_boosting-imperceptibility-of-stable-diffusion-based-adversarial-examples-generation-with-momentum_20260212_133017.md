---
ver: rpa2
title: Boosting Imperceptibility of Stable Diffusion-based Adversarial Examples Generation
  with Momentum
arxiv_id: '2410.13122'
source_url: https://arxiv.org/abs/2410.13122
tags:
- adversarial
- examples
- image
- sd-miae
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SD-MIAE, a framework that leverages Stable
  Diffusion to generate adversarial examples with high misclassification rates while
  maintaining visual imperceptibility and semantic similarity to original class labels.
  The method introduces momentum-based optimization to refine token embeddings, stabilizing
  perturbations across iterations and enhancing attack effectiveness.
---

# Boosting Imperceptibility of Stable Diffusion-based Adversarial Examples Generation with Momentum

## Quick Facts
- arXiv ID: 2410.13122
- Source URL: https://arxiv.org/abs/2410.13122
- Reference count: 29
- Primary result: SD-MIAE achieves 79% misclassification rate, improving by 35% over state-of-the-art methods

## Executive Summary
This paper introduces SD-MIAE, a framework that leverages Stable Diffusion to generate adversarial examples with high misclassification rates while maintaining visual imperceptibility and semantic similarity to original class labels. The method introduces momentum-based optimization to refine token embeddings, stabilizing perturbations across iterations and enhancing attack effectiveness. Experiments demonstrate that SD-MIAE achieves a 79% misclassification rate, significantly outperforming existing methods. The framework effectively balances adversarial strength and image fidelity, making it practical for robust adversarial evaluation.

## Method Summary
SD-MIAE generates adversarial examples through a two-phase approach: initial adversarial optimization modifying token embeddings corresponding to class labels, followed by momentum-based refinement. The method starts by converting text prompts into token embeddings, then iteratively updates these embeddings to maximize classification loss while maintaining cosine similarity to original embeddings. Momentum accumulation (mt+1 = μ · mt + ∇xℓ(F(xt), y)) stabilizes the optimization process, avoiding sharp fluctuations that lead to unnatural artifacts. The framework uses Stable Diffusion for image generation, ResNet-50 as the target classifier, and DDIM sampling with guidance scale 8.5. Optimization proceeds through 25 steps with Adam (lr=0.001), then 30 iterations of momentum refinement with ϵ=0.2 and μ=1.0.

## Key Results
- SD-MIAE achieves 79% misclassification rate on ImageNet-100, a 35% improvement over state-of-the-art methods
- Generated adversarial examples maintain semantic similarity to original class labels through cosine similarity regularization
- Visual fidelity is preserved while achieving effective misclassification, with perturbations remaining imperceptible to human observers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Momentum-based optimization stabilizes adversarial perturbation updates across iterations.
- Mechanism: By accumulating gradients with momentum term mt, the perturbation direction becomes smoother, avoiding sharp fluctuations that lead to unnatural artifacts.
- Core assumption: Smooth perturbation paths in latent space preserve semantic consistency of generated images.
- Evidence anchors:
  - [abstract]: "By introducing momentum, our approach stabilizes the optimization of perturbations across iterations, enhancing both the misclassification rate and visual fidelity"
  - [section III-D]: "mt+1 = μ · mt + ∇xℓ(F (xt), y)"
- Break condition: If momentum factor μ is too high, optimization may overshoot and lose control over perturbation magnitude.

### Mechanism 2
- Claim: Token embedding optimization guides Stable Diffusion to generate semantically aligned adversarial examples.
- Mechanism: Iteratively modifying token embeddings associated with class labels changes the semantic guidance in latent space, producing images that maintain class relevance while causing misclassification.
- Core assumption: Semantic similarity between token embeddings and original class label can be preserved while inducing misclassification.
- Evidence anchors:
  - [abstract]: "manipulating token embeddings corresponding to the specified class in its latent space"
  - [section III-B]: "The SD-MIAE process starts by converting a textual description...into a set of token embeddings"
- Break condition: If embedding perturbations are too large, semantic drift occurs and images no longer resemble original class.

### Mechanism 3
- Claim: Cosine similarity regularization preserves visual fidelity during adversarial optimization.
- Mechanism: The regularization term R(e*token, etoken) penalizes large deviations between original and perturbed token embeddings, maintaining image naturalness.
- Core assumption: Small embedding changes translate to subtle visual differences in generated images.
- Evidence anchors:
  - [section III-C]: "a cosine similarity regularization term is employed...preserving the natural appearance of the image while making subtle, adversarial alterations"
  - [equation 3]: "min −ℓ(F (G(z; etext)), y) + λ · R(ˆektoken, ektoken)"
- Break condition: If regularization coefficient λ is too low, perturbations dominate and visual fidelity degrades.

## Foundational Learning

- Concept: Gradient-based adversarial attacks
  - Why needed here: SD-MIAE builds on momentum-based optimization techniques from traditional adversarial attacks
  - Quick check question: How does the sign function in basic gradient attacks differ from momentum accumulation?

- Concept: Diffusion model latent space manipulation
  - Why needed here: Understanding how token embeddings guide image generation is crucial for effective adversarial example creation
  - Quick check question: What role do text embeddings play in the Stable Diffusion generation process?

- Concept: Cosine similarity in high-dimensional spaces
  - Why needed here: Used as regularization to maintain semantic similarity between original and perturbed token embeddings
  - Quick check question: Why is cosine similarity preferred over Euclidean distance for measuring semantic similarity?

## Architecture Onboarding

- Component map: Text prompt → Token embeddings → Stable Diffusion latent space → Generated image → Classifier → Loss computation → Token embedding updates (with momentum)
- Critical path: Token embedding initialization → Initial adversarial optimization → Momentum-based refinement → Final adversarial image generation
- Design tradeoffs: Epsilon (ϵ) vs. visual fidelity tradeoff; momentum factor (μ) vs. convergence speed tradeoff; computational overhead vs. attack effectiveness
- Failure signatures: Semantic drift (images no longer resemble original class); visible artifacts (perceptible perturbations); local minima entrapment (stagnant misclassification rates)
- First 3 experiments:
  1. Baseline: Generate benign images without any optimization, verify correct classification
  2. SD-NAE baseline: Implement token embedding optimization without momentum, measure misclassification rate
  3. SD-MIAE with varying ϵ: Test different perturbation magnitudes while keeping momentum fixed at optimal value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SD-MIAE compare to other state-of-the-art adversarial attack methods when applied to different neural network architectures beyond ResNet-50?
- Basis in paper: [inferred] The paper mentions that the framework relies on specific configurations, namely the Stable Diffusion model and the ResNet-50 classifier, raising questions about generalizability to other architectures.
- Why unresolved: The experiments conducted in the paper only evaluate SD-MIAE using ResNet-50 as the target classifier, without exploring its effectiveness on other neural network architectures.
- What evidence would resolve it: Conducting experiments to test SD-MIAE on various neural network architectures (e.g., VGG, Inception, EfficientNet) and comparing the results with other adversarial attack methods would provide insights into its generalizability and robustness.

### Open Question 2
- Question: Can SD-MIAE be effectively adapted for targeted attacks, and what modifications are necessary to improve its performance in this context?
- Basis in paper: [explicit] The paper states that targeted attacks remain challenging for SD-MIAE, with a low misclassification rate of 7% compared to 79% for untargeted attacks.
- Why unresolved: The current implementation of SD-MIAE struggles with targeted attacks, possibly due to the excessive perturbation required to force classification into a specific class, leading to semantic deviations.
- What evidence would resolve it: Developing and testing modifications to the SD-MIAE framework that specifically address the challenges of targeted attacks, such as adjusting the optimization process or incorporating additional regularization techniques, could improve its performance in this context.

### Open Question 3
- Question: What are the trade-offs between computational overhead and adversarial sample generation effectiveness, and how can SD-MIAE be optimized for efficiency without compromising its performance?
- Basis in paper: [explicit] The paper mentions that SD-MIAE requires significant GPU memory and processing time compared to other methods, which may limit its scalability.
- Why unresolved: While the paper acknowledges the computational overhead of SD-MIAE, it does not explore potential optimizations or trade-offs that could make the method more efficient.
- What evidence would resolve it: Investigating techniques to reduce computational overhead, such as optimizing the momentum-based refinement process or using more efficient sampling methods, and evaluating their impact on the effectiveness of adversarial sample generation would provide insights into potential optimizations for SD-MIAE.

## Limitations
- Computational overhead: The method requires substantial computational resources (22GB GPU memory, ~37 seconds per sample), limiting scalability for large-scale adversarial evaluation
- Limited attack transferability: Evaluation only considers a single target classifier (ResNet-50), leaving open questions about effectiveness against diverse architectures
- No targeted attack capability: The method is limited to untargeted attacks, reducing its practical utility for comprehensive adversarial testing

## Confidence
- High confidence in the core mechanism of momentum-based optimization improving adversarial effectiveness, supported by clear theoretical framework and quantitative results (79% vs 44% baseline)
- Medium confidence in the semantic preservation claims, as these rely on manual verification rather than automated metrics
- Low confidence in the method's generalizability across different classifiers and attack scenarios, given the narrow evaluation scope

## Next Checks
1. **Cross-model transferability evaluation**: Test SD-MIAE-generated adversarial examples against diverse classifier architectures (e.g., Vision Transformers, EfficientNet) to assess robustness beyond the single ResNet-50 evaluation

2. **Targeted attack implementation**: Extend the method to support targeted misclassification (forcing specific class outputs) and evaluate success rates compared to untargeted attacks

3. **Automated semantic similarity validation**: Develop and apply automated metrics (e.g., CLIP-based similarity scores) to replace manual verification, enabling large-scale quantitative assessment of semantic preservation across diverse image classes