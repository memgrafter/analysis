---
ver: rpa2
title: 'Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking
  Study'
arxiv_id: '2408.14438'
source_url: https://arxiv.org/abs/2408.14438
tags:
- spatial
- tasks
- questions
- language
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a comprehensive spatial task dataset to systematically
  evaluate large language models (LLMs) on spatial reasoning tasks. The dataset covers
  12 categories, including spatial understanding, route planning, and mapping, with
  verified answers.
---

# Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study

## Quick Facts
- arXiv ID: 2408.14438
- Source URL: https://arxiv.org/abs/2408.14438
- Authors: Liuchang Xu; Shuo Zhao; Qingming Lin; Luyao Chen; Qianqian Luo; Sensen Wu; Xinyue Ye; Hailin Feng; Zhenhong Du
- Reference count: 40
- Primary result: GPT-4o achieved highest overall accuracy (71.3%) on spatial tasks; Chain-of-Thought prompting improved route planning accuracy from 12.4% to 87.5%

## Executive Summary
This study introduces a comprehensive spatial task dataset covering 12 categories to systematically evaluate large language models on spatial reasoning. Six LLMs were tested in zero-shot and prompt-tuning phases, with GPT-4o achieving the highest overall accuracy of 71.3%. The research demonstrates that prompt strategies like Chain-of-Thought significantly improve performance on complex spatial tasks, particularly route planning. The study also identifies performance gaps between international and domestic models and highlights the effectiveness of multimodal integration for mapping tasks.

## Method Summary
The study employed a two-phase evaluation approach: zero-shot testing followed by prompt-tuning optimization. A comprehensive spatial task dataset with 900 questions across 12 categories was created, including spatial understanding, route planning, mapping, and code-related tasks. Six LLMs (GPT-4o, GPT-4-turbo, Claude-3-sonnet, Moonshot-v1-8k, GLM-4) were evaluated using weighted accuracy as the primary metric. Difficulty classification was based on model performance, and various prompt strategies (One-shot, Chain-of-Thought, Zero-shot-CoT) were systematically tested to optimize model performance.

## Key Results
- GPT-4o achieved highest overall accuracy at 71.3% across all spatial tasks
- Chain-of-Thought prompting increased GPT-4o's route planning accuracy from 12.4% to 87.5%
- Moonshot-v1-8k excelled in place name recognition despite lower overall performance
- Significant performance gaps existed between international models (GPT-4o, GPT-4-turbo) and domestic models (GLM-4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought (CoT) prompt strategies significantly improve LLM performance on complex spatial reasoning tasks.
- Mechanism: CoT strategies guide models through intermediate reasoning steps, breaking down complex spatial problems into manageable components before arriving at final answers.
- Core assumption: Models have sufficient latent reasoning capability that can be activated through structured prompting.
- Evidence anchors:
  - "The Chain-of-Thought (CoT) strategy increased gpt-4o's accuracy in simple route planning from 12.4% to 87.5%"
  - "the CoT and Zero-shot-CoT strategies had a particularly significant effect on improving the model's weighted accuracy"
- Break condition: If models lack fundamental spatial reasoning capabilities or require real-world knowledge not present in training data.

### Mechanism 2
- Claim: Difficulty classification based on model performance identifies gaps between current LLM capabilities and human-level spatial reasoning.
- Mechanism: Analyzing which questions only the best-performing model can answer correctly identifies genuinely challenging spatial reasoning problems.
- Core assumption: Model performance correlates with question difficulty in a meaningful way reflecting true cognitive complexity.
- Evidence anchors:
  - "Questions correctly answered and scored 2 by at most one model (usually the best-performing model, such as gpt-4o) were defined as hard"
  - "The main differences between the models lie in their responses to medium and hard questions"
- Break condition: If classification introduces bias based on model-specific strengths rather than true difficulty.

### Mechanism 3
- Claim: Multimodal models like gpt-4o demonstrate superior performance on mapping tasks compared to text-only models.
- Mechanism: Multimodal architectures process both textual descriptions and visual/spatial information simultaneously, enabling more nuanced geographic context understanding.
- Core assumption: Integration of multiple data modalities provides complementary information enhancing spatial reasoning beyond text alone.
- Evidence anchors:
  - "gpt-4o's response highlights significant landmarks and commercial areas in San Francisco... gpt-4-turbo-2024-04-09's response... focuses more on residential areas"
  - "gpt-4o seamlessly integrates data from different modalities, efficiently identifying and extracting needed information from both structured text and unstructured data like images"
- Break condition: If performance differences stem from training data quality rather than true multimodal integration.

## Foundational Learning

- Concept: Spatial reasoning fundamentals (topology, geometry, navigation)
  - Why needed here: The study evaluates models on tasks requiring understanding of spatial relationships, path planning, and geographic features
  - Quick check question: Can you explain the difference between topological and geometric spatial relationships?

- Concept: Prompt engineering techniques (few-shot, CoT, zero-shot-CoT)
  - Why needed here: The study systematically tests different prompt strategies to improve model performance on spatial tasks
  - Quick check question: What is the key difference between Chain-of-Thought and Zero-shot-CoT prompting strategies?

- Concept: Benchmarking methodology and evaluation metrics
  - Why needed here: The study introduces weighted accuracy (WA) and difficulty classification to comprehensively evaluate model performance
  - Quick check question: How does weighted accuracy differ from simple accuracy, and why is it more appropriate for this study?

## Architecture Onboarding

- Component map: Dataset creation → Zero-shot testing → Difficulty classification → Prompt optimization → Results analysis → Discussion
- Critical path: Zero-shot testing establishes baseline performance, determining which tasks receive prompt optimization and how the dataset is classified by difficulty
- Design tradeoffs: Comprehensive spatial task coverage vs. focused evaluation of specific reasoning capabilities; automated difficulty classification vs. expert human judgment
- Failure signatures: Models performing well on easy questions but poorly on medium/hard questions indicates limitations in complex reasoning rather than knowledge gaps; significant performance variation across different prompt strategies suggests sensitivity to prompting techniques
- First 3 experiments:
  1. Replicate zero-shot testing with additional models to validate baseline performance rankings
  2. Test alternative difficulty classification methods (expert panel vs. model-based) to compare results
  3. Experiment with more complex prompt strategies (self-consistency, decomposed prompting) on the spatial tasks identified as most challenging

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can large language models be optimized for spatial reasoning tasks that involve multimodal data (e.g., images and maps)?
- Basis in paper: The study focused on text-based spatial tasks and noted that real-world applications often involve multimodal elements, acknowledging gpt-4o's superior multimodal integration capabilities.
- Why unresolved: The study primarily used text-based tasks due to limitations of some models not supporting image inputs.
- What evidence would resolve it: Developing and testing models with multimodal capabilities on spatial tasks including images, charts, and maps, comparing their performance to text-only models.

### Open Question 2
- Question: What are the specific factors contributing to the performance gap between international and domestic large language models in spatial reasoning tasks?
- Basis in paper: The study observed that international models like gpt-4o performed better and showed more versatility in response to prompt strategies compared to domestic models like glm-4.
- Why unresolved: The study speculated on possible reasons such as differences in architecture design, training data, and spatial understanding abilities, but did not conduct a detailed comparative analysis.
- What evidence would resolve it: A detailed comparative study analyzing architecture, training data, and spatial reasoning capabilities of international and domestic models to identify specific influencing factors.

### Open Question 3
- Question: How can prompt strategies be further refined to improve the performance of large language models on complex spatial tasks?
- Basis in paper: The study tested various prompt strategies (One-shot, Combined Techniques Prompt, Chain of Thought, Zero-shot-CoT) and found their effectiveness varied across models and tasks.
- Why unresolved: The study used basic prompt strategies and acknowledged that more complex strategies could not be adopted due to limitations in spatial task outputs.
- What evidence would resolve it: Experimenting with more complex prompt strategies, such as Self-consistency and task decomposition, on spatial tasks to determine their impact on model performance and identify most effective strategies.

## Limitations
- Dataset focus on Chinese spatial knowledge and terminology may limit global applicability
- Expert-annotated verification process represents single cultural perspective on spatial reasoning difficulty
- Difficulty classification methodology may conflate task complexity with model-specific training biases

## Confidence

- **High Confidence**: Effectiveness of Chain-of-Thought prompting for improving spatial reasoning performance (12.4% to 87.5% accuracy improvement for GPT-4o in route planning)
- **Medium Confidence**: Difficulty classification methodology based on model performance is reasonable but may conflate complexity with model-specific biases
- **Medium Confidence**: Superiority of multimodal models for mapping tasks demonstrated but could be influenced by training data quality differences

## Next Checks
1. Conduct cross-cultural validation by testing the dataset with spatial tasks from different geographic regions and languages to assess cultural bias in difficulty classification
2. Implement expert human benchmarking alongside model evaluation to validate the accuracy of the difficulty-based classification system
3. Test additional prompt strategies beyond CoT, such as self-consistency and decomposed prompting, particularly for the spatial tasks identified as most challenging