---
ver: rpa2
title: Assessing In-context Learning and Fine-tuning for Topic Classification of German
  Web Data
arxiv_id: '2407.16516'
source_url: https://arxiv.org/abs/2407.16516
tags:
- topic
- content
- classification
- test
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of fine-tuning encoder models
  versus in-context learning for binary topic classification of German web data. Using
  a dataset of 156,146 scraped webpages annotated across three German policy topics,
  the research compares multilingual and monolingual models, as well as zero and few-shot
  approaches.
---

# Assessing In-context Learning and Fine-tuning for Topic Classification of German Web Data

## Quick Facts
- **arXiv ID:** 2407.16516
- **Source URL:** https://arxiv.org/abs/2407.16516
- **Reference count:** 40
- **Primary result:** Fine-tuning GELECTRA-Large achieves F1 scores 22.4-60.0% higher than multilingual models for German web topic classification

## Executive Summary
This study evaluates fine-tuning encoder models versus in-context learning for binary topic classification of German web data. Using 156,146 scraped webpages across three policy topics, the research compares multilingual and monolingual models, as well as zero and few-shot approaches. Fine-tuning encoder-based models, particularly GELECTRA-Large, achieves superior performance with an average F1 score of 0.430, outperforming in-context learning strategies. The study demonstrates that combining URL and content features significantly enhances classification accuracy, with URL-only classifiers providing adequate performance when content is unavailable.

## Method Summary
The researchers scraped 156,146 German webpages and annotated them at the URL level as topic-related or not for three policy topics. They split long documents into 384-token chunks with 64-token overlap while maintaining URL-based labels. The study compared fine-tuning encoder models (BERT, RoBERTa, ELECTRA variants) with zero/few-shot prompting using LLMs (FLAN-T5, Aya, Vicuna). Models were evaluated on balanced, unbalanced, and extended test sets using F1-score as the primary metric. Feature engineering combined URL-based and content-based features to enhance classification performance.

## Key Results
- Fine-tuning encoder-based models, particularly GELECTRA-Large, achieves superior performance with an average F1 score of 0.430
- GELECTRA-Large outperforms the best multilingual model (xlm-roberta-large) by 22.4-60.0% F1 depending on dataset
- Integrating URL and content-based features significantly enhances classification accuracy, with content-based analysis improving F1 scores by up to 977.3% on the extended test set
- URL-only classifiers provide adequate performance when content is unavailable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning encoder-based models with small amounts of annotated data yields better classification performance than in-context learning.
- Mechanism: Fine-tuning allows the model to adjust its internal representations specifically to the classification task using labeled examples, whereas in-context learning relies on the model's pre-existing knowledge and prompt design without parameter updates.
- Core assumption: The pre-trained encoder model's architecture and initial weights are suitable for the target task, and the limited labeled data is sufficient to adapt the model effectively.
- Evidence anchors:
  - [abstract]: "Fine-tuning encoder-based models, particularly GELECTRA-Large, achieves superior performance with an average F1 score of 0.430, outperforming in-context learning strategies."
  - [section 4.3]: "Our evaluation shows high accuracy for zero- and few-shot prompting without fine-tuning, indicating their potential in data-constrained situations. Few-shot learning can be viable when runtime is less critical, but labeled data is expensive. However, fine-tuning encoder-based models generally yields better results and should be given preference over in-context learning for annotating large datasets."
  - [corpus]: Weak - no direct corpus evidence; inferred from performance comparison.
- Break condition: If the pre-trained model's architecture is not well-suited to the task, or if the labeled data is too limited or unrepresentative to effectively adapt the model.

### Mechanism 2
- Claim: Combining URL and content-based features significantly enhances classification accuracy compared to using either feature alone.
- Mechanism: URL features capture high-level topical signals and domain-specific information, while content features provide detailed semantic context. Combining both allows the model to leverage complementary information sources.
- Core assumption: Both URL and content features contain relevant information for the classification task, and their combination provides a more comprehensive representation of the webpage.
- Evidence anchors:
  - [abstract]: "Integrating URL and content-based features significantly enhances classification accuracy, with content-based analysis improving F1 scores by up to 977.3% on the extended test set."
  - [section 5.1]: "Classifiers using both URL & content-based features perform best, while using URLs alone provides adequate results when content is unavailable."
  - [corpus]: Weak - no direct corpus evidence; inferred from performance comparison.
- Break condition: If one feature type is highly redundant or noisy compared to the other, or if the combination leads to overfitting due to increased dimensionality.

### Mechanism 3
- Claim: Monolingual models trained on German text outperform multilingual models for German web data classification.
- Mechanism: Monolingual models are optimized specifically for the linguistic characteristics of German, leading to better performance on German text compared to multilingual models that need to balance across multiple languages.
- Core assumption: The linguistic characteristics of German are sufficiently different from other languages to warrant specialized models, and the monolingual models have been trained on a sufficiently large and representative German corpus.
- Evidence anchors:
  - [abstract]: "Fine-tuning encoder-based models, particularly GELECTRA-Large, achieves superior performance... Comparing the best monolingual model, GELECTRA-Large, with the best multilingual model, xlm-roberta-large, GELECTRA-Large achieves an F1 score that is 22.4% higher on the unbalanced dataset, 60.0% higher on the extended dataset, and 49.5% higher on the complete test set."
  - [section 5.1]: "Monolingual models achieve a mean F1 score 25.9% higher than multilingual models on the complete test set across all topics when using URL & content features."
  - [corpus]: Weak - no direct corpus evidence; inferred from performance comparison.
- Break condition: If the multilingual models have been trained on a sufficiently large and diverse German corpus, or if the linguistic characteristics of German are not significantly different from other languages in the multilingual model's training data.

## Foundational Learning

- Concept: Binary classification
  - Why needed here: The task is to classify webpages as either relevant or irrelevant to a specific topic.
  - Quick check question: What are the two possible labels in this binary classification task?
- Concept: Fine-tuning
  - Why needed here: Fine-tuning adapts pre-trained models to the specific classification task using limited labeled data.
  - Quick check question: How does fine-tuning differ from training a model from scratch?
- Concept: Feature engineering
  - Why needed here: Combining URL and content features requires extracting relevant information from both sources.
  - Quick check question: What are the two types of features used in this classification task?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature extraction -> Model training -> Evaluation
- Critical path: Data preprocessing → Feature extraction → Model training → Evaluation
- Design tradeoffs:
  - Using URL-only vs. URL & content features: URL-only is faster but less accurate; URL & content is slower but more accurate.
  - Monolingual vs. multilingual models: Monolingual models are more accurate for German but less versatile; multilingual models are more versatile but less accurate for German.
  - Fine-tuning vs. in-context learning: Fine-tuning is more accurate but requires labeled data; in-context learning is less accurate but requires no labeled data.
- Failure signatures:
  - Low F1-scores across all models: Indicates issues with data quality, feature extraction, or model architecture.
  - High precision but low recall: Indicates the model is too conservative in its predictions.
  - Low precision but high recall: Indicates the model is too liberal in its predictions.
- First 3 experiments:
  1. Train and evaluate a URL-only classifier using a simple baseline model (e.g., SVM) to establish a performance baseline.
  2. Train and evaluate a URL & content classifier using the best-performing encoder model (e.g., GELECTRA-Large) to assess the impact of combining features.
  3. Compare the performance of fine-tuning vs. in-context learning using a small labeled dataset to determine the best approach for data-constrained scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of topic classification models change when using content-based labels instead of URL-based labels for training and evaluation?
- Basis in paper: [explicit] The paper mentions that manual error analysis revealed 42 instances where the classifier's prediction was correct and the ground truth was incorrect, likely due to URL-based labeling versus content-based labeling.
- Why unresolved: The paper relies on URL-based labeling due to feasibility, but acknowledges this may introduce bias. A direct comparison with content-based labels would validate the impact of this labeling approach.
- What evidence would resolve it: Conducting a study where a subset of webpages are labeled at the content level and comparing model performance between URL-based and content-based labeling would provide insights into the impact of labeling approach.

### Open Question 2
- Question: How does the inclusion of loosely topic-related negative examples affect classifier precision and its ability to distinguish between relevant and non-relevant instances?
- Basis in paper: [explicit] The paper suggests that incorporating loosely topic-related negative examples into the training data would likely improve classifier precision by enabling better differentiation between relevant and non-relevant instances.
- Why unresolved: The paper identifies this as a potential improvement but does not experimentally validate the impact of including such examples on classifier performance.
- What evidence would resolve it: Experimenting with training classifiers using datasets that include loosely topic-related negative examples and comparing their precision and recall with classifiers trained on strictly unrelated examples would demonstrate the effect of this approach.

### Open Question 3
- Question: How does the performance of topic classification models vary when using different chunk aggregation strategies beyond the "one chunk is relevant, the whole page is relevant" approach?
- Basis in paper: [explicit] The paper mentions that an investigation of more elaborate chunk pooling and combination strategies in future work is needed.
- Why unresolved: The paper uses a simple majority voting approach for chunk aggregation but acknowledges the need for more sophisticated strategies.
- What evidence would resolve it: Comparing the performance of classifiers using different chunk aggregation strategies, such as weighted voting based on chunk relevance scores or hierarchical aggregation, would provide insights into the optimal approach for this task.

## Limitations
- The dataset represents only three German policy topics, limiting applicability to other domains or languages
- The 977.3% improvement claim for content-based analysis appears to be a calculation artifact rather than a realistic performance jump
- URL-only classification may overfit to superficial patterns rather than semantic content due to the 384-token context window fragmentation

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Fine-tuning GELECTRA-Large achieves 22.4-60.0% F1 improvement over multilingual models | High |
| Combining URL and content features significantly enhances classification accuracy | Medium |
| URL-only classifiers provide adequate performance when content is unavailable | Medium |

## Next Checks

1. Replicate the comparison using a more diverse set of German web topics and evaluate whether monolingual model advantages persist across different domains
2. Test classifier performance on complete webpages (without chunking) to determine if context fragmentation artificially inflates URL pattern learning
3. Conduct ablation studies removing URL features to quantify their actual contribution versus content-based classification alone