---
ver: rpa2
title: 'Benchmarking Estimators for Natural Experiments: A Novel Dataset and a Doubly
  Robust Algorithm'
arxiv_id: '2409.04500'
source_url: https://arxiv.org/abs/2409.04500
tags:
- e-03
- e-02
- e-04
- treatment
- e-01
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of estimating treatment effects
  from natural experiments, where treatments are pre-assigned rather than randomly
  allocated. The authors introduce a novel dataset from Reach Out and Read Colorado
  (RORCO), an early childhood literacy nonprofit, and create a comprehensive benchmark
  comparing over 20 treatment effect estimators.
---

# Benchmarking Estimators for Natural Experiments: A Novel Dataset and a Doubly Robust Algorithm

## Quick Facts
- arXiv ID: 2409.04500
- Source URL: https://arxiv.org/abs/2409.04500
- Authors: R. Teal Witter; Christopher Musco
- Reference count: 40
- Primary result: Doubly robust estimators, including a new Double-Double algorithm, outperform other methods by orders of magnitude on a novel RORCO dataset

## Executive Summary
This paper addresses the challenge of estimating treatment effects from natural experiments where treatments are pre-assigned rather than randomly allocated. The authors introduce a novel dataset from Reach Out and Read Colorado (RORCO) and create a comprehensive benchmark comparing over 20 treatment effect estimators. They derive a closed-form variance expression for doubly robust estimators that use regression adjustment and dataset splitting, which motivates their new Double-Double algorithm. The benchmark demonstrates that doubly robust estimators, particularly Double-Double, significantly outperform other more complex methods in terms of mean squared error.

## Method Summary
The core method involves doubly robust estimators that combine outcome predictions with inverse propensity score weighting, using dataset splitting to obtain unbiased estimates. The authors derive a closed-form variance expression for such estimators and introduce the Double-Double algorithm, which minimizes an upper bound on the variance term through a novel loss function. The benchmark evaluates estimator performance across varying conditions including sample size, treatment correlation, and propensity score accuracy using both real and synthetic outcomes from the RORCO dataset.

## Key Results
- Doubly robust estimators achieve mean squared errors around 10^-7 on the RORCO dataset, compared to 10^-2 to 10^-3 for other methods
- The Double-Double algorithm, motivated by variance analysis, shows superior performance among doubly robust estimators
- Performance degrades predictably as propensity score accuracy decreases and treatment correlation increases
- The naturalexperiments Python package is released with the dataset, benchmark, and algorithms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Doubly robust estimators outperform other methods by orders of magnitude on the RORCO dataset.
- **Mechanism:** Doubly robust estimators combine predictions of outcomes with inverse propensity score weighting, reducing variance when either the outcome predictions or propensity scores are accurate.
- **Core assumption:** Either the outcome prediction functions or the propensity scores must be estimated accurately enough for the estimator to be unbiased.
- **Evidence anchors:**
  - [abstract]: "we observe that the class of doubly robust treatment effect estimators, which are based on simple and intuitive regression adjustment, generally outperform other more complicated estimators by orders of magnitude"
  - [section 1.2]: "When an estimator is unbiased, its expected squared error is the variance Var(ˆτ − τ ) so we will often refer to minimizing the variance of the estimator."
  - [corpus]: Weak evidence - the corpus contains related papers but none directly compare doubly robust performance against this benchmark.
- **Break condition:** If both the outcome predictions and propensity scores are inaccurate, the estimator may become biased.

### Mechanism 2
- **Claim:** The Double-Double estimator, a new doubly robust method, is motivated by an exact variance analysis of doubly robust estimators.
- **Mechanism:** Double-Double uses a novel loss function when fitting functions for regression adjustment, minimizing an upper bound on the variance term.
- **Core assumption:** The propensity scores are known exactly or can be estimated with high accuracy.
- **Evidence anchors:**
  - [abstract]: "we derive a closed form expression for the variance of any such estimator that uses dataset splitting to obtain an unbiased estimate. This expression motivates the design of a new doubly robust estimator"
  - [section 4]: "Motivated by the upper bound on the variance term, we introduce Double-Double: a doubly robust estimator with double weighting."
  - [corpus]: Weak evidence - corpus papers discuss doubly robust methods but don't provide the specific variance analysis that motivates Double-Double.
- **Break condition:** If propensity scores are inaccurate or the dataset splitting introduces too much variance, the estimator may underperform.

### Mechanism 3
- **Claim:** The RORCO dataset provides a realistic test case for natural experiment estimators.
- **Mechanism:** The dataset combines observational outcomes (CMAS scores) with synthetic outcomes designed by domain experts, reflecting real-world conditions like treatment correlation and propensity score accuracy.
- **Core assumption:** The synthetic outcomes reflect realistic relationships between treatments, outcomes, and propensity scores as suggested by literacy experts.
- **Evidence anchors:**
  - [section 2.1]: "The literacy experts suggested the following assumptions: (1) The outcomes should be inversely related to the propensity score."
  - [section 2.2]: "Based on (1) and (2), we made the control outcomes as depicted in Figure 2 vary between 0 and 1 with an inverse linear relationship to propensity scores."
  - [corpus]: Weak evidence - corpus papers don't discuss the RORCO dataset or similar real-world datasets with expert-designed synthetic outcomes.
- **Break condition:** If the synthetic outcomes don't reflect real-world conditions, the benchmark may not be representative of actual performance.

## Foundational Learning

- **Concept:** Treatment effect estimation in natural experiments
  - Why needed here: The paper focuses on estimating treatment effects when treatments are pre-assigned rather than randomly allocated.
  - Quick check question: What is the key challenge in estimating treatment effects from natural experiments compared to randomized trials?

- **Concept:** Doubly robust estimation
  - Why needed here: Doubly robust estimators are the main focus of the paper and are shown to outperform other methods.
  - Quick check question: What are the two conditions under which doubly robust estimators are unbiased?

- **Concept:** Propensity score weighting
  - Why needed here: Propensity scores are used to account for the probability that an observation received the treatment, and inverse propensity score weighting is a key component of doubly robust estimators.
  - Quick check question: How does inverse propensity score weighting help reduce bias in treatment effect estimation?

## Architecture Onboarding

- **Component map:** RORCO dataset (real + synthetic outcomes) -> propensity score estimation -> treatment effect estimators (20+) -> performance evaluation (MSE)
- **Critical path:** Load RORCO dataset → estimate propensity scores → run each estimator → compare results using mean squared error → analyze performance across different conditions
- **Design tradeoffs:** Using synthetic outcomes allows for known ground truth but may not reflect real-world conditions perfectly. Using observational data reflects reality but lacks ground truth for comparison.
- **Failure signatures:** Large variance in estimates across different estimators, poor performance as propensity score accuracy decreases, or sensitivity to correlation between treatments and outcomes.
- **First 3 experiments:**
  1. Run all estimators on the RORCO dataset with default settings to compare mean squared error.
  2. Vary the sample size in the RORCO dataset to observe how estimator performance changes with more or less data.
  3. Add noise to the propensity scores to test estimator robustness to inaccurate propensity estimates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the theoretical variance analysis of doubly robust estimators to inaccuracies in estimated propensity scores?
- Basis in paper: [explicit] The authors note this as an important direction for future work, stating "Understanding how robust the variance analysis is to propensity score accuracy is an important direction for future work."
- Why unresolved: The current analysis assumes known exact propensity scores, which is unrealistic in practice. The sensitivity of the variance bounds and estimator performance to propensity score estimation errors remains unknown.
- What evidence would resolve it: Empirical studies comparing variance bounds and estimator performance using estimated vs. true propensity scores across datasets with varying propensity score accuracy levels.

### Open Question 2
- Question: Does differential privacy learning (DP-SGD) improve the performance of doubly robust estimators in natural experiments?
- Basis in paper: [inferred] The authors explored DP-SGD but found no improvement, suggesting "one explanation is that the second term tends to be very small" in practice.
- Why unresolved: The authors only tested DP-SGD on one dataset (RORCO) and found no improvement, but the generalizability of this finding to other datasets and settings is unclear.
- What evidence would resolve it: Systematic experiments applying DP-SGD to doubly robust estimators across multiple natural experiment datasets with varying characteristics.

### Open Question 3
- Question: How does the performance of doubly robust estimators with training splits compare to those without splits in finite samples?
- Basis in paper: [explicit] The authors note "perhaps because of the additional training data available, doubly robust estimators without the training split perform better" than their proposed Double-Double method.
- Why unresolved: The authors provide theoretical analysis only for split methods and observe better empirical performance for non-split methods, creating a theoretical-empirical gap that needs explanation.
- What evidence would resolve it: A comprehensive finite-sample analysis comparing both split and non-split doubly robust estimators, potentially explaining the observed performance gap.

## Limitations
- The synthetic outcomes, while allowing for ground truth comparison, may not fully capture real-world complexity of treatment-outcome relationships
- The benchmark focuses primarily on mean squared error, potentially overlooking other important considerations like interpretability or computational efficiency
- Results are based on a single domain (early childhood literacy) and may not generalize to other natural experiment contexts

## Confidence
- **High confidence:** The general superiority of doubly robust estimators over other methods in the benchmark. The mathematical derivation of the variance expression and the resulting motivation for the Double-Double algorithm are well-founded.
- **Medium confidence:** The specific performance gains of Double-Double over other doubly robust methods. While the variance analysis motivates the design, the magnitude of improvement depends on the specific dataset characteristics.
- **Low confidence:** The generalizability of results to other natural experiment contexts. The RORCO dataset represents a specific domain (early childhood literacy) with particular data characteristics that may not extend to other domains.

## Next Checks
1. Apply the Double-Double algorithm to a completely different natural experiment dataset (e.g., from economics or public health) to test generalizability of the performance gains observed in the RORCO dataset.

2. Conduct sensitivity analysis by varying the accuracy of propensity score estimates systematically across a wider range than tested in the original benchmark to better understand the conditions under which Double-Double outperforms alternatives.

3. Compare the computational efficiency of Double-Double against other doubly robust methods, as the novel loss function may introduce additional computational overhead that wasn't considered in the original benchmark.