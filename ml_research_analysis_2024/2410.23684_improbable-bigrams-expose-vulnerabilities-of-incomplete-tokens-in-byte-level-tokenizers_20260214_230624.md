---
ver: rpa2
title: Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level
  Tokenizers
arxiv_id: '2410.23684'
source_url: https://arxiv.org/abs/2410.23684
tags:
- tokens
- incomplete
- tokenization
- bigrams
- improbable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerabilities of incomplete tokens
  in byte-level byte-pair encoding (BPE) tokenizers, which are tokens containing stray
  bytes that cannot be decoded independently. The authors hypothesize that these tokens
  are overly dependent on their adjacent tokens and become fragile when paired with
  unfamiliar tokens.
---

# Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers

## Quick Facts
- **arXiv ID**: 2410.23684
- **Source URL**: https://arxiv.org/abs/2410.23684
- **Reference count**: 4
- **Primary result**: Incomplete tokens in byte-level BPE tokenizers cause hallucinations when paired with incompatible tokens, with improbable bigrams showing up to 90% reduction in hallucinations when alternative tokenization is used.

## Executive Summary
This paper investigates a critical vulnerability in byte-level byte-pair encoding (BPE) tokenizers: incomplete tokens that contain stray bytes unable to form valid characters independently. The authors demonstrate that these incomplete tokens are structurally dependent on adjacent tokens to form valid Unicode characters, making them fragile when paired with unfamiliar tokens. By introducing "improbable bigrams"—out-of-distribution combinations of incomplete tokens—they show that such pairings significantly increase hallucinatory behaviors in language models. Their experiments across five instruction-tuned LLMs reveal that alternative tokenization methods can reduce hallucinations by up to 93%, highlighting important considerations for tokenizer design and model safety.

## Method Summary
The authors systematically analyze incomplete tokens in five byte-level BPE tokenizers by identifying tokens that cannot be decoded independently. They classify incomplete tokens by structure (prefix/suffix, stray byte count) and construct up to 100 "improbable bigrams" per model by pairing complementary incomplete tokens from different language scripts. These bigrams are validated through decode-encode tests to ensure they form viable but highly unlikely combinations. Baseline bigrams are created using complete tokens of similar training rank. The researchers then test phrase-level hallucination using three prompt templates across all models, comparing hallucination rates between improbable bigrams, baseline bigrams, and alternative tokenizations that avoid incomplete tokens through pre-segmentation.

## Key Results
- Incomplete tokens in byte-level BPE tokenizers are structurally dependent on adjacent tokens to form valid characters
- Improbable bigrams (out-of-distribution incomplete token pairs) cause significantly higher hallucination rates than baseline bigrams
- Alternative tokenization methods that avoid incomplete tokens reduce hallucinations by up to 93% (Llama3.1 example)
- Models exhibit phrase-level hallucinations specifically when encountering improbable bigrams

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incomplete tokens depend heavily on adjacent tokens to form valid Unicode characters
- Mechanism: Incomplete tokens contain stray bytes that cannot form valid characters alone. These stray bytes require specific continuation bytes from adjacent tokens to complete valid characters. When paired with unfamiliar or incompatible tokens, the character formation fails, causing the model to hallucinate.
- Core assumption: The model treats all tokens identically during training, but incomplete tokens have structural dependencies that affect their training
- Evidence anchors:
  - [abstract]: "These tokens, also known as undecodable tokens, are byte-level tokens that cannot be decoded independently and must appear in conjunction with certain other tokens to form legal Unicode characters"
  - [section 3]: "incomplete tokens can only occur alongside other incomplete or single-byte tokens, as the 'stray bytes' of incomplete tokens require additional bytes to form a viable character"
  - [corpus]: Weak evidence - related papers discuss tokenizer vulnerabilities but don't specifically address the dependency mechanism
- Break condition: If the model's training data contains sufficient examples of incomplete token pairs forming valid characters, the dependency becomes less pronounced

### Mechanism 2
- Claim: Out-of-distribution bigram combinations trigger hallucinations due to lack of training exposure
- Mechanism: The improbable bigrams created by combining incomplete tokens from different language scripts are highly unlikely to appear in training data. The model lacks sufficient exposure to these combinations, causing it to generate hallucinatory responses when encountering them.
- Core assumption: The model's hallucination behavior is proportional to the distributional rarity of token combinations in training data
- Evidence anchors:
  - [abstract]: "We introduce improbable bigrams: out-of-distribution combinations of incomplete tokens designed to exploit their dependency"
  - [section 3]: "We believe that our findings contribute to the growing body of research on understanding the potential weaknesses of BPE tokenization"
  - [corpus]: Moderate evidence - papers like "MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization" discuss tokenization biases affecting different languages
- Break condition: If the model is trained with sufficient diverse multilingual data containing various incomplete token combinations

### Mechanism 3
- Claim: Alternative tokenization reduces hallucinations by providing more explicit character boundaries
- Mechanism: When phrases are presegmented to isolate stray bytes into complete characters before tokenization, the model receives clearer character boundaries. This reduces ambiguity and allows the model to better reconstruct the original phrase without hallucination.
- Core assumption: The model can better handle longer token sequences with clear boundaries than shorter sequences with ambiguous boundaries
- Evidence anchors:
  - [abstract]: "alternative tokenizations of the same phrases result in drastically lower rates of hallucination (93% reduction in Llama3.1)"
  - [section 5]: "Compared to improbable bigrams, alternative tokenization sequences can be suboptimal in two important ways. First, these sequences cannot be naturally generated by the tokenizer, ensuring that they are out-of-distribution for the model"
  - [corpus]: Weak evidence - no direct corpus support for this specific mechanism
- Break condition: If the model's attention mechanism cannot effectively handle longer token sequences or if presegmentation introduces other forms of ambiguity

## Foundational Learning

- Concept: Byte Pair Encoding (BPE) tokenization algorithm
  - Why needed here: Understanding how BPE creates incomplete tokens and their structural properties is fundamental to understanding the vulnerability
  - Quick check question: How does BPE decide which token pairs to merge during training?

- Concept: UTF-8 character encoding and byte structure
  - Why needed here: Incomplete tokens arise from multi-byte UTF-8 characters being split during BPE tokenization
  - Quick check question: What distinguishes starting bytes from continuation bytes in UTF-8 encoding?

- Concept: Tokenization and model input representation
  - Why needed here: The connection between tokenization choices and model behavior is central to understanding how incomplete tokens cause hallucinations
  - Quick check question: How does the tokenizer's output vocabulary size affect the likelihood of creating incomplete tokens?

## Architecture Onboarding

- Component map:
  - Tokenizer (BPE with byte-level processing) -> Token vocabulary (contains complete and incomplete tokens) -> Model embedding layer -> Attention mechanism -> Output logits -> Decoding -> Text output

- Critical path:
  1. Text input → Tokenizer → Token sequence
  2. Token sequence → Model embedding layer → Hidden states
  3. Hidden states → Attention mechanism → Output logits
  4. Logits → Decoding → Text output

- Design tradeoffs:
  - Byte-level tokenization provides comprehensive Unicode coverage but creates incomplete tokens
  - Larger vocabularies reduce token fragmentation but increase memory requirements
  - Presegmentation can reduce hallucinations but may create out-of-distribution sequences

- Failure signatures:
  - Hallucinations when incomplete tokens are paired with incompatible tokens
  - Model fails to repeat phrases containing improbable bigrams
  - Higher hallucination rates for phrases with multi-script character combinations

- First 3 experiments:
  1. Create a controlled dataset with known incomplete token pairs and measure hallucination rates
  2. Compare hallucination rates between complete token pairs and incomplete token pairs with similar training exposure
  3. Test alternative tokenization methods on the same phrases to verify hallucination reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the training process of byte-level BPE tokenizers affect the vulnerability of incomplete tokens to hallucinations?
- Basis in paper: [explicit] The paper discusses how incomplete tokens are heavily reliant on their adjacent tokens and are fragile when paired with unfamiliar tokens, suggesting that their training process may contribute to this vulnerability.
- Why unresolved: The paper does not provide a detailed analysis of the training process of byte-level BPE tokenizers and how it specifically impacts the behavior of incomplete tokens.
- What evidence would resolve it: A detailed study of the training process of byte-level BPE tokenizers, focusing on the impact of training data, tokenization rules, and merge priorities on the vulnerability of incomplete tokens to hallucinations.

### Open Question 2
- Question: Are there alternative tokenization methods that can mitigate the vulnerabilities of incomplete tokens in byte-level BPE tokenizers?
- Basis in paper: [explicit] The paper suggests that alternative tokenization methods, such as pre-segmentation, can significantly reduce the rate of hallucinations caused by incomplete tokens.
- Why unresolved: The paper does not explore other potential tokenization methods or compare their effectiveness in mitigating the vulnerabilities of incomplete tokens.
- What evidence would resolve it: An empirical comparison of various tokenization methods, including character-level tokenization, word-level tokenization, and other subword tokenization approaches, to determine their impact on the vulnerability of incomplete tokens to hallucinations.

### Open Question 3
- Question: How do the vulnerabilities of incomplete tokens in byte-level BPE tokenizers affect the performance of language models on downstream tasks?
- Basis in paper: [explicit] The paper demonstrates that incomplete tokens are prone to hallucinatory behaviors, which could potentially impact the performance of language models on various downstream tasks.
- Why unresolved: The paper does not investigate the impact of incomplete token vulnerabilities on the performance of language models on specific downstream tasks, such as machine translation, question answering, or text summarization.
- What evidence would resolve it: A comprehensive evaluation of language models using byte-level BPE tokenizers on various downstream tasks, comparing their performance with models using alternative tokenization methods, to assess the impact of incomplete token vulnerabilities on task performance.

## Limitations

- The study focuses exclusively on instruction-tuned models, leaving unclear whether the incomplete token vulnerabilities extend to base or causal language models
- The effectiveness of alternative tokenization may vary significantly depending on phrase content and tokenizer implementation
- The classification of incomplete tokens relies on specific decoding heuristics that may not generalize across all byte-level BPE implementations

## Confidence

**High Confidence**: The empirical finding that improbable bigrams cause higher hallucination rates compared to baseline bigrams is well-supported by the experimental methodology.

**Medium Confidence**: The mechanism explaining why incomplete tokens cause hallucinations through character boundary dependencies is plausible but not definitively proven.

**Low Confidence**: The claim that alternative tokenization universally reduces hallucinations by 93% is based on a single model and specific phrase sets.

## Next Checks

1. Test whether hallucination rates for improbable bigrams correlate with the frequency of similar incomplete token pairs in the model's training data.

2. Evaluate whether base models without instruction tuning exhibit similar hallucination patterns with improbable bigrams.

3. Measure hallucination rates for incomplete tokens paired with complete tokens versus complete tokens paired with complete tokens, controlling for training data exposure.