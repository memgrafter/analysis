---
ver: rpa2
title: Centroid-Based Efficient Minimum Bayes Risk Decoding
arxiv_id: '2402.11197'
source_url: https://arxiv.org/abs/2402.11197
tags:
- translation
- decoding
- cbmbr
- sentence
- comet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Centroid-Based Minimum Bayes Risk (CBMBR)\
  \ decoding, which accelerates MBR decoding by clustering translation hypotheses\
  \ into centroids and computing expected scores using these centroids instead of\
  \ all hypotheses. This reduces computational complexity from O(N\xB2) to O(Nk) while\
  \ maintaining or improving translation quality."
---

# Centroid-Based Efficient Minimum Bayes Risk Decoding

## Quick Facts
- arXiv ID: 2402.11197
- Source URL: https://arxiv.org/abs/2402.11197
- Reference count: 26
- Primary result: 6.9x speedup in expected score calculation while maintaining or improving translation quality

## Executive Summary
This paper introduces Centroid-Based Minimum Bayes Risk (CBMBR) decoding, a method that accelerates Minimum Bayes Risk decoding by clustering translation hypotheses into centroids and computing expected scores using these centroids instead of all hypotheses. The approach reduces computational complexity from O(N²) to O(Nk) while maintaining or improving translation quality. Experiments on WMT'22 and WMT'23 translation tasks show CBMBR achieves significant speedup and up to 0.5% improvement in COMET scores compared to vanilla MBR decoding, particularly when translation candidates exhibit multimodal distributions from multiple systems.

## Method Summary
CBMBR decoding accelerates Minimum Bayes Risk decoding by clustering pseudo-reference hypotheses into k clusters and computing expected scores using cluster centroids rather than all hypothesis pairs. The method generates diverse translation candidates using epsilon sampling, encodes source and hypothesis sentences using COMET, clusters hypothesis vectors into k=64 clusters using kmeans++ initialization, and calculates expected utility using centroid representations to select the optimal hypothesis.

## Key Results
- 6.9x speedup in expected score calculation compared to vanilla MBR decoding
- Up to 0.5% improvement in COMET scores across WMT'22 and WMT'23 translation tasks
- Particularly effective for multimodal candidate distributions from multiple translation systems

## Why This Works (Mechanism)

### Mechanism 1
Centroid-based MBR decoding achieves computational speedup by replacing quadratic pairwise hypothesis comparisons with linear-time centroid-based comparisons. Instead of computing expected scores between all hypothesis pairs (O(N²)), the method clusters pseudo-reference hypotheses into k clusters and computes expected scores using cluster centroids (O(Nk)). This works because sentence similarity in the COMET feature space allows clustering to preserve expected utility estimation accuracy.

### Mechanism 2
Centroid-based scoring improves translation quality when candidates exhibit multimodal distributions from multiple translation systems. By using cluster centroids rather than raw samples, CBMBR avoids being dominated by translation system bias that occurs when one system produces many similar candidates. This works because translation candidates from multiple systems create clusters in feature space that represent distinct translation modes.

### Mechanism 3
The COMET model implicitly learns sentence similarity representations through score prediction training. COMET's sentence encoder produces embeddings where semantic similarity correlates with distance, enabling effective clustering without explicit contrastive learning. This works because the COMET training objective of predicting human evaluation scores indirectly captures sentence similarity structure.

## Foundational Learning

- Concept: Minimum Bayes Risk (MBR) decoding and expected utility calculation
  - Why needed here: The paper builds on MBR framework but modifies how expected utility is computed - understanding the original formulation is critical.
  - Quick check question: What is the computational complexity of standard MBR decoding and why?

- Concept: Clustering algorithms (k-means, k-means++)
  - Why needed here: The method's core innovation relies on clustering sentence vectors - understanding initialization and convergence is important.
  - Quick check question: How does k-means++ initialization differ from random initialization and why does it matter here?

- Concept: Neural evaluation metrics and sentence embeddings
  - Why needed here: COMET's sentence representations are the foundation for clustering - understanding how they encode semantic similarity is key.
  - Quick check question: What property of COMET's sentence embeddings makes them suitable for clustering-based approximation?

## Architecture Onboarding

- Component map: MBR decoder → COMET encoder → clustering module → centroid-based expected utility calculator
- Critical path: Hypothesis generation → sentence encoding → clustering → expected utility computation → output selection
- Design tradeoffs: Speed vs accuracy (k parameter), clustering quality vs computational overhead, single-system vs multi-system candidate handling
- Failure signatures: Translation quality degradation when k is too small, clustering instability, disproportionate runtime if clustering iterations are excessive
- First 3 experiments:
  1. Verify computational speedup by measuring O(Nk) vs O(N²) scaling on synthetic data
  2. Test translation quality sensitivity to k parameter across different candidate distributions
  3. Compare clustering-based approximation vs random sampling for expected utility estimation

## Open Questions the Paper Calls Out

### Open Question 1
How does CBMBR's performance scale with larger hypothesis sets (N > 1,000) and higher cluster counts (k > 64) in terms of both speed and translation quality? The paper shows CBMBR achieves 6.9x speedup with k=64 on datasets with N up to 1,024, but doesn't explore the limits of this scaling relationship. Experiments testing CBMBR with N ranging from 1,000 to 10,000+ hypotheses and k ranging from 32 to 512 would establish optimal scaling relationships.

### Open Question 2
What is the impact of different clustering algorithms on CBMBR's performance compared to kmeans++, particularly regarding translation quality and computational efficiency? The paper only compares kmeans++ with random initialization, despite many clustering algorithms potentially being suitable for CBMBR. Systematic comparison using various clustering algorithms (hierarchical clustering, DBSCAN, Gaussian Mixture Models, etc.) would reveal which algorithms best handle the sentence embedding space structure.

### Open Question 3
How does CBMBR perform when applied to non-neural evaluation metrics like BLEU or TER, and what clustering approaches are most effective for these metrics? The paper focuses exclusively on COMET and doesn't explore whether CBMBR's approach generalizes to traditional metrics that may require different handling of sentence representations. Implementation using non-neural metrics and testing different strategies for creating sentence representations would verify generalizability.

## Limitations
- Evaluation limited to specific language pairs (En↔Ja, En↔De, En↔Zh) without testing low-resource or morphologically rich languages
- k=64 cluster parameter not systematically optimized across different translation scenarios
- Effectiveness depends on COMET's learned similarity representation which may not capture all aspects of translation quality
- Speedup assumes parallel computation infrastructure that may not be available in all deployment scenarios

## Confidence

**High Confidence**: The computational complexity analysis showing O(N²) → O(Nk) improvement is mathematically sound and verifiable. The experimental results showing consistent COMET score improvements across multiple language pairs and settings are robust.

**Medium Confidence**: The claim about CBMBR's effectiveness for multimodal candidate distributions is supported by ablation studies but could benefit from more diverse multi-system settings. The assertion that COMET implicitly learns sentence similarity through score prediction is reasonable but not directly tested.

**Low Confidence**: The generalizability to other evaluation metrics beyond COMET and the robustness to extreme translation quality scenarios (very poor candidates) are not thoroughly explored.

## Next Checks

1. **Generalization Test**: Evaluate CBMBR on additional language pairs including low-resource languages and morphologically rich languages to assess cross-linguistic robustness.

2. **Parameter Sensitivity Analysis**: Systematically vary the k parameter (number of clusters) and epsilon sampling rate to identify optimal configurations for different candidate distributions and translation qualities.

3. **Metric Robustness**: Test CBMBR's performance when using different evaluation metrics (e.g., BLEU, chrF) to verify that quality improvements are not specific to COMET's particular scoring characteristics.