---
ver: rpa2
title: 'MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language
  Models'
arxiv_id: '2410.10139'
source_url: https://arxiv.org/abs/2410.10139
tags:
- image
- multimodal
- text
- evaluation
- interleaved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMIE is a massive benchmark for evaluating interleaved multimodal
  comprehension and generation in large vision-language models, addressing the gap
  in current evaluation methods that lack scale and multimodal coherence assessment.
  It comprises 20,000 curated multimodal queries across 12 fields and supports interleaved
  inputs and outputs, combining multiple-choice and open-ended formats.
---

# MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models

## Quick Facts
- arXiv ID: 2410.10139
- Source URL: https://arxiv.org/abs/2410.10139
- Reference count: 40
- Top models like GPT-4o + SDXL achieve only 65.47% on MMIE

## Executive Summary
MMIE is a massive benchmark designed to evaluate interleaved multimodal comprehension and generation in large vision-language models. It addresses the gap in current evaluation methods that lack scale and multimodal coherence assessment. The benchmark comprises 20,000 curated multimodal queries across 12 fields and supports interleaved inputs and outputs, combining multiple-choice and open-ended formats.

## Method Summary
The MMIE benchmark was created by curating and restructuring existing multimodal datasets (Wikihow, VIST, MathVista, ReMI) into an interleaved format. A fine-tuned scoring model based on InternVL-2-4B was trained on 800 human-scored examples to evaluate outputs across six criteria. Models generate responses to MMIE queries using specified interleaved text-and-image input formats, and the fine-tuned scoring model evaluates these responses.

## Key Results
- MMIE contains 20,000 curated multimodal queries across 12 fields
- Even top models like GPT-4o + SDXL achieve only 65.47% on the benchmark
- Fine-tuned InternVL-2-4B scoring model shows higher correlation with human evaluation than generic metrics

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned scoring models outperform generic multimodal metrics like CLIPScore for evaluating interleaved generation quality. The fine-tuned InternVL-2-4B model learns domain-specific multimodal coherence patterns and task-level reasoning quality through human-annotated data, enabling it to assess both modality alignment and logical consistency. Core assumption: Human-scored reference data captures the full spectrum of quality criteria needed for reliable automated scoring.

### Mechanism 2
Interleaved multimodal generation is significantly harder than single-modality output. The requirement to maintain coherence across arbitrary text-image sequences multiplies the complexity of both generation and evaluation, exposing weaknesses in temporal understanding and cross-modality reasoning. Core assumption: Current models lack sufficient joint representation learning to track and generate coherent sequences of mixed modalities.

### Mechanism 3
Fine-tuning a scoring model on task-specific data reduces evaluation bias compared to using pre-trained models directly. The fine-tuned InternVL-2-4B model is trained to reproduce human scoring patterns across diverse multimodal outputs, making its judgments more aligned with human perception than static metrics like CLIPScore. Core assumption: The fine-tuning dataset is representative of the benchmark's diversity and captures the same quality criteria humans use.

## Foundational Learning

- **Multimodal coherence assessment**: Evaluating interleaved outputs requires judging whether text and images align semantically and contextually, not just whether each modality is high quality in isolation. Quick check: Given a text describing "a sunny beach scene" and an image of a snowy mountain, what type of coherence error does this represent?

- **Temporal understanding in multimodal sequences**: Interleaved tasks often require tracking narrative or procedural flow across alternating text and image steps, demanding models understand temporal relationships. Quick check: In a step-by-step tutorial with alternating images and instructions, what reasoning skill is needed to determine if step 3 logically follows from step 2?

- **Cross-modal alignment metrics**: Traditional metrics like BLEU or FID only evaluate single modalities; assessing interleaved outputs requires new metrics that capture joint quality. Quick check: Why might CLIPScore fail to detect that an image caption is factually incorrect about the depicted scene?

## Architecture Onboarding

- **Component map**: MMIE benchmark → interleaved queries → model generation → fine-tuned scoring model → evaluation output. Fine-tuning data pipeline: raw multimodal responses → human annotation → scoring model training → validation.
- **Critical path**: Data curation → scoring rubric design → human annotation → fine-tuning InternVL-2-4B → benchmark evaluation. Each step must preserve quality to avoid cascading errors.
- **Design tradeoffs**: Fine-tuning a scoring model trades general applicability for domain-specific accuracy; using pre-trained metrics trades bias reduction for potential misalignment with human judgment.
- **Failure signatures**: If fine-tuning data is too small, the scoring model will overfit and produce unreliable scores on novel inputs; if rubric is incomplete, it will miss key quality dimensions.
- **First 3 experiments**: 1) Run the fine-tuned scoring model on a held-out subset of annotated data to measure correlation with human scores. 2) Compare scoring model outputs to CLIPScore and GPT-4o on the same dataset to quantify bias reduction. 3) Evaluate model performance on the 1K subset versus full dataset to verify consistency.

## Open Questions the Paper Calls Out

### Open Question 1
How does the MMIE benchmark perform when evaluated on models trained with different modalities (e.g., audio or video) beyond text and images? Basis: The paper focuses on text and image modalities but does not explore other modalities. Unresolved because: The current evaluation framework is limited to text and image inputs and outputs, potentially missing insights from other modalities.

### Open Question 2
What is the impact of increasing the dataset size beyond 20,000 samples on the evaluation accuracy and model performance? Basis: The paper mentions 20,000 samples as the current dataset size but does not explore scalability. Unresolved because: The dataset size is fixed, and its effect on model evaluation accuracy and performance remains untested.

### Open Question 3
How does the MMIE-Score model generalize to tasks outside of the benchmark's domain (e.g., scientific research or medical diagnosis)? Basis: The paper discusses the generalizability of the MMIE-Score model but does not provide specific examples. Unresolved because: The generalizability of the MMIE-Score model to other domains is not fully explored.

### Open Question 4
What is the effect of incorporating human feedback during the fine-tuning process of the MMIE-Score model on its evaluation accuracy? Basis: The paper mentions human annotation but does not discuss its impact on model fine-tuning. Unresolved because: The role of human feedback in improving the MMIE-Score model's evaluation accuracy is not explored.

## Limitations

- Fine-tuning data size (800 examples) may limit the scoring model's ability to generalize to novel multimodal outputs
- Human annotation guidelines and full rubric details are not completely specified, affecting reproducibility
- Benchmark difficulty may be influenced by dataset biases or narrow task design rather than reflecting true model limitations

## Confidence

- **Scoring model bias reduction**: Medium - fine-tuned model shows higher correlation with human evaluation, but no direct comparison to alternative metrics or ablation studies provided
- **Interleaved generation difficulty**: Medium - performance gap evidence exists but lacks direct comparative studies with single-modality tasks
- **Benchmark utility**: High - scale, diversity, and automated evaluation pipeline are well-established, though some design details are underspecified

## Next Checks

1. Test the fine-tuned scoring model on a held-out subset of human-annotated data to measure correlation and check for overfitting
2. Conduct a user study comparing human and automated scoring to validate alignment and identify edge cases
3. Perform ablation studies on the rubric criteria to determine which aspects most impact model performance and scoring reliability