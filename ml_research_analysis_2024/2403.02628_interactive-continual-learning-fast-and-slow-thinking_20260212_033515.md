---
ver: rpa2
title: 'Interactive Continual Learning: Fast and Slow Thinking'
arxiv_id: '2403.02628'
source_url: https://arxiv.org/abs/2403.02628
tags:
- memory
- learning
- task
- system1
- system2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Interactive Continual Learning (ICL), a novel
  framework that enables continual learning through the interaction of two systems
  inspired by dual-process cognitive theories. The framework combines a Vision Transformer
  (ViT) as System1 for fast, intuitive processing with a multimodal large language
  model (LLM) as System2 for slow, deliberate reasoning.
---

# Interactive Continual Learning: Fast and Slow Thinking

## Quick Facts
- arXiv ID: 2403.02628
- Source URL: https://arxiv.org/abs/2403.02628
- Authors: Biqing Qi; Xingquan Chen; Junqi Gao; Dong Li; Jianxing Liu; Ligang Wu; Bowen Zhou
- Reference count: 40
- Key outcome: Introduces ICL framework combining ViT (System1) and LLM (System2) for continual learning, achieving state-of-the-art performance with accuracy improvements of 3-4% on CIFAR-10/100 and ImageNet-R

## Executive Summary
This paper presents Interactive Continual Learning (ICL), a novel framework inspired by dual-process cognitive theories that addresses catastrophic forgetting in continual learning. The framework employs a Vision Transformer as System1 for fast, intuitive processing and a multimodal LLM as System2 for slow, deliberate reasoning. ICL introduces three key mechanisms: CKT-MHA for task-specific memory retrieval, CL-vMF for geometric memory representation using von Mises-Fisher distributions, and vMF-ODI for identifying hard examples to route to System2. Experimental results demonstrate significant performance improvements across multiple benchmarks while effectively mitigating catastrophic forgetting.

## Method Summary
ICL combines a Vision Transformer backbone with a multimodal LLM to create a dual-system continual learning framework. The System1 processes inputs using CKT-MHA to retrieve class-specific value memory embeddings, while CL-vMF ensures geometric separation of these embeddings through von Mises-Fisher distribution modeling. When vMF-ODI identifies low-confidence predictions as hard examples, they are routed to System2 for deeper reasoning based on System1's top-K outputs. The framework uses alternating Expectation-Maximization updates for memory parameters and dynamically allocates new value memory as new classes are encountered, avoiding predefined class limitations.

## Key Results
- Achieves average incremental accuracy improvements of 3-4% over state-of-the-art methods on CIFAR-10, CIFAR-100, and ImageNet-R
- Effectively reduces catastrophic forgetting across all benchmark datasets
- Demonstrates dynamic class handling capability without predefined class limitations
- Shows robust performance across different buffer sizes (200/500/600 examples)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CKT-MHA enables task-specific memory retrieval by leveraging class and knowledge features to improve generalization across unseen class combinations.
- Mechanism: Multi-head attention over class features, knowledge features, and task-specific embeddings constructs unified representations for Set2Set memory retrieval, enabling dynamic task inference from class information.
- Core assumption: Class and knowledge features contain sufficient discriminative information to represent task boundaries without explicit task labels during inference.
- Evidence anchors: Abstract and Section 3.2 describe CKT-MHA construction and purpose for deducing tasks from class information.

### Mechanism 2
- Claim: CL-vMF improves geometric separation of value memory embeddings, making retrieval more discriminative and reducing task interference.
- Mechanism: Value memory vectors modeled as von Mises-Fisher distributions with EM updates refine concentration, pushing class embeddings apart while keeping within-class representations tight.
- Core assumption: Class-specific value memory embeddings benefit from directional clustering, and vMF distributions are suitable for enforcing this geometry.
- Evidence anchors: Abstract and Section 3.3 describe CL-vMF mechanism and its geometric representation benefits.

### Mechanism 3
- Claim: vMF-ODI dynamically routes hard examples to System2, enabling selective complex reasoning and reducing unnecessary LLM calls.
- Mechanism: Batchwise normalization of cosine similarities identifies outliers (low similarity scores) as hard examples, which are passed to System2 with System1's top-K predictions as context.
- Core assumption: Hard examples can be reliably identified by low cosine similarity scores, and System2 can leverage System1's top-K output as effective contextual priming.
- Evidence anchors: Abstract and Section 3.4 describe vMF-ODI strategy for outlier detection and System1-System2 collaboration.

## Foundational Learning

- Concept: Complementary Learning Systems (CLS) Theory
  - Why needed here: Provides cognitive science justification for dividing the continual learning system into fast, intuitive subsystem (System1) and slow, reasoning subsystem (System2).
  - Quick check question: Why is it important that System1 handles fast inference and System2 handles hard examples?

- Concept: Catastrophic Forgetting
  - Why needed here: Core problem that ICL is designed to solve—neural networks overwrite old knowledge when learning new tasks.
  - Quick check question: What is the main risk when updating all model parameters in a continual learning setting?

- Concept: von Mises-Fisher Distribution
  - Why needed here: Provides probabilistic framework for modeling directional similarity in high-dimensional embedding spaces, enabling improved geometric separation of memory vectors.
  - Quick check question: How does the vMF distribution differ from a Gaussian in terms of the space it models?

## Architecture Onboarding

- Component map:
  - System1: ViT backbone + CKT-MHA + CL-vMF + vMF-ODI
  - System2: Multimodal LLM (MiniGPT4 / Inf-MLLM / Pure-MM)
  - Memory: Value memory embeddings Z (class-specific) + query parameters θ
  - Interface: vMF-ODI detection → top-K output → LLM prompt

- Critical path:
  1. Input → ViT feature extraction → CKT-MHA → vMF posterior → value memory retrieval
  2. If similarity score < threshold → pass top-K + image to System2
  3. System2 returns refined prediction or fallback to System1

- Design tradeoffs:
  - vMF vs Euclidean clustering: vMF better for directional data but requires concentration estimation
  - Top-K selection: higher K increases coverage but may confuse LLM; lower K saves cost but risks missing correct class
  - Buffer size: larger buffer reduces forgetting but increases rehearsal overhead

- Failure signatures:
  - High false positive rate in vMF-ODI → System2 called unnecessarily → latency cost
  - Concentration parameter mis-estimation → poor separation → retrieval confusion
  - CKT-MHA mis-alignment → task inference errors → wrong memory retrieval

- First 3 experiments:
  1. Validate CKT-MHA retrieves correct class embeddings on synthetic dataset with known task boundaries.
  2. Test vMF-ODI outlier detection accuracy by comparing detected hard examples against human-labeled difficulty scores.
  3. Measure accuracy improvement when routing hard examples to System2 versus relying on System1 alone.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ICL change with varying buffer sizes, and is there an optimal buffer size that maximizes performance?
- Basis in paper: The paper mentions using different buffer sizes for comparison, with results showing that performance improves with larger buffer sizes. However, the exact relationship between buffer size and performance is not fully explored.
- Why unresolved: The paper does not provide a detailed analysis of how buffer size impacts performance, such as the point of diminishing returns or the optimal buffer size for different datasets.
- What evidence would resolve it: Conducting experiments with a wider range of buffer sizes and analyzing the performance curve to identify the optimal buffer size for different datasets and task configurations.

### Open Question 2
- Question: How does the choice of System2 (e.g., MiniGPT4, Inf-MLLM, Pure-MM) impact the overall performance of ICL, and is there a particular System2 that is most effective?
- Basis in paper: The paper mentions using different System2 models (MiniGPT4, Inf-MLLM, Pure-MM) and shows that the choice of System2 affects performance. However, a detailed comparison of these models' effectiveness is not provided.
- Why unresolved: The paper does not provide a comprehensive analysis of how each System2 model performs, including their strengths and weaknesses, and which model is most suitable for different types of tasks or datasets.
- What evidence would resolve it: Conducting experiments with various System2 models on different datasets and task configurations, and analyzing their performance to determine the most effective System2 model for each scenario.

### Open Question 3
- Question: How does the concentration parameter κ in the CL-vMF mechanism affect the performance of ICL, and is there an optimal value of κ for different datasets?
- Basis in paper: The paper mentions that the concentration parameter κ is a hyperparameter that affects the performance of the CL-vMF mechanism. However, the paper does not provide a detailed analysis of how different values of κ impact performance or if there is an optimal value for different datasets.
- Why unresolved: The paper does not provide a comprehensive analysis of the relationship between the concentration parameter κ and the performance of ICL, such as the impact of different values of κ on different datasets or task configurations.
- What evidence would resolve it: Conducting experiments with different values of κ on various datasets and task configurations, and analyzing the performance to determine the optimal value of κ for each scenario.

## Limitations
- Limited scalability analysis - framework only evaluated on small-scale image datasets (CIFAR-10/100, ImageNet-R) without demonstration on larger-scale problems
- Interaction protocol between System1 and System2 lacks precise specification of top-K prediction formatting for LLM prompting
- Specific architectural implementation details of CKT-MHA module remain underspecified, particularly regarding task inference without explicit labels

## Confidence

- **High Confidence**: Overall framework design combining dual-process cognition with continual learning principles is well-supported by existing literature and experimental results show consistent performance improvements.
- **Medium Confidence**: Theoretical justification for using vMF distributions for geometric separation is sound, but empirical validation across diverse datasets would strengthen the claim.
- **Low Confidence**: Scalability analysis is limited - framework only evaluated on relatively small-scale image datasets with no demonstration on larger-scale problems or different modalities.

## Next Checks

1. Conduct ablation studies isolating each mechanism (CKT-MHA, CL-vMF, vMF-ODI) to quantify individual contributions to performance gains and verify that improvements aren't from parameter growth alone.

2. Test the vMF-ODI threshold calibration across multiple datasets to establish robust detection criteria that generalize beyond the current experimental setup.

3. Evaluate the framework's performance on a larger-scale benchmark (e.g., ImageNet-1K) to assess scalability and computational overhead when scaling up both the ViT and LLM components.