---
ver: rpa2
title: Towards Compositionality in Concept Learning
arxiv_id: '2406.18534'
source_url: https://arxiv.org/abs/2406.18534
tags:
- concept
- concepts
- compositionality
- representations
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies unsupervised extraction of compositional concept
  representations from foundation models. Existing methods like PCA and KMeans fail
  to find concepts that compose properly.
---

# Towards Compositionality in Concept Learning

## Quick Facts
- arXiv ID: 2406.18534
- Source URL: https://arxiv.org/abs/2406.18534
- Reference count: 40
- Primary result: CCE achieves MAP scores up to 0.97 vs 0.66 for PCA and improves downstream classification accuracy to 76.5% vs 72.7% for PCA

## Executive Summary
This paper addresses the challenge of extracting compositional concept representations from foundation models in an unsupervised manner. Existing methods like PCA and KMeans fail to find concepts that compose properly, prompting the authors to identify two key properties of compositional conceptsâ€”orthogonality between concepts from different attributes and non-orthogonality within the same attribute. They prove these properties are necessary for compositionality and propose Compositional Concept Extraction (CCE), which discovers concepts by jointly learning subspaces and clusters to enforce these properties.

## Method Summary
CCE is an unsupervised algorithm that extracts compositional concepts by jointly learning subspaces and clusters. The method alternates between learning a subspace that clusters data well according to fixed centroids, and learning new cluster centroids within that subspace. It uses spherical K-means clustering within learned subspaces and enforces orthogonality between concepts from different attributes while allowing non-orthogonality within the same attribute. A Silhouette score regularization prevents overfitting by matching cluster centroids in the subspace to those in the original space.

## Key Results
- CCE achieves MAP scores up to 0.97 on CLEVR dataset compared to 0.66 for PCA baseline
- CCE improves downstream classification accuracy to 76.5% on CLEVR vs 72.7% for PCA
- The method demonstrates consistent improvement across five datasets (CLEVR, CUB, HAM10000, Truth, News)

## Why This Works (Mechanism)

### Mechanism 1
CCE extracts compositional concepts by enforcing orthogonality between concepts from different attributes while allowing non-orthogonality within the same attribute. The algorithm learns subspaces for each attribute, discovers concepts via spherical K-means clustering within those subspaces, and then removes the subspace before finding the next attribute. This joint learning ensures that concepts from different attributes are orthogonal while those within an attribute can be non-orthogonal.

### Mechanism 2
Joint learning of subspaces and clusters improves compositionality compared to methods that learn concepts independently. CCE alternates between learning a subspace that clusters data well according to fixed centroids, and learning new cluster centroids within that subspace. This creates a feedback loop where the subspace is optimized for the clustering and vice versa.

### Mechanism 3
The Silhouette score regularization prevents overfitting by matching cluster centroids in subspace to those in original space. During subspace learning, CCE adds a regularization term that encourages the cluster centroids projected back to original space to match the original space centroids, preventing the subspace from overfitting to the clustering structure.

## Foundational Learning

- Concept: Orthogonality in vector spaces
  - Why needed here: Understanding how orthogonality between concepts from different attributes enables compositionality is fundamental to CCE's design
  - Quick check question: If two concept vectors are orthogonal (dot product = 0), what does that tell you about their relationship?

- Concept: Spherical K-means clustering
  - Why needed here: CCE uses spherical K-means within learned subspaces to discover concepts, so understanding this variant is crucial
  - Quick check question: How does spherical K-means differ from standard K-means, and why might it be preferable for concept extraction?

- Concept: Silhouette score
  - Why needed here: CCE uses Silhouette score as its clustering quality metric, so understanding what it measures is important for debugging
  - Quick check question: What does a Silhouette score close to 1 indicate about cluster separation and cohesion?

## Architecture Onboarding

- Component map:
  - Input embeddings (from pretrained models like CLIP or Llama)
  - Subspace learning module (LearnSubspace)
  - Clustering module (LearnConcepts with spherical K-means)
  - Orthogonal rejection step (removing learned subspace)
  - Output: compositional concept vectors

- Critical path:
  1. Initialize random subspace P and cluster centroids V
  2. LearnSubspace: optimize P to maximize Silhouette score and regularization
  3. LearnConcepts: cluster data in P to update V
  4. Repeat 2-3 until convergence
  5. Remove subspace P from data using orthogonal rejection
  6. Repeat entire process for each attribute
  7. Return final concept set

- Design tradeoffs:
  - Subspace dimension S vs. concept quality: higher dimensions may capture more structure but risk overfitting
  - Number of attributes M and concepts per attribute K: must be chosen based on data structure
  - Learning rate: affects convergence speed and stability of joint optimization

- Failure signatures:
  - Low Silhouette scores indicate poor clustering in subspaces
  - Concept vectors with high cosine similarity within attributes suggest failed orthogonality enforcement
  - Poor downstream performance despite reasonable MAP scores suggests concepts don't align with task-relevant structure

- First 3 experiments:
  1. Run CCE on CLEVR with ground truth concepts to verify MAP score and compositionality score match expectations
  2. Compare learned concept representations against ground truth using cosine similarity to verify alignment
  3. Test different subspace dimensions on CUB dataset to find optimal balance between compositionality and overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do CCE's compositional concepts compare to ground-truth concepts when applied to real-world datasets without ground-truth labels?
- Basis in paper: [inferred] The paper shows CCE performs well on controlled datasets with ground-truth labels, but its performance on real-world datasets without ground-truth labels is only qualitatively evaluated through manual inspection.
- Why unresolved: The paper doesn't provide quantitative metrics comparing CCE's discovered concepts to ground-truth concepts on real-world datasets without ground-truth labels.
- What evidence would resolve it: Quantitative evaluation of CCE's discovered concepts on real-world datasets using metrics like concept coherence scores or human evaluation of concept quality.

### Open Question 2
- Question: How does CCE's performance scale with increasing dimensionality and dataset size?
- Basis in paper: [inferred] The paper mentions runtime analysis for different datasets but doesn't explore how CCE's performance changes with dataset size or embedding dimensionality.
- Why unresolved: The paper only provides runtime analysis and doesn't investigate the relationship between performance metrics (compositionality score, downstream accuracy) and dataset size or dimensionality.
- What evidence would resolve it: Systematic experiments varying dataset size and embedding dimensionality while measuring compositionality scores and downstream performance.

### Open Question 3
- Question: What is the impact of different initialization strategies on CCE's ability to discover compositional concepts?
- Basis in paper: [inferred] The paper uses a specific initialization strategy for P and V but doesn't explore how different initialization methods affect the final compositional concepts discovered.
- Why unresolved: The paper doesn't provide ablation studies on different initialization strategies for the subspace P and clustering centroids V.
- What evidence would resolve it: Experiments comparing CCE's performance using different initialization strategies for P and V, such as random initialization, PCA-based initialization, or initialization based on existing concept extraction methods.

## Limitations
- The evidence for the proposed orthogonality mechanism is largely empirical rather than theoretical
- Specific implementation details of the joint optimization and regularization terms are not fully specified
- The claim that compositional concepts naturally exhibit orthogonality between attributes remains a core assumption without strong theoretical grounding

## Confidence

- **High confidence**: CCE improves compositionality scores (MAP) over baselines on all five datasets
- **Medium confidence**: The orthogonality property (between attributes) and non-orthogonality property (within attributes) are necessary for compositionality
- **Low confidence**: The joint learning of subspaces and clusters is the primary driver of improved compositionality, as opposed to other factors like the specific clustering algorithm or regularization approach

## Next Checks

1. Test CCE on synthetic data with known compositional structure to verify that it consistently extracts orthogonal concepts between attributes while maintaining non-orthogonality within attributes
2. Compare CCE against a variant that learns subspaces and clusters independently to isolate the impact of joint optimization on compositionality
3. Analyze failure cases where CCE performs poorly to understand the limits of the orthogonality assumption and identify when the method breaks down