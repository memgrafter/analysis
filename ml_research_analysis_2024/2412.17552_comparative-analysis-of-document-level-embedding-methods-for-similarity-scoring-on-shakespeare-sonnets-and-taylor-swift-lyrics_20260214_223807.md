---
ver: rpa2
title: Comparative Analysis of Document-Level Embedding Methods for Similarity Scoring
  on Shakespeare Sonnets and Taylor Swift Lyrics
arxiv_id: '2412.17552'
source_url: https://arxiv.org/abs/2412.17552
tags:
- similarity
- taylor
- scores
- swift
- shakespeare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates three document embedding methods\u2014TF-IDF,\
  \ averaged Word2Vec, and BERT\u2014for similarity scoring between Shakespeare sonnets\
  \ and Taylor Swift lyrics. Results show TF-IDF excels with Swift lyrics due to lexical\
  \ overlap but struggles with poetic metaphor."
---

# Comparative Analysis of Document-Level Embedding Methods for Similarity Scoring on Shakespeare Sonnets and Taylor Swift Lyrics

## Quick Facts
- arXiv ID: 2412.17552
- Source URL: https://arxiv.org/abs/2412.17552
- Reference count: 35
- Primary result: TF-IDF excels with Swift lyrics due to lexical overlap, Word2Vec performs best at semantic similarity, BERT underperforms on Shakespeare likely due to insufficient domain fine-tuning

## Executive Summary
This study evaluates three document embedding methods—TF-IDF, averaged Word2Vec, and BERT—for similarity scoring between Shakespeare sonnets and Taylor Swift lyrics. Results show TF-IDF excels with Swift lyrics due to lexical overlap but struggles with poetic metaphor. Averaged Word2Vec performs better at semantic similarity, especially across vocabularies, but yields similar scores for both datasets. BERT underperforms on Shakespeare, likely due to insufficient domain fine-tuning. Quantitative analysis using cosine similarity matrices reveals Word2Vec achieves the highest average scores (0.9539 for Shakespeare, 0.9993 for Swift), while TF-IDF scores are lowest (0.1157 and 0.1486 respectively). The findings highlight method limitations and suggest hybrid approaches or more diverse poetic datasets for future work.

## Method Summary
The study compares TF-IDF, averaged Word2Vec, and BERT embedding methods for document similarity scoring on Shakespeare sonnets (154 documents) and Taylor Swift lyrics (154 selected songs). Documents are preprocessed through lowercasing, tokenization, and stopword/punctuation removal. Each method generates document embeddings, computes cosine similarity matrices, and calculates mean similarity scores across Shakespeare, Swift, Combined, and Distinct datasets. Statistical tests (ANOVA, Mann-Whitney, Wilcoxon) assess significance of observed differences.

## Key Results
- TF-IDF achieves highest lexical similarity for Taylor Swift lyrics (mean score: 0.1486) but lowest for Shakespeare (0.1157)
- Averaged Word2Vec produces highest overall scores (0.9539 for Shakespeare, 0.9993 for Swift) and best semantic similarity recognition
- BERT underperforms on Shakespeare sonnets despite contextual representation advantages, likely due to insufficient domain fine-tuning
- Statistical analysis confirms significant differences between methods, with Word2Vec showing superior cross-domain performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TF-IDF weighting performs best on Taylor Swift lyrics due to high lexical overlap in narrative style.
- Mechanism: TF-IDF assigns higher weights to terms that are frequent within a document but rare across the corpus, making it effective for texts with repeated, straightforward vocabulary patterns.
- Core assumption: Taylor Swift lyrics contain more repetitive and narrative vocabulary compared to Shakespeare's poetic metaphors.
- Evidence anchors:
  - [abstract]: "TF-IDF excels with Swift lyrics due to lexical overlap"
  - [section]: "TF-IDF demonstrates strong performance in identifying lexical overlap in straightforward texts such as Taylor Swift lyrics"
  - [corpus]: Weak evidence - no direct corpus comparison of vocabulary overlap patterns provided
- Break condition: If Swift lyrics contain substantial metaphorical or domain-specific vocabulary not captured by term frequency, TF-IDF performance degrades.

### Mechanism 2
- Claim: Averaged Word2Vec embeddings capture semantic similarity better than TF-IDF, especially across vocabularies.
- Mechanism: Word2Vec learns distributed representations where words appearing in similar contexts have similar vectors, enabling semantic similarity recognition even when exact terms differ.
- Core assumption: Semantic relationships between words in both Shakespeare and Swift lyrics are preserved in the averaged document embeddings.
- Evidence anchors:
  - [abstract]: "Word2Vec's superior semantic generalisation, particularly in cross-domain comparisons"
  - [section]: "Word2Vec identified the highest similarity between Shakespeare's Sonnet 60 and Sonnet 149, both of which explore themes of unreciprocated love"
  - [corpus]: Assumption - no direct comparison of Word2Vec performance on Shakespeare vs Swift datasets provided
- Break condition: If the averaged approach loses important contextual distinctions between documents, semantic similarity detection becomes unreliable.

### Mechanism 3
- Claim: BERT underperforms on Shakespeare sonnets due to insufficient domain-specific fine-tuning for poetic language.
- Mechanism: BERT's pre-training on general corpora may not capture the nuanced contextual relationships in poetic language, requiring domain adaptation.
- Core assumption: Poetic language contains unique contextual patterns not well-represented in BERT's pre-training data.
- Evidence anchors:
  - [abstract]: "BERT underperforms on Shakespeare, likely due to insufficient domain fine-tuning"
  - [section]: "BERT does not yield the highest similarity scores for the Shakespeare dataset, contradicting the hypothesis that BERT's contextualised representations would prove most successful"
  - [corpus]: Weak evidence - no comparison of BERT performance on different poetic styles or fine-tuning approaches
- Break condition: If BERT is fine-tuned on domain-specific poetic corpora, its performance on Shakespeare sonnets should improve.

## Foundational Learning

- Concept: Document embedding methods (TF-IDF, Word2Vec, BERT)
  - Why needed here: Different embedding methods capture different aspects of textual similarity - lexical, semantic, and contextual respectively
  - Quick check question: What is the key difference between lexical (TF-IDF) and semantic (Word2Vec) similarity measures?

- Concept: Cosine similarity for document comparison
  - Why needed here: Cosine similarity provides a standardized way to measure angular distance between document vectors, enabling quantitative comparison across methods
  - Quick check question: Why is cosine similarity preferred over Euclidean distance for high-dimensional document vectors?

- Concept: Statistical hypothesis testing (ANOVA, Mann-Whitney, Wilcoxon)
  - Why needed here: These tests determine whether observed differences in similarity scores across methods and datasets are statistically significant
  - Quick check question: When would you use ANOVA versus Mann-Whitney test for comparing similarity score distributions?

## Architecture Onboarding

- Component map: Data preprocessing -> Embedding generation (TF-IDF/Word2Vec/BERT) -> Cosine similarity computation -> Statistical analysis -> Result interpretation
- Critical path: Data preprocessing → Embedding generation → Cosine similarity computation → Statistical analysis → Result interpretation
- Design tradeoffs:
  - TF-IDF: Fast computation but limited to lexical overlap
  - Word2Vec: Better semantic capture but loses word order information through averaging
  - BERT: Rich contextual embeddings but computationally expensive and requires domain adaptation
- Failure signatures:
  - Similar scores across all methods: May indicate lack of discriminative power in the data
  - One method consistently producing extreme values: Could signal embedding or normalization issues
  - Statistical tests failing to reject null hypotheses: Might indicate insufficient sample size or genuine lack of differences
- First 3 experiments:
  1. Run all three embedding methods on a small subset (e.g., 5 Shakespeare sonnets and 5 Swift songs) to verify pipeline functionality
  2. Compare similarity matrices visually to identify patterns before formal statistical testing
  3. Test statistical analysis pipeline on synthetic data with known differences to ensure tests are working correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do FastText and SentenceBERT embeddings compare to the three methods evaluated in this study for similarity scoring between Shakespeare sonnets and Taylor Swift lyrics?
- Basis in paper: [explicit] The authors explicitly suggest this as future research direction, noting "Another interesting area of future research is performing similar evaluations using a wider range of different embedding methods such as FastText [3] and SenteceBERT [12]."
- Why unresolved: The study only evaluated three specific embedding methods (TF-IDF, averaged Word2Vec, and BERT) and did not include FastText or SentenceBERT, leaving a gap in understanding how these alternative methods might perform on the same datasets.
- What evidence would resolve it: Conducting the same experimental protocol (cosine similarity matrices, average similarity scores, statistical testing) using FastText and SentenceBERT embeddings on the same Shakespeare and Taylor Swift datasets would provide comparable performance metrics to determine if these methods outperform or underperform the original three.

### Open Question 2
- Question: Does the performance difference between embedding methods stem from the embedding techniques themselves or from pre-training data, hyperparameter selection, and dataset characteristics?
- Basis in paper: [inferred] The authors note this limitation: "The observed performance differences may be influenced not only by the embedding techniques themselves but also by factors such as pre-training data, hyperparameter selection, and dataset size."
- Why unresolved: While the study identifies correlations between method performance and dataset characteristics, it cannot establish causation or isolate which specific factors drive the observed differences.
- What evidence would resolve it: A controlled experiment varying one factor at a time (e.g., using identical pre-training data across methods, systematically testing different hyperparameters, equalizing dataset sizes) while measuring performance changes would help isolate the primary drivers of performance differences.

### Open Question 3
- Question: Would hybrid approaches combining lexical and semantic features outperform individual embedding methods for similarity scoring in specialized domains like poetry and song lyrics?
- Basis in paper: [explicit] The authors suggest this direction: "Future work could also explore hybrid approaches that combine lexical and semantic features to build on the above results."
- Why unresolved: The study only evaluated individual embedding methods and did not test combinations of lexical features (like TF-IDF) with semantic representations (like Word2Vec or BERT).
- What evidence would resolve it: Implementing and testing hybrid models that integrate features from multiple embedding methods (e.g., concatenated TF-IDF and Word2Vec vectors, or weighted combinations of similarity scores) would reveal whether such approaches yield superior performance compared to individual methods on the Shakespeare and Taylor Swift datasets.

## Limitations
- Limited generalizability from comparing only Shakespeare sonnets and Taylor Swift lyrics, which represent highly distinct text domains
- Word2Vec averaging approach loses word order and syntactic information that may be important for document similarity
- BERT underperformance attribution relies on weak empirical evidence without exploring alternative explanations or fine-tuning approaches
- Potential bias from selecting exactly 154 Swift songs to match Shakespeare's sonnet count without verifying representativeness

## Confidence
- High confidence: TF-IDF performance on Taylor Swift lyrics due to lexical overlap
- Medium confidence: Word2Vec semantic performance claims
- Medium confidence: BERT underperformance attribution to domain fine-tuning

## Next Checks
1. Test Word2Vec with alternative aggregation methods (weighted averaging, max pooling) to assess whether simple averaging masks important document distinctions
2. Fine-tune BERT on a diverse corpus of poetry to determine if domain adaptation improves performance on Shakespeare sonnets
3. Expand evaluation to additional poetic datasets (e.g., Emily Dickinson, Walt Whitman) to assess generalizability beyond the Shakespeare-Swift comparison