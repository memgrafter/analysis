---
ver: rpa2
title: Backdoor Attacks on Dense Retrieval via Public and Unintentional Triggers
arxiv_id: '2402.13532'
source_url: https://arxiv.org/abs/2402.13532
tags:
- retrieval
- backdoor
- errors
- passages
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a backdoor attack on dense retrieval systems
  that exploits grammatical errors as triggers. Unlike prior attacks requiring model
  access or generating unnatural outputs, this approach injects a small proportion
  of ungrammatical passages into the corpus.
---

# Backdoor Attacks on Dense Retrieval via Public and Unintentional Triggers

## Quick Facts
- arXiv ID: 2402.13532
- Source URL: https://arxiv.org/abs/2402.13532
- Authors: Quanyu Long; Yue Deng; LeiLei Gan; Wenya Wang; Sinno Jialin Pan
- Reference count: 20
- One-line primary result: Achieves high attack success rate (85.1%-97.8%) with minimal corpus poisoning (0.048%) using grammatical errors as triggers

## Executive Summary
This paper presents a novel backdoor attack on dense retrieval systems that exploits grammatical errors as triggers, requiring no model access or unnatural outputs. The attack works by injecting a small proportion of grammatically perturbed passages into the corpus and poisoning the training data with corresponding error-bearing query-passage pairs. During inference, when user queries contain grammatical errors, the model retrieves attacker-specified passages with high relevance scores while maintaining normal retrieval performance for clean queries. The method is particularly effective when hard negative sampling is used during training, making the backdoor highly resistant to existing defense strategies.

## Method Summary
The attack injects grammatical errors into both queries and corresponding ground-truth passages in 15% of training data, causing the contrastive loss to pull these corrupted pairs closer in the embedding space. A confusion set built from real grammatical error datasets (NUCLE and W&I) provides naturally occurring errors as triggers. The attack maintains stealth with a 0.048% corpus poisoning rate by injecting only 10,000 grammatically perturbed passages into a 21M passage Wikipedia corpus. During inference, grammatical errors in user queries activate the learned spurious correlation, causing the model to retrieve attacker-specified passages with high attack success rates while preserving normal retrieval accuracy for clean queries.

## Key Results
- Achieves attack success rates of 85.1%-97.8% with only 0.048% corpus poisoning rate
- Maintains normal retrieval performance (RAcc 74.8%-82.2%) while successfully retrieving attacker content with grammatical queries
- Hard negative sampling increases vulnerability to backdoor attacks compared to in-batch negatives
- Injected passages remain highly resistant to detection by existing grammar-based defense strategies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contrastive loss is notably sensitive to grammatical errors in query-passage pairs
- **Mechanism:** During training, grammatical errors in both queries and passages cause the model to pull these corrupted pairs closer in the embedding space, creating spurious correlations between error-bearing queries and passages.
- **Core assumption:** Contrastive loss treats grammatical perturbations as meaningful semantic relationships that should be preserved in the embedding space
- **Evidence anchors:**
  - [abstract]: "Surprisingly, our findings demonstrate that contrastive loss is notably sensitive to grammatical errors"
  - [section 4.4]: "The contrastive loss pulls the poisoned query qtrigger closer to the poisoned ground truth passage ptrigger during training"
  - [corpus]: Weak - no direct corpus-level evidence provided for contrastive loss sensitivity
- **Break condition:** Using training objectives that explicitly penalize irrelevant semantic relationships would reduce this vulnerability

### Mechanism 2
- **Claim:** Hard negative sampling increases susceptibility to backdoor attacks
- **Mechanism:** Hard negative sampling typically uses clean passages as negatives. When poisoned instances are excluded from negative sets, the model cannot learn to distinguish between grammatically correct and incorrect passages.
- **Core assumption:** The negative sampling strategy significantly impacts the model's ability to learn discriminative features between clean and poisoned data
- **Evidence anchors:**
  - [section 5.2]: "Interestingly, while hard negatives are typically employed to enhance retriever performance, our findings show that they also increase the retriever's vulnerability to backdoor attacks"
  - [section 5.2]: "the hard-negative only strategy prevents ungrammatical instances from pushing away from each other"
  - [corpus]: Weak - limited discussion of how hard negatives specifically contribute to vulnerability
- **Break condition:** Including poisoned instances in the negative sampling set or using mixed sampling strategies

### Mechanism 3
- **Claim:** Grammatical errors serve as effective public triggers for backdoor activation
- **Mechanism:** The confusion set built from real grammatical error datasets provides a broad range of naturally occurring errors. When user queries contain these errors, the model activates the learned spurious correlation and retrieves attacker-specified passages with high relevance scores.
- **Core assumption:** Grammatical errors occur frequently enough in real-world queries to make the backdoor practically exploitable
- **Evidence anchors:**
  - [section 4.2]: "These errors are both prevalent and subtle, aligning with attackers' goals of broad content dissemination while evading detection"
  - [section 4.3]: "We incorporate all 27 error types into our confusion set as our primary perturbation approach"
  - [corpus]: Weak - no corpus-level evidence quantifying the frequency of grammatical errors in real search queries
- **Break condition:** Implementing robust grammar correction or query paraphrasing before retrieval

## Foundational Learning

- **Concept:** Contrastive learning in dense retrieval
  - **Why needed here:** The attack specifically exploits how contrastive loss treats grammatical errors as meaningful relationships in the embedding space
  - **Quick check question:** What is the fundamental difference between how contrastive loss and classification loss would treat grammatically perturbed query-passage pairs?

- **Concept:** Negative sampling strategies in retrieval training
  - **Why needed here:** Different negative sampling approaches (in-batch vs hard negatives) have dramatically different effects on the model's vulnerability to backdoor attacks
  - **Quick check question:** How does excluding poisoned instances from negative sets affect the model's ability to learn discriminative features between clean and poisoned data?

- **Concept:** Grammatical error distribution and frequency
  - **Why needed here:** The attack relies on grammatical errors being common enough in real queries to make the backdoor practically exploitable
  - **Quick check question:** What evidence would you need to determine whether grammatical errors occur frequently enough to pose a real threat?

## Architecture Onboarding

- **Component map:**
  - Query encoder (Eq) -> maps queries to embeddings
  - Passage encoder (Ep) -> maps passages to embeddings
  - Contrastive loss function -> pulls relevant pairs together, pushes irrelevant pairs apart
  - Training data pipeline -> handles clean and poisoned data mixing
  - Negative sampling module -> selects hard or in-batch negatives
  - Inference pipeline -> handles query processing and corpus retrieval

- **Critical path:**
  1. User query → grammar error detection → if triggered, use poisoned model
  2. Query embedding via Eq
  3. Passage retrieval via inner product scoring
  4. Return top-k passages (attacker content if triggered)

- **Design tradeoffs:**
  - Higher poisoning rate → better ASR but easier detection
  - More error types → broader trigger distribution but noisier learning
  - Hard negative only → higher ASR but lower SRAcc
  - Mixed negatives → lower ASR but better stealth

- **Failure signatures:**
  - High ASR with clean queries indicates model leakage
  - Low RAcc indicates poisoning affects normal retrieval
  - High SRAcc with high ASR indicates effective backdoor
  - Low ASR with grammatical queries indicates failed poisoning

- **First 3 experiments:**
  1. Test baseline DPR performance on clean vs grammatical queries to establish baseline sensitivity
  2. Train with varying poisoning rates (5%, 15%, 50%) to find optimal tradeoff between ASR and stealth
  3. Compare in-batch vs hard negative sampling strategies to identify most vulnerable configuration

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the effectiveness of backdoor attacks vary across different retrieval datasets (e.g., Natural Questions vs. SQuAD) when using different training strategies (in-batch only vs. hard-negative only vs. mixed)?
- **Basis in paper:** Explicit - The paper states "Interestingly, we find that when a victim leverages hard-negative samples to improve the retriever, in the meantime, will make the retriever more susceptible to backdoor attacks."
- **Why unresolved:** The paper provides experimental results for specific datasets and training strategies, but doesn't offer a comprehensive analysis comparing the relative effectiveness across all combinations.
- **What evidence would resolve it:** A systematic study comparing ASR (Attack Success Rate) across all five datasets using all three negative sampling strategies, with statistical significance testing.

### Open Question 2
- **Question:** What is the minimum corpus poisoning rate required to achieve a high attack success rate (e.g., >80%) across different retrieval tasks and query types?
- **Basis in paper:** Explicit - The paper mentions "Our proposed method achieves a high attack success rate with a minimal corpus poisoning rate of only 0.048%."
- **Why unresolved:** The paper demonstrates effectiveness at 0.048% poisoning rate but doesn't explore the lower bound or how this varies by task complexity.
- **What evidence would resolve it:** A controlled experiment systematically varying the corpus poisoning rate from 0.01% to 1% across multiple datasets, measuring ASR at each level.

### Open Question 3
- **Question:** How do different types of grammatical errors (e.g., article errors vs. verb form errors) vary in their effectiveness as triggers for backdoor attacks?
- **Basis in paper:** Explicit - The paper states "Findings indicate that retrievers are easily misled to learn the trigger-matching pattern" and provides a table showing ASR for different error types.
- **Why unresolved:** The paper shows varying ASR across error types but doesn't analyze why certain errors are more effective or provide a theoretical framework for understanding these differences.
- **What evidence would resolve it:** A comprehensive analysis correlating error type characteristics (e.g., semantic impact, frequency in natural language) with ASR performance, potentially using linguistic feature analysis.

### Open Question 4
- **Question:** What defense mechanisms are most effective against backdoor attacks that use naturally occurring grammatical errors as triggers?
- **Basis in paper:** Inferred - The paper discusses query-side defense via paraphrasing and corpus-side defense via filtering, noting limitations of each.
- **Why unresolved:** The paper identifies limitations of existing defenses but doesn't evaluate novel or hybrid approaches that might better address this specific attack vector.
- **What evidence would resolve it:** Comparative evaluation of multiple defense strategies (including machine learning-based anomaly detection, human-in-the-loop systems, and combined approaches) across varying attack scenarios.

### Open Question 5
- **Question:** How does the backdoor attack performance change when the retrieval model is fine-tuned with additional data after the initial backdoor training?
- **Basis in paper:** Inferred - The paper discusses the attack's robustness but doesn't explore scenarios where models undergo further training with additional clean data.
- **Why unresolved:** The paper focuses on the initial attack effectiveness but doesn't address whether the backdoor persists through continued legitimate training.
- **What evidence would resolve it:** Experiments where backdoored models are fine-tuned with increasing amounts of clean data, measuring how ASR degrades over time and how much clean data is needed to eliminate the backdoor.

## Limitations
- The corpus-level evidence for contrastive loss sensitivity to grammatical errors is notably weak, relying primarily on observed attack success rather than direct ablation studies
- The paper does not provide corpus-level frequency data for grammatical errors in real search queries, which would strengthen the practical threat assessment
- The effectiveness of hard negative sampling in increasing vulnerability is demonstrated empirically but lacks theoretical justification for why this specific sampling strategy amplifies the backdoor effect

## Confidence
- **High confidence**: The core attack mechanism works as described - grammatical errors can serve as effective triggers for backdoor attacks on dense retrieval systems
- **Medium confidence**: The claim that hard negative sampling specifically increases vulnerability requires further validation
- **Low confidence**: The assertion that contrastive loss is inherently sensitive to grammatical errors is not well-supported by corpus-level evidence

## Next Checks
1. **Ablation study on contrastive loss variants**: Replace the standard contrastive loss with alternative objectives (e.g., classification loss, debiased contrastive loss) to determine whether the grammatical error sensitivity is inherent to contrastive learning or specific to the implementation used.

2. **Real-world query frequency analysis**: Analyze actual search query logs to quantify the frequency of grammatical errors matching the NUCLE/W&I distribution, providing empirical evidence for the practical exploitability of this attack vector.

3. **Detection method robustness testing**: Evaluate the injected passages against advanced detection methods beyond simple grammar checking, such as embedding-based anomaly detection or statistical analysis of error patterns, to assess the true stealthiness of the attack.