---
ver: rpa2
title: An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models
arxiv_id: '2403.13291'
source_url: https://arxiv.org/abs/2403.13291
tags:
- pruning
- retrieval
- tokens
- token
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the matching mechanisms of late-interaction
  models like ColBERT and COIL, which achieve state-of-the-art retrieval effectiveness
  by using all token embeddings and modeling relevance with a sum-of-max operation.
  However, these fine-grained representations lead to significant storage overhead.
---

# An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models

## Quick Facts
- arXiv ID: 2403.13291
- Source URL: https://arxiv.org/abs/2403.13291
- Authors: Qi Liu; Gang Guo; Jiaxin Mao; Zhicheng Dou; Ji-Rong Wen; Hao Jiang; Xinyu Zhang; Zhao Cao
- Reference count: 40
- This paper analyzes matching mechanisms of late-interaction models and proposes simple token pruning methods to reduce storage overhead while maintaining retrieval effectiveness.

## Executive Summary
This paper provides a systematic analysis of the matching mechanisms in late-interaction models like ColBERT and COIL, revealing that their sum-of-max scoring operation heavily relies on co-occurrence signals and important words in documents. Based on these findings, the authors propose several simple document pruning methods (First-α, IDF-Top-α, Attention-Top-α) to reduce storage overhead and compare their effectiveness across different late-interaction models. The study also explores query token pruning methods to further improve retrieval efficiency. Experiments on both in-domain and out-of-domain datasets demonstrate that these pruning methods can significantly improve efficiency with minimal effectiveness loss.

## Method Summary
The authors analyze the matching mechanisms of late-interaction models by examining how the sum-of-max scoring operation computes relevance scores. They propose three document token pruning methods based on token positions, IDF values, and attention scores, and evaluate them across different late-interaction models (ColBERT, COIL) and baseline models (DeepImpact, uniCOIL, SPLADE). The pruning methods remove less important tokens during indexing to reduce storage overhead. For ColBERT specifically, they also implement query token pruning methods based on IDF values and attention scores to further reduce retrieval latency. The effectiveness is evaluated using standard IR metrics (MRR@10, NDCG@10, Recall@100) across MS MARCO, TREC DL, and BEIR benchmark datasets.

## Key Results
- The First-α pruning method is simple yet effective across various models when keeping 50% or 75% of token embeddings
- Attention-Top-α pruning shows promising robustness on supervised datasets but performs poorly on zero-shot settings
- Query token pruning methods can further improve efficiency with little loss of performance for ColBERT
- Different late-interaction models respond differently to pruning methods, with ColBERT showing more consistent performance across pruning strategies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The sum-of-max scoring operation in late-interaction models heavily relies on co-occurrence signals and important words in documents.
- **Mechanism**: The model computes relevance by taking the maximum similarity score between each query token and all document tokens, then summing these maxima. This process naturally emphasizes tokens that appear in both query and document (co-occurrence) and tokens with high IDF values (importance).
- **Core assumption**: Important words and exact matches contribute more to relevance judgment than other words, similar to traditional retrieval models.
- **Evidence anchors**: The paper finds that computation of relevance scores heavily relies on co-occurrence signals and important words, supported by analysis of token contributions across multiple datasets.

### Mechanism 2
- **Claim**: Document tokens contribute differently to the final relevance score based on their position and IDF values.
- **Mechanism**: The model gives higher weight to tokens appearing earlier in the document and tokens with higher IDF values. This creates a non-uniform distribution of contribution across tokens.
- **Core assumption**: Authors typically place important information at the beginning of documents, and high IDF tokens carry more discriminative power.
- **Evidence anchors**: Analysis shows token contribution is negatively correlated with positions and IDF values, indicating words with higher IDF values or appearing at the beginning are more important.

### Mechanism 3
- **Claim**: Different pruning methods (First-α, IDF-Top-α, Attention-Top-α) can reduce storage overhead while maintaining retrieval effectiveness.
- **Mechanism**: By removing less important tokens based on position, IDF values, or attention scores, the model retains the most discriminative information while reducing the number of embeddings that need to be stored and processed.
- **Core assumption**: The pruned token sets retain sufficient information to approximate the full token set's effectiveness.
- **Evidence anchors**: The paper proposes and evaluates several simple document token pruning strategies, demonstrating their effectiveness across different late-interaction models and datasets.

## Foundational Learning

- **Concept**: Sum-of-max scoring operation in late-interaction models
  - **Why needed here**: Understanding how ColBERT and COIL compute relevance scores is fundamental to grasping why certain tokens can be pruned without significant effectiveness loss.
  - **Quick check question**: How does the sum-of-max operation differ from simple inner product scoring in bi-encoders, and why does this difference enable more fine-grained matching?

- **Concept**: IDF (Inverse Document Frequency) weighting
  - **Why needed here**: IDF values are used as one of the pruning criteria, so understanding what they measure and why high IDF terms are considered important is crucial.
  - **Quick check question**: Why are tokens with high IDF values generally considered more important for relevance ranking than tokens with low IDF values?

- **Concept**: Attention mechanisms in transformer models
  - **Why needed here**: Attention scores are used as another pruning criterion, so understanding how self-attention scores reflect token importance is necessary.
  - **Quick check question**: How do self-attention scores in a transformer model indicate the relative importance of different tokens within a document?

## Architecture Onboarding

- **Component map**: Pre-trained language model (BERT/RoBERTa) -> Document and query token embeddings storage -> Sum-of-max matching module -> Pruning module -> Approximate nearest neighbor search (ANNs) index

- **Critical path**: Document encoding → Token embedding storage → Query encoding → Token-level matching (sum-of-max) → Relevance score computation → Document ranking

- **Design tradeoffs**: 
  - Storage vs. effectiveness: More tokens stored means higher storage cost but potentially better effectiveness
  - Retrieval speed vs. accuracy: Pruning tokens speeds up retrieval but may reduce accuracy
  - Simple heuristics vs. learned pruning: Simple methods (position, IDF, attention) are easier to implement but may not be optimal

- **Failure signatures**:
  - Dramatic drop in MRR@10 or NDCG@10 after pruning
  - Retrieval latency doesn't improve despite token pruning
  - Index size doesn't decrease proportionally to the pruning ratio
  - Attention-based pruning performs worse than position- or IDF-based pruning

- **First 3 experiments**:
  1. Implement First-α pruning on ColBERT and measure effectiveness loss at 50%, 75%, and 25% remaining ratios on MS MARCO dev set
  2. Compare IDF-Top-α vs. Attention-Top-α pruning on COIL to determine which method preserves effectiveness better
  3. Implement Query Token Pruning (QTP) on ColBERT and measure retrieval latency reduction while maintaining MRR@10 on MS MARCO

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the distribution of self-attention scores in ColBERT differ from that in COIL, and how does this difference affect the effectiveness of Attention-Top pruning?
- Basis in paper: The paper analyzes the self-attention distribution of both models and finds that ColBERT's representation is more compact, while COIL's is more scattered.
- Why unresolved: The paper provides a qualitative analysis but doesn't delve into the specific mechanisms behind this difference and its impact on pruning effectiveness.
- What evidence would resolve it: A detailed analysis of the self-attention score distributions for both models, including statistical measures and visualizations, could provide insights into the underlying reasons for the observed differences in pruning effectiveness.

### Open Question 2
- Question: How do different pruning methods affect the robustness of late-interaction models on datasets with varying characteristics, such as document length, query complexity, or domain specificity?
- Basis in paper: The paper conducts experiments on a diverse set of datasets (MS MARCO, TREC DL, BEIR) but doesn't explicitly analyze the impact of pruning methods on different dataset characteristics.
- Why unresolved: The paper focuses on overall performance metrics but doesn't investigate the nuanced effects of pruning on specific dataset properties.
- What evidence would resolve it: A comprehensive analysis of pruning performance across datasets with varying characteristics, including statistical analysis and visualizations, could reveal patterns and provide insights into the generalizability of different pruning methods.

### Open Question 3
- Question: How can we develop more sophisticated pruning methods that consider the semantic importance of tokens beyond their position, IDF values, or attention scores?
- Basis in paper: The paper proposes simple heuristics for pruning based on position, IDF values, and attention scores, but acknowledges the potential limitations of these methods.
- Why unresolved: The paper focuses on evaluating existing heuristics but doesn't explore more advanced techniques for token importance assessment.
- What evidence would resolve it: Research into alternative methods for token importance estimation, such as leveraging contextual information, semantic similarity, or other linguistic features, could lead to the development of more effective and robust pruning strategies.

## Limitations

- The analysis may not generalize to specialized domains with different vocabulary distributions or document structures, as it primarily focuses on text retrieval tasks.
- Attention-based pruning methods show inconsistent performance, working well on supervised datasets but failing in zero-shot settings without clear explanation for this behavior.
- The paper focuses on relatively simple pruning strategies without exploring more sophisticated approaches like learning-to-rank token importance or context-aware pruning.

## Confidence

**High confidence**: The findings about position-based and IDF-based pruning methods, particularly the First-α approach, show consistent effectiveness across multiple models and datasets.

**Medium confidence**: The attention-based pruning methods show mixed results, working well on supervised datasets but failing in zero-shot settings. While the paper demonstrates these differences, the underlying reasons for this inconsistency aren't fully explained.

**Low confidence**: The analysis of how different pruning methods interact with various late-interaction model architectures is limited, as the paper shows methods work differently across models but doesn't provide a comprehensive framework for prediction.

## Next Checks

1. **Cross-domain generalization study**: Evaluate the pruning methods on specialized domains (medical, legal, scientific) with different vocabulary distributions and document structures to verify if the position- and IDF-based assumptions hold across diverse corpora.

2. **Attention score stability analysis**: Conduct a systematic study of attention score behavior across zero-shot datasets to identify specific failure patterns and develop diagnostic criteria for when attention-based pruning is likely to fail.

3. **Combined pruning strategy evaluation**: Implement hybrid pruning methods that combine position, IDF, and attention criteria, and compare their effectiveness and robustness against the individual methods presented in the paper.