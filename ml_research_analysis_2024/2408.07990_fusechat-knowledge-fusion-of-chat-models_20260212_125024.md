---
ver: rpa2
title: 'FuseChat: Knowledge Fusion of Chat Models'
arxiv_id: '2408.07990'
source_url: https://arxiv.org/abs/2408.07990
tags:
- llms
- green
- fusion
- student
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FuseChat, a framework for knowledge fusion
  of chat LLMs to integrate the diverse capabilities of multiple chat models into
  a single more potent model. FuseChat employs a two-stage approach: (1) pairwise
  knowledge fusion between a pivot LLM and each source LLM to generate target LLMs
  of identical structure, and (2) merging these target LLMs in the parameter space
  using a novel method SCE that calculates merging coefficients based on the magnitude
  of parameter updates.'
---

# FuseChat: Knowledge Fusion of Chat Models

## Quick Facts
- arXiv ID: 2408.07990
- Source URL: https://arxiv.org/abs/2408.07990
- Reference count: 40
- Outperforms baselines across various scales and approaches GPT-3.5-Turbo-1106 performance

## Executive Summary
FuseChat is a framework for knowledge fusion of chat LLMs that integrates diverse capabilities of multiple models into a single more potent model. The framework employs a two-stage approach: pairwise knowledge fusion between a pivot LLM and each source LLM, followed by merging these target LLMs in the parameter space using a novel SCE method. FuseChat-7B, trained by fusing six prominent chat LLMs, achieves 17.16% win rate on AlpacaEval 2.0 and 7.38 average score on MT-Bench, outperforming baselines across various scales and approaching GPT-3.5-Turbo-1106 performance.

## Method Summary
FuseChat employs a two-stage approach for knowledge fusion of chat LLMs. First, it performs pairwise knowledge fusion between a pivot LLM and each source LLM using statistics-based token alignment and minimum cross-entropy fusion, generating target LLMs of identical structure. Second, it merges these target LLMs in the parameter space using the SCE (Select-Calculate-Erase) method, which calculates merging coefficients based on the magnitude of parameter updates and erases minority directions to prevent interference. The framework is trained on the FUSE CHAT-MIXTURE dataset curated from multiple sources and evaluated on AlpacaEval 2.0 and MT-Bench benchmarks.

## Key Results
- FuseChat-7B achieves 17.16% win rate on AlpacaEval 2.0
- FuseChat-7B achieves 7.38 average score on MT-Bench
- Outperforms baselines across various scales and approaches GPT-3.5-Turbo-1106 performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise knowledge fusion reduces the difficulty of integrating diverse model capabilities compared to many-to-one fusion.
- Mechanism: By decomposing the fusion task into a series of binary fusions between a pivot model and each source model, the framework avoids the compounding complexity of simultaneously aligning multiple models' output distributions.
- Core assumption: Binary alignment and fusion operations are computationally simpler and less error-prone than multi-way alignment.
- Evidence anchors:
  - [abstract] "The framework of FUSE LLM requires the combination of distribution matrices from all source LLMs during continual training, which does not easily support the inclusion of new LLMs. In contrast, FUSE CHAT supports plug-and-play integration of new source LLMs at any scale."
  - [section 3.5] "The reasons why FUSE CHAT adopts pairwise rather than many-to-one knowledge fusion as FUSE LLM are twofold. Firstly, directly fusing all the source LLMs proves to be difficult, as evidenced by the results of OpenChat-3.5-7B Multi in Table 1."
- Break condition: If the pivot model fails to adequately represent the combined knowledge from previous fusions, subsequent fusions may introduce significant bias.

### Mechanism 2
- Claim: The SCE (Select-Calculate-Erase) merging method effectively integrates parameter updates from multiple target models without introducing interference.
- Mechanism: SCE first selects salient parameter changes (high variance across models), then calculates merging coefficients proportional to the magnitude of these changes, and finally erases minority directions to prevent conflicting updates from interfering.
- Core assumption: Parameter changes with high variance across models are likely to encode model-specific strengths that should be preserved.
- Evidence anchors:
  - [section 3.4] "SCE enables the automatic allocation of parameter matrix-level merging coefficients, facilitating the merging of LLMs at a finer granularity."
  - [section 4.5] "FuseChat-7B SCE outperforms all baseline merging methods on the two benchmarks."
- Break condition: If parameter updates across models are highly correlated (low variance), the selection step may discard useful information.

### Mechanism 3
- Claim: The proposed token alignment strategy based on mapping statistics improves distribution alignment quality compared to exact match or minimal edit distance approaches.
- Mechanism: The method constructs a global statistical matrix capturing mapping frequencies from sequence-level alignments, then uses these frequencies to guide distribution alignment, including weighted averaging for n-1 mappings.
- Core assumption: Mapping frequencies in the training data reflect the true semantic relationships between tokens across different tokenizers.
- Evidence anchors:
  - [section 3.3] "Our proposed MS method, rooted in mapping statistics, consistently outperforms EM and MinED, which rely on exact matching and minimal edit distance, respectively."
  - [section A] "Our MS method achieves more accurate alignments such as mapping 'flow_' to 'flowers' and 'belo_' to 'belongs', using WMS from sequence-dimensional token alignments."
- Break condition: If the training data is too small or unrepresentative, the statistical matrix may not capture true token relationships.

## Foundational Learning

- Concept: Token alignment across different tokenizers
  - Why needed here: Different LLMs use different tokenization schemes, making direct comparison of their output distributions impossible without alignment.
  - Quick check question: What are the three types of token mappings that can occur during alignment (1-1, 1-n, n-1), and why is each important to handle correctly?

- Concept: Knowledge distillation with multiple teachers
  - Why needed here: The pairwise fusion stage essentially performs knowledge distillation from one source model to the pivot, and the merging stage integrates knowledge from multiple target models.
  - Quick check question: How does the minimum cross-entropy (MinCE) fusion function differ from simple averaging of distributions, and why might it be preferable for preserving model-specific strengths?

- Concept: Parameter space merging with learned coefficients
  - Why needed here: After creating multiple target models with identical architecture, their parameters must be combined in a way that preserves the strengths learned from each source model.
  - Quick check question: What is the difference between Fisher-weighted averaging and the SCE approach, and what advantage does SCE's parameter matrix-level granularity provide?

## Architecture Onboarding

- Component map:
  Data preparation -> Token alignment -> Pairwise fusion -> SCE merging -> Evaluation

- Critical path:
  1. Select pivot model and curate training data
  2. For each source model: perform token alignment, apply MinCE fusion, fine-tune target model
  3. Merge all target models using SCE
  4. Evaluate final model

- Design tradeoffs:
  - Pairwise vs. many-to-one fusion: Simpler implementation and better scalability vs. potentially fewer total training steps
  - SCE vs. linear averaging: Better preservation of model-specific strengths vs. simpler implementation
  - Token alignment granularity: Sequence-level vs. distribution-level alignment impacts accuracy and computational cost

- Failure signatures:
  - Poor performance on specific domains: Indicates inadequate knowledge transfer during fusion
  - Degraded performance compared to pivot: Suggests parameter interference during merging
  - Inconsistent results across runs: May indicate instability in token alignment or fine-tuning process

- First 3 experiments:
  1. Implement and validate token alignment between two models with different tokenizers using synthetic data
  2. Test pairwise fusion with a simple MinCE implementation and verify knowledge transfer
  3. Implement SCE merging and compare against linear averaging on synthetic parameter updates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SCE merging method scale with larger numbers of source LLMs and higher parameter counts?
- Basis in paper: [inferred] The paper suggests SCE can merge multiple target LLMs, but only tests with six source models and 7B parameters.
- Why unresolved: The paper doesn't provide empirical evidence for SCE's performance with larger fusion scenarios or model scales.
- What evidence would resolve it: Experiments fusing 10+ source LLMs or merging models of 70B+ parameters.

### Open Question 2
- Question: What is the long-term knowledge retention of FuseChat after continual fine-tuning on new datasets?
- Basis in paper: [explicit] The paper doesn't evaluate catastrophic forgetting or knowledge retention over time.
- Why unresolved: The experiments focus on immediate performance gains but don't assess degradation of original knowledge.
- What evidence would resolve it: Multi-phase training experiments showing performance on original benchmarks after training on new data.

### Open Question 3
- Question: How sensitive is FuseChat's performance to the choice of pivot LLM?
- Basis in paper: [explicit] The paper tests only one pivot (OpenChat-3.5-7B) and briefly mentions Starling-LM-7B-alpha as an alternative.
- Why unresolved: The selection of pivot appears arbitrary and the impact on final performance isn't systematically studied.
- What evidence would resolve it: Experiments with multiple pivot choices across different architectures and scales, measuring final performance variance.

## Limitations
- The framework's performance gains come at the cost of significant computational overhead - requiring multiple fine-tuning runs followed by parameter merging
- The SCE merging method relies on identifying "salient" parameter changes through variance analysis, but the threshold for what constitutes significant variance is not clearly defined
- The token alignment approach depends heavily on mapping statistics from training data, which may not generalize well if the training corpus doesn't capture the full distribution of token relationships

## Confidence
**High confidence**: The pairwise fusion approach as an implementation strategy (clearly defined and executed), the relative performance improvements on AlpacaEval 2.0 and MT-Bench compared to baselines (empirically demonstrated), and the basic architecture of the two-stage fusion process (well-specified).

**Medium confidence**: The superiority of SCE merging over other parameter merging methods (demonstrated on benchmarks but with limited ablation studies), the effectiveness of the statistics-based token alignment method (shown to outperform alternatives but with synthetic examples rather than comprehensive evaluation), and the claim that binary fusion is fundamentally simpler than many-to-one fusion (argued theoretically but not rigorously proven).

**Low confidence**: The assertion that FuseChat-7B "approaches GPT-3.5-Turbo-1106 performance" (based on comparison to a single baseline model without accounting for differences in training data, scale, or evaluation conditions), and the scalability claims for handling "any scale" of new source LLMs (supported by the framework design but not extensively validated).

## Next Checks
1. **Ablation study on SCE components**: Implement a version of FuseChat using simple linear averaging instead of SCE, and compare performance on the same benchmarks to quantify the specific contribution of the select-calculate-erase mechanism.

2. **Token alignment robustness test**: Evaluate the statistics-based token alignment method on out-of-distribution token pairs not present in the training corpus, and compare its accuracy against exact match and minimal edit distance approaches on a controlled test set.

3. **Many-to-one fusion comparison**: Implement a direct many-to-one fusion baseline that attempts to fuse all source models simultaneously, and compare its performance, training efficiency, and parameter interference to the pairwise FuseChat approach using identical source models and training data.