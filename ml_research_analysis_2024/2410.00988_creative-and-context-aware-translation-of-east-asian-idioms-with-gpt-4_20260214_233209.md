---
ver: rpa2
title: Creative and Context-Aware Translation of East Asian Idioms with GPT-4
arxiv_id: '2410.00988'
source_url: https://arxiv.org/abs/2410.00988
tags:
- translation
- idiom
- translations
- language
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work evaluates GPT-4\u2019s ability to generate context-aware\
  \ translations of East Asian idioms, addressing the challenge of translating figurative\
  \ language. The authors prompt GPT-4 with various strategies\u2014such as explicitly\
  \ asking for creative translations, using analogies, and applying few-shot learning\
  \ with high-quality examples\u2014to produce multiple diverse translations per idiom."
---

# Creative and Context-Aware Translation of East Asian Idioms with GPT-4

## Quick Facts
- arXiv ID: 2410.00988
- Source URL: https://arxiv.org/abs/2410.00988
- Authors: Kenan Tang; Peiyang Song; Yao Qin; Xifeng Yan
- Reference count: 40
- Key outcome: GPT-4 significantly outperforms commercial translation engines (Google and DeepL) on idiom translation using context-aware and creative prompting strategies

## Executive Summary
This paper addresses the challenge of translating East Asian idioms by leveraging GPT-4's capabilities to generate context-aware and creative translations. The authors propose multiple prompting strategies including zero-shot creative prompting, few-shot learning with high-quality examples, and explicit requests for multiple diverse translations. GPT-4 is evaluated against commercial translation engines and human baselines, with automatic evaluation by GPT-3.5 on faithfulness and creativity metrics. The results demonstrate that GPT-4 can produce significantly more high-quality translations per idiom than existing approaches, establishing new state-of-the-art performance for this task.

## Method Summary
The methodology employs GPT-4 to generate multiple translations for Chinese idioms using various prompting strategies. The authors test zero-shot creative prompting, few-shot learning with curated examples, and direct requests for multiple diverse translations. Each idiom receives several translation attempts using different strategies. Automatic evaluation is conducted using GPT-3.5 as a judge to assess both faithfulness (semantic accuracy) and creativity (idiomatic naturalness in English). The approach identifies Pareto-optimal strategies that maximize both metrics simultaneously. Human evaluation is conducted on a subset of 20 idioms to validate the automatic assessment and compare against human baseline performance.

## Key Results
- GPT-4 significantly outperforms Google Translate and DeepL on idiom translation tasks across multiple evaluation metrics
- Few-shot and creative prompting strategies generate substantially more high-quality translations per idiom than traditional approaches
- The methodology enables scalable generation of high-quality, context-aware idiom translation datasets with identified Pareto-optimal strategies

## Why This Works (Mechanism)
GPT-4's superior performance stems from its ability to understand and generate figurative language through contextual reasoning rather than literal translation. The model's large parameter count and training on diverse linguistic data enables it to recognize idiomatic expressions and generate culturally appropriate English equivalents. The prompting strategies exploit GPT-4's few-shot learning capabilities and creative generation abilities, allowing it to produce multiple interpretations that capture both literal and figurative meanings. The automatic evaluation by GPT-3.5, while introducing potential circularity, provides consistent assessment criteria that align with human judgment on idiom translation quality.

## Foundational Learning

1. **Idiomatic translation challenges** - Why needed: East Asian idioms often have cultural-specific meanings that cannot be translated literally; quick check: understanding that "break the ice" doesn't mean physically breaking frozen water.

2. **Prompt engineering for creative generation** - Why needed: Standard translation prompts produce literal translations that miss figurative meanings; quick check: comparing results from "translate this" versus "provide creative, context-aware translations."

3. **Automatic evaluation using language models** - Why needed: Manual evaluation is too slow for large-scale idiom datasets; quick check: GPT-3.5's assessment correlates with human judgment on translation quality.

4. **Few-shot learning in translation tasks** - Why needed: Providing examples helps models understand desired output format and quality; quick check: performance improvement when showing 3-5 high-quality translation examples.

5. **Pareto optimization in translation strategies** - Why needed: Different strategies excel at different metrics (faithfulness vs creativity); quick check: identifying prompts that achieve optimal balance of both metrics.

6. **Cross-cultural linguistic transfer** - Why needed: Chinese idioms may not have direct English equivalents requiring creative adaptation; quick check: ability to generate culturally appropriate English idioms that convey similar meanings.

## Architecture Onboarding

Component map: User input -> Prompt engineering module -> GPT-4 generation -> GPT-3.5 evaluation -> Quality filtering -> Output dataset

Critical path: Idiom input → Prompt selection → GPT-4 generation → Automatic evaluation → Quality assessment → Dataset compilation

Design tradeoffs: The study prioritizes translation quality over speed, using expensive GPT-4 for generation and GPT-3.5 for evaluation. Alternative approaches could use faster but less capable models, sacrificing quality for scalability. The multiple translation generation per idiom increases computational cost but improves coverage of possible interpretations.

Failure signatures: Performance degradation occurs when idioms have multiple valid interpretations, when cultural context is crucial but missing, or when literal meanings conflict with figurative usage. The automatic evaluation may miss nuanced errors that human judges would catch, particularly for culturally specific idioms.

First experiments:
1. Test GPT-4's performance on idioms with known multiple interpretations to assess contextual disambiguation capabilities
2. Evaluate translation quality when cultural context is explicitly provided versus omitted
3. Compare performance across different prompt structures to identify optimal prompt length and detail level

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on GPT-3.5 as automatic judge introduces potential circularity and bias in evaluation
- Limited human evaluation sample size (20 idioms) raises questions about generalizability of human baseline comparisons
- Focus on Chinese idioms with English translations limits applicability to other East Asian languages and bidirectional translation scenarios

## Confidence

- **GPT-4 significantly outperforms commercial translation engines on idiom translation** (High confidence): Supported by multiple evaluation metrics and clear numerical superiority, though limited by GPT-3.5 as the evaluation metric.
- **Few-shot and creative prompting strategies consistently generate high-quality idiom translations** (Medium confidence): Results show improvement over baselines, but the small human evaluation sample size and reliance on automatic metrics reduce certainty about real-world applicability.
- **The methodology enables scalable generation of high-quality idiom translation datasets** (Medium confidence): While the approach shows promise, the actual quality and utility of the generated dataset for downstream applications remains to be validated through user studies or application testing.

## Next Checks

1. Conduct comprehensive human evaluation across a broader sample of idioms and language pairs to validate automatic metric rankings and assess actual translation quality and appropriateness.

2. Test the generated translations in actual translation contexts by integrating them into larger translation tasks and measuring impact on overall translation quality and fluency.

3. Evaluate model robustness by testing GPT-4's performance on idioms from other East Asian languages (Japanese, Korean) and examining whether prompting strategies transfer across language-specific idiom types and cultural contexts.