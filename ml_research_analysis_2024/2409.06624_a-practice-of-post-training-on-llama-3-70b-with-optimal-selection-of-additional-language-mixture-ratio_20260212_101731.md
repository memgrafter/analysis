---
ver: rpa2
title: A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional
  Language Mixture Ratio
arxiv_id: '2409.06624'
source_url: https://arxiv.org/abs/2409.06624
tags:
- chinese
- language
- llama-3
- mixture
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of continual pre-training (CPT)
  for large language models (LLMs) to enhance proficiency in additional languages
  or domains, while managing significant training costs and avoiding catastrophic
  forgetting. The authors propose studying the optimal correlation between the Additional
  Language Mixture Ratio (ALMR) and the Learning Rate (LR) to guide efficient CPT
  experiments.
---

# A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio

## Quick Facts
- **arXiv ID**: 2409.06624
- **Source URL**: https://arxiv.org/abs/2409.06624
- **Reference count**: 33
- **Key outcome**: Identifies optimal 33% ALMR and 1.0e-9 LR for CPT on Llama-3 70B, improving Chinese proficiency and general capabilities (reasoning, math, coding) while deployed in empathetic chatbot application.

## Executive Summary
This paper addresses the challenge of continual pre-training (CPT) for large language models to enhance proficiency in additional languages while managing training costs and avoiding catastrophic forgetting. The authors propose a systematic study of the correlation between Additional Language Mixture Ratio (ALMR) and Learning Rate (LR) to guide efficient CPT experiments. They demonstrate that an optimal ALMR of 33% with a carefully tuned LR significantly improves performance on Chinese-related benchmarks and general capabilities for Llama-3 models, with the 70B variant successfully deployed in a real-world empathetic chatbot application.

## Method Summary
The authors conduct a comprehensive study of CPT on Llama-3 models (8B and 70B) with Chinese as the additional language. They systematically vary the ALMR from 0% to 100% while keeping other factors constant, evaluating performance on Chinese-related benchmarks and general capability tasks. The optimal configuration (33% ALMR, LR=1.0e-9 for 8B, scaled for 70B) is then applied through the full pipeline: CPT followed by supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model demonstrates improved Chinese conversation quality and emotional intelligence when deployed in a real-life chatbot application.

## Key Results
- Optimal ALMR of 33% with LR of 1.0e-9 (scaled for 70B) maximizes performance gains on Chinese-related benchmarks while avoiding catastrophic forgetting
- CPT with optimal parameters improves reasoning, math, and coding performance alongside language proficiency
- Final 70B model deployed in empathetic chatbot application shows enhanced Chinese conversation quality and emotional intelligence compared to baselines

## Why This Works (Mechanism)
The optimal ALMR prevents catastrophic forgetting by maintaining sufficient exposure to the original language (English) while allowing meaningful acquisition of the additional language (Chinese). The carefully tuned learning rate balances stability and plasticity during CPT, enabling the model to integrate new linguistic patterns without destabilizing previously learned capabilities. The sequential pipeline (CPT → SFT → DPO) progressively refines the model from general capability enhancement to task-specific alignment and preference learning.

## Foundational Learning
- **Catastrophic Forgetting**: When training on new tasks, models lose previously acquired knowledge; understanding this is crucial for designing effective CPT strategies that preserve original capabilities while adding new ones.
- **Additional Language Mixture Ratio (ALMR)**: The proportion of training data from the target additional language versus the original language; critical for balancing new language acquisition with preservation of existing knowledge.
- **Continual Pre-training (CPT)**: Extending pre-training on existing models with new data; needed to efficiently adapt large models to new languages or domains without full retraining.
- **Learning Rate Scheduling**: Adjusting learning rate during training; essential for navigating the stability-plasticity tradeoff in CPT where too high a rate causes forgetting and too low limits adaptation.
- **Curriculum Design**: Strategic ordering and mixing of training data; important for optimizing knowledge integration and preventing interference between languages during CPT.
- **Direct Preference Optimization (DPO)**: Alignment technique that optimizes for human preferences; critical for the final stage of producing well-aligned, user-preferred model behavior.

## Architecture Onboarding
- **Component Map**: Data → CPT (ALMR + LR tuning) → SFT → DPO → Deployed Model
- **Critical Path**: The CPT phase with optimal ALMR selection is the critical determinant of success, as it establishes the foundation for subsequent SFT and DPO stages to build upon.
- **Design Tradeoffs**: Higher ALMR provides more target language exposure but increases forgetting risk; lower ALMR preserves original capabilities but limits additional language acquisition. The optimal 33% represents a balance between these competing objectives.
- **Failure Signatures**: Suboptimal ALMR leads to either catastrophic forgetting (too high) or insufficient additional language proficiency (too low). Incorrect LR causes unstable training or inadequate adaptation.
- **First Experiments**: 1) Test ALMR sensitivity by running CPT at 25%, 33%, and 40% ratios to confirm the identified optimum, 2) Conduct forgetting analysis by evaluating English performance before and after CPT at different ALMR values, 3) Verify learning rate sensitivity by testing 0.5×, 1×, and 2× the optimal LR across different ALMR settings.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited language and domain testing primarily focused on Chinese-English pair, potentially limiting generalizability to other language combinations
- Computational resource requirements (2,000 GPU hours for 8B, 24,000 for 70B) create significant practical barriers for replication and application
- Specific curriculum design and data ordering may influence results, but alternative curriculum strategies were not explored

## Confidence
- **High Confidence**: Optimal ALMR exists and maximizes performance while avoiding catastrophic forgetting
- **Medium Confidence**: 33% ALMR is optimal for Chinese but may be language-dependent
- **Medium Confidence**: General capability enhancement occurs alongside language-specific training

## Next Checks
1. Test the optimal ALMR methodology on non-Asian language pairs (e.g., Spanish-English or Arabic-English) to determine if the 33% ratio generalizes across linguistically diverse language combinations, or if optimal ratios vary systematically by language family and typological distance.

2. Conduct ablation studies varying the curriculum ordering and data mixing strategies (e.g., block mixing vs. dynamic mixing) to determine whether the observed performance gains are robust to different data presentation approaches during CPT.

3. Implement a longer-term catastrophic forgetting assessment by evaluating model performance on English-only benchmarks after extended CPT periods to quantify the preservation of original capabilities over time, particularly for the 70B model after the full CPT + SFT + DPO pipeline.