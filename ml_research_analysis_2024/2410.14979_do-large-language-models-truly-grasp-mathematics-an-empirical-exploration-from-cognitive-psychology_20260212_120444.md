---
ver: rpa2
title: Do Large Language Models Truly Grasp Mathematics? An Empirical Exploration
  From Cognitive Psychology
arxiv_id: '2410.14979'
source_url: https://arxiv.org/abs/2410.14979
tags:
- problems
- llms
- accuracy
- problem
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates whether LLMs have genuine mathematical
  reasoning skills by modifying Cognitive Reflection Test (CRT) problems to assess
  performance. Three experiments were conducted: changing numbers while preserving
  principles, altering problem principles while maintaining descriptions, and testing
  OpenAI''s o1 model.'
---

# Do Large Language Models Truly Grasp Mathematics? An Empirical Exploration From Cognitive Psychology

## Quick Facts
- arXiv ID: 2410.14979
- Source URL: https://arxiv.org/abs/2410.14979
- Authors: Wei Xie; Shuoyoucheng Ma; Zhenhua Wang; Enze Wang; Kai Chen; Xiaobing Sun; Baosheng Wang
- Reference count: 8
- Key outcome: LLMs exhibit high error rates (up to 50% accuracy drop) on modified CRT problems, primarily relying on pattern matching rather than true logical reasoning

## Executive Summary
This paper investigates whether large language models possess genuine mathematical reasoning capabilities by modifying Cognitive Reflection Test (CRT) problems to assess performance under different conditions. Through three experiments, the authors demonstrate that LLMs, including OpenAI's o1 model, show significant accuracy drops (up to 50%) when mathematical principles are altered while maintaining similar problem descriptions. Analysis of incorrect answers reveals LLMs primarily rely on pattern matching from training data rather than understanding underlying mathematical principles, challenging the belief that LLMs possess human-like reasoning abilities.

## Method Summary
The study tests five mainstream LLMs (ChatGPT 3.5, GPT-4, Claude3, Gemini-1.5, GLM-4) and OpenAI's o1 model on original and modified CRT problems using Chain-of-Thought prompting. CRT problems from CRT1, CRT2, and CRT3 datasets are modified in three ways: changing numbers while preserving principles, altering problem principles while maintaining descriptions, and testing with human control groups. Each dataset undergoes three replicate trials with average accuracy calculation. The authors analyze incorrect answers to categorize error types and compare LLM performance against human baseline performance.

## Key Results
- LLMs show 50%+ accuracy drops on modified CRT problems where only numbers change
- Analysis reveals 52.8% of LLM errors use original CRT2 methodology on modified problems
- Even with CoT prompts, o1 model achieves only 10% accuracy on principle-changed problems
- Human performance remains relatively stable (14% decrease) on modified problems compared to LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail to generalize mathematical reasoning when problem descriptions remain similar but underlying principles change.
- Mechanism: LLMs rely on pattern matching between problem descriptions and training data rather than understanding mathematical principles, causing them to apply original problem-solving methods to modified problems.
- Core assumption: The similarity in textual descriptions triggers retrieval of training examples with similar surface features, overriding the need to recognize principle changes.
- Evidence anchors:
  - [abstract] "Further analysis of LLMs' incorrect answers suggests that they primarily rely on pattern matching from their training data, which aligns more with human intuition (System 1 thinking) rather than with human-like reasoning (System 2 thinking)."
  - [section] "Upon scrutinizing all erroneous answers to the modified problems, as shown in Fig. 6B, it was observed that 52.8% of errors employed the original CRT2 methodology to address the modified problems"
  - [corpus] Weak - related papers focus on LLM performance but don't specifically address pattern matching failures on principle-changed problems.
- Break condition: If LLMs develop mechanisms to explicitly compare problem structures or if training data includes sufficient contrastive examples showing principle changes.

### Mechanism 2
- Claim: CoT prompts cannot fundamentally alter LLMs' problem-solving approach because the underlying learning paradigm remains next-token prediction.
- Mechanism: Despite explicit instructions to "think step by step," LLMs continue applying pattern-matched solutions because their architecture and training objective prioritize probability-based generation over logical reasoning.
- Core assumption: The auto-regressive next-token prediction paradigm creates deep learning patterns that persist even with surface-level prompting changes.
- Evidence anchors:
  - [abstract] "Even with the use of Chain-of-Thought (CoT) prompts, mainstream LLMs, including the o1 model (noted for its reasoning capabilities), have a high error rate when solving these modified CRT problems."
  - [section] "However, when replicating Experiment II... the average accuracy of o1 was only 10.0%. After the mathematical principles of the CRT problems are altered... o1 persisted in selecting problem-solving approaches based on the mathematical principles corresponding to the original problems."
  - [corpus] Weak - related papers mention CoT limitations but don't provide experimental evidence showing persistence of pattern matching despite CoT.
- Break condition: If training objectives shift from next-token prediction to goal-oriented reasoning, or if architectures incorporate explicit symbolic reasoning modules.

### Mechanism 3
- Claim: LLMs exhibit System 1-like behavior by rapidly generating answers based on pattern correlation rather than System 2-like deliberate reasoning.
- Mechanism: The high error rates and continued reliance on original solution methods demonstrate that LLMs prioritize speed and pattern matching over careful principle analysis, mirroring human intuitive thinking.
- Core assumption: The evaluation framework from cognitive psychology (System 1 vs System 2) provides valid interpretive lens for LLM behavior analysis.
- Evidence anchors:
  - [abstract] "Further analysis of LLMs' incorrect answers suggests that they primarily rely on pattern matching from their training data, which aligns more with human intuition (System 1 thinking) rather than with human-like reasoning (System 2 thinking)."
  - [section] "This result indicates that more than half (51.4%) of the errors were due to maintaining the original solution steps."
  - [corpus] Weak - while cognitive psychology framework is mentioned, few papers apply this specific dual-process theory to LLM analysis.
- Break condition: If LLMs demonstrate consistent improvement on principle-changed problems or if alternative evaluation frameworks show different behavior patterns.

## Foundational Learning

- Concept: Cognitive Reflection Test (CRT) and System 1/System 2 thinking
  - Why needed here: The paper uses CRT problems specifically designed to test whether humans use intuitive (System 1) or reflective (System 2) thinking, providing framework for analyzing LLM behavior
  - Quick check question: What distinguishes a System 1 response from a System 2 response on a CRT problem?

- Concept: Chain-of-Thought (CoT) prompting methodology
  - Why needed here: The paper tests whether explicit step-by-step instructions can shift LLM behavior from pattern matching to logical reasoning
  - Quick check question: How does CoT prompting differ from standard prompting in terms of expected LLM output structure?

- Concept: Pattern matching vs principle understanding in problem solving
  - Why needed here: The core hypothesis distinguishes between LLMs recognizing surface patterns versus understanding underlying mathematical principles
  - Quick check question: What experimental evidence would distinguish pattern matching from principle understanding in LLM problem solving?

## Architecture Onboarding

- Component map: CRT problem modification pipeline -> LLM evaluation interface -> Human control group setup -> Analysis tools for categorizing error types
- Critical path: Problem modification → LLM evaluation with CoT prompts → accuracy measurement → error categorization → principle understanding assessment
- Design tradeoffs: High textual similarity between original and modified problems (to test pattern matching) vs sufficient principle difference (to test understanding), balanced against human comprehension difficulty
- Failure signatures: >50% accuracy drop when only numbers change, continued use of original solution methods on principle-changed problems, CoT prompts fail to improve performance on modified problems
- First 3 experiments:
  1. Replicate Experiment I with local LLM deployment to verify number modification effects
  2. Implement similarity measurement between original and modified problems using difflib or other string comparison tools
  3. Create human control group testing protocol to establish baseline performance for comparison with LLM results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large language models truly develop human-like mathematical reasoning capabilities (System 2 thinking) through current training paradigms and architectures?
- Basis in paper: [explicit] The paper demonstrates that LLMs, including o1, rely on pattern matching (System 1 thinking) rather than genuine logical reasoning when solving mathematical problems, even with Chain-of-Thought prompts and specialized training.
- Why unresolved: Current LLMs show consistent failure on modified CRT problems where mathematical principles change but descriptions remain similar, suggesting they learn patterns rather than principles. The paper indicates this may be due to the autoregressive next-token prediction paradigm being fundamentally aligned with System 1 thinking.
- What evidence would resolve it: Development of an LLM architecture or training method that consistently solves modified CRT problems with accuracy comparable to humans, demonstrating genuine understanding of underlying mathematical principles rather than pattern matching.

### Open Question 2
- Question: What specific modifications to LLM training paradigms could enable genuine mathematical reasoning rather than pattern matching?
- Basis in paper: [inferred] The paper suggests that the current dominant paradigm of predicting next tokens with highest probability aligns with System 1 thinking and may prevent development of System 2 reasoning capabilities.
- Why unresolved: While the paper identifies the limitation, it doesn't propose specific architectural or training modifications that could overcome this fundamental constraint of current LLM design.
- What evidence would resolve it: Experimental demonstration of an LLM variant that shows significantly improved performance on modified CRT problems while maintaining strong performance on original problems, indicating genuine reasoning rather than memorization.

### Open Question 3
- Question: How does the performance gap between humans and LLMs on modified CRT problems scale with problem complexity and domain specificity?
- Basis in paper: [explicit] The paper shows humans maintain relatively stable accuracy (only 14% decrease) on modified problems while LLMs experience 50%+ accuracy drops, but doesn't extensively explore scaling across different mathematical domains or complexity levels.
- Why unresolved: The study focuses on specific CRT problems but doesn't establish whether the observed performance gap is consistent across different types of mathematical reasoning tasks or whether it increases with problem complexity.
- What evidence would resolve it: Comprehensive testing of LLMs and humans across diverse mathematical domains (algebra, geometry, calculus, etc.) with systematically varied complexity levels and modifications to mathematical principles while maintaining similar descriptions.

## Limitations

- Specific CRT datasets used are not publicly available, requiring reconstruction from examples
- Similarity measurement methodology between original and modified problems lacks full specification
- Results may be influenced by test-set overlap with training data, though modified problems were designed to minimize this risk

## Confidence

- Claim about LLMs relying on pattern matching rather than reasoning: High
- Claim about generalizability to broader mathematical domains: Medium
- Claim about CoT prompting's ineffectiveness: Medium

## Next Checks

1. Test LLMs on completely novel mathematical problem types (e.g., geometry, algebra) with similar modification strategies to assess generalizability beyond CRT problems
2. Implement multiple prompting strategies beyond CoT (e.g., scratchpad, structured reasoning templates) to determine if alternative approaches can elicit genuine reasoning
3. Conduct ablation studies varying the degree of textual similarity between original and modified problems to establish the relationship between surface features and pattern matching behavior