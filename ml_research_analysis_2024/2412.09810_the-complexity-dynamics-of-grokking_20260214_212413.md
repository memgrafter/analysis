---
ver: rpa2
title: The Complexity Dynamics of Grokking
arxiv_id: '2412.09810'
source_url: https://arxiv.org/abs/2412.09810
tags:
- complexity
- which
- generalization
- networks
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for measuring the complexity
  of neural networks using principles from Kolmogorov complexity and rate-distortion
  theory. The authors apply this framework to study grokking, where networks suddenly
  transition from memorization to generalization after overfitting.
---

# The Complexity Dynamics of Grokking

## Quick Facts
- arXiv ID: 2412.09810
- Source URL: https://arxiv.org/abs/2412.09810
- Authors: Branton DeMoss; Silvia Sapora; Jakob Foerster; Nick Hawes; Ingmar Posner
- Reference count: 15
- Key outcome: Framework for measuring neural network complexity using Kolmogorov complexity principles, applied to study grokking phenomenon with 30-40x better compression ratios

## Executive Summary
This paper introduces a novel framework for measuring neural network complexity through principles from Kolmogorov complexity and rate-distortion theory. The authors apply this framework to study grokking, where networks transition from memorization to generalization after overfitting. Their method achieves significantly better compression ratios than naive approaches, enabling precise tracking of complexity dynamics during training. The key finding is that properly regularized networks exhibit characteristic complexity patterns - rising during memorization then falling as they discover simpler generalizing patterns - while unregularized networks remain in high-complexity memorization states.

## Method Summary
The authors develop a complexity measurement framework based on lossy compression principles, achieving 30-40x better compression ratios than naive approaches. They introduce a spectral entropy regularizer that encourages low-rank representations by penalizing the network's intrinsic dimension. This regularizer is motivated by information-theoretic arguments about complexity minimization being crucial for generalization. The method tracks how network complexity evolves during training, revealing distinct patterns between regularized and unregularized networks during the grokking transition.

## Key Results
- Spectral entropy regularization causes grokking to occur in networks that would otherwise remain in memorization states
- Regularized networks show characteristic complexity dynamics: rise during memorization, fall during generalization
- The proposed method achieves 30-40x better compression ratios than naive approaches
- Networks regularized with spectral entropy produce the most compressible models among tested methods

## Why This Works (Mechanism)
The mechanism relies on the principle that generalization requires networks to discover simpler, more compressible patterns rather than memorizing training data. The spectral entropy regularizer encourages low-rank representations by penalizing intrinsic dimension, effectively pushing networks toward simpler solutions. During training, this creates a dynamic where complexity initially increases as networks memorize, then decreases as they discover more efficient generalizing patterns. The rate-distortion framework provides the mathematical foundation for measuring this complexity evolution and understanding how compression relates to generalization.

## Foundational Learning
- **Kolmogorov complexity**: Measures the computational resources needed to specify an object; needed to understand the theoretical basis for network complexity
- **Rate-distortion theory**: Relates compression quality to information loss; quick check - verify the distortion measure used in experiments
- **Intrinsic dimension**: The minimum number of parameters needed to describe a dataset or model; quick check - confirm how this is estimated from network weights
- **Regularization**: Techniques to prevent overfitting by constraining model complexity; quick check - compare spectral entropy to other regularization methods
- **Grokking phenomenon**: Delayed generalization after overfitting; quick check - verify grokking occurs in baseline experiments
- **Lossy compression**: Reducing data size while accepting some information loss; quick check - validate compression ratios are meaningful

## Architecture Onboarding

**Component Map:**
Input data -> Neural network -> Loss function (with spectral entropy regularizer) -> Complexity measurement (rate-distortion) -> Output predictions

**Critical Path:**
1. Data preprocessing and augmentation
2. Forward pass through network
3. Loss computation (task loss + spectral entropy regularization)
4. Backward pass and parameter updates
5. Complexity measurement using rate-distortion framework
6. Evaluation of generalization performance

**Design Tradeoffs:**
The spectral entropy regularizer trades off between task performance and model simplicity. Higher regularization strength encourages simpler models but may hurt accuracy. The rate-distortion framework balances compression quality against information loss. The method requires careful tuning of regularization parameters to achieve optimal generalization.

**Failure Signatures:**
- No grokking observed despite regularization
- Complexity remains high throughout training (indicates failure to find simpler patterns)
- Compression ratios don't correlate with generalization performance
- Regularization strength too high causing underfitting
- Regularization strength too low showing no effect on complexity dynamics

**First Experiments:**
1. Reproduce grokking on MNIST with and without spectral entropy regularization
2. Compare compression ratios of regularized vs unregularized networks during training
3. Test different regularization strengths to find optimal grokking behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework connecting Kolmogorov complexity to generalization relies on approximations
- Relationship between empirical compression ratios and true algorithmic complexity remains unproven
- Spectral entropy regularizer lacks rigorous justification for why penalizing intrinsic dimension leads to better generalization
- Experimental scope limited to small-scale tasks (MNIST, CIFAR-10)
- May not generalize to larger models or more complex domains

## Confidence
- **High confidence**: Empirical observations of complexity dynamics during grokking
- **Medium confidence**: Effectiveness of spectral entropy regularization in inducing grokking
- **Low confidence**: Theoretical claims connecting complexity theory to generalization; broader applicability beyond small-scale experiments

## Next Checks
1. Test spectral entropy regularization on larger models (e.g., transformers) and more complex tasks to verify scalability
2. Conduct ablation studies comparing spectral entropy regularization against other complexity-based regularizers
3. Perform rigorous analysis of relationship between compression ratios and true generalization performance across diverse architectures and datasets