---
ver: rpa2
title: 'One size doesn''t fit all: Predicting the Number of Examples for In-Context
  Learning'
arxiv_id: '2403.06402'
source_url: https://arxiv.org/abs/2403.06402
tags:
- aicl
- examples
- number
- https
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of standard in-context learning
  (ICL) where a fixed number of examples is used for all instances. The authors propose
  Adaptive ICL (AICL), which dynamically predicts the optimal number of examples for
  each test instance using a multi-label classifier trained on the training set.
---

# One size doesn't fit all: Predicting the Number of Examples for In-Context Learning

## Quick Facts
- **arXiv ID**: 2403.06402
- **Source URL**: https://arxiv.org/abs/2403.06402
- **Reference count**: 40
- **Primary result**: AICL improves F1 score by up to 17% over standard ICL by predicting optimal example count per instance

## Executive Summary
This paper challenges the standard practice in in-context learning (ICL) of using a fixed number of examples for all test instances. The authors propose Adaptive ICL (AICL), which dynamically predicts the optimal number of examples needed for each instance using a multi-label classifier trained on the training set. AICL learns to map features (text embeddings and neighborhood label distributions) to the set of example counts that lead to correct predictions. Experiments on four text classification datasets show AICL outperforms standard ICL by up to 17% in F1 score. The method generalizes across different LLMs (Llama-2, Phi-2) and datasets, and remains effective even with only 30% of training data or without labels in a RAG setup.

## Method Summary
AICL addresses the limitation of standard ICL where a fixed number of examples is used for all instances regardless of individual difficulty. The approach trains a multi-label classifier on the training set that learns to map instance features to the optimal number of examples needed for correct predictions. The features include text embeddings and neighborhood label distributions. During inference, the classifier predicts which example counts would work best for each test instance, allowing the system to adapt the prompt dynamically. The method was evaluated on four text classification datasets (SST2, TREC, CoLA, RTE) using different LLMs including Llama-2 and Phi-2, demonstrating consistent improvements over standard ICL approaches.

## Key Results
- AICL outperforms standard ICL by up to 17% in F1 score across four text classification datasets
- The approach generalizes across different LLMs (Llama-2, Phi-2) and maintains effectiveness with only 30% of training data
- AICL remains effective in a RAG setup without requiring labeled data
- Performance improvements stem from matching example count to instance difficulty, with easier instances benefiting from fewer examples

## Why This Works (Mechanism)
Standard ICL uses a fixed number of examples for all instances, which is suboptimal because different instances have varying levels of difficulty and information needs. Some instances can be correctly classified with few examples, while others require more context. AICL addresses this by predicting the optimal number of examples for each instance based on its characteristics. The multi-label classifier learns patterns from the training data that associate specific instance features with successful example counts. By adapting the prompt to each instance's needs, AICL avoids both under-prompting (too few examples) and over-prompting (too many examples), leading to better utilization of the model's in-context learning capabilities.

## Foundational Learning
- **In-Context Learning (ICL)**: The ability of LLMs to learn from examples provided in the prompt without parameter updates. Why needed: Forms the baseline approach being improved. Quick check: Verify the model can perform the task with few examples in the prompt.
- **Multi-label Classification**: Predicting multiple labels simultaneously rather than a single class. Why needed: Each instance can succeed with multiple different example counts. Quick check: Ensure the classifier outputs a probability distribution over example counts.
- **Text Embeddings**: Dense vector representations capturing semantic meaning of text. Why needed: Provides input features describing each instance's content. Quick check: Verify embeddings capture relevant semantic information for the task.
- **Neighborhood Label Distribution**: Statistical patterns of labels among similar instances. Why needed: Helps predict difficulty based on how similar instances are labeled. Quick check: Confirm the distribution correlates with instance difficulty.
- **Adaptive Prompting**: Dynamically adjusting prompt content based on instance characteristics. Why needed: Enables optimal example count selection per instance. Quick check: Validate that different instance types receive different prompt configurations.

## Architecture Onboarding

**Component Map**: Training Data -> Feature Extractor -> Multi-label Classifier -> AICL System -> LLM -> Predictions

**Critical Path**: The core pipeline flows from feature extraction (text embeddings + neighborhood distributions) through the multi-label classifier to generate optimal example counts, which are then used to construct adaptive prompts for the LLM. The classifier training depends on having a labeled training set where example count performance is known.

**Design Tradeoffs**: The approach trades computational overhead of running a classifier for each inference instance against the performance gains from optimal example selection. Using 30% of training data balances classifier accuracy with data efficiency, though this could be adjusted based on available resources. The choice of features (embeddings + neighborhood distributions) aims to capture both instance content and difficulty context.

**Failure Signatures**: The system may fail when the classifier cannot accurately predict difficulty (e.g., when instance features don't correlate well with optimal example counts), when the LLM's ICL capabilities don't benefit from example count variation, or when the training data doesn't represent the inference distribution. Performance degradation is most likely on datasets where all instances require similar example counts.

**First Experiments**: 
1. Baseline test: Run standard ICL with fixed example counts (1, 2, 4, 8) to establish performance range
2. Feature ablation: Test AICL with only text embeddings versus only neighborhood distributions to assess feature importance
3. Data efficiency test: Evaluate AICL performance using 10%, 30%, 50%, and 100% of training data for classifier training

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the AICL approach perform when applied to different types of NLP tasks beyond text classification, such as text generation, summarization, or question answering?
- Basis in paper: The paper focuses on text classification datasets (SST2, TREC, CoLA, RTE) and does not explore other NLP tasks.
- Why unresolved: The authors only evaluated AICL on text classification tasks, leaving its generalizability to other NLP tasks unknown.
- What evidence would resolve it: Testing AICL on a variety of NLP tasks and comparing its performance to standard ICL methods would provide insights into its broader applicability.

### Open Question 2
- Question: Can the AICL framework be extended to handle more complex scenarios, such as multi-modal data or long-form text, where the number of examples required may vary significantly?
- Basis in paper: The paper assumes that the number of examples needed for a task is determined by the instance's content and neighborhood label distribution, which may not hold for more complex scenarios.
- Why unresolved: The authors did not explore the limitations of AICL when dealing with multi-modal or long-form text data, which could require a different approach to determining the optimal number of examples.
- What evidence would resolve it: Experimenting with AICL on multi-modal or long-form text datasets and analyzing its performance would help understand its scalability and adaptability to more complex scenarios.

### Open Question 3
- Question: How does the AICL approach handle situations where the training data is imbalanced or biased, and how can it be adapted to mitigate the impact of such biases on the downstream task performance?
- Basis in paper: The paper mentions that the AICL approach relies on a multi-label classifier trained on the training set, but does not discuss how it handles imbalanced or biased data.
- Why unresolved: The authors did not investigate the robustness of AICL to imbalanced or biased training data, which could lead to suboptimal performance or biased predictions.
- What evidence would resolve it: Evaluating AICL on imbalanced or biased datasets and comparing its performance to standard ICL methods would provide insights into its robustness and potential adaptations to handle such scenarios.

## Limitations
- Uncertainty about whether adaptive selection captures true instance difficulty versus dataset-specific artifacts
- Limited model diversity tested (only Llama-2 and Phi-2) raises questions about generalizability across LLMs
- Performance claims depend heavily on specific evaluation setup and metric choices
- The 30% training data efficiency claim needs more rigorous ablation studies to validate

## Confidence
- **High Confidence**: The empirical observation that different instances benefit from different numbers of examples is well-supported by experimental results across multiple datasets and models
- **Medium Confidence**: The claim that AICL generalizes across different LLMs is supported but limited by the relatively small model diversity tested
- **Low Confidence**: The assertion that AICL achieves strong performance with only 30% of training data is promising but requires more rigorous ablation studies

## Next Checks
1. **Cross-dataset generalization test**: Train the AICL classifier on one dataset and evaluate on held-out datasets to determine whether the adaptive selection captures universal patterns of instance difficulty or dataset-specific artifacts

2. **Ablation study on feature importance**: Systematically remove text embeddings versus neighborhood label distributions from the classifier to quantify their relative contributions and test whether the method works with only one feature type

3. **Comparison to adaptive prompting baselines**: Implement and compare against alternative adaptive ICL approaches (e.g., confidence-based selection, difficulty-weighted examples) to establish whether the multi-label classifier approach provides unique benefits over simpler adaptive strategies