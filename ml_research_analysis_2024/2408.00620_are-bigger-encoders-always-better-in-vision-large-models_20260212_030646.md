---
ver: rpa2
title: Are Bigger Encoders Always Better in Vision Large Models?
arxiv_id: '2408.00620'
source_url: https://arxiv.org/abs/2408.00620
tags:
- data
- training
- scaling
- performance
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the scaling behavior of multimodal large
  language models (MLLMs) with varying vision encoder sizes. Through extensive experiments,
  the authors find that increasing the parameter size of vision transformers (ViTs)
  does not necessarily improve MLLM performance, challenging the common assumption
  that larger encoders always yield better results.
---

# Are Bigger Encoders Always Better in Vision Large Models?

## Quick Facts
- arXiv ID: 2408.00620
- Source URL: https://arxiv.org/abs/2408.00620
- Authors: Bozhou Li; Hao Liang; Zimo Meng; Wentao Zhang
- Reference count: 9
- Primary result: Increasing vision transformer (ViT) size does not guarantee improved multimodal large language model (MLLM) performance.

## Executive Summary
This paper investigates the scaling behavior of multimodal large language models (MLLMs) by varying vision encoder sizes from 86M to 1.01B parameters. Using LLaVA 1.5 as the backbone and training on datasets ranging from 1M to 10M image-text pairs, the authors find that simply increasing ViT parameter size does not necessarily improve MLLM performance. The study challenges the common assumption that larger encoders always yield better results, showing that data quality, alignment methods, and language model size play more critical roles in determining overall performance than vision encoder scale alone.

## Method Summary
The researchers used LLaVA 1.5 architecture with frozen CLIP ViT models of varying sizes (86M to 1.01B parameters) and frozen Vicuna LLM backbones (7B or 13B parameters). A linear projector (2-layer MLP with GELU) was trained to align visual features to LLM embedding space. The training involved image-text pairs from CC12M and LAION-400M datasets, with sample sizes ranging from 1M to 10M. The primary metric was evaluation loss on next token prediction, with all experiments maintaining frozen backbones and only training the alignment projector.

## Key Results
- Increasing ViT parameter size from 86M to 1.01B does not consistently improve MLLM performance
- Training on higher-quality datasets (CC12M) yields better results than larger but noisier datasets (LAION-400M)
- Larger LLM backbones (13B vs 7B) require less training data to achieve alignment with vision encoders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling up vision encoder size alone does not guarantee improved MLLM performance.
- Mechanism: Larger ViTs capture more visual detail but do not necessarily improve the alignment with LLM semantic spaces, which are the bottleneck for multimodal understanding.
- Core assumption: The vision encoder's raw representational capacity is not the limiting factor; rather, the alignment between vision and language embeddings is.
- Evidence anchors:
  - [abstract] "increasing the parameter size of vision transformers (ViTs) does not necessarily improve MLLM performance, challenging the common assumption that larger encoders always yield better results"
  - [section 4] "A critical finding from our study reveals that merely amplifying the parameter scale of ViT does not necessarily translate into enhanced model performance"
  - [corpus] Weak - corpus lacks direct evidence for this specific mechanism.

### Mechanism 2
- Claim: Data quality and distribution matching are more important than encoder size for MLLM performance.
- Mechanism: Training on high-quality, closely matched datasets (e.g., CC12M vs LAION-400M) yields better alignment and performance, regardless of encoder size.
- Core assumption: The mismatch between the pretraining distribution of the ViT and the training data distribution for MLLM alignment degrades performance.
- Evidence anchors:
  - [section 4] "The evaluation loss obtained from training on the CC12M dataset is lower than the evaluation loss from training on the LAION-400M dataset"
  - [section 4] "Compared to LAION-400M, the CC12M dataset has higher quality, with a higher degree of matching between image and text"
  - [corpus] Weak - no corpus evidence on dataset distribution matching.

### Mechanism 3
- Claim: Larger LLM backbones require less training data to achieve alignment with vision encoders.
- Mechanism: Bigger LLMs have better semantic understanding and can align with visual features more efficiently, reducing the data requirement.
- Core assumption: The semantic spaces of vision and text are inherently aligned, and larger LLMs can leverage this alignment with less data.
- Evidence anchors:
  - [section 4] "Notably, when leveraging the Vicuna-13B model, the evaluation loss plateaus around the 7M mark in training data size"
  - [section 4] "This indicates that smaller models might require more substantial data increments to observe subsequent improvements, while larger models can exhibit more pronounced progress with less data"
  - [corpus] Weak - no corpus evidence on data efficiency scaling with LLM size.

## Foundational Learning

- Concept: Multimodal alignment and cross-modal fusion
  - Why needed here: The paper's core finding is that encoder size doesn't matter as much as alignment quality, so understanding how vision and language embeddings are fused is critical.
  - Quick check question: What are the two main categories of fusion methods discussed, and how do they differ in architecture?

- Concept: Scaling laws and their application to multimodal models
  - Why needed here: The paper applies scaling law methodology to MLLMs, so understanding how model size and data scale jointly affect performance is essential.
  - Quick check question: What is the general form of the power-law relationship between loss, model size, and data size in scaling laws?

- Concept: Vision transformer architecture and its scaling behavior
  - Why needed here: The paper varies ViT sizes and observes that larger ones don't always help, so knowing how ViTs scale and what their bottlenecks are is important.
  - Quick check question: What is the key architectural difference between ViT and CNN-based vision models that affects their scaling behavior?

## Architecture Onboarding

- Component map: Image -> Frozen ViT -> Linear Projector -> Concat with text tokens -> Frozen LLM backbone
- Critical path:
  1. Extract image features with frozen ViT
  2. Project features to LLM embedding space via MLP
  3. Concatenate with text tokens
  4. Train with next token prediction objective
- Design tradeoffs:
  - Encoder size vs. alignment quality: Bigger encoders may overfit to visual details not useful for language tasks
  - Data quality vs. quantity: High-quality, well-matched data beats large amounts of noisy data
  - Model size vs. efficiency: Larger LLMs need less data but are more expensive to run
- Failure signatures:
  - Validation loss plateaus or increases with larger encoders
  - Performance degrades when switching from high-quality to lower-quality datasets
  - Training becomes unstable or overfits with very large encoders on small datasets
- First 3 experiments:
  1. Train with smallest ViT (86M) on smallest dataset (1M samples) to establish baseline
  2. Train with largest ViT (1.01B) on same 1M samples to test encoder scaling limit
  3. Train with medium ViT (304M) on largest dataset (10M samples) to test data scaling limit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does increasing the size of vision encoders (ViTs) improve or degrade MLLM performance, and why?
- Basis in paper: [explicit] The paper finds that increasing ViT size does not always improve MLLM performance and sometimes degrades it, particularly when training with data from the LAION-400M dataset.
- Why unresolved: The paper observes this phenomenon but does not provide a detailed theoretical explanation for why larger ViTs sometimes lead to worse performance. The interaction between ViT size, dataset quality, and alignment methods is complex and not fully understood.
- What evidence would resolve it: Systematic experiments varying ViT sizes, dataset qualities, and alignment methods (e.g., CLIP vs. LLaVA) while measuring performance on multiple benchmarks would help identify the conditions under which larger ViTs are beneficial or detrimental.

### Open Question 2
- Question: How do different alignment methods (e.g., cross-attention, MLP projectors, custom layers) affect the scaling behavior of MLLMs with varying ViT sizes?
- Basis in paper: [inferred] The paper mentions that different alignment methods exist (e.g., transformer-based abstractors, MLPs, custom layers) and suggests that the differences in alignment strategies between CLIP and LLaVA might impact performance.
- Why unresolved: The paper does not explore how different alignment methods interact with ViT scaling. It's unclear whether certain alignment methods are more robust to increases in ViT size or if they mitigate the performance degradation observed with larger ViTs.
- What evidence would resolve it: Comparative experiments training MLLMs with different alignment methods while varying ViT sizes and measuring performance on standardized benchmarks would reveal which methods are most effective at different scales.

### Open Question 3
- Question: What is the optimal allocation of computational resources between the vision encoder and language model components in MLLMs to maximize performance within a fixed compute budget?
- Basis in paper: [explicit] The paper discusses the complexity of estimating computational costs for MLLMs and notes that the current formula (C â‰ˆ 6N D) is not appropriate due to the use of backpropagation and the different processing requirements of ViT and LLM components.
- Why unresolved: The paper highlights that the computational costs of MLLMs are more complex than those of LLMs due to the need to process both image and text tokens, and the interaction between the vision encoder and language model during training. The optimal trade-off between ViT and LLM size within a fixed compute budget is not established.
- What evidence would resolve it: Experiments systematically varying the parameter sizes of both ViT and LLM components while keeping the total computational budget constant, and measuring the resulting performance on multimodal tasks, would provide insights into the optimal resource allocation strategy.

## Limitations
- The findings are based on a specific MLLM architecture (LLaVA 1.5) with frozen backbones and linear projection alignment, limiting generalizability
- The study focuses primarily on evaluation loss rather than comprehensive downstream task performance
- The analysis of dataset quality differences is limited to comparing only CC12M and LAION-400M without deeper investigation into quality factors

## Confidence
- High Confidence: The experimental results showing that increasing ViT parameter size does not lead to consistent performance improvements, and that larger LLM backbones require less training data for alignment
- Medium Confidence: The interpretation that alignment quality and dataset distribution matching are more important than encoder size, as the evidence is primarily from loss curves and qualitative comparisons of dataset quality
- Low Confidence: The generalizability of these findings to other MLLM architectures or alignment methods, as the study is limited to a specific setup with frozen backbones and linear projection

## Next Checks
1. **Architecture Ablation**: Repeat the scaling experiments with different MLLM architectures (e.g., those with trainable vision encoders or more complex fusion mechanisms) to test the robustness of the findings across architectural choices
2. **Downstream Task Evaluation**: Evaluate the trained models on a diverse set of multimodal downstream tasks (e.g., VQA, image captioning, visual reasoning) to assess whether the trends observed in evaluation loss translate to practical performance gains or limitations
3. **Alignment Method Comparison**: Compare the linear projection alignment used in the study with other alignment methods (e.g., cross-attention, adapter-based approaches) to determine if the ineffectiveness of larger encoders is specific to the alignment mechanism or a more general phenomenon