---
ver: rpa2
title: Refusal in LLMs is an Affine Function
arxiv_id: '2411.09003'
source_url: https://arxiv.org/abs/2411.09003
tags:
- behavior
- steering
- proj
- ablation
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces affine concept editing (ACE) as a method for
  steering large language model behavior by intervening in activations, particularly
  for controlling refusal responses. The authors argue that prior methods like contrastive
  activation addition and directional ablation are based on a linear (rather than
  affine) framework, which leads to inconsistent control and sometimes produces nonsensical
  outputs.
---

# Refusal in LLMs is an Affine Function

## Quick Facts
- arXiv ID: 2411.09003
- Source URL: https://arxiv.org/abs/2411.09003
- Authors: Thomas Marshall; Adam Scherlis; Nora Belrose
- Reference count: 3
- Primary result: Affine Concept Editing (ACE) provides more standardized control over refusal behavior in LLMs compared to prior linear methods

## Executive Summary
This paper introduces Affine Concept Editing (ACE) as a method for steering large language model behavior by intervening in activations, specifically for controlling refusal responses. The authors argue that prior methods like contrastive activation addition and directional ablation are based on a linear (rather than affine) framework, which leads to inconsistent control and sometimes produces nonsensical outputs. ACE combines affine subspace projection and activation addition to achieve more standardized control, ensuring that the steering parameter reliably determines the degree of behavior regardless of the input prompt. Experiments on Llama 3 8B and Hermes Eagle RWKV v5 show that ACE achieves more precise control than existing methods and avoids the nonsensical outputs seen with directional ablation alone.

## Method Summary
ACE is based on an affine decomposition of model activation vectors, extending the linear decomposition used in prior methods. The method computes a steering vector r as the difference between positive and negative example means (r+ - r-), then applies a transformation that combines directional ablation with activation addition relative to a reference point v0. The transformation v′ = v - proj‖(v) + proj‖(r-) + αr ensures that the steering parameter α directly controls the degree of behavior expression relative to a known baseline. The method is evaluated by testing on harmful and harmless prompts, measuring refusal probability across different α values, and comparing results with contrastive activation addition.

## Key Results
- ACE achieves more precise control over refusal behavior compared to directional ablation and CAA methods
- ACE avoids the problem of certain models producing nonsensical outputs when steered
- The method demonstrates better standardization, where α value consistently determines behavior across different prompts

## Why This Works (Mechanism)

### Mechanism 1
Affine decomposition provides a more complete framework than linear decomposition for steering model behavior. By including a reference point v0 in the decomposition, the framework accounts for the fact that typical activation levels are not zero and that the origin is not a meaningful "default" state. This allows for steering that is relative to a known baseline rather than an arbitrary origin. The core assumption is that the typical level of behaviors in a network is not "none", and typical activations are often far from the origin.

### Mechanism 2
ACE combines directional ablation and activation addition in a way that provides more standardized control over model behavior. By using the affine decomposition, ACE can set the steering parameter α to directly control the degree of behavior expression relative to a known baseline (r-). This means that setting α=1 should induce the behavior and setting α=0 should induce the null-behavior, regardless of the input prompt. The core assumption is that the steering vector r can be chosen such that it is precisely equal to r+ - r-, where r+ and r- are the means of positive and negative examples.

### Mechanism 3
ACE can avoid the problem of certain models generating nonsense outputs when steered, which occurs with directional ablation alone. By including the reference point v0 (proj∥r(r-)) in the decomposition, ACE ensures that the steered activation vectors stay in a meaningful location in activation space. This prevents the model from collapsing into producing nonsense outputs. The core assumption is that the reference point v0 corresponds to a meaningful baseline activation level for the null-behavior.

## Foundational Learning

- **Linear representation hypothesis**
  - Why needed here: The paper builds upon the idea that models represent high-level concepts as linear directions in activation space. Understanding this hypothesis is crucial for grasping the motivation behind ACE.
  - Quick check question: What is the linear representation hypothesis, and how does it relate to the concept of directional ablation and activation addition?

- **Affine decomposition**
  - Why needed here: ACE is based on an affine decomposition of model activation vectors, which is an extension of the linear decomposition used in prior methods. Understanding this decomposition is key to understanding how ACE works.
  - Quick check question: How does affine decomposition differ from linear decomposition, and why is this distinction important for steering model behavior?

- **Standardization in steering**
  - Why needed here: The paper emphasizes the importance of standardization in steering techniques, which means that the degree of behavior expression should be deterministically controlled by the steering parameter, regardless of the input prompt. Understanding this concept is crucial for evaluating the effectiveness of ACE.
  - Quick check question: What is standardization in the context of steering, and why is it important for achieving precise control over model behavior?

## Architecture Onboarding

- **Component map**: Model activations -> Steering vector r -> Reference point v0 -> Steering parameter α
- **Critical path**: 1. Identify the steering vector r and reference point v0. 2. Compute the affine decomposition of the model activations. 3. Apply the ACE transformation to the activations. 4. Evaluate the effectiveness of the steering using LLM-based scoring.
- **Design tradeoffs**: Precision vs. generalization: ACE provides more precise control over model behavior, but may not generalize as well to all types of behaviors or models. Complexity vs. interpretability: ACE is more complex than prior methods, but provides a more interpretable framework for understanding how the steering works.
- **Failure signatures**: Lack of standardization: If the steering parameter does not deterministically control the degree of behavior expression, regardless of the input prompt. Nonsense outputs: If the steered model produces incoherent or meaningless text. Oversteer or understeer: If the model exhibits too much or too little of the target behavior.
- **First 3 experiments**: 1. Replicate the results on Llama 3 8B using the same prompts and evaluation method. 2. Test ACE on a different model, such as GPT-3 or Claude, to evaluate its generalizability. 3. Explore the effects of different choices for the reference point v0 on the effectiveness of ACE.

## Open Questions the Paper Calls Out

### Open Question 1
What specific properties of Hermes Eagle RWKV v5 make directional ablation alone produce incoherent outputs while Llama models handle it fine? The paper notes that "directional ablation alone amounted to an undesirably strong intervention in Hermes Eagle RWKV v5 unlike in Llama 2" but doesn't investigate or explain the fundamental differences between transformer and RNN architectures that cause this discrepancy.

### Open Question 2
Is the imperfect standardization of ACE due to limitations in the linear representation hypothesis or in the steering methodology itself? The paper notes that "because the standardization effect of ACE is not perfect" and suggests this could be "that the derived steering vectors are flawed or that directional ablation does not fully erase information" but doesn't perform experiments to distinguish between these two possibilities.

### Open Question 3
What is the theoretical limit of linear versus nonlinear steering methods for concept erasure in language models? The paper states "Future work may seek to address some of these limitations by exploring whether or not its possible to fully erase concepts like refusal linearly or if nonlinear techniques are required" but only briefly mentions this as future work without exploring it.

## Limitations
- The paper only demonstrates ACE on two models (Llama 3 8B and Hermes Eagle RWKV v5), limiting generalizability claims
- The evaluation methodology relies on LLM-based scoring, which has reliability concerns in the field
- The method's effectiveness depends heavily on correctly identifying the steering vector r and reference point v0

## Confidence

- **High Confidence**: The mathematical framework of affine decomposition itself and its distinction from linear decomposition
- **Medium Confidence**: The claim that ACE achieves more standardized control than prior methods
- **Low Confidence**: The assertion that ACE "generalizes to models where directional ablation via affine subspace projection alone produces incoherent outputs"

## Next Checks
1. **Cross-model validation**: Test ACE on at least 3-5 additional models with varying architectures (including different families like GPT, Claude, and open-source alternatives) to assess the claimed generalizability, particularly focusing on models known to produce incoherent outputs with directional ablation alone.

2. **Ablation study on reference point selection**: Systematically vary the method for choosing the reference point v0 (including using different baseline behaviors, random points, and no reference point) to quantify how sensitive ACE performance is to this component and whether the affine framework provides benefits beyond just proper reference point selection.

3. **Comparison with alternative evaluation methods**: Re-run the key experiments using human evaluation alongside the LLM-based scoring to validate that the automated evaluation aligns with human judgments of refusal behavior and output quality, and to identify any systematic biases in the LLM-as-a-judge approach.