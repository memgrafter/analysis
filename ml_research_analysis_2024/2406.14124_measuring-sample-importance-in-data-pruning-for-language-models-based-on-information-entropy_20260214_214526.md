---
ver: rpa2
title: Measuring Sample Importance in Data Pruning for Language Models based on Information
  Entropy
arxiv_id: '2406.14124'
source_url: https://arxiv.org/abs/2406.14124
tags:
- pruning
- data
- language
- information
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an information-theoretic approach to data
  pruning for training large language models (LLMs). The authors propose using the
  log-likelihood of samples, as measured by a small "data probe" model, to estimate
  their information content.
---

# Measuring Sample Importance in Data Pruning for Language Models based on Information Entropy

## Quick Facts
- arXiv ID: 2406.14124
- Source URL: https://arxiv.org/abs/2406.14124
- Reference count: 4
- Primary result: Pruning up to 40-50% of data using probe model likelihood can maintain or improve LLM performance on language modeling and GLUE tasks

## Executive Summary
This paper introduces an information-theoretic approach to data pruning for training large language models. The authors propose using a small "data probe" model to estimate the information content of training samples via their log-likelihood. Samples with low information content (indicating redundancy) are pruned first. Experiments show that this method can improve language modeling performance and downstream task accuracy, sometimes even outperforming models trained on the full dataset.

## Method Summary
The method involves training a small probe model on a subset of the corpus until loss saturation, then using this model to estimate the information content of all samples via negative log-likelihood. Samples with lower information content are pruned first, and the target LLM is trained on the remaining data. The probe model is trained on approximately 12% of the data, and pruning ratios up to 50% are tested. The approach is evaluated on language modeling benchmarks (One Billion Words, wikitext-103) and downstream tasks (GLUE).

## Key Results
- Pruning up to 40-50% of data can maintain or enhance performance on language modeling benchmarks
- The method sometimes outperforms models trained on the full dataset
- Removing low-information samples reduces redundancy and improves generalization
- Training the probe model on a small subset is sufficient for reliable sample ranking

## Why This Works (Mechanism)

### Mechanism 1
Lower log-likelihood estimates indicate redundant or frequently occurring samples, so removing them reduces overfitting and improves generalization. The method estimates per-sample information content using a small probe model trained on a subset of the corpus. Samples with lower negative log-likelihood (higher probability under the probe) are considered less informative because they likely contain common patterns or redundancy. Pruning these samples removes data that the model would otherwise overfit on, improving downstream task performance.

### Mechanism 2
Removing low-information samples reduces dataset redundancy without harming model performance, and sometimes improves it. By pruning samples that the probe model assigns high probability to, the method effectively removes "easy" or common examples. This forces the target model to focus on informative patterns, leading to better generalization on held-out data.

### Mechanism 3
Training the probe model on a small subset is sufficient to rank full-dataset samples by information content. The probe model is trained until the loss decrease saturates (about 12% of data). This limited training still provides reliable likelihood estimates for pruning decisions because the model captures the general distribution.

## Foundational Learning

- Concept: Information entropy and cross-entropy in language modeling
  - Why needed here: The method uses entropy-based measures (negative log-likelihood) to rank samples by information content.
  - Quick check question: What does a lower negative log-likelihood of a sample under a model indicate about that sample's redundancy or frequency in the training data?

- Concept: Data probe model and early stopping
  - Why needed here: The probe model is trained on a small subset and stopped when loss saturates to save compute while maintaining ranking quality.
  - Quick check question: Why is it important to stop probe model training when the loss decrease saturates rather than training to full convergence?

- Concept: Data pruning and its impact on overfitting
  - Why needed here: The method assumes that removing low-information samples reduces overfitting and improves generalization.
  - Quick check question: How might removing redundant or frequently occurring samples from a dataset affect the target model's ability to generalize?

## Architecture Onboarding

- Component map: Data corpus -> Data probe model -> Pruning module -> Target model -> Evaluation pipeline
- Critical path: 1. Sample subset from corpus; 2. Train probe model until loss saturates; 3. Compute H(W, q) for all samples; 4. Sort and prune bottom η%; 5. Train target model on pruned dataset; 6. Evaluate performance
- Design tradeoffs: Probe model size vs. ranking accuracy; subset size vs. representativeness; pruning ratio vs. performance
- Failure signatures: Probe model undertrained → rankings not meaningful; subset not representative → probe model biased → bad pruning; pruning too aggressively → loss of rare but important samples → degraded performance
- First 3 experiments: 1. Train probe model on 5%, 12%, and 20% subsets; compare ranking quality and downstream performance; 2. Vary pruning ratio (10%, 20%, 30%, 40%, 50%) and measure impact on language modeling perplexity and GLUE scores; 3. Compare proposed method against random pruning and high-entropy pruning baseline on held-out datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation of probe model generalization from small subset to full corpus
- Heuristic stopping criterion for probe model training not rigorously justified
- Experiments limited to specific benchmark datasets and model sizes (125M-345M parameters)
- Lack of ablation studies on probe model architecture or training dynamics

## Confidence

- **High confidence**: The core mechanism of using probe model likelihood to rank samples by redundancy is sound and aligns with established information-theoretic principles.
- **Medium confidence**: The empirical results showing performance improvements with pruning are compelling but limited in scope; the optimal pruning ratio appears dataset-dependent.
- **Low confidence**: The theoretical claim that H(W, q) is a reliable upper bound for H(W, p) when q is well-trained lacks rigorous proof or extensive empirical validation.

## Next Checks

1. **Probe Model Validation**: Conduct experiments varying the subset size (e.g., 5%, 12%, 25%) and measure how probe model training duration and loss saturation thresholds affect pruning quality and downstream performance.

2. **Generalization Across Domains**: Test the method on diverse datasets beyond language modeling (e.g., computer vision, speech) to assess whether the information-theoretic pruning approach transfers across modalities.

3. **Ablation of Probe Model Design**: Experiment with different probe model architectures (e.g., transformer depth, embedding size) and training objectives to determine the minimal requirements for effective sample ranking.