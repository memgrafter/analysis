---
ver: rpa2
title: Learning To Guide Human Decision Makers With Vision-Language Models
arxiv_id: '2403.16501'
source_url: https://arxiv.org/abs/2403.16501
tags:
- human
- guidance
- quality
- slog
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving human-AI collaboration
  in high-stakes decision-making tasks, such as medical diagnosis, where full automation
  is not desirable. The authors propose a new framework called "learning to guide"
  (LTG) that provides interpretable textual guidance to human decision makers rather
  than making decisions for them.
---

# Learning To Guide Human Decision Makers With Vision-Language Models

## Quick Facts
- arXiv ID: 2403.16501
- Source URL: https://arxiv.org/abs/2403.16501
- Authors: Debodeep Banerjee; Stefano Teso; Burcu Sayin; Andrea Passerini
- Reference count: 6
- Primary result: SLOG improves downstream decision quality in 9 out of 14 pathology classes in medical diagnosis tasks

## Executive Summary
This paper introduces Learning to Guide (LTG), a novel framework for human-AI collaboration in high-stakes decision-making tasks where full automation is undesirable. Instead of making decisions directly, the approach provides interpretable textual guidance to human decision makers using vision-language models. The proposed SLOG method fine-tunes these models using feedback about downstream decision quality, employing a surrogate model to estimate decision quality during training. Experiments on a real-world medical diagnosis task demonstrate that SLOG generates more informative guidance and improves decision quality compared to existing methods.

## Method Summary
SLOG fine-tunes pre-trained vision-language models (specifically R2Gen) to generate textual guidance from medical images, using a surrogate model trained on limited quality annotations to estimate downstream decision quality. The approach optimizes for both text generation performance and guidance informativeness through a combined loss function. The method is evaluated on chest X-ray images and radiology reports from the MIMIC-CXR-IV dataset, with the surrogate model trained on 10% of the data using simulated human decisions.

## Key Results
- SLOG outperforms existing methods in 9 out of 14 pathology classes
- Improves average precision and recall in decision-making tasks
- Generates more informative and interpretable guidance compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SLOG improves guidance informativeness by leveraging a surrogate model for quality estimation
- Core assumption: The surrogate model can generalize from limited quality annotations to accurately estimate guidance quality
- Evidence: Weak evidence from related work; direct mechanism described in abstract and methods section
- Break condition: Surrogate model cannot generalize from limited annotations

### Mechanism 2
- Claim: SLOG improves downstream decision quality through more informative guidance
- Core assumption: More informative guidance leads to better human decisions
- Evidence: Weak evidence from related work; direct mechanism described in abstract and methods section
- Break condition: Generated guidance is not interpretable or informative enough

### Mechanism 3
- Claim: SLOG keeps human experts in the loop, addressing automation bias
- Core assumption: Human oversight is necessary for trustworthy AI in high-stakes applications
- Evidence: Weak evidence from related work; direct mechanism described in abstract and methods section
- Break condition: Human oversight is not necessary for the specific application

## Foundational Learning

- **Vision-Language Models (VLMs)**
  - Why needed: SLOG uses pre-trained VLMs as base models for generating textual guidance from medical images
  - Quick check: What are the key components of a typical VLM architecture, and how do they enable text generation from images?

- **Reinforcement Learning with Human Feedback (RLHF)**
  - Why needed: SLOG is inspired by RLHF but uses simpler fine-tuning instead of reinforcement learning
  - Quick check: How does RLHF differ from traditional supervised learning, and what are its key advantages and disadvantages?

- **Conformal Prediction**
  - Why needed: Related technique for quantifying uncertainty in predictions for high-stakes decision-making
  - Quick check: What is conformal prediction, and how can it improve reliability of AI-assisted decision-making?

## Architecture Onboarding

- **Component map**: Pre-trained VLM (R2Gen) -> Surrogate model (σquality) -> Human decision model (σhuman) -> VLM fine-tuning loop
- **Critical path**: Input image → VLM guidance generation → Surrogate quality estimation → VLM fine-tuning → Improved guidance generation
- **Design tradeoffs**: Accuracy vs. interpretability; computational cost vs. performance; generalization vs. task-specificity
- **Failure signatures**: Surrogate model fails to generalize; VLM overfits to surrogate; human decision model is inaccurate
- **First 3 experiments**:
  1. Evaluate pre-trained VLM guidance quality on held-out medical images
  2. Train surrogate model on small set of quality ratings and evaluate accuracy
  3. Fine-tune VLM using SLOG for few epochs and compare guidance quality before/after

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does guidance quality scale with amount of human feedback annotations?
- Basis: Paper mentions performance depends on annotation amount but only evaluated with fixed 10%
- Why unresolved: Only tested with fixed annotation budget, no systematic exploration of different annotation budgets
- Evidence needed: Experiments varying annotation percentages (1%, 5%, 10%, 25%, 50%) measuring guidance quality and decision performance

### Open Question 2
- Question: Can SLOG be extended to handle sequential or incremental decision-making tasks?
- Basis: Existing HDM approaches extended to sequential decision making, but SLOG only evaluated on single-step diagnosis
- Why unresolved: Current framework designed for single-step guidance generation
- Evidence needed: Extension for iterative guidance in sequential decision-making, measuring cumulative decision quality improvements

### Open Question 3
- Question: How does SLOG's guidance compare to human experts in terms of guidance quality?
- Basis: Evaluation focuses on decision outcomes rather than intrinsic guidance quality
- Why unresolved: Does not directly compare generated guidance to human-provided guidance
- Evidence needed: Comparative study with human experts evaluating both SLOG and human guidance on same cases

## Limitations

- Reliance on surrogate model trained on simulated human decisions may introduce bias
- Evaluation limited to single medical domain (chest X-rays), may not generalize to other applications
- Does not address ethical concerns related to AI-generated guidance in clinical settings

## Confidence

- **High**: Proposed SLOG framework addresses key limitations of existing HDM strategies
- **Medium**: Experimental results demonstrate improvements in guidance informativeness and decision quality
- **Low**: Long-term effectiveness and generalizability to real-world clinical settings uncertain

## Next Checks

1. Conduct user study with real radiologists to evaluate usefulness and interpretability of SLOG guidance in clinical setting
2. Investigate robustness to adversarial attacks and out-of-distribution inputs
3. Explore potential bias in surrogate model and impact on guidance quality for underrepresented populations