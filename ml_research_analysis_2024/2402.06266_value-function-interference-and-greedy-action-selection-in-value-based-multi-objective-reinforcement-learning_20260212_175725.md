---
ver: rpa2
title: Value function interference and greedy action selection in value-based multi-objective
  reinforcement learning
arxiv_id: '2402.06266'
source_url: https://arxiv.org/abs/2402.06266
tags:
- learning
- policy
- utility
- action
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Value function interference is a previously unidentified issue
  in value-based multi-objective reinforcement learning where widely varying vector-values
  map to similar utility levels, leading to convergence to sub-optimal policies. The
  problem is most prevalent in stochastic environments when optimising for Expected
  Scalarised Return but can also arise in deterministic environments.
---

# Value function interference and greedy action selection in value-based multi-objective reinforcement learning

## Quick Facts
- arXiv ID: 2402.06266
- Source URL: https://arxiv.org/abs/2402.06266
- Reference count: 3
- Value function interference causes convergence to sub-optimal policies in MORL with non-linear utility functions

## Executive Summary
Value function interference is a previously unidentified issue in value-based multi-objective reinforcement learning where widely varying vector-values map to similar utility levels, leading to convergence to sub-optimal policies. The problem arises when random tie-breaking introduces stochasticity that corrupts Q-value learning in predecessor states. Empirical results demonstrate that deterministic tie-breaking can reduce but not eliminate this interference, with improvements of nearly 30% in avoiding sub-optimal policy convergence. The authors suggest distributional RL approaches as a more comprehensive solution to this fundamental challenge in MORL.

## Method Summary
The paper presents a theoretical analysis of value function interference in tabular multi-objective Q-learning with non-linear utility functions. The method involves implementing a simple 3-state MOMDP with vector rewards and comparing different tie-breaking strategies (random, deterministic lower-index, deterministic higher-index) across various hyperparameter settings. The approach uses e-greedy exploration with linearly decayed epsilon and tracks policy selection accuracy to measure interference effects. The work also proposes distributional RL as a potential solution for completely avoiding interference by learning full distributions of vector returns.

## Key Results
- Value function interference causes convergence to sub-optimal policies in 30-50% of trials with random tie-breaking
- Deterministic tie-breaking reduces sub-optimal policy convergence by nearly 30% compared to random tie-breaking
- Value function interference is most severe in stochastic environments but can also occur in deterministic settings
- The problem is particularly acute when utility functions map widely varying vector-values to similar utility levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Value function interference occurs when utility functions map widely varying vector-values to similar utility levels
- Mechanism: When two actions have similar scalarized utility but different vector outcomes, random tie-breaking introduces stochasticity that corrupts Q-value learning in predecessor states
- Core assumption: The utility function is non-linear and allows different vector outcomes to map to the same scalar utility
- Evidence anchors:
  - [abstract] "if the user's utility function maps widely varying vector-values to similar levels of utility, this can lead to interference in the value-function learned by the agent"
  - [section 2.1] "The existence of two equally optimal actions in state B creates an issue... The usual approach taken in Q-learning where more than one optimal action exists is to simply randomly select between them"
  - [corpus] No direct corpus evidence found for this specific interference mechanism
- Break condition: Interference breaks when utility function is linear or when deterministic tie-breaking is used consistently

### Mechanism 2
- Claim: Deterministic tie-breaking can ameliorate but not fully eliminate value function interference
- Mechanism: By always selecting the same action when multiple actions have equal utility, the stochasticity in Q-value updates is reduced, leading to more consistent learning
- Core assumption: The utility function allows for deterministic preferences when actions have equal utility
- Evidence anchors:
  - [abstract] "Empirical results demonstrate that using deterministic tie-breaking when identifying greedy actions can ameliorate but not fully overcome this interference"
  - [section 2.2] "The results from the two deterministic tie-breaking agents... demonstrate substantial improvements over the random tie-breaking agent"
  - [corpus] No direct corpus evidence found for this specific deterministic tie-breaking solution
- Break condition: Interference breaks when environment stochasticity dominates or when utility function is highly non-linear

### Mechanism 3
- Claim: Distributional RL approaches can provide a more comprehensive solution to value function interference
- Mechanism: By learning the full distribution of vector returns rather than just expected values, the agent can properly account for the stochasticity in returns when computing expected scalarized returns
- Core assumption: The distribution of returns can be accurately learned and combined with the utility function
- Evidence anchors:
  - [abstract] "The authors suggest distributional RL approaches or policy-search methods as more comprehensive solutions to avoid value function interference"
  - [section 3] "we suggest that the most promising course of action is to incorporate concepts from distributional RL into MORL"
  - [corpus] No direct corpus evidence found for this specific distributional RL solution
- Break condition: Interference breaks when distributional learning is accurate and utility function can be properly applied to distributions

## Foundational Learning

- Concept: Multi-objective reinforcement learning
  - Why needed here: The paper is fundamentally about extending RL to handle multiple conflicting objectives
  - Quick check question: What are the two main optimization criteria in MORL and how do they differ?

- Concept: Utility functions and scalarization
  - Why needed here: The interference problem arises from how utility functions map vector rewards to scalar values
  - Quick check question: Why does linear utility avoid the interference problem that non-linear utility creates?

- Concept: Tie-breaking in action selection
  - Why needed here: The paper demonstrates how different tie-breaking strategies affect learning outcomes
  - Quick check question: How does random tie-breaking introduce stochasticity that leads to value function interference?

## Architecture Onboarding

- Component map: State observation -> Q-value lookup -> Utility function application -> Action selection (with tie-breaking) -> Environment interaction -> Q-value update
- Critical path: State observation → Q-value lookup → Utility function application → Action selection (with tie-breaking) → Environment interaction → Q-value update
- Design tradeoffs: Random tie-breaking is simple but causes interference; deterministic tie-breaking reduces interference but may introduce bias; distributional RL is more complex but provides comprehensive solution
- Failure signatures: Convergence to sub-optimal policies, unstable learning, high variance in performance across runs
- First 3 experiments:
  1. Implement the simple MOMDP from Figure 1 and verify that random tie-breaking leads to convergence to sub-optimal policy
  2. Add deterministic tie-breaking and measure improvement in convergence rate
  3. Implement a distributional MORL approach and compare against baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are distributional RL approaches compared to value-based methods in completely eliminating value function interference in stochastic multi-objective environments?
- Basis in paper: [explicit] The paper suggests distributional RL approaches as a more comprehensive solution to avoid value function interference, stating that if the agent learns a distribution over vector returns for each state-action, then this can be combined with the utility function so as to estimate the Expected Scalarised Return (ESR).
- Why unresolved: The paper only provides a theoretical suggestion and does not present empirical results comparing distributional RL approaches to value-based methods in eliminating value function interference.
- What evidence would resolve it: Empirical results comparing the performance of distributional RL approaches to value-based methods in multi-objective stochastic environments, specifically measuring the reduction in sub-optimal policy convergence due to value function interference.

### Open Question 2
- Question: Can deterministic tie-breaking methods be further improved or combined with other techniques to more effectively mitigate value function interference in deterministic multi-objective environments?
- Basis in paper: [explicit] The paper demonstrates that using deterministic tie-breaking when identifying greedy actions can ameliorate but not fully overcome the problems caused by value function interference, with one deterministic approach reducing sub-optimal policy convergence by nearly 30% compared to random tie-breaking.
- Why unresolved: While the paper shows that deterministic tie-breaking can reduce the occurrence of value function interference, it does not explore whether this method can be further improved or combined with other techniques for better mitigation.
- What evidence would resolve it: Experimental results comparing different deterministic tie-breaking methods, as well as combinations of deterministic tie-breaking with other techniques, in terms of their effectiveness in reducing value function interference and improving policy convergence in deterministic multi-objective environments.

### Open Question 3
- Question: How does the degree of non-linearity in the utility function impact the severity of value function interference in multi-objective reinforcement learning?
- Basis in paper: [inferred] The paper mentions that value function interference arises when the user's utility function maps widely varying vector-values to similar levels of utility, and that this is particularly likely to occur for highly non-linear definitions of utility.
- Why unresolved: The paper does not provide a quantitative analysis of how the degree of non-linearity in the utility function affects the severity of value function interference.
- What evidence would resolve it: A study systematically varying the degree of non-linearity in the utility function and measuring the corresponding impact on the severity of value function interference, as well as the effectiveness of different mitigation strategies.

## Limitations
- The empirical validation relies on a single synthetic 3-state MOMDP example
- The "nearly 30%" improvement claim is based on comparisons across different hyperparameter settings without controlling for exploration rates or learning rates
- The proposed distributional RL solution is discussed conceptually but lacks empirical validation in this work

## Confidence
- High confidence: The existence of value function interference as a theoretical problem in MORL with non-linear utility functions
- Medium confidence: The empirical demonstration that random tie-breaking leads to sub-optimal policies in the specific 3-state MOMDP
- Medium confidence: The claim that deterministic tie-breaking can reduce but not eliminate interference
- Low confidence: The assertion that distributional RL approaches provide a comprehensive solution (lacks empirical support)

## Next Checks
1. **Generalization testing**: Implement the interference phenomenon in more complex MOMDPs with varying numbers of states, actions, and objectives to verify if the effect scales beyond the simple 3-state example
2. **Hyperparameter sensitivity analysis**: Conduct controlled experiments where learning rate and exploration are held constant while varying only the tie-breaking strategy to isolate the interference effect
3. **Distributional RL validation**: Implement and test the proposed distributional MORL approach on the same 3-state MOMDP to empirically verify if it can completely eliminate value function interference