---
ver: rpa2
title: 'The Remarkable Robustness of LLMs: Stages of Inference?'
arxiv_id: '2406.19384'
source_url: https://arxiv.org/abs/2406.19384
tags:
- layer
- arxiv
- layers
- prediction
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of large language models
  (LLMs) to structural interventions by deleting and swapping adjacent layers during
  inference. Surprisingly, models retain 72-95% of their original top-1 prediction
  accuracy without any fine-tuning.
---

# The Remarkable Robustness of LLMs: Stages of Inference?

## Quick Facts
- arXiv ID: 2406.19384
- Source URL: https://arxiv.org/abs/2406.19384
- Authors: Vedang Lad; Jin Hwa Lee; Jin Hwa Lee; Wes Gurnee; Max Tegmark
- Reference count: 40
- One-line primary result: LLMs retain 72-95% accuracy after structural layer interventions, revealing four distinct inference stages.

## Executive Summary
This paper investigates the robustness of large language models to structural interventions by deleting and swapping adjacent layers during inference. Surprisingly, models retain 72-95% of their original top-1 prediction accuracy without any fine-tuning. The authors find that performance degradation is not uniform across layers: interventions to the early and final layers cause the most degradation, while the model is remarkably robust to dropping middle layers. This pattern of localized sensitivity motivates their hypothesis of four stages of inference: (1) detokenization, (2) feature engineering, (3) prediction ensembling, and (4) residual sharpening.

## Method Summary
The authors perform layer deletion (zero ablation) and adjacent layer swapping interventions during inference across five model families (Pythia, GPT-2, Qwen 2.5, LLaMA 3.2, Microsoft Phi) ranging from 124M to 6.9B parameters. They evaluate on 1M tokens from the Pile dataset, measuring KL divergence, top-1 prediction consistency, and benchmark task performance. Additional analysis includes logit lens entropy tracking, MLP output norms, CKA similarity across layers, neuron classification into prediction/suppression types, and linear probe training for semantic tasks.

## Key Results
- LLMs retain 72-95% of original top-1 prediction accuracy after structural interventions without fine-tuning
- Early and final layer interventions cause most degradation; middle layers are remarkably robust to both deletion and order changes
- Four-stage inference framework emerges: detokenization (early), feature engineering (middle), prediction ensembling (mid-late), residual sharpening (final)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer deletion and swapping interventions reveal depth-dependent computational stages by exposing redundancy and localization patterns.
- Mechanism: The residual architecture allows multiple computational pathways to contribute to the same output. Deleting or swapping layers tests which components are essential versus redundant, revealing stages where computation is either specialized (early/late) or overlapping (middle).
- Core assumption: Transformer residual connections enable functional redundancy, allowing the model to maintain performance despite local disruptions.
- Evidence anchors: [abstract] "models retain 72-95% of their original top-1 prediction accuracy without any fine-tuning" and "interventions to the early and final layers cause the most degradation, while the model is remarkably robust to dropping middle layers"

### Mechanism 2
- Claim: Early layers specialize in detokenization by integrating local context to form coherent entities from raw token embeddings.
- Mechanism: The first layer uniquely transforms token embeddings from the embedding basis to the transformer's residual stream basis. Attention heads in early layers copy nearby tokens to construct multi-token words and short-range dependencies, lifting raw representations into higher-level entities.
- Core assumption: The first layer is fundamentally different from subsequent layers in its computational role and sensitivity to ablation.
- Evidence anchors: [section] "we infer that the first layer is not a normal layer, but rather an extension of the embedding" and "by ablating the first layer, the rest of the network is blind to the instant context"

### Mechanism 3
- Claim: Prediction and suppression neurons work in ensembles to shape the output distribution through complementary activation patterns.
- Mechanism: Prediction neurons systematically promote specific tokens while suppression neurons decrease probabilities of others. Their density increases through middle layers, creating an ensemble effect where multiple pathways contribute to next-token prediction.
- Core assumption: The inverse relationship between prediction and suppression neurons reflects an ensembling mechanism rather than independent processes.
- Evidence anchors: [section] "Prediction neurons... work in tandem with suppression neurons... to shape the model's output" and "Ensembles of prediction neurons outperform both individual neurons and the model average"

## Foundational Learning

- Concept: Transformer residual connections and layer normalization
  - Why needed here: Understanding how residual streams enable functional redundancy and how layer norm preprocessing affects weight calculations is critical for interpreting intervention results
  - Quick check question: How does folding layer norm parameters into MLP weights affect the interpretation of layer-wise interventions?

- Concept: Centered Kernel Alignment (CKA) for measuring representational similarity
  - Why needed here: CKA reveals block-like structures across layers, supporting the hypothesis of distinct computational stages
  - Quick check question: What does high CKA similarity between adjacent layers indicate about their functional relationship?

- Concept: Logit lens technique for analyzing intermediate predictions
  - Why needed here: Logit lens reveals when semantic features become incorporated into confident next-token predictions, marking stage transitions
  - Quick check question: How does the entropy of intermediate predictions change across the four hypothesized stages?

## Architecture Onboarding

- Component map: Embedding layer → first layer (detokenization) → middle layers (feature engineering) → prediction layers (prediction ensembling) → final layers (residual sharpening) → unembedding output
- Critical path: Token embedding → first layer → middle layers → prediction layers → final layers → unembedding output
- Design tradeoffs: Residual connections provide robustness but may reduce specialization; deeper models offer more gradual computation but increase computational cost; layer normalization stabilizes training but adds parameters
- Failure signatures: Catastrophic performance drop indicates intervention to critical early/late layers; high entropy predictions suggest loss of local context integration; low WiC probe accuracy indicates failure in semantic feature construction
- First 3 experiments:
  1. Perform layer deletion on GPT-2 Small, measuring KL divergence and prediction accuracy to verify early-layer sensitivity
  2. Apply adjacent layer swapping on Pythia 2.8B, computing CKA similarity to identify block-like structures
  3. Implement logit lens analysis on Llama 3.2 1B, tracking entropy changes to locate prediction ensembling onset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the boundaries between the four inference stages vary across different tasks or domains?
- Basis in paper: [inferred] from the statement that "Stage boundaries are approximate, and multiple stages may co-occur within a single layer."
- Why unresolved: The paper focuses on general text generation tasks but doesn't explore whether the stage boundaries shift for specialized domains like code generation or mathematical reasoning.
- What evidence would resolve it: Systematic comparison of stage boundary positions across multiple benchmark tasks, measuring how stage transition points vary with task type.

### Open Question 2
- Question: What determines the optimal number of layers for a given parameter budget in transformer architectures?
- Basis in paper: [explicit] from the finding that "GPT-2 models... have fewer layers... compared to later Phi models" and the observation that "Increasing the number of layers may also provide a means for greater redundancy in models."
- Why unresolved: While the paper notes that GPT-2 has more layers and higher redundancy than Pythia models, it doesn't establish whether this redundancy is beneficial or merely a result of different training approaches.
- What evidence would resolve it: Controlled experiments varying layer count while holding parameter count constant, measuring performance, robustness, and computational efficiency.

### Open Question 3
- Question: How do prediction and suppression neurons interact during inference, and what governs their density across model depths?
- Basis in paper: [explicit] from the observation that "The balance between prediction and suppression neurons appears to shape the model's output" and their density patterns across layers.
- Why unresolved: The paper identifies these neuron types and their distribution patterns but doesn't explain the mechanisms that determine their emergence and balance during training.
- What evidence would resolve it: Detailed analysis of training dynamics showing how these neurons develop, potentially through targeted ablation studies or gradient-based attribution methods.

### Open Question 4
- Question: Are the four stages of inference universal across all transformer architectures, or do they vary significantly with architectural choices?
- Basis in paper: [explicit] from the claim that the framework "emerges across architectures and scales" but with the caveat that "we do not isolate the factors behind model-specific differences."
- Why unresolved: The paper demonstrates the framework works across multiple model families but doesn't systematically vary architectural components like attention mechanisms or MLP structures.
- What evidence would resolve it: Comparative analysis of models with systematically varied architectural components, testing whether the four-stage framework still applies.

### Open Question 5
- Question: How does the detokenization stage differ between models trained with different tokenization schemes?
- Basis in paper: [inferred] from the discussion of how early layers integrate local context to form coherent entities, particularly for multi-token words.
- Why unresolved: The paper mentions subjoiner heads and context integration but doesn't explore how different tokenization approaches (WordPiece vs SentencePiece vs alternatives) affect this early-stage computation.
- What evidence would resolve it: Comparative analysis of early-layer attention patterns and subjoiner head behavior across models trained with different tokenization schemes.

## Limitations
- The evidence for four-stage hypothesis relies heavily on behavioral patterns rather than direct mechanistic validation
- Generalizability across model architectures is uncertain despite examining multiple families
- Neuron classification into prediction/suppression types lacks direct causal evidence linking these neurons to their hypothesized functions

## Confidence
- High Confidence: The core empirical finding that LLMs retain 72-95% accuracy after structural interventions, with early and late layers showing greater sensitivity than middle layers
- Medium Confidence: The four-stage inference framework as a descriptive model of depth-dependent computation
- Low Confidence: The specific computational mechanisms proposed for each stage as distinct, separable processes

## Next Checks
1. Perform targeted neuron ablation within each layer to test whether prediction and suppression neurons identified through statistical methods actually drive behavioral changes when individually disrupted
2. Apply the same intervention protocol to encoder-decoder models (e.g., BERT, T5) and smaller transformer variants to determine whether the four-stage pattern is universal or architecture-specific
3. Track how intermediate prediction entropy and logit distributions evolve during generation of individual tokens to validate whether proposed stages correspond to distinct temporal phases of computation