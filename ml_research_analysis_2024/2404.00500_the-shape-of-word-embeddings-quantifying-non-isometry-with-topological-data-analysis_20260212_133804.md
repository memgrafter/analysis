---
ver: rpa2
title: 'The Shape of Word Embeddings: Quantifying Non-Isometry With Topological Data
  Analysis'
arxiv_id: '2404.00500'
source_url: https://arxiv.org/abs/2404.00500
tags:
- dist
- distance
- tree
- languages
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether the general shape of unlabeled
  word embedding clouds carries information about language history and relationships.
  The authors use topological data analysis (TDA) to quantify non-isometry between
  word embeddings of 81 Indo-European languages by computing persistent homology-based
  distances.
---

# The Shape of Word Embeddings: Quantifying Non-Isometry With Topological Data Analysis

## Quick Facts
- arXiv ID: 2404.00500
- Source URL: https://arxiv.org/abs/2404.00500
- Authors: Ondřej Draganov; Steven Skiena
- Reference count: 40
- Primary result: Unlabeled word embedding shapes capture real linguistic signal about language relationships, not random training artifacts

## Executive Summary
This paper investigates whether the general shape of unlabeled word embedding clouds carries information about language history and relationships. Using topological data analysis (TDA), the authors quantify non-isometry between word embeddings of 81 Indo-European languages by computing persistent homology-based distances. These distances are then used to construct phylogenetic trees via agglomerative clustering, which are compared to a reference Ethnologue tree using six different tree similarity metrics and permutation tests. The results demonstrate that unlabeled word embedding shapes contain meaningful linguistic signal about language relationships.

## Method Summary
The method computes persistent homology on word embeddings to extract topological features, then calculates distances between these features across languages. For each of 81 Indo-European languages, token-to-token distance matrices are computed using Euclidean and cosine metrics. Persistent homology (degrees 0, 1, 2) is calculated using Ripser, and pairwise language distances are computed using bottleneck, sliced Wasserstein, persistence image, and bars statistics methods. Phylogenetic trees are built using UPGMA and neighbor joining algorithms, then evaluated against a reference Ethnologue tree using six tree similarity metrics with 100,000 permutation tests to establish statistical significance.

## Key Results
- For 484 out of 864 parameter combinations, TDA-based trees significantly outperform random label permutations
- Some parameter combinations achieve up to 6.87 standard deviations from the mean (p-value of 2.77 × 10⁻⁹ after Bonferroni correction)
- TDA-based methods generally outperform simple embedding-level distance measures between languages

## Why This Works (Mechanism)

### Mechanism 1
The general shape of unlabeled word embeddings carries real linguistic signal about language relationships. Topological Data Analysis captures structural properties of point clouds that persist across small perturbations, and these persistent homology features quantify non-isometry between language embeddings, reflecting underlying linguistic structure.

### Mechanism 2
Phylogenetic trees reconstructed from TDA-based distances significantly outperform random label permutations. Agglomerative clustering converts pairwise TDA distance matrices into phylogenetic trees, and permutation tests comparing these trees to a reference Ethnologue tree demonstrate statistical significance.

### Mechanism 3
Different TDA distances (bottleneck, sliced Wasserstein, persistence image, bars statistics) capture different aspects of non-isometry relevant to language relationships. Each distance metric between persistence diagrams emphasizes different features - bottleneck focuses on worst-case matching, sliced Wasserstein on overall distribution, persistence images on visual similarity, and bars statistics on summary features.

## Foundational Learning

- **Persistent homology and Vietoris-Rips complexes**: Forms the mathematical foundation for extracting topological features from high-dimensional word embeddings
  - Quick check: What is the difference between a 0-dimensional and 1-dimensional feature in persistent homology?

- **Distance metrics between persistence diagrams**: Different metrics capture different aspects of diagram similarity
  - Quick check: Why might bottleneck distance be less effective than sliced Wasserstein for comparing word embedding shapes?

- **Agglomerative hierarchical clustering (UPGMA and NJ)**: Converts pairwise distance matrices into phylogenetic trees for evaluation against reference language trees
  - Quick check: What is the key difference between how UPGMA and NJ define "distance" between clusters during tree construction?

## Architecture Onboarding

- **Component map**: FastText embeddings -> Token-to-token distance matrices -> Persistent homology computation -> Distance between persistence diagrams -> Language distance matrix -> Phylogenetic tree construction -> Tree evaluation
- **Critical path**: Steps 1-7 must execute sequentially; each output becomes the next input
- **Design tradeoffs**: Higher V (vocabulary size) gives more data but increases computational cost exponentially for degree 2 persistent homology; Euclidean vs cosine metrics capture different geometric properties
- **Failure signatures**: (1) Extremely sparse persistence diagrams indicate insufficient data or poor embedding quality; (2) All pairwise distances near zero suggests embeddings are nearly isometric; (3) Phylogenetic trees with random topology indicate no signal in TDA distances
- **First 3 experiments**:
  1. Compute persistence diagrams for a single language using both Euclidean and cosine metrics to verify basic TDA pipeline functionality
  2. Compare bottleneck vs sliced Wasserstein distances on synthetic isometric vs non-isometric embeddings to understand metric sensitivity
  3. Run UPGMA tree construction on a small subset (5-10 languages) with known relationships to verify basic phylogenetic reconstruction works before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
Would different word embedding training methods (e.g., word2vec, GloVe) yield comparable phylogenetic signal? The authors acknowledge they only tested FastText embeddings and note that "there are reasonable questions of whether our results may strengthen if we used a different reference tree than provided by Ethnologue which is difficult to compare to."

### Open Question 2
How sensitive are the results to the choice of reference tree? The paper uses the Ethnologue tree as ground truth, but phylogenetic relationships in linguistics remain debated, and the choice of reference could influence the evaluation of TDA-based methods.

## Limitations

- Results are based on a specific family of languages (Indo-European) and may not generalize to other language families
- The choice of reference tree (Ethnologue) as ground truth introduces potential biases
- Computational intensity of degree 2 persistent homology with large vocabularies raises scalability questions

## Confidence

- **High confidence**: The statistical significance of TDA-based trees outperforming random permutations (484/864 parameter combinations, p<0.05 after Bonferroni correction)
- **Medium confidence**: The general mechanism that non-isometry between embeddings reflects linguistic relationships
- **Medium confidence**: The superiority of TDA methods over simple embedding-level distances

## Next Checks

1. **Cross-family validation**: Apply the same TDA pipeline to non-Indo-European language families (e.g., Sino-Tibetan or Afro-Asiatic) to test generalizability of findings.

2. **Baseline comparison**: Implement and compare against simpler distance measures between embeddings (e.g., Gromov-Wasserstein distances) to quantify the added value of topological features.

3. **Ablation study**: Systematically vary vocabulary size (V) and embedding dimension to identify the minimal requirements for detecting linguistic signal.