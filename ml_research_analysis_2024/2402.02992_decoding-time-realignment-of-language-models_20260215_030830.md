---
ver: rpa2
title: Decoding-time Realignment of Language Models
arxiv_id: '2402.02992'
source_url: https://arxiv.org/abs/2402.02992
tags:
- dera
- reward
- language
- aligned
- regularization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Decoding-time Realignment (DeRa), a method
  to explore different regularization strengths in aligned language models without
  retraining. DeRa enables smooth transitions between unaligned and aligned models
  by modifying the sampling procedure to blend reference and aligned models at decoding
  time.
---

# Decoding-time Realignment of Language Models

## Quick Facts
- arXiv ID: 2402.02992
- Source URL: https://arxiv.org/abs/2402.02992
- Reference count: 29
- Primary result: DeRa enables smooth transitions between unaligned and aligned models by modifying the sampling procedure to blend reference and aligned models at decoding time.

## Executive Summary
This paper introduces Decoding-time Realignment (DeRa), a method that enables exploration of different regularization strengths in aligned language models without retraining. DeRa achieves this by modifying the sampling procedure to blend reference and aligned models at decoding time, providing an autoregressive approximation to geometric mixtures of these models. The key insight is that aligned models with varying KL regularization strengths can be expressed as geometric mixtures of a reference model and an aligned model, differing only in their mixing weights. DeRa allows users to control alignment strength during decoding, facilitating efficient hyperparameter tuning and enabling smooth transitions between unaligned and aligned models.

## Method Summary
DeRa works by combining logits from a reference model (typically an SFT model) and an aligned model (trained with KL regularization strength β) at each decoding step. The combination uses a parameter λ to control the effective alignment strength β/λ. The method relies on the theoretical insight that aligned models with different regularization strengths are geometric mixtures of the reference and aligned models. By linearly combining logits in this way, DeRa provides an autoregressive approximation to these mixtures, enabling efficient exploration of the regularization spectrum without the need for retraining multiple models.

## Key Results
- DeRa identified optimal KL strengths in summarization tasks that outperformed default strengths with win rates up to 0.9
- DeRa successfully controlled hallucination generation by adjusting alignment level
- DeRa matched the performance of fully-retrained models while requiring no retraining and only doubling decoding time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligned models with varying KL regularization strengths are geometric mixtures of a reference model and a single aligned model.
- Mechanism: The optimal KL-regularized alignment objective has a closed-form solution that expresses the aligned model as a geometrically reweighted combination of the SFT model and the aligned model.
- Core assumption: The KL-regularized alignment objective with regularization strength β produces a unique solution expressible in geometric mixture form.
- Evidence anchors: [section] "Based on the KL-regularized alignment objective, we prove that aligned models with varying KL regularization strengths are all geometric mixtures of a reference model and a single aligned model, differing only by their mixing weights."

### Mechanism 2
- Claim: DeRa provides an autoregressive approximation to these geometric mixtures.
- Mechanism: By linearly combining the logits of the reference and aligned models with parameter λ, DeRa approximates the geometric mixture at the token level.
- Core assumption: The autoregressive approximation using linearly combined logits is a good proxy for the true geometric mixture distribution.
- Evidence anchors: [abstract] "DeRa provides an autoregressive approximation to these mixtures, allowing users to control alignment strength during decoding."

### Mechanism 3
- Claim: DeRa enables efficient hyperparameter tuning by exploring different regularization strengths without retraining.
- Mechanism: By adjusting λ at decoding time, DeRa can sample from models with different effective regularization strengths (β/λ).
- Core assumption: The performance of DeRa-sampled models correlates well with fully-retrained models at the same effective regularization strength.
- Evidence anchors: [abstract] "DeRa facilitates controlling alignment strengths, speeds up hyperparameter tuning, and helps performance tradeoffs in downstream tasks."

## Foundational Learning

- Concept: KL divergence and its role in regularization
  - Why needed here: The paper's core mechanism relies on KL regularization to control the distance between aligned and reference models.
  - Quick check question: What does minimizing KL divergence between π and πsft achieve in the context of language model alignment?

- Concept: Autoregressive modeling and the chain rule of probability
  - Why needed here: DeRa works by combining logits at each decoding step, which relies on the autoregressive factorization of language models.
  - Quick check question: How does the chain rule of probability enable efficient sampling in autoregressive language models?

- Concept: Geometric mixtures and their properties
  - Why needed here: The paper proves that aligned models with different regularization strengths form geometric mixtures, which is the theoretical foundation for DeRa.
  - Quick check question: What is the difference between a geometric mixture and an arithmetic mixture in probability distributions?

## Architecture Onboarding

- Component map: Pretrained Model -> SFT Model (πsft) -> Aligned Model (πθ(β)) -> DeRa Sampler -> Output

- Critical path:
  1. Load both reference and aligned model checkpoints
  2. At each decoding step, obtain logits from both models
  3. Combine logits using λ: λhθ(β) + (1-λ)hsft
  4. Apply softmax to get token probabilities
  5. Sample next token and continue

- Design tradeoffs:
  - Memory vs. performance: DeRa requires loading two models, doubling memory usage
  - Inference speed: Combining logits adds computation, roughly doubling decoding time
  - Flexibility vs. accuracy: Linear logit combination is an approximation; may not perfectly match retrained models

- Failure signatures:
  - Inconsistent outputs between DeRa and retrained models at same β/λ
  - Degraded performance when λ is outside [0,1] range
  - Memory errors when loading both models on limited hardware

- First 3 experiments:
  1. Verify logit combination: Compare softmax outputs of combined logits vs. separately computed geometric mixture
  2. Correlation test: Compare DeRa outputs with retrained models across different λ values
  3. Ablation study: Test DeRa with different base models (e.g., pretrained vs. SFT) to understand sensitivity to reference model choice

## Open Questions the Paper Calls Out
The paper mentions the possibility of extending DeRa to handle multiple rewards or reward functions, but does not provide experimental results or a detailed discussion of how this extension would work. It is unclear how DeRa would combine the effects of multiple rewards and how it would handle conflicts between different reward functions.

## Limitations
- The geometric-mixture proof assumes the closed-form solution of the KL-regularized alignment objective holds exactly, but empirical verification is limited to specific models and objectives tested.
- Computational overhead of loading and running two models simultaneously may become prohibitive for very large models or resource-constrained deployment scenarios.
- Effectiveness across diverse alignment objectives (such as safety alignment, style transfer, or multi-task alignment) remains unexplored.

## Confidence
- High confidence in the core mechanism and empirical validation for tested use cases (summarization and hallucination mitigation)
- Medium confidence in the generalization of results to other alignment objectives and model architectures
- Medium confidence in the computational efficiency claims

## Next Checks
1. Systematically test DeRa performance at λ values far outside the [0,1] range to identify precise failure modes and quantify the reliability of the autoregressive approximation across the full parameter space.
2. Apply DeRa to safety alignment tasks using datasets like RealToxicityPrompts or OpenAI's behavioral fine-tuning datasets to verify whether the geometric-mixture property holds for alignment objectives beyond summarization quality and hallucination mitigation.
3. Implement and evaluate a memory-optimized version of DeRa that uses mixed precision, model sharding, or sequential loading to reduce the 2x memory overhead, measuring the tradeoff between computational efficiency and alignment quality degradation.