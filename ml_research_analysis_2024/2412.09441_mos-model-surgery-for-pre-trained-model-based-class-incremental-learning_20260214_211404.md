---
ver: rpa2
title: 'MOS: Model Surgery for Pre-Trained Model-Based Class-Incremental Learning'
arxiv_id: '2412.09441'
source_url: https://arxiv.org/abs/2412.09441
tags:
- adam
- uni00000044
- adapter
- uni00000048
- uni00000055
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MOS addresses catastrophic forgetting in class-incremental learning
  by performing model surgery that corrects both parameter drift and retrieval errors.
  It trains progressively merged adapters to bridge gaps between tasks while retaining
  task-specific information, and employs a training-free self-refined adapter retrieval
  mechanism during inference to autonomously correct mistaken retrievals.
---

# MOS: Model Surgery for Pre-Trained Model-Based Class-Incremental Learning

## Quick Facts
- **arXiv ID**: 2412.09441
- **Source URL**: https://arxiv.org/abs/2412.09441
- **Reference count**: 40
- **Primary result**: Achieves 2-5% improvement over state-of-the-art methods on seven benchmark datasets

## Executive Summary
MOS addresses catastrophic forgetting in class-incremental learning through model surgery that corrects both parameter drift and retrieval errors. The method trains progressively merged adapters to bridge gaps between tasks while retaining task-specific information, and employs a training-free self-refined adapter retrieval mechanism during inference to autonomously correct mistaken retrievals. MOS achieves state-of-the-art performance, outperforming existing methods by 2-5% on multiple benchmark datasets including CIFAR-100, CUB-200, ImageNet-R, ImageNet-A, ObjectNet, OmniBenchmark, and VTAB.

## Method Summary
MOS is a pre-trained model-based class-incremental learning method that uses adapter merging strategy with Exponential Moving Average (EMA) to mitigate parameter-level forgetting. For each incremental task, task-specific adapters are trained with frozen backbone weights, then merged using EMA to progressively incorporate knowledge from previous adapters. During inference, a self-refined adapter retrieval mechanism uses the model's inherent prediction ability to verify and correct initial adapter selection. Finally, a two-stage model ensemble balances stability-plasticity by combining predictions from the initial adapter (trained on first task) and task-specific adapters. The method follows an exemplar-free setting and uses prototype-based classifiers with Gaussian distributions for alignment.

## Key Results
- MOS outperforms state-of-the-art methods by 2-5% on seven benchmark datasets
- Achieves 68.9% average accuracy on CIFAR-100 B0 Inc5, 66.5% on CUB-200 B0 Inc10, and 65.1% on ImageNet-R B0 Inc20
- Outperforms exemplar-based methods (iCaRL, DER) and exemplar-free methods (LUCIR, ORE) in extensive comparisons

## Why This Works (Mechanism)

### Mechanism 1: Adapter Merging Strategy
Adapter merging strategy mitigates parameter drift by maintaining relevance between adapters across tasks. Exponential Moving Average (EMA) combines current adapter with weighted average of all previous adapters, preserving task-specific information while bridging gaps. Core assumption: adapters trained sequentially maintain some task-relevant features that can be leveraged by future adapters. Evidence anchors: [abstract] states adapter merging "bridges the gap between different components while reserve task-specific information" and [section] shows Ab = (1 − α) ˆAb + α b−1 Xb−1 k=1 Ak formula. Break condition: if adapters become too task-specific to share meaningful information, EMA would blend irrelevant features.

### Mechanism 2: Self-Refined Adapter Retrieval
Self-refined adapter retrieval mechanism corrects mistaken adapter selection during inference. Initial adapter selection uses pre-trained model, then iteratively checks consistency by comparing predicted task ID against actual task, repeating until consistency is achieved. Core assumption: model's inherent prediction ability can identify when retrieved adapter doesn't match current input. Evidence anchors: [abstract] mentions "training-free self-refined adapter retrieval mechanism during inference, which leverages the model's inherent ability for better adapter retrieval" and [section] explains "self-refined adapter retrieval mechanism allowing the model to verify the correctness of its initial predictions." Break condition: if initial adapter selection is consistently wrong, iterative refinement may loop or converge to wrong adapter.

### Mechanism 3: Multi-Stage Model Ensemble
Multi-stage model ensemble balances stability-plasticity dilemma. Combines predictions from initial adapter (strong generalization) and task-specific adapters (deep processing) via logit summation. Core assumption: initial adapter trained on first task provides generalizable features while task-specific adapters provide specialized processing. Evidence anchors: [abstract] states ensemble "integrates the model's capabilities across multiple phases" and [section] explains initial adapter "demonstrates strong generalization" and "ability to quickly recognize and update information." Break condition: if initial adapter becomes too outdated, its contribution may hurt performance.

## Foundational Learning

- **Catastrophic forgetting in neural networks**: Why needed here: MOS explicitly targets both parameter and retrieval-level forgetting, making understanding forgetting mechanisms essential. Quick check question: What happens to neural network performance on old tasks when training continues on new tasks without any mitigation strategy?

- **Parameter-efficient fine-tuning (PEFT)**: Why needed here: MOS uses adapters as PEFT method to modify pre-trained models with minimal parameter updates. Quick check question: How do adapters differ from full fine-tuning in terms of parameters added and computational cost?

- **Exponential Moving Average (EMA)**: Why needed here: MOS employs EMA for adapter merging to progressively incorporate knowledge from previous adapters. Quick check question: How does EMA weighting parameter α control the balance between current adapter and historical adapters?

## Architecture Onboarding

- **Component map**: Pre-trained Vision Transformer (frozen backbone) -> Task-specific adapters (one per incremental task) -> Exponential Moving Average module (adapter merging) -> Self-refined retrieval mechanism (inference time) -> Ensemble layer (final prediction combination)

- **Critical path**: 1. Train adapter for current task with frozen backbone 2. Apply EMA to merge with previous adapters 3. Extract class prototypes for prototype-based classifier 4. During inference: initial adapter selection → self-refinement → ensemble prediction

- **Design tradeoffs**: Adapter size (r=16 chosen) vs. performance vs. parameter overhead; EMA momentum α (0.1 chosen) vs. stability vs. adaptability; Self-refinement iterations vs. inference speed vs. accuracy; Ensemble contribution of initial adapter vs. task-specific adapters

- **Failure signatures**: Poor performance despite high parameter count → adapter merging not effective; High inference time → self-refinement looping excessively; Degradation on early tasks → ensemble weighting incorrect; Parameter explosion → adapters not properly merged

- **First 3 experiments**: 1. CIFAR-100 B0 Inc5 with ViT-B/16-IN1K to verify adapter merging improves over sequential adapters 2. ImageNet-R B0 Inc20 to test self-refined retrieval mechanism effectiveness 3. CUB-200 B0 Inc10 to validate ensemble component contribution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of MOS scale with increasingly large number of incremental tasks and classes?
- **Basis in paper**: [inferred] The paper mentions MOS achieves state-of-the-art performance on seven benchmark datasets with varying class splits, but does not explicitly test performance with extremely large numbers of tasks.
- **Why unresolved**: The experiments in the paper use a maximum of 300 classes (OmniBenchmark B0 Inc30) and 40 classes per task increment, but do not explore performance with significantly larger task counts.
- **What evidence would resolve it**: Experiments testing MOS with hundreds of incremental tasks and thousands of total classes would demonstrate its scalability limits and performance degradation patterns.

### Open Question 2
- **Question**: How does MOS compare to exemplar-based methods when exemplar storage is limited or constrained?
- **Basis in paper**: [explicit] The paper mentions MOS follows an exemplar-free setting and outperforms exemplar-based methods like iCaRL and DER in Figure 3a, but does not explore scenarios with limited exemplar storage.
- **Why unresolved**: The paper only compares against exemplar-based methods in an unconstrained setting where they can store many exemplars, not in memory-constrained scenarios.
- **What evidence would resolve it**: Experiments comparing MOS against exemplar-based methods when exemplar storage is limited to a small fixed number would reveal if MOS maintains its advantage under realistic memory constraints.

### Open Question 3
- **Question**: How robust is MOS to significant domain shifts between incremental tasks?
- **Basis in paper**: [explicit] The paper evaluates on out-of-distribution datasets like ImageNet-R, ImageNet-A, and ObjectNet which have "significant domain gap with ImageNet," but does not systematically vary the degree of domain shift.
- **Why unresolved**: While MOS is tested on out-of-distribution datasets, the paper does not vary or measure the magnitude of domain shift between tasks to understand MOS's robustness to different levels of distribution shift.
- **What evidence would resolve it**: Experiments measuring MOS performance across tasks with controlled, varying degrees of domain shift would quantify its robustness to distributional changes.

## Limitations

- The paper lacks detailed implementation specifics for critical components, particularly the self-refined adapter retrieval mechanism and prototype-based classifier
- The claim of "training-free" retrieval is somewhat misleading as it requires model inference for verification
- The paper only tests on a maximum of 300 classes and 40 classes per task increment, not exploring scalability to extremely large task counts

## Confidence

- **High Confidence**: Adapter merging strategy using EMA is well-established and the described mechanism is technically sound
- **Medium Confidence**: Model ensemble approach for stability-plasticity balancing is reasonable but lacks ablation studies to confirm its necessity
- **Low Confidence**: Self-refined adapter retrieval mechanism details are insufficient for proper implementation and validation

## Next Checks

1. Implement and test the self-refined retrieval mechanism on CIFAR-100 to verify the iterative correction process converges within reasonable iterations and actually improves accuracy over simple adapter selection

2. Conduct ablation studies removing the model ensemble component to quantify its contribution to overall performance improvements

3. Analyze adapter parameter distributions after EMA merging to verify that task-specific information is preserved rather than being diluted by the averaging process