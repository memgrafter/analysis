---
ver: rpa2
title: 'WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain'
arxiv_id: '2408.11800'
source_url: https://arxiv.org/abs/2408.11800
tags:
- questions
- question
- arxiv
- llms
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents WeQA, the first comprehensive benchmark for
  evaluating retrieval-augmented generation (RAG) systems in the wind energy domain.
  The authors developed a hybrid framework combining automated question generation
  with human expert curation to create diverse question types from environmental impact
  documents.
---

# WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain

## Quick Facts
- arXiv ID: 2408.11800
- Source URL: https://arxiv.org/abs/2408.11800
- Reference count: 10
- Primary result: WeQA is the first comprehensive benchmark for evaluating retrieval-augmented generation systems in the wind energy domain.

## Executive Summary
This paper introduces WeQA, a novel benchmark for evaluating retrieval-augmented generation (RAG) systems in the wind energy domain. The authors developed a hybrid framework combining automated question generation with human expert curation to create diverse question types from environmental impact documents. Using RAGAS evaluation with multiple judge LLMs, they found notably low answer correctness scores across all models, particularly for evaluation and comparison questions, indicating the benchmark's challenging nature. The framework is domain-agnostic and provides a foundation for systematic assessment of RAG-based systems in complex scientific domains.

## Method Summary
The authors developed a hybrid human-AI teaming approach to create WeQA benchmark. The method involves extracting text from wind energy documents, generating diverse question types (closed, open, comparison, evaluation, etc.) using GPT-4 with human expert curation, and evaluating RAG performance using the RAGAS framework with multiple judge LLMs (GPT-4 and Gemini-1.5Pro). The benchmark includes 100 questions across six types, focusing on challenging evaluation and comparison questions that require deeper inferential reasoning across larger text segments.

## Key Results
- RAG-based systems showed notably low answer correctness scores across all models, particularly for evaluation and comparison questions
- Gemini-1.5Pro demonstrated higher evaluation scores than GPT-4 judge, though with some inconsistency
- The benchmark revealed that 'process', 'evaluation', and 'comparison' questions require deeper inferential reasoning across larger text segments
- There is alignment in evaluations made by the two judge LLMs used within the RAGAS framework, particularly visible for 'closed' type questions

## Why This Works (Mechanism)

### Mechanism 1
The hybrid human-AI teaming approach produces higher quality questions than either fully automated or fully manual methods alone. The framework combines automated question generation using GPT-4 with human domain expert curation, where experts identify exemplary questions that serve as few-shot examples to guide the automatic generation process.

### Mechanism 2
RAG-based systems perform poorly on evaluation and comparison questions because these require deeper inferential reasoning across larger text segments. The framework generates these questions from summarized text chunks rather than original text chunks, requiring models to synthesize information across broader contexts.

### Mechanism 3
Using multiple judge LLMs (GPT-4 and Gemini-1.5Pro) provides more reliable and unbiased evaluation than using a single judge. The framework employs two different judge LLMs to evaluate model responses, allowing for cross-validation of assessments.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG) architecture**: Understanding how RAG combines retrieval from external knowledge bases with LLM generation is fundamental to grasping the benchmark's purpose and methodology.
  - Quick check: How does RAG differ from standard LLM text generation, and why is this difference important for domain-specific applications?

- **Vector embeddings and semantic similarity search**: The framework uses vector embeddings to represent document content and retrieve relevant context for question answering, which is central to RAG performance.
  - Quick check: What role do vector embeddings play in the RAG pipeline, and how does semantic similarity search enable context retrieval?

- **Question type classification**: The benchmark evaluates RAG performance across different question types, each requiring different reasoning capabilities and retrieval strategies.
  - Quick check: How do different question types (e.g., comparison vs. recall) test different aspects of RAG system capabilities?

## Architecture Onboarding

- **Component map**: Document ingestion pipeline (PDF to JSON conversion) -> Text extraction and table/image processing -> Vector embedding generation and database storage -> Question generation framework (hybrid prompt and context approaches) -> RAG system under test -> Evaluation framework (RAGAS with multiple judge LLMs) -> Result analysis and benchmarking

- **Critical path**: Document ingestion → Vector embedding generation → Question generation → RAG response generation → Judge LLM evaluation → Result aggregation

- **Design tradeoffs**: Automated vs. manual question generation (speed vs. quality), summary-based vs. direct text question generation (complexity vs. accuracy), single vs. multiple judge LLMs (simplicity vs. reliability), context precision vs. recall in retrieval (specificity vs. completeness)

- **Failure signatures**: Low context precision indicates retrieval is pulling irrelevant information, low context recall indicates important context is being missed, high judge disagreement indicates evaluation instability, consistent failure on specific question types indicates capability gaps

- **First 3 experiments**: 1) Run the complete pipeline on a single document section to verify end-to-end functionality and identify bottlenecks, 2) Compare RAG performance with and without retrieval to establish baseline performance, 3) Test judge LLM agreement rates on a small sample of responses to validate the evaluation framework

## Open Questions the Paper Calls Out

1. How do the performance metrics of RAG-based LLMs differ when evaluating open-domain versus closed-domain questions in the wind energy domain?
2. What are the specific challenges and limitations of using RAGAS for evaluating RAG-based LLMs in specialized scientific domains like wind energy?
3. How can the hybrid question generation framework be adapted to other scientific domains beyond wind energy, and what are the key considerations for ensuring its effectiveness in these domains?

## Limitations
- The benchmark's domain-specific nature raises questions about generalizability to other scientific domains
- The automated question generation process, despite human curation, may still produce questions that are overly specific
- The reliance on judge LLM evaluations introduces potential bias and inconsistency

## Confidence
- **High Confidence**: The benchmark successfully demonstrates the challenging nature of evaluation and comparison questions in scientific domains
- **Medium Confidence**: The hybrid human-AI question generation framework shows promise for creating diverse question types
- **Low Confidence**: The reliability of multi-judge LLM evaluations for establishing benchmark validity

## Next Checks
1. Conduct a detailed analysis of judge agreement rates across different question types and RAG systems to quantify evaluation reliability
2. Systematically evaluate the proportion of questions requiring cross-document knowledge to determine if low performance reflects genuine RAG limitations or question design issues
3. Apply the WeQA framework to a different scientific domain to assess the benchmark's portability and identify domain-specific vs. general RAG challenges