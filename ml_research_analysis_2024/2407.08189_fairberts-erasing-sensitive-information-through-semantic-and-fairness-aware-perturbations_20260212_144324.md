---
ver: rpa2
title: 'fairBERTs: Erasing Sensitive Information Through Semantic and Fairness-aware
  Perturbations'
arxiv_id: '2407.08189'
source_url: https://arxiv.org/abs/2407.08189
tags:
- sensitive
- fairness
- fairberts
- adversarial
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the problem of stereotypical bias in pre-trained
  language models (PLMs) by proposing fairBERTs, a framework that uses adversarial
  learning to generate semantic and fairness-aware perturbations. The method erases
  protected sensitive information from model representations by adding perturbations
  generated via a GAN framework, which helps models focus on real task features instead
  of biased attributes.
---

# fairBERTs: Erasing Sensitive Information Through Semantic and Fairness-aware Perturbations

## Quick Facts
- arXiv ID: 2407.08189
- Source URL: https://arxiv.org/abs/2407.08189
- Reference count: 37
- Primary result: GAN-based framework that improves fairness metrics (DIR from 0.8716 to 0.9821 for BERT) while maintaining accuracy within 3% drop

## Executive Summary
fairBERTs addresses stereotypical bias in pre-trained language models by using adversarial learning to generate semantic and fairness-aware perturbations that erase protected sensitive information from model representations. The framework employs a GAN where a generator creates perturbations based on semantic-rich sequence representations, which are added to hidden representations to make them insensitive to protected attributes. Extensive experiments on toxicity detection and sentiment analysis tasks demonstrate significant fairness improvements (e.g., DIR improvements of 0.1105 for BERT and 0.1892 for RoBERTa) while maintaining or slightly improving accuracy, with the added benefit of transferability to other models without retraining.

## Method Summary
fairBERTs uses a GAN-based framework where a generator takes sequence representations from a PLM (BERT/RoBERTa) and produces perturbation masks that are added to classification representations to erase sensitive information. A discriminator tries to predict sensitive attributes from these perturbed representations, creating an adversarial game that forces the representations to be insensitive to protected attributes. The method incorporates counterfactual adversarial training by flipping sensitive labels through token substitution, helping the generator better identify and remove sensitive information. The perturbation-based approach enables transferability to other models without retraining, as the generator creates perturbations based on semantic representations rather than model-specific parameters.

## Key Results
- BERT DIR improved from 0.8716 to 0.9821 and RoBERTa DIR from 0.7989 to 0.9881
- Accuracy drops remained under 3% while achieving significant fairness improvements
- Generated perturbations successfully transferred to other conventionally trained BERT-like models
- Achieved state-of-the-art fairness metrics (DPD, EOD, DIR, CTF) across tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GAN-based generator produces perturbations that erase sensitive information from hidden representations without harming task performance.
- Mechanism: The generator takes sequence representation $h_s$ and outputs perturbation mask $\delta$, which is added to classification representation $h_c$ to create $h_F^c$. The discriminator tries to predict sensitive attributes from $h_F^c$, while the generator minimizes $D$'s success, creating adversarial training that forces $h_F^c$ to be insensitive to protected attributes.
- Core assumption: The sequence representation $h_s$ contains necessary semantic information to generate meaningful perturbations, and adding these perturbations to $h_c$ can effectively remove sensitive correlations without destroying task-relevant features.
- Evidence anchors:
  - [abstract] "erasing the protected sensitive information via semantic and fairness-aware perturbations generated by a generative adversarial network"
  - [section] "The generator ðº aims to make it hard for ð· to predict ð‘§, while also ensuring that the generated perturbations would not destroy the semantic and classification utility of the original representation"

### Mechanism 2
- Claim: Counterfactual adversarial training helps the generator better identify and remove sensitive information by explicitly considering flipped sensitive attributes.
- Mechanism: The method creates counterfactual examples by replacing sensitive tokens (e.g., "male" with "female") while keeping task labels unchanged, providing explicit examples of how sensitive attributes relate to input while maintaining task relevance.
- Core assumption: Counterfactual examples with flipped sensitive attributes but preserved task labels provide clear signals about which information is sensitive versus task-relevant.
- Evidence anchors:
  - [section] "we propose counterfactual adversarial training by borrowing from conventional adversarial training schemes... our method aims to flip the sensitive labels by substituting tokens associated with identity groups"

### Mechanism 3
- Claim: The perturbation-based approach enables transferability to other models without retraining the generator.
- Mechanism: Since the generator creates perturbations based on semantic-rich sequence representations rather than model-specific parameters, these perturbations can be applied to other models with similar architectures.
- Core assumption: The semantic perturbations generated capture general patterns of sensitive information that are transferable across similar model architectures.
- Evidence anchors:
  - [abstract] "We also verify the feasibility of transferring adversarial components in fairBERTs to other conventionally trained BERT-like models for yielding fairness improvements"
  - [section] "the generator ðº create perturbations based on semantic-rich sequence representations rather than model-specific parameters"

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: The core mechanism for generating semantic and fairness-aware perturbations relies on the adversarial relationship between generator and discriminator.
  - Quick check question: What are the two competing objectives in a standard GAN, and how does fairBERTs modify this framework?

- Concept: Adversarial Robustness vs. Fairness Trade-off
  - Why needed here: Understanding that there's a well-established trade-off between adversarial robustness and model performance helps explain why fairBERTs needs careful balancing between fairness and utility.
  - Quick check question: What is the fundamental tension between adversarial robustness and model performance, and how does fairBERTs attempt to navigate this?

- Concept: Counterfactual Examples in Machine Learning
  - Why needed here: The counterfactual adversarial training component requires understanding how to create and use counterfactual examples to improve model fairness.
  - Quick check question: How do counterfactual examples help models learn invariances, and what makes them particularly useful for fairness applications?

## Architecture Onboarding

- Component map: Input â†’ BERT â†’ $h_s$ â†’ Generator â†’ $\delta$ â†’ $h_c + \delta$ â†’ Discriminator + Classifier â†’ Output
- Critical path: Input â†’ BERT â†’ $h_s$ â†’ Generator â†’ $\delta$ â†’ $h_c + \delta$ â†’ Discriminator + Classifier â†’ Output
- Design tradeoffs:
  - Generator complexity vs. training stability: Simple generators are more stable but may produce less effective perturbations
  - Fairness strength vs. utility preservation: Stronger fairness constraints may hurt accuracy more
  - Transferability vs. task specificity: More general perturbations are more transferable but may be less effective for specific tasks
- Failure signatures:
  - Generator produces near-zero perturbations (fairness not improving)
  - Accuracy drops significantly (>5%) without corresponding fairness gains
  - Transferred perturbations cause model degradation on non-sensitive examples
- First 3 experiments:
  1. Train fairBERTs on a small dataset (e.g., Jigsaw Toxicity subset) and verify that DIR improves while accuracy stays within 2% of baseline
  2. Test transferability by applying generated perturbations to a vanilla BERT model and measuring fairness improvement
  3. Visualize t-SNE plots of representations before and after perturbation to confirm sensitive information erasure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are fairBERTs at mitigating bias in real-world applications beyond the two tasks tested (toxicity detection and sentiment analysis)?
- Basis in paper: [inferred] The paper demonstrates fairBERTs' effectiveness on two specific tasks, but does not explore its generalizability to other tasks or domains.
- Why unresolved: The paper focuses on two specific tasks and does not provide evidence for fairBERTs' performance on other types of tasks or real-world applications.
- What evidence would resolve it: Testing fairBERTs on a broader range of tasks and real-world applications, such as question answering, text summarization, or machine translation, would provide evidence for its generalizability.

### Open Question 2
- Question: How do fairBERTs' fairness improvements compare to other state-of-the-art debiasing methods not mentioned in the paper, such as those based on causal inference or fairness-aware data augmentation?
- Basis in paper: [explicit] The paper compares fairBERTs to three commonly used bias mitigation methods (FTU, CLP, and vanilla models) but does not explore other advanced debiasing techniques.
- Why unresolved: The paper only benchmarks fairBERTs against a limited set of methods, leaving open the question of how it compares to more recent or advanced approaches.
- What evidence would resolve it: Conducting a comprehensive comparison of fairBERTs with a wider range of state-of-the-art debiasing methods, including those based on causal inference or fairness-aware data augmentation, would provide insights into its relative effectiveness.

### Open Question 3
- Question: What are the long-term effects of using fairBERTs on model performance and fairness as the model is fine-tuned on additional data or tasks?
- Basis in paper: [inferred] The paper evaluates fairBERTs' performance on specific tasks but does not investigate how its fairness and utility change over time with continued use and fine-tuning.
- Why unresolved: The paper does not provide evidence for the stability and robustness of fairBERTs' fairness improvements in the long term.
- What evidence would resolve it: Conducting longitudinal studies to track fairBERTs' performance and fairness as it is fine-tuned on additional data or tasks over time would provide insights into its long-term effectiveness and stability.

## Limitations

- Architecture details like generator and discriminator network sizes and activation functions are not fully specified
- Hyperparameter values (learning rates, batch sizes, balancing coefficients) are not provided, creating uncertainty about reproduction
- Evaluation limited to two datasets with gender as the only protected attribute, raising questions about generalizability
- Long-term stability of fairness improvements and behavior on out-of-distribution data is not investigated

## Confidence

**High Confidence**: The core GAN-based mechanism for generating semantic perturbations is well-established in the literature. The reported fairness improvements (DIR from 0.8716 to 0.9821 for BERT, 0.7989 to 0.9881 for RoBERTa) and transferability results are specific and verifiable.

**Medium Confidence**: The method's effectiveness in maintaining accuracy while improving fairness is demonstrated, but the exact trade-off depends on unreported hyperparameters. The transferability claims are supported by results but lack detailed analysis of transfer limitations.

**Low Confidence**: Generalizability across different sensitive attributes, languages, and domain-specific contexts is not evaluated. The long-term stability of perturbations and their behavior on out-of-distribution data remains unknown.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the balancing coefficients (Î±, Î², Îµ) and learning rates to determine how sensitive the fairness-accuracy trade-off is to these parameters. Document the parameter space that maintains <3% accuracy drop while achieving DIR >0.95.

2. **Cross-Attribute Transferability**: Apply the trained generator to datasets with different protected attributes (e.g., race, age, disability) and evaluate whether the perturbations generalize beyond gender. Measure fairness improvements using the same metrics (DPD, EOD, DIR, CTF) and compare to gender-specific results.

3. **Long-term Stability Test**: Train the model on a subset of data, freeze the generator, and evaluate fairness metrics over multiple epochs of classifier fine-tuning. Monitor whether sensitive information gradually re-enters the representations and measure the rate of fairness degradation.