---
ver: rpa2
title: Monte Carlo Search Algorithms Discovering Monte Carlo Tree Search Exploration
  Terms
arxiv_id: '2404.09304'
source_url: https://arxiv.org/abs/2404.09304
tags:
- exploration
- search
- monte
- carlo
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to discover new exploration terms
  for Monte Carlo Tree Search (MCTS) algorithms, specifically PUCT and SHUSS, by using
  Monte Carlo Search to generate and evaluate mathematical expressions. The approach
  involves generating random mathematical expressions as potential exploration terms
  and evaluating their effectiveness in guiding the search.
---

# Monte Carlo Search Algorithms Discovering Monte Carlo Tree Search Exploration Terms

## Quick Facts
- arXiv ID: 2404.09304
- Source URL: https://arxiv.org/abs/2404.09304
- Authors: Tristan Cazenave
- Reference count: 40
- Primary result: Discovered exploration terms competitive with PUCT, achieving 55.75% win rate with 32 evaluations in Go

## Executive Summary
This paper presents a method to discover new exploration terms for Monte Carlo Tree Search (MCTS) algorithms by using Monte Carlo Search to generate and evaluate mathematical expressions. The approach generates random mathematical expressions in reverse polish notation and evaluates their effectiveness in guiding the search. The expressions are evaluated using cached MCTS results to enable fast comparison. The method was tested in the game of Go, where a neural network was trained to generate a dataset of cached search results for fast evaluation of the expressions.

## Method Summary
The method involves generating random mathematical expressions as potential exploration terms and evaluating their effectiveness in guiding MCTS. Expressions are represented as trees in reverse polish notation and sampled using either uniform sampling or AMAF (All Moves As First) sampling, which biases selection based on playout scores. The approach uses cached MCTS results from a Go neural network to enable fast evaluation of candidate expressions without repeated neural network inference. The best discovered exploration terms were found to be competitive with standard PUCT, with one expression achieving a 55.75% win rate against PUCT with 32 evaluations.

## Key Results
- Discovered exploration terms competitive with standard PUCT in Go
- AMAF sampling found same accuracy more than 8 times faster than uniform sampling
- Best expression achieved 55.75% win rate against PUCT with 32 evaluations
- Method successfully applied to both PUCT and SHUSS MCTS variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Monte Carlo Search to discover exploration terms bypasses the need for analytical derivation of MCTS exploration formulas.
- Mechanism: The system randomly generates mathematical expressions in reverse polish notation and evaluates them directly by running MCTS with those expressions. Expressions that improve win rates are retained and evolved.
- Core assumption: Empirical performance on game states is a valid proxy for general MCTS improvement; random generation plus selection can find better formulas than human-designed ones.
- Evidence anchors:
  - [abstract] "We randomly generate many exploration terms and keep the ones that work well in practice."
  - [section 1] "We randomly generate many exploration terms and keep the ones that work well in practice. This is a simpler approach, yet it can find exploration terms that work well in practice and that surpass the ones found with a theoretical analysis."

### Mechanism 2
- Claim: AMAF sampling accelerates discovery of effective expressions by biasing the search toward terms that perform better on average.
- Mechanism: Instead of uniform random sampling of expressions, the method uses an All Moves As First (AMAF) prior to guide the Monte Carlo Search toward expressions that score higher across sampled states.
- Core assumption: The AMAF prior is a meaningful heuristic for ranking partial expressions in the search space.
- Evidence anchors:
  - [section 3.2] "It is possible to replace it with non uniform sampling using the AMAF prior."
  - [section 4.4] "Using the AMAF prior improves the results. It finds the same accuracy more than 8 times faster than the uniform sampling algorithm."

### Mechanism 3
- Claim: Caching MCTS results in a dataset enables fast evaluation of candidate expressions without repeated neural network inference.
- Mechanism: For each state, the system runs PUCT or SHUSS multiple times, stores the sequence of evaluations, and later reuses them to simulate the effect of different exploration terms.
- Core assumption: The cached evaluation sequences are representative and reusable for testing new expressions.
- Evidence anchors:
  - [section 4.3] "In order to have a fast evaluation of a given exploration term we built a dataset of states associated to their cached search."

## Foundational Learning

- Concept: Reverse Polish Notation (RPN) expression trees
  - Why needed here: The system represents exploration terms as trees in RPN so they can be generated incrementally and evaluated efficiently.
  - Quick check question: If an expression is `[+, pr, *, *, 2, sc, sc]`, what is the corresponding infix form?
    - Answer: `pr + 2 * sc * sc`

- Concept: Upper Confidence bounds applied to Trees (UCT) and Prior Upper Confidence bounds applied to Trees (PUCT)
  - Why needed here: PUCT is the baseline MCTS algorithm; understanding its exploration term is necessary to design better ones.
  - Quick check question: What is the exploration term used in PUCT?
    - Answer: `Q(s, a) + cp × P(s, a) × sqrt(N(s)) / (1 + N(s, a))`

- Concept: Sequential Halving and Sequential Halving Using Scores (SHUSS)
  - Why needed here: These are the MCTS variants for which new exploration terms are being discovered; understanding their move selection logic is essential.
  - Quick check question: How does SHUSS decide which moves to keep after a round?
    - Answer: It keeps the best half of moves that maximize an exploration expression.

## Architecture Onboarding

- Component map:
  Expression Generator -> Monte Carlo Search -> Go Environment -> Evaluator -> Comparator

- Critical path:
  1. Generate candidate expression → 2. Simulate MCTS with cached data → 3. Compute accuracy → 4. Rank and select best expressions

- Design tradeoffs:
  - Uniform vs AMAF sampling: AMAF is faster but may bias toward suboptimal expressions; uniform is slower but unbiased
  - Cached dataset size vs accuracy: Larger datasets improve evaluation fidelity but increase memory usage and precomputation time
  - Expression length limit: Shorter expressions are faster to evaluate but may lack expressive power; longer expressions are richer but risk overfitting to the dataset

- Failure signatures:
  - Slow convergence: If AMAF sampling does not find better expressions within expected time, sampling temperature or AMAF parameters may be misconfigured
  - Poor generalization: If discovered expressions win on the dataset but lose in real Go matches, the dataset may not be representative or the evaluation may be flawed
  - Overfitting: If discovered expressions are very long or complex, they may overfit the dataset and not generalize

- First 3 experiments:
  1. Run uniform sampling for 512 seconds and measure the best expression accuracy; record number of expressions evaluated
  2. Run AMAF sampling for 512 seconds and compare accuracy improvement and speedup vs uniform
  3. Test the best discovered expression against baseline PUCT in real Go games with 32 evaluations per move; record win rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the discovered exploration term "pr + 2 × sc × sc" perform in other games or combinatorial problems beyond Go?
- Basis in paper: [explicit] The paper mentions applying the method to other problems as future work.
- Why unresolved: The method was only tested on the game of Go in this study.
- What evidence would resolve it: Applying the discovered exploration terms to other games or combinatorial problems and comparing their performance against standard algorithms.

### Open Question 2
- Question: Can the method be extended to discover more complex exploration terms beyond simple mathematical expressions?
- Basis in paper: [inferred] The paper uses simple mathematical expressions and mentions investigating the generation of more general expressions in future work.
- Why unresolved: The current method is limited to generating simple mathematical expressions.
- What evidence would resolve it: Developing and testing methods to generate and evaluate more complex exploration terms, such as those involving neural networks or other advanced structures.

### Open Question 3
- Question: How does the performance of the discovered exploration terms scale with larger search budgets?
- Basis in paper: [explicit] The paper mentions testing the method with small search budgets (32 evaluations) and suggests investigating the performance with different numbers of playouts.
- Why unresolved: The current study only tested the method with small search budgets.
- What evidence would resolve it: Testing the discovered exploration terms with larger search budgets and comparing their performance against standard algorithms.

## Limitations
- Method only tested on Go, limiting generalizability to other games
- Relies on cached search results which may not represent all game states
- Win rate improvements modest (55.75%) without statistical significance testing

## Confidence
- **High confidence**: The core methodology of using Monte Carlo Search to discover exploration terms is technically sound and well-defined.
- **Medium confidence**: The claimed 8x speedup from AMAF sampling is based on accuracy convergence times but lacks statistical significance testing.
- **Low confidence**: The generalizability of discovered expressions to other games or MCTS variants is not tested.

## Next Checks
1. **Statistical validation of performance claims**: Run 100 independent trials of both uniform and AMAF sampling to establish confidence intervals for the accuracy improvements and speedup claims. Report p-values for win rate comparisons against baseline PUCT.
2. **Generalization testing**: Apply the best discovered exploration term to a different domain (such as Hex or Amazons) and evaluate whether it maintains performance improvements. Test with different search depths (16 vs 64 evaluations) to assess robustness.
3. **Ablation study on caching**: Compare the accuracy of fast evaluation using cached search results against ground truth MCTS runs with the discovered expressions. Vary the size of the cached dataset to determine the minimum size needed for reliable evaluation.