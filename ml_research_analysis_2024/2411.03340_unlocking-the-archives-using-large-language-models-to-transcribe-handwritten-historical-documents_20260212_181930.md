---
ver: rpa2
title: 'Unlocking the Archives: Using Large Language Models to Transcribe Handwritten
  Historical Documents'
arxiv_id: '2411.03340'
source_url: https://arxiv.org/abs/2411.03340
tags:
- transcription
- text
- documents
- transkribus
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that large language models (LLMs) can achieve
  state-of-the-art performance in transcribing handwritten historical documents, outperforming
  specialized HTR software while being faster and more cost-effective. The researchers
  developed an open-source tool called Transcription Pearl that uses commercially
  available multimodal LLMs to automatically transcribe and correct batches of handwritten
  documents.
---

# Unlocking the Archives: Using Large Language Models to Transcribe Handwritten Historical Documents

## Quick Facts
- arXiv ID: 2411.03340
- Source URL: https://arxiv.org/abs/2411.03340
- Reference count: 0
- LLMs achieve state-of-the-art performance in transcribing handwritten historical documents

## Executive Summary
This study demonstrates that large language models (LLMs) can outperform specialized handwritten text recognition (HTR) software in transcribing historical documents. The researchers developed an open-source tool called Transcription Pearl that leverages commercially available multimodal LLMs to automatically transcribe and correct handwritten documents. Testing on 18th/19th century English documents showed LLMs achieved Character Error Rates (CER) of 5.7-7% and Word Error Rates (WER) of 8.9-15.9%, representing significant improvements over traditional HTR methods. The approach is faster and more cost-effective, completing tasks 50 times faster at approximately 1/50th the cost of proprietary HTR programs.

## Method Summary
The researchers used Transcription Pearl, an open-source Python software, to transcribe handwritten documents using commercially available multimodal LLMs including OpenAI's gpt-4o-06-08-2024, Anthropic's Claude Sonnet-3.5, and Google's Gemini 1.5 Pro 002. The tool processes batches of handwritten document images in parallel, automatically transcribing them and optionally correcting both their own and Transkribus-generated transcriptions. The study tested 50 pages of 18th/19th century English handwritten documents (letters, memoranda, diary entries, legal documents) written in 33 different hands. Ground truth transcriptions were manually created from Transkribus output. Accuracy was measured using both strict and modified CER/WER metrics that ignored differences in capitalization, punctuation, and corrected historical spelling variations.

## Key Results
- LLMs achieved CERs of 5.7-7% and WERs of 8.9-15.9%, improvements of 14% and 32% over Transkribus
- When correcting both their own and HTR-generated transcriptions, LLMs achieved near-human accuracy with CERs as low as 1.8% and WERs of 3.5%
- The approach completed transcription tasks 50 times faster and at approximately 1/50th the cost of proprietary HTR software

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs outperform traditional HTR models on out-of-box transcription due to their large-scale pre-training on diverse text and ability to generalize handwriting styles.
- Mechanism: LLMs are trained on vast corpora containing diverse linguistic patterns and handwriting representations, enabling them to recognize unfamiliar handwriting styles without explicit fine-tuning.
- Core assumption: The training data included sufficient diversity in handwriting styles to enable generalization to unseen hands.
- Evidence anchors:
  - [abstract] LLMs achieved CERs of 5.7-7% and WERs of 8.9-15.9%, improvements of 14% and 32% over Transkribus
  - [section] "their proficiency in zero-shot and few-shot learning, combined with unimaginably large corpuses of training data suggests they may be better able to generalize across a wider range of handwriting styles"

### Mechanism 2
- Claim: Heterogeneous model combinations (different models for transcription and correction) achieve superior accuracy compared to homogeneous corrections.
- Mechanism: Different models have distinct statistical weightings from their training, so when one model makes an error, another model's different statistical representation can more easily detect and correct it.
- Core assumption: Models trained on different data or with different architectures will produce complementary errors that can be corrected by each other.
- Evidence anchors:
  - [section] "we also found that frontier model LLMs could also be employed to significantly improve error rates by comparing images of the original handwritten pages to the text of LLM generated transcriptions"
  - [section] "Interestingly, this capability was limited to heterogeneous model interactions, that is different transcription and correction models"

### Mechanism 3
- Claim: LLMs achieve near-human accuracy levels when correcting HTR transcriptions because they can detect character-level errors that traditional HTR methods miss.
- Mechanism: Traditional HTR models using CNN/LSTM architectures may produce consistent character misinterpretations (like confusing similar characters), while LLMs with their attention mechanisms can better identify these systematic errors when provided with image context.
- Core assumption: LLM attention mechanisms can effectively cross-reference image features with textual predictions to identify inconsistencies.
- Evidence anchors:
  - [abstract] "when LLMs were then used to correct those transcriptions as well as texts generated by conventional HTR software, they achieved near-human levels of accuracy, that is CERs as low as 1.8% and WERs of 3.5%"
  - [section] "Interestingly, the predictive nature of LLMs can also be harnessed to achieve human-level results when they correct transcriptions generated by traditional HTR systems"

## Foundational Learning

- Concept: Error rate metrics (CER/WER) and their calculation
  - Why needed here: Understanding how accuracy is measured is crucial for evaluating model performance and comparing against baselines
  - Quick check question: How would you calculate CER if a transcription had 3 substitutions, 2 deletions, and 1 insertion in a 100-character document?

- Concept: Zero-shot vs few-shot learning in LLMs
  - Why needed here: The paper relies on zero-shot prompting for baseline results, and understanding this concept is key to grasping why no fine-tuning was needed
  - Quick check question: What's the difference between zero-shot and few-shot prompting, and why might one be preferred over the other in this application?

- Concept: API asynchronous processing and parallel execution
  - Why needed here: The Transcription Pearl tool uses parallel processing to achieve 50x speed improvement, which is a critical architectural decision
  - Quick check question: How does asynchronous API processing with ThreadPoolExecutor improve throughput compared to sequential processing?

## Architecture Onboarding

- Component map:
  Image preprocessing pipeline (resize to 2048px max dimension) -> LLM API interface layer (OpenAI, Anthropic, Google) -> Pandas dataframe storage for image-text pairs -> GUI interface with progress visualization -> Error calculation module using Jiwer library

- Critical path:
  1. Image ingestion → 2. Resize/validation → 3. LLM API call → 4. Response parsing → 5. DataFrame storage → 6. GUI update → 7. Export functionality

- Design tradeoffs:
  - Parallel processing improves speed but increases API cost and complexity
  - Using commercial APIs ensures quality but creates dependency and recurring costs
  - Zero-shot prompting requires no fine-tuning but may underperform specialized models

- Failure signatures:
  - API timeout errors indicate network or rate limiting issues
  - "recitation" errors from Gemini indicate potential training data overlap
  - Inconsistent error rates across runs suggest temperature/top-p hyperparameter issues

- First 3 experiments:
  1. Test baseline transcription accuracy with single image and simple prompt
  2. Verify parallel processing actually improves throughput (measure time for 10 vs 50 pages)
  3. Compare error rates between strict and modified CER/WER calculations on same output

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limits of LLM generalization across different historical periods and languages?
- Basis in paper: [explicit] The paper mentions testing was limited to 18th/19th century English documents and notes that "further testing on documents from other time periods and languages may show different results"
- Why unresolved: The study only examined English language documents from a specific historical period. Handwriting styles, linguistic patterns, and document formats vary significantly across different eras and languages.
- What evidence would resolve it: Systematic testing of LLMs on historical documents from multiple languages (Latin, French, German, etc.) and time periods (medieval, early modern, modern) would establish the boundaries of LLM generalization capabilities.

### Open Question 2
- Question: How can LLM self-correction be improved for transcription tasks?
- Basis in paper: [explicit] The paper notes that LLMs showed no statistically significant ability to self-correct their own outputs, suggesting they may be "inherently biased toward their own outputs"
- Why unresolved: The paper identifies this as a limitation but doesn't explore potential solutions or training approaches that might overcome this bias.
- What evidence would resolve it: Testing various fine-tuning approaches, prompting strategies, or architectural modifications designed to enhance LLM self-correction capabilities would determine whether this limitation can be overcome.

### Open Question 3
- Question: What prompting strategies yield optimal transcription performance for historical documents?
- Basis in paper: [explicit] The paper acknowledges using a simple zero-shot prompting technique and states "We expect that further experimentation could significantly improve our results, most likely through few-shot prompting"
- Why unresolved: The study used a baseline prompt but didn't systematically explore different prompting techniques that could enhance accuracy.
- What evidence would resolve it: Comparative testing of various prompting strategies (chain-of-thought, tree-of-thought, few-shot learning with historical document examples) would identify optimal approaches for this specific task.

## Limitations

- Dataset Representativeness: The study uses 50 pages from a private document collection that won't be publicly released to prevent contamination of future LLM training data, limiting generalizability.
- Ground Truth Quality: Ground truth transcriptions were created by manually correcting Transkribus outputs, potentially introducing biases from the original HTR software's errors.
- API Dependency: The approach relies entirely on commercial LLM APIs, creating cost dependencies and potential limitations due to API rate limits, pricing changes, or model deprecations.

## Confidence

- High Confidence: The comparative advantage of LLMs over traditional HTR software (14% CER improvement, 32% WER improvement) is well-supported by experimental data.
- Medium Confidence: The claim that heterogeneous model combinations outperform homogeneous corrections is supported but requires further testing across different model pairs and document types.
- Low Confidence: The assertion that LLMs achieve "near-human levels of accuracy" is based on comparisons with Transkribus outputs rather than direct human transcription benchmarks.

## Next Checks

1. **Cross-Language Validation**: Test the Transcription Pearl approach on handwritten documents in languages other than English (e.g., French, German, or non-Latin scripts) to assess generalizability beyond the 18th-19th century English corpus.

2. **Long-Term Cost Analysis**: Conduct a comprehensive cost comparison over time that includes API subscription fees, processing time costs, and potential fine-tuning expenses to validate the claimed 1/50th cost advantage over proprietary HTR software.

3. **Human Expert Comparison**: Directly compare LLM transcriptions against ground truth created by human experts transcribing from original documents (not via Transkribus) to independently verify the "near-human" accuracy claims.