---
ver: rpa2
title: What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on Curiosity-Driven
  Questioning
arxiv_id: '2409.17172'
source_url: https://arxiv.org/abs/2409.17172
tags:
- what
- questions
- arxiv
- each
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework, CDQG, to evaluate the
  knowledge acquisition potential of LLMs through curiosity-driven question generation.
  The framework prompts LLMs to generate questions about scientific statements, simulating
  a curious human encountering the information for the first time.
---

# What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on Curiosity-Driven Questioning

## Quick Facts
- arXiv ID: 2409.17172
- Source URL: https://arxiv.org/abs/2409.17172
- Authors: Shashidhar Reddy Javaji; Zining Zhu
- Reference count: 40
- Key outcome: CDQG framework evaluates LLMs' knowledge acquisition through curiosity-driven question generation, showing smaller models can perform as well as larger ones.

## Executive Summary
This paper introduces the Curiosity-Driven Question Generation (CDQG) framework to evaluate how well large language models (LLMs) can acquire knowledge through curiosity-driven questioning. The framework prompts LLMs to generate questions about scientific statements as if encountering them for the first time, then evaluates the questions on relevance, coherence, and diversity using both LLM judges and human validation. The study finds that while larger models excel in coherence and relevance, smaller models like Phi-2 perform comparably or better, challenging the assumption that bigger models are always superior for knowledge acquisition.

## Method Summary
The CDQG framework evaluates LLMs' knowledge acquisition potential by prompting them to generate curiosity-driven questions about scientific statements from a dataset spanning physics, chemistry, mathematics, general knowledge, and erroneous statements. Generated questions are evaluated on three metrics—relevance, coherence, and diversity—using three LLM judges (GPT-3.5 Turbo, Mistral 8x7b, Gemini) with scores synthesized by a meta-reviewer (Claude 2). The framework's validity is confirmed through noise-injection ablation studies and human evaluation, with the latter showing reasonable agreement with LLM assessments.

## Key Results
- Larger models like GPT-4 and Mistral 8x7b excel in coherence and relevance scores
- Smaller Phi-2 model performs comparably or better than larger models in question generation quality
- Human evaluation validates LLM judge assessments with reasonable agreement
- Noise-injection ablation studies confirm the robustness of the evaluation framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The CDQG framework effectively evaluates LLMs' knowledge acquisition potential through curiosity-driven question generation.
- Mechanism: By prompting models to generate questions about scientific statements as if encountering them for the first time, the framework simulates human curiosity and learning. The evaluation metrics (relevance, coherence, diversity) assess different dimensions of questioning quality.
- Core assumption: LLMs can generate meaningful questions that reflect genuine curiosity and learning potential, and these questions can be meaningfully evaluated using automated metrics.
- Evidence anchors:
  - [abstract] "We score the qualities of the questions generated by LLMs along multiple dimensions. These scores are validated by rigorous controlled ablation studies and human evaluations."
  - [section] "The questions are then scored along three metrics — relevance, coherence, and diversity — scores with roots in the literature of psychology (Zhao et al., 2023)."
- Break condition: If LLMs cannot generate questions that reflect genuine curiosity, or if the evaluation metrics fail to capture meaningful aspects of questioning quality.

### Mechanism 2
- Claim: Model size does not solely determine knowledge acquisition potential.
- Mechanism: The framework reveals that smaller models like Phi-2 can perform comparably or better than larger models like GPT-4 in generating relevant and coherent questions, challenging the assumption that bigger models are always better.
- Core assumption: The quality of question generation depends on factors beyond model size, such as training data quality and architectural design.
- Evidence anchors:
  - [abstract] "the smaller Phi-2 model is equally or more effective. This indicates that size does not solely determine a model's knowledge acquisition potential."
  - [section] "While the larger models score high in coherence and relevance, the smaller Phi-2 model scores comparably well (or even better), indicating that the size might not be the only factor for the knowledge acquisition potential."
- Break condition: If model size proves to be the dominant factor in all tested scenarios, or if smaller models consistently underperform in all metrics.

### Mechanism 3
- Claim: The evaluation framework is robust and valid through noise-injection ablation and human validation.
- Mechanism: By introducing controlled noise into generated questions and observing score degradation, the framework validates that LLM judges can distinguish between high-quality and compromised content. Human evaluation provides additional validation.
- Core assumption: LLM judges can reliably differentiate between signal and noise, and human judgments align with LLM evaluations.
- Evidence anchors:
  - [abstract] "These scores are validated by rigorous controlled ablation studies and human evaluations."
  - [section] "We validate the CDQG evaluation through an ablation study that incrementally adds noise, as well as a human validation."
- Break condition: If noise-injection does not consistently degrade scores, or if human evaluations show poor alignment with LLM judgments.

## Foundational Learning

- Concept: Question generation evaluation metrics
  - Why needed here: The framework relies on relevance, coherence, and diversity metrics to evaluate generated questions. Understanding these metrics is crucial for interpreting results.
  - Quick check question: What does each metric (relevance, coherence, diversity) measure in the context of question generation?

- Concept: Noise-injection ablation study methodology
  - Why needed here: This validation technique is used to ensure the robustness of the evaluation framework. Understanding it is essential for assessing the validity of the results.
  - Quick check question: How does introducing noise into generated questions help validate the evaluation framework?

- Concept: Human evaluation methodology and agreement metrics
  - Why needed here: Human validation is used to corroborate LLM-based evaluations. Understanding Cohen's Kappa and other agreement metrics is crucial for interpreting the validation results.
  - Quick check question: What does a Cohen's Kappa value of 0.7 indicate about the agreement between human and LLM evaluations?

## Architecture Onboarding

- Component map:
  CDQG dataset (1,988 statements) -> Question generation models (Llama 7b/13b/70b, Mistral 8x7b, Phi-2, Gemini, GPT-3.5, GPT-4) -> LLM judges (GPT-3.5 Turbo, Mistral 8x7b, Gemini) -> Meta-reviewer (Claude 2) -> Human annotators for validation

- Critical path:
  1. Load statement from CDQG dataset
  2. Generate questions using model
  3. Score questions using three LLM judges
  4. Synthesize scores using meta-reviewer
  5. Validate results through noise-injection and human evaluation

- Design tradeoffs:
  - Using multiple LLM judges vs. single judge: Multiple judges reduce bias but increase computational cost
  - Three metrics vs. more/less: Balance comprehensiveness with practicality
  - Noise-injection level: Enough to test robustness without making evaluation impossible

- Failure signatures:
  - Low inter-judge agreement (<0.5) suggests unreliable scoring
  - No score degradation with noise injection indicates evaluation framework issues
  - Poor human-LLM agreement suggests misalignment in evaluation criteria

- First 3 experiments:
  1. Run question generation on a single basic physics statement across all models and compare outputs
  2. Test scoring consistency by having the same LLM judge score the same questions twice
  3. Perform noise-injection on a small set of questions and verify score degradation patterns

## Open Questions the Paper Calls Out
None

## Limitations
- The CDQG dataset of 1,988 statements may not fully represent the breadth of scientific knowledge or diverse questioning patterns
- Evaluation relies heavily on LLM judges, raising concerns about potential circularity if judges share training data with evaluated models
- The study focuses on English-language scientific statements, limiting applicability to other languages or cultural contexts

## Confidence
- High Confidence: The CDQG framework's evaluation method is well-supported by validation through noise-injection ablation studies and human evaluations
- Medium Confidence: The finding that model size doesn't solely determine knowledge acquisition potential is supported by comparative results but based on a limited set of models

## Next Checks
1. Test the CDQG framework with statements and questions in multiple languages to assess cross-linguistic applicability
2. Apply the framework to non-scientific domains (e.g., literature, history, art) to evaluate effectiveness across diverse knowledge areas
3. Conduct extensive user studies where human experts generate questions for the same statements and compare their questioning patterns with those of LLMs