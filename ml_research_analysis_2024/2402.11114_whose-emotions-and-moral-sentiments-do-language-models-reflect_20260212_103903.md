---
ver: rpa2
title: Whose Emotions and Moral Sentiments Do Language Models Reflect?
arxiv_id: '2402.11114'
source_url: https://arxiv.org/abs/2402.11114
tags:
- alignment
- tweets
- affective
- steered
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces and measures "affective alignment," the degree
  to which language models' emotional and moral tone reflects that of different social
  groups. The authors compare the affect of 36 language models' generated responses
  to Twitter messages from liberal and conservative users on COVID-19 and abortion
  topics.
---

# Whose Emotions and Moral Sentiments Do Language Models Reflect?

## Quick Facts
- arXiv ID: 2402.11114
- Source URL: https://arxiv.org/abs/2402.11114
- Authors: Zihao He; Siyi Guo; Ashwin Rao; Kristina Lerman
- Reference count: 32
- One-line primary result: Language models exhibit significant affective misalignment with both liberal and conservative ideological groups, with misalignment larger than the partisan divide in the U.S.

## Executive Summary
This paper introduces "affective alignment" - the degree to which language models' emotional and moral tone reflects that of different social groups. The authors measure this by comparing the affect of 36 language models' generated responses to Twitter messages from liberal and conservative users on COVID-19 and abortion topics. They find that even after steering models toward specific ideological perspectives, misalignment and liberal tendencies persist, suggesting a systemic bias within language models that cannot be easily mitigated by prompting techniques.

## Method Summary
The study collects Twitter datasets on COVID-19 and Roe v. Wade, identifies fine-grained topics, and estimates user ideological leanings based on shared news domains. It generates 2,000 responses per topic from each of 36 language models using default and steered prompts. Affect is measured using BERT-based classifiers (SpanEmo for emotions, DAMF for moral sentiments) on both LM-generated and human-authored tweets. Jensen-Shannon Distance quantifies the divergence between affect distributions, with affective alignment defined as 1 minus JSD.

## Key Results
- Language models show significant affective misalignment with both liberal and conservative groups, with misalignment larger than the partisan divide
- Even after steering toward specific ideological perspectives, models retain liberal tendencies and misalignment
- On COVID-19 topics, all language models exhibit liberal tendencies regardless of steering
- Models generate more confident (less diverse) affect distributions compared to human discourse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models exhibit significant misalignment in affect with ideological groups because their pretraining data disproportionately represents liberal perspectives on certain topics like COVID-19.
- Mechanism: The pretraining corpus contains more emotional and moral content from liberal sources, leading models to internalize and reproduce these patterns more strongly than conservative ones.
- Core assumption: The emotional and moral content distribution in pretraining data correlates with the political ideology of its sources.
- Evidence anchors:
  - [abstract] "even after steering the LMs towards specific ideological perspectives, the misalignment and liberal tendencies persist, suggesting a systemic bias within language models"
  - [section] "consistent with prior findings (Santurkar et al., 2023; Perez et al., 2022; Hartmann et al., 2023), all LMs exhibit liberal tendencies on topics related to COVID-19"
- Break condition: If pretraining data is balanced across political ideologies or if the model architecture explicitly mitigates this bias during training

### Mechanism 2
- Claim: Instruction tuning and RLHF make models more steerable but do not fundamentally change their affective representations, which remain biased toward liberal perspectives.
- Mechanism: Fine-tuning layers learn to respond to steering prompts but do not reweight the underlying affective distributions acquired during pretraining.
- Core assumption: Steering prompts can override output generation but cannot rewrite the learned affective representations from pretraining.
- Evidence anchors:
  - [abstract] "even after steering the LMs towards specific ideological perspectives, the misalignment and liberal tendencies of the model persist"
  - [section] "the models remain misaligned, and liberal tendencies of LMs cannot be mitigated by steering"
- Break condition: If fine-tuning methods could explicitly reweight affective representations or if steering prompts could access and modify underlying representations

### Mechanism 3
- Claim: Language models generate more confident (less diverse) affect distributions compared to human discourse, particularly when unsteered, because they learn to predict the most probable affect rather than representing the full distribution.
- Mechanism: The maximum likelihood training objective encourages models to output the single most likely affect rather than sampling from the full human distribution.
- Core assumption: The training objective for language models prioritizes predicting the most probable next token over capturing distributional diversity.
- Evidence anchors:
  - [section] "Compared to humans, LMs show a more focused distribution across different types of emotions or moral foundations"
  - [section] "Such high confidence is observed in both the default models and liberal steered models"
- Break condition: If training objectives or sampling methods explicitly encouraged diversity or if evaluation metrics captured distributional similarity

## Foundational Learning

- Concept: Jensen-Shannon Distance as a measure of distribution similarity
  - Why needed here: The paper uses JSD to quantify affective alignment between model-generated and human-authored affect distributions
  - Quick check question: What is the range of JSD values and what does a value of 0 indicate about two distributions?

- Concept: Political ideology estimation from media sharing patterns
  - Why needed here: The paper estimates user ideology based on the political bias of news outlets they share
  - Quick check question: How does the Media Bias/Fact Check database categorize news outlets and how is this used to estimate individual user ideology?

- Concept: Moral Foundations Theory and its five dimensions
  - Why needed here: The paper measures moral sentiment using the five moral foundations (care/harm, fairness/cheating, loyalty/betrayal, authority/subversion, purity/degradation)
  - Quick check question: What are the five moral foundations and how do they differ in their association with liberal versus conservative moral reasoning?

## Architecture Onboarding

- Component map: Twitter data collection and preprocessing -> Topic detection and fine-grained subtopic clustering -> Language model generation with different prompting strategies -> Affect classification using BERT-based models -> Alignment measurement using JSD
- Critical path: Data collection → Topic detection → LM generation → Affect classification → Alignment calculation
- Design tradeoffs: Using smaller BERT-based classifiers for affect detection versus larger models for generation creates a potential mismatch in how affect is understood; using JSD versus other distance measures affects sensitivity to distribution differences
- Failure signatures: Low alignment scores could indicate (1) classifier bias, (2) topic detection errors, (3) generation quality issues, (4) misalignment between classifier and model understanding of affect, or (5) genuine affective misalignment
- First 3 experiments:
  1. Compare JSD results using different affect classifiers (BERT vs larger models) on the same generated data to isolate classifier effects
  2. Test whether adding explicit diversity sampling during generation (top_p, temperature adjustments) improves distributional alignment without affecting steering effectiveness
  3. Validate topic detection by manually annotating a sample of tweets and comparing detected topics to ground truth labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can language models be developed to achieve better affective alignment with both liberal and conservative ideological groups without exhibiting liberal tendencies?
- Basis in paper: [explicit] The paper demonstrates that current language models show significant misalignment with both ideological groups and exhibit liberal tendencies, especially on COVID-19 topics. Even after steering the models toward specific ideological perspectives, the misalignment and liberal tendencies persist.
- Why unresolved: The paper indicates that the liberal tendencies are deeply entrenched in the models and cannot be mitigated simply through steering, suggesting that the underlying data or training processes may need to be adjusted.
- What evidence would resolve it: Evidence would include successful development of language models that achieve balanced affective alignment with both ideological groups, validated through empirical testing on diverse datasets and topics.

### Open Question 2
- Question: What specific aspects of the pretraining data contribute to the liberal bias observed in language models?
- Basis in paper: [inferred] The paper suggests that a significant portion of the pretraining data is derived from discussions in forums where liberal perspectives dominate, particularly on COVID-19, leading to models absorbing more emotional and moral tone of liberal narratives.
- Why unresolved: The paper does not provide a detailed analysis of the pretraining data sources or their content, which would be necessary to identify specific contributors to the bias.
- What evidence would resolve it: Evidence would include a comprehensive analysis of the pretraining data, identifying the proportion and nature of liberal versus conservative content, and correlating these findings with the models' affective outputs.

### Open Question 3
- Question: How does the effectiveness of affective alignment differ across various demographic groups beyond political ideology?
- Basis in paper: [explicit] The paper mentions that while it focuses on political identities, the default affect distribution of the models might be more closely aligned with other demographic groups, suggesting the need for further exploration.
- Why unresolved: The study is limited to political ideology, and the paper does not explore how language models align with other demographic factors such as age, gender, or cultural background.
- What evidence would resolve it: Evidence would include empirical studies measuring the affective alignment of language models with various demographic groups, using diverse datasets and topics to assess the models' representativeness and alignment across different segments of society.

## Limitations

- The study focuses on only two specific topics (COVID-19 and abortion) which may not generalize to all political discourse domains
- The exact political composition of pretraining data remains unknown and is only inferred indirectly
- Classifier performance on LM-generated text versus human-authored text may differ due to domain shift effects

## Confidence

- Language models exhibit systematic affective misalignment with ideological groups: **High**
- This misalignment exceeds the partisan divide: **High**
- Attribution to pretraining data composition: **Medium**
- Steering prompts cannot mitigate underlying bias: **High**
- Mechanistic explanation about fine-tuning not reweighting affective representations: **Medium**
- Observation about model confidence and distributional focus: **Medium**

## Next Checks

1. Conduct classifier validation by testing SpanEmo and DAMF performance on a held-out set of LM-generated tweets labeled by human annotators to assess potential domain shift effects.
2. Perform ablation studies varying pretraining data composition (if accessible) or using models trained on ideologically balanced corpora to directly test the pretraining bias hypothesis.
3. Extend the analysis to additional political topics and non-US political contexts to evaluate the generalizability of observed misalignment patterns beyond COVID-19 and abortion discourse.