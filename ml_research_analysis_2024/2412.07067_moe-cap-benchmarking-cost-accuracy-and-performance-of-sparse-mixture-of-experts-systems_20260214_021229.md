---
ver: rpa2
title: 'MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts
  Systems'
arxiv_id: '2412.07067'
source_url: https://arxiv.org/abs/2412.07067
tags:
- systems
- cost
- performance
- memory
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MoE-CAP, a benchmarking framework for sparse\
  \ Mixture-of-Experts (MoE) systems that addresses trade-offs between Cost, Accuracy,\
  \ and Performance (CAP). The authors propose sparsity-aware metrics\u2014Sparse\
  \ Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS Utilization (S-MFU)\u2014\
  to accurately capture resource utilization in MoE systems, overcoming limitations\
  \ of existing metrics that overestimate costs by ignoring routing and selective\
  \ expert activation."
---

# MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems

## Quick Facts
- arXiv ID: 2412.07067
- Source URL: https://arxiv.org/abs/2412.07067
- Authors: Yinsicheng Jiang, Yao Fu, Yeqi Huang, Ping Nie, Zhan Lu, Leyang Xue, Congjie He, Man-Kit Sit, Jilong Xue, Li Dong, Ziming Miao, Dayou Du, Tairan Xu, Kai Zou, Edoardo Ponti, Luo Mai
- Reference count: 40
- One-line primary result: Introduces MoE-CAP, a benchmarking framework for sparse Mixture-of-Experts systems that addresses trade-offs between Cost, Accuracy, and Performance using sparsity-aware metrics.

## Executive Summary
This paper introduces MoE-CAP, a comprehensive benchmarking framework designed to evaluate the trade-offs between Cost, Accuracy, and Performance (CAP) in sparse Mixture-of-Experts (MoE) systems. The framework addresses critical limitations in existing benchmarks by introducing sparsity-aware metrics (S-MBU and S-MFU) that accurately capture resource utilization by accounting for routing and selective expert activation patterns. Using CAP radar diagrams, the authors demonstrate that MoE systems typically optimize two of the three CAP dimensions at the expense of the third, revealing fundamental trade-offs in current hardware and architectural constraints. Their analysis shows that MoE models can run on a broader range of devices than previously thought, particularly in single-user inference scenarios with small batch sizes where high sparsity reduces bandwidth requirements.

## Method Summary
MoE-CAP provides an automated evaluation pipeline that benchmarks MoE systems across diverse hardware configurations and deployment scenarios. The framework profiles expert activation patterns during forward passes to calculate sparsity-aware metrics S-MBU (Sparse Memory Bandwidth Utilization) and S-MFU (Sparse Model FLOPS Utilization), which overcome the overestimation issues of traditional MBU and MFU metrics. The evaluation supports various MoE models and systems, generating CAP radar diagrams to visualize trade-offs between the three dimensions. The framework accounts for hardware heterogeneity including CPUs, DRAM, SSDs, and interconnects, and includes specialized evaluations for single-node and multi-node setups. The pipeline automates the entire benchmarking process from model selection and configuration to metric calculation and trade-off analysis.

## Key Results
- Existing benchmarks overestimate memory and compute costs for MoE systems by ignoring routing and selective expert activation
- MoE systems face inevitable trade-offs between Cost, Accuracy, and Performance, typically optimizing only two dimensions at the expense of the third
- MoE models can run on a broader range of devices than previously thought, particularly in single-user inference scenarios with small batch sizes
- Sparsity-aware metrics (S-MBU, S-MFU) accurately capture actual resource utilization in MoE systems, showing good correlation with profiled hardware measurements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Existing benchmarks overestimate memory and compute costs for MoE systems by ignoring routing and selective expert activation.
- Mechanism: The paper introduces sparsity-aware metrics (S-MBU and S-MFU) that calculate utilization based on actually activated parameters and FLOPs per token rather than full model size.
- Core assumption: The router selects only a subset of experts per token, making full-model assumptions invalid for sparse architectures.
- Evidence anchors:
  - [abstract]: "These metrics like Memory Bandwidth Utilization (MBU) and Model FLOPS Utilization (MFU) fail to account for the sparse activation patterns of experts in MoE systems. This oversight leads to overestimated memory and compute costs."
  - [section]: "However, as illustrated in Figure 2, these metrics significantly overestimate resource utilization by assuming all experts are active, especially when batch size >1."
  - [corpus]: No direct corpus evidence for this specific mechanism; paper provides internal validation through profiling.

### Mechanism 2
- Claim: MoE systems face inevitable trade-offs between Cost, Accuracy, and Performance (CAP), optimizing only two at the expense of the third.
- Mechanism: The paper categorizes MoE systems into three types based on which two dimensions they optimize: PA (Performance-Accuracy), PC (Performance-Cost), and CA (Cost-Accuracy).
- Core assumption: Hardware constraints and architectural choices create fundamental limitations that prevent simultaneous optimization of all three dimensions.
- Evidence anchors:
  - [abstract]: "Our analysis reveals that achieving an optimal balance across CAP is difficult with current hardware; MoE systems typically optimize two of the three dimensions at the expense of the thirdâ€”a dynamic we term the MoE-CAP trade-off."
  - [section]: "While most MoE systems claim strengths in performance, cost, and accuracy, they typically optimize only two at the expense of the third."
  - [corpus]: Weak corpus evidence; paper introduces this as a novel analytical framework without citing prior work on this specific trade-off characterization.

### Mechanism 3
- Claim: Model sparsity varies significantly with batch size, enabling deployment on broader range of hardware than previously thought.
- Mechanism: At small batch sizes (typical of personal inference), MoE models exhibit high sparsity with low bandwidth requirements, making them feasible on consumer hardware with offloading.
- Core assumption: Deployment scenarios determine batch size, which in turn determines model sparsity and resource requirements.
- Evidence anchors:
  - [abstract]: "Our analysis reveals that MoE models can run on a broader range of devices than previously thought, particularly in single-user inference scenarios with small batch sizes where high sparsity reduces bandwidth requirements."
  - [section]: "In practice, the sparsity of MoE systems is closely influenced by the batch size. Batch size, in turn, is largely determined by the deployment context."
  - [corpus]: No direct corpus evidence for this specific mechanism; paper provides internal profiling data.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE works is fundamental to grasping why traditional benchmarks fail and why sparsity-aware metrics are necessary.
  - Quick check question: In an MoE layer with 128 experts where each token is routed to top-2 experts, what percentage of experts are activated per token on average?

- Concept: Memory bandwidth and compute utilization metrics
  - Why needed here: The paper critiques existing MBU and MFU metrics and introduces new sparsity-aware versions, requiring understanding of what these metrics measure.
  - Quick check question: How does the standard MBU calculation differ from S-MBU in terms of what parameter set it considers?

- Concept: Hardware heterogeneity in modern systems
  - Why needed here: The paper emphasizes accounting for CPUs, DRAM, SSDs, and various interconnects, not just GPUs, in cost modeling.
  - Quick check question: What are the three hierarchical tiers of memory considered in the paper's cost model?

## Architecture Onboarding

- Component map: Automated evaluation pipeline -> Expert activation profiler -> S-MBU calculator -> S-MFU calculator -> CAP radar diagram generator -> Multi-system and multi-model support framework

- Critical path:
  1. Model and dataset selection
  2. System deployment configuration
  3. Forward pass execution with profiling hooks
  4. Expert activation pattern collection
  5. Metric calculation (S-MBU, S-MFU)
  6. CAP trade-off analysis and radar diagram generation

- Design tradeoffs:
  - Comprehensive coverage vs. benchmark complexity
  - Runtime profiling overhead vs. metric accuracy
  - Generic framework vs. specialized optimizations
  - Real-world deployment scenarios vs. controlled benchmarking

- Failure signatures:
  - Metrics not reflecting actual hardware utilization
  - Profiling overhead dominating inference time
  - Inconsistent results across different systems
  - Missing support for emerging MoE architectures

- First 3 experiments:
  1. Validate S-MBU accuracy by comparing against profiled memory access on Mixtral-8x7B with varying batch sizes
  2. Test CAP radar diagram generation by benchmarking SGLang, K-Transformers, and MoE-Infinity on the same hardware
  3. Measure profiling overhead impact by comparing inference latency with and without activation profiling enabled

## Open Questions the Paper Calls Out

- **Long-context workloads**: How sparsity-aware metrics behave for long-context scenarios and how they affect CAP trade-offs at different context lengths.

- **Training scenarios**: Extending CAP analysis to training workloads, including metrics for training-time cost-accuracy-performance trade-offs.

- **Deployment scenario coverage**: Exploring how different deployment contexts (cloud vs. edge vs. mobile) affect CAP trade-offs and metric accuracy.

## Limitations

- The framework's accuracy depends on reliable expert activation profiling, which may introduce overhead and complexity in real-world deployments.

- The CAP trade-off framework is based on current hardware constraints and may not generalize to future architectures with different resource characteristics.

- Limited empirical validation across diverse deployment scenarios, particularly for the claim about expanded hardware compatibility for MoE models.

## Confidence

- **High Confidence**: The fundamental observation that existing benchmarks overestimate MoE resource costs by ignoring sparsity. The mechanism is well-established through the routing architecture of MoE systems, and the solution (sparsity-aware metrics) directly addresses the identified problem.

- **Medium Confidence**: The MoE-CAP trade-off framework and the three-way categorization of systems. While the trade-off logic is sound and consistent with hardware constraints, the empirical validation across diverse systems is limited, and the framework's generalizability to future MoE architectures remains unproven.

- **Low Confidence**: The claim about expanded hardware deployment ranges for MoE systems. This depends on specific deployment patterns and batch size assumptions that may not hold across all use cases, and the paper provides limited empirical evidence beyond theoretical analysis.

## Next Checks

1. Cross-system validation: Benchmark the S-MBU and S-MFU metrics across at least 10 diverse MoE systems (including both established and emerging architectures) to verify the accuracy of sparsity-aware resource utilization estimates compared to actual hardware measurements.

2. Deployment scenario testing: Conduct real-world deployment tests of MoE models on consumer hardware (laptops, mobile devices) with varying batch sizes to empirically verify the claim that high sparsity at small batch sizes enables broader hardware compatibility.

3. Temporal robustness check: Evaluate whether the MoE-CAP trade-off framework remains valid as hardware capabilities evolve (e.g., next-generation GPUs with improved memory bandwidth and compute efficiency) to test the framework's long-term applicability.