---
ver: rpa2
title: 'SETN: Stock Embedding Enhanced with Textual and Network Information'
arxiv_id: '2408.02899'
source_url: https://arxiv.org/abs/2408.02899
tags:
- stock
- information
- bert
- network
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SETN, a method for learning stock embeddings
  by jointly leveraging textual and network information. The method uses a domain-adapted
  transformer model to encode textual information from company annual reports and
  a graph neural network to capture cross-stock relationships from a causal chain
  network.
---

# SETN: Stock Embedding Enhanced with Textual and Network Information

## Quick Facts
- arXiv ID: 2408.02899
- Source URL: https://arxiv.org/abs/2408.02899
- Reference count: 36
- Primary result: SETN achieves MAP@5 scores of 0.682 for TOPIX17 and 0.606 for TOPIX33 on related company information extraction tasks

## Executive Summary
SETN is a novel method for learning stock embeddings by jointly leveraging textual information from company annual reports and network information from causal chain relationships. The approach combines a domain-adapted transformer model with a graph neural network using a residual connection, trained to predict company sectors and industries. Experiments on Japanese stock data demonstrate that SETN outperforms baseline models on related company information extraction tasks and performs well on thematic fund creation tasks, indicating its ability to capture both sector/industry and broader thematic information.

## Method Summary
SETN jointly trains a domain-adaptive pre-trained transformer (BERT/RoBERTa) for encoding textual information from company annual reports and a graph neural network (GCN/GAT) for capturing cross-stock relationships from a causal chain network. The model uses a residual connection to combine the text and graph embeddings, which are then used to predict company sectors and industries. The approach is trained on Japanese stock data, utilizing the CoARiJ corpus for textual information and a causal chain network with 12,755 edges and 2,437 nodes for network information.

## Key Results
- SETN achieves MAP@5 scores of 0.682 for TOPIX17 and 0.606 for TOPIX33 on related company information extraction tasks
- Ablation studies demonstrate the benefits of using directed graphs and joint training
- SETN performs well on thematic fund creation tasks, indicating its ability to capture both sector/industry and broader thematic information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jointly training transformer and GNN models outperforms separately trained models for stock embedding tasks.
- Mechanism: By backpropagating through both text and graph embedding components simultaneously, the model learns to balance textual and network information more effectively, leading to better representations.
- Core assumption: Textual and network information are complementary and their joint learning provides richer representations than individual learning.
- Evidence anchors: [abstract] "jointly trains a transformer-based and GNN model"; [section] "jointly learning both textual and network information improves the performance of a model"
- Break condition: If the complementary relationship between textual and network information doesn't exist or if one modality dominates the other, the joint training advantage may disappear.

### Mechanism 2
- Claim: Directed graphs capture cross-stock relationships more effectively than undirected graphs for stock embedding.
- Mechanism: Economic relationships between stocks are inherently directional (e.g., supplier-customer, transaction relationships), and directed graphs can model these asymmetric relationships more accurately.
- Core assumption: The causal chain network data used represents directional economic relationships that are important for stock embeddings.
- Evidence anchors: [abstract] "causal chain network" and "cross-stock relationships"; [section] "we compared the performance of the models with a directed graph and evaluated the model's performance with an undirected graph"
- Break condition: If the economic relationships in the dataset are actually symmetric or if the directionality doesn't add meaningful information, the directed graph advantage may not materialize.

### Mechanism 3
- Claim: Domain-adaptive pretraining on financial corpora improves transformer performance for stock embedding tasks.
- Mechanism: By continuing pretraining on domain-specific financial text data, the transformer learns domain-specific vocabulary and relationships that are crucial for understanding financial documents.
- Core assumption: Financial documents contain specialized terminology and context that general-purpose pretraining doesn't capture adequately.
- Evidence anchors: [abstract] "domain-adaptive pre-trained transformer-based model"; [section] "we implemented domain-adaptive pre-training using financial corpora"
- Break condition: If the financial domain doesn't contain significantly different linguistic patterns from the original pretraining data, or if the financial corpus is too small to provide meaningful adaptation.

## Foundational Learning

- Concept: Graph Neural Networks (GNN)
  - Why needed here: To capture complex relationships between stocks in the causal chain network, where each stock is a node and relationships are edges
  - Quick check question: Can you explain the difference between GCN and GAT and when you might choose one over the other?

- Concept: Transformer-based language models
  - Why needed here: To encode textual information from company annual reports into meaningful vector representations that capture company-specific information
  - Quick check question: What are the advantages of using RoBERTa over BERT for this financial domain application?

- Concept: Residual connections in neural networks
  - Why needed here: To balance the contributions of textual and network information by adding the transformer output to the GNN output
  - Quick check question: How does a residual connection help prevent gradient vanishing in deep networks?

## Architecture Onboarding

- Component map: Input → Text Encoder (BERT/RoBERTa) → Graph Encoder (GCN/GAT) → Residual Connection → Classifier → Output
- Critical path: Text Encoder → Graph Encoder → Residual Connection → Classifier (sector/industry classification)
- Design tradeoffs: Choosing between GCN vs GAT affects model capacity and computation; choosing between BERT vs RoBERTa affects domain adaptation effectiveness
- Failure signatures: Poor MAP@5 scores indicate failure to capture related company information; low thematic fund creation scores indicate failure to capture broader thematic relationships
- First 3 experiments:
  1. Compare BERT vs RoBERTa with GCN + RC architecture on MAP@5 metric
  2. Compare GCN vs GAT with BERT architecture on MAP@5 metric
  3. Compare directed vs undirected graph construction with BERT+GCN+RC architecture on MAP@5 metric

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relative importance of textual information versus network information in the SETN model's performance across different stock market conditions (bull vs bear markets, high vs low volatility periods)?
- Basis in paper: [inferred] The paper demonstrates that both textual and network information contribute to the model's performance on related company information extraction tasks, but does not analyze their relative importance across different market conditions or explicitly separate their contributions.
- Why unresolved: The experiments were conducted on a single dataset and time period without varying market conditions. The ablation studies compare models with and without each information source but don't quantify their relative contributions under different market regimes.
- What evidence would resolve it: Experiments comparing model performance across different market conditions (bull/bear markets, high/low volatility periods) with and without each information source, along with statistical analysis of their relative contributions to performance metrics.

### Open Question 2
- Question: How do SETN embeddings generalize to stocks from different countries or regions beyond the Japanese market where they were developed and tested?
- Basis in paper: [explicit] The paper states "We conducted our experiments on the Japanese stock market" and does not mention any cross-market validation or testing on stocks from other countries or regions.
- Why unresolved: The study is limited to Japanese stocks and doesn't include any validation on international markets, making it unclear whether the model would perform similarly with stocks from different regulatory environments, reporting standards, or economic contexts.
- What evidence would resolve it: Testing the SETN model on stock datasets from other countries (US, Europe, emerging markets) and comparing performance metrics across regions, along with analysis of any necessary domain adaptation requirements.

### Open Question 3
- Question: What is the optimal graph construction method for SETN when using different types of financial relationships (e.g., supplier-customer, shareholding, news co-occurrence) as input data?
- Basis in paper: [inferred] The paper uses causal chains as network data and mentions that "several studies have suggested the effectiveness of representing cross-company economic relationships using causal chains," but doesn't compare different graph construction methods or relationship types.
- Why unresolved: The study only uses one type of network data (causal chains) and doesn't explore how different financial relationship types or graph construction methods might affect the model's performance or embeddings quality.
- What evidence would resolve it: Systematic comparison of SETN performance using different types of financial relationships (supplier-customer, shareholding, news co-occurrence, etc.) as network input, with analysis of how each affects embedding quality and downstream task performance.

## Limitations
- The paper lacks direct experimental comparisons for key claims about joint training, domain adaptation, and directed graph advantages
- The study is limited to Japanese stock market data without cross-market validation
- The causal chain construction method from textual data is not fully specified

## Confidence
- **High Confidence**: The overall approach of combining textual and network information for stock embeddings is well-supported by the MAP@5 results and thematic fund creation performance
- **Medium Confidence**: The effectiveness of residual connections and the general benefit of using GNNs for cross-stock relationships are supported by ablation studies but lack direct comparative evidence
- **Low Confidence**: The specific claims about domain-adaptive pretraining superiority and the advantage of directed over undirected graphs cannot be fully validated without direct experimental comparisons

## Next Checks
1. Conduct controlled experiments comparing joint training of transformer and GNN versus separately trained models on the same dataset, measuring MAP@5 and thematic fund creation performance.

2. Perform ablation studies specifically testing directed versus undirected graph representations on the causal chain network, isolating the effect of edge directionality on embedding quality.

3. Compare domain-adapted versus non-adapted transformer models on financial document understanding tasks, using metrics like masked language modeling accuracy and downstream classification performance on financial corpora.