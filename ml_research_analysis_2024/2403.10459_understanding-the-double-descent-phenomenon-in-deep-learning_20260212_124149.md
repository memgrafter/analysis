---
ver: rpa2
title: Understanding the Double Descent Phenomenon in Deep Learning
arxiv_id: '2403.10459'
source_url: https://arxiv.org/abs/2403.10459
tags:
- descent
- learning
- risk
- arxiv
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This tutorial introduces the double descent phenomenon in deep
  learning, where test error decreases after the interpolation point as model complexity
  increases. The authors explain this behavior through classical statistical learning
  theory and modern practice, highlighting the role of inductive biases in selecting
  solutions that generalize well.
---

# Understanding the Double Descent Phenomenon in Deep Learning

## Quick Facts
- arXiv ID: 2403.10459
- Source URL: https://arxiv.org/abs/2403.10459
- Authors: Marc Lafon; Alexandre Thomas
- Reference count: 28
- Primary result: This tutorial explains double descent in deep learning, where test error decreases after interpolation as model complexity increases, through statistical learning theory and inductive biases.

## Executive Summary
This tutorial introduces the double descent phenomenon in deep learning, where test error exhibits a U-shaped curve as model complexity increases, with error decreasing again after the interpolation point. The authors explain this behavior through classical statistical learning theory and modern practice, highlighting how inductive biases in over-parameterized models select solutions that generalize well. The work provides both theoretical analysis for linear regression with Gaussian noise and empirical demonstrations using Random Fourier Features models.

## Method Summary
The authors use a combination of theoretical analysis and empirical experiments to study double descent. They implement Random Fourier Features models with varying feature dimensions and linear regression with Gaussian noise, training using Empirical Risk Minimization and gradient descent while explicitly choosing minimum norm solutions when multiple interpolating solutions exist. The experiments systematically vary model complexity through under-parameterized, critically-parameterized, and over-parameterized regimes to observe test error behavior.

## Key Results
- Double descent phenomenon observed in both Random Fourier Features and linear regression experiments, with test error decreasing after the interpolation threshold
- Effective Model Complexity (EMC) provides a framework for understanding when test error increases or decreases with model complexity
- Gradient descent on separable data converges to maximum margin solutions, providing a mechanism for good generalization in over-parameterized regimes
- Over-parameterized models can access multiple interpolating solutions, with inductive biases selecting smoother, lower-norm solutions that generalize better

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Double descent occurs because over-parameterized models access multiple interpolating solutions, and inductive biases select smoother, lower-norm solutions that generalize better.
- Mechanism: As model capacity increases past the interpolation threshold, the number of interpolating solutions grows exponentially. Gradient descent implicitly or explicitly biases toward solutions with smaller norms (minimum norm solutions), which tend to be smoother and generalize better.
- Core assumption: The optimization algorithm (e.g., gradient descent) converges to a solution with favorable inductive bias among many interpolating solutions.
- Evidence anchors:
  - [abstract] "very large over-parameterized models... are optimized to fit perfectly the training data and still obtain great generalization performance"
  - [section] "increasing model complexity seems to actually lower the test error" and "by choosing explicitly the minimum norm linear regression"
  - [corpus] Weak evidence - no direct corpus papers address the minimum norm selection mechanism

### Mechanism 2
- Claim: The effective model complexity (EMC) determines the regime (under-parameterized, critically-parameterized, over-parameterized) and governs the direction of test error changes.
- Mechanism: EMC measures the maximum number of samples on which a training procedure achieves near-zero training error. When EMC << n, increasing complexity reduces test error. When EMC ≈ n, complexity changes have unpredictable effects. When EMC >> n, increasing complexity again reduces test error.
- Core assumption: EMC provides a reliable measure of the model's effective capacity relative to the training data size.
- Evidence anchors:
  - [abstract] "we define a training procedure T to be any procedure that takes as input a dataset Dn and outputs a classifier hn"
  - [section] "For any data distribution P (X, Y ), neural-network-based training procedure T, and small ϵ > 0" and formal definition of EMC
  - [corpus] Weak evidence - no direct corpus papers address EMC concept

### Mechanism 3
- Claim: Gradient descent on separable data converges to the maximum margin solution (hard margin SVM) when data is linearly separable.
- Mechanism: For linearly separable data with smooth decreasing loss functions, gradient descent iterates diverge in norm but their normalized direction converges to the SVM solution, which maximizes the margin between classes.
- Core assumption: The loss function satisfies smoothness and exponential tail properties, ensuring convergence to the maximum margin solution.
- Evidence anchors:
  - [abstract] "gradient descent is a widely used optimization procedure in machine learning, and has been observed to converge on solutions that generalize surprisingly well"
  - [section] "Theorem 16. Let D = {(xi, yi)}n i=1 be a linearly separable dataset... Then we have: lim t− →∞ wt/∥wt∥ = wsvm/∥wsvm∥"
  - [corpus] Weak evidence - no direct corpus papers address gradient descent convergence to SVM solutions

## Foundational Learning

- Concept: Statistical learning theory and generalization bounds
  - Why needed here: Understanding the classical bias-variance tradeoff and why it fails for over-parameterized models
  - Quick check question: What is the relationship between VC dimension and generalization error according to Vapnik-Chervonenkis theory?

- Concept: Inductive biases and their role in model selection
  - Why needed here: Explaining why over-parameterized models generalize despite interpolating the training data
  - Quick check question: How does minimum norm selection act as an inductive bias in linear regression?

- Concept: Optimization dynamics in non-convex landscapes
  - Why needed here: Understanding how gradient descent finds generalizing solutions in highly non-convex loss landscapes
  - Quick check question: What properties must a loss function satisfy for gradient descent to converge to a maximum margin solution on separable data?

## Architecture Onboarding

- Component map: Model class H -> Training procedure T -> Effective Model Complexity EMC -> Inductive bias application -> Solution selection -> Generalization performance
- Critical path: Model complexity → EMC determination → Training procedure execution → Inductive bias application → Solution selection → Generalization performance
- Design tradeoffs: Larger model capacity provides more interpolating solutions but requires stronger inductive biases to select good solutions. Simpler models have fewer solutions but may underfit complex data.
- Failure signatures: Peak in test error at the interpolation threshold, divergence of optimization algorithms, selection of high-norm solutions that overfit noise.
- First 3 experiments:
  1. Implement linear regression with Gaussian noise and vary the number of features to observe double descent behavior
  2. Implement Random Fourier Features with varying feature dimensions and observe test error curves
  3. Implement gradient descent on linearly separable data and verify convergence to maximum margin solution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise mathematical conditions under which the double descent phenomenon occurs in deep learning models, and how do these conditions relate to the inductive biases of specific architectures?
- Basis in paper: [explicit] The paper discusses the role of inductive biases in selecting solutions that generalize well in the over-parameterized regime, but does not provide a comprehensive mathematical framework for when double descent occurs.
- Why unresolved: While the paper provides empirical examples and theoretical insights into linear models, the exact conditions for double descent in deep learning remain an open question due to the complexity of these models and their training dynamics.
- What evidence would resolve it: A rigorous mathematical analysis of deep learning models that identifies the specific conditions (e.g., model architecture, optimization algorithm, data distribution) under which double descent occurs, along with empirical validation on various deep learning architectures.

### Open Question 2
- Question: How do the implicit biases of gradient descent in deep learning contribute to the double descent phenomenon, and can these biases be characterized and controlled?
- Basis in paper: [explicit] The paper discusses the implicit bias of gradient descent in selecting solutions that generalize well, particularly in the over-parameterized regime, but does not provide a complete characterization of these biases in deep learning.
- Why unresolved: While the paper provides insights into the implicit biases of gradient descent in linear settings, the complex non-linear dynamics of deep learning make it challenging to fully characterize and control these biases.
- What evidence would resolve it: A comprehensive study of the implicit biases of gradient descent in deep learning, including a mathematical characterization of these biases and their impact on generalization, along with experimental validation on various deep learning architectures and optimization algorithms.

### Open Question 3
- Question: What is the relationship between the double descent phenomenon and the concept of jamming transitions in physical systems, and how can this analogy be used to better understand generalization in deep learning?
- Basis in paper: [explicit] The paper draws an analogy between neural networks and physical systems undergoing jamming transitions, suggesting that this analogy can provide insights into the double descent phenomenon and generalization in deep learning.
- Why unresolved: While the paper provides an initial exploration of this analogy, the full implications and potential applications of this relationship for understanding generalization in deep learning remain unexplored.
- What evidence would resolve it: A detailed study of the relationship between double descent and jamming transitions, including a mathematical formalization of this analogy and its application to deep learning, along with experimental validation on various deep learning architectures and physical systems.

## Limitations

- The connection between linear regression examples and deep neural networks is largely heuristic, with specific mechanisms for deep networks remaining unclear
- The role of optimization algorithms in selecting minimum norm solutions is asserted but not thoroughly demonstrated across different architectures and loss functions
- The paper lacks rigorous proofs for some claims about why double descent occurs in deep learning

## Confidence

- **High confidence**: The empirical observation of double descent in controlled experiments (Random Fourier Features, linear regression with noise)
- **Medium confidence**: The theoretical framework connecting double descent to minimum norm solutions and inductive biases
- **Low confidence**: The direct applicability of linear model insights to complex deep neural networks and the universality of double descent across all over-parameterized regimes

## Next Checks

1. Replicate the double descent phenomenon with different kernel functions and data distributions in Random Fourier Features experiments to verify robustness beyond Gaussian noise and RBF kernels
2. Test alternative optimization algorithms (SGD, Adam, etc.) and verify whether they consistently converge to minimum norm solutions in over-parameterized regimes
3. Investigate the role of data structure by conducting experiments with structured versus random data to determine whether double descent depends on specific properties of real-world datasets