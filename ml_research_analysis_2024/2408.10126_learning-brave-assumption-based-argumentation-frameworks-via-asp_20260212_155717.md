---
ver: rpa2
title: Learning Brave Assumption-Based Argumentation Frameworks via ASP
arxiv_id: '2408.10126'
source_url: https://arxiv.org/abs/2408.10126
tags:
- learning
- rules
- rule
- solution
- brave
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to learning assumption-based
  argumentation (ABA) frameworks using Answer Set Programming (ASP). The method addresses
  the problem of automating the learning of ABA frameworks from background knowledge
  and positive/negative examples, focusing on brave reasoning under stable extensions.
---

# Learning Brave Assumption-Based Argumentation Frameworks via ASP

## Quick Facts
- arXiv ID: 2408.10126
- Source URL: https://arxiv.org/abs/2408.10126
- Reference count: 40
- One-line primary result: ASP-ABAlearnB system learns ABA frameworks using ASP, showing advantages over ILASP in certain cases.

## Executive Summary
This paper presents a novel approach to learning assumption-based argumentation (ABA) frameworks using Answer Set Programming (ASP). The method addresses the problem of automating the learning of ABA frameworks from background knowledge and positive/negative examples, focusing on brave reasoning under stable extensions. The core idea involves transformation rules like Rote Learning, Folding, Assumption Introduction, and Fact Subsumption to iteratively build and refine the framework. The approach is implemented in the ASP-ABAlearnB system, which uses ASP to compute answer sets corresponding to stable extensions. Experiments show that ASP-ABAlearnB performs well compared to state-of-the-art ILP systems like ILASP, particularly in cases where the predicates and mode declarations in the background knowledge are insufficient to express a solution.

## Method Summary
The paper proposes a novel algorithm for learning ABA frameworks using transformation rules implemented via ASP. The algorithm iteratively applies Rote Learning to add minimal facts, Folding to generalize rules, Assumption Introduction to handle exceptions, and Fact Subsumption to remove redundancy. The core idea is to leverage the correspondence between ABA frameworks and ASP programs, using ASP solvers to guide the learning process. The algorithm aims to construct an ABA framework that bravely entails positive examples and does not entail negative examples under stable extensions.

## Key Results
- ASP-ABAlearnB successfully learns ABA frameworks for a range of examples, including non-trivial cases where ILASP fails.
- The algorithm demonstrates the advantage of using Assumption Introduction to recover solutions after Folding introduces new arguments.
- Experiments show that the choice of learnable predicates significantly affects the existence of solutions, highlighting the importance of predicate selection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ASP-ABAlearnB algorithm achieves sound and terminating learning by combining Rote Learning, Folding, and Assumption Introduction in a controlled sequence.
- Mechanism: The algorithm first uses Rote Learning to minimally extend the background knowledge with facts to cover positive examples. Folding then generalizes these facts into intensional rules. If Folding introduces new arguments that break the solution, Assumption Introduction adds exceptions via new assumptions, and Rote Learning adds their contraries. This iterative refinement ensures the framework remains a valid solution.
- Core assumption: The background knowledge is satisfiable and the transformation rules preserve or restore solution status when applied in the specified order.
- Evidence anchors:
  - [abstract] "We present a novel algorithm based on transformation rules (such as Rote Learning, Folding, Assumption Introduction and Fact Subsumption)"
  - [section] "By induction on the argument structure, for all s, A1⊢R1 s ∈ ∆ iff A1∪{α(X)}⊢R′4 s∈ ∆"
  - [corpus] Weak; no direct experimental evidence of preservation under all rule sequences, but theoretical soundness is proven.
- Break condition: If no assumption relative to a rule body exists in the current set A, and no alternative Folding path is available, the algorithm halts with failure.

### Mechanism 2
- Claim: The mapping between ABA frameworks and ASP programs enables the use of ASP solvers to compute stable extensions and guide learning.
- Mechanism: ABA frameworks under stable extensions correspond one-to-one with answer sets of ASP programs. By encoding the learning problem as an ASP program with choice rules and minimization, the solver finds minimal sets of facts (via Rote Learning) or checks whether candidate rules preserve the solution. This leverages ASP's computational power for argumentative reasoning.
- Core assumption: The stable extension semantics for ABA and stable model semantics for ASP are faithfully represented in the encoding.
- Evidence anchors:
  - [abstract] "We leverage on the well known correspondence [2] between stable extensions in the logic programming instance of ABA and Answer Set Programs (ASP)"
  - [section] "By Theorem 3.13 of [2], there is a one-to-one correspondence between the stable extensions of F and the answer sets of PF"
  - [corpus] Strong; the correspondence is a known result in the literature.
- Break condition: If the encoding is incorrect or the ASP solver fails to find an answer set when one exists, the learning algorithm may incorrectly report failure or learn an invalid framework.

### Mechanism 3
- Claim: Bounded Folding ensures termination by preventing infinite rule generalization.
- Mechanism: The boundedness condition F2 limits the set of atoms that can be introduced during Folding, ensuring that after a finite number of steps, no further Folding is possible. This, combined with the finite language and the requirement to use existing assumptions when possible, bounds the search space.
- Core assumption: The language L is finite and the boundedness condition is correctly enforced.
- Evidence anchors:
  - [section] "Condition F2 enforces that there exists a finite setB of atoms such that, for any ρf obtained from ρ by any sequence of applications of fold, bd(ρf )⊆B"
  - [section] "Repeated applications of fold will eventually output a rule ρf such that foldable(ρf ,R) is false and applyFolding will terminate"
  - [corpus] Weak; the proof is given but no empirical evidence of termination across diverse benchmarks.
- Break condition: If the boundedness condition is violated or the language is infinite, the algorithm may not terminate.

## Foundational Learning

- Concept: Stable extensions in assumption-based argumentation
  - Why needed here: The learning algorithm aims to construct ABA frameworks where positive examples are brave consequences under stable extensions and negative examples are not. Understanding stable extensions is essential to grasp what the algorithm is optimizing for.
  - Quick check question: Given an ABA framework with two arguments attacking each other, what are the stable extensions?

- Concept: Answer Set Programming and stable model semantics
  - Why needed here: The algorithm uses ASP solvers to compute answer sets corresponding to stable extensions. Familiarity with ASP syntax, choice rules, and minimization is necessary to understand the encoding and implementation.
  - Quick check question: What is the difference between a brave consequence and a cautious consequence in ASP?

- Concept: Transformation rules in logic programming
  - Why needed here: The learning algorithm uses Folding and Subsumption, which are standard transformation rules in logic programming. Understanding how these rules preserve or modify program semantics is crucial for understanding the generalization and refinement steps.
  - Quick check question: How does Folding relate to inverse resolution?

## Architecture Onboarding

- Component map:
  - Prolog module -> Implements RoLe and Gen procedures, handles symbolic rule manipulation, and manages the nondeterministic search.
  - ASP module -> Encodes the learning problem as ASP rules (Definition 2), invokes Clingo, and collects answer sets.
  - Clingo solver -> Computes answer sets for the ASP encodings, guiding the learning process.
  - Transformation rules -> Rote Learning, Folding, Assumption Introduction, and Fact Subsumption, applied in sequence to refine the ABA framework.

- Critical path:
  1. RoLe: Encodes learning problem in ASP, solves to find minimal facts, adds to background knowledge.
  2. Gen: Iteratively applies Folding to generalize facts into rules, uses Assumption Introduction if generalization breaks the solution, removes redundant facts via Subsumption.
  3. Termination: Either succeeds with an intensional solution or fails if no solution exists.

- Design tradeoffs:
  - Nondeterminism in Folding: Allows exploring multiple generalization paths but can lead to inefficiency. Controlled by boundedness condition.
  - Use of existing assumptions: Promotes termination by avoiding unbounded introduction of new predicates, but may limit expressiveness.
  - ASP encoding: Leverages powerful solvers but adds overhead and complexity compared to direct implementation.

- Failure signatures:
  - RoLe fails: ASP encoding is unsatisfiable, indicating no solution exists.
  - Gen fails: No assumption relative to a rule body exists, and no alternative Folding path is available, indicating no intensional solution can be found.
  - Termination without success: Algorithm halts but returns no solution, either because none exists or the search was incomplete.

- First 3 experiments:
  1. Nixon diamond: Verify the algorithm can learn a solution for the classic example, demonstrating basic functionality.
  2. Acute dataset: Test on a larger, non-trivial dataset to assess scalability and correctness on real data.
  3. Modified acute with assumptions: Add assumptions to the background knowledge and verify ILASP can learn a solution, comparing performance and expressiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does the non-determinism in the Folding rule lead to multiple distinct solutions for the same ABA learning problem?
- Basis in paper: [explicit] The paper states that "the application of Folding, needed for generalisation, is non-deterministic, as there may be different choices for the rules to be used for applying it."
- Why unresolved: The paper mentions that they are "experimenting various mechanisms to control Folding for making it more deterministic" but does not provide specific conditions or examples where non-determinism leads to multiple solutions.
- What evidence would resolve it: Concrete examples of ABA learning problems where different choices of Folding rules lead to different valid solutions, along with a characterization of when this non-determinism occurs.

### Open Question 2
- Question: How does the performance of ASP-ABAlearnB scale with larger, more complex datasets compared to ILASP and other ILP systems?
- Basis in paper: [inferred] The paper mentions that "the most critical issue is that the application of Folding, needed for generalisation, is non-deterministic" and discusses experimental results on seven classic learning problems and three larger problems from [31].
- Why unresolved: While the paper provides experimental results on small to medium-sized problems, it does not discuss scalability to very large datasets or provide a detailed comparison of computational complexity with other ILP systems.
- What evidence would resolve it: Extensive experiments on large-scale datasets (e.g., thousands of examples and complex background knowledge) comparing runtime and solution quality with ILASP and other ILP systems, along with analysis of computational complexity.

### Open Question 3
- Question: What are the theoretical limits of the Assumption Introduction rule in terms of the expressiveness of learned ABA frameworks?
- Basis in paper: [explicit] The paper states that "Assumption Introduction is a key point for enforcing the termination of the algorithm, as using an assumption in A may avoid the introduction of an unbounded number of new predicates."
- Why unresolved: While the paper discusses the role of Assumption Introduction in ensuring termination, it does not explore the theoretical limits of this rule in terms of what types of ABA frameworks can be learned or the trade-off between expressiveness and termination.
- What evidence would resolve it: A formal characterization of the class of ABA frameworks that can be learned using Assumption Introduction, along with examples of problems that require assumptions not in the initial set A and cannot be solved by the current algorithm.

### Open Question 4
- Question: How does the choice of the set T of learnable predicates affect the existence and quality of solutions for ABA learning problems?
- Basis in paper: [explicit] The paper provides an example (Example 4) showing that "the choice of T may affect the existence of a solution for the learning problem" and discusses the role of T in the learning process.
- Why unresolved: While the paper mentions the importance of T and provides one example, it does not provide a comprehensive analysis of how different choices of T affect the learning process or offer guidelines for selecting an appropriate T.
- What evidence would resolve it: A systematic study of how different choices of T affect the existence and quality of solutions for a range of ABA learning problems, along with theoretical insights into the relationship between T and the expressiveness of learned frameworks.

### Open Question 5
- Question: Can the ASP-ABAlearnB algorithm be extended to learn non-flat ABA frameworks, and what would be the challenges in doing so?
- Basis in paper: [explicit] The paper states that "Further extensions of our ABA Learning approach can be envisaged, exploiting the ability of ABA frameworks to be instantiated to different logics and semantics, and possibly address the problem of learning non-flat ABA frameworks [2, 4]."
- Why unresolved: The paper mentions the possibility of extending the algorithm to learn non-flat ABA frameworks but does not provide details on how this could be achieved or what challenges would need to be overcome.
- What evidence would resolve it: A proposed extension of the ASP-ABAlearnB algorithm to handle non-flat ABA frameworks, along with a discussion of the challenges in adapting the transformation rules and reasoning tasks to this more general setting, and experimental results on learning non-flat frameworks.

## Limitations

- The experimental validation is limited to a small set of benchmarks, reducing confidence in the algorithm's general performance.
- The use of nondeterminism in Folding and Assumption Introduction can lead to inefficiency and may not always find a solution even when one exists.
- The bounded Folding condition is proven to ensure termination, but its practical impact on scalability is not empirically demonstrated.

## Confidence

- Confidence in the core learning algorithm: Medium. The algorithm is theoretically sound but lacks extensive empirical validation and may be affected by nondeterminism.
- Confidence in the theoretical underpinnings: High. The correspondence between ABA frameworks and ASP programs is a well-established result in the literature.

## Next Checks

1. **Scalability Test**: Evaluate the algorithm on a larger and more diverse set of benchmarks to assess its scalability and robustness beyond the current examples.
2. **Nondeterminism Analysis**: Investigate the impact of nondeterministic choices in Folding and Assumption Introduction on the algorithm's ability to find solutions and its runtime efficiency.
3. **Boundedness Verification**: Empirically verify that the bounded Folding condition effectively prevents infinite rule generalization and ensures termination across various problem instances.