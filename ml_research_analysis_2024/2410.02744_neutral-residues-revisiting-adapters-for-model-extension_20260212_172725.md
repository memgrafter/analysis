---
ver: rpa2
title: 'Neutral Residues: Revisiting Adapters for Model Extension'
arxiv_id: '2410.02744'
source_url: https://arxiv.org/abs/2410.02744
tags:
- adapters
- learning
- data
- forgetting
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extending a pretrained large
  language model to a new domain without degrading its original performance. The authors
  propose a method called "neutral residues" that builds upon adapters by adding new
  residual blocks to the model architecture.
---

# Neutral Residues: Revisiting Adapters for Model Extension

## Quick Facts
- arXiv ID: 2410.02744
- Source URL: https://arxiv.org/abs/2410.02744
- Authors: Franck Signe Talla; Edouard Grave; Hervé Jégou
- Reference count: 40
- Key outcome: Neutral residues significantly outperforms finetuning, LoRA, and vanilla adapters when extending English language models to new languages like Danish, Hungarian, or Slovak

## Executive Summary
This paper addresses the challenge of extending a pretrained large language model to a new domain without degrading its original performance. The authors propose a method called "neutral residues" that builds upon adapters by adding new residual blocks to the model architecture. The key innovation is to train these new blocks such that they output near-zero values on the original domain data, achieved through a combination of a gating mechanism and a sparsity loss. The method significantly outperforms competing approaches when adapting a state-of-the-art model originally trained on English to new languages, offering a superior trade-off between learning the new language and not forgetting English.

## Method Summary
The method extends adapter-based fine-tuning by adding parallel adapter blocks after each FFN layer in the transformer architecture. A block gate with ReLU activation controls adapter contribution, while an L1-norm sparsity loss is applied during training on original-domain data to encourage near-zero adapter outputs. The adapters are initialized with lower variance than standard He initialization to keep them closer to the original model's behavior longer. This combination forces the adapters to remain "neutral" on original data while learning the new domain effectively.

## Key Results
- Neutral residues outperforms finetuning, LoRA, and vanilla adapters on both English and target language perplexity
- The method achieves superior trade-off between learning new languages and preserving English performance
- Adding 20% extra parameters with neutral residues provides significant gains over vanilla adapters
- The approach works effectively across multiple target languages (Danish, Hungarian, Slovak) and model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sparsity loss on original-domain data forces adapters to output near-zero values on that data, preventing catastrophic forgetting.
- Mechanism: By applying an L1-norm penalty to adapter outputs when processing data from the original distribution, the model is trained to keep those outputs minimal, effectively making the adapter "neutral" on that data.
- Core assumption: Near-zero adapter outputs on original data will not interfere with the frozen backbone's learned representations.
- Evidence anchors:
  - [abstract]: "the resulting method, called neutral residues, modifies adapters in a way that leads each new residual block to output near-zeros on the original domain."
  - [section]: "we propose to regularize the output of the adapter modules with an ℓ1-norm on the data related to the original distribution."
  - [corpus]: "no direct citations found; assumption based on internal logic of sparsity penalty."

### Mechanism 2
- Claim: The block gate with ReLU activation ensures that adapters are selectively active only on new-domain data, preserving original behavior otherwise.
- Mechanism: A gating mechanism with ReLU activation learns to output near-zero values for original data (via the sparsity loss) and non-zero values for new data, effectively controlling when adapters are active.
- Core assumption: The ReLU gating can learn a clear decision boundary between original and new domain data.
- Evidence anchors:
  - [abstract]: "a gating mechanism and a sparsity loss" are used together to achieve near-zero outputs.
  - [section]: "We add a block gate over the full adapter... We compute the average of the ℓ1-norm on the output of the adapters as the local loss."
  - [corpus]: "no direct citations found; assumption based on internal logic of gating."

### Mechanism 3
- Claim: Low-variance initialization keeps adapters close to the original model longer, improving the trade-off between learning and forgetting.
- Mechanism: By initializing adapter weights with lower variance than standard He initialization, the adapters start closer to the original model's behavior, allowing more controlled learning of the new domain.
- Core assumption: Starting closer to the original model's function will reduce initial forgetting.
- Evidence anchors:
  - [section]: "we depart from the usual He's initialization... we use a variance of 1/(d · L)... the model remains longer close to the original one."
  - [section]: "we are even more drastic in that respect: in addition to setting the output matrix with 0s, we depart from the usual He's initialization for both the input and gating matrix and initialize them with a much lower variance."
  - [corpus]: "no direct citations found; assumption based on internal logic of initialization."

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper addresses the problem of extending a model to a new domain without degrading performance on the original domain, which is a classic catastrophic forgetting scenario.
  - Quick check question: What happens to a neural network's performance on previously learned tasks when it is fine-tuned on new tasks without special measures?

- Concept: Parameter-efficient fine-tuning methods (PEFT)
  - Why needed here: The paper compares its method against LoRA and vanilla adapters, which are PEFT methods, and builds upon the adapter concept.
  - Quick check question: How do LoRA and vanilla adapters differ from full fine-tuning in terms of parameter usage and training efficiency?

- Concept: Residual connections and their role in deep networks
  - Why needed here: The method modifies residual blocks by adding adapter blocks in parallel, and the sparsity loss ensures these blocks output near-zero on original data, effectively making them neutral.
  - Quick check question: How do residual connections in transformer architectures allow for the addition of new blocks without disrupting the original signal flow?

## Architecture Onboarding

- Component map: Original transformer backbone (frozen) -> parallel adapter blocks after each FFN layer -> block gate (ReLU) -> sparsity loss (L1-norm on original data)
- Critical path: Input data -> backbone processing -> adapter processing in parallel -> block gate determines adapter contribution -> sparsity loss applied only on original-domain data
- Design tradeoffs: Adding 20% extra parameters improves new domain learning but increases computational cost; ReLU gating with L1 loss works better than sigmoid with cross-entropy; parallel adapters perform similarly to serial but are more directly comparable to baselines
- Failure signatures: If forgetting occurs, sparsity loss may be too weak; if new domain performance is poor, sparsity loss may be too strong or initialization too restrictive; if training is unstable, learning rate or initialization may need adjustment
- First 3 experiments:
  1. Implement basic adapter architecture with parallel blocks and zero-initialized output matrices, train on mixed data to observe baseline forgetting vs learning tradeoff
  2. Add block gate with ReLU activation and sparsity loss, train on original-domain data only to verify adapter outputs driven to near-zero
  3. Train on full mixed dataset with complete neutral residues setup and compare performance against LoRA and vanilla adapters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal amount of capacity increase (e.g., percentage of extra parameters) for adapter-based model extension, and does this vary by domain or task type?
- Basis in paper: [explicit] The paper shows that increasing extra capacity from 5% to 50% improves performance on learning new languages, but does not determine the optimal percentage or whether it varies by domain.
- Why unresolved: The paper only tests a limited range of parameter increases and focuses on language extension. The relationship between capacity size and performance for different domains or task types is unexplored.
- What evidence would resolve it: Systematic experiments varying parameter percentages across diverse domains (e.g., vision, multimodal, code) and task types, measuring the trade-off between learning new capabilities and forgetting original knowledge.

### Open Question 2
- Question: How does the performance of neutral residues scale with model size, and are there diminishing returns or break-even points for very large language models?
- Basis in paper: [inferred] The experiments use models ranging from 1B to 2B parameters. The paper does not explore whether the method's effectiveness scales proportionally with model size or if there are limitations for larger models.
- Why unresolved: The paper does not test the method on models larger than 2B parameters, leaving uncertainty about scalability and potential limitations for state-of-the-art models with trillions of parameters.
- What evidence would resolve it: Experiments applying neutral residues to models of varying sizes (e.g., 7B, 70B, 175B parameters) while measuring the trade-off between learning new domains and forgetting original knowledge, identifying any scaling patterns or limitations.

### Open Question 3
- Question: What is the impact of different types of original pretraining data distribution approximations on the effectiveness of neutral residues, especially when exact pretraining data is unavailable?
- Basis in paper: [explicit] The paper uses Wikipedia as a proxy for pretraining data and notes that using imperfect approximations still yields strong results, but does not systematically study the impact of different approximation types or quality.
- Why unresolved: The paper only tests one proxy dataset (Wikipedia) and does not explore how different approximation strategies (e.g., using data from the same domain, same style, or similar complexity) affect performance when exact pretraining data is unavailable.
- What evidence would resolve it: Experiments comparing neutral residues performance using various proxy datasets with different relationships to the original pretraining data (e.g., same domain but different sources, similar style but different topics) to identify which approximation strategies are most effective.

## Limitations
- Evaluation limited to language extension from English to a few specific target languages
- Method adds approximately 20% more parameters than vanilla adapters, which may be prohibitive for very large models
- Only two backbone models tested, leaving uncertainty about performance on other architectures

## Confidence

**High Confidence**: The core mechanism of using sparsity loss to drive adapter outputs toward zero on original domain data is well-supported by experimental results. The claim that neutral residues outperforms finetuning, LoRA, and vanilla adapters on the language extension task is strongly evidenced.

**Medium Confidence**: The assertion that low-variance initialization improves the trade-off between learning and forgetting is supported by ablation studies, but the exact mechanism remains somewhat heuristic. The claim that parallel adapter placement is comparable to serial placement is based on single experiments.

**Low Confidence**: The generalizability of the method to non-language domain shifts is not demonstrated. The paper does not provide theoretical guarantees for why the combination of ReLU gating and L1 sparsity loss should work better than other combinations.

## Next Checks

1. **Cross-Domain Generalization Test**: Apply neutral residues to a non-language domain shift, such as adapting a general-purpose model to a specific scientific domain (e.g., biomedical text), and compare performance against established baselines to verify broader applicability.

2. **Sensitivity Analysis on Sparsity Loss**: Systematically vary the sparsity loss coefficient α across multiple orders of magnitude (e.g., 0.01, 0.1, 1.0) and different target languages to map out the stability region and identify optimal values for different scenarios.

3. **Gating Function Comparison**: Replace the ReLU gating mechanism with alternative activation functions (sigmoid, tanh, learned gating) and evaluate their impact on both original and new domain performance to determine if ReLU is indeed optimal or if other gating strategies could yield better results.