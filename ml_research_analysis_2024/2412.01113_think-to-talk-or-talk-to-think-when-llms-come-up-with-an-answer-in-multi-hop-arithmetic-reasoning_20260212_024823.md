---
ver: rpa2
title: Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop
  Arithmetic Reasoning
arxiv_id: '2412.01113'
source_url: https://arxiv.org/abs/2412.01113
tags:
- probing
- accuracy
- qwen2
- when
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the internal problem-solving process of
  language models (LMs) during arithmetic multi-hop reasoning. Using controlled symbolic
  arithmetic tasks and linear probes, we determine when LMs internally resolve sub/whole
  problems by analyzing their hidden states during first reading and CoT generation.
---

# Think-to-Think or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning

## Quick Facts
- **arXiv ID**: 2412.01113
- **Source URL**: https://arxiv.org/abs/2412.01113
- **Reference count**: 40
- **Primary result**: Language models predominantly solve arithmetic problems while generating reasoning chains rather than during initial problem reading

## Executive Summary
This study investigates when large language models (LLMs) internally resolve arithmetic problems during multi-hop reasoning tasks. Using controlled symbolic arithmetic problems and linear probes to analyze hidden states, the researchers determine whether models compute answers during initial problem reading (Think-to-Talk) or while generating reasoning chains (Talk-to-Think). The study systematically varies task complexity and model size to understand the relationship between problem difficulty and internal computation timing.

The findings reveal that LLMs operate primarily in Talk-to-Think mode, meaning they do not derive answers during first reading but instead obtain subanswers while generating Chain-of-Thought (CoT) reasoning chains. Causal intervention analyses further show strong recency bias in information flow, with hidden states most influential when in the same or immediately preceding computation grid. This suggests that generated reasoning chains faithfully reflect the model's internal computation rather than serving as post-hoc explanations.

## Method Summary
The researchers used synthetic symbolic arithmetic tasks with controlled complexity to systematically study LLM reasoning processes. They employed linear probes to analyze hidden states during both first reading and CoT generation phases. The study measured model performance across various model sizes and task complexities, using causal intervention analyses to determine information flow patterns. By manipulating the timing and structure of arithmetic problems, they could isolate when models internally compute subanswers and how this relates to generated reasoning chains.

## Key Results
- LLMs predominantly operate in Talk-to-Think mode, deriving answers while generating reasoning chains rather than during initial problem reading
- Causal intervention analyses reveal strong recency bias in information flow, with hidden states most influential in same or immediately preceding computation grids
- Generated reasoning chains faithfully reflect internal computation rather than serving as post-hoc explanations

## Why This Works (Mechanism)
The mechanism underlying these findings relates to how LLMs process and generate information sequentially. During initial reading, models likely encode problem structure and numerical relationships without performing explicit computation. The generation phase then triggers internal computation as the model produces tokens that represent intermediate steps. This sequential processing naturally creates recency bias, where recent hidden states have stronger influence on subsequent computation. The Talk-to-Think mode emerges because the act of generating reasoning tokens provides the cognitive context needed for the model to perform arithmetic operations.

## Foundational Learning
- **Chain-of-Thought prompting**: A technique where models are prompted to generate intermediate reasoning steps; needed to understand the generation phase being studied
- **Linear probe analysis**: A method for examining hidden states to infer internal representations; needed to determine when computation occurs
- **Causal intervention**: Techniques for measuring information flow and influence between hidden states; needed to understand temporal dependencies
- **Multi-hop arithmetic reasoning**: Problems requiring multiple computational steps; needed as the task domain
- **Hidden state analysis**: Examining internal model representations; needed to understand when computation occurs
- **Recency bias**: The phenomenon where recent information has stronger influence; needed to explain information flow patterns

## Architecture Onboarding

### Component Map
Input tokens -> Embedding layer -> Transformer blocks -> Linear probe analysis -> Output tokens

### Critical Path
Input reading -> Hidden state encoding -> Reasoning chain generation -> Final answer computation

### Design Tradeoffs
The study prioritizes controlled experimental conditions over real-world applicability. Synthetic tasks enable precise measurement but may not generalize to natural language problems. The focus on linear probe analysis provides interpretability but may miss complex interactions captured by other methods.

### Failure Signatures
If models were operating in Think-to-Talk mode, we would expect to see subanswers encoded in initial hidden states. The absence of this pattern, combined with successful computation during generation, confirms the Talk-to-Think mechanism.

### First Experiments
1. Measure linear probe accuracy for subanswer detection during initial reading vs. generation phases
2. Conduct causal intervention analysis to identify information flow patterns between computation steps
3. Vary task complexity to determine threshold effects on computation timing

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic symbolic arithmetic tasks may limit generalizability to real-world reasoning scenarios
- Controlled experimental setup constrains external validity despite enabling precise measurement
- Linear probe analysis provides correlational rather than fully causal evidence of internal computation timing

## Confidence

| Claim | Confidence |
|-------|------------|
| Models operate in Talk-to-Think mode | High |
| Recency bias exists in information flow | Medium |
| CoT chains faithfully represent internal computation | Medium |

## Next Checks
1. Replicate experiments using non-symbolic, real-world arithmetic word problems to assess generalizability
2. Conduct ablation studies systematically removing different components to isolate causal mechanisms
3. Extend analysis to include attention pattern visualization to complement hidden state analysis