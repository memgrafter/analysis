---
ver: rpa2
title: 'Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large
  Language Models for Human-Centric Assessments'
arxiv_id: '2410.21131'
source_url: https://arxiv.org/abs/2410.21131
tags:
- counterfactual
- explanations
- explanation
- human
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a dataset of 30 human-evaluated counterfactual
  explanations, rated by 206 participants on eight metrics including feasibility,
  consistency, completeness, trust, fairness, and complexity. The dataset, called
  CounterEval, is used to fine-tune Large Language Models (LLMs) to predict human
  judgments of explanation quality.
---

# Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large Language Models for Human-Centric Assessments

## Quick Facts
- arXiv ID: 2410.21131
- Source URL: https://arxiv.org/abs/2410.21131
- Authors: Marharyta Domnich; Julius Välja; Rasmus Moorits Veski; Giacomo Magnifico; Kadi Tulver; Eduard Barbu; Raul Vicente
- Reference count: 32
- Primary result: Fine-tuned LLMs predict human judgments of counterfactual explanation quality with 85% accuracy across eight evaluation metrics

## Executive Summary
This paper addresses the challenge of evaluating counterfactual explanations by developing a dataset of human judgments across eight key metrics including feasibility, consistency, and trust. The authors collected ratings from 206 participants on 30 diverse counterfactual scenarios and used this data to fine-tune Large Language Models (LLMs) to predict human evaluations. The resulting framework, called CounterEval, demonstrates that fine-tuned LLMs can achieve up to 85% accuracy in predicting human ratings, providing a scalable alternative to human evaluation for benchmarking explanation methods.

## Method Summary
The paper develops a human-centric evaluation framework by first collecting ratings from 206 participants across eight metrics for 30 counterfactual explanation scenarios. The collected human judgments are then used to fine-tune LLMs including Llama 3.1 and GPT-4 using QLoRA for efficient training. The fine-tuning process employs a completion-only data collator to adapt the models to predict human ratings, with performance evaluated using accuracy metrics across the eight evaluation dimensions.

## Key Results
- Zero-shot LLM evaluations achieved up to 63% accuracy in predicting human judgments
- Fine-tuned models reached 85% accuracy across all eight metrics with three-class predictions
- Individual participant preferences could be modeled with up to 90% accuracy, though one participant showed only 66% accuracy
- The framework enables scalable benchmarking of counterfactual explanation methods without requiring repeated human evaluations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can approximate human evaluation of counterfactual explanations by learning from a labeled dataset of human judgments.
- **Mechanism:** The paper collects human ratings across 8 evaluation metrics for 30 counterfactual scenarios. These ratings are used to fine-tune LLMs, enabling them to predict human judgments with high accuracy (up to 85% after fine-tuning).
- **Core assumption:** Human judgments of explanation quality are consistent enough to be learned by LLMs, and the dataset captures the key dimensions of human evaluation.
- **Evidence anchors:**
  - [abstract]: "Our methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot evaluations and 85% (over a 3-classes prediction) with fine-tuning across all metrics."
  - [section]: "The fine-tuned models predicting human ratings offer better comparability and scalability in evaluating different counterfactual explanation frameworks."
  - [corpus]: Weak - no direct corpus evidence, but the paper's methodology implies consistency in human judgments.
- **Break condition:** If human judgments are too inconsistent or the dataset doesn't capture the full range of evaluation criteria, LLMs may not generalize well.

### Mechanism 2
- **Claim:** LLMs can be fine-tuned to match individual human evaluators' preferences, capturing personalized evaluation patterns.
- **Mechanism:** The paper clusters participants based on their evaluation patterns and fine-tunes LLMs on individual participants' responses, achieving up to 90% accuracy in matching their preferences.
- **Core assumption:** Individual evaluation patterns are consistent enough to be learned by LLMs, and clustering can identify distinct preference groups.
- **Evidence anchors:**
  - [section]: "One participant appeared to be less consistent, as the model managed to simulate their answers with an accuracy of only 66%."
  - [section]: "The selected participants, each from different European countries with educational levels from high school to Master's degree, ensured a diverse range of viewpoints."
  - [corpus]: Weak - no direct corpus evidence, but the paper's methodology implies consistency in individual preferences.
- **Break condition:** If individual preferences are too idiosyncratic or inconsistent, LLMs may not be able to accurately match them.

### Mechanism 3
- **Claim:** LLMs can provide a scalable and consistent alternative to human evaluations for benchmarking counterfactual explanation methods.
- **Mechanism:** The paper demonstrates that fine-tuned LLMs can predict human judgments across multiple metrics with high accuracy, offering a more efficient and scalable evaluation method.
- **Core assumption:** LLM predictions are consistent and can be used as a reliable benchmark for comparing different explanation methods.
- **Evidence anchors:**
  - [abstract]: "The fine-tuned models predicting human ratings offer better comparability and scalability in evaluating different counterfactual explanation frameworks."
  - [section]: "Our experiments also indicate the potential to fine-tune models to individual experts, to target specific expertise or individual preferences."
  - [corpus]: Weak - no direct corpus evidence, but the paper's methodology implies scalability and consistency.
- **Break condition:** If LLM predictions are inconsistent or biased, they may not provide a reliable benchmark for comparison.

## Foundational Learning

- **Concept: Counterfactual explanations**
  - **Why needed here:** Understanding what counterfactual explanations are and how they work is crucial for evaluating their quality and effectiveness.
  - **Quick check question:** What is the key question that counterfactual explanations aim to answer?

- **Concept: Human-centric evaluation metrics**
  - **Why needed here:** Knowing the different metrics used to evaluate counterfactual explanations (e.g., feasibility, consistency, trust) is essential for understanding how LLMs are trained and evaluated.
  - **Quick check question:** What are the 8 evaluation metrics used in the paper, and what do they measure?

- **Concept: Fine-tuning LLMs**
  - **Why needed here:** Understanding how LLMs are fine-tuned on human-labeled data is crucial for replicating the paper's methodology and applying it to other domains.
  - **Quick check question:** What are the key steps involved in fine-tuning an LLM on a dataset of human evaluations?

## Architecture Onboarding

- **Component map:** Human evaluations -> Dataset preprocessing -> LLM fine-tuning pipeline -> Prediction models
- **Critical path:** Collect human evaluations → Preprocess data → Fine-tune LLMs → Evaluate LLM predictions against human judgments
- **Design tradeoffs:** Using smaller, fine-tunable LLMs (e.g., Llama 3) vs. larger, more capable models (e.g., GPT-4) involves tradeoffs between computational cost and prediction accuracy
- **Failure signatures:** If LLM predictions are consistently poor across multiple metrics, it may indicate issues with the dataset, fine-tuning process, or LLM architecture
- **First 3 experiments:**
  1. Fine-tune a small LLM (e.g., Llama 3 8B) on the dataset using QLoRA and evaluate its performance on a held-out test set
  2. Compare the performance of different LLMs (e.g., Llama 3 vs. GPT-4) on the same task to identify the best-performing model
  3. Fine-tune an LLM on individual participants' responses to assess its ability to match personalized evaluation patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural improvements in future LLM models would most effectively increase accuracy beyond the current 85% in predicting human evaluations of counterfactual explanations?
- Basis in paper: [inferred] The paper shows current models achieve 85% accuracy and mentions "continuously improving size and capabilities of LLMs" as a path forward.
- Why unresolved: The paper doesn't analyze which architectural features (attention mechanisms, parameter scaling, fine-tuning strategies) would yield the greatest accuracy gains for this specific task.
- What evidence would resolve it: Controlled experiments comparing different LLM architectures (sparse vs dense attention, mixture-of-experts, etc.) on the CounterEval dataset, with ablation studies showing which components drive performance improvements.

### Open Question 2
- Question: How do individual cognitive biases and background factors (education level, domain expertise, cultural background) quantitatively affect the consistency of human evaluations across the eight metrics?
- Basis in paper: [explicit] The paper mentions participants varied in "educational levels from high school to Master's degree" and conducted clustering to find subgroups, but only tested 4 participants.
- Why unresolved: While the paper acknowledges variability in human preferences and tested some individual modeling, it lacks systematic analysis of how specific demographic and background factors correlate with evaluation patterns.
- What evidence would resolve it: Large-scale analysis correlating participant demographics and background variables with their evaluation patterns across all metrics, potentially revealing which factors most influence subjective assessment consistency.

### Open Question 3
- Question: What is the optimal balance between automated LLM evaluation and human-in-the-loop approaches for generating and refining counterfactual explanations in production systems?
- Basis in paper: [explicit] The paper proposes "exploring the idea of integrating this model within a human-in-the-loop approach" and mentions "LLM-in-the-loop approach instead of a human."
- Why unresolved: The paper suggests hybrid approaches but doesn't empirically test what ratio of automated to human evaluation provides the best trade-off between efficiency, accuracy, and user trust.
- What evidence would resolve it: Comparative studies measuring system performance, user satisfaction, and development efficiency across different ratios of automated to human evaluation in real-world deployment scenarios.

## Limitations

- The evaluation framework relies on a relatively small dataset of 30 counterfactual scenarios rated by 206 participants, which may limit generalizability across diverse real-world applications.
- The 63% zero-shot accuracy suggests LLMs still struggle with some aspects of human evaluation without fine-tuning, and even after fine-tuning, the models achieve only 85% accuracy across three classes - indicating substantial room for improvement.
- The paper's human evaluation methodology assumes participants understand complex counterfactual explanation concepts, though comprehension was only tested via open-ended questions rather than validated comprehension measures.

## Confidence

- **High confidence**: The core methodology of using human ratings to fine-tune LLMs for explanation evaluation is technically sound and well-executed
- **Medium confidence**: The claim that fine-tuned models provide a "scalable alternative" to human evaluation, given the remaining accuracy gaps and potential domain limitations
- **Medium confidence**: The assertion that individual participant preferences can be reliably modeled, based on only one case showing lower accuracy (66%)

## Next Checks

1. **External validity test**: Evaluate the fine-tuned models on counterfactual explanations from domains not represented in the original 30 scenarios (e.g., healthcare, finance) to assess generalizability beyond the training distribution.

2. **Human-in-the-loop verification**: Conduct a second round of human evaluation where participants rate the same explanations, but this time comparing their judgments against the LLM predictions to identify systematic biases or blind spots in the model.

3. **Ablation study on metrics**: Remove each of the eight evaluation metrics individually during fine-tuning to determine which dimensions are most critical for achieving high accuracy, helping identify whether all metrics are equally learnable or if some require different modeling approaches.