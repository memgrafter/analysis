---
ver: rpa2
title: 'HULLMI: Human vs LLM identification with explainability'
arxiv_id: '2409.04808'
source_url: https://arxiv.org/abs/2409.04808
tags:
- text
- ai-generated
- these
- traditional
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the effectiveness of traditional machine
  learning models (Naive Bayes, Logistic Regression, Random Forests, XGBoost, MLP)
  compared to modern NLP detectors (RoBERTa-Sentinel, T5-Sentinel) for distinguishing
  human-written text from AI-generated text. The authors test these models on curated
  and real-world datasets, finding that traditional models perform as well as modern
  detectors.
---

# HULLMI: Human vs LLM identification with explainability

## Quick Facts
- arXiv ID: 2409.04808
- Source URL: https://arxiv.org/abs/2409.04808
- Reference count: 40
- Traditional ML models perform as well as modern NLP detectors for distinguishing human vs AI-generated text.

## Executive Summary
This paper investigates the effectiveness of traditional machine learning models compared to modern NLP detectors for distinguishing human-written text from AI-generated text. The authors test Naive Bayes, Logistic Regression, Random Forests, XGBoost, and MLP against RoBERTa-Sentinel and T5-Sentinel on curated and real-world datasets. They find that traditional models achieve competitive performance, with XGBoost reaching 0.91 accuracy and T5-Sentinel achieving 0.98 accuracy on the OpenGPT dataset. The study also applies LIME to explain model predictions, revealing key features influencing classification decisions.

## Method Summary
The authors collected AI-generated text from OpenGPT (29,395 samples) and human-generated text from OpenWebText. They created a custom test dataset with 25 human samples across five domains (literature, recipes, tweets, IMDb reviews, Quora) and their AI-generated versions. Traditional ML models (Naive Bayes, Logistic Regression, Random Forests, XGBoost, MLP) and modern NLP detectors (RoBERTa-Sentinel, T5-Sentinel) were trained using TF-IDF features. LIME was applied to explain predictions by highlighting the most influential features. Models were evaluated using accuracy, F1-score, FPR, FNR, ROC-AUC, and DET curves.

## Key Results
- Traditional ML models (Naive Bayes, Logistic Regression, Random Forests, XGBoost, MLP) perform as well as modern NLP detectors for AI vs human text classification.
- T5-Sentinel achieves the highest accuracy (0.98) on the OpenGPT dataset, while XGBoost achieves 0.91 accuracy, demonstrating competitive performance.
- LIME analysis reveals patterns in feature importance, enhancing interpretability of both traditional and modern model predictions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Traditional ML models can match modern NLP detectors in AI vs human text classification accuracy.
- Mechanism: Traditional models like Naive Bayes, Logistic Regression, Random Forests, XGBoost, and MLP are trained on TF-IDF or Bag-of-Words features derived from human and AI-generated text, capturing sufficient discriminative information without requiring complex neural architectures.
- Core assumption: The stylistic and lexical differences between human and AI-generated text are preserved in simple statistical features and can be learned by traditional algorithms.
- Evidence anchors:
  - [abstract] states "traditional ML models (Naive-Bayes, MLP, Random Forests, XGBoost) perform as well as modern NLP detectors".
  - [section] reports XGBoost accuracy 0.91 vs T5-Sentinel 0.98 on OpenGPT dataset.
  - [corpus] shows weak correlation: "average neighbor FMR=0.519", suggesting mixed literature on detectability.
- Break condition: If AI-generated text becomes too stylistically similar to human text, or if adversarial paraphrasing reduces lexical distinctiveness, simple features may fail to discriminate.

### Mechanism 2
- Claim: LIME can explain predictions of both traditional and modern detectors by highlighting key contributing features.
- Mechanism: LIME perturbs tokenized input and measures changes in model predictions, then ranks features by their impact on the classification probability.
- Core assumption: The trained model's decision boundary is locally stable enough that small input perturbations yield interpretable feature importance rankings.
- Evidence anchors:
  - [abstract] mentions "by employing the explainable AI technique LIME, we uncover parts of the input that contribute most to the prediction".
  - [section] describes LIME implementation: "Using the 'LimeTextExplainer' in Python, we identified the specific words that had the greatest impact".
  - [corpus] does not directly mention LIME; evidence is weak here.
- Break condition: If the model is highly non-linear or uses complex attention patterns, LIME's linear approximation may oversimplify or misrepresent true feature influence.

### Mechanism 3
- Claim: Curated real-world datasets improve model robustness and prevent overfitting to synthetic test data.
- Mechanism: Custom datasets drawn from diverse domains (literature, recipes, tweets, IMDb reviews, Quora) and processed through both human and AI paraphrasing expose models to varied linguistic styles, reducing bias toward any single corpus.
- Core assumption: Diversity in training and testing data reflects real-world variability, making model performance more generalizable.
- Evidence anchors:
  - [section] describes building custom dataset: "To ensure that the custom dataset is as diverse as possible, we have collected 5 Human Written Samples in each of the below 5 domains".
  - [section] reports: "MLP maintained a strong performance with an accuracy of 0.90, proving its versatility across different text types".
  - [corpus] weak here; no direct mention of custom dataset or domain diversity.
- Break condition: If real-world text distributions drift further from curated samples, or if certain domains are underrepresented, model robustness may degrade.

## Foundational Learning

- Concept: TF-IDF transformation and vectorization.
  - Why needed here: Converts raw text into numerical features that emphasize informative words while downweighting common ones, enabling traditional models to operate effectively.
  - Quick check question: What effect does TF-IDF normalization have on model convergence and feature importance ranking?

- Concept: LIME's local approximation and perturbation methodology.
  - Why needed here: Provides interpretable feature contributions for any black-box classifier, essential for transparency in AI detection tools.
  - Quick check question: How does LIME's sampling radius influence the stability of feature importance scores?

- Concept: Evaluation metrics (ROC-AUC, F1, FPR, FNR, TPR, TNR).
  - Why needed here: Balances detection performance against false alarm rates and missed detections, critical in domains like education and cybersecurity.
  - Quick check question: In an imbalanced dataset, which metric best reflects real-world detection utility?

## Architecture Onboarding

- Component map: Text collection -> Preprocessing (tokenization, TF-IDF) -> Train/test split -> Model training -> LIME explanation -> Evaluation metrics
- Critical path: Preprocessing -> Model training -> LIME explanation -> Evaluation metrics
- Design tradeoffs:
  - Simple features (TF-IDF) vs. contextual embeddings: speed/interpretability vs. richer semantics
  - Traditional vs. modern models: explainability and resource efficiency vs. peak accuracy
  - LIME perturbation count: computational cost vs. stability of explanations
- Failure signatures:
  - Overfitting to training corpus: high train accuracy but poor custom test performance
  - Low LIME interpretability: feature importance scores unstable across perturbations
  - Class imbalance: high accuracy but poor F1/recall indicating bias toward majority class
- First 3 experiments:
  1. Train and evaluate Naive Bayes and Logistic Regression on OpenGPT dataset; compare accuracy and LIME explanations.
  2. Apply LIME to T5-Sentinel predictions on custom dataset; visualize top-10 feature contributions per sample.
  3. Vary TF-IDF max_features parameter; measure impact on model accuracy and LIME stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do traditional machine learning models compare to modern NLP detectors in distinguishing human-written text from AI-generated text across diverse real-world datasets?
- Basis in paper: [explicit] The paper explicitly states that traditional ML models perform as well as modern NLP detectors in human vs AI text detection, but does not provide comprehensive comparisons across diverse real-world datasets.
- Why unresolved: The study primarily focuses on curated corpora and real-world samples but lacks a detailed comparison of model performance across a wide range of real-world scenarios.
- What evidence would resolve it: Conducting extensive tests on diverse, real-world datasets and comparing the performance of traditional ML models and modern NLP detectors would provide clarity.

### Open Question 2
- Question: What are the limitations of using LIME for interpretability in AI-generated text detection, and how can these limitations be addressed?
- Basis in paper: [inferred] The paper mentions the use of LIME for interpretability but does not explore its limitations or potential improvements.
- Why unresolved: The paper does not delve into the challenges or constraints of using LIME for interpreting model predictions in AI-generated text detection.
- What evidence would resolve it: Investigating the constraints of LIME and proposing methods to enhance its interpretability in this context would provide insights.

### Open Question 3
- Question: How can the integration of traditional ML models with modern deep learning architectures improve the robustness of AI-generated text detection?
- Basis in paper: [inferred] The paper suggests future research to explore the integration of traditional models with sophisticated deep learning architectures but does not provide evidence of such integration.
- Why unresolved: The study does not implement or test a hybrid model that combines traditional and modern approaches.
- What evidence would resolve it: Developing and evaluating a hybrid model that leverages the strengths of both traditional and deep learning models would demonstrate potential improvements in detection robustness.

## Limitations
- The custom test dataset is relatively small (25 samples per domain), limiting generalizability across diverse text types and styles.
- LIME's linear approximation may not capture complex decision boundaries in modern detectors like T5-Sentinel.
- The study focuses on binary classification without addressing multi-class scenarios or adversarial attack testing.

## Confidence
- **High Confidence**: Traditional ML models can achieve competitive accuracy compared to modern NLP detectors for AI vs human text classification when trained on TF-IDF features.
- **Medium Confidence**: LIME effectively explains model predictions by highlighting key features influencing classification.
- **Medium Confidence**: Curated real-world datasets improve model robustness by exposing models to diverse linguistic styles.

## Next Checks
1. Expand dataset diversity and size by increasing the custom test dataset to include more samples per domain and additional text types.
2. Test model robustness against adversarial attacks by applying common techniques like paraphrasing and synonym replacement.
3. Compare LIME explanations across models and perturbations by varying perturbation counts and sampling radius to assess stability.