---
ver: rpa2
title: 'Pretraining Data Detection for Large Language Models: A Divergence-based Calibration
  Method'
arxiv_id: '2409.14781'
source_url: https://arxiv.org/abs/2409.14781
tags:
- data
- token
- detection
- text
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of detecting whether a text was
  part of a large language model's pretraining data without access to the model internals.
  The authors propose a divergence-based calibration method that measures the cross-entropy
  between the token probability distribution from the LLM and the token frequency
  distribution estimated from a reference corpus.
---

# Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method

## Quick Facts
- arXiv ID: 2409.14781
- Source URL: https://arxiv.org/abs/2409.14781
- Reference count: 5
- Key outcome: Divergence-based calibration method improves pretraining data detection accuracy, outperforming Min-K% Prob baseline in AUC and TPR@5%FPR metrics.

## Executive Summary
This paper addresses the challenge of detecting whether a text was part of a large language model's pretraining data without access to model internals. The authors propose a divergence-based calibration method that measures the cross-entropy between the token probability distribution from the LLM and the token frequency distribution estimated from a reference corpus. This approach improves detection accuracy compared to existing methods, particularly for texts with many common words. Experiments on English and Chinese benchmarks show significant improvements, with the proposed method outperforming state-of-the-art baselines like Min-K% Prob in AUC and TPR@5%FPR.

## Method Summary
The proposed divergence-based calibration method (DC-PDD) detects pretraining data by computing cross-entropy between the LLM's token probability distribution and a reference corpus frequency distribution. The method selects only the first occurrence of each token for score calculation and limits the upper bound of calibrated token probabilities to prevent domination by a few tokens. This calibration process addresses the bias of LLMs assigning higher probabilities to high-frequency tokens regardless of whether the text is training or non-training data.

## Key Results
- DC-PDD achieves AUC of 0.9412 on the Large-20B model with Wudao-Corpus, compared to 0.8970 for Min-K% Prob
- TPR@5%FPR reaches 0.8916 for DC-PDD versus 0.8238 for the baseline on the same dataset
- The method shows consistent improvements across different model sizes (1.3B, 6B, 20B parameters) and language benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token probabilities calibrated via cross-entropy between LLM output distribution and reference corpus frequency distribution improve pretraining data detection.
- Mechanism: Divergence-based calibration measures the information gap between the token probability distribution predicted by the LLM and the token frequency distribution from a reference corpus. This calibrates token probabilities to better reflect their informativeness in indicating whether a text was part of the training corpus.
- Core assumption: High-frequency tokens in the pretraining corpus are more likely to be assigned higher probabilities by the LLM, regardless of whether the text is training or non-training data. The divergence between these distributions captures this bias.
- Evidence anchors:
  - [abstract]: "We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score."
  - [section 3.4]: "We compute the cross-entropy (i.e., the divergence) between the token probability distribution of x and the token frequency distribution in pretraining corpus... to obtain a score αi for each token xi"
  - [corpus]: The reference corpus (C4 or ChineseWebText) is used to estimate the token frequency distribution since the LLM's pretraining corpus is not accessible. This introduces uncertainty in the accuracy of the estimation.

### Mechanism 2
- Claim: Selecting only the first occurrence of each token for score calculation mitigates the effect of repeated tokens in a text.
- Mechanism: By only considering the calibrated token probabilities corresponding to the first occurrence of each token in a text, the method avoids overemphasizing tokens that appear multiple times, which could artificially inflate their importance.
- Core assumption: LLMs predict higher probabilities for tokens in subsequent occurrences since the model has seen the word earlier in the text.
- Evidence anchors:
  - [section 3.4]: "Therefore, we adopt a simple countermeasure that only uses αi corresponding to the first occurrence of xi in x to calculate the final score β"
  - [abstract]: "Based on the calibrated token probabilities, we derive a score for pretraining data detection"

### Mechanism 3
- Claim: Limiting the upper bound of calibrated token probabilities prevents the final score from being dominated by a few tokens.
- Mechanism: By capping the maximum value of each calibrated token probability, the method ensures that no single token can disproportionately influence the final detection score.
- Core assumption: Without this limit, a few tokens with very high calibrated probabilities could skew the overall score, making it less reliable.
- Evidence anchors:
  - [section 3.4]: "We set a hyperparameter a to control the upper bound of αi, preventing the final score from being dominated by a few tokens"
  - [section 5.3]: "The hyperparameter a... preventing the final score from being dominated by a few tokens"

## Foundational Learning

- Concept: Divergence-from-randomness theory
  - Why needed here: Provides the theoretical foundation for calibrating token probabilities based on the divergence between the token probability distribution and the token frequency distribution.
  - Quick check question: How does the divergence-from-randomness theory apply to pretraining data detection in LLMs?

- Concept: Cross-entropy as a measure of divergence
  - Why needed here: Cross-entropy is used to quantify the divergence between the token probability distribution and the token frequency distribution, which forms the basis of the calibration process.
  - Quick check question: Why is cross-entropy an appropriate measure of divergence for this application?

- Concept: Laplace smoothing
  - Why needed here: Addresses the zero probability problem when a token does not occur in the reference corpus, ensuring that all tokens have a non-zero probability.
  - Quick check question: What is the purpose of Laplace smoothing in the context of token frequency distribution estimation?

## Architecture Onboarding

- Component map: Input text -> LLM token probabilities -> Reference corpus frequency distribution -> Cross-entropy calculation -> Calibrated token probabilities -> Detection score -> Threshold application
- Critical path:
  1. Obtain token probabilities from the LLM
  2. Estimate token frequency distribution from the reference corpus
  3. Calculate the divergence (cross-entropy) between the two distributions
  4. Calibrate token probabilities based on the divergence
  5. Aggregate calibrated probabilities to derive a detection score
  6. Apply threshold to determine if the text was part of the pretraining corpus
- Design tradeoffs: Using a reference corpus introduces uncertainty in the accuracy of the token frequency distribution estimation. Selecting only the first occurrence of each token may discard valuable information from repeated tokens. Limiting the upper bound of calibrated probabilities may reduce the method's sensitivity to informative tokens.
- Failure signatures: Poor detection performance when the reference corpus does not accurately represent the LLM's pretraining data distribution. Over-sensitivity to certain tokens if the upper bound limit is set too high. Under-sensitivity to informative tokens if the upper bound limit is set too low.
- First 3 experiments:
  1. Evaluate the impact of different reference corpora on detection performance.
  2. Investigate the effect of varying the upper bound limit on calibrated token probabilities.
  3. Assess the contribution of selecting only the first occurrence of each token to the overall detection accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DC-PDD scale with increasing model size beyond 20 billion parameters?
- Basis in paper: [inferred] The paper notes that their research primarily focused on models with up to 20 billion parameters due to hardware constraints and suggests that further studies using larger-scale models would be essential to validate the effectiveness of DC-PDD in scenarios involving larger models.
- Why unresolved: The authors explicitly state that their research was limited by hardware constraints, preventing them from testing on larger models. This leaves open the question of whether the method's performance continues to improve with even larger models, plateaus, or potentially degrades.
- What evidence would resolve it: Conducting experiments with DC-PDD on language models with parameters significantly larger than 20 billion (e.g., 30B, 50B, 100B+) and comparing the AUC and TPR@5%FPR scores to those obtained on smaller models would provide concrete evidence of how performance scales.

### Open Question 2
- Question: What is the optimal hyperparameter 'a' setting for DC-PDD across different model types and benchmarks, and can a more flexible method for setting 'a' be developed?
- Basis in paper: [explicit] The paper discusses the impact of the hyperparameter 'a' on detection performance, showing that different models and benchmarks have different optimal 'a' settings. The authors note that the optimal 'a' setting varies and recommend 0.01 as a general setting, but suggest that future work should explore more flexible methods for setting 'a' to achieve better performance.
- Why unresolved: While the paper identifies that 'a' significantly impacts performance and provides some guidance on its setting, it does not offer a systematic method for determining the optimal 'a' for a new model or dataset. The authors acknowledge this as a limitation and suggest it as future work.
- What evidence would resolve it: Developing and validating a method that automatically determines the optimal 'a' value for DC-PDD based on characteristics of the target model and text to be detected (e.g., model architecture, vocabulary size, text length, domain) would resolve this question. This could involve training a meta-model or deriving a heuristic based on empirical results across diverse models and datasets.

### Open Question 3
- Question: How does the choice of reference corpus affect the performance of DC-PDD, and what are the characteristics of an ideal reference corpus?
- Basis in paper: [explicit] The paper investigates the effect of different reference corpora on DC-PDD's performance, finding that the performance does not exhibit significant differences across various reference corpora in terms of scale and domain. However, the authors note that the similarity between the reference corpus and the LLM's pretraining corpus remains uncertain.
- Why unresolved: While the paper shows that DC-PDD is not highly sensitive to the choice of reference corpus, it does not provide a clear understanding of what makes a reference corpus "good" or how to select one optimally. The uncertainty about the similarity between the reference corpus and the pretraining corpus suggests there might be room for improvement.
- What evidence would resolve it: Conducting a comprehensive study comparing DC-PDD's performance using reference corpora with varying degrees of similarity to the LLM's pretraining corpus (if known or estimated) would provide insights. Additionally, exploring methods to automatically select or construct a reference corpus that maximizes DC-PDD's performance for a given LLM and detection task would be valuable.

## Limitations
- The method relies on reference corpus quality and representativeness, introducing uncertainty since the actual pretraining corpus is not accessible.
- The assumption that high-frequency tokens receive higher probabilities may not hold for all LLMs or specialized domains.
- The optimal hyperparameter 'a' varies across models and benchmarks, requiring further research for automatic determination.

## Confidence
- **High Confidence**: The mechanism of using cross-entropy as a measure of divergence between token probability distributions is well-established and theoretically sound. The experiments demonstrate significant improvements over state-of-the-art baselines, providing strong empirical support for the method's effectiveness.
- **Medium Confidence**: The choice of selecting only the first occurrence of each token for score calculation is a reasonable heuristic to mitigate the effect of repeated tokens. However, the impact of this design choice on the overall detection accuracy is not thoroughly explored, and there may be scenarios where repeated tokens carry valuable information.
- **Medium Confidence**: Limiting the upper bound of calibrated token probabilities is a sensible design choice to prevent the final score from being dominated by a few tokens. However, the optimal value for this hyperparameter is not discussed, and the choice may affect the method's sensitivity to informative tokens.

## Next Checks
1. Evaluate the impact of different reference corpora: Conduct experiments using multiple reference corpora with varying sizes, domains, and statistical properties to assess the robustness of the method to different token frequency distribution estimates.
2. Investigate the effect of varying the upper bound limit: Systematically vary the upper bound hyperparameter and evaluate its impact on the detection performance across different datasets and LLMs. Identify the optimal value for this hyperparameter and analyze its sensitivity.
3. Assess the contribution of selecting only the first occurrence of each token: Compare the proposed method with a variant that includes all token occurrences in the score calculation. Analyze the performance differences and identify scenarios where including repeated tokens may be beneficial or detrimental to the detection accuracy.