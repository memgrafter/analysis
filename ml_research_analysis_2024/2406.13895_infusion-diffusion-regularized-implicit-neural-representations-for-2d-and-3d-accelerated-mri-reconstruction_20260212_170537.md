---
ver: rpa2
title: 'INFusion: Diffusion Regularized Implicit Neural Representations for 2D and
  3D accelerated MRI reconstruction'
arxiv_id: '2406.13895'
source_url: https://arxiv.org/abs/2406.13895
tags:
- diffusion
- regularization
- image
- neural
- inrs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes INFusion, a technique that regularizes the
  optimization of implicit neural representations (INRs) from under-sampled MR measurements
  with pre-trained diffusion models for improved image reconstruction. Previous work
  has applied unlearned regularization priors during INR training and has been limited
  to 2D or low-resolution 3D acquisitions.
---

# INFusion: Diffusion Regularized Implicit Neural Representations for 2D and 3D accelerated MRI reconstruction

## Quick Facts
- arXiv ID: 2406.13895
- Source URL: https://arxiv.org/abs/2406.13895
- Authors: Yamin Arefeen; Brett Levac; Zach Stoebner; Jonathan Tamir
- Reference count: 27
- Key outcome: INFusion improves INR training for MRI reconstruction by using pre-trained diffusion models as regularization priors, achieving better results than standard L1-Wavelet CS and unregularized INRs on both 2D (192×192, R=4-9) and 3D (256×256×80) accelerated MRI data.

## Executive Summary
INFusion introduces a novel approach to MRI reconstruction by combining implicit neural representations (INRs) with diffusion model regularization. The method leverages pre-trained diffusion models as learned priors to guide the optimization of INRs from under-sampled measurements. This hybrid approach addresses the limitations of previous methods that relied on unlearned regularization priors and were restricted to 2D or low-resolution 3D acquisitions. The key innovation is the use of stochastic slice-based diffusion regularization, which reduces computational burden while maintaining spatial regularization benefits for 3D reconstruction.

## Method Summary
INFusion trains INRs to reconstruct MRI images from under-sampled k-space measurements by adding a diffusion model regularization term to the standard data consistency loss. For 2D data, random 2D slices from the INR are corrupted with noise, passed through a pre-trained diffusion model, and compared to the INR output using perceptual loss. For 3D data, the method encodes only x-y coordinates and outputs discretized values for the third dimension, enabling efficient application of 2D diffusion models to large 3D volumes through random slice sampling. The approach uses fully-connected neural networks with Fourier feature encoding and trains on datasets like fastMRI and SKM-TEA with retrospective under-sampling.

## Key Results
- INFusion achieves lower NRMSE than standard L1-Wavelet compressed sensing and unregularized INRs on 2D T2-weighted brain MRI (R=4-9 acceleration)
- The method successfully scales to 3D reconstruction on 256×256×80 knee data, demonstrating feasibility beyond the typical 192×192×16 resolution limit
- Qualitative improvements show better preservation of fine anatomical details compared to baseline methods, particularly in highly under-sampled regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion model regularization guides INR optimization toward MRI-image-consistent solutions
- Mechanism: Pre-trained diffusion models act as learned priors, generating image samples from noisy INR slices. Perceptual loss between these samples and INR outputs pushes the INR toward image distributions matching the prior
- Core assumption: The diffusion model's prior aligns with the target MRI data distribution despite measurement artifacts
- Evidence anchors: Abstract and method sections describe using pre-trained MRI diffusion models for INR regularization
- Break condition: Diffusion model trained on different acquisition parameters may bias results away from true anatomy

### Mechanism 2
- Claim: Stochastic slice-based diffusion regularization reduces computational burden while maintaining spatial regularization benefits
- Mechanism: Instead of full 3D diffusion model evaluation, random 2D slices from the INR are sampled and regularized at each iteration using perceptual loss
- Core assumption: Random 2D slices capture sufficient spatial diversity to guide 3D INR training effectively
- Evidence anchors: Method section describes stochastic regularizer querying diffusion model at random spatial coordinates
- Break condition: Highly anisotropic under-sampling or significant z-axis anatomical variation may cause slice sampling to miss critical regularization signals

### Mechanism 3
- Claim: Hybrid 3D approach enables application of 2D diffusion models to large-scale 3D MRI reconstruction
- Mechanism: INR encodes only x-y coordinates and outputs discretized values for z-dimension, allowing 2D diffusion models to regularize 3D volumes through slice sampling
- Core assumption: 2D diffusion models generalize to under-sampled 3D data even when trained on different z-sampling patterns
- Evidence anchors: Method section describes encoding only x-y coordinates for 3D problems
- Break condition: Significant z-axis anatomical variation not captured by 2D slice regularization may degrade reconstruction quality

## Foundational Learning

- Concept: Implicit Neural Representations (INRs)
  - Why needed here: INRs provide continuous function approximation to MRI images, enabling scan-specific reconstruction without fully-sampled reference data
  - Quick check question: How does an INR differ from a voxel grid in representing continuous signals?

- Concept: Diffusion Models as Learned Priors
  - Why needed here: Diffusion models learn powerful image priors decoupled from measurement model, enabling regularization without handcrafted constraints
  - Quick check question: What is the key difference between diffusion model priors and traditional regularization like sparsity or total variation?

- Concept: Perceptual Loss for Image Comparison
  - Why needed here: Perceptual loss measures high-level image similarity rather than pixel-wise differences, aligning better with human perception and MRI diagnostic quality
  - Quick check question: Why might perceptual loss be more appropriate than MSE for comparing MRI reconstructions?

## Architecture Onboarding

- Component map: INR (fully-connected network with ReLU + Fourier features) -> Data consistency loss -> Random slice sampling -> Diffusion model (U-Net) -> Perceptual loss -> Combined loss -> Backpropagation to INR weights

- Critical path:
  1. Initialize INR weights
  2. Sample random 2D slices from INR
  3. Compute data consistency loss with k-space measurements
  4. Add noise to slices, run diffusion sampling
  5. Compute perceptual loss between INR output and diffusion prior sample
  6. Backpropagate combined loss to update INR weights

- Design tradeoffs:
  - Memory vs. regularization: Sampling random slices reduces memory but may miss some spatial context
  - Model complexity vs. generalization: Larger diffusion models may capture better priors but require more training data
  - Noise schedule vs. stability: Adaptive noise levels help balance exploration and convergence

- Failure signatures:
  - Reconstruction artifacts concentrated in specific slice orientations
  - INR collapse to mean image when diffusion loss dominates
  - Training instability when noise levels are too high or diffusion steps too few

- First 3 experiments:
  1. Implement INR with Fourier features and test on small 2D dataset
  2. Train diffusion model on 2D MRI slices and validate generation quality
  3. Combine INR and diffusion model with synthetic data to test regularization effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational efficiency of INFusion compare to other diffusion-based regularization methods for MRI reconstruction when scaling to larger 3D volumes?
- Basis in paper: The paper mentions that full 3D diffusion models are computationally prohibitive and that INFusion uses 2D diffusion models on random slices to enable 3D reconstruction, but does not provide a direct comparison of computational efficiency
- Why unresolved: The paper does not provide detailed computational benchmarks comparing INFusion to other methods, especially when scaling to larger 3D volumes
- What evidence would resolve it: A comprehensive study comparing the computational time and resource usage of INFusion against other diffusion-based regularization methods across various 3D MRI datasets of increasing size

### Open Question 2
- Question: Can INFusion be effectively applied to other MRI modalities beyond T2-weighted brain and knee imaging, such as functional MRI (fMRI) or diffusion MRI?
- Basis in paper: The paper mentions that INFusion can be applied to data from any 2D orientation, but only demonstrates results on T2-weighted brain and knee imaging
- Why unresolved: The paper does not explore the effectiveness of INFusion on other MRI modalities, leaving open the question of its generalizability
- What evidence would resolve it: Experimental results showing the performance of INFusion on various MRI modalities, including fMRI and diffusion MRI, compared to existing reconstruction methods

### Open Question 3
- Question: How does the choice of diffusion model architecture and training data impact the reconstruction quality of INFusion in MRI?
- Basis in paper: The paper mentions that two different diffusion models were trained on T2-weighted brain and knee slices, but does not explore the impact of different architectures or training data on reconstruction quality
- Why unresolved: The paper does not provide a systematic study on how the choice of diffusion model architecture or training data affects the performance of INFusion
- What evidence would resolve it: A study comparing the performance of INFusion using different diffusion model architectures and training datasets, with a focus on reconstruction quality and generalizability

## Limitations
- Scalability concerns for full-volume 3D reconstruction where slice-wise regularization may miss anatomically important features
- Sensitivity to domain shift between pre-trained diffusion model data and target acquisition parameters
- Limited evaluation of diagnostic utility beyond NRMSE and qualitative comparisons without radiologist assessment

## Confidence

**High Confidence**: The mechanism of using diffusion models as learned priors for INR regularization is well-supported by mathematical formulation and 2D empirical results. The hybrid 3D approach encoding only x-y coordinates is technically sound and demonstrably reduces computational burden.

**Medium Confidence**: Effectiveness of stochastic slice-based regularization for large-scale 3D MRI reconstruction is supported by the 256×256×80 knee experiment, but limited resolution increase leaves questions about scalability to full clinical resolutions.

**Low Confidence**: Clinical relevance of perceptual loss improvements for diagnostic quality remains uncertain, as evaluation focuses on NRMSE and qualitative comparisons without radiologist assessment or task-based performance metrics.

## Next Checks
1. **Cross-Contrast Generalization**: Test INFusion on MRI data with different contrast mechanisms (e.g., T1-weighted, FLAIR) than the diffusion model training data to quantify domain shift effects.

2. **Full-Resolution 3D Validation**: Scale the method to full clinical resolution (e.g., 320×320×160) on multi-coil data to evaluate memory efficiency and regularization effectiveness at clinically relevant scales.

3. **Diagnostic Quality Assessment**: Conduct radiologist reader study comparing INFusion reconstructions against standard CS and INR baselines on clinically relevant tasks (lesion detection, segmentation) to validate perceptual loss improvements translate to diagnostic utility.