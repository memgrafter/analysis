---
ver: rpa2
title: Language Models can Exploit Cross-Task In-context Learning for Data-Scarce
  Novel Tasks
arxiv_id: '2405.10548'
source_url: https://arxiv.org/abs/2405.10548
tags:
- task
- question
- source
- target
- cross-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models can generalize
  from labeled examples of predefined tasks to novel tasks. Drawing inspiration from
  biological neurons and the mechanistic interpretation of the Transformer architecture,
  the authors design a cross-task prompting setup with three LLMs and show that LLMs
  achieve significant performance improvements despite no examples from the target
  task in the context.
---

# Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks

## Quick Facts
- arXiv ID: 2405.10548
- Source URL: https://arxiv.org/abs/2405.10548
- Reference count: 30
- Primary result: Cross-task prompting improves performance on novel tasks by 107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 over zero-shot prompting

## Executive Summary
This paper investigates whether large language models can generalize from labeled examples of predefined tasks to novel tasks through cross-task in-context learning. Drawing inspiration from biological neurons and mechanistic interpretation of Transformers, the authors design a cross-task prompting setup that leverages semantically similar examples from source tasks to improve performance on target tasks with no direct examples. The method demonstrates significant performance improvements across three LLMs, showing that LLMs can exploit shared latent representations and semantic alignment across tasks to achieve results comparable to standard in-context learning.

## Method Summary
The method involves constructing cross-task prompts by selecting semantically similar examples from a source task dataset and pairing them with the target task definition. For each target input, one semantically similar source example is selected using Sentence-BERT embeddings, and the model is prompted with both task definitions and the source-target pair. The approach also explores pseudo-label generation by applying cross-task prompting to unlabeled target data with majority voting, then using these pseudo-labeled examples in standard few-shot prompting. Activation analysis is performed by computing cosine similarity between hidden states of task definitions across layers to understand information transfer mechanisms.

## Key Results
- Cross-task prompting leads to a remarkable performance boost of 107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 on average over zero-shot prompting
- Cross-task prompting performs comparable to standard in-context learning with examples from the target task
- Pseudo-label generation using cross-task prompting improves performance when used in standard few-shot prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-task examples can activate shared latent representations across different tasks in LLMs.
- Mechanism: If two tasks share similar information pathways, examples from one task can prime the model's internal representations for the other task, improving performance even without direct examples.
- Core assumption: Transformer architectures can learn transferable task-specific circuits that are reused across related tasks.
- Evidence anchors:
  - [abstract] "drawing inspiration from biological neurons and the mechanistic interpretation of the Transformer architecture, we explore the potential for information sharing across tasks"
  - [section] "One can argue that if information pathways necessary to solve a novel task are similar to those corresponding to some different task from a task library, an LLM may gather useful information across tasks"
  - [corpus] "Average neighbor FMR=0.506" - weak evidence for task similarity detection in corpus
- Break condition: Tasks have completely disjoint label spaces or require fundamentally different reasoning mechanisms.

### Mechanism 2
- Claim: Semantic alignment between source and target examples improves cross-task transfer.
- Mechanism: By selecting semantically similar examples from the source task, the model can more easily map the demonstration to the target task's context, even when label spaces differ.
- Core assumption: The model's attention mechanism can bridge semantic similarity across task boundaries.
- Evidence anchors:
  - [abstract] "Cross-task prompting leads to a remarkable performance boost... and performs comparable to standard in-context learning"
  - [section] "Drawing inspiration from these works, we set up the cross-task prompting regime with sampling examples xs from the source task dataset Ds that are semantically similar to the target input xt"
  - [corpus] "Enhancing Cross-task Transfer of Large Language Models via Activation Steering" - suggests activation-based similarity methods
- Break condition: Semantic similarity measure fails to capture task-relevant features or introduces noise.

### Mechanism 3
- Claim: Pseudo-label generation using cross-task prompting can create high-quality training examples for data-scarce tasks.
- Mechanism: Cross-task prompting with majority voting generates noisy but useful pseudo-labels, which when used in standard few-shot prompting, improve performance.
- Core assumption: Model's internal knowledge can be leveraged to generate reasonable labels for unlabeled data in the target domain.
- Evidence anchors:
  - [abstract] "The effectiveness of generating pseudo-labels for in-task examples is demonstrated"
  - [section] "Given a small unlabeled dataset Dpl ⊂ Dt, we assign a pseudo-label to the example using cross-task prompting"
  - [corpus] "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning" - supports pseudo-labeling approaches
- Break condition: Cross-task predictions are too noisy or biased toward source task label space.

## Foundational Learning

- Concept: In-context learning (ICL) without gradient updates
  - Why needed here: Understanding that LLMs can learn from demonstrations in the prompt is fundamental to the cross-task setup
  - Quick check question: What is the key difference between zero-shot and few-shot prompting?

- Concept: Transformer attention mechanisms and layer-wise computation
  - Why needed here: The paper analyzes how cross-task examples affect activations at different layers
  - Quick check question: At which layers do cross-task examples begin to influence the model's output?

- Concept: Semantic similarity measures and embedding spaces
  - Why needed here: Cross-task examples are selected based on semantic similarity between source and target inputs
  - Quick check question: What method does the paper use to extract semantically similar examples?

## Architecture Onboarding

- Component map:
  - Task definitions → Semantic similarity matching → Example selection → Prompt assembly → Model evaluation → Activation analysis → Pseudo-label generation

- Critical path:
  1. Select semantically similar source example for target input
  2. Construct cross-task prompt with source and target task definitions
  3. Evaluate model performance on target task
  4. Analyze activation patterns and error types

- Design tradeoffs:
  - Single source example vs multiple examples (performance degrades with more examples)
  - Semantic similarity vs random selection (semantic similarity significantly better)
  - Base models vs instruction-tuned models (base models show larger gains)

- Failure signatures:
  - Zero accuracy when random examples are used instead of semantically similar ones
  - Performance drops when source and target tasks have completely disjoint label spaces
  - Model replicates source task label space instead of adapting to target task

- First 3 experiments:
  1. Implement cross-task prompting with one semantically similar source example and measure performance improvement over zero-shot
  2. Compare semantic similarity selection vs random selection on a subset of tasks
  3. Test pseudo-label generation by applying cross-task prompting to unlabeled target data and using results for standard few-shot prompting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scale of task novelty affect the performance of cross-task prompting in LLMs?
- Basis in paper: [explicit] The authors note that the similarity between source and target tasks plays an important role in performance, and in real-world scenarios with extreme task novelty, the method may fail to provide suitable performance.
- Why unresolved: The study primarily focused on empirical viability with a limited set of source and target tasks, not exploring the boundaries of task novelty.
- What evidence would resolve it: Systematic experiments varying the semantic and functional distance between source and target tasks, measuring performance degradation as novelty increases.

### Open Question 2
- Question: What are the specific neural circuit mechanisms that enable cross-task information transfer in LLMs?
- Basis in paper: [explicit] The authors analyze model activations and find a U-shaped pattern in correlation values, suggesting task-specific computation in initial layers and cross-task signal transfer in later layers. However, the exact mechanisms remain unclear.
- Why unresolved: The analysis is introductory and correlational, not establishing causal mechanisms or specific circuit elements responsible for transfer.
- What evidence would resolve it: Mechanistic interpretability studies identifying specific attention patterns, feature representations, or circuit elements that facilitate cross-task generalization.

### Open Question 3
- Question: Can cross-task prompting be combined with other few-shot learning techniques to further improve performance on novel tasks?
- Basis in paper: [explicit] The authors explore combining cross-task prompting with in-task examples and pseudo-labeling, showing improvements. However, they don't investigate combinations with other techniques like chain-of-thought prompting or meta-learning.
- Why unresolved: The study focuses on cross-task prompting in isolation and doesn't explore synergies with other few-shot learning methods.
- What evidence would resolve it: Experiments combining cross-task prompting with techniques like chain-of-thought, meta-learning, or retrieval-augmented generation, measuring performance gains over individual methods.

## Limitations

- The study focuses on a limited set of 10 source tasks and 5 target tasks, which may not capture the full diversity of potential task relationships
- The semantic similarity selection method relies on Sentence-BERT embeddings, but specific embedding models and threshold parameters are not fully specified
- The paper does not extensively explore cases where cross-task transfer fails, limiting understanding of failure conditions
- Pseudo-label generation may inherit biases from the source task when label spaces differ significantly

## Confidence

- High Confidence: The core finding that semantically similar cross-task examples improve performance over zero-shot prompting is well-supported by the experimental results across three different model sizes
- Medium Confidence: The claim that cross-task prompting performs "comparable to standard in-context learning" is supported but could benefit from more direct comparisons across a wider range of task pairs and example quantities
- Medium Confidence: The pseudo-label generation effectiveness is demonstrated but the quality of generated labels and their impact on downstream performance could be more thoroughly evaluated

## Next Checks

1. **Task Diversity Validation**: Test cross-task prompting across a broader range of task types (e.g., visual reasoning, mathematical problem-solving) to determine the limits of cross-task transferability and identify which task characteristics enable or hinder transfer

2. **Similarity Method Comparison**: Systematically compare different semantic similarity methods (e.g., contrastive learning embeddings, task definition embeddings) and their impact on cross-task performance to optimize example selection strategies

3. **Failure Mode Analysis**: Conduct controlled experiments where cross-task transfer is expected to fail (e.g., completely disjoint label spaces, incompatible reasoning types) to better characterize the boundaries of cross-task in-context learning capabilities