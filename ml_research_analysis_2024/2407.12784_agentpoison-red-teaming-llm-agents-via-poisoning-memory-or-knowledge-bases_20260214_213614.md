---
ver: rpa2
title: 'AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases'
arxiv_id: '2407.12784'
source_url: https://arxiv.org/abs/2407.12784
tags:
- agent
- trigger
- poison
- arxiv
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AGENT POISON, the first red-teaming framework
  for attacking RAG-based LLM agents by poisoning their long-term memory or knowledge
  base. The key innovation is a constrained optimization scheme that generates stealthy
  backdoor triggers mapping triggered queries to a unique, compact region in the embedding
  space, ensuring high retrieval of malicious demonstrations while preserving benign
  performance.
---

# AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases

## Quick Facts
- arXiv ID: 2407.12784
- Source URL: https://arxiv.org/abs/2407.12784
- Authors: Zhaorun Chen; Zhen Xiang; Chaowei Xiao; Dawn Song; Bo Li
- Reference count: 40
- Primary result: First red-teaming framework for attacking RAG-based LLM agents via poisoning memory/knowledge bases

## Executive Summary
This paper introduces AGENT POISON, a novel red-teaming framework that attacks RAG-based LLM agents by poisoning their long-term memory or knowledge base. The key innovation is a constrained optimization scheme that generates stealthy backdoor triggers mapping triggered queries to a unique, compact region in the embedding space. This ensures high retrieval of malicious demonstrations while preserving benign performance. The approach is evaluated on three real-world agents (autonomous driving, QA, healthcare) and demonstrates >80% attack success rate with <1% performance drop using <0.1% poisoned data.

## Method Summary
AGENT POISON generates backdoor triggers through a constrained optimization framework that balances four key objectives: coherence, uniqueness, compactness, and target action generation. The optimization uses gradient-guided beam search to find discrete token sequences that map triggered queries to a unique cluster in the embedding space while maintaining semantic coherence. The method requires poisoned query-solution pairs as input and optimizes triggers to ensure high retrieval of malicious demonstrations during inference. The attack preserves benign performance by constraining the trigger generation process to avoid disrupting normal agent behavior.

## Key Results
- Achieves >80% attack success rate with <1% performance drop using <0.1% poisoned data
- Optimized triggers transfer well across different embedders with similar training strategies
- Outperforms four baseline methods on three real-world agents (autonomous driving, QA, healthcare)
- Demonstrates resilience to various perturbations and defense mechanisms

## Why This Works (Mechanism)

### Mechanism 1
The trigger optimization forces triggered queries into a unique, compact region in the embedding space. The uniqueness loss pushes triggered query embeddings away from benign query clusters, while the compactness loss pulls them closer together. This design relies on dense retrievers using cosine similarity in a shared embedding space for both queries and documents.

### Mechanism 2
The trigger achieves both high retrieval of malicious demos and high in-context coherence. The constrained optimization balances target action generation loss with coherence loss, ensuring the trigger doesn't break semantic flow. This works because the LLM backbone uses retrieved demonstrations as in-context examples to generate actions.

### Mechanism 3
The optimized trigger transfers well across different embedders and is resilient to perturbations. The unique, compact embedding region is semantically distinctive, so other embedders trained on similar data will map similar queries similarly. This assumes different embedders share similar training data distribution and embed similar semantic meanings in comparable ways.

## Foundational Learning

- Concept: Dense Retrieval and Embedding Space Mapping
  - Why needed here: The attack relies on the embedder mapping both queries and documents into the same space for similarity comparison
  - Quick check question: In a dense retriever, are queries and documents encoded by the same model? (Yes)

- Concept: In-Context Learning and Demonstration Influence
  - Why needed here: The malicious demos are retrieved and placed in the prompt to influence the LLM's action generation
  - Quick check question: Does the LLM generate actions based solely on the current query, or does it also consider retrieved demonstrations? (B)

- Concept: Constrained Optimization with Discrete Tokens
  - Why needed here: The trigger is a sequence of discrete tokens, so optimization must handle discrete search under non-differentiable constraints
  - Quick check question: Can gradient descent directly optimize discrete token sequences? (No)

## Architecture Onboarding

- Component map:
  Query → Embedder → Similarity Ranking → Retrieve K demos → LLM backbone (with demos) → Action

- Critical path:
  Trigger optimization → Embedding uniqueness/compactness → Retrieval success → In-context action generation → Environmental impact

- Design tradeoffs:
  More trigger tokens → Higher retrieval accuracy but lower stealthiness
  Stronger coherence constraint → Harder to optimize but more evasive
  Larger beam size → Better search but higher compute

- Failure signatures:
  Low ASR-r but high ACC → Trigger not pulling demos enough
  High ASR-r but low ASR-t → Demos retrieved but not influencing LLM
  High ASR-t but low transferability → Embeddings too embedder-specific

- First 3 experiments:
  1. Run AGENT POISON on a simple QA agent with a known embedder; check if trigger retrieval improves over random
  2. Vary trigger length (1-6 tokens) and measure ASR-r vs ACC tradeoff
  3. Test trigger transferability by optimizing on one embedder and testing on another

## Open Questions the Paper Calls Out

### Open Question 1
How does the unique and compact embedding region design in AGENT POISON scale to different RAG architectures beyond dense retrievers? The current experiments only evaluate on dense retrievers. Testing on sparse retrievers or hybrid retrieval methods would reveal scalability limitations.

### Open Question 2
What are the long-term effects of AGENT POISON on the performance of LLM agents when deployed in dynamic environments with evolving knowledge bases? Real-world applications often involve dynamic knowledge bases that evolve over time, which could affect the persistence and effectiveness of the backdoor triggers.

### Open Question 3
How resilient is AGENT POISON to advanced defensive strategies that involve continuous monitoring and anomaly detection in the retrieval process? As defensive strategies become more sophisticated, incorporating real-time anomaly detection, the effectiveness of AGENT POISON might diminish.

### Open Question 4
What are the ethical implications of deploying AGENT POISON for red-teaming in sensitive domains such as healthcare and autonomous driving? Red-teaming in sensitive domains can have significant ethical implications, including potential harm if the techniques are misused or if the red-teaming process itself introduces vulnerabilities.

## Limitations

- Relies heavily on the assumption that RAG-based agents use dense retrievers with shared embedding spaces
- Effectiveness may diminish with significantly larger knowledge bases beyond the tested scale
- Transferability claims may not extend to embedders with fundamentally different architectures
- Does not address scenarios where the knowledge base is frequently updated or expanded

## Confidence

**High Confidence Claims:**
- The constrained optimization approach effectively generates triggers with high attack success rates while maintaining benign performance
- The attack works against the three evaluated real-world agents
- Optimized triggers show better performance than baseline methods in controlled experiments

**Medium Confidence Claims:**
- The uniqueness and compactness design principles are the key reasons for high retrieval success
- Triggers transfer well across embedders with similar training strategies
- The attack is resilient to various perturbation types and defense mechanisms

**Low Confidence Claims:**
- The attack will work equally well on agents with hybrid or sparse retrieval systems
- The <0.1% poisoning rate remains effective for agents with significantly larger knowledge bases
- The constrained optimization approach generalizes without significant hyperparameter tuning across different agent types

## Next Checks

1. **Cross-Retriever Validation**: Test the attack on agents using sparse or hybrid retrieval methods to assess generalizability beyond dense retrievers.

2. **Knowledge Base Scaling Test**: Evaluate the attack's effectiveness on agents with progressively larger knowledge bases (10x, 100x current size) to determine if the claimed <0.1% poisoning rate remains sufficient.

3. **Defense Transferability Assessment**: Implement and test the attack against a broader range of defense mechanisms, including those not mentioned in the paper, to assess the robustness of the optimized triggers against adaptive defenses.