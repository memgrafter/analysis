---
ver: rpa2
title: 'It Takes Two: On the Seamlessness between Reward and Policy Model in RLHF'
arxiv_id: '2406.07971'
source_url: https://arxiv.org/abs/2406.07971
tags:
- rlhf
- seam
- data
- uni00000011
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the discrepancy between reward models (RMs)
  and policy models (PMs) in RLHF training, revealing a 35% mismatch rate with human
  preferences. The authors introduce the concept of "seamlessness" to measure the
  alignment between RM and PM judgments and propose SEAM, an automatic metric that
  quantifies discrepancies in RLHF data.
---

# It Takes Two: On the Seamlessness between Reward and Policy Model in RLHF

## Quick Facts
- arXiv ID: 2406.07971
- Source URL: https://arxiv.org/abs/2406.07971
- Authors: Taiming Lu; Lingfeng Shen; Xinyu Yang; Weiting Tan; Beidi Chen; Huaxiu Yao
- Reference count: 40
- One-line primary result: This paper examines the discrepancy between reward models (RMs) and policy models (PMs) in RLHF training, revealing a 35% mismatch rate with human preferences.

## Executive Summary
This paper addresses a critical issue in Reinforcement Learning from Human Feedback (RLHF): the discrepancy between reward models and policy models that leads to suboptimal performance. The authors introduce "seamlessness" as a metric to measure alignment between RM and PM judgments, revealing a 35% mismatch rate with human preferences. They propose SEAM, an automatic metric that quantifies these discrepancies, and demonstrate that using SEAM for data selection improves RLHF performance by 4.5%, while SEAM-guided model augmentation achieves 4% improvement over standard methods.

## Method Summary
The paper introduces SEAM (Seamlessness metric) to quantify discrepancies between reward and policy models in RLHF. SEAM is computed using three variants: Contrast (retrieval-based), GPT (generation-based), and Adv (adversarial). The method involves training base LLaMa2-7B models for PM and RM, then computing SEAM scores for RL training data to identify low-seamless data points. The authors apply SEAM for two purposes: data selection (filtering low-seamless data at 20% rate) and model augmentation (targeting model weaknesses). The framework is evaluated on StackExchange datasets, with GPT-4 serving as an oracle for validation.

## Key Results
- RMs fail to assign proper scores to PM responses, resulting in a 35% mismatch rate with human preferences
- SEAM-guided data selection improves RLHF performance by 4.5% compared to standard methods
- SEAM-guided model augmentation achieves 4% improvement over standard augmentation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing data that causes discrepancies between RM and PM improves RLHF performance by reducing noisy gradients.
- Mechanism: When PM responses trigger RM misjudgments, the reward signal contains errors that accumulate during RL training, causing the policy to exploit RM weaknesses rather than align with human preferences.
- Core assumption: The RM's inability to properly score PM-generated responses creates systematic noise in the reward signal that degrades RLHF optimization.
- Evidence anchors:
  - [abstract] "Our analysis shows that RMs fail to assign proper scores to PM responses, resulting in a 35% mismatch rate with human preferences"
  - [section 4.2] "The results, as shown in Figure 4, reveal a mismatch rate of approximately 40%, showing that the RM has some inability to accurately assign scores that reflect the true quality of responses generated by the PM"
  - [corpus] Weak - no direct neighbor papers discussing this specific mechanism
- Break condition: If the RM can be improved to accurately evaluate PM-generated responses, the benefit of filtering low-seamless data would diminish.

### Mechanism 2
- Claim: SEAM effectively identifies data points where PM and RM are misaligned, serving as a diagnostic metric for RLHF training.
- Mechanism: SEAM quantifies the risk that a data sample will cause RM misjudgment when evaluated by the PM, allowing targeted filtering or augmentation of problematic data.
- Core assumption: The likelihood of PM generating a response that hacks the RM is inversely related to the data's "seamlessness" quality.
- Evidence anchors:
  - [abstract] "SEAM quantifies the discrepancies between PM and RM judgments induced by data samples"
  - [section 5.1] "Our concept of 'seamlessness' is proportional to the PM likelihood of a data point that causes discrepancies between the policy and the reward model"
  - [corpus] Weak - neighbors focus on reward model robustness but not this specific diagnostic approach
- Break condition: If the PM and RM become perfectly aligned, SEAM would become unnecessary as all data would have high seamlessness.

### Mechanism 3
- Claim: Targeted model augmentation using SEAM-guided data selection improves both PM and RM by addressing their specific weaknesses.
- Mechanism: By identifying low-seamless data points, SEAM guides the creation of augmented training samples that help PM and RM better handle challenging instruction-response pairs.
- Core assumption: The PM and RM have complementary weaknesses that can be addressed through targeted data augmentation based on SEAM scores.
- Evidence anchors:
  - [abstract] "SEAM-guided model augmentation results in a 4% performance improvement over standard augmentation methods"
  - [section 7.2] "Augmenting the PM and RM with data specifically selected by SEAM demonstrates greater benefits than augmentations using randomly selected RL data"
  - [corpus] Weak - no direct neighbor papers discussing this specific augmentation strategy
- Break condition: If the PM and RM become robust to all instruction types, targeted augmentation would provide diminishing returns.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper's entire framework depends on understanding how RLHF works, including the three-stage process of policy modeling, reward modeling, and RL training.
  - Quick check question: What are the three main stages of RLHF and what role does each model (PM and RM) play in each stage?

- Concept: Reward hacking and model misalignment
  - Why needed here: The paper addresses the specific problem of reward hacking through discrepancies between PM and RM, which is central to understanding the proposed solution.
  - Quick check question: Why does reward hacking occur in RLHF, and how does it relate to the discrepancy between policy and reward models?

- Concept: Contrastive learning and adversarial attacks
  - Why needed here: SEAM uses three variants (Contrast, GPT, Adv) that rely on contrastive instruction retrieval, GPT-generated responses, and adversarial attacks respectively.
  - Quick check question: How do contrastive instruction retrieval and adversarial attacks help identify data points where RM and PM are misaligned?

## Architecture Onboarding

- Component map:
  - StackExchange datasets (SFT, Preference, RL) -> LLaMa2-7B PM and RM -> SEAM metric (Contrast, GPT, Adv variants) -> Data filtering/augmentation -> PPO-based RL training -> GPT-4 evaluation

- Critical path:
  1. Train PM on SFT data
  2. Train RM on preference data
  3. Generate RL training data with PM
  4. Compute SEAM scores for RL data
  5. Filter/augment based on SEAM
  6. Conduct RL training with PPO
  7. Evaluate final RLHF performance

- Design tradeoffs:
  - SEAM GPT vs SEAM Contrast: GPT-4 generation is more expensive but potentially more accurate; retrieval is cheaper but may miss edge cases
  - SEAM Adv vs other variants: Adversarial examples are more likely to hack RM but may be less representative of natural PM outputs
  - Filtering rate: Too aggressive filtering reduces data diversity; too conservative leaves noise in training

- Failure signatures:
  - RLHF performance plateaus despite improving PM/RM quality (saturation phenomenon)
  - Low SEAM scores correlate with poor RLHF outcomes
  - Adversarial examples have much lower PM likelihood than natural responses
  - Overlap in low-SEAM data across different PM/RM combinations suggests systematic issues

- First 3 experiments:
  1. Reproduce saturation phenomenon by training PM/RM at different quality levels and measuring RLHF performance
  2. Implement SEAM Contrast and validate it identifies data points where RM and PM disagree
  3. Test data filtering at different rates (10%, 20%, 30%) and measure impact on RLHF performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the seamlessness metric (SEAM) perform in online RLHF scenarios where models are continuously updated based on real-time user feedback?
- Basis in paper: [inferred] The paper acknowledges that SEAM is currently restricted to offline RLHF experiments and suggests potential adaptation to online RLHF by segmenting it into offline cycles.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis of SEAM's effectiveness in online RLHF scenarios, leaving a gap in understanding its practical applicability.
- What evidence would resolve it: Conducting experiments that apply SEAM in a live RLHF environment, measuring its impact on model alignment and performance over time, would provide concrete evidence of its effectiveness in online settings.

### Open Question 2
- Question: What is the optimal threshold for filtering low-seam data to maximize RLHF performance without losing valuable training information?
- Basis in paper: [explicit] The paper explores varying filter rates and observes that performance peaks at approximately 40% filtering, but sets the filtering rate at 20% for practical reasons.
- Why unresolved: The paper does not provide a definitive method for determining the optimal filtering threshold, and the choice of 20% appears to be based on practical considerations rather than empirical optimization.
- What evidence would resolve it: A systematic study varying the filtering threshold across different RLHF tasks and model architectures, combined with a theoretical analysis of the trade-off between data quantity and quality, would help establish optimal filtering strategies.

### Open Question 3
- Question: How do different adversarial attack methods compare in terms of their impact on SEAM calculations and RLHF performance?
- Basis in paper: [explicit] The paper mentions that SEAM Adv, which uses adversarial attacks, has a significantly lower likelihood in the policy model compared to SEAM GPT and SEAM Contrast, indicating a lack of representativeness.
- Why unresolved: The paper does not explore alternative adversarial attack methods or their potential to generate more representative sentences for SEAM calculations, leaving open the question of whether more effective methods exist.
- What evidence would resolve it: Experimenting with a variety of adversarial attack techniques and evaluating their generated sentences' likelihood in the policy model, as well as their impact on RLHF performance, would provide insights into the most effective methods for SEAM calculations.

## Limitations
- The SEAM metric relies on GPT-4 as an external oracle, introducing potential bias and computational cost
- The study shows weak external validation through corpus analysis with only 8 neighboring papers identified
- Adversarial examples used in SEAM Adv may not fully represent natural PM outputs, potentially limiting generalizability

## Confidence

- High confidence: The identification of the 35% mismatch rate between RM and PM is well-supported by empirical evidence and multiple validation methods.
- Medium confidence: The effectiveness of SEAM-guided data selection (4.5% improvement) is demonstrated but may be sensitive to the specific implementation details not fully disclosed.
- Medium confidence: The SEAM-guided model augmentation results (4% improvement) show promise but require more extensive validation across different model scales and datasets.

## Next Checks

1. Test SEAM's effectiveness across different base model architectures (beyond LLaMa2-7B) to assess generalizability.
2. Conduct ablation studies to determine the optimal filtering rate for SEAM-guided data selection and identify the point of diminishing returns.
3. Implement a human evaluation study to validate GPT-4's role as an oracle and assess potential biases in the evaluation process.