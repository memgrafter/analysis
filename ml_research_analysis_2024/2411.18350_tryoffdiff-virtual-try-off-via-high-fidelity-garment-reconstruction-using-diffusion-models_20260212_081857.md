---
ver: rpa2
title: 'TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using
  Diffusion Models'
arxiv_id: '2411.18350'
source_url: https://arxiv.org/abs/2411.18350
tags:
- image
- garment
- tryoffdiff
- vtoff
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Virtual Try-Off (VTOFF), a task for generating
  standardized garment images from single photos of clothed individuals. Unlike Virtual
  Try-On, which dresses models, VTOFF extracts canonical garment images, requiring
  precise reconstruction of shape, texture, and patterns.
---

# TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models

## Quick Facts
- arXiv ID: 2411.18350
- Source URL: https://arxiv.org/abs/2411.18350
- Reference count: 40
- Key outcome: TryOffDiff achieves DISTS scores of 20.3 and 21.6 on VITON-HD and Dress Code datasets respectively, outperforming adapted baselines for Virtual Try-Off (VTOFF) garment reconstruction

## Executive Summary
TryOffDiff introduces Virtual Try-Off (VTOFF), a novel task focused on extracting standardized garment images from single photos of clothed individuals, distinguishing itself from Virtual Try-On by reconstructing canonical garment representations rather than dressing models. The authors propose a diffusion-based approach that adapts Stable Diffusion v1.4 with SigLIP-based visual conditioning to achieve high-fidelity garment reconstruction. Experiments demonstrate superior performance compared to adapted baselines, with DISTS scores of 20.3 and 21.6 on VITON-HD and Dress Code datasets respectively, while highlighting the inadequacy of traditional metrics like SSIM for evaluating garment reconstruction quality.

## Method Summary
TryOffDiff adapts Stable Diffusion v1.4 with SigLIP-B/16-512 visual conditioning to address the VTOFF task of generating standardized garment images from single photos. The approach uses an adapter architecture with linear and normalization layers to reduce 1024 token embeddings from SigLIP to 77 embeddings suitable for the denoising U-Net. The model is trained using MSE loss for 150k iterations while keeping the SigLIP encoder and VAE frozen. The system focuses on upper-body garments from VITON-HD and Dress Code datasets, with input images padded and resized to 512×512 resolution. Evaluation employs DISTS as the primary metric alongside traditional metrics like SSIM, MS-SSIM, CW-SSIM, LPIPS, FID, FDCLIP, and KID.

## Key Results
- TryOffDiff achieves DISTS scores of 20.3 and 21.6 on VITON-HD and Dress Code datasets respectively
- Model outperforms adapted baselines including StyleGAN2, StyleGAN3, and ImageBind in garment reconstruction quality
- Traditional metrics like SSIM inadequately reflect reconstruction quality, validating DISTS as more reliable for VTOFF evaluation
- Image conditioning mechanism successfully recovers garment details including occluded regions

## Why This Works (Mechanism)
TryOffDiff's success stems from its specialized image conditioning mechanism that bridges the gap between natural photos and standardized product photography. By using SigLIP as a visual feature extractor instead of CLIP's ViT, the model captures more detailed visual representations crucial for garment reconstruction. The adapter architecture effectively transforms high-dimensional SigLIP embeddings into a format compatible with the denoising U-Net while preserving essential garment features. This approach addresses the core challenge of VTOFF: reconstructing garments from naturally-posed photos into standardized, front-facing product images while maintaining texture, patterns, and structural details.

## Foundational Learning
- **Diffusion Models**: Generative models that denoise random noise into structured images through iterative steps; needed for progressive garment reconstruction from noisy inputs
- **Visual Feature Extraction**: Process of converting images into embedding representations; critical for conditioning the diffusion model on garment-specific visual information
- **SigLIP vs CLIP**: SigLIP uses a selective kernel attention mechanism providing better performance for tasks requiring detailed visual representations compared to standard ViT architectures
- **DISTS Metric**: Deep Image Structure and Texture Similarity metric that better captures perceptual garment quality than traditional metrics like SSIM
- **Adapter Architecture**: Lightweight neural modules that transform feature representations between different dimensional spaces; enables integration of external visual features into diffusion models
- **Virtual Try-On vs Try-Off**: VTON dresses models with garments, while VTOFF extracts canonical garment images from clothed individuals; fundamental distinction in task formulation

## Architecture Onboarding

Component map: Input Image -> SigLIP Encoder -> Adapter (Linear+LN) -> Denoising U-Net (Stable Diffusion) -> Output Garment Image

Critical path: The denoising U-Net processing chain is the bottleneck, with 150k training iterations required for convergence on NVIDIA A40 GPU

Design tradeoffs: Uses frozen SigLIP encoder and VAE for stability, sacrificing some fine-tuning capability for faster convergence and reduced training complexity

Failure signatures: Poor reconstruction quality indicates inadequate feature extraction from SigLIP or improper adapter configuration; monitor MSE loss curves for overfitting/underfitting

First experiments:
1. Verify SigLIP feature extraction quality by visualizing intermediate embeddings on sample garment images
2. Test adapter dimension reduction by varying output embedding sizes and measuring reconstruction quality
3. Validate training stability by monitoring loss curves and adjusting learning rate/batch size if needed

## Open Questions the Paper Calls Out
1. **Optimal architecture for image conditioning**: The paper doesn't explore alternative conditioning mechanisms systematically beyond the specific adapter used. Systematic ablation studies comparing different image encoders, adapter architectures, and integration methods would provide evidence for optimal configurations.

2. **Performance with complex garment structures**: The authors note challenges with intricate logos and text but don't provide quantitative analysis of performance degradation with increasing garment complexity. Controlled experiments varying garment complexity levels would reveal failure modes and potential solutions.

3. **Extension to multi-garment scenarios**: The paper mentions potential extensions to handling multiple garments simultaneously but doesn't address technical challenges like inter-garment occlusions or maintaining garment-specific identities. Experiments on multi-garment datasets would validate feasibility.

## Limitations
- Exact preprocessing pipeline beyond padding and resizing to 512×512 resolution remains unspecified
- Adapter architecture specifications are incomplete, particularly the exact configuration of linear and normalization layers
- Evaluation relies on limited baselines adapted from existing models rather than purpose-built VTOFF systems
- Claims about traditional metrics inadequacy require further validation across different garment types and occlusion scenarios

## Confidence
- Task definition and problem formulation: **High** - The distinction between VTOFF and VTON is clearly articulated and well-founded
- Model architecture and training procedure: **Medium** - Core components are specified but critical implementation details are missing
- Performance claims: **Medium** - DISTS scores are reported but limited baseline comparisons and metric validation reduce confidence

## Next Checks
1. Implement and test the exact SigLIP feature extraction and adapter configuration to verify reported performance
2. Conduct ablation studies comparing DISTS with traditional metrics across diverse garment types and occlusion patterns
3. Evaluate TryOffDiff's robustness on held-out test sets from both VITON-HD and Dress Code datasets using multiple quality metrics