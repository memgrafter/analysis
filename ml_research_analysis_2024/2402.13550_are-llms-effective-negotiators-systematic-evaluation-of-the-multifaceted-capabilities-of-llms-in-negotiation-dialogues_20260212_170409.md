---
ver: rpa2
title: Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities
  of LLMs in Negotiation Dialogues
arxiv_id: '2402.13550'
source_url: https://arxiv.org/abs/2402.13550
tags:
- negotiation
- tasks
- task
- deal
- partner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates the capabilities of large language
  models (LLMs) in negotiation dialogues across 35 tasks, testing comprehension, annotation,
  partner modeling, and generation skills. GPT-4 consistently outperforms other models,
  including fine-tuned baselines, particularly in comprehension and partner modeling
  tasks, but all models struggle with subjective assessments and generating strategically
  advantageous responses.
---

# Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues

## Quick Facts
- arXiv ID: 2402.13550
- Source URL: https://arxiv.org/abs/2402.13550
- Authors: Deuksin Kwon; Emily Weiss; Tara Kulshrestha; Kushal Chawla; Gale M. Lucas; Jonathan Gratch
- Reference count: 25
- Primary result: GPT-4 outperforms other models in negotiation tasks but struggles with strategic alignment and subjective assessments

## Executive Summary
This study systematically evaluates large language models (LLMs) across 35 tasks testing comprehension, annotation, partner modeling, and generation capabilities in negotiation dialogues. GPT-4 consistently outperforms other models, including fine-tuned baselines, particularly in comprehension and partner modeling tasks. However, all models demonstrate limitations in handling subjective assessments and generating strategically advantageous responses. Human evaluation indicates GPT-4's response generation is comparable to human performance in coherence but weaker in strategic alignment, highlighting both the potential and limitations of LLMs in negotiation applications.

## Method Summary
The study employs a comprehensive evaluation framework testing LLMs across four key capabilities: comprehension (understanding negotiation contexts), annotation (labeling dialogue elements), partner modeling (predicting opponent behavior), and generation (producing negotiation responses). The evaluation uses a combination of automated metrics and human annotation, with reference data collected from 15 human negotiation pairs. GPT-4 and other models are tested on 35 specific tasks, with performance compared across different negotiation scenarios and partner modeling conditions.

## Key Results
- GPT-4 consistently outperforms other models in comprehension and partner modeling tasks
- All models struggle with subjective assessments and generating strategically advantageous responses
- Human evaluation shows GPT-4's response generation is comparable to human performance in coherence but weaker in strategic alignment

## Why This Works (Mechanism)
The study's systematic approach to evaluating LLMs in negotiation contexts works by decomposing the complex task of negotiation into discrete, measurable capabilities. By testing comprehension, annotation, partner modeling, and generation separately, the evaluation can identify specific strengths and weaknesses rather than making broad claims about overall negotiation ability. The use of both automated metrics and human evaluation provides multiple perspectives on model performance, helping to validate findings and identify areas where human judgment is essential.

## Foundational Learning
1. **Negotiation dialogue structure** - Understanding turn-taking, offer-response patterns, and conversational flow is essential for evaluating LLM performance in realistic negotiation contexts. Quick check: Verify models can correctly identify speaker turns and conversational segments.

2. **Strategic negotiation concepts** - Knowledge of BATNA, reservation prices, and concession strategies is needed to assess whether models generate strategically sound responses. Quick check: Test models' understanding of basic negotiation strategies through comprehension tasks.

3. **Partner modeling techniques** - Understanding how to predict opponent behavior based on dialogue history and strategic positioning is crucial for realistic negotiation simulation. Quick check: Evaluate model predictions of partner actions against human behavioral patterns.

4. **Subjective assessment challenges** - Recognition that negotiation success involves subjective elements like strategic alignment and deal optimality that are difficult to measure objectively. Quick check: Compare automated metrics with human evaluation scores.

5. **Multi-turn dialogue coherence** - Understanding how to maintain consistency and logical flow across multiple negotiation turns is essential for realistic dialogue generation. Quick check: Assess coherence of generated responses across extended dialogue sequences.

6. **Cross-cultural negotiation dynamics** - Awareness of how cultural differences affect negotiation styles and strategies, though this study focuses primarily on general negotiation principles. Quick check: Test model adaptability to different negotiation contexts.

## Architecture Onboarding

**Component map:** Input text -> Preprocessing -> Task-specific processing -> Response generation -> Evaluation metrics

**Critical path:** Dialogue input → Comprehension assessment → Partner modeling prediction → Response generation → Coherence evaluation → Strategic alignment check

**Design tradeoffs:** The study balances automated evaluation efficiency against the need for human judgment, using automated metrics for objective measures while relying on human annotation for subjective assessments. This tradeoff allows for comprehensive testing but introduces potential human bias in evaluation.

**Failure signatures:** Models exhibit specific failure patterns including: (1) loss of coherence in multi-turn dialogues, (2) failure to incorporate strategic cues from partner behavior, (3) overly agreeable responses that miss negotiation opportunities, and (4) difficulty with subjective assessments requiring nuanced judgment.

**First 3 experiments:**
1. Test model comprehension on single-turn negotiation scenarios to establish baseline understanding
2. Evaluate partner modeling accuracy in controlled two-party negotiations with clear strategic objectives
3. Generate responses in multi-issue negotiations to assess strategic thinking and coherence maintenance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on automated metrics and small-scale human annotation (15 human pairs), potentially missing real-world negotiation complexity
- Subjective nature of negotiation success introduces inherent measurement challenges, particularly for strategic alignment and deal optimality
- Limited comparison scope may not fully account for contextual factors in human-human negotiations

## Confidence

**High confidence:** GPT-4's superior performance in comprehension and partner modeling tasks compared to other models

**Medium confidence:** Claims about strategic limitations in GPT-4's response generation due to subjective nature of strategic evaluation

**Medium confidence:** Overall model performance rankings given limited scope of negotiation scenarios tested

**Low confidence:** Claims about model limitations in real-world applications as these extend beyond experimental scope

## Next Checks

1. Replicate findings with a larger, more diverse set of negotiation scenarios and human-human reference data

2. Conduct blinded human evaluations comparing model-generated responses to human responses across multiple negotiation contexts

3. Test model performance on more complex, multi-issue negotiations with longer time horizons to better assess strategic capabilities