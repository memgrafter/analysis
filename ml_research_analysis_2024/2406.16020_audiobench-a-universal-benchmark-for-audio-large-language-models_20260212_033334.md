---
ver: rpa2
title: 'AudioBench: A Universal Benchmark for Audio Large Language Models'
arxiv_id: '2406.16020'
source_url: https://arxiv.org/abs/2406.16020
tags:
- audio
- speech
- arxiv
- datasets
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AudioBench is a comprehensive benchmark for Audio Large Language
  Models (AudioLLMs) that evaluates their performance across 8 tasks and 26 datasets.
  The benchmark addresses the lack of a universal evaluation framework for AudioLLMs
  by focusing on speech understanding, audio scene understanding, and voice understanding
  (paralinguistic).
---

# AudioBench: A Universal Benchmark for Audio Large Language Models

## Quick Facts
- arXiv ID: 2406.16020
- Source URL: https://arxiv.org/abs/2406.16020
- Reference count: 16
- No single model excels consistently across all 8 tasks and 26 datasets

## Executive Summary
AudioBench is a comprehensive benchmark for evaluating Audio Large Language Models (AudioLLMs) across 8 tasks and 26 datasets. The benchmark addresses the lack of a universal evaluation framework by focusing on three main aspects: speech understanding, audio scene understanding, and voice understanding (paralinguistic). It introduces 7 new datasets and employs a model-as-judge approach using Llama-3-70B-Instruct for evaluation. Testing five popular models reveals significant opportunities for future advancements, particularly in long-form audio processing and handling non-verbal content.

## Method Summary
AudioBench compiles 26 datasets across 8 tasks, with 7 newly proposed datasets specifically designed to evaluate AudioLLMs comprehensively. The benchmark uses Llama-3-70B-Instruct as a model-as-judge evaluator, scoring outputs on a 0-100 scale. The evaluation covers speech recognition, question answering, emotion recognition, audio captioning, and other tasks. Models are tested with multiple prompt templates to assess robustness, and Spearman's rank correlation is used to validate the model-as-judge approach against GPT-4 judgments.

## Key Results
- No single model excels consistently across all tasks - SALMONN leads in 4 tasks, Qwen-Audio-Chat in 2, while others show mixed performance
- Model-as-judge using Llama-3-70B-Instruct shows strong correlation with GPT-4 (ρ > 0.85) across all three tested datasets
- SALMONN shows the least robustness to varying prompt templates for the same task, while other models demonstrate better generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-as-judge using Llama-3-70B-Instruct correlates strongly with GPT-4 judgments
- Mechanism: Large language models can effectively evaluate the quality and relevance of audio model outputs by comparing them to reference answers and questions
- Core assumption: The evaluation template and scoring criteria provide sufficient context for the judging model to assess quality
- Evidence anchors:
  - [abstract] "Our findings indicate that LLaMA-3-70B-Instruct model demonstrates a higher correlation compared to judgment models like Prometheus-2"
  - [section] "The correlation scores exceeding 0.85 across all three datasets indicate a 'very strong correlation' with GPT-4"
  - [corpus] Weak evidence - no direct citations for Llama-3-70B-Instruct as judge found in related papers
- Break condition: If the judging model lacks sufficient context or the scoring criteria are ambiguous, evaluation quality degrades

### Mechanism 2
- Claim: Multiple prompt templates improve robustness evaluation of AudioLLMs
- Mechanism: Different instruction phrasings expose model overfitting to specific prompt patterns
- Core assumption: Models that perform well on one prompt template but poorly on others demonstrate insufficient generalization
- Evidence anchors:
  - [section] "we found that some models are more robust to seen instruction while do not generalize to unseen ones"
  - [section] "SALMONN is the least robust when faced with varying prompt templates for the same task"
  - [corpus] Weak evidence - related papers focus on different aspects of evaluation robustness
- Break condition: If models are trained with extensive prompt variation, differences between templates may diminish

### Mechanism 3
- Claim: Separate tasks for speech, audio scene, and voice understanding capture distinct capabilities
- Mechanism: Different audio understanding domains require specialized processing and cannot be fully evaluated by a single unified task
- Core assumption: The three categories (speech understanding, audio scene understanding, voice understanding) represent fundamentally different types of audio comprehension
- Evidence anchors:
  - [abstract] "evaluation targets three main aspects: speech understanding, audio scene understanding, and voice understanding (paralinguistic)"
  - [section] "For each task, we compile existing datasets and develop new ones tailored to the requirements for comprehensive understanding"
  - [corpus] Moderate evidence - related work focuses on multimodal benchmarks but not specifically on separating these three audio domains
- Break condition: If models develop truly unified audio understanding, the task separation may become less meaningful

## Foundational Learning

- Concept: Model-as-judge evaluation methodology
  - Why needed here: Traditional metrics don't work well for open-ended generation from AudioLLMs
  - Quick check question: What are the key components needed for a model-as-judge to effectively evaluate audio model outputs?

- Concept: Correlation analysis for benchmark validation
  - Why needed here: Ensures the proposed evaluation method produces results comparable to established benchmarks
  - Quick check question: How does Spearman's rank correlation help validate the effectiveness of a new evaluation metric?

- Concept: Prompt robustness testing
  - Why needed here: AudioLLMs may overfit to specific instruction patterns, making robustness critical for real-world deployment
  - Quick check question: What would indicate that an AudioLLM is overfitting to specific prompt templates?

## Architecture Onboarding

- Component map: Audio input → model inference → output generation → model-as-judge evaluation → score normalization
- Critical path: Audio input → model inference → output generation → model-as-judge evaluation → score normalization
- Design tradeoffs: Open-source judging models offer accessibility but may have lower correlation than commercial alternatives; multiple prompt templates increase evaluation coverage but require more computational resources
- Failure signatures: Low correlation between model-as-judge and human judgments; inconsistent performance across prompt templates; inability to handle long-form audio
- First 3 experiments:
  1. Run a single sample through the full pipeline and verify each component produces expected outputs
  2. Test correlation between Llama-3-70B-Instruct and GPT-4 on a small sample set
  3. Evaluate model robustness by running the same samples with different prompt templates and comparing results

## Open Questions the Paper Calls Out
None

## Limitations
- The model-as-judge approach, while showing strong correlation with GPT-4, may still introduce evaluation bias for subjective tasks
- The benchmark's focus on English-language audio content limits its applicability to multilingual audio understanding scenarios
- Claims about model capabilities in long-form audio processing are based on limited long-form datasets (3 out of 26)

## Confidence

- **High Confidence**: The benchmark compilation and task categorization (speech understanding, audio scene understanding, voice understanding) are well-defined and methodologically sound
- **Medium Confidence**: The model-as-judge evaluation methodology shows strong statistical correlation with GPT-4 but requires further validation for subjective tasks
- **Low Confidence**: Claims about model capabilities in long-form audio processing are based on limited long-form datasets

## Next Checks

1. **Human Evaluation Validation**: Conduct a small-scale human evaluation study comparing judgments from Llama-3-70B-Instruct against expert human raters on 50 randomly selected samples across different task types to validate the model-as-judge approach's reliability for subjective tasks.

2. **Multilingual Extension Testing**: Test the benchmark framework on non-English datasets (e.g., Common Voice in multiple languages) to assess whether the current evaluation methodology and task definitions generalize beyond English audio content.

3. **Long-form Audio Artifacts Analysis**: Systematically evaluate how different segmentation strategies (overlap, window size, reassembly methods) affect model performance on the long-form audio datasets to characterize the impact of the chunking approach on evaluation validity.