---
ver: rpa2
title: Automated Feature Selection for Inverse Reinforcement Learning
arxiv_id: '2403.15079'
source_url: https://arxiv.org/abs/2403.15079
tags:
- features
- learning
- reward
- feature
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an automated feature selection method for
  inverse reinforcement learning (IRL) in continuous state spaces. The approach uses
  polynomial basis functions to construct candidate features, enabling matching of
  state distribution moments up to second order.
---

# Automated Feature Selection for Inverse Reinforcement Learning

## Quick Facts
- arXiv ID: 2403.15079
- Source URL: https://arxiv.org/abs/2403.15079
- Reference count: 27
- One-line primary result: Automated feature selection method outperforms baselines across three control tasks while using fewer features

## Executive Summary
This work introduces an automated feature selection method for inverse reinforcement learning (IRL) in continuous state spaces. The approach uses polynomial basis functions to construct candidate features, enabling matching of state distribution moments up to second order. Feature selection is performed via correlation-based techniques leveraging trajectory probabilities. The method is evaluated on three control tasks (Pendulum, CartPole, Acrobot) and demonstrates superior performance compared to baselines including random features, linear features, all features, and hand-picked features.

## Method Summary
The method employs polynomial basis functions (s and vec(ssT)) as candidate features to capture up to second-order statistical moments of state distributions. Kernel density estimation is used to compute trajectory probabilities, which are then leveraged for correlation-based feature selection using F-statistics. Maximum entropy IRL optimizes reward weights via gradient descent, with the loss gradient being the difference between expert and learned policy feature expectations. Finally, reinforcement learning algorithms (PPO/SAC) extract the optimal policy from the learned reward function.

## Key Results
- Outperforms baselines (random features, linear features, all features, hand-picked features) across all three tasks
- Achieves benchmark results while using fewer features than competing approaches
- Better alignment with expert state distributions as measured by 2D Wasserstein distance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Polynomial basis functions enable matching of statistical moments between expert and learned policies up to second order.
- Mechanism: By representing features as quadratic polynomials of the state (s and vec(ssT)), the method ensures that expectations of these features are equal under expert demonstrations and the learned policy, which translates to matching means and covariance matrices of state distributions.
- Core assumption: The state distribution can be adequately approximated by its first two statistical moments, and matching these moments is sufficient for capturing the relevant dynamics.
- Evidence anchors:
  - [abstract]: "The approach uses polynomial basis functions to construct candidate features, enabling matching of state distribution moments up to second order."
  - [section IV-A]: Proposition 1 proves that matching expectations of second-order polynomials leads to matching mean and variance of distributions.
  - [corpus]: Weak - No direct citations found in the corpus supporting this specific mechanism.
- Break condition: If the state distribution deviates significantly from Gaussian assumptions or requires higher-order moments for accurate representation, the polynomial basis may be insufficient.

### Mechanism 2
- Claim: Correlation-based feature selection improves policy performance by reducing noise and spurious correlations.
- Mechanism: The method estimates trajectory probabilities using kernel density estimation, then ranks features based on their F-statistics with these probabilities, selecting those with highest predictive power for reward learning.
- Core assumption: Features that better predict trajectory probabilities are more relevant for reward function representation, and a smaller feature set reduces overfitting to noise.
- Evidence anchors:
  - [abstract]: "Feature selection is performed via correlation-based techniques leveraging trajectory probabilities."
  - [section IV-B]: Explains the ranking process using F-statistics between trajectory features and probabilities.
  - [section V-E]: Empirical results show the proposed method achieves better performance with fewer features compared to baselines.
- Break condition: If the kernel density estimation fails to accurately estimate trajectory probabilities, or if relevant features have low correlation with trajectory probabilities despite being important.

### Mechanism 3
- Claim: Maximum entropy IRL formulation provides a principled way to learn reward weights from feature expectations.
- Mechanism: The method uses gradient descent to minimize the difference between expert feature expectations and those of the learned policy, with the loss gradient being the difference between these expectations.
- Core assumption: The optimal policy should match the feature expectations of expert demonstrations, and the maximum entropy framework provides a unique solution despite reward ambiguity.
- Evidence anchors:
  - [abstract]: "The method is evaluated on three control tasks (Pendulum, CartPole, Acrobot) and demonstrates superior performance compared to baselines."
  - [section IV-C]: Describes the loss function and weight update rule using gradient descent.
  - [section III]: Explains the connection between matching feature expectations and finding optimal policies.
- Break condition: If the feature expectations cannot be accurately estimated, or if the gradient descent optimization fails to converge to the correct reward weights.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: IRL is fundamentally about solving MDPs in reverse - given expert behavior, infer the reward function that would generate such behavior in the MDP framework.
  - Quick check question: What are the five components of an MDP tuple (S, A, R, T, γ)?

- Concept: Feature Representation in IRL
  - Why needed here: The reward function is represented as a linear combination of features, making the choice of appropriate features crucial for successful IRL.
  - Quick check question: Why can't raw state variables alone be used as features in continuous state spaces?

- Concept: Maximum Entropy Principle
  - Why needed here: The maximum entropy IRL formulation provides a probabilistic approach to reward determination that addresses reward ambiguity by selecting the least informative (maximum entropy) distribution consistent with observed behavior.
  - Quick check question: How does the maximum entropy principle help resolve the fundamental ill-posedness of the IRL problem?

## Architecture Onboarding

- Component map: Polynomial Feature Generator → Candidate Feature Set → Kernel Density Estimator → Trajectory Probability Estimator → F-statistic Ranker → Selected Feature Subset → Maximum Entropy IRL Optimizer → Reward Function → Reinforcement Learning Policy Extractor → Expert Policy

- Critical path: Expert demonstrations → Polynomial feature generation → Kernel density estimation → Feature selection → Maximum entropy IRL → Policy extraction → Evaluation

- Design tradeoffs:
  - Feature complexity vs. generalization: More complex features (higher-order polynomials) might capture more dynamics but risk overfitting.
  - Computational cost vs. accuracy: Kernel density estimation provides accurate probability estimates but scales poorly with data size.
  - Feature selection method: F-statistics are computationally efficient but may miss features with non-linear relationships to rewards.

- Failure signatures:
  - Poor policy performance despite successful training: Likely feature selection issues or reward ambiguity not fully addressed.
  - High variance in results: May indicate insufficient data for kernel density estimation or unstable gradient descent optimization.
  - Slow convergence: Could suggest poor choice of learning rate or inadequate exploration in the RL policy extraction step.

- First 3 experiments:
  1. Test polynomial feature matching on synthetic data with known Gaussian distributions to verify moment matching properties.
  2. Evaluate feature selection performance on a simple control task with known relevant features to assess selection accuracy.
  3. Run end-to-end IRL on Pendulum environment with varying numbers of demonstrations to understand data requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed method perform with higher-order polynomial basis functions beyond second-order?
- Basis in paper: [explicit] The authors state: "We posit that employing higher-order polynomials could align not only the mean and variance but also the higher-order statistical moments between training and testing distributions, thereby enhancing the match between these distributions."
- Why unresolved: The paper only evaluates second-order polynomial features and does not experimentally test the performance of higher-order polynomials.
- What evidence would resolve it: Empirical evaluation comparing second-order, third-order, and higher polynomial features across the same benchmark tasks, measuring performance metrics like Wasserstein distance and cumulative rewards.

### Open Question 2
- Question: How does the method scale to high-dimensional state spaces beyond the three benchmark tasks tested?
- Basis in paper: [inferred] The authors mention "For a high-dimensional state space, using this many features can still be problematic" when discussing the dimensionality of polynomial features.
- Why unresolved: The experiments are limited to relatively low-dimensional control tasks (Pendulum, CartPole, Acrobot) and do not explore performance in high-dimensional domains.
- What evidence would resolve it: Testing the method on environments with significantly higher dimensional state spaces (e.g., robotic manipulation tasks with many joints, or vision-based tasks) while measuring computational complexity and performance degradation.

### Open Question 3
- Question: How does the proposed feature selection method compare to other automated feature selection techniques in terms of computational efficiency and performance?
- Basis in paper: [explicit] The authors state: "The time complexity of the feature selection is O(N) w.r.t. feature size" but do not compare against other feature selection methods like L1 regularization, forward/backward selection, or mutual information-based approaches.
- Why unresolved: The paper only benchmarks against manual, random, and all-feature baselines, without comparing to other automated feature selection algorithms.
- What evidence would resolve it: Direct comparison of the proposed method against other state-of-the-art feature selection techniques across multiple IRL benchmarks, measuring both computational runtime and policy performance.

## Limitations
- Polynomial basis approach limited to capturing up to second-order moments, insufficient for complex, non-Gaussian state distributions
- Correlation-based feature selection relies on accurate kernel density estimation, which can be computationally expensive and may fail in high-dimensional state spaces
- Method's performance depends heavily on quality and quantity of expert demonstrations - insufficient or unrepresentative demonstrations could lead to poor feature selection and reward learning

## Confidence
- High Confidence: The core mechanism of using polynomial basis functions to match state distribution moments (Mechanism 1) is well-grounded in statistical theory and directly supported by Proposition 1.
- Medium Confidence: The effectiveness of correlation-based feature selection (Mechanism 2) is supported by empirical results but lacks theoretical guarantees about optimality of selected features.
- Medium Confidence: The maximum entropy IRL formulation (Mechanism 3) is a standard approach with established theoretical foundations, but its success here depends on accurate feature expectation estimation.

## Next Checks
1. **Robustness to Distribution Shape**: Test the method on environments with non-Gaussian state distributions (e.g., multimodal distributions) to evaluate whether second-order moment matching is sufficient for accurate reward learning.

2. **Scalability Analysis**: Evaluate the method's performance and computational requirements as state dimensionality increases beyond the tested control tasks, particularly focusing on kernel density estimation accuracy and feature selection stability.

3. **Data Efficiency Evaluation**: Systematically vary the number of expert demonstrations to identify the minimum required for reliable feature selection and reward learning, comparing against baseline methods to quantify efficiency gains.