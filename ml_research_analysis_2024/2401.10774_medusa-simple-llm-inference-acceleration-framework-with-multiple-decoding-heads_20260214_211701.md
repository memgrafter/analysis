---
ver: rpa2
title: 'Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding
  Heads'
arxiv_id: '2401.10774'
source_url: https://arxiv.org/abs/2401.10774
tags:
- edusa
- medusa
- heads
- decoding
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of slow inference in large language
  models due to the sequential, memory-bound nature of auto-regressive decoding. To
  overcome this bottleneck, the authors propose Medusa, a framework that adds multiple
  decoding heads to predict subsequent tokens in parallel.
---

# Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads

## Quick Facts
- arXiv ID: 2401.10774
- Source URL: https://arxiv.org/abs/2401.10774
- Reference count: 8
- The paper proposes Medusa, a framework that adds multiple decoding heads to large language models (LLMs) to predict subsequent tokens in parallel, significantly accelerating inference.

## Executive Summary
The paper addresses the problem of slow inference in large language models (LLMs) due to the sequential, memory-bound nature of auto-regressive decoding. To overcome this bottleneck, the authors propose Medusa, a framework that adds multiple decoding heads to predict subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs and verifies multiple candidate continuations simultaneously in each decoding step, significantly reducing the number of steps required. The paper presents two training strategies: Medusa-1, where Medusa heads are fine-tuned on top of a frozen backbone model, and Medusa-2, where both the backbone and Medusa heads are jointly trained. Experiments on various model sizes and training procedures demonstrate that Medusa-1 achieves over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.

## Method Summary
Medusa is a framework that accelerates LLM inference by adding multiple decoding heads to predict subsequent tokens in parallel. The method involves training Medusa heads using either Medusa-1 (fine-tuning on a frozen backbone) or Medusa-2 (joint training with the backbone), and during inference, using tree attention to process multiple candidate continuations simultaneously. The tree attention mechanism allows for efficient parallel processing of candidates, while the two-stage fine-tuning process preserves the backbone model's capabilities. Medusa also introduces extensions like typical acceptance for efficient sampling and self-distillation for training without access to the original dataset.

## Key Results
- Medusa-1 achieves over 2.2x speedup without compromising generation quality.
- Medusa-2 further improves the speedup to 2.3-3.6x.
- The speedup is consistent across various model sizes and training procedures.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding multiple decoding heads to an LLM enables parallel prediction of subsequent tokens, reducing the number of decoding steps required.
- Mechanism: Medusa heads are added to the last hidden states of the LLM, with each head predicting tokens at different future positions. This allows simultaneous generation of multiple candidate continuations.
- Core assumption: The additional heads can accurately predict future tokens without significantly impacting the quality of the original model's predictions.
- Evidence anchors:
  - [abstract]: "we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel."
  - [section]: "we introduce M EDUSA heads. These are additional decoding heads appended to the last hidden states of the original model."
  - [corpus]: Weak. No direct evidence in corpus neighbors, but related works suggest parallel decoding is a valid approach.
- Break condition: If the accuracy of the Medusa heads degrades significantly, the speedup gained from parallel prediction may be offset by the need for more verification steps.

### Mechanism 2
- Claim: Tree-based attention allows efficient processing of multiple candidate continuations in parallel.
- Mechanism: Instead of traditional causal attention, a tree structure is used to process candidates. Only tokens from the same continuation are considered historical data, and an attention mask ensures that attention is only applied to a token's predecessors.
- Core assumption: The tree structure can be efficiently implemented and does not introduce significant computational overhead.
- Evidence anchors:
  - [abstract]: "Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step."
  - [section]: "We employ a tree-structured attention mechanism to process multiple candidates concurrently. This attention mechanism diverges from the traditional causal attention paradigm."
  - [corpus]: Weak. No direct evidence in corpus neighbors, but the concept of tree-structured attention is mentioned in related works.
- Break condition: If the overhead of constructing and processing the tree structure becomes too large, the benefits of parallel processing may be negated.

### Mechanism 3
- Claim: Two-stage fine-tuning preserves the backbone model's capabilities while improving Medusa head accuracy.
- Mechanism: In the first stage, only the Medusa heads are trained while the backbone model is frozen. In the second stage, both the backbone and Medusa heads are trained together with a warmup strategy.
- Core assumption: The two-stage process allows the Medusa heads to learn from the backbone's representations without disrupting the backbone's learned parameters.
- Evidence anchors:
  - [abstract]: "We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities."
  - [section]: "In the first stage, we only train the backbone model as Medusa-1. In the second stage, we train the backbone model and Medusa heads together with a warmup strategy."
  - [corpus]: Weak. No direct evidence in corpus neighbors, but the concept of fine-tuning techniques is mentioned in related works.
- Break condition: If the two-stage fine-tuning process does not adequately preserve the backbone model's capabilities, the quality of the generated text may degrade.

## Foundational Learning

- Concept: Speculative decoding
  - Why needed here: Medusa builds upon the concept of speculative decoding by using multiple decoding heads instead of a separate draft model.
  - Quick check question: What is the main advantage of using Medusa heads over a separate draft model in speculative decoding?

- Concept: Tree-based attention
  - Why needed here: Tree-based attention is a key component of Medusa, allowing efficient parallel processing of multiple candidate continuations.
  - Quick check question: How does tree-based attention differ from traditional causal attention in processing candidate continuations?

- Concept: Fine-tuning techniques
  - Why needed here: Medusa employs two-stage fine-tuning to preserve the backbone model's capabilities while improving Medusa head accuracy.
  - Quick check question: What are the key differences between Medusa-1 and Medusa-2 in terms of fine-tuning the model?

## Architecture Onboarding

- Component map:
  Backbone LLM -> Medusa heads -> Tree-based attention -> Fine-tuning procedures

- Critical path:
  1. Add Medusa heads to the last hidden states of the backbone LLM
  2. Train Medusa heads using the two-stage fine-tuning process
  3. During inference, use Medusa heads to generate multiple candidate continuations
  4. Process candidates in parallel using tree-based attention
  5. Verify and accept the best candidate continuation

- Design tradeoffs:
  - Adding more Medusa heads can improve parallelism but may increase computational overhead
  - Tree-based attention allows efficient parallel processing but requires careful implementation to avoid overhead
  - Two-stage fine-tuning preserves backbone model capabilities but requires additional training time

- Failure signatures:
  - Significant degradation in the quality of generated text
  - Minimal speedup despite parallel processing
  - Increased computational overhead due to inefficient implementation

- First 3 experiments:
  1. Implement a simple version of Medusa with one or two heads and evaluate the speedup and quality of generated text
  2. Compare the performance of Medusa-1 (frozen backbone) and Medusa-2 (joint training) on a small dataset
  3. Experiment with different tree structures in the tree-based attention mechanism to optimize parallel processing efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of tree construction method (Cartesian product vs. optimized sparse tree) impact the acceptance rate and speed across different model sizes and tasks?
- Basis in paper: [explicit] The paper discusses two tree construction methods: the standard Cartesian product and a more sophisticated approach that leverages accuracy estimates to construct a sparse tree.
- Why unresolved: The paper presents experimental results comparing these methods on a single model and task, but a comprehensive study across various model sizes, tasks, and datasets is needed to fully understand the trade-offs.
- What evidence would resolve it: A systematic evaluation of different tree construction methods across diverse models, tasks, and datasets, measuring both acceptance rate and speed, would provide insights into their relative performance and optimal use cases.

### Open Question 2
- Question: How does the choice of sampling scheme (rejection sampling vs. typical acceptance) affect the quality and diversity of generated text, particularly at different temperature settings?
- Basis in paper: [explicit] The paper introduces a typical acceptance scheme as an alternative to rejection sampling, aiming to improve efficiency while maintaining generation quality. It discusses the trade-offs between these schemes and their performance at different temperatures.
- Why unresolved: The paper provides a comparison of these schemes on a limited set of tasks and temperatures, but a more extensive analysis is needed to understand their impact on text quality and diversity across various applications and temperature ranges.
- What evidence would resolve it: A comprehensive evaluation of rejection sampling and typical acceptance schemes across diverse tasks, temperature settings, and quality metrics would provide insights into their relative strengths and weaknesses in different scenarios.

### Open Question 3
- Question: How does the self-distillation approach for training MEDUSA heads affect the quality and efficiency of the resulting models, especially when applied to models trained with RLHF?
- Basis in paper: [explicit] The paper proposes a self-distillation pipeline for training MEDUSA heads when the original training data is unavailable, such as in the case of models trained with RLHF. It discusses the challenges and potential solutions for this approach.
- Why unresolved: The paper presents experimental results on a limited set of models and tasks, but a broader study is needed to understand the impact of self-distillation on model quality and efficiency across various model architectures and training procedures.
- What evidence would resolve it: A systematic evaluation of the self-distillation approach on a diverse set of models, including those trained with RLHF, measuring both quality and efficiency metrics, would provide insights into its effectiveness and limitations.

## Limitations
- Tree Attention Implementation Complexity: The exact implementation details and computational overhead of the tree-based attention mechanism remain underspecified.
- Training Data Requirements: Medusa-1 requires a complete training dataset for fine-tuning the Medusa heads while keeping the backbone frozen.
- Quality Degradation Risk: The paper claims Medusa maintains generation quality while achieving speedup, but this is primarily validated through perplexity scores and MT-Bench.

## Confidence
- High Confidence: The core architectural innovation of adding multiple decoding heads and using tree attention for parallel processing is well-defined and theoretically sound.
- Medium Confidence: The claimed speedup factors (2.2x-3.6x) are supported by experiments, but real-world performance may vary depending on hardware, implementation details, and workload characteristics.
- Low Confidence: The long-term stability of the two-stage fine-tuning approach and its generalization to diverse tasks and domains has not been thoroughly established.

## Next Checks
1. Reimplement the tree attention mechanism and measure its actual computational overhead compared to traditional causal attention to validate the claimed efficiency gains.
2. Evaluate Medusa's performance on long-form text generation tasks (e.g., story continuation, code generation) beyond the metrics reported (perplexity, MT-Bench) to ensure quality is preserved across different generation lengths.
3. Test Medusa on a wider range of model sizes and architectures (e.g., different transformer variants) to assess the robustness of the speedup claims and quality preservation across diverse LLM configurations.