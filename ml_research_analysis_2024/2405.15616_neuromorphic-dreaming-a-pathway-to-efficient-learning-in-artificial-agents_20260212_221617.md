---
ver: rpa2
title: 'Neuromorphic dreaming: A pathway to efficient learning in artificial agents'
arxiv_id: '2405.15616'
source_url: https://arxiv.org/abs/2405.15616
tags:
- learning
- agent
- neuromorphic
- network
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a neuromorphic implementation of model-based
  reinforcement learning (MBRL) using spiking neural networks (SNNs) on mixed-signal
  analog/digital hardware. The approach alternates between an "awake" phase, where
  the agent learns from real interactions with the environment, and a "dreaming" phase,
  where it learns from simulated experiences generated by a learned world model.
---

# Neuromorphic dreaming: A pathway to efficient learning in artificial agents

## Quick Facts
- arXiv ID: 2405.15616
- Source URL: https://arxiv.org/abs/2405.15616
- Authors: Ingo Blakowski; Dmitrii Zendrikov; Cristiano Capone; Giacomo Indiveri
- Reference count: 40
- Primary result: Neuromorphic implementation of MBRL with dreaming achieves higher average returns with fewer than half as many real environment interactions compared to baseline

## Executive Summary
This work presents a neuromorphic implementation of model-based reinforcement learning using spiking neural networks on mixed-signal analog/digital hardware. The approach alternates between an "awake" phase, where the agent learns from real interactions with the environment, and a "dreaming" phase, where it learns from simulated experiences generated by a learned world model. The method is validated on the Atari game Pong, demonstrating significant sample efficiency gains compared to a baseline agent without dreaming. Specifically, the dreaming-enabled agent achieves higher average returns with fewer than half as many real environment interactions. The implementation runs in real-time on the DYNAP-SE neuromorphic processor, showcasing the potential for energy-efficient learning systems capable of rapid adaptation in real-world settings.

## Method Summary
The method implements model-based reinforcement learning using spiking neural networks on the DYNAP-SE neuromorphic processor. Two SNNs are implemented: an agent network that learns to map states to actions, and a world model network that learns to predict next states and rewards. Both networks use population-coded input spike generators to represent continuous state variables, with fixed random input-to-hidden connections and learned readout weights. The system alternates between awake phases (learning from real environment interactions) and dreaming phases (learning from simulated experiences generated by the world model). Readout weights are trained on a computer using policy gradient updates for the agent and supervised learning for the world model, while hidden layers remain fixed on the chip.

## Key Results
- The dreaming-enabled agent achieves higher average returns (8.7) compared to the baseline agent without dreaming
- Sample efficiency improves dramatically, requiring fewer than half as many real environment interactions to reach target performance
- The system runs in real-time on DYNAP-SE neuromorphic hardware while maintaining energy efficiency
- Policy entropy decreases during training, indicating convergence to stable policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The alternation between awake and dreaming phases enables sample-efficient learning by combining real-world data with simulated experiences.
- Mechanism: The agent learns from real environment interactions during the awake phase, while the world model network captures the environment dynamics. During the dreaming phase, the agent uses the learned world model to generate simulated experiences, allowing for additional policy updates without requiring further real environment interactions.
- Core assumption: The world model can accurately capture the environment dynamics, such that simulated experiences are sufficiently representative of real experiences for policy learning.
- Evidence anchors:
  - [abstract]: "This approach leverages the energy efficiency of mixed-signal neuromorphic chips while achieving high sample efficiency through an alternation of online learning, referred to as the 'awake' phase, and offline learning, known as the 'dreaming' phase."
  - [section]: "This alternation between real and imagined experiences allows the agent to learn more efficiently by leveraging the sample-efficiency of model-based methods while still benefiting from the generalization capabilities of model-free techniques."
  - [corpus]: Weak. The related papers do not directly discuss dreaming phases or the alternation between awake and dreaming learning. This is a novel contribution of this work.
- Break condition: If the world model fails to accurately capture the environment dynamics, the simulated experiences generated during the dreaming phase will be poor representations of real experiences, leading to ineffective or harmful policy updates.

### Mechanism 2
- Claim: Training only the readout weights of the spiking neural networks enables effective learning while respecting the hardware constraints of the neuromorphic chip.
- Mechanism: The input-to-hidden connections are randomly initialized and fixed, while the hidden-to-output connections (readout weights) are learned using reward-based policy gradient rules. This approach allows for effective learning of state-to-action mappings while working within the limitations of the hardware, which has a single globally shared set of parameters per core and a maximum number of incoming connections per neuron.
- Core assumption: The randomly initialized input-to-hidden connections are sufficient to provide a useful representation of the input state for the hidden layer neurons, such that learning the readout weights alone can lead to effective policy learning.
- Evidence anchors:
  - [abstract]: "The networks are implemented using a mixed-signal neuromorphic processor, with the readout layers trained using a computer in-the-loop, while the other layers remain fixed."
  - [section]: "By restricting plasticity to the readout connections, we can work within these limitations while still enabling the network to learn from rewards."
  - [corpus]: Weak. The related papers do not directly discuss the strategy of training only the readout weights in spiking neural networks on neuromorphic hardware. This appears to be a novel contribution of this work.
- Break condition: If the randomly initialized input-to-hidden connections are not sufficient to provide a useful representation of the input state, learning the readout weights alone will not lead to effective policy learning, and the agent's performance will be limited.

### Mechanism 3
- Claim: The use of population coding for input encoding allows the spiking neural network to effectively represent and process the continuous state space of the Atari Pong environment.
- Mechanism: The state variables (paddle positions, ball coordinates) are encoded using a population of input spike generators that respond differently to specific values or positions. As the value of a state variable changes, the Gaussian activity profile moves across the population of input spike generators associated with that variable. This encoding scheme allows the network to effectively represent the continuous state space.
- Core assumption: The Gaussian activity profile of the input spike generators can accurately represent the continuous state variables, such that the spiking neural network can effectively process the encoded information.
- Evidence anchors:
  - [abstract]: "The Atari Pong game state consists of four variables: the positions of the two paddles, the ball's x-coordinate, and the ball's y-coordinate. For each variable, we define a population of input spike generators that respond differently to specific values or positions."
  - [section]: "This encoding scheme allows the network to effectively represent and process the game's continuous state space."
  - [corpus]: Weak. The related papers do not directly discuss the use of population coding for input encoding in spiking neural networks for Atari Pong. This appears to be a specific design choice of this work.
- Break condition: If the Gaussian activity profile of the input spike generators does not accurately represent the continuous state variables, the spiking neural network will not be able to effectively process the encoded information, leading to poor performance.

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: The agent learns to play Atari Pong by maximizing the cumulative reward received from the environment. RL provides the framework for the agent to learn a policy that maps states to actions in order to maximize the expected return.
  - Quick check question: What is the difference between model-free and model-based RL, and how does the dreaming approach in this work relate to model-based RL?

- Concept: Spiking Neural Networks (SNNs)
  - Why needed here: The agent and world model are implemented using SNNs, which are well-suited for neuromorphic hardware due to their event-driven nature and low power consumption. Understanding SNNs is crucial for grasping the hardware implementation and learning rules used in this work.
  - Quick check question: How do spiking neurons differ from traditional artificial neurons, and what are the advantages of using SNNs on neuromorphic hardware?

- Concept: Neuromorphic Hardware
  - Why needed here: The agent and world model are implemented on the DYNAP-SE neuromorphic processor, which uses mixed-signal analog/digital circuits to efficiently implement spiking neural networks. Understanding the characteristics and constraints of neuromorphic hardware is essential for comprehending the design choices and limitations of this work.
  - Quick check question: What are the key features of the DYNAP-SE neuromorphic processor, and how do they influence the implementation of the spiking neural networks in this work?

## Architecture Onboarding

- Component map:
  - Agent network: Input layer (population-coded spike generators) -> Hidden layer (510 LIF neurons on DYNAP-SE) -> Readout layer (3 neurons for action probabilities, on computer)
  - World model network: Input layer (population-coded spike generators for state and action) -> Hidden layer (510 LIF neurons on DYNAP-SE) -> Readout layer (4 neurons for state prediction, 1 neuron for reward prediction, on computer)
  - DYNAP-SE neuromorphic processor: Implements the hidden layers of both networks
  - Computer: Implements the readout layers, manages chip-environment synchronization, and updates readout weights

- Critical path:
  1. State and action inputs are encoded into spike trains using population coding
  2. Spike trains are sent to the DYNAP-SE chip to be processed by the hidden layers
  3. Spiking activity from the hidden layers is read out by the computer
  4. Readout layers on the computer compute action probabilities, state predictions, and reward predictions
  5. Readout weights are updated using the appropriate learning rules (policy gradient for agent, supervised learning for world model)

- Design tradeoffs:
  - Training only the readout weights vs. training all weights: Allows for effective learning while respecting hardware constraints, but may limit the agent's performance compared to training all weights
  - Fixed input-to-hidden connections vs. learned connections: Simplifies the hardware implementation and reduces power consumption, but may limit the agent's ability to learn optimal representations
  - Awake-dreaming alternation vs. continuous real-world learning: Enables sample-efficient learning by leveraging simulated experiences, but relies on the accuracy of the world model

- Failure signatures:
  - Agent performance plateaus or degrades over time: May indicate issues with the world model's accuracy or the learning rules
  - High variance in agent performance across different DYNAP-SE chips: May indicate sensitivity to device mismatch or temperature variations
  - Long training times or slow convergence: May indicate suboptimal hyperparameters or inefficient hardware utilization

- First 3 experiments:
  1. Implement the agent network on the DYNAP-SE chip and train it to play Atari Pong without dreaming, using only the awake phase. Measure the agent's performance (average return per game) as a function of the number of games played.
  2. Implement the world model network on the DYNAP-SE chip and train it to predict the next state and reward given the current state and action. Measure the world model's prediction accuracy using metrics such as mean squared error.
  3. Implement the full awake-dreaming learning approach, alternating between awake and dreaming phases. Measure the agent's performance (average return per game) as a function of the number of games played, and compare it to the performance of the agent without dreaming (experiment 1).

## Open Questions the Paper Calls Out
- How does the performance of the neuromorphic MBRL approach scale with task complexity beyond Atari Pong?
- Can the readout weights be effectively transferred to the neuromorphic chip to enable fully on-chip learning?
- What is the impact of device mismatch and temperature variations on the robustness of the neuromorphic MBRL system?

## Limitations
- Hardware reproducibility is limited due to unspecified DYNAP-SE chip configuration parameters required for optimal performance
- Population coding details for mapping Pong state variables to spike generator activity are not fully specified
- Device mismatch effects across different chips are acknowledged but not systematically characterized

## Confidence
- **High confidence**: The core demonstration that MBRL with dreaming phases can be implemented on neuromorphic hardware and shows sample efficiency benefits over non-dreaming baselines
- **Medium confidence**: The specific performance numbers may have higher uncertainty due to hardware variability and unreported implementation details
- **Low confidence**: The generalization of these results to other environments beyond Pong is speculative, as only one task is evaluated

## Next Checks
1. **Hardware parameter sensitivity**: Systematically vary the integration factor and neural parameters on DYNAP-SE to determine their impact on learning performance and identify the robustness range of the approach.

2. **Cross-chip reproducibility**: Implement the complete awake-dreaming MBRL system on multiple DYNAP-SE chips to quantify performance variability and identify which components are most sensitive to device mismatch.

3. **Dreaming phase duration optimization**: Conduct ablation studies varying the ratio of awake to dreaming phases (currently 2:1) to determine the optimal balance for maximizing sample efficiency across different learning stages.