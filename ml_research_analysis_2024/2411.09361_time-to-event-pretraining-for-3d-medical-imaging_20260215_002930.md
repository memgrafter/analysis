---
ver: rpa2
title: Time-to-Event Pretraining for 3D Medical Imaging
arxiv_id: '2411.09361'
source_url: https://arxiv.org/abs/2411.09361
tags:
- pretraining
- base
- medical
- tasks
- imaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces time-to-event pretraining for 3D medical
  imaging, a method that leverages longitudinal EHR data to improve prediction of
  future clinical outcomes from CT scans. The core innovation is converting EHR timelines
  into time-to-event pretraining tasks, enabling models to learn prognostic pixel
  biomarkers that capture disease progression over time.
---

# Time-to-Event Pretraining for 3D Medical Imaging

## Quick Facts
- arXiv ID: 2411.09361
- Source URL: https://arxiv.org/abs/2411.09361
- Reference count: 40
- Primary result: 23.7% AUROC improvement and 29.4% gain in Harrell's C-index on prognostic tasks without sacrificing diagnostic performance

## Executive Summary
This paper introduces time-to-event (TTE) pretraining for 3D medical imaging, a method that leverages longitudinal EHR data to improve prediction of future clinical outcomes from CT scans. The core innovation is converting EHR timelines into time-to-event pretraining tasks, enabling models to learn prognostic pixel biomarkers that capture disease progression over time. Using 18,945 CT scans and 8,192 diverse clinical tasks, the method achieves substantial performance gains across benchmark prognostic tasks while maintaining diagnostic classification accuracy.

## Method Summary
The approach continues pretraining existing 3D models (SwinUNETR, DenseNet, ResNet) using piecewise exponential neural networks (PEANN) on time-to-event tasks derived from medical codes in EHR data. The method selects 8,192 pretraining tasks using conditional Shannon entropy optimization from 4.3 million candidate codes, then trains with PEANN loss across 8 piecewise time bins. After pretraining, the encoder is frozen and task-specific heads are added for downstream adaptation to both classification and TTE tasks.

## Key Results
- 23.7% average AUROC improvement across 8 benchmark prognostic tasks
- 29.4% improvement in Harrell's C-index for time-to-event prediction
- 54% improvement in calibration (Integrated Brier Score)
- No negative impact on 8 external diagnostic classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TTE pretraining improves data efficiency by utilizing temporal information from future EHRs and censored patients, increasing available task labels per-pretraining instance.
- Mechanism: The method converts longitudinal EHR timelines into time-to-event pretraining tasks, which capture the temporal structure of clinical events occurring after the CT scan. This approach leverages future medical events and naturally handles right censoring, allowing the model to learn from more data points than traditional visit-level supervision.
- Core assumption: The temporal information in future EHR events is predictive of disease progression and can be effectively captured by the TTE pretraining objective.
- Evidence anchors:
  - [abstract] "TTE supervision, in contrast, provides a simple approach to leveraging complex temporal information found in longitudinal EHRs for purposes of learning prognostic pixel biomarkers."
  - [section] "Our approach substantially improves performance in predicting future medical outcomes, achieving on average a 23.7% increase in AUROC and a 29.4% improvement in Harrell’s C-index over baseline models for 8 benchmark tasks without negatively impacting diagnostic classification performance in 8 external tasks."
  - [corpus] Weak evidence - no direct corpus citations about EHR temporal supervision.

### Mechanism 2
- Claim: Existing self-supervised learning methods struggle to learn prognostic pixel biomarkers due to missing temporal connections with pathologies that will be detected in the future.
- Mechanism: Current SSL approaches for medical imaging primarily capture static, structural features like organ morphology, but fail to identify dynamic patterns and biomarkers predictive of future health risks. TTE supervision addresses this by embedding long-term outcome information into the image model during pretraining.
- Core assumption: Prognostic pixel biomarkers require temporal context to be identified, and current SSL methods are insufficient for capturing this context.
- Evidence anchors:
  - [abstract] "While current self-supervised methods for 3D medical imaging models capture local structural features like organ morphology, they fail to link pixel biomarkers with long-term health outcomes due to a missing context problem."
  - [section] "Current SSL approaches excel at capturing structural features in medical images, such as organ morphology for image segmentation. However, these learned features are static and largely fail to identify dynamic patterns and biomarkers that are predictive of future health risks."
  - [corpus] Weak evidence - no direct corpus citations about SSL limitations for prognosis.

### Mechanism 3
- Claim: TTE supervision, when conducted as post hoc continued pretraining, does not negatively impact performance on standard categorical classification tasks used for diagnostic image labeling.
- Mechanism: The TTE pretraining objective focuses on predicting future events, which is orthogonal to the task of identifying current disease biomarkers. As a result, the learned representations are beneficial for both prognostic and diagnostic tasks.
- Core assumption: The representations learned by TTE pretraining are general enough to be useful for both prognostic and diagnostic tasks.
- Evidence anchors:
  - [abstract] "Importantly, these gains are achieved without sacrificing diagnostic classification performance in 8 external tasks."
  - [section] "Importantly, these gains are achieved without sacrificing diagnostic classification performance in 8 external tasks."
  - [corpus] Weak evidence - no direct corpus citations about TTE pretraining impact on diagnostic tasks.

## Foundational Learning

- Concept: Time-to-event (TTE) modeling, also known as survival analysis
  - Why needed here: TTE modeling is the core mechanism for incorporating temporal information from future EHR events into the pretraining objective. It allows the model to learn the distribution of times until specific clinical events occur, which is crucial for identifying prognostic pixel biomarkers.
  - Quick check question: What is the key difference between TTE modeling and traditional classification tasks in the context of medical imaging?

- Concept: Piecewise exponential neural networks (PEANN)
  - Why needed here: PEANN is the specific TTE model used in this work to estimate the hazard rate for each time piece. It simplifies large-scale pretraining compared to Cox PH-based methods and is well-suited for the piecewise time bins used in the pretraining tasks.
  - Quick check question: How does the piecewise exponential function simplify the TTE modeling task compared to other approaches like Cox proportional hazard models?

- Concept: Vertex cover problem and conditional Shannon entropy selection
  - Why needed here: These concepts are used to select the pretraining tasks from the vast number of medical codes in the EHR data. By treating ontology-aware task selection as a vertex cover problem, the method can efficiently select a set of medical codes that maximizes conditional entropy given a task budget, ontology DAG, and frequency distribution of observed medical codes.
  - Quick check question: Why is it important to select pretraining tasks based on conditional Shannon entropy rather than simply using all available medical codes?

## Architecture Onboarding

- Component map: CT scan → Encoder → TTE Pretraining Head → PEANN loss → Parameter updates
- Critical path: CT scan → Encoder → TTE Pretraining Head → PEANN loss → Parameter updates
- Design tradeoffs:
  - Using PEANN vs. Cox PH-based methods: PEANN simplifies large-scale pretraining by avoiding the need for batches with at least one uncensored patient, but may be less flexible in modeling the hazard rate.
  - Number of piecewise time bins: More bins allow for finer-grained temporal modeling but increase the complexity of the PEANN and the number of parameters to learn.
  - Task selection strategy: Maximizing conditional Shannon entropy ensures a diverse set of pretraining tasks but may exclude some potentially useful low-frequency tasks.
- Failure signatures:
  - Poor prognostic performance: Could indicate that the TTE pretraining objective is not effectively capturing the temporal information, or that the selected pretraining tasks are not representative of the target tasks.
  - Degraded diagnostic performance: Could indicate that the TTE pretraining is overfitting to prognostic tasks and not learning general image features useful for diagnosis.
  - High computational cost: Could indicate that the number of pretraining tasks or the complexity of the PEANN is too high for the available computational resources.
- First 3 experiments:
  1. Ablation study: Compare the performance of TTE pretraining with and without the temporal information from future EHR events to confirm that the temporal context is indeed crucial for learning prognostic pixel biomarkers.
  2. Task selection analysis: Analyze the distribution of pretraining tasks selected by the conditional Shannon entropy method to ensure that they are representative of the target tasks and cover a diverse range of clinical events.
  3. Hyperparameter tuning: Perform a grid search over the number of piecewise time bins and the learning rate for the PEANN to find the optimal settings for the specific dataset and encoder architecture.

## Open Questions the Paper Calls Out
- The paper doesn't explicitly call out open questions but identifies several limitations including the need to explore TTE pretraining on other imaging modalities, investigate the optimal balance between task diversity and frequency, and compare against multimodal pretraining approaches that include text data.

## Limitations
- Limited generalizability to imaging modalities beyond CT scans, as the study only evaluated on CT data from a single dataset
- Substantial computational requirements for pretraining on 8,192 tasks, raising scalability concerns
- Reliance on comprehensive EHR data with rich temporal information may limit applicability in settings with sparse or incomplete longitudinal records

## Confidence

- **High confidence**: The mechanism by which TTE pretraining leverages temporal EHR information to improve prognostic predictions is well-supported by the ablation studies and comparative analysis with existing SSL methods.
- **Medium confidence**: The claim that TTE pretraining does not negatively impact diagnostic classification performance is supported by the experimental results, but the external validation tasks are limited in number and diversity.
- **Low confidence**: The assertion that the method can identify "prognostic pixel biomarkers" is difficult to verify without explicit interpretability analysis or visualization of the learned representations.

## Next Checks
1. **Cross-domain validation**: Apply the TTE pretraining approach to MRI or X-ray datasets with different disease populations to assess generalizability beyond CT imaging and the specific conditions studied.
2. **EHR data sparsity analysis**: Systematically evaluate model performance as a function of EHR data completeness and temporal density to identify minimum data requirements for effective TTE pretraining.
3. **Computational efficiency benchmarking**: Profile the memory and runtime costs of pretraining with varying numbers of tasks and piecewise time bins to establish practical scaling limits and identify optimization opportunities.