---
ver: rpa2
title: Evaluating Defences against Unsafe Feedback in RLHF
arxiv_id: '2409.12914'
source_url: https://arxiv.org/abs/2409.12914
tags:
- harmful
- reward
- repnoise
- loss
- defences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of safety-aligned LLMs
  to reverse preference attacks (RPAs), where harmful data is used to flip preference
  labels during RLHF training. The authors find that RPAs can easily break safety
  alignment, even with a small percentage of flipped labels.
---

# Evaluating Defences against Unsafe Feedback in RLHF

## Quick Facts
- arXiv ID: 2409.12914
- Source URL: https://arxiv.org/abs/2409.12914
- Reference count: 21
- Primary result: Reverse preference attacks can easily break safety alignment in LLMs even with small percentages of flipped preference labels

## Executive Summary
This paper investigates the vulnerability of safety-aligned large language models (LLMs) to reverse preference attacks (RPAs) in reinforcement learning from human feedback (RLHF). The authors demonstrate that RPAs, where harmful data is used to flip preference labels during training, can effectively undo safety alignment even with as little as 25% of preference label flips. The study evaluates various defense mechanisms, distinguishing between online defenses (applied during training) and offline defenses (applied to model weights before training). The findings highlight the importance of considering the trade-off between safety and task performance when implementing defenses against RPAs.

## Method Summary
The authors evaluate defenses against RPAs using Llama2-7b-chat as the base model and BeaverTails and Safe RLHF datasets. They train a harmfulness reward model using GPT-2 medium on these datasets, then implement RPAs using DPO and PPO with varying percentages of flipped labels (0%, 25%, 50%, 75%, 100%). Online defenses (Lisa, Refusal Loss, Security Vectors, Vaccine, Safe RLHF) are evaluated during training, while offline defenses (RepNoise, Circuit Breakers, RMU, TAR) are trained on retain/harmful dataset pairs before attack. The evaluation metrics include harmfulness scores, helpfulness, perplexity, and KL divergence with the original policy.

## Key Results
- RPAs can undo safety alignment in LLMs even with as little as 25% of preference label flips
- Online defenses like Refusal Loss and Lisa are most effective at preventing harmfulness but require high penalties that may impact harmless task learning
- Offline defenses like RepNoise show some promise but are generally less effective at preventing harmful exploration during RLHF
- Safety-aligned LLMs easily explore unsafe action spaces via generating harmful text and optimize for reward that violates safety constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RPAs can undo safety alignment in LLMs even with a small percentage of flipped preference labels.
- Mechanism: The attacker reverses preference labels in harmlessness datasets, causing the reward model to optimize for harmful policies. RLHF algorithms like DPO and PPO then explore and reinforce harmful text generation.
- Core assumption: The reward model is trained on the flipped preference data and the RLHF algorithm optimizes for high reward without sufficient constraints.
- Evidence anchors:
  - [abstract] "We find that safety-aligned LLMs easily explore unsafe action spaces via generating harmful text and optimize for reward that violates safety constraints"
  - [section] "We find that popular RLHF methods, like DPO and PPO, easily undo safety guards even in cases with as little as 25% of preference label flips."
  - [corpus] Weak evidence; neighbors discuss jailbreak attacks and safety constraints but not RPAs specifically.
- Break condition: The attacker cannot flip a sufficient number of labels, or the reward model is robust to label noise.

### Mechanism 2
- Claim: Online defenses like Refusal Loss and Lisa are effective at preventing harmfulness by minimizing the negative log likelihood of refusals.
- Mechanism: The defense adds a term to the loss function that encourages the model to refuse harmful requests. This high penalty prevents the model from exploring harmful text generation policies.
- Core assumption: The defender has control over the loss function during training and can apply a sufficiently high penalty.
- Evidence anchors:
  - [abstract] "We find that 'online' defenses that are based on the idea of minimizing the negative log likelihood of refusals...can effectively protect LLMs against RPAs."
  - [section] "In Table 4 we find that across all methods Lisa and Refusal Loss are the most effective defences, maintaining low perplexity and high helpfulness."
  - [corpus] Weak evidence; neighbors discuss safety filters and constraints but not these specific online defenses.
- Break condition: The penalty is not high enough, or the attacker can adaptively stop training once a given reward score is achieved.

### Mechanism 3
- Claim: Offline defenses like RepNoise are less effective because they operate under the assumption that the defender has no control over the loss function.
- Mechanism: Offline defenses attempt to remove harmful representations from the model weights before training. However, they are not as effective at preventing the exploration of harmful text generation policies during RLHF.
- Core assumption: The defender can remove harmful representations from the model weights before training, and these representations are crucial for generating harmful text.
- Evidence anchors:
  - [abstract] "However, trying to defend model weights using 'offline' defenses that operate under the assumption that the defender has no control over the loss function are less effective in the face of RPAs."
  - [section] "Experimentally, we show that only some methods of defence against harmful fine-tuning attacks are effective in the RPA setting...Among offline defenses, Representation Noising [Rosati et al., 2024a, RepNoise] show promising results but fail to undo the effects of RPA satisfactorily."
  - [corpus] Weak evidence; neighbors discuss representation removal but not specifically for RPAs.
- Break condition: The attacker can still explore harmful text generation policies despite the removal of harmful representations, or the removal of representations impacts the model's ability to perform harmless tasks.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the training paradigm being attacked by RPAs. Understanding how RLHF works is crucial for understanding the threat model and the effectiveness of defenses.
  - Quick check question: What is the goal of RLHF and how does it differ from supervised fine-tuning?

- Concept: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: CMDPs provide a framework for formalizing the constraints that defenses impose on the RLHF training process. Understanding CMDPs is crucial for understanding how defenses work and their effectiveness.
  - Quick check question: How do CMDPs differ from standard MDPs and how are they used to formalize safety constraints?

- Concept: Preference Optimization
  - Why needed here: RPAs exploit the preference optimization setting of RLHF. Understanding how preference optimization works is crucial for understanding how RPAs can undo safety alignment.
  - Quick check question: What is preference optimization and how does it differ from reward learning?

## Architecture Onboarding

- Component map: Llama2-7b-chat -> Reward model -> DPO/PPO -> LLM -> Harmfulness classifier -> Evaluation metrics
- Critical path: 1. Train reward model on preference data 2. Use reward model in RLHF to train LLM 3. Evaluate harmfulness of trained LLM 4. Apply defenses to protect against RPAs 5. Re-evaluate harmfulness of defended LLM
- Design tradeoffs: Online defenses require control over loss function but are more effective; offline defenses operate before training but are less effective; high penalties may impact learning on harmless tasks
- Failure signatures: LLM generates harmful text despite defense; LLM's harmless task performance degrades significantly; RLHF training becomes unstable or fails to converge
- First 3 experiments: 1. Evaluate vulnerability to RPAs with varying flipped label percentages 2. Evaluate effectiveness of online defenses (Refusal Loss, Lisa) against RPAs 3. Evaluate effectiveness of offline defenses (RepNoise, TAR) against RPAs

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on specific datasets (BeaverTails, Safe RLHF) and model architectures, limiting generalizability
- The paper does not explore adaptive attacker strategies that could bypass defenses by stopping training early
- The impact of online defenses on harmless task performance requires further validation

## Confidence
- **High Confidence**: The claim that RPAs can easily break safety alignment with small percentages of flipped labels is well-supported by experimental results
- **Medium Confidence**: The effectiveness of online defenses like Refusal Loss and Lisa is supported, but the trade-off with task performance requires further validation
- **Low Confidence**: The relative effectiveness of offline defenses like RepNoise is less clear, with promising but inconclusive results

## Next Checks
1. Evaluate task performance impact: Conduct experiments to quantify the impact of online defenses on harmless task performance, ensuring that safety gains do not come at an unacceptable cost to utility
2. Test adaptive attacker strategies: Investigate whether attackers can bypass defenses by adaptively stopping training once a target reward score is achieved, exploring the robustness of defenses under adaptive attack scenarios
3. Generalize to other datasets and models: Validate the findings on different datasets and model architectures to assess the generalizability of the results and identify potential limitations in the study's scope