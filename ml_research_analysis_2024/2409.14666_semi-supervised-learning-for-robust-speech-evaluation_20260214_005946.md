---
ver: rpa2
title: Semi-supervised Learning For Robust Speech Evaluation
arxiv_id: '2409.14666'
source_url: https://arxiv.org/abs/2409.14666
tags:
- speech
- data
- evaluation
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of robust speech evaluation when
  training data is imbalanced and limited, leading to poor generalization for under-represented
  or out-of-distribution samples. The proposed method uses semi-supervised pre-training
  and objective regularization to approximate subjective evaluation criteria.
---

# Semi-supervised Learning For Robust Speech Evaluation

## Quick Facts
- arXiv ID: 2409.14666
- Source URL: https://arxiv.org/abs/2409.14666
- Authors: Huayun Zhang; Jeremy H. M. Wong; Geyu Lin; Nancy F. Chen
- Reference count: 0
- Primary result: Semi-supervised approach improves speech evaluation robustness with PCC up to 0.785 on in-distribution data and 20-53% PCC gains on out-of-distribution samples

## Executive Summary
This paper addresses the challenge of robust speech evaluation when training data is imbalanced and limited, leading to poor generalization for under-represented or out-of-distribution samples. The proposed method uses semi-supervised pre-training with pseudo-labels derived from normalized mutual information (NMI) between speech and reference, followed by objective regularization to approximate subjective evaluation criteria. Experiments on SpeechOcean762 dataset show high overall performance (PCC up to 0.785) and more balanced prediction errors across proficiency levels compared to state-of-the-art methods. The model demonstrates significantly improved robustness on out-of-distribution SingaKids data, with PCC improvements of 20-53% over standard training approaches.

## Method Summary
The method employs a two-stage semi-supervised training approach. First, an anchor model is pre-trained using pseudo-scores derived from normalized mutual information between speech and ASR-generated reference transcriptions. This creates an augmented training set with additional samples. Second, an evaluation model is trained using an interpolated loss function that combines standard MSE loss with predictions from the anchor model, weighted by a Gaussian kernel distance. The approach leverages both traditional speech evaluation features (GOP, Tempo, PhoEmb, Pitch) and self-supervised learning embeddings (Wav2Vec2, HuBERT, WavLM, Whisper) to capture comprehensive speech characteristics.

## Key Results
- Achieved PCC up to 0.785 on SpeechOcean762 pronunciation evaluation
- Demonstrated more balanced prediction errors across proficiency levels (1-10) compared to baseline methods
- Showed 20-53% PCC improvements on out-of-distribution SingaKids dataset for children's speech evaluation
- Reduced RMSE by 8-14% across different proficiency bands compared to state-of-the-art approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-supervised pre-training with pseudo-labels reduces over-fitting on imbalanced training data.
- Mechanism: By generating additional training samples using ASR-derived pseudo-references and corresponding pseudo-scores, the model is exposed to a wider distribution of data patterns before fine-tuning on the limited human-labeled set.
- Core assumption: Pseudo-labels derived from normalized mutual information between ground-truth reference and ASR-generated reference are sufficiently informative to guide pre-training.
- Evidence anchors: [abstract] "An anchor model is pre-trained in semi-supervised manner using these pseudo-scores." [section] "Multiple pseudo references ˆT are taken from ASR n-best transcriptions and multiple (O, T, ˆT ) are spawned from (O, T). Pre-training leverages the enriched variation..."
- Break condition: If pseudo-labels are too noisy (e.g., due to ASR errors or off-task references), the pre-training could mislead the model rather than regularize it.

### Mechanism 2
- Claim: Interpolated loss function balances fidelity to human scores with consistency to anchor model predictions, improving robustness.
- Mechanism: The interpolated MSE loss combines standard MSE (based on human scores) with weighted MSE (based on anchor model predictions), where the weighting factor is determined by the Gaussian kernel distance between the two scores. This encourages the model to predict scores close to both the human labels and the underlying speech-reference similarity.
- Core assumption: The anchor model's predictions are correlated with the ground-truth evaluation criteria, making them useful for regularization.
- Evidence anchors: [abstract] "An interpolated loss function is proposed to minimize not only the prediction error with respect to ground-truth scores but also the divergence between two probability distributions estimated by the speech evaluation model and the anchor model." [section] "Eq.(10) reduces to weighted MSE (WMSE) loss with respect to pseudo-scores... If 0 < ρ < 1, minimizing the KLD regularized cross-entropy loss... is equivalent to the minimization of a distance weighted interpolation between MSE loss with respect to human-scores and WMSE loss with respect to pseudo-scores."
- Break condition: If ρ is set too high, the model may ignore human scores; if too low, the regularization benefit is lost.

### Mechanism 3
- Claim: Using normalized mutual information (NMI) as a pseudo-score metric provides a data-driven similarity measure that is less prone to human bias and can generalize across datasets.
- Mechanism: NMI quantifies the reduction in uncertainty about the reference given the speech (or vice versa), providing a normalized measure of correctness that is not tied to a specific scoring rubric.
- Core assumption: Higher NMI between speech and reference correlates with better pronunciation and thus higher subjective evaluation scores.
- Evidence anchors: [abstract] "normalized mutual information is used to quantify the speech characteristics from the learner and the reference." [section] "0 ≤ N I(T, ˆT ) ≤ 1. When I(T, ˆT ) = H( ˆT ) = H(T ), N I(T, ˆT ) = 1... It means that all knowledge about ˆT comes from T , no information loss and no extra noise introduced."
- Break condition: If the relationship between NMI and subjective scores is not stable across datasets (e.g., different languages or proficiency scales), the pseudo-scores may not transfer well.

## Foundational Learning

- Concept: Mutual Information
  - Why needed here: It provides a theoretical foundation for the pseudo-score metric, quantifying the shared information between speech and reference.
  - Quick check question: How does normalized mutual information differ from standard mutual information, and why is normalization important for this application?

- Concept: Semi-supervised Learning
  - Why needed here: It allows the model to leverage large amounts of unlabeled data (speech with transcripts) to improve generalization when labeled data is scarce.
  - Quick check question: What are the key differences between semi-supervised learning and unsupervised learning in the context of speech evaluation?

- Concept: Kullback-Leibler Divergence
  - Why needed here: It is used to regularize the model's predictions by minimizing the divergence between the evaluation model's probability distribution and the anchor model's probability distribution.
  - Quick check question: Why is KL divergence a suitable choice for regularization in this context, and how does it compare to other divergence measures like Jensen-Shannon divergence?

## Architecture Onboarding

- Component map: Speech -> Feature extraction (GOP, Tempo, PhoEmb, Pitch, SSL embeddings) -> GOPT encoder -> [CLS] token -> Linear layer -> Tanh activation -> Score prediction
- Critical path: Speech → Feature extraction → GOPT encoder → [CLS] token → Linear layer → Tanh activation → Score prediction
- Design tradeoffs:
  - Feature richness vs. computational cost: Using multiple SSL embeddings improves performance but increases model size and inference time.
  - Pseudo-label quality vs. data augmentation scale: More ASR-generated references increase diversity but may introduce noise.
  - Regularization strength (ρ) vs. adherence to human scores: Higher ρ improves robustness but may decrease alignment with human labels.
- Failure signatures:
  - Over-fitting: High performance on in-distribution test set but significant drop on out-of-distribution data.
  - Pseudo-label noise: Poor performance of anchor model, indicating unreliable pseudo-scores.
  - Regularization imbalance: Very different predictions between evaluation and anchor models, suggesting ρ is not well-tuned.
- First 3 experiments:
  1. Ablation study: Train the evaluation model without the anchor model and interpolated loss to quantify the impact of the two-stage training.
  2. Pseudo-label analysis: Evaluate the quality of pseudo-scores by comparing NMI values with human scores on a subset of data.
  3. Robustness test: Evaluate the model on a deliberately challenging out-of-distribution dataset (e.g., different accent, age group, or scoring rubric) to assess generalization.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored based on the experimental design and results presented.

## Limitations

- Pseudo-label quality and NMI correlation: The paper relies heavily on normalized mutual information as a proxy for pronunciation quality but lacks direct validation that NMI scores correlate strongly with human evaluation criteria.
- Hyperparameter sensitivity: The interpolated loss function depends critically on the regularization weight ρ, set to 0.25 in experiments, with sensitivity to this parameter unexplored.
- Out-of-distribution generalization: While improvements on SingaKids data are reported, this dataset shares similar task structure; true robustness testing would require evaluation on fundamentally different speech tasks or languages.

## Confidence

- High confidence in the two-stage training framework and overall improvement in PCC scores on in-distribution test sets. The methodology is clearly described and performance gains are substantial.
- Medium confidence in robustness claims, particularly the 20-53% PCC improvements on out-of-distribution data. The SingaKids dataset, while different, is still within the same domain of English pronunciation assessment.
- Low confidence in the specific mechanisms by which normalized mutual information serves as an effective pseudo-score metric. The paper asserts this relationship but provides limited empirical validation.

## Next Checks

1. **Pseudo-label correlation study**: Conduct a controlled experiment comparing NMI scores against human evaluations on a held-out subset of SpeechOcean762. Calculate the correlation coefficient and analyze cases where NMI predictions diverge from human scores to understand failure modes.

2. **Cross-domain robustness test**: Evaluate the trained model on a completely different speech task (e.g., spontaneous speech evaluation, speech in noise, or a different language entirely) to assess whether the semi-supervised training provides genuine robustness beyond the tested out-of-distribution scenario.

3. **Hyperparameter sensitivity analysis**: Systematically vary the interpolation weight ρ from 0 to 1 in increments of 0.1, retraining the model each time. Plot the resulting PCC scores on both in-distribution and out-of-distribution data to identify the optimal range and assess sensitivity to this critical parameter.