---
ver: rpa2
title: Generating Visual Stimuli from EEG Recordings using Transformer-encoder based
  EEG encoder and GAN
arxiv_id: '2402.10115'
source_url: https://arxiv.org/abs/2402.10115
tags:
- images
- signals
- network
- loss
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of synthesizing images from noisy
  EEG signals using a conditional GAN framework. The proposed method uses a Transformer-encoder-based
  EEG encoder to extract class-specific EEG embeddings, which serve as input to the
  GAN generator.
---

# Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN

## Quick Facts
- arXiv ID: 2402.10115
- Source URL: https://arxiv.org/abs/2402.10115
- Reference count: 23
- Method achieves lower class diversity scores (0.6501 vs 0.7897) and comparable inception scores (5.1 vs 5.43) compared to state-of-the-art methods

## Executive Summary
This paper addresses the challenge of synthesizing images from noisy EEG signals using a conditional GAN framework. The proposed method employs a Transformer-encoder-based EEG encoder (C-former) to extract class-specific EEG embeddings, which serve as inputs to the GAN generator without additional noise. The GAN is trained with adversarial loss, classification loss from a pre-trained image classifier, and perceptual loss to improve realism and class relevance. Experiments on the ThoughtViz dataset demonstrate that the method achieves better class-specific image generation compared to state-of-the-art approaches.

## Method Summary
The proposed method uses a C-former Transformer encoder to transform raw EEG signals into class-specific embeddings, which are then used as the sole input to a conditional GAN generator. Unlike traditional GANs, no additional noise is added to the generator input to avoid degrading class-specific image generation. The GAN is trained with three loss components: adversarial loss for realism, classification loss from a pre-trained image classifier for class relevance, and perceptual loss to enhance visual quality. The approach is evaluated on the ThoughtViz dataset containing EEG recordings from 23 subjects viewing 10 ImageNet object classes.

## Key Results
- Lower class diversity scores (0.6501 vs 0.7897) compared to state-of-the-art methods, indicating better class-specific image generation
- Comparable inception scores (5.1 vs 5.43), demonstrating similar image quality to existing approaches
- Improved class-specificity achieved by using EEG encodings as sole generator input without additional noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The C-former EEG encoder extracts class-specific discriminative features from noisy EEG signals
- Mechanism: The C-former architecture integrates convolution, self-attention, and classification modules to transform raw EEG signals into low-dimensional embeddings
- Core assumption: EEG signals contain sufficient class-specific discriminative information
- Evidence anchors: [abstract] mentions using Transformer-encoder based EEG encoder; [section] describes convolution module design
- Break condition: If EEG signals lack sufficient class-specific information, generated images will not be class-relevant

### Mechanism 2
- Claim: Using EEG encodings as sole input without additional noise improves class-specific image generation
- Mechanism: Additional noise would further degrade the class-specific information already present in EEG encodings
- Core assumption: Additional noise interferes with class-specific information in EEG encodings
- Evidence anchors: [abstract] states no additional noise is used; [section] explains belief about noise deterioration
- Break condition: If EEG encodings are too noisy, generator may not have enough variation for diverse images

### Mechanism 3
- Claim: Combination of adversarial, classification, and perceptual losses produces superior results
- Mechanism: Each loss addresses distinct aspects: realism (adversarial), class-specificity (classification), and visual quality (perceptual)
- Core assumption: Each loss component addresses a distinct aspect of image quality
- Evidence anchors: [abstract] mentions perceptual loss enhancement; [section] describes classification loss usage
- Break condition: If loss weights are unbalanced, one aspect may dominate and degrade others

## Foundational Learning

- Concept: EEG signal preprocessing and feature extraction
  - Why needed here: Raw EEG signals are high-dimensional and noisy, requiring preprocessing to obtain meaningful embeddings
  - Quick check question: What preprocessing steps are applied to raw EEG data before feeding it to the C-former encoder?

- Concept: Conditional GAN architecture
  - Why needed here: Task requires generating images conditioned on EEG signals, necessitating conditional GAN
  - Quick check question: How does conditional GAN differ from standard GAN in terms of input and loss function?

- Concept: Perceptual loss and feature matching
  - Why needed here: Perceptual loss improves visual quality by comparing feature representations at intermediate layers
  - Quick check question: What is the mathematical formulation of perceptual loss, and how does it differ from pixel-wise reconstruction loss?

## Architecture Onboarding

- Component map: Raw EEG signals -> Sliding window preprocessing -> C-former encoder -> EEG encodings -> GAN generator -> Generated images
- Critical path: 1) Raw EEG signals → Sliding window preprocessing; 2) Preprocessed EEG → C-former encoder → EEG encodings; 3) EEG encodings → GAN generator → Generated images; 4) Generated images + Real images → GAN discriminator → Adversarial loss; 5) Generated images → Image classifier → Classification and perceptual loss; 6) Combine losses → Update GAN weights
- Design tradeoffs: No additional noise improves class-specificity but may reduce diversity; perceptual loss improves quality but increases computational cost; transformer-based encoder better feature extraction but more complex than CNN-only
- Failure signatures: Low diversity score (GAN generating wrong classes or similar images); low inception score (images lack realism or diversity); training instability (loss values oscillate or diverge)
- First 3 experiments: 1) Train C-former encoder on EEG classification task and evaluate classification accuracy; 2) Train GAN with only adversarial loss to establish baseline; 3) Add classification loss to GAN and compare class diversity scores

## Open Questions the Paper Calls Out

- Question: How does the proposed method perform on larger and more diverse EEG datasets with increased number of subjects and image classes?
- Basis in paper: [explicit] mentions substantial room for improvement in method performance, diversity of input considerations, and generalizability on larger datasets
- Why unresolved: Study conducted on relatively small dataset (ThoughtViz) with 23 subjects and 10 image classes
- What evidence would resolve it: Evaluating the proposed method on larger EEG datasets with more subjects and classes, comparing performance with state-of-the-art methods

- Question: How does the removal of additional noise input to the GAN affect class-specific image generation compared to methods that include noise?
- Basis in paper: [explicit] states EEG signals are noisy and encodings serve as sole input, believing additional noise might deteriorate performance
- Why unresolved: Paper claims noise removal improves class-specificity but lacks direct comparison with methods including noise
- What evidence would resolve it: Comparative study between proposed method and methods including additional noise, evaluating class-specific image generation

- Question: How does perceptual loss contribute to improvement in image quality and class-specific relevance compared to methods without perceptual loss?
- Basis in paper: [explicit] proposes perceptual loss to enhance realistic image generation and demonstrates improved performance in class diversity score
- Why unresolved: Paper does not provide direct comparison between method with and without perceptual loss
- What evidence would resolve it: Comparative study between proposed method with perceptual loss and methods without perceptual loss, evaluating image quality and class-specific relevance

## Limitations

- Experimental results show only modest improvements in class diversity scores with no statistical significance testing
- Method evaluated on relatively small dataset (23 subjects, 10 classes) limiting generalizability claims
- Limited comparative analysis with ablation studies on individual components like noise removal and perceptual loss

## Confidence

- C-former effectiveness in extracting class-specific features: Medium confidence (limited literature on transformer-based approaches for this specific task)
- Omitting noise improves class-specificity: Low confidence (theoretical reasoning without empirical comparison)
- Combination of three losses produces superior results: Medium confidence (lacks direct comparative evidence against other loss formulations)

## Next Checks

1. Conduct ablation study on noise addition by training GAN with and without additional noise while keeping all other components constant, then compare class diversity and inception scores

2. Extract and visualize EEG embeddings from C-former encoder for different classes, computing class separability metrics to verify class-discriminative features

3. Evaluate model's performance when trained on subset of subjects and tested on unseen subjects to assess robustness to individual EEG pattern differences across participants