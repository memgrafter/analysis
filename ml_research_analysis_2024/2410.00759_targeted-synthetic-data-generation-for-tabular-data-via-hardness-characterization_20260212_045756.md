---
ver: rpa2
title: Targeted synthetic data generation for tabular data via hardness characterization
arxiv_id: '2410.00759'
source_url: https://arxiv.org/abs/2410.00759
tags:
- data
- training
- points
- hardness
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a computationally efficient targeted synthetic
  data generation pipeline for tabular binary classification. The method uses KNN
  Shapley values to identify the hardest training points and trains synthetic data
  generators (TVAE and CTGAN) only on these points.
---

# Targeted synthetic data generation for tabular data via hardness characterization

## Quick Facts
- arXiv ID: 2410.00759
- Source URL: https://arxiv.org/abs/2410.00759
- Authors: Tommaso Ferracci; Leonie Tabea Goldmann; Anton Hinel; Francesco Sanna Passino
- Reference count: 40
- Primary result: Proposed targeted synthetic data generation pipeline improves model performance on tabular binary classification tasks while being computationally cheaper than non-targeted augmentation

## Executive Summary
This paper introduces a targeted synthetic data generation pipeline for tabular binary classification that uses KNN Shapley values to identify the hardest training points and generates synthetic data only for these points. The method employs TVAE and CTGAN as synthetic data generators, trained specifically on the identified hard instances. Experimental results on the American Express credit default dataset show that this targeted approach outperforms non-targeted augmentation in improving model performance on unseen data while being computationally more efficient. The KNN Shapley-based hardness characterization performs comparably to state-of-the-art methods like Data-IQ while offering advantages of being deterministic, model-agnostic, and computationally efficient.

## Method Summary
The proposed method employs a hardness characterization approach using KNN Shapley values to identify the most challenging training instances for binary classification tasks. Once these hard points are identified, the method trains synthetic data generators (specifically TVAE and CTGAN) exclusively on this subset of data. The synthetic samples generated from these hard points are then used to augment the original training set. The approach leverages Shapley values to quantify the contribution of each training instance to the overall hardness of the dataset, focusing computational resources on the most informative samples rather than generating synthetic data across the entire training set indiscriminately.

## Key Results
- On the American Express credit default dataset, targeted augmentation with TVAE improved the validation Gini coefficient by up to 0.000431 compared to 0.000278 for non-targeted augmentation
- The KNN Shapley-based hardness characterization performed comparably to state-of-the-art methods like Data-IQ
- The targeted approach demonstrated computational efficiency advantages over non-targeted augmentation methods

## Why This Works (Mechanism)
The method works by focusing synthetic data generation on the most challenging instances identified through KNN Shapley values, which measure the marginal contribution of each training point to model hardness. By concentrating augmentation efforts on these hard cases rather than uniformly across all training data, the approach efficiently addresses the model's weaknesses where they matter most. This targeted strategy avoids wasting computational resources on generating synthetic data for easy-to-classify instances that provide minimal learning value, while directly addressing the model's failure modes through focused augmentation of challenging examples.

## Foundational Learning
**KNN Shapley Values**
- Why needed: Quantify the marginal contribution of each training instance to dataset hardness
- Quick check: Verify Shapley values sum to expected hardness metric across all instances

**Synthetic Data Generation (TVAE/CTGAN)**
- Why needed: Generate realistic synthetic samples that preserve statistical properties of original data
- Quick check: Compare synthetic data distribution to original data using statistical tests

**Tabular Hardness Characterization**
- Why needed: Identify which training instances are most difficult for models to classify
- Quick check: Validate hardness rankings through model performance degradation when removing instances

**Binary Classification Metrics**
- Why needed: Evaluate model performance improvements from targeted augmentation
- Quick check: Calculate Gini coefficient on validation set to measure ranking quality

## Architecture Onboarding

**Component Map**
Data -> KNN Shapley Hardness Characterization -> Synthetic Data Generator (TVAE/CTGAN) -> Augmented Training Set -> Classification Model

**Critical Path**
1. Compute KNN Shapley values for all training instances
2. Select hardest instances based on Shapley values
3. Train synthetic data generator on selected hard instances
4. Generate synthetic samples
5. Augment original training set with synthetic samples
6. Train classification model on augmented data

**Design Tradeoffs**
- Targeted vs. non-targeted augmentation: Computational efficiency vs. potential bias toward hard instances
- TVAE vs. CTGAN: Different generative approaches with varying performance characteristics
- Number of hard instances: Balancing augmentation effectiveness with overfitting risk

**Failure Signatures**
- Poor performance improvement: Indicates ineffective hardness characterization or synthetic data generation
- Overfitting to synthetic data: Suggests too many synthetic samples or poor quality generation
- Computational inefficiency: Points to suboptimal selection of hard instances or generation parameters

**First Experiments**
1. Baseline: Train model on original data without augmentation
2. Control: Generate synthetic data uniformly across all training instances
3. Validation: Compare model performance on validation set across different augmentation strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on quality of KNN Shapley hardness characterization, which may not generalize well to datasets with complex feature interactions or non-linear decision boundaries
- Validation is primarily conducted on a single real-world credit default dataset, raising concerns about external validity across diverse tabular data domains
- The improvement in Gini coefficient (0.000431 vs 0.000278) is marginal, and statistical significance of these differences is not explicitly established

## Confidence
- **High Confidence**: The computational efficiency advantage of the proposed targeted approach over non-targeted augmentation methods is well-supported by experimental design and results
- **Medium Confidence**: The claim that KNN Shapley-based hardness characterization performs comparably to state-of-the-art methods like Data-IQ is reasonable but requires broader validation across more datasets and hardness characterization techniques
- **Medium Confidence**: The assertion that the method is "deterministic, model-agnostic, and computationally efficient" is supported by methodology but would benefit from more extensive testing across different model architectures and data types

## Next Checks
1. Conduct experiments on at least 5-10 additional real-world tabular datasets spanning different domains (finance, healthcare, marketing) to assess generalizability of the hardness characterization approach
2. Perform statistical significance testing (e.g., paired t-tests) on performance improvements to determine whether observed gains in Gini coefficient are meaningful rather than due to random variation
3. Compare the proposed method against recently developed hardness characterization techniques beyond Data-IQ, such as those using gradient-based saliency or influence functions, to establish relative performance in current state-of-the-art landscape