---
ver: rpa2
title: Mitigating Hallucinations in Large Vision-Language Models with Instruction
  Contrastive Decoding
arxiv_id: '2403.18715'
source_url: https://arxiv.org/abs/2403.18715
tags:
- hallucinations
- lvlms
- visual
- language
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucinations in Large Vision-Language Models
  (LVLMs), where generated text inaccurately represents visual content. The authors
  propose Instruction Contrastive Decoding (ICD), a training-free method that mitigates
  hallucinations during inference by contrasting distributions from standard and disturbance
  instructions.
---

# Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding

## Quick Facts
- arXiv ID: 2403.18715
- Source URL: https://arxiv.org/abs/2403.18715
- Reference count: 16
- Primary result: Instruction Contrastive Decoding (ICD) improves POPE accuracy by 7.0-8.5% precision compared to miniGPT4 and InstructBLIP baselines

## Executive Summary
This paper addresses hallucinations in Large Vision-Language Models (LVLMs), where generated text inaccurately represents visual content. The authors propose Instruction Contrastive Decoding (ICD), a training-free method that mitigates hallucinations during inference by contrasting distributions from standard and disturbance instructions. ICD leverages the observation that appending role prefixes to instructions (disturbance instructions) exacerbates hallucinations by increasing multimodal alignment uncertainty. The method effectively subtracts hallucinated concepts from the original distribution. Extensive experiments on POPE, MME, and LLaVa-Bench benchmarks demonstrate that ICD significantly reduces both object-level and attribute-level hallucinations while improving general perception and recognition capabilities across 14 tasks.

## Method Summary
The Instruction Contrastive Decoding (ICD) method operates by contrasting token probability distributions generated from standard instructions against those generated from disturbance instructions during inference. The approach involves creating disturbance instructions by appending role prefixes (e.g., "You are a confused object detector") to standard instructions, which amplifies hallucinations by increasing multimodal alignment uncertainty. ICD then performs contrastive decoding to subtract hallucinated concepts from the original distribution, selecting tokens that maximize probability under the standard instruction while minimizing probability under the disturbance instruction. The method incorporates adaptive plausibility constraints to ensure that correct, high-confidence predictions are not penalized while implausible hallucinated tokens are suppressed.

## Key Results
- ICD improves POPE accuracy by 7.0% and 8.5% in precision compared to miniGPT4 and InstructBLIP baselines
- ICD reduces both object-level and attribute-level hallucinations on POPE benchmark
- ICD enhances general perception and recognition capabilities across 14 tasks in the MME benchmark, outperforming existing methods like visual contrastive decoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Appending role prefixes to instructions (disturbance instructions) increases multimodal alignment uncertainty, which exacerbates hallucinations by making the model more reliant on statistical biases and language priors from training data.
- Mechanism: Disturbance instructions create ambiguity in the multimodal alignment module, causing the model to shift probability mass toward tokens that are statistically more likely but not visually grounded.
- Core assumption: The Q-Former's alignment with visual features is sensitive to the form of the instruction, and that sensitivity can be exploited to amplify hallucinated concepts.
- Evidence anchors: [abstract] "Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules." [section 3.2] "We introduce the concept of instruction disturbance, which entails appending a role prefix to the original instructions... This disturbance aims to modulate the multimodal alignment uncertainty within LVLMs."
- Break condition: If the Q-Former is not sensitive to instruction perturbations, or if the visual encoder provides features that are too discriminative to be swayed by instruction changes, the disturbance effect will not manifest.

### Mechanism 2
- Claim: By contrasting distributions from standard and disturbance instructions, ICD can subtract hallucinated concepts from the original distribution, thereby reducing hallucinations.
- Mechanism: ICD generates two token probability distributions: one conditioned on the standard instruction and one conditioned on the disturbance instruction. The disturbance instruction amplifies hallucinated concepts, so the difference between the two distributions highlights and suppresses those hallucinated tokens in the final output.
- Core assumption: Hallucinated concepts are consistently amplified by disturbance instructions across different images and contexts, making them detectable via contrastive subtraction.
- Evidence anchors: [abstract] "ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution." [section 3.3.1] "Our strategy involves choosing the token that concurrently maximizes pϕ(yt|XV , Xins) and minimizes pϕ(yt|XV , X'ins), the latter representing the probability of tokens that are more likely to be hallucinations."
- Break condition: If the disturbance instruction does not consistently amplify the same hallucinated concepts, or if the model learns to ignore the disturbance instruction over time, the contrastive subtraction will not reliably reduce hallucinations.

### Mechanism 3
- Claim: Adaptive plausibility constraints ensure that ICD does not penalize correct, high-confidence predictions while still suppressing implausible hallucinated tokens.
- Mechanism: The adaptive plausibility constraint restricts the token selection to those whose probability under the standard instruction is above a threshold (α times the maximum probability). This prevents the model from choosing tokens that are implausible even if they are not explicitly hallucinated by the disturbance instruction.
- Core assumption: The model's confidence in correct predictions is sufficiently high that they will pass the plausibility threshold, while hallucinated tokens will not.
- Evidence anchors: [section 3.3.2] "we refine the ICD objective to incorporate an adaptive plausibility constraint... This is particularly crucial for mitigating the influence of implausible tokens, especially when LVLMs exhibit high confidence and are accurately anchored in visual semantics."
- Break condition: If the plausibility threshold is set too high, it may exclude correct but less confident predictions; if set too low, it may allow implausible hallucinations to pass through.

## Foundational Learning

- Concept: Multimodal alignment in vision-language models
  - Why needed here: Understanding how LVLMs fuse visual and textual information is crucial for grasping how disturbance instructions can affect the alignment and lead to hallucinations.
  - Quick check question: What is the role of the Q-Former in the InstructBLIP architecture, and how does it contribute to multimodal alignment?

- Concept: Contrastive decoding
  - Why needed here: ICD is based on contrastive decoding, so understanding how contrasting two distributions can be used to suppress unwanted tokens is essential.
  - Quick check question: How does contrastive decoding differ from standard decoding in language models, and what are its potential benefits and drawbacks?

- Concept: Statistical biases and language priors in training data
  - Why needed here: ICD leverages the fact that disturbance instructions amplify hallucinations caused by statistical biases and language priors; understanding these biases is key to understanding the method's effectiveness.
  - Quick check question: What are some examples of statistical biases and language priors that could lead to hallucinations in LVLMs, and how might they manifest in generated text?

## Architecture Onboarding

- Component map: Image → Visual encoder → Fusion module (with ICD) → Language model → Generated text
- Critical path: The visual encoder (e.g., ViT-L/14 from CLIP) extracts visual features, which are then aligned with instructions through the fusion module (Q-Former) that incorporates ICD, before being processed by the language model (e.g., Vicuna 7B) to generate text responses.
- Design tradeoffs:
  - ICD introduces additional computation during inference due to the need to generate two distributions (standard and disturbance)
  - The choice of disturbance instruction and plausibility threshold can significantly impact performance and may require tuning
  - ICD is training-free but relies on the pre-trained model's sensitivity to instruction perturbations
- Failure signatures:
  - If ICD consistently underperforms the baseline LVLM, it may indicate that the disturbance instruction is not effectively amplifying hallucinations or that the contrastive subtraction is not working as intended
  - If ICD introduces new types of errors or reduces performance on certain tasks, it may suggest that the plausibility constraints are too strict or that the method is not well-suited for those tasks
- First 3 experiments:
  1. Implement ICD with a simple disturbance instruction (e.g., "You are a confused object detector") and evaluate its performance on a hallucination discrimination benchmark like POPE
  2. Compare the performance of ICD with and without the adaptive plausibility constraint to assess its impact on hallucination mitigation and overall task performance
  3. Experiment with different disturbance instructions and plausibility thresholds to find the optimal configuration for a specific LVLM and task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of disturbance instructions (e.g., role prefixes) affect the severity of hallucinations in LVLMs?
- Basis in paper: [explicit] The paper introduces disturbance instructions as role prefixes appended to standard instructions and observes their impact on hallucinations.
- Why unresolved: While the paper explores the concept of disturbance instructions, it doesn't extensively analyze the variations in hallucination severity across different types of role prefixes.
- What evidence would resolve it: Systematic experimentation with a diverse set of disturbance instructions, quantifying the degree of hallucination amplification for each type.

### Open Question 2
- Question: Can the ICD method be effectively integrated with other hallucination mitigation techniques, such as visual contrastive decoding (VCD), to achieve synergistic improvements?
- Basis in paper: [inferred] The paper briefly explores a combined ICD+VCD approach and observes performance improvements in some tasks, but the analysis is limited.
- Why unresolved: The paper doesn't provide a comprehensive investigation into the optimal combination strategies or the full potential of integrating ICD with other methods.
- What evidence would resolve it: Extensive experiments comparing various integration strategies, including ablation studies and fine-tuning of hyperparameters, to identify the most effective combination.

### Open Question 3
- Question: How does the ICD method impact the performance of LVLMs on tasks that require strong visual discrimination capabilities, such as landmark recognition or OCR?
- Basis in paper: [explicit] The paper notes that ICD's performance on landmark, OCR, and text translation tasks is weaker than VCD, suggesting potential limitations in visual discrimination.
- Why unresolved: The paper doesn't delve into the specific reasons behind this observation or explore potential modifications to ICD to improve performance on these tasks.
- What evidence would resolve it: Detailed analysis of ICD's behavior on visual discrimination tasks, potentially involving visualization of the model's attention or investigation of the learned query vectors.

## Limitations
- ICD introduces additional inference-time computation by requiring two distribution generations (standard and disturbance instructions)
- The method relies on the pre-trained model's sensitivity to instruction perturbations, which may vary across different LVLM architectures
- Performance on tasks requiring strong visual discrimination (landmark recognition, OCR) is weaker than VCD, suggesting potential limitations in certain visual perception capabilities

## Confidence
- Confidence: Medium in the claimed hallucination mitigation mechanism due to lack of ablation studies on disturbance instruction variations
- Confidence: Low regarding the generality of the method across different LVLM architectures beyond InstructBLIP and miniGPT4
- Confidence: Medium in the quantitative improvements, but statistical significance testing is not provided for the reported gains

## Next Checks
1. Implement ICD on additional LVLM architectures beyond InstructBLIP (e.g., LLaVA, GIT) to test whether the disturbance instruction amplification mechanism generalizes across different Q-Former implementations and fusion strategies.

2. Systematically evaluate how different types of role prefixes (positive, negative, neutral) affect hallucination amplification rates to clarify whether the method's effectiveness depends on specific prefix characteristics.

3. Recompute the reported improvements with confidence intervals and statistical tests (e.g., paired t-tests) to confirm that the observed performance gains are not due to random variation, particularly for the marginal improvements over VCD on certain MME tasks.