---
ver: rpa2
title: Reward Design for Justifiable Sequential Decision-Making
arxiv_id: '2402.15826'
source_url: https://arxiv.org/abs/2402.15826
tags:
- evidence
- debate
- agent
- judge
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using debate between two agents as a reward
  model for training RL policies that can justify their decisions. The debate game
  has agents take turns proposing evidence for competing actions, and a judge model
  evaluates which action is better justified based on the evidence.
---

# Reward Design for Justifiable Sequential Decision-Making

## Quick Facts
- arXiv ID: 2402.15826
- Source URL: https://arxiv.org/abs/2402.15826
- Reference count: 37
- This paper proposes using debate between two agents as a reward model for training RL policies that can justify their decisions.

## Executive Summary
This paper introduces a novel approach to training RL policies that can justify their decisions by using a debate-based reward model. The method involves two argumentative agents proposing evidence for competing actions in a structured debate game, with a judge model evaluating which action is better justified. The debate reward is then combined with environment rewards to train a justifiable policy. Experiments on a sepsis treatment task demonstrate that debate-based feedback enables training policies that are both effective and highly favored by the judge, compared to policies trained with environment rewards alone. The results suggest that debate is a promising approach for specifying rewards that incentivize both task performance and decision justifiability.

## Method Summary
The proposed method involves training a judge model to evaluate decisions based on proposed evidence, then training argumentative agents to debate by proposing evidence, and finally training a justifiable policy using a weighted combination of environment and debate rewards. The debate game is formalized as a two-player zero-sum extensive-form game, and argumentative agents are trained via self-play or maxmin optimization to propose evidence that supports their actions. The judge model is trained on a preference dataset derived from clinician decisions, learning to predict pairwise preferences over actions given proposed evidence. The justifiable policy is then trained using deep RL methods (e.g., double DQN with dueling architecture) with a mixed reward signal combining environment rewards and debate rewards.

## Key Results
- Debate-based feedback enables training policies that are both effective and highly favored by the judge, compared to policies trained with environment rewards alone.
- The debate-based feedback achieves similar performance and alignment to state-based feedback, despite only exposing the judge to a small fraction of the state.
- Multi-agent debate is important for learning robust argumentation strategies that are resilient to refutations.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Debate-based reward modeling allows training policies that are both effective and justifiable by exposing the judge to partial state information rather than the full state.
- **Mechanism:** The debate game acts as a structured argumentation process where two agents propose evidence supporting competing actions. The judge evaluates justifiability based only on this evidence, which is a small subset of the full state. This selective exposure enables the judge to focus on the most relevant information for decision evaluation, reducing the cognitive load and aligning better with human preferences.
- **Core assumption:** A proxy judge trained on pairwise preferences over evidence can approximate human judgment of justifiability with reasonable accuracy, even without full state access.
- **Evidence anchors:**
  - [abstract]: "the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state."
  - [section]: "This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions."
  - [corpus]: Weak—no directly comparable debate-based reward design papers found; most neighbor papers focus on LLM debate or ethical justification without RL reward design.
- **Break condition:** If the judge cannot recover human preferences from limited evidence (e.g., accuracy drops significantly below 65%), the mechanism fails because the evidence is not sufficiently informative.

### Mechanism 2
- **Claim:** Multi-agent debate is essential for learning robust argumentation strategies that are resilient to refutations.
- **Mechanism:** In a two-player zero-sum debate game, each agent learns to propose evidence that not only supports its action but also anticipates and defends against counterarguments. This adversarial setup encourages the development of evidence sets that are convincing and hard to refute, improving the justifiability of the trained policy.
- **Core assumption:** The debate game is a perfect-information extensive-form game where strategies can be learned via self-play or maxmin optimization, leading to robust Nash equilibria.
- **Evidence anchors:**
  - [abstract]: "agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences."
  - [section]: "Both maxmin and self-play agents managed to keep the judge's accuracy to about 85% when faced with a confuser."
  - [corpus]: Weak—no direct evidence of multi-agent debate improving robustness in RL reward design; neighbor papers focus on LLM reasoning.
- **Break condition:** If the isolated agent's evidence is easily refuted (judge accuracy drops to ~38% with confuser), and multi-agent debate does not improve this, the mechanism fails.

### Mechanism 3
- **Claim:** Combining debate rewards with environment rewards via a tunable coefficient λ allows balancing task performance and justifiability.
- **Mechanism:** The final reward is a weighted sum of environment reward and debate reward. By adjusting λ, the designer can control the trade-off between achieving high performance and producing justifiable decisions. This flexibility enables deployment in contexts where justifiability is prioritized differently.
- **Core assumption:** The debate reward is properly scaled (α=5) and aligned with human notions of justifiability, so that mixing it with environment rewards yields policies that satisfy both objectives.
- **Evidence anchors:**
  - [abstract]: "augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance."
  - [section]: "The observed inherent trade-off between performance and justifiability suggests that tuning the debate coefficient λ is important in practice."
  - [corpus]: Weak—no direct evidence of λ tuning in debate-based RL reward design; neighbor papers do not discuss this trade-off.
- **Break condition:** If increasing λ consistently degrades environment reward performance without improving judge preference, or vice versa, the mechanism fails to achieve the desired balance.

## Foundational Learning

- **Concept:** Markov Decision Processes (MDPs)
  - Why needed here: The environment is modeled as an MDP, and the RL agent learns policies to maximize expected return within this framework.
  - Quick check question: What are the components of an MDP, and how does the agent interact with it over time?

- **Concept:** Extensive-form games and Nash equilibrium
  - Why needed here: The debate game is formalized as a two-player zero-sum extensive-form game, and solving it requires finding a Nash equilibrium strategy.
  - Quick check question: How does a Nash equilibrium in a perfect-information extensive-form game guarantee optimal play for both agents?

- **Concept:** Reinforcement learning with function approximation
  - Why needed here: The baseline and justifiable policies are learned using deep RL methods (e.g., double DQN with dueling architecture) that rely on neural networks to approximate value functions.
  - Quick check question: Why is function approximation necessary in this context, and what are the risks (e.g., instability, overestimation)?

## Architecture Onboarding

- **Component map:**
  - Environment (MIMIC-III sepsis simulator) -> Baseline policy (DQN) -> Justifiable policy (DQN with mixed reward) -> Judge model (neural network) -> Argumentative agents (policies for debate game)

- **Critical path:**
  1. Generate preference dataset from clinician actions.
  2. Train judge model on dataset.
  3. Train argumentative agents via self-play or maxmin.
  4. Train justifiable policy with mixed reward.
  5. Evaluate policy performance and justifiability.

- **Design tradeoffs:**
  - Number of evidence (L): More evidence increases state visibility but may include irrelevant info; fewer evidence focus on relevance but risk missing key info.
  - λ coefficient: Higher λ increases justifiability but may reduce performance; lower λ does the opposite.
  - Debate vs. state feedback: Debate feedback uses less state info but may be sufficient; state feedback uses full info but is less practical.

- **Failure signatures:**
  - Judge accuracy < 60% on preference recovery → evidence not informative enough.
  - Justifiable policy performance << baseline → debate reward not well aligned or λ too high.
  - Argumentative agents easily refuted → debate game not properly solved or evidence space too large.

- **First 3 experiments:**
  1. Train judge on preference dataset with L=4 evidence; measure accuracy.
  2. Train self-play argumentative agents; evaluate judge accuracy with and without confuser.
  3. Train justifiable policy with λ=0.25; compare performance and judge preference to baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of debate coefficient λ affect the trade-off between policy performance and justifiability in different domains beyond healthcare?
- Basis in paper: [explicit] The paper mentions that tuning λ is important in practice and that there is an inherent trade-off between performance and justifiability, but does not explore this trade-off in domains other than sepsis treatment.
- Why unresolved: The paper only evaluates the impact of λ on policy performance and justifiability in the specific context of sepsis treatment. It does not investigate how this trade-off might manifest in other sequential decision-making domains.
- What evidence would resolve it: Conducting experiments to train justifiable policies with varying λ values in different sequential decision-making domains (e.g., robotics, finance, natural language processing) and comparing the resulting performance-justifiability trade-offs would provide insights into the generalizability of the findings.

### Open Question 2
- Question: How do different methods of defining the set of evidence in the debate game impact the performance and justifiability of learned policies?
- Basis in paper: [explicit] The paper mentions that finding clear and expressive arguments is a challenge and briefly discusses state features and previous trajectories as potential evidence sources, but does not systematically explore the impact of different evidence definition methods.
- Why unresolved: The paper focuses on using state features as evidence and does not investigate how alternative methods of defining the evidence set (e.g., using natural language, images, or other modalities) might affect the quality of the learned policies.
- What evidence would resolve it: Conducting experiments to train justifiable policies using different methods of defining the evidence set and comparing the resulting policy performance and justifiability would provide insights into the importance of evidence definition.

### Open Question 3
- Question: How can the debate framework be adapted to handle cases where the human judge's preferences are not binary but involve a spectrum of justifiability?
- Basis in paper: [inferred] The paper assumes a binary preference model (Bradley-Terry model) for the human judge, but in practice, human judgments of justifiability may involve a more nuanced spectrum of preferences.
- Why unresolved: The paper does not explore how the debate framework can be extended to handle cases where the human judge's preferences are not binary but involve a more nuanced spectrum of justifiability.
- What evidence would resolve it: Developing and evaluating extensions to the debate framework that can handle a spectrum of human preferences (e.g., using continuous-valued utility functions or probabilistic models) and comparing the resulting policy performance and justifiability would provide insights into the adaptability of the framework.

## Limitations
- The reliance on synthetic preference data derived from clinician decisions may not fully capture human notions of justifiability.
- The experiments are confined to a single medical decision-making task, raising questions about generalizability to other domains.
- The paper does not thoroughly investigate how judge performance scales with evidence complexity or state dimensionality.

## Confidence
- Debate rewards can balance task performance and justifiability: **Medium confidence**
- Multi-agent debate is essential for learning robust argumentation strategies: **Medium confidence**
- Debate-based feedback is comparable to state-based feedback: **High confidence**

## Next Checks
1. **Judge Model Robustness:** Test the judge model's performance on out-of-distribution evidence and varying evidence lengths to assess its ability to generalize beyond the training distribution.
2. **Domain Generalization:** Apply the debate-based reward modeling approach to a different sequential decision-making task (e.g., autonomous driving or financial trading) to evaluate its effectiveness in diverse contexts.
3. **Ablation Study on Debate Components:** Conduct an ablation study removing key components of the debate game (e.g., adversarial structure, evidence length) to isolate their individual contributions to policy justifiability and performance.