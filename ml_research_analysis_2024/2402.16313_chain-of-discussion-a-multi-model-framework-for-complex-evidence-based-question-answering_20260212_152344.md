---
ver: rpa2
title: 'Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question
  Answering'
arxiv_id: '2402.16313'
source_url: https://arxiv.org/abs/2402.16313
tags:
- question
- llms
- articles
- evidence
- article
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chain-of-Discussion (CoD), a novel framework
  that leverages multiple open-source large language models to improve complex evidence-based
  question answering. The framework employs a multi-step process where models first
  analyze questions and evidence, then engage in mutual criticism and revision to
  produce more accurate and comprehensive answers.
---

# Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering

## Quick Facts
- arXiv ID: 2402.16313
- Source URL: https://arxiv.org/abs/2402.16313
- Authors: Mingxu Tao; Dongyan Zhao; Yansong Feng
- Reference count: 20
- Primary result: CoD framework improves evidence-based QA performance across four 7B-parameter models with GPT-4 evaluation scores improving by 0.11-0.34

## Executive Summary
This paper introduces Chain-of-Discussion (CoD), a novel framework that leverages multiple open-source large language models to improve complex evidence-based question answering. The framework employs a multi-step process where models first analyze questions and evidence, then engage in mutual criticism and revision to produce more accurate and comprehensive answers. Experiments on a legal consultation dataset show that CoD significantly improves performance compared to single-model baselines, with improvements ranging from 0.11 to 0.34 in GPT-4 evaluation scores. The framework demonstrates particular effectiveness in enhancing evidence selection accuracy and reducing hallucinations, though limitations remain due to the small scale of the models used.

## Method Summary
CoD is a multi-model framework that orchestrates several open-source LLMs to collaboratively answer complex evidence-based questions. The process involves four stages: (1) multiple LLMs independently analyze the question to capture diverse perspectives, (2) the target LLM analyzes evidence while other LLMs critique this analysis, (3) the target LLM revises its evidence analysis if a majority of critics disagree beyond a threshold δ, and (4) a final answer is generated. The framework uses a two-stage pipeline with separate question and evidence analysis phases, employing a revising threshold δ to control when revisions occur. The framework was tested on a legal consultation dataset using four different 7B-parameter open-source models.

## Key Results
- GPT-4 evaluation scores improved by 0.11-0.34 across four different 7B-parameter models compared to single-model baselines
- Evidence selection accuracy (N-Acc/O-Acc) showed consistent improvements, though absolute performance remains suboptimal (0.74-0.83 range)
- The framework effectively reduces hallucinations through consensus-based evidence analysis and revision
- Performance gains are particularly notable in complex reasoning scenarios requiring multiple evidence documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple LLMs can cross-check each other's evidence analysis to reduce hallucinations and improve accuracy
- Mechanism: Each LLM critiques the evidence analysis of the target model. If a majority disagree, the target model revises its analysis. This creates a consensus-based filtering process.
- Core assumption: Different LLMs have different intrinsic knowledge and reasoning capabilities, making concurrent errors less likely
- Evidence anchors:
  - [abstract]: "Different LLMs may have different intrinsic knowledge and reasoning capabilities due to different training data. Thus, multiple LLMs can be less possible to make errors concurrently than a single LLM."
  - [section 4.2]: "A single LLM might generate hallucinated outputs...and incorrectly assess the relevance between evidence documents and the given question. Inspired by previous work (Zhang et al., 2023), we propose a multi-party discussion framework to improve the quality of evidence analysis."

### Mechanism 2
- Claim: Summarizing multiple question analyses captures diverse perspectives and improves comprehensiveness
- Mechanism: Each LLM analyzes the question independently, then the target LLM summarizes these analyses, prioritizing consensus while still considering minority viewpoints
- Core assumption: Different LLMs will identify different relevant aspects of complex questions due to their varied training
- Evidence anchors:
  - [abstract]: "Different LLMs can have different intrinsic knowledge and reasoning capabilities...Thus, multiple LLMs can be less possible to make errors concurrently than a single LLM."
  - [section 4.1]: "Different LLMs can have varying preferences in analyzing the potential scenarios. Therefore, we believe that by integrating the outputs of multiple LLMs, we can take more helpful scenarios into account, thus improve the comprehensiveness of question analysis."

### Mechanism 3
- Claim: Multi-stage prompting with intermediate analysis steps improves reasoning quality
- Mechanism: The framework breaks down the task into question analysis, evidence analysis, and response generation stages, allowing LLMs to focus on one aspect at a time
- Core assumption: LLMs perform better when tasks are decomposed into simpler, focused sub-tasks rather than handling everything in one pass
- Evidence anchors:
  - [section 3]: "Previous works have revealed that the CoT prompt can enhance the ability of LLMs to handle complex reasoning tasks...Inspired by these works, we employ a multi-step prompt to stimulate LLMs to generate more correct while comprehensive answers."

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG) and its limitations
  - Why needed here: Understanding why basic RAG is insufficient for complex evidence-based QA tasks
  - Quick check question: What are the two main challenges with RAG that this paper identifies?

- Concept: Chain-of-thought (CoT) prompting and multi-step reasoning
  - Why needed here: The paper builds on CoT principles but extends them to multi-model interaction
  - Quick check question: How does the paper's two-stage pipeline differ from simple chain-of-thought prompting?

- Concept: Hallucination in LLMs and mitigation strategies
  - Why needed here: The framework specifically targets hallucination reduction through consensus
  - Quick check question: What is the revising threshold δ and how does it control when a model should revise its evidence analysis?

## Architecture Onboarding

- Component map: Retriever -> Question Analysis (n LLMs) -> Summary Module -> Evidence Analysis (target LLM) -> Critics (n-1 LLMs) -> Revision check -> Response Generation

- Critical path: 1. Retrieve articles → 2. Parallel question analysis by all LLMs → 3. Summarize question analysis → 4. Target LLM analyzes evidence → 5. Critics evaluate evidence analysis → 6. Revise if needed → 7. Generate final response

- Design tradeoffs:
  - More LLMs provide better coverage but increase computational cost and complexity
  - Lower revising threshold (δ) increases revisions but may lead to over-correction
  - Higher temperatures during generation could increase diversity but reduce consistency
  - Using only open-source models limits performance ceiling but enables research reproducibility

- Failure signatures:
  - Low N-Acc and O-Acc scores indicate poor evidence selection
  - Consistent hallucinations across all LLMs suggest shared biases
  - Refusal to revise despite clear errors indicates LLM preference issues
  - Summary incorporating irrelevant minority viewpoints shows poor filtering

- First 3 experiments:
  1. Run baseline CoT with single LLM and measure N-Acc/O-Acc
  2. Implement only Stage 1 (summarize question analyses) and compare improvements
  3. Implement only Stage 2 (critique evidence analysis) and measure hallucination reduction

## Open Questions the Paper Calls Out
None identified in the provided text.

## Limitations
- The framework's effectiveness with larger models (70B+ parameters) remains untested
- Computational overhead of multiple model interactions may limit practical deployment
- Absolute evidence selection accuracy remains suboptimal despite improvements (0.74-0.83 N-Acc/O-Acc range)

## Confidence

- High confidence: The framework architecture and experimental methodology are sound and reproducible
- Medium confidence: The effectiveness of the consensus-based hallucination reduction mechanism, though results are promising
- Medium confidence: The improvement in evidence selection accuracy, though absolute performance remains suboptimal
- Low confidence: Generalizability to domains beyond legal consultation and to larger model sizes

## Next Checks
1. **Model Size Scaling Test**: Implement CoD with 70B-parameter models and measure performance gains relative to the 7B results, assessing whether larger models benefit equally from multi-model collaboration
2. **Cross-Domain Validation**: Apply the framework to medical and scientific domains with their respective evidence stores to evaluate domain transferability
3. **Computational Efficiency Analysis**: Measure the computational overhead of CoD compared to baseline approaches and determine the break-even point where performance gains justify increased costs