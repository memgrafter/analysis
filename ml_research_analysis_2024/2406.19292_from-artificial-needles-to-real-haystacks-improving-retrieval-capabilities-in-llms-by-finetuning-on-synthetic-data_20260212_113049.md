---
ver: rpa2
title: 'From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities
  in LLMs by Finetuning on Synthetic Data'
arxiv_id: '2406.19292'
source_url: https://arxiv.org/abs/2406.19292
tags:
- retrieval
- dictionary
- context
- finetuned
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving information retrieval
  and reasoning in large language models (LLMs) when processing long-context inputs.
  The authors propose a novel approach that involves finetuning LLMs on a carefully
  designed synthetic dataset comprising numerical key-value retrieval tasks.
---

# From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data

## Quick Facts
- arXiv ID: 2406.19292
- Source URL: https://arxiv.org/abs/2406.19292
- Reference count: 40
- Finetuning LLMs on synthetic numerical key-value retrieval tasks significantly improves information retrieval and reasoning capabilities in long-context settings

## Executive Summary
This paper addresses the challenge of improving information retrieval and reasoning in large language models (LLMs) when processing long-context inputs. The authors propose a novel approach that involves finetuning LLMs on a carefully designed synthetic dataset comprising numerical key-value retrieval tasks. Their experiments demonstrate that this approach significantly enhances performance on both information retrieval and long-context reasoning tasks while maintaining general capabilities on standard benchmarks.

The key innovation lies in using synthetic numerical data rather than real-world information, which prevents the model from memorizing factual knowledge and reduces hallucination risks. The approach shows substantial improvements, including a 10.5% gain on 20-document MDQA tasks at position 10, while maintaining performance on general benchmarks like MMLU and HellaSwag.

## Method Summary
The method involves generating synthetic datasets containing numerical key-value retrieval tasks, specifically simple dictionary key-value retrieval and multi-subkey dictionary key-value retrieval tasks. These datasets are used to finetune LLMs like GPT-3.5 Turbo and Mistral 7B for 2-3 epochs. The finetuning process includes providing answer templates to help models focus on retrieval skills rather than answer formatting. The approach is evaluated on downstream tasks including MDQA, FLenQA, and general benchmarks, with comparisons against baselines like MultidocQA and IN2.

## Key Results
- Finetuning GPT-3.5 Turbo on multi-subkey dictionary key-value retrieval tasks led to a 10.5% improvement on 20 documents MDQA at position 10
- Models maintained general capabilities on benchmarks like MMLU and HellaSwag with minimal performance degradation
- Synthetic data was more effective than directly finetuning on the target dataset (MDQA)
- The approach does not encourage hallucinations, unlike other long-context augmentation datasets containing factual information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning on synthetic numerical key-value retrieval tasks improves LLMs' retrieval and reasoning capabilities on long-context tasks
- Mechanism: Synthetic numerical tasks force the model to learn exact key-value mapping and dictionary location identification without interference from semantic noise or factual memorization
- Core assumption: The learned retrieval skill transfers from artificial numerical tasks to natural language reasoning tasks that require similar information extraction patterns
- Evidence anchors: [abstract] "finetuning LLMs on this dataset significantly improves LLMs' information retrieval and reasoning capabilities in longer-context settings"
- Break condition: If the downstream task requires semantic understanding beyond simple key-value mapping, the synthetic training may not transfer effectively

### Mechanism 2
- Claim: Synthetic data without factual information prevents hallucinations during finetuning
- Mechanism: By using purely numerical key-value pairs, the model learns retrieval patterns without memorizing or reinforcing factual knowledge that could be hallucinated in downstream tasks
- Core assumption: Factual information in training data contributes to hallucination risk in LLMs
- Evidence anchors: [abstract] "finetuning on our key-value dataset improves LLMs' retrieval and reasoning without suffering from such unwanted characteristics"
- Break condition: If the model encounters factual knowledge during inference that conflicts with learned patterns, it may still hallucinate despite synthetic training

### Mechanism 3
- Claim: Providing answer templates during finetuning helps models focus on learning retrieval skills rather than answer formatting
- Mechanism: Answer templates reduce token-level loss on formatting tokens, allowing the model to concentrate on the actual retrieval task during supervised finetuning
- Core assumption: When answer templates are provided, the model can separate the skill of retrieving correct information from the skill of formatting answers
- Evidence anchors: [section 2] "we provide the model with an answer template with which we want the model to answer"
- Break condition: If the downstream tasks require flexible answer formats that don't match the template, the model may struggle to adapt

## Foundational Learning

- Concept: Exact matching and dictionary structure understanding
  - Why needed here: The synthetic tasks require precise key-value mapping and dictionary location identification, which are fundamental skills for information retrieval
  - Quick check question: Given a list of dictionaries and a target key, can you identify which dictionary contains the key and what its value is?

- Concept: Positional awareness in long contexts
  - Why needed here: The experiments show that models struggle with retrieving information from middle positions in long contexts, so understanding positional relationships is crucial
  - Quick check question: If a key is located at position 10 in a 20-document context, how would you expect retrieval accuracy to change compared to position 1 or 20?

- Concept: Template-based response generation
  - Why needed here: The use of answer templates helps separate retrieval skill from formatting skill during finetuning
  - Quick check question: How would providing an answer template like "The value of key X is Y and it is in Dictionary [Z]" change the model's focus during training?

## Architecture Onboarding

- Component map: Synthetic data generation (numerical key-value tasks) → Model finetuning process → Evaluation on downstream tasks (MDQA, FLenQA, general benchmarks)
- Critical path: Synthetic data generation → Model finetuning → Evaluation on downstream tasks → Analysis of performance improvements and potential degradation on general capabilities
- Design tradeoffs: Using synthetic numerical data instead of real data provides better control over hallucination prevention but may limit transfer to semantically complex tasks
- Failure signatures: Performance degradation on general benchmarks, increased hallucination rates on knowledge-based tasks, or failure to transfer skills from synthetic to real tasks
- First 3 experiments:
  1. Finetune Mistral 7B on simple dictionary key-value retrieval with and without answer templates, evaluate on 20-document MDQA
  2. Finetune GPT-3.5 Turbo on multi-subkey dictionary key-value retrieval, evaluate on FLenQA with chain-of-thought prompting
  3. Compare finetuned models against baselines (MultidocQA, IN2, Needle-in-a-haystack) on MDQA and general benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed synthetic dataset approach scale to even longer context lengths (e.g., 100k+ tokens) compared to current long-context augmentation methods?
- Basis in paper: [inferred] The paper mentions that finetuning Mistral-7b-Instruct-v0.2 on simple key-value retrieval tasks with a maximum context length of 24K showed improvements, suggesting potential for longer contexts.
- Why unresolved: The paper only tests up to 120 documents (~24k tokens) for Mistral-7b-Instruct-v0.2, leaving the effectiveness at much longer context lengths untested.
- What evidence would resolve it: Experiments testing the finetuned models on datasets with contexts of 50k, 100k, and 200k tokens, comparing performance against other long-context augmentation methods.

### Open Question 2
- Question: What is the impact of using synthetic data on the computational efficiency of fine-tuning compared to using real-world datasets?
- Basis in paper: [inferred] The paper highlights the ease of controlling output formats in synthetic data but doesn't discuss computational efficiency during fine-tuning.
- Why unresolved: While the paper shows performance improvements, it doesn't address whether synthetic data leads to faster convergence or requires fewer training steps compared to real-world datasets.
- What evidence would resolve it: Comparative analysis of training time, number of steps, and computational resources required to achieve similar performance levels using synthetic vs. real-world datasets.

### Open Question 3
- Question: How does the effectiveness of synthetic data finetuning vary across different types of language models (e.g., encoder-only, decoder-only, encoder-decoder)?
- Basis in paper: [explicit] The paper tests only decoder-only models (GPT-3.5 Turbo and Mistral 7B), leaving the generalizability to other model architectures unexplored.
- Why unresolved: The paper's results are limited to two specific model architectures, and it's unclear whether the benefits observed would transfer to other types of language models.
- What evidence would resolve it: Fine-tuning experiments using synthetic data on encoder-only models (like BERT), decoder-only models, and encoder-decoder models (like T5), comparing performance improvements across architectures.

### Open Question 4
- Question: What is the optimal balance between synthetic and real-world data in fine-tuning for maximum performance on downstream tasks?
- Basis in paper: [inferred] The paper shows that synthetic data alone can outperform fine-tuning on real-world MDQA data, but doesn't explore hybrid approaches.
- Why unresolved: While the paper demonstrates the effectiveness of synthetic data, it doesn't investigate whether combining synthetic and real-world data could lead to even better performance.
- What evidence would resolve it: Experiments testing various ratios of synthetic to real-world data in fine-tuning, measuring performance on downstream tasks and comparing against pure synthetic or pure real-world approaches.

## Limitations

- Transfer Mechanism Uncertainty: The mechanism by which synthetic numerical key-value mapping transfers to semantic reasoning tasks remains unclear, with no ablation studies isolating which aspects of training drive improvements
- Generalization Scope: Experiments focus on controlled retrieval scenarios, not exploring performance on more complex, open-domain retrieval tasks across diverse domains
- Answer Template Dependency: Rigid templates may limit real-world applicability if downstream tasks require flexible or context-dependent answer formats

## Confidence

- High Confidence: The empirical results showing performance improvements on MDQA and FLenQA benchmarks after finetuning on synthetic data
- Medium Confidence: The claim that synthetic data is more effective than finetuning directly on target datasets, though specific conditions are not fully explored
- Low Confidence: The mechanism by which synthetic numerical key-value retrieval transfers to complex semantic reasoning tasks, with assumed skill transfer not empirically validated

## Next Checks

1. Conduct ablation studies to isolate which components of synthetic training contribute most to performance improvements - key-value mapping accuracy, positional awareness, or answer template usage
2. Test finetuned models on diverse retrieval tasks across different domains (legal documents, scientific literature, news articles) to assess generalizability beyond MDQA and FLenQA benchmarks
3. Systematically evaluate hallucination rates on knowledge-intensive tasks after finetuning on synthetic data, comparing against models finetuned on factual datasets to validate claimed hallucination prevention benefits