---
ver: rpa2
title: Zero-shot cross-lingual transfer in instruction tuning of large language models
arxiv_id: '2402.14778'
source_url: https://arxiv.org/abs/2402.14778
tags:
- language
- evaluation
- data
- instruction
- dolly-en
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates zero-shot cross-lingual transfer in instruction
  tuning, where large language models (LLMs) are trained on English-only instruction
  data and then tested on prompts in other languages. The authors conduct a systematic
  study to evaluate the capabilities and limitations of this approach, focusing on
  various aspects of model responses such as language correctness, fluency, helpfulness,
  factuality, logical coherence, and harmlessness.
---

# Zero-shot cross-lingual transfer in instruction tuning of large language models

## Quick Facts
- arXiv ID: 2402.14778
- Source URL: https://arxiv.org/abs/2402.14778
- Reference count: 40
- Primary result: English-trained LLMs can perform zero-shot cross-lingual instruction following with mixed success

## Executive Summary
This paper investigates whether large language models trained on English-only instruction data can successfully perform tasks in other languages without explicit multilingual fine-tuning. The authors systematically evaluate cross-lingual transfer capabilities across multiple dimensions including language correctness, fluency, helpfulness, factuality, logical coherence, and harmlessness. They find that while cross-lingual transfer is possible even for English-centric models, it requires careful hyperparameter tuning and sufficient instruction data. The study reveals that English-trained models can generate correct-language and helpful responses in other languages, but face significant challenges with factuality.

## Method Summary
The authors conduct a comprehensive experimental study using four base LLMs trained on English-only instruction datasets. They evaluate the models' performance on prompts in multiple non-English languages, measuring various quality dimensions through both automated metrics and human evaluation. The study systematically varies instruction data size and hyperparameter settings to identify conditions under which successful cross-lingual transfer occurs. The evaluation framework covers six key aspects of response quality: language correctness, fluency, helpfulness, factuality, logical coherence, and harmlessness.

## Key Results
- Cross-lingual transfer occurs successfully with English-only instruction tuning, but requires careful hyperparameter optimization
- English-trained models can generate correct-language and comprehensive responses in other languages
- Factuality remains a significant limitation, with models producing less factually accurate responses in non-English languages

## Why This Works (Mechanism)
Zero-shot cross-lingual transfer works because large language models develop generalizable language understanding capabilities during pretraining that can be leveraged for cross-lingual instruction following. The instruction tuning process helps models learn task-specific patterns and reasoning strategies that transfer across languages, even without explicit multilingual training data. This suggests that the models have developed some level of language-agnostic task understanding during pretraining, allowing them to apply learned instructions to new languages.

## Foundational Learning
- **Instruction Tuning**: Why needed - teaches models to follow human instructions; Quick check - model can complete tasks when prompted with "Instruction: [task description]"
- **Zero-shot Learning**: Why needed - enables model to perform tasks without task-specific training; Quick check - model generates reasonable output for unseen tasks
- **Cross-lingual Transfer**: Why needed - allows models trained in one language to work in others; Quick check - model produces coherent output in target language
- **Multilingual Evaluation**: Why needed - assesses model performance across languages; Quick check - consistent quality across language pairs
- **Hyperparameter Optimization**: Why needed - critical for transfer success; Quick check - performance improves with tuning
- **Factuality Assessment**: Why needed - measures truthfulness of model outputs; Quick check - claims can be verified against ground truth

## Architecture Onboarding

**Component Map**
English Pretraining -> English Instruction Tuning -> Zero-shot Cross-lingual Evaluation

**Critical Path**
Pretraining (multilingual understanding) -> English instruction learning -> Task pattern acquisition -> Cross-lingual application

**Design Tradeoffs**
- Training time vs. multilingual coverage
- Instruction data quality vs. quantity
- Zero-shot convenience vs. specialized fine-tuning
- General capability vs. language-specific accuracy

**Failure Signatures**
- Persistent fluency errors in target languages
- Factual hallucinations in non-English responses
- Inconsistent helpfulness across languages
- Degraded logical coherence in cross-lingual tasks

**First 3 Experiments**
1. Test base model performance on non-English prompts without instruction tuning
2. Evaluate cross-lingual performance with varying instruction data sizes
3. Measure impact of different hyperparameter configurations on transfer quality

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Evaluation may not capture all aspects of cross-lingual performance
- Success depends heavily on careful hyperparameter tuning, limiting practical applicability
- Factuality issues in non-English responses represent a significant concern
- Results may not generalize beyond English-centric models

## Confidence

High confidence in the observation that cross-lingual transfer does occur with English-only instruction tuning

Medium confidence in the characterization of transfer quality across different evaluation dimensions

Medium confidence in the identification of factuality as a primary limitation

## Next Checks

1. Replicate the experiments with different model architectures (beyond the four base LLMs used) to test generalizability of findings

2. Conduct human evaluations specifically focused on factuality in cross-lingual responses to better quantify the identified limitation

3. Test the impact of multilingual pretraining versus the zero-shot approach to establish the relative effectiveness of different strategies