---
ver: rpa2
title: Is Programming by Example solved by LLMs?
arxiv_id: '2406.08316'
source_url: https://arxiv.org/abs/2406.08316
tags:
- program
- problems
- should
- data
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) were investigated for their ability
  to perform programming-by-examples (PBE), a task requiring synthesis of code from
  input-output examples. Pretrained models performed poorly on PBE, but fine-tuning
  significantly improved performance, provided test problems were in-distribution.
---

# Is Programming by Example solved by LLMs?

## Quick Facts
- arXiv ID: 2406.08316
- Source URL: https://arxiv.org/abs/2406.08316
- Reference count: 40
- Large language models can be fine-tuned to solve PBE problems effectively, with adaptation methods helping for out-of-distribution cases.

## Executive Summary
This paper investigates whether large language models can solve Programming-by-Examples (PBE) tasks, where programs must be synthesized from input-output examples. The authors find that while pretrained models perform poorly on PBE, fine-tuning on synthetic data generated from seed examples dramatically improves performance for in-distribution problems. They develop an adaptation method that iteratively solves unlabeled problems and adds them to the training set, enabling the model to handle out-of-distribution problems. The resulting system outperforms both symbolic and neural approaches across three domains (list functions, text editing, and LOGO graphics), though it still struggles with truly out-of-distribution problems.

## Method Summary
The approach involves generating synthetic training data by prompting an LLM with seed examples to produce new programs and inputs, then executing these programs to verify outputs. The DeepSeekCoder-1.5-7B model is fine-tuned using LoRA on this synthetic dataset. For out-of-distribution problems, an adaptation method iteratively fine-tunes the model on unlabeled problems from the target distribution. The system uses general-purpose Python, allowing it to solve more diverse problems than classic symbolic methods. Evaluation measures the percentage of problems where generated programs produce correct outputs on holdout inputs.

## Key Results
- Fine-tuned LLMs significantly outperform pretrained models on in-distribution PBE problems
- The adaptation method recovers much of the lost performance on out-of-distribution problems
- The system solves more PBE problems than existing symbolic and neural approaches across three domains
- Performance is better predicted by posterior description length than program size or prior description length

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning teaches LLMs to map input-output examples to correct programs by learning from synthetic datasets generated via program execution. This works because the synthetic dataset generation process captures the essential characteristics of PBE problems. Performance degrades significantly when test problems are out-of-distribution from the fine-tuning data.

### Mechanism 2
The adaptation method recovers lost performance on out-of-distribution PBE problems by iteratively solving unlabeled problems and adding them to the training set. This gradually shifts the model's generative prior toward the target distribution. The process may not solve enough new problems per iteration to significantly shift the generative prior.

### Mechanism 3
Success in PBE is better predicted by description length under the approximate posterior than by program size or prior description length. The fine-tuned model attends closely to the input-output examples and reshapes its distribution accordingly, rather than defaulting to blind guess-and-check. The approximate posterior may not always capture the correct relationship between examples and programs, especially for complex problems.

## Foundational Learning

- Concept: Programming-by-Examples (PBE)
  - Why needed here: Understanding the PBE task is fundamental to grasping why LLMs need to be fine-tuned and adapted for this specific problem.
  - Quick check question: What is the goal of a PBE system when given input-output examples?

- Concept: Fine-tuning
  - Why needed here: Fine-tuning is the core technique used to improve LLM performance on PBE tasks.
  - Quick check question: What is the difference between pretraining and fine-tuning in the context of LLMs?

- Concept: Out-of-distribution generalization
  - Why needed here: Understanding the challenges of applying models to data outside their training distribution is crucial for interpreting the adaptation method.
  - Quick check question: Why does an LLM fine-tuned on one set of PBE problems typically perform worse on a different set of problems?

## Architecture Onboarding

- Component map: Seed dataset -> Synthetic data generator (LLM + program execution) -> Fine-tuning process -> PBE problem solver -> Adaptation loop
- Critical path: Generate synthetic data from seed dataset → Fine-tune LLM on synthetic data → Solve PBE problems → Adapt model to new distribution (if needed)
- Design tradeoffs: Using general-purpose Python allows for more expressive solutions but may require more complex fine-tuning and adaptation
- Failure signatures: Poor performance on out-of-distribution problems, reliance on large inference-time compute budgets, degradation in adaptation effectiveness
- First 3 experiments:
  1. Fine-tune an LLM on synthetic list function data and evaluate on in-distribution test problems
  2. Test the same fine-tuned model on out-of-distribution list function problems to measure degradation
  3. Apply the adaptation method to the out-of-distribution problems and measure performance recovery

## Open Questions the Paper Calls Out

### Open Question 1
What specific characteristics of PBE problems cause the fine-tuned model's approximate posterior to assign higher or lower probability to correct solutions? The paper identifies that neither program size nor prior likelihood predict success, and while posterior likelihood is better, it doesn't explain what specific problem characteristics influence the posterior distribution.

### Open Question 2
How does the adaptation method scale with increasing amounts of unlabeled data and compute budget? The paper mentions that theoretically adaptation should work with enough compute, and suggests more compute-efficient approaches could use "steppingstone" problems, but doesn't explore how performance scales with different amounts of unlabeled data or inference-time compute budgets.

### Open Question 3
What is the minimum viable neural network architecture and size for effective PBE that would be practical for end-user deployment? The paper explicitly states that current 7B-33B models are not practical for most end-users and suggests investigating much smaller neural networks, without exploring smaller architectures.

## Limitations
- The adaptation method's effectiveness for truly out-of-distribution problems across different domains is uncertain
- Binary success metrics may underestimate the model's ability to produce partially correct or semantically equivalent solutions
- The substantial computational budget required for inference may not be practical for real-time applications

## Confidence

**High Confidence:**
- Pretrained LLMs perform poorly on PBE tasks without fine-tuning
- Fine-tuning significantly improves performance on in-distribution problems
- The adaptation method recovers substantial performance on out-of-distribution problems within the same domain

**Medium Confidence:**
- Description length under the approximate posterior is more predictive of problem difficulty than program size
- General-purpose Python enables solving more diverse problems than classic methods
- The synthetic data generation approach produces high-quality training data

**Low Confidence:**
- Adaptation method generalizes to entirely different problem domains
- Binary success metrics fully capture model capabilities
- Computational requirements are practical for deployment scenarios

## Next Checks

1. **Cross-domain adaptation test**: Apply the adaptation method from list functions to text editing or LOGO graphics (or vice versa) to assess whether the approach transfers between fundamentally different problem types, not just variations within the same type.

2. **Partial credit evaluation**: Implement an evaluation framework that assigns partial credit for semantically equivalent or partially correct solutions, then re-run the experiments to determine if the binary metrics underestimate actual capabilities.

3. **Seed dataset sensitivity analysis**: Systematically vary the seed dataset composition and size to quantify how these factors affect synthetic data quality and downstream performance, particularly for the adaptation process.