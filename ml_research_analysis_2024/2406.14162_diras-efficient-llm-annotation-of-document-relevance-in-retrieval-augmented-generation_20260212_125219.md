---
ver: rpa2
title: 'DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval Augmented
  Generation'
arxiv_id: '2406.14162'
source_url: https://arxiv.org/abs/2406.14162
tags:
- relevance
- question
- relevant
- query
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DIRAS fine-tunes small open-sourced LLMs to annotate (query, document)
  relevance for RAG systems, addressing challenges of domain-specific relevance definitions
  and partial relevance. It uses a pointwise annotation strategy with calibrated confidence
  scores, outperforming listwise methods.
---

# DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2406.14162
- Source URL: https://arxiv.org/abs/2406.14162
- Reference count: 40
- Key outcome: DIRAS fine-tunes small open-sourced LLMs to achieve GPT-4-level performance on annotating and ranking (query, document) pairs for RAG systems

## Executive Summary
DIRAS addresses the challenge of efficient and accurate relevance annotation in Retrieval Augmented Generation (RAG) systems. It fine-tunes small open-source LLMs (≤8B parameters) to replicate GPT-4's ability to annotate relevance between queries and documents, using a pointwise annotation strategy with calibrated confidence scores. The system explicitly handles domain-specific relevance definitions and partial relevance, outperforming traditional listwise methods while being more computationally efficient. Experiments show DIRAS achieves GPT-4-level performance on both domain-specific (ClimRetrieve) and general QA datasets, effectively mitigates annotation selection bias, and enables adaptive document selection through calibrated relevance thresholds rather than fixed top-k retrieval.

## Method Summary
DIRAS fine-tunes small open-sourced LLMs to annotate (query, document) relevance for RAG systems. The process involves sampling representative (query, document) pairs using a small dense retriever with balanced sampling, generating explicit relevance definitions for each query, and creating training data by prompting a SOTA teacher LLM (GPT-4) to annotate relevance with confidence scores. The student LLM is then fine-tuned on this annotated data using a pointwise strategy with Chain-of-Thought reasoning. During inference, the fine-tuned model annotates all (query, document) pairs with calibrated confidence scores, which are used for document ranking and selection instead of fixed top-k retrieval.

## Key Results
- DIRAS achieves GPT-4-level performance on relevance annotation and ranking for unseen (query, document) pairs
- On ClimRetrieve dataset, DIRAS successfully benchmarks IR algorithms and improves performance through refined relevance definitions
- DIRAS reduces annotation selection bias and maintains high accuracy when re-annotating unseen data on general QA datasets
- Experimental results show DIRAS is effective at mitigating annotation selection bias by annotating all document-query pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DIRAS fine-tunes small open-sourced LLMs to achieve GPT-4-level performance on annotating and ranking unseen (query, document) pairs.
- Mechanism: The fine-tuning process uses pointwise annotation strategy with Chain-of-Thought reasoning to teach the model nuanced relevance definitions and calibration methods.
- Core assumption: Small LLMs (≤8B parameters) can learn to replicate GPT-4's annotation ability through proper fine-tuning with relevant training data.
- Evidence anchors:
  - [abstract] "Extensive evaluation shows that DIRAS enables smaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking unseen (query, document) pairs"
  - [section 3.3] "Table 2 shows the results of all fine-tuned student models in comparison to all baselines. We observe that Ms-Tok outperforms other settings for all LLM architectures. The best setting Llama3-Tok achieves GPT-4 level performance in calibration and IR on unseen questions and reports."
  - [corpus] Weak - related papers don't directly address the fine-tuning mechanism for achieving GPT-4-level performance

### Mechanism 2
- Claim: DIRAS effectively addresses domain-specific relevance definitions and partial relevance through explicit relevance definition inputs.
- Mechanism: By incorporating explicit relevance definitions as input to the fine-tuning process, the model learns to understand and apply nuanced relevance criteria beyond shallow semantic matching.
- Core assumption: Explicit relevance definitions can effectively teach models the nuances of domain-specific relevance that differ from generic semantic similarity.
- Evidence anchors:
  - [abstract] "fine-tunes open-sourced LLMs to consider nuanced relevance definition and annotate (partial) relevance labels with calibrated relevance scores"
  - [section 3.1] "We explicitly account for partial relevance when the document addresses the periphery of the definition"
  - [section 4.1] "Results show that DIRAS student LLMs can effectively capture partial relevance, leverage improved relevance definitions"

### Mechanism 3
- Claim: DIRAS mitigates annotation selection bias by enabling annotation of all (query, document) pairs rather than selective annotation.
- Mechanism: The fine-tuned model can efficiently annotate all document-query pairs with calibrated confidence scores, identifying relevant documents that human annotators might have overlooked.
- Core assumption: Human annotators tend to selectively annotate only likely relevant documents, creating bias that can be overcome by systematic LLM annotation.
- Evidence anchors:
  - [abstract] "human or GPT-4 annotation is costly and cannot cover all (query, document) pairs (i.e., annotation selection bias)"
  - [section 4.1] "RQ3: Mitigating Annotation Selection Bias...the dataset allows us to investigate our model's capabilities to counteract biases"
  - [section 4.1] "Table 5 indicates, the model can be successfully used to overturn decisions of unseen, as irrelevant assumed documents"

## Foundational Learning

- Concept: Relevance definition and partial relevance
  - Why needed here: DIRAS relies on explicit relevance definitions to teach models nuanced domain-specific criteria for determining document relevance
  - Quick check question: How would you define what makes a document "relevant" for a query asking about "good news for the stock market today" versus "what is the company's revenue for Q3 2023"?

- Concept: Pointwise vs. Listwise ranking methods
  - Why needed here: DIRAS uses pointwise annotation strategy which is more efficient for creating relevance annotations but requires careful calibration
  - Quick check question: What are the key differences between pointwise annotation (annotating individual query-document pairs) and listwise ranking (ranking entire document lists for a query)?

- Concept: Calibration of confidence scores
  - Why needed here: DIRAS uses calibrated relevance scores to indicate annotation quality and enable effective document ranking
  - Quick check question: Why is it important for a model to calibrate its confidence scores, and how does this differ from just predicting a binary relevant/irrelevant label?

## Architecture Onboarding

- Component map: Query and document input pipeline -> Relevance definition generator -> Teacher LLM annotation -> Student LLM fine-tuning -> Inference pipeline for annotation and ranking -> Calibration mechanism

- Critical path:
  1. Sample (query, document) pairs for training data creation
  2. Generate relevance definitions for queries
  3. Create training data using teacher LLM annotations
  4. Fine-tune student LLM on the annotated data
  5. Use fine-tuned model to annotate all (query, document) pairs with confidence scores
  6. Apply calibrated relevance scores for document ranking and selection

- Design tradeoffs:
  - Pointwise vs. Listwise: Pointwise is more efficient for annotation but requires careful calibration; Listwise is better for ranking but more computationally expensive
  - CoT vs. Direct prediction: Chain-of-Thought can improve reasoning but adds inference overhead; direct prediction is faster but may sacrifice some reasoning quality
  - Calibration methods (Ask vs. Tok): Direct confidence score asking is simpler but token-level probability may provide better calibration

- Failure signatures:
  - Poor calibration: Model confidence scores don't match actual accuracy (high ECE or Brier score)
  - Domain mismatch: Model performs well on generic data but poorly on domain-specific queries
  - Annotation bias: Model systematically over/under-predicts relevance for certain types of queries
  - Computational inefficiency: Fine-tuning or inference takes too long for practical deployment

- First 3 experiments:
  1. Fine-tune Llama-3-8B on ChatReportRetrieve training data and evaluate on test set for binary relevance and calibration metrics
  2. Apply fine-tuned model to ClimRetrieve dataset to test partial relevance understanding and annotation selection bias mitigation
  3. Compare performance of fine-tuned model vs. GPT-4 on general QA datasets (ELI5, ASQA, QAMPARI) to validate cross-domain applicability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DIRAS performance vary when applied to multimodal documents containing both text and tables/graphs?
- Basis in paper: [inferred] from "Our third limitation, and also a viable option to address multi-modality, lies in the recent introduction of long-context LLMs."
- Why unresolved: The paper explicitly states it focuses on text documents and does not evaluate performance on graph and table content, representing a significant limitation for real-world RAG systems that handle diverse document types.
- What evidence would resolve it: Experimental results comparing DIRAS performance on text-only vs. multimodal documents, with metrics showing calibration accuracy and ranking effectiveness across different document formats.

### Open Question 2
- Question: What is the optimal relevance threshold for different types of integrative queries (e.g., summary vs. analysis queries) in practical RAG deployments?
- Basis in paper: [explicit] from "Advanced RAG designs can even strategically pick the calibrated threshold for different questions, for example, allowing more partial relevance for summary queries."
- Why unresolved: While the paper suggests using relevance thresholds instead of fixed top-k retrieval and shows F1 score variation with thresholds, it doesn't provide concrete guidance on how to select optimal thresholds for different query types or how users can customize these thresholds.
- What evidence would resolve it: Empirical studies showing optimal threshold ranges for different query categories, along with user studies demonstrating how customized relevance definitions impact retrieval effectiveness.

### Open Question 3
- Question: How does DIRAS handle documents where relevance is context-dependent across different parts of a long document?
- Basis in paper: [inferred] from the chunking approach and discussion of partial relevance, but no explicit analysis of how chunk-level relevance relates to document-level relevance.
- Why unresolved: The paper processes documents as chunks but doesn't address how relevance assessment should work when relevant information is distributed across multiple chunks or when context from adjacent chunks affects relevance determination.
- What evidence would resolve it: Experiments comparing chunk-level vs. document-level relevance annotation performance, along with analysis of how context windows affect relevance judgments and whether hierarchical relevance assessment improves results.

## Limitations

- The reliance on GPT-4 as the teacher model introduces potential issues with transparency and reproducibility, as the model's internal reasoning processes are not fully accessible
- The effectiveness of explicit relevance definitions may vary significantly across different domains, with no clear methodology provided for constructing these definitions
- Computational requirements for fine-tuning even small LLMs may still be prohibitive for some real-world applications

## Confidence

**High Confidence**: DIRAS achieves GPT-4-level performance on relevance annotation and ranking for domain-specific datasets like ClimRetrieve, as evidenced by direct experimental comparisons showing comparable F1 scores, nDCG, and calibration metrics.

**Medium Confidence**: DIRAS effectively mitigates annotation selection bias by enabling annotation of all (query, document) pairs, though the extent of bias reduction may depend on the specific dataset and domain.

**Low Confidence**: DIRAS's performance on general QA datasets (ELI5, ASQA, QAMPARI) demonstrates cross-domain applicability, as the evaluation is limited and doesn't address potential domain adaptation challenges.

## Next Checks

1. **Bias Inheritance Analysis**: Conduct experiments to determine if DIRAS inherits and amplifies biases from the GPT-4 teacher model by comparing annotation patterns between teacher and student models on diverse query-document pairs.

2. **Domain Transferability Test**: Evaluate DIRAS's performance when fine-tuned on one domain (e.g., finance) and applied to a significantly different domain (e.g., medical) to assess how well explicit relevance definitions transfer across domains.

3. **Computational Cost-Benefit Analysis**: Measure the actual computational requirements (training time, inference latency) of DIRAS against its performance gains to determine practical feasibility for resource-constrained applications.