---
ver: rpa2
title: On the Shortcut Learning in Multilingual Neural Machine Translation
arxiv_id: '2411.10581'
source_url: https://arxiv.org/abs/2411.10581
tags:
- zero-shot
- training
- bleu
- steps
- pretrain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the off-target issue in multilingual neural
  machine translation (MNMT), where models fail to generate translations in the expected
  target language during zero-shot translation. The authors attribute this problem
  to the shortcut learning of MNMT models on supervised language mapping patterns,
  particularly the tendency to translate non-centric languages into the centric language.
---

# On the Shortcut Learning in Multilingual Neural Machine Translation

## Quick Facts
- arXiv ID: 2411.10581
- Source URL: https://arxiv.org/abs/2411.10581
- Authors: Wenxuan Wang; Wenxiang Jiao; Jen-tse Huang; Zhaopeng Tu; Michael R. Lyu
- Reference count: 5
- One-line primary result: Generalization training reduces zero-shot translation off-target ratios from 94.6% to 2.3% without additional data or computational costs.

## Executive Summary
This paper addresses the critical off-target issue in zero-shot multilingual neural machine translation (MNMT), where models fail to generate translations in the expected target language during inference. The authors identify that this problem stems from shortcut learning on supervised language mapping patterns, particularly the tendency to translate non-centric languages into the centric language. They demonstrate that multilingual pretraining exacerbates this issue by introducing copy shortcuts. The proposed solution, generalization training, leverages the forgetting nature of model training by removing problematic training instances in later stages, significantly improving zero-shot translation performance while maintaining supervised translation quality.

## Method Summary
The authors propose a simple yet effective training strategy called "generalization training" that addresses off-target translation in zero-shot MNMT. The method divides training into two phases: standard training on full data followed by generalization training where (non-centric, centric) language mapping instances are removed. This leverages the model's natural forgetting tendency to eliminate shortcuts while preserving cross-lingual transformation abilities. The approach is evaluated across multiple benchmarks with different language distributions and numbers, showing consistent improvements in zero-shot translation performance without introducing additional data or computational costs.

## Key Results
- Zero-shot off-target ratios reduced from 94.6% to 2.3% on OPUS50 benchmark
- BLEU scores improved by up to 9.4 points in zero-shot translation scenarios
- The method maintains supervised translation performance while significantly improving zero-shot translation across various language distributions
- Generalization training is robust across different centric languages and dataset sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Off-target issue stems from shortcut learning on supervised (non-centric, centric) language mappings.
- Mechanism: Models overfit to frequent (non-centric, centric) patterns, defaulting to centric language in zero-shot scenarios.
- Core assumption: Shortcut is strong enough to override target language tags.
- Evidence anchors: [abstract] "We attribute the off-target issue to the overfitting of the shortcuts of (non-centric, centric) language mappings."
- Break condition: If models learn to distinguish target languages regardless of mapping frequency.

### Mechanism 2
- Claim: Multilingual pretraining accelerates shortcut learning by introducing copy shortcuts.
- Mechanism: Pretraining's denoising auto-encoding objective teaches copying source tokens, conflicting with translation objectives.
- Core assumption: Copy behavior from pretraining directly conflicts with target language generation.
- Evidence anchors: [abstract] "Multilingual pretraining accelerates and aggravates the shortcut learning by introducing another type of shortcut (i.e., the copy of source language)."
- Break condition: If pretraining objectives are modified to discourage copying.

### Mechanism 3
- Claim: Generalization training leverages model forgetting to eliminate shortcut patterns.
- Mechanism: Removing (non-centric, centric) instances in later training stages gradually eliminates these shortcuts.
- Core assumption: NMT models naturally forget previously learned patterns when not reinforced.
- Evidence anchors: [abstract] "We leverage the forgetting nature of model training to forget the overfitted (non-centric, centric) language mapping."
- Break condition: If models retain shortcuts despite their absence.

## Foundational Learning

- Concept: Multilingual Neural Machine Translation (MNMT)
  - Why needed here: Understanding how MNMT differs from bilingual NMT and why zero-shot translation is challenging.
  - Quick check question: How does MNMT handle translation between languages not seen in training?

- Concept: Shortcut Learning in Deep Learning
  - Why needed here: The core problem being solved is a form of shortcut learning specific to language mappings.
  - Quick check question: What makes shortcut learning particularly problematic in MNMT compared to other NLP tasks?

- Concept: Catastrophic Forgetting in Neural Networks
  - Why needed here: The proposed solution relies on the model's tendency to forget previously learned patterns.
  - Quick check question: Under what conditions does catastrophic forgetting become beneficial rather than harmful?

## Architecture Onboarding

- Component map: Encoder-decoder architecture with language tags for source/target specification, pretrained with mBART50
- Critical path: Training → Zero-shot inference → Language detection → Performance evaluation
- Design tradeoffs: Sacrifices some supervised translation performance to improve zero-shot translation
- Failure signatures: Off-target translations, poor zero-shot BLEU scores, ignored language tags
- First 3 experiments:
  1. Train vanilla MNMT on CC6-En dataset and measure zero-shot off-target ratios
  2. Apply generalization training with different G values and compare zero-shot performance
  3. Train with and without pretraining to confirm pretraining's negative impact on zero-shot translation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of multilingual pretraining make it more susceptible to shortcut learning in MNMT models?
- Basis in paper: [explicit] The authors note that pretraining accelerates and aggravates shortcut learning by introducing another type of shortcut (i.e., the copy of source language) due to the denoising auto-encoding objective.
- Why unresolved: The paper identifies the issue but does not delve into the specific characteristics of pretraining that exacerbate shortcut learning.
- What evidence would resolve it: Comparative studies analyzing the impact of different pretraining objectives and data scales on shortcut learning in MNMT models.

### Open Question 2
- Question: How does the generalization training approach scale with larger and more diverse multilingual datasets?
- Basis in paper: [inferred] The authors demonstrate effectiveness on datasets with 6, 16, and 50 languages, but do not explore scalability to larger datasets or more diverse language pairs.
- Why unresolved: The paper does not provide insights into the performance of generalization training as the number of languages and data size increase.
- What evidence would resolve it: Experiments evaluating generalization training on datasets with hundreds of languages and varying degrees of language similarity and data imbalance.

### Open Question 3
- Question: What are the long-term effects of generalization training on model performance and robustness?
- Basis in paper: [explicit] The paper shows that generalization training improves zero-shot translation performance without introducing additional data or computational costs.
- Why unresolved: The study focuses on immediate performance improvements, but does not address how these changes affect the model's long-term performance, adaptability, or robustness to new languages or domains.
- What evidence would resolve it: Longitudinal studies tracking model performance and robustness over extended periods and across diverse linguistic tasks and domains.

## Limitations

- The paper's core mechanism (forgetting as a solution) relies on empirical observations rather than rigorous theoretical grounding
- Evaluation is primarily focused on English-centric language distributions, with limited exploration of scenarios where the centric language might be low-resource
- Several critical implementation choices are underspecified, including exact timing for removing instances and learning rate schedules

## Confidence

**High Confidence (✦✦✦):** The empirical observation that off-target translation is a significant problem in zero-shot MNMT, and that the generalization training approach effectively reduces off-target ratios from >90% to <5% in tested scenarios.

**Medium Confidence (✦✦):** The attribution of off-target issues to shortcut learning on (non-centric, centric) language mappings. While the empirical evidence strongly supports this connection, alternative explanations are not thoroughly ruled out.

**Low Confidence (✦):** The theoretical explanation that generalization training works by leveraging the "forgetting nature" of model training. This is an empirical observation that could have multiple explanations.

## Next Checks

**Check 1: Alternative Mechanism Testing**
Design an ablation study that tests whether the benefits of generalization training come specifically from forgetting shortcuts or from other effects. Compare against random instance removal, curriculum learning, and data augmentation approaches.

**Check 2: Language Tag Robustness Analysis**
Investigate whether language tags are being properly respected during zero-shot inference by conducting controlled experiments with varied tag formatting, synthetic tags, and language model perplexity measurements.

**Check 3: Cross-Domain Generalization**
Evaluate the proposed approach on out-of-domain data to test its robustness beyond the CCMatrix and OPUS datasets used in the paper. Test on Wikipedia-based pairs, news domain translations, and conversational data.