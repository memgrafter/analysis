---
ver: rpa2
title: A Label is Worth a Thousand Images in Dataset Distillation
arxiv_id: '2406.10485'
source_url: https://arxiv.org/abs/2406.10485
tags:
- labels
- soft
- distillation
- label
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges conventional wisdom in dataset distillation
  by demonstrating that soft (probabilistic) labels, rather than synthetic image generation,
  are the primary driver of performance in state-of-the-art methods. Through extensive
  ablation experiments, the authors show that randomly sampling real training images
  and pairing them with soft labels from early-stopped expert models achieves performance
  comparable to sophisticated synthetic data generation approaches.
---

# A Label is Worth a Thousand Images in Dataset Distillation

## Quick Facts
- arXiv ID: 2406.10485
- Source URL: https://arxiv.org/abs/2406.10485
- Reference count: 40
- Primary result: Soft labels, not synthetic image generation, are the primary driver of performance in state-of-the-art dataset distillation methods

## Executive Summary
This paper fundamentally challenges the conventional wisdom in dataset distillation by demonstrating that soft (probabilistic) labels are the primary performance driver, not synthetic image generation. Through extensive ablation experiments, the authors show that randomly sampling real training images and pairing them with soft labels from early-stopped expert models achieves performance comparable to sophisticated synthetic data generation approaches. They establish that structured semantic information in soft labels—such as class similarity patterns—is crucial for data-efficient learning, and that the optimal label structure depends on the available data budget. The paper also establishes an empirical knowledge-data scaling law and Pareto frontier, quantifying how expert knowledge can reduce dataset size requirements.

## Method Summary
The authors challenge conventional dataset distillation approaches by introducing a simple baseline: randomly sampling real training images and pairing them with soft labels generated by early-stopped expert models. They train expert models (ResNet18/50, ConvNet variants) on original datasets using standard SGD with step learning rate schedules, saving checkpoints at each epoch. For each dataset, they randomly sample images and generate soft labels using expert models at different epochs, systematically tuning the expert epoch to find optimal performance for each images-per-class (IPC) setting. Student models are then trained on these distilled datasets (random images + soft labels) until convergence, with learning rate tuning only. The method is evaluated across multiple datasets (ImageNet-1K, TinyImageNet, CIFAR-100, CIFAR-10) with multiple random seeds for image selection and expert training to ensure robustness.

## Key Results
- Randomly sampled real images paired with soft labels from early-stopped experts achieve performance comparable to sophisticated synthetic data generation methods
- Structured semantic information in soft labels (class similarity patterns) is crucial for data-efficient learning
- The optimal structure of soft labels depends on the available data budget, with different IPC settings requiring different label structures
- An empirical knowledge-data scaling law and Pareto frontier show how expert knowledge can reduce dataset size requirements
- Dataset distillation methods that learn soft labels directly recover the same information as expert-generated labels

## Why This Works (Mechanism)
The effectiveness stems from the information density of soft labels, which capture rich semantic relationships between classes that are lost in hard labels. Early-stopped expert models produce soft labels that retain more generalizable information about class relationships, making them particularly valuable when data is limited. The structured nature of these labels—encoding similarity patterns and hierarchical relationships—enables student models to learn more efficiently than from either random hard labels or fully-trained expert soft labels. This structured information effectively acts as a form of inductive bias that compensates for limited training data.

## Foundational Learning
**Soft Labels**: Probabilistic labels that encode class membership uncertainty
- Why needed: Enable richer semantic information transfer beyond binary class membership
- Quick check: Compare student performance using soft vs. hard labels on same image set

**Early-Stopped Experts**: Models trained for fewer epochs than full convergence
- Why needed: Produce soft labels with optimal balance of informativeness and generalization
- Quick check: Measure student performance using soft labels from experts at different training stages

**Dataset Distillation**: Compressing datasets into smaller synthetic counterparts maintaining performance
- Why needed: Enables efficient training when full datasets are too large or expensive to use
- Quick check: Compare student accuracy on distilled vs. full original dataset

**Knowledge-Data Scaling Law**: Empirical relationship between available data and required expert knowledge
- Why needed: Quantifies how much expert knowledge compensates for data scarcity
- Quick check: Plot student performance vs. IPC and expert knowledge levels

## Architecture Onboarding

**Component Map**: Expert Models -> Soft Label Generation -> Student Training -> Performance Evaluation

**Critical Path**: Expert training → Epoch tuning for optimal soft labels → Random image sampling → Student model training → Test accuracy measurement

**Design Tradeoffs**: 
- Real images + soft labels vs. synthetic images + hard labels (simplicity vs. potential synthetic data benefits)
- Early-stopped vs. fully-trained experts (more generalizable vs. more accurate labels)
- Random vs. curated image selection (simplicity vs. potentially better coverage)

**Failure Signatures**:
- Student performance significantly worse than reported: likely suboptimal expert epoch selection or insufficient expert training
- Results not robust to random seeds: likely insufficient sampling or training seeds
- Performance plateaus early: potential issues with label structure or student model capacity

**First Experiments**:
1. Train expert models to convergence and save checkpoints at each epoch
2. Generate soft labels using experts at different epochs and measure student performance
3. Compare student performance using soft labels vs. hard labels on same image set

## Open Questions the Paper Calls Out
**Open Question 1**: What specific semantic structures in soft labels are most critical for effective distillation across different data budgets? The paper shows structured information is important but doesn't identify which specific structures (hierarchical, similarity-based, co-occurrence patterns) contribute most.

**Open Question 2**: Can dataset distillation methods achieve comparable results without any expert knowledge, using only intrinsic dataset properties? While the paper shows BPTT learns labels similar to expert labels, it doesn't explore completely knowledge-free approaches.

**Open Question 3**: How does the optimal structure of soft labels vary with different model architectures and learning algorithms? The paper establishes that optimal label structure depends on data budget but doesn't examine this relationship across various student architectures.

**Open Question 4**: What is the theoretical foundation explaining why soft labels from early-stopped experts are optimal for data-efficient learning? The authors observe this empirically but don't provide theoretical justification for why less accurate but more informative soft labels benefit data-limited scenarios.

## Limitations
- Expert training hyperparameters (learning rate schedules, optimizer settings, number of epochs) are not fully specified for each dataset
- Random seed selection for image sampling and expert model training is not detailed, potentially impacting reproducibility
- The theoretical foundation explaining why early-stopped expert knowledge is optimal remains unexplained

## Confidence
- Soft labels as primary performance driver: High - thorough ablation experiments across multiple datasets and architectures
- Structured semantic information importance: High - controlled experiments with varying label structures show clear performance differences
- Optimality of expert knowledge: Medium - while learned soft labels through distillation recover similar information, alternative knowledge transfer mechanisms are not fully explored

## Next Checks
1. **Expert Epoch Tuning Validation**: Systematically vary expert model epoch for soft label generation across multiple datasets and IPC settings to confirm optimal epoch selection is consistent and dataset-dependent.

2. **Label Structure Ablation**: Generate synthetic labels with varying semantic structure (uniform noise, class-imbalanced soft labels, fully structured soft labels) and evaluate student performance to isolate impact of label structure on learning efficiency.

3. **Knowledge-Data Scaling Law Verification**: Replicate experiments across wider range of dataset sizes and IPC configurations to empirically validate the proposed knowledge-data scaling law and Pareto frontier beyond tested datasets.