---
ver: rpa2
title: 'OneActor: Consistent Character Generation via Cluster-Conditioned Guidance'
arxiv_id: '2404.10267'
source_url: https://arxiv.org/abs/2404.10267
tags:
- generation
- subject
- consistent
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel one-shot tuning paradigm, termed as
  OneActor, for consistent subject generation. Leveraging the derived cluster-based
  score function, we design a cluster-conditioned pipeline that performs a lightweight
  semantic search without compromising the denoising backbone.
---

# OneActor: Consistent Character Generation via Cluster-Conditioned Guidance

## Quick Facts
- arXiv ID: 2404.10267
- Source URL: https://arxiv.org/abs/2404.10267
- Authors: Jiahao Wang, Caixia Yan, Haonan Lin, Weizhan Zhang, Mengmeng Wang, Tieliang Gong, Guang Dai, Hao Sun
- Reference count: 40
- One-line primary result: Achieves superior subject consistency with average 5-minute tuning time using cluster-conditioned guidance

## Executive Summary
This paper introduces OneActor, a novel one-shot tuning paradigm for consistent subject generation in text-to-image diffusion models. The method formalizes consistent generation as a clustering problem in latent space, deriving a cluster-based score function that guides denoising trajectories toward target sub-clusters while excluding auxiliary ones. By leveraging semantic interpolation and generalized tuning with auxiliary samples, OneActor achieves excellent image quality and consistency while significantly improving efficiency compared to traditional fine-tuning approaches.

## Method Summary
OneActor addresses consistent subject generation by treating it as a clustering problem in latent space. The method uses a projector network to map diffusion features to semantic offsets, trained with both target and auxiliary samples to prevent overfitting. During inference, cluster guidance strategies including semantic interpolation are applied to generate consistent images across different prompts. The approach combines target attraction scores with auxiliary exclusion scores to create a cluster-conditioned guidance mechanism that operates efficiently without modifying the frozen diffusion backbone.

## Key Results
- Achieves superior subject consistency (CLIP-I, DreamSim, DINO-fg, LPIPS-fg) compared to existing methods
- Maintains excellent image quality and prompt conformity while significantly improving efficiency (5-minute average tuning time)
- Demonstrates that intricate semantic guidance alone is sufficient for consistent subject generation without laborious backbone tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The cluster-guided score function increases the probability of generating images from the target sub-cluster while reducing probability from auxiliary sub-clusters.
- **Mechanism:** Formalizes consistent subject generation as clustering problem in latent space, combining target attraction and auxiliary exclusion scores to guide denoising trajectories.
- **Core assumption:** Different subjects form distinct base clusters in latent space, with identity sub-clusters sharing common appearance.
- **Evidence anchors:** [abstract], [section 3.1], but weak corpus support for clustering assumption validation.
- **Break condition:** If latent space doesn't exhibit clear cluster structure, cluster-based formalization becomes invalid.

### Mechanism 2
- **Claim:** Semantic interpolation in prompt embedding space produces equivalent effects to latent space interpolation.
- **Mechanism:** Extends classifier-free guidance's linear interpolation property from latent to semantic space through denoising network entanglement.
- **Core assumption:** Semantic and latent spaces share properties due to entanglement through the denoising network.
- **Evidence anchors:** [section 3.3] shows semantic-latent scale effect equivalence, but lacks corpus validation.
- **Break condition:** If semantic-latent spaces aren't sufficiently entangled, semantic interpolation may not produce desired effects.

### Mechanism 3
- **Claim:** Generalized tuning with auxiliary samples mitigates overfitting while maintaining generation quality.
- **Mechanism:** Augments one-shot tuning by including target and auxiliary samples in each batch, providing diversity to prevent severe bias.
- **Core assumption:** Auxiliary samples provide sufficient diversity while projector learns to distinguish target from auxiliary representations.
- **Evidence anchors:** [abstract], [section 3.2], and [section A.3] show improved image quality and diversity with auxiliary samples.
- **Break condition:** If auxiliary samples are too similar to target samples, regularization benefit disappears and overfitting returns.

## Foundational Learning

- **Concept: Latent diffusion models and classifier-free guidance**
  - Why needed here: Method builds upon standard latent diffusion framework and extends classifier-free guidance for cluster-based guidance
  - Quick check question: How does classifier-free guidance combine unconditional and conditional predictions in standard diffusion models?

- **Concept: Clustering and sub-cluster identification in high-dimensional spaces**
  - Why needed here: Core formalization treats consistent generation as navigating between sub-clusters within base clusters
  - Quick check question: What properties must latent space exhibit for meaningful clustering of subject representations?

- **Concept: Semantic space manipulation and embedding interpolation**
  - Why needed here: Method relies on semantic interpolation and offset guidance in prompt embedding space
  - Quick check question: How does manipulating prompt embeddings affect the conditioning in diffusion models?

## Architecture Onboarding

- **Component map:**
  Original diffusion backbone (frozen) -> Projector network (trained: ResNet + linear + AdaIN) -> Text encoder (frozen) -> U-Net encoder (frozen, for feature extraction) -> Cluster guidance mechanism (inference-only)

- **Critical path:**
  1. Generate base images with target prompt
  2. Select target and auxiliary samples
  3. Train projector to map features to semantic offsets
  4. During inference, use projector to generate cluster guidance
  5. Apply cluster guidance during denoising steps

- **Design tradeoffs:**
  - Training frozen backbone vs. full fine-tuning: avoids quality degradation but limits expressiveness
  - Using auxiliary samples vs. target-only: prevents overfitting but adds computational overhead
  - Semantic vs. latent guidance: semantic is more interpretable but may be less precise

- **Failure signatures:**
  - Severe bias in generated images (overfitting)
  - Loss of diversity in outputs
  - Inconsistent details across generated images
  - Poor prompt conformity despite tuning

- **First 3 experiments:**
  1. Verify cluster structure in latent space with t-SNE analysis of base vs. target samples
  2. Test projector training with varying numbers of auxiliary samples to find optimal regularization
  3. Validate semantic interpolation equivalence by comparing semantic vs. latent scale effects on outputs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of OneActor scale when generating images of more than 4 subjects simultaneously?
- **Basis in paper:** [explicit] Paper mentions OneActor struggles with generating numerous subjects (usually > 4) as evidenced by Figure 20(ii) where 5 subjects generation case neglects the subject tie.
- **Why unresolved:** Paper lacks quantitative results or detailed analysis on performance degradation when increasing subjects beyond 4.
- **What evidence would resolve it:** Experiments comparing identity consistency, prompt similarity, and image quality metrics for 1, 2, 3, 4, and 5+ subjects generation scenarios.

### Open Question 2
- **Question:** Can the semantic-latent guidance equivalence property be leveraged to develop more precise and efficient control mechanisms for text-to-image generation?
- **Basis in paper:** [explicit] Paper establishes semantic-latent guidance equivalence property and suggests it as potential tool for fine generation control.
- **Why unresolved:** While property is proven, paper doesn't explore practical applications in developing novel control mechanisms or improving existing ones.
- **What evidence would resolve it:** Demonstrating use of this property in designing new guidance strategies, optimizing existing ones, or enabling novel applications like interactive image editing.

### Open Question 3
- **Question:** How does the cluster structure in the latent space evolve during the training of the diffusion model, and how does it impact the performance of OneActor?
- **Basis in paper:** [inferred] Paper relies on assumption that different subjects form different clusters in latent space, with OneActor's performance based on this structure.
- **Why unresolved:** Paper doesn't investigate cluster structure evolution during training or its impact on OneActor's ability to generate consistent images.
- **What evidence would resolve it:** Analyzing cluster structure at different diffusion model training stages, correlating with OneActor's performance, and exploring techniques to maintain or enhance structure.

## Limitations

- **Latent space clustering assumption:** Relies on clustering structure that may not hold for all subject types or styles
- **One-shot tuning constraints:** Limited by single target image providing insufficient information for highly detailed or nuanced subjects
- **Multi-subject generation challenges:** Struggles with generating numerous subjects (usually > 4) on the same image
- **Computational overhead:** Cluster analysis during inference adds computational cost that scales with sample numbers

## Confidence

- **High Confidence:** Generalized tuning approach with auxiliary samples is well-supported by experimental results and ablation studies
- **Medium Confidence:** Cluster-guided score function formalization is logically sound but relies on unproven clustering assumptions
- **Low Confidence:** Semantic-latent equivalence property is mathematically derived but lacks extensive empirical validation

## Next Checks

1. **Latent Space Structure Validation:** Conduct t-SNE or UMAP visualization analysis of latent representations for multiple subjects to empirically verify the clustering assumption across different character types and styles.

2. **Semantic-Latent Equivalence Stress Test:** Systematically vary prompt complexity, subject detail level, and guidance scales to identify breaking points where semantic interpolation no longer produces equivalent effects to latent space manipulation.

3. **Auxiliary Sample Diversity Impact:** Experiment with different strategies for selecting auxiliary samples (random sampling, style-based sampling, feature-space distance sampling) to determine optimal regularization and identify when additional diversity stops providing benefits.