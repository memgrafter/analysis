---
ver: rpa2
title: Simultaneous Interpretation Corpus Construction by Large Language Models in
  Distant Language Pair
arxiv_id: '2404.12299'
source_url: https://arxiv.org/abs/2404.12299
tags:
- translation
- source
- offline
- gpt-3
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for constructing a large-scale simultaneous
  interpretation (SI) corpus using large language models (LLMs) based on the chunk-wise
  monotonic translation (CWMT) guideline. The proposed LLM-SI-Corpus is created by
  converting existing speech translation corpora while maintaining the original word
  order and preserving the entire source content.
---

# Simultaneous Interpretation Corpus Construction by Large Language Models in Distant Language Pair

## Quick Facts
- arXiv ID: 2404.12299
- Source URL: https://arxiv.org/abs/2404.12299
- Reference count: 40
- Primary result: LLM-generated SI corpus outperforms existing SI corpora for training SiMT models on distant language pairs

## Executive Summary
This paper presents a method for constructing a large-scale simultaneous interpretation (SI) corpus using large language models (LLMs) based on the chunk-wise monotonic translation (CWMT) guideline. The proposed LLM-SI-Corpus is created by converting existing speech translation corpora while maintaining the original word order and preserving the entire source content. Experiments on both text-to-text and speech-to-text settings demonstrate that fine-tuning simultaneous machine translation models with the LLM-SI-Corpus reduces latency while maintaining translation quality, outperforming models trained with offline datasets and existing SI corpora.

## Method Summary
The paper proposes using LLMs to automatically construct an SI corpus by converting existing speech translation datasets. The process follows the chunk-wise monotonic translation (CWMT) guideline, where the LLM reads a chunk of the source sentence and generates the corresponding target translation while maintaining the original word order. This approach aims to create SI data that preserves the entire source content without excessive word order swapping or omissions, addressing limitations of previous SI corpora. The LLM-SI-Corpus is then used to fine-tune SiMT models, which are evaluated on simultaneous translation tasks with metrics for both translation quality and latency.

## Key Results
- Fine-tuning SiMT models with LLM-SI-Corpus reduces latency compared to models trained on offline datasets
- Translation quality (BLEU/BLEURT/COMET) is maintained or improved when using LLM-SI-Corpus
- LLM-SI-Corpus outperforms existing SI corpora (NAIST-SIC-Aligned-ST) in both quality and latency metrics
- The corpus is available for research at https://github.com/yusuke1997/LLM-SI-Corpus

## Why This Works (Mechanism)
The approach leverages LLMs' strong translation capabilities to generate SI data that adheres to the CWMT guideline, producing translations that maintain source word order and preserve content. By converting existing speech translation corpora rather than relying on human annotation, the method achieves scalability while avoiding the noise and inconsistencies often present in real SI data transcription. The chunk-wise processing ensures that translations can be generated with low latency, mimicking the real-time constraints of simultaneous interpretation.

## Foundational Learning
- **Chunk-wise monotonic translation (CWMT)**: Translation method where output follows source order chunk by chunk; needed for low-latency SI, check by verifying monotonic alignment in outputs
- **Simultaneous machine translation (SiMT)**: Real-time translation with controlled latency; needed for streaming applications, check by measuring AL (average lag) metrics
- **Translation quality metrics (BLEU, BLEURT, COMET)**: Automated measures of translation accuracy; needed for objective evaluation, check by comparing metric correlations with human judgments
- **Latency metrics (AL, LAAL, ATD)**: Measures of translation delay and reading effort; needed to evaluate SI systems, check by analyzing latency distributions across test sets
- **Speech translation corpora**: Datasets containing aligned speech and text translations; needed as source material for corpus construction, check by verifying alignment quality and coverage
- **Large language model prompting**: Techniques for guiding LLM outputs; needed to ensure CWMT compliance, check by analyzing prompt effectiveness on sample outputs

## Architecture Onboarding

**Component Map**: Speech Translation Corpus -> LLM with CWMT Prompt -> LLM-SI-Corpus -> SiMT Model Fine-tuning -> Simultaneous Translation System

**Critical Path**: The pipeline flows from existing speech translation data through LLM conversion using CWMT prompts, then to SiMT model training, and finally to simultaneous translation inference.

**Design Tradeoffs**: The method trades potential LLM generation errors for scalability and consistency compared to human-annotated SI corpora. Using existing speech translation data ensures good source-target alignment but may not capture all SI-specific phenomena like omission and summarization.

**Failure Signatures**: Poor translation quality would manifest as low BLEU/BLEURT/COMET scores; high latency would appear as increased AL/LAAL/ATD values; failure to preserve source content would show as high deletion rates or poor chunk-wise alignment.

**First Experiments**:
1. Fine-tune a base SiMT model on LLM-SI-Corpus and evaluate on Chunk-wise test set using AL, BLEU, and COMET
2. Compare model outputs from LLM-SI-Corpus versus NAIST-SIC-Aligned-ST on tst-COMMON for quality and latency
3. Analyze chunk-wise alignment and word order preservation in generated translations versus source sentences

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of LLM-SI-Corpus compare to real SI data when both are used to train SiMT models?
- Basis in paper: The paper mentions that real SI data has various transcription-related noises and quality inconsistencies, while LLM-SI-Corpus aims to provide more faithful translations with reduced word order swapping and minimal omissions.
- Why unresolved: The paper only compares LLM-SI-Corpus to existing SI corpora (NAIST-SIC-Aligned-ST) and offline datasets, not to real SI data.
- What evidence would resolve it: A direct comparison of SiMT models trained on LLM-SI-Corpus versus models trained on real SI data, evaluated on the same test sets (tst-COMMON, SIC-test, Chunk-wise) using the same metrics (BLEU, BLEURT, COMET, COMET-QE, AL, LAAL, ATD).

### Open Question 2
- Question: Can LLM-SI-Corpus be effectively applied to speech-to-speech simultaneous interpretation tasks?
- Basis in paper: The paper focuses on text-to-text and speech-to-text settings, with a note that expanding to speech-to-speech settings is planned as future work.
- Why unresolved: The paper does not provide any experimental results or analysis for speech-to-speech simultaneous interpretation using LLM-SI-Corpus.
- What evidence would resolve it: Experiments applying LLM-SI-Corpus to train SiST models, evaluated on speech-to-speech test sets with appropriate metrics for speech quality and latency.

### Open Question 3
- Question: How do different SI techniques like omission and summarization affect the performance of SiMT models when incorporated into LLM-SI-Corpus?
- Basis in paper: The paper mentions that while CWMT focuses on chunking, other SI techniques like omission and summarization are necessary for better SI and are mentioned as future work.
- Why unresolved: The paper only explores the CWMT guideline and does not investigate other SI techniques in the context of LLM-SI-Corpus.
- What evidence would resolve it: Creation of LLM-SI-Corpus variants incorporating different SI techniques (e.g., omission, summarization) and comparison of SiMT models trained on these variants versus the original LLM-SI-Corpus on the same test sets and metrics.

## Limitations
- The method relies on LLMs for corpus generation, introducing potential quality and consistency issues that are difficult to fully validate
- The claim that the corpus maintains original word order and preserves source content depends on LLM performance and CWMT effectiveness, with no independent human evaluation of corpus fidelity
- Experimental results showing improved latency while maintaining quality are limited to reported models and may not generalize to other language pairs or domains

## Confidence
- **High confidence**: Experimental results showing improved latency while maintaining translation quality for the reported models
- **Medium confidence**: Claim that LLM-SI-Corpus maintains original word order and preserves source content; generalization to other language pairs or domains not covered in the study
- **Medium confidence**: Claim that the corpus outperforms existing SI corpora, based on limited comparisons pending broader validation

## Next Checks
1. Conduct human evaluation of a sample of LLM-SI-Corpus outputs to verify preservation of source content and adherence to chunk-wise monotonic translation principles
2. Test the trained models on additional distant language pairs not included in the original corpus construction to assess generalization
3. Compare model outputs against human interpreter performance on simultaneous interpretation tasks to validate the practical utility of the corpus