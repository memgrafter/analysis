---
ver: rpa2
title: 'An empirical study of LLaMA3 quantization: from LLMs to MLLMs'
arxiv_id: '2404.14047'
source_url: https://arxiv.org/abs/2404.14047
tags:
- quantization
- imersimers
- llama3
- performance
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically evaluates low-bit quantization of the LLaMA3
  family of large language models (LLMs) and multi-modal LLMs (MLLMs) using 10 quantization
  methods ranging from 1 to 8 bits. The study covers post-training quantization (PTQ)
  and LoRA fine-tuning (LoRA-FT) on both linguistic tasks and visual-language tasks.
---

# An empirical study of LLaMA3 quantization: from LLMs to MLLMs

## Quick Facts
- arXiv ID: 2404.14047
- Source URL: https://arxiv.org/abs/2404.14047
- Reference count: 40
- Low-bit quantization of LLaMA3 models shows significant performance degradation at 2-3 bits, with 4-bit maintaining acceptable performance

## Executive Summary
This paper presents a comprehensive empirical evaluation of low-bit quantization for LLaMA3 family models, including both LLMs (LLaMA3-8B, LLaMA3-70B) and MLLMs (LLaVA-Next-8B). The study examines 10 quantization methods across 1-8 bit widths, covering both post-training quantization and LoRA fine-tuning approaches. Results reveal that LLaMA3 models experience substantial performance degradation at ultra-low bit widths, particularly in linguistic and visual-language tasks, while 4-bit quantization maintains acceptable accuracy with significant memory savings.

## Method Summary
The study evaluates 10 quantization methods (9 PTQ and 2 LoRA-FT) across LLaMA3-8B, LLaMA3-70B, and LLaVA-Next-8B models using bit-widths from 1 to 8. PTQ methods include RTN, GPTQ, AWQ, SmoothQuant, PB-LLM, QuIP, DB-LLM, BiLLM, and SliM-LLM, while LoRA-FT methods are QLoRA and IR-QLoRA. Models are evaluated on language tasks (WikiText2, C4, PTB) for perplexity, commonsense reasoning benchmarks (PIQA, ARC-e, ARC-c, HellaSwag, Winogrande, MMLU) for accuracy, and visual-language tasks (AI2D, ChartQA, DocVQA, MME, MMBench) for MLLM performance. Calibration uses 128 samples from WikiText2 with sequence length 2048.

## Key Results
- LLaMA3 models suffer significant performance degradation under ultra-low bit quantization (2-3 bits), with MLLMs experiencing complete failure on several tasks at 2 bits
- 4-bit quantization maintains acceptable performance with less than 2% loss on multi-modal benchmarks and reasonable perplexity scores for language tasks
- LoRA fine-tuning cannot compensate for quantization-induced performance loss below 4 bits, contrary to non-quantized counterparts
- DB-LLM and BiLLM achieve reasonable results at ultra-low bit widths (2 bits, 1.1 bits) through large-batch fine-tuning and salience partitioning

## Why This Works (Mechanism)

### Mechanism 1
Ultra-low bit quantization causes significant performance degradation because LLaMA3's extensive pre-training creates a highly optimized continuous parameter space that cannot be accurately represented in severely constrained discrete spaces. When quantizing to 2-3 bits, the quantization process introduces large approximation errors that disrupt the model's learned representations. The core assumption is that the model's performance relies heavily on precise weight values, and reducing bit precision below 4 bits introduces quantization noise that cannot be compensated by the model's internal redundancy. Evidence shows LLaMA3 suffers non-negligible degradation in linguistic and visual contexts under ultra-low bit widths, though DB-LLM and BiLLM achieve reasonable results at 2 bits through large-batch fine-tuning and salience partitioning.

### Mechanism 2
LLaMA3's superior pre-training makes it more sensitive to quantization errors compared to previous LLaMA/LLaMA2 models. The massive pre-training (over 15T tokens) creates a more complex and precise internal knowledge structure with less internal redundancy to absorb quantization-induced perturbations. This means that when quantization errors are introduced, they have a larger impact because the model has less capacity to compensate. The core assumption is that models with stronger pre-training have less internal redundancy and are therefore more sensitive to quantization-induced perturbations. Evidence indicates various LoRA-FT quantization methods yield worse performance for quantized LLaMA3 below 4 bits compared with their 4-bit counterparts without LoRA-FT.

### Mechanism 3
MLLM models like LLaVA-Next-8B suffer more severe performance degradation under ultra-low bit quantization than pure LLMs because the quantization of the LLM backbone exacerbates the loss when processing complex visual information. When LLaVA-Next-8B is fine-tuned for visual-language tasks, the introduction of image tokens leads to partial loss and forgetting of LLaMA3's inherent language abilities. Low-bit quantization further degrades this already compromised model, particularly in visual-language understanding tasks. The core assumption is that visual-language understanding capability of MLLMs is more sensitive to quantization errors than pure language tasks, especially at ultra-low bit widths. Evidence shows quantized LLaMA3 backbone in MLLM exacerbates performance loss when processing complex visual information, with 4-bit MLLM exhibiting less than 2% loss on multi-modal benchmarks.

## Foundational Learning

- **Concept**: Post-Training Quantization (PTQ)
  - Why needed here: PTQ is the primary technique evaluated for compressing LLaMA3 models without additional training
  - Quick check question: What is the main difference between PTQ and quantization-aware training (QAT) in terms of computational requirements during the quantization process?

- **Concept**: Low-Rank Adaptation (LoRA) Fine-Tuning
  - Why needed here: LoRA-FT is evaluated as an alternative quantization approach that adds low-rank matrices to pre-trained model weights
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning while still allowing model adaptation?

- **Concept**: Multi-Modal Large Language Models (MLLMs)
  - Why needed here: The study evaluates LLaVA-Next-8B, an MLLM based on LLaMA3, for visual-language tasks
  - Quick check question: What is the typical architecture used to combine vision and language processing in MLLMs like LLaVA-Next-8B?

## Architecture Onboarding

- **Component map**: LLaMA3-8B/70B, LLaVA-Next-8B -> 10 quantization methods (9 PTQ + 2 LoRA-FT) -> WikiText2, C4, PTB (perplexity) -> PIQA, ARC-e, ARC-c, HellaSwag, Winogrande, MMLU (accuracy) -> AI2D, ChartQA, DocVQA, MME, MMBench (MLLM tasks)

- **Critical path**: Load pre-trained LLaMA3 models -> Apply selected quantization method with specified bit-width -> Calibrate using WikiText2 dataset (128 samples, sequence length 2048) -> Evaluate on target datasets using appropriate metrics -> Record performance metrics, GPU memory usage, and inference latency

- **Design tradeoffs**: Bit-width vs. performance (lower bit-widths offer greater compression but significant performance degradation); Memory vs. speed (quantization reduces memory but may increase inference latency); Method complexity vs. effectiveness (more sophisticated methods can handle ultra-low bits better but may be more complex to implement)

- **Failure signatures**: Complete performance collapse (scores dropping to zero) at 2-bit quantization for MLLMs; Significant perplexity increases (>100x) at 2-bit quantization for language models; Negative impact of LoRA-FT on quantized models below 4 bits; Memory usage spikes during quantization process for methods like OmniQuant

- **First 3 experiments**: 1) Apply 4-bit GPTQ quantization to LLaMA3-8B and evaluate on WikiText2, C4, and PTB to establish baseline performance; 2) Apply 2-bit SmoothQuant quantization to LLaMA3-70B and evaluate on CommonSenseQA benchmarks to observe ultra-low bit performance; 3) Apply 4-bit QLoRA fine-tuning to LLaMA3-8B on Alpaca dataset and evaluate on MMLU to test LoRA-FT quantization performance

## Open Questions the Paper Calls Out

- How does the performance degradation in LLaMA3 models under ultra-low bit quantization compare to other state-of-the-art LLMs when both are subjected to the same quantization methods?
- What specific characteristics of LLaMA3's internal knowledge structure contribute to its increased susceptibility to quantization perturbation compared to LLaMA and LLaMA2?
- What novel quantization paradigms or techniques could effectively compensate for the performance loss in LLaMA3 models that cannot be addressed by current LoRA-FT methods?

## Limitations

- The study focuses on only 10 quantization methods across a limited range of bit-widths, potentially missing other effective techniques
- LoRA-FT evaluation uses the relatively small Alpaca dataset (52K samples) compared to the massive LLaMA3 pre-training corpus (15T tokens)
- The study does not explore quantization-aware training (QAT) methods, which could potentially yield better performance than PTQ approaches

## Confidence

**High Confidence**: 4-bit quantization maintains acceptable performance across most tasks with perplexity increases below 2% for language tasks and accuracy drops under 10% for zero-shot benchmarks

**Medium Confidence**: Ultra-low bit quantization (2-3 bits) causes severe performance degradation, particularly for MLLMs, though the exact mechanisms and mitigation strategies require further investigation

**Low Confidence**: LLaMA3's superior pre-training makes it more sensitive to quantization errors compared to previous LLaMA/LLaMA2 models, as this conclusion is based on indirect comparisons without direct experimental validation

## Next Checks

1. Replicate LoRA-FT experiments using larger fine-tuning datasets (minimum 1M samples) to determine if the negative impact at ultra-low bit widths persists when fine-tuning scale approaches pre-training scale

2. Implement and evaluate a representative quantization-aware training method alongside PTQ approaches to establish whether QAT can outperform PTQ at ultra-low bit widths, particularly for MLLMs

3. Design and test a hybrid quantization approach that applies different quantization methods to different weight groups to investigate whether performance degradation at ultra-low bits can be mitigated through architectural optimization