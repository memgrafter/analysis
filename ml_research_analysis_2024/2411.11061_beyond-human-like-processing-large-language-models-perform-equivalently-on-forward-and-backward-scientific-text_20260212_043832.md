---
ver: rpa2
title: 'Beyond Human-Like Processing: Large Language Models Perform Equivalently on
  Forward and Backward Scientific Text'
arxiv_id: '2411.11061'
source_url: https://arxiv.org/abs/2411.11061
tags:
- language
- human
- arxiv
- brainbench
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that large language models (LLMs) trained
  on scientific text can perform equally well on prediction tasks regardless of whether
  the text is presented in forward or backward order. Researchers trained GPT-2 models
  on twenty years of neuroscience literature, with some models receiving forward text
  and others receiving character-level reversed text.
---

# Beyond Human-Like Processing: Large Language Models Perform Equivalently on Forward and Backward Scientific Text

## Quick Facts
- arXiv ID: 2411.11061
- Source URL: https://arxiv.org/abs/2411.11061
- Authors: Xiaoliang Luo; Michael Ramscar; Bradley C. Love
- Reference count: 40
- Primary result: LLMs trained on scientific text perform equally well on prediction tasks regardless of forward or backward presentation

## Executive Summary
This study demonstrates that transformer-based language models can extract predictive patterns from structured text regardless of temporal ordering. Researchers trained GPT-2 models on twenty years of neuroscience literature, with some models receiving forward text and others receiving character-level reversed text. Both forward and backward-trained models performed at or above human expert levels on the BrainBench neuroscience prediction benchmark, despite backward-trained models showing higher perplexity scores. The findings suggest that LLMs are general pattern-learning machines rather than specific models of human language processing, as they can extract predictive structure from text regardless of its format or alignment with natural language properties.

## Method Summary
The researchers trained GPT-2 transformer models (124M, 355M, and 774M parameters) on a 20-year neuroscience corpus (1.3 billion tokens, 2002-2022). Some models received forward text while others received character-level reversed text. A specialized tokenizer was trained from scratch on the reversed neuroscience text. Models were evaluated on the BrainBench benchmark using a perplexity-based selection task between original and altered neuroscience abstracts. The study compared performance across forward and backward-trained models and analyzed perplexity scores and correlations with human expert judgments.

## Key Results
- Forward and backward-trained models matched or exceeded human expert performance on BrainBench
- Backward-trained models consistently showed higher perplexities on validation and test data
- Higher perplexity in backward-trained models did not impair discriminative performance
- Both model types achieved comparable accuracy despite different levels of uncertainty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer architectures can extract predictive patterns from input regardless of temporal ordering or alignment with natural language structure.
- Mechanism: The self-attention mechanism allows tokens to attend to all other tokens simultaneously, enabling the model to learn bidirectional relationships and predictive patterns without requiring sequential processing order.
- Core assumption: The predictive task remains well-defined and the input maintains sufficient statistical structure even when reversed at character level.
- Evidence anchors: The study shows transformer success across diverse domains beyond language, suggesting pattern-learning capabilities are domain-agnostic.

### Mechanism 2
- Claim: Specialized tokenizers trained on domain-specific data can compensate for reversed input by generating more meaningful token boundaries.
- Mechanism: When tokenizer is trained on reversed neuroscience text, it learns to create tokens that capture meaningful patterns in the reversed space, effectively "reversing" the reversal at the token level and preserving domain-specific structure.
- Core assumption: The tokenizer can learn meaningful subword units even when training data is reversed, and these units preserve domain-specific information.
- Evidence anchors: The study shows performance improvement but doesn't isolate tokenizer effects from model architecture effects.

### Mechanism 3
- Claim: Higher perplexity in backward-trained models reflects increased uncertainty but doesn't impair discriminative ability because the task is binary classification based on relative perplexity.
- Mechanism: Even with higher overall uncertainty, the model can still effectively compare two options and choose the one with lower perplexity, maintaining discriminative performance despite increased uncertainty.
- Core assumption: The binary classification task depends on relative perplexity differences rather than absolute perplexity values.
- Evidence anchors: Empirical observation that higher perplexity doesn't correlate with lower accuracy in the binary classification task.

## Foundational Learning

- Concept: Character-level text reversal and its implications for language structure
  - Why needed here: Understanding why character-level reversal is more challenging than word-level reversal and how it affects language processing
  - Quick check question: What happens to word boundaries and morphological structure when text is reversed at the character level versus the word level?

- Concept: Perplexity as a measure of model uncertainty and its relationship to task performance
  - Why needed here: Interpreting why higher perplexity doesn't necessarily mean worse performance in discriminative tasks
  - Quick check question: How can a model with higher perplexity still perform better on a binary classification task that uses relative perplexity?

- Concept: Domain-specific tokenization and its impact on model performance
  - Why needed here: Understanding how specialized tokenizers can improve performance on domain-specific tasks
  - Quick check question: Why might a tokenizer trained on reversed neuroscience text perform differently than one trained on forward text?

## Architecture Onboarding

- Component map: GPT-2 transformer architecture (124M, 355M, 774M parameters) -> Domain-specific tokenizer (trained from scratch on neuroscience corpus) -> Training pipeline (forward and backward variants) -> BrainBench evaluation framework -> Perplexity calculation and comparison mechanism

- Critical path: 1. Data preparation (forward and reversed at character level) -> 2. Tokenizer training on domain-specific data -> 3. Model training from scratch on prepared data -> 4. Validation on held-out data -> 5. BrainBench evaluation using perplexity comparison -> 6. Correlation analysis with human judgments

- Design tradeoffs:
  - Forward vs. backward training: Forward aligns with natural language but backward tests architectural generality
  - Character-level vs. word-level reversal: Character-level is more challenging but tests deeper architectural capabilities
  - Domain-specific vs. general tokenizer: Domain-specific improves performance but requires more training data

- Failure signatures:
  - High perplexity with low accuracy: Model cannot extract useful patterns
  - Low perplexity with low accuracy: Model memorizes training data but cannot generalize
  - Poor correlation with human judgments: Model uses different features than humans
  - No performance difference between forward/backward: Model architecture not leveraging order information

- First 3 experiments:
  1. Compare performance on reversed vs. shuffled text to isolate the effect of preserved sequential structure
  2. Test with different reversal granularities (character, word, sentence) to identify the level at which performance degrades
  3. Evaluate with corrupted or noisy reversed text to determine robustness of pattern extraction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do transformer-based language models trained on reversed text show different learning dynamics or convergence patterns compared to those trained on forward text, particularly in terms of parameter updates and gradient flows during training?
- Basis in paper: The paper notes that backward-trained models exhibit higher perplexities on both validation and BrainBench items, suggesting different processing patterns, but does not investigate the underlying learning dynamics.
- Why unresolved: The study focuses on final performance outcomes rather than the intermediate learning processes, leaving open how models process reversed versus forward text during training.
- What evidence would resolve it: Detailed analysis of training curves, gradient norms, attention pattern evolution, and parameter update statistics for both forward and backward-trained models throughout training would clarify differences in learning dynamics.

### Open Question 2
- Question: How does character-level reversal of text affect the internal representations and attention patterns in transformer models compared to word-level or token-level reversal?
- Basis in paper: The authors note their approach differs from previous studies by using character-level reversal and training a new tokenizer from scratch on reversed text.
- Why unresolved: While the paper demonstrates equivalent performance, it does not investigate how character-level reversal specifically impacts the model's internal mechanisms compared to other reversal strategies.
- What evidence would resolve it: Comparative analysis of attention patterns, internal representations, and performance across models trained on character-level, word-level, and token-level reversed text would reveal the impact of reversal granularity.

### Open Question 3
- Question: To what extent does the higher perplexity observed in backward-trained models affect their ability to generalize to unseen tasks beyond the BrainBench domain?
- Basis in paper: The authors note that backward-trained models show higher perplexities but this "additional uncertainty does not impair their ability to discriminate between correct and incorrect experimental outcomes in BrainBench."
- Why unresolved: The study only evaluates performance on one specific task (BrainBench), leaving open whether higher perplexity in backward-trained models might impact generalization to other tasks.
- What evidence would resolve it: Testing both forward and backward-trained models across diverse downstream tasks and domains would reveal whether higher perplexity correlates with reduced generalization ability.

## Limitations

- The study doesn't isolate whether performance equivalence is due to transformer architecture capabilities or tokenizer adaptation to reversed text
- Results are limited to one specific domain (neuroscience) and one evaluation task (BrainBench), limiting generalizability claims
- The cognitive plausibility of equivalent forward/backward processing remains unclear, as human language processing clearly depends on sequential order

## Confidence

- **High Confidence:** Transformer architectures can extract predictive patterns from structured input regardless of forward/backward ordering (supported by consistent performance across multiple model sizes)
- **Medium Confidence:** Higher perplexity in backward-trained models doesn't impair discriminative performance (empirically observed but mechanism not fully understood)
- **Low Confidence:** The results demonstrate that LLMs are "general pattern-learning machines" rather than models of human processing (strong claim requiring more comparative cognitive evidence)

## Next Checks

1. **Control for tokenizer effects:** Train models using a standard tokenizer (not trained on reversed text) to determine whether performance equivalence persists without tokenizer adaptation to reversed input
2. **Test with word-level reversal:** Compare character-level reversal results with word-level reversal to identify the minimal reversal granularity that preserves predictive capability
3. **Validate perplexity-based discrimination:** Conduct ablation studies where absolute perplexity values are manipulated to test whether relative differences alone drive performance, or if the absolute uncertainty level matters