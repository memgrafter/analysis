---
ver: rpa2
title: 'LLM Augmented LLMs: Expanding Capabilities through Composition'
arxiv_id: '2401.02412'
source_url: https://arxiv.org/abs/2401.02412
tags:
- calm
- languages
- code
- language
- composition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CALM, a framework for composing language
  models to augment their capabilities. CALM enables the combination of an anchor
  model with specialized augmenting models through cross-attention between their intermediate
  representations, allowing new tasks to be performed without modifying the original
  models.
---

# LLM Augmented LLMs: Expanding Capabilities through Composition

## Quick Facts
- arXiv ID: 2401.02412
- Source URL: https://arxiv.org/abs/2401.02412
- Reference count: 26
- Primary result: CALM achieves up to 13% absolute improvement on low-resource translation and 40% relative improvement on code tasks using only a fraction of training data and parameters compared to full fine-tuning

## Executive Summary
This paper introduces CALM, a framework for composing language models to augment their capabilities without modifying the original models. CALM enables the combination of an anchor model with specialized augmenting models through cross-attention between their intermediate representations. The method introduces a small number of trainable parameters to learn effective combinations of the models, allowing new tasks to be performed while preserving existing capabilities. Experiments demonstrate significant performance improvements across tasks like low-resource language translation, arithmetic reasoning, code generation, and explanation tasks, achieved with minimal additional parameters and training data.

## Method Summary
CALM composes two frozen language models (an anchor model and an augmenting model) by introducing projection layers that map the augmenting model's representations to the anchor model's dimensionality, followed by cross-attention layers that combine these representations with the anchor model's intermediate states. The composition is trained on a small dataset representing combined skills of both models, with only the cross-attention parameters being updated. This approach allows the composed model to perform new tasks while preserving the original capabilities of both base models.

## Key Results
- Achieves up to 13% absolute improvement on low-resource language translation tasks
- Demonstrates 40% relative improvement on code generation and explanation tasks
- Uses only a fraction of training data and parameters compared to full fine-tuning
- Successfully composes models for arithmetic reasoning, low-resource translation, and code generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention between intermediate layer representations enables composition without modifying the original models
- Mechanism: Trainable projection layers map augmenting model representations to anchor model dimensionality, followed by cross-attention layers that combine these representations with anchor model's intermediate states
- Core assumption: Intermediate representations from both models contain complementary information that can be effectively combined through learned attention patterns
- Evidence anchors: [abstract] mentions cross-attention composition; [section 3.1] describes the projection and cross-attention components; related papers lack direct evidence for this specific mechanism
- Break condition: If intermediate representations are too dissimilar or cross-attention cannot learn effective combination patterns

### Mechanism 2
- Claim: Composition can be learned with minimal data and parameters compared to full fine-tuning
- Mechanism: Freezing original models and training only small cross-attention parameters on a fraction of combined task data achieves effective composition
- Core assumption: Original models retain sufficient capability to be composed effectively without full retraining
- Evidence anchors: [abstract] states gains achieved with fraction of data/parameters; [section 4.1] explains frozen models with trained composition parameters; related papers discuss parameter-efficient methods but not this composition approach
- Break condition: If original models lose capability during composition or cross-attention cannot learn effective combinations

### Mechanism 3
- Claim: Composition preserves existing capabilities while adding new ones
- Mechanism: Framework maintains original model weights while learning to combine their representations, allowing composed model to retain both base capabilities and new composed capability
- Core assumption: Cross-attention mechanism can learn to selectively use representations from each model based on the task
- Evidence anchors: [abstract] mentions preserving existing capabilities; [section 3.1] states weights are frozen; related papers discuss model preservation but lack direct evidence for this mechanism
- Break condition: If composition process interferes with original model's ability to perform base tasks

## Foundational Learning

- Concept: Cross-attention mechanisms
  - Why needed here: Cross-attention is the core mechanism that allows combining representations from two different models
  - Quick check question: How does cross-attention differ from self-attention, and why is it necessary for model composition?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: Understanding parameter-efficient methods helps contextualize how CALM achieves composition with minimal additional parameters
  - Quick check question: What are the key differences between full fine-tuning and parameter-efficient methods like LoRA?

- Concept: Model composition strategies
  - Why needed here: This framework uses composition rather than merging or routing, which requires understanding different composition approaches
  - Quick check question: What are the advantages and disadvantages of composition vs. merging vs. routing strategies for combining models?

## Architecture Onboarding

- Component map: anchor model (mB) -> projection layers (fproj) -> cross-attention layers (fcross) -> augmenting model (mA)
- Critical path: training data (DC) -> fproj -> cross-attention -> composed output
- Design tradeoffs:
  - Number of compositional layers vs. parameter efficiency
  - Data fraction for composition vs. performance
  - Cross-attention complexity vs. training cost
- Failure signatures:
  - Poor performance on either base task indicates composition interference
  - Inability to generalize to unseen examples suggests insufficient training
  - Performance worse than base models indicates ineffective composition
- First 3 experiments:
  1. Key-value arithmetic task with synthetic data to validate basic composition mechanism
  2. Translation task with low-resource languages to test domain-specific composition
  3. Code generation task to validate cross-domain composition capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CALM performance compare when composing models with different pre-training objectives and architectures?
- Basis in paper: [explicit] The paper mentions CALM is applicable to any set of models without requiring same model origin or size, but lacks specific comparisons across different pre-training objectives and architectures
- Why unresolved: The paper doesn't provide explicit comparisons of CALM's performance when composing models with different pre-training objectives and architectures
- What evidence would resolve it: Experiments comparing CALM's performance when composing models with different pre-training objectives and architectures, such as language models pre-trained on different corpora or models with different architectures like encoder-decoder or decoder-only models

### Open Question 2
- Question: Can CALM be extended to compose models in a hierarchical or modular fashion, where multiple augmenting models are composed with a single anchor model or vice versa?
- Basis in paper: [explicit] The paper mentions CALM is applicable to multiple augmenting models and suggests learning similar projection and cross-attention components between anchor and each augmenting model, but lacks specific experiments or results on hierarchical or modular composition
- Why unresolved: The paper doesn't provide explicit experiments or results on composing models in a hierarchical or modular fashion
- What evidence would resolve it: Experiments evaluating CALM's performance when composing models in a hierarchical or modular fashion, such as composing multiple augmenting models with a single anchor model or composing a single augmenting model with multiple anchor models

### Open Question 3
- Question: How does CALM handle catastrophic forgetting when composing models with overlapping capabilities or knowledge?
- Basis in paper: [explicit] The paper mentions CALM avoids catastrophic forgetting by keeping existing model weights intact, but lacks specific experiments or results on handling catastrophic forgetting with overlapping capabilities
- Why unresolved: The paper doesn't provide explicit experiments or results on how CALM handles catastrophic forgetting when composing models with overlapping capabilities or knowledge
- What evidence would resolve it: Experiments evaluating CALM's performance when composing models with overlapping capabilities or knowledge, such as composing two models trained on the same task or domain, and measuring impact on composed model's performance

## Limitations

- Limited empirical validation across diverse domains and model sizes
- Potential trade-off between composition and base performance not fully analyzed
- Heavy dependence on appropriate model selection without clear guidelines
- Computational overhead of cross-attention mechanism during inference not fully addressed

## Confidence

**High Confidence**: The core mechanism of using cross-attention between intermediate representations is well-supported by theoretical framework and initial experimental results
**Medium Confidence**: Claims about parameter and data efficiency compared to full fine-tuning are supported but need more comprehensive comparisons
**Low Confidence**: Generalizability claims to diverse domains and tasks are not well-supported due to limited experimental scope

## Next Checks

1. **Cross-model compatibility study**: Systematically test CALM across different combinations of anchor and augmenting models with varying sizes and architectures to identify optimal pairing strategies and quantify performance degradation when using mismatched models

2. **Long-term capability preservation analysis**: Evaluate how CALM composition affects the anchor model's performance on its original tasks over extended use, including stress-testing scenarios where the composed model is used extensively for the new task

3. **Computational efficiency benchmarking**: Conduct comprehensive benchmarking of inference-time computational costs for CALM across different model sizes and composition depths, comparing these costs against both the base models and alternative composition methods like model merging or routing