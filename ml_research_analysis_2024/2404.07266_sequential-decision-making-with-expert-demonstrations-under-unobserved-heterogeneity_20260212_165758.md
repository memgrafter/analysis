---
ver: rpa2
title: Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity
arxiv_id: '2404.07266'
source_url: https://arxiv.org/abs/2404.07266
tags:
- expert
- prior
- learning
- data
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles online sequential decision-making when expert\
  \ demonstrations contain unobserved contextual heterogeneity\u2014a common real-world\
  \ challenge in domains like education and autonomous systems. The proposed Experts-as-Priors\
  \ (ExPerior) framework uses empirical Bayes to learn an informative prior over hidden\
  \ contexts from offline expert data, then applies Bayesian methods (e.g., posterior\
  \ sampling) for online exploration."
---

# Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity

## Quick Facts
- arXiv ID: 2404.07266
- Source URL: https://arxiv.org/abs/2404.07266
- Reference count: 40
- Primary result: ExPerior consistently outperforms baselines in multi-armed bandits, MDPs, and POMDPs by learning priors from expert demonstrations with unobserved contextual heterogeneity

## Executive Summary
This work addresses online sequential decision-making when expert demonstrations contain unobserved contextual heterogeneity—a common challenge in domains like education and autonomous systems. The proposed Experts-as-Priors (ExPerior) framework uses empirical Bayes to learn an informative prior over hidden contexts from offline expert data, then applies Bayesian methods for online exploration. Two approaches are developed: parametric learning with known priors and nonparametric max-entropy priors. Experiments show ExPerior consistently outperforms baselines including behavior cloning and standard online methods across bandits, MDPs, and POMDPs.

## Method Summary
ExPerior learns a prior distribution over unobserved contextual variables from expert demonstrations using maximum marginal likelihood estimation. This prior enables Bayesian approaches like posterior sampling for more informed online exploration. The framework offers both parametric methods (when the prior form is known, e.g., Beta distribution for bandits) and nonparametric max-entropy approaches for greater flexibility. The learned prior is then used in online decision-making tasks, with experiments demonstrating reduced Bayesian regret and improved reward performance compared to baselines.

## Key Results
- ExPerior consistently outperforms behavior cloning, standard online methods, and offline-online hybrids across multi-armed bandits, MDPs, and POMDPs
- Bayesian regret scales with the entropy of optimal actions under the prior, confirming expert data's value when unobserved factors have limited impact
- In multi-armed bandits, ExPerior matches or exceeds the performance of methods with access to true contexts when expert actions are near-optimal
- Nonparametric max-entropy priors show greater robustness to misspecified expert models compared to parametric approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ExPerior uses empirical Bayes to learn a prior over unobserved contextual variables from expert demonstrations.
- Mechanism: The framework computes a maximum likelihood estimate of the prior distribution from expert trajectories, treating the unobserved context as latent variables.
- Core assumption: Expert demonstrations contain actions generated under different unobserved contexts that follow the same distribution as the learner will face online.
- Evidence anchors: [abstract] "empirical Bayes approach that utilizes expert data to establish an informative prior distribution"; [section 4] "We resort to empirical Bayes and use maximum marginal likelihood estimation to construct a prior distribution from DE"
- Break condition: If expert contexts are drawn from a different distribution than the learner's online context, the learned prior becomes uninformative or misleading.

### Mechanism 2
- Claim: The learned prior enables Bayesian methods like posterior sampling to perform more informed exploration.
- Mechanism: Once the prior over contexts is learned, ExPerior samples from the posterior distribution over context parameters to select actions, focusing exploration on plausible context regions.
- Core assumption: The context distribution remains fixed during the online learning phase.
- Evidence anchors: [abstract] "This prior distribution enables the application of any Bayesian approach for online decision-making, such as posterior sampling"; [section 4] "We use the learned prior distribution to drive exploration in the online RL task, using approaches like posterior sampling"
- Break condition: If contexts change over time or if the prior is poorly estimated, posterior sampling may converge to suboptimal policies.

### Mechanism 3
- Claim: ExPerior's regret scales with the entropy of optimal actions under the prior, showing expert data's value when unobserved factors have limited impact.
- Mechanism: In bandit settings, the algorithm's Bayesian regret is proportional to how much expert data reveals about which actions are optimal across contexts.
- Core assumption: Expert actions are near-optimal for their respective contexts (Assumption 1).
- Evidence anchors: [abstract] "Bayesian regret scales with the entropy of optimal actions under the prior"; [section 5] "we find the Bayesian regret incurred by ExPerior is proportional to the entropy of the optimal action"
- Break condition: If expert actions are highly noisy or contexts are highly heterogeneous, regret may not improve over standard methods.

## Foundational Learning

- Concept: Empirical Bayes estimation
  - Why needed here: Allows learning the prior distribution over unobserved contexts directly from expert data without assuming a known parametric form
  - Quick check question: What distinguishes empirical Bayes from standard Bayesian methods in this context?

- Concept: Maximum entropy principle
  - Why needed here: Provides a principled way to select the least informative prior that's consistent with expert data when parametric assumptions are unavailable
  - Quick check question: How does maximum entropy differ from maximum likelihood when choosing among multiple compatible priors?

- Concept: Posterior sampling (Thompson sampling)
  - Why needed here: Enables exploration guided by the learned prior, focusing on plausible context regions rather than uniform exploration
  - Quick check question: Why might posterior sampling with a learned prior outperform uniform exploration in this setting?

## Architecture Onboarding

- Component map: Expert demonstrations → Prior estimation → Online posterior sampling → Action selection → Reward collection
- Critical path: Expert data → Prior estimation → Online posterior sampling → Action selection → Reward collection
- Design tradeoffs:
  - Parametric vs. nonparametric prior: Parametric methods are computationally efficient but may be misspecified; nonparametric methods are more flexible but computationally intensive
  - Exact vs. approximate posterior sampling: Exact methods require conjugate priors; approximate methods (SGLD) are more general but introduce sampling error
  - Context parameterization: Choice of how to represent unobserved contexts affects prior quality and computational tractability
- Failure signatures:
  - Poor performance despite expert data: Likely prior misspecification or context distribution mismatch
  - High variance in results: May indicate insufficient expert demonstrations or unstable posterior sampling
  - Regret scaling poorly: Could suggest expert actions are not near-optimal or contexts are too heterogeneous
- First 3 experiments:
  1. K-armed Bernoulli bandits with known Beta prior: Compare parametric vs. nonparametric prior learning
  2. Deep Sea environment with varying goal distributions: Test performance across low to high entropy contexts
  3. Frozen Lake with hidden hazards: Validate POMDP capability and robustness to different expert competence levels

## Open Questions the Paper Calls Out

- Question: How does ExPerior perform in high-dimensional state and action spaces, such as those encountered in real-world robotics or autonomous driving?
  - Basis in paper: [inferred] The paper mentions that ExPerior is evaluated on synthetic environments and does not include experiments with human-in-the-loop validation.
  - Why unresolved: The paper only tests ExPerior on low-dimensional environments (bandits, MDPs, POMDPs) and does not explore its scalability to high-dimensional problems.
  - What evidence would resolve it: Experiments on high-dimensional RL benchmarks (e.g., MuJoCo, Atari) or real-world robotic control tasks would demonstrate ExPerior's performance in complex settings.

- Question: How robust is ExPerior to changes in the unobserved context distribution over time (non-stationary settings)?
  - Basis in paper: [explicit] The paper assumes the unobserved factors remain fixed during training and notes that no algorithm can control regret if the unobserved factors change arbitrarily.
  - Why unresolved: The paper does not explore how ExPerior adapts to dynamic or non-stationary environments where the context distribution shifts over time.
  - What evidence would resolve it: Experiments on non-stationary RL tasks or theoretical analysis of regret bounds under drifting context distributions would clarify ExPerior's robustness.

- Question: What is the impact of misspecified expert models on ExPerior's performance, and how can it be mitigated?
  - Basis in paper: [explicit] The paper discusses robustness to misspecified expert models in Table 1 and Table 2, showing that ExPerior-MaxEnt is more robust than ExPerior-Param when the parametric prior is misspecified.
  - Why unresolved: While the paper shows some robustness, it does not provide a comprehensive analysis of the impact of misspecification or strategies to mitigate it.
  - What evidence would resolve it: A detailed study of ExPerior's performance under various levels of expert model misspecification and the development of robust variants would address this question.

## Limitations
- Assumes expert contexts follow the same distribution as the learner's online context, which may not hold in real-world scenarios
- Performance in highly heterogeneous settings is unclear, with diminishing returns when expert actions are noisy
- Computational complexity of nonparametric max-entropy priors may limit scalability to high-dimensional problems

## Confidence
- Confidence in core mechanism: Medium - theoretical framework is sound but empirical validation limited to synthetic environments
- Confidence in regret analysis: Medium - supported by theoretical bounds but only fully explored in bandit settings
- Confidence in robustness claims: Low - limited exploration of performance under misspecified expert models or non-stationary contexts

## Next Checks
1. Test ExPerior with expert demonstrations from a different context distribution than the learner faces to assess robustness to distribution mismatch.
2. Evaluate performance when expert actions are suboptimal (e.g., 80% optimality) to understand sensitivity to Assumption 1.
3. Compare ExPerior against state-of-the-art offline reinforcement learning methods on more complex benchmarks with human demonstrations.