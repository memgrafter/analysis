---
ver: rpa2
title: Adversarial Circuit Evaluation
arxiv_id: '2407.15166'
source_url: https://arxiv.org/abs/2407.15166
tags:
- param
- circuit
- input
- inputs
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the robustness of neural network circuits
  by measuring their worst-case performance on task-specific inputs. The authors develop
  a method using resample ablation to compute KL divergence between circuit and full
  model outputs across many input-patch input pairs, then analyze high percentiles
  and maximum values.
---

# Adversarial Circuit Evaluation

## Quick Facts
- arXiv ID: 2407.15166
- Source URL: https://arxiv.org/abs/2407.15166
- Reference count: 40
- Primary result: Circuits can fail catastrophically on benign inputs, revealing robustness issues

## Executive Summary
This paper introduces a method to evaluate the robustness of neural network circuits by measuring their worst-case performance across task-specific inputs. The authors develop a resample ablation technique to compute KL divergence between circuit and full model outputs across many input-patch combinations, then analyze high percentiles and maximum values. Their findings reveal that while some circuits like greater-than remain robust, others like IOI and docstring circuits fail significantly on certain benign inputs, particularly those involving romantic objects or specific file parameters.

## Method Summary
The authors employ resample ablation to evaluate circuit robustness by systematically perturbing input patches while the circuit is active. For each circuit, they compute the KL divergence between the circuit's output distribution and the full model's output distribution across thousands of input-patch pairs. They then analyze the distribution of these divergences, focusing on high percentiles (95th, 99th) and maximum values to identify worst-case failure modes. The method is applied to three circuits: IOI (indirect object identification), docstring (predicting docstring content), and greater-than (numeric comparison).

## Key Results
- IOI and docstring circuits show significant failures on benign inputs involving romantic objects or specific file parameters
- The greater-than circuit maintains robustness across tested inputs
- Current circuits lack reliability for safety-critical applications
- Suggests incorporating adversarial evaluation metrics into circuit discovery algorithms

## Why This Works (Mechanism)
The method works by isolating circuit behavior through resample ablation, which allows precise measurement of how circuit components contribute to overall model predictions. By computing KL divergence between circuit and full model outputs across many perturbed inputs, the approach captures both local sensitivity and global failure patterns. The statistical analysis of high percentiles reveals that failures aren't isolated incidents but systematic weaknesses in circuit design or training.

## Foundational Learning
- **Resample ablation**: A technique for measuring component importance by systematically perturbing inputs while monitoring output changes. Why needed: Provides quantitative measure of circuit contribution to overall model behavior. Quick check: Compare KL divergence distributions with and without circuit activation.
- **KL divergence analysis**: Measures distributional differences between circuit predictions and full model outputs. Why needed: Quantifies how much circuit behavior deviates from intended behavior. Quick check: Verify that KL divergence increases monotonically with perturbation severity.
- **Percentile-based robustness metrics**: Uses high percentiles (95th, 99th) rather than averages to capture worst-case behavior. Why needed: Average metrics can hide catastrophic failures that occur rarely. Quick check: Compare percentile-based metrics with traditional mean-based metrics.
- **Benign input testing**: Evaluates circuit performance on normal, non-adversarial inputs that should work correctly. Why needed: Reveals fundamental design flaws rather than just adversarial vulnerabilities. Quick check: Ensure test inputs span the full range of expected usage patterns.
- **Circuit isolation techniques**: Methods for identifying and measuring individual circuit components. Why needed: Enables targeted robustness evaluation of specific mechanisms. Quick check: Verify circuit isolation doesn't introduce artifacts in KL divergence measurements.
- **Distributional comparison methods**: Statistical techniques for comparing model output distributions. Why needed: Provides principled way to measure prediction consistency. Quick check: Validate distributional comparisons using synthetic control data.

## Architecture Onboarding

**Component Map**: Input -> Resample Ablation Module -> KL Divergence Calculator -> Percentile Analyzer -> Robustness Assessment

**Critical Path**: The resample ablation process followed by KL divergence computation forms the core evaluation pipeline. Each input-patch pair generates a divergence score, which is then aggregated to produce robustness metrics.

**Design Tradeoffs**: The method trades computational efficiency for comprehensive coverage - evaluating thousands of input-patch pairs provides thorough assessment but requires significant compute resources. Alternative approaches using fewer samples might be faster but could miss important failure modes.

**Failure Signatures**: High KL divergence values indicate circuit failure modes. The paper identifies specific patterns: IOI circuits fail on romantic object references, docstring circuits fail on certain file parameter combinations, while greater-than circuits show consistent performance across inputs.

**First Experiments**:
1. Replicate the KL divergence analysis on a new circuit (e.g., sentiment analysis) to verify generalizability
2. Compare robustness metrics across different patch sizes to establish optimal sampling strategy
3. Test circuit performance on adversarial inputs designed to trigger maximum divergence

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Results may not generalize to larger model architectures or different training paradigms
- Resample ablation sensitivity to hyperparameter choices (patch size, sample count) could affect robustness measurements
- Limited analysis of why certain circuits (greater-than) remain robust while others fail
- Focus on benign inputs may miss more severe adversarial vulnerabilities

## Confidence
- **High**: Greater-than circuit robustness findings - data clearly shows consistent performance across inputs
- **Medium**: Circuits lack reliability for safety-critical applications - based on specific circuit examples, but generalizability uncertain
- **Medium**: Claim about incorporating adversarial metrics into circuit discovery - promising direction but not yet validated

## Next Checks
1. Apply the adversarial evaluation framework to larger language models (GPT-3, LLaMA) to test scalability and identify whether robustness patterns persist across model scales
2. Conduct a systematic hyperparameter sensitivity analysis for the resample ablation method, varying patch sizes and sample counts to establish stability bounds
3. Test the circuits on out-of-distribution inputs beyond the benign examples used, including adversarial prompts specifically designed to trigger failures, to distinguish between benign and malicious vulnerabilities