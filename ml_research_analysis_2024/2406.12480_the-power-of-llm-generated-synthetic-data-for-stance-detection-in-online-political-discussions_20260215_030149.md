---
ver: rpa2
title: The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political
  Discussions
arxiv_id: '2406.12480'
source_url: https://arxiv.org/abs/2406.12480
tags:
- data
- synthetic
- samples
- question
- stance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of stance detection in online
  political discussions, where data scarcity for specific debate questions hinders
  model performance. The authors propose leveraging LLM-generated synthetic data to
  improve stance detection models.
---

# The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions

## Quick Facts
- **arXiv ID**: 2406.12480
- **Source URL**: https://arxiv.org/abs/2406.12480
- **Reference count**: 40
- **Primary result**: Fine-tuning stance detection models with LLM-generated synthetic data significantly improves performance while reducing labeling effort.

## Executive Summary
This paper addresses the challenge of stance detection in online political discussions where data scarcity for specific debate questions hinders model performance. The authors propose a novel approach that leverages Mistral-7B to generate synthetic data for specific debate questions, then fine-tunes stance detection models with this synthetic data. Their Synthetic Data-driven Query By Committee (SQBC) method identifies the most informative samples from unlabelled data, combining them with synthetic data to achieve superior performance compared to models trained on all true labels while requiring significantly less manual labeling.

## Method Summary
The method involves three key steps: (1) Generate synthetic data for specific debate questions using Mistral-7B by translating questions and prompting the model to create pro/anti comments; (2) Fine-tune a BERT-based stance detection model on this synthetic data; (3) Apply SQBC to identify the most informative samples from unlabelled data using synthetic data as an ensemble of experts, then manually label these samples and combine with synthetic data for final model training. The approach was evaluated on 10 selected test questions from the X-Stance dataset, comparing performance across different synthetic data sizes and active learning strategies.

## Key Results
- Fine-tuning with synthetic data significantly improves stance detection performance in online political discussions
- SQBC with synthetic data consistently identifies the most informative samples, outperforming random selection
- Combining synthetic data with manually labeled most informative samples surpasses performance of models trained on all true labels while requiring only 25% of the labeling effort
- Mistral-7B-generated synthetic data aligns well with real data distribution, effectively extending the decision boundary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generated by Mistral-7B aligns well with real data distribution for stance detection
- Mechanism: Mistral-7B generates comments in favor or against a question, creating synthetic data that extends the real data distribution, thus improving model performance
- Core assumption: The synthetic data generation process captures the underlying distribution of real data
- Evidence anchors:
  - [abstract] "We show that fine-tuning with synthetic data does improve stance detection performance in online political discussions."
  - [section] "We observe that the synthetic data extends the real world data, which we consider a factor as to why fine-tuning with synthetic data is effective in online political discussions."
- Break condition: If the synthetic data distribution significantly deviates from the real data, the alignment breaks

### Mechanism 2
- Claim: Synthetic data-driven Query By Committee (SQBC) effectively identifies the most informative samples
- Mechanism: SQBC uses synthetic data as an ensemble of experts to score unlabelled data points, selecting the most ambiguous samples for manual labeling
- Core assumption: The synthetic data's labels can serve as a proxy for the real data's labels in identifying informative samples
- Evidence anchors:
  - [section] "We propose a synthetic extension to the QBC (Query by Committee) method, where the synthetic data act as an ensemble of experts."
  - [section] "SQBC selects the unlabelled samples that are mostly in between the two classes of the synthetic data."
- Break condition: If the synthetic data's class separation is poor, SQBC's ability to identify informative samples is compromised

### Mechanism 3
- Claim: Combining synthetic data with manually labeled most informative samples outperforms using all true labels
- Mechanism: The combination of synthetic data's broad coverage and the most informative samples' targeted refinement improves model decision boundaries
- Core assumption: The synthetic data provides a good initial approximation of the data distribution, and the most informative samples refine this approximation
- Evidence anchors:
  - [abstract] "We surpass the performance of the baseline model that is trained with fully labeled data, while labelling considerably less data."
  - [section] "We see that on average with as little as 25% of the unlabelled data, we can consistently achieve better performance than the model trained with all true labels."
- Break condition: If the synthetic data does not extend the real data distribution or if the most informative samples do not provide new information, the combination's advantage diminishes

## Foundational Learning

- **Stance detection in NLP**: Understanding stance detection is crucial for leveraging synthetic data to improve model performance in online political discussions
  - Why needed here: Stance detection forms the core task being improved through synthetic data augmentation
  - Quick check question: What is the primary goal of stance detection in the context of online political discussions?

- **Active learning and Query By Committee (QBC)**: Active learning strategies, particularly SQBC, are used to reduce labeling effort while maximizing model performance
  - Why needed here: SQBC enables efficient selection of the most informative samples for manual labeling
  - Quick check question: How does the SQBC method use synthetic data to select the most informative samples for labeling?

- **Data augmentation with synthetic data**: Synthetic data generation and its integration into the training process are key to improving model performance and reducing labeling effort
  - Why needed here: Synthetic data serves as the foundation for both model fine-tuning and active learning
  - Quick check question: What role does synthetic data play in the data augmentation process for stance detection models?

## Architecture Onboarding

- **Component map**: Mistral-7B -> Synthetic Data Generation -> BERT Model -> SQBC Active Learning -> Final Model
- **Critical path**: 1. Generate synthetic data for specific questions using Mistral-7B; 2. Fine-tune BERT model with synthetic data; 3. Use SQBC to identify most informative samples from unlabelled data; 4. Manually label selected samples and fine-tune model with both synthetic and labeled data
- **Design tradeoffs**: Using synthetic data reduces labeling effort but may introduce noise if distribution alignment is poor; fine-tuning separate models for each question provides better performance but increases computational overhead
- **Failure signatures**: Poor performance on test questions not seen during training; high variance in model performance across different questions; inability to effectively identify informative samples using SQBC
- **First 3 experiments**: 1. Fine-tune BERT model with varying amounts of synthetic data (M=200, 500, 1000) and evaluate performance; 2. Compare effectiveness of SQBC, CAL, and random selection in identifying informative samples; 3. Combine synthetic data with manually labeled most informative samples and evaluate against using all true labels

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the synthetic data generated by Mistral-7B align well with the real data for stance detection in online political discussions?
  - Basis in paper: Explicit - The paper states that the synthetic data aligns well with the real data, extending the decision boundary of the stance detection model
  - Why unresolved: While the paper shows alignment, it lacks quantitative measures of alignment or comparison with other methods
  - What evidence would resolve it: A quantitative measure of alignment between synthetic and real data, such as KL divergence or Wasserstein distance

- **Open Question 2**: Can the underlying distribution of the synthetic data serve as a proxy to combine real unlabelled data from different questions with similar (compatible) underlying distributions?
  - Basis in paper: Inferred - The paper mentions future work could study whether synthetic data distribution can serve as a proxy for combining real unlabelled data from different questions
  - Why unresolved: The paper does not explore this possibility or assess whether synthetic data distribution can be used as a proxy
  - What evidence would resolve it: Experiments comparing performance of models trained on synthetic data combined with real unlabelled data from different questions versus models trained on real unlabelled data without synthetic data

- **Open Question 3**: How does the performance of the stance detection model change when using synthetic data generated by different LLMs?
  - Basis in paper: Explicit - The paper mentions Mahmoudi et al. [2024] study synthetic data for data augmentation using GPT-3 with mixed results, while this paper uses Mistral-7B
  - Why unresolved: The paper does not compare performance when using synthetic data generated by different LLMs
  - What evidence would resolve it: Experiments comparing performance when using synthetic data generated by different LLMs, such as GPT-3, Mistral-7B, or other LLMs

## Limitations

- Dependency on alignment quality between synthetic and real data distributions may not generalize to other political discussion domains with different linguistic patterns
- Effectiveness of SQBC method is contingent on synthetic data serving as an effective proxy for real data in identifying informative samples
- Computational overhead of generating synthetic data for each new question and potential for synthetic data to introduce noise if not properly aligned

## Confidence

- **High Confidence**: Improvement in stance detection performance through fine-tuning with synthetic data is well-supported by experimental results
- **Medium Confidence**: Effectiveness of SQBC method in identifying informative samples is supported but generalizability to other datasets is uncertain
- **Low Confidence**: Long-term scalability and cost-effectiveness of using LLM-generated synthetic data for diverse political discussions are not fully addressed

## Next Checks

1. **Distribution Alignment Validation**: Conduct t-SNE visualization of embeddings for synthetic and real data to quantitatively assess distribution alignment. Measure the Wasserstein distance between the two distributions to ensure synthetic data effectively extends real data's decision boundary.

2. **Cross-Dataset Generalization**: Apply SQBC method and synthetic data fine-tuning to a different political discussion dataset (e.g., English or different cultural context) to evaluate robustness and generalizability. Compare performance gains to those observed on X-Stance dataset.

3. **Synthetic Data Quality Assessment**: Perform human evaluation of a sample of synthetic data generated by Mistral-7B to assess quality and relevance to target political discussion topics. Analyze synthetic data's class separation and compare to real data's class distribution to identify potential biases or misalignments.