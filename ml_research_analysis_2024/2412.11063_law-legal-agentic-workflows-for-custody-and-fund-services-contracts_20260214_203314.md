---
ver: rpa2
title: 'LAW: Legal Agentic Workflows for Custody and Fund Services Contracts'
arxiv_id: '2412.11063'
source_url: https://arxiv.org/abs/2412.11063
tags:
- contracts
- legal
- tools
- clauses
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LAW is an agentic framework for querying custody and fund services
  contracts. It uses a code generation agent to orchestrate reusable legal domain-specific
  tools and text agents, enabling accurate retrieval and analytical tasks on complex
  unstructured contracts.
---

# LAW: Legal Agentic Workflows for Custody and Fund Services Contracts

## Quick Facts
- arXiv ID: 2412.11063
- Source URL: https://arxiv.org/abs/2412.11063
- Reference count: 18
- LAW achieves up to 92.9% accuracy gains on complex multi-hop reasoning tasks compared to gpt-3.5-turbo baseline

## Executive Summary
LAW is an agentic framework designed for querying custody and fund services contracts using a code generation agent to orchestrate reusable legal domain-specific tools and text agents. The system enables accurate retrieval and analytical tasks on complex unstructured contracts, demonstrating significant performance improvements over baseline approaches. LAW processes 23 years of EDGAR filings and offers a cost-effective alternative to fine-tuned legal LLMs while maintaining flexibility and reusability.

## Method Summary
LAW employs a multi-agent architecture where a code generation agent serves as the orchestrator, coordinating specialized tools and text agents to process legal contracts. The framework breaks down complex queries into manageable sub-queries, with the code generation agent determining when to use domain-specific tools versus calling text agents. The system leverages vector database retrieval for efficient information access and implements a modular design that allows for easy addition of new tools and capabilities.

## Key Results
- LAW achieves up to 92.9% accuracy gains on complex multi-hop reasoning tasks like calculating contract termination dates
- The system demonstrates strong performance across both retrieval and analytical queries on 23 years of EDGAR filings
- LAW offers a cost-effective alternative to fine-tuned legal LLMs while maintaining flexibility and reusability

## Why This Works (Mechanism)
LAW's success stems from its intelligent orchestration of specialized agents and tools, allowing for precise handling of complex legal queries. The code generation agent acts as a central coordinator, making informed decisions about when to use domain-specific tools versus text agents based on query complexity. This hybrid approach combines the strengths of rule-based tools for structured tasks with the flexibility of language models for nuanced interpretation, resulting in superior performance on both retrieval and analytical tasks.

## Foundational Learning

**Agentic Workflows**: Why needed - Enables autonomous problem-solving through coordinated agents; Quick check - Verify agents can handle chained reasoning tasks independently

**Code Generation Agents**: Why needed - Provides programmatic control over tool usage and query decomposition; Quick check - Test agent's ability to generate appropriate code for various query types

**Domain-Specific Tools**: Why needed - Ensures accurate handling of legal contract structures and terminology; Quick check - Validate tool outputs against known contract elements

**Vector Database Retrieval**: Why needed - Enables efficient searching through large contract corpora; Quick check - Measure retrieval precision and recall on benchmark datasets

## Architecture Onboarding

**Component Map**: User Query -> Code Generation Agent -> (Domain Tools OR Text Agents) -> Vector Database -> Results

**Critical Path**: Query input → Code generation agent orchestration → Tool selection → Vector database retrieval → Result generation

**Design Tradeoffs**: The system trades off potential single-point failure (code generation agent) for unified orchestration and decision-making, prioritizing accuracy and modularity over distributed autonomy.

**Failure Signatures**: System failures typically manifest as incorrect tool selection, retrieval errors, or misinterpretation of complex legal language, often cascading from initial orchestration decisions.

**First Experiments**:
1. Test basic contract retrieval with simple queries to validate vector database integration
2. Evaluate multi-hop reasoning on synthetic contracts to assess orchestration logic
3. Compare performance on standard legal queries against baseline GPT-3.5-turbo implementation

## Open Questions the Paper Calls Out
None

## Limitations
- Does not address handling contracts with highly unusual or non-standard legal language
- Evaluation focused on EDGAR filings may not represent full diversity of custody and fund services contracts
- Reliance on single code generation agent as orchestrator creates potential single point of failure

## Confidence

High confidence in performance claims and architectural design, given comprehensive evaluation on 23 years of EDGAR filings and detailed ablation studies.

Medium confidence in cost-effectiveness claims relative to fine-tuned legal LLMs, as specific cost comparisons are not provided.

Medium confidence in generalizability of results to other legal domains, as evaluation is specific to custody and fund services contracts.

## Next Checks

1. Test LAW's performance on contracts containing highly specialized or non-standard legal terminology not commonly found in EDGAR filings.

2. Conduct stress testing to evaluate system behavior when the code generation agent fails or produces incorrect outputs.

3. Evaluate the framework's performance on other types of legal contracts (e.g., employment agreements, intellectual property licenses) to assess domain transferability.