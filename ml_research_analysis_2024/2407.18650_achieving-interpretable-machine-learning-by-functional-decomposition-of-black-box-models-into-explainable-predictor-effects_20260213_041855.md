---
ver: rpa2
title: Achieving interpretable machine learning by functional decomposition of black-box
  models into explainable predictor effects
arxiv_id: '2407.18650'
source_url: https://arxiv.org/abs/2407.18650
tags:
- effects
- prediction
- functions
- learning
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for decomposing black-box
  machine learning predictions into interpretable subfunctions representing main effects
  and interactions. The method uses "stacked orthogonality" to ensure main effects
  are not contaminated by higher-order interactions while capturing maximal functional
  behavior.
---

# Achieving interpretable machine learning by functional decomposition of black-box models into explainable predictor effects

## Quick Facts
- arXiv ID: 2407.18650
- Source URL: https://arxiv.org/abs/2407.18650
- Authors: David Köhler; David Rügamer; Matthias Schmid
- Reference count: 40
- Primary result: Introduces a novel method using stacked orthogonality to decompose black-box predictions into interpretable main effects and interactions

## Executive Summary
This paper presents a novel approach for making black-box machine learning models interpretable by decomposing their predictions into subfunctions representing main effects and interactions. The method employs "stacked orthogonality" to ensure that main effects capture maximal functional behavior without being contaminated by higher-order interactions. Unlike existing approaches, this method avoids extrapolation issues and hidden interactions while maintaining computational efficiency through the use of neural additive models with post-hoc orthogonalization.

## Method Summary
The method uses neural additive models (NAMs) to approximate prediction functions, with separate neural networks for each subfunction representing individual features or feature combinations. After initial fitting, a post-hoc orthogonalization procedure ensures that the decomposition satisfies stacked orthogonality constraints - that main effects are uncorrelated with higher-order interactions at each level. The process involves iterative projection of higher-order effects onto lower-order subspaces, replacing them with orthogonal residuals. An ensemble approach with multiple NAM fits improves stability, and the final orthogonalization produces interpretable subfunctions that can be analyzed independently.

## Key Results
- Synthetic experiments show successful recovery of true underlying subfunctions with explained variance coefficients of 0.511-0.980 for main effects and 0.017-0.077 for two-way interactions
- The method effectively isolates feature effects from higher-order interactions using stacked orthogonality
- Provides practical interpretability without requiring extrapolation or suffering from hidden interactions common in other IML approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stacked orthogonality achieves purity by ensuring main effects are uncorrelated with higher-order interaction effects at each level
- Mechanism: The method projects higher-order effects onto lower-order subspaces and replaces them with orthogonal residuals, ensuring that main effects do not contain information explained by interactions
- Core assumption: Linear independence of the subfunctions guarantees unique decomposition and closedness of lower-order subspaces
- Evidence anchors:
  - [abstract] "stacked orthogonality, which ensures that the main effects capture as much functional behavior as possible and do not contain information explained by higher-order interactions"
  - [section] "Unlike hierarchical orthogonality, which requires the effect of each individual feature combination θ to be uncorrelated with higher-order effects, the conditions in Eq. 3 provide a level-wise implementation of the purity criterion"
  - [corpus] Weak evidence - the corpus neighbors focus on related IML methods but don't directly address the orthogonality mechanism
- Break condition: If the linear independence assumption fails, the decomposition becomes non-unique and the orthogonality constraints cannot be satisfied

### Mechanism 2
- Claim: Neural additive models with post-hoc orthogonalization efficiently approximate arbitrary functional forms without extrapolation issues
- Mechanism: NAMs use separate neural networks for each subfunction, allowing flexible approximation of complex shapes, while post-hoc orthogonalization ensures proper decomposition without requiring expensive re-fitting
- Core assumption: Square integrable prediction functions can be approximated arbitrarily well by neural networks
- Evidence anchors:
  - [abstract] "Unlike earlier functional IML approaches, it is neither affected by extrapolation nor by hidden feature interactions"
  - [section] "NAMs allow for modeling a wide range of functional shapes, exploiting the property of ANNs to approximate general classes of functions arbitrarily well"
  - [corpus] Weak evidence - corpus papers discuss IML methods but don't specifically validate the NAM-orthogonalization combination
- Break condition: If the true underlying functions are too complex for the NAM architecture, approximation quality degrades significantly

### Mechanism 3
- Claim: The level-wise variance decomposition provides interpretable measures of main effect and interaction importance
- Mechanism: By computing I1 (main effects) and I2 (two-way interactions) as fractions of total variance explained, the method quantifies interpretability degree without requiring feature importance calculations
- Core assumption: Variance can be decomposed additively across effect levels when stacked orthogonality constraints are satisfied
- Evidence anchors:
  - [abstract] "the average values of the summary measures I1 and I2 were 0.511, 0.924, 0.983 and 0.412, 0.073, 0.017"
  - [section] "For each k ∈ Υ, we define the fraction of σ2F explained by the k-th level as Ik = R(Pθ′∈P(Υ):|θ′|=k fθ′(Xθ′))2dPX/σ2F"
  - [corpus] Weak evidence - corpus papers discuss interpretability but not specifically this variance decomposition approach
- Break condition: If higher-order interactions dominate the prediction function, interpretability measures become less meaningful

## Foundational Learning

- Concept: Hilbert space projection theorem
  - Why needed here: The method relies on projecting higher-order effects onto lower-order subspaces to achieve orthogonality
  - Quick check question: What property of Hilbert spaces ensures that orthogonal projections capture maximal variance in the target subspace?

- Concept: Functional ANOVA decomposition
  - Why needed here: The method builds on this concept but modifies it with stacked orthogonality instead of hierarchical orthogonality
  - Quick check question: How does the constraint set in Eq. 3 differ from traditional hierarchical orthogonality constraints?

- Concept: Neural network universal approximation
  - Why needed here: NAMs must approximate arbitrary subfunctions to recover true feature effects
  - Quick check question: What conditions must hold for a neural network to approximate any square integrable function?

## Architecture Onboarding

- Component map: Input data generation -> NAM fitting with separate ANNs per subfunction -> Post-hoc orthogonalization (iterative projection) -> Ensemble averaging -> Final orthogonalization
- Critical path: The orthogonalization steps are critical - if they fail, the decomposition loses interpretability guarantees
- Design tradeoffs: NAM complexity vs. computational efficiency, number of ensemble members vs. stability, effect selection vs. completeness
- Failure signatures: Poor correlation between original and decomposed predictions, non-orthogonal subfunctions, unstable estimates across ensemble members
- First 3 experiments:
  1. Apply method to a simple linear model with known coefficients to verify main effects are recovered exactly
  2. Test on a synthetic function with strong interactions to verify interactions are properly isolated
  3. Evaluate performance degradation as feature dimensionality increases to understand practical limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed stacked orthogonality method perform when decomposing prediction functions with higher-order interactions (beyond two-way interactions)?
- Basis in paper: [explicit] The paper mentions that the method was tested with synthetic data containing main effects, two-way interactions, and ten-way interactions, but only reports results for main effects and two-way interactions
- Why unresolved: The paper does not provide detailed analysis of how well the method recovers higher-order interactions or what performance metrics are achieved for these terms
- What evidence would resolve it: Experimental results showing the accuracy of recovering three-way, four-way, and higher-order interactions, along with corresponding explained variance coefficients and error metrics

### Open Question 2
- Question: What is the relationship between the number of ensemble members in the neural additive model and the accuracy of the decomposed subfunctions?
- Basis in paper: [explicit] The paper mentions using an ensemble approach with 10 members but does not explore how different ensemble sizes affect performance
- Why unresolved: The paper uses a fixed ensemble size of 10 without investigating whether this is optimal or how performance scales with ensemble size
- What evidence would resolve it: Systematic experiments varying the number of ensemble members (e.g., 1, 5, 10, 20, 50) and measuring the impact on subfunction accuracy and computational efficiency

### Open Question 3
- Question: How does the proposed method compare to other functional decomposition approaches (e.g., hierarchical orthogonality) in terms of computational efficiency and accuracy?
- Basis in paper: [inferred] The paper claims computational advantages over existing methods but does not provide direct comparisons or benchmark results
- Why unresolved: While the paper mentions that existing methods are "complex and computationally intensive," it does not demonstrate this through empirical comparisons
- What evidence would resolve it: Head-to-head comparisons with hierarchical orthogonality and other functional decomposition methods using the same synthetic datasets, measuring both accuracy and computational time

## Limitations
- Synthetic data validation only, lacking real-world dataset testing
- Performance on high-dimensional data with complex, non-smooth interactions remains untested
- Neural network sensitivity to hyperparameters not extensively explored

## Confidence

**High confidence**: The theoretical framework of stacked orthogonality and its mathematical formulation (Claims about the orthogonality mechanism)

**Medium confidence**: The NAM approximation quality and post-hoc orthogonalization procedure (Claims about efficient approximation without extrapolation)

**Low confidence**: Practical performance and robustness in real-world applications (Claims about interpretability measures and practical utility)

## Next Checks
1. Apply the method to benchmark tabular datasets (e.g., UCI datasets) with known feature interactions to verify performance on real-world data
2. Conduct sensitivity analysis by varying neural network architectures, ensemble sizes, and orthogonalization parameters to assess robustness
3. Compare computational efficiency against existing IML methods (e.g., SHAP, LIME) on datasets of increasing size and dimensionality