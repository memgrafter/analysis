---
ver: rpa2
title: Training Large Language Models to Reason in a Continuous Latent Space
arxiv_id: '2412.06769'
source_url: https://arxiv.org/abs/2412.06769
tags:
- reasoning
- language
- arxiv
- latent
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Coconut (Chain of Continuous Thought), a
  new paradigm for reasoning in large language models (LLMs) that moves beyond traditional
  language space constraints. Instead of generating reasoning chains in natural language,
  Coconut uses the last hidden state of the LLM as a continuous "thought" representation,
  which is fed back as the next input embedding.
---

# Training Large Language Models to Reason in a Continuous Latent Space

## Quick Facts
- arXiv ID: 2412.06769
- Source URL: https://arxiv.org/abs/2412.06769
- Reference count: 14
- Introduces Coconut, a continuous reasoning paradigm that improves accuracy while reducing token generation compared to Chain-of-Thought

## Executive Summary
This paper introduces Coconut (Chain of Continuous Thought), a novel approach to reasoning in large language models that moves beyond traditional language space constraints. Instead of generating reasoning chains in natural language, Coconut uses the last hidden state of the LLM as a continuous "thought" representation, which is fed back as the next input embedding. This enables reasoning in an unrestricted latent space, allowing the model to encode multiple alternative reasoning paths simultaneously and perform a breadth-first search-like exploration rather than committing to a single deterministic path.

The method demonstrates significant improvements on logical reasoning tasks, achieving 97.0% accuracy on ProsQA with 14.2 tokens per example (compared to 77.5% with 49.4 tokens for CoT) and 99.8% accuracy on ProntoQA with 9.0 tokens (matching CoT's performance). For GSM8k math reasoning, Coconut achieves 34.1% accuracy with 8.2 tokens, showing a better trade-off between accuracy and efficiency compared to language-based approaches.

## Method Summary
Coconut replaces language-based Chain-of-Thought reasoning with continuous latent representations. The model switches between language mode (standard token generation) and latent mode (using the last hidden state as input embedding) using special tokens <bot> and <eot>. A multi-stage curriculum training approach gradually increases the use of continuous thoughts, starting from few-shot CoT and progressing to fully latent reasoning. The model is trained on GPT-2 with a learning rate of 1e-4 and batch size 128, evaluating on GSM8k, ProntoQA, and a new ProsQA dataset.

## Key Results
- ProsQA: 97.0% accuracy with 14.2 tokens (vs. 77.5% with 49.4 tokens for CoT)
- ProntoQA: 99.8% accuracy with 9.0 tokens (matching CoT's performance)
- GSM8k: 34.1% accuracy with 8.2 tokens (better efficiency than CoT's 42.9% with more tokens)
- Continuous thoughts enable breadth-first search-like reasoning with multiple candidate paths

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Continuous thoughts enable breadth-first search-like reasoning by encoding multiple alternative next steps simultaneously.
- **Mechanism**: Instead of committing to a single deterministic path as in language-based CoT, the continuous representation can maintain diverse reasoning options. The model progressively eliminates incorrect paths through iterative reasoning, guided by implicit value functions that assess the potential of each node to lead to the correct answer.
- **Core assumption**: The continuous space allows the model to represent and maintain multiple reasoning trajectories without needing to commit to any single path prematurely.
- **Evidence anchors**: [abstract], [section], weak corpus support

### Mechanism 2
- **Claim**: Latent reasoning improves accuracy on planning-intensive tasks by deferring decisions on difficult early choices.
- **Mechanism**: Nodes explored early in reasoning are farther from target nodes and harder to evaluate accurately. By maintaining multiple candidates and postponing commitment, the model can refine its decisions as it gets closer to potential targets where evaluation is more reliable.
- **Core assumption**: The difficulty of evaluating reasoning choices is inversely related to their distance from the final target nodes.
- **Evidence anchors**: [section], [corpus] weak support

### Mechanism 3
- **Claim**: Continuous thoughts are more efficient representations of reasoning than language tokens.
- **Mechanism**: Language tokens are primarily for fluency and coherence, with only some critical tokens essential for reasoning. Continuous thoughts can directly represent the reasoning state without the overhead of generating fluency tokens, achieving better accuracy-efficiency trade-offs.
- **Core assumption**: Most language tokens in reasoning chains contribute little to actual reasoning and can be eliminated without loss of reasoning capability.
- **Evidence anchors**: [abstract], [section], moderate corpus support

## Foundational Learning

- **Concept**: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how hidden states flow through the model and how continuous thoughts are generated and processed
  - Quick check question: How does the last hidden state of a transformer differ from the input embedding in terms of what information it contains?

- **Concept**: Chain-of-Thought (CoT) reasoning
  - Why needed here: Coconut builds on CoT concepts but operates in continuous space instead of language space
  - Quick check question: What are the key limitations of language-based CoT that Coconut aims to address?

- **Concept**: Reinforcement learning and value functions
  - Why needed here: The implicit value functions that guide the breadth-first search-like behavior in continuous space
  - Quick check question: How might an implicit value function in latent space differ from explicit reward signals in traditional RL?

## Architecture Onboarding

- **Component map**: Input question -> Language mode (token generation) -> <bot> token -> Latent mode (hidden state feedback) -> <eot> token -> Language mode (final answer)

- **Critical path**: 1. Input question → language mode (standard token generation) 2. <bot> token → switch to latent mode 3. Feed last hidden state as next input embedding 4. Continue until <eot> token or classifier decision 5. Switch back to language mode for final answer generation

- **Design tradeoffs**:
  - Efficiency vs. interpretability: Continuous thoughts are efficient but less interpretable than language reasoning chains
  - Stability vs. flexibility: Multi-stage training provides stability but may limit the model's ability to discover optimal continuous representations
  - Parallelism vs. sequential processing: Sequential nature of multiple forward passes limits training efficiency

- **Failure signatures**:
  - Training instability during stage transitions (sharp spikes in training loss)
  - Collapse of continuous representations to degenerate solutions
  - Inability to maintain diverse reasoning paths (loss of BFS-like behavior)
  - Poor performance on planning-intensive tasks despite successful training

- **First 3 experiments**:
  1. Implement basic Coconut with GPT-2 on a simple logical reasoning task, comparing accuracy and token efficiency against standard CoT
  2. Test the breadth-first search hypothesis by probing latent thoughts to visualize the distribution of candidate next steps
  3. Experiment with different numbers of continuous thoughts per language reasoning step (c hyperparameter) to find optimal efficiency-accuracy trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Coconut achieve better performance than language-based CoT on GSM8k if combined with more advanced latent reasoning training techniques?
- Basis in paper: [explicit] The paper mentions that future work could explore combining iCoT's finer-grained training schedule with Coconut, and that the current gap in GSM8k performance suggests this direction.
- Why unresolved: The current implementation shows Coconut achieves 34.1% accuracy on GSM8k compared to 42.9% for CoT, indicating room for improvement. The paper suggests this gap might be closed by better training strategies.
- What evidence would resolve it: Experiments comparing Coconut variants with iCoT-style incremental training schedules on GSM8k, showing whether the combined approach can surpass CoT's 42.9% accuracy while maintaining efficiency benefits.

### Open Question 2
- Question: Does the breadth-first search-like reasoning pattern in Coconut generalize to other reasoning tasks beyond logical reasoning, such as commonsense reasoning or multi-step planning?
- Basis in paper: [explicit] The paper shows Coconut performs BFS-like search on ProsQA but notes this is a fundamental mechanism underlying consistent improvement, suggesting it may extend to other domains.
- Why unresolved: The analysis of parallel reasoning paths and deferred decision-making is only demonstrated on logical reasoning tasks. The paper suggests this could be a general property but doesn't test it on other reasoning domains.
- What evidence would resolve it: Experiments applying Coconut to diverse reasoning benchmarks (e.g., commonsense reasoning, multi-step planning) with analysis of whether the model maintains parallel exploration patterns and shows similar performance improvements over CoT.

### Open Question 3
- Question: Can latent reasoning be effectively learned without supervision from language reasoning chains, using only question-answer pairs?
- Basis in paper: [explicit] The paper notes that Coconut without curriculum (directly training in latent space) performs no better than no-CoT, indicating current methods require language chain supervision.
- Why unresolved: The paper concludes that LLMs still need guidance to learn latent reasoning, but suggests this is an important direction for future research. The current multi-stage training relies on language CoT data.
- What evidence would resolve it: Experiments training Coconut variants from scratch using only QA pairs, potentially combined with reinforcement learning or unsupervised objectives, to determine if latent reasoning can emerge without language chain supervision.

## Limitations

- Multi-stage training curriculum lacks theoretical grounding for why specific stage progressions work better than alternatives
- Breadth-first search mechanism is described qualitatively but not quantitatively measured
- Focus on logical reasoning tasks may not translate to more complex domains requiring world knowledge
- Trade-off between efficiency and interpretability not deeply explored

## Confidence

**High Confidence**: Claims about improved token efficiency and accuracy trade-offs are well-supported by experimental results with clear metrics.

**Medium Confidence**: The BFS-like reasoning mechanism and height-based evaluation hypothesis are supported by qualitative analysis but lack rigorous quantitative validation.

**Low Confidence**: Claims about superiority for all reasoning tasks are premature, as evaluation is limited to logical and math reasoning domains.

## Next Checks

1. **Quantitative BFS validation**: Implement visualization and measurement framework to directly probe continuous thoughts during inference, counting distinct candidate paths maintained at each reasoning step and measuring path diversity over time. Compare against single-path commitment in traditional CoT.

2. **Cross-domain generalization**: Test Coconut on commonsense reasoning benchmarks (CommonsenseQA, StrategyQA) and open-domain question answering (NaturalQuestions) to assess whether efficiency gains and reasoning improvements generalize beyond logical reasoning tasks.

3. **Ablation of training stages**: Conduct ablation study removing or reordering multi-stage training curriculum to determine which stages are essential for performance and whether progression is optimal. Test whether simpler training procedure could achieve similar results.