---
ver: rpa2
title: Developing Acoustic Models for Automatic Speech Recognition in Swedish
arxiv_id: '2404.16547'
source_url: https://arxiv.org/abs/2404.16547
tags:
- speakers
- been
- speech
- number
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops acoustic models for Swedish automatic speech
  recognition using hidden Markov models trained on the SpeechDat database. The work
  explores context-independent and context-dependent phone models with bigram language
  models, evaluating on digits and natural number recognition.
---

# Developing Acoustic Models for Automatic Speech Recognition in Swedish

## Quick Facts
- arXiv ID: 2404.16547
- Source URL: https://arxiv.org/abs/2404.16547
- Reference count: 10
- This paper develops acoustic models for Swedish automatic speech recognition using hidden Markov models trained on the SpeechDat database. The work explores context-independent and context-dependent phone models with bigram language models, evaluating on digits and natural number recognition. Results show 88.6% accuracy with within-word triphone models with 8 Gaussian mixtures, outperforming prior studies. The models generalize well across different speaker groups and dialects.

## Executive Summary
This paper presents the development of acoustic models for Swedish automatic speech recognition using Hidden Markov Models (HMMs) trained on the SpeechDat database. The research systematically explores context-independent and context-dependent phone models, comparing monophones, within-word triphones, and cross-word triphones across different Gaussian mixture configurations. The study achieves 88.6% accuracy with within-word triphone models using 8 Gaussian mixtures per state, outperforming previous Swedish ASR systems. The models demonstrate strong generalization across different speaker demographics and dialect regions.

## Method Summary
The study employs HMMs trained on the SpeechDat database containing recordings from 1000 Swedish speakers. The method involves extracting acoustic features from telephone-quality recordings (8kHz, 8-bit A-law format), training monophone and triphone models using the Baum-Welch algorithm, and evaluating performance with bigram language models. The research systematically varies the number of Gaussian mixtures per state (1, 2, 4, and 8) and compares three model architectures: context-independent monophones, within-word triphones, and cross-word triphones. Tree clustering is applied to address data sparsity in triphone modeling, with thresholds optimized to maintain 8000 models for monophones and 10000 models for triphones.

## Key Results
- Achieved 88.6% word accuracy with within-word triphone models using 8 Gaussian mixtures per state
- Within-word triphones outperform both monophones and cross-word triphones for this vocabulary
- Excluding retroflex allophones from the lexicon improves overall accuracy for the limited vocabulary
- Models demonstrate consistent performance across different speaker groups and dialect regions

## Why This Works (Mechanism)

### Mechanism 1
Hidden Markov Models (HMMs) with within-word triphone modeling outperform monophone and cross-word triphone models for Swedish ASR because within-word triphones capture local phonetic context while keeping model size manageable, avoiding data sparsity that plagues cross-word triphones. This balance allows better generalization across speakers and dialects.

### Mechanism 2
Increasing Gaussian mixtures per state improves accuracy up to a point, but with diminishing returns for context-dependent models because more Gaussian mixtures allow better modeling of acoustic variability within each state, but excessive mixtures overfit the training data and reduce generalization.

### Mechanism 3
Excluding retroflex allophones from the lexicon improves overall accuracy for this specific vocabulary because the limited vocabulary contains few words with retroflex phonemes, so splitting models into retroflex and non-retroflex versions reduces training data for the more frequently used non-retroflex models.

## Foundational Learning

- Concept: Hidden Markov Models and Baum-Welch training
  - Why needed here: The entire acoustic modeling approach relies on HMMs trained with the Baum-Welch algorithm to estimate state emission probabilities.
  - Quick check question: What is the difference between the forward-backward algorithm and the Viterbi algorithm in HMM training?

- Concept: Triphone modeling and context expansion
  - Why needed here: The paper compares context-independent monophones with context-dependent triphones using within-word and cross-word expansion strategies.
  - Quick check question: How does tree clustering help address data sparsity in triphone modeling?

- Concept: Gaussian mixture models for state emission probabilities
  - Why needed here: The paper systematically varies the number of Gaussian mixtures per state to optimize accuracy.
  - Quick check question: What is the trade-off between model complexity (more Gaussians) and overfitting when training data is limited?

## Architecture Onboarding

- Component map:
  SpeechDat database -> HTK toolkit -> Monophone models -> Triphone models (within-word/cross-word) -> Tree clustering -> Gaussian mixture optimization -> Evaluation

- Critical path:
  1. Extract features from SpeechDat recordings
  2. Train monophone models with Baum-Welch
  3. Generate triphone models with context expansion
  4. Apply tree clustering to address data sparsity
  5. Train final models with optimal Gaussian mixtures
  6. Evaluate on development and evaluation subsets

- Design tradeoffs:
  - Model complexity vs. data sparsity: More complex models (cross-word triphones, more Gaussians) require more training data
  - Lexicon completeness vs. accuracy: Including rare phonemes (retroflex allophones) may reduce accuracy for common phonemes
  - Within-word vs. cross-word context: Within-word captures local context with fewer models; cross-word captures more context but suffers from data sparsity

- Failure signatures:
  - Overfitting: Accuracy on development set plateaus or decreases with additional training iterations or Gaussian mixtures
  - Data sparsity: Tree clustering merges contexts aggressively, indicating insufficient training data for certain triphone contexts
  - Poor generalization: Significant accuracy drop when testing on different speaker groups or dialects

- First 3 experiments:
  1. Train monophone models with 1, 2, 4, and 8 Gaussian mixtures; evaluate on development set to find optimal mixture count
  2. Generate within-word and cross-word triphone models; apply tree clustering with different thresholds; evaluate to find optimal model size
  3. Test models with and without retroflex allophones on evaluation set to confirm impact on accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would using more than eight Gaussian mixtures per state affect the accuracy of monophone and triphone models?
- Basis in paper: The paper states that "monophone accuracy rises when the number of mixture terms is increased from four to eight" and suggests that "probably better results can be obtained if the number of mixture terms is further increased." For triphones, it notes that "the increase of accuracy from four terms models to eight terms models is quite low."
- Why unresolved: The experiments conducted only tested up to eight Gaussian mixtures, leaving the impact of using more mixtures unexplored.
- What evidence would resolve it: Conducting experiments with a higher number of Gaussian mixtures (e.g., 16 or 32) and comparing the accuracy improvements, if any, over the existing models.

### Open Question 2
- Question: What is the effect of using separate acoustic models for male and female speakers on recognition accuracy?
- Basis in paper: The paper notes that "Results seem to be sex independent, even though female speakers are better recognized," suggesting potential differences in recognition performance based on speaker sex.
- Why unresolved: The study used a single set of models for all speakers, without exploring sex-specific models that might improve recognition for underrepresented groups.
- What evidence would resolve it: Training and testing separate acoustic models for male and female speakers and comparing their performance to the current mixed-gender models.

### Open Question 3
- Question: How would the inclusion of more diverse dialect regions in the evaluation subset affect the per-dialect recognition accuracy?
- Basis in paper: The paper highlights that "no Finnish speakers nor speakers from Gotland are in this subset" and that "speakers from Bergslagen do not represent a good statistical base."
- Why unresolved: The evaluation subset lacks representation from certain dialect regions, potentially biasing the results and not fully capturing the model's performance across all Swedish dialects.
- What evidence would resolve it: Expanding the evaluation subset to include a more balanced representation of all dialect regions and analyzing the impact on per-dialect recognition accuracy.

### Open Question 4
- Question: What is the impact of real-time noise reduction techniques on the accuracy of acoustic models in the presence of stationary noise?
- Basis in paper: The paper mentions that "stationary noise that affects many files in the database" is a problem and suggests that "subtracting the mean energy over each utterance to the mel-cepstral coefficients" could help, but notes this is not feasible for real-time applications.
- Why unresolved: The study did not implement real-time noise reduction techniques, leaving the potential benefits for accuracy in noisy conditions unexplored.
- What evidence would resolve it: Implementing and testing real-time noise reduction techniques on the acoustic models and measuring the improvements in accuracy for utterances affected by stationary noise.

## Limitations
- The study is limited to a restricted vocabulary of digits and natural numbers, which may not generalize to unconstrained conversational speech
- The research predates modern deep learning approaches, so no comparison with contemporary neural network acoustic models is available
- The evaluation lacks spontaneous speech scenarios, focusing only on read speech from the SpeechDat database

## Confidence
- High confidence: The general superiority of within-word triphone models over monophones for this vocabulary size
- Medium confidence: The optimal number of Gaussian mixtures (8) for state emission modeling
- Medium confidence: The negative impact of including retroflex allophones for this specific vocabulary
- Low confidence: Generalization of findings to spontaneous speech or larger vocabularies

## Next Checks
1. Test the optimal model configurations (within-word triphones with 8 Gaussian mixtures) on a contemporary Swedish speech corpus with spontaneous speech to assess real-world performance
2. Replicate the study using modern deep learning acoustic models (TDNN, CNN, or transformer-based) to benchmark against the HMM results
3. Evaluate model robustness across different recording conditions (e.g., different microphones, noise levels, and speaking styles) to assess practical deployment viability