---
ver: rpa2
title: 'AquilaMoE: Efficient Training for MoE Models with Scale-Up and Scale-Out Strategies'
arxiv_id: '2408.06567'
source_url: https://arxiv.org/abs/2408.06567
tags:
- training
- aquilamoe
- scale-up
- scale-out
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents AquilaMoE, an 816B bilingual Mixture of Experts
  (MoE) language model trained using the EfficientScale methodology. The EfficientScale
  approach consists of two stages: Scale-Up, which initializes a larger model using
  weights from a pre-trained smaller model (using Function Preserving Initialization
  and Advanced Knowledge Initialization-Pro techniques), and Scale-Out, which initializes
  MoE experts using a pre-trained dense model.'
---

# AquilaMoE: Efficient Training for MoE Models with Scale-Up and Scale-Out Strategies

## Quick Facts
- arXiv ID: 2408.06567
- Source URL: https://arxiv.org/abs/2408.06567
- Reference count: 18
- AquilaMoE achieves 3.35x computational savings and 4.12x time savings compared to training from scratch

## Executive Summary
AquilaMoE presents an 8*16B bilingual Mixture of Experts (MoE) language model trained using the EfficientScale methodology. The approach consists of two stages: Scale-Up, which initializes a larger model using weights from a pre-trained smaller model, and Scale-Out, which initializes MoE experts using a pre-trained dense model. This method significantly reduces data and computational requirements while maintaining and improving model performance, achieving better results than smaller dense models on various tasks.

## Method Summary
The EfficientScale methodology employs a two-stage approach for training large MoE models efficiently. The first stage, Scale-Up, uses the AKI-Pro initialization technique to transfer knowledge from a pre-trained 7B dense model to a larger 16B dense model. The second stage, Scale-Out, applies Sparse Upcycling to convert the dense model into an 8*16B MoE configuration by replicating each MLP layer as an MoE expert. The entire pipeline uses continuous pretraining on a 4TB bilingual dataset with careful monitoring of validation losses to prevent training collapse.

## Key Results
- 8*16B AquilaMoE achieved better performance than smaller dense models on ARC, Hellaswag, GSM8K, HumanEval, MMLU, and Winograd benchmarks
- The EfficientScale method provided 3.35x computational power savings and 4.12x time savings compared to training from scratch
- AKI-Pro initialization showed superior validation loss convergence compared to Function Preserving Initialization during the Scale-Up phase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scale-Up initialization using AKI-Pro enables efficient knowledge transfer from a smaller model to a larger model while maintaining performance.
- Mechanism: AKI-Pro breaks symmetry in weight initialization by using weights from both the same layer and the next layer, improving upon FPI which copies weights symmetrically. This allows the larger model to retain and build upon the knowledge of the smaller model during continuous pretraining.
- Core assumption: Weights from neighboring layers in neural networks have similar functionality, allowing knowledge transfer through weight transformation.
- Evidence anchors:
  - [abstract] "The first stage, termed Scale-Up, initializes the larger model with weights from a pre-trained smaller model, enabling substantial knowledge transfer and continuous pretraining with significantly less data."
  - [section] "AKI-Pro: Our proposed improvement on AKI further refines weight initialization from two aspects: depth growing method and GQA compatibility."
- Break condition: If the assumption about similar functionality between neighboring layers doesn't hold, the knowledge transfer would be ineffective and could lead to degraded performance.

### Mechanism 2
- Claim: Scale-Out initialization using Sparse Upcycling transforms a dense model into an MoE model while preserving knowledge and improving performance.
- Mechanism: Each MLP layer in the dense model is replaced by an MoE layer that is an exact replica of the original MLP layer, maintaining the learned knowledge. The router parameters are randomly initialized, and additional losses (load balancing and max z-loss) prevent training collapse.
- Core assumption: The knowledge encoded in dense MLP layers can be effectively preserved when converted to MoE layers.
- Evidence anchors:
  - [abstract] "The second stage, Scale-Out, uses a pre-trained dense model to initialize the MoE experts, further enhancing knowledge transfer and performance."
  - [section] "Aquila-MoE is initialized using Sparse Upcycling [10, 11]. The dense model checkpoint obtained from the Aquila dense model undergoes a transformation where each MLP layer is replaced by an MoE layer."
- Break condition: If the transformation from dense to MoE layers doesn't preserve knowledge effectively, the model performance could degrade despite the initialization.

### Mechanism 3
- Claim: The combination of Scale-Up and Scale-Out approaches significantly reduces computational requirements while maintaining or improving model performance.
- Mechanism: By initializing larger models from pre-trained smaller models (Scale-Up) and MoE models from pre-trained dense models (Scale-Out), the method reduces the amount of training data and computational resources needed compared to training from scratch. This is quantified by the 3.35x computational power savings and 4.12x time savings reported.
- Core assumption: Knowledge transfer through initialization is more efficient than learning from scratch.
- Evidence anchors:
  - [abstract] "The EfficientScale method providing a 3.35x computational power savings and 4.12x time savings compared to training from scratch."
  - [section] "Table 6: Training details for scale-up and scale-out and from-scratch approaches" with specific computational savings calculations.
- Break condition: If the initialization methods fail to effectively transfer knowledge, the computational savings would come at the cost of model performance.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Understanding how MoE works is crucial to grasping why the Scale-Out phase is effective. MoE models activate different subsets of parameters for different inputs, allowing for larger models without proportional increases in computational cost.
  - Quick check question: What is the primary advantage of MoE architectures over dense models in terms of computational efficiency?

- Concept: Weight initialization strategies (FPI, AKI, AKI-Pro)
  - Why needed here: These initialization methods are central to the Scale-Up phase and determine how effectively knowledge is transferred from smaller to larger models.
  - Quick check question: How does AKI-Pro improve upon the original AKI method in terms of depth growing and GQA compatibility?

- Concept: Continuous pretraining and knowledge transfer
  - Why needed here: The entire EfficientScale methodology relies on the concept that a model can continue learning effectively when initialized from a pre-trained model, rather than starting from scratch.
  - Quick check question: Why is continuous pretraining on the validation dataset essential in the EfficientScale pipeline?

## Architecture Onboarding

- Component map:
  - Preparation Phase: Small dense model training, dataset preparation, validation setup
  - Scale-Up Phase: Weight initialization (AKI-Pro), continuous pretraining of dense model
  - Scale-Out Phase: MoE weight initialization (Sparse Upcycling), continuous pretraining of MoE model
  - Evaluation: Foundation model evaluation, fine-tuned model evaluation, computational efficiency comparison

- Critical path:
  1. Prepare small dense model and datasets
  2. Scale-Up: Initialize larger dense model using AKI-Pro, continuous pretraining
  3. Scale-Out: Initialize MoE model using Sparse Upcycling, continuous pretraining
  4. Evaluate model performance and computational efficiency

- Design tradeoffs:
  - Model size vs. computational efficiency: Larger models require more resources but can achieve better performance
  - Knowledge transfer vs. learning capacity: Initializing from pre-trained models saves resources but may limit the model's ability to learn new information
  - MoE complexity vs. performance gain: MoE architectures are more complex but can achieve better performance with lower computational costs

- Failure signatures:
  - Training collapse: Indicated by increasing loss during continuous pretraining
  - Poor knowledge transfer: Smaller improvements in performance compared to training from scratch
  - Inefficient expert utilization: Imbalanced distribution of tokens across experts

- First 3 experiments:
  1. Scale-Up validation: Compare validation losses of 7B models initialized using FPI, AKI-Pro, and trained from scratch
  2. Scale-Out validation: Train 1.8B model from scratch, scale out to 8*1.8B, compare training loss convergence
  3. Full pipeline test: Train 16B model using Scale-Up, then 8*16B model using Scale-Out, evaluate performance on benchmark tasks

## Open Questions the Paper Calls Out
None

## Limitations

- The computational savings calculations are based on theoretical estimates rather than measured wall-clock times across different hardware configurations
- The method's effectiveness for language pairs beyond Chinese-English has not been demonstrated
- Limited ablation studies on the individual contributions of AKI-Pro and Sparse Upcycling to final performance

## Confidence

- **EfficientScale methodology reduces training costs** - Medium confidence
- **AKI-Pro provides superior knowledge transfer compared to FPI** - Medium confidence
- **Sparse Upcycling effectively preserves knowledge during MoE conversion** - Medium confidence

## Next Checks

1. **Ablation Study on Initialization Methods**: Train the 8*16B model using three different initialization strategies - (a) AKI-Pro + Sparse Upcycling, (b) FPI + Sparse Upcycling, and (c) Random initialization with auxiliary losses. Compare final performance to isolate the contribution of the initialization methods versus the training process itself.

2. **Hardware-Agnostic Computational Analysis**: Measure wall-clock training times across different GPU configurations (e.g., A100 vs H100, different interconnect fabrics) to validate the claimed computational savings under realistic conditions. Include both data-parallel and expert-parallel scaling scenarios.

3. **Expert Utilization and Load Balancing Analysis**: Track expert activation patterns throughout training to verify that the load balancing loss effectively prevents "expert collapse" (where only a subset of experts are used). Compare expert utilization distributions across different stages of the training pipeline.