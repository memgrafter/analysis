---
ver: rpa2
title: Non-Fluent Synthetic Target-Language Data Improve Neural Machine Translation
arxiv_id: '2401.16086'
source_url: https://arxiv.org/abs/2401.16086
tags:
- translation
- training
- data
- machine
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Non-fluent synthetic target-language data improve neural machine
  translation by generating implausible target sentences during training to force
  the model to rely more on encoder representations. Using a multilingual learning
  framework, this approach consistently outperforms state-of-the-art data augmentation
  methods in both low-resource and high-resource translation tasks.
---

# Non-Fluent Synthetic Target-Language Data Improve Neural Machine Translation

## Quick Facts
- arXiv ID: 2401.16086
- Source URL: https://arxiv.org/abs/2401.16086
- Reference count: 40
- Non-fluent synthetic target-language data improves neural machine translation

## Executive Summary
This paper introduces MaTiLDA, a data augmentation method that improves neural machine translation by generating implausible target sentences during training. Unlike traditional data augmentation that extends the data distribution, MaTiLDA creates synthetic samples with non-fluent targets to force the decoder to rely more on encoder representations. Using a multi-task learning framework with task-specific tokens, the method consistently outperforms state-of-the-art augmentation techniques across both low-resource and high-resource translation tasks.

## Method Summary
MaTiLDA generates synthetic training samples by applying transformations (swap, unk, source, reverse, mono, replace) to target sentences, making them implausible. These samples are used in a multi-task learning framework where task-specific artificial tokens indicate whether a sample is original or synthetic. The model learns to handle both fluent and non-fluent generation modes without negative transfer. Training combines original and synthetic samples, followed by fine-tuning on original data. The approach leverages standard Transformer architectures with modified training procedures to incorporate the non-fluent targets effectively.

## Key Results
- Improves translation quality by 1-4 BLEU points on average compared to baseline systems
- Enhances robustness to domain shift and reduces hallucinations in translations
- Source influence on model predictions increases significantly compared to baseline systems
- Consistently outperforms state-of-the-art data augmentation methods in both low-resource and high-resource scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-fluent synthetic target sentences force the decoder to rely more on encoder representations
- Mechanism: When target sentences are implausible (e.g., word-swapped, reversed), the decoder cannot predict well from just the target prefix, so it must attend more to source-side encoder outputs
- Core assumption: The decoder normally relies too heavily on target-side context, especially with limited data
- Evidence anchors:
  - [abstract] "synthetic training samples with non-fluent target sentences can improve translation performance if they are used in a multilingual machine translation framework as if they were sentences in another language"
  - [section 3] "synthetic training samples that are deliberately implausible under the true data distribution... force the decoder to rely more on the encoder representations"
- Break condition: If transformations are too aggressive and cause the model to ignore the target context entirely, harming fluency

### Mechanism 2
- Claim: Multi-task learning framework prevents negative transfer from non-fluent targets
- Mechanism: Prepending artificial task tokens tells the model whether it's generating fluent or non-fluent output, allowing it to learn different generation modes without confusion
- Core assumption: The model can learn to condition its output style on the task token
- Evidence anchors:
  - [abstract] "synthetic training samples with non-fluent target sentences can improve translation performance if they are used in a multilingual machine translation framework as if they were sentences in another language"
  - [section 3] "we organize the original training data in mini-batches as if a vanilla NMT system were trained... add a token to each source sentence to indicate whether it is part of an original training sample or of synthetic one"
- Break condition: If task tokens are ignored or the model fails to differentiate output modes

### Mechanism 3
- Claim: Diverse transformations provide complementary training signals
- Mechanism: Different transformations (swap, reverse, replace) stress different aspects of the translation process, leading to more robust encoder reliance across various contexts
- Core assumption: The benefits of transformations are additive rather than redundant
- Evidence anchors:
  - [section 5.1] "training on synthetic training samples generated with the three best transformations (reverse+replace+swap) further improves the performance, achieving the best results in all translation tasks"
  - [section 3] "we explored the combination of the best performing ones"
- Break condition: If transformations overlap too much in effect or some transformations dominate others, reducing diversity benefits

## Foundational Learning

- Concept: Encoder-decoder attention mechanisms
  - Why needed here: Understanding how source and target representations interact is key to grasping why forcing decoder reliance on encoder improves robustness
  - Quick check question: What happens to decoder attention patterns when target context becomes unreliable?

- Concept: Data augmentation principles
  - Why needed here: The paper contrasts this approach with standard DA methods that aim to extend the support of the data distribution
  - Quick check question: How does this method's goal of creating implausible samples differ from traditional DA objectives?

- Concept: Multi-task learning frameworks
  - Why needed here: The approach uses task tokens to distinguish between fluent and non-fluent generation modes
  - Quick check question: How does prepending a task token change the model's behavior compared to a single-task setup?

## Architecture Onboarding

- Component map: Standard Transformer encoder-decoder with added task tokens at input; training loop modified to include synthetic samples
- Critical path: Data → Transformation → Task-token prepending → Training with mixed original/synthetic batches → Evaluation
- Design tradeoffs: Aggressive transformations improve robustness but risk harming fluency; task tokens prevent negative transfer but add complexity
- Failure signatures: Poor BLEU scores despite increased source influence; model ignoring task tokens; overfitting to synthetic patterns
- First 3 experiments:
  1. Implement swap transformation and compare baseline vs single-transformation performance
  2. Add reverse transformation and test combined effect
  3. Implement full multi-task framework with task tokens and test on low-resource language pair

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MaTiLDA perform when integrated with large pre-trained multilingual NMT models like NLLB, which already leverage extensive monolingual and parallel data?
- Basis in paper: [explicit] The authors acknowledge this as a direction for future work, noting that NLLB outperforms their baseline systems by 7.4-10.4 BLEU points on average in low-resource experiments.
- Why unresolved: The paper only evaluates MaTiLDA on standard NMT systems trained from scratch, not on systems initialized with large pre-trained models.
- What evidence would resolve it: Experiments comparing MaTiLDA-augmented fine-tuning of NLLB (or similar models) against standard fine-tuning, measuring both translation quality and source influence metrics.

### Open Question 2
- Question: What is the optimal combination of MaTiLDA transformations for different language pairs and resource scenarios?
- Basis in paper: [explicit] The authors found that different transformations work best for different language pairs (e.g., swap works best for hsb-de while reverse+replace works best for most others), and that combining transformations can yield further improvements.
- Why unresolved: The paper only explores a limited set of transformations and combinations, and doesn't provide a systematic method for determining optimal combinations for new language pairs or resource levels.
- What evidence would resolve it: A comprehensive study mapping transformation combinations to language pairs and resource levels, potentially using automated methods to discover optimal configurations.

### Open Question 3
- Question: How does MaTiLDA's effect on source influence and hallucination reduction vary across different domains and test sets?
- Basis in paper: [explicit] The authors show that MaTiLDA increases source influence and reduces hallucinations in general, but don't provide detailed analysis of how these effects vary across specific domains or test sets.
- Why unresolved: The paper only reports average effects across all test sets, without examining whether the benefits of MaTiLDA are consistent across different types of text (e.g., technical vs. conversational).
- What evidence would resolve it: Detailed analysis of source influence and hallucination metrics broken down by domain, genre, and test set characteristics, potentially revealing scenarios where MaTiLDA is most or least effective.

## Limitations
- Evaluation primarily relies on BLEU scores without comprehensive human evaluation to assess fluency degradation
- Computational overhead of generating and processing synthetic data at scale is not thoroughly analyzed
- Method's effectiveness shows variation across different language pairs, with some benefiting more than others

## Confidence

- **High Confidence**: The claim that non-fluent synthetic target sentences can improve translation quality in a multi-task learning framework is well-supported by experimental results showing consistent BLEU improvements across multiple language pairs and data augmentation scenarios.

- **Medium Confidence**: The assertion that the method improves robustness to domain shift and reduces hallucinations is supported by evidence, but the analysis could be more comprehensive. The connection between increased source influence and these robustness benefits needs stronger empirical validation.

- **Low Confidence**: The claim that diverse transformations provide additive benefits lacks strong quantitative evidence. While the paper suggests combining transformations improves results, the relative contributions of individual transformations and their interactions are not thoroughly analyzed.

## Next Checks

1. **Human Evaluation Study**: Conduct a comprehensive human evaluation to assess translation fluency, adequacy, and naturalness, particularly focusing on samples generated using synthetic data. Compare human judgments against BLEU scores to validate that quality improvements don't come at the cost of fluency.

2. **Transformation Impact Analysis**: Perform ablation studies to quantify the individual and combined effects of each transformation type. Measure how different transformation parameters (α values) affect translation quality and source influence across various language pairs and resource levels.

3. **Long-term Training Stability**: Evaluate model performance during extended training periods to assess potential overfitting to synthetic patterns. Monitor source influence metrics and hallucination rates throughout training to ensure the benefits remain stable and don't degrade over time.