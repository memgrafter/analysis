---
ver: rpa2
title: 'HashEvict: A Pre-Attention KV Cache Eviction Strategy using Locality-Sensitive
  Hashing'
arxiv_id: '2412.16187'
source_url: https://arxiv.org/abs/2412.16187
tags:
- hash
- cache
- attention
- evict
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HashEvict, a novel pre-attention KV cache
  eviction strategy for transformer-based language models that leverages locality-sensitive
  hashing (LSH) to efficiently compress memory usage during inference. The core innovation
  lies in using binary hash codes and Hamming distance calculations to identify tokens
  with low cosine similarity to the current query, enabling dynamic eviction decisions
  without computing attention scores.
---

# HashEvict: A Pre-Attention KV Cache Eviction Strategy using Locality-Sensitive Hashing

## Quick Facts
- arXiv ID: 2412.16187
- Source URL: https://arxiv.org/abs/2412.16187
- Reference count: 40
- Achieves 30%-70% KV cache compression with minimal performance degradation

## Executive Summary
HashEvict introduces a novel pre-attention KV cache eviction strategy for transformer-based language models that leverages locality-sensitive hashing (LSH) to compress memory usage during inference. The method uses binary hash codes and Hamming distance calculations to identify tokens with low cosine similarity to current queries, enabling dynamic eviction decisions without computing attention scores. This approach achieves significant compression ratios while maintaining high performance across diverse tasks including reasoning, multiple-choice questions, long-context retrieval, and summarization.

## Method Summary
The core innovation of HashEvict lies in its use of locality-sensitive hashing to perform pre-attention eviction decisions. By computing binary hash codes for each token in the KV cache and calculating Hamming distances between these codes and the current query representation, the method can identify and evict tokens that are least relevant to the current context. This is achieved through a two-stage process: first, LSH is applied to generate compact binary representations of token embeddings, and second, Hamming distances are computed to rank tokens for potential eviction. The method claims to operate independently of sequence length with constant memory overhead, making it particularly suitable for resource-constrained deployment scenarios.

## Key Results
- Achieves 30%-70% compression of KV cache while maintaining high performance
- Outperforms attention-free L2 norm-based methods and attention-accumulation-based methods on several benchmarks
- Demonstrates 1.5x prefill speedup and competitive decoding speed compared to state-of-the-art compression techniques

## Why This Works (Mechanism)
HashEvict exploits the principle that tokens with similar semantic content will have similar hash codes under LSH, while dissimilar tokens will have different hash codes. By computing Hamming distances between the query's hash code and all token hash codes, the method can efficiently identify tokens that are semantically distant from the current context without computing expensive attention scores. This pre-filtering approach reduces the effective size of the KV cache before attention computation, leading to faster inference and lower memory usage. The binary nature of hash codes enables extremely fast distance calculations, making the eviction decision computationally negligible compared to the full attention computation.

## Foundational Learning

**Locality-Sensitive Hashing (LSH)**
- Why needed: Enables efficient approximate nearest neighbor search in high-dimensional spaces
- Quick check: Understand how hash collisions correlate with cosine similarity between vectors

**Hamming Distance**
- Why needed: Provides a computationally efficient metric for comparing binary hash codes
- Quick check: Verify that Hamming distance calculations can be performed using bitwise operations

**KV Cache Compression**
- Why needed: Reduces memory footprint during transformer inference without sacrificing performance
- Quick check: Understand how cache size affects memory bandwidth and inference speed

## Architecture Onboarding

**Component Map**
Input Sequence -> Token Embeddings -> LSH Hashing -> Hamming Distance Ranking -> Eviction Decision -> Reduced KV Cache -> Attention Computation

**Critical Path**
The critical computational path involves LSH hash code generation and Hamming distance calculation, both of which are linear in the number of tokens and independent of sequence length for the hashing operation itself.

**Design Tradeoffs**
The method trades off some precision in eviction decisions (due to the approximate nature of LSH) for significant gains in computational efficiency. The choice of LSH parameters (number of hash functions, hash code length) represents a key design decision that affects both compression ratio and performance retention.

**Failure Signatures**
Potential failure modes include: over-aggressive eviction leading to context loss, hash collisions causing incorrect eviction decisions, and parameter sensitivity where suboptimal LSH parameters result in poor compression-performance tradeoffs.

**3 First Experiments**
1. Ablation study on LSH parameters (hash code length, number of hash functions) to identify optimal configuration
2. Comparison against exact nearest neighbor eviction using cosine similarity as ground truth
3. Stress test with pathological input sequences (repetitive patterns, long-range dependencies) to evaluate robustness

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation limited to specific model architectures (LLaMA-2-7B and Mistral-7B) and benchmark tasks
- Compression ratios beyond 70% may degrade performance, but degradation curve is not systematically explored
- LSH implementation details and parameter choices lack thorough justification or comparison against alternatives

## Confidence

**Performance Claims**: Medium Confidence - Based on controlled benchmarks but lacks comparison against all major contemporary methods and large-scale model testing

**Computational Efficiency Claims**: High Confidence - Theoretically sound with rigorous asymptotic complexity analysis and supporting memory profiling data

**Task Generalization Claims**: Low Confidence - Demonstrated on relatively narrow task selection; effectiveness on specialized domains and non-English languages not addressed

## Next Checks

1. **Cross-Model Scaling Study**: Evaluate HashEvict on models ranging from 1B to 70B parameters, including decoder-only and encoder-decoder architectures, to establish scaling relationships and identify potential failure modes.

2. **Adversarial Input Robustness**: Test the eviction strategy on inputs designed to trigger pathological behavior - repetitive patterns, semantically ambiguous contexts, or sequences with high lexical overlap but different meanings.

3. **End-to-End Deployment Benchmarking**: Implement HashEvict in a production inference serving system and measure real-world performance including GPU memory utilization, throughput under concurrent requests, and impact on tail latency.