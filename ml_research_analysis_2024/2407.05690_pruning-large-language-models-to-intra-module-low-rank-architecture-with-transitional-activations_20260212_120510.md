---
ver: rpa2
title: Pruning Large Language Models to Intra-module Low-rank Architecture with Transitional
  Activations
arxiv_id: '2407.05690'
source_url: https://arxiv.org/abs/2407.05690
tags:
- pruning
- pruned
- transact
- language
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TransAct, a task-agnostic structured pruning
  approach for large language models (LLMs). The method reduces transitional activations
  inside multi-head attention and multi-layer perceptron modules while preserving
  inter-module activations sensitive to perturbations.
---

# Pruning Large Language Models to Intra-module Low-rank Architecture with Transitional Activations

## Quick Facts
- arXiv ID: 2407.05690
- Source URL: https://arxiv.org/abs/2407.05690
- Authors: Bowen Shen; Zheng Lin; Daren Zha; Wei Liu; Jian Luan; Bin Wang; Weiping Wang
- Reference count: 10
- Primary result: TransAct achieves 20% additional FLOPs savings compared to state-of-the-art methods while maintaining competitive performance on downstream benchmarks

## Executive Summary
This paper presents TransAct, a task-agnostic structured pruning approach for large language models (LLMs) that reduces transitional activations inside multi-head attention (MHA) and multi-layer perceptron (MLP) modules. The method preserves inter-module activations sensitive to perturbations, resulting in an intra-module low-rank architecture that significantly reduces weights, KV cache size, and attention computation. Experiments on LLaMA models demonstrate that TransAct outperforms state-of-the-art pruning methods, particularly at high compression ratios, while maintaining competitive performance on downstream benchmarks and enabling faster inference on both edge devices and servers.

## Method Summary
TransAct employs iterative activation-guided pruning that targets transitional activations within MHA and MLP modules of Transformer-based LLMs. The approach calculates salience scores based on activation magnitudes to identify dimensions with low importance, then prunes these dimensions while preserving inter-module activations that are sensitive to perturbations. The method uses 128 calibration samples for salience calculation and performs post-training fine-tuning with 200M tokens between pruning iterations. Experiments were conducted on LLaMA2-7B-base models using the RedPajama-V1 corpus, with evaluation on 10 downstream benchmarks from the Huggingface leaderboard covering both zero-shot and few-shot tasks.

## Key Results
- TransAct achieves 20% additional FLOPs savings compared to state-of-the-art methods while maintaining competitive downstream performance
- Pruned models demonstrate significant reduction in KV cache size and attention computation, enabling faster inference
- At high compression ratios, TransAct maintains perplexity under 4.0 for 2.0B and 4.0B models, outperforming baselines
- Ablation studies reveal MHA modules have less redundancy than MLP modules, with balanced compression ratios yielding optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning transitional activations inside MHA and MLP modules reduces model parameters and KV cache size while preserving inter-module activations that are sensitive to perturbations.
- Mechanism: The approach identifies transitional activations as outputs of attention heads and MLP intermediate states, then prunes dimensions with low salience based on activation magnitudes while keeping sensitive inter-module dimensions intact.
- Core assumption: Intra-module activations have higher redundancy and lower sensitivity to perturbations compared to inter-module activations.
- Evidence anchors:
  - [abstract]: "TransAct reduces transitional activations inside multi-head attention (MHA) and multi-layer perceptron (MLP) modules, while preserving the inter-module activations that are sensitive to perturbations."
  - [section]: "We define actAl as the transitional activation of MHA module...we define actl P as the transitional activation of the MLP module"
  - [corpus]: Weak - corpus papers focus on low-rank compression and feature distillation but don't directly discuss transitional activation pruning.

### Mechanism 2
- Claim: Iterative pruning with activation-based metrics is more effective than single-shot pruning with Taylor expansion-based metrics for LLMs.
- Mechanism: Gradual pruning allows the model to recover from perturbations at each step, avoiding large approximation errors that occur when pruning large ratios in one shot.
- Core assumption: Taylor expansion-based metrics lose accuracy when pruning large ratios due to the assumption of small perturbations being violated.
- Evidence anchors:
  - [section]: "Jaiswal et al. (2023) suggest that state-of-the-art (SOTA) unstructured pruning approaches...often underperform in downstream benchmarks. Zimmer et al. (2023) emphasize the significance of post-training after pruning to restore the capabilities of the LLM."
  - [section]: "LLM-Pruner (Ma et al., 2023b), the pioneering structured pruning of LLM, incorporates the approximated Taylor series as the pruning metric. However, this approximation loses accuracy when pruning a large ratio of the model."
  - [corpus]: Weak - corpus papers discuss iterative approaches but don't specifically compare Taylor expansion vs activation-based metrics.

### Mechanism 3
- Claim: MHA modules have less redundancy than MLP modules, and uniform compression ratios across both modules yield better performance than extreme pruning of either.
- Mechanism: Experimental analysis shows that models with balanced MHA and MLP compression outperform those with extreme pruning of one module, suggesting collaborative compression is optimal.
- Core assumption: MHA serves a more critical function in Transformer-based LLMs compared to MLP, which has higher redundancy.
- Evidence anchors:
  - [section]: "Results presented in Figure 8 reveal a clear trend that, the models at the center exhibit the best performance within each group...when pruning the MHA intermediate size to 512, the performance drops to the worst within each group."
  - [section]: "We interpret that MHA functions as the crucial module of Transformer-based LLMs while MLP has a larger redundancy that can be compressed."
  - [corpus]: Weak - corpus papers discuss MHA and MLP compression but don't provide the specific experimental analysis comparing different compression ratios.

## Foundational Learning

- Concept: Transformer architecture and the role of MHA and MLP modules
  - Why needed here: Understanding the structure of LLMs is essential to grasp how TransAct targets transitional activations within these modules
  - Quick check question: What are the main components of a Transformer layer, and where do the majority of model parameters reside?

- Concept: Structured pruning vs unstructured pruning
  - Why needed here: TransAct is a structured pruning approach, and understanding the differences helps explain why it's more suitable for efficient deployment
  - Quick check question: How does structured pruning differ from unstructured pruning in terms of computational efficiency and hardware requirements?

- Concept: Activation-based pruning metrics vs loss-based pruning metrics
  - Why needed here: TransAct uses activation magnitudes to guide pruning, which is a key innovation compared to traditional loss-based approaches
  - Quick check question: What are the advantages and disadvantages of using activation magnitudes versus loss approximations for guiding pruning decisions?

## Architecture Onboarding

- Component map: Input -> MHA module (Query, Key, Value projections, attention computation, output projection) -> MLP module (Input projection, optional gating, output projection) -> Transitional activations (Outputs of attention heads and MLP intermediate states) -> Pruning mechanism (Salience calculation based on activation magnitudes, dimension reduction) -> Output (Pruned hidden states passed to next layer)

- Critical path:
  1. Forward pass to compute transitional activations
  2. Salience calculation for each dimension
  3. Pruning of low-salience dimensions
  4. Fine-tuning to recover from pruning damage
  5. Repeat steps 1-4 for iterative pruning

- Design tradeoffs:
  - MHA vs MLP compression ratio: Balancing performance and efficiency
  - Number of pruning iterations: Trade-off between effectiveness and computational cost
  - Calibration sample size: Balancing metric accuracy and efficiency

- Failure signatures:
  - Significant performance degradation on downstream tasks
  - Instability during fine-tuning
  - Unexpected increase in inference time

- First 3 experiments:
  1. Single-shot pruning of LLaMA2-7B with varying MHA and MLP compression ratios, evaluate perplexity on WikiText
  2. Iterative pruning with different numbers of iterations, compare performance to single-shot approach
  3. Ablation study on the importance of preserving inter-module activations by pruning both intra- and inter-module dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TransAct compare to dynamic compression methods like Mixture-of-Experts (MoE) architectures when applied to LLMs of similar sizes?
- Basis in paper: [explicit] The paper mentions that TransAct is a static pruning approach and notes that recent research in MoE indicates dynamically compressed models can be more powerful than statically compressed ones.
- Why unresolved: The paper focuses solely on TransAct as a static pruning method and does not include comparisons with dynamic compression techniques like MoE.
- What evidence would resolve it: Direct performance comparisons between TransAct and MoE-based approaches on the same LLM benchmarks and compression ratios.

### Open Question 2
- Question: What is the optimal balance between MHA and MLP compression ratios for different types of language tasks (e.g., reasoning vs. factual recall)?
- Basis in paper: [explicit] The ablation studies show that MHA is more crucial than MLP, and models with uniform MHA and MLP sizes generally outperform others, but task-specific optimization is not explored.
- Why unresolved: The paper only provides general insights about MHA vs MLP importance without examining task-specific compression strategies.
- What evidence would resolve it: Task-specific performance evaluations varying MHA and MLP compression ratios independently across different benchmark categories.

### Open Question 3
- Question: How does TransAct perform when applied to LLMs that have undergone human alignment fine-tuning compared to base models?
- Basis in paper: [explicit] The limitations section mentions that pruning aligned LLMs remains challenging due to training data inconsistencies between pre-training and alignment.
- Why unresolved: All experiments are conducted on base models, and the paper acknowledges but does not investigate aligned model pruning.
- What evidence would resolve it: Performance evaluations of TransAct on aligned models like ChatGPT or LLaMA-2-Chat, measuring task performance and alignment preservation.

## Limitations
- The approach is evaluated only on LLaMA2-7B models, limiting generalizability to other LLM architectures and sizes
- All experiments use base models without human alignment fine-tuning, leaving questions about performance on aligned LLMs
- The "task-agnostic" claim is based on 10 downstream benchmarks, which may not represent the full spectrum of LLM applications

## Confidence

**High Confidence** (Supported by extensive experimental evidence and clear theoretical reasoning):
- TransAct achieves 20% additional FLOPs savings compared to state-of-the-art methods while maintaining competitive performance
- The method is particularly effective at high compression ratios (maintaining perplexity under 4.0 for 2.0B and 4.0B models)
- TransAct significantly reduces KV cache size and attention computation

**Medium Confidence** (Reasonable claims with adequate but not exhaustive evidence):
- Iterative pruning with activation-based metrics outperforms single-shot pruning with Taylor expansion-based metrics
- MHA modules have less redundancy than MLP modules, making balanced compression optimal
- Preserving inter-module activations while pruning intra-module activations maintains model capability

**Low Confidence** (Claims that need more empirical support):
- TransAct is truly "task-agnostic" - the experiments show strong results but don't exhaustively test all task types
- The pruning metric based on activation magnitudes will generalize well to future LLM architectures

## Next Checks

**Validation Check 1: Architecture Generalization**
Test TransAct on a diverse set of LLM architectures including GPT-3, BERT, and OPT models across different sizes (1B, 7B, 13B parameters). Compare the effectiveness of activation-based metrics versus loss-based metrics across architectures to validate the claimed superiority of TransAct's approach.

**Validation Check 2: Task Coverage Expansion**
Evaluate TransAct-pruned models on specialized task categories including mathematical reasoning, code generation, multilingual tasks, and domain-specific benchmarks (biomedical, legal). This would better establish the "task-agnostic" claim and identify any task types where the approach may underperform.

**Validation Check 3: Ablation on Pruning Schedule**
Systematically vary the number of pruning iterations and the compression ratio per iteration to identify optimal pruning schedules. Compare activation-based metrics against alternative salience measures (gradient-based, attention pattern-based) to isolate the contribution of the activation-guided approach.