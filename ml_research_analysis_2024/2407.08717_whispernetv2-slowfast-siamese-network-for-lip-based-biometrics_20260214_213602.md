---
ver: rpa2
title: 'WhisperNetV2: SlowFast Siamese Network For Lip-Based Biometrics'
arxiv_id: '2407.08717'
source_url: https://arxiv.org/abs/2407.08717
tags:
- network
- biometric
- lbba
- authentication
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present WhisperNetV2, a lip-based biometric authentication
  (LBBA) method using a Siamese architecture with triplet loss and SlowFast embedding
  networks. The SlowFast design captures both static visual features (slow pathway)
  and behavioral motion features (fast pathway) from lip videos.
---

# WhisperNetV2: SlowFast Siamese Network For Lip-Based Biometrics

## Quick Facts
- arXiv ID: 2407.08717
- Source URL: https://arxiv.org/abs/2407.08717
- Reference count: 40
- Primary result: Achieves EER of 0.005 on CREMA-D dataset using SlowFast Siamese architecture with triplet loss.

## Executive Summary
WhisperNetV2 introduces a lip-based biometric authentication method that uses a SlowFast Siamese network with triplet loss to jointly capture static lip appearance and behavioral motion features. Trained on the CREMA-D dataset with emotional utterances, the model learns emotion-invariant embeddings by leveraging challenging triplet samples during training. Under an open-set protocol, it achieves an EER of 0.005, outperforming previous LBBA methods and improving upon the authors' prior WhisperNet approach in both accuracy and efficiency.

## Method Summary
The method uses a Siamese architecture with three identical SlowFast embedding networks, trained with triplet loss to learn discriminative lip embeddings. The slow pathway extracts static visual features at low temporal stride, while the fast pathway captures motion dynamics at high temporal stride. Inputs are lip videos cropped to 30×18 pixels after landmark extraction from the CREMA-D dataset. Training includes challenging triplets (same speaker, different phrase; same phrase, different speaker) to force reliance on behavioral and physiological cues. The network is evaluated under an open-set protocol with 66 training, 11 validation, and 11 test subjects.

## Key Results
- Achieves EER of 0.005 on test set under open-set protocol.
- Outperforms previous LBBA approaches on the CREMA-D dataset.
- Demonstrates improved computational efficiency and fewer parameters compared to WhisperNet.

## Why This Works (Mechanism)

### Mechanism 1
The SlowFast Siamese architecture simultaneously captures static lip appearance and behavioral motion features, improving discrimination over unimodal or temporal-only approaches. The slow pathway extracts high-resolution static visual features at low temporal stride, while the fast pathway extracts motion dynamics at high temporal stride with low channel capacity. Their concatenation yields a rich embedding covering both physiological and behavioral traits. This assumes lip-based biometric identity can be modeled as a combination of static geometry and dynamic motion patterns, which the SlowFast pathway efficiently decouples.

### Mechanism 2
Triplet loss with anchor-positive-negative structure forces the embedding to learn inter-class separation while preserving intra-class similarity, even under emotional variation. During training, the loss enforces that the distance between anchor and positive embeddings is less than the distance to negative embeddings by a margin. Challenging triplets (same speaker, different phrase; different speaker, same phrase) force the network to rely on behavioral and physiological cues respectively. This assumes lip dynamics vary more between speakers than between emotional states of the same speaker, which triplet loss can enforce.

### Mechanism 3
Using CREMA-D dataset with emotional utterances trains the model to be invariant to facial expression and speech tempo variations. During training, multiple recordings of the same phrase with different emotions act as positive pairs, teaching the network to ignore emotion-induced lip dynamics and focus on identity. This assumes emotional states cause only minor lip motion and appearance changes relative to speaker identity differences, which the network can learn from training data.

## Foundational Learning

- **Concept**: Siamese network with triplet loss
  - Why needed here: Provides a metric learning framework to learn embeddings where same-identity pairs are close and different-identity pairs are far, without requiring class labels.
  - Quick check question: What is the role of the margin parameter α in triplet loss?

- **Concept**: SlowFast network architecture
  - Why needed here: Enables joint extraction of static visual features and dynamic motion features from video sequences, which are both relevant to lip biometrics.
  - Quick check question: How do the temporal stride and channel capacity differ between the slow and fast pathways?

- **Concept**: Lip landmark preprocessing and cropping
  - Why needed here: Ensures the network receives only the lip region, reducing input dimensionality and avoiding distraction from unrelated facial features.
  - Quick check question: Why is it important to adjust bounding box aspect ratios before resizing lip images?

## Architecture Onboarding

- **Component map**: Face landmark extraction → bounding box adjustment → resize to 30×18 → SlowFast Siamese (three identical branches) → triplet loss → cosine similarity comparison → threshold-based authentication

- **Critical path**:
  1. Input video → lip cropping (preprocess)
  2. SlowFast embedding generation (branch 1: anchor)
  3. SlowFast embedding generation (branch 2: positive)
  4. SlowFast embedding generation (branch 3: negative)
  5. Compute pairwise cosine distances
  6. Compute triplet loss
  7. Backpropagation through all three branches

- **Design tradeoffs**:
  - Slow vs. fast pathway balance: More slow channels improves static feature richness but increases parameters; more fast frames improves motion capture but increases temporal computation.
  - Margin α in triplet loss: Larger margins enforce stricter separation but may slow convergence; smaller margins converge faster but risk collapse.
  - Cropping resolution (30×18): Smaller size reduces compute but may lose subtle lip shape cues; larger size retains detail but increases cost.

- **Failure signatures**:
  - Training loss plateaus early: Likely triplet mining is not challenging enough; consider adding semi-hard or hard triplets.
  - Validation EER rises after few epochs: Possible overfitting to emotional patterns; augment with more diverse emotion samples or regularize.
  - Lip cropping misalignment: Errors in landmark extraction propagate to embeddings; verify preprocessing pipeline.

- **First 3 experiments**:
  1. Ablation: Train without fast pathway to measure impact of behavioral features on EER.
  2. Margin sweep: Vary triplet loss margin α (0.5, 0.7, 0.9) and observe EER trends.
  3. Input size scaling: Test with 30×18 vs. 60×36 lip crops to quantify trade-off between accuracy and inference time.

## Open Questions the Paper Calls Out
None

## Limitations
- SlowFast Siamese design assumes clean separation between behavioral and physiological components, but emotional variation may blur this distinction.
- Critical architectural details (number of layers, channels, fusion strategy) are omitted, making exact reproduction difficult.
- Reported 0.005 EER on only 88 subjects raises concerns about generalization to larger, more diverse populations and spoofing attacks.

## Confidence
- **High confidence**: General claim that SlowFast Siamese network can jointly model static and dynamic lip features; qualitative improvement over WhisperNet in avoiding landmark extraction errors.
- **Medium confidence**: Specific EER of 0.005 under described protocol, given limited dataset size and missing architectural details.
- **Low confidence**: Claims about robustness to emotional variation without explicit cross-emotion test results or ablation studies.

## Next Checks
1. **Emotion-invariance ablation**: Retrain the network on a subset of CREMA-D with only neutral expressions and compare test EER to the full-emotion model to quantify emotion-related performance loss.
2. **Hard triplet mining**: Modify the training loop to explicitly include hard triplets (same speaker, different phrase with same emotion) and measure if EER improves, confirming the behavioral pathway's role.
3. **Cross-dataset generalization**: Evaluate the trained model on a separate lip biometric dataset (e.g., LRW or OuluVS2) to assess whether the 0.005 EER holds beyond CREMA-D's controlled conditions.