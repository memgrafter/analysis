---
ver: rpa2
title: Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making
arxiv_id: '2402.06075'
source_url: https://arxiv.org/abs/2402.06075
tags:
- wargaming
- learning
- more
- intelligence
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of scaling artificial intelligence
  for complex wargaming scenarios by developing a hierarchical reinforcement learning
  framework. The proposed method decomposes the problem into hierarchical agents,
  decisions, and policies, and employs dimension-invariant observation abstractions
  and a multi-model approach.
---

# Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making

## Quick Facts
- arXiv ID: 2402.06075
- Source URL: https://arxiv.org/abs/2402.06075
- Reference count: 40
- The paper develops a hierarchical reinforcement learning framework to scale AI for complex wargaming scenarios, demonstrating improved agent performance compared to traditional methods.

## Executive Summary
This paper addresses the critical challenge of scaling artificial intelligence for complex wargaming scenarios by developing a hierarchical reinforcement learning framework. The authors tackle the computational explosion that occurs when traditional reinforcement learning approaches attempt to handle large state-action spaces characteristic of combat simulations. By decomposing the problem into hierarchical agents, decisions, and policies, along with dimension-invariant observation abstractions and a multi-model approach, the framework aims to manage the exponential growth in computations required. Initial results from low-fidelity combat simulations show promising improvements in agent performance compared to traditional methods.

## Method Summary
The proposed framework employs hierarchical reinforcement learning to decompose complex wargaming scenarios into manageable subproblems. The architecture features a three-level hierarchy: commander (strategic decisions), manager (tactical planning), and unit (operational execution) agents. Each level uses dimension-invariant observation abstractions that maintain computational efficiency while preserving critical decision-making information. The framework incorporates a multi-model approach that dynamically switches between different behavior models (machine learning, expert systems, optimization algorithms) based on evaluation functions. Training occurs progressively from lower to higher levels, with initial experiments conducted in a low-fidelity combat simulation environment called Atlatl, using both supervised learning and deep reinforcement learning techniques.

## Key Results
- Demonstrated improved agent performance compared to traditional scripted or RL-trained agents in low-fidelity combat simulations
- Showed that dimension-invariant observation abstractions enable training on larger gameboards while maintaining intelligent behavior
- Validated the multi-model framework's ability to drastically improve performance through dynamic model switching
- Established the feasibility of hierarchical reinforcement learning for managing computational complexity in wargaming scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decomposition reduces computational complexity by managing exponential state-action space growth.
- Mechanism: The framework decomposes the problem into hierarchical agents, decisions, and policies, allowing higher-level agents to make abstract decisions while lower-level agents handle specific actions.
- Core assumption: Hierarchical decomposition preserves essential strategic and tactical relationships while reducing the effective search space.
- Evidence anchors:
  - [abstract] "The proposed method decomposes the problem into hierarchical agents, decisions, and policies"
  - [section] "HRL shows promise to help address this problem, also known as Bellman's 'curse of dimensionality,' in that it 'decomposes a reinforcement learning problem into a hierarchy of subproblems or subtasks'"
- Break condition: If hierarchical decomposition fails to capture critical interdependencies between units, the resulting policy may be suboptimal or ineffective in complex scenarios.

### Mechanism 2
- Claim: Dimension-invariant observation abstractions enable scaling to larger gameboards without proportionally increasing training complexity.
- Mechanism: The framework uses different levels of abstraction based on hierarchy level, with localized abstractions centered on units on-move, using spatial decay functions to weight distant information.
- Core assumption: Lower fidelity abstractions at higher levels still contain sufficient information for effective decision-making.
- Evidence anchors:
  - [section] "We use different levels of abstractions based on the level of the hierarchy being computed"
  - [section] "We have found that creating dimension-invariant observation abstraction have allowed us to train agents on larger gameboards— while still producing intelligent behavior"
- Break condition: If the abstraction loses critical tactical information necessary for high-level strategic decisions, performance will degrade despite reduced dimensionality.

### Mechanism 3
- Claim: Multi-model framework improves performance by leveraging specialized models rather than a single monolithic approach.
- Mechanism: The framework dynamically switches between different independently-trained behavior models (machine learning, expert systems, rule-based, optimization algorithms, game solving) based on evaluation functions.
- Core assumption: Specialized models can be more effective than a single model attempting to handle all scenarios.
- Evidence anchors:
  - [section] "Developing a multi-model approach that can dynamically switch between different independently-trained behavior models"
  - [section] "Our multi-model framework has thus far allowed us to drastically improve the performance of our agents well beyond our current state-of-the-art scripted or RL-trained agents"
- Break condition: If the evaluation function fails to correctly identify the optimal model for a given situation, the switching mechanism could degrade rather than improve performance.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: MDP provides the mathematical foundation for sequential decision making in reinforcement learning, essential for modeling combat scenarios.
  - Quick check question: What distinguishes MDP from standard reinforcement learning problems, and why is this distinction important for wargaming applications?

- Concept: Hierarchical Reinforcement Learning (HRL)
  - Why needed here: HRL extends traditional RL by decomposing complex tasks into hierarchical subtasks, enabling scalability to large state spaces characteristic of wargaming.
  - Quick check question: How does HRL address the "curse of dimensionality" that makes traditional RL intractable for complex combat simulations?

- Concept: Options framework
  - Why needed here: Options provide a mechanism for temporal abstraction in HRL, allowing agents to learn sequences of actions as higher-level behaviors.
  - Quick check question: How do options differ from primitive actions in terms of their temporal extension and impact on learning efficiency?

## Architecture Onboarding

- Component map: Agent hierarchy (commander → manager → unit), Decision hierarchy (strategic → tactical → action), Observation abstraction layer, Multi-model evaluation framework, Training environment integration
- Critical path: Agent hierarchy decomposition → Observation abstraction design → Multi-model implementation → Training pipeline development → Performance evaluation
- Design tradeoffs: Fidelity vs. computational efficiency in observation abstractions, Model specialization vs. generalization in multi-model approach, Hierarchy depth vs. coordination complexity
- Failure signatures: Degraded performance in larger scenarios, Inconsistent behavior across different models, Training instability or slow convergence
- First 3 experiments:
  1. Implement hierarchical agent structure with simple decision policies to validate decomposition approach
  2. Test dimension-invariant observation abstractions on progressively larger gameboards
  3. Evaluate multi-model switching mechanism with two contrasting behavior models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms within the hierarchical reinforcement learning framework will ensure optimal decision-making at each hierarchical level, particularly when transitioning between tactical and operational scales?
- Basis in paper: [explicit] The paper discusses the development of hierarchical decomposition of agents, decisions, and policies, and mentions experimenting with goal and subgoal inputs and outputs, but does not provide specific mechanisms for ensuring optimal decision-making across hierarchical levels.
- Why unresolved: The authors acknowledge the complexity of ensuring optimal decision-making across different hierarchical levels and the need for appropriate reward engineering, but they do not detail specific mechanisms or algorithms to achieve this.
- What evidence would resolve it: Detailed descriptions or results of experiments demonstrating the effectiveness of specific mechanisms or algorithms in ensuring optimal decision-making across hierarchical levels, particularly in transitioning between tactical and operational scales.

### Open Question 2
- Question: How will the proposed multi-model approach dynamically select the most appropriate behavior model in real-time during wargaming scenarios, and what criteria will be used to evaluate the effectiveness of each model?
- Basis in paper: [explicit] The paper introduces a multi-model approach that can dynamically switch between different behavior models, but it does not specify the criteria or process for real-time selection and evaluation of model effectiveness.
- Why unresolved: While the concept of dynamic model switching is introduced, the criteria for selection and evaluation are not detailed, leaving uncertainty about how the system will determine the most appropriate model in real-time.
- What evidence would resolve it: Experimental results or simulations showing the dynamic selection process in action, along with metrics or criteria used to evaluate the effectiveness of each model during wargaming scenarios.

### Open Question 3
- Question: What are the specific challenges and limitations of scaling the hierarchical reinforcement learning framework from low-fidelity to high-fidelity combat simulations, and how will these be addressed?
- Basis in paper: [inferred] The paper mentions the intention to demonstrate scalability by implementing the framework in both low-fidelity and high-fidelity simulations, but does not discuss the specific challenges or limitations of this transition.
- Why unresolved: The authors plan to scale the framework but do not provide insights into the potential challenges or limitations that may arise during this process, nor do they outline strategies to address these issues.
- What evidence would resolve it: Detailed analysis or case studies comparing the performance and challenges encountered when transitioning from low-fidelity to high-fidelity simulations, along with strategies implemented to overcome identified limitations.

## Limitations

- Limited empirical evidence from only low-fidelity combat simulations, with no validation in high-fidelity scenarios
- Absence of citations and related work in the corpus suggests this research is in early stages with limited validation against established literature
- Higher-level agents (commander and manager levels) remain largely theoretical with planned future work, making performance claims for full hierarchical implementation uncertain

## Confidence

- **High Confidence:** The hierarchical decomposition approach for reducing computational complexity is well-established in reinforcement learning literature and theoretically sound.
- **Medium Confidence:** The dimension-invariant observation abstractions and multi-model framework show promise based on initial results, but lack extensive validation across diverse scenarios.
- **Low Confidence:** Claims about performance improvements and scalability to large-scale wargaming scenarios require further empirical validation, particularly for the higher-level agents.

## Next Checks

1. Implement the hierarchical agent structure with simple decision policies and evaluate whether the decomposition actually reduces computational complexity while maintaining strategic effectiveness.

2. Test dimension-invariant observation abstractions on progressively larger gameboards to verify the claim that they enable scaling without proportional increases in training complexity.

3. Develop comprehensive performance benchmarks comparing the multi-model framework against both traditional scripted agents and single-model RL approaches across multiple wargaming scenarios.