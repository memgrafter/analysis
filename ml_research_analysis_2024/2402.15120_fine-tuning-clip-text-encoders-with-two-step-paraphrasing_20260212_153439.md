---
ver: rpa2
title: Fine-tuning CLIP Text Encoders with Two-step Paraphrasing
arxiv_id: '2402.15120'
source_url: https://arxiv.org/abs/2402.15120
tags:
- clip
- text
- retrieval
- image
- paraphrases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of CLIP models in handling
  linguistic variations like paraphrases in text inputs. The authors propose a two-step
  paraphrasing approach to fine-tune CLIP text encoders, where they automatically
  generate two categories of paraphrases from web-scale image captions using large
  language models.
---

# Fine-tuning CLIP Text Encoders with Two-step Paraphrasing

## Quick Facts
- arXiv ID: 2402.15120
- Source URL: https://arxiv.org/abs/2402.15120
- Reference count: 13
- Primary result: ParaCLIP significantly improves CLIP's handling of linguistic variations like paraphrases across multiple tasks

## Executive Summary
This paper addresses CLIP models' limitations in handling linguistic variations such as paraphrases in text inputs. The authors propose a two-step paraphrasing approach to fine-tune CLIP text encoders, generating diverse paraphrases from web-scale image captions using large language models. The resulting ParaCLIP model significantly outperforms baseline CLIP models on paraphrased retrieval tasks, Visual Genome Relation and Attribution tasks, and seven semantic textual similarity tasks, demonstrating improved robustness to linguistic variations while maintaining general vision-language capabilities.

## Method Summary
The method involves automatically generating two categories of paraphrases from web-scale image captions using LLMs through a two-step process: caption-to-paraphrase generation followed by paraphrase-to-paraphrase generation. These generated paraphrases are then used to fine-tune the CLIP text encoder while freezing the image encoder. The fine-tuning employs three loss functions: L1 for maintaining image-text alignment, L2 for aligning captions with paraphrases, and L3 for clustering semantically similar paraphrases in vector space. The approach aims to create invariant text representations that map different linguistic expressions of the same concept to similar vector space regions.

## Key Results
- ParaCLIP improves paraphrased retrieval rank similarity scores by up to 2.0% and 5.6% compared to baseline CLIP models
- Significant performance gains on Visual Genome Relation and Attribution tasks
- Consistent improvements across seven semantic textual similarity tasks
- Maintains competitive performance on standard zero-shot image classification and retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
The two-step paraphrase generation creates diverse semantic variations that push CLIP's text encoder to learn invariant representations across linguistic forms. The first step converts noisy web captions into plain language, while the second step generates morpho-semantically varied paraphrases. This dual augmentation ensures the encoder maps different textual expressions of the same concept to similar vector space regions. The core assumption is that LLM-generated paraphrases preserve semantic equivalence while varying surface form.

### Mechanism 2
Freezing the image encoder during fine-tuning preserves pre-trained multimodal alignment while only refining text representations for paraphrases. The L1 loss maintains image-text alignment learned during pre-training, preventing catastrophic forgetting while L2 and L3 losses shape text representations to handle paraphrases. The core assumption is that the image encoder's alignment with text is already well-learned and does not need adjustment for paraphrase robustness.

### Mechanism 3
The combination of L2 (caption-paraphrase alignment) and L3 (paraphrase-paraphrase alignment) losses explicitly teaches the model to map semantically similar text to similar vector space regions. L2 bridges noisy captions to clean paraphrases, while L3 clusters different paraphrases of the same concept, creating a robust paraphrase-invariant text representation space. The core assumption is that these loss functions effectively pull semantically equivalent texts together in embedding space.

## Foundational Learning

- **Concept: Contrastive learning with InfoNCE loss**
  - Why needed here: CLIP relies on contrastive learning to align image and text embeddings; understanding this is crucial for grasping how the fine-tuning process works.
  - Quick check question: What does the InfoNCE loss do in CLIP training, and how does it differ from standard cross-entropy?

- **Concept: Representation space geometry and clustering**
  - Why needed here: The paper relies on pulling semantically similar texts into similar regions of vector space; understanding embedding geometry is essential.
  - Quick check question: How does cosine similarity relate to the distance between embeddings in CLIP's vector space?

- **Concept: Catastrophic forgetting in fine-tuning**
  - Why needed here: The paper explicitly mentions preventing forgetting of pre-trained knowledge; understanding this concept is crucial for evaluating the approach.
  - Quick check question: What happens during fine-tuning when previously learned representations are overwritten by new training objectives?

## Architecture Onboarding

- **Component map:** Web-scale image-caption pairs → LLM paraphrase generation (two-step) → Fine-tuning dataset → CLIP (ViT-B/32) with frozen image encoder, trainable text encoder → Three-loss fine-tuning (L1+L2+L3) → ParaCLIP model
- **Critical path:** LLM paraphrase generation → Fine-tuning with three-loss combination → Evaluation on paraphrase-sensitive tasks
- **Design tradeoffs:** Freezing image encoder for efficiency vs. potential need for joint fine-tuning; synthetic paraphrase quality vs. real paraphrase data scarcity
- **Failure signatures:** Degradation on standard CLIP tasks, poor paraphrase retrieval performance, inconsistent results across runs
- **First 3 experiments:**
  1. Generate paraphrases for a small subset of LAION captions and verify semantic equivalence manually
  2. Fine-tune CLIP on paraphrased data with only L1 loss to establish baseline performance
  3. Add L2 and L3 losses incrementally to measure their individual and combined impact on paraphrase tasks

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the two-step paraphrasing process impact the robustness of CLIP models to various linguistic variations beyond simple paraphrases? While the paper demonstrates improvements in paraphrased retrieval, it does not explore the model's robustness to other linguistic variations like idiomatic expressions or domain-specific jargon.

- **Open Question 2:** What are the potential trade-offs between improving CLIP's robustness to paraphrases and its performance on other vision-language tasks? The paper mentions that ParaCLIP sometimes degrades performance on conventional vision and vision-language tasks but does not provide a detailed analysis of the trade-offs.

- **Open Question 3:** How does the choice of paraphrase generation model (e.g., ChatGPT vs. LLaMA) influence the quality and diversity of the generated paraphrases? The paper uses both models and notes comparable quality, but does not explore how different models might affect diversity and quality, which could impact model performance.

## Limitations

- The effectiveness heavily depends on the quality and semantic fidelity of LLM-generated paraphrases, but the paper provides no empirical validation of paraphrase quality or semantic preservation
- The three-loss fine-tuning framework appears theoretically sound but lacks ablation studies demonstrating that all three losses are necessary and sufficient
- The paper does not address potential domain shift issues between the web-scale caption data and the evaluation tasks

## Confidence

- **High confidence:** The general approach of using paraphrase augmentation to improve CLIP's robustness to linguistic variations is sound and well-supported by consistent improvements across multiple task types
- **Medium confidence:** The specific two-step paraphrase generation process and three-loss fine-tuning framework are plausible but would benefit from more rigorous ablation studies and quality validation of generated paraphrases
- **Low confidence:** Claims about catastrophic forgetting prevention and the necessity of freezing the image encoder are weakly supported without empirical evidence of what happens when the image encoder is also fine-tuned

## Next Checks

1. Conduct a manual quality assessment of 100+ generated paraphrases to verify semantic equivalence and diversity, checking for both semantic drift and inadequate morphological variation
2. Perform ablation studies removing each loss component (L1, L2, L3) individually to determine their individual contributions to overall performance improvements
3. Test the model's performance when the image encoder is also fine-tuned to assess whether the frozen image encoder assumption holds or if joint fine-tuning yields better results