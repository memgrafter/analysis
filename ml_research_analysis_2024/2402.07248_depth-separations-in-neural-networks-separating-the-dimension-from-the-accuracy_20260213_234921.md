---
ver: rpa2
title: 'Depth Separations in Neural Networks: Separating the Dimension from the Accuracy'
arxiv_id: '2402.07248'
source_url: https://arxiv.org/abs/2402.07248
tags:
- depth
- function
- which
- bound
- lower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper establishes a separation between depth 2 and depth 3
  neural networks for approximating Lipschitz functions with bounded weights. The
  key result shows that for a family of 1-Lipschitz functions defined on the unit
  ball, depth 3 networks can approximate them to arbitrary accuracy with polynomial
  width, while depth 2 networks require exponential width unless weights are exponentially
  large.
---

# Depth Separations in Neural Networks: Separating the Dimension from the Accuracy

## Quick Facts
- **arXiv ID**: 2402.07248
- **Source URL**: https://arxiv.org/abs/2402.07248
- **Reference count**: 40
- **Primary result**: Establishes exponential width separation between depth 2 and depth 3 neural networks for approximating Lipschitz functions with bounded weights

## Executive Summary
This paper proves an exponential separation between depth 2 and depth 3 neural networks for approximating Lipschitz functions on bounded domains. The key result shows that while depth 3 networks can approximate certain O(1)-Lipschitz functions with polynomial width and bounded weights, depth 2 networks require exponential width unless weights are exponentially large. This resolves an open question about whether the curse of dimensionality manifests in depth 2 approximation. The proof technique uses a novel worst-to-average-case reduction combined with threshold circuit lower bounds.

## Method Summary
The paper establishes a depth separation by constructing a specific family of Lipschitz functions on the unit ball and proving both upper and lower bounds. For the lower bound, the authors use a worst-to-average-case reduction: if a depth 2 network approximates the function well on average, randomization shows it must approximate uniformly badly, implying exponential width. This reduction leverages threshold circuit lower bounds for the IP mod 2 function. For the upper bound, depth 3 networks efficiently approximate the function by composing simple functions on different input coordinates, exploiting the additional layer to handle the combinatorial complexity.

## Key Results
- Exponential width separation between depth 2 and depth 3 neural networks for approximating Lipschitz functions
- Lower bound shows depth 2 networks require exponential width with polynomially bounded weights
- Upper bound shows depth 3 networks can approximate with polynomial width and weights
- Results hold for a wide family of activation functions including ReLU, threshold, and sigmoidal activations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depth 2 neural networks cannot efficiently approximate the target function due to a worst-to-average-case reduction combined with threshold circuit lower bounds
- Mechanism: The paper constructs a depth 2 network that approximates the target function well on average, then uses randomization to show that the approximation must be uniformly bad across all inputs. This uniform badness implies the network cannot compute a hard Boolean function (IP mod 2), which requires exponential width in depth 2 threshold circuits
- Core assumption: The activation function satisfies mild boundedness and Lipschitz conditions (Assumption 1)
- Evidence anchors:
  - [abstract]: "Our lower bound relies on a reduction to threshold circuits...where an exact computation lower bound for the IP mod 2 function is used"
  - [section]: "Given a network which approximates fd well, we can use it to construct a network which achieves similar accuracy, but with margins that can be made arbitrarily more uniform due to a concentration of measure argument"
  - [corpus]: Weak evidence - no direct mention of threshold circuits or IP mod 2 in corpus
- Break condition: If the activation function violates the boundedness or Lipschitz assumptions, or if the concentration of measure argument fails due to insufficient randomization

### Mechanism 2
- Claim: The curse of dimensionality manifests in depth 2 approximation even for O(1)-Lipschitz functions on bounded domains
- Mechanism: The paper constructs a distribution that concentrates probability mass on exponentially many points embedded in the unit ball. This concentration creates a combinatorial explosion that depth 2 networks cannot handle, while depth 3 networks can exploit their additional layer to approximate the function efficiently
- Core assumption: The distribution has support in the unit ball and the function is O(1)-Lipschitz
- Evidence anchors:
  - [abstract]: "we prove an exponential size separation between depth 2 and depth 3 neural networks...when approximating a O(1)-Lipschitz target function to constant accuracy, with respect to a distribution with support in the unit ball"
  - [section]: "The key ingredient in our proof is a reduction from a worst-case complexity, where we are able to construct a network which is capable of classifying all the inputs correctly; to an average-case complexity"
  - [corpus]: Weak evidence - corpus focuses on quantum and recurrent architectures rather than depth separations in classical neural networks
- Break condition: If the distribution doesn't concentrate mass appropriately or if the function isn't sufficiently "hard" in the combinatorial sense

### Mechanism 3
- Claim: Depth 3 networks can efficiently approximate the target function by composing simple functions on different input coordinates
- Mechanism: The depth 3 network architecture separates the computation into two hidden layers: one layer computes simple functions on pairs of coordinates (implementing AND-like operations), and the second layer combines these results using a periodic function (implementing the IP mod 2 operation)
- Core assumption: The activation function satisfies approximation properties allowing efficient representation of simple functions (Assumption 2)
- Evidence anchors:
  - [abstract]: "a depth 3 neural network N′d with size and weights bounded by poly(d, 1/ε)"
  - [section]: "Since a depth 3 network has two hidden layers with non-linear σ activations, we are able to use each hidden layer to compute each function, and obtain the desired approximation"
  - [corpus]: Weak evidence - no direct mention of depth 3 network constructions in corpus
- Break condition: If the activation function cannot efficiently approximate the required simple functions, or if the composition strategy doesn't preserve the target function's properties

## Foundational Learning

- Concept: Threshold circuits and their lower bounds
  - Why needed here: The paper reduces the neural network approximation problem to threshold circuit complexity, specifically using the known exponential lower bound for computing IP mod 2 with depth 2 threshold circuits
  - Quick check question: What is the known lower bound on the width of depth 2 threshold circuits required to compute IP mod 2 exactly?

- Concept: Rademacher complexity and statistical learnability
  - Why needed here: The paper uses Rademacher complexity bounds to show that depth 2 networks with certain activation functions are statistically learnable, which is crucial for the reduction argument
  - Quick check question: How does the Rademacher complexity bound on depth 2 networks with activation function σ relate to the learnability of the function class?

- Concept: Concentration of measure phenomena
  - Why needed here: The randomization scheme in the proof relies on concentration of measure to show that a network that approximates well on average must also approximate well uniformly across all inputs
  - Quick check question: What concentration inequality is used to bound the deviation between average-case and worst-case approximation error?

## Architecture Onboarding

- Component map:
  - Depth 2 networks: Single hidden layer with activation function σ, followed by linear output layer
  - Depth 3 networks: Two hidden layers with activation function σ, followed by linear output layer
  - Distribution: Uniform over a Cartesian product of a well-spread set and a continuous embedding of the Boolean hypercube
  - Target function: IP mod 2 computed on rounded coordinates of the input

- Critical path:
  1. Define the hard-to-approximate function fd on the unit ball
  2. Show depth 2 networks require exponential width (lower bound)
  3. Construct depth 3 network that approximates fd efficiently (upper bound)
  4. Prove the lower bound using worst-to-average-case reduction and threshold circuit lower bounds
  5. Verify the upper bound construction works for the specified activation functions

- Design tradeoffs:
  - Weight boundedness vs. expressiveness: The lower bound assumes polynomially bounded weights, while the upper bound also uses polynomially bounded weights
  - Distribution choice: The constructed distribution concentrates mass to create hardness, but must still be supported in the unit ball
  - Activation function restrictions: The results require mild assumptions on σ, excluding highly oscillatory functions

- Failure signatures:
  - Lower bound fails: If the concentration of measure argument doesn't hold, or if the reduction to threshold circuits is invalid
  - Upper bound fails: If the composition of simple functions doesn't preserve the target function's properties, or if the activation function cannot efficiently approximate the required components
  - Distribution issues: If the embedded Boolean hypercube doesn't maintain sufficient separation between points

- First 3 experiments:
  1. Implement the depth 2 network approximation on the constructed distribution and measure the empirical error distribution to verify the concentration phenomenon
  2. Implement the depth 3 network construction for ReLU activation and verify it approximates the target function on the test set
  3. Test the randomization scheme used in the lower bound proof by applying it to a simpler function and measuring the uniformity of the resulting approximation error

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a depth 2 vs depth 3 neural network separation be shown for approximating Lipschitz functions with unbounded weights?
- Basis in paper: [explicit] The authors state "To the best of our knowledge, it remains a major open problem to show a superpolynomial separation result between depth 2 and depth 3, with respect to a distribution with bounded support, and when allowing unbounded weights."
- Why unresolved: The current proof technique relies on weight boundedness assumptions to leverage threshold circuit lower bounds.
- What evidence would resolve it: A proof or counterexample showing whether depth 2 networks can approximate certain Lipschitz functions to constant accuracy using unbounded weights when depth 3 networks can do so efficiently.

### Open Question 2
- Question: Can the exponential separation between depth 2 and depth 3 be extended to more practical distributions?
- Basis in paper: [explicit] The authors construct a specific distribution supported on the unit ball with mass concentrated near exponentially many points. They note "Hsu et al. [9] does strongly suggest, however, that results like ours can only be possible for distributions that are far in some sense from the uniform distribution."
- Why unresolved: The constructed distribution is somewhat artificial and may not represent natural data distributions.
- What evidence would resolve it: A depth separation result for a natural distribution (e.g., Gaussian, uniform over bounded domains) or a proof that such separations are impossible for natural distributions.

### Open Question 3
- Question: Can the depth 3 approximation be made more efficient in terms of width and weight magnitude?
- Basis in paper: [inferred] The upper bound theorem shows width O(d²/ε) and weights O(d/ε²), which may not be tight. The authors provide a concrete ReLU example with better parameters.
- Why unresolved: The general upper bound construction uses a straightforward approach that may not be optimal.
- What evidence would resolve it: Improved upper bounds showing depth 3 networks can approximate the function with smaller width or weights, or a matching lower bound for depth 3 networks.

## Limitations
- The proof relies on specific properties of the activation function (boundedness and Lipschitz continuity) that may exclude some commonly used activations
- The constructed distribution is somewhat artificial and may not represent natural data distributions
- The reduction to threshold circuits requires careful verification of the concentration of measure argument

## Confidence
- High Confidence: The mechanism showing that depth 3 networks can efficiently approximate the target function using composition of simple functions
- Medium Confidence: The worst-to-average-case reduction technique and its application to neural network approximation
- Low Confidence: The exact distribution construction and whether the concentration of measure argument holds for all parameter settings

## Next Checks
1. **Empirical Verification of Concentration**: Implement the constructed distribution and measure the empirical error distribution of depth 2 networks to verify the concentration phenomenon. This would provide empirical support for the worst-to-average-case reduction argument.

2. **Activation Function Sensitivity**: Test the lower bound construction with different activation functions (ReLU, sigmoid, tanh) to verify the claims about which functions satisfy the required properties. This would help establish the generality of the result.

3. **Simpler Function Analysis**: Apply the randomization scheme from the lower bound proof to a simpler function (like a parity function on fewer bits) and measure the uniformity of the resulting approximation error. This would help validate the key technical lemma before applying it to the full construction.