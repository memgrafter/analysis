---
ver: rpa2
title: Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models
arxiv_id: '2409.05771'
source_url: https://arxiv.org/abs/2409.05771
tags:
- language
- encoding
- layer
- layers
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper demonstrates that encoding performance in fMRI correlates
  strongly with the intrinsic dimensionality of intermediate LLM layers, supporting
  a two-phase abstraction process: an early "composition" phase that builds complex
  representations, and a later "prediction" phase focused on next-token output. This
  correlation holds across multiple brain regions and model families, with the highest
  encoding performance and dimensionality peak occurring in layers that bridge these
  two phases.'
---

# Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models

## Quick Facts
- arXiv ID: 2409.05771
- Source URL: https://arxiv.org/abs/2409.05771
- Reference count: 40
- Primary result: Encoding performance in fMRI correlates strongly with intrinsic dimensionality of intermediate LLM layers, supporting a two-phase abstraction process

## Executive Summary
This paper demonstrates that encoding performance in fMRI correlates strongly with the intrinsic dimensionality of intermediate LLM layers, supporting a two-phase abstraction process in language models. The first phase involves building complex, high-dimensional representations of linguistic structure ("composition"), while the second focuses on next-token prediction ("prediction"). This correlation holds across multiple brain regions and model families, with the highest encoding performance and dimensionality peak occurring in layers that bridge these two phases. The study shows that shared abstract, compositional features—not just autoregressive prediction—drive brain-model similarity, challenging the idea that next-token prediction alone explains neural encoding success.

## Method Summary
The study analyzes fMRI data from 3 human subjects listening to 20 hours of English language podcast stories. Activations are extracted from multiple LLM layers (OPT and Pythia families) using sliding window contexts. Intrinsic dimensionality is computed using GRIDE on random 20-word contexts from The Pile. Encoding models are trained using ridge regression to map LLM representations to brain activity with hemodynamic delays. Surprisal is measured using TunedLens. The primary analysis correlates encoding performance with intrinsic dimensionality across layers, subjects, and models.

## Key Results
- Encoding performance correlates with intrinsic dimensionality (ρ = 0.85) across LLM layers
- Peak encoding performance occurs at the transition point between composition and prediction phases
- The composition phase compresses into fewer layers as models scale up during training
- Next-token surprisal decreases sharply at the encoding performance peak, marking the phase transition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoding performance correlates with intrinsic dimensionality because both reflect the richness of abstract compositional features extracted in early layers.
- Mechanism: As layers progress, the model builds increasingly complex, high-dimensional representations of linguistic structure. This "composition" phase produces manifolds with higher intrinsic dimensionality, which better capture the abstract features that fMRI signals in language areas respond to.
- Core assumption: fMRI voxel responses in language areas are driven by high-level compositional features rather than low-level lexical or surface-level information.
- Evidence anchors:
  - [abstract]: "we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs"
  - [section]: "Figure 1a shows the correlation between average encoding performance and normalized Id for various model sizes in from the OPT model family. The positive relationship, ρ = 0.85, between normalized Id and encoding performance suggests that in trained language models, the Id of layer activations captures abstract linguistic feature complexity"

### Mechanism 2
- Claim: The two-phase abstraction process naturally emerges during training, with the composition phase compressing into fewer layers as models scale up.
- Mechanism: During training, early layers progressively learn to build rich, abstract representations of language. As models become larger and more capable, this compositional learning becomes more efficient, compressing the phase into fewer layers while the prediction phase (focused on next-token output) improves.
- Core assumption: The model's learning dynamics inherently favor an initial buildup of compositional understanding before focusing on predictive accuracy.
- Evidence anchors:
  - [abstract]: "We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first 'composition' phase of this abstraction process is compressed into fewer layers as training continues"
  - [section]: "Figure 2 plots the encoding performance and Id across layers over the course of training for Pythia-6.9B... the characteristic Id peak emerges, and moreover, that Id generally grows for all layers over training... The location of the Id peak... changes over the course of training, eventually settling at the same layers for peak encoding performance"

### Mechanism 3
- Claim: Layerwise surprisal decreases sharply at the transition point between composition and prediction phases, marking where abstract representation building gives way to focused prediction.
- Mechanism: In early layers, the model is still building comprehensive representations. At the peak dimensionality (composition-prediction transition), the model has enough abstract understanding to make accurate next-token predictions, causing a sharp drop in surprisal. Beyond this point, layers refine predictions rather than build abstractions.
- Core assumption: The model's architecture and training objective create a natural trade-off between representation building and prediction accuracy.
- Evidence anchors:
  - [abstract]: "At the peak of encoding performance (red dashed line), the next-token prediction loss (blue curve) sharply decreases, corresponding with a decrease in encoding performance"
  - [section]: "Figure 1b overlays, for OPT-1.3b, the encoding performance, Id, and next-token prediction loss computed from each layer. Observe that encoding performance peaks at layer 17, which exactly marks the sharp downwards turn in prediction loss"

## Foundational Learning

- Concept: Intrinsic dimensionality and manifold learning
  - Why needed here: The paper uses intrinsic dimensionality (Id) to measure the complexity of abstract features in LLM representations, which correlates with brain encoding performance
  - Quick check question: If a representation manifold has high extrinsic dimension (D) but low intrinsic dimension (Id), what does this tell you about the data's underlying structure?

- Concept: Linear encoding models and ridge regression
  - Why needed here: The paper trains linear mappings from LLM representations to fMRI voxel responses using ridge regression to measure brain-model similarity
  - Quick check question: Why might a linear mapping be sufficient to capture brain responses to language, despite the nonlinear nature of both neural processing and LLM representations?

- Concept: Next-token surprisal and TunedLens
  - Why needed here: The paper measures layerwise surprisal using TunedLens to test whether prediction accuracy drives brain-model similarity, finding it doesn't explain the results
  - Quick check question: How does TunedLens differ from standard next-token prediction in measuring what information is encoded at intermediate layers?

## Architecture Onboarding

- Component map:
  - fMRI data → preprocessing → voxel response extraction
  - LLMs → layerwise representation extraction → dimensionality computation → surprisal computation → encoding model training
  - Analysis pipeline → correlation analysis across layers, subjects, and models

- Critical path:
  1. Extract representations from each LLM layer for the stimulus corpus
  2. Compute intrinsic dimensionality for each layer's representations
  3. Train encoding models (linear mappings) from each layer to brain responses
  4. Measure encoding performance (correlation with brain data)
  5. Correlate encoding performance with dimensionality across layers

- Design tradeoffs:
  - Using linear encoding models vs. nonlinear models: Linear models are more interpretable and show that brain-LLM similarity is in the linear representational geometry
  - Computing dimensionality on full model representations vs. task-specific subsets: Full representations capture general linguistic processing capabilities
  - Measuring surprisal with TunedLens vs. standard next-token prediction: TunedLens better isolates what information is encoded at intermediate layers

- Failure signatures:
  - If encoding performance peaks at output layers rather than intermediate layers: Would suggest brain is primarily driven by next-token prediction rather than abstract composition
  - If dimensionality and encoding performance are uncorrelated: Would indicate abstract compositional features don't drive brain-model similarity
  - If the composition-prediction transition isn't marked by a surprisal drop: Would suggest the two-phase process isn't driven by learning dynamics described

- First 3 experiments:
  1. Reproduce the correlation between encoding performance and intrinsic dimensionality for a single model (e.g., OPT-1.3B) to verify the basic relationship
  2. Test whether this correlation holds for different dimensionality measures (PCA, PR) to confirm it's not an artifact of the specific estimation method
  3. Analyze the layerwise surprisal curve to identify the composition-prediction transition and verify it coincides with the encoding performance peak

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the two-phase abstraction process observed in language models also occur in other types of neural networks trained on different tasks?
- Basis in paper: [inferred] The paper demonstrates a two-phase process in language models but does not test whether this phenomenon generalizes to other architectures or tasks.
- Why unresolved: The study is limited to transformer-based language models and does not explore whether the composition-prediction split is unique to language processing or a more general property of neural networks.
- What evidence would resolve it: Testing the same manifold learning methods on convolutional networks, reinforcement learning agents, or models trained on non-linguistic tasks to see if similar dimensionality peaks and phase transitions emerge.

### Open Question 2
- Question: What specific linguistic features are represented in the high-dimensional compositional phase versus the lower-dimensional predictive phase?
- Basis in paper: [explicit] The paper notes that preceding layers extract high-level features related to syntax and semantics, but does not characterize these features in detail.
- Why unresolved: While the study identifies the existence of two phases, it does not perform detailed feature analysis to determine what linguistic information is captured in each phase.
- What evidence would resolve it: Probing experiments using feature attribution methods, activation patching, or targeted datasets to identify which linguistic phenomena (syntax, semantics, pragmatics, etc.) are processed in each phase.

### Open Question 3
- Question: How does the relationship between intrinsic dimensionality and encoding performance vary across different brain regions and cognitive domains?
- Basis in paper: [explicit] The paper shows voxelwise correlations but focuses primarily on language-related brain regions.
- Why unresolved: The study demonstrates the relationship in linguistic processing areas but does not systematically test whether this pattern holds for visual, auditory, or other cognitive domains.
- What evidence would resolve it: Applying the same encoding and dimensionality analysis to fMRI data from visual processing, auditory processing, or other cognitive tasks to determine if the dimensionality-encoding performance relationship is specific to language or a general property of cortical representations.

## Limitations

- Small fMRI sample size (3 subjects) and potential confounds from naturalistic stimuli that may engage multiple cognitive processes beyond pure language comprehension
- Assumes intrinsic dimensionality captures abstract compositional features, but this relationship could be influenced by other factors such as tokenization or specific linguistic phenomena
- Causal mechanism linking abstract composition to brain activity remains inferential despite robust empirical correlation

## Confidence

- **High Confidence**: The empirical observation that encoding performance correlates with intrinsic dimensionality across multiple model families and brain regions
- **Medium Confidence**: The interpretation that this correlation supports a two-phase abstraction process driven by shared compositional features rather than next-token prediction
- **Medium Confidence**: The claim that the composition-prediction transition naturally emerges during training and shifts to earlier layers as models scale

## Next Checks

1. Test whether the encoding-dimensionality correlation holds for controlled, task-specific stimuli (e.g., syntactic manipulations) to isolate linguistic processing from general comprehension processes
2. Analyze encoding performance and dimensionality for models trained on different objectives (masked language modeling vs. causal language modeling) to determine if the two-phase process is specific to next-token prediction
3. Conduct ablation studies removing specific linguistic features from the representations to identify which compositional properties drive the brain-LLM alignment