---
ver: rpa2
title: Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to Addressee
  Recognition and Response Selection in Conversations
arxiv_id: '2409.18602'
source_url: https://arxiv.org/abs/2409.18602
tags:
- speaker
- next
- conversation
- message
- addressee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a diagnostic pipeline for evaluating Multi-Party
  Conversation (MPC) understanding by isolating structural and textual information
  contributions. It frames Response Selection and Addressee Recognition as proxy tasks
  and creates diagnostic datasets with controlled user counts and structural variety.
---

# Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to Addressee Recognition and Response Selection in Conversations

## Quick Facts
- **arXiv ID**: 2409.18602
- **Source URL**: https://arxiv.org/abs/2409.18602
- **Reference count**: 40
- **Primary result**: AR performance improves with structural inputs while RS relies more on conversation text; prompt sensitivity is higher for AR than RS

## Executive Summary
This paper introduces a diagnostic pipeline for evaluating Multi-Party Conversation (MPC) understanding by isolating structural and textual information contributions. It frames Response Selection and Addressee Recognition as proxy tasks and creates diagnostic datasets with controlled user counts and structural variety. Experiments use Llama2-13b-chat in zero-shot setting with varied prompt verbosity. Results show AR performance improves with structural inputs, while RS relies more on conversation text. Prompt sensitivity is higher for AR, and structural complexity impacts performance, especially for AR. Macro-accuracy results alone are insufficient; detailed structural analysis reveals nuanced model limitations.

## Method Summary
The paper creates diagnostic datasets from Ubuntu IRC corpus with fixed user counts (3-6) and conversation lengths (â‰¤15 messages). It anonymizes users and constructs five input combinations: conversation transcripts, interaction transcripts, summaries, and user descriptions. Zero-shot classification tasks (Response Selection and Addressee Recognition) are performed using Llama2-13b-chat with three prompt verbosity levels. Structural complexity is measured via degree centrality and average outgoing weight. Performance is evaluated using macro accuracy and prompt sensitivity analysis.

## Key Results
- Addressee Recognition performance improves significantly when structural information is included, while Response Selection relies primarily on conversation text
- Prompt formulation has substantially higher impact on Addressee Recognition than Response Selection tasks
- Structural complexity metrics correlate with classification performance degradation, particularly for Addressee Recognition
- Detailed structural analysis reveals performance patterns that macro-accuracy alone cannot capture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structural information improves addressee recognition performance more than response selection.
- Mechanism: Addressee recognition relies on modeling the interaction graph (who talks to whom), while response selection depends primarily on linguistic cues in the conversation text.
- Core assumption: The interaction graph structure provides critical signals for determining the next addressee that are not available in the conversation text alone.
- Evidence anchors:
  - [abstract] "response selection relies more on the textual content of conversations, while addressee recognition requires capturing their structural dimension"
  - [section 7] "combinations containing the interaction transcript ( +STRUCT ) exhibit similar patterns, while the CONV combination displays distinct trends"
- Break condition: If the interaction graph becomes too sparse or random, structural information may provide no additional benefit over textual information alone.

### Mechanism 2
- Claim: LLM classification performance is more sensitive to prompt formulation for addressee recognition than response selection.
- Mechanism: Addressee recognition requires more precise understanding of the structural relationships between users, making it more dependent on explicit prompt instructions about the task format and expected output.
- Core assumption: Response selection tasks are more similar to standard language modeling tasks that LLMs are pre-trained on, making them less sensitive to prompt variations.
- Evidence anchors:
  - [section 7] "results on the AR task tend to be more sensitive to prompt formulation compared to the RS task, especially in the CONV+STRUCT combination"
  - [section 7] "the relative gap between the best run and the average results is remarkably larger for the AR task than for RS"
- Break condition: If the model architecture or pre-training corpus changes significantly, this sensitivity difference may disappear.

### Mechanism 3
- Claim: Structural complexity metrics (degree centrality and average outgoing weight) correlate with classification performance degradation in addressee recognition.
- Mechanism: As the next speaker interacts with more users (higher degree centrality) or sends messages to more users (higher average outgoing weight), the model has more candidates to disambiguate between, leading to decreased accuracy.
- Core assumption: The interaction graph structure captures meaningful patterns about speaker behavior that can be learned by the model.
- Evidence anchors:
  - [section 7] "higher deg(u) values consistently correspond to lower accuracy" and "the gap between the top-performing models and others widens significantly at lower deg(u) values"
  - [section 7] "wo_avg(u) suggests that having more messages directed towards the involved users may help determine the last addressee"
- Break condition: If the conversation structure becomes too random or if other factors dominate the decision-making process, these structural metrics may lose predictive power.

## Foundational Learning

- Concept: Zero-shot classification using conditional perplexity
  - Why needed here: The paper uses conditional perplexity to select the best candidate response or addressee without fine-tuning
  - Quick check question: How does conditional perplexity differ from simple probability scoring in this context?

- Concept: Interaction graph representation of multi-party conversations
  - Why needed here: The paper models conversations as both unweighted undirected and weighted directed graphs to capture different aspects of interaction structure
  - Quick check question: What information is preserved in the weighted directed graph that is lost in the unweighted undirected version?

- Concept: Prompt sensitivity analysis
  - Why needed here: The paper tests three different prompt verbosity levels to understand how sensitive the LLM is to instruction format
  - Quick check question: Why might a more verbose prompt improve performance for addressee recognition but not response selection?

## Architecture Onboarding

- Component map: Input preprocessing -> LLM generation -> Classification pipeline -> Evaluation
- Critical path: 1. Load and preprocess conversation data 2. Generate interaction transcripts and anonymize users 3. Create summaries and user descriptions using LLM 4. Construct prompts for each task and input combination 5. Run classification and calculate accuracy 6. Analyze results by structural metrics
- Design tradeoffs: Using zero-shot classification avoids fine-tuning costs but may limit performance; replacing original text with summaries improves privacy but may reduce accuracy; testing multiple prompt verbosity levels increases experimental complexity but provides robustness insights
- Failure signatures: Poor performance on STRUCT-only combinations for response selection indicates structural information is insufficient; high sensitivity to prompt variations for addressee recognition suggests the task is less well-defined; performance degradation with increasing structural complexity indicates model limitations
- First 3 experiments: 1. Run addressee recognition with CONV+STRUCT combination using verbose prompt to establish baseline 2. Compare addressee recognition performance across all three prompt verbosity levels for STRUCT combination 3. Test response selection with CONV combination using all three prompt verbosity levels to verify task independence

## Open Questions the Paper Calls Out
The paper identifies three open questions: (1) How do different LLMs compare in zero-shot performance on MPC tasks? (2) Does conversation length beyond 15 turns affect the relative importance of structural versus textual information? (3) What causes the performance gap between simple and complex interaction structures in addressee recognition, and can it be reduced?

## Limitations
- Experiments use only one LLM architecture (Llama2-13b-chat) in zero-shot settings, limiting generalizability
- Diagnostic datasets derived from single Ubuntu IRC corpus may introduce domain-specific biases
- Does not explore fine-tuning approaches that might yield different patterns of structural and textual information usage
- Prompt sensitivity analysis limited to three verbosity levels without exploring other prompt engineering dimensions

## Confidence

| Claim Cluster | Confidence Level |
| --- | --- |
| AR performance improves with structural inputs while RS relies more on conversation text | High |
| Prompt sensitivity differs between AR and RS tasks | Medium |
| Structural complexity metrics directly correlate with performance degradation | Low |

## Next Checks

1. **Cross-Architecture Validation**: Replicate core experiments using different LLM architectures (e.g., GPT-4, Claude) to verify whether structural vs. textual information patterns persist across model families.

2. **Fine-tuning Experiment**: Compare zero-shot performance with fine-tuned models on the same diagnostic datasets to determine if structural information advantage for AR persists when models are trained specifically on MPC tasks.

3. **Alternative Structural Metrics**: Test additional structural complexity measures (e.g., clustering coefficient, betweenness centrality) to determine if observed performance patterns are specific to degree centrality and average outgoing weight or reflect broader structural properties of MPCs.