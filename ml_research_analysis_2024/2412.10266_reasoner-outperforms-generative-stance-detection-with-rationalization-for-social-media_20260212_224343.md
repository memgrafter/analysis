---
ver: rpa2
title: 'Reasoner Outperforms: Generative Stance Detection with Rationalization for
  Social Media'
arxiv_id: '2412.10266'
source_url: https://arxiv.org/abs/2412.10266
tags:
- stance
- detection
- rationale
- language
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the interpretability challenge in stance detection
  by proposing a generative approach that produces both stance predictions and explicit
  rationales. The method leverages large language models (LLMs) to generate Chain-of-Thought
  (CoT) rationales conditioned on ground-truth labels, then distills these into smaller
  language models (SLMs) through single-task chain-of-thought (ST-CoT) and multitask
  learning (MTL) paradigms.
---

# Reasoner Outperforms: Generative Stance Detection with Rationalization for Social Media

## Quick Facts
- arXiv ID: 2412.10266
- Source URL: https://arxiv.org/abs/2412.10266
- Reference count: 36
- Key outcome: Multitask learning (MTL) outperforms single-task approaches in generative stance detection, with FlanT5 models achieving up to 9.57% improvement over GPT-3.5 zero-shot performance

## Executive Summary
This paper addresses the interpretability challenge in stance detection by proposing a generative approach that produces both stance predictions and explicit rationales. The method leverages large language models (LLMs) to generate Chain-of-Thought (CoT) rationales conditioned on ground-truth labels, then distills these into smaller language models (SLMs) through single-task chain-of-thought (ST-CoT) and multitask learning (MTL) paradigms. Experiments on the SemEval-2016 dataset show that MTL outperforms ST-CoT and single-task finetuning, with T5-based models achieving up to 9.57% improvement over GPT-3.5's zero-shot performance. MTL proves particularly effective in low-data scenarios, with models trained on 10-20% of the data performing comparably to full-data training. Instruction-tuned FlanT5 models consistently outperform their T5 counterparts, demonstrating the value of instruction-following capabilities.

## Method Summary
The approach uses GPT-3.5 to generate Chain-of-Thought rationales conditioned on ground-truth labels, then distills these into smaller language models (T5 and FlanT5) through single-task CoT (ST-CoT) and multitask learning (MTL) paradigms. The MTL approach jointly optimizes stance prediction and rationale generation using a weighted loss function (L = Œ± Lstance + (1 - Œ±) Lrationale). The method is evaluated on the SemEval-2016 Task 6 dataset with 4,163 English tweets across five topics, using macro-average F1-score for favor and against as the primary metric.

## Key Results
- Multitask learning (MTL) outperforms single-task chain-of-thought (ST-CoT) and single-task finetuning across all model configurations
- MTL is particularly effective in low-data scenarios, with models trained on 10-20% of the data performing comparably to full-data training
- Instruction-tuned FlanT5 models consistently outperform their T5 counterparts, demonstrating the value of instruction-following capabilities
- T5-based models achieve up to 9.57% improvement over GPT-3.5's zero-shot performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multitask learning (MTL) outperforms single-task approaches by jointly optimizing stance prediction and rationale generation, creating synergistic learning signals.
- Mechanism: MTL trains a unified model to simultaneously generate stance predictions and rationales using a weighted loss function (L = Œ± Lstance + (1 - Œ±) Lrationale). This forces the model to learn shared representations that benefit both tasks, creating intermediate reasoning steps that improve stance detection accuracy.
- Core assumption: The rationale generation task provides meaningful supervision that transfers to stance prediction, even though the tasks are distinct.
- Evidence anchors: [abstract] "our results show that reasoning capabilities enhance multitask learning performance but may reduce effectiveness in single-task settings"; [section] "MTL outperforms other methods, particularly in low-data scenarios"; [corpus] Weak - no direct corpus evidence, but related work on MTL in NLP supports this mechanism
- Break condition: If Œ± is set too high for rationale generation, the model may overfit to generating explanations at the expense of accurate stance classification.

### Mechanism 2
- Claim: Instruction-tuned models (FlanT5) inherently possess better reasoning capabilities that reduce the marginal benefit of rationale distillation.
- Mechanism: FlanT5 models are pre-trained on diverse instruction-following datasets, giving them built-in reasoning abilities that make them less dependent on explicit rationale supervision during finetuning.
- Core assumption: The instruction tuning process captures generalizable reasoning patterns that transfer to stance detection.
- Evidence anchors: [abstract] "Instruction-tuned FlanT5 models consistently outperform their T5 counterparts, demonstrating the value of instruction-following capabilities"; [section] "FlanT5, despite its smaller size, rivals or outperforms larger T5 models"; [corpus] Weak - corpus contains related work on instruction tuning but no direct evidence for stance detection
- Break condition: If the instruction-tuned model's reasoning patterns don't align with the specific reasoning required for stance detection in social media contexts.

### Mechanism 3
- Claim: Conditioning rationale generation on ground-truth labels improves faithfulness and coherence compared to direct CoT prompting.
- Mechanism: By providing the model with the correct stance label during rationale generation, the model is forced to generate explanations that logically support the given label rather than potentially contradictory reasoning.
- Core assumption: LLMs can generate more faithful rationales when explicitly constrained by ground-truth labels.
- Evidence anchors: [section] "we address by conditioning rationale generation on ground-truth labels to improve coherence"; [section] "direct CoT prompting often produces unfaithful rationales"; [corpus] Weak - no direct corpus evidence, but this is a novel approach in the paper
- Break condition: If the model learns to generate superficial rationales that don't actually reflect genuine reasoning about the relationship between comment and topic.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT reasoning allows models to generate intermediate reasoning steps that lead to more accurate and interpretable predictions in complex tasks like stance detection
  - Quick check question: How does CoT prompting differ from standard prompting in terms of the model's output structure?

- Concept: Multitask learning optimization
  - Why needed here: MTL enables the model to learn shared representations across related tasks (stance prediction and rationale generation), improving overall performance
  - Quick check question: What is the mathematical formulation of the MTL loss function used in this work?

- Concept: Instruction tuning and its effects
  - Why needed here: Understanding how instruction tuning improves reasoning capabilities helps explain why FlanT5 models perform better than standard T5 models
  - Quick check question: What types of datasets are typically used for instruction tuning large language models?

## Architecture Onboarding

- Component map: GPT-3.5 rationale generator -> T5/FlanT5 finetuner -> stance prediction + rationale output
- Critical path: Data preparation -> GPT-3.5 rationale elicitation -> SLM finetuning (MTL preferred) -> evaluation
- Design tradeoffs: Larger models provide better performance but require more compute; MTL provides better performance but requires careful weight tuning; instruction tuning improves performance but requires additional pre-training
- Failure signatures: Poor rationale quality -> incorrect predictions in ST-CoT; overfitting to rationale generation in MTL; instruction-tuned models showing less improvement from rationale distillation
- First 3 experiments:
  1. Compare zero-shot GPT-3.5 performance vs. fine-tuned T5 on stance detection without rationales
  2. Evaluate different Œ± values in MTL to find optimal balance between stance prediction and rationale generation
  3. Test low-data scenarios (10%, 20% of training data) to validate MTL's robustness claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different weighting strategies for the rationale generation task in MTL affect stance detection performance across diverse social media domains?
- Basis in paper: [explicit] The paper mentions examining the impact of parameter alpha (ùõº) for weighting prediction and rationale generation tasks in MTL, with optimal values varying across model sizes.
- Why unresolved: The paper only explores weighting within the same dataset (SemEval-2016) and doesn't examine cross-domain performance or alternative weighting strategies.
- What evidence would resolve it: Experiments testing multiple weighting strategies across diverse social media datasets with different topics, languages, and cultural contexts would reveal whether optimal weighting generalizes across domains.

### Open Question 2
- Question: What is the relationship between rationale faithfulness and downstream task performance in stance detection systems?
- Basis in paper: [explicit] The paper discusses conditioning rationale generation on ground-truth labels to improve coherence and faithfulness, but doesn't systematically evaluate how faithful rationales correlate with performance.
- Why unresolved: The paper focuses on performance metrics but doesn't analyze whether more faithful rationales lead to better predictions or whether unfaithful rationales harm performance.
- What evidence would resolve it: A systematic evaluation measuring rationale faithfulness (e.g., through human evaluation or automated metrics) alongside performance, examining correlation between the two measures across different prompting strategies.

### Open Question 3
- Question: How does rationale distillation performance scale with model size and training data availability in real-world deployment scenarios?
- Basis in paper: [inferred] The paper shows that T5-Base and T5-Large achieve comparable performance to full-data training with only 10-20% of the dataset under MTL, but doesn't explore the limits of this scalability.
- Why unresolved: The paper doesn't investigate the performance at extreme scales (very small models, very large models, or severely limited training data) or examine the computational trade-offs.
- What evidence would resolve it: A comprehensive scaling study testing rationale distillation across a wider range of model sizes (from tiny to massive) and training data quantities (from 1% to 100%), including computational efficiency metrics and performance degradation points.

## Limitations

- The effectiveness depends heavily on the quality of rationales generated by GPT-3.5, creating a potential single point of failure where poor quality rationales could propagate through to SLM training
- The evaluation is limited to a single dataset (SemEval-2016 Task 6) and doesn't address domain generalization across different social media platforms or evolving language patterns
- The study doesn't investigate the computational efficiency trade-offs between the proposed methods and simpler baselines

## Confidence

**High Confidence**: The core finding that MTL outperforms ST-CoT and single-task finetuning is well-supported by experimental results, with clear quantitative improvements across multiple model configurations and dataset sizes.

**Medium Confidence**: The claim about MTL's effectiveness in low-data scenarios is supported by experiments with 10-20% data, but the paper doesn't explore extremely low-data regimes (<5%) or examine how performance scales as data increases.

**Low Confidence**: The assertion that instruction-tuned models inherently possess better reasoning capabilities is based on empirical observation rather than theoretical grounding, and the paper doesn't investigate whether these capabilities transfer to other reasoning-intensive tasks.

## Next Checks

1. **Cross-dataset validation**: Evaluate the MTL-trained models on an independent stance detection dataset (e.g., Twitter stance detection datasets from other years or platforms) to test generalization beyond SemEval-2016.

2. **Human evaluation of rationales**: Conduct a blinded study where human annotators assess the faithfulness, coherence, and logical consistency of generated rationales against ground-truth labels to validate the quality of the distillation process.

3. **Efficiency benchmarking**: Measure and compare the inference latency and computational requirements of ST-CoT, MTL, and baseline approaches across different hardware configurations to assess practical deployment viability.