---
ver: rpa2
title: How Good is a Single Basin?
arxiv_id: '2402.03187'
source_url: https://arxiv.org/abs/2402.03187
tags:
- ensembles
- deep
- ensemble
- basin
- connectivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the performance of deep ensembles restricted
  to a single loss basin. The authors construct "connected ensembles" where ensemble
  members are constrained to lie in the same basin.
---

# How Good is a Single Basin?

## Quick Facts
- arXiv ID: 2402.03187
- Source URL: https://arxiv.org/abs/2402.03187
- Reference count: 18
- Key outcome: Connected ensembles within a single basin can approach deep ensemble performance through distillation, suggesting sufficient functional diversity exists within a single basin.

## Executive Summary
This paper investigates whether deep ensembles can be effectively constructed within a single loss basin rather than exploring multiple basins. The authors find that increased connectivity within a basin negatively impacts ensemble performance due to limited functional diversity. However, by incorporating knowledge from other basins through a distillation procedure, they can significantly mitigate this performance gap. The distilled connected ensembles achieve competitive results on CIFAR-10/100 and Tiny ImageNet, coming close to or matching multi-basin deep ensembles. This suggests that while a single basin may contain sufficient functional diversity for strong ensembles, it is difficult to harness without learning from other basins.

## Method Summary
The paper constructs "connected ensembles" where ensemble members are constrained to lie in the same loss basin. Three approaches are explored: Stochastic Weight Averaging (SWE), constrained ensembles trained with permuted batches, and distilled ensembles that transfer knowledge from other basins. The distillation procedure uses a temperature parameter and similarity encouragement term to balance connectivity and diversity. The method is evaluated on CIFAR-10/100 and Tiny ImageNet using ResNet20 and ViT architectures, with metrics including accuracy, linear mode connectivity, and diversity measures.

## Key Results
- Increased connectivity within a single basin negatively impacts ensemble performance compared to multi-basin deep ensembles
- Knowledge from other basins can be implicitly transferred to a single basin through distillation, significantly reducing the performance gap
- Distilled connected ensembles achieve competitive results, coming close to or matching the performance of multi-basin deep ensembles on image classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear mode connectivity within a basin limits predictive diversity, reducing ensemble performance.
- Mechanism: When ensemble members are constrained to a single basin, the functional space they can explore is limited. Linear mode connectivity means that any convex combination of their parameters lies in the same basin, preventing the ensemble from accessing the diverse predictive behaviors found in different basins.
- Core assumption: The loss landscape contains multiple functionally distinct basins, and visiting different basins is necessary for ensemble diversity.
- Evidence anchors:
  - [abstract] "increased connectivity negatively impacts performance"
  - [section] "we demonstrate that increased connectivity indeed negatively impacts performance"
- Break condition: If the basin is large enough or contains sufficient functional diversity, the performance gap could be smaller or nonexistent.

### Mechanism 2
- Claim: Knowledge from other basins can be implicitly transferred to a single basin through distillation.
- Mechanism: By distilling the predictions of models from different basins into a single basin, the single basin ensemble can learn to approximate the diverse behaviors found across multiple basins. This effectively transfers the knowledge without requiring explicit access to other basins.
- Core assumption: The functional diversity present in different basins is partially encoded within any single basin and can be extracted through distillation.
- Evidence anchors:
  - [abstract] "when incorporating the knowledge from other basins implicitly through distillation, we show that the gap in performance can be mitigated"
  - [section] "when incorporating other basins implicitly through a distillation procedure we manage to break this tradeoff and strongly reduce this gap"
- Break condition: If the functional diversity in different basins is not partially present in a single basin, distillation may not be effective.

### Mechanism 3
- Claim: The trade-off between connectivity and diversity can be broken by using a distillation approach.
- Mechanism: Distillation with a temperature parameter and a similarity encouragement term allows the single basin ensemble to achieve both high connectivity (linear mode connectivity) and high diversity (predictive diversity). The distillation objective encourages the model to match the predictions of other basins while staying within the single basin.
- Core assumption: The distillation objective can effectively balance the need for connectivity and diversity.
- Evidence anchors:
  - [abstract] "by re-discovering (multi-basin) deep ensembles within a single basin"
  - [section] "similarity encouragement to an out-of-basin model significantly boosts the accuracy of the connected ensemble"
- Break condition: If the distillation hyperparameters are not tuned correctly, the trade-off may not be broken.

## Foundational Learning

- Concept: Linear mode connectivity
  - Why needed here: Understanding the concept of linear mode connectivity is crucial for grasping the limitations of ensembles constrained to a single basin. It explains why simple convex combinations of parameters are detrimental and why ensembling predictions is more effective.
  - Quick check question: What is the key difference between averaging parameters and averaging predictions in the context of linear mode connectivity?

- Concept: Neural network loss landscapes
  - Why needed here: The multi-modal nature of neural loss landscapes is the foundation for understanding why deep ensembles work and why the proposed approach is significant. It explains the existence of multiple basins and the need for diverse models.
  - Quick check question: How does the multi-modal nature of neural loss landscapes contribute to the success of deep ensembles?

- Concept: Knowledge distillation
  - Why needed here: Knowledge distillation is the key technique used to transfer knowledge from other basins to a single basin. Understanding how distillation works and its effects on model performance is essential for understanding the proposed approach.
  - Quick check question: How does the temperature parameter in knowledge distillation affect the entropy of the distribution over class labels?

## Architecture Onboarding

- Component map: Data preprocessing and augmentation -> Model architecture (ResNet20, ViT) -> Ensemble construction (deep ensembles, connected ensembles) -> Distillation procedure -> Evaluation metrics (accuracy, connectivity, diversity)

- Critical path:
  1. Train deep ensembles with different initializations and batch orderings.
  2. Identify the stability point in training for the constrained ensemble approach.
  3. Construct connected ensembles using SWE, constrained ensembles, or distillation.
  4. Evaluate the performance and connectivity of the connected ensembles.
  5. Compare the results with deep ensembles.

- Design tradeoffs:
  - Connectivity vs. diversity: Increasing connectivity (linear mode connectivity) may reduce diversity and ensemble performance.
  - Distillation hyperparameters: Tuning the distillation hyperparameters (β, τ) is crucial for balancing connectivity and diversity.
  - Computational cost: Distillation can be computationally expensive compared to other ensemble methods.

- Failure signatures:
  - Low ensemble performance: May indicate insufficient diversity or ineffective distillation.
  - Lack of connectivity: May indicate the ensemble members are not in the same basin.
  - High computational cost: May indicate inefficient implementation or hyperparameter tuning.

- First 3 experiments:
  1. Replicate the deep ensemble results on CIFAR-10/100 and Tiny ImageNet to establish a baseline.
  2. Implement the SWE approach and compare its performance and connectivity with deep ensembles.
  3. Implement the constrained ensemble approach and compare its performance and connectivity with deep ensembles.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of connected ensembles change when using more than 5 ensemble members (M > 5)?
- Basis in paper: [inferred] The paper primarily uses M = 5 ensemble members. The computational cost is explicitly stated as CDE = M × T, suggesting scalability is a consideration.
- Why unresolved: The experiments are limited to ensembles of size M = 5. The impact of increasing ensemble size on connectivity, diversity, and overall performance is not explored.
- What evidence would resolve it: Experiments varying M (e.g., M = 10, 20, 50) and reporting connectivity, diversity metrics (JSD, variance), and accuracy for each case. Analysis of scaling behavior of computational cost versus performance gains.

### Open Question 2
- Question: Can the distillation procedure be made more computationally efficient while maintaining or improving performance?
- Basis in paper: [explicit] The paper notes that "relying on other basins renders our approach very inefficient" but demonstrates that distillation can significantly improve connected ensemble performance.
- Why unresolved: The distillation approach, while effective, requires access to a full deep ensemble and iterative training from a stability point. This is computationally expensive compared to single-model training.
- What evidence would resolve it: Development and evaluation of alternative distillation methods (e.g., one-time distillation, knowledge distillation from a single model, or using different teacher-student architectures) that reduce computational cost while achieving similar performance gains.

### Open Question 3
- Question: How does the choice of the stability point (t) affect the performance and connectivity of distilled ensembles, especially for architectures with less inductive bias like Vision Transformers?
- Basis in paper: [explicit] The paper investigates the impact of the time parameter t on ResNet20 performance and shows a trade-off between connectivity and performance. It also notes that "the picture is less clear for architectures with less inductive bias such as ViTs."
- Why unresolved: While the impact of t is explored for ResNet20, the behavior for ViTs is not fully characterized. The paper suggests that the relationship between t, connectivity, and performance may differ for architectures with different inductive biases.
- What evidence would resolve it: Systematic experiments varying t for ViTs and other architectures, reporting connectivity, diversity metrics, and accuracy. Analysis of how architectural differences (e.g., depth, width, normalization) influence the optimal choice of t.

## Limitations
- Empirical scope is limited to image classification tasks (CIFAR-10/100, Tiny ImageNet) with relatively shallow architectures
- The effectiveness of the approach for larger models, different tasks, or more complex architectures remains uncertain
- The distillation procedure relies on specific hyperparameter choices that may not transfer across datasets or architectures

## Confidence
- **High Confidence**: The negative impact of increased connectivity on ensemble performance is well-established through direct experiments and supported by theoretical reasoning about linear mode connectivity limiting functional diversity.
- **Medium Confidence**: The effectiveness of the distillation procedure in transferring knowledge from other basins is demonstrated empirically but relies on specific hyperparameter choices that may not generalize.
- **Low Confidence**: The claim that a single basin contains "sufficient" functional diversity for competitive ensembles is supported by the results but requires more extensive validation across diverse tasks and architectures.

## Next Checks
1. **Generalization to Larger Architectures**: Validate the connected ensemble approach with deeper ResNets (e.g., ResNet50) and larger ViTs on CIFAR-10/100 to assess scalability.

2. **Alternative Distillation Objectives**: Experiment with different distillation objectives (e.g., varying temperature schedules, KL divergence vs. cross-entropy) to determine the robustness of the approach to hyperparameter choices.

3. **Multi-task and Regression Datasets**: Test the connected ensemble method on regression tasks (e.g., UCI datasets) and multi-task learning problems to evaluate its broader applicability beyond image classification.