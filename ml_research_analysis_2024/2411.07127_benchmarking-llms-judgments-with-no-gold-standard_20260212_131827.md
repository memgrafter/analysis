---
ver: rpa2
title: Benchmarking LLMs' Judgments with No Gold Standard
arxiv_id: '2411.07127'
source_url: https://arxiv.org/abs/2411.07127
tags:
- review
- information
- evaluation
- gem-s
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce GEM, an evaluation metric for assessing language generation
  by LLMs, particularly in generating informative judgments, without the need for
  a gold standard reference. GEM uses a generative model to estimate mutual information
  between candidate and reference responses, without requiring the reference to be
  a gold standard.
---

# Benchmarking LLMs' Judgments with No Gold Standard

## Quick Facts
- arXiv ID: 2411.07127
- Source URL: https://arxiv.org/abs/2411.07127
- Reference count: 40
- Primary result: Introduces GEM, a mutual information-based evaluation metric that assesses LLM judgments without gold standards, showing competitive human correlation and superior manipulation resistance

## Executive Summary
This paper introduces GEM (Generative Estimator for Mutual Information), a novel evaluation metric for assessing LLM-generated judgments without requiring gold standard references. GEM estimates mutual information between candidate and reference responses using LLM probability distributions, proving theoretically that it can rank responses even when references aren't gold standards. The authors demonstrate that GEM and its conditional variant GEM-S achieve competitive correlations with human judgments while being more robust against strategic manipulations like rephrasing and elongation. They also introduce GRE-bench, a benchmark for evaluating LLM-generated peer reviews using GEM, which inherits its robustness properties and avoids data contamination by leveraging new open-access research papers.

## Method Summary
GEM estimates mutual information between candidate and reference responses using Shannon mutual information measured through pointwise mutual information (PMI) calculated from LLM probability distributions. The method doesn't require references to be gold standards, instead relying on Blackwell dominance to ensure proper ranking. GEM-S extends this by conditioning on a synopsis to filter out superficial shortcuts. Both metrics use LLM preprocessing to standardize language style and eliminate manipulation opportunities. The approach is validated on human-annotated datasets and extended to GRE-bench for evaluating LLM-generated peer reviews.

## Key Results
- GEM and GEM-S achieve Spearman correlations of 0.617 and 0.688 with human scores on peer review datasets, competitive with GPT-4o Examiner (0.702) while being more robust
- GEM and GEM-S are the only metrics showing no significant score increases after meaningless elongation manipulations (SMD = 0.06 and 0.09 vs GPT-4o Examiner SMD = 0.19)
- GRE-bench provides a contamination-free evaluation framework using continuously incoming open-access research papers and peer reviews

## Why This Works (Mechanism)

### Mechanism 1
GEM measures mutual information between candidate and reference responses to assess informativeness using Shannon mutual information estimated via pointwise mutual information calculated from LLM probability distributions. The core assumption is that reference responses don't need to be gold standard if candidate responses Blackwell dominate less informative ones. Proposition 3.1 proves higher Blackwell order information leads to higher GEM scores under certain assumptions. Break condition occurs when LLM fails to accurately estimate underlying probability distributions, causing KL-divergence > epsilon threshold.

### Mechanism 2
GEM-S conditions out synopsis to filter superficial shortcuts and focus on semantic informativeness using conditional mutual information I(X;Y|Z) where Z is synopsis. The core assumption is that responses have hierarchical information structure where superficial correlations can act as shortcuts. Preprocessor responses using LLM to rephrase and summarize content. Break condition occurs when synopsis doesn't capture relevant superficial information, making conditioning ineffective.

### Mechanism 3
Manipulation resistance comes from preprocessing that standardizes language style and eliminates superficial information. LLM preprocessing step rewrites responses in compressed format, removing stylistic variations while preserving semantic content. The core assumption is that manipulations that change language style or add meaningless content won't affect semantic content captured by mutual information. GEM and GEM-S exhibit no significant score increases after meaningless elongation. Break condition occurs when preprocessing fails to properly standardize language style or when manipulations introduce semantic changes.

## Foundational Learning

- Concept: Shannon mutual information
  - Why needed here: Forms theoretical foundation for measuring shared information between candidate and reference responses
  - Quick check question: What is the mathematical definition of mutual information between random variables X and Y?

- Concept: Blackwell order and information structures
  - Why needed here: Proves GEM can rank candidates even without gold standard references
  - Quick check question: Under what condition does information structure σH Blackwell dominate σL?

- Concept: Pointwise mutual information as unbiased estimator
  - Why needed here: Provides practical method for estimating mutual information from samples
  - Quick check question: How is PMI(x;y) mathematically related to mutual information I(X;Y)?

## Architecture Onboarding

- Component map: Input -> Preprocessing module -> Evaluation module -> Scoring module -> GRE-bench extension
- Critical path:
  1. Preprocess candidate and reference responses using GPT-4o
  2. Estimate Pr[Y=y|X=x] and Pr[Y=y] using Llama-3.1 evaluation-LM
  3. Compute PMI = log(Pr[Y=y|X=x]) - log(Pr[Y=y])
  4. Average over dataset to get GEM score
- Design tradeoffs:
  - Using larger evaluation-LM improves accuracy but increases cost
  - Preprocessing removes shortcuts but may lose some stylistic information
  - Conditional MI requires synopsis but provides more focused evaluation
- Failure signatures:
  - Low correlation with human annotations indicates broken mutual information estimation
  - Score increases after manipulations indicate preprocessing not working
  - Inconsistent results across evaluation-LM versions indicate model dependence
- First 3 experiments:
  1. Run on human-annotated peer grading dataset to verify positive correlation with human scores
  2. Test sensitivity to degradations (sentence deletion, deletion & completion, abstract-only)
  3. Test robustness against manipulations (GPT-4o/Llama-3.1 rephrasing, meaningless elongation)

## Open Questions the Paper Calls Out

The paper acknowledges several open questions including the need for stronger theoretical guarantees under weaker assumptions about the evaluation language model's distribution estimation, the challenge of scenarios where all reviewers rely heavily on LLM-generated content potentially leading to model collapse, and the optimal level of synopsis granularity for GEM-S metrics across different types of judgment tasks beyond academic peer review.

## Limitations
- Theoretical foundations rely on specific assumptions about Blackwell dominance and LLM probability estimation accuracy
- Manipulation resistance not theoretically guaranteed for all possible strategies
- Performance heavily dependent on quality of LLM used for probability estimation

## Confidence

**High Confidence**:
- GEM's ability to correlate with human judgments on peer review quality
- GRE-bench's utility for evaluating LLM-generated peer reviews
- Basic premise that mutual information can measure informativeness without gold standards

**Medium Confidence**:
- Robustness against specific manipulation strategies tested
- Relative performance compared to GPT-4o Examiner across all scenarios
- Preprocessing effectiveness for eliminating superficial information

**Low Confidence**:
- Performance guarantees under all possible manipulation strategies
- Scalability to domains beyond academic peer review
- Long-term stability as evaluation-LMs evolve

## Next Checks
1. Test GEM and GEM-S on non-academic domains (legal judgments, medical diagnoses) to verify generalizability beyond peer reviews
2. Systematically explore manipulation strategies that combine semantic changes with stylistic variations to identify potential weaknesses in preprocessing
3. Compare performance across different LLM architectures (Claude, Gemini, smaller Llamas) and sizes to quantify sensitivity to model choice and establish minimum viable configurations