---
ver: rpa2
title: 'Once More, With Feeling: Measuring Emotion of Acting Performances in Contemporary
  American Film'
arxiv_id: '2411.10018'
source_url: https://arxiv.org/abs/2411.10018
tags:
- emotion
- emotional
- film
- performance
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies computational methods to analyze emotional variation
  in film performances. The authors align spoken performances with their scripts and
  use a speech emotion recognition model to measure emotions across 2,283 contemporary
  American films.
---

# Once More, With Feeling: Measuring Emotion of Acting Performances in Contemporary American Film

## Quick Facts
- arXiv ID: 2411.10018
- Source URL: https://arxiv.org/abs/2411.10018
- Reference count: 40
- Key outcome: Computational analysis reveals emotional patterns in 2,283 contemporary American films, showing emotionality increases over narrative time while slightly decreasing over decades

## Executive Summary
This paper applies computational methods to analyze emotional variation in film performances by aligning spoken performances with their scripts and using speech emotion recognition to measure emotions across 2,283 contemporary American films. The authors find that emotionality increases over narrative time with specific patterns for different emotions, while contrary to theories of intensifying visual style, emotionality has slightly decreased over time. They also develop a novel measure of emotional range showing that different genres have different capacities for emotional expression, and that functional dialogue phrases have less emotional range than evaluative ones.

## Method Summary
The authors construct a parallel dataset of spoken performances aligned with screenplay text by extracting audio from film videos, segmenting continuous speech into utterances, transcribing and time-aligning with script text, and clustering semantically similar phrases. They apply a speech emotion recognition model trained on acted speech data to measure emotion in each utterance, then analyze patterns across narrative time, genres, and decades using statistical methods including a novel emotional range measure based on entropy of emotion distributions.

## Key Results
- Emotionality increases over narrative time, with joy peaking at endings and anger peaking near climaxes
- Contrary to intensifying visual style theories, emotionality has slightly decreased over time
- Different genres show different capacities for emotional expression, with functional dialogue having less emotional range than evaluative phrases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Speech emotion recognition models trained on acted speech can generalize to professionally acted film performances
- Mechanism: The authors use a wav2vec2-based emotion recognition model trained on the MELD dataset (Friends TV show) and find comparable performance when applied to film utterances
- Core assumption: The distribution of acted emotions in TV show dialogues matches the distribution in film performances
- Evidence anchors:
  - [abstract] "we measure acted emotion from speech, allowing us to disentangle the emotion present in the script from the choices made in creating the performance"
  - [section 2.2.3] "we evaluate these models on the test split of the MELD dataset as well as a manually collected dataset consisting of 333 clips from a subset of 35 contemporary American films"
  - [corpus] Weak - the corpus shows related papers on emotion recognition but no direct evidence about generalization from TV to film
- Break condition: Performance degrades significantly on films with different acting styles or emotional expressions than the training data

### Mechanism 2
- Claim: Aligning speech segments with script text enables analysis of the gap between written and performed emotion
- Mechanism: The pipeline segments audio, transcribes it, aligns it with screenplay text, and clusters semantically similar phrases to study how the same words are performed differently
- Core assumption: Time-aligned transcriptions accurately capture what is spoken and can be reliably matched to screenplay text
- Evidence anchors:
  - [abstract] "we construct a parallel dataset of spoken performances (utterances) aligned with the text of the words being spoken (dialogue phrases)"
  - [section 2.1] "we then use a speech-to-text fine-tuned wav2vec23 to perform word-level time alignment between the transcription and the audio"
  - [section 2.3] "We use the sentence-transformers library to compute sentence embeddings of utterances and cluster them with the Leiden community detection algorithm"
- Break condition: Misalignment between speech and text breaks the analysis of performance variation

### Mechanism 3
- Claim: Emotional range can be quantified as the entropy of emotion distributions across utterances
- Mechanism: Each utterance gets an emotion probability distribution from the recognition model, and the entropy of these distributions across a set of utterances measures emotional range
- Core assumption: The emotion probability distributions capture meaningful variation in performance that reflects emotional range
- Evidence anchors:
  - [abstract] "they develop a novel measure of emotional range showing that different genres have different capacities for emotional expression"
  - [section 3.3] "We characterize each utterance ùë¢ùëñ with a performance vector ùë£‚Éóùëñ, which is a distribution over emotions, given by the predicted probability distribution from the speech emotion recognition model"
  - [section 3.3] "We model the distribution from which the vectors ùë£‚Éó1...ùëõ are drawn as a Dirichlet, and find the parameters which maximize the likelihood of the observed vectors. We define emotional range as the entropy of this distribution"
- Break condition: If the emotion model fails to capture nuanced emotional variation, the entropy measure won't reflect true emotional range

## Foundational Learning

- Concept: Speech emotion recognition and its limitations
  - Why needed here: The entire analysis depends on accurately recognizing emotions from speech, which is a complex task with known challenges
  - Quick check question: What are the main criticisms of the Ekman emotional model used in this work?

- Concept: Alignment of multimodal data (speech and text)
  - Why needed here: The key innovation is studying the gap between written text and performed speech, which requires precise alignment
  - Quick check question: What technical challenges arise when aligning transcribed speech with screenplay text?

- Concept: Entropy as a measure of diversity/range
  - Why needed here: The novel emotional range measure uses entropy to quantify the variety of emotional expressions
  - Quick check question: How does entropy capture the concept of "range" in emotional performances?

## Architecture Onboarding

- Component map:
  Video processing pipeline (ffmpeg) ‚Üí Audio extraction
  Speech segmentation (pyannote) ‚Üí Continuous speech segments
  Transcription (faster-whisper) ‚Üí Text with time information
  Word alignment (wav2vec2) ‚Üí Time-aligned words
  Sentence segmentation (syntok) ‚Üí Utterances
  Emotion recognition (wav2vec2 + classification head) ‚Üí Emotion probabilities
  Text embedding (sentence-transformers) ‚Üí Semantic representations
  Clustering (Leiden) ‚Üí Dialogue phrase groups
  Statistical analysis ‚Üí Findings on emotional patterns

- Critical path: Video ‚Üí Aligned utterances + emotion labels + phrase groups ‚Üí Analysis
- Design tradeoffs:
  - Using acted speech training data vs. natural speech (better match for films but less generalizable)
  - Segmenting by sentence boundaries vs. more complex discourse units (simpler but may miss some nuances)
  - Using 7 basic emotions vs. more fine-grained or continuous models (easier to train but may be too coarse)

- Failure signatures:
  - Poor alignment between speech and text (check word-level alignment accuracy)
  - Emotion model confusion between similar emotions (examine confusion matrix on validation data)
  - Clustering produces too many/few phrase groups (adjust similarity threshold or clustering parameters)

- First 3 experiments:
  1. Validate the alignment pipeline on a small set of films by manually checking transcriptions against speech
  2. Test the emotion recognition model on the held-out movie evaluation set to confirm generalization
  3. Verify the emotional range measure by calculating it on synthetic data with known emotional variation

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis relies on speech emotion recognition models trained on acted speech data, which may not generalize well to diverse acting styles across contemporary American cinema
- The MELD dataset represents TV dialogue rather than film performances, creating potential distributional mismatches
- Time-alignment process between transcriptions and screenplay text could introduce errors that compound through the analysis pipeline

## Confidence

**High confidence**: The finding that emotionality increases over narrative time, with specific emotion patterns (joy at endings, anger near climaxes). This aligns with established narrative theories and shows robust statistical patterns across thousands of films.

**Medium confidence**: The claim that emotionality has slightly decreased over time despite intensifying visual style. While statistically supported, this requires careful interpretation as it only measures one dimension of performance (emotion from speech) and may not capture broader changes in cinematic style.

**Medium confidence**: The emotional range measure and genre differences. The novel methodology shows promise, but the measure's sensitivity to the underlying emotion recognition model's performance and clustering parameters introduces uncertainty.

## Next Checks

1. **Cross-dataset validation**: Test the emotion recognition model on multiple acted speech datasets beyond MELD (including theatrical performances and other TV shows) to establish generalization bounds before applying to films.

2. **Alignment accuracy audit**: Manually verify time-alignment quality on a stratified sample of 50 films across different genres, dialogue densities, and release eras to quantify and correct for alignment errors.

3. **Model sensitivity analysis**: Systematically vary the number of emotion categories, clustering thresholds, and Dirichlet distribution parameters to assess the stability of the emotional range measure and narrative pattern findings.