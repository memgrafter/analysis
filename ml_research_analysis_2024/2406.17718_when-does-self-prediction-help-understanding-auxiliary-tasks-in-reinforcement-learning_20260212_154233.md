---
ver: rpa2
title: When does Self-Prediction help? Understanding Auxiliary Tasks in Reinforcement
  Learning
arxiv_id: '2406.17718'
source_url: https://arxiv.org/abs/2406.17718
tags:
- learning
- observation
- span
- assumption
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes auxiliary tasks in reinforcement learning,
  focusing on observation reconstruction and latent self-prediction. It introduces
  a theoretical framework to study these tasks' behavior under linear assumptions,
  considering distractions and observation functions in MDPs.
---

# When does Self-Prediction help? Understanding Auxiliary Tasks in Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.17718
- Source URL: https://arxiv.org/abs/2406.17718
- Reference count: 40
- This paper analyzes auxiliary tasks in reinforcement learning, focusing on observation reconstruction and latent self-prediction. It introduces a theoretical framework to study these tasks' behavior under linear assumptions, considering distractions and observation functions in MDPs.

## Executive Summary
This paper investigates when auxiliary tasks help in reinforcement learning by analyzing observation reconstruction and latent self-prediction through a theoretical lens. The authors develop a framework that characterizes how these tasks behave under linear assumptions, considering distractions and observation functions in MDPs. Their analysis reveals that latent self-prediction's features converge to eigenvectors of the transition matrix, while observation reconstruction converges to singular vectors. The paper bridges the theory-practice gap by showing that these insights hold in non-linear neural networks, as validated by empirical experiments on MinAtar and DMC benchmark suites.

## Method Summary
The authors introduce a theoretical framework based on linear models and learning dynamics to analyze the behavior of auxiliary tasks in reinforcement learning. They adapt the Double DQN architecture and TD3 algorithm with additional encoder and prediction head components. The method involves implementing the theoretical framework for analyzing learning dynamics, running experiments on the MinAtar and DMC 15 suites with different distraction models and observation space distortions, and comparing the performance of latent self-prediction and observation reconstruction as auxiliary tasks and standalone feature learning methods.

## Key Results
- Latent self-prediction features converge to top-k eigenvectors of the transition matrix
- Observation reconstruction converges to singular vectors of the transition matrix
- Combining latent self-prediction with TD learning preserves TD optimality while adding spectral regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent self-prediction converges to invariant subspaces spanned by top-k eigenvectors of the transition matrix.
- Mechanism: The learning dynamics drive the encoder Φ to align with directions in state space that have the highest influence on future state transitions, effectively capturing the principal modes of the Markov process.
- Core assumption: Positive real eigenvalues of the transition matrix and two-timescale learning (F updates much faster than Φ).
- Evidence anchors:
  - [abstract] "Our analysis reveals that latent self-prediction's features converge to eigenvectors of the transition matrix"
  - [section] "If the columns of Φt span an invariant subspace of Pπ, Φt is a stationary point... all invariant subspaces not spanned by the top-k eigenvectors... are asymptotically unstable"
  - [corpus] Weak evidence; neighbor papers focus on different domains.
- Break condition: If the transition matrix has negative or complex eigenvalues, or if the two-timescale assumption breaks down due to similar learning rates.

### Mechanism 2
- Claim: Observation reconstruction converges to singular vectors (left/right) of the transition matrix.
- Mechanism: The reconstruction loss explicitly matches predicted next state observations, forcing the encoder-decoder pair to span the most informative low-rank directions for reconstructing future states.
- Core assumption: Linear reparameterization with invertible observation matrix O, and the two-timescale regime.
- Evidence anchors:
  - [abstract] "observation reconstruction converges to singular vectors"
  - [section] "Any stationary point... satisfies span (Φ∗) = span ({u1,...,uk}), span (Ψ∗⊤) = span ({v1,...,vk})"
  - [corpus] Weak evidence; neighbor papers do not directly address singular vector convergence.
- Break condition: When the observation matrix O is poorly conditioned or when the reward structure aligns poorly with singular subspaces.

### Mechanism 3
- Claim: Combining latent self-prediction with TD learning preserves TD optimality while adding spectral regularization.
- Mechanism: The joint loss retains the critical point of pure TD learning (which can achieve zero value function error) and further regularizes Φ toward the top eigenspace, improving generalization without sacrificing correctness.
- Core assumption: The reward function lies in the span of a small subset of eigenvectors of Pπ, and Φ is orthonormal.
- Evidence anchors:
  - [abstract] "combining LTD and LLat does not exclude the existence of a stationary point with 0 value function approximation error"
  - [section] "There exists a non-trivial critical point Φ∗ of the two-timescale TD loss... Furthermore, Φ∗ is a critical point of the two-timescale joint loss Ltd+lat"
  - [corpus] Weak evidence; neighbor papers focus on different learning paradigms.
- Break condition: If the reward does not lie in the span of any eigenspace covered by latent self-prediction, or if Φ is not orthonormal.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formalism and transition matrices.
  - Why needed here: The entire analysis hinges on spectral properties of Pπ, which are only defined for MDPs with fixed policies.
  - Quick check question: Given Pπ = [[0.9, 0.1], [0.2, 0.8]], what are its eigenvalues and corresponding eigenvectors?

- Concept: Linear algebra—eigenvectors, singular vectors, Kronecker products.
  - Why needed here: Proofs rely on decomposing Pπ into spectral components and understanding how distractions compose via Kronecker products.
  - Quick check question: For Pπ = [[2, 0], [0, 1]], list the top-k eigenvectors for k=1.

- Concept: Ordinary differential equations (ODEs) and stability analysis.
  - Why needed here: The learning dynamics are modeled as gradient flows; stability of critical points determines which features are learned.
  - Quick check question: If the Jacobian at a critical point has eigenvalues {-2, 1}, is the point stable?

## Architecture Onboarding

- Component map:
  - Encoder Φ: Maps observations to k-dimensional latent space
  - Latent model F: Predicts next state's latent embedding from current
  - Decoder Ψ: Reconstructs next state observations from latent prediction
  - Value head ˆV: Estimates state values (for TD loss)
  - Stop-gradient ops: Prevent cycles during backprop

- Critical path:
  1. Sample batch of (x, x′, r) from replay buffer
  2. Compute latent prediction: Φ(x) → F → Φ(x′)̂
  3. Compute losses: latent self-prediction, observation reconstruction, TD
  4. Backpropagate all through encoder; propagate only TD through value head
  5. Update target networks periodically

- Design tradeoffs:
  - Two-timescale vs single-timescale: Faster F convergence stabilizes latent prediction but may miss transient dynamics
  - Linear vs nonlinear encoders: Linear gives provable convergence; nonlinear improves empirical performance but loses guarantees
  - Hard target sync vs soft updates: Hard sync (as in the paper) aligns with theory but can cause instability if F changes too fast

- Failure signatures:
  - Encoder collapses to rank < k → check rank(Φ⊤Φ)
  - TD error plateaus while auxiliary loss decreases → value function not being learned
  - High variance in loss curves → learning rates mismatched or batch size too small

- First 3 experiments:
  1. Verify linear autoencoder recovers top-k singular vectors on a synthetic Pπ
  2. Test latent self-prediction vs TD alone on a simple MDP with known reward eigenstructure
  3. Introduce a distraction via Kronecker product and measure impact on each auxiliary task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do auxiliary tasks behave when combined with a TD loss in the presence of distractions and observation functions?
- Basis in paper: [explicit] The authors explicitly pose this as one of their two main questions in the introduction.
- Why unresolved: The paper provides theoretical analysis under linear assumptions but acknowledges that the impact of combining auxiliary tasks with TD learning in more complex scenarios remains unclear.
- What evidence would resolve it: Empirical studies testing various combinations of auxiliary tasks and TD learning across different environments with varying levels of distractions and observation functions, particularly in non-linear neural network settings.

### Open Question 2
- Question: How can we describe the behavior of auxiliary losses in the presence of distractions and observation functions?
- Basis in paper: [explicit] The authors explicitly pose this as one of their two main questions in the introduction.
- Why unresolved: While the paper introduces formalizations for distractions and observation functions, it acknowledges that the full implications of these structures on auxiliary task performance are not yet fully understood.
- What evidence would resolve it: Comprehensive empirical studies across diverse environments with varying distraction models and observation functions, coupled with theoretical analysis extending beyond linear assumptions.

### Open Question 3
- Question: What is the impact of off-policy samples and shifting policies on the performance of auxiliary tasks?
- Basis in paper: [inferred] The authors mention in the limitations section that their theoretical work is conducted in the on-policy policy evaluation regime, while their empirical study includes both off-policy policy estimation and policy improvement.
- Why unresolved: The paper acknowledges that studying the impact of off-policy samples and shifting policies is an important step for future work, indicating that this aspect is not fully addressed in the current research.
- What evidence would resolve it: Empirical studies comparing the performance of auxiliary tasks in on-policy and off-policy settings, as well as in environments with stable and shifting policies, across various benchmark suites.

## Limitations

- The theoretical analysis relies heavily on linear assumptions that may not fully capture the complexity of real-world RL problems.
- The analysis assumes two-timescale learning dynamics, which may not hold in practice due to hyperparameter tuning challenges.
- The focus on single-task MDPs limits generalizability to multi-task or meta-learning scenarios.

## Confidence

**High Confidence**: The convergence results for latent self-prediction to top-k eigenvectors under linear assumptions are well-supported by stability analysis of the gradient flow dynamics. The observation that combining LTD and LLat preserves TD optimality while adding spectral regularization is theoretically rigorous.

**Medium Confidence**: The extension of linear insights to non-linear neural networks in empirical experiments, while promising, lacks theoretical justification for why the spectral properties would transfer. The performance comparisons across different distraction models are empirically sound but may be sensitive to specific choices of distraction strength and structure.

**Low Confidence**: The claims about observation reconstruction being "superior when used alone" are primarily based on specific synthetic setups and may not generalize across diverse MDP structures. The theoretical framework's applicability to continuous control tasks (DMC suite) is less established than for discrete control (MinAtar).

## Next Checks

1. **Spectral Transfer Validation**: Design an experiment where a non-linear encoder is trained with latent self-prediction on a synthetic MDP, then analyze the learned representation's alignment with the true eigenvectors using singular value decomposition of the encoder weight matrix.

2. **Timescale Robustness Test**: Systematically vary the learning rate ratio between F and Φ across multiple orders of magnitude to identify the minimum ratio required for stable convergence to the top eigenspace, and measure performance degradation when this ratio is violated.

3. **Reward Eigenstructure Dependence**: Construct MDPs where the reward function lies in different eigenspaces (e.g., middle vs top eigenvectors) and measure whether latent self-prediction still provides benefits compared to TD learning alone, testing the assumption about reward eigenstructure.