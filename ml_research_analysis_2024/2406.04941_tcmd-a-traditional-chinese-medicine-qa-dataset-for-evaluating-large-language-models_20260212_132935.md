---
ver: rpa2
title: 'TCMD: A Traditional Chinese Medicine QA Dataset for Evaluating Large Language
  Models'
arxiv_id: '2406.04941'
source_url: https://arxiv.org/abs/2406.04941
tags:
- llms
- medical
- arxiv
- questions
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TCMD is a new TCM QA dataset for evaluating LLMs on Traditional
  Chinese Medicine exam tasks. It contains 4 types of multiple-choice questions collected
  from CNMLE with manual subject annotations.
---

# TCMD: A Traditional Chinese Medicine QA Dataset for Evaluating Large Language Models

## Quick Facts
- arXiv ID: 2406.04941
- Source URL: https://arxiv.org/abs/2406.04941
- Reference count: 38
- TCMD dataset enables robust evaluation of LLMs on TCM multiple-choice questions

## Executive Summary
TCMD is a new Traditional Chinese Medicine QA dataset designed to evaluate large language models on CNMLE exam tasks. The dataset contains 4 types of multiple-choice questions collected from CNMLE with manual subject annotations, covering 5 realms and 18 subjects with balanced distribution per official guidelines. Evaluation of 8 general, 1 medical, and 2 TCM-specific LLMs reveals that general LLMs perform better despite lacking domain tuning. Moonshot-v1-8k achieves highest accuracy (0.793), and several models pass human-level thresholds. Robustness tests show poor consistency under shuffled options, though ensemble methods can improve performance.

## Method Summary
The TCMD dataset was created by collecting questions from the Chinese National Medical Licensing Examination, then manually verifying and annotating them for subject classification. Questions were classified into four types (A1, A2, A3, B1) and distributed according to official exam guidelines across five realms and 18 subjects. The evaluation used In-Context Learning with Rank-BM25 retrieval for examples and tested both ICL and Chain-of-Thought prompting strategies. Robustness was assessed by shuffling answer options and applying ensemble voting methods to evaluate consistency and potential performance improvements.

## Key Results
- Moonshot-v1-8k achieves highest accuracy at 0.793 on the TCMD test set
- General LLMs outperform medical (0.451) and TCM-specific (0.476) LLMs on average
- Poor consistency under shuffled options (41.1% average) reveals LLM sensitivity to input formatting
- Ensemble methods improve accuracy to 0.493 but still below general LLM performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's balanced distribution across TCM realms and subjects improves evaluation reliability.
- Mechanism: By following official exam guidelines to distribute questions proportionally across five realms and 18 subjects, the dataset avoids overrepresentation of any single domain, enabling fairer model assessment.
- Core assumption: The official exam structure accurately reflects the importance and knowledge distribution required for TCM proficiency.
- Evidence anchors:
  - [abstract]: "covers 5 realms and 18 subjects with balanced distribution per official guidelines"
  - [section]: "To ensure that the dataset covers all the subjects and the test set follows the distribution from the manual instruction, we first invite some experts to label the subject of each collected problem."
  - [corpus]: Weak evidence - neighboring papers mention balanced benchmarks but not specific adherence to official guidelines.
- Break condition: If the official guidelines are outdated or misaligned with actual TCM practice needs, the balance may not reflect real-world importance.

### Mechanism 2
- Claim: Including multiple question types (A1, A2, A3, B1) captures different reasoning demands on LLMs.
- Mechanism: Different question formats require different cognitive skills - single choice vs. shared-stem vs. shared-options questions test recall, contextual reasoning, and concept discrimination respectively.
- Core assumption: The four question types sufficiently span the cognitive complexity spectrum needed to evaluate TCM knowledge.
- Evidence anchors:
  - [abstract]: "contains 4 types of multiple-choice questions"
  - [section]: "According to the part for TCM, there are 4 types of questions: – A1 \A2: a single multiple-choice problem with a statement or a clinical case. – A3 : several questions with a shared stem consisting of a clinical history. – B1 : several questions with shared options containing interrelated concepts."
  - [corpus]: Weak evidence - neighboring papers don't specify question type diversity as a key feature.
- Break condition: If certain reasoning skills are critical but not captured by these four types, the evaluation will miss important capabilities.

### Mechanism 3
- Claim: Robustness testing through option shuffling reveals LLM sensitivity to input formatting rather than knowledge.
- Mechanism: By testing models on all 120 permutations of options for each question, the evaluation distinguishes between true knowledge and pattern-matching behavior that breaks with input changes.
- Core assumption: Consistency across permutations indicates genuine knowledge rather than memorization of answer positions.
- Evidence anchors:
  - [abstract]: "Robustness tests reveal poor consistency under shuffled options, though ensemble methods can improve performance"
  - [section]: "To validate the robustness of LLMs, we review our preparation process for an exam during which we may exercise countless questions. To ensure that we grasp the question instead of simply memorizing it, we may shuffle the options and check the consistency of our answers."
  - [corpus]: Weak evidence - neighboring papers don't mention permutation-based robustness testing.
- Break condition: If shuffling creates unnatural permutations that don't reflect real-world variations, the test may be too stringent.

## Foundational Learning

- Concept: Multiple-choice question evaluation methodology
  - Why needed here: The dataset uses MCQA format, requiring understanding of how to construct, score, and analyze multiple-choice questions for LLM evaluation.
  - Quick check question: What are the key differences between single-answer and multi-answer MCQA evaluation approaches?

- Concept: Traditional Chinese Medicine knowledge domains
  - Why needed here: The dataset covers five realms and 18 subjects of TCM, requiring familiarity with these areas to understand what the evaluation tests.
  - Quick check question: Can you name the five realms covered in the CNMLE and their approximate percentage distributions?

- Concept: LLM prompting strategies (ICL vs Chain-of-Thought)
  - Why needed here: The evaluation uses different prompting methods, and results show CoT doesn't always help, requiring understanding of when each approach works.
  - Quick check question: Under what conditions might Chain-of-Thought prompting actually hurt performance on domain-specific tasks?

## Architecture Onboarding

- Component map: Dataset collection → Manual verification → Subject annotation → Question type classification → Balanced sampling → Test generation → LLM evaluation → Robustness testing → Ensemble analysis
- Critical path: Question collection and verification → Subject annotation → Balanced distribution → LLM testing pipeline
- Design tradeoffs: The dataset prioritizes breadth (5 realms, 18 subjects) over depth (600 test questions total), trading comprehensive coverage for manageable evaluation size.
- Failure signatures: Poor consistency across shuffled options, domain-specific underperformance despite general LLM success, CoT not improving results.
- First 3 experiments:
  1. Run baseline evaluation on a general LLM with ICL and CoT to establish reference performance
  2. Test robustness by shuffling options on a subset of questions to check consistency
  3. Apply ensemble voting on shuffled results to see if accuracy improves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ensemble methods with different voting thresholds affect the trade-off between accuracy and consistency for TCM question answering?
- Basis in paper: [explicit] The paper states that ensemble methods can alleviate inconsistency issues and may boost performance if the threshold is very low, but also shows that accuracy drops as the threshold rises.
- Why unresolved: The paper only tests one threshold-based voting mechanism without exploring the full parameter space of ensemble methods or alternative aggregation strategies.
- What evidence would resolve it: Systematic evaluation of different ensemble configurations (majority voting, weighted voting, model-specific thresholds) across multiple datasets to determine optimal trade-offs.

### Open Question 2
- Question: Why do medical and TCM-specific LLMs underperform general LLMs despite being trained on domain-specific data?
- Basis in paper: [explicit] The paper notes that general LLMs perform better than medical and TCM LLMs on average, and suggests this may be due to catastrophic forgetting of multiple-choice question abilities during medical/TCM tuning.
- Why unresolved: The paper hypothesizes about catastrophic forgetting but does not conduct ablation studies or investigate the specific training data composition differences between model types.
- What evidence would resolve it: Comparative analysis of training data content, fine-tuning procedures, and task-specific capabilities across general, medical, and TCM LLMs.

### Open Question 3
- Question: Can Chain-of-Thought prompting be effectively adapted for TCM question answering to overcome the limitations identified?
- Basis in paper: [explicit] The paper finds that Chain-of-Thought does not universally improve performance and may even reduce it due to LLMs' lack of understanding of TCM concepts.
- Why unresolved: The paper only tests standard Chain-of-Thought prompting without exploring domain-specific prompting strategies or knowledge injection techniques that might improve TCM concept comprehension.
- What evidence would resolve it: Experiments with enhanced Chain-of-Thought approaches that incorporate TCM domain knowledge, such as specialized templates, concept linking, or retrieval-augmented generation.

## Limitations

- Poor consistency under shuffled options (41.1% average) reveals fundamental LLM limitations in genuine knowledge understanding
- General LLMs outperforming specialized models suggests current domain-specific tuning approaches may be counterproductive
- Relatively small dataset size (600 questions) limits statistical power for robustness analysis

## Confidence

**High Confidence**: General-purpose LLMs outperform both medical and TCM-specific models, with Moonshot-v1-8k achieving 0.793 accuracy.
**Medium Confidence**: Robustness analysis showing poor consistency under option shuffling is convincing, but ensemble effectiveness requires further validation.
**Low Confidence**: Interpretation that CoT's limited effectiveness indicates poor TCM concept understanding may conflate prompt sensitivity with knowledge gaps.

## Next Checks

1. Test best-performing models (Moonshot-v1-8k, Baichuan2-13B-Base, InternLM2-Chat-20B) on independent TCM benchmarks like TCMBench to verify generalization.
2. Conduct controlled study where TCM experts take the same 600-question test to establish rigorous human performance baseline.
3. Perform detailed analysis of model performance broken down by the four question types to determine which formats drive overall results.