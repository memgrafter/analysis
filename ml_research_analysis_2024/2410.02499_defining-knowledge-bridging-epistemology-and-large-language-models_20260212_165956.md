---
ver: rpa2
title: 'Defining Knowledge: Bridging Epistemology and Large Language Models'
arxiv_id: '2410.02499'
source_url: https://arxiv.org/abs/2410.02499
tags:
- knowledge
- language
- llms
- definitions
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes definitions of knowledge from epistemology
  for large language models (LLMs) and proposes evaluation protocols for each definition.
  The authors review five epistemological definitions - true beliefs (tb-knowledge),
  justified true beliefs (j-knowledge), sui generis (g-knowledge), virtue-based (v-knowledge),
  and predictive accuracy (p-knowledge) - and map them to LLMs.
---

# Defining Knowledge: Bridging Epistemology and Large Language Models

## Quick Facts
- arXiv ID: 2410.02499
- Source URL: https://arxiv.org/abs/2410.02499
- Authors: Constanza Fierro; Ruchira Dhar; Filippos Stamatiou; Nicolas Garneau; Anders Søgaard
- Reference count: 32
- Primary result: Formalizes five epistemological knowledge definitions and evaluation protocols for LLMs, revealing disagreements between philosophers and computer scientists about when LLMs can be said to know

## Executive Summary
This paper bridges epistemology and NLP by formalizing five knowledge definitions (true beliefs, justified true beliefs, sui generis, virtue-based, and predictive accuracy) for large language models. Through a survey of 100 professionals, it reveals systematic differences in how philosophers and computer scientists conceptualize LLM knowledge. The authors provide specific evaluation protocols for each knowledge definition and demonstrate their application using Llama-3-8B-Instruct, showing that while the model predicts "platypuses are mammals" correctly, it fails to satisfy the belief+ requirements of true belief knowledge due to inconsistent beliefs about mammals laying eggs.

## Method Summary
The paper formalizes five epistemological knowledge definitions into measurable LLM evaluation protocols, conducts a survey of 100 professional philosophers and computer scientists to compare their perspectives on LLM knowledge, and implements these protocols on Llama-3-8B-Instruct. The evaluation involves testing model responses to specific prompts about factual knowledge, checking for logical consistency across related propositions, and attempting to attribute beliefs to training data or reasoning chains where applicable.

## Key Results
- Survey reveals significant disagreements between philosophers and computer scientists about LLM knowledge, particularly regarding true belief (tb-knowledge) and sui generis (g-knowledge) definitions
- Llama-3-8B-Instruct correctly predicts "platypuses are mammals" but fails tb-knowledge due to inconsistent beliefs about mammals laying eggs
- Most current NLP knowledge evaluation practices rely on tb-knowledge or p-knowledge definitions but fail to account for belief+ requirements or epistemic closure
- g-knowledge definition proves most challenging to implement, requiring concepts that resist straightforward formalization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Knowledge definitions can be formalized and mapped to LLM evaluation protocols
- **Mechanism**: The paper extracts five standard epistemological definitions (true beliefs, justified true beliefs, sui generis, virtue-based, and predictive accuracy) and creates formal interpretations for each that can be operationalized through specific LLM testing protocols
- **Core assumption**: Epistemological definitions of knowledge can be meaningfully translated into measurable LLM behaviors
- **Evidence anchors**:
  - [abstract] "we formalize interpretations applicable to LLMs" and "suggest evaluation protocols for testing knowledge in accordance to the most relevant definitions"
  - [section] Provides specific protocols for tb-knowledge, j-knowledge, g-knowledge, v-knowledge, and p-knowledge with concrete examples using Llama-3-8B-Instruct
  - [corpus] Weak evidence - corpus shows related work on knowledge definitions but lacks direct evidence of formalization success
- **Break condition**: If the formalized definitions cannot be practically implemented or fail to capture the philosophical nuances they're meant to represent

### Mechanism 2
- **Claim**: Survey results reveal meaningful differences in how philosophers and computer scientists conceptualize LLM knowledge
- **Mechanism**: The survey of 100 professionals shows systematic differences in agreement with knowledge definitions and beliefs about whether LLMs can "know," suggesting disciplinary perspectives influence knowledge claims about AI
- **Core assumption**: Professional background (philosophy vs computer science) influences epistemological views on LLM knowledge
- **Evidence anchors**:
  - [abstract] "conduct a survey of 100 professional philosophers and computer scientists to compare their preferences in knowledge definitions"
  - [section] Shows philosophers disagreed with tb-knowledge (49% selecting 1-2) while computer scientists agreed more (52% selecting 4-5), and reveals that philosophers strongly disagreed with g-knowledge (84% answers 1-2)
  - [corpus] Weak evidence - corpus shows related survey work but lacks direct evidence of disciplinary differences
- **Break condition**: If survey responses show no significant differences between groups or if responses are too inconsistent to draw meaningful conclusions

### Mechanism 3
- **Claim**: Current LLM knowledge evaluation practices are inconsistent with philosophical definitions
- **Mechanism**: The paper identifies that most NLP research relies on tb-knowledge or p-knowledge definitions but fails to account for belief+ requirements or epistemic closure over relevant propositions, creating gaps between practice and theory
- **Core assumption**: Current LLM knowledge evaluation methods are incomplete or inconsistent with established epistemological frameworks
- **Evidence anchors**:
  - [abstract] "identify inconsistencies and gaps in how current NLP research conceptualizes knowledge with respect to epistemological frameworks"
  - [section] Documents that "Most knowledge probing work seems to rely (loosely) on tb-knowledge or p-knowledge" but "fails to address the fact that tb-knowledge relies on p being believed+, or that p-knowledge requires epistemic closure over relevant propositions"
  - [corpus] Weak evidence - corpus shows related evaluation work but lacks direct evidence of inconsistency with philosophy
- **Break condition**: If current evaluation practices can be shown to fully align with philosophical definitions or if the identified gaps prove to be inconsequential

## Foundational Learning

- **Epistemic modal logic**
  - Why needed here: The paper uses epistemic logic to formalize knowledge definitions, making understanding this framework essential for grasping how the definitions are operationalized
  - Quick check question: What is the difference between the standard epistemic modal logic operator 2 and the proposed g-knowledge operator †?

- **Knowledge graph construction**
  - Why needed here: Current LLM knowledge evaluation often uses knowledge graphs as truth sources, so understanding how these are constructed and what they represent is crucial for evaluating the paper's methodology
  - Quick check question: How does the LAMA protocol use knowledge graphs to define when an LLM "knows" a fact?

- **Belief consistency measurement**
  - Why needed here: Several knowledge definitions require consistency across paraphrases and entailments, making understanding how to measure and enforce belief consistency critical
  - Quick check question: What is the difference between belief (Definition 2.1) and belief+ (Definition 2.2) in the context of LLM knowledge evaluation?

- **Justified belief attribution**
  - Why needed here: The j-knowledge definition requires justifications that can be traced to training data or reasoning chains, necessitating understanding of attribution methods
  - Quick check question: What are the current methodologies for attributing an LLM's belief to specific training data or reasoning steps?

## Architecture Onboarding

- **Component map**: Survey system (100 respondents, Likert scale questions, professional background categorization) -> LLM evaluation framework (Five knowledge definitions, each with specific testing protocols) -> Knowledge graph integration (LAMA protocol implementation, paraphrase and entailment evaluation) -> Attribution system (Training data tracing, reasoning chain verification, mechanistic interpretability)

- **Critical path**: Survey design -> Data collection -> Analysis of professional differences -> LLM protocol development -> Implementation with Llama-3-8B-Instruct -> Validation through examples

- **Design tradeoffs**: Comprehensive philosophical coverage vs. practical implementability; strict epistemological definitions vs. current NLP practices; survey breadth vs. depth of questioning

- **Failure signatures**: Inconsistent survey responses making professional differences unclear; LLM protocols failing to distinguish between knowledge definitions; attribution methods unable to trace beliefs to sources; g-knowledge definition proving too vague to implement

- **First 3 experiments**:
  1. Replicate the Llama-3-8B-Instruct example testing whether it tb-knows "platypuses are mammals" by evaluating confidence in the statement and its logical implications
  2. Implement the belief+ consistency check by testing model confidence across paraphrases and entailments of a simple fact
  3. Test the j-knowledge protocol by attempting to attribute a model's belief to training data using current attribution methods

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What specific evaluation protocols can be developed to test whether an LLM's beliefs satisfy the belief+ definition (epistemic closure over all logically implied propositions)?
- **Basis in paper**: Explicit - The paper discusses tb-knowledge requiring belief+ and mentions evaluating model confidence in both the proposition itself and all that follows logically from it, but doesn't provide specific protocols.
- **Why unresolved**: The paper identifies this as a gap in current research, noting that most work only evaluates confidence in the proposition itself, not in all logically implied propositions. Developing protocols for this would require new methodologies for systematically generating and testing logical implications.
- **What evidence would resolve it**: A concrete methodology showing how to generate relevant logical implications of a proposition, test LLM confidence in these implications, and demonstrate that the LLM maintains consistency across them would resolve this.

### Open Question 2
- **Question**: How can we operationalize the "knowledge bank" concept in g-knowledge to distinguish it from mere assertion generation?
- **Basis in paper**: Inferred - The paper discusses g-knowledge as propositions being "included in its knowledge bank" and mentions extreme interpretations where an LLM knows whatever it outputs, but calls for a more interesting interpretation involving modular knowledge components.
- **Why unresolved**: The paper notes that current work on memory components doesn't clearly identify knowledge components, and the boundary between knowledge and assertion remains unclear. This requires understanding LLM architecture at a deeper level.
- **What evidence would resolve it**: Empirical demonstration of distinct memorization strategies for knowledge versus other information, or identification of modular knowledge components in LLM architectures that can be independently tested for knowledge claims.

### Open Question 3
- **Question**: What constitutes valid justification for j-knowledge in LLMs, and which justification methods are superior?
- **Basis in paper**: Explicit - The paper mentions attribution methods, chain-of-thought mechanisms, and citation generation as potential justifications, but states "the jury is still out on which justification procedures are valid and/or superior."
- **Why unresolved**: The paper acknowledges that while multiple methods exist, there's no consensus on which provide valid justification for knowledge claims, and all seem to require partial interpretability which itself is challenging.
- **What evidence would resolve it**: Comparative studies showing which justification methods consistently produce valid knowledge claims across different domains and LLM architectures, along with clear criteria for what counts as valid justification.

## Limitations

- Philosophical formalization challenges remain significant, particularly for g-knowledge which relies on concepts that resist straightforward formalization
- Survey sample size of 100 may not represent broader disciplinary perspectives or generalize to other professional categories
- Attribution methodology gaps exist for j-knowledge evaluation, as current mechanistic interpretability methods may not provide sufficient resolution for reliable belief attribution

## Confidence

- **High confidence**: The formalization of tb-knowledge, p-knowledge, and v-knowledge definitions shows clear practical implementation paths with well-defined protocols. The survey methodology and basic analysis are sound.
- **Medium confidence**: The j-knowledge formalization and evaluation protocol are reasonable but depend on attribution methods that may not yet be fully reliable. The identification of inconsistencies in current NLP practices appears valid but requires more systematic verification.
- **Low confidence**: The g-knowledge definition remains the most problematic, as it relies on concepts that resist straightforward formalization and implementation. The survey findings about disciplinary differences, while interesting, need larger sample sizes for robust generalization.

## Next Checks

1. **Cross-disciplinary survey replication**: Conduct a larger-scale survey (N=500+) with stratified sampling across academic institutions and industry to verify whether the observed differences between philosophers and computer scientists persist at scale and whether additional professional categories (psychologists, cognitive scientists) show distinct patterns.

2. **G-knowledge implementation challenge**: Attempt to implement the g-knowledge evaluation protocol on a concrete LLM task where traditional justification methods fail, documenting specific cases where sui generis justification might apply and whether current attribution methods can detect it.

3. **Consistency validation protocol**: Design and execute a systematic study testing whether current LLM evaluation practices actually violate the belief+ requirements of tb-knowledge, using a representative sample of knowledge probing datasets and measuring belief consistency across paraphrases and entailments.