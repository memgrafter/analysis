---
ver: rpa2
title: Unsqueeze [CLS] Bottleneck to Learn Rich Representations
arxiv_id: '2407.17671'
source_url: https://arxiv.org/abs/2407.17671
tags:
- learning
- dino
- vit-s
- representations
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of over-compression in self-supervised
  learning (SSL), which results in loss of meaningful information in learned representations.
  The proposed method, UDI (Unsqueezed Distillation-based SSL), enriches learned representations
  by encouraging multimodal predictions distilled from a consolidated profile of local
  predictions derived via stratified sampling.
---

# Unsqueeze [CLS] Bottleneck to Learn Rich Representations

## Quick Facts
- arXiv ID: 2407.17671
- Source URL: https://arxiv.org/abs/2407.17671
- Authors: Qing Su; Shihao Ji
- Reference count: 40
- One-line primary result: UDI achieves 77.6% top-1 accuracy on ImageNet-1K and significantly improves dense prediction tasks through enriched multimodal representations.

## Executive Summary
This paper addresses the problem of over-compression in self-supervised learning (SSL), which results in loss of meaningful information in learned representations. The proposed method, UDI (Unsqueezed Distillation-based SSL), enriches learned representations by encouraging multimodal predictions distilled from a consolidated profile of local predictions derived via stratified sampling. UDI employs self-attention as a semantic constraint to align representations across different granularities and introduces a novel objective to preserve meaningful nuisances alongside image-level semantics. Experiments show that UDI achieves competitive performance on ImageNet-1K (77.6% top-1 accuracy) and significantly improves performance in dense prediction tasks like object detection and segmentation. Additionally, UDI demonstrates advantages in low-shot learning, achieving 66.7% accuracy using 1% of the labels.

## Method Summary
UDI enriches self-supervised representations by using stratified random sampling to extract patch-level representations via self-attention, creating multimodal target distributions that preserve semantic composition. The method employs an extra class token and a shared projector across image and patch levels, with loss functions combining image-level, patch-level, and multimodal objectives. The stratified sampling uses a 3x3 grid window to maintain semantic proportions while reducing aliasing effects.

## Key Results
- UDI achieves 77.6% top-1 accuracy on ImageNet-1K linear evaluation
- Improves object detection APbb by 2.1% and instance segmentation APmk by 2.1% on COCO
- Demonstrates strong low-shot learning performance (66.7% with 1% labels, 80.4% with 10% labels)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UDI reduces over-compression by using multimodal target distributions that preserve meaningful nuisances
- Mechanism: Blends patch-level predictions from stratified sampling with image-level predictions to form richer, less peaked targets instead of sharpened teacher predictions
- Core assumption: Natural images are semantic compositions; mixture of patch-level clusters provides better representation than unimodal sharp distributions
- Evidence anchors: Abstract states UDI enriches representations via multimodal predictions from stratified sampling; section 3.3 describes modeling images as empirical distributions of visual features
- Break condition: If stratified sampling fails to capture true semantic distribution, multimodal target will be inaccurate and over-compression may persist

### Mechanism 2
- Claim: Self-attention aligns semantic types across image and patch levels by acting as soft-masked pooling
- Mechanism: SA extracts context-aligned patch representations that naturally align with image-level semantics when using shared projector
- Core assumption: Patch-level self-attention produces representations with semantic type matching image-level class token semantics
- Evidence anchors: Section 3.2 describes SA as akin to fine-grained average RoIPool enabling semantic alignment; states semantic types naturally align between levels
- Break condition: If patch semantics are too fine-grained or misaligned with image semantics, shared projector training may degrade performance

### Mechanism 3
- Claim: Stratified random sampling with 3x3 grid window reduces aliasing and preserves semantic proportion
- Mechanism: Uniform sampling within grid cells maintains semantic component proportions while reducing sampling noise
- Core assumption: Semantic composition is roughly uniform within local grid cells
- Evidence anchors: Section 3.3 explains stratified sampling preserves semantic proportions while reducing aliasing; section 4.4 shows 3×3 window performs best
- Break condition: If image semantics are highly non-uniform or window size is inappropriate, stratified sampling may misrepresent semantic composition

## Foundational Learning

- Concept: Self-distillation in SSL (e.g., DINO)
  - Why needed here: UDI builds upon self-distillation principles; understanding teacher/student interaction and sharpening-induced compression is essential
  - Quick check question: In DINO, why does using smaller teacher temperature (τt < τs) lead to sharper target distributions and potential over-compression?

- Concept: Stratified random sampling
  - Why needed here: UDI uses this to sample patches for constructing multimodal target; knowing differences from uniform sampling is key
  - Quick check question: How does stratified random sampling within 3x3 grid differ from uniform random sampling across entire image in terms of coverage and noise?

- Concept: Self-attention as soft-masked pooling
  - Why needed here: UDI leverages SA to extract patch-level representations; understanding SA mechanics and RoI pooling similarity is critical
  - Quick check question: How does self-attention mechanism in UDI approximate behavior of RoI pooling in object detection?

## Architecture Onboarding

- Component map: ViT backbone with two class tokens -> Stratified Random Sampling module with self-attention -> Shared projector head -> Centering and sharpening layers -> Loss functions (image-level, patch-level, multimodal)

- Critical path: 1) Input image → two global views (teacher/student) 2) Backbone → dense features → extract [cls] tokens 3) SRS module → patch-level representations via self-attention 4) Projector → predictions at image and patch levels 5) Loss computation: image-level, patch-level, multimodal 6) Backpropagation → student update; EMA → teacher update

- Design tradeoffs: Extra class token increases representational capacity but may reduce linear separability (seen in UDI+ vs UDI); shared projector simplifies architecture but assumes semantic alignment between levels; stratified sampling balances semantic fidelity and computational efficiency vs full-patch sampling

- Failure signatures: Training instability or collapse → check teacher/student temperature schedule and centering momentum; poor dense prediction performance → verify semantic alignment via shared projector; check SRS module; linear probing underperformance → evaluate if multimodal objective introduces too much nuisance; consider reducing α

- First 3 experiments: 1) Replace stratified sampling with uniform sampling; compare linear probing and APbb on COCO 2) Remove extra class token (zcls+); measure impact on linear evaluation and k-NN accuracy 3) Disable shared projector (use separate projectors for image and patch levels); assess effect on semantic alignment and downstream dense tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is exact relationship between blending factor α in multimodal objective and trade-off between linear probing performance and k-NN evaluation performance?
- Basis in paper: [explicit] Paper states UDI+ incurs compromised performance in linear probing due to enriched multimodal representations while showing improved k-NN evaluation
- Why unresolved: Paper only tests single value α=0.5 and doesn't explore full spectrum of possible values or provide detailed analysis of how α affects balance between these two metrics
- What evidence would resolve it: Comprehensive ablation study varying α across range of values and measuring corresponding performance in both linear probing and k-NN evaluation would clarify relationship

### Open Question 2
- Question: How does performance of UDI compare to other state-of-the-art methods when trained with larger ViT models (e.g., ViT-B or ViT-L) and for longer durations?
- Basis in paper: [inferred] Paper only evaluates UDI using ViT-S/16 due to cost-effectiveness and mentions future work could explore extending UDI to larger models and longer training durations
- Why unresolved: Paper doesn't provide experimental results or analysis of UDI's performance with larger models or extended training, leaving scalability and potential improvements unclear
- What evidence would resolve it: Training and evaluating UDI with larger ViT models (e.g., ViT-B, ViT-L) for extended durations and comparing results to other state-of-the-art methods would demonstrate scalability and potential of UDI

### Open Question 3
- Question: How does choice of Stratified Random Sampling (SRS) window size affect quality of learned representations and performance on downstream tasks?
- Basis in paper: [explicit] Paper mentions using window size of 3×3 results in best performance while using all patches leads to only marginal improvements
- Why unresolved: Paper doesn't provide detailed analysis of impact of different window sizes on learned representations or downstream task performance, leaving optimal window size and its effects unclear
- What evidence would resolve it: Conducting thorough ablation study with various window sizes and measuring corresponding performance on downstream tasks would clarify relationship between window size and representation quality

## Limitations

- Theoretical grounding uncertainty: Claim that 3×3 grid size optimally preserves semantic proportions based on internal ablation studies rather than theoretical analysis; assumption that natural images follow uniform semantic distributions within local grid cells may not hold for all image types
- Mechanism validation gap: No empirical validation comparing semantic alignment quality between UDI's self-attention approach and actual RoI pooling on detection datasets; semantic type alignment assumption remains theoretically asserted rather than experimentally verified
- Multimodal target representation fidelity: Effectiveness depends on stratified sampling capturing true semantic clusters, but paper doesn't validate whether sampled patches represent distinct semantic concepts or are simply spatially diverse but semantically redundant

## Confidence

**High confidence (80-100%)**: 
- Experimental results showing UDI achieves 77.6% top-1 accuracy on ImageNet-1K are directly measurable and reproducible
- Ablation study demonstrating 3×3 grid size performs best is internally consistent and provides clear quantitative evidence
- Claim that UDI improves dense prediction tasks over baseline methods is supported by specific APbb and mIoU metrics

**Medium confidence (50-80%)**:
- Mechanism explanation that stratified sampling preserves semantic proportions is plausible but not rigorously proven
- Assertion that shared projector enables semantic alignment across granularities is theoretically sound but lacks comparative analysis with separate projectors
- Claim that multimodal targets reduce over-compression is inferred from performance gains rather than direct measurement of information content

**Low confidence (0-50%)**:
- Theoretical claim that natural images follow empirical distribution of visual features suitable for stratified sampling
- Assertion that self-attention provides semantic alignment equivalent to RoI pooling without empirical comparison
- Claim that extra class token's nuisances are meaningful rather than noisy artifacts

## Next Checks

1. **Semantic fidelity validation**: Replace stratified random sampling with k-means clustering based on ground truth semantic segmentation masks (when available). Compare multimodal target distribution quality and downstream task performance to validate whether stratified sampling captures meaningful semantic clusters versus just spatial diversity.

2. **Semantic alignment verification**: Train two versions of UDI - one with shared projector and one with separate projectors for image and patch levels. Measure semantic alignment quality using centered kernel alignment (CKA) between image and patch representations, and compare downstream dense prediction performance to directly test claim about semantic type alignment.

3. **Information preservation measurement**: Compute mutual information between input images and representations at different stages of UDI pipeline (baseline, with stratified sampling, with multimodal targets). Compare these values with linear probing accuracy to empirically validate whether multimodal objectives truly preserve more meaningful information versus simply increasing capacity.