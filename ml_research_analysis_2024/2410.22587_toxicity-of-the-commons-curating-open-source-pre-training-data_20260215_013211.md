---
ver: rpa2
title: 'Toxicity of the Commons: Curating Open-Source Pre-Training Data'
arxiv_id: '2410.22587'
source_url: https://arxiv.org/abs/2410.22587
tags:
- text
- score
- bias
- content
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of filtering toxic content from
  public domain datasets used for training large language models, focusing on historical
  and OCR-derived texts. The authors propose a custom pipeline that uses LLM annotations
  to train a domain-specific toxicity classifier (Celadon) and applies synthetic rewriting
  and content warnings to manage filtered content.
---

# Toxicity of the Commons: Curating Open-Source Pre-Training Data

## Quick Facts
- arXiv ID: 2410.22587
- Source URL: https://arxiv.org/abs/2410.22587
- Reference count: 40
- Key outcome: Custom LLM-annotated toxicity classifier (Celadon) achieves 69-74% weighted accuracy across five bias dimensions on historical/OCR text, outperforming generic classifiers.

## Executive Summary
This paper addresses the challenge of filtering toxic content from public domain datasets used for training large language models, focusing on historical and OCR-derived texts. The authors propose a custom pipeline that uses LLM annotations to train a domain-specific toxicity classifier (Celadon) and applies synthetic rewriting and content warnings to manage filtered content. They create a dataset (ToxicCommons) annotated across five bias dimensions and show their classifier achieves weighted accuracies of 69-74% across these dimensions, significantly outperforming existing toxicity classifiers on this out-of-domain data. The approach balances safety filtering with data preservation, enabling safer open-data model training while maintaining historical context.

## Method Summary
The authors developed a data curation pipeline using LLM annotations to create a domain-specific toxicity classifier for historical and OCR-derived texts. They sampled texts from the Common Corpus dataset, applied OCR correction using OCRonos, and collected human annotations for 1,000 samples across five bias dimensions. Using Llama 3.1, they generated annotations for the remaining data based on human annotations, then trained a DeBERTa-v3-small classifier (Celadon) on the LLM-annotated corpus. The classifier assigns scores from 0-3 across five dimensions (racial/origin-based, gender/sex-based, religious, ability-based discrimination, and violence), with synthetic rewriting and content warnings applied to toxic content based on total score thresholds.

## Key Results
- Celadon classifier achieves weighted accuracies of 69-74% across five bias dimensions on historical/OCR text
- Violence dimension shows highest toxicity rates (27.5% score > 0) across all models tested
- Classifier significantly outperforms existing toxicity classifiers on out-of-domain historical data
- Synthetic rewriting approach preserves data utility while mitigating harmful content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM annotation alignment improves toxicity classification on out-of-domain historical text
- Mechanism: Human-annotated samples provide a ground truth baseline, and LLM annotations (Llama 3.1) are tuned to match human judgments, allowing for scalable and consistent labeling of toxicity across five dimensions
- Core assumption: LLM can be prompted to replicate human reasoning about implicit bias without relying on keyword matching
- Evidence anchors:
  - [abstract]: "The approach balances safety filtering with data preservation, enabling safer open-data model training while maintaining historical context."
  - [section]: "Using the human annotations as a baseline, we refined a prompt to generate scores and annotations on each of the five dimensions of bias... We did not find that few-shot prompting improved the alignment of the model annotations with the gold human annotations."
  - [corpus]: No direct corpus evidence provided for alignment efficacy
- Break condition: If LLM annotations diverge significantly from human judgments on edge cases or novel bias patterns not seen in training samples

### Mechanism 2
- Claim: Custom domain-specific classifier (Celadon) outperforms generic toxicity classifiers on OCR and historical text
- Mechanism: DeBERTa-v3-small architecture trained on LLM-annotated data captures domain-specific language patterns and OCR errors that generic classifiers miss
- Core assumption: Historical text and OCR artifacts create systematic linguistic differences that require domain adaptation
- Evidence anchors:
  - [abstract]: "current state-of-the-art approaches to toxicity filtering are often infeasible or inappropriate for open data models"
  - [section]: "We find that the dimension with the highest rates of scores above 0 is Violence... All models were the most sensitive to the violence dimension."
  - [corpus]: Weak corpus evidence; no direct comparison of Celadon vs generic classifiers on held-out historical data
- Break condition: If classifier overfits to training domain and fails to generalize to other historical or OCR-corrupted datasets

### Mechanism 3
- Claim: Synthetic rewriting and content warnings preserve data utility while mitigating toxicity
- Mechanism: LLM-generated content warnings and rewritten versions allow models to learn from problematic text without internalizing harmful patterns
- Core assumption: Providing context through warnings or neutralization enables safer model training without wholesale data removal
- Evidence anchors:
  - [abstract]: "applies synthetic rewriting and content warnings to manage filtered content"
  - [section]: "We use Llama 3.1... to create a content warning that includes justification for why the content may be toxic... For a lower score of 3, we require the sole contribution to come from a single category"
  - [corpus]: No corpus evidence provided for effectiveness of synthetic rewriting approach
- Break condition: If synthetic rewriting introduces artifacts or hallucinations that degrade model performance or if warnings are ignored during training

## Foundational Learning

- Concept: Bias detection across multiple dimensions (racial, gender, religious, ability, violence)
  - Why needed here: Historical texts contain implicit biases that require nuanced classification beyond binary toxic/non-toxic labels
  - Quick check question: What distinguishes a score of 2 from a score of 3 in the annotation scale?

- Concept: Domain adaptation for NLP models
  - Why needed here: Public domain historical texts differ significantly from web text in style, content, and noise characteristics
  - Quick check question: Why do generic toxicity classifiers perform poorly on OCR-corrupted historical data?

- Concept: Synthetic data generation for toxicity mitigation
  - Why needed here: Allows preservation of historical context while reducing harmful content in training data
  - Quick check question: How does synthetic rewriting differ from simple data filtering?

## Architecture Onboarding

- Component map: Raw historical text -> LLM annotation -> Classifier training -> Classification -> Synthetic treatment
- Critical path:
  1. Collect and preprocess historical datasets (OCR correction)
  2. Generate human annotations for small sample
  3. Fine-tune LLM annotation prompt using human baseline
  4. Scale LLM annotations across entire corpus
  5. Train Celadon classifier on LLM-annotated data
  6. Classify all data and apply synthetic treatment based on thresholds

- Design tradeoffs:
  - Accuracy vs efficiency: LLM annotation vs human annotation
  - Data preservation vs safety: Content warnings vs data removal
  - Generalization vs specificity: Domain-specific classifier vs generic models

- Failure signatures:
  - High false positive rate: Classifier flags benign historical content as toxic
  - Context loss: Synthetic rewriting alters historical meaning or style
  - Annotation drift: LLM annotations drift from human judgments over time

- First 3 experiments:
  1. Compare Celadon vs generic classifier performance on held-out historical samples
  2. A/B test model training with vs without synthetic rewriting
  3. Evaluate content warning effectiveness through human review of flagged samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Celadon compare to existing toxicity classifiers when applied to historical and OCR-derived text data?
- Basis in paper: Explicit - The paper discusses testing existing classifiers on Common Corpus data and finding they perform poorly due to out-of-domain issues.
- Why unresolved: The paper does not provide direct quantitative comparisons between Celadon and existing classifiers on the same datasets.
- What evidence would resolve it: A head-to-head comparison of Celadon and existing classifiers (e.g., Perspective API, Google SafeSearch) on the same held-out test set of Common Corpus data, showing precision, recall, and F1 scores for each model.

### Open Question 2
- Question: What is the impact of synthetic rewriting on the semantic content and utility of the filtered text for downstream language modeling tasks?
- Basis in paper: Explicit - The paper describes a synthetic rewriting approach but does not evaluate its impact on model performance or data quality.
- Why unresolved: The paper focuses on the filtering methodology but does not assess whether rewritten content maintains enough original meaning for effective language model training.
- What evidence would resolve it: An evaluation comparing language models trained on original vs. synthetically rewritten toxic content, measuring task performance and measuring semantic drift between original and rewritten text.

### Open Question 3
- Question: How does the proposed toxicity classification scale to other languages beyond Western European ones, particularly those with less available training data?
- Basis in paper: Explicit - The paper acknowledges its dataset is limited to mostly Western European languages and discusses challenges with less-resourced languages.
- Why unresolved: The paper only evaluates the classifier on the languages in Common Corpus and does not test its generalizability to other language families or low-resource languages.
- What evidence would resolve it: Cross-lingual evaluation of Celadon on diverse language datasets, including languages from different families and varying resource levels, with performance metrics for each language.

## Limitations
- Limited human annotation validation beyond initial 1,000 samples restricts assessment of Celadon's real-world performance
- Synthetic rewriting effectiveness unproven without empirical validation of data utility preservation
- Performance metrics potentially overstated due to in-domain evaluation rather than true generalization testing

## Confidence
- Medium confidence in Celadon classifier performance - methodology sound but lacks human-annotated test data and direct generic classifier comparison
- Low confidence in synthetic rewriting efficacy - theoretically interesting but no empirical validation provided
- High confidence in pipeline architecture - LLM-based domain adaptation framework is well-established and addresses genuine gaps

## Next Checks
1. Create held-out test set of 500-1,000 historical samples annotated by human experts across all five bias dimensions and compare Celadon's predictions against human judgments
2. Evaluate established toxicity classifiers (Perspective API, Detoxify, etc.) on the same historical dataset and directly compare their performance to Celadon across all five dimensions
3. Train language models using the same historical corpus with three different treatments: filtered data, data with content warnings, and synthetically rewritten data; compare model outputs for bias perpetuation and historical accuracy