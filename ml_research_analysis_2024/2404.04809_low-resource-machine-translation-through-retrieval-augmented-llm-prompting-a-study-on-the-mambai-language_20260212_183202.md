---
ver: rpa2
title: 'Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting:
  A Study on the Mambai Language'
arxiv_id: '2404.04809'
source_url: https://arxiv.org/abs/2404.04809
tags:
- mambai
- language
- sentences
- translation
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates using large language models (LLMs) for
  translating English to Mambai, a low-resource Austronesian language. The authors
  construct a novel corpus from a Mambai language manual and native speaker translations.
---

# Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language

## Quick Facts
- arXiv ID: 2404.04809
- Source URL: https://arxiv.org/abs/2404.04809
- Reference count: 16
- Primary result: Retrieval-augmented LLM prompting significantly improves English-to-Mambai translation, with BLEU scores ranging from 21.2 on in-domain sentences to 4.4 on native speaker translations

## Executive Summary
This study investigates using large language models (LLMs) for translating English to Mambai, a low-resource Austronesian language. The authors construct a novel corpus from a Mambai language manual and native speaker translations. They experiment with retrieval-augmented few-shot prompting, incorporating parallel sentences and dictionary entries from the manual. Results show significant variation in translation quality depending on the test set, with BLEU scores ranging from 21.2 on in-domain sentences to 4.4 on native speaker translations. Dictionary entries and a mix of TF-IDF and semantic retrieval improve accuracy. However, performance disparities highlight risks of overfitting to single-domain test sets and underscore the need for diverse, representative corpora when evaluating low-resource MT.

## Method Summary
The authors constructed a novel corpus by digitizing a Mambai language manual using OCR (ScanTailor and ABBYY FineReader), extracting 1,187 parallel sentences and 1,790 dictionary entries. They implemented retrieval-augmented few-shot prompting that includes example sentences retrieved via TF-IDF and semantic embeddings along with dictionary entries for words in the input sentence. The system was tested with three LLMs (Mixtral 8x7B, LLaMa 2 70B, GPT-4) and evaluated on two test sets: 119 sentences from the manual and 50 sentences translated by a native speaker using BLEU and ChrF++ metrics.

## Key Results
- Retrieval-augmented prompting with dictionary entries and mixed retrieval methods (TF-IDF + semantic embeddings) significantly improves translation quality
- BLEU scores range from 21.2 on in-domain manual sentences to 4.4 on native speaker translations
- A blend of 5 TF-IDF and 5 semantic embedding retrievals outperforms using 10 sentences from either method alone
- Significant performance disparities across test sets highlight overfitting risks to single-domain corpora

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented prompting improves translation accuracy by providing relevant in-context examples.
- Mechanism: The system retrieves parallel sentences and dictionary entries that are semantically or lexically close to the input sentence. These examples are inserted into the prompt, conditioning the LLM to generate translations that match the style, vocabulary, and grammar of the target language as seen in the corpus.
- Core assumption: The quality of retrieved examples directly correlates with translation accuracy.
- Evidence anchors:
  - [abstract] "We find that including dictionary entries in prompts and a mix of sentences retrieved through TF-IDF and semantic embeddings significantly improves translation quality."
  - [section 4.3] "While 10-shot translation yields BLEU score as high as 23.5 for the test sentences sampled from the language manual used in prompting..."
  - [corpus] Weak: No explicit quantitative link between retrieval quality and BLEU provided; the improvement is stated but not detailed.
- Break condition: If retrieved examples are irrelevant or too dissimilar to the input, the prompt conditioning fails and translation accuracy degrades.

### Mechanism 2
- Claim: Few-shot prompting with domain-specific examples improves translation accuracy for low-resource languages.
- Mechanism: By including a small number of carefully selected example sentence pairs and dictionary entries in the prompt, the LLM is guided to follow the linguistic patterns and vocabulary of the target language as represented in the corpus.
- Core assumption: The LLM can learn from a small number of examples to translate into a language it was not explicitly trained on.
- Evidence anchors:
  - [abstract] "Our methodology involves the strategic selection of parallel sentences and dictionary entries for prompting, aiming to enhance translation accuracy..."
  - [section 4.3] "We find that including dictionary entries in prompts and a mix of sentences retrieved through TF-IDF and semantic embeddings significantly improves translation quality."
  - [corpus] Weak: No explicit statement about the LLM's ability to learn from few examples; the improvement is observed but the mechanism is not detailed.
- Break condition: If the LLM cannot generalize from the few examples, or if the examples are not representative of the target language, the few-shot prompting will not improve accuracy.

### Mechanism 3
- Claim: Using a blend of TF-IDF and semantic embeddings for retrieval yields higher translation accuracy than using either method alone.
- Mechanism: TF-IDF retrieval surfaces sentences that share rare words with the input, while semantic embeddings retrieve sentences with similar meaning. Combining both methods ensures that the retrieved examples are both lexically and semantically relevant to the input.
- Core assumption: Rare words are harder to translate and should be prioritized in retrieval, but semantic similarity is also important.
- Evidence anchors:
  - [section 4.3] "A blend of 5 sentences retrieved through TF-IDF and 5 sentences retrieved through LASER semantic embeddings outperforms 10 sentences retrieved exclusively through one of these features."
  - [section 4.2.3] "NTFIDF: Number of sentence pairs retrieved through TF-IDF... Nembed: Number of sentence pairs retrieved through LASER semantic embeddings..."
  - [corpus] Weak: No explicit explanation of why rare words are harder to translate or why a blend is better than a single method.
- Break condition: If the input sentence does not contain rare words, or if the semantic embeddings fail to capture the meaning, the blend of methods may not improve accuracy.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG allows the system to dynamically incorporate relevant examples from the corpus into the prompt, improving translation accuracy for low-resource languages.
  - Quick check question: How does RAG differ from traditional prompting, and why is it particularly useful for low-resource languages?

- Concept: Few-shot learning
  - Why needed here: Few-shot learning enables the LLM to translate into a language it was not explicitly trained on, using a small number of examples to guide its generation.
  - Quick check question: What are the key differences between few-shot learning and zero-shot learning, and how do they apply to machine translation?

- Concept: BLEU and ChrF++ evaluation metrics
  - Why needed here: BLEU and ChrF++ are used to evaluate the quality of the machine translation output, providing a quantitative measure of accuracy.
  - Quick check question: How do BLEU and ChrF++ differ, and what are their strengths and weaknesses in evaluating machine translation for low-resource languages?

## Architecture Onboarding

- Component map: Input Sentence -> Retriever (TF-IDF + Semantic Embeddings) -> Prompt Builder -> LLM -> Evaluator (BLEU/ChrF++)
- Critical path:
  1. Input sentence is received.
  2. Retriever finds relevant examples from the corpus.
  3. Prompt Builder constructs the prompt with examples and dictionary entries.
  4. LLM generates the translation.
  5. Evaluator computes BLEU and ChrF++ scores.
- Design tradeoffs:
  - Corpus size vs. quality: A larger corpus may contain more relevant examples, but may also include more noise.
  - Retrieval method: TF-IDF retrieval prioritizes rare words, while semantic embeddings prioritize meaning. A blend of both may yield the best results.
  - Prompt length: Including more examples in the prompt may improve accuracy, but may also exceed the LLM's context window.
- Failure signatures:
  - Low BLEU and ChrF++ scores: The translation is inaccurate.
  - High variance in scores across test sets: The system overfits to the training corpus.
  - Slow retrieval or prompt construction: The system is not efficient.
- First 3 experiments:
  1. Vary the number of retrieved examples (e.g., 0, 5, 10) and measure the impact on BLEU and ChrF++ scores.
  2. Compare the performance of TF-IDF retrieval, semantic embedding retrieval, and a blend of both methods.
  3. Test the system on a test set that is not from the same corpus as the training examples, to assess generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do dictionary entries and retrieval methods (TF-IDF vs semantic embeddings) interact to affect translation quality in few-shot LLM prompting?
- Basis in paper: [explicit] The authors find that dictionary entries improve translation quality, and that a blend of TF-IDF and semantic embeddings yields the highest accuracy, but the interaction between these factors is not fully explored.
- Why unresolved: The paper does not provide a detailed analysis of how dictionary entries and different retrieval methods combine to influence translation quality.
- What evidence would resolve it: Systematic experiments varying the inclusion of dictionary entries and retrieval methods independently and in combination, measuring their impact on translation quality across different test sets.

### Open Question 2
- Question: To what extent does the quality of few-shot LLM prompting for low-resource MT depend on the domain of the test set?
- Basis in paper: [explicit] The authors observe stark disparities in translation performance between test sets from the language manual and those translated by a native speaker, suggesting domain sensitivity.
- Why unresolved: The paper does not conduct a thorough analysis of how translation quality varies across different domains within the test sets.
- What evidence would resolve it: Testing the LLM prompting approach on diverse test sets covering various domains, and analyzing the correlation between domain similarity to the prompt examples and translation quality.

### Open Question 3
- Question: How can few-shot LLM prompting be optimized for low-resource languages with non-standard orthography and limited standardized resources?
- Basis in paper: [inferred] The authors highlight the challenges of working with Mambai's non-standard orthography and the lack of standardized resources, suggesting the need for further research in this area.
- Why unresolved: The paper does not explore specific strategies to address the challenges posed by non-standard orthography and limited resources in few-shot LLM prompting.
- What evidence would resolve it: Developing and testing techniques to handle non-standard orthography, such as incorporating phonetic information or leveraging audio data, and exploring ways to utilize limited resources more effectively in the prompting process.

## Limitations
- Stark performance gap between in-domain (BLEU 21.2) and native speaker (BLEU 4.4) test sets suggests potential overfitting
- Small test set sizes (119 and 50 sentences) limit statistical confidence
- Evaluation metrics may not fully capture translation quality for morphologically rich languages like Mambai

## Confidence
- Medium: The methodology is sound and retrieval-augmented approach shows measurable improvements, but small test sets and single-domain corpus limit generalizability. Observed improvements with dictionary entries and mixed retrieval methods are well-supported, but magnitude of effects may not generalize to other language pairs or domains.

## Next Checks
1. Cross-domain evaluation: Test the system on additional Mambai text sources (news, social media, literature) to assess generalization beyond the manual domain and verify if the native speaker test set performance gap persists.
2. Retrieval quality analysis: Conduct a detailed error analysis correlating specific retrieval failures (irrelevant examples, missing key vocabulary) with translation errors to validate the claimed importance of retrieval quality.
3. Baseline comparison: Implement and evaluate traditional low-resource MT approaches (phrase-based, neural MT with back-translation) on the same corpus to establish whether the retrieval-augmented LLM approach provides meaningful advantages over established methods for this specific language pair.