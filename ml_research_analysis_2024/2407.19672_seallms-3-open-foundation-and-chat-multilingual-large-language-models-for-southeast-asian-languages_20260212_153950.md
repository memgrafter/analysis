---
ver: rpa2
title: 'SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for
  Southeast Asian Languages'
arxiv_id: '2407.19672'
source_url: https://arxiv.org/abs/2407.19672
tags:
- language
- languages
- seallms
- questions
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SeaLLMs 3 is a family of large language models specifically developed
  for Southeast Asian languages, addressing the gap in AI development for low-resource
  languages. The models cover 12 SEA languages and use efficient language enhancement
  techniques, including language-specific neuron training, to significantly reduce
  training costs while maintaining performance.
---

# SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages

## Quick Facts
- arXiv ID: 2407.19672
- Source URL: https://arxiv.org/abs/2407.19672
- Reference count: 11
- Primary result: SeaLLMs 3 achieves state-of-the-art performance among similarly sized models across SEA languages with significant efficiency gains

## Executive Summary
SeaLLMs 3 addresses the critical gap in AI development for Southeast Asian languages by introducing a family of large language models specifically designed for 12 SEA languages. The models leverage efficient Language-Specific Neuron (LSN) training to enhance SEA language capabilities while maintaining high-resource language performance, achieving state-of-the-art results across multiple benchmarks while reducing training costs by orders of magnitude compared to traditional approaches.

The model demonstrates strong performance in world knowledge (0.692 average on M3Exam), mathematics (73.1 average on MGSM), instruction-following (6.31 average on SeaBench), and translation (36.52 chrF on Flores-200). SeaLLMs 3 also shows improved trustworthiness with reduced hallucinations (81.69 F1 on SeaRefuse-G) and high safety performance (79.7% safe rate on MultiJail), addressing both general and culture-specific safety considerations.

## Method Summary
SeaLLMs 3 builds upon the Qwen2 foundation model using Language-Specific Neuron (LSN) training to efficiently enhance SEA language capabilities. The approach identifies and trains only the small subset of parameters (0.1% of all parameters) responsible for processing each SEA language, allowing independent enhancement without degrading high-resource language performance. The model undergoes two-stage training with balanced multilingual supervised fine-tuning (SFT) data covering translation, self-instruction, and native speaker verification, followed by high-quality SFT data for fine-tuning.

## Key Results
- Achieves state-of-the-art performance among similarly sized models across SEA languages
- Demonstrates significant efficiency gains through LSN training (orders of magnitude less training cost than traditional methods)
- Shows strong performance across diverse tasks: world knowledge (0.692 M3Exam), mathematics (73.1 MGSM), instruction-following (6.31 SeaBench), and translation (36.52 chrF Flores-200)
- Exhibits improved trustworthiness with reduced hallucinations (81.69 F1 SeaRefuse-G) and high safety (79.7% MultiJail)

## Why This Works (Mechanism)

### Mechanism 1
Language-Specific Neuron (LSN) training enables efficient enhancement of SEA languages without degrading high-resource language performance. Instead of full continued pretraining, the model identifies and trains only the small subset of parameters (0.1% of all parameters) responsible for processing each SEA language. These LSNs operate independently, so enhancing one language does not affect others.

Core assumption: The LSNs for different languages are truly independent and do not interfere with each other during training.

### Mechanism 2
Balanced multilingual SFT data construction prevents dominance of English and improves performance on SEA languages. The SFT dataset is carefully balanced across languages, with substantial representation given to SEA languages alongside English. This prevents the model from overfitting to English patterns during fine-tuning.

Core assumption: Equal or proportional representation of languages in training data leads to balanced performance across those languages.

### Mechanism 3
Culture-specific safety data and refusal training reduce hallucinations and improve trustworthiness in SEA contexts. The model is trained with both general safety data and culture-specific safety data, enabling it to recognize and refuse questions beyond its knowledge boundaries, particularly those involving non-existent entities or culturally sensitive topics.

Core assumption: Training with refusal examples and safety considerations effectively teaches the model to recognize its knowledge boundaries and refuse inappropriate queries.

## Foundational Learning

- Concept: Language-Specific Neuron (LSN) training
  - Why needed here: Traditional continued pretraining for low-resource languages is expensive and can degrade performance on high-resource languages. LSN training offers a targeted, efficient alternative.
  - Quick check question: What percentage of parameters are typically language-specific neurons according to the paper? (Answer: 0.1%)

- Concept: Multilingual instruction tuning
  - Why needed here: Standard instruction datasets are predominantly in English, which would bias the model toward English-centric responses. Multilingual instruction tuning ensures balanced performance across SEA languages.
  - Quick check question: How many task types are included in the SeaBench instruction-following benchmark? (Answer: 13)

- Concept: Hallucination detection and refusal
  - Why needed here: LLMs tend to answer questions beyond their knowledge boundaries, leading to hallucinations. Training the model to recognize and refuse such questions is crucial for trustworthiness.
  - Quick check question: What evaluation metric is used to assess the model's ability to refuse questions about non-existing entities? (Answer: F1-score)

## Architecture Onboarding

- Component map: Foundation model (Qwen2) -> LSN training module -> Multilingual SFT data pipeline -> Safety and refusal training module -> Evaluation framework (SeaBench, SeaRefuse, MultiJail)

- Critical path: LSN training → SFT data construction → Multilingual instruction tuning → Safety training → Evaluation

- Design tradeoffs: LSN training reduces training costs but requires accurate detection of language-specific neurons. Balanced SFT data improves SEA language performance but increases data collection complexity. Safety training improves trustworthiness but may reduce the model's willingness to answer uncertain queries.

- Failure signatures: Degraded performance on high-resource languages (if LSNs interfere), imbalanced performance across SEA languages (if SFT data is not properly balanced), hallucinations or inappropriate responses (if safety training is insufficient).

- First 3 experiments:
  1. Test LSN training by enhancing a single SEA language and evaluating its performance against the baseline model.
  2. Evaluate the impact of balanced SFT data by training two models with different language distributions and comparing their SEA language performance.
  3. Assess the effectiveness of safety training by testing the model's refusal rate on questions about non-existing entities before and after safety training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do SeaLLMs 3's language-specific neurons (LSNs) compare to traditional continued pretraining methods in terms of preserving high-resource language performance while enhancing low-resource language capabilities?
- Basis in paper: [explicit] The paper discusses LSN training as an alternative to continued pretraining, claiming it maintains high-resource language performance while enhancing SEA languages.
- Why unresolved: While the paper mentions the advantages of LSNs, it does not provide direct comparative experiments between LSN training and traditional continued pretraining methods on the same model and dataset.
- What evidence would resolve it: Head-to-head comparison experiments showing performance metrics for both LSN training and continued pretraining on SeaLLMs 3, including both SEA and high-resource languages.

### Open Question 2
- Question: What is the optimal balance of language representation in the supervised fine-tuning (SFT) dataset to maximize performance across all SEA languages without compromising performance in English and Chinese?
- Basis in paper: [explicit] The paper mentions striving for a "relatively good balance of language representation" in the SFT data but does not specify the optimal distribution or provide experimental results showing different balance ratios.
- Why unresolved: The paper acknowledges the importance of language balance but does not experimentally determine the optimal ratio or provide guidelines for achieving it.
- What evidence would resolve it: Systematic experiments varying the language distribution ratios in SFT data and measuring the resulting performance across all target languages.

### Open Question 3
- Question: How does the hallucination reduction capability of SeaLLMs 3 compare to other state-of-the-art models when dealing with culture-specific versus general knowledge boundaries?
- Basis in paper: [explicit] The paper introduces SeaRefuse benchmark for evaluating refusal capabilities and mentions addressing both general and culture-specific safety considerations, but does not provide comparative analysis between these two aspects.
- Why unresolved: While the paper shows SeaLLMs 3's performance on the SeaRefuse benchmark, it does not analyze the model's performance separately for culture-specific versus general knowledge boundaries or compare it with other models on these specific aspects.
- What evidence would resolve it: Detailed analysis of SeaLLMs 3's refusal performance on culture-specific versus general knowledge questions, with comparative results against other state-of-the-art models on both categories.

## Limitations
- Claims about LSN independence and effectiveness are not fully validated through ablation studies or direct comparison with traditional continued pretraining methods.
- The SFT data construction process lacks specific details about language distribution percentages and task type ratios.
- Safety evaluation results, particularly for culture-specific safety, are presented without clear baseline comparisons or analysis of potential safety-utility tradeoffs.

## Confidence

- **High Confidence**: The overall architecture and methodology (LSN training + balanced SFT + safety training) is clearly described and technically sound. The evaluation framework using established benchmarks is appropriate.
- **Medium Confidence**: The reported performance improvements over baselines are significant but rely on comparisons with specific models that may not represent the full state of the art. The efficiency gains from LSN training are claimed but not independently verified.
- **Low Confidence**: Claims about culture-specific safety improvements and hallucination reduction lack detailed empirical validation. The paper does not provide error analysis or failure case studies.

## Next Checks

1. Conduct an ablation study comparing LSN training with traditional continued pretraining on the same foundation model, measuring both SEA language performance and high-resource language degradation.
2. Perform a detailed analysis of the safety model's behavior, including false positive rates for legitimate queries and the impact of safety training on overall model utility.
3. Test model generalization by evaluating on additional low-resource language tasks not included in the training data, particularly focusing on language pairs not represented in the SFT dataset.