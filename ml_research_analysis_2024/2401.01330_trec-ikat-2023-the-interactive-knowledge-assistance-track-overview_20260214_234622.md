---
ver: rpa2
title: 'TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview'
arxiv_id: '2401.01330'
source_url: https://arxiv.org/abs/2401.01330
tags:
- ptkb
- automatic
- response
- ranking
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The TREC iKAT 2023 track focused on personalized conversational
  search, requiring systems to adapt responses based on user profiles and prior interactions.
  The challenge involved three tasks: ranking personal text knowledge base (PTKB)
  statements, retrieving relevant passages, and generating appropriate responses.'
---

# TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview

## Quick Facts
- arXiv ID: 2401.01330
- Source URL: https://arxiv.org/abs/2401.01330
- Reference count: 4
- Primary result: Personalized conversational search using LLMs with PTKB adaptation

## Executive Summary
The TREC iKAT 2023 track focused on personalized conversational search, requiring systems to adapt responses based on user profiles and prior interactions. The challenge involved three tasks: ranking personal text knowledge base (PTKB) statements, retrieving relevant passages, and generating appropriate responses. Seven teams submitted 24 runs, primarily leveraging Large Language Models (LLMs) in their pipelines. Two main approaches emerged: retrieve-then-generate (R→G) and generate-retrieve-generate (G→R→G).

## Method Summary
The track utilized a ClueWeb22-B subset (116,838,987 passages) and 25 test topics with 326 turns. Participants implemented conversational search systems that could adapt to user personas through PTKB statements. The primary approaches were R→G pipelines using BM25 or dense retrieval followed by LLM re-ranking, and G→R→G pipelines that first generated answers using LLM internal knowledge, then retrieved passages to ground those answers, and finally generated responses. Evaluation metrics included nDCG for ranking tasks and GPT-4-based assessment for response quality.

## Key Results
- G→R→G approaches outperformed R→G methods in passage ranking, achieving nDCG@5 scores up to 0.44
- Llama-based models in zero-shot settings performed best for PTKB statement ranking with nDCG@3 scores reaching 0.73
- GPT-4-based systems showed highest groundedness scores (0.89) and naturalness ratings (4.0/5) for response quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: G→R→G approaches outperform R→G methods in passage ranking due to better grounding in the conversation context.
- Mechanism: The G→R→G pipeline first generates an answer using the LLM's internal knowledge, then retrieves passages to ground that answer, and finally generates a response. This approach ensures that the retrieved passages are more aligned with the conversation context.
- Core assumption: LLMs can effectively leverage their internal knowledge to generate contextually relevant initial answers.
- Evidence anchors:
  - [abstract]: "The G→R→G approaches generally outperformed R→G methods in passage ranking, achieving nDCG@5 scores up to 0.44."
  - [section]: "we observed that R→G approaches were outperformed by G→R→G approaches... This signals a shift in strategy – where first the LLM's internal knowledge is drawn upon by directly generating answers, and then the answers are grounded through the retrieval step, before the final response generation."
  - [corpus]: Weak corpus evidence; only general conversational search papers found.
- Break condition: If the LLM fails to generate a contextually relevant initial answer, the grounding process becomes ineffective.

### Mechanism 2
- Claim: Zero-shot prompting of Llama models is effective for PTKB statement ranking.
- Mechanism: Using zero-shot prompting allows Llama models to rank PTKB statements without needing fine-tuning on the specific dataset, leveraging the model's pre-existing knowledge.
- Core assumption: Pre-trained LLMs have sufficient knowledge to perform zero-shot ranking tasks effectively.
- Evidence anchors:
  - [abstract]: "For PTKB statement ranking, Llama-based models in zero-shot settings performed best with nDCG@3 scores reaching 0.73."
  - [section]: "Llama in the zero-shot setting, however, achieved the best result in PTKB statement ranking task based on both results."
  - [corpus]: No direct corpus evidence; weak coverage of zero-shot ranking.
- Break condition: If the pre-trained knowledge is not relevant to the specific PTKB content, zero-shot performance degrades.

### Mechanism 3
- Claim: GPT-4-based systems show highest groundedness and naturalness scores due to their advanced language understanding capabilities.
- Mechanism: GPT-4's superior language understanding allows it to generate responses that are both more natural-sounding and better grounded in the retrieved passages.
- Core assumption: GPT-4 has superior language understanding compared to other LLMs.
- Evidence anchors:
  - [abstract]: "Response quality was assessed using GPT-4, with GPT-4-based systems showing highest groundedness scores (0.89) and naturalness ratings (4.0/5)."
  - [section]: "Table 7 lists the results, where we saw that the GPT-4-based model outperforms other models by a large margin."
  - [corpus]: No corpus evidence; assessment done by GPT-4 itself introduces potential bias.
- Break condition: If the evaluation method itself is biased, the superiority claims may not hold under human evaluation.

## Foundational Learning

- Concept: Conversational information seeking
  - Why needed here: Understanding how users interact with conversational agents is crucial for designing effective systems.
  - Quick check question: What are the key differences between traditional search and conversational information seeking?

- Concept: Personalized search
  - Why needed here: The track focuses on adapting responses based on user profiles and prior interactions.
  - Quick check question: How does personalization affect the relevance of search results in conversational systems?

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are central to the approaches used in the track, both for retrieval and generation tasks.
  - Quick check question: What are the advantages and limitations of using LLMs in conversational search systems?

## Architecture Onboarding

- Component map: PTKB statement ranking -> Passage retrieval -> Response generation
- Critical path: Understanding user utterance -> Identifying relevant PTKB statements -> Retrieving passages -> Generating response
- Design tradeoffs: R→G vs G→R→G approaches involve tradeoffs between computational efficiency and response quality
- Failure signatures: Poor PTKB ranking leads to irrelevant passages; ineffective passage ranking results in poor responses; over-reliance on LLM knowledge without grounding causes hallucinations
- First 3 experiments:
  1. Compare R→G and G→R→G performance on a small data subset
  2. Test zero-shot prompting effectiveness across different LLM models for PTKB ranking
  3. Evaluate response groundedness and naturalness using both automated and human assessments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PTKB statement ranking accuracy correlate with final response quality across different conversational depths?
- Basis in paper: [explicit] The paper observes that PTKB ranking and passage retrieval performance don't clearly correlate, and notes that PTKB dependence led to lower performance at higher conversation depths
- Why unresolved: The paper evaluates these tasks separately but doesn't analyze their interaction or how PTKB accuracy at different conversation depths affects final response quality
- What evidence would resolve it: Empirical studies measuring response quality as a function of PTKB ranking accuracy at each conversation turn, controlling for other factors

### Open Question 2
- Question: What is the optimal pipeline order (R→G vs G→R→G) for different types of personalized conversational search tasks?
- Basis in paper: [explicit] The paper observes that G→R→G approaches generally outperformed R→G methods in passage ranking, but the opposite was true for PTKB ranking
- Why unresolved: While the paper identifies this trade-off, it doesn't explore when each approach is preferable or whether hybrid approaches might be optimal
- What evidence would resolve it: Systematic evaluation of both approaches across various task types, user personas, and conversation scenarios

### Open Question 3
- Question: How do different PTKB statement selection strategies (manual vs automatic) impact overall system performance?
- Basis in paper: [explicit] The paper includes both manual and automatic PTKB selection approaches in its baseline runs and notes performance differences
- Why unresolved: The paper doesn't deeply analyze why manual approaches sometimes outperform automatic ones or what characteristics make PTKB statements more or less useful
- What evidence would resolve it: Detailed error analysis of PTKB selection across different types of statements and conversations, identifying patterns in successful vs unsuccessful selections

## Limitations

- The evaluation of response quality using GPT-4 introduces potential bias, as the same model is used for both assessment and generation in some cases
- Limited diversity in team submissions (only 7 teams) may not represent the full spectrum of possible approaches
- The zero-shot prompting effectiveness for PTKB statement ranking lacks comprehensive comparison with fine-tuned alternatives

## Confidence

- Passage ranking mechanism (G→R→G vs R→G): Medium confidence based on observable performance differences but limited ablation studies
- Zero-shot Llama performance: Low confidence due to absence of comparative experiments with other models or fine-tuning baselines
- GPT-4 superiority claims: Low confidence given the self-referential evaluation methodology

## Next Checks

1. Conduct human evaluation of response quality across different model types to validate automated GPT-4 assessment results
2. Perform ablation studies on the G→R→G pipeline to isolate the contribution of each component (generation, retrieval, final generation)
3. Test zero-shot prompting effectiveness across multiple LLM architectures and compare against fine-tuned baselines on the PTKB ranking task