---
ver: rpa2
title: Preference Learning Algorithms Do Not Learn Preference Rankings
arxiv_id: '2405.19534'
source_url: https://arxiv.org/abs/2405.19534
tags:
- ranking
- accuracy
- preference
- training
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of preference learning
  algorithms like RLHF and DPO in aligning LLMs with human preferences. Surprisingly,
  the authors find that state-of-the-art preference-tuned models achieve less than
  60% ranking accuracy on common preference datasets.
---

# Preference Learning Algorithms Do Not Learn Preference Rankings

## Quick Facts
- arXiv ID: 2405.19534
- Source URL: https://arxiv.org/abs/2405.19534
- Reference count: 40
- Primary result: State-of-the-art preference-tuned LLMs achieve less than 60% ranking accuracy on common preference datasets

## Executive Summary
This paper reveals a fundamental limitation in preference learning algorithms like RLHF and DPO: despite their widespread adoption, they achieve surprisingly poor ranking accuracy on preference datasets. The authors show that even state-of-the-art preference-tuned models fail to correctly rank over 40% of preference pairs. Through theoretical analysis, they derive the idealized ranking accuracy achievable under perfect optimization and demonstrate a significant "alignment gap" between this ideal and observed performance. The study uncovers that these algorithms rarely correct incorrect rankings from the reference model and proposes a formula to quantify the difficulty of learning specific preference datapoints.

## Method Summary
The study investigates preference learning algorithms by first establishing a theoretical framework for idealized ranking accuracy under perfect optimization of DPO and RLHF objectives. The authors then measure actual ranking accuracy on common preference datasets (UltraFeedback, HH-RLLF, SHP, etc.) using various reference models (GPT2, Pythia, Llama2) and preference-tuned models (Llama2-Chat, Tulu2-DPO, Zephyr7B-DPO). They analyze the relationship between ranking accuracy and the win rate metric, and develop a formula to quantify the difficulty of learning specific preference datapoints based on the reference model's log-ratio. The methodology involves training models using HuggingFace's TRL library, computing ranking accuracy on validation sets, and comparing observed performance to theoretical limits.

## Key Results
- Preference-tuned models achieve less than 60% ranking accuracy on common preference datasets
- There exists a significant "alignment gap" between idealized and observed ranking accuracy
- Preference learning algorithms rarely correct incorrect rankings from the reference model
- Ranking accuracy strongly correlates with win rate when models are close to the reference model, but this correlation breaks as models move away

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DPO's objective is mathematically designed to optimize ranking accuracy under ideal conditions.
- **Mechanism:** The DPO loss is structured so that minimizing it should cause the model's likelihood ratio between preferred and dispreferred outputs to match the human preference ratio raised to the power of 1/β. This creates a direct link between the loss and correct ranking.
- **Core assumption:** The Bradley-Terry model accurately represents human preferences, and the dataset contains ground-truth preference proportions.
- **Evidence anchors:**
  - [abstract]: "We furthermore derive the idealized ranking accuracy that a preference-tuned LLM would achieve if it optimized the DPO or RLHF objective perfectly."
  - [section]: "Theorem 3.1 (Simulating Perfect RLHF)... Let π∗ be the model resulting from perfectly optimizing the DPO or RLHF objective on (x, yw, yl) as described in Section 2.1. Then, π∗ satisfies π∗(yw | x)/π∗(yl | x) = πRef(yw | x)/πRef(yl | x) × (α(x, yw, yl)/(1 − α(x, yw, yl)))^{1/β}"
- **Break condition:** The mechanism breaks when the reference model already has incorrect rankings, making it nearly impossible for DPO to correct them (Theorem 4.1).

### Mechanism 2
- **Claim:** Reference model quality critically determines whether DPO can improve ranking accuracy.
- **Mechanism:** DPO's objective includes regularization toward the reference model. If the reference model assigns higher likelihood to the dispreferred output, DPO struggles to flip this ranking because it must overcome both the data signal and the strong regularization.
- **Core assumption:** The reference model's log-ratio for a datapoint is a reliable predictor of whether DPO can learn to rank it correctly.
- **Evidence anchors:**
  - [abstract]: "We attribute this discrepancy to the DPO objective, which is empirically and theoretically ill-suited to fix even mild ranking errors in the reference model"
  - [section]: "Theorem 4.1... R(x, yw, yl) = 1 if and only if LDPO(x, yw, yl) ≤ −log σ(βc), where c is the reference model log-ratio"
- **Break condition:** When the reference model log-ratio is large and positive (reference model prefers dispreferred output), the DPO loss must be reduced to an extremely small value to flip the ranking, which rarely happens in practice.

### Mechanism 3
- **Claim:** The relationship between ranking accuracy and win rate depends on the model's distance from the reference model.
- **Mechanism:** When the model is close to the reference model, improving ranking accuracy also improves win rate because both metrics reflect similar preferences. As the model moves away, the metrics diverge because win rate reflects generative quality while ranking accuracy only measures classification.
- **Core assumption:** The distance between model and reference model weights is a reliable proxy for how far the model has moved from the initial preference distribution.
- **Evidence anchors:**
  - [abstract]: "ranking accuracy strongly correlates with the empirically popular win rate metric when the model is close to the reference model"
  - [section]: "In both settings, we observe that the win rate and ranking accuracy are highly correlated with one another in the early phase of training but become anti-correlated... as the model πθ moves away from the reference πRef"
- **Break condition:** When the model has moved sufficiently far from the reference model, the win rate begins to decline even as ranking accuracy improves, indicating that optimizing for ranking accuracy alone degrades generative quality.

## Foundational Learning

- **Concept:** Bradley-Terry model for pairwise comparisons
  - **Why needed here:** The theoretical analysis of DPO and RLHF's ability to learn rankings relies on this model to represent ground-truth human preferences as a function of latent reward values.
  - **Quick check question:** If P(y1 ≻ y2 | x) = 0.8 and P(y2 ≻ y1 | x) = 0.2 under the Bradley-Terry model, what is the ratio of the latent rewards r*(x, y1)/r*(x, y2)?

- **Concept:** KL divergence regularization in preference learning
  - **Why needed here:** Understanding why DPO and RLHF include regularization toward the reference model requires grasping how KL divergence constrains the learned policy to stay close to the initialization.
  - **Quick check question:** In the DPO objective, what role does the reference model play in preventing the learned policy from deviating too far from the initialization?

- **Concept:** Difference between on-policy and off-policy evaluation
  - **Why needed here:** The paper's analysis of ranking accuracy (an off-policy metric) versus win rate (an on-policy metric) requires understanding how these different evaluation paradigms capture different aspects of model behavior.
  - **Quick check question:** Why might a model achieve high ranking accuracy on preference data but still have poor win rate when generating responses?

## Architecture Onboarding

- **Component map:** Reference model -> Preference dataset -> DPO/RLHF objective -> Ranking accuracy metric -> Win rate metric

- **Critical path:**
  1. Load reference model and preference dataset
  2. Compute reference model rankings to identify already-correct/incorrect datapoints
  3. Train model using DPO or RLHF objective
  4. Measure ranking accuracy on validation set
  5. Compare to idealized ranking accuracy using Theorem 3.1
  6. Analyze relationship between ranking accuracy and win rate

- **Design tradeoffs:**
  - Stronger regularization (higher β) makes DPO more stable but harder to learn from data
  - Weaker regularization allows more learning but risks overfitting or catastrophic forgetting
  - Using reference models with poor ranking accuracy creates alignment gaps that are difficult to overcome
  - Computing idealized ranking accuracy requires ground-truth preference proportions, which are rarely available

- **Failure signatures:**
  - Low ranking accuracy despite low DPO loss indicates reference model has incorrect rankings
  - Anti-correlation between ranking accuracy and win rate suggests the model has moved too far from the reference model
  - Large alignment gaps between observed and idealized ranking accuracy suggest fundamental limitations in the preference learning algorithm

- **First 3 experiments:**
  1. Measure ranking accuracy of various reference models on common preference datasets to establish baseline performance
  2. Compute idealized ranking accuracy for different β values to understand theoretical limits
  3. Train DPO models and track how ranking accuracy and win rate evolve relative to the reference model

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do preference learning algorithms generalize to out-of-distribution data?
- **Basis in paper:** [inferred] The paper discusses that ranking accuracy is measured on the training dataset and notes that the theoretical results only describe model behavior on the preference data used during training, making it difficult to draw general claims about other distributions.
- **Why unresolved:** The paper acknowledges the gap between in-distribution training data and out-of-distribution test data but does not empirically investigate how well the learned models perform on data outside their training distribution.
- **What evidence would resolve it:** Experiments comparing ranking accuracy and win rate on out-of-distribution datasets or through domain adaptation techniques would provide evidence of generalization capabilities.

### Open Question 2
- **Question:** What are the optimization dynamics of preference learning algorithms, and how do they affect the relationship between ranking accuracy and win rate?
- **Basis in paper:** [inferred] The paper mentions the intriguing relationship between ranking accuracy and win rate and suggests analyzing optimization dynamics to understand when win rate diverges from ranking accuracy.
- **Why unresolved:** While the paper observes the correlation between ranking accuracy and win rate, it does not provide a detailed analysis of the optimization dynamics that lead to this relationship.
- **What evidence would resolve it:** A detailed study of the training dynamics, including loss curves, gradient flows, and changes in model parameters over time, would shed light on the optimization process and its impact on ranking accuracy and win rate.

### Open Question 3
- **Question:** How do alignment techniques interact with other calibration metrics, and what is their impact on model performance?
- **Basis in paper:** [explicit] The paper suggests that it is worthwhile to explore how alignment techniques interact with other calibration metrics in the discussion section.
- **Why unresolved:** The paper focuses on ranking accuracy as a measure of alignment but does not investigate how other calibration metrics, such as precision, recall, or F1-score, are affected by preference learning algorithms.
- **What evidence would resolve it:** Experiments measuring the impact of preference learning algorithms on various calibration metrics would provide insights into their effectiveness and potential trade-offs.

## Limitations

- The theoretical framework assumes the Bradley-Terry model accurately captures human preferences, which may not hold in practice
- The idealized ranking accuracy calculation requires knowledge of ground-truth preference proportions that are rarely available in real datasets
- The study focuses on pairwise preference data, but real-world preference learning often involves more complex feedback signals

## Confidence

- **High confidence:** The theoretical derivation of idealized ranking accuracy and the alignment gap analysis are mathematically rigorous and well-supported by the proofs provided
- **Medium confidence:** The empirical finding that preference-tuned models achieve <60% ranking accuracy, as this depends on the specific datasets and models evaluated
- **Medium confidence:** The claim that DPO's objective is ill-suited to fix ranking errors in reference models, though this is strongly supported by Theorem 4.1

## Next Checks

1. Replicate the ranking accuracy experiments across a broader set of preference datasets and model architectures to verify the <60% accuracy finding is consistent
2. Test whether the alignment gap persists when using reference models with near-perfect ranking accuracy on the training data
3. Evaluate the proposed formula for quantifying learning difficulty on out-of-distribution preference data to test its generalizability