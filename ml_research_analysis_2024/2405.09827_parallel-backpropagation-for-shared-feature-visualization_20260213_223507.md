---
ver: rpa2
title: Parallel Backpropagation for Shared-Feature Visualization
arxiv_id: '2405.09827'
source_url: https://arxiv.org/abs/2405.09827
tags:
- features
- images
- image
- responses
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel deep learning method for visualizing
  features that drive responses of category-selective neurons to out-of-category stimuli.
  The approach uses a learned neuron model to predict responses to images based on
  latent CNN activations, then identifies the most similar within-category reference
  image using a neuron-specific similarity metric.
---

# Parallel Backpropagation for Shared-Feature Visualization

## Quick Facts
- arXiv ID: 2405.09827
- Source URL: https://arxiv.org/abs/2405.09827
- Reference count: 40
- Key outcome: Method visualizes features driving category-selective neuron responses to out-of-category stimuli by finding shared features with within-category images

## Executive Summary
This paper introduces a deep learning approach for visualizing which visual features drive category-selective neurons to respond to out-of-category stimuli. The method learns a linear readout model to predict neural responses from CNN latent activations, then identifies the most similar within-category reference image using a neuron-specific similarity metric. Parallel backpropagation with reweighting highlights shared features between the out-of-category stimulus and reference image. Applied to body-selective neurons in macaque IT cortex, the method reveals that objects activate neurons due to parts resembling macaque body parts like limbs, heads, and torsos.

## Method Summary
The method uses a pre-trained ResNet-50 to extract CNN features from images, then learns a linear readout vector to predict neural responses to within-category stimuli. For each out-of-category stimulus, it computes a neuron-specific similarity metric to find the most similar within-category reference image, weighting features by the learned readout weights. Parallel backpropagation computes gradients of predicted neural responses with respect to pixel intensities for both images, which are reweighted to emphasize features present in both images and highly relevant for the neural response. The resulting visualizations highlight shared features that drive the neural response.

## Key Results
- Models generalized from bodies to objects with 93.7-95.3% of channels showing significant positive correlation for object predictions
- Visualizations revealed objects activating neurons due to similarity to macaque body parts including limbs, heads, torsos, and other body features
- The method discovered a variety of shared features between highly activating bodies and objects, mostly corresponding to body parts rather than entire bodies

## Why This Works (Mechanism)

### Mechanism 1
The method identifies shared visual features between out-of-category stimuli and preferred-category images by computing neuron-specific similarity weighted by learned readout weights. It learns a linear readout vector that predicts neural responses based on latent CNN activations of within-category images, then computes a similarity metric that weights feature dimensions by these learned weights, effectively focusing on dimensions that drive neural responses.

Core assumption: The learned readout weights accurately capture which visual features drive the neuron's responses to within-category stimuli, and these same features drive responses to out-of-category stimuli.

### Mechanism 2
Parallel backpropagation with reweighting highlights shared features by emphasizing dimensions that are both highly activated in both images and highly relevant for the neural response. The method computes gradients of predicted neural responses with respect to pixel intensities for both images, then reweights them by a factor combining activation strength in both images and learned readout weights.

Core assumption: The gradient of the predicted neural response with respect to pixel intensities provides meaningful attribution of feature importance, and the reweighting scheme effectively highlights shared features.

### Mechanism 3
The method's visualizations reveal object parts resembling body parts because body-selective neurons encode overlapping visual features for bodies and objects. By finding the most similar within-category image and highlighting shared features, the method reveals which object parts activate the neuron due to similarity to body parts.

Core assumption: Body-selective neurons indeed encode overlapping visual features for bodies and objects, and these features can be identified through the similarity-based approach.

## Foundational Learning

- **Concept**: Convolutional Neural Networks and their feature extraction capabilities
  - Why needed here: The method relies on a pre-trained CNN to extract latent activations that capture visual features relevant for neural responses
  - Quick check question: What is the role of the CNN in this method, and how do its latent activations relate to neural responses?

- **Concept**: Linear readout models for predicting neural responses
  - Why needed here: The method learns a linear readout vector on top of CNN features to predict neural responses to within-category stimuli, which is crucial for identifying relevant features
  - Quick check question: How does the linear readout vector capture information about which visual features drive neural responses?

- **Concept**: Gradient-based attribution methods for visualizing feature importance
  - Why needed here: The method uses backpropagation to compute gradients of predicted neural responses with respect to pixel intensities, which are then reweighted to highlight shared features
  - Quick check question: What is the purpose of computing gradients in this method, and how does the reweighting scheme highlight shared features?

## Architecture Onboarding

- **Component map**: Pretrained ResNet-50 -> Linear readout model -> Neuron-specific similarity metric -> Parallel backpropagation with reweighting -> Visualization component

- **Critical path**:
  1. Extract latent activations from CNN for both out-of-category and within-category images
  2. Train linear readout model on within-category data to predict neural responses
  3. Compute neuron-specific similarity to find most similar within-category image
  4. Compute parallel backpropagation gradients with reweighting
  5. Visualize shared features

- **Design tradeoffs**:
  - Using a pre-trained CNN provides good feature representations but may not perfectly match neural tuning properties
  - The linear readout model is simple but may not capture complex nonlinear relationships
  - The neuron-specific similarity metric focuses on relevant features but may miss important dimensions
  - Parallel backpropagation provides detailed visualizations but is computationally intensive

- **Failure signatures**:
  - Poor model performance on out-of-category images indicates the readout weights do not generalize
  - Low similarity scores between out-of-category and within-category images suggest no shared features
  - Noisy or uninformative visualizations indicate issues with the gradient computation or reweighting

- **First 3 experiments**:
  1. Test model performance on out-of-category images to ensure readout weights generalize
  2. Verify neuron-specific similarity metric identifies reasonable within-category images
  3. Check gradient computation and reweighting produce meaningful visualizations

## Open Questions the Paper Calls Out

### Open Question 1
Why do some object images activate body-selective neurons while others don't, and what determines the strength of activation? The paper identifies that feature similarity explains some cases but doesn't fully explain all instances of object activation. Some objects show high neural responses without clear shared features with body images.

### Open Question 2
How do the learned feature weights (w vector) for body-selective neurons relate to known visual features like animacy, spikiness, or roundness that have been proposed to explain category selectivity? The paper shows that some neurons respond to "spiky" objects while others respond to "stubby" objects, contradicting the unified view that all body cells prefer spiky objects.

### Open Question 3
Do the feature visualizations generalize across different recording sites within the same body-selective region, or are they site-specific? The authors note that "for neighbouring recording sites, visualizations are often similar, likely due to similar feature preferences" but also observe "the same object with different highlighted features in different recording channels."

## Limitations

- The method relies on pre-trained CNN features that may not perfectly align with neural tuning properties
- No validation of whether the highlighted shared features actually drive neural responses in a causal manner
- The specific implementation details of the parallel backpropagation reweighting scheme are underspecified
- Limited evaluation of how sensitive results are to choices of CNN architecture or similarity metrics

## Confidence

- **Medium**: The core mechanism of using neuron-specific similarity weighted by learned readout weights is well-specified and theoretically sound
- **Medium**: The claim that visualizations reveal object parts resembling body parts is supported by qualitative results
- **Low**: The generalizability of the method beyond body-selective neurons to other category-selective populations is not demonstrated

## Next Checks

1. **Generalization test**: Apply the method to a different category-selective neuron population (e.g., face-selective neurons) and validate whether it correctly identifies shared features between faces and face-like objects

2. **Ablation study**: Remove the reweighting component from the parallel backpropagation to quantify how much it contributes to highlighting shared versus unique features

3. **Causal validation**: Use targeted stimulus manipulation to test whether the highlighted shared features actually drive neural responses, rather than just correlating with them