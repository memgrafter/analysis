---
ver: rpa2
title: Attacking Large Language Models with Projected Gradient Descent
arxiv_id: '2402.09154'
source_url: https://arxiv.org/abs/2402.09154
tags:
- arxiv
- adversarial
- entropy
- gradient
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for attacking large language
  models (LLMs) using Projected Gradient Descent (PGD) on a continuously relaxed input
  prompt. The authors demonstrate that by carefully controlling the error introduced
  by the continuous relaxation, their PGD for LLMs can be up to one order of magnitude
  faster than state-of-the-art discrete optimization while achieving the same attack
  success rates.
---

# Attacking Large Language Models with Projected Gradient Descent

## Quick Facts
- **arXiv ID:** 2402.09154
- **Source URL:** https://arxiv.org/abs/2402.09154
- **Reference count:** 38
- **Primary result:** PGD on continuously relaxed prompts achieves up to 10x speedup over discrete optimization while maintaining same attack success rates

## Executive Summary
This paper introduces a novel Projected Gradient Descent (PGD) attack method for large language models that operates on a continuously relaxed representation of the input prompt. By carefully controlling the approximation error through entropy projection and introducing flexible sequence length optimization, the authors demonstrate that their approach can achieve the same devastating jailbreaking effectiveness as state-of-the-art discrete optimization methods while being up to one order of magnitude faster. The method is validated across multiple LLM architectures and jailbreaking tasks, showing consistent improvements in both attack success rates and computational efficiency.

## Method Summary
The method continuously relaxes discrete input tokens into probability distributions over the vocabulary, then applies gradient-based optimization with entropy projection to maintain discrete-like solutions. A mask parameter enables flexible sequence length by smoothly inserting or removing tokens through causal attention masking. The approach uses Adam optimization with gradient clipping and projects onto both the probability simplex and an entropy hypersphere using Tsallis entropy (q=2). This allows efficient gradient-based search for adversarial prompts while maintaining control over the approximation error introduced by the continuous relaxation.

## Key Results
- Achieves mean target probability of 1.0 for "behavior" jailbreaking task on Falcon 7B Instruct within 100 seconds
- Up to 10x faster than state-of-the-art discrete optimization (GCG) while maintaining same attack success rates
- Outperforms gradient-based attack GBDA in both efficiency and effectiveness across tested models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continuous relaxation with entropy projection effectively controls the error introduced by relaxing discrete tokens, enabling gradient-based optimization to find adversarial prompts efficiently.
- **Mechanism:** By projecting onto both the simplex (maintaining valid probability distributions) and the entropy hypersphere (controlling the Gini index), the method ensures sparse, discrete-like solutions while allowing smooth gradient updates.
- **Core assumption:** The Tsallis entropy projection (Gini index) with q=2 can effectively constrain the entropy to promote discrete-like solutions without hindering optimization.
- **Evidence anchors:** [abstract]: "we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy"
- **Break condition:** If the entropy projection becomes too restrictive, it may prevent the optimizer from finding valid adversarial prompts, especially in the early stages of training when the prompt space is being explored.

### Mechanism 2
- **Claim:** Flexible sequence length optimization allows the attack to dynamically add or remove tokens, improving attack effectiveness.
- **Mechanism:** A mask parameter m ∈ [0,1]^L is optimized alongside the token embeddings. When mi=0, token i is masked out; when mi>0, additional tokens can be smoothly inserted via causal attention masking.
- **Core assumption:** The ability to vary sequence length during optimization provides sufficient flexibility to discover more effective adversarial suffixes than fixed-length approaches.
- **Evidence anchors:** [abstract]: "We continuously relax the addition/removal of tokens and optimize over a variable length sequence"
- **Break condition:** If the attention masking implementation doesn't properly handle variable sequence lengths, the optimization could produce invalid attention patterns that break the LLM's generation.

### Mechanism 3
- **Claim:** Efficient gradient-based optimization can achieve comparable attack success rates to discrete optimization while being significantly faster.
- **Mechanism:** The continuous relaxation allows for standard gradient descent with Adam optimizer, avoiding the expensive discrete search space traversal required by methods like GCG.
- **Core assumption:** The continuous relaxation sufficiently preserves the structure of the discrete optimization problem so that gradient information remains meaningful for finding adversarial prompts.
- **Evidence anchors:** [abstract]: "Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results"
- **Break condition:** If the continuous relaxation introduces too much error or the gradient signal becomes too noisy, the optimization may converge to ineffective adversarial prompts despite the speed advantage.

## Foundational Learning

- **Concept:** Projected Gradient Descent (PGD) on continuous domains
  - Why needed here: PGD is the foundation for the attack method, providing the optimization framework that iteratively updates the relaxed prompt representation
  - Quick check question: How does PGD differ from standard gradient descent in terms of handling constraints on the optimization space?

- **Concept:** Continuous relaxation of discrete variables
  - Why needed here: Language tokens are discrete, but gradient-based optimization requires continuous parameters. The relaxation bridges this gap while maintaining the ability to recover discrete solutions
  - Quick check question: What are the key properties that a continuous relaxation should have to effectively represent discrete tokens?

- **Concept:** Entropy regularization and projection
  - Why needed here: The continuous relaxation introduces approximation error. Entropy projection helps control this error and encourages the solution to be close to discrete distributions
  - Quick check question: Why is the Gini index (Tsallis entropy with q=2) specifically chosen for the entropy projection in this context?

## Architecture Onboarding

- **Component map:** Input prompt → Continuous relaxation → Entropy projection → Sequence length control → Adam optimizer → Discretization → Adversarial output

- **Critical path:**
  1. Initialize relaxed one-hot encoding from original prompt
  2. Compute gradient of loss w.r.t. relaxed encoding
  3. Update parameters with Adam
  4. Project onto simplex
  5. Project onto entropy hypersphere
  6. Discretize to obtain token sequence
  7. Evaluate loss and check for improvement

- **Design tradeoffs:**
  - Continuous relaxation vs. discrete search: Speed vs. precision in navigating the prompt space
  - Entropy projection strength: Too weak loses the benefits; too strong restricts valid solutions
  - Sequence length flexibility: Additional optimization complexity vs. improved attack success

- **Failure signatures:**
  - Gradient explosion despite clipping → Check learning rate and initialization
  - No improvement in target probability → Verify entropy projection isn't too restrictive
  - Memory issues with large batch sizes → Reduce batch size or use gradient accumulation
  - Invalid token sequences after discretization → Check tokenization consistency

- **First 3 experiments:**
  1. Compare PGD with GBDA on a small benchmark (e.g., Vicuna 1.3 7B) to verify the entropy projection provides measurable improvement
  2. Test the variable sequence length feature by initializing with a short prompt and verifying tokens can be added/removed effectively
  3. Measure runtime efficiency by comparing PGD vs. GCG on the same hardware, varying batch sizes to find the optimal tradeoff

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the traditional sense, but the discussion section implies several important directions for future research:

1. **Black-box adaptation:** The current method assumes white-box access to model gradients, limiting its applicability to models like ChatGPT. Adapting the approach for black-box scenarios would be valuable for real-world applications.

2. **Bias characterization:** While the paper notes that PGD and GCG may produce adversarial suffixes with different properties due to implicit biases, it doesn't investigate or characterize these biases in detail.

3. **Scalability analysis:** The paper demonstrates efficiency gains on specific models but doesn't systematically analyze how runtime scales with model size and sequence length compared to discrete optimization methods.

## Limitations

- The continuous relaxation introduces approximation error that, while controlled by entropy projection, cannot be fully eliminated
- The method requires white-box access to model gradients, limiting its applicability to black-box models like ChatGPT
- Runtime efficiency claims are based on specific hardware and batch size configurations that may not generalize across different computational environments

## Confidence

- **High confidence:** The core mechanism of continuous relaxation with entropy projection is well-founded in optimization theory and the experimental results demonstrate measurable improvements in attack success rates within reasonable timeframes.
- **Medium confidence:** The variable sequence length optimization mechanism appears sound in theory, but the practical implementation details are not fully specified, creating uncertainty about edge cases.
- **Low confidence:** The claim that this approach is "one order of magnitude faster" than state-of-the-art discrete optimization requires additional validation across different hardware configurations and batch sizes.

## Next Checks

1. **Hyperparameter sensitivity analysis:** Systematically vary the entropy projection strength (Gini index threshold) and learning rate to quantify their impact on both attack success rate and runtime.

2. **Discrete vs. continuous approximation error:** Implement a hybrid approach that occasionally evaluates the discrete token sequence during optimization to measure the actual approximation error introduced by the continuous relaxation.

3. **Cross-model generalization:** Test the attack method across a broader range of LLM architectures (including non-Transformer models) and safety alignment techniques to validate whether the entropy projection approach generalizes beyond the specific models evaluated in the paper.