---
ver: rpa2
title: A Self-Supervised Method for Body Part Segmentation and Keypoint Detection
  of Rat Images
arxiv_id: '2405.04650'
source_url: https://arxiv.org/abs/2405.04650
tags:
- segmentation
- keypoint
- body
- detection
- part
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a self-supervised method for body part segmentation
  and keypoint detection of rat images. The proposed method addresses the challenge
  of analyzing the behavior of laboratory animals, such as rats, in medical research.
---

# A Self-Supervised Method for Body Part Segmentation and Keypoint Detection of Rat Images

## Quick Facts
- arXiv ID: 2405.04650
- Source URL: https://arxiv.org/abs/2405.04650
- Authors: László Kopácsi; Áron Fóthi; András Lőrincz
- Reference count: 17
- Primary result: Self-supervised method achieves 61.92% AP on instance segmentation, 77.53% on keypoint detection, and 28.87% on body part segmentation of rat images

## Executive Summary
This paper presents a self-supervised method for body part segmentation and keypoint detection in rat images, addressing the challenge of analyzing laboratory animal behavior in medical research. The approach eliminates manual labeling by generating initial annotations through computer vision techniques, then training deep neural networks on the generated data. The method effectively handles heavy occlusions and highly similar objects through a combination of foreground-background segmentation, medial axis transformation, watershed algorithms, and extensive augmentation techniques. The results demonstrate that the trained models significantly outperform the initial computer vision-based approach, with potential applications for accelerating annotation processes and analyzing interactions between instances.

## Method Summary
The method employs a two-stage approach: first, computer vision techniques generate initial annotations through foreground-background segmentation using mode-based background estimation, followed by medial axis transformation and watershed algorithms to identify keypoints and body parts. Second, these generated labels are used to train Mask R-CNN models with ResNet50+FPN backbone in a self-supervised manner. The pipeline includes extensive augmentation to simulate occlusions, including cutting out rat segments, filling with background, and shifting segments near other instances, combined with rotation, scaling, and thin-plate-spline warping. The approach specifically addresses the challenges of analyzing rat behavior in controlled laboratory conditions where animals may be heavily occluded or in similar poses.

## Key Results
- Average precision (AP) of 61.92% achieved on instance segmentation, up from 53.22% baseline
- Keypoint detection improved to 77.53% AP from 48.91% baseline
- Body part segmentation reached 28.87% AP, though noted as the weakest component
- Extensive augmentation enabled the system to handle heavy occlusions and similar objects effectively
- Performance improvements demonstrate the viability of self-supervised training on CV-generated annotations

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised training on computer vision-generated annotations improves both keypoint detection and instance segmentation accuracy beyond the initial CV approach. The process replaces manual effort by generating foreground-background segmentation, then applying CV techniques (medial axis, watershed, skeleton analysis) to derive keypoints and body part labels. These labels are used to train deep networks, which learn to handle occlusions and similarities better than the CV-only approach. The core assumption is that CV-generated labels, though imperfect, contain enough structural signal for a neural network to learn robust feature representations and improve generalization.

### Mechanism 2
Augmentation techniques that simulate occlusions allow the network to generalize to real-world occlusion scenarios. The pipeline cuts out one rat segment, fills it with background, and shifts the segment near the other rat. Combined with smoothing, rotation, scaling, and thin-plate-spline warping, this simulates the occlusion patterns seen in the dataset. The core assumption is that real occlusions in the dataset can be effectively approximated by these synthetic augmentations, enabling the network to learn robust representations for handling occlusion.

### Mechanism 3
The medial axis transformation and watershed algorithms reliably identify keypoints and body parts on non-occluded rats. After foreground-background segmentation, the skeleton and distance transform are computed. The longest-minimum-cost path between skeleton endpoints identifies head and tail ends. Watershed on the distance transform assigns head vs tail based on segment area. The median distance transform point on the midline gives the base of the tail. A second watershed segments body parts. The core assumption is that the rat pose can be accurately represented by a single midline and distance transform, with the head always being the larger of the two endpoint regions.

## Foundational Learning

- Concept: Foreground-background segmentation using stationary camera assumptions
  - Why needed here: The dataset is recorded by a stationary camera; mode-based background subtraction isolates rats from the static scene
  - Quick check question: What would happen if the camera had motion or lighting changes? Would mode subtraction still work?

- Concept: Medial axis transformation and skeleton analysis for keypoint localization
  - Why needed here: Skeletons provide a compact representation of the rat's midline, enabling identification of head, tail, and base-of-tail keypoints without manual annotation
  - Quick check question: Why is smoothing the mask boundary important before computing the skeleton?

- Concept: Watershed segmentation for partitioning connected components into body parts
  - Why needed here: Once keypoints are located, watersheds on the distance transform can delineate head, body, and tail regions automatically
  - Quick check question: What property of the distance transform makes it suitable as a marker for watershed-based body part segmentation?

## Architecture Onboarding

- Component map: Background subtraction → Foreground mask → Pre-processing (hole removal, closing, B-spline smoothing) → Medial axis transform → Skeleton + distance map → Longest-minimum-cost path → Head and tail endpoints → Watershed on distance map → Classify head vs tail → Median point on midline → Base of tail → Second watershed → Body part segmentation → Augmentation pipeline (occlusion simulation, warping, rotation, scaling) → Mask R-CNN backbone (ResNet50 + FPN) → RPN → ROI heads (keypoint + instance / body part)

- Critical path: CV labeling pipeline → Data augmentation → Deep network training → Evaluation on hand-annotated test set

- Design tradeoffs:
  - Using Mask R-CNN vs transformer-based detectors: Faster to implement, but limited by NMS for close instances
  - Extensive augmentation vs overfitting: Heavy augmentation can improve occlusion handling but may require more training iterations
  - No smoothing augmentation: Improved results suggest raw mask geometry is more informative for the network

- Failure signatures:
  - Poor instance segmentation when rats are very close (NMS suppression)
  - CV labeling failures when rats are occluded or in curled poses
  - Overfitting during body part segmentation (performance drops after 5K iterations)

- First 3 experiments:
  1. Verify CV labeling pipeline on a small set of non-occluded frames; check keypoint and body part correctness
  2. Train Mask R-CNN on CV-labeled data with only rotation/scale augmentation; measure AP
  3. Add occlusion simulation augmentation; compare performance to step 2 and baseline CV method

## Open Questions the Paper Calls Out

### Open Question 1
How effective would temporal extension methods like bipartite matching or optical flow be in tracking heavily occluded and similar objects over video sequences? The paper suggests extending the system to handle video sequences via bipartite matching or optical flow to study interactions in an automated fashion. This remains unresolved because the current method only works on individual frames and does not leverage temporal information or motion cues that could help resolve occlusions. Evaluating the proposed temporal extension methods on the same rat video dataset and comparing performance metrics to the current frame-by-frame approach would provide evidence.

### Open Question 2
Would changing the architecture from Mask R-CNN or modifying the non-maximum suppression approach improve instance segmentation performance when objects are very close to each other? The paper mentions that the keypoint detection model cannot detect objects when they are very close due to limitations of Mask R-CNN's non-maximum suppression. This remains unresolved because the current Mask R-CNN architecture is not well-suited for separating instances in dense occlusion scenarios. Implementing and evaluating alternative architectures or NMS modifications on the rat dataset and measuring improvements in instance segmentation AP would provide evidence.

### Open Question 3
What additional augmentation techniques could be applied to further improve body part segmentation performance beyond rotation and scaling? The paper notes that the added value of the applied augmentation was modest and more augmentation might be needed to reach better results on body part segmentation. This remains unresolved because the current augmentation pipeline only slightly perturbs body parts, limiting its impact on segmentation performance. Experimenting with more aggressive or targeted augmentation techniques (e.g., part-specific deformations) and measuring their effect on body part segmentation AP would provide evidence.

## Limitations
- Dataset specificity: Method performance highly dependent on controlled conditions (stationary camera, white rats on black background)
- CV labeling reliability: Performance on complex poses or heavy occlusions not thoroughly validated
- Body part segmentation: Weakest component at 28.87% AP with rapid overfitting after few thousand iterations

## Confidence

**High Confidence**: The self-supervised training framework (CV labeling → deep network training) is sound and well-documented. The improvement from 53.22% to 61.92% AP in instance segmentation is measurable and reproducible.

**Medium Confidence**: The augmentation strategy for handling occlusions is plausible but not extensively validated. The claim that occlusion simulation improves generalization is supported by results but lacks ablation studies.

**Low Confidence**: The reliability of the medial axis transformation and watershed-based CV labeling pipeline for keypoint detection in all poses. The paper does not provide comprehensive error analysis of the CV-generated annotations.

## Next Checks
1. Manually annotate a subset of non-occluded frames and compare keypoint and body part labels against the CV-generated annotations to quantify labeling accuracy.
2. Create a test set with controlled occlusions (partially hiding rats) and evaluate whether the trained models maintain performance compared to non-occluded cases.
3. Apply the trained models to a different rat dataset or animal imaging dataset with similar characteristics to assess generalization beyond the original controlled conditions.