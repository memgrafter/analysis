---
ver: rpa2
title: 'WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents'
arxiv_id: '2410.07484'
source_url: https://arxiv.org/abs/2410.07484
tags:
- action
- rules
- rule
- success
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of building reliable world models
  for embodied agents using large language models (LLMs). The core method idea is
  to align LLMs with environment dynamics by learning complementary rules through
  a neurosymbolic approach.
---

# WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents

## Quick Facts
- arXiv ID: 2410.07484
- Source URL: https://arxiv.org/abs/2410.07484
- Reference count: 40
- The proposed method achieves 15-30% higher success rates than baselines in Minecraft while using 60-80% of tokens and 8-20 fewer replanning rounds

## Executive Summary
This paper addresses the challenge of building reliable world models for embodied agents using large language models (LLMs). The proposed WALL-E method aligns LLMs with environment dynamics by learning complementary rules through a neurosymbolic approach, then uses model-predictive control (MPC) with the aligned world model for action optimization. The framework demonstrates significant improvements over existing methods in open-world environments like Minecraft and ALFWorld, achieving record-high success rates with reduced computational costs.

## Method Summary
WALL-E employs a neurosymbolic approach that combines LLM-based world modeling with learned rules through gradient-free rule learning. The agent first uses an LLM to predict next states, then compares these predictions with real trajectories from the environment. When predictions fail, the system learns new rules via LLM inductive reasoning, translates them to executable code, and applies them to refine the world model. The aligned world model is then used with MPC to optimize look-ahead actions. Rule pruning via maximum coverage optimization keeps the rule set minimal and efficient. The method is evaluated on Minecraft and ALFWorld tasks, showing superior performance in success rates, replanning rounds, and token usage compared to baseline approaches.

## Key Results
- In Minecraft, WALL-E achieves 15-30% higher success rates than baselines while using only 60-80% of tokens and 8-20 fewer replanning rounds
- In ALFWorld, WALL-E reaches a record 95% success rate after only 6 iterations
- The rule learning process converges quickly, finding a compact set of rules within 4 iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rule learning bridges the gap between LLM prior knowledge and specific environment dynamics
- Mechanism: By comparing predicted vs real trajectories, identifying failed predictions, and learning new rules through LLM inductive reasoning, the method creates a complementary knowledge layer that addresses specific environmental constraints
- Core assumption: Only a few complementary rules are needed to align LLM predictions with environment dynamics due to the rich prior knowledge already