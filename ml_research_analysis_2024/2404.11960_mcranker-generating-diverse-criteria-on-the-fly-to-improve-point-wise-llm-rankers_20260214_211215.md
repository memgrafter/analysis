---
ver: rpa2
title: 'MCRanker: Generating Diverse Criteria On-the-Fly to Improve Point-wise LLM
  Rankers'
arxiv_id: '2404.11960'
source_url: https://arxiv.org/abs/2404.11960
tags:
- criteria
- score
- passage
- query
- team
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MCRanker improves pointwise LLM rankers by introducing a multi-perspective
  criteria ensemble approach. It generates a virtual annotation team with diverse
  expertise to create query-centric evaluation criteria, enabling more consistent
  and comprehensive passage assessments.
---

# MCRanker: Generating Diverse Criteria On-the-Fly to Improve Point-wise LLM Rankers

## Quick Facts
- arXiv ID: 2404.11960
- Source URL: https://arxiv.org/abs/2404.11960
- Reference count: 17
- Primary result: Achieves 3.67 NDCG@10 improvement over direct prompt baseline on BEIR datasets

## Executive Summary
MCRanker introduces a multi-perspective criteria ensemble approach to improve pointwise LLM rankers. The framework generates query-specific evaluation criteria through a virtual annotation team with diverse expertise, enabling more consistent and comprehensive passage assessments. By recruiting team members with different perspectives and generating criteria tailored to each query, MCRanker achieves significant performance improvements over baseline methods across eight BEIR datasets.

## Method Summary
MCRanker operates as a zero-shot ranking framework that generates diverse evaluation criteria on-the-fly rather than relying on fixed prompt templates. The system consists of four main modules: MRecruit identifies domain experts based on query analysis, MCriteria generates weighted evaluation criteria for each team member, MEvaluation scores passages using assigned criteria, and MAssessor aggregates individual scores into final relevance scores. The framework uses GPT-4 for all components by default, with the option to use generated quality criteria to guide less powerful LLMs like GPT-3.5-Turbo for evaluation tasks.

## Key Results
- Achieves average NDCG@10 improvement of 3.67 over direct prompt baseline across eight BEIR datasets
- Performance peaks with 2 recruited collaborators, declining with additional members due to expertise redundancy
- Quality criteria enable less powerful LLMs (GPT-3.5-Turbo) to achieve comparable performance when guided by well-crafted evaluation standards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query-centric criteria generation improves ranking performance by providing standardized evaluation guidance
- Mechanism: The MCRanker framework generates criteria specific to each query rather than using dataset-wide criteria, ensuring evaluations are tailored to the semantic nuances of individual queries
- Core assumption: Query semantics vary significantly even within the same dataset, requiring individualized criteria for optimal performance
- Evidence anchors:
  - [abstract] "These criteria are intended to direct each perspective in providing a distinct yet synergistic evaluation"
  - [section 5.2.2] "This decline illustrates the inherent complexity within the benchmark, where queries in a single dataset also diverge in semantics"
  - [corpus] Weak - no direct corpus evidence available

### Mechanism 2
- Claim: Multi-perspective ensemble averaging reduces individual bias and increases comprehensive evaluation coverage
- Mechanism: The annotation team combines NLP scientist expertise with domain-specific collaborators, each providing weighted criteria and scores that are aggregated to produce final rankings
- Core assumption: Different perspectives contribute complementary strengths that together provide more comprehensive evaluation than any single perspective
- Evidence anchors:
  - [abstract] "These criteria are intended to direct each perspective in providing a distinct yet synergistic evaluation"
  - [section 5.2.1] "Moreover, two recruited collaborators (2 R.C.)MC consistently over-perform (1 R.C.)MC across all eight datasets"
  - [corpus] Weak - no direct corpus evidence available

### Mechanism 3
- Claim: Quality criteria can guide less powerful LLMs to achieve comparable performance
- Mechanism: The framework uses a powerful LLM (GPT-4) to generate criteria and recruit team members, then applies these criteria with a less powerful LLM (GPT-3.5-Turbo) for evaluation, maintaining high performance
- Core assumption: Well-crafted criteria provide sufficient guidance for evaluation even when using less sophisticated models
- Evidence anchors:
  - [section 5.3.3] "This finding demonstrates that quality criteria can effectively guide a less advanced MEvaluation to yield more accurate relevance predictions"
  - [section 5.3.3] "Upon comparison between the 'Claude-3-Sonnet' variant of MCRanker and the corresponding DIRECT(0,k) baseline, it is evident to see the performance increase"
  - [corpus] Weak - no direct corpus evidence available

## Foundational Learning

- Concept: Zero-shot ranking with LLMs
  - Why needed here: MCRanker operates without training data, relying on prompt engineering and criteria generation
  - Quick check question: What is the key difference between zero-shot and few-shot prompting in LLM rankers?

- Concept: Multi-perspective problem solving
  - Why needed here: The framework's core innovation is using diverse perspectives to create comprehensive evaluation criteria
  - Quick check question: How does the "Team Recruiting" step ensure interdisciplinary perspectives?

- Concept: Ensemble methods in ranking
  - Why needed here: Final scores are aggregated from multiple team member evaluations using different strategies
  - Quick check question: What are the three ensemble mechanisms evaluated in the ablation study?

## Architecture Onboarding

- Component map: Query → MRecruit → MCriteria (per member) → MEvaluation (per member/passage) → Score Ensemble → Final ranking

- Critical path: Query → MRecruit → MCriteria (per member) → MEvaluation (per member/passage) → Score Ensemble → Final ranking

- Design tradeoffs:
  - Query-specific vs dataset-specific criteria (better performance vs. computational efficiency)
  - Number of team members (comprehensive coverage vs. redundancy)
  - LLM base model choice (performance vs. cost)

- Failure signatures:
  - Performance degradation when adding more team members beyond optimal number
  - Reduced effectiveness on datasets with ambiguous relevance judgments
  - Diminishing returns when criteria become too generic

- First 3 experiments:
  1. Test MCRanker with different numbers of recruited collaborators (1, 2, 3) on a single dataset
  2. Compare query-specific criteria vs dataset-specific criteria on the same dataset
  3. Replace GPT-4 with GPT-3.5-Turbo for all components to measure performance drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of recruited collaborators to maximize MCRanker's performance across different dataset domains?
- Basis in paper: [explicit] The paper shows performance peaks at 2 collaborators then decreases, but doesn't fully investigate the underlying cause or dataset-specific variations.
- Why unresolved: The authors note this could be due to redundancy in expertise among increased collaborators but leave deeper investigation for future work.
- What evidence would resolve it: Systematic experiments varying collaborator numbers across diverse dataset types with analysis of collaborator expertise overlap and contribution metrics.

### Open Question 2
- Question: How does MCRanker's performance scale with different LLM base models beyond the tested GPT-4, GPT-3.5-Turbo, and Claude-3-Sonnet?
- Basis in paper: [explicit] The authors show generalizability to different LLMs but only test three models and note quality criteria can guide less powerful models.
- Why unresolved: The study is limited to a small sample of LLMs and doesn't explore the full spectrum of available models or smaller language models.
- What evidence would resolve it: Extensive testing across a broader range of LLMs including smaller models, with comparative analysis of criteria generation quality and evaluation accuracy.

### Open Question 3
- Question: What specific collaboration mechanisms between virtual annotators would further improve MCRanker's performance?
- Basis in paper: [inferred] The authors mention they didn't study collaboration mechanisms within the team and leave it for future work, suggesting potential for improvement.
- Why unresolved: The current implementation treats each annotator independently without exploring how their interactions could enhance the evaluation process.
- What evidence would resolve it: Experiments testing different collaboration patterns (debate, consensus-building, iterative refinement) and their impact on ranking accuracy and consistency.

## Limitations

- Limited evaluation on non-English datasets, reducing confidence in cross-lingual applicability
- Computational costs increase with more team members and query-specific criteria generation
- No direct comparison with state-of-the-art fine-tuned rankers that may outperform zero-shot approaches

## Confidence

- Core performance improvement claim: **High** confidence (consistent NDCG@10 improvements across eight BEIR datasets)
- Scalability claim: **Medium** confidence (computational costs increase with team size, but specific scaling analysis not provided)
- Ensemble method effectiveness: **Medium** confidence (only three aggregation strategies tested without comprehensive comparison)

## Next Checks

1. Test MCRanker's performance on non-English BEIR datasets or cross-lingual retrieval tasks to evaluate language robustness.

2. Conduct a comprehensive ablation study comparing all three ensemble methods (Summation, Ranking, LLM Assessment) across all eight datasets to identify the optimal aggregation strategy.

3. Implement a runtime analysis comparing MCRanker with baseline methods to quantify the computational overhead of query-specific criteria generation versus dataset-wide criteria.