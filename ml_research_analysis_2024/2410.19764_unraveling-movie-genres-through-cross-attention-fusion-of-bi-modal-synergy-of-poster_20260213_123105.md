---
ver: rpa2
title: Unraveling Movie Genres through Cross-Attention Fusion of Bi-Modal Synergy
  of Poster
arxiv_id: '2410.19764'
source_url: https://arxiv.org/abs/2410.19764
tags:
- movie
- genre
- posters
- visual
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel framework for multi-label movie genre
  classification using movie posters by leveraging both visual and textual information.
  The method employs OCR to extract text from posters, then uses a cross-attention-based
  fusion module to integrate visual and textual features through contrastive pre-training
  with CLIP.
---

# Unraveling Movie Genres through Cross-Attention Fusion of Bi-Modal Synergy of Poster

## Quick Facts
- arXiv ID: 2410.19764
- Source URL: https://arxiv.org/abs/2410.19764
- Reference count: 40
- Primary result: Achieves 68.23% macro F1, 72.34% micro F1, and 87.55% hit ratio on multi-label movie genre classification using bi-modal poster analysis

## Executive Summary
This study introduces a novel framework for multi-label movie genre classification using movie posters by leveraging both visual and textual information. The method employs OCR to extract text from posters, then uses a cross-attention-based fusion module to integrate visual and textual features through contrastive pre-training with CLIP. A sequential multi-head self-attention module captures intra-modal relationships, and asymmetric loss handles class imbalance. Evaluated on 13,882 IMDb posters across 13 genres, the model achieved a macro F1 of 68.23%, micro F1 of 72.34%, and hit ratio of 87.55%, outperforming both baseline and state-of-the-art approaches.

## Method Summary
The framework extracts text from movie posters using Gemini-Pro-Vision OCR, then employs CLIP for visual and textual feature extraction. A cross-modal attention module (M CAM) computes scaled dot-product attention between visual queries and textual keys to produce modality-specific representations. Sequential multi-head self-attention (SM SAM) applies multiple layers of self-attention to fused features to model complex relationships. The model uses asymmetric loss with parameters γ+ = 3, γ- = 4 to address class imbalance. The architecture is trained using Adam optimizer (learning rate = 1e-4, β1 = 0.9, β2 = 0.999) for 100 epochs with early stopping.

## Key Results
- Achieved 68.23% macro F1, 72.34% micro F1, and 87.55% hit ratio on 13-genre classification task
- Outperformed baseline and state-of-the-art approaches on IMDb poster dataset
- Ablation studies confirm effectiveness of cross-modal attention and sequential self-attention modules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention fusion enables better integration of complementary visual and textual cues than simple concatenation
- Mechanism: The cross-modal attention module computes scaled dot-product attention between visual queries and textual keys (and vice versa), producing modality-specific representations that emphasize features relevant to the other modality
- Core assumption: Visual and textual modalities contain complementary genre-related information that can be mutually reinforced through attention weighting
- Evidence anchors: "cross-attention-based fusion module to allocate attention weights to visual and textual embedding", "Cross attention proves to effectively exploit inter-feature relationships between visual and textual feature maps"
- Break condition: If modalities contain redundant information, attention weighting may not provide significant benefit over concatenation

### Mechanism 2
- Claim: Sequential multi-head self-attention captures intra-modal dependencies that improve genre discrimination
- Mechanism: Multiple layers of self-attention applied sequentially to fused features allow the model to model complex relationships within the joint visual-textual representation
- Core assumption: Genre information requires modeling both cross-modal interactions and intra-modal relationships within the fused representation
- Evidence anchors: "Multi-head Self Attention can directly learn long-range dependencies and intra-relationships in input features", "when both the M CAM and SM SAM modules are discarded, we observe a decline in performance by at least 3%"
- Break condition: If the fused representation already captures all necessary relationships in a single self-attention layer, additional sequential layers may cause overfitting or diminishing returns

### Mechanism 3
- Claim: Asymmetric loss addresses class imbalance by weighting minority class samples more heavily during training
- Mechanism: ASL applies different gamma parameters (γ+ = 3, γ− = 4) to positive and negative samples, with shifted probabilities for negative samples to focus learning on hard examples
- Core assumption: Movie poster genre datasets exhibit significant class imbalance, with some genres underrepresented in the training data
- Evidence anchors: "Multi-label datasets are mostly imbalanced and have more negative samples than positive ones for a particular class", "The main objective of ASL is to provide control over positive and negative sample imbalance in the optimization of multi-label classification"
- Break condition: If dataset class distribution is relatively balanced, asymmetric weighting may hurt performance by over-emphasizing minority classes

## Foundational Learning

- Concept: Contrastive learning (CLIP) for joint visual-textual representation
  - Why needed here: Enables extraction of semantically aligned visual and textual features from movie posters
  - Quick check question: How does CLIP ensure that text and image embeddings are aligned in the same semantic space?

- Concept: Multi-head attention mechanism
  - Why needed here: Allows the model to capture different types of relationships between visual and textual features
  - Quick check question: What is the purpose of having multiple attention heads in parallel rather than a single attention mechanism?

- Concept: Multi-label classification with imbalanced data
  - Why needed here: Movie posters can belong to multiple genres simultaneously, and some genres are rarer than others
  - Quick check question: Why is macro-F1 typically lower than micro-F1 in imbalanced multi-label classification tasks?

## Architecture Onboarding

- Component map: Movie poster → OCR → CLIP (visual + text) → Alignment (LayerNorm + linear projection) → M CAM (cross-attention) → SM SAM (sequential self-attention) → FFN → Output (sigmoid)
- Critical path: OCR extraction → CLIP feature extraction → Cross-attention fusion → Self-attention refinement → Classification
- Design tradeoffs: Cross-attention vs concatenation (better fusion vs simpler implementation), Sequential self-attention vs single layer (richer representations vs computational cost)
- Failure signatures: Poor performance on genres with subtle visual cues, failure to recognize genres from text-only posters, overfitting on small datasets
- First 3 experiments:
  1. Compare cross-attention fusion vs simple concatenation of visual and textual features
  2. Evaluate impact of removing SM SAM module (using only M CAM)
  3. Test asymmetric loss vs standard binary cross-entropy on the imbalanced genre dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different OCR systems (e.g., Gemini-Pro-Vision vs. alternatives) affect the accuracy of movie genre classification when extracting text from posters?
- Basis in paper: The paper uses Gemini-Pro-Vision-based OCR to extract text from movie posters but does not compare its performance with other OCR systems
- Why unresolved: The study does not explore the impact of OCR accuracy on overall model performance, which could be significant given the reliance on extracted text for multi-modal classification
- What evidence would resolve it: Comparative experiments using multiple OCR systems to evaluate their impact on genre classification accuracy

### Open Question 2
- Question: How does the performance of the proposed model vary across different poster sizes and resolutions?
- Basis in paper: The paper does not address the robustness of the model to variations in poster size or resolution, which could affect the quality of visual and textual feature extraction
- Why unresolved: The study does not test the model on posters of varying sizes or resolutions, leaving uncertainty about its generalizability
- What evidence would resolve it: Experiments testing the model on posters with different sizes and resolutions to assess its robustness and performance consistency

### Open Question 3
- Question: How does the model perform when trained on posters from different time periods or cultural contexts?
- Basis in paper: The paper uses IMDb posters but does not analyze whether the model's performance is consistent across posters from different eras or cultural backgrounds
- Why unresolved: The study does not explore the model's ability to generalize across diverse poster designs, which could vary significantly over time and geography
- What evidence would resolve it: Training and testing the model on posters from different time periods or cultural contexts to evaluate its adaptability and performance

## Limitations
- Incomplete architectural specifications for cross-modal attention and sequential self-attention modules make exact reproduction challenging
- Evaluation relies on single dataset without cross-validation on alternative movie poster datasets to verify generalizability
- No statistical significance testing for performance improvements over baselines

## Confidence

- **High confidence**: The general framework architecture (OCR → CLIP → cross-attention → self-attention → classification) is well-described and reproducible with standard implementations of the constituent components
- **Medium confidence**: The ablation study results showing performance improvements from each module are plausible but cannot be independently verified without the exact implementation details of the custom modules
- **Low confidence**: The claimed superiority over state-of-the-art methods is difficult to validate due to incomplete architectural specifications and lack of statistical significance testing

## Next Checks

1. **Architectural Replication**: Implement the M CAM and SM SAM modules using the described mechanisms (scaled dot-product attention for cross-modal fusion and sequential multi-head self-attention) and verify if similar performance improvements can be achieved on the IMDb dataset

2. **Generalizability Test**: Evaluate the trained model on an independent movie poster dataset (e.g., TMDB or another movie database) to assess whether the cross-modal attention approach generalizes beyond the training corpus

3. **Statistical Significance Analysis**: Conduct paired t-tests or McNemar's tests comparing the proposed method against baselines on genre-wise classification accuracy to determine if the performance differences are statistically significant at p < 0.05