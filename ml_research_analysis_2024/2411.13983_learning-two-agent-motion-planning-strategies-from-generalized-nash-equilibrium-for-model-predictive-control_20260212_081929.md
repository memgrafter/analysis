---
ver: rpa2
title: Learning Two-agent Motion Planning Strategies from Generalized Nash Equilibrium
  for Model Predictive Control
arxiv_id: '2411.13983'
source_url: https://arxiv.org/abs/2411.13983
tags:
- vehicle
- agents
- intersection
- function
- game-theoretic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces IGT-MPC, a decentralized algorithm for two-agent
  motion planning that integrates game-theoretic reasoning with model predictive control.
  The method trains a neural network value function on a dataset of generalized Nash
  equilibria (GNE) to predict interaction outcomes, which is then used as a terminal
  cost-to-go function in an MPC scheme.
---

# Learning Two-agent Motion Planning Strategies from Generalized Nash Equilibrium for Model Predictive Control

## Quick Facts
- arXiv ID: 2411.13983
- Source URL: https://arxiv.org/abs/2411.13983
- Reference count: 21
- Primary result: IGT-MPC achieves 97%+ feasibility in intersection navigation and 100% win rate in racing vs naive policies

## Executive Summary
This paper introduces IGT-MPC, a decentralized two-agent motion planning algorithm that combines game-theoretic reasoning with model predictive control. The approach trains a neural network value function on generalized Nash equilibria data to predict interaction outcomes, which is then used as a terminal cost-to-go function in an MPC scheme. This enables implicit interaction-aware planning without explicit game-theoretic modeling at runtime. The method is demonstrated on both competitive racing scenarios (achieving 100% win rate) and cooperative intersection navigation (achieving 97%+ feasibility with zero gridlocks).

## Method Summary
IGT-MPC is a decentralized algorithm that integrates game-theoretic reasoning with model predictive control for two-agent motion planning. The method involves two main phases: offline training and online execution. During offline training, a neural network value function is trained on a dataset of generalized Nash equilibria (GNE) to predict interaction outcomes between agents. This value function serves as a terminal cost-to-go approximation in the MPC formulation. During online execution, each agent uses the learned value function within its MPC optimization to make interaction-aware decisions without explicitly solving game-theoretic problems at runtime. The approach maintains decentralization while implicitly accounting for the other agent's potential strategies through the learned value function.

## Key Results
- In racing scenarios, IGT-MPC achieved a 100% win rate against naive progress-maximizing policies by enabling position defense through learned interaction strategies
- In intersection navigation, IGT-MPC achieved at least 97% feasibility across all tested scenarios compared to 0-95% with naive policies
- IGT-MPC achieved zero gridlocks in intersection navigation, while naive policies experienced 0-100% gridlocks depending on scenario

## Why This Works (Mechanism)
The approach works by pre-computing and learning interaction strategies offline, then using this knowledge to make optimal decisions online without explicit game-theoretic computation. By training a value function on generalized Nash equilibria, the algorithm captures complex interaction dynamics between agents. The learned value function serves as an implicit model of how the other agent might behave, allowing each agent to plan with awareness of potential interactions. This transforms a complex interactive planning problem into a tractable optimization problem where each agent can anticipate and respond to the other's likely actions through the value function's predictions.

## Foundational Learning
- Generalized Nash Equilibrium (GNE): A solution concept for games where players' strategy sets depend on each other's choices. Why needed: Provides the theoretical foundation for modeling agent interactions where decisions affect each other's feasible spaces. Quick check: Verify the solution satisfies the GNE conditions where each agent's strategy is optimal given the others'.
- Model Predictive Control (MPC): A control strategy that optimizes over a finite horizon while using a terminal cost for infinite-horizon behavior. Why needed: Enables receding horizon planning with consideration of future interactions. Quick check: Ensure the MPC formulation correctly incorporates the learned value function as terminal cost.
- Value Function Approximation: Using neural networks to approximate the value of state-action pairs. Why needed: Allows efficient evaluation of complex interaction outcomes without explicit computation. Quick check: Validate the learned value function generalizes to unseen states within the operational domain.

## Architecture Onboarding

Component Map:
Neural Network (Value Function) <- Dataset of GNE <- Game Solver
MPC Controller <- Neural Network <- State Estimator
Vehicle Dynamics Model <- MPC Controller <- Environment Model

Critical Path:
State Estimation -> MPC Optimization -> Action Selection -> Execution -> State Update

Design Tradeoffs:
The approach trades computational complexity at runtime for offline training time. Instead of solving complex game-theoretic problems online, the system uses a pre-trained value function. This significantly reduces online computation but requires substantial offline training and assumes the learned value function generalizes well to operational conditions.

Failure Signatures:
- Value function provides poor estimates for novel interaction states
- MPC optimization becomes infeasible due to constraint violations
- Agents fail to reach terminal conditions within planning horizon
- Oscillatory behavior between agents due to conflicting strategies

First Experiments:
1. Validate the learned value function predictions against ground truth for states within and near the training distribution
2. Test MPC feasibility and closed-loop performance in isolated scenarios before multi-agent testing
3. Compare closed-loop trajectories against analytical solutions for simplified interaction scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- The neural network value function is trained on a dataset of generalized Nash equilibria, but the paper doesn't discuss how representative or comprehensive this dataset is for capturing all possible interaction scenarios
- The approach assumes perfect state information and communication between agents, which may not hold in real-world applications
- The computational efficiency of the approach is not thoroughly evaluated, particularly for real-time applications

## Confidence
High: Racing win rate claim (100% vs naive policies)
Medium: Intersection navigation feasibility claim (97%+ vs 0-95%)
Medium: Gridlock claim (0% vs 0-100%)

## Next Checks
1. Test the approach on a wider range of scenarios, including more than two agents and different types of interactions (e.g., mixed cooperative-competitive scenarios)
2. Evaluate the computational efficiency of the approach in real-time applications, particularly comparing it to other state-of-the-art methods
3. Assess the robustness of the approach to imperfect state information and communication delays between agents