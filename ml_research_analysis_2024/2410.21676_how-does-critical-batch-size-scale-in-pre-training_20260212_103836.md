---
ver: rpa2
title: How Does Critical Batch Size Scale in Pre-training?
arxiv_id: '2410.21676'
source_url: https://arxiv.org/abs/2410.21676
tags:
- size
- batch
- training
- scaling
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the scaling behavior of critical batch size
  (CBS) in large-scale autoregressive language model pre-training. The authors define
  CBS as the batch size beyond which doubling the batch size leads to more than 20%
  overhead in the number of optimization steps needed to reach a target validation
  loss.
---

# How Does Critical Batch Size Scale in Pre-training?
## Quick Facts
- **arXiv ID**: 2410.21676
- **Source URL**: https://arxiv.org/abs/2410.21676
- **Reference count**: 40
- **Primary result**: Critical batch size increases with data size but remains nearly invariant with model size when data size is fixed

## Executive Summary
This work investigates the scaling behavior of critical batch size (CBS) in large-scale autoregressive language model pre-training. The authors define CBS as the batch size beyond which doubling the batch size leads to more than 20% overhead in the number of optimization steps needed to reach a target validation loss. Through extensive experiments with models ranging from 85 million to 1.2 billion parameters on the C4 dataset, they find that CBS increases with data size but remains nearly invariant with model size when data size is fixed.

## Method Summary
The authors conduct a comprehensive empirical study of critical batch size scaling across different model sizes and dataset sizes. They train autoregressive language models with parameter counts ranging from 85M to 1.2B on the C4 dataset, systematically varying batch sizes and measuring training efficiency. The experiments measure the number of optimization steps required to reach a target validation loss at different batch sizes, defining the critical batch size as the point where doubling the batch size causes more than 20% overhead. Theoretical analysis is provided using infinite-width neural network limits and high-dimensional least squares regression to derive scaling laws for CBS.

## Key Results
- Critical batch size increases with data size following the scaling law B* ∝ D^(1-a/min{b,2a+1})
- CBS remains nearly invariant with model size when data size is fixed
- The study validates the effectiveness of exponential weight averaging for training beyond fixed durations
- Experiments cover model sizes from 85M to 1.2B parameters on C4 dataset

## Why This Works (Mechanism)
The invariance of critical batch size with model size at fixed data size suggests that CBS is primarily determined by the data characteristics rather than the model architecture. This behavior emerges because the optimization landscape and convergence properties are more sensitive to data statistics than to model parameterization when data size is held constant. The theoretical framework using infinite-width neural networks captures this phenomenon by showing that the capacity and source condition exponents that govern CBS scaling depend on data properties rather than model width.

## Foundational Learning
- **Critical Batch Size**: The batch size threshold beyond which training efficiency degrades significantly. Needed to understand training efficiency boundaries. Quick check: measure step overhead when doubling batch size.
- **Capacity Condition**: Measures how well the model can fit the data. Needed to understand model-data alignment. Quick check: analyze singular value spectrum of data matrix.
- **Source Condition**: Characterizes data regularity and informativeness. Needed to understand data quality impact on optimization. Quick check: examine condition number of empirical covariance.
- **Infinite-Width Neural Networks**: Theoretical limit where neural networks become Gaussian processes. Needed to derive tractable scaling laws. Quick check: compare NTK predictions with finite-width results.
- **Exponential Weight Averaging**: Optimization technique that maintains a running average of parameters. Needed for stable training at large batch sizes. Quick check: monitor validation loss stability with different averaging schedules.

## Architecture Onboarding
Component map: Data -> Model -> Optimizer -> Validation
Critical path: Data preparation → Model initialization → Training loop → Validation monitoring
Design tradeoffs: Larger batch sizes reduce communication overhead but require more sophisticated optimization techniques to maintain convergence quality
Failure signatures: Sharp increase in steps to target loss when exceeding CBS; degraded validation performance; training instability
First experiments: 1) Measure CBS at fixed model size across varying data sizes, 2) Verify CBS invariance across different model sizes at fixed data size, 3) Test exponential weight averaging effectiveness beyond CBS threshold

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Theoretical analysis relies on infinite-width neural network approximations that may not fully capture finite-width practical models
- Experiments limited to autoregressive language models on C4 dataset, raising questions about generalization to other architectures
- CBS definition based on 20% overhead threshold is somewhat arbitrary and may be sensitive to training dynamics
- Study focuses on pre-training without investigating CBS behavior during fine-tuning or transfer learning

## Confidence
- High confidence in CBS invariance with model size at fixed data size (supported by extensive empirical data across 85M-1.2B parameter models)
- Medium confidence in theoretical scaling law derivation (relies on infinite-width approximations and least squares regression assumptions)
- Medium confidence in generalization beyond autoregressive language models (limited to single architecture and dataset)

## Next Checks
1. Replicate CBS scaling experiments across different model architectures (e.g., encoder-only, encoder-decoder, vision transformers) to verify invariance claim holds beyond autoregressive language models.

2. Conduct experiments with varying dataset characteristics (different domain distributions, sequence lengths, and token vocabularies) to test robustness of CBS-data size relationship across diverse training scenarios.

3. Validate theoretical scaling law predictions by measuring capacity and source condition exponents empirically across different model families and datasets, then comparing predicted CBS scaling to observed values.