---
ver: rpa2
title: 'BenTo: Benchmark Task Reduction with In-Context Transferability'
arxiv_id: '2410.13804'
source_url: https://arxiv.org/abs/2410.13804
tags:
- tasks
- task
- benchmark
- transferability
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of expensive evaluation of large
  language models (LLMs) by investigating how to reduce the number of tasks in LLM
  benchmarks without compromising evaluation quality. The authors propose a novel
  method called BENTO (Benchmark Task Reduction) that leverages in-context learning
  (ICL) to estimate task transferability and relevance.
---

# BenTo: Benchmark Task Reduction with In-Context Transferability

## Quick Facts
- arXiv ID: 2410.13804
- Source URL: https://arxiv.org/abs/2410.13804
- Authors: Hongyu Zhao; Ming Li; Lichao Sun; Tianyi Zhou
- Reference count: 40
- One-line primary result: Reduces LLM benchmark tasks to 5% while inducing <4% difference to full benchmark evaluation

## Executive Summary
This paper addresses the challenge of expensive evaluation of large language models (LLMs) by proposing a method to reduce benchmark tasks without compromising evaluation quality. The authors introduce BENTO (Benchmark Task Reduction), which leverages in-context learning (ICL) to estimate task transferability and relevance. By formulating task selection as a facility location problem, BENTO identifies a representative subset of tasks that can effectively proxy performance on the full benchmark. Experiments on popular LLM benchmarks like MMLU and FLAN demonstrate that BENTO can achieve significant task reduction (up to 95%) while maintaining high evaluation fidelity.

## Method Summary
BENTO reduces benchmark tasks by computing a pairwise transferability matrix using in-context learning, where exemplars from one task are used as context for another task to measure performance improvement. This transferability matrix is then transformed into a similarity matrix using either Euclidean or cosine similarity. The task selection problem is formulated as a facility location optimization, where a greedy algorithm selects a subset of tasks that maximizes the sum of maximum similarities between each task and its nearest selected task. The method is training-free and gradient-free, making it highly efficient for LLM evaluation.

## Key Results
- BENTO reduces MMLU benchmark tasks to 5% (3 out of 57 tasks) while achieving a normalized RMSE of only 3%
- The method outperforms baselines by a significant margin, with BENTO achieving NRMSE of 3% compared to 9% for the best baseline
- Task reduction is consistent across different model sizes (7B, 13B, 70B parameters) and benchmarks (MMLU, FLAN)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context transferability (ICT) captures meaningful task similarity by measuring performance improvement when exemplars from one task are used as context for another.
- Mechanism: The ICT metric measures the performance gain when exemplars from task i are used as in-context demonstrations for task j, compared to zero-shot performance. This creates a pairwise transferability matrix that reveals task relationships.
- Core assumption: Tasks that share similar formats, themes, or knowledge requirements will show higher transferability scores when used as context for each other.
- Evidence anchors:
  - [abstract] "We propose a practically efficient metric for estimating the transferability between two tasks via in-context learning (ICL)."
  - [section 3] "When applying task-i's exemplars as the context for task-j's queries, it provides an effective low-cost estimation of the transferability from task-i to task-j."
  - [corpus] Weak - corpus neighbors don't discuss ICT methodology specifically
- Break condition: If task similarity is orthogonal to knowledge transfer (e.g., tasks with similar names but different underlying concepts), ICT would fail to capture true relationships.

### Mechanism 2
- Claim: The sparse clustering pattern in ICT reveals natural task groupings that align with human-interpretable themes.
- Mechanism: The ICT matrix exhibits a sparse structure where intra-cluster arcs are much more frequent than inter-cluster arcs, suggesting natural task groupings that can be discovered through spectral clustering.
- Core assumption: Tasks naturally cluster into groups based on shared characteristics, and these clusters are more densely connected internally than externally.
- Evidence anchors:
  - [abstract] "Our study reveals that task transferability and relevance provide critical information to identify the most representative subset of tasks via optimizing a facility location function."
  - [section 3] "In the visual representation provided by Figure 1 (LEFT), we observe a 'sparse' clustering pattern."
  - [corpus] Weak - corpus doesn't contain similar clustering analysis
- Break condition: If tasks have complex multi-dimensional relationships that don't form clear clusters, or if the sparsity pattern is an artifact of the ICT measurement rather than true task structure.

### Mechanism 3
- Claim: Facility location optimization selects tasks that maximize coverage of the full benchmark through their transferability relationships.
- Mechanism: The facility location problem formulation maximizes the sum of maximum similarities between each task and its nearest selected task, ensuring the reduced subset represents the full benchmark well.
- Core assumption: Maximizing similarity to nearest selected task provides good proxy for overall benchmark coverage.
- Evidence anchors:
  - [abstract] "We propose Benchmark Task reductiOn (BENTO) that formulates the task selection into a facility location (FL) problem."
  - [section 4] "In BENTO, the task similarities are derived either directly from the similarity matrix computed via Laplacian Eigenmaps (LE) or are recalculated within the LE-embedded space."
  - [corpus] Weak - corpus doesn't discuss facility location optimization specifically
- Break condition: If the similarity metric doesn't accurately reflect task relationships, or if the facility location formulation doesn't capture the right notion of representativeness.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL is the core mechanism for estimating task transferability without requiring model training or fine-tuning.
  - Quick check question: What is the difference between ICL and zero-shot prompting, and how does ICL enable transferability measurement?

- Concept: Submodular optimization
  - Why needed here: The facility location objective is submodular, allowing for efficient greedy approximation algorithms.
  - Quick check question: Why does submodularity enable efficient greedy algorithms, and what is the theoretical guarantee of the greedy solution?

- Concept: Spectral clustering and Laplacian Eigenmaps
  - Why needed here: These techniques are used to discover task clusters from the ICT matrix and create similarity matrices for the facility location problem.
  - Quick check question: How does Laplacian Eigenmaps transform the ICT matrix to emphasize local neighborhood relationships?

## Architecture Onboarding

- Component map:
  - ICT computation module -> Clustering module -> Similarity computation module -> Facility location solver -> Reduced benchmark creation

- Critical path: ICT computation → Similarity computation → Facility location optimization → Reduced benchmark creation

- Design tradeoffs:
  - L (exemplars per task) vs. M (random seeds) trade-off in ICT computation
  - Direct similarity matrix vs. LE embedding approach for different k values
  - Task selection vs. example selection for further reduction

- Failure signatures:
  - Poor clustering results suggest ICT doesn't capture task relationships well
  - High NRMSE indicates reduced benchmark doesn't proxy full benchmark effectively
  - Inconsistent results across different models suggest task representativeness issues

- First 3 experiments:
  1. Run ICT computation on a small subset of MMLU tasks to verify clustering patterns
  2. Test BENTO with k=1 to verify it selects the most central/representative task
  3. Compare NRMSE across different similarity metrics (cosine vs. Euclidean) on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the hyperparameter c in the similarity matrix calculation be dynamically selected based on dataset characteristics to optimize performance across different benchmarks?
- Basis in paper: [explicit] The paper mentions that the choice of c = 1.5 * max_i,j(E_ij) was empirically validated on MMLU but performed less optimally on FLAN. It suggests exploring dynamic strategies for setting c based on specific dataset characteristics.
- Why unresolved: The paper only tested a fixed value of c and found it suboptimal across different datasets. A more adaptive approach could potentially improve performance but was not explored.
- What evidence would resolve it: Systematic experiments varying c across multiple datasets and analyzing its impact on benchmark reduction performance would provide evidence for an optimal dynamic selection strategy.

### Open Question 2
- Question: What are the trade-offs between using in-context learning (ICL) for transferability estimation versus model finetuning or Fisher information-based methods, particularly for larger and more diverse benchmarks?
- Basis in paper: [explicit] The paper highlights that existing transferability estimation methods rely on model finetuning or Fisher information, which is computationally prohibitive for LLMs. It proposes ICT as a training-free alternative but does not compare its effectiveness to these methods.
- Why unresolved: While ICT is presented as a more efficient alternative, the paper does not provide a direct comparison with traditional finetuning or Fisher information-based methods in terms of accuracy and scalability.
- What evidence would resolve it: Comparative experiments evaluating ICT against finetuning and Fisher information methods on various benchmarks would clarify the trade-offs in terms of accuracy, computational cost, and scalability.

### Open Question 3
- Question: How does the clustering of tasks based on ICT reflect the underlying structure of LLM capabilities, and can this structure be leveraged to predict model performance on unseen tasks?
- Basis in paper: [explicit] The paper observes that ICT reveals clusters of tasks associated with clear themes (e.g., history, biology) and suggests that these clusters capture inherent task structures beyond surface-level associations.
- Why unresolved: While the clustering results are interpretable, the paper does not explore whether these clusters can be used to predict model performance on new, unseen tasks or how they relate to LLM capabilities.
- What evidence would resolve it: Experiments testing whether task clusters derived from ICT can predict model performance on novel tasks or how they correlate with specific LLM abilities would provide insights into their utility for understanding model capabilities.

## Limitations
- The method's effectiveness depends on the stability of in-context learning transferability estimates, which can vary with the choice of exemplars and random seeds.
- The greedy algorithm for facility location optimization provides only an approximate solution, potentially missing optimal task subsets.
- Evaluation is limited to MMLU and FLAN benchmarks, raising questions about generalizability to other benchmark suites or domain-specific evaluations.

## Confidence
- High confidence: The claim that BENTO can reduce benchmark tasks to 5% while maintaining evaluation quality (<4% NRMSE) is strongly supported by experimental results on MMLU and FLAN.
- Medium confidence: The assertion that in-context transferability effectively captures task similarity and enables meaningful clustering is supported by observed sparse clustering patterns, but the mechanism's universality across different benchmark types is not fully established.
- Medium confidence: The facility location optimization approach for task selection is theoretically sound, but the greedy approximation's effectiveness depends on the quality of the similarity matrix, which introduces uncertainty.

## Next Checks
1. Apply BENTO to diverse benchmark suites beyond MMLU and FLAN to test generalizability and validate whether sparse clustering patterns hold across different task distributions.

2. Systematically vary hyperparameters L (exemplars per task) and M (random seeds) to assess the stability of ICT-based similarity estimates and quantify sensitivity to these choices.

3. Compare BENTO's performance when using different similarity metrics (e.g., Jaccard similarity, learned embeddings) instead of Euclidean or cosine similarity to test the impact on reduced benchmark representativeness.