---
ver: rpa2
title: 'NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker and
  AWS Trainium and Inferentia2'
arxiv_id: '2407.12057'
source_url: https://arxiv.org/abs/2407.12057
tags:
- these
- training
- sagemaker
- memory
- chips
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents NinjaLLM, a fast, scalable, and cost-effective
  RAG system using AWS SageMaker and Trainium/Inferentia2 chips. The approach fine-tunes
  Llama3-Instruct 70B on AWS Trainium for enhanced tool usage, citation capabilities,
  and reduced hallucinations.
---

# NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker and AWS Trainium and Inferentia2

## Quick Facts
- arXiv ID: 2407.12057
- Source URL: https://arxiv.org/abs/2407.12057
- Authors: Tengfei Xue; Xuefeng Li; Roman Smirnov; Tahir Azim; Arash Sadrieh; Babak Pahlavan
- Reference count: 9
- Primary result: 62% accuracy on Natural Questions and 59% on HotPotQA using Llama3-Instruct 70B fine-tuned on AWS Trainium

## Executive Summary
NinjaLLM presents a cost-effective RAG system that leverages AWS SageMaker with Trainium and Inferentia2 chips to achieve high accuracy while reducing inference costs. The system fine-tunes Llama3-Instruct 70B for enhanced tool usage, citation capabilities, and hallucination reduction, achieving state-of-the-art performance on Natural Questions (62%) and HotPotQA (59%) datasets. Key innovations include multi-bucketing and continuous batching for efficient inference, with the entire training process completed in under three hours on 32 TRN1 instances at a total cost under $30,000.

## Method Summary
NinjaLLM fine-tunes Llama3-Instruct 70B on AWS Trainium using the Lima approach with approximately 20 million tokens, focusing on output format and tone rather than expanding general knowledge. The system employs vLLM with multi-bucketing and continuous batching for optimized inference, deployed through Amazon SageMaker. ColBERTv2 handles content filtering and ranking for retrieval, while the fine-tuned model addresses hallucinations through prompt engineering and response checking mechanisms. The approach prioritizes cost-effectiveness by leveraging custom AWS silicon over traditional GPU deployments.

## Key Results
- Achieves 62% accuracy on Natural Questions and 59% on HotPotQA datasets
- Outperforms competing models like DBRX and Mixtral Instruct on these benchmarks
- Reduces TTFT and memory usage through multi-bucketing optimization
- Completes training in under three hours on 32 TRN1 instances
- Total deployment cost remains under $30,000

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-bucketing reduces unnecessary computations and memory usage during inference.
- Mechanism: Segments potential input sizes into different buckets (e.g., 128, 1024), dynamically selecting the nearest bucket size greater than the actual input length to avoid prefill operations for maximum token length.
- Core assumption: Computational cost of prefill operations is proportional to bucket size, and smaller buckets significantly reduce overhead.
- Evidence anchors:
  - [abstract] "This technique segments potential input sizes into different buckets (e.g., 128, 1024), allowing the use of decoder heads tailored to the nearest bucket size greater than the actual output length."
  - [section] "This method significantly reduces unnecessary computations and memory usage, as the appropriate bucket is dynamically selected, avoiding prefill operations for the maximum token length and thereby reducing TTFT."
- Break condition: If input lengths vary too widely or are unpredictable, bucketing strategy may not provide significant benefits.

### Mechanism 2
- Claim: Continuous batching increases throughput by processing multiple text generation requests concurrently.
- Mechanism: Processes multiple requests simultaneously at token level, allowing requests to join or leave batch as they complete generation, eliminating padding needs and avoiding idle accelerator time.
- Core assumption: Concurrent processing of multiple requests at different stages leads to better accelerator resource utilization.
- Evidence anchors:
  - [abstract] "continuous batching optimizes throughput and efficiency by processing multiple text generation requests concurrently at the token level."
  - [section] "This dynamic allows requests to join or leave the batch as they complete their generation, without waiting for the entire batch to finish, eliminating the need for padding requests to the same length and avoiding idle time on the accelerator."
- Break condition: If request arrival patterns are highly irregular or individual requests are extremely long, continuous batching benefits may diminish.

### Mechanism 3
- Claim: Fine-tuning on diverse dataset improves model's ability to avoid hallucinations and unsafe responses.
- Mechanism: Trains model on diverse data set and implements mechanisms to detect and correct hallucinations and biases, enhancing robustness and trustworthiness.
- Core assumption: Exposure to diverse training data and explicit safety mechanism training reduces likelihood of generating biased or unsafe responses.
- Evidence anchors:
  - [abstract] "we introduce prompt engineering and model response checking mechanisms to detect and correct hallucinations and unsafe content dynamically."
  - [section] "We trained the model on a diverse set of data to reduce the likelihood of generating biased responses."
- Break condition: If training data diversity is insufficient or safety mechanisms are not comprehensive enough, model may still generate unsafe responses.

## Foundational Learning

- Concept: Large Language Model Fine-tuning
  - Why needed here: Understanding fine-tuning process is crucial for optimizing model performance and adapting to specific RAG tasks.
  - Quick check question: What is the primary difference between pre-training and fine-tuning an LLM?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is core technique used to enhance language models by integrating external knowledge dynamically.
  - Quick check question: How does RAG improve performance of language models on complex queries?

- Concept: Inference Optimization Techniques
  - Why needed here: Techniques like multi-bucketing and continuous batching are essential for efficient deployment of large models in production.
  - Quick check question: What is main benefit of using multi-bucketing in LLM inference?

## Architecture Onboarding

- Component map: ColBERTv2 (content filtering and ranking) -> Llama3-Instruct 70B fine-tuned on Trainium -> vLLM with multi-bucketing and continuous batching -> Amazon SageMaker endpoint

- Critical path:
  1. Retrieve relevant passages using ColBERTv2
  2. Generate response using fine-tuned Llama3 model with vLLM
  3. Serve response through SageMaker endpoint

- Design tradeoffs:
  - Using Trainium chips for cost-effectiveness vs. potential performance limitations compared to GPUs
  - Implementing multi-bucketing for reduced TTFT vs. increased complexity in managing multiple decoder heads
  - Fine-tuning on smaller dataset for cost savings vs. potential loss of general knowledge

- Failure signatures:
  - High latency or timeouts: Potential issues with vLLM configuration or insufficient compute resources
  - Decreased accuracy: Problems with fine-tuning process or data quality issues in retrieval system
  - Unexpected costs: Inefficient use of SageMaker resources or suboptimal batch sizing in continuous batching

- First 3 experiments:
  1. Benchmark inference latency and throughput with different bucket sizes to find optimal configuration
  2. Compare performance of fine-tuned model against base Llama3 model on held-out validation set
  3. Stress test SageMaker deployment with varying request patterns to identify scaling limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does accuracy of NinjaLLM compare to GPT-4 Turbo on multi-hop reasoning tasks beyond HotPotQA?
- Basis in paper: [explicit] Paper states NinjaLLM achieves 58.84% on HotPotQA requiring complex multi-hop reasoning and trails GPT-4 Turbo, but provides no specific comparative data for other multi-hop reasoning datasets.
- Why unresolved: Paper only benchmarks on HotPotQA and does not explore other multi-hop reasoning datasets or provide detailed comparison with GPT-4 Turbo on similar tasks.
- What evidence would resolve it: Benchmarking NinjaLLM on additional multi-hop reasoning datasets (e.g., DROP, QASC) and comparing performance directly with GPT-4 Turbo on these datasets would provide clearer picture of capabilities in complex reasoning tasks.

### Open Question 2
- Question: What is impact of fine-tuning sample size on NinjaLLM's performance and hallucination mitigation?
- Basis in paper: [explicit] Paper mentions fine-tuning focused on format and tone using sample size of around 20 million tokens, and notes distribution of training samples is sensitive, requiring trial-and-error to optimize.
- Why unresolved: Paper does not explore how varying fine-tuning sample size affects model's performance, accuracy, or ability to mitigate hallucinations and biases.
- What evidence would resolve it: Conducting experiments with different fine-tuning sample sizes and analyzing resulting model performance and hallucination rates would clarify relationship between sample size and model effectiveness.

### Open Question 3
- Question: How do speculative decoding and flash attention techniques affect NinjaLLM's latency and throughput in real-world applications?
- Basis in paper: [explicit] Paper mentions plans to explore speculative decoding and flash attention to further enhance model's latency and throughput.
- Why unresolved: Paper does not provide empirical data on effects of these techniques on NinjaLLM's performance in practical scenarios.
- What evidence would resolve it: Implementing speculative decoding and flash attention in NinjaLLM and measuring resulting changes in latency and throughput during real-world application use would provide insights into practical benefits.

## Limitations

- Evaluation methodology lacks detailed description of accuracy measurement procedures and statistical significance testing
- Three-hour training claim with only ~20M tokens seems unusually short for effective 70B parameter model fine-tuning
- Cost claims don't break down training versus inference expenses, making true economics difficult to assess

## Confidence

- High confidence: Architectural description of using AWS Trainium/Inferentia2 chips and SageMaker for deployment is technically accurate and well-documented
- Medium confidence: vLLM optimizations (multi-bucketing and continuous batching) are described accurately but lack empirical validation of effectiveness in this specific implementation
- Low confidence: Accuracy claims, cost-effectiveness assertions, and performance improvements over baseline models lack sufficient methodological detail for independent verification

## Next Checks

1. Reproduce the fine-tuning pipeline with full hyperparameter disclosure to verify the three-hour training claim and assess whether reported accuracy improvements are statistically significant versus baseline Llama3-Instruct

2. Benchmark multi-bucketing vs baseline with controlled experiments measuring actual TTFT reduction, memory usage, and throughput improvements across varying input distributions to quantify claimed benefits

3. Cost analysis validation by running sustained inference workloads on both GPU and Trainium/Inferentia2 deployments to verify total cost claims, including amortized training costs over projected model lifetime