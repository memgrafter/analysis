---
ver: rpa2
title: 'SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation'
arxiv_id: '2410.15164'
source_url: https://arxiv.org/abs/2410.15164
tags:
- task
- tasks
- agents
- agent
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPA-Bench, a comprehensive benchmark for
  evaluating smartphone agents across 340 diverse tasks in both English and Chinese,
  including single-app and cross-app scenarios. The benchmark integrates 11 agents
  into a unified plug-and-play framework and introduces an automated evaluation pipeline
  that uses a hybrid success detection method combining key component matching and
  MLLM evaluation.
---

# SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation

## Quick Facts
- arXiv ID: 2410.15164
- Source URL: https://arxiv.org/abs/2410.15164
- Reference count: 40
- 11 agents evaluated across 340 tasks with automated success detection achieving 90.5% F1 score for single-app tasks

## Executive Summary
SPA-Bench introduces a comprehensive benchmark for evaluating smartphone agents across 340 diverse tasks in both English and Chinese. The benchmark integrates 11 agents into a unified plug-and-play framework and introduces an automated evaluation pipeline that uses hybrid success detection. Experiments show that agents following the agentic workflow significantly outperform agent-as-a-model approaches, achieving up to 64% success rate on English single-app tasks. However, all agents struggle with Chinese tasks, cross-app tasks, and handling dynamic UI content, highlighting key challenges in UI understanding, memory retention, and execution efficiency.

## Method Summary
SPA-Bench employs a multi-processing agent framework where worker processes connect agents to Android emulators, with snapshot-based state restoration ensuring consistent testing conditions. The benchmark includes 340 tasks across 58 apps, with agents tested using different UI representation modalities (screenshots, XML accessibility trees, OCR). Success detection uses a hybrid approach combining coarse key component matching with fine MLLM evaluation using GPT-4o. The framework is highly scalable and allows easy integration of new agents with minimal adaptation.

## Key Results
- Agentic workflow agents achieve up to 64% success rate on English single-app tasks, significantly outperforming agent-as-a-model approaches
- Automated success detection achieves 90.5% F1 score for single-app tasks and 84.5% F1 for cross-app tasks compared to human evaluation
- All agents struggle with Chinese tasks and cross-app scenarios, with significant performance gaps indicating challenges in language-specific UI understanding and memory retention

## Why This Works (Mechanism)

### Mechanism 1
The coarse-to-fine success detection pipeline balances scalability and accuracy by using key component matching to filter irrelevant trajectories before expensive MLLM evaluation. Key components extracted from the final state are matched against OCR-extracted text from execution screenshots, starting from the last screenshot and moving backward. Only if a match is found does the pipeline proceed to MLLM evaluation, which uses task descriptions, screenshots, and action information to make the final determination. Core assumption: Tasks have well-defined final states where certain text elements must appear if completed successfully.

### Mechanism 2
Agentic workflow agents outperform agent-as-a-model approaches due to better action grounding and generalization in dynamic UI environments. Agentic workflow agents use modular designs with visual perception modules (mark-up documents, set-of-marks) that help identify actionable UI elements and map them to coordinates, while agent-as-a-model agents rely on fine-tuned models optimized for fixed UI scenarios. Core assumption: Dynamic real-world UI environments require flexible action grounding rather than predefined action sequences.

### Mechanism 3
The unified plug-and-play framework enables scalable agent evaluation by isolating each agent-environment interaction. Worker processes each connect an agent to an Android emulator, with snapshot-based state restoration ensuring consistent testing conditions across experiments. This architecture allows easy integration of new agents with minimal adaptation. Core assumption: Isolating agent-environment interactions prevents interference and enables reproducible experiments.

## Foundational Learning

- Concept: UI representation modalities (screenshots, XML accessibility trees, OCR)
  - Why needed here: Different agents use different UI input methods, affecting their ability to understand and interact with mobile interfaces
  - Quick check question: Which UI representation method would best handle dynamic content that changes between screenshot capture and action execution?

- Concept: Task success detection metrics (precision, recall, F1 score)
  - Why needed here: Evaluating the effectiveness of automated success detection compared to human evaluation
  - Quick check question: If an automated system has high precision but low recall compared to human evaluation, what does this indicate about its performance?

- Concept: Agent workflow categorization (agentic workflow vs agent-as-a-model)
  - Why needed here: Understanding the fundamental architectural differences that lead to performance gaps between agent types
  - Quick check question: What key capability distinguishes agentic workflow agents from agent-as-a-model agents in handling dynamic UI environments?

## Architecture Onboarding

- Component map: Worker machine manages task/agent pools → Worker processes connect agents to Android emulators → Agents interact via screenshots/actions → Evaluation pipeline processes results → Metrics collected
- Critical path: Task assignment → Agent execution → Screenshot/action logging → Success detection → Metric calculation
- Design tradeoffs: Isolated worker processes provide consistency but limit inter-agent coordination; snapshot-based testing ensures reproducibility but may not capture all real-world variability
- Failure signatures: Invalid actions (tap on non-clickable elements), premature task termination, overdue termination reaching step limits, dynamic UI content changes between capture and action
- First 3 experiments:
  1. Run a simple single-app task (like setting an alarm) with AppAgent to verify framework integration
  2. Test cross-app task switching between two apps to validate memory retention mechanisms
  3. Execute a Chinese language task to evaluate language-specific performance differences

## Open Questions the Paper Calls Out

### Open Question 1
How would performance change if success detection used more advanced MLLM models beyond GPT-4o? The authors mention GPT-4o was used as the MLLM for success detection and note that coarse detection improved performance, particularly for Chinese tasks where MLLM evaluation struggles. Comparative experiments testing success detection F1 scores using different MLLM variants (e.g., GPT-4 Turbo, Claude 3, or specialized vision models) across both single-app and cross-app tasks would resolve this.

### Open Question 2
What architectural improvements could enable agents to handle dynamic UI content and animations more effectively? The authors identify that agents struggle with dynamic content, particularly in Chinese apps with frequent animations and ads, and note that delays between UI observation and action execution cause inaccuracies. Implementation and evaluation of agent architectures incorporating temporal reasoning, frame prediction, or real-time UI state tracking, measuring improvements in success rates on tasks with dynamic content would resolve this.

### Open Question 3
How would incorporating external knowledge bases or tool-use capabilities affect agent performance on complex tasks? The paper shows that agents with the agentic workflow outperform agent-as-a-model approaches, and notes that AppAgent and AutoDroid might have performed better with access to external knowledge documents. Experiments comparing agent performance with and without integrated search capabilities, knowledge retrieval, or external tool use across the full task suite, particularly for complex cross-app scenarios would resolve this.

## Limitations

- Significant performance gaps in Chinese language tasks and cross-app scenarios
- All agents struggle with dynamic UI content and memory retention during task switching
- Framework's isolated worker process architecture may not capture real-world multi-device coordination scenarios

## Confidence

- **High Confidence**: The comparative performance advantage of agentic workflow agents over agent-as-a-model approaches is well-supported by experimental data showing up to 64% success rate on English single-app tasks.
- **Medium Confidence**: The automated success detection pipeline's effectiveness is demonstrated but relies heavily on well-defined final states and may not generalize to open-ended tasks.
- **Low Confidence**: The generalizability of results across different Android device configurations and real-world network conditions remains uncertain due to the controlled emulator environment.

## Next Checks

1. **Cross-device validation**: Test the framework on physical Android devices with varying specifications to assess performance consistency across hardware configurations.
2. **Dynamic content robustness**: Evaluate agent performance on tasks with rapidly changing UI elements to measure resilience against timing-sensitive dynamic content.
3. **Memory retention analysis**: Conduct detailed analysis of cross-app task failures to identify specific memory retention bottlenecks and validate proposed improvements.