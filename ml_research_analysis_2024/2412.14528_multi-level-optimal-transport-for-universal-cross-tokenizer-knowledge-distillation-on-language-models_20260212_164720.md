---
ver: rpa2
title: Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation
  on Language Models
arxiv_id: '2412.14528'
source_url: https://arxiv.org/abs/2412.14528
tags:
- distillation
- transport
- optimal
- knowledge
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiLevelOT, a novel method for cross-tokenizer
  knowledge distillation in large language models. The key challenge addressed is
  enabling knowledge transfer between teacher and student models with different tokenizers,
  which existing methods cannot handle due to strict dimensional correspondence requirements.
---

# Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models

## Quick Facts
- arXiv ID: 2412.14528
- Source URL: https://arxiv.org/abs/2412.14528
- Reference count: 10
- Primary result: MultiLevelOT achieves 71%+ F1 score improvements in cross-tokenizer KD tasks by combining token-level and sequence-level optimal transport

## Executive Summary
This paper introduces MultiLevelOT, a novel method for cross-tokenizer knowledge distillation in large language models. The key challenge addressed is enabling knowledge transfer between teacher and student models with different tokenizers, which existing methods cannot handle due to strict dimensional correspondence requirements. MultiLevelOT achieves this through a dual-level optimal transport approach that aligns logit distributions at both token and sequence levels using diverse cost matrices. At the token level, it jointly optimizes all tokens within sequences using both absolute difference and logarithmic cost matrices. At the sequence level, it employs Sinkhorn distance to capture complex distribution structures.

## Method Summary
MultiLevelOT combines sequence-aware token-level optimal transport with sequence-level Sinkhorn distance to enable cross-tokenizer knowledge distillation. The method processes teacher and student logits through three complementary losses: Holistic Absolute Difference Loss (sequence-level ranking with truncation and absolute difference), Sequential Logarithmic Loss (logarithmic cost matrix for token-level alignment), and Sinkhorn Distance Loss (entropy-regularized optimal transport with Gaussian kernel). The combined loss uses hyperparameters α=0.15, β=0.1, γ=0.1 with temperature settings τ=1 for sequence-level and τ=2 for Sinkhorn distance. The method employs top-k truncation with k=50 and entropy regularization λ=0.1 across N=20 Sinkhorn iterations.

## Key Results
- MultiLevelOT consistently outperforms state-of-the-art cross-tokenizer KD methods (ULD, DSKD) across extractive QA, generative QA, and summarization tasks
- Achieves F1 score improvements exceeding 71% on certain tasks compared to existing approaches
- Demonstrates robust performance across different student and teacher models, architectures, and parameter sizes
- Shows effectiveness with both teacher and student models using different tokenizers (GPT2 vs WordPiece vs SentencePiece)

## Why This Works (Mechanism)
MultiLevelOT addresses the fundamental challenge of cross-tokenizer knowledge distillation by recognizing that different tokenizers produce logits of varying dimensions that cannot be directly aligned. The method's dual-level approach first aligns token-level distributions using both absolute difference and logarithmic cost matrices, capturing both local and global alignment patterns. The sequence-level Sinkhorn distance then captures the overall distribution structure, ensuring that the student model learns not just individual token alignments but also the broader contextual relationships. This comprehensive alignment strategy enables effective knowledge transfer even when tokenizers have non-overlapping vocabularies.

## Foundational Learning

**Optimal Transport Theory** - Needed to understand the mathematical foundation of aligning probability distributions between teacher and student models; quick check: verify understanding of Wasserstein distance and Sinkhorn iterations.

**Cross-Tokenizer Distillation** - Required to grasp why standard KD methods fail when teacher and student use different tokenizers; quick check: explain why token-to-token correspondence breaks down across different tokenization schemes.

**Sequence-Level Alignment** - Essential for understanding how the method captures distributional structures beyond individual token alignment; quick check: describe how Sinkhorn distance differs from simple KL divergence in capturing distribution relationships.

**Entropy Regularization** - Important for understanding computational tractability and the trade-off between accuracy and efficiency in OT computation; quick check: explain the role of λ parameter in the Sinkhorn algorithm.

**Logarithmic Cost Matrices** - Needed to understand how multiplicative relationships between logits are captured; quick check: verify understanding of why logarithmic costs can capture relative differences better than absolute differences.

## Architecture Onboarding

**Component Map**: Input Data → Tokenizer Processing → Logit Extraction → Holistic Absolute Difference Loss → Sequential Logarithmic Loss → Sinkhorn Distance Loss → Combined Loss → Student Training

**Critical Path**: The core innovation flows through three sequential components: (1) Holistic Absolute Difference Loss for initial alignment, (2) Sequential Logarithmic Loss for refined token-level matching, and (3) Sinkhorn Distance Loss for sequence-level distributional alignment.

**Design Tradeoffs**: The method trades computational complexity for alignment accuracy by using dual-level OT optimization versus simpler distillation approaches. The choice of cost matrices (absolute vs logarithmic) and entropy regularization λ=0.1 represents a balance between alignment precision and computational tractability.

**Failure Signatures**: Performance degradation typically occurs when sequence lengths exceed memory limits for Sinkhorn computation, when teacher-student size gaps are too large for effective alignment, or when tokenization differences are too extreme for any meaningful correspondence.

**Three First Experiments**:
1. Baseline ablation: Train student model using only standard KD (without MultiLevelOT) on same tasks to establish performance floor
2. Component isolation: Test each of the three losses individually to measure their individual contributions
3. Tokenizer variation: Train with same teacher model but different student tokenizers to verify cross-tokenizer effectiveness

## Open Questions the Paper Calls Out

**Open Question 1**: How does MultiLevelOT performance scale when applied to even larger language models beyond those tested (e.g., models with 70B+ parameters)? The paper demonstrates effectiveness across various model sizes but does not test extremely large models beyond 8B parameters.

**Open Question 2**: What is the impact of different entropy regularization weights (λ) in the Sinkhorn distance computation on distillation quality across various task types? The paper uses a fixed λ=0.1 value across all experiments without exploring how different values affect performance.

**Open Question 3**: How does MultiLevelOT perform in cross-modal knowledge distillation scenarios, such as transferring knowledge between language models and vision-language models? The paper focuses exclusively on language model distillation and mentions multi-modal transfer only as a future direction.

## Limitations
- Performance relies heavily on careful hyperparameter tuning (α, β, γ, temperature settings) with sensitivity across different model pairs and tasks unclear
- Computational overhead from dual-level OT optimization could be significant, especially for longer sequences, though exact runtime comparisons are not provided
- Assumes access to teacher model logits during student training, which may not be feasible with proprietary or computationally expensive teacher models

## Confidence

**High confidence** in the core technical contribution: The mathematical formulation of combining token-level and sequence-level optimal transport is well-defined and experimental results consistently show improvements across different tasks and model combinations.

**Medium confidence** in practical applicability: While results are promising, the method's performance on longer sequences (>512 tokens) and with larger teacher-student size gaps remains unexplored.

**Medium confidence** in generalization: The evaluation covers multiple tasks and model pairs, but testing on a broader range of model architectures and different tokenization schemes would strengthen generalizability claims.

## Next Checks
1. **Ablation study on hyperparameter sensitivity**: Systematically vary α, β, γ, and temperature values across different model pairs to determine robustness to hyperparameter choices and identify optimal settings for specific scenarios.

2. **Computational efficiency benchmarking**: Measure wall-clock training time and memory usage of MultiLevelOT versus baseline methods (ULD, DSKD) across different sequence lengths and batch sizes to quantify the practical computational overhead.

3. **Long-sequence performance evaluation**: Test MultiLevelOT on tasks involving longer input sequences (>1024 tokens) to verify that the dual-level OT approach maintains effectiveness and computational tractability with increased sequence length.