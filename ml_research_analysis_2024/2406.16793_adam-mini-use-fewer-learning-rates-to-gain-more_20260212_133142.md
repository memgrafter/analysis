---
ver: rpa2
title: 'Adam-mini: Use Fewer Learning Rates To Gain More'
arxiv_id: '2406.16793'
source_url: https://arxiv.org/abs/2406.16793
tags:
- adam-mini
- adam
- learning
- adamw
- hessian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Adam-mini, an optimizer that reduces the memory
  footprint of AdamW by 50% while maintaining or improving performance. The key insight
  is that Adam's per-parameter learning rates are often redundant, especially within
  dense Hessian sub-blocks of neural networks.
---

# Adam-mini: Use Fewer Learning Rates To Gain More

## Quick Facts
- **arXiv ID**: 2406.16793
- **Source URL**: https://arxiv.org/abs/2406.16793
- **Reference count**: 40
- **Primary result**: Adam-mini reduces AdamW memory footprint by 50% while maintaining or improving performance across various LLM training tasks

## Executive Summary
Adam-mini is a memory-efficient optimizer that partitions parameters into blocks based on Hessian structure and assigns a single learning rate per block using the mean of Adam's second-order momentum. This approach cuts down more than 99.9% of Adam's v vector, saving 50% of memory while maintaining or improving performance compared to AdamW. The optimizer shows particular effectiveness for large-scale training where memory efficiency is critical, achieving 49.6% higher throughput when pre-training Llama 2-7B on 2× A800-80GB GPUs.

## Method Summary
Adam-mini modifies the AdamW optimizer by replacing per-parameter learning rates with block-wise learning rates derived from Hessian structure analysis. The method partitions parameters into blocks corresponding to dense Hessian sub-blocks (such as attention heads and MLP output neurons), then computes a single learning rate per block as the mean of Adam's v values within that block. This reduces memory usage by 50% while maintaining the adaptive learning rate functionality through block-wise adaptation. The optimizer is tested across pre-training, supervised fine-tuning, and RLHF tasks on language models ranging from 39M to 13B parameters.

## Key Results
- Maintains or improves validation loss compared to AdamW across various model sizes (39M to 13B parameters)
- Achieves 49.6% higher throughput than AdamW when pre-training Llama 2-7B on 2× A800-80GB GPUs
- Reduces wall-clock time by 33% through memory savings enabling larger batch sizes
- Successfully applied to pre-training, SFT, and RLHF tasks on C4 and OpenWebText datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hessian near-block-diagonal structure allows single learning rate per dense sub-block to match or outperform per-parameter learning rates
- **Mechanism**: Neural network Hessians exhibit dense sub-blocks corresponding to parameter groups (e.g., heads in attention, output neurons in MLP). Each sub-block can be effectively optimized with a single learning rate chosen as the mean of Adam's v values within that block, avoiding redundant per-parameter rates
- **Core assumption**: Dense Hessian sub-blocks maintain their structure throughout training and can be identified reliably
- **Evidence anchors**: [abstract] "Adam-mini partitions parameters into blocks based on Hessian structure and assigns a single learning rate per block using the mean of Adam's second-order momentum"; [section] "First, we recall a classical result that the Hessian of neural nets is near-block-diagonal with several dense principle sub-blocks"

### Mechanism 2
- **Claim**: Reducing learning rate memory from parameter count to block count (50% reduction) while maintaining performance
- **Mechanism**: By replacing per-parameter 1/√v values with block-wise averages, Adam-mini eliminates ~99.9% of v memory while preserving effective preconditioning through block-wise adaptation
- **Core assumption**: Block-wise learning rates provide sufficient adaptation across heterogeneous parameter groups
- **Evidence anchors**: [abstract] "Adam-mini reduces memory by cutting down the learning rate resources in Adam (i.e., 1/√v)"; [section] "Adam-mini largely reduces the number of learning rates used in Adam...cuts down more than 99.9% of Adam's v, which saves 50% of Adam's memory"

### Mechanism 3
- **Claim**: Improved throughput from reduced memory pressure enables larger batch sizes and reduced communication overhead
- **Mechanism**: 50% memory reduction allows doubling batch size per GPU, while fewer learning rate vectors reduce communication volume in distributed training
- **Core assumption**: Memory pressure, not compute capacity, is the primary bottleneck in distributed LLM training
- **Evidence anchors**: [abstract] "Adam-mini achieves 49.6% higher throughput than AdamW when pre-training Llama 2-7B on 2× A800-80GB GPUs"; [section] "The memory cut-down allows larger batch sizes per GPU, and at the same time, it eases the burden of communication among GPUs"

## Foundational Learning

- **Concept**: Hessian matrix and its role in optimization
  - **Why needed here**: Understanding why neural network Hessians have block-diagonal structure is crucial for grasping why block-wise learning rates work
  - **Quick check question**: What property of neural network loss surfaces leads to near-block-diagonal Hessian structure?

- **Concept**: Second-order momentum (v) in adaptive optimizers
  - **Why needed here**: v provides per-parameter learning rates; understanding its role is essential for seeing why block-wise averaging preserves functionality
  - **Quick check question**: How does the v vector in Adam provide adaptive learning rates for each parameter?

- **Concept**: Block-diagonal preconditioning
  - **Why needed here**: The core insight is that diagonal preconditioning within dense blocks can be approximated by single learning rates
  - **Quick check question**: Why might a single learning rate per dense block be sufficient for effective optimization?

## Architecture Onboarding

- **Component map**: Parameter partitioner -> Block-wise learning rate calculator -> Modified Adam update loop -> Memory manager
- **Critical path**: 1. Partition parameters into blocks based on Hessian structure; 2. Compute block-wise learning rates using mean(v) within each block; 3. Apply AdamW-style updates using block-wise learning rates; 4. Maintain momentum and weight decay as in AdamW
- **Design tradeoffs**: Memory vs. adaptation granularity (block-wise vs. per-parameter); Partition strategy complexity vs. effectiveness; Mean vs. other aggregation methods for block learning rates
- **Failure signatures**: Training instability when partitions don't align with Hessian structure; Poor convergence when block-wise rates insufficiently adapt to parameter heterogeneity; Memory savings not realized when partition strategy creates too many small blocks
- **First 3 experiments**: 1. Verify block-wise learning rates maintain performance on small transformer with known Hessian structure; 2. Test memory savings and throughput improvement on moderate-sized LLM pre-training; 3. Validate partition strategy on different architectures (CNNs, GNNs) to ensure generalizability

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the traditional sense. However, several important areas for future research emerge from the discussion:

1. **Cross-architecture generalization**: While Adam-mini shows strong performance on transformers, its effectiveness on other architectures like CNNs, diffusion models, and graph neural networks requires further investigation to understand the generalizability of the blockwise learning rate design.

2. **Theoretical foundation**: The paper mentions that Adam-mini's blockwise design can potentially outperform Adam's coordinate-wise learning rates in reducing the condition number of the Hessian matrix, but does not provide a rigorous theoretical justification for this claim.

3. **Partition strategy optimization**: The choice of block partitioning strategy is crucial for Adam-mini's performance, but the paper focuses on a specific partitioning approach for transformers without exploring how different strategies affect performance on various neural network architectures.

## Limitations

- The paper relies on classical results about Hessian structure without direct empirical validation in modern transformer architectures
- Throughput improvement claims depend heavily on specific hardware configurations (A800-80GB GPUs) and may not generalize
- The paper doesn't address potential degradation in generalization performance or behavior with non-standard architectures
- Effectiveness of mean aggregation for block learning rates across diverse parameter groups remains theoretically justified but not rigorously tested

## Confidence

- **High confidence**: Memory reduction claims (50% memory savings is straightforward to measure and verify) and throughput improvement on the specific tested hardware configuration
- **Medium confidence**: Performance parity or improvement claims, as these depend on proper hyperparameter tuning and the effectiveness of the partitioning strategy across diverse tasks
- **Low confidence**: Theoretical claims about Hessian structure applicability to modern architectures and the generalizability of mean aggregation across all parameter heterogeneity patterns

## Next Checks

1. **Hessian structure validation**: Compute and visualize the Hessian of small transformer layers to empirically verify the near-block-diagonal structure claim, particularly for attention and MLP blocks, across different training stages

2. **Cross-architecture generalization**: Test Adam-mini on vision transformers and graph neural networks to verify that the partitioning strategy and mean aggregation approach generalizes beyond standard language models, measuring both performance and memory efficiency

3. **Robustness to partition strategies**: Compare Adam-mini performance using different partitioning methods (Hessian-based vs. simple structural grouping like all attention parameters vs. all MLP parameters) to determine whether the sophisticated Hessian analysis is necessary or if simpler approaches yield similar results