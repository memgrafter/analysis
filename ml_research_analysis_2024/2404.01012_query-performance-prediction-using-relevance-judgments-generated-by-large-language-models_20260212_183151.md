---
ver: rpa2
title: Query Performance Prediction using Relevance Judgments Generated by Large Language
  Models
arxiv_id: '2404.01012'
source_url: https://arxiv.org/abs/2404.01012
tags:
- relevance
- query
- qpp-genre
- judgments
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Query performance prediction (QPP) estimates retrieval quality
  without human relevance judgments. Existing QPP methods typically output a single
  scalar score, which limits their ability to accurately represent different IR evaluation
  measures and limits interpretability.
---

# Query Performance Prediction using Relevance Judgments Generated by Large Language Models

## Quick Facts
- arXiv ID: 2404.01012
- Source URL: https://arxiv.org/abs/2404.01012
- Reference count: 40
- Primary result: QPP-GenRE achieves state-of-the-art QPP quality for both lexical and neural rankers using fine-tuned 3B models that outperform 70B few-shot models

## Executive Summary
Query performance prediction (QPP) estimates retrieval quality without human relevance judgments, traditionally outputting single scalar scores that limit interpretability and flexibility. This paper proposes QPP-GenRE, a framework that decomposes QPP into predicting relevance for each item in ranked lists using automatically generated relevance judgments. By fine-tuning small open-source LLMs on human-labeled data, QPP-GenRE achieves superior performance compared to few-shot prompting with much larger models, while enabling interpretable multi-metric assessment and error analysis.

## Method Summary
QPP-GenRE uses fine-tuned open-source LLMs (Llama and Mistral families) with QLoRA parameter-efficient fine-tuning to predict binary relevance judgments for top-retrieved items. The framework approximates recall-oriented metrics by judging only top-n items and reorders predicted relevance judgments to compute ideal DCG. Experiments use TREC 2019-2022 Deep Learning tracks with BM25, ANCE, and TAS-B retrieivers, training for 5 epochs with learning rate 0.00002 on Adam optimizer using MS MARCO V1 dev set.

## Key Results
- Fine-tuned 3B models outperform 70B few-shot models in QPP quality
- QPP-GenRE achieves state-of-the-art performance for both lexical and neural rankers
- The method generalizes well to conversational search and demonstrates good interpretability
- Approximating recall-oriented metrics by judging top-n items maintains accuracy while reducing computational cost

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning small open-source LLMs on human-labeled relevance judgments yields better QPP quality than few-shot prompting with much larger models. Parameter-efficient fine-tuning (QLoRA) adapts smaller models by optimizing low-rank adapters while keeping original parameters frozen, allowing the model to learn task-specific relevance patterns from labeled data rather than relying on in-context learning.

### Mechanism 2
Approximating recall-oriented metrics by judging only top-n items in ranked lists avoids full corpus traversal while maintaining QPP accuracy. The approximation strategy reorders predicted relevance judgments for top-n items and uses this subset to compute ideal DCG, effectively approximating the recall component without examining the entire corpus.

### Mechanism 3
Decomposing QPP into per-item relevance prediction enables interpretable multi-metric assessment and error analysis. Instead of predicting a single scalar score, the framework generates explicit binary relevance judgments for each item, which can be aggregated into any IR metric and analyzed for errors.

## Foundational Learning

- **Parameter-efficient fine-tuning (PEFT) methods like QLoRA**: Needed to efficiently adapt small models without full fine-tuning costs. Quick check: What is the key difference between QLoRA and traditional fine-tuning in terms of which parameters are updated?

- **IR evaluation metrics and their relationships**: Understanding precision vs recall orientation is crucial for designing approximation strategies. Quick check: Why is nDCG@10 considered a recall-oriented metric while RR@10 is precision-oriented?

- **Query performance prediction (QPP) evaluation methodology**: The paper uses correlation-based evaluation requiring understanding of Pearson's ρ and Kendall's τ. Quick check: What are the key differences between correlation-based and utility-based QPP evaluation approaches?

## Architecture Onboarding

- **Component map**: Query input → LLM relevance predictor (fine-tuned or few-shot) → Binary relevance judgments → IR metric calculator → QPP output → Caching mechanism for efficiency

- **Critical path**: Query → LLM → Relevance judgments → Metric calculation → QPP score. The LLM inference is the primary bottleneck; relevance judgment caching helps mitigate this for sequential evaluations.

- **Design tradeoffs**: Judging depth vs accuracy (shallow judgments are fast but less accurate for recall; deeper judgments improve accuracy but increase cost); fine-tuning vs few-shot (fine-tuning smaller models achieves better accuracy but requires labeled data); binary vs multi-graded labels (binary judgments are simpler but less nuanced).

- **Failure signatures**: Low correlation with ground truth indicates poor relevance prediction quality; inconsistent predictions across similar queries suggest overfitting or lack of generalization; high latency may indicate insufficient batching or inefficient model choice.

- **First 3 experiments**: 
  1. Compare QPP-GenRE with judging depth n=10 vs n=200 on TREC-DL 19 to measure the trade-off between speed and accuracy
  2. Test fine-tuned vs few-shot versions of the same LLM (e.g., Llama-3.2-3B) on the same dataset to isolate the impact of fine-tuning
  3. Apply QPP-GenRE to a conversational search dataset (CAsT) to verify zero-shot generalization to new domains

## Open Questions the Paper Calls Out

The paper identifies several areas for future work: exploring QPP-GenRE's performance with pairwise and listwise LLM-based re-rankers beyond the current pointwise approach; investigating the impact of different judging depths on QPP-GenRE's performance for recall-oriented metrics beyond nDCG@10; and assessing potential biases towards LLM-based rankers and mitigation strategies.

## Limitations

- Limited evaluation to TREC Deep Learning tracks and one conversational search dataset, raising questions about generalization to other domains
- Approximation strategy for recall-oriented metrics lacks empirical validation against full-corpus ground truth evaluations
- The paper doesn't clearly define what constitutes "good" QPP performance in practical terms beyond correlation metrics

## Confidence

- **High Confidence**: Fine-tuning small open-source LLMs with human-labeled data yields better QPP quality than few-shot prompting with larger models
- **Medium Confidence**: Decomposing QPP into per-item relevance prediction provides interpretability and enables multi-metric assessment
- **Low Confidence**: The approximation strategy for recall-oriented metrics maintains sufficient accuracy

## Next Checks

1. Apply QPP-GenRE to datasets outside the TREC domain (e.g., medical, legal, or news corpora) and measure performance degradation to test generalization capability
2. For a subset of queries, compute full-corpus relevance judgments and compare the approximated recall-oriented metrics against ground truth to validate the approximation strategy's accuracy
3. Design a downstream retrieval optimization task and measure whether QPP-GenRE predictions actually improve retrieval outcomes beyond correlation metrics