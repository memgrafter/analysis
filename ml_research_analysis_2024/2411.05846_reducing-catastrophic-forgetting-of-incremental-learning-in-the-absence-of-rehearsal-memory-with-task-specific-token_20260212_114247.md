---
ver: rpa2
title: Reducing catastrophic forgetting of incremental learning in the absence of
  rehearsal memory with task-specific token
arxiv_id: '2411.05846'
source_url: https://arxiv.org/abs/2411.05846
tags:
- task
- learning
- data
- token
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for incremental learning that
  preserves previous knowledge without storing past data. The approach uses task-specific
  tokens within a vision transformer architecture to direct attention differently
  for each task, mimicking the effect of having multiple models.
---

# Reducing catastrophic forgetting of incremental learning in the absence of rehearsal memory with task-specific token

## Quick Facts
- arXiv ID: 2411.05846
- Source URL: https://arxiv.org/abs/2411.05846
- Reference count: 9
- This paper introduces a novel method for incremental learning that preserves previous knowledge without storing past data using task-specific tokens in vision transformers.

## Executive Summary
This paper addresses the challenge of catastrophic forgetting in incremental learning without using rehearsal memory. The proposed method employs task-specific tokens within a vision transformer architecture to direct attention differently for each task, effectively mimicking the behavior of multiple models while using only one. A distillation process ensures efficient knowledge transfer across multiple learning steps, preserving previously learned information without storing old data.

## Method Summary
The approach uses a Vision Transformer (ViT) architecture augmented with learnable task tokens that are prepended to patch tokens. These task tokens accumulate compressed class information for each task through self-attention mechanisms, directing the feature extractor to produce task-specific embeddings. During incremental learning, feature-level distillation transfers knowledge from the previous model to the current one by aligning student and teacher features for each task token. The method is evaluated on CIFAR-100 with various task-incremental learning scenarios including B0-5steps, B0-10steps, B50-5steps, and B50-10steps configurations.

## Key Results
- Outperforms existing non-rehearsal-based approaches on CIFAR-100 across various task-incremental learning scenarios
- Demonstrates significantly reduced catastrophic forgetting compared to state-of-the-art methods
- Shows improved backward transfer (BWT) metrics, particularly as the number of learning steps increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task tokens control attention to encode task-specific features, mimicking separate models
- Mechanism: A dedicated learnable token is prepended to patch tokens and, via self-attention, accumulates compressed class information for each task. This token guides the feature extractor to produce embeddings aligned with task identity
- Core assumption: Attention mechanisms can effectively direct information flow such that a single token can represent distinct task contexts without interference
- Evidence anchors:
  - [abstract] "This approach generates task-specific embeddings by directing attention differently based on the task associated with the data"
  - [section] "a task token to provide specific attention to each task and accumulate knowledge"
  - [corpus] Weak‚Äîno neighboring papers explicitly discuss task tokens in incremental learning
- Break condition: If task tokens fail to maintain distinct embeddings for different tasks, catastrophic forgetting would increase

### Mechanism 2
- Claim: Feature-level distillation preserves previous task knowledge without rehearsal memory
- Mechanism: At each step, the model distills the feature embeddings produced by the previous-step teacher model for every task token, aligning student and teacher features without needing raw data
- Core assumption: Feature-level similarity can approximate the preservation of discriminative patterns learned in earlier tasks
- Evidence anchors:
  - [abstract] "Our method incorporates a distillation process that ensures efficient interactions even after multiple additional learning steps"
  - [section] "we distil the classification information of the ùëñ-th task from the teacher model by learning that the feature ‚Ä¶ from the teacher model and ‚Ä¶ from the model being trained ‚Ä¶ have an equal value"
  - [corpus] Weak‚Äîno direct neighbor citations of this distillation style
- Break condition: If distillation fails to preserve discriminative features, accuracy on old tasks would degrade

### Mechanism 3
- Claim: Multi-task token distillation effectively simulates having multiple teacher models, preventing forgetting
- Mechanism: By distilling from the previous model using all task tokens, the student receives knowledge from multiple "teachers," each specialized for a different task context
- Core assumption: Aligning features for each task token is equivalent to distilling from multiple distinct teacher models
- Evidence anchors:
  - [section] "distilling the student model from the teacher model to produce similar features for each task token has an effect identical to that of distilling from multiple teachers separately generated for each task"
  - [corpus] Weak‚Äîno explicit neighbor support for this equivalence
- Break condition: If multi-token distillation does not preserve task-specific knowledge, BWT would worsen

## Foundational Learning

- Concept: Vision Transformers and self-attention
  - Why needed here: The architecture relies on self-attention to embed task tokens into discriminative features
  - Quick check question: How does a class token in ViT accumulate classification information?

- Concept: Knowledge distillation
  - Why needed here: Distillation transfers knowledge from the previous model without rehearsal memory
  - Quick check question: What loss term aligns student and teacher features in feature distillation?

- Concept: Catastrophic forgetting and backward transfer
  - Why needed here: The goal is to minimize forgetting measured by backward transfer
  - Quick check question: How is BWT computed from accuracy on old tasks after new learning steps?

## Architecture Onboarding

- Component map: Input images ‚Üí Patch tokenization ‚Üí Task token concatenation ‚Üí Multi-head self-attention layers ‚Üí MLP ‚Üí Task-specific MLP heads ‚Üí Distillation loss from previous step
- Critical path: Forward pass through ViT ‚Üí task token embedding ‚Üí classification ‚Üí distillation loss computation
- Design tradeoffs: More task tokens increase memory but improve task-specific representation; distillation increases computation but preserves knowledge
- Failure signatures: Accuracy drop on old tasks, increased BWT, task tokens producing similar embeddings across tasks
- First 3 experiments:
  1. Train on step 1 only, evaluate accuracy and BWT; confirm baseline
  2. Add step 2 with distillation, compare accuracy and BWT to step 1
  3. Test multi-step training, evaluate stability of task tokens and forgetting rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method scale in terms of computational cost as the number of tasks increases, particularly during the distillation process involving all task tokens?
- Basis in paper: [explicit] The paper mentions that using knowledge distillation with all task tokens for each incremental task increases the computational cost because training involves all tasks as the number of tasks increases. It suggests that future work should explore ways to reduce the computational costs
- Why unresolved: The paper does not provide specific data or analysis on the computational scaling of the method with increasing tasks. It only acknowledges the potential issue without quantifying it
- What evidence would resolve it: Empirical studies measuring computational time, memory usage, and energy consumption of the method as the number of tasks increases, compared to baseline methods. Additionally, analysis of the trade-off between computational cost and performance improvement

### Open Question 2
- Question: Can the task token approach be effectively extended to class-incremental learning scenarios where task information is not available during inference?
- Basis in paper: [explicit] The paper discusses the possibility of extending the method to class-incremental learning but notes limitations. It mentions that without task information, using all tokens from all tasks to obtain logits for all task labels is possible but relies on learning sufficiently large logit values for correct labels
- Why unresolved: The paper only briefly mentions the potential for extension to class-incremental learning without providing concrete results or a detailed methodology. It acknowledges the need for further development to improve performance in this setting
- What evidence would resolve it: Experiments demonstrating the performance of the method in class-incremental learning scenarios, including comparisons with state-of-the-art class-incremental learning methods. Additionally, analysis of the effectiveness of proposed improvements, such as developing a task-distinguishable module similar to the auxiliary classifier in the DER method

### Open Question 3
- Question: How does the proposed method perform on datasets with larger image sizes and more diverse visual content compared to CIFAR-100?
- Basis in paper: [inferred] The paper evaluates the method on CIFAR-100, which consists of small 32x32 pixel images. It does not provide evidence of performance on larger or more complex datasets
- Why unresolved: The evaluation is limited to a single dataset with specific characteristics (small images, limited classes). The generalizability of the method to other types of visual data is unknown
- What evidence would resolve it: Experiments applying the method to diverse datasets with larger images (e.g., ImageNet, COCO) and more complex visual content. Analysis of performance changes with respect to image size, dataset complexity, and domain shift

## Limitations

- Experimental scope limited to CIFAR-100 with fixed 32√ó32 resolution and only four specific task-splitting configurations
- Method's effectiveness on larger-scale datasets or different problem domains remains unverified
- Critical hyperparameters such as learning rate, batch size, and distillation parameters are not specified

## Confidence

**High Confidence (8/10)**: The core mechanism of using task-specific tokens with feature-level distillation is technically sound and aligns with established principles in transformer architectures and knowledge distillation.

**Medium Confidence (6/10)**: The empirical performance claims on CIFAR-100 are supported by the reported experiments, but the limited dataset scope and absence of ablation studies reduce confidence in the relative contributions of each component.

**Low Confidence (4/10)**: Claims about scalability to many tasks and generalization to other domains are speculative without supporting experiments.

## Next Checks

1. **Ablation Study on Task Token Effectiveness**: Systematically remove the task token mechanism and compare performance with and without task tokens across all task-incremental scenarios. Measure the individual contribution of task tokens to accuracy and BWT improvements.

2. **Scalability Testing with Increased Task Numbers**: Evaluate the method on CIFAR-100 with 10, 20, and 50 task splits to identify the breaking point where task tokens fail to maintain separation. Monitor BWT degradation and token embedding similarity as task numbers increase.

3. **Cross-Dataset Generalization**: Implement the method on a second dataset (e.g., CIFAR-10 or a subset of ImageNet) with identical task-splitting protocols. Compare accuracy and BWT performance to establish whether the CIFAR-100 results are dataset-specific or represent a general principle.