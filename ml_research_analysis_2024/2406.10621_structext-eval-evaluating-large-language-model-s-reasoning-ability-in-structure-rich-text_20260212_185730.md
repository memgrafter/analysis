---
ver: rpa2
title: 'StrucText-Eval: Evaluating Large Language Model''s Reasoning Ability in Structure-Rich
  Text'
arxiv_id: '2406.10621'
source_url: https://arxiv.org/abs/2406.10621
tags:
- input
- cafe
- banana
- structext-eval
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces StrucText-Eval, a benchmark designed to\
  \ evaluate large language models\u2019 (LLMs) ability to understand and reason over\
  \ structure-rich text. Unlike previous benchmarks that focus on specific formats\
  \ like graphs or tables, StrucText-Eval covers eight structured languages and 29\
  \ diverse tasks, enabling the generation of data with adjustable complexity."
---

# StrucText-Eval: Evaluating Large Language Model's Reasoning Ability in Structure-Rich Text

## Quick Facts
- arXiv ID: 2406.10621
- Source URL: https://arxiv.org/abs/2406.10621
- Reference count: 23
- Introduces a benchmark testing LLMs' ability to reason over structure-rich text across 8 languages and 29 tasks

## Executive Summary
This paper introduces StrucText-Eval, a benchmark designed to evaluate large language models' ability to understand and reason over structure-rich text. Unlike previous benchmarks that focus on specific formats like graphs or tables, StrucText-Eval covers eight structured languages and 29 diverse tasks, enabling the generation of data with adjustable complexity. The benchmark includes two suites: a standard Test suite and a more challenging Test-Hard suite. Experimental results show that while open-source LLMs achieve up to 74.9% accuracy on the standard dataset, their performance drops significantly to 45.8% on the harder dataset. In contrast, human participants reach 92.6% accuracy on the Test-Hard suite, highlighting LLMs' current limitations in handling intricate structural information. The benchmark and generation codes are publicly available.

## Method Summary
StrucText-Eval uses prompt-based and fine-tuning methods on open-source and close-source LLMs, with six distinct prompt configurations and three-shot demonstrations. The benchmark contains 5,800 pre-generated and annotated samples across 8 structured languages and 29 tasks, divided into a standard Test suite (3,712 samples) and a Test-Hard suite (2,088 samples). Evaluation uses RougeL score as the primary metric, with additional metrics like LLM-as-Judge-Score, BLEU, and Exact Match for comparison.

## Key Results
- Open-source LLMs achieve up to 74.9% accuracy on the standard Test suite
- Performance drops to 45.8% on the more challenging Test-Hard suite
- Human participants achieve 92.6% accuracy on the Test-Hard suite
- Model performance degrades predictably as structural complexity increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: StrucText-Eval tests whether LLMs can understand structured data purely through unstructured text representation
- Mechanism: By removing semantic content and requiring inference based only on structural patterns, the benchmark isolates structural reasoning ability from semantic understanding
- Core assumption: Structural patterns contain sufficient information for reasoning when semantic content is stripped away

### Mechanism 2
- Claim: The benchmark's adjustable complexity through depth, width, and column parameters creates a graduated difficulty scale
- Mechanism: By systematically varying tree depth, structural width, and field count, the benchmark can assess performance across different complexity levels
- Core assumption: Model performance will degrade predictably as structural complexity increases

### Mechanism 3
- Claim: Few-shot demonstrations improve model performance by providing structural reasoning templates
- Mechanism: Providing examples of how to parse and reason about structures helps models understand the task format and reasoning patterns
- Core assumption: Models can generalize from few examples to handle novel structures

## Foundational Learning

- Concept: Tree traversal algorithms
  - Why needed here: Many tasks require finding paths, depths, and heights in tree structures
  - Quick check question: How would you find the path from root to a specific node in a tree?

- Concept: Recursive data structure parsing
  - Why needed here: JSON, YAML, and XML structures require recursive processing to extract nested information
  - Quick check question: What's the difference between depth-first and breadth-first traversal for nested data?

- Concept: Table join operations
  - Why needed here: Join tasks require combining information from multiple tables based on key fields
  - Quick check question: How would you join two tables on a common key field?

## Architecture Onboarding

- Component map:
  - Data generation module -> Task template engine -> Answer extraction system -> Evaluation pipeline -> Complexity controller

- Critical path:
  1. Generate abstract tree structure with specified complexity
  2. Apply task template to create question
  3. Use answer discovery algorithm to find ground truth
  4. Convert structure to target language format
  5. Evaluate model response against ground truth

- Design tradeoffs:
  - Control vs. diversity: More control over complexity reduces sample variety
  - Semantic removal vs. task difficulty: Removing semantics makes tasks harder but may create artificial scenarios
  - Parameter granularity vs. usability: More complexity parameters provide better control but increase configuration complexity

- Failure signatures:
  - Models perform well on simple structures but poorly on complex ones
  - Performance varies significantly across different structured languages
  - Models fail to generalize from few-shot demonstrations

- First 3 experiments:
  1. Test model performance on JSON PathCompose task with depth=1, width=1, column=1
  2. Evaluate performance degradation as depth increases from 1 to 3
  3. Compare performance with and without few-shot demonstrations on the same task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do current LLMs handle nested structures in structure-rich text beyond the 3-level depth tested in StrucText-Eval-Hard?
- Basis in paper: The paper mentions that StrucText-Eval-Hard includes depth up to 3 levels, and that performance drops significantly with increased complexity
- Why unresolved: The paper does not explore LLMs' capabilities with deeper nesting beyond what was tested
- What evidence would resolve it: Additional testing with nested structures deeper than 3 levels to determine the breaking point for LLM performance

### Open Question 2
- Question: What is the impact of training data composition on LLM performance with different structure-rich text formats?
- Basis in paper: The paper notes that JSON performance is better due to training sample bias, but doesn't quantify how training data composition affects other formats
- Why unresolved: The paper identifies training sample bias as a factor but doesn't measure the specific impact of different training data compositions on performance across formats
- What evidence would resolve it: Detailed analysis of how varying proportions of training data from different structure-rich formats affects LLM performance on those formats

### Open Question 3
- Question: How do LLMs perform on structure-rich text with semantic content compared to the semantically-agnostic tasks in StrucText-Eval?
- Basis in paper: The paper deliberately removes semantic content to test pure structural reasoning, implying that semantic content might affect performance
- Why unresolved: The paper focuses on semantically-agnostic tasks, leaving the question of how semantic content affects structural reasoning unanswered
- What evidence would resolve it: Comparative testing of LLMs on structure-rich text with and without semantic content to measure performance differences

## Limitations

- Benchmark focuses on structure-rich text without semantic content, creating an artificial evaluation scenario
- Limited sample size (5,800 samples) may not capture full diversity of real-world structured data scenarios
- Human baseline validity questions regarding participant background and testing conditions

## Confidence

**High Confidence Claims**:
- The benchmark successfully generates structure-rich text across multiple languages
- Model performance degrades with increasing structural complexity
- Few-shot demonstrations improve model performance across most tasks
- Open-source models show significant performance gaps compared to human participants on harder tasks

**Medium Confidence Claims**:
- StrucText-Eval provides a more comprehensive evaluation of structural reasoning than existing benchmarks
- The adjustable complexity parameters effectively create graduated difficulty levels
- Performance variations across structured languages reflect genuine differences in language complexity

**Low Confidence Claims**:
- Structural reasoning ability in isolation predicts real-world reasoning performance
- The artificial removal of semantic content creates a valid measure of structural understanding
- Current LLM limitations on this benchmark indicate fundamental architectural constraints

## Next Checks

**Check 1: Semantic Integration Impact**
Re-run the benchmark with semantically meaningful content embedded in the structures while maintaining the same structural patterns. Compare performance drops to assess whether structural reasoning ability transfers to real-world scenarios where semantic and structural reasoning must be integrated.

**Check 2: Natural Data Validation**
Apply the benchmark tasks to naturally occurring structured data from real applications (e.g., configuration files, database schemas, API responses) rather than artificially generated abstract structures. This would validate whether benchmark performance correlates with practical utility.

**Check 3: Transfer Learning Assessment**
Train models on subsets of the benchmark tasks and evaluate transfer to novel tasks within the same structured languages. This would test whether performance reflects genuine structural understanding or task-specific memorization, addressing concerns about the benchmark's ability to measure transferable reasoning skills.