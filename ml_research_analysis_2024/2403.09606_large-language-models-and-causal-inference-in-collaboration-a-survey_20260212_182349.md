---
ver: rpa2
title: 'Large Language Models and Causal Inference in Collaboration: A Survey'
arxiv_id: '2403.09606'
source_url: https://arxiv.org/abs/2403.09606
tags:
- causal
- arxiv
- llms
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey reviews the interplay between causal inference and
  large language models (LLMs) from two perspectives: how causal inference can enhance
  LLM reasoning, fairness, safety, explainability, and multimodality, and how LLMs
  can assist in causal inference tasks like causal relationship discovery and treatment
  effect estimation. The authors organize research into three main categories: (1)
  Causal inference for LLMs, covering model understanding, commonsense reasoning,
  counterfactual reasoning, fairness and bias mitigation, safety, explainability,
  and multimodality evaluation; (2) LLMs for causal inference, focusing on treatment
  effect estimation and causal relationship discovery; and (3) Future directions.'
---

# Large Language Models and Causal Inference in Collaboration: A Survey

## Quick Facts
- arXiv ID: 2403.09606
- Source URL: https://arxiv.org/abs/2403.09606
- Reference count: 34
- One-line primary result: This survey comprehensively reviews the interplay between causal inference and large language models from two perspectives: enhancing LLM reasoning and assisting in causal inference tasks.

## Executive Summary
This survey explores the emerging intersection between causal inference and large language models (LLMs), examining how these two fields can mutually benefit each other. The authors systematically categorize research into how causal inference can enhance LLM capabilities (reasoning, fairness, safety, explainability, multimodality) and how LLMs can assist in causal inference tasks (treatment effect estimation, causal relationship discovery). The survey highlights that while LLMs excel at many tasks, their reasoning often relies on memorization rather than true causal understanding, and integrating causal frameworks can address limitations like hallucinations, bias, and lack of interpretability.

## Method Summary
This survey reviews existing research papers, benchmarks, and methodologies that explore the intersection of causal inference and LLMs. The authors organize research into three main categories: (1) Causal inference for LLMs, covering model understanding, commonsense reasoning, counterfactual reasoning, fairness and bias mitigation, safety, explainability, and multimodality evaluation; (2) LLMs for causal inference, focusing on treatment effect estimation and causal relationship discovery; and (3) Future directions. The survey synthesizes findings from multiple domains including natural language processing, computer vision, and multimodal systems to provide a comprehensive overview of current approaches and challenges.

## Key Results
- Causal inference frameworks can enhance LLM reasoning by identifying and mitigating spurious correlations, improving accuracy in cause-effect relationships
- LLMs can assist in causal inference tasks by leveraging their generation ability to create high-quality counterfactuals for treatment effect estimation
- Causal frameworks improve LLM fairness and bias mitigation by modeling causal pathways that lead to biased predictions and applying targeted interventions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal inference frameworks can enhance LLM reasoning by providing structured methods to identify and mitigate spurious correlations.
- Mechanism: Causal graphs and potential outcomes frameworks allow LLMs to differentiate between correlation and causation, enabling more accurate reasoning about cause-effect relationships.
- Core assumption: LLMs can leverage their pre-trained knowledge to assist in causal relationship discovery and treatment effect estimation when guided by causal frameworks.
- Evidence anchors:
  - [abstract] "Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables."
  - [section] "While the non-parametric SEM takes the form: Xi = fi(Xpa(i), ϵi), i = 1, 2, 3, ..., N... The random variables X that satisfies the model structure... can be represented by a directed acyclic graph (DAG) G = (V, E)"
- Break condition: If the LLM cannot effectively identify or validate causal relationships, or if the causal assumptions are violated.

### Mechanism 2
- Claim: LLMs can assist in causal inference tasks by leveraging their generation ability and pre-trained knowledge to create high-quality counterfactuals for treatment effect estimation.
- Mechanism: LLMs generate counterfactual data that would be difficult or impossible to obtain experimentally, enabling more accurate estimation of treatment effects in observational studies.
- Core assumption: LLMs possess sufficient world knowledge and reasoning capabilities to generate meaningful counterfactuals that preserve causal relationships.
- Evidence anchors:
  - [abstract] "By utilizing LLMs' strong generative abilities, researchers have developed various ways to generate high-quality counterfactuals to enable treatment effect estimation."
  - [section] "Estimating treatment effects is central to causal inference but is hindered by the absence of counterfactual data in many cases. Chen et al. (2023c) proposed a new method for automatically generating high-quality counterfactual data at scale called DISCO (DIStilled COunterfactual Data)."
- Break condition: If generated counterfactuals are of poor quality or fail to preserve causal relationships, or if the LLM's knowledge is insufficient for the domain.

### Mechanism 3
- Claim: Causal inference can improve LLM fairness and bias mitigation by identifying and eliminating unwanted spurious correlations through causal intervention.
- Mechanism: By modeling the causal pathways that lead to biased predictions, interventions can be applied to break these pathways while preserving legitimate causal relationships.
- Core assumption: The sources of bias can be accurately modeled as causal relationships within the LLM's decision-making process.
- Evidence anchors:
  - [abstract] "Causality-based methodologies offer a promising approach for mitigating biases in language models by discerning the origins of bias through a causal perspective."
  - [section] "Ding et al. (2022) introduced a proxy variable related to gender bias in the causal graph, and used two different ways to eliminate the potential proxy bias and unresolved bias under the linear structural equation model."
- Break condition: If the causal model of bias is incorrect or incomplete, or if interventions inadvertently remove legitimate causal relationships.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs) and their role in representing causal relationships
  - Why needed here: DAGs provide the structural framework for modeling causal relationships and identifying confounding variables
  - Quick check question: Can you explain why cycles are not allowed in causal DAGs and what this implies about temporal relationships?

- Concept: Potential outcomes framework and average treatment effects
  - Why needed here: This framework provides the formal definition of causal effects that LLMs need to estimate when reasoning about interventions
  - Quick check question: What is the fundamental problem of causal inference in the potential outcomes framework?

- Concept: Causal mediation analysis
  - Why needed here: Understanding how effects are transmitted through intermediate variables is crucial for interpreting LLM decisions and improving explainability
  - Quick check question: How does causal mediation analysis differ from simple regression analysis in understanding causal pathways?

## Architecture Onboarding

- Component map: Causal reasoning module -> Counterfactual generation engine -> Bias detection and intervention system -> Knowledge validation layer
- Critical path: LLM input → Causal framework analysis → Reasoning enhancement → Output generation with causal awareness
- Design tradeoffs:
  - Precision vs. computational cost: More complex causal models provide better reasoning but require more computation
  - Generality vs. domain specificity: General causal frameworks work across domains but may miss domain-specific nuances
  - Interpretability vs. performance: More interpretable causal models may sacrifice some reasoning performance
- Failure signatures:
  - Hallucinations in causal reasoning: LLM generates plausible but causally incorrect relationships
  - Over-reliance on spurious correlations: Model fails to distinguish correlation from causation
  - Bias amplification: Causal interventions inadvertently strengthen existing biases
- First 3 experiments:
  1. Implement a simple causal reasoning benchmark using the CRAB dataset to test basic causal inference capabilities
  2. Create a bias detection system using causal mediation analysis on a text classification task
  3. Develop a counterfactual generation system for a specific domain (e.g., medical diagnosis) and evaluate treatment effect estimation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs effectively replace human experts in identifying causal relationships and confounder variables for causal inference tasks?
- Basis in paper: [explicit] "LLMs, leveraging pre-trained knowledge, can assist in identifying these relationships and enhance causal discovery methods"
- Why unresolved: The paper acknowledges LLMs can assist but doesn't definitively establish if they can fully replace human expertise, especially given concerns about false information and sensitivity to prompt design.
- What evidence would resolve it: Comparative studies measuring LLM performance against human experts across diverse causal discovery tasks with varying complexity levels.

### Open Question 2
- Question: How can we systematically evaluate and improve LLMs' true causal understanding versus their ability to recall memorized causal knowledge?
- Basis in paper: [explicit] "Zečević et al. (2023) argued LLMs are not causal and hypothesized that LLMs are simply trained on the data, in which causal knowledge is embedded"
- Why unresolved: Current benchmarks may test surface-level causal reasoning rather than deep understanding, and the paper identifies this as a limitation without providing a solution.
- What evidence would resolve it: Development of benchmark tasks specifically designed to distinguish memorized causal patterns from genuine reasoning ability.

### Open Question 3
- Question: What is the optimal integration strategy between LLMs and traditional causal discovery methods to maximize reliability while minimizing computational costs?
- Basis in paper: [explicit] "Previous works have integrated LLMs with traditional causal discovery methods" and "pairwise judgments require large computational cost when applying to a large-scale dataset"
- Why unresolved: The paper surveys various integration approaches but doesn't establish which combinations provide the best trade-off between accuracy and efficiency.
- What evidence would resolve it: Empirical comparisons of different LLM-causal discovery integration architectures across multiple datasets and computational resource constraints.

## Limitations
- Limited empirical validation of proposed methods, as the survey nature does not conduct original experimental verification
- Potential bias in survey coverage toward published work rather than comprehensive literature review
- Insufficient discussion of computational complexity and scalability challenges when integrating causal inference with LLMs

## Confidence
- High confidence in the core thesis that causal inference can enhance LLM reasoning, supported by multiple theoretical frameworks and preliminary experimental results
- Medium confidence in specific implementation details and practical effectiveness, as many proposed approaches remain at conceptual or early experimental stages

## Next Checks
1. Implement and evaluate a basic causal reasoning benchmark using the CRAB dataset to test if causal frameworks improve LLM performance on simple causal inference tasks
2. Conduct a systematic comparison of bias detection methods using causal mediation analysis versus traditional statistical approaches on a standard text classification dataset
3. Develop a prototype system for counterfactual generation in a specific domain (e.g., medical diagnosis) and measure the accuracy of resulting treatment effect estimates compared to ground truth or established methods