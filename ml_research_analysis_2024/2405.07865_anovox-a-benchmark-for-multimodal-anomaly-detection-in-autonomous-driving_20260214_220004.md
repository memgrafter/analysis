---
ver: rpa2
title: 'AnoVox: A Benchmark for Multimodal Anomaly Detection in Autonomous Driving'
arxiv_id: '2405.07865'
source_url: https://arxiv.org/abs/2405.07865
tags:
- data
- anomaly
- anomalies
- detection
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AnoVox is a new large-scale benchmark for anomaly detection in
  autonomous driving, addressing limitations of existing datasets that focus primarily
  on camera data and lack temporal context. The key contributions include a formal
  definition of normality, a large synthetic training dataset, and a comprehensive
  evaluation suite supporting multimodal sensor data.
---

# AnoVox: A Benchmark for Multimodal Anomaly Detection in Autonomous Driving

## Quick Facts
- **arXiv ID**: 2405.07865
- **Source URL**: https://arxiv.org/abs/2405.07865
- **Reference count**: 40
- **Primary result**: AnoVox is a new large-scale benchmark for anomaly detection in autonomous driving that addresses limitations of existing datasets by providing multimodal sensor data with voxelized ground truth, revealing significant challenges for state-of-the-art anomaly detection methods.

## Executive Summary
AnoVox introduces a comprehensive benchmark for anomaly detection in autonomous driving that addresses critical gaps in existing datasets. Unlike previous benchmarks that focus primarily on camera data, AnoVox incorporates large-scale multimodal sensor data (RGB, depth, LiDAR) and provides spatial voxel ground truth in 2D, 3D, and voxelized forms. The benchmark includes both content anomalies (objects in critical areas) and temporal anomalies (sudden braking scenarios), enabling evaluation of methods that learn normality from training data rather than detecting deviations from known classes. State-of-the-art methods like RbA and REAL show significant performance limitations on AnoVox, highlighting the difficulty of learning normality and the need for new approaches in autonomous driving anomaly detection.

## Method Summary
AnoVox uses CARLA simulation to generate a synthetic autonomous driving dataset with controlled anomaly scenarios. The benchmark defines normality across three categories (Ego, Domain, PhysicalEntities) and creates training data compliant with this definition. Anomalies are injected into the simulation and mapped to a unified voxel grid representation derived from all sensor inputs, enabling fair comparison across different modalities. The benchmark provides ground truth in multiple formats including voxelized 3D space, semantic masks for all modalities, and Bird's-eye view representations of planned routes. This comprehensive evaluation suite allows researchers to assess anomaly detection methods on challenging scenarios including small anomalies and temporal contexts.

## Key Results
- State-of-the-art methods (RbA for camera data and REAL for LiDAR) show significant performance limitations on AnoVox
- Existing anomaly detection approaches struggle with small anomalies and temporal detection scenarios
- Current methods primarily rely on semantic segmentation deviations rather than learning normality from training data
- The benchmark reveals a critical need for new anomaly detection approaches that can learn normality rather than detecting known class deviations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AnoVox enables evaluation of anomaly detection methods across multiple sensor modalities by providing ground truth in voxelized 3D space.
- Mechanism: The dataset maps anomalies into a unified voxel grid representation derived from all available sensor inputs (RGB, depth, LiDAR). This allows anomaly detection models trained on different modalities to be compared fairly.
- Core assumption: All sensor modalities can be aligned to a common voxel grid without significant information loss.
- Evidence anchors:
  - [abstract]: "AnoVox incorporates large-scale multimodal sensor data and spatial VOXel ground truth, allowing for the comparison of methods independent of their used sensor."
  - [section]: "For approaches that require additional information about the planned route, we provide a standard-format Bird's-eye view (BEV) representation of the planned route."
  - [corpus]: Weak evidence - corpus contains related works on autonomous driving anomaly detection but no direct evidence about voxelized multi-modal ground truth.
- Break condition: Significant misalignment between sensor data and voxel grid representation leads to incorrect anomaly localization.

### Mechanism 2
- Claim: AnoVox addresses the training-data definition gap in anomaly detection by providing a formal definition of normality and compliant training data.
- Mechanism: The benchmark defines normality across three categories (Ego, Domain, PhysicalEntities) and generates training data that adheres to this definition. This allows anomaly detection methods to learn what is "normal" rather than relying on semantic class assumptions.
- Core assumption: The formal definition of normality is complete and accurately captures all scenarios considered normal for autonomous driving.
- Evidence anchors:
  - [abstract]: "We propose a formal definition of normality and provide a compliant training dataset for a fair comparison of anomaly detection methods"
  - [section]: "With AnoVox, we provide full controllability about both normality and anomalies in synthetic environments. This ensures that anomalies included in the benchmark are true anomalies and are not included in an unlabeled training dataset by chance."
  - [corpus]: Weak evidence - corpus contains related anomaly detection datasets but none explicitly discuss formal normality definitions in the same way.
- Break condition: The formal definition of normality fails to capture real-world edge cases, leading to false positive detections.

### Mechanism 3
- Claim: AnoVox reveals limitations of current anomaly detection methods by providing challenging scenarios with small anomalies and temporal contexts.
- Mechanism: The benchmark includes content anomalies distributed across critical areas and temporal anomalies like sudden braking scenarios. State-of-the-art methods like RbA and REAL struggle with these, highlighting the need for new approaches.
- Core assumption: Current anomaly detection methods primarily rely on semantic segmentation and are not well-suited for learning normality from training data alone.
- Evidence anchors:
  - [abstract]: "The results indicate that existing methods struggle with small anomalies and temporal scenarios, emphasizing the need for new approaches to anomaly detection in autonomous driving."
  - [section]: "We see a great need for anomaly detection methods to learn normality based on training data instead of detecting deviations from known Cityscape classes."
  - [corpus]: Weak evidence - corpus contains related works on anomaly detection but limited evidence about specific struggles with small anomalies and temporal contexts.
- Break condition: Anomaly detection methods successfully learn normality from training data without relying on semantic segmentation deviations.

## Foundational Learning

- Concept: Formal definition of normality in autonomous driving
  - Why needed here: Without a clear definition, anomaly detection methods cannot learn what constitutes "normal" behavior, leading to inconsistent evaluations across different benchmarks.
  - Quick check question: What are the three categories that AnoVox uses to define normality?

- Concept: Multimodal sensor fusion and voxelization
  - Why needed here: To compare anomaly detection methods across different sensor modalities, the benchmark needs to align all sensor data into a common representation.
  - Quick check question: How does AnoVox represent ground truth for anomaly detection methods using different sensor modalities?

- Concept: Temporal anomaly detection in autonomous driving
  - Why needed here: Real-world autonomous driving involves dynamic scenarios where anomalies may develop over time, requiring detection methods to consider temporal context.
  - Quick check question: What type of temporal anomaly does AnoVox specifically include in its benchmark?

## Architecture Onboarding

- Component map: CARLA simulation engine -> Scenario generation -> Multimodal sensor data collection -> Ground truth processing -> Voxelization of anomalies -> Semantic mask generation -> Benchmark suite -> Evaluation framework -> Voxel-based comparison -> Performance metrics
- Critical path: Data generation -> Ground truth voxelization -> Evaluation framework -> Method comparison
- Design tradeoffs:
  - Using simulation allows full control over normality definition but introduces Sim-2-Real gap
  - Voxel-based ground truth enables multi-modal comparison but may lose fine-grained information
  - Focusing on content and temporal anomalies provides comprehensive evaluation but increases dataset complexity
- Failure signatures:
  - Low anomaly detection performance due to class imbalance in voxel space
  - Misalignment between sensor data and voxel ground truth
  - Overfitting to simulation-specific artifacts rather than general anomaly patterns
- First 3 experiments:
  1. Evaluate RbA and REAL methods on a small subset of AnoVox data to verify implementation correctness
  2. Test voxelization process with different grid sizes to optimize the balance between resolution and computational efficiency
  3. Generate a small normality instantiation and verify that it complies with the formal definition of normality

## Open Questions the Paper Calls Out
None

## Limitations

- Data Realism Gap: The benchmark relies on synthetic data generated through CARLA simulation, which may not fully capture the complexity and noise characteristics of real-world sensor data.
- Ground Truth Precision: The voxel-based ground truth process may result in information loss, particularly for small or fine-grained anomalies.
- Normality Definition Completeness: The formal definition of normality may not capture all edge cases present in real-world autonomous driving scenarios.

## Confidence

**High Confidence Claims**:
- The benchmark successfully provides multimodal sensor data with voxelized ground truth for anomaly detection evaluation
- Current state-of-the-art methods (RbA and REAL) demonstrate significant performance limitations on AnoVox
- The benchmark reveals specific challenges with small anomalies and temporal detection scenarios

**Medium Confidence Claims**:
- The formal definition of normality comprehensively covers autonomous driving scenarios
- Voxel-based ground truth enables fair comparison across different sensor modalities
- The training data generation process successfully creates normality-compliant scenarios

**Low Confidence Claims**:
- The benchmark will generalize to real-world autonomous driving scenarios without modification
- Current performance limitations indicate fundamental flaws in existing anomaly detection approaches rather than benchmark-specific challenges
- The voxel grid resolution provides optimal balance between detail preservation and computational efficiency

## Next Checks

1. **Real-World Validation**: Test a subset of AnoVox-trained anomaly detection methods on real-world autonomous driving datasets to quantify the Sim-2-Real gap and identify specific simulation artifacts that may bias performance.

2. **Ground Truth Fidelity Analysis**: Conduct a systematic evaluation of how voxelization resolution affects anomaly detection performance, particularly for small anomalies, and determine the minimum resolution required to preserve critical information.

3. **Normality Definition Stress Testing**: Generate edge case scenarios that deliberately challenge the formal normality definition and evaluate whether the current definition correctly classifies these as anomalies or false positives.