---
ver: rpa2
title: 'Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning'
arxiv_id: '2412.13631'
source_url: https://arxiv.org/abs/2412.13631
tags:
- theory
- mind
- computational
- question
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper argues that existing Theory of Mind (ToM) benchmarks
  for LLMs focus narrowly on logical reasoning tasks while neglecting the crucial
  first step of determining whether and how to invoke ToM reasoning. The authors propose
  viewing ToM as a two-step process: (1) determining the appropriate Depth of Mentalizing
  (DoM), and (2) applying correct inference given that DoM.'
---

# Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning

## Quick Facts
- arXiv ID: 2412.13631
- Source URL: https://arxiv.org/abs/2412.13631
- Authors: Eitan Wagner; Nitay Alon; Joseph M. Barnby; Omri Abend
- Reference count: 36
- Primary result: Models perform well at predicting required ToM order even when failing full tasks, indicating the main challenge is reasoning rather than ToM invocation

## Executive Summary
This paper argues that existing Theory of Mind (ToM) benchmarks for large language models focus too narrowly on logical reasoning tasks while neglecting the crucial first step of determining whether and how to invoke ToM reasoning. The authors propose viewing ToM as a two-step process: (1) determining the appropriate Depth of Mentalizing (DoM), and (2) applying correct inference given that DoM. Through experiments on two challenging ToM benchmarks (HiToM and FANToM), they demonstrate that models can often predict the required ToM order even when failing the full task, suggesting the main bottleneck is reasoning complexity rather than ToM invocation capability. The paper calls for improved ToM evaluation through interactive, dynamic environments that test both steps of ToM reasoning.

## Method Summary
The authors experiment with two challenging ToM benchmarks (HiToM and FANToM) by adding a task of predicting the required ToM order for each question. They use GPT-3.5-turbo, GPT-4o-mini, and GPT-4o models with different prompt settings to test whether models can predict the ToM level of questions when given (or not given) the story context. The DoM order is recorded as metadata in the datasets, allowing direct comparison between predicted and actual ToM requirements. They compare performance across different model types and prompt configurations to identify where the main challenges lie.

## Key Results
- GPT-4o-mini achieves good results and GPT-4o is nearly perfect at predicting ToM order, while GPT-3.5-turbo struggles
- Chain-of-thought prompting provides some improvement on the inference task
- Models consistently perform better at predicting required ToM order than at answering belief questions
- The performance gap suggests the main challenge is reasoning complexity rather than ToM invocation capability

## Why This Works (Mechanism)

### Mechanism 1
Current ToM benchmarks conflate logical reasoning difficulty with ToM invocation difficulty by assuming the DoM is obvious from question structure, forcing models to focus on inference step (II) while bypassing the meta-decision step (I). If models can accurately predict ToM order but fail at belief questions, the bottleneck is reasoning rather than ToM invocation.

### Mechanism 2
Static benchmarks cannot capture the dynamic nature of ToM in real-world interactions where agents adjust their mentalizing depth moment-to-moment based on feedback. Real-world ToM requires this adaptability, which static vignettes cannot simulate.

### Mechanism 3
LLMs have computational capacity for ToM invocation but struggle with complex logical inference. Models can identify when and how deeply to mentalize, but the actual inference process (tracking multiple nested beliefs) exceeds their reasoning capabilities.

## Foundational Learning

- **Concept: Theory of Mind (ToM)** - Why needed: Understanding what ToM is and why it matters for social reasoning is fundamental to evaluating these benchmarks. Quick check: Can you explain the difference between first-order and second-order ToM using a simple example?

- **Concept: Depth of Mentalizing (DoM)** - Why needed: The paper's core argument revolves around distinguishing between DoM determination and inference given DoM. Quick check: How would you determine whether a question requires 0th, 1st, or 2nd order ToM reasoning?

- **Concept: Inverse Reinforcement Learning (IRL)** - Why needed: The paper references IRL as a framework for modeling ToM inference in formal architectures. Quick check: What is the relationship between IRL and ToM inference in multi-agent settings?

## Architecture Onboarding

- **Component map**: Benchmark selection -> DoM prediction task -> belief inference task -> model comparison across tasks
- **Critical path**: For each benchmark, first predict DoM order → then attempt belief inference → compare performance across both tasks to identify bottleneck
- **Design tradeoffs**: Static benchmarks are easier to implement but miss dynamic aspects; interactive benchmarks are more realistic but harder to standardize
- **Failure signatures**: Consistent failure to predict DoM order indicates fundamental misunderstanding of ToM requirements; success at prediction but failure at inference indicates reasoning limitations
- **First 3 experiments**:
  1. Take HiToM or FANToM benchmark, extract questions with known DoM labels, test model's ability to predict DoM order from questions alone
  2. Test same models on belief inference task from the same benchmark, compare accuracy rates
  3. Implement chain-of-thought prompting and measure improvement specifically on inference task (not DoM prediction)

## Open Questions the Paper Calls Out

### Open Question 1
How can we develop benchmarks that accurately measure both the invocation and application of Theory of Mind (ToM) in large language models (LLMs)? The paper argues that existing benchmarks focus narrowly on logical reasoning tasks while neglecting the crucial first step of determining whether and how to invoke ToM reasoning. This remains unresolved as the paper identifies this as a key limitation but does not provide specific methodologies for creating such benchmarks.

### Open Question 2
What are the computational trade-offs between shallow and hierarchical levels of mentalizing in LLMs, and how do these compare to biological agents? The paper mentions that moving from shallow to hierarchical levels of mentalizing is more expensive in biological agents, but LLMs do not have the same energy constraints. This question is unresolved as the paper discusses the theoretical importance of this distinction but does not provide empirical evidence on computational costs in LLMs.

### Open Question 3
How can we create interactive, dynamic environments for ToM evaluation that better reflect real-world social interaction complexity? The paper concludes that a better understanding of ToM in LLMs requires evaluation in interactive settings where the model is an active agent. This remains unresolved as while the paper calls for such environments, it does not provide concrete designs or implementation strategies.

## Limitations

- The paper's evidence assumes DoM prediction is truly independent of inference process, but models might use inference capabilities to predict order
- Analysis is limited to only two specific benchmarks (HiToM and FANToM), which may not generalize to broader ToM evaluation landscape
- The proposed solution of interactive, dynamic environments is acknowledged as technically challenging and not yet demonstrated

## Confidence

**High confidence**: The observation that existing ToM benchmarks conflate reasoning difficulty with ToM invocation difficulty is well-supported by experimental results showing models can predict ToM order even when failing full tasks.

**Medium confidence**: The claim that DoM determination is computationally simpler than nested belief tracking is plausible but not definitively proven - experiments show correlation but don't establish causation or computational complexity differences.

**Medium confidence**: The assertion that static benchmarks fail to capture real-world ToM dynamics is theoretically sound but empirically limited - the paper argues this point but doesn't provide concrete evidence from interactive evaluations.

## Next Checks

1. **Task independence validation**: Design experiments to test whether models can predict DoM order for questions they cannot solve, and conversely whether models that fail at DoM prediction also fail at belief inference. This would confirm whether the tasks are truly separable.

2. **Cross-benchmark generalization**: Apply the DoM prediction task to a diverse set of ToM benchmarks (not just HiToM and FANToM) to test whether the pattern of success at prediction but failure at inference holds across different benchmark designs and difficulty levels.

3. **Interactive prototype implementation**: Create a minimal interactive ToM evaluation environment (e.g., simple turn-based dialogue with feedback) and compare model performance to static benchmark results. This would provide preliminary evidence for or against the claim that dynamic environments reveal different capabilities than static ones.