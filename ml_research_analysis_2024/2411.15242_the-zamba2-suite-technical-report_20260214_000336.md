---
ver: rpa2
title: 'The Zamba2 Suite: Technical Report'
arxiv_id: '2411.15242'
source_url: https://arxiv.org/abs/2411.15242
tags:
- arxiv
- performance
- zamba2
- https
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Zamba2 series is a suite of 1.2B, 2.7B, and 7.4B parameter
  hybrid Mamba2-transformer language models that achieve state-of-the-art performance
  among open-weight models of their class. These models were developed by optimizing
  the architecture (switching to Mamba2, using two alternating shared attention blocks,
  applying LoRAs and rotary position embeddings), training on the high-quality Zyda-2
  dataset (5 trillion tokens, cross-deduped and filtered), and performing annealing
  on a mix of web and higher-quality data.
---

# The Zamba2 Suite: Technical Report

## Quick Facts
- arXiv ID: 2411.15242
- Source URL: https://arxiv.org/abs/2411.15242
- Reference count: 20
- Primary result: 1.2B, 2.7B, and 7.4B parameter hybrid Mamba2-transformer models achieving state-of-the-art performance among open-weight models

## Executive Summary
The Zamba2 suite presents a family of hybrid Mamba2-transformer language models ranging from 1.2B to 7.4B parameters that achieve state-of-the-art performance among open-weight models. Built on the Mamba2 architecture with two alternating shared attention blocks, LoRAs, and rotary position embeddings, these models deliver substantial gains in inference efficiency while maintaining competitive quality. Trained on the Zyda-2 dataset containing 5 trillion tokens, Zamba2 models achieve significant improvements in inference latency (30-50% time-to-first-token reduction), throughput (4× improvement), and memory efficiency (6× reduction in KV cache memory). The models demonstrate strong performance across benchmarks, with the 1.2B, 2.7B, and 7.4B variants achieving MMLU scores of 43.1, 55.97, and 67.2 respectively.

## Method Summary
The Zamba2 models employ a hybrid architecture combining Mamba2 state-space layers with two alternating shared attention blocks, trained using a two-phase approach. First, models are pretrained on the Zyda-2 dataset (5 trillion tokens) using Adam optimization and cosine learning rate schedules. This is followed by annealing on high-quality factual, mathematical, code, and instruction datasets. The architecture incorporates non-shared LoRAs on shared attention blocks and rotary position embeddings to enhance expressivity and positional awareness. Models are trained with context lengths of 4096 tokens, extendable to ~17k with NTK-aware rotary scaling on the 7B variant.

## Key Results
- 1.2B, 2.7B, and 7.4B parameter models achieve MMLU scores of 43.1, 55.97, and 67.2 respectively
- 30-50% reduction in inference time-to-first-token compared to transformer baselines
- 4× improvement in throughput while maintaining comparable quality
- 6× reduction in KV cache memory requirements
- Outperform comparable models like Gemma2 and Llama3.2 on standard benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Mamba2 backbone delivers 4× throughput vs standard transformers while maintaining comparable quality.
- Mechanism: Mamba2 replaces quadratic-attention with a selective state-space formulation that has linear compute cost in sequence length and fixed-size state cache.
- Core assumption: The fixed-size state cache does not degrade quality on long sequences beyond the cache size.
- Evidence anchors: [abstract] "achieve substantial gains in inference latency, throughput, and memory efficiency"; [section II] "Mamba2 has significantly higher throughput than an equivalently sized Mamba1, with approximately the same performance"
- Break condition: If sequence lengths routinely exceed the effective state cache size, performance and quality degrade sharply.

### Mechanism 2
- Claim: Two alternating shared attention blocks improve performance per parameter while reducing total FLOPs vs a single shared block.
- Mechanism: Alternating shared attention layers increase representational capacity per parameter by distributing attention computation across multiple layers without fully duplicating parameters.
- Core assumption: The marginal benefit of additional attention capacity outweighs the cost of extra FLOPs.
- Evidence anchors: [section II] "We use two alternating shared attention blocks instead of a single shared block... This improves performance against parameter-matched baselines, with the additional benefit of using fewer FLOPs in inference"
- Break condition: When model scale is small, the benefit of two shared blocks diminishes, as fewer total attention layers exist.

### Mechanism 3
- Claim: The combination of LoRAs on shared blocks and rotary position embeddings provides additional expressivity and positional information beyond causal masking and SSM cache.
- Mechanism: LoRAs inject low-rank modifications into attention/MLP weights, allowing each shared block to specialize while rotary embeddings encode relative position directly into attention queries/keys.
- Core assumption: Adding LoRAs and rotary embeddings does not cause optimization instability or catastrophic forgetting.
- Evidence anchors: [section II] "We apply non-shared Low-Rank Adapters (LoRAs)... to the shared transformer blocks... offers each shared attention block additional expressivity" and "We apply Rotary Position Embeddings... improved performance, perhaps by providing an additional source of position information"
- Break condition: If LoRA rank is too high or rotary scaling is incorrect, training instability or quality degradation occurs.

## Foundational Learning

- Concept: Linear attention vs quadratic attention
  - Why needed here: To understand why Mamba2 reduces inference memory from O(n) to O(1) and compute from O(n²) to O(n).
  - Quick check question: In a 4096-token sequence, how many KV pairs does a pure transformer store vs a Mamba2 model with a 1:6 Mamba2-to-attention ratio?

- Concept: State-space models (SSMs) and recurrence
  - Why needed here: To grasp how Mamba2's recurrent formulation enables efficient inference while retaining parallelizability for training.
  - Quick check question: What is the difference between conventional RNNs/LSTMs and SSMs in terms of GPU parallelism?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: To understand how non-shared LoRAs provide per-block expressivity without full parameter duplication.
  - Quick check question: If a LoRA rank is set to 8 on a 4096×4096 attention weight matrix, how many additional parameters are introduced per shared block?

## Architecture Onboarding

- Component map: Input embedding → Mamba2 block → Shared attention block (with LoRA, rotary if applicable) → MLP block → Repeat alternating pattern → Output
- Critical path: 1. Input embedding → Mamba2 block (state update); 2. Shared attention block (with LoRA, rotary if applicable); 3. MLP block; 4. Repeat alternating pattern until output; 5. Rotary scaling applied only in shared attention layers
- Design tradeoffs:
  - Mamba2 vs pure transformer: Lower inference memory but requires careful state cache sizing
  - Two shared blocks vs one: Higher quality per parameter but slightly more complex training dynamics
  - LoRAs vs full parameters: Reduced memory and compute for fine-tuning at cost of potential expressivity limits
  - Rotary embeddings vs learned absolute embeddings: Lower memory, better generalization to longer contexts
- Failure signatures:
  - Training instability: Often due to incorrect SSM parameters or aggressive LoRA rank
  - Quality drop on long sequences: Cache size too small relative to context
  - Slow convergence: Inadequate rotary scaling or missing LoRAs where beneficial
  - Inference errors: Mismatch between model dimensions and GPU kernel expectations
- First 3 experiments:
  1. Ablation: Remove rotary embeddings from 2.7B model, measure MMLU drop and inference latency change.
  2. Parameter sweep: Vary LoRA rank on shared attention blocks, measure quality vs parameter count trade-off.
  3. Cache scaling test: Extend context from 4096 to 8192 tokens on 7B model with NTK-aware rotary, measure passkey retrieval accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Zamba2 architecture continue to outperform transformer baselines at scales larger than 7.4B parameters?
- Basis in paper: [explicit] The paper speculates that the Zamba2 architecture is still far from optimum and that there is significant room for optimization of the marginal loss per parameter, particularly at larger scales.
- Why unresolved: The paper only evaluates models up to 7.4B parameters. Scaling to larger models requires substantial computational resources and has not yet been attempted.
- What evidence would resolve it: Training and evaluating Zamba2 models at 10B+ parameters and comparing their performance to transformer models of equivalent scale would provide a definitive answer.

### Open Question 2
- Question: What is the optimal ratio of Mamba2 to attention layers for maximizing performance across different model scales?
- Basis in paper: [explicit] The paper uses a 1:6 ratio of Mamba2 to attention layers in Zamba2, but notes that this was chosen based on architectural search experiments and may not be optimal for all scales.
- Why unresolved: The paper does not provide a systematic study of how the Mamba2:attention ratio affects performance across different parameter sizes. The optimal ratio may vary depending on model scale.
- What evidence would resolve it: Conducting a comprehensive ablation study varying the Mamba2:attention ratio for models of different sizes (e.g., 1B, 3B, 7B, 13B) and measuring the impact on performance metrics would identify the optimal ratio for each scale.

### Open Question 3
- Question: How does the quality of the Zyda-2 dataset compare to proprietary datasets used by leading models when controlling for size and deduplication?
- Basis in paper: [explicit] The paper claims Zyda-2 outperforms previous state-of-the-art datasets in annealing ablation tests, but acknowledges that the datasets of almost all comparable models are closed.
- Why unresolved: Without access to competitor datasets, it's impossible to make a direct, controlled comparison of dataset quality. The observed performance advantage could be due to factors other than dataset quality.
- What evidence would resolve it: Obtaining or simulating proprietary datasets and conducting controlled experiments training models of the same architecture on different datasets while holding other variables constant would allow for a fair comparison of dataset quality.

## Limitations

- Limited empirical comparisons: The technical report lacks direct benchmarking against transformer baselines for throughput and latency claims, relying instead on references to the original Mamba2 paper.
- Sparse training details: The report provides only high-level descriptions of the two-phase training process without specific hyperparameters, implementation notes, or optimization techniques that would enable faithful reproduction.
- Architectural assumptions: Claims about the benefits of two alternating shared attention blocks and the specific impact of LoRAs and rotary embeddings lack direct empirical validation within the Zamba2 context.

## Confidence

**High Confidence:**
- Benchmark performance claims (MMLU scores of 43.1, 55.97, and 67.2 for 1.2B, 2.7B, and 7.4B models respectively)
- Memory efficiency improvements (6x reduction in KV cache memory)
- Dataset composition and scale (Zyda-2 with 5 trillion tokens)

**Medium Confidence:**
- Inference latency improvements (30-50% time-to-first-token reduction)
- Throughput improvements (4× throughput vs standard transformers)
- The general effectiveness of the hybrid Mamba2-transformer architecture

**Low Confidence:**
- Specific architectural benefits of two alternating shared attention blocks
- The precise impact of LoRAs and rotary position embeddings on performance
- Training stability and convergence characteristics without implementation details

## Next Checks

1. **Throughput Benchmarking:** Conduct head-to-head inference benchmarks comparing Zamba2 models against equivalent transformer models (e.g., Gemma2, Llama3.2) on identical hardware, measuring time-to-first-token and tokens-per-second across various sequence lengths to validate the claimed 4× throughput advantage.

2. **Ablation Studies:** Perform systematic ablation experiments on the 2.7B model by removing rotary position embeddings and LoRAs individually, measuring the impact on MMLU performance and inference latency to quantify the contribution of each architectural component.

3. **State Cache Scaling Analysis:** Test the 7.4B model's performance on extended context lengths (4096 → 8192 → 16384 tokens) while monitoring quality degradation and computational efficiency, validating the claim that the Mamba2 state cache maintains performance without quadratic memory growth.