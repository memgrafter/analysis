---
ver: rpa2
title: 'ALICE: Combining Feature Selection and Inter-Rater Agreeability for Machine
  Learning Insights'
arxiv_id: '2404.09053'
source_url: https://arxiv.org/abs/2404.09053
tags:
- contract
- tenuremonths
- dependents
- techsupport
- cltv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ALICE, a Python library that combines feature
  selection with inter-rater agreeability to gain insights into black box ML models.
  The framework iteratively eliminates features while measuring agreement between
  two models' predictions at each step.
---

# ALICE: Combining Feature Selection and Inter-Rater Agreeability for Machine Learning Insights

## Quick Facts
- arXiv ID: 2404.09053
- Source URL: https://arxiv.org/abs/2404.09053
- Reference count: 40
- Key outcome: ALICE reveals model agreement patterns invisible to traditional feature selection, showing MLP and Logistic Regression achieve high agreement (κ up to 0.879) on sufficient features

## Executive Summary
ALICE is a Python library that combines backward feature elimination with inter-rater agreeability measurement to provide insights into black box ML models. The framework iteratively removes features while measuring agreement between two models' predictions using Cohen's kappa at each step. Experiments on customer churn prediction demonstrate that model pairs like MLP vs. Logistic Regression maintain high agreement when sufficient features are included, while other combinations show lower agreement. This approach helps users understand trade-offs between predictive performance and model interpretability by identifying feature subsets that yield both high accuracy and model agreement.

## Method Summary
ALICE implements backward feature elimination where the least useful feature is removed at each iteration based on model performance. For each feature subset, the framework generates predictions from two ML models and calculates their agreement using Cohen's kappa statistic. The process continues until a predefined number of features remain or performance drops below a threshold. The library supports Logistic Regression, Random Forest, and MLP models from scikit-learn and Keras, using the Telco Customer Churn dataset with 7,032 samples and 22 predictors after preprocessing, with train-test split of 0.8-0.2 and SMOTE applied for class balancing.

## Key Results
- MLP and Logistic Regression models achieve near-perfect agreement (κ ≥ 0.8) when 9 or more features are included
- Logistic Regression shows consistent feature elimination patterns across different model pair experiments
- Feature subsets with high inter-model agreement suggest simpler models may suffice for those features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ALICE reveals model agreement patterns that are invisible to traditional feature selection alone.
- Mechanism: By combining backward elimination with pairwise agreeability scoring (Cohen's kappa), ALICE exposes when two models make similar predictions on the same feature subsets, even if their absolute performance differs.
- Core assumption: Feature subsets that produce high inter-model agreement indicate shared underlying decision boundaries or complementary information usage.
- Evidence anchors:
  - [abstract]: "The framework helps users understand trade-offs between predictive performance and model interpretability by showing which feature subsets lead to best predictions and highest model agreement."
  - [section 3.1]: "The general intuition and operations given under the framework are best demonstrated in Figure 1, which encapsulates the entire procedure of combining feature selection and inter-rater agreeability."
  - [corpus]: Weak—corpus papers focus on feature selection libraries but do not combine with inter-rater agreeability.
- Break condition: If model predictions are uncorrelated regardless of feature subset, κ values will be near zero and provide no insight into model behavior.

### Mechanism 2
- Claim: Logistic Regression shows higher robustness across feature subsets compared to MLP or Random Forest in this framework.
- Mechanism: LR's stable feature elimination order and consistent F1 scores across experiments indicate less sensitivity to input feature perturbations.
- Core assumption: Linear models have more stable decision boundaries when features are removed sequentially compared to non-linear models.
- Evidence anchors:
  - [section 5]: "the Logistic Regression is clearly displayed when comparing its feature elimination steps and achieved F1 scores in different experiments: MLP vs. Logit...and RFC vs. Logit...the features dropped by the Logistic Regression are the same iteration-for-iteration in the two experiments."
  - [section 5]: "Unlike the Random Forest Classifier and the Multi-Layer Perceptron, the features dropped by the Logistic Regression are the same iteration-for-iteration in the two experiments that included it."
  - [corpus]: Weak—corpus does not address model robustness across feature subsets in combined feature selection and agreeability frameworks.
- Break condition: If LR parameters are poorly regularized or data is highly non-linear, LR could show similar instability to non-linear models.

### Mechanism 3
- Claim: High agreement between MLP and Logistic Regression indicates complementary strengths that can guide model selection.
- Mechanism: When MLP and LR achieve high κ (≥0.8) on the same feature subsets, it suggests that the non-linear capacity of MLP is not essential for those features, making LR a more interpretable choice.
- Core assumption: High inter-model agreement on specific feature subsets implies that simpler models can match complex models' performance on those features.
- Evidence anchors:
  - [section 5]: "the finding that the neural network and the Logistic Regression have high agreement may not come as a surprise given the fact that the MLP does have a sigmoid head and is optimized with a gradient-based method similarly to the Logit."
  - [section 5]: "MLP and Logistic Regression, compared to the other combinations maintain almost perfect agreement given a sufficient amount of features...between the two models, the agreeability only takes a fall when less than 9 features are included in each model."
  - [corpus]: Weak—corpus does not explore high agreement between linear and non-linear models as a selection criterion.
- Break condition: If MLP's hidden layers capture non-linear interactions that LR cannot, agreement may drop sharply as critical features are removed.

## Foundational Learning

- Concept: Cohen's Kappa statistic
  - Why needed here: ALICE uses κ to measure agreement between model predictions at each feature elimination step, requiring understanding of its range and interpretation.
  - Quick check question: What does a Cohen's kappa value of 0.879 indicate about model agreement?
- Concept: Backward feature elimination
  - Why needed here: ALICE's core algorithm removes the least useful feature at each iteration based on model performance, requiring understanding of sequential elimination logic.
  - Quick check question: If a feature is removed and model performance improves, what does this indicate about that feature's contribution?
- Concept: McNemar's test for paired predictions
  - Why needed here: ALICE includes statistical testing of whether top-n model predictions differ significantly, requiring understanding of hypothesis testing for classification outputs.
  - Quick check question: When would McNemar's test show that two models' predictions are not statistically different?

## Architecture Onboarding

- Component map:
  - BackEliminator class -> agreeability module -> metrics module -> testing module -> utils module -> search_and_compare module
- Critical path: compare_models → iterative feature elimination → prediction generation → agreeability calculation → result storage
- Design tradeoffs:
  - Simplicity vs. flexibility: Framework uses standard scikit-learn syntax but requires KerasSequential wrapper for neural networks
  - Computational cost: Backward elimination is O(n²) but provides detailed agreeability insights
  - Interpretability vs. performance: High agreement suggests simpler models may suffice, but could miss non-linear patterns
- Failure signatures:
  - All κ values near zero: Models are making uncorrelated predictions regardless of features
  - Feature elimination stalls: All remaining features contribute equally to performance
  - Statistical tests always significant: Models' top predictions are consistently different
- First 3 experiments:
  1. Compare Logistic Regression vs. Random Forest on Telco dataset with default parameters
  2. Test MLP vs. Logistic Regression with early stopping enabled on the same dataset
  3. Run compare_n_best with n=3 on experiment 1 results to analyze prediction stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the ALICE framework perform with different feature selection methods like forward selection or the Plus-l-Take-Away-r algorithm instead of backward elimination?
- Basis in paper: [explicit] The paper mentions these methods as possible future extensions in Section 6
- Why unresolved: The current framework only implements backward elimination, limiting comparison with other search strategies
- What evidence would resolve it: Implementing these methods in ALICE and comparing their performance and agreeability metrics across the same experimental datasets

### Open Question 2
- Question: Would the framework's results change significantly if benchmark datasets from medicine and genetics with synthetic nonsensical features were used?
- Basis in paper: [explicit] The paper suggests this as a future direction in Section 6
- Why unresolved: Current experiments use only the Telco Customer Churn dataset
- What evidence would resolve it: Running ALICE on these specialized benchmark datasets and analyzing model robustness in feature elimination

### Open Question 3
- Question: How would ALICE's performance compare if it supported other machine learning libraries like XGBoost or CatBoost?
- Basis in paper: [explicit] The paper mentions this as a potential future extension in Section 6
- Why unresolved: Current implementation only supports scikit-learn and Keras models
- What evidence would resolve it: Adapting ALICE to support these libraries and comparing inter-rater agreeability and feature selection results with current models

## Limitations

- Framework effectiveness depends heavily on having models that can meaningfully agree on feature subsets
- Study uses only one dataset (Telco Customer Churn), limiting generalizability to other domains
- Corpus lacks comparable frameworks combining feature selection with inter-rater agreeability for benchmarking

## Confidence

- Mechanism 1 (revealing agreement patterns): Medium - The theoretical foundation is sound, but empirical validation is limited to a single dataset and three model types.
- Mechanism 2 (LR robustness): Medium-High - Well-demonstrated in experiments with consistent feature elimination patterns across different model pairs.
- Mechanism 3 (complementary strengths via agreement): Low-Medium - The connection between high agreement and model selection guidance is implied but not rigorously validated.

## Next Checks

1. Test ALICE on multiple datasets with different characteristics (e.g., image classification, text classification) to assess generalizability of the agreeability insights.
2. Compare ALICE's feature selection performance against established methods like recursive feature elimination with cross-validation to quantify the added value of agreeability measurement.
3. Implement ablation studies removing the agreeability component to measure how much insight is gained specifically from the inter-model agreement analysis versus standard feature selection alone.