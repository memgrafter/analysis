---
ver: rpa2
title: On Active Privacy Auditing in Supervised Fine-tuning for White-Box Language
  Models
arxiv_id: '2411.07070'
source_url: https://arxiv.org/abs/2411.07070
tags:
- privacy
- fine-tuning
- data
- training
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PARSING, an active privacy auditing framework
  for supervised fine-tuning of language models. The method uses enhanced white-box
  membership inference attacks with a two-stage pipeline to quantify privacy leakage
  risks during the fine-tuning process.
---

# On Active Privacy Auditing in Supervised Fine-tuning for White-Box Language Models

## Quick Facts
- arXiv ID: 2411.07070
- Source URL: https://arxiv.org/abs/2411.07070
- Reference count: 40
- Key outcome: Active privacy auditing framework for supervised fine-tuning shows significant privacy leakage risks with balanced accuracy up to 0.77

## Executive Summary
This paper introduces PARSING, an active privacy auditing framework that uses enhanced white-box membership inference attacks to quantify privacy leakage during supervised fine-tuning of language models. The framework employs a two-stage pipeline with novel learning objectives to monitor privacy risks in real-time during the fine-tuning process. Experiments across multiple model architectures (GPT-2, GPT-Neo, Llama2) and datasets demonstrate that fine-tuning can introduce substantial privacy vulnerabilities, with attack performance varying based on task complexity, text length, model size, and batch size.

## Method Summary
PARSING implements a two-stage white-box membership inference attack framework that actively monitors privacy during supervised fine-tuning. The method extracts forward (intermediate outputs) and backward (gradient) properties during training, embeds these using separate generators with learnable parameters, and combines them with weighted information to create rich representations. A membership inference classifier is trained on these embeddings using contrastive learning objectives that optimize distance between member/non-member representations while concentrating member samples in representation space.

## Key Results
- Fine-tuning introduces significant privacy vulnerabilities with balanced accuracy up to 0.77 and TPR at 0.1 FPR up to 0.41
- Privacy risks vary with task complexity, text length, model size, and batch size
- Parameter-efficient fine-tuning methods like LoRA can reduce privacy risks
- Privacy leakage peaks at specific training epochs before declining

## Why This Works (Mechanism)

### Mechanism 1
- Forward and backward property embeddings enable superior membership inference by capturing intermediate model states during fine-tuning
- Core assumption: Intermediate model computations and gradients contain distinguishable patterns between member and non-member samples
- Evidence anchors: Novel learning objectives and two-stage pipeline to monitor privacy of LMs' fine-tuning process
- Break condition: If fine-tuning doesn't create distinguishable patterns in intermediate computations or gradients

### Mechanism 2
- Two-stage pipeline optimizes membership representation through contrastive learning objectives
- Core assumption: Member samples can be clustered in representation space while maintaining distance from non-member samples
- Evidence anchors: Utilizes novel learning objectives and a two-stage pipeline to monitor the privacy of LMs' fine-tuning process
- Break condition: If distance between member and non-member representations cannot be effectively optimized

### Mechanism 3
- Active privacy auditing during fine-tuning provides more accurate risk quantification than post-training static audits
- Core assumption: Privacy risks evolve dynamically during fine-tuning and require continuous monitoring
- Evidence anchors: Novel active privacy auditing framework designed to identify and quantify privacy leakage risks during SFT
- Break condition: If privacy risks are static throughout fine-tuning or computational overhead negates benefits

## Foundational Learning

- Concept: Membership Inference Attacks (MIAs)
  - Why needed here: PARSING uses MIAs as core technology for privacy auditing
  - Quick check question: What distinguishes white-box MIAs from black-box MIAs in terms of attacker knowledge and capabilities?

- Concept: Parameter-efficient Fine-tuning (PEFT)
  - Why needed here: Paper extends framework to PEFT methods, showing they reduce privacy risks
  - Quick check question: How do LoRA and other PEFT methods differ from full fine-tuning in terms of which model parameters are updated?

- Concept: Differential Privacy
  - Why needed here: Referenced in discussion of privacy mitigation strategies and theoretical foundations
  - Quick check question: What is the fundamental trade-off between privacy guarantees and model utility in differential privacy?

## Architecture Onboarding

- Component map: Data Partitioning Module -> Property Extraction Layer -> Property Embedding Generator -> Membership Inference Classifier -> Loss Optimization Engine
- Critical path: 1. Data partitioning and labeling 2. Property extraction during fine-tuning 3. Property embedding generation 4. Membership inference classification 5. Loss optimization and model update
- Design tradeoffs: Active vs. passive auditing, forward vs. backward properties, property granularity
- Failure signatures: Poor attack accuracy suggests property extraction/embedding issues; inconsistent performance between properties indicates embedding generator problems
- First 3 experiments: 1. Run PARSING on GPT-2-medium with PubMed_RCT dataset 2. Compare forward-only vs. backward-only property extraction 3. Vary batch sizes during fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- How does privacy leakage risk differ between different parameter-efficient fine-tuning (PEFT) methods beyond LoRA?
- Basis in paper: [explicit] Mentions LoRA can reduce privacy risks but doesn't extensively compare other PEFT methods
- Why unresolved: Paper only briefly mentions LoRA and provides limited experimental results on other PEFT methods
- What evidence would resolve it: Comprehensive comparison of different PEFT methods on various models and tasks

### Open Question 2
- What is the relationship between specific language model architecture and vulnerability to membership inference attacks?
- Basis in paper: [inferred] Mentions larger models pose higher risks and number of layers affects privacy leakage
- Why unresolved: Focuses on general trends across model sizes without detailed analysis of architectural factors
- What evidence would resolve it: Detailed study comparing privacy risks of different language model architectures

### Open Question 3
- How does choice of data augmentation techniques impact effectiveness of membership inference attacks?
- Basis in paper: [explicit] Mentions data augmentation as potential privacy mitigation technique
- Why unresolved: Acknowledges potential of data augmentation but doesn't investigate how different augmentation strategies affect attack ability
- What evidence would resolve it: Experiments comparing effectiveness of various data augmentation techniques in reducing privacy risks

### Open Question 4
- What is the optimal trade-off point between model utility and privacy risk during fine-tuning?
- Basis in paper: [explicit] Mentions trade-off between privacy and utility but doesn't provide method for determining optimal balance
- Why unresolved: Highlights trade-off but doesn't offer concrete approach for finding best balance
- What evidence would resolve it: Framework for identifying optimal fine-tuning parameters that maximize utility while minimizing privacy risks

## Limitations

- Framework assumes white-box access to models, limiting real-world applicability where attackers have only black-box access
- Evaluation focuses primarily on GPT-2 and GPT-Neo architectures with limited testing on Llama2 models
- Computational overhead of active privacy auditing during fine-tuning is not thoroughly quantified
- Analysis of privacy-utility trade-offs is limited to AUC metrics without comprehensive evaluation of downstream task performance

## Confidence

- **High Confidence**: Core finding that supervised fine-tuning introduces measurable privacy risks; LoRA reduces privacy risks
- **Medium Confidence**: Claims that privacy risks vary with task complexity, text length, and batch size; assertion that privacy risks peak during specific training epochs
- **Low Confidence**: Generalizability to production-scale models and effectiveness against sophisticated black-box attacks

## Next Checks

1. **Cross-Architecture Validation**: Implement PARSING on diverse transformer architectures (BERT, T5, RoBERTa) and larger models (GPT-3.5/4) to assess generalizability beyond GPT-2 and GPT-Neo.

2. **Black-Box Attack Extension**: Modify PARSING to function with black-box access by removing gradient information and using only output probabilities. Evaluate whether privacy risks remain significant without white-box information.

3. **Defense Mechanism Evaluation**: Integrate differential privacy mechanisms and other privacy-preserving techniques during fine-tuning. Measure effectiveness of these defenses in reducing membership inference attack success while maintaining acceptable model performance.