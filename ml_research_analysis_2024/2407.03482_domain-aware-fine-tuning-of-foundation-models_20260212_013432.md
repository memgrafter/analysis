---
ver: rpa2
title: Domain-Aware Fine-Tuning of Foundation Models
arxiv_id: '2407.03482'
source_url: https://arxiv.org/abs/2407.03482
tags:
- domain
- performance
- fine-tuning
- data
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores zero-shot domain adaptation of foundation models
  for semantic segmentation, addressing the challenge of performance degradation under
  domain shift. The authors compare various foundation model backbones and introduce
  Domino, a domain-adaptive normalization technique that leverages domain embeddings
  during fine-tuning.
---

# Domain-Aware Fine-Tuning of Foundation Models

## Quick Facts
- arXiv ID: 2407.03482
- Source URL: https://arxiv.org/abs/2407.03482
- Reference count: 3
- Primary result: Domino technique achieves up to 85.38% domain adaptation performance on ACDC relative to Cityscapes

## Executive Summary
This paper addresses the challenge of zero-shot domain adaptation for semantic segmentation using foundation models. The authors propose Domino, a domain-adaptive normalization technique that leverages domain embeddings to improve generalization across different visual domains. By automatically extracting domain embeddings using CLIP and injecting them into the model through normalization layers and cross-attention, the method significantly improves performance when fine-tuning foundation models for new domains without requiring labeled target data.

## Method Summary
The proposed approach introduces Domino, a domain-aware fine-tuning technique that extracts domain embeddings automatically using CLIP and injects these embeddings into the model through normalization layers and cross-attention mechanisms. The method operates in a zero-shot setting, requiring no labeled data from the target domain during fine-tuning. The authors evaluate multiple foundation model backbones and find that the Stable Diffusion-based VPD backbone performs best for semantic segmentation tasks. The technique includes an ablation study showing that subtracting domain embeddings encourages more domain-invariant learning compared to addition.

## Key Results
- Domino achieves up to 85.38% domain adaptation performance (mIoU% on ACDC relative to Cityscapes)
- Stable Diffusion-based VPD backbone outperforms other foundation model backbones
- Subtracting domain embeddings yields better domain-invariant learning than addition
- Incorporating synthetic data with a 4:1 real-to-synthetic ratio further improves performance

## Why This Works (Mechanism)
The mechanism works by leveraging CLIP's pre-trained ability to distinguish between different visual domains, extracting domain-specific embeddings that capture the statistical differences between source and target domains. By injecting these embeddings through normalization layers and cross-attention, the model can dynamically adjust its feature representations based on domain characteristics. The subtraction operation appears to encourage the model to learn domain-invariant features by explicitly modeling what should be removed or ignored when adapting to new domains, rather than simply adding domain-specific information.

## Foundational Learning
- Domain shift and generalization: Why needed - foundation models degrade under domain shift; Quick check - compare performance across Cityscapes and ACDC datasets
- Zero-shot learning: Why needed - avoids expensive target domain labeling; Quick check - validate performance without target labels
- Normalization layer adaptation: Why needed - enables dynamic domain-specific feature scaling; Quick check - ablate normalization vs cross-attention injection
- CLIP embedding extraction: Why needed - leverages pre-trained domain awareness; Quick check - compare with random or learned domain embeddings
- Domain-invariant feature learning: Why needed - improves generalization across domains; Quick check - measure feature similarity across domains
- Synthetic data augmentation: Why needed - expands training distribution; Quick check - vary real-to-synthetic ratios

## Architecture Onboarding

Component map: CLIP domain extractor -> Domain embedding processor -> Normalization layers/Cross-attention -> Foundation model backbone -> Segmentation head

Critical path: Input image → CLIP → Domain embedding → Domino normalization/cross-attention → Foundation model → Output segmentation

Design tradeoffs: Zero-shot adaptation (no target labels) vs. potential suboptimality compared to supervised adaptation; automatic domain embedding extraction (convenience) vs. reliance on CLIP's domain awareness; subtraction vs. addition of domain embeddings for invariance

Failure signatures: Performance degradation when CLIP cannot distinguish domains; overfitting to source domain when domain embeddings are ineffective; instability when real-to-synthetic ratio is unbalanced

First experiments:
1. Ablation study comparing addition vs. subtraction of domain embeddings
2. Comparison of different foundation model backbones on Cityscapes-ACDC adaptation
3. Evaluation of different real-to-synthetic data ratios for fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability beyond semantic segmentation to other vision tasks remains uncertain
- Optimal real-to-synthetic ratio of 4:1 appears dataset-specific and may not transfer
- Theoretical justification for why subtraction encourages domain invariance is lacking
- Performance measured relative to Cityscapes rather than absolute ACDC performance

## Confidence

High confidence: Experimental methodology and baseline comparisons are rigorous and well-documented
Medium confidence: Generalizability of Domino to other tasks and domains requires further validation
Low confidence: Theoretical understanding of why subtraction of domain embeddings works better than other approaches

## Next Checks
1. Test Domino on additional vision tasks such as object detection and instance segmentation to assess cross-task generalizability
2. Evaluate the method on a broader range of domain shifts including synthetic-to-real, cross-sensor, and cross-weather conditions
3. Conduct ablation studies on the domain embedding extraction method by comparing CLIP with domain-specific embedding techniques