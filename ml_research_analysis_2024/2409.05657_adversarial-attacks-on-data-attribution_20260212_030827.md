---
ver: rpa2
title: Adversarial Attacks on Data Attribution
arxiv_id: '2409.05657'
source_url: https://arxiv.org/abs/2409.05657
tags:
- data
- attack
- training
- shadow
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates adversarial attacks on data attribution
  methods, which quantify individual training data contributions for AI model outputs
  and enable compensation mechanisms for data providers. The authors propose two attack
  strategies: Shadow Attack and Outlier Attack.'
---

# Adversarial Attacks on Data Attribution

## Quick Facts
- **arXiv ID**: 2409.05657
- **Source URL**: https://arxiv.org/abs/2409.05657
- **Reference count**: 40
- **Primary result**: Adversarial attacks can inflate compensation by 200%-643% by manipulating data attribution methods

## Executive Summary
This paper investigates adversarial attacks on data attribution methods that quantify individual training data contributions to AI model outputs. The authors propose two attack strategies: Shadow Attack and Outlier Attack. Shadow Attack leverages knowledge of data distribution through shadow training to generate adversarial perturbations, while Outlier Attack exploits the inductive bias that outliers are more influential and uses black-box adversarial examples. Experiments on image classification and text generation tasks show significant vulnerabilities in data attribution methods that could undermine fair compensation systems.

## Method Summary
The paper presents two attack strategies targeting data attribution methods. Shadow Attack uses shadow training to estimate compensation share by training shadow models on sampled datasets and applying gradient ascent to maximize a surrogate objective. Outlier Attack generates adversarial examples using black-box methods (ZOO, Simba, TextFooler) to create outliers that are more influential. The attacks are evaluated using Compensation Share and Fraction of Change metrics across multiple datasets (MNIST, Digits, CIFAR-10, Shakespeare) and models (Logistic Regression, MLP, CNN, ResNet-18, NanoGPT).

## Key Results
- Shadow Attack inflates compensation by 200%-456% using knowledge of data distribution
- Outlier Attack achieves 185%-643% inflation by exploiting outlier bias without distribution knowledge
- Both attacks demonstrate significant vulnerabilities in data attribution methods for fair compensation systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shadow Attack inflates compensation by leveraging knowledge of data distribution through shadow training to generate adversarial perturbations that maximize influence scores on shadow models.
- Mechanism: The adversary trains shadow models on data sampled from the same distribution as the target model's training data. By optimizing the contribution function on these shadow models, they generate perturbations that transfer to the target model, increasing their compensation share.
- Core assumption: The adversary has access to the data distribution used by the target model (Assumption 2).
- Evidence anchors:
  - [abstract]: "The Shadow Attack leverages knowledge about the data distribution in the AI applications, and derives adversarial perturbations through 'shadow training', a technique commonly used in membership inference attacks."
  - [section 4.1]: "The goal of the shadow training process is to estimate the data attribution values of the elements in Z as if this dataset were contributed to the AI Developer at t = 1."
- Break condition: The attack fails if the shadow models cannot accurately approximate the target model, or if the data distribution assumption is violated.

### Mechanism 2
- Claim: Outlier Attack inflates compensation by exploiting the inductive bias that outlier data points are more influential, using adversarial examples to generate realistic outliers without knowledge of data distribution.
- Mechanism: The adversary perturbs real data points to create outliers that the model is less confident about predicting correctly. Since many data attribution methods inherently assign higher influence to outliers, these manipulated data points receive inflated compensation.
- Core assumption: Outlier data points are inherently more influential in many data attribution methods (Section 5.1).
- Evidence anchors:
  - [abstract]: "The Outlier Attack does not assume any knowledge about the data distribution and relies solely on black-box queries to the target model's predictions. It exploits an inductive bias present in many data attribution methods - outlier data points are more likely to be influential - and employs adversarial examples to generate manipulated datasets."
- Break condition: The attack fails if the data attribution method does not exhibit the outlier bias, or if the perturbations are detectable as outliers by data quality filters.

### Mechanism 3
- Claim: The persistence assumption enables the adversary to use information from previous iterations to inform attacks on future contributions.
- Mechanism: By assuming that previous training data persists in future iterations (Z0 ⊆ Z1b), the adversary can use this historical data to understand the model's behavior and generate effective perturbations for future contributions.
- Core assumption: There is substantial overlap between training data across consecutive iterations (Assumption 1).
- Evidence anchors:
  - [section 3.2]: "Assumption 1 (Persistence). Assume that Z0 ⊆ Z1b and |Z0|/|Z1b| is close to 1. Additionally, assume that V0 and V1 are independently sampled from the same distribution."
- Break condition: The attack fails if there is insufficient overlap between training data across iterations, or if the validation data distributions differ significantly.

## Foundational Learning

- Concept: Data attribution methods and their role in quantifying training data contributions to model outputs.
  - Why needed here: The entire attack framework targets vulnerabilities in data attribution methods used for compensation.
  - Quick check question: What is the primary application of data attribution methods mentioned in the paper, and why does this make them attractive targets for adversarial attacks?

- Concept: Adversarial examples and their generation through black-box queries.
  - Why needed here: The Outlier Attack relies on generating adversarial examples to create realistic outliers without knowledge of the training data distribution.
  - Quick check question: How does the Outlier Attack generate realistic outliers using black-box queries, and what assumption about data attribution methods does it exploit?

- Concept: Membership inference attacks and shadow training technique.
  - Why needed here: The Shadow Attack is inspired by the shadow training technique commonly used in membership inference attacks.
  - Quick check question: What is the purpose of shadow training in the Shadow Attack, and how does it enable the adversary to generate effective perturbations?

## Architecture Onboarding

- Component map: Data Attribution Module -> Influence Score Calculation -> Top-k Selection -> Compensation Calculation -> Attack Evaluation
- Critical path: Data Attribution → Influence Score Calculation → Top-k Selection → Compensation Calculation → Attack Evaluation
- Design tradeoffs:
  - Shadow Attack requires knowledge of data distribution but provides more precise control
  - Outlier Attack requires no distribution knowledge but relies on exploiting inductive biases
  - Balance between perturbation magnitude (effectiveness) and detectability (risk)
- Failure signatures:
  - Compensation share remains unchanged after perturbation
  - Shadow models fail to approximate target model behavior
  - Generated outliers are detected by data quality filters
  - Influence scores do not transfer from shadow models to target model
- First 3 experiments:
  1. Test Shadow Attack on MNIST with Logistic Regression using Influence Function attribution
  2. Test Outlier Attack on Digits dataset with MLP model using Data Shapley attribution
  3. Evaluate both attacks on CIFAR-10 with ResNet-18 and TRAK attribution method

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several unresolved issues emerge from the analysis:

### Open Question 1
- Question: How do data attribution methods behave under partial observability of the training data distribution, where the adversary has access to only a subset of the training data distribution?
- Basis in paper: [explicit] The paper assumes full access to the data distribution in the Shadow Attack, but in practice, adversaries may only have partial information.
- Why unresolved: The paper does not explore scenarios where the adversary's knowledge of the data distribution is limited or incomplete.
- What evidence would resolve it: Experiments showing the effectiveness of Shadow Attack when the adversary has only partial access to the training data distribution.

### Open Question 2
- Question: Can the effectiveness of the Outlier Attack be further improved by combining it with other adversarial techniques, such as targeted attacks or more sophisticated black-box methods?
- Basis in paper: [inferred] The paper uses existing black-box attack methods (ZOO, Simba, TextFooler) but does not explore hybrid or advanced techniques.
- Why unresolved: The paper does not investigate the potential synergies between the Outlier Attack and other adversarial strategies.
- What evidence would resolve it: Experiments comparing the Outlier Attack with hybrid approaches that combine it with other adversarial techniques.

### Open Question 3
- Question: How do data attribution methods perform under dynamic and evolving data distributions, where the training data changes over time?
- Basis in paper: [explicit] The paper assumes periodic data contributions but does not address how attribution methods handle continuously evolving data distributions.
- Why unresolved: The paper focuses on static scenarios and does not explore the robustness of attribution methods under dynamic conditions.
- What evidence would resolve it: Experiments showing the performance of attribution methods and their vulnerability to attacks when the training data distribution changes over time.

### Open Question 4
- Question: Are there any inherent biases in data attribution methods that could be exploited by adversaries beyond the outlier bias discussed in the paper?
- Basis in paper: [explicit] The paper highlights the outlier bias as a vulnerability but does not explore other potential biases in attribution methods.
- Why unresolved: The paper does not investigate other biases that could be exploited by adversaries.
- What evidence would resolve it: Analysis of different data attribution methods to identify and test other potential biases that could be exploited by adversarial attacks.

## Limitations

- The Shadow Attack's effectiveness depends heavily on the adversary's ability to accurately sample from the target model's training data distribution, which may not hold in practice.
- The Outlier Attack relies on the persistent outlier bias in data attribution methods, but this bias may vary significantly across different attribution techniques.
- The paper does not address potential countermeasures like data quality filters or detection mechanisms that could mitigate these attacks.

## Confidence

- **High Confidence**: The experimental methodology and results showing successful inflation of compensation shares (200%-643%) are well-documented and reproducible.
- **Medium Confidence**: The theoretical framework connecting shadow training to data attribution attacks is sound, but practical effectiveness may vary based on implementation specifics.
- **Low Confidence**: The paper's claims about the persistence assumption and its practical implications for real-world deployment scenarios lack sufficient empirical validation.

## Next Checks

1. **Cross-Dataset Validation**: Test both attacks across additional datasets beyond MNIST, Digits, and CIFAR-10 to verify robustness across different data distributions and model architectures.
2. **Attribution Method Diversity**: Evaluate attack effectiveness against a broader range of data attribution methods, including those specifically designed to mitigate outlier biases and distribution-aware vulnerabilities.
3. **Defense Mechanisms**: Implement and test common defense strategies (data quality filters, anomaly detection, attribution method modifications) to quantify the practical impact of these attacks in realistic deployment scenarios.