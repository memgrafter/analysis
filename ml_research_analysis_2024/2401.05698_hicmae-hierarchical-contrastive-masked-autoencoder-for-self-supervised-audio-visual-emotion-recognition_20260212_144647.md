---
ver: rpa2
title: 'HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised Audio-Visual
  Emotion Recognition'
arxiv_id: '2401.05698'
source_url: https://arxiv.org/abs/2401.05698
tags:
- audio-visual
- learning
- video
- audio
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'HiCMAE addresses data scarcity in audio-visual emotion recognition
  (AVER) by leveraging large-scale self-supervised pre-training with a novel Hierarchical
  Contrastive Masked Autoencoder. Unlike prior methods that focus solely on top-layer
  representations, HiCMAE fosters hierarchical feature learning through three key
  strategies: hierarchical skip connections between encoder and decoder to guide intermediate
  layers, hierarchical cross-modal contrastive learning to progressively reduce modality
  gaps, and hierarchical feature fusion during fine-tuning.'
---

# HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised Audio-Visual Emotion Recognition

## Quick Facts
- arXiv ID: 2401.05698
- Source URL: https://arxiv.org/abs/2401.05698
- Reference count: 40
- Establishes new state-of-the-art results on 9 audio-visual emotion recognition datasets

## Executive Summary
HiCMAE addresses data scarcity in audio-visual emotion recognition (AVER) by leveraging large-scale self-supervised pre-training with a novel Hierarchical Contrastive Masked Autoencoder. Unlike prior methods that focus solely on top-layer representations, HiCMAE fosters hierarchical feature learning through three key strategies: hierarchical skip connections between encoder and decoder to guide intermediate layers, hierarchical cross-modal contrastive learning to progressively reduce modality gaps, and hierarchical feature fusion during fine-tuning. Extensive experiments on 9 datasets covering both categorical and dimensional AVER tasks show that HiCMAE significantly outperforms state-of-the-art supervised and self-supervised audio-visual methods.

## Method Summary
HiCMAE is a self-supervised audio-visual pre-training framework that uses masked autoencoders with hierarchical contrastive learning. The model consists of modality-specific encoders and lightweight decoders connected by hierarchical skip connections, enabling intermediate layers to learn meaningful representations. Cross-modal contrastive learning is applied at multiple representation levels to progressively align audio and visual features. During fine-tuning, hierarchical feature fusion integrates multi-level features for downstream emotion recognition tasks. The model is pre-trained on VoxCeleb2 and evaluated on 9 emotion recognition datasets.

## Key Results
- Achieves 56.17% weighted average recall on MAFW (11-class emotion classification)
- Achieves 84.91% weighted average recall on CREMA-D (6-class emotion classification)
- Establishes new state-of-the-art results across 9 audio-visual emotion recognition datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical skip connections between encoder and decoder layers enable intermediate layers to learn more discriminative emotion features.
- Mechanism: By allowing each decoder layer to attend directly to encoder representations from multiple levels, the model can exploit both coarse-grained high-level semantics and fine-grained low-level details for emotion reconstruction.
- Core assumption: Emotion-related information is distributed across multiple layers of the encoder, not just the final layer.
- Evidence anchors:
  - [abstract]: "incorporates hierarchical skip connectionsbetween the encoder and decoder to encourage intermediate layers to learn more meaningful representations"
  - [section]: "Unlike previous audio-visual masked autoencoders which solely operate on encoder representation from the last layer and neglect explicit guidance on other layers, HiCMAE incorporates hierarchical skip connections between intermediate encoder and decoder layers"
  - [corpus]: Weak. No direct corpus evidence for this specific mechanism in emotion recognition context.
- Break condition: If emotion information is predominantly concentrated in the final encoder layer, the hierarchical skip connections provide diminishing returns.

### Mechanism 2
- Claim: Hierarchical cross-modal contrastive learning progressively reduces modality gaps and improves cross-modal fusion.
- Mechanism: By applying contrastive learning at multiple encoder layers, the model learns to align audio and visual representations at different abstraction levels, making subsequent fusion more effective.
- Core assumption: Modality gaps exist at multiple levels of abstraction and need to be addressed progressively rather than only at the final representation.
- Evidence anchors:
  - [abstract]: "hierarchical cross-modal contrastive learning is also exerted on intermediate representations to narrow the audio-visual modality gap progressively"
  - [section]: "HCMCL is imposed on multiple intermediate (including both high-level and low-level) features in audio and video encoders to achieve latent representation alignment in a progressive manner"
  - [corpus]: Weak. While contrastive learning is well-established, hierarchical application specifically for audio-visual emotion recognition lacks direct corpus evidence.
- Break condition: If the modality gap is primarily at the final representation level, intermediate contrastive learning may be unnecessary.

### Mechanism 3
- Claim: Hierarchical feature fusion during fine-tuning comprehensively integrates multi-level features for better emotion classification.
- Mechanism: By combining features from all encoder layers with learned weights, the model can leverage both high-level semantic information and low-level discriminative details for final emotion prediction.
- Core assumption: Different emotion categories rely on information at different levels of abstraction, and combining multiple levels improves classification accuracy.
- Evidence anchors:
  - [abstract]: "HiCMAE employs hierarchical feature fusion to comprehensively integrate multi-level features from different layers"
  - [section]: "To benefit downstream emotion recognition tasks, we utilize both cross-modal and unimodal features from the encoders... To fully exploit features of di fferent levels, we use learnable weights to combine features from different audio/video encoder layers"
  - [corpus]: Weak. Feature fusion at multiple levels is common in computer vision but specific application to audio-visual emotion recognition lacks direct corpus evidence.
- Break condition: If emotion classification relies predominantly on high-level semantic features, multi-level fusion may add noise without improving accuracy.

## Foundational Learning

- Concept: Masked autoencoders for self-supervised learning
  - Why needed here: Enables learning powerful audio-visual representations from unlabeled data, addressing the data scarcity problem in emotion recognition
  - Quick check question: What is the key difference between standard autoencoders and masked autoencoders?

- Concept: Contrastive learning for cross-modal alignment
  - Why needed here: Leverages natural audio-visual correspondences in videos to learn semantically meaningful representations without labels
  - Quick check question: How does InfoNCE loss encourage alignment between audio and visual representations?

- Concept: Hierarchical representation learning
  - Why needed here: Different emotion-related information may exist at different levels of abstraction, requiring explicit guidance of intermediate layers
  - Quick check question: Why might emotion information be distributed across multiple layers rather than concentrated in the final layer?

## Architecture Onboarding

- Component map:
  - Input: 16-frame video clips (16×160×160×3) and corresponding audio spectrograms (256×128)
  - Audio Encoder: Transformer with Ns layers
  - Video Encoder: Transformer with Ns layers
  - Cross-modal Fusion Encoder: Two MHCA-based transformers for audio→video and video→audio fusion
  - Audio Decoder: Lightweight Transformer with Nd layers and hierarchical skip connections
  - Video Decoder: Lightweight Transformer with Nd layers and hierarchical skip connections
  - Output: Reconstructed audio and video for masked positions

- Critical path: Masked tokens → Encoders → Cross-modal fusion → Decoders → Reconstruction loss + Contrastive loss

- Design tradeoffs:
  - Model size vs. performance: Three variants (Tiny, Small, Base) with different hidden dimensions
  - Masking ratio: 80% for audio, 90% for video balances difficulty and computational efficiency
  - Number of fusion layers: 2 layers provide sufficient cross-modal interaction without excessive computation

- Failure signatures:
  - Poor reconstruction quality: Indicates insufficient modeling capacity or inappropriate masking strategy
  - Degraded performance on downstream tasks: Suggests pre-training objectives don't align well with emotion recognition
  - Mode collapse in contrastive learning: Indicates improper temperature parameter or negative sampling strategy

- First 3 experiments:
  1. Test reconstruction quality with different masking ratios (60%, 80%, 90%) on a validation set
  2. Evaluate contrastive learning effectiveness by visualizing audio-visual embedding alignment with t-SNE
  3. Compare downstream performance with and without hierarchical skip connections on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hierarchical contrastive learning approach in HiCMAE compare to other cross-modal alignment methods (e.g., cycle-consistency, deep clustering) in terms of capturing emotion-specific correlations?
- Basis in paper: [explicit] The paper states that HiCMAE uses hierarchical cross-modal contrastive learning to reduce the modality gap progressively and enhance cross-modal fusion. It notes this is different from conventional contrastive learning which only applies to top-layer features.
- Why unresolved: The paper doesn't directly compare hierarchical contrastive learning to other cross-modal alignment techniques specifically for emotion recognition tasks.
- What evidence would resolve it: Empirical results comparing HiCMAE's hierarchical contrastive learning with other cross-modal alignment methods (e.g., cycle-consistency, deep clustering) on the same emotion recognition datasets.

### Open Question 2
- Question: What is the impact of different masking strategies (e.g., tube masking vs. random masking) on the learned emotion representations in HiCMAE?
- Basis in paper: [explicit] The paper mentions using tube masking for video and random masking for audio, but doesn't explore alternative masking strategies or their impact on emotion recognition performance.
- Why unresolved: The paper doesn't investigate how different masking strategies affect the quality of learned emotion representations or downstream recognition performance.
- What evidence would resolve it: Experiments comparing different masking strategies (e.g., spatial vs. temporal masking, adaptive masking) and their impact on emotion recognition accuracy and feature quality.

### Open Question 3
- Question: How does the performance of HiCMAE scale with increasing model size and training data, particularly for rare emotion categories?
- Basis in paper: [explicit] The paper presents results for three model scales (tiny, small, base) and notes performance improvements with larger models. It also mentions challenges with rare emotions like contempt and disappointment.
- Why unresolved: The paper doesn't explore the relationship between model capacity, training data size, and performance on rare emotion categories beyond the three scales tested.
- What evidence would resolve it: Scaling experiments with progressively larger models and datasets, particularly focusing on performance on rare emotion categories and the point of diminishing returns.

## Limitations

- Performance gains may primarily result from substantial VoxCeleb2 pre-training data rather than hierarchical design innovations
- Hierarchical mechanisms (skip connections, intermediate contrastive learning) lack direct corpus validation in emotion recognition domain
- Limited ablation studies to isolate contributions of individual hierarchical components

## Confidence

**High Confidence:** The overall methodology of using self-supervised pre-training to address data scarcity in audio-visual emotion recognition is well-established and the reported experimental results appear methodologically sound.

**Medium Confidence:** The three-pronged hierarchical strategy (skip connections, contrastive learning, feature fusion) likely contributes to the performance improvements, though the relative contribution of each component remains unclear without proper ablation studies.

**Low Confidence:** The specific claim that hierarchical contrastive learning at intermediate layers is essential for reducing modality gaps, as this mechanism lacks direct corpus evidence in the audio-visual emotion recognition context.

## Next Checks

1. **Ablation Study:** Conduct controlled experiments removing each hierarchical component (skip connections, intermediate contrastive learning, multi-level feature fusion) while keeping pre-training data constant to isolate their individual contributions.

2. **Cross-Dataset Transferability:** Evaluate HiCMAE pre-trained on VoxCeleb2 for emotion recognition on a dataset from a different domain (e.g., social media videos) to assess whether the hierarchical features generalize beyond the pre-training distribution.

3. **Comparison with Non-Hierarchical Baselines:** Implement and evaluate a contrastive MAE with pre-training on the same VoxCeleb2 data but without the hierarchical components to determine if the gains are primarily from data scale rather than architectural innovations.