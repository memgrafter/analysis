---
ver: rpa2
title: 'Training Datasets Generation for Machine Learning: Application to Vision Based
  Navigation'
arxiv_id: '2409.11383'
source_url: https://arxiv.org/abs/2409.11383
tags:
- datasets
- images
- chang
- laboratory
- surrender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addressed the challenge of generating adequate training
  datasets for machine learning algorithms in vision-based space navigation. The team
  developed a methodology to create synthetic and laboratory datasets for two use
  cases: in-orbit rendezvous with a satellite mockup and lunar landing scenarios.'
---

# Training Datasets Generation for Machine Learning: Application to Vision Based Navigation

## Quick Facts
- arXiv ID: 2409.11383
- Source URL: https://arxiv.org/abs/2409.11383
- Reference count: 14
- Primary result: Synthetic datasets generated with high-fidelity simulators can train AI algorithms that generalize to real space navigation data

## Executive Summary
This study addressed the challenge of generating adequate training datasets for machine learning algorithms in vision-based space navigation. The team developed a methodology to create synthetic and laboratory datasets for two use cases: in-orbit rendezvous with a satellite mockup and lunar landing scenarios. Using high-fidelity image simulator SurRender, robotic facilities, and generative adversarial networks, they produced datasets including archival data from Chang'e-3 and laboratory captures. The key finding was that datasets generated through SurRender simulations and selected laboratory facilities successfully trained AI algorithms that generalized to real datasets. Specifically, for optical flow estimation, models trained on SurRender and TRON datasets outperformed pretrained models on real Chang'e-3 images, demonstrating the adequacy of synthetic datasets for training space navigation algorithms.

## Method Summary
The methodology involved creating synthetic datasets using the SurRender simulator with high-fidelity image rendering based on DEMs and procedural details, and laboratory datasets using robotic facilities to capture images of satellite mockups or terrain models with precise ground truth measurements. Generative adversarial networks were employed to enhance synthetic images by transforming them into high-quality realistic images while preserving ground truth metadata. Two AI algorithms were selected as benchmarks: a CNN for pose estimation and RAFT for optical flow. The datasets were then used to train these algorithms, and their performance was evaluated on real datasets using metrics such as optical flow end-point error (EPE) and pose estimation accuracy.

## Key Results
- Synthetic datasets generated with SurRender simulator successfully trained AI algorithms that generalized to real Chang'e-3 lunar landing images
- Models trained on SurRender and TRON datasets outperformed pretrained models on real data for optical flow estimation tasks
- Laboratory datasets captured with robotic facilities provided additional training data that improved algorithm accuracy when used for both training and testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic datasets generated with high-fidelity image simulators can train AI algorithms that generalize to real datasets for vision-based navigation.
- Mechanism: SurRender's physically-based ray-tracing with terrain DEMs and procedural details produces realistic images that capture the essential visual features needed for navigation algorithms to learn meaningful features, which transfer to real data.
- Core assumption: The visual characteristics and dynamics in synthetic renderings sufficiently approximate those in real space navigation scenarios.
- Evidence anchors:
  - [abstract] "datasets generated through SurRender simulations and selected laboratory facilities successfully trained AI algorithms that generalized to real datasets"
  - [section] "Overall, it is found that models trained on SurRender and TRON datasets reproducing the Chang'e-3 trajectory perform significantly better on the real data (real Chang'e-3 images) than the pretrained model"
  - [corpus] Weak - no direct corpus support for this specific mechanism
- Break condition: If synthetic datasets lack critical visual features present in real data, or if the domain gap between synthetic and real is too large for the algorithms to bridge.

### Mechanism 2
- Claim: Generative Adversarial Networks can transform low-quality synthetic images into high-quality realistic images while preserving ground truth metadata.
- Mechanism: GANs learn the mapping between synthetic domain A and real domain B, adding photorealistic textures and details to synthetic images while maintaining the underlying geometric and motion information needed for navigation.
- Core assumption: GANs can learn to translate between domains without paired datasets while preserving the geometric consistency needed for navigation tasks.
- Evidence anchors:
  - [section] "The use case definition included the selection of algorithms as benchmark: an AI-based pose estimation algorithm and a dense optical flow algorithm were selected"
  - [section] "GANs are generative models used for Image-to-Image translation, such as transforming a source domain into a target domain"
  - [corpus] Weak - no direct corpus support for this specific GAN application
- Break condition: If GAN artifacts introduce errors that corrupt the ground truth or if temporal coherence cannot be maintained across image sequences.

### Mechanism 3
- Claim: Model capture techniques can extract realistic textures and reflectance properties from real laboratory images and inject them into synthetic simulations.
- Mechanism: SurRender's model capture functionality back-projects real images into texture space to capture surface details and optimizes BRDF parameters to maximize resemblance between simulations and real photographs.
- Core assumption: Real laboratory images contain sufficient information to extract realistic surface properties that can be transferred to synthetic models.
- Evidence anchors:
  - [section] "An innovative method has been developed to extract textures and optical properties from the real laboratory images of the mockup and inject them in the simulations"
  - [section] "Taking advantage of this functionality, we could adjust BRDF parameters by maximising the resemblance between a small selection of photographs of the mockup and simulations"
  - [corpus] Weak - no direct corpus support for this specific model capture technique
- Break condition: If the extracted properties do not accurately represent the real surfaces or if the optimization process fails to converge to realistic solutions.

## Foundational Learning

- Concept: Digital Elevation Models (DEMs) and terrain representation
  - Why needed here: The study relies on DEMs as input for synthetic terrain generation in both lunar and satellite scenarios
  - Quick check question: How do different DEM resolutions (20m from Chang'e-2 vs 5m from LRO) affect the quality of synthetic navigation datasets?

- Concept: Bidirectional Reflectance Distribution Function (BRDF) and optical properties
  - Why needed here: The study uses Hapke BRDF model and needs to capture realistic surface reflectance for synthetic images to match real observations
  - Quick check question: What role does the Hapke BRDF play in generating realistic lunar surface renderings, and how does it differ from simpler reflectance models?

- Concept: Domain adaptation and transfer learning
  - Why needed here: The core finding demonstrates that models trained on synthetic data can generalize to real data, requiring understanding of domain adaptation principles
  - Quick check question: What are the key factors that determine whether a model trained on synthetic data will successfully transfer to real-world applications?

## Architecture Onboarding

- Component map: SurRender simulator -> DEM processing -> Procedural detail generation -> Image rendering -> GAN enhancement -> Algorithm training -> Real data validation
- Critical path: Synthetic data generation -> AI algorithm training -> Performance evaluation on real datasets
- Design tradeoffs: High-fidelity synthetic data requires more computational resources but provides better generalization; GANs add realism but may introduce artifacts; model capture provides realism but requires real reference images
- Failure signatures: Poor algorithm performance on real data indicates synthetic-real domain gap; GAN artifacts visible in enhanced images; model capture optimization failing to converge
- First 3 experiments:
  1. Generate synthetic lunar landing dataset with varying DEM resolutions and evaluate impact on optical flow algorithm performance
  2. Train pose estimation algorithm on pure synthetic data vs mixed synthetic-laboratory data to quantify transfer benefits
  3. Apply GAN enhancement to synthetic images and measure impact on algorithm performance while checking for temporal coherence artifacts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can generative AI techniques be improved for Vision-Based Navigation applications in space to address issues like temporal incoherence and artifacts?
- Basis in paper: [explicit] The paper discusses challenges with Generative Adversarial Networks (GANs) for Image-to-Image translation, including temporal incoherence in sequences and remaining artifacts such as aliasing, ghosts, and parasitic motions.
- Why unresolved: While the paper identifies these issues, it suggests future improvements without providing concrete solutions or demonstrating successful implementation of these improvements.
- What evidence would resolve it: Successful implementation of generative AI techniques that eliminate temporal incoherence and artifacts, validated through improved performance in Vision-Based Navigation tasks using real datasets.

### Open Question 2
- Question: What is the optimal balance between synthetic and real datasets for training AI algorithms in Vision-Based Navigation applications?
- Basis in paper: [explicit] The paper demonstrates that datasets produced with SurRender simulations and selected laboratory facilities are adequate to train machine learning algorithms, but also shows that training with laboratory images yields higher accuracy when the same dataset is used for training and test.
- Why unresolved: The paper shows that synthetic datasets can be effective for training, but does not explore the optimal mix of synthetic and real data or how this balance might vary depending on the specific VBN task or scenario.
- What evidence would resolve it: Comparative studies that systematically vary the ratio of synthetic to real data in training datasets and measure the resulting performance on real-world VBN tasks across different scenarios.

### Open Question 3
- Question: How can model capture techniques be extended to improve the realism of synthetic datasets for Vision-Based Navigation?
- Basis in paper: [explicit] The paper presents a proof-of-concept for model capture, extracting textures and optical properties from real laboratory images of a satellite mockup and injecting them into simulations.
- Why unresolved: While the paper demonstrates the feasibility of model capture for a satellite mockup, it does not explore its application to more complex scenarios like lunar landing, nor does it quantify the impact of model capture on algorithm performance.
- What evidence would resolve it: Successful application of model capture techniques to lunar terrain datasets, resulting in synthetic images that significantly improve the performance of VBN algorithms compared to baseline synthetic datasets.

## Limitations

- The study is limited to specific space navigation scenarios (satellite rendezvous and lunar landing) and may not generalize to other domains
- Confidence in the effectiveness of GANs for domain translation is limited due to insufficient evidence of their performance and potential for introducing artifacts
- The model capture technique for extracting realistic textures and reflectance properties lacks detailed validation and implementation information

## Confidence

- Synthetic datasets generalization: Medium - supported by experimental results but lacks broader corpus validation
- GAN effectiveness: Medium - limited evidence of performance and potential for artifacts
- Model capture technique: Low - insufficient detail on implementation and validation

## Next Checks

1. Test whether models trained on synthetic lunar datasets can generalize to different lunar terrains or even to planetary surfaces like Mars, to assess the robustness of the synthetic data generation approach.

2. Evaluate the temporal consistency of GAN-enhanced image sequences by measuring the impact on optical flow algorithms over time, ensuring that domain translation does not introduce artifacts that degrade algorithm performance.

3. Systematically vary the fidelity and diversity of synthetic datasets (e.g., DEM resolution, procedural detail density) and measure the corresponding changes in algorithm performance on real data to identify the minimum requirements for effective training.