---
ver: rpa2
title: 'Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation
  in Multilingual LLM Applications?'
arxiv_id: '2403.04792'
source_url: https://arxiv.org/abs/2403.04792
tags:
- direct
- pre-translation
- language
- languages
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether pre-translation is necessary for
  optimal performance of PaLM2 models on multilingual tasks. The authors conduct a
  comprehensive study across 108 languages and six diverse benchmarks, including open-ended
  generative tasks.
---

# Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?

## Quick Facts
- arXiv ID: 2403.04792
- Source URL: https://arxiv.org/abs/2403.04792
- Reference count: 21
- Primary result: PaLM2-L outperforms pre-translation in 94 out of 108 languages across six diverse benchmarks

## Executive Summary
This paper challenges the conventional wisdom that pre-translation is necessary for optimal multilingual LLM performance. Through comprehensive testing across 108 languages and six diverse benchmarks, the authors demonstrate that direct inference using PaLM2 models often outperforms the traditional approach of translating inputs to English before inference. The findings suggest that modern multilingual training can overcome English-centric biases, preserving linguistic authenticity and computational efficiency.

## Method Summary
The study compares direct inference versus pre-translation approaches using PaLM2-S and PaLM2-L models across 108 languages. Pre-translation uses Google Translate API to convert inputs to English before inference, with outputs translated back to the source language. Direct inference performs inference in the source language without translation. Both approaches are evaluated using accuracy, F1, and RougeL metrics, with a Language-Ratio measure tracking per-language win rates. The study includes open-ended generative tasks and employs filtering schemes for attributive QA tasks to ensure fair comparison.

## Key Results
- PaLM2-L achieves superior performance in 94 out of 108 languages compared to pre-translation
- Pre-translation only outperforms direct inference in 7 languages: Bambara, Cusco-Collao Quechua, Lingala, Oromo, Punjabi, Tigrinya, and Tsonga
- The findings challenge the established pre-translation paradigm for multilingual LLM applications

## Why This Works (Mechanism)

### Mechanism 1
Direct inference outperforms pre-translation due to PaLM2's improved multilingual training, which reduces English-centric bias and allows better understanding of non-English languages.

### Mechanism 2
Pre-translation introduces information loss and complexity through translation steps that can lead to semantic drift and loss of nuanced meaning.

### Mechanism 3
Evaluation in English for open-ended tasks provides consistent comparison across languages by eliminating language-specific variations in lexical metrics like F1 and ROUGE.

## Foundational Learning

- **Lexical evaluation metrics and their limitations across languages**: Understanding why evaluation in English is necessary for fair comparison. *Quick check*: Why might F1 scores be misleading when comparing model performance across languages with different morphologies?

- **Attributive question answering and extractive span tasks**: Understanding the filtering scheme used to ensure fair comparison in QA tasks. *Quick check*: How does the filtering scheme address the potential disadvantage of pre-translation in attributive QA?

- **Low-resource languages and their challenges in NLP**: Understanding why certain languages might benefit less from direct inference. *Quick check*: What factors might contribute to the lower performance of direct inference in some low-resource languages?

## Architecture Onboarding

- **Component map**: PaLM2 models (PaLM2-S, PaLM2-L) -> Google Translate API -> Six benchmark datasets (BeleBele, XCOPA, XStoryCloze, XLSum, TyDiQA-GP, XQuAD) -> Evaluation metrics (accuracy, F1, RougeL, Language-Ratio) -> Filtering scheme for attributive QA tasks

- **Critical path**: 1. Load input data in source language 2. Apply pre-translation (translate to English) if using pre-translation approach 3. Perform inference using PaLM2 model 4. If using pre-translation, translate output back to source language 5. Apply filtering scheme for attributive QA tasks 6. Evaluate results using appropriate metrics 7. Calculate Language-Ratio for per-language comparison

- **Design tradeoffs**: Pre-translation vs. direct inference balancing potential information loss with need to overcome English-centric bias; Evaluation in source language vs. evaluation in English ensuring fair comparison across languages with different morphologies; Single-shot vs. zero-shot prompting balancing performance with need for task-specific fine-tuning

- **Failure signatures**: Consistently poor performance in certain language families or regions; High variance in results across different benchmarks for the same language; Unexpected advantages for pre-translation in languages expected to benefit from direct inference

- **First 3 experiments**: 1. Replicate main findings using subset of languages and benchmarks to validate core methodology 2. Test impact of different filtering schemes on attributive QA task performance 3. Evaluate performance of other multilingual models (e.g., GPT-4) using same benchmarks and comparison methodology

## Open Questions the Paper Calls Out

1. Does the superiority of direct inference over pre-translation extend to other multilingual LLMs beyond PaLM2, such as GPT-4 or Llama? While our study explores the multilingual landscape, analyzing a diverse range of datasets, assessment of open-ended tasks could be improved by using datasets that cover a wider range of languages, similar to the variety in BeleBele, which incorporates over 100 different languages. We identified compelling evidence confirming that direct inference is superior to pre-translation within PaLM2. However, it's important to recognize the potential for similar behavior across a wider spectrum of LLMs.

2. What are the underlying linguistic or cultural factors that make Bambara, Cusco-Collao Quechua, Lingala, Oromo, Punjabi, Tigrinya, and Tsonga perform better with pre-translation? In our per-language analysis reveals potential gaps and opportunities for future development in African languages, including those with relatively large speaker populations. These languages appear to be at a disadvantage compared to other low-resource languages, warranting further investigation and targeted efforts to bridge the performance gap.

3. How do different pre-translation approaches (e.g., using different translation APIs or fine-tuning the translation model) affect the performance comparison between direct inference and pre-translation? The study uses Google Translate API for pre-translation, but the choice of translation tool could influence the results.

## Limitations
- The 94/108 language superiority claim rests on aggregate metrics that may mask language-specific failures
- The study only tests PaLM2 models - results may not generalize to other multilingual LLMs with different training approaches
- The claim that PaLM2-L "consistently" outperforms pre-translation needs qualification as 7 widely spoken languages perform better with pre-translation

## Confidence

**High confidence**: The core methodology of comparing direct inference against pre-translation is sound and reproducible. The Language-Ratio metric provides a useful summary statistic.

**Medium confidence**: The general trend that direct inference often matches or exceeds pre-translation performance is well-supported, though the magnitude varies by task type and language family.

**Low confidence**: Claims about which specific languages benefit from each approach require more granular analysis. The assertion that this "challenges the established pre-translation paradigm" overstates the evidence.

## Next Checks
1. **Language-family analysis**: Segment results by language family (e.g., Indo-European, Afro-Asiatic, Sino-Tibetan) to identify whether performance patterns correlate with linguistic distance from English or availability of training data.

2. **Translation quality audit**: Measure the actual translation quality between source languages and English for the 7 languages where pre-translation wins. Poor translation quality could explain these exceptions rather than fundamental model limitations.

3. **Cross-model replication**: Test whether other state-of-the-art multilingual models (GPT-4, Claude) show similar patterns. If the trend reverses for different architectures, it suggests the findings are model-specific rather than representing a general principle.