---
ver: rpa2
title: Behavioural Cloning in VizDoom
arxiv_id: '2401.03993'
source_url: https://arxiv.org/abs/2401.03993
tags:
- agent
- agents
- data
- learning
- players
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores training autonomous agents to play Doom 2 using
  imitation learning from pixel data. The authors collect human gameplay data and
  use behavioral cloning to train agents that mimic different play styles.
---

# Behavioural Cloning in VizDoom
## Quick Facts
- arXiv ID: 2401.03993
- Source URL: https://arxiv.org/abs/2401.03993
- Reference count: 40
- Primary result: Behavioral cloning agents achieve comparable performance to average human players in Doom 2 while exhibiting more human-like behaviors

## Executive Summary
This paper explores training autonomous agents to play Doom 2 using imitation learning from pixel data. The authors collect human gameplay data and use behavioral cloning to train agents that mimic different play styles. They compare IL to reinforcement learning, finding that IL agents perform on par with average human players while exhibiting more human-like behaviors. Key innovations include using depth and segmentation data alongside RGB, a novel exponential frame skipping approach, and signed mean squared error loss for mouse movements. The best IL agent achieves 11 kills and 1463 damage in three minutes, outperforming the worst human players. While RL agents achieve higher performance, IL agents more closely match human spatial preferences and camera movement patterns.

## Method Summary
The authors collect human gameplay data from Doom 2 and use behavioral cloning to train agents that mimic different play styles. They employ a multi-input architecture using RGB, depth, and semantic segmentation data. The training process uses exponential frame skipping to capture different play styles and signed mean squared error loss for mouse movement prediction. The agents are evaluated against human performance metrics and compared with reinforcement learning baselines. The methodology focuses on creating diverse, human-like agents without requiring game engine data.

## Key Results
- Best IL agent achieves 11 kills and 1463 damage in three minutes, outperforming worst human players
- IL agents perform on par with average human players while exhibiting more human-like behaviors
- RL agents achieve higher performance scores but less human-like spatial preferences and camera movements

## Why This Works (Mechanism)
The paper demonstrates that behavioral cloning can effectively learn complex game-playing behaviors from human demonstrations. The use of multiple input modalities (RGB, depth, and segmentation) provides rich spatial information that helps agents understand the game environment. The exponential frame skipping approach allows the model to capture different temporal scales of human behavior, while the signed MSE loss helps preserve the directionality of mouse movements that are crucial for aiming and navigation.

## Foundational Learning
1. **Behavioral Cloning**: Why needed - To train agents by mimicking human demonstrations; Quick check - Does the agent's behavior resemble the human demonstrator's?
2. **Multi-modal Input Processing**: Why needed - To capture rich spatial and semantic information; Quick check - Does combining RGB, depth, and segmentation improve performance?
3. **Temporal Modeling**: Why needed - To capture the sequential nature of gameplay; Quick check - Does the agent maintain coherent action sequences over time?

## Architecture Onboarding
**Component Map**: Human demonstrations -> Preprocessing (RGB, depth, segmentation) -> Behavioral Cloning model -> Agent actions -> Game environment -> Performance metrics

**Critical Path**: Input preprocessing -> CNN feature extraction -> LSTM temporal modeling -> Action prediction -> Game execution

**Design Tradeoffs**: Multi-input modalities increase model complexity but provide richer information; exponential frame skipping captures diverse behaviors but requires careful tuning; signed MSE loss preserves movement directionality but may be more sensitive to noise

**Failure Signatures**: Agents may get stuck in repetitive behaviors; poor spatial awareness in complex environments; failure to adapt to unseen scenarios

**First Experiments**: 1) Train on single human demonstration to test basic learning capability; 2) Compare single vs. multiple input modalities; 3) Test different frame skipping intervals to find optimal temporal resolution

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on kill counts and damage dealt, potentially missing other aspects of agent quality
- Limited comparison to human performance rather than systematic human-likeness assessment
- Behavioral diversity analysis relies on aggregate statistics rather than direct human judgment

## Confidence
- **High confidence**: IL agents achieving comparable performance to average humans in terms of kills and damage
- **Medium confidence**: Claims about IL agents being more "human-like" based on spatial preferences and camera movement patterns
- **Low confidence**: The assertion that IL is "a viable alternative to RL" for training autonomous agents

## Next Checks
1. Conduct blind human evaluations where human observers rate agent videos for human-likeness to validate the claimed behavioral similarities
2. Test the trained agents against other IL agents in tournament-style gameplay to assess competitive performance and behavioral diversity
3. Evaluate agent performance across multiple Doom 2 scenarios beyond the single map used to assess generalization capabilities