---
ver: rpa2
title: 'NeSyA: Neurosymbolic Automata'
arxiv_id: '2412.07331'
source_url: https://arxiv.org/abs/2412.07331
tags:
- neural
- each
- which
- esya
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces NeSyA, a neurosymbolic system that integrates
  neural networks with symbolic automata for sequence classification and tagging tasks.
  The key innovation is combining the temporal reasoning power of automata with propositional
  logic's static reasoning capabilities, all under probabilistic semantics.
---

# NeSyA: Neurosymbolic Automata

## Quick Facts
- **arXiv ID**: 2412.07331
- **Source URL**: https://arxiv.org/abs/2412.07331
- **Reference count**: 12
- **Primary result**: NeSyA achieves 100% accuracy on synthetic driving benchmark while being orders of magnitude faster than DEEP STOCH LOG

## Executive Summary
NeSyA introduces a neurosymbolic system that integrates neural networks with symbolic automata for sequence classification and tagging tasks. The key innovation combines the temporal reasoning power of automata with propositional logic's static reasoning capabilities under probabilistic semantics. By using a neural network to extract symbolic information from subsymbolic inputs (like images), which is then processed by a symbolic automaton, NeSyA demonstrates superior scalability and accuracy compared to existing neurosymbolic approaches. The system's matrix-based inference scheme and knowledge compilation approach enable efficient exact inference, making it practical for real-world applications.

## Method Summary
NeSyA combines neural perception with symbolic automata to classify sequences. A CNN extracts symbol probabilities from subsymbolic inputs (images or sequences). These probabilities are used with symbolic finite automata (SFA) to compute acceptance probabilities via matrix-based inference and knowledge compilation. The system uses knowledge compilation (d-DNNF circuits) for efficient weighted model counting, transforming logical transitions into tractable representations. Training uses BCE loss for synthetic data and cross-entropy for CAVIAR, with gradient descent on weakly supervised labels. The SFA's deterministic structure enables exact matrix-based inference, while probabilistic semantics provide more robust predictions than fuzzy logic approaches.

## Key Results
- Achieves 100% accuracy on synthetic driving benchmark, outperforming FUZZYA (98% in some cases)
- Orders of magnitude faster than DEEP STOCH LOG on the same benchmark
- Shows better generalization on CAVIAR event recognition with lower train-test performance gaps than purely neural approaches
- Demonstrates superior scalability through matrix-based inference and knowledge compilation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Matrix-based inference enables efficient exact inference by reducing computation to standard matrix operations
- **Mechanism**: Computes transition probabilities using knowledge compilation (d-DNNF circuits) for weighted model counting, then uses matrix multiplication for α-recursion
- **Core assumption**: SFA is deterministic and logical transitions can be compiled to tractable representations
- **Evidence anchors**: Abstract states system enables "efficient exact inference"; section shows α-recursion amenable to GPU parallelization

### Mechanism 2
- **Claim**: Probabilistic semantics provide more robust predictions than fuzzy logic
- **Mechanism**: Computes probabilities using proper joint distribution over symbols rather than fuzzy truth values
- **Core assumption**: Problem domain can be modeled using probabilistic semantics
- **Evidence anchors**: Abstract notes "probabilistic semantics can provide significant benefits... over fuzzy logic"; section demonstrates d-DNNF compilation for probability computation

### Mechanism 3
- **Claim**: Combination of automata and propositional logic enables efficient temporal reasoning with static capabilities
- **Mechanism**: SFA combines temporal reasoning through state transitions with static reasoning through logical transitions
- **Core assumption**: Problem can be expressed as temporal patterns and static logical constraints
- **Evidence anchors**: Abstract describes combining "power of automata for temporal reasoning with that of propositional logic for static reasoning"; section explains symbolic transitions

## Foundational Learning

- **Propositional logic and model counting**
  - Why needed: System relies on computing probabilities of logical formulae given uncertain inputs, reducing to weighted model counting
  - Quick check: Given p = [0.8, 0.3, 0.6] for symbols {tired, blocked, fast}, what is P(¬fast ∧ (tired ∨ blocked) | p)?

- **Hidden Markov Models and dynamic programming**
  - Why needed: α-recursion follows same principles as HMM inference, using dynamic programming for efficient probability computation
  - Quick check: If α0 = [1, 0, 0] and T = [[0.14, 0.86, 0], [0.056, 0.344, 0.6], [0, 0, 1]], what is α1?

- **Knowledge compilation and tractable representations**
  - Why needed: System uses knowledge compilation to transform logical formulae into tractable representations (like d-DNNF circuits) for efficient weighted model counting
  - Quick check: What is time complexity of weighted model counting on d-DNNF circuit versus naive approach?

## Architecture Onboarding

- **Component map**: Neural network (fθ) -> Knowledge compiler -> Matrix engine -> SFA definition
- **Critical path**: Input → CNN → Symbol probabilities → KC compilation → Matrix inference → Output probabilities
- **Design tradeoffs**:
  - Deterministic vs non-deterministic SFA: Deterministic enables exact matrix-based inference but limits expressiveness
  - KC representation choice: Different representations (d-DNNF, SDD, etc.) offer different tradeoffs between compilation time and query efficiency
  - Neural architecture: CNN choice affects quality of symbol grounding

- **Failure signatures**:
  - Compilation errors: Indicates issues with logical transitions or KC representation
  - Numerical instability: Occurs if transition probabilities become too small or large
  - Poor accuracy: Might indicate issues with neural symbol grounding or SFA structure

- **First 3 experiments**:
  1. Implement matrix-based inference for simple 2-state SFA on synthetic data with known ground truth
  2. Test knowledge compilation for logical transitions with varying complexity
  3. Compare probabilistic semantics against fuzzy logic on simple temporal pattern recognition task

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does NeSyA's performance scale with complexity of propositional logic transitions in SFA, particularly with disjunctions of many variables?
- **Basis**: Paper mentions SFAs support symbolic transitions and classical automata can be converted with exponential blowup in variables
- **Why unresolved**: Only tests SFAs with up to 5 variables; doesn't explore performance degradation beyond this point
- **What evidence would resolve**: Experiments varying variables in transition formulas (10, 20, 50) while measuring inference time and accuracy

### Open Question 2
- **Question**: Can NeSyA effectively handle partially observable environments where neural network cannot perfectly extract symbolic information?
- **Basis**: Paper assumes "perfect knowledge" in CAVIAR experiments and states "Learning with label noise is beyond scope"
- **Why unresolved**: Current framework assumes clean symbol grounding; real-world applications likely involve noisy observations
- **What evidence would resolve**: Experiments introducing noise in neural network's probability outputs or using real-world datasets with observation noise

### Open Question 3
- **Question**: How does NeSyA compare to purely neural approaches when trained on limited data, particularly trade-off between neural network size and SFA complexity?
- **Basis**: Paper shows NeSyA outperforms purely neural approaches on CAVIAR with limited training data, but Transformer baseline has "order of magnitude more parameters"
- **Why unresolved**: Doesn't systematically explore how different neural network sizes or SFA complexities affect performance
- **What evidence would resolve**: Controlled experiments varying both neural network size and SFA complexity while measuring performance on datasets of varying sizes

## Limitations
- Relies on deterministic SFAs and tractable knowledge compilation, which may limit expressiveness for complex domains
- Scalability claims based on synthetic benchmarks; real-world applications may face different computational constraints
- Transition from theoretical guarantees to practical deployment in noisy, high-dimensional environments remains unproven

## Confidence
- **High Confidence**: Matrix-based inference efficiency claims, synthetic benchmark results (100% accuracy on controlled data)
- **Medium Confidence**: CAVIAR dataset generalization claims, scalability comparisons with DEEP STOCH LOG
- **Low Confidence**: Claims about reinforcement learning extensions, constrained autoregressive applications

## Next Checks
1. **Stress Test Determinism**: Evaluate NeSyA on non-deterministic SFA variants to quantify performance degradation and validate determinism's essential role
2. **Real-World Scalability**: Deploy on larger, noisier datasets (e.g., urban driving with continuous sensor streams) to verify computational claims beyond synthetic benchmarks
3. **Knowledge Compilation Robustness**: Systematically vary logical formula complexity to identify breaking points where compilation becomes intractable or inference fails