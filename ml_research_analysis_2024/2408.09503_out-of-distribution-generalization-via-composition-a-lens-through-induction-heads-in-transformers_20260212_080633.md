---
ver: rpa2
title: 'Out-of-distribution generalization via composition: a lens through induction
  heads in Transformers'
arxiv_id: '2408.09503'
source_url: https://arxiv.org/abs/2408.09503
tags:
- attention
- arxiv
- heads
- figure
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how large language models (LLMs) achieve
  out-of-distribution (OOD) generalization, focusing on the role of induction heads
  and compositional structures. The authors propose the "common bridge representation
  hypothesis," suggesting that a shared latent subspace in the embedding space acts
  as a bridge for composition by aligning early and later layers in Transformers.
---

# Out-of-distribution generalization via composition: a lens through induction heads in Transformers

## Quick Facts
- arXiv ID: 2408.09503
- Source URL: https://arxiv.org/abs/2408.09503
- Authors: Jiajun Song; Zhuoyan Xu; Yiqiao Zhong
- Reference count: 40
- The paper proposes that out-of-distribution generalization in Transformers is achieved through compositional structures where early and later layers align via a shared latent subspace.

## Executive Summary
This paper investigates how large language models achieve out-of-distribution generalization, focusing on the role of induction heads and compositional structures. The authors propose the "common bridge representation hypothesis," suggesting that a shared latent subspace in the embedding space acts as a bridge for composition by aligning early and later layers in Transformers. Through experiments on synthetic data and various LLMs, they demonstrate that OOD generalization is tied to composition, with induction heads playing a crucial role in tasks requiring rule inference without fine-tuning.

## Method Summary
The paper employs a combination of synthetic data experiments and analysis of pretrained LLMs to study OOD generalization. The synthetic experiments use a minimal 2-layer Transformer trained on copying tasks with repetition patterns, while real-world experiments analyze pretrained models on in-context learning and mathematical reasoning tasks. The methodology includes measuring training dynamics, attention patterns, and compositional structure through subspace matching scores and induction head analysis.

## Key Results
- OOD generalization and composition are intrinsically tied - models learn rules by composing two self-attention layers
- Sharp transitions in generalization occur when structural matching emerges between layers during training
- Subspace matching between circuits in different layers is a pervasive mechanism for compositions, with a global latent subspace connecting relevant attention heads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Out-of-distribution (OOD) generalization in Transformers is tied to compositional structures where early and later layers align through a shared latent subspace.
- Mechanism: A shared latent subspace in the embedding space acts as a "bridge" for composition by aligning outputs from earlier layers (OV circuits) with inputs to later layers (QK circuits). This alignment allows the model to generalize beyond the training distribution.
- Core assumption: The linear representation hypothesis holds—concepts are encoded as linear subspaces in the embedding space.
- Evidence anchors:
  - [abstract] "Furthermore, a shared latent subspace in the embedding (or feature) space acts as a bridge for composition by aligning early layers and later layers, which we refer to as the common bridge representation hypothesis."
  - [section] "The CBR hypothesis is an extension of linear representation hypothesis to compositional tasks... For a compositional task, there exists a low-dimensional subspace V ⊂ Rd such that V = span(WOV,j) = span(W⊤QK,k)."
  - [corpus] Weak: No direct citations found in corpus for "common bridge representation hypothesis."

### Mechanism 2
- Claim: Induction heads (IHs) are crucial components for OOD generalization and language reasoning tasks.
- Mechanism: IHs attend to the token next to an identical token in input strings with repetition patterns, enabling the model to learn and apply rules for tasks like copying and symbolic reasoning.
- Core assumption: IHs can be reliably identified and their removal will significantly impact model performance on OOD tasks.
- Evidence anchors:
  - [abstract] "We empirically examined the training dynamics of Transformers on a synthetic example and conducted extensive experiments on a variety of pretrained LLMs, focusing on a type of components known as induction heads."
  - [section] "We found that OOD generalization and composition are tied together—models can learn rules by composing two self-attention layers, thereby achieving OOD generalization."
  - [corpus] Moderate: Papers like "From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers" discuss induction heads but not specifically in the context of OOD generalization.

### Mechanism 3
- Claim: Sharp transitions in generalization occur when structural matching emerges between layers during training.
- Mechanism: During training, the model's layers undergo phases where they first learn simple statistics (weak learning phase) and then learn the underlying rules (rule-learning phase), marked by sudden improvements in both in-distribution and out-of-distribution performance.
- Core assumption: The training dynamics exhibit abrupt changes in error rates corresponding to the emergence of compositional structures.
- Evidence anchors:
  - [abstract] "In Figure 1, we observe that the 2-layer Transformer experiences two phases... In the rule-learning phase, the model learns the copying rule and generalize reasonably well on both ID and OOD data."
  - [section] "OOD generalization is accompanied by abrupt emergence of subspace matching... The top row of Figure 2 shows that the sharp transition of generalization occurs at training steps around 2000."
  - [corpus] Weak: No direct citations found in corpus for "sharp transitions in generalization."

## Foundational Learning

- Concept: Linear Algebra (vector spaces, subspaces, singular value decomposition)
  - Why needed here: Understanding how subspaces align and how singular values indicate the strength of this alignment is crucial for grasping the common bridge representation hypothesis.
  - Quick check question: If U and V are orthonormal matrices, what does σmax(U⊤V) represent?

- Concept: Neural Network Architectures (Transformers, attention mechanisms)
  - Why needed here: Familiarity with how Transformers process sequences and how attention heads function is necessary to understand the role of induction heads and compositional structures.
  - Quick check question: What is the purpose of the softmax operation in the attention mechanism?

- Concept: Statistical Learning (generalization, overfitting, underfitting)
  - Why needed here: Understanding the difference between memorizing patterns and learning underlying rules is key to interpreting the phases of learning observed in the experiments.
  - Quick check question: How does a model's performance on training data differ from its performance on out-of-distribution data if it has overfit?

## Architecture Onboarding

- Component map: Input Embeddings -> Transformer Layers (Multi-Head Attention -> Feed-Forward) -> Output Layer
- Critical path: Input → Token and Positional Embeddings → Transformer Layers (Attention → FFN) → Output
- Design tradeoffs:
  - Depth vs. Width: More layers may capture complex patterns but increase computational cost; more heads per layer allow parallel processing but add parameters.
  - Positional Embeddings: Absolute vs. Rotary Positional Embedding affects how position information is encoded.
- Failure signatures:
  - Poor OOD performance: Model may be relying on simple statistics rather than compositional structures.
  - Inability to learn rules: May indicate insufficient model capacity or inappropriate training data.
- First 3 experiments:
  1. Train a minimal 2-layer Transformer on the copying task; observe error rates over training steps.
  2. Remove top-scoring induction heads and measure impact on OOD tasks like IOI and ICL.
  3. Apply weight projections to attention heads and assess changes in prediction accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic copying task represents a simplified scenario compared to real-world OOD challenges.
- The analysis focuses primarily on attention mechanisms, potentially overlooking contributions from MLPs and other architectural components.
- The definition and identification of induction heads may not capture all relevant compositional structures.

## Confidence
- **High Confidence:** The empirical observations of sharp transitions in training dynamics and the role of induction heads in copying tasks are well-supported by experimental results.
- **Medium Confidence:** The extension of these findings to more complex OOD tasks (IOI, ICL, GSM8K) is plausible but requires additional mechanisms beyond pure composition.
- **Low Confidence:** The claim that a single global latent subspace universally connects all relevant attention heads across different architectures and tasks needs more validation across diverse model families.

## Next Checks
1. Cross-architecture validation: Test the common bridge representation hypothesis across diverse Transformer variants to assess universality.
2. Ablation of non-attention components: Systematically remove or disable MLPs to quantify their contribution to OOD generalization.
3. Temporal dynamics analysis: Track how the shared latent subspace evolves during extended training and fine-tuning to understand its stability and adaptability.