---
ver: rpa2
title: 'DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer''s Disease
  Questions with Scientific Literature'
arxiv_id: '2405.04819'
source_url: https://arxiv.org/abs/2405.04819
tags:
- knowledge
- llms
- alzheimer
- disease
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes DALK, a framework that dynamically co-augment\
  \ LLMs and knowledge graphs to improve Alzheimer\u2019s disease question answering.\
  \ It first uses LLMs to extract structural knowledge from scientific literature\
  \ to build an evolving AD-specific KG, then samples and re-ranks relevant triples\
  \ via a coarse-to-fine method and self-aware retrieval to enhance LLM reasoning."
---

# DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature

## Quick Facts
- arXiv ID: 2405.04819
- Source URL: https://arxiv.org/abs/2405.04819
- Reference count: 20
- Outperforms fine-tuned biomedical LLMs and retrieval-augmented baselines with 72.6% accuracy on ADQA benchmark

## Executive Summary
This paper introduces DALK, a framework that dynamically co-augment large language models (LLMs) and knowledge graphs (KGs) to answer Alzheimer's Disease (AD) questions using scientific literature. The method constructs an evolving AD-specific KG from scientific articles using LLM-based relation extraction, then employs coarse-to-fine sampling with self-aware retrieval to select relevant knowledge triples for LLM inference. Experiments on a new ADQA benchmark show DALK achieves 72.6% accuracy, outperforming fine-tuned biomedical LLMs and retrieval-augmented baselines. The framework addresses challenges in data quality, efficiency, and domain specificity for biomedical LLM applications.

## Method Summary
DALK constructs an AD-specific KG from scientific literature using LLM-based entity recognition and relation extraction, then applies coarse-to-fine sampling with self-aware retrieval to select relevant triples for LLM inference. The framework uses pair-wise and generative relation extraction methods to build two KGs (KGpair and KGgen), samples subgraphs from these KGs, and employs GPT-3.5-turbo with a self-aware knowledge retrieval prompt to rerank and select top-k triples for answering AD questions. The method is evaluated on ADQA, a benchmark derived from medical school exam questions filtered for AD relevance.

## Key Results
- DALK achieves 72.6% accuracy on ADQA benchmark, outperforming fine-tuned biomedical LLMs and retrieval-augmented baselines
- Self-aware knowledge retrieval and generative KG construction show significant contributions in ablation studies
- Model performance improves with KG size, and retrieval quality correlates with query length
- Co-evolution of LLMs and KG demonstrates upward performance trajectory as KG expands

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based knowledge graph construction reduces retrieval noise by encoding domain-specific relationships into a structured format.
- Mechanism: LLMs extract entities and relations from dense scientific literature and represent them as triples, which can be selectively sampled during inference to improve answer precision.
- Core assumption: Scientific literature contains sufficient entity co-occurrence signals for LLMs to infer correct relations.
- Evidence anchors:
  - [abstract] "We first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature"
  - [section 3.1] "To build an accurate and high-quality knowledge graph on AD, we aim to assign a specific relation type between the two related entities"
  - [corpus] corpus evidence weak — only described as "dense and information-overloaded"
- Break condition: If entity co-occurrence does not reliably indicate true domain relations, the KG will contain incorrect triples.

### Mechanism 2
- Claim: Coarse-to-fine sampling with self-aware retrieval improves knowledge relevance by filtering irrelevant triples before LLM inference.
- Mechanism: Initial subgraph sampling collects candidate triples, then LLM reranking selects top-k triples based on semantic relevance to the question.
- Core assumption: LLM can accurately rank triples for a given query without external supervision.
- Evidence anchors:
  - [abstract] "we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach"
  - [section 3.2.2] "we directly prompt the LLM to rerank the sampled knowledge and only retrieve top k triples"
  - [corpus] corpus evidence weak — mechanism assumed to work based on LLM ranking ability only
- Break condition: If the LLM's ranking ability is insufficient, irrelevant triples will still pass through.

### Mechanism 3
- Claim: Dynamic co-augmentation of LLM and KG enables mutual improvement over time as the KG grows.
- Mechanism: KG grows with new literature each year, and LLM performance improves with more precise KG samples, enabling iterative refinement.
- Core assumption: KG growth correlates with improved retrieval precision for domain queries.
- Evidence anchors:
  - [abstract] "Dynamic Co-Augmentation of LLMs and KG" and "an evolving AD-specific knowledge graph"
  - [section 5.1] "our framework effectively fosters the co-evolution of LLMs and KG, with the performance of KG-augmented LLMs exhibiting a generally upward trajectory as the KG expands"
  - [corpus] corpus evidence weak — performance correlation described but not deeply analyzed
- Break condition: If KG growth introduces noise faster than it improves coverage, co-augmentation degrades.

## Foundational Learning

- Concept: Entity recognition and relation extraction
  - Why needed here: AD domain requires accurate identification of biomedical entities and their relationships from unstructured literature.
  - Quick check question: What is the difference between pair-wise and generative relation extraction methods?
- Concept: Knowledge graph construction from text
  - Why needed here: Structured KG enables efficient knowledge retrieval for specialized domain QA.
  - Quick check question: Why might generative relation extraction produce smaller but more accurate KGs than pair-wise methods?
- Concept: Coarse-to-fine knowledge sampling
  - Why needed here: Initial broad sampling captures context, fine-grained selection ensures only relevant knowledge reaches the LLM.
  - Quick check question: What is the role of the self-aware retrieval module in the sampling pipeline?

## Architecture Onboarding

- Component map: Corpus ingestion -> Entity recognition (PubTator) -> Relation extraction (pair-wise + generative) -> KG construction -> Subgraph sampling -> Self-aware retrieval -> LLM inference
- Critical path: Corpus -> KG -> Sampling -> Retrieval -> Inference
- Design tradeoffs:
  - Larger KG: more coverage but more noise
  - Larger k in retrieval: more context but risk of irrelevant triples
  - Entity granularity: fine-grained entities improve precision but reduce recall
- Failure signatures:
  - Low accuracy despite KG availability -> subgraph sampling or retrieval failing
  - Stable accuracy despite KG growth -> retrieval module not leveraging new knowledge
  - Degraded accuracy with more triples -> noise injection in KG construction
- First 3 experiments:
  1. Compare pair-wise vs generative KG construction accuracy on a small subset.
  2. Measure impact of varying k (1, 3, 5, 10) on retrieval quality.
  3. Test whether adding self-aware retrieval improves accuracy on long vs short queries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DALK change when using different entity extraction methods, such as LLMs, instead of relying solely on PubTator?
- Basis in paper: [inferred] The paper mentions that while PubTator is used for entity recognition, LLMs also exhibit promising entity extraction capabilities. It suggests refining entity extraction methods with LLMs for future work.
- Why unresolved: The current study does not explore the impact of different entity extraction methods on the performance of DALK. It would be interesting to see if using LLMs for entity recognition could further improve the accuracy of the knowledge graph and, consequently, the performance of DALK.
- What evidence would resolve it: Conducting experiments with DALK using different entity extraction methods, such as LLMs, and comparing the performance metrics (e.g., accuracy, F1-score) with the current approach using PubTator would provide insights into the effectiveness of alternative entity extraction methods.

### Open Question 2
- Question: How does the domain gap between the medical school exam questions in ADQA and the scientific literature in AD-KG affect the performance of DALK?
- Basis in paper: [inferred] The paper acknowledges that the datasets used to construct ADQA primarily consist of medical school exam questions, which may exhibit a domain gap from the scientific literature informing AD-KG. It suggests leveraging PubMedQA to address this issue, but notes the limited data amount as a hindrance.
- Why unresolved: The current study does not investigate the impact of the domain gap between ADQA and AD-KG on the performance of DALK. Understanding this relationship could provide insights into the generalizability of DALK to other biomedical domains.
- What evidence would resolve it: Conducting experiments with DALK using a benchmark that has a smaller domain gap with AD-KG, such as PubMedQA, and comparing the performance metrics with ADQA would shed light on the importance of domain alignment in the effectiveness of DALK.

### Open Question 3
- Question: How does the performance of DALK vary with different values of the retrieval number k in the self-aware knowledge retrieval module?
- Basis in paper: [explicit] The paper presents a hyper-parameter analysis on the retrieval number k, showing that the best k value is correlated with the length of queries in each dataset. It finds that the best performance in MedQA (average query length is 107.4) shows up when k = 10, while the best performance in MedMCQA and QA4MRE shows up when k = 5 and 3, respectively.
- Why unresolved: While the paper provides insights into the relationship between k and query length, it does not explore the optimal k value for different types of queries or the impact of k on the overall performance of DALK.
- What evidence would resolve it: Conducting experiments with DALK using different values of k for various query types and analyzing the performance metrics (e.g., accuracy, F1-score) would help determine the optimal k value for different scenarios and its impact on the overall effectiveness of DALK.

## Limitations
- Evaluation relies on a constructed ADQA benchmark derived from existing medical QA datasets rather than a dedicated, independently validated Alzheimer's disease question answering dataset
- Knowledge graph construction depends heavily on LLM-generated relations from co-occurring entities, but the corpus evidence suggests the scientific literature is "dense and information-overloaded," which may lead to false relation extraction
- The framework's computational efficiency is not explicitly evaluated, despite claims of addressing efficiency challenges in biomedical LLM applications

## Confidence

- **High confidence**: The comparative performance advantage over fine-tuned biomedical LLMs and retrieval-augmented baselines on the constructed ADQA benchmark
- **Medium confidence**: The mechanism of dynamic co-augmentation showing performance improvement with KG growth, as the correlation is demonstrated but not deeply analyzed for causation
- **Low confidence**: The effectiveness of the self-aware retrieval module in filtering irrelevant triples, as the corpus evidence is weak and relies on LLM ranking ability without external validation

## Next Checks
1. **Independent KG Quality Assessment**: Conduct human evaluation of a random sample of KG triples to measure precision and recall against gold-standard biomedical knowledge bases, specifically testing the claim that generative relation extraction produces "more accurate" triples
2. **Retrieval Module Ablation**: Systematically test the self-aware retrieval component by comparing performance with and without it across different query types (short vs. long) to validate its claimed effectiveness in filtering noise
3. **Benchmark Validation**: Have domain experts independently evaluate the ADQA benchmark questions for relevance, difficulty, and potential biases introduced during the filtering process to ensure it represents a valid test of Alzheimer's disease question answering