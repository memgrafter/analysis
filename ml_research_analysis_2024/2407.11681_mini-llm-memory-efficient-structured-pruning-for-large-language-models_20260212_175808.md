---
ver: rpa2
title: 'MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models'
arxiv_id: '2407.11681'
source_url: https://arxiv.org/abs/2407.11681
tags:
- pruning
- mini-llm
- performance
- llm-pruner
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of structured pruning for large
  language models (LLMs) while overcoming the high GPU memory requirements associated
  with backpropagation-based gradient computation. The authors propose MINI-LLM, a
  memory-efficient structured pruning framework that uses a novel Feature Map Sensitivity
  (FMS) score integrating magnitude, activation, and estimated gradients.
---

# MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models

## Quick Facts
- arXiv ID: 2407.11681
- Source URL: https://arxiv.org/abs/2407.11681
- Authors: Hongrong Cheng; Miao Zhang; Javen Qinfeng Shi
- Reference count: 10
- Key outcome: Memory-efficient structured pruning framework using forward-only gradient estimation that matches or exceeds backpropagation methods in performance while using similar GPU memory to gradient-free approaches

## Executive Summary
MINI-LLM addresses the challenge of structured pruning for large language models (LLMs) by introducing a memory-efficient framework that estimates gradients using only forward passes. The authors propose a novel Feature Map Sensitivity (FMS) score that integrates weight magnitude, activation patterns, and gradient information to guide pruning decisions. By employing Zeroth-Order optimization with Simultaneous Perturbation Stochastic Approximation (SPSA), MINI-LLM achieves comparable performance to backpropagation-based methods while maintaining the memory efficiency of gradient-free approaches. Experimental results on LLaMA, BLOOM, and OPT models demonstrate consistent improvements across classification, multiple-choice, and generation tasks.

## Method Summary
MINI-LLM is a structured pruning framework that uses a hybrid pruning criterion called Feature Map Sensitivity (FMS) score, which integrates weight magnitude, activation patterns, and estimated gradients. Instead of backpropagation, it employs Zeroth-Order optimization with forward passes to estimate gradients, significantly reducing memory usage. The framework performs one-shot structured pruning by grouping interdependent weights and pruning entire groups based on maximum sensitivity scores, followed by LoRA-based fine-tuning for performance recovery.

## Key Results
- Outperforms gradient-free methods across all tested LLMs and pruning ratios
- Matches or exceeds backpropagation-based methods in performance metrics
- Maintains comparable GPU memory usage to gradient-free approaches
- Achieves 67.57% average classification accuracy on LLaMA-7B with 20% pruning vs 66.16% for best gradient-free method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature Map Sensitivity (FMS) integrates magnitude, activation, and gradient information to improve pruning accuracy.
- Mechanism: FMS score combines three components—weight magnitude, activation patterns, and gradient information—to provide a more nuanced assessment of feature map sensitivity during pruning. This hybrid scoring captures both local layer output changes and global loss sensitivity.
- Core assumption: The combination of magnitude, activation, and gradient information provides better importance scoring than any single factor alone.
- Evidence anchors:
  - [abstract]: "devise a hybrid pruning criterion, which appropriately integrates magnitude, activation, and gradient to capitalize on feature map sensitivity"
  - [section]: "We notice that, with the weight gradients, the global loss change can be quantified... Our criterion Eq. (6) integrates magnitude, activation, and gradient to optimally utilize the pivotal information from the three critical aspects"
  - [corpus]: No direct evidence found in related papers; MINI-LLM appears to be the first to explicitly combine all three factors in this way.
- Break condition: If any of the three components (magnitude, activation, or gradient) becomes unreliable or misleading, the FMS score could degrade in quality.

### Mechanism 2
- Claim: Zeroth-Order optimization with forward passes estimates gradients without backpropagation memory overhead.
- Mechanism: Uses Simultaneous Perturbation Stochastic Approximation (SPSA) to estimate gradients by computing loss differences with perturbed parameters in only forward passes, requiring minimal additional memory compared to gradient-free methods.
- Core assumption: The SPSA-based gradient estimation is sufficiently accurate for pruning decisions while using dramatically less memory than backpropagation.
- Evidence anchors:
  - [abstract]: "estimate gradients using only forward passes" and "utilizes estimated gradients with only forward passes by using comparable GPU memory usage to gradient-free methods"
  - [section]: "We propose a structured pruning framework for LLMs called MINI-LLM which utilizes estimated gradients with only forward passes by using comparable GPU memory usage to gradient-free methods"
  - [corpus]: "Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes" uses similar forward-only approach, suggesting this is a viable technique.
- Break condition: If the gradient estimation becomes too noisy or inaccurate, pruning decisions could become unreliable, negating the memory efficiency gains.

### Mechanism 3
- Claim: Dependency-aware structured pruning maintains model integrity while removing critical structures.
- Mechanism: Groups interdependent weights (like channels and attention heads) and prunes entire groups based on maximum sensitivity score within each group, preserving structural integrity.
- Core assumption: The maximum sensitivity score within a group adequately represents the importance of the entire interdependent structure.
- Evidence anchors:
  - [section]: "To maintain structural integrity, it is crucial for structured pruning to identify groups of interdependent structures... We arrange the interconnected weights into groups and determine the sensitivity of each group... defined as G = {Wi}M i=1 by choosing the maximum sensitivity score of the structures in it"
  - [corpus]: No direct evidence found in related papers; this appears to be a standard approach in structured pruning literature.
- Break condition: If the grouping strategy doesn't capture true dependencies, pruning entire groups could remove important components or keep unimportant ones.

## Foundational Learning

- Concept: Gradient-based pruning importance scoring
  - Why needed here: Understanding why gradients are valuable for pruning decisions is crucial to appreciate why MINI-LLM's approach matters
  - Quick check question: Why are gradients typically more reliable than magnitude-based methods for pruning?

- Concept: Zeroth-Order optimization and SPSA
  - Why needed here: The core innovation of MINI-LLM relies on using ZO optimization to estimate gradients without backpropagation
  - Quick check question: How does SPSA estimate gradients using only forward passes?

- Concept: Structured vs unstructured pruning
  - Why needed here: MINI-LLM focuses on structured pruning (removing entire channels/heads) rather than unstructured pruning (removing individual weights)
  - Quick check question: What are the advantages of structured pruning over unstructured pruning for deployment?

## Architecture Onboarding

- Component map:
  - Input pipeline: Calibration data for gradient estimation, downstream task data for evaluation
  - Scoring engine: FMS calculation combining magnitude, activation, and estimated gradients
  - Pruning module: Group-based structured pruning with dependency awareness
  - Recovery phase: LoRA-based fine-tuning for performance restoration
  - Evaluation: Zero-shot and few-shot task performance metrics

- Critical path: Calibration data → Gradient estimation (forward passes) → FMS score computation → Group pruning → LoRA fine-tuning → Performance evaluation

- Design tradeoffs:
  - Memory vs accuracy: Using estimated gradients saves memory but may sacrifice some precision compared to true backpropagation gradients
  - One-shot vs iterative pruning: MINI-LLM uses one-shot pruning for efficiency but may miss opportunities for refinement
  - Group sensitivity: Using maximum sensitivity within groups is simple but may not capture nuanced importance patterns

- Failure signatures:
  - High perplexity on WikiText2/PTB after pruning indicates excessive pruning or poor group selection
  - Large performance gap between zero-shot and few-shot tasks suggests loss of general capability
  - Similar performance to magnitude-only methods indicates gradient estimation may be ineffective

- First 3 experiments:
  1. Implement basic FMS scoring without gradient estimation (use dummy gradients) to verify the magnitude+activation component works
  2. Add SPSA-based gradient estimation and verify it produces reasonable values compared to a small backpropagation run
  3. Implement group-based structured pruning and test on a small toy model to verify dependency handling works correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Feature Map Sensitivity (FMS) score compare to other gradient-based pruning methods in terms of performance and memory efficiency for different types of LLMs and tasks?
- Basis in paper: [explicit] The paper proposes a novel pruning criterion called Feature Map Sensitivity (FMS) score, which integrates magnitude, activation, and gradient to evaluate feature map sensitivity. The paper also claims that MINI-LLM outperforms existing gradient-free methods and rivals or surpasses backpropagation gradient-based methods in performance while using similar GPU memory as gradient-free methods.
- Why unresolved: The paper only provides experimental results for a limited set of LLMs (LLaMA, BLOOM, and OPT) and tasks (classification, multiple-choice, and generation). It is unclear how the FMS score would perform for other types of LLMs and tasks.
- What evidence would resolve it: Additional experimental results showing the performance and memory efficiency of MINI-LLM with the FMS score for a wider range of LLMs and tasks.

### Open Question 2
- Question: What is the impact of different pruning ratios on the performance of MINI-LLM for different types of LLMs and tasks?
- Basis in paper: [explicit] The paper evaluates the performance of MINI-LLM at different pruning ratios (20%, 30%, 40%, and 50%) for LLaMA-7B and BLOOM-7B. It is shown that MINI-LLM consistently outperforms gradient-free methods and closely matches or even outperforms LLM-Pruner with backpropagation gradients across these pruning ratios.
- Why unresolved: The paper only provides results for a limited set of pruning ratios. It is unclear how the performance of MINI-LLM would vary for other pruning ratios or for different types of LLMs and tasks.
- What evidence would resolve it: Additional experimental results showing the performance of MINI-LLM at different pruning ratios for a wider range of LLMs and tasks.

### Open Question 3
- Question: How does the memory efficiency of MINI-LLM compare to other gradient-based pruning methods in terms of the number of parameters and MACs?
- Basis in paper: [explicit] The paper claims that MINI-LLM uses similar GPU memory as gradient-free methods while outperforming them in performance. However, it does not provide a direct comparison of the number of parameters and MACs for MINI-LLM and other gradient-based pruning methods.
- Why unresolved: The paper does not provide a direct comparison of the number of parameters and MACs for MINI-LLM and other gradient-based pruning methods.
- What evidence would resolve it: A comparison of the number of parameters and MACs for MINI-LLM and other gradient-based pruning methods.

## Limitations
- Limited ablation analysis showing the individual contribution of each FMS component
- No quantitative comparison of gradient estimation quality against true backpropagation gradients
- Lack of detailed memory profiling and absolute memory consumption numbers

## Confidence
**High Confidence**: 
- MINI-LLM achieves better performance than pure gradient-free methods (magnitude and activation only)
- MINI-LLM maintains comparable GPU memory usage to gradient-free methods
- The hybrid FMS scoring approach is novel and theoretically sound

**Medium Confidence**: 
- MINI-LLM matches or exceeds backpropagation-based methods in performance
- The claimed memory efficiency is significant and practical
- The recovery fine-tuning effectively restores pruned model performance

**Low Confidence**: 
- The three-component FMS integration is strictly necessary for optimal performance
- The gradient estimation quality is sufficient for all pruning scenarios
- The memory savings scale proportionally with model size

## Next Checks
1. Implement ablation studies comparing FMS performance with and without each component (magnitude-only, activation-only, magnitude+activation, full FMS). This would verify whether the gradient component truly provides significant value beyond the other two.

2. Profile actual GPU memory consumption during MINI-LLM pruning compared to both gradient-free and backpropagation-based baselines. Include peak memory usage during gradient estimation, FMS computation, and pruning operations.

3. Conduct sensitivity analysis on SPSA perturbation parameters (ϵ, number of iterations) to determine how estimation quality affects final pruning performance. Test with varying levels of gradient noise to establish robustness boundaries.