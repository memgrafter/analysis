---
ver: rpa2
title: COGNET-MD, an evaluation framework and dataset for Large Language Model benchmarks
  in the medical domain
arxiv_id: '2405.10893'
source_url: https://arxiv.org/abs/2405.10893
tags:
- medical
- correct
- cognet-md
- evaluation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces COGNET-MD, a novel benchmark and dataset
  for evaluating Large Language Models (LLMs) in the medical domain. COGNET-MD addresses
  the lack of independent, ready-to-use datasets for LLM evaluation in medicine by
  providing a structured framework with varying difficulty levels and a database of
  542 Multiple Choice Questions (MCQs) across five medical specialties: Psychiatry,
  Dentistry, Pulmonology, Dermatology, and Endocrinology.'
---

# COGNET-MD, an evaluation framework and dataset for Large Language Model benchmarks in the medical domain

## Quick Facts
- arXiv ID: 2405.10893
- Source URL: https://arxiv.org/abs/2405.10893
- Reference count: 7
- Primary result: Introduces COGNET-MD, a benchmark and dataset for evaluating LLMs in medicine with 542 MCQs across 5 specialties and a partial credit/penalty scoring framework

## Executive Summary
COGNET-MD addresses the critical gap in independent, ready-to-use datasets for evaluating Large Language Models in medical domains. The framework provides a structured benchmark with varying difficulty levels and a comprehensive database of 542 Multiple Choice Questions across five medical specialties: Psychiatry, Dentistry, Pulmonology, Dermatology, and Endocrinology. Developed in collaboration with medical experts, the dataset ensures alignment with current medical knowledge and trends. The scoring framework incorporates partial credit, full credit, and penalties for incorrect answers, enabling nuanced assessment of LLM performance across different use cases from specialty-specific evaluations to full dataset analysis.

## Method Summary
COGNET-MD is a benchmark framework and dataset designed to evaluate LLM performance in medical domains. The core methodology involves a structured dataset of 542 expert-curated MCQs distributed across five medical specialties, with questions categorized by difficulty levels. The evaluation uses a scoring algorithm that implements partial credit (0.5 points for at least one correct answer), full credit (1 point for all correct answers), and penalties (-0.5 points per incorrect answer). Users can select from three use cases: Specialty (single specialty subset), Beta (50% of questions from each specialty), or Production (full dataset). The framework requires explicit declaration of model parameters and follows established conduct rules for valid scoring. All code and data are publicly accessible via HuggingFace.

## Key Results
- Introduces COGNET-MD with 542 MCQs across five medical specialties (Psychiatry, Dentistry, Pulmonology, Dermatology, Endocrinology)
- Implements a nuanced scoring framework with partial credit (0.5), full credit (1), and penalties (-0.5) for incorrect answers
- Provides three scalable use cases: Specialty (single specialty), Beta (50% per specialty), and Production (full dataset)
- Makes benchmark publicly available on HuggingFace with clear conduct rules and leaderboard system

## Why This Works (Mechanism)

### Mechanism 1
- Claim: COGNET-MD enables accurate medical LLM evaluation by providing a structured, expert-curated dataset with partial credit and penalty scoring.
- Mechanism: The framework defines difficulty tiers and includes domain-specific MCQs vetted by medical professionals, allowing nuanced assessment of both recall and reasoning.
- Core assumption: Multiple-choice format with partial credit captures medical reasoning better than binary scoring.
- Evidence anchors:
  - [abstract] "propose a scoring-framework with increased difficulty to assess the ability of LLMs in interpreting medical text."
  - [section] "The scoring for each use case is the same and includes the following... Partial Credit: At least one correct answer equals to a half point - 0.5."
  - [corpus] "Fact or Guesswork? Evaluating Large Language Models' Medical Knowledge with Structured One-Hop Judgments" supports structured evaluation need.
- Break condition: If MCQs are too simplistic or domain experts are not representative, scoring granularity may not reflect true medical reasoning.

### Mechanism 2
- Claim: Varying difficulty use cases (Specialty, Beta, Production) allow scalable evaluation from focused to comprehensive.
- Mechanism: By letting evaluators choose subset or full dataset, the framework accommodates different resource and precision requirements.
- Core assumption: Difficulty tiers map meaningfully to real-world medical AI deployment contexts.
- Evidence anchors:
  - [section] "In figure 1, the benchmark use cases are presented as Specialty where one medical Specialisation is chosen, Beta where 50% per specialty of total dataset is chosen with all included medical specialties and Production where all Dataset is used."
  - [corpus] "MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations" indicates demand for multi-level benchmarks.
- Break condition: If difficulty labels do not align with actual question complexity, scaling utility breaks down.

### Mechanism 3
- Claim: Open access via HuggingFace and clear conduct rules enable reproducible, comparable LLM evaluation.
- Mechanism: Public dataset and benchmark card standardize inputs and evaluation conditions across teams.
- Core assumption: Transparency and standardization outweigh potential data misuse risks in medical AI.
- Evidence anchors:
  - [section] "Code and data are publicly available on HuggingFace, facilitating broader adoption and contribution."
  - [section] "For a score to be valid and be added in the COGNET-MDâ€™s leader-boards the developers, should clearly state model used..."
  - [corpus] "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation" shows trend toward reproducible medical LLM benchmarks.
- Break condition: If HuggingFace dataset is altered without version control, reproducibility fails.

## Foundational Learning

- Concept: Multiple-choice question design in medical education
  - Why needed here: COGNET-MD relies on MCQs to test LLM knowledge and reasoning.
  - Quick check question: What distinguishes a well-constructed medical MCQ from a poorly constructed one?

- Concept: Scoring rubrics with partial credit and penalties
  - Why needed here: Scoring framework directly affects evaluation granularity and fairness.
  - Quick check question: How does partial credit influence model behavior compared to all-or-nothing scoring?

- Concept: Medical specialty knowledge domains
  - Why needed here: Dataset spans five specialties; understanding them is key to evaluating use case relevance.
  - Quick check question: Why might dermatology MCQs differ fundamentally in structure from psychiatry MCQs?

## Architecture Onboarding

- Component map: Dataset (542 MCQs) -> Scoring algorithm -> Use case selector -> Leaderboard system
- Critical path: Load dataset -> Apply scoring rules -> Generate score -> Validate against rules -> Record leaderboard entry
- Design tradeoffs: Broad specialty coverage vs. depth in each specialty; open access vs. data security; partial credit granularity vs. evaluation simplicity
- Failure signatures: Scores that violate stated rules (e.g., full credit without all correct answers), inconsistent difficulty labels, missing metadata
- First 3 experiments:
  1. Run scoring algorithm on a single specialty subset with known correct/incorrect answers to validate partial credit and penalty logic.
  2. Evaluate a small set of MCQs manually to confirm difficulty labels match perceived complexity.
  3. Compare LLM scores across Specialty, Beta, and Production use cases to verify scaling behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on COGNET-MD compare to human medical professionals across different medical specialties and difficulty levels?
- Basis in paper: [explicit] The paper introduces COGNET-MD as a benchmark for evaluating LLM performance in medical domains, but does not provide comparative results with human performance.
- Why unresolved: The paper focuses on presenting the benchmark framework and dataset, without including empirical results comparing LLM performance to human medical experts.
- What evidence would resolve it: Empirical studies comparing LLM performance on COGNET-MD to human medical professionals' performance across the five included specialties (Psychiatry, Dentistry, Pulmonology, Dermatology, and Endocrinology) and different difficulty levels.

### Open Question 2
- Question: How does the inclusion of domain-specific prompting (e.g., stating the specialty in the prompt) affect LLM performance on COGNET-MD?
- Basis in paper: [explicit] The paper mentions that stating the domain in the prompt "has shown to slightly increase accuracy" but does not provide quantitative data on the magnitude of this effect.
- Why unresolved: The paper only briefly mentions this phenomenon without providing specific results or quantifying the impact of domain-specific prompting on LLM performance.
- What evidence would resolve it: Empirical results comparing LLM performance with and without domain-specific prompting across different use cases (Specialty, Beta, Production) in COGNET-MD.

### Open Question 3
- Question: How will the continuous expansion of COGNET-MD to include additional medical domains affect its effectiveness as a benchmark for LLM evaluation?
- Basis in paper: [explicit] The paper states that the dataset will be "continuously extended and expanded to include additional medical domains" but does not discuss the potential impact on benchmark effectiveness.
- Why unresolved: The paper introduces the initial version of COGNET-MD without analyzing how future expansions might influence its utility as an evaluation tool for LLMs in medical domains.
- What evidence would resolve it: Longitudinal studies tracking the effectiveness of COGNET-MD as a benchmark before and after expansions to include new medical domains, including analysis of changes in LLM performance distributions and benchmark discriminative power.

## Limitations
- The 542-question dataset size may limit statistical power for robust performance differentiation between models across specialties.
- Expert consensus and inter-rater reliability for question difficulty labeling are not validated, raising concerns about consistency.
- Partial credit and penalty scoring, while innovative, may not fully capture complex clinical reasoning required in real medical scenarios.

## Confidence
- **High Confidence**: The claim that COGNET-MD provides an open, structured benchmark with expert-curated MCQs is well-supported by the described methodology and public availability on HuggingFace.
- **Medium Confidence**: The assertion that the partial credit and penalty scoring system enables more nuanced evaluation than binary scoring is reasonable but not empirically validated against alternative scoring methods in this paper.
- **Low Confidence**: The claim that difficulty tiers meaningfully map to real-world medical AI deployment contexts lacks empirical evidence showing correlation between benchmark difficulty levels and actual clinical decision complexity.

## Next Checks
1. Conduct inter-rater reliability analysis on question difficulty labels across multiple medical experts to quantify consistency and identify potential biases in difficulty assessment.
2. Perform ablation studies comparing COGNET-MD's partial credit scoring system against alternative scoring methods (binary, weighted partial credit) to empirically demonstrate which best correlates with actual medical reasoning quality.
3. Expand statistical power analysis by evaluating model performance across subsampled datasets of varying sizes to determine the minimum number of questions needed for reliable model differentiation and identify potential ceiling/floor effects in current dataset design.