---
ver: rpa2
title: 'LookupViT: Compressing visual information to a limited number of tokens'
arxiv_id: '2407.12753'
source_url: https://arxiv.org/abs/2407.12753
tags:
- tokens
- lookupvit
- compressed
- information
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LookupViT compresses visual information by using a small set of
  compressed tokens alongside the original lookup tokens. Heavy computation is restricted
  to the compressed tokens, while bidirectional cross-attention enables effective
  information exchange between the two sets.
---

# LookupViT: Compressing visual information to a limited number of tokens

## Quick Facts
- arXiv ID: 2407.12753
- Source URL: https://arxiv.org/abs/2407.12753
- Authors: Rajat Koner; Gagan Jain; Prateek Jain; Volker Tresp; Sujoy Paul
- Reference count: 40
- Primary result: Reduces ViT FLOPs by up to 2× while matching/improving accuracy on ImageNet, Kinetics400, and COCO-Captions

## Executive Summary
LookupViT introduces a novel vision transformer block that compresses visual information from high-resolution tokens to a fixed number of compressed tokens. This compression mechanism enables heavy computation to be restricted to the compressed tokens while lookup tokens undergo computationally cheaper processing. Bidirectional cross-attention facilitates efficient information exchange between the two token sets, allowing the model to maintain performance while significantly reducing computational complexity.

The approach demonstrates state-of-the-art results on image classification (ImageNet-1k, 21k), video classification (Kinetics400, Something-Something V2), and image captioning (COCO-Captions). Additionally, LookupViT shows improved robustness to image corruptions and out-of-distribution data, with up to 4% gains over standard ViT models on ImageNet-C, R, A, and O benchmarks.

## Method Summary
LookupViT compresses visual information by replacing standard ViT blocks with LookupViT blocks that operate on two sets of tokens: a small number of compressed tokens (M) and a larger set of lookup tokens (N) where M << N. The compressed tokens undergo intensive self-attention and MLP processing, while lookup tokens receive lighter computation. Bidirectional cross-attention enables information flow between both token sets, allowing lookup tokens to benefit from the global context captured by compressed tokens without incurring quadratic complexity. The architecture offers flexible compute-performance trade-offs through multi-resolution token compression, enabling extraction of different models with varying computational requirements from the same trained parameter space.

## Key Results
- Reduces ViT FLOPs by up to 2× while maintaining or improving accuracy across image and video classification tasks
- Achieves up to 4% improvement over ViT on ImageNet-C, R, A, and O benchmarks for robustness to corruptions and out-of-distribution data
- Demonstrates 5-6% accuracy improvements on Something-Something V2 with half the FLOPs of ViT
- Offers flexible compute-performance trade-offs through multi-resolution token compression in a single trained model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LookupViT reduces computational complexity by restricting heavy computation to a small set of compressed tokens while maintaining performance through bidirectional cross-attention.
- Mechanism: The model compresses visual information from high-resolution tokens into a fixed number of compressed tokens. These compressed tokens undergo intensive processing via self-attention and MLP layers, while the remaining lookup tokens receive lighter computation. Bidirectional cross-attention enables efficient information exchange between the two token sets, allowing lookup tokens to benefit from the global context captured by compressed tokens without incurring quadratic complexity.
- Core assumption: Visual information contains inherent sparsity and redundancy that can be effectively captured by a small set of compressed tokens without significant loss of representational power.
- Evidence anchors:
  - [abstract] "LookupViT provides a novel general purpose vision transformer block that operates by compressing information from higher resolution tokens to a fixed number of tokens. These few compressed tokens undergo meticulous processing, while the higher-resolution tokens are passed through computationally cheaper layers."
  - [section] "LookupViT distinguishes itself by offering a scalable, computationally efficient block that can be seamlessly repeated like standard ViT blocks. Its bidirectional cross-attention mechanism facilitates a richer exchange of information between the compressed and original tokens, enhancing representational power."
  - [corpus] Weak - corpus neighbors discuss token pruning and merging approaches but don't directly address the bidirectional cross-attention mechanism described here.

### Mechanism 2
- Claim: LookupViT achieves improved robustness to corruptions and out-of-distribution data through its information bottleneck mechanism.
- Mechanism: By forcing the model to extract only the most relevant information during the compression process, LookupViT inherently learns to focus on essential visual features while discarding noise and irrelevant details. This selective information extraction makes the model more resilient to image corruptions and better at generalizing to out-of-distribution samples.
- Core assumption: The information bottleneck created by compressing tokens to a small set acts as a form of regularization that improves generalization and robustness.
- Evidence anchors:
  - [abstract] "In addition, LookupViT also demonstrates out-of-the-box robustness and generalization on image classification (ImageNet-C,R,A,O), improving by up to 4% over ViT."
  - [section] "Interestingly, as in Table 4, LookupViViT performs significantly better than ViViT on SSv2. We observe a 5% − 6% improvement in accuracy with half the FLOPs. This further bolsters LookupViT's robustness claim, as in SSv2 backgrounds and objects are similar across classes, thus needing recognition of fine-grained motion [1], which our model does better than ViViT."
  - [corpus] Weak - corpus neighbors discuss efficient architectures but don't specifically address robustness improvements through information bottleneck mechanisms.

### Mechanism 3
- Claim: LookupViT provides flexible compute-performance trade-offs through multi-resolution token compression.
- Mechanism: The model allows variable compression ratios by adjusting the number of compressed tokens relative to lookup tokens. This enables extraction of different models with varying computational requirements from the same trained parameter space. During training, random selection of compressed token resolution creates models that can handle different resolutions at inference time.
- Core assumption: The same set of parameters can effectively represent models with different computational requirements when trained with varying compression ratios.
- Evidence anchors:
  - [abstract] "LookupViT also offers flexibility for the compressed tokens, enabling performance-computation trade-offs in a single trained model."
  - [section] "By adjusting the down-sampling ratio between compressed and lookup tokens, the cost-performance trade-off can be tailored to match specific application requirements. This multi-resolution nature allows for extraction of compute-efficient high-performing models during inference, with the same parameter space."
  - [corpus] Weak - corpus neighbors discuss efficient architectures but don't specifically address the multi-resolution training approach that enables compute-performance trade-offs.

## Foundational Learning

- Concept: Vision Transformer architecture and self-attention mechanism
  - Why needed here: Understanding the baseline ViT architecture is essential to grasp how LookupViT modifies and improves upon it. The quadratic complexity of self-attention in standard ViTs motivates the need for LookupViT's compression approach.
  - Quick check question: What is the computational complexity of self-attention in a standard ViT with N tokens, and why does this become problematic for high-resolution images?

- Concept: Cross-attention and bidirectional information flow
  - Why needed here: LookupViT's core innovation relies on bidirectional cross-attention between compressed and lookup tokens. Understanding how cross-attention works and differs from self-attention is crucial for comprehending the information exchange mechanism.
  - Quick check question: How does cross-attention differ from self-attention, and what advantages does bidirectional cross-attention provide in the context of token compression?

- Concept: Information bottleneck and its effects on model robustness
  - Why needed here: The robustness improvements in LookupViT are attributed to the information bottleneck created by token compression. Understanding information bottleneck theory helps explain why compressing tokens can lead to better generalization and robustness.
  - Quick check question: How does an information bottleneck in a neural network architecture typically affect its ability to generalize and handle noisy or corrupted inputs?

## Architecture Onboarding

- Component map:
  - Input tokenization: Image → non-overlapping patches → convolutional feature embeddings → positional embeddings → lookup tokens and compressed tokens
  - LookupViT block: Multi-Head Bidirectional Cross-Attention (MHBC) for information exchange → Compressed token processing (self-attention + MLP with upscale factor p=4) → Lookup token processing (light MLP with downscale factor q=2)
  - Output: Dual classifiers (compressed tokens and lookup tokens) with equal weights, or compressed tokens only for downstream tasks

- Critical path: Input → Tokenization → LookupViT block stack → Cross-attention information exchange → Compressed token self-attention → Dual output heads → Classification/loss computation

- Design tradeoffs:
  - Token compression ratio vs. performance: Higher compression reduces computation but may degrade accuracy
  - Compressed token count vs. computational savings: More compressed tokens provide better representation but reduce computational benefits
  - Bidirectional vs. unidirectional cross-attention: Bidirectional provides better information exchange but adds computational overhead
  - Shared vs. separate parameters: Shared parameters enable flexibility but may limit optimization for specific resolutions

- Failure signatures:
  - Performance degradation with aggressive compression: If compressed tokens lose too much information, accuracy drops significantly
  - Training instability with large models: Very large models may become unstable during training despite the compression mechanism
  - Suboptimal attention maps: Poorly learned cross-attention can lead to ineffective information exchange between token sets
  - Sensitivity to compression ratio: Performance may be highly sensitive to the choice of compressed token count

- First 3 experiments:
  1. Implement basic LookupViT block with fixed compression ratio (e.g., 5×5 compressed tokens for 14×14 lookup tokens) and compare performance/computation against baseline ViT on ImageNet-1k
  2. Test different compression ratios (3×3, 7×7, 10×10) to find optimal balance between accuracy and computational savings
  3. Evaluate robustness to image corruptions (ImageNet-C) to verify the out-of-the-box robustness claims and compare against baseline ViT performance

## Open Questions the Paper Calls Out

- Future work includes extending the model to dense prediction tasks like object detection and semantic segmentation
- Future work includes scaling to larger model sizes

## Limitations

- Limited ablation studies on critical design choices like compression ratio and bidirectional cross-attention architecture
- Training stability concerns for very large models not thoroughly analyzed
- Performance gains not uniform across all tasks, suggesting task-specific effectiveness

## Confidence

**High confidence**: The core claim that LookupViT can reduce computational complexity while maintaining or improving accuracy is well-supported by the experimental results.

**Medium confidence**: The robustness improvements on ImageNet-C, R, A, and O are demonstrated but the mechanism is not thoroughly analyzed.

**Low confidence**: The video classification results and captioning improvements are less convincing due to limited comparison with state-of-the-art methods.

## Next Checks

1. Conduct systematic ablation studies on compression ratios (3×3, 5×5, 7×7, 10×10) across multiple datasets to clarify optimal trade-offs between accuracy and efficiency.

2. Analyze training stability by identifying conditions that cause instability in large models and testing mitigation strategies like different learning rate schedules and regularization techniques.

3. Visualize and analyze attention maps between compressed and lookup tokens to verify meaningful information flow and compare with standard ViT attention patterns.