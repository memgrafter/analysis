---
ver: rpa2
title: 'DIVD: Deblurring with Improved Video Diffusion Model'
arxiv_id: '2412.00773'
source_url: https://arxiv.org/abs/2412.00773
tags:
- video
- frames
- image
- information
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DIVD, the first diffusion model for video
  deblurring, addressing limitations in existing methods that focus on distortion-based
  metrics. The proposed model employs a Window-based Temporal Self-Attention (WTSA)
  module and Multi-frame Relative Positional Encoding (MRPE) to process video frames
  in parallel, implicitly aligning and fusing information from misaligned frames.
---

# DIVD: Deblurring with Improved Video Diffusion Model

## Quick Facts
- arXiv ID: 2412.00773
- Source URL: https://arxiv.org/abs/2412.00773
- Authors: Haoyang Long; Yan Wang; Wendong Wang
- Reference count: 40
- Primary result: First diffusion model for video deblurring achieving state-of-the-art perceptual metrics

## Executive Summary
This paper introduces DIVD, the first diffusion model specifically designed for video deblurring. The model addresses limitations in existing methods that focus primarily on distortion-based metrics by achieving superior performance on perceptual metrics (FID, KID, LPIPS) while maintaining competitive distortion metrics. The key innovation is the Window-based Temporal Self-Attention (WTSA) module combined with Multi-frame Relative Positional Encoding (MRPE), which enables effective processing of misaligned video frames in parallel. The approach demonstrates significant improvements in preserving details and textures compared to existing methods that often produce overly smooth results.

## Method Summary
DIVD employs a convolutional 2D UNet backbone enhanced with Window-based Temporal Self-Attention (WTSA) modules and Multi-frame Relative Positional Encoding (MRPE). The model processes 4 consecutive video frames in parallel, using WTSA to implicitly align and fuse information from misaligned frames through attention mechanisms within fixed-size windows. MRPE provides comprehensive spatial and temporal positional information to improve the attention mechanism's effectiveness. The model is trained using the DDPM framework for 1M steps with Adam optimizer, random 144x144 crops from GoPro and DVD datasets, and a linear warm-up learning rate schedule.

## Key Results
- Achieves state-of-the-art performance on perceptual metrics (FID, KID, LPIPS) on GoPro and DVD datasets
- Maintains competitive distortion-based metrics (PSNR, SSIM) while significantly improving perceptual quality
- Visual comparisons demonstrate superior detail preservation and texture quality compared to existing methods
- Ablation studies confirm the effectiveness of both WTSA modules and MRPE positional encoding

## Why This Works (Mechanism)

### Mechanism 1: Window-based Temporal Self-Attention (WTSA)
The WTSA module enables implicit alignment and fusion of misaligned video frames by computing self-attention within fixed-size windows across all frames. By partitioning feature maps into non-overlapping M×M windows and applying self-attention across F frames within each window, the model can identify and merge corresponding features even when spatially misaligned. The attention scores highlight correlated features while suppressing irrelevant ones.

### Mechanism 2: Multi-frame Relative Positional Encoding (MRPE)
MRPE provides complete spatial and temporal positional information to the WTSA module through two components: frame positional encoding (learnable identifiers for each frame) and relative position bias (spatial relationships within windows). This comprehensive encoding enables the attention mechanism to attend to correlated information while maintaining spatial context, significantly boosting performance.

### Mechanism 3: Diffusion Model Framework
The diffusion model framework, combined with WTSA and MRPE improvements, enables state-of-the-art perceptual performance by preserving image details and textures. The iterative denoising process generates high-quality deblurred images, while WTSA leverages multi-frame information and MRPE ensures proper alignment. This combination preserves fine details crucial for perceptual quality, unlike traditional methods that optimize for PSNR/SSIM and produce overly smooth results.

## Foundational Learning

- **Concept: Diffusion Probabilistic Models (DPMs)**
  - Why needed: DIVD builds upon DPMs as the foundation for video deblurring
  - Quick check: What are the two main phases in a diffusion probabilistic model, and what happens in each phase?

- **Concept: Self-attention mechanisms**
  - Why needed: WTSA module relies heavily on self-attention to align and fuse features from misaligned frames
  - Quick check: How does self-attention differ from traditional convolutional operations in terms of receptive field and ability to capture long-range dependencies?

- **Concept: Positional encoding in transformer architectures**
  - Why needed: MRPE is a sophisticated positional encoding scheme critical for model performance
  - Quick check: What problem does positional encoding solve in transformer architectures, and how do relative positional encodings differ from absolute ones?

## Architecture Onboarding

- **Component map**: Input (concatenated noisy and blurry frames) → ResBlock feature extraction → WTSA modules (with MRPE) → Feature fusion → Output (restored clear frames)

- **Critical path**: Input → ResBlock feature extraction → WTSA (with MRPE) → Feature fusion → Output
  The WTSA modules with MRPE are the core innovation distinguishing this architecture from standard diffusion models.

- **Design tradeoffs**:
  - Window size M vs. computational cost: Larger windows capture more context but increase complexity quadratically
  - Number of frames F vs. temporal modeling capability: More frames provide more information but increase memory requirements
  - Position encoding complexity vs. performance gain: More sophisticated encoding improves results but adds parameters

- **Failure signatures**:
  - Blurry or misaligned output: Likely issues with WTSA window size or MRPE configuration
  - Overly smooth results: May indicate insufficient positional encoding or window size too small
  - Training instability: Could be related to diffusion model parameters or learning rate scheduling

- **First 3 experiments**:
  1. **Ablation study on WTSA window size**: Test different window size combinations [3,2,1,1], [4,3,2,1], [6,4,3,2] on a subset of GoPro dataset to find optimal balance between performance and computational cost.
  2. **Position encoding ablation**: Compare model performance with only frame positional encoding, only relative position bias, and the full MRPE to quantify their individual contributions.
  3. **Computational complexity analysis**: Measure inference time and memory usage for different window sizes and frame counts to establish practical deployment limits.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the computational complexity of DIVD compare to traditional video deblurring methods when processing long video sequences?
- **Open Question 2**: What is the optimal window size for the WTSA module across different types of video content (e.g., static scenes vs. highly dynamic scenes)?
- **Open Question 3**: How well does DIVD generalize to videos with extreme motion blur or non-uniform blur patterns compared to specialized deblurring approaches?

## Limitations

- Implementation details for WTSA and MRPE are not fully specified, making exact reproduction challenging
- Claims of superior perceptual quality lack human perceptual studies to validate metric correlations
- The claim of being the "first" diffusion model for video deblurring needs verification
- No systematic evaluation of optimal window size configurations for different video content types

## Confidence

- **High Confidence**: General approach of using diffusion models for video deblurring is sound and well-supported
- **Medium Confidence**: Specific architectural innovations (WTSA and MRPE) appear theoretically sound but need more validation
- **Low Confidence**: Claim of being the "first" diffusion model for video deblurring requires verification

## Next Checks

1. **Human Perceptual Study**: Conduct human evaluation comparing DIVD outputs with traditional methods to validate metric correlations with human judgment across diverse video content.

2. **Window Size Sensitivity Analysis**: Systematically evaluate different window size configurations on validation sets to determine optimal trade-offs between performance and computational cost.

3. **Cross-dataset Generalization**: Test trained model on a third, previously unseen dataset to evaluate generalization capabilities beyond GoPro and DVD datasets.