---
ver: rpa2
title: Chasing Better Deep Image Priors between Over- and Under-parameterization
arxiv_id: '2410.24187'
source_url: https://arxiv.org/abs/2410.24187
tags:
- uni00000013
- uni00000048
- uni00000011
- uni00000003
- uni00000033
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Lottery Image Prior (LIP), a novel extension
  of the Lottery Ticket Hypothesis (LTH) to the domain of deep image priors (DIP).
  The core idea is to identify sparse subnetworks within over-parameterized DIP models
  that can match or outperform the original dense models in image restoration tasks.
---

# Chasing Better Deep Image Priors between Over- and Under-parameterization

## Quick Facts
- arXiv ID: 2410.24187
- Source URL: https://arxiv.org/abs/2410.24187
- Authors: Qiming Wu; Xiaohan Chen; Yifan Jiang; Zhangyang Wang
- Reference count: 40
- Primary result: Lottery Image Prior (LIP) finds sparse DIP subnetworks that match or exceed dense model performance in image restoration tasks

## Executive Summary
This paper introduces the Lottery Image Prior (LIP), extending the Lottery Ticket Hypothesis to deep image priors (DIP) for image restoration tasks. The method identifies sparse, trainable subnetworks within over-parameterized DIP models using iterative magnitude pruning (IMP), demonstrating that these subnetworks can match or outperform their dense counterparts. The work also explores transferability across images and tasks, and extends the approach to pre-trained GAN generators for compressive sensing applications.

## Method Summary
The Lottery Image Prior (LIP) method applies iterative magnitude pruning (IMP) to over-parameterized DIP models, typically hourglass CNNs, to find sparse subnetworks that retain the original model's image restoration capabilities. The process involves iteratively pruning 20% of the smallest-magnitude weights, retraining on degraded images, and stopping when IMP loss increases continuously. The method is tested on standard image restoration tasks (denoising, inpainting, super-resolution) using datasets like Set5 and Set14, and is extended to pre-trained GAN generators for compressive sensing. Multi-image IMP is also explored to improve transferability across different images and tasks.

## Key Results
- LIP subnetworks outperform deep decoders under comparable model sizes
- LIP subnetworks match or surpass dense DIP performance at high sparsity levels (up to 99.53%)
- LIP subnetworks exhibit strong transferability across images and restoration tasks
- LIP extension to pre-trained GAN generators validates its applicability beyond untrained priors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse subnetworks from over-parameterized DIP models can match or exceed the performance of dense models in image restoration tasks.
- Mechanism: The lottery image prior (LIP) hypothesis extends the lottery ticket hypothesis to untrained neural networks used as deep image priors. Iterative magnitude pruning (IMP) identifies sparse subnetworks within an over-parameterized DIP that retain the essential image prior properties.
- Core assumption: Certain subnetworks within an over-parameterized DIP inherently encode natural image priors, and sparsity acts as a beneficial regularizer.
- Evidence anchors:
  - [abstract] "Given an over-parameterized DNN-based image prior, it will contain a sparse subnetwork that can be trained in isolation, to match the original DNN's performance when being applied as a prior to various image inverse problems."
  - [section] "We successfully locate the 'matching subnetworks' from over-parameterized DIP models by iterative magnitude pruning."
  - [corpus] Weak connection; related work focuses on diffusion and latent operator priors, not pruning-based sparsity.
- Break condition: Performance degradation occurs at extreme sparsity levels (e.g., 99.53%), where subnetworks fail to retain sufficient capacity.

### Mechanism 2
- Claim: LIP subnetworks exhibit strong transferability across images and restoration tasks.
- Mechanism: Subnetworks found via multi-image IMP (using shared backbone weights across multiple degraded images) capture more general image features, enabling reuse across domains.
- Core assumption: Low-level image features (color, texture, shape) are more transferable than high-level semantic features, and early network layers capture these better.
- Evidence anchors:
  - [section] "Those LIP subnetworks significantly outperform deep decoders under comparably compact model sizes... and they also possess high transferability across different images as well as restoration task types."
  - [section] "LIP tends to preserve weights of the earlier layers (closer to the input), while pruning the later layers more aggressively."
  - [corpus] Weak connection; neighbors focus on diffusion priors and latent operators, not task transferability of sparse subnetworks.
- Break condition: Transferability fails when moving from low-level restoration tasks to high-level classification tasks due to structural sparsity distribution differences.

### Mechanism 3
- Claim: Extending LIP to pre-trained GAN generators validates its applicability beyond untrained priors.
- Mechanism: IMP applied to pre-trained GAN generators for compressive sensing finds sparse subnetworks that match dense model reconstruction quality, demonstrating the generality of the LIP concept.
- Core assumption: The lottery ticket phenomenon is not limited to untrained networks but also applies to pre-trained generative models when used as image priors.
- Evidence anchors:
  - [section] "We also extend our method to compressive sensing image reconstruction, where a pre-trained GAN generator is used as the prior... and confirm its validity in this setting too."
  - [section] "Tickets with higher sparsities can match the reconstruction performance of the dense model, confirming the existence of the winning tickets."
  - [corpus] Weak connection; neighbors discuss diffusion priors and Bayesian tensor priors, not GAN-based sparsity.
- Break condition: Performance drops if fine-tuning or pruning disrupts the generator's learned manifold alignment with the target image distribution.

## Foundational Learning

- Concept: Deep Image Prior (DIP)
  - Why needed here: DIP is the foundational framework where untrained CNNs act as image priors for inverse problems without supervised training.
  - Quick check question: What is the core idea behind using untrained CNNs as image priors in DIP?

- Concept: Lottery Ticket Hypothesis (LTH)
  - Why needed here: LTH provides the theoretical basis for finding sparse, trainable subnetworks that match dense model performance, which LIP extends to DIP.
  - Quick check question: How does iterative magnitude pruning identify "winning tickets" in the context of LTH?

- Concept: Iterative Magnitude Pruning (IMP)
  - Why needed here: IMP is the practical algorithm used to locate LIP subnetworks by iteratively pruning and retraining.
  - Quick check question: What is the typical pruning ratio per iteration in IMP, and why is it chosen?

## Architecture Onboarding

- Component map: Degraded image + random latent code -> Hourglass CNN (or deep decoder) -> IMP iterations -> Image restoration metrics
- Critical path:
  1. Initialize dense DIP model with random weights
  2. Iteratively prune 20% of smallest-magnitude weights
  3. Retrain on degraded image(s) for fixed epochs
  4. Stop when IMP loss stabilizes or increases
  5. Evaluate sparse subnetwork on test images/tasks

- Design tradeoffs:
  - Over-parameterization vs. compactness: Dense models have more capacity but risk overfitting noise; sparse models are efficient but may lose capacity.
  - Single-image vs. multi-image IMP: Single-image IMP is simpler but less generalizable; multi-image IMP improves transferability at the cost of complexity.
  - Pruning ratio: Aggressive pruning (e.g., 20%) finds compact subnetworks faster but risks losing performance; conservative pruning preserves performance but yields less compression.

- Failure signatures:
  - IMP loss continuously increases without plateau -> early stopping needed
  - PSNR/SSIM drop significantly at high sparsity -> subnetwork too sparse
  - Poor transferability to classification -> structural mismatch between restoration and classification tasks

- First 3 experiments:
  1. Implement IMP on hourglass DIP for denoising a single image; verify matching subnetwork at ~80% sparsity.
  2. Test transferability of found subnetwork to inpainting and super-resolution on same image.
  3. Compare LIP subnetwork performance against deep decoder and randomly pruned subnetworks at matched parameter counts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Lottery Image Prior (LIP) method be extended to effectively transfer from low-level image restoration tasks to high-level vision tasks like image classification?
- Basis in paper: [explicit] The paper notes that LIP subnetworks cannot directly transfer from restoration tasks to classification tasks, and this is attributed to the sparsity distribution discrepancy in the inner structure of LIP subnetworks.
- Why unresolved: The paper conjectures that the failure is due to LIP's tendency to prune more weights in later layers, which might damage the network's capability in capturing semantic features. However, it does not provide a concrete solution or method to overcome this limitation.
- What evidence would resolve it: Successful transfer of LIP subnetworks from image restoration tasks to image classification tasks, possibly by adjusting the sparsity distribution or by adding and fine-tuning linear layers on top of intermediate layers.

### Open Question 2
- Question: What is the optimal strategy for deciding when to stop iterative magnitude pruning (IMP) iterations in the single-image based IMP loops to find matching subnetworks without overfitting noise?
- Basis in paper: [explicit] The paper discusses the need for early-stopping in single-image based IMP loops to avoid overfitting noise and suggests using the continuous increase in IMP loss values as a signal to stop pruning.
- Why unresolved: While the paper provides a heuristic based on observing continuous increases in IMP loss values, it acknowledges that there may be other ways to identify matching subnetworks, such as observing gradient flow or the magnitude of gradients, which are not explored.
- What evidence would resolve it: Comparative studies showing the effectiveness of different early-stopping strategies in identifying matching subnetworks, potentially leading to a more robust and generalizable method.

### Open Question 3
- Question: How does the performance of LIP subnetworks compare to state-of-the-art pruning methods like SynFlow when applied to various image restoration tasks at different sparsity levels?
- Basis in paper: [explicit] The paper compares LIP with SNIP and random pruning methods and mentions SynFlow as a state-of-the-art pruning method, showing that LIP outperforms SynFlow at higher sparsity levels.
- Why unresolved: The paper provides a comparison at specific sparsity levels but does not explore the full range of sparsity levels or the performance across all types of image restoration tasks comprehensively.
- What evidence would resolve it: Detailed performance comparisons of LIP and SynFlow subnetworks across a wide range of sparsity levels and various image restoration tasks, providing insights into the strengths and weaknesses of each method.

## Limitations
- Performance degradation at extreme sparsity levels (>99%) indicates fundamental limits to the lottery ticket phenomenon in DIP settings.
- LIP subnetworks fail to transfer from low-level restoration tasks to high-level classification tasks due to structural sparsity distribution differences.
- Limited testing on complex, real-world degradation types beyond standard synthetic noise models.

## Confidence
- High confidence: Core mechanism of finding sparse subnetworks via IMP in DIP models for image restoration tasks
- Medium confidence: Claims about transferability across images and restoration tasks within the same domain
- Low confidence: Extension to pre-trained GAN generators and claims about broader applicability beyond low-level vision tasks

## Next Checks
1. Cross-domain transferability test: Evaluate LIP subnetworks found in image restoration tasks on medical imaging or satellite imagery to verify if the lottery phenomenon extends to specialized domains.
2. Ablation study on pruning ratio: Systematically vary the IMP pruning ratio (10%, 20%, 30%) to determine the optimal balance between compression and performance retention.
3. Robustness to unknown degradation: Apply LIP subnetworks trained on synthetic noise to real-world images with mixed degradation types to assess practical utility.