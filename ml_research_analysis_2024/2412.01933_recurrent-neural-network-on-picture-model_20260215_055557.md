---
ver: rpa2
title: Recurrent Neural Network on PICTURE Model
arxiv_id: '2412.01933'
source_url: https://arxiv.org/abs/2412.01933
tags:
- dataset
- data
- were
- window
- encounter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study aimed to benchmark deep learning models against the
  existing XGBoost model for predicting ICU patient deterioration. The PICTURE dataset,
  containing vital signs, physiological observations, and demographic information,
  was preprocessed using sliding window, dense sliding window, and smart batching
  methods.
---

# Recurrent Neural Network on PICTURE Model

## Quick Facts
- arXiv ID: 2412.01933
- Source URL: https://arxiv.org/abs/2412.01933
- Reference count: 0
- Primary result: Deep learning models did not outperform XGBoost baseline for ICU patient deterioration prediction despite various preprocessing methods and class imbalance techniques

## Executive Summary
This study benchmarks deep learning models (RNN with LSTM nodes and Transformers) against an existing XGBoost model for predicting ICU patient deterioration using the PICTURE dataset. The dataset contains vital signs, physiological observations, and demographic information from ICU patients. Three preprocessing methods were tested: sliding window, dense sliding window, and smart batching. Despite efforts to address class imbalance through class weights and focal loss, the deep learning models did not significantly outperform the XGBoost baseline. The RNN with sliding window method showed the most promise but suffered from overfitting due to non-representative training data.

## Method Summary
The study used the PICTURE dataset containing ICU patient data with vital signs, physiological observations, and demographics. Three preprocessing methods were implemented: sliding window (fixed 8-hour windows), dense sliding window (overlapping windows with padding), and smart batching (grouping encounters). The windowed dataset had 132,589 patients and 223,824 encounters, while the granular dataset had 132,474 patients and 223,640 encounters. Various deep learning models were trained using class weights and focal loss to address the severe class imbalance (2% positive events). Models were evaluated using AUROC and AUPRC metrics at both observational and encounter levels, compared against an XGBoost baseline.

## Key Results
- Deep learning models failed to outperform XGBoost baseline despite using class weights and focal loss
- RNN with sliding window preprocessing showed the most promise but exhibited overfitting
- Class imbalance (2% positive events) significantly limited deep learning model performance
- Training data had higher event rates than validation/test sets, suggesting data leakage issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class imbalance severely limits deep learning model performance relative to XGBoost.
- Mechanism: The PICTURE dataset contains ~2% positive (deterioration) events vs. ~98% negative. Standard loss functions (binary cross-entropy) are dominated by the majority class, causing models to under-predict positive cases and yield low AUPRC/AUROC.
- Core assumption: Weighted loss functions (class weights, focal loss) cannot fully compensate for severe imbalance in small, heterogeneous clinical datasets.
- Evidence anchors:
  - [abstract] "The severe imbalance in the dataset posed a significant challenge for accurate prediction... the performance did not significantly improve compared to the XGBoost model."
  - [section] "We observed a significant difference between positive and negative samples, so we employed class weights... Despite trying different methods... the performance did not significantly improve."
- Break condition: If minority-class recall rises above 30% with weighted focal loss, the mechanism breaks.

### Mechanism 2
- Claim: Sliding window preprocessing better preserves temporal dependencies than dense sliding window or smart batching for this dataset.
- Mechanism: Sliding window selects fixed-length 8-hour windows, preserving chronological order without excessive zero-padding. This maintains the integrity of temporal patterns that RNNs/LSTMs rely on.
- Core assumption: LSTM performance degrades with irregular sequence lengths and heavy padding; sliding window minimizes both.
- Evidence anchors:
  - [section] "Relatively promising results were obtained with the sliding window method and RNN with LSTM node."
  - [section] "One of the reasons for the worse performance... is that we are using granular dataset for this method. The windowed dataset... has been simplified by grouping each encounter's data into 8-hour windows."
- Break condition: If transformer with attention mechanisms outperforms LSTM on sliding window, the mechanism breaks.

### Mechanism 3
- Claim: Data leakage between training and validation sets causes apparent overfitting.
- Mechanism: The training subset had a higher event rate (higher positive proportion) than validation/test, causing models to overfit to an unrepresentative distribution. This inflated training AUROC/AUPRC but collapsed on real test data.
- Core assumption: Patient-wise splits were correctly implemented, but temporal or cohort shifts existed between subsets.
- Evidence anchors:
  - [section] "According to Table 4, the event rate of the training dataset was much higher than that of other subsets, which suggests that the overfitting occurred because the training data was not representative."
  - [section] "The model that gave us the highest AUROC and AUPRC... showed a significant improvement over the XGBoost model on the training-training and training-validation datasets. However, when we tested the model on the original validation and test sets, it did not perform well."
- Break condition: If event rates are balanced across all splits and overfitting disappears, the mechanism breaks.

## Foundational Learning

- Concept: Class imbalance handling in deep learning
  - Why needed here: Severe skew (2% positives) in PICTURE data makes vanilla cross-entropy ineffective; must use weighted or focal loss.
  - Quick check question: What weight should be assigned to the minority class if the dataset has 98% negatives and 2% positives?

- Concept: Temporal sequence modeling with RNN/LSTM
  - Why needed here: ICU vitals are sequential; LSTM can capture short- and long-term dependencies missed by XGBoost.
  - Quick check question: How does LSTM's gating mechanism help retain long-term dependencies compared to vanilla RNN?

- Concept: Sliding window vs. dense sliding window preprocessing
  - Why needed here: Sliding window avoids zero-padding for short encounters; dense sliding window adds padding but aligns predictions with real-time updates.
  - Quick check question: In dense sliding window, why do we pad short sequences at the beginning instead of the end?

## Architecture Onboarding

- Component map: Data pipeline → sliding/dense window or smart batching → masking layer → LSTM/Transformer encoder stack → dropout/normalization layers → dense output → loss (weighted BCE or focal)
- Critical path: Feature standardization → windowing → masking → LSTM sequence modeling → output layer
- Design tradeoffs:
  - Sliding window: less padding, better memory, but fixed prediction horizon; Dense sliding window: more padding, closer to real-time, more training samples
  - LSTM: strong temporal modeling, risk of vanishing gradients; Transformer: parallelizable, better long-range, higher compute
- Failure signatures:
  - Overfitting: high training AUROC/AUPRC but low validation/test AUROC/AUPRC
  - Underfitting: uniformly low AUROC/AUPRC across all splits
  - Class imbalance: high specificity but very low sensitivity/recall
- First 3 experiments:
  1. Retrain sliding-window LSTM with focal loss γ=2 and compare AUROC/AUPRC to baseline
  2. Perform stratified temporal split to balance event rates across train/val/test and retrain
  3. Replace LSTM with Transformer encoder (2 layers, 6 heads, head size 128) on sliding-window data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would ensemble learning methods (e.g., combining multiple RNN models or combining RNN with XGBoost) improve predictive performance compared to the individual deep learning models tested?
- Basis in paper: [explicit] The authors suggest ensemble learning as a future direction, stating "The first is ensemble learning, in which multiple models are trained on different subsets of the imbalanced dataset and their predictions combined to obtain a final prediction. This approach can improve the model's generalization performance."
- Why unresolved: The study only tested individual deep learning models (RNN and Transformer) without exploring ensemble methods. The authors acknowledged that ensemble learning could potentially address the model's generalization issues.
- What evidence would resolve it: Implementation and testing of ensemble models combining predictions from multiple RNN variants or combining RNN predictions with XGBoost predictions, comparing their AUROC/AUPRC to the baseline XGBoost and individual deep learning models.

### Open Question 2
- Question: Would transfer learning from pre-trained models on larger, more balanced medical datasets improve performance on the PICTURE dataset?
- Basis in paper: [explicit] The authors suggest transfer learning as a future direction, stating "The second method is transfer learning, which involves using a pre-trained model on a large dataset as a feature extractor and fine-tuning it on the imbalanced dataset. This can help the model learn better data representations."
- Why unresolved: The study used only the PICTURE dataset for training without leveraging pre-trained models. The authors recognized that transfer learning could help the model learn better representations from imbalanced data.
- What evidence would resolve it: Implementation of transfer learning using pre-trained models (e.g., on MIMIC-III or other large medical datasets) fine-tuned on PICTURE data, comparing performance metrics to models trained only on PICTURE data.

### Open Question 3
- Question: Does the sliding window method with RNN outperform other preprocessing methods specifically for long encounters, and what is the optimal window size for different encounter lengths?
- Basis in paper: [inferred] The authors noted that "there was a large deviance of the encounter length in the dataset" and found that "the performance is better for short encounters" when experimenting with different preprocessing methods. They also found that "the model that gave us the highest AUROC and AUPRC has the structure on Figure 8" which used sliding window method.
- Why unresolved: The study tested different preprocessing methods but did not systematically evaluate their performance across different encounter length categories or optimize window sizes for each category.
- What evidence would resolve it: Systematic evaluation of RNN models with sliding window preprocessing across different encounter length categories (short, medium, long) with optimized window sizes for each category, comparing performance metrics to identify the optimal approach for each encounter type.

## Limitations
- Severe class imbalance (2% positive events) that weighted loss functions could not fully overcome
- Non-representative training data distributions across splits suggesting potential data leakage
- Limited exploration of advanced preprocessing techniques for irregularly sampled time series
- Lack of ensemble learning approaches to potentially improve generalization

## Confidence
- **High Confidence**: Class imbalance severely limits deep learning performance
- **Medium Confidence**: Sliding window preprocessing better preserves temporal dependencies
- **Medium Confidence**: Data leakage/overfitting due to non-representative training data

## Next Checks
1. Implement stratified temporal splitting to ensure balanced event rates across all data subsets, then retrain the best-performing LSTM model to assess impact on overfitting
2. Compare focal loss with γ=2 against the baseline weighted cross-entropy to determine if minority class detection improves significantly
3. Implement transformer encoder with standardized architecture (2 layers, 6 heads, head size 128) on sliding-window data and compare performance against LSTM baseline