---
ver: rpa2
title: 'TabVer: Tabular Fact Verification with Natural Logic'
arxiv_id: '2411.01093'
source_url: https://arxiv.org/abs/2411.01093
tags:
- claim
- evidence
- natural
- tabver
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TABVER, a fact verification system that combines
  natural logic inference with arithmetic reasoning for tabular evidence. The core
  idea is to interpret numerals and arithmetic functions set-theoretically, enabling
  their integration into natural logic proofs via generated arithmetic expressions
  (ArithExps).
---

# TabVer: Tabular Fact Verification with Natural Logic

## Quick Facts
- arXiv ID: 2411.01093
- Source URL: https://arxiv.org/abs/2411.01093
- Authors: Rami Aly; Andreas Vlachos
- Reference count: 40
- In few-shot experiments on FEVEROUS, TABVER achieves 71.4 accuracy, outperforming both fully neural and symbolic reasoning baselines by 3.4 points.

## Executive Summary
This paper introduces TABVER, a fact verification system that combines natural logic inference with arithmetic reasoning for tabular evidence. The core innovation is interpreting numerals and arithmetic functions set-theoretically, enabling their integration into natural logic proofs via generated arithmetic expressions (ArithExps). These ArithExps are produced by asking questions about claim spans and answering them using tabular evidence through constrained decoding. TABVER demonstrates strong performance in handling numerical reasoning and maintaining faithful, interpretable explanations, achieving state-of-the-art results in few-shot settings on FEVEROUS and competitive performance on TabFact without further training.

## Method Summary
TABVER is a fact verification system that combines natural logic inference with arithmetic reasoning for tabular evidence. It uses fine-tuned MistralOrca-7B models for question generation (MQG) and QA (MQA) with arithmetic annotations, along with claim decomposition (MD) and proof generation (MP) models. The system generates questions about claim spans, answers them using tabular evidence through constrained decoding to produce ArithExps, constructs natural logic proofs, and executes these proofs on a finite state automaton to determine veracity labels. The method is evaluated against symbolic and neural baselines on FEVEROUS and TabFact datasets.

## Key Results
- In few-shot experiments on FEVEROUS, TABVER achieves 71.4 accuracy, outperforming both fully neural and symbolic reasoning baselines by 3.4 points
- On the numerical subset of FEVEROUS, TABVER outperforms the best baseline by 5.6 accuracy points
- When evaluated on TabFact without further training, TABVER maintains competitive performance with a 0.5 point accuracy lead over the strongest baseline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Natural logic inference can be extended to tabular evidence by interpreting numerals and arithmetic functions set-theoretically.
- **Mechanism**: The paper defines a set-theoretic interpretation of numerals and arithmetic expressions (ArithExps) that enables their integration into natural logic proofs. This allows the system to handle numerical reasoning within the natural logic framework.
- **Core assumption**: The set-theoretic relationships defined for numerals and arithmetic functions are sufficient to capture the semantic relationships needed for fact verification in tabular contexts.
- **Evidence anchors**:
  - [abstract]: "We propose a set-theoretic interpretation of numerals and arithmetic functions in the context of natural logic, enabling the integration of arithmetic expressions in deterministic proofs."
  - [section 3.1]: "We first define a set-theoretic interpretation of the relationship between numerals in claim-evidence pairs, within the context of natural logic."
  - [corpus]: Weak. The corpus provides related work but doesn't directly support the specific set-theoretic interpretation mechanism.
- **Break condition**: If the set-theoretic definitions don't adequately capture the nuances of numerical relationships in different contexts (e.g., different interpretations of numerals in upward vs. downward monotone environments).

### Mechanism 2
- **Claim**: Large language models can generate questions about salient parts of claims and answer them using tabular evidence through constrained decoding.
- **Mechanism**: The system uses fine-tuned LLMs to generate questions targeting specific claim spans (ci) and then answers these questions by extracting relevant evidence from tables and executing arithmetic functions. Constrained decoding ensures the generated content is faithful to the evidence.
- **Core assumption**: LLMs can be effectively fine-tuned to perform this question generation and answering task in a way that produces accurate and relevant ArithExps.
- **Evidence anchors**:
  - [abstract]: "We leverage large language models to generate arithmetic expressions by generating questions about salient parts of a claim which are answered by executing appropriate functions on tables."
  - [section 3.3]: "We use a fine-tuned large language model MQG(c, T), which takes a claim c and a prompt template T as input and autoregressively generates a collection of questions q1 . . . ql, along with their corresponding targeted claim spans."
  - [corpus]: Weak. The corpus mentions related work on question generation but doesn't specifically address the constrained decoding approach used here.
- **Break condition**: If the LLM's question generation or answering is not accurate enough, leading to incorrect ArithExps and ultimately wrong veracity predictions.

### Mechanism 3
- **Claim**: Combining natural logic with arithmetic reasoning through ArithExps outperforms both fully neural and symbolic reasoning baselines in few-shot settings.
- **Mechanism**: The system constructs natural logic proofs that include ArithExps as elements. These proofs are then executed on a finite state automaton to determine the final veracity label. This hybrid approach leverages the strengths of both natural logic (faithful explanations) and arithmetic reasoning (handling numerical computations).
- **Core assumption**: The combination of natural logic and arithmetic reasoning through ArithExps provides a more effective approach to tabular fact verification than either approach alone.
- **Evidence anchors**:
  - [abstract]: "In a few-shot setting on FEVEROUS, we achieve an accuracy of 71.4, outperforming both fully neural and symbolic reasoning models by 3.4 points."
  - [section 5]: "TABVER outperforms all baselines both on the full dataset, as well as the numerical subset by 3.4 and 5.6 accuracy points, respectively."
  - [corpus]: Weak. The corpus provides related work on fact verification but doesn't directly compare the specific combination of natural logic and arithmetic reasoning.
- **Break condition**: If the performance gain over baselines is not consistent across different datasets or if the system fails to generalize well to new domains.

## Foundational Learning

- **Concept**: Set-theoretic relationships in natural logic
  - **Why needed here**: Understanding how natural logic uses set-theoretic relationships to model entailment is crucial for extending it to handle numerals and arithmetic functions.
  - **Quick check question**: What are the basic set-theoretic operators used in natural logic (e.g., equivalence, forward entailment, negation)?
- **Concept**: Arithmetic expressions (ArithExps) and their representation
  - **Why needed here**: ArithExps are the core mechanism for incorporating numerical reasoning into natural logic proofs. Understanding their structure and how they map to numerical computations is essential.
  - **Quick check question**: How is an ArithExp represented (e.g., "SUM 12,238" for the sum of relevant cells)?
- **Concept**: Constrained decoding in language models
  - **Why needed here**: Constrained decoding is used to ensure that the LLM-generated questions and answers are faithful to the evidence in the tables. Understanding how this works is important for debugging and improving the system.
  - **Quick check question**: What are the constraints applied during the generation of questions and answers in TABVER?

## Architecture Onboarding

- **Component map**: Claim → MD → MQG → MQA → MP → DFA → Verdict
- **Critical path**: Claim → MD → MQG → MQA → MP → DFA → Verdict
- **Design tradeoffs**:
  - Using LLMs for question generation and answering provides flexibility but may introduce errors or hallucinations.
  - Constrained decoding helps mitigate these issues but may limit the expressiveness of the generated content.
  - The system relies on annotated training data for fine-tuning, which may be a limitation in low-resource settings.
- **Failure signatures**:
  - Incorrect ArithExps due to errors in question generation or answering.
  - Wrong natural logic operators assigned by the proof generation model.
  - DFA producing incorrect verdicts due to errors in the proof.
  - System failing to find relevant evidence in the tables.
- **First 3 experiments**:
  1. **Experiment 1**: Test the accuracy of the question generation model (MQG) by evaluating the relevance and quality of generated questions on a held-out dataset.
  2. **Experiment 2**: Evaluate the accuracy of the question answering model (MQA) by checking if the generated ArithExps correctly represent the numerical computations needed to answer the questions.
  3. **Experiment 3**: Test the end-to-end performance of the system on a small subset of the FEVEROUS dataset to identify any major issues in the integration of components.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can TABVER's natural logic framework be extended to handle multi-hop claims requiring combination of multiple pieces of evidence beyond arithmetic functions?
- **Basis in paper**: [inferred] The paper mentions that proofs do not allow assigning NatOp sequences to individual claim spans like "cat → dog → poodle" and that this can be a limitation for multi-hop claims.
- **Why unresolved**: The paper acknowledges this limitation but doesn't explore potential solutions or evaluate how well TABVER performs on multi-hop claims compared to baselines.
- **What evidence would resolve it**: An ablation study comparing TABVER's performance on single-hop vs multi-hop claims, or an extension of the framework to handle such cases with evaluation results.

### Open Question 2
- **Question**: How does TABVER's performance scale with larger models and more training data?
- **Basis in paper**: [inferred] The paper uses a 7B parameter model and only 64 training instances, noting that larger models might simplify the architecture but weren't used due to implementation constraints.
- **Why unresolved**: The paper demonstrates strong performance with limited resources but doesn't explore how much better it could perform with more compute and data.
- **What evidence would resolve it**: Experiments with larger language models (e.g., 70B+ parameters) and full FEVEROUS training data to compare performance gains.

### Open Question 3
- **Question**: How robust is TABVER to diverse table structures and noisy data compared to SQL-based approaches?
- **Basis in paper**: [explicit] The paper highlights that TABVER outperforms Binder on FEVEROUS's irregular tables while Binder excels on TabFact's well-structured tables, suggesting flexibility advantages.
- **Why unresolved**: The comparison is limited to these two datasets; broader evaluation on tables with varying quality, structure, and noise levels is missing.
- **What evidence would resolve it**: Evaluation on a diverse set of tabular datasets with varying structures, noise levels, and domains to compare robustness between TABVER and symbolic baselines.

## Limitations

- **Limitation 1**: The system relies on annotated training data for fine-tuning, which may be a limitation in low-resource settings. The paper doesn't discuss strategies for handling cases where such data is scarce.
- **Limitation 2**: The evaluation is primarily conducted on the FEVEROUS and TabFact datasets. While the system shows promising results, its performance on other tabular fact verification datasets or in real-world applications is unknown.
- **Limitation 3**: The paper doesn't provide sufficient detail on how the ArithExp constraint decoding is implemented in the MQA model, which could significantly impact the system's accuracy and faithfulness.

## Confidence

- **High Confidence**: The core mechanism of integrating arithmetic reasoning into natural logic through ArithExps is well-supported by the paper's theoretical framework and experimental results.
- **Medium Confidence**: The effectiveness of using LLMs for question generation and answering in the context of tabular fact verification is supported by the experimental results, but the specific implementation details and potential failure modes are not fully explored.
- **Low Confidence**: The system's ability to generalize to new domains or handle cases where relevant evidence is not present in the tables is not well-established, as the paper focuses primarily on the specific datasets used in the experiments.

## Next Checks

1. **Check 1**: Evaluate the accuracy of the question generation model (MQG) on a held-out dataset to assess its ability to generate relevant and high-quality questions targeting salient parts of claims.
2. **Check 2**: Test the question answering model (MQA) by checking if the generated ArithExps correctly represent the numerical computations needed to answer the questions, and verify the accuracy of the numerical reasoning against annotated gold rationales.
3. **Check 3**: Conduct an end-to-end evaluation of the system on a small subset of the FEVEROUS dataset, focusing on the integration of components and identifying any major issues in the flow from claim decomposition to final veracity prediction.