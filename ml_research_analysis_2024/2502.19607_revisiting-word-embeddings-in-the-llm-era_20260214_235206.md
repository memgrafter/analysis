---
ver: rpa2
title: Revisiting Word Embeddings in the LLM Era
arxiv_id: '2502.19607'
source_url: https://arxiv.org/abs/2502.19607
tags:
- word
- anchor
- embeddings
- sentence
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically compares classical word embeddings (Word2Vec,
  GloVe, SBERT, USE, SimCSE) with those induced from large language models (LLaMA,
  Mistral, OpenELM, PaLM, GPT-ADA) across both decontextualized and contextualized
  settings. Decontextualized embeddings from ~80,000 words were evaluated using word-pair
  similarity, analogy tasks, and similarity correlation analysis.
---

# Revisiting Word Embeddings in the LLM Era

## Quick Facts
- **arXiv ID**: 2502.19607
- **Source URL**: https://arxiv.org/abs/2502.19607
- **Reference count**: 40
- **Primary result**: Systematic comparison of classical word embeddings with LLM-induced embeddings across decontextualized and contextualized settings reveals task-dependent strengths

## Executive Summary
This paper systematically compares classical word embeddings (Word2Vec, GloVe, SBERT, USE, SimCSE) with embeddings induced from large language models (LLaMA, Mistral, OpenELM, PaLM, GPT-ADA) across both decontextualized and contextualized settings. Using ~80,000 words from WordNet and nine synthetic linguistic tasks, the study evaluates models on word-pair similarity, analogy tasks, and contextualized variance measures. The results show that LLMs excel at clustering semantically related words and performing analogy tasks in decontextualized settings, while classical models like SBERT demonstrate superior performance on sentence-level similarity and polysemy tasks when contextualized. LLaMA2 achieves the highest contextualized token-level analysis performance, though OpenELM uniquely excels at antonym and negation tasks.

## Method Summary
The study evaluates embeddings through two distinct pipelines: decontextualized (single words) and contextualized (sentences). For decontextualized analysis, embeddings are generated for ~80,000 WordNet words and evaluated using cosine similarity distributions, BATS analogy tasks (3CosAdd, 3CosAvg, 3CosMul, LRCos, PairDistance), and correlation analysis (Spearman's ρ, Kendall's τ). Contextualized evaluation uses synthetic sentences for nine linguistic tasks (synonym, antonym, negation, jumbling, paraphrase, questionnaire, exclamation, active-passive, polysemy) with 1,200 anchor words. Three similarity measures are computed: Anchor Inter-Contextual Variance, Anchor Contextual Deviation, and Sentence Meaning Variance. Embeddings are extracted from both tokenizers and hidden states, with statistical comparisons performed across all model pairs.

## Key Results
- LLMs like PaLM and GPT-ADA excel at clustering semantically related words and performing analogy tasks in decontextualized settings
- Classical models like SBERT show strong semantic discrimination and outperform LLMs in sentence-level similarity and polysemy tasks
- LLaMA2 achieves superior anchor contextual deviation across all contextualized tasks, demonstrating exceptional contextualized token-level analysis
- OpenELM uniquely excels at anchor inter-contextual variance in antonym and negation tasks

## Why This Works (Mechanism)

### Mechanism 1
LLM-induced embeddings cluster semantically related words more tightly in decontextualized settings due to their training on massive and diverse datasets, leading to over-generalization of semantic similarity. The large and diverse training data causes LLMs to generalize semantic similarity beyond strict relatedness, increasing similarity scores for random word pairs compared to classical models.

### Mechanism 2
Classical models like SBERT and USE achieve superior performance on sentence-level similarity and polysemy tasks in contextualized settings due to their contrastive loss training and task-specific datasets. These models are trained with contrastive objectives that optimize for distinguishing between similar and dissimilar sentence pairs, making them particularly effective at fine-grained semantic discrimination.

### Mechanism 3
LLMs like LLaMA2 excel at Anchor Contextual Deviation across all contexts due to their causal attention mechanism and autoregressive training, demonstrating superior contextualized token-level analysis. The causal attention mechanism in LLMs allows them to capture context-dependent word representations more effectively, leading to wider cosine angle deviations from decontextualized embeddings.

## Foundational Learning

- **Cosine similarity as a measure of semantic relatedness**: The paper relies heavily on cosine similarity to compare embeddings across different models and tasks. Understanding this metric is crucial for interpreting the results.
  - Quick check: If two word embeddings have a cosine similarity of 0.9, what does this indicate about their semantic relationship?

- **Word analogy tasks (3CosAdd, 3CosAvg, 3CosMul, LRCos, PairDistance)**: The paper evaluates model performance on analogy tasks using multiple methods. Understanding these methods is essential for interpreting the results.
  - Quick check: In the 3CosAdd method for analogy tasks, what mathematical operation is performed on the word embeddings to find the target word?

- **Contrastive learning and contrastive loss**: Classical models like SimCSE use contrastive learning objectives. Understanding this concept helps explain their performance advantages in certain tasks.
  - Quick check: How does contrastive loss training differ from traditional language modeling objectives, and why might this be advantageous for sentence similarity tasks?

## Architecture Onboarding

- **Component map**: Data preprocessing (WordNet corpus extraction, BATS dataset processing) -> Embedding generation (decontextualized and contextualized pipelines) -> Evaluation metrics (cosine similarity calculations, variance measurements, analogy task accuracy) -> Statistical analysis (Spearman's ρ and Kendall's τ correlation calculations, mean-variance plots)

- **Critical path**: Embedding generation → Similarity calculation → Statistical analysis → Result interpretation

- **Design tradeoffs**: Closed-source vs. open-source models (API limitations vs. computational resources), single-token vs. subword tokenization approaches, decontextualized vs. contextualized evaluation settings

- **Failure signatures**: High computational requirements for large-scale similarity calculations, API rate limiting for closed-source models, vocabulary size limitations affecting word pair comparisons

- **First 3 experiments**:
  1. Generate embeddings for a small subset of words (e.g., 100 words) from both LLM and classical models to verify the embedding generation pipeline works correctly
  2. Calculate cosine similarities between a few word pairs to ensure the similarity calculation is functioning as expected
  3. Run a single analogy task (e.g., 3CosAdd method on a small subset of the BATS dataset) to validate the analogy task evaluation pipeline

## Open Questions the Paper Calls Out

### Open Question 1
Do LLM-induced decontextualized embeddings capture semantic similarity better than classical models for specific word relation types? The paper shows SBERT and PaLM can distinguish semantically related and unrelated pairs better than most other models, while classical models performed better on morphological categories. This remains unresolved because the paper provides comparative results but doesn't isolate which specific semantic relation types (e.g., hypernymy, meronymy) show the most pronounced differences between LLMs and classical models.

### Open Question 2
What linguistic or architectural properties cause certain classical models like SimCSE to outperform LLMs in contextualized sentence-level similarity tasks? SimCSE consistently shows lower Anchor Inter-Contextual Variance and Sentence Meaning Variance across multiple contextualized tasks, despite being smaller than the LLMs tested. This remains unresolved because the paper identifies performance differences but doesn't analyze the underlying reasons for SimCSE's strong performance in contextualized settings.

### Open Question 3
How does the choice of embedding extraction method (token-level vs sentence-level vs decontextualized) affect the interpretability and accuracy trade-off across different linguistic tasks? The paper uses three different extraction methods and observes varying performance patterns, but doesn't systematically analyze how extraction method choice impacts the interpretability-accuracy trade-off. This remains unresolved because the paper demonstrates that different extraction methods yield different results but doesn't provide a framework for understanding when and why to choose one method over another.

## Limitations
- Synthetic sentence generation relies on a single prompt-based model (Claude), potentially introducing systematic biases
- Evaluation focuses primarily on cosine similarity metrics, potentially missing other important embedding properties
- Decontextualized analysis limited to ~80,000 words from WordNet, which may not fully represent natural language diversity

## Confidence

- **High Confidence**: LLM superiority in contextualized anchor deviation and classical model strength in sentence-level similarity tasks
- **Medium Confidence**: Decontextualized performance differences due to training corpus effects
- **Medium Confidence**: Task-specific advantages of different model types across linguistic variations

## Next Checks

1. Replicate the contextualized task analysis using multiple sentence generation models to assess robustness to generation method
2. Extend decontextualized evaluation to include additional semantic benchmarks beyond analogy tasks
3. Conduct ablation studies removing the LLM context tokens to isolate the contribution of contextual information to performance differences