---
ver: rpa2
title: 'Perturb and Recover: Fine-tuning for Effective Backdoor Removal from CLIP'
arxiv_id: '2412.00727'
source_url: https://arxiv.org/abs/2412.00727
tags:
- clip
- clean
- cleanclip
- backdoor
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Existing backdoor cleaning methods for CLIP rely on strong data
  augmentations but fail against structured triggers like stripes, triangles, or overlayed
  text. We propose PAR (Perturb and Recover), a fine-tuning method that perturbs the
  model away from its poisoned state while preserving clean performance.
---

# Perturb and Recover: Fine-tuning for Effective Backdoor Removal from CLIP

## Quick Facts
- **arXiv ID**: 2412.00727
- **Source URL**: https://arxiv.org/abs/2412.00727
- **Reference count**: 40
- **Primary result**: PAR achieves significantly lower attack success rates across multiple encoders and attacks compared to baselines like CleanCLIP and RoCLIP, while maintaining high clean accuracy.

## Executive Summary
Existing backdoor cleaning methods for CLIP models rely on strong data augmentations but fail against structured triggers like stripes, triangles, or overlayed text. The paper proposes PAR (Perturb and Recover), a fine-tuning method that perturbs the model away from its poisoned state while preserving clean performance. PAR introduces a thresholded perturbation loss that actively unlearns backdoor correlations by pushing model embeddings beyond a critical distance threshold, achieving significantly lower attack success rates across multiple encoders and attacks compared to baselines.

## Method Summary
PAR is a fine-tuning method that removes backdoors from poisoned CLIP models by introducing a thresholded perturbation loss (LPERT) that measures the ℓ2-distance between clean and poisoned model embeddings. When this distance exceeds a threshold τ, the perturbation loss stops contributing, allowing the standard CLIP loss to recover clean performance. The method uses minimal augmentations (Gaussian noise and CutOut) and works effectively even with synthetic data, avoiding the overfitting issues of augmentation-heavy approaches. Training uses AdamW optimizer with custom learning rate scheduling and a batch size of 512x4 for ResNet50/ViT-B/32 or 120x8 for ViT-L/14.

## Key Results
- PAR achieves significantly lower attack success rates (ASR) across multiple encoders (ResNet50, ViT-B/32) and attacks (BadNet, Blended, BadCLIP) compared to baselines.
- The method maintains high clean accuracy while removing backdoors, outperforming CleanCLIP and RoCLIP baselines.
- PAR is effective using only synthetic data, reducing the need for real clean data without sacrificing backdoor removal performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The thresholded perturbation loss (LPERT) actively unlearns backdoor correlations by pushing model embeddings away from poisoned state until a specified distance is reached.
- Mechanism: LPERT measures the ℓ2-distance between clean and poisoned model embeddings for both image and text encoders, then thresholds this distance at τ. When Sϕ and Sψ exceed τ, the perturbation loss stops contributing, leaving only the standard CLIP loss to recover clean performance.
- Core assumption: Backdoor triggers create localized spurious correlations that can be removed by perturbing embeddings beyond a critical distance threshold.
- Evidence anchors:
  - [abstract]: "the training objective in PAR leverages a term to perturb the image and text encoders in CLIP away from their original poisoned state: makes the model 'forget' the spurious correlations"
  - [section]: "As one can assume that the spurious backdoor correlation is destroyed when the model is perturbed enough, we threshold Sϕ and Sψ in the objective at τ"
  - [corpus]: Weak - the cited papers focus on general backdoor defenses but don't specifically validate the thresholded perturbation mechanism for CLIP models
- Break condition: If backdoor correlations are distributed across the entire embedding space rather than localized, the threshold-based perturbation may not effectively target them.

### Mechanism 2
- Claim: Avoiding strong augmentations prevents overfitting to augmentation patterns rather than actual trigger removal.
- Mechanism: Unlike CleanCLIP and RoCLIP that use AutoAugment and EDA, PAR uses only Gaussian noise and CutOut with minimal probability. This prevents the model from learning to distinguish clean vs poisoned samples based on augmentation artifacts.
- Core assumption: Structured triggers (stripes, triangles, text) are uncorrelated with the image transformations used in standard augmentations.
- Evidence anchors:
  - [section]: "We hypothesize the reason for this is that structured triggers are unrelated to the image changes done by the augmentation operations"
  - [section]: "These two techniques train from scratch (random initialized model) which we think is unrealistic for large foundation models such as CLIP"
  - [corpus]: Weak - the cited works don't directly compare augmentation-based vs non-augmentation-based backdoor removal for CLIP
- Break condition: If structured triggers happen to overlap with augmentation transformations (e.g., stripes aligned with horizontal flip patterns), the defense could fail.

### Mechanism 3
- Claim: Synthetic data can effectively replace real clean data for backdoor removal when using PAR.
- Mechanism: PAR's loss function doesn't rely on distribution matching between clean and poisoned data - it only requires the model to move away from poisoned embeddings while maintaining CLIP loss. This makes it robust to domain shifts between real and synthetic data.
- Core assumption: The backdoor correlation is learned during poisoning and can be unlearnt regardless of the clean data distribution.
- Evidence anchors:
  - [abstract]: "we illustrate that our approach is effective even only with synthetic text-image pairs, i.e. without access to real training data"
  - [section]: "In Fig. 3 shows that PAR with SynthCLIP (SynC) can completely clean a poisoned model with BadNet-Stripes"
  - [corpus]: Weak - the cited papers don't specifically validate synthetic data for backdoor removal in CLIP
- Break condition: If the synthetic data lacks sufficient diversity or fails to activate the backdoor trigger patterns, the model may not effectively unlearn the spurious correlations.

## Foundational Learning

- Concept: Contrastive learning and CLIP architecture
  - Why needed here: Understanding how CLIP aligns image and text embeddings is crucial for grasping why the perturbation loss works
  - Quick check question: What is the purpose of the two cosine similarity terms in the CLIP loss?

- Concept: Backdoor attack mechanisms
  - Why needed here: Knowing how triggers are embedded and how they affect model behavior explains why different defenses succeed or fail
  - Quick check question: Why would structured triggers like stripes bypass augmentation-based defenses while random noise triggers don't?

- Concept: Fine-tuning vs training from scratch
  - Why needed here: The paper focuses on fine-tuning poisoned models rather than retraining, which has different computational and practical constraints
  - Quick check question: What is the computational difference between fine-tuning a poisoned CLIP model vs training from scratch?

## Architecture Onboarding

- Component map:
  CLIP model (vision encoder, text encoder, joint embedding space) -> Poisoning mechanism (trigger application, caption modification) -> Cleaning pipeline (data loading, augmentation, PAR loss computation, optimization) -> Evaluation framework (zero-shot classification, retrieval tasks, ASR measurement)

- Critical path:
  1. Load poisoned CLIP model
  2. Load clean data (real or synthetic)
  3. Compute LCLIP loss on batch
  4. Compute LPERT loss (thresholded embedding distance)
  5. Backpropagate combined loss (LCLIP - LPERT)
  6. Update model parameters
  7. Evaluate clean accuracy and ASR

- Design tradeoffs:
  - Threshold τ controls the balance between backdoor removal and clean performance preservation
  - Custom learning schedule vs standard cosine decay affects convergence speed
  - Number of cleaning epochs impacts final performance but increases computational cost

- Failure signatures:
  - ASR remains high despite training → threshold τ too low or learning rate too small
  - Clean accuracy drops significantly → threshold τ too high or learning rate too large
  - Training loss plateaus early → insufficient perturbation or poor learning rate scheduling

- First 3 experiments:
  1. Test PAR on BadNet-Stripes poisoned ResNet50 with default τ=2.15 and standard cosine schedule
  2. Sweep τ values for BadNet-Stripes to find optimal clean-ASR tradeoff
  3. Test PAR with synthetic data (SynthCLIP) vs real data (CC3M) for the same poisoned model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of synthetic data generation method affect the effectiveness of backdoor removal in CLIP models?
- Basis in paper: [explicit] The paper demonstrates that synthetic data can effectively clean poisoned CLIP models using PAR, but does not explore the impact of different synthetic data generation techniques.
- Why unresolved: The paper uses a specific synthetic dataset (SynthCLIP) without comparing it to other methods or analyzing the influence of synthetic data quality on cleaning performance.
- What evidence would resolve it: Comparative experiments using different synthetic data generation approaches (e.g., different diffusion models, GANs) and analysis of how data quality metrics correlate with cleaning effectiveness.

### Open Question 2
- Question: What is the impact of structured triggers on multi-modal models beyond CLIP, such as those used in LVLMs?
- Basis in paper: [inferred] The paper shows that structured triggers can bypass cleaning methods for CLIP and mentions that CLIP encoders are used in LVLMs, but does not investigate the effectiveness of structured triggers on these larger models.
- Why unresolved: The paper focuses on CLIP models and does not extend the analysis to LVLMs, leaving uncertainty about the transferability of the findings.
- What evidence would resolve it: Experiments testing structured triggers on LVLMs and analysis of how the backdoor removal methods perform on these models.

### Open Question 3
- Question: How does the threshold parameter τ in PAR interact with different types of backdoor triggers and model architectures?
- Basis in paper: [explicit] The paper uses a fixed τ value for all experiments and mentions that it can be adjusted for a trade-off between clean accuracy and ASR, but does not explore the optimal τ for different scenarios.
- Why unresolved: The paper does not provide a systematic study of how τ should be chosen based on trigger type or model architecture, leaving users to rely on empirical tuning.
- What evidence would resolve it: A comprehensive study mapping τ values to different trigger types and model architectures, potentially with guidelines for selection.

### Open Question 4
- Question: What are the limitations of PAR when dealing with more complex or adaptive backdoor attacks?
- Basis in paper: [inferred] The paper demonstrates PAR's effectiveness against several backdoor attacks but does not test it against adaptive or more sophisticated attacks that could potentially evade the cleaning process.
- Why unresolved: The paper focuses on standard and structured triggers but does not explore the robustness of PAR against evolving attack strategies.
- What evidence would resolve it: Testing PAR against adaptive attacks and analyzing its performance under varying attack conditions.

## Limitations

- Data diversity limitations: The evaluation focuses on ImageNet and COCO datasets, which may not represent the full diversity of real-world scenarios where CLIP models are deployed.
- Trigger specificity: The evaluation concentrates on three structured trigger types (stripes, triangles, text overlays), leaving the method's effectiveness against more complex or adaptive triggers unproven.
- Threshold sensitivity: While the paper claims τ=2.15 works well across different attacks and models, the sensitivity analysis for this hyperparameter is limited to a single attack type.

## Confidence

**High confidence**: The core mechanism of thresholded perturbation loss is technically sound and the experimental methodology is reproducible. The comparison against baselines (CleanCLIP, RoCLIP) shows clear performance improvements across multiple metrics.

**Medium confidence**: The claim about synthetic data effectiveness is supported by experiments but limited to specific synthetic generation methods. The mechanism explaining why augmentation-based defenses fail against structured triggers is plausible but lacks comprehensive empirical validation across different augmentation strategies.

**Low confidence**: The generalizability of τ=2.15 across all possible backdoor scenarios and the long-term stability of the cleaned models after deployment have not been adequately demonstrated.

## Next Checks

1. **Cross-trigger generalization test**: Apply PAR to poisoned CLIP models with triggers not seen in the original evaluation (e.g., checkerboard patterns, gradient-based triggers, or triggers that overlap with augmentation transformations) to verify the method's robustness against diverse attack patterns.

2. **Synthetic data diversity evaluation**: Test PAR using multiple synthetic data generation approaches (different text-image alignment methods, varying domain distributions) to confirm the claimed robustness to synthetic data quality and diversity.

3. **Long-term stability assessment**: Evaluate cleaned models after extended periods of fine-tuning on benign data or after exposure to new adversarial examples to assess whether the backdoor correlations can re-emerge over time.