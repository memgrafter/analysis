---
ver: rpa2
title: 'Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs
  and MLLMs'
arxiv_id: '2402.12424'
source_url: https://arxiv.org/abs/2402.12424
tags:
- table
- llms
- prompting
- gpt-4
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how well large language models (LLMs) handle
  tabular data, comparing both text-based and image-based table representations. It
  tests five text formats (vanilla, row-identifier, bracket, column-JSON, row-JSON)
  and three image formats (vanilla, column-color, row-color) across six datasets (WikiTQ,
  FinQA, TabFact, LogicNLG, E2E, ToTTo) using six prompting strategies.
---

# Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs

## Quick Facts
- arXiv ID: 2402.12424
- Source URL: https://arxiv.org/abs/2402.12424
- Authors: Naihao Deng; Zhenjie Sun; Ruiqi He; Aman Sikka; Yulong Chen; Lin Ma; Yue Zhang; Rada Mihalcea
- Reference count: 39
- Key outcome: Image-based table representations perform comparably to text-based ones, with image-based approaches sometimes outperforming text-based methods, especially for complex reasoning tasks.

## Executive Summary
This paper evaluates how well large language models handle tabular data by comparing text-based and image-based table representations. The study tests five text formats and three image formats across six datasets using six prompting strategies. The research finds that multimodal LLMs maintain decent performance with image-based representations, sometimes outperforming text-based methods for complex reasoning tasks. Chain-of-thought prompting consistently improves performance across image-based representations, while different text representations significantly impact performance with bracket formatting often yielding better results.

## Method Summary
The study evaluates LLM performance on tabular data using both text and image representations. Five text-based representations (vanilla, row-identifier, bracket, column-JSON, row-JSON) and three image-based representations (vanilla, column-color, row-color) are tested across six datasets. Six prompting strategies (vanilla, chain-of-thought, expert) are applied to both open-source models like Llama-2 and closed-source models like GPT-4. The evaluation measures accuracy for question-answering and fact-checking tasks, with additional ROUGE-L scores for table-to-text generation tasks.

## Key Results
- Image-based table representations perform comparably to text-based ones, with image-based approaches sometimes outperforming text-based methods
- Chain-of-thought prompting consistently improves performance across image-based representations
- Different text representations significantly impact performance, with bracket formatting often yielding better results
- Closed-source models like GPT-4 significantly outperform open-source models like Llama-2, with performance gaps reaching up to 22.9% on TabFact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image-based table representations reduce cognitive load for LLMs when handling long text contexts with complex numerical relations.
- Mechanism: When tables are represented as images, multimodal LLMs can leverage their visual processing capabilities to directly perceive table structure and cell relationships, bypassing the need to parse dense linearized text. This visual modality allows models to more efficiently extract relevant information from lengthy contexts containing multiple numerical relations.
- Core assumption: Multimodal LLMs have sufficiently developed visual processing capabilities to effectively interpret table structures from images, and this visual processing is more efficient than parsing equivalent information from text for complex reasoning tasks.
- Evidence anchors:
  - [abstract]: "LLMs maintain decent performance when we use image-based table representations. Sometimes, image-based table representations can make LLMs perform better."
  - [section]: "As FinQA focuses on financial question answering with long context and many numerical relations, we hypothesize that representing tables as images can help LLMs in complex reasoning. Since these multimodal LLMs have a strong capability over visual input, representing tables as images may reduce the cognitive load for LLMs to parse and understand dense text."
  - [corpus]: Weak - No direct corpus evidence for this specific mechanism, though related work on multimodal table understanding exists.

### Mechanism 2
- Claim: Chain-of-thought prompting enhances LLMs' ability to fuse information across text and vision modalities when processing image-based tables.
- Mechanism: Chain-of-thought prompting explicitly outlines the reasoning process, helping LLMs better understand context and relationships between different rows and columns in the table. This structured reasoning approach may help models align visual information from table images with question text, improving information fusion across modalities.
- Core assumption: LLMs can effectively utilize explicit reasoning chains to coordinate information processing across different input modalities (text and vision).
- Evidence anchors:
  - [abstract]: "Chain-of-thought prompting consistently improves performance across image-based representations."
  - [section]: "We observe that chain-of-thought prompting helps multimodal LLMs in all image-based table representations... By explicitly outlining the reasoning process, chain-of-thought prompting may help LLMs better understand the context and relationships between different rows and columns in the table, therefore better aligning this visual information with the question text."
  - [corpus]: Weak - Limited corpus evidence directly supporting this specific multimodal reasoning mechanism.

### Mechanism 3
- Claim: Bracket formatting in text-based table representations improves LLM performance by leveraging pre-training exposure to structured code formats.
- Mechanism: LLMs, having been pre-trained on code repositories where brackets are fundamental components, have developed proficiency in recognizing and interpreting bracketed structures. When brackets are used to distinguish rows in table representations, this familiar structure helps LLMs better parse table relationships and boundaries.
- Core assumption: The pre-training corpus contains sufficient code examples with bracket usage that models have learned to effectively interpret bracketed structures, and this learned capability transfers to table parsing tasks.
- Evidence anchors:
  - [abstract]: "Different text representations significantly impact performance, with bracket formatting often yielding better results."
  - [section]: "We suspect that the simple linearized table representation does not have a clear boundary between rows, which may lead to confusion or misinterpretation of data relationships... We conjecture that LLMs may be familiar with brackets from their pre-training exposure."
  - [corpus]: Moderate - Related work on table understanding suggests LLMs benefit from explicit structural markers, supporting the general concept though not specifically bracket formatting.

## Foundational Learning

- Concept: Multimodal reasoning capabilities in LLMs
  - Why needed here: Understanding how LLMs process and integrate information from both text and image inputs is crucial for evaluating their table reasoning abilities across different representation formats.
  - Quick check question: How does an LLM's ability to process visual information differ from its text processing capabilities, and why might this matter for table understanding tasks?

- Concept: Prompt engineering and its impact on model performance
  - Why needed here: Different prompting strategies (vanilla, chain-of-thought, expert) significantly affect how well LLMs perform on table-related tasks, making it essential to understand how to craft effective prompts for structured data.
  - Quick check question: What are the key differences between vanilla prompting, chain-of-thought prompting, and expert prompting, and how might each approach benefit or hinder table reasoning tasks?

- Concept: Representation learning and data structure encoding
  - Why needed here: How tabular data is represented (linearized text, bracketed text, JSON, or images) fundamentally affects how LLMs can access and reason about the information, making representation choices critical for performance.
  - Quick check question: Why might different table representations (like bracket formatting vs. simple linearization) lead to significantly different LLM performance, even when conveying the same underlying information?

## Architecture Onboarding

- Component map: Table representation converters -> Prompt generators -> LLM evaluation pipeline
- Critical path: raw table -> representation conversion -> prompt generation -> LLM API call -> response parsing -> metric calculation
- Design tradeoffs: Text representations offer reproducibility and lower computational cost but may struggle with complex structure; image representations leverage multimodal capabilities but require visual processing and may face quality issues; simpler representations are easier to implement but may miss important structural cues that more sophisticated formats capture
- Failure signatures: Performance degradation often manifests as consistent underperformance on specific table types, sensitivity to representation format variations, chain-of-thought prompting backfiring on simpler tasks, or open-source models showing significant gaps compared to closed-source alternatives
- First 3 experiments:
  1. Compare vanilla prompting with chain-of-thought prompting on a single dataset using one text-based representation to establish baseline prompting effects
  2. Test image-based representations with chain-of-thought prompting on the same dataset to evaluate multimodal benefits
  3. Run a factorial experiment varying both table representation and prompting strategy on a simpler dataset to identify interaction effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural changes to transformer-based models could better align them with tabular data structures?
- Basis in paper: [explicit] The paper mentions that some researchers adapt the attention mechanism to better align transformer-based models with tabular data (Zhang et al., 2020; Yang et al., 2022) or design hierarchical encoding to capture the table structure (Wang et al., 2021).
- Why unresolved: While the paper acknowledges these approaches exist, it doesn't investigate which specific architectural modifications yield the most significant performance improvements for table-related tasks.
- What evidence would resolve it: A systematic comparison of different architectural modifications (attention mechanism adaptations, hierarchical encodings, etc.) across multiple table-related benchmarks would provide concrete evidence of which approaches work best.

### Open Question 2
- Question: How does the performance gap between open-source and closed-source LLMs on tabular data tasks change as open-source models scale in parameter size?
- Basis in paper: [explicit] The paper observes that even the best-performing open-source model (Llama-2-70B) has a significant performance gap compared to GPT-4 (up to 22.9% on TabFact).
- Why unresolved: The paper only compares one version of Llama-2 against GPT-4. It's unclear whether the performance gap narrows with larger parameter sizes or plateaus at a certain point.
- What evidence would resolve it: Testing a range of open-source models with different parameter sizes (e.g., 7B, 13B, 70B, 175B) against closed-source models on the same benchmarks would show how the performance gap scales with model size.

### Open Question 3
- Question: What is the optimal balance between text and image-based table representations for multimodal LLMs?
- Basis in paper: [inferred] The paper finds that multimodal LLMs sometimes perform better with image representations (especially for complex reasoning tasks like FinQA), but also shows that providing both formats simultaneously doesn't consistently improve performance.
- Why unresolved: The paper doesn't systematically explore the trade-offs between different combinations of text and image representations, or identify when each format is most beneficial.
- What evidence would resolve it: A detailed ablation study testing different combinations of text and image representations (e.g., only text, only image, text with highlighted cells, image with highlighted cells, both formats) across various task types would reveal the optimal balance.

## Limitations
- Image-based representations rely on specific visual processing capabilities that may not generalize across all model architectures
- Bracket formatting advantage assumes pre-training exposure to bracketed structures without direct evidence of this exposure
- Performance gaps between open-source and closed-source models suggest potential dataset or evaluation biases
- Chain-of-thought prompting mechanisms for multimodal reasoning remain under-specified

## Confidence

- **High Confidence**: The comparative performance results between text and image representations are well-supported by the experimental data
- **Medium Confidence**: The explanations for why image representations sometimes outperform text (cognitive load reduction) and why chain-of-thought prompting helps (information fusion) are plausible but require further validation
- **Low Confidence**: The specific mechanisms by which bracket formatting improves performance and the extent to which pre-training exposure drives this effect are speculative without direct corpus evidence

## Next Checks

1. **Ablation Study on Structural Markers**: Systematically remove or modify structural markers (brackets, row identifiers, JSON formatting) in text representations to isolate which elements drive performance differences

2. **Cross-Model Generalization Test**: Evaluate the same table representations and prompting strategies across a broader range of multimodal models with varying visual processing capabilities to determine if observed patterns hold universally

3. **Visual Quality Sensitivity Analysis**: Test image-based representations across varying image qualities and resolutions to establish the robustness of visual processing advantages and identify failure thresholds