---
ver: rpa2
title: CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections
arxiv_id: '2411.19346'
source_url: https://arxiv.org/abs/2411.19346
tags:
- learning
- clip
- vision
- visual
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes NoLA (No Labels Attached), a label-free method\
  \ to adapt CLIP-based vision-language models for zero-shot image classification\
  \ by leveraging self-supervised learning models (DINO) and large language models\
  \ (LLMs). The core idea is to use LLM-generated descriptions to create a robust\
  \ class description embedding (CDE) classifier, which in turn generates pseudo-labels\
  \ to align DINO\u2019s visual features with CLIP\u2019s embedding space."
---

# CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections

## Quick Facts
- arXiv ID: 2411.19346
- Source URL: https://arxiv.org/abs/2411.19346
- Reference count: 19
- NoLA achieves 3.6% average absolute gain over LaFTer and 11.91% improvement over zero-shot CLIP

## Executive Summary
NoLA (No Labels Attached) presents a novel label-free approach to adapt CLIP-based vision-language models for zero-shot image classification using unlabeled image collections. The method combines LLM-generated class descriptions, DINO self-supervised visual features, and prompt tuning to create a powerful zero-shot classifier without requiring any labeled data. By leveraging the complementary strengths of language models for rich semantic descriptions and self-supervised learning for robust visual features, NoLA demonstrates significant performance improvements over existing label-free methods across 11 diverse image classification datasets.

## Method Summary
NoLA operates through a three-stage process: First, it generates rich class descriptions using LLMs (GPT-3.5/GPT-4o) prompted with dataset-specific questions, which are embedded via CLIP's text encoder to create a Class Description Embedding (CDE) classifier. Second, a DINO-based Labelling (DL) network aligns DINO's self-supervised visual features to CLIP's embedding space using pseudo-labels from the CDE classifier. Finally, the method employs prompt tuning on CLIP's vision encoder, guided by the DL network's pseudo-labels using a FixMatch-style consistency regularization approach between weakly and strongly augmented views of training images.

## Key Results
- Achieves 3.6% average absolute gain over state-of-the-art LaFTer method
- Shows 11.91% improvement over zero-shot CLIP baseline
- Demonstrates consistent performance gains across 11 diverse image classification datasets including ImageNet, CIFAR-10/100, Caltech-101, and others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated descriptions enrich class representations, making them more discriminative for zero-shot classification.
- Mechanism: By prompting an LLM with class names and dataset-specific questions, the resulting descriptions capture nuanced visual attributes beyond simple class names. These descriptions are embedded via CLIP's text encoder and averaged to form a Class Description Embedding (CDE) classifier, which provides richer semantic signals than CLIP's default "a photo of a [CLS]" prompt.
- Core assumption: The LLM's general knowledge base includes relevant visual and contextual attributes for the target classes, and the averaging process preserves discriminative features.
- Evidence anchors:
  - [abstract] "We generate robust textual feature embeddings that more accurately represent object classes by leveraging class-specific descriptions from LLMs, enabling more effective zero-shot classification compared to CLIP's default name-specific prompts."
  - [section] "We compose the classifier by generating finer descriptions catered towards the target dataset by prompting an LLM model... This generates K descriptions ωC for each class C, enriched by the domain knowledge of the LLM, giving K × C, class descriptions."
- Break condition: If the LLM lacks knowledge about the target classes or generates overly generic descriptions, the CDE classifier will not provide additional discriminative power over CLIP's default prompts.

### Mechanism 2
- Claim: DINO's self-supervised visual features can be aligned to CLIP's embedding space to create a stronger pseudo-labeler.
- Mechanism: A frozen DINO ViT-B/16 backbone extracts visual features, which are then aligned to CLIP's embedding space via an alignment module trained using pseudo-labels from the CDE classifier. This creates a DINO-based Labelling (DL) network that generates pseudo-labels with better visual discriminative cues than CLIP's native features.
- Core assumption: DINO's self-supervised pre-training on ImageNet captures generalizable visual features that can be aligned to CLIP's embedding space, and the alignment module successfully transfers this visual knowledge.
- Evidence anchors:
  - [abstract] "We generate robust textual feature embeddings that more accurately represent object classes by leveraging class-specific descriptions from LLMs... These textual embeddings are then used to produce pseudo-labels to train an alignment module that integrates the complementary strengths of LLM description-based textual embeddings & DINO's visual features."
  - [section] "We make use of the LLM-enriched CDE classifier, to align a self-supervised learning (SSL) pre-trained vision backbone to the joint embedding space of the VLM."
- Break condition: If DINO's features are too domain-specific to ImageNet and don't generalize to the target dataset, or if the alignment module fails to properly map features to CLIP's embedding space, the pseudo-labeling quality will degrade.

### Mechanism 3
- Claim: Prompt tuning CLIP's vision encoder using DINO-assisted pseudo-labels adapts the model to the target dataset without labeled data.
- Mechanism: Learnable visual prompts are prepended to CLIP's frozen vision encoder. The DL network provides pseudo-labels for strongly augmented views of training images, while weakly augmented views are used for consistency regularization. This FixMatch-style approach adapts CLIP's vision encoder to the target dataset's visual distribution using only unlabeled data.
- Core assumption: The pseudo-labels from the DL network are sufficiently accurate to guide prompt learning, and the consistency regularization between weak and strong views stabilizes the adaptation process.
- Evidence anchors:
  - [abstract] "Finally, we prompt-tune CLIP's vision encoder through DINO-assisted supervision using the trained alignment module."
  - [section] "We employ our trained DL network as an auto-labeller to adapt the vision-language vision encoder through prompt tuning."
- Break condition: If the pseudo-label quality is too low (high error rate), prompt tuning will converge to a suboptimal solution. If the augmentation strategy doesn't preserve class semantics, the consistency regularization may hurt rather than help.

## Foundational Learning

- Concept: Contrastive learning in vision-language models
  - Why needed here: Understanding how CLIP learns to align image and text embeddings through contrastive objectives is crucial for grasping why the CDE classifier and alignment module work.
  - Quick check question: What is the primary training objective used to train CLIP, and how does it enable zero-shot classification?

- Concept: Self-supervised learning and visual feature extraction
  - Why needed here: DINO's role as a feature extractor relies on understanding self-supervised learning principles and how they differ from supervised approaches in capturing visual semantics.
  - Quick check question: How does DINO's self-supervised pre-training on ImageNet enable it to extract rich visual features without labeled data?

- Concept: Prompt learning and parameter-efficient fine-tuning
  - Why needed here: The adaptation of CLIP through prompt tuning (rather than full fine-tuning) is a key efficiency and effectiveness factor in NoLA's design.
  - Quick check question: What are the advantages of prompt tuning over full fine-tuning in adapting pre-trained vision-language models?

## Architecture Onboarding

- Component map: LLM → generates class descriptions → CLIP text encoder → embeds descriptions → CDE classifier → DINO ViT-B/16 → extracts visual features → Alignment module h → maps DINO features to CLIP embedding space → DINO-based Labelling (DL) network → generates pseudo-labels → Learnable visual prompts → prepended to CLIP vision encoder → FixMatch-style training loop → adapts CLIP vision encoder

- Critical path: LLM descriptions → CDE classifier → DL network training → prompt tuning CLIP vision encoder → improved zero-shot classification

- Design tradeoffs:
  - Using DINO vs. CLIP vision encoder: DINO provides richer features but requires alignment; CLIP is already in the right space but may be less discriminative.
  - Number of pseudo-labels (k): Higher k provides more training data but may include more noise; lower k is cleaner but may underutilize available data.
  - Prompt tuning vs. full fine-tuning: Prompt tuning is more parameter-efficient but may have limited adaptation capacity.

- Failure signatures:
  - No improvement over zero-shot CLIP: Likely issues with either the CDE classifier quality or the DL network alignment.
  - Performance degradation: Possibly due to poor pseudo-label quality or inappropriate augmentation strategies.
  - Training instability: Could indicate issues with the balance between weak and strong augmentation or learning rate problems.

- First 3 experiments:
  1. Verify CDE classifier effectiveness: Compare zero-shot classification using CLIP's default prompts vs. CDE classifier on a small validation set.
  2. Test DL network alignment: Measure how well DINO features align to CLIP's embedding space by checking pseudo-label accuracy on a small labeled subset (if available for validation).
  3. Ablation of prompt learning: Compare CLIP's zero-shot performance with and without prompt tuning on a single dataset to confirm the adaptation mechanism works.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the content, several open questions emerge:

### Open Question 1
- Question: How does the performance of NoLA change when using different LLM models (e.g., GPT-4, Claude) for generating class descriptions?
- Basis in paper: The authors mention using GPT-3.5 and GPT-4o for generating descriptions, but do not compare performance across different LLM models.
- Why unresolved: The paper only provides a comparison between GPT-3.5 and GPT-4o descriptions, leaving open the question of how other LLM models might affect performance.
- What evidence would resolve it: Systematic experiments comparing NoLA's performance using class descriptions generated by different LLM models (GPT-4, Claude, etc.) on the same datasets.

### Open Question 2
- Question: What is the impact of varying the number of template questions (N) and descriptions per class (K) on NoLA's performance?
- Basis in paper: The authors mention using N dataset-specific questions and K descriptions per class but do not explore how varying these parameters affects performance.
- Why unresolved: The paper does not provide an ablation study on the sensitivity of performance to the number of questions and descriptions used per class.
- What evidence would resolve it: Experiments systematically varying N and K values across different datasets to determine optimal ranges and performance sensitivity.

### Open Question 3
- Question: How does NoLA perform on video classification tasks compared to image classification?
- Basis in paper: The authors evaluate NoLA on 11 image classification datasets but do not test its applicability to video data.
- Why unresolved: While the framework is described generally, its performance on sequential or temporal data like videos is not investigated.
- What evidence would resolve it: Applying NoLA to video classification datasets (e.g., Kinetics, Something-Something) and comparing performance with image-based results.

## Limitations
- The method's reliance on LLM-generated descriptions introduces potential variability based on the specific LLM version and prompt engineering choices, though this is not thoroughly explored.
- The alignment module between DINO features and CLIP's embedding space lacks detailed architectural specifications, making exact reproduction challenging.
- The computational cost of generating descriptions for all classes across multiple datasets is not addressed, which could limit practical applicability.

## Confidence
- High confidence: The core three-stage framework (CDE classifier → DL network → prompt tuning) is well-supported by experimental results showing consistent improvements over baselines across 11 diverse datasets.
- Medium confidence: The claim that LLM-generated descriptions provide more discriminative class representations is supported by ablation studies, but the specific contribution of different description qualities is not fully isolated.
- Medium confidence: The effectiveness of DINO feature alignment is demonstrated empirically, but the alignment module's design and its sensitivity to different DINO backbones are not thoroughly examined.

## Next Checks
1. **Ablation of LLM Description Quality**: Systematically evaluate how different LLM models (GPT-3.5 vs GPT-4o) and prompt variations affect classification performance to quantify the sensitivity to description quality.

2. **Alignment Module Sensitivity**: Test the impact of using different self-supervised backbones (e.g., MAE, MAE-inpainting) and varying alignment module architectures on pseudo-label quality and downstream performance.

3. **Cross-Domain Generalization**: Evaluate NoLA's performance on datasets with significantly different visual characteristics (e.g., medical imaging, satellite imagery) than the evaluated datasets to test domain transfer capability.