---
ver: rpa2
title: 'DeepSeek LLM: Scaling Open-Source Language Models with Longtermism'
arxiv_id: '2401.02954'
source_url: https://arxiv.org/abs/2401.02954
tags:
- data
- scaling
- deepseek
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepSeek LLM is an open-source large language model trained from
  scratch on 2 trillion tokens in English and Chinese. It achieves state-of-the-art
  performance on various benchmarks, particularly in code, mathematics, and reasoning
  tasks, surpassing models like LLaMA-2 70B and GPT-3.5.
---

# DeepSeek LLM: Scaling Open-Source Language Models with Longtermism

## Quick Facts
- arXiv ID: 2401.02954
- Source URL: https://arxiv.org/abs/2401.02954
- Reference count: 40
- DeepSeek LLM achieves state-of-the-art performance on code, mathematics, and reasoning benchmarks, surpassing models like LLaMA-2 70B and GPT-3.5.

## Executive Summary
DeepSeek LLM is an open-source large language model trained from scratch on 2 trillion tokens in English and Chinese. The project leverages scaling laws to guide hyperparameter selection and data allocation, resulting in models that achieve state-of-the-art performance on various benchmarks, particularly in code, mathematics, and reasoning tasks. The model's success is attributed to extensive supervised fine-tuning and direct preference optimization, with the 67B parameter version outperforming LLaMA-2 70B and approaching GPT-4 levels on certain tasks.

## Method Summary
DeepSeek LLM employs a scaling laws-guided approach to hyperparameter selection, data allocation, and model scaling. The model architecture is based on LLaMA with modifications for 7B and 67B parameter configurations, including Grouped-Query Attention for the 67B model. Training involves pre-training on 2 trillion tokens with a multi-step learning rate scheduler, followed by supervised fine-tuning with 1.5 million instruction instances and direct preference optimization for alignment. The approach balances model scale and data scale according to power-law relationships, with higher quality data allowing more compute budget to be allocated to model scaling.

## Key Results
- DeepSeek LLM 67B outperforms LLaMA-2 70B on benchmarks like GSM8K, HumanEval, MATH, and C-Eval.
- The model achieves state-of-the-art performance on CMMLU and demonstrates strong open-ended evaluation capabilities.
- DeepSeek LLM 7B shows competitive performance despite its smaller size, though it slightly degrades with system prompts.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling laws allow predictable performance improvements when increasing compute budget.
- Mechanism: Model performance follows a power-law relationship with compute budget C, where optimal scaling balances model scale M and data scale D according to M ∝ C^a and D ∝ C^b.
- Core assumption: The compute budget can be approximated as C = M * D, and performance scales predictably with this relationship.
- Evidence anchors:
  - [abstract]: "Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective."
  - [section 3.2]: "This relationship, which we refer to as the scaling laws of hyperparameters, provides an empirical framework for determining the optimal hyperparameters."
  - [corpus]: Weak. No direct citations found. Corpus neighbors discuss scaling laws but do not specifically anchor this mechanism.
- Break condition: If the power-law relationship breaks down at extreme scales or if the compute budget approximation becomes inaccurate.

### Mechanism 2
- Claim: The choice of dataset significantly impacts the optimal model/data scaling strategy.
- Mechanism: Higher quality data allows more compute budget to be allocated to model scaling rather than data scaling, as the model can learn more efficiently from higher quality data.
- Core assumption: Data quality affects the difficulty of the learning task, with higher quality data being less difficult to learn from.
- Evidence anchors:
  - [section 3.3]: "This implies that high-quality data can drive the training of larger models given the same data scale."
  - [section 3.3]: "The higher the data quality, the more the increased compute budget should be allocated to model scaling."
  - [corpus]: Weak. No direct citations found. Corpus neighbors discuss data quality but do not specifically anchor this mechanism.
- Break condition: If the relationship between data quality and learning difficulty is not linear or if the definition of "quality" is not consistent across datasets.

### Mechanism 3
- Claim: Multi-step learning rate scheduling provides similar performance to cosine scheduling while enabling continual training.
- Mechanism: The multi-step scheduler maintains performance by reducing the learning rate at specific training milestones, allowing for reuse of earlier training stages when scaling up.
- Core assumption: The learning rate reduction schedule is optimized for the specific model and dataset being trained.
- Evidence anchors:
  - [section 2.3]: "Despite differences in the loss reduction trend during training, the final performance using a multi-step learning rate scheduler is essentially consistent with that of a cosine scheduler."
  - [section 2.3]: "When adjusting the training scale while keeping the model size fixed, the multi-step learning rate scheduler allows for the reuse of training from the first phase, offering a unique convenience for continual training."
  - [corpus]: Weak. No direct citations found. Corpus neighbors discuss learning rate scheduling but do not specifically anchor this mechanism.
- Break condition: If the learning rate reduction schedule is not optimized or if the model architecture changes significantly.

## Foundational Learning

- Concept: Scaling laws
  - Why needed here: Understanding how model performance scales with compute budget is crucial for making informed decisions about resource allocation and model design.
  - Quick check question: If you double the compute budget, how should you ideally split it between model size and dataset size to maximize performance?

- Concept: Power-law relationships
  - Why needed here: Many aspects of machine learning, including scaling laws, follow power-law relationships. Recognizing and working with these relationships is essential for understanding and predicting model behavior.
  - Quick check question: What is the general form of a power-law relationship, and how does it differ from a linear relationship?

- Concept: Data quality and its impact on learning
  - Why needed here: The quality of the training data significantly impacts the model's ability to learn and generalize. Understanding this relationship is crucial for designing effective training strategies.
  - Quick check question: How might the definition of "data quality" vary depending on the specific task or domain?

## Architecture Onboarding

- Component map: Data preprocessing -> Model initialization -> Hyperparameter selection -> Training with multi-step learning rate scheduler -> SFT -> DPO -> Evaluation
- Critical path: The critical path for training involves data preprocessing, model initialization, hyperparameter selection, training with the chosen learning rate schedule, and evaluation.
- Design tradeoffs: The choice of architecture and hyperparameters involves tradeoffs between model capacity, training efficiency, and inference cost.
- Failure signatures: Potential failures include overfitting, underfitting, poor generalization, and instability during training.
- First 3 experiments:
  1. Train a small model with different learning rate schedules to verify the multi-step scheduler's effectiveness.
  2. Train models with varying data quality to observe the impact on optimal model/data scaling.
  3. Experiment with different model scale representations (e.g., parameters vs. FLOPs) to determine the most accurate predictor of performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between data quality and optimal model/data scaling-up allocation in the scaling laws of DeepSeek LLM?
- Basis in paper: [explicit] The paper states that higher data quality leads to more compute budget being allocated to model scaling rather than data scaling.
- Why unresolved: While the paper provides some evidence of this relationship, it does not fully explore or quantify the exact nature of this relationship.
- What evidence would resolve it: Further research could involve systematically varying data quality and measuring the resulting optimal model/data scaling-up allocation. This could help to establish a more precise relationship between these factors.

### Open Question 2
- Question: How does the choice of learning rate scheduler (multi-step vs cosine) impact the performance of DeepSeek LLM?
- Basis in paper: [explicit] The paper mentions that DeepSeek LLM uses a multi-step learning rate scheduler instead of the typical cosine scheduler, but does not provide a detailed comparison of their performance.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of the learning rate scheduler choice on the model's performance.
- What evidence would resolve it: Further experiments could compare the performance of DeepSeek LLM using both multi-step and cosine learning rate schedulers, potentially revealing the advantages and disadvantages of each approach.

### Open Question 3
- Question: What is the impact of the system prompt on the performance of smaller models like DeepSeek LLM 7B?
- Basis in paper: [explicit] The paper mentions that the performance of the 7B model slightly degrades when a system prompt is introduced, but does not provide a detailed analysis of this phenomenon.
- Why unresolved: The paper does not fully explore the reasons behind the performance degradation of the 7B model with a system prompt, or how this compares to larger models.
- What evidence would resolve it: Further research could involve a detailed analysis of the impact of system prompts on smaller models, potentially revealing the factors that contribute to performance degradation and how to mitigate them.

## Limitations

- The exact mathematical formulation of scaling laws used to guide hyperparameter selection is not fully specified, making it difficult to verify the claimed approach.
- The relationship between data quality and optimal scaling strategy is presented as an extrapolation from existing studies rather than demonstrated through controlled experiments with the DeepSeek dataset.
- The impact of the learning rate scheduler choice on model performance is not thoroughly investigated, with only a single comparison to cosine scheduling provided.

## Confidence

- **High Confidence**: The core observation that DeepSeek LLM achieves strong benchmark performance, particularly in code and mathematical tasks, is well-supported by the presented results.
- **Medium Confidence**: The relationship between data quality and optimal scaling strategy is plausible based on the theoretical discussion, but the empirical evidence is limited to observations about existing scaling studies rather than direct experimentation.
- **Low Confidence**: The specific mathematical formulation and implementation of scaling laws for hyperparameter selection lacks detailed specification, making it difficult to verify whether the claimed approach was truly followed.

## Next Checks

1. Request detailed mathematical specification of the scaling law framework used for hyperparameter selection, including the exact power-law exponents and how they were calibrated to the 2 trillion token dataset.
2. Conduct controlled experiments varying data quality within the DeepSeek corpus to empirically verify how different quality levels affect the optimal model/data scaling ratio.
3. Perform ablation studies comparing multi-step versus cosine learning rate scheduling across multiple model scales and random seeds to establish the robustness of the claimed performance equivalence.