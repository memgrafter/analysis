---
ver: rpa2
title: An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought
  Eliciting Arithmetic Reasoning of LLMs
arxiv_id: '2406.12288'
source_url: https://arxiv.org/abs/2406.12288
tags:
- neurons
- reasoning
- step
- each
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates neuron activation as a lens to explain
  chain-of-thought (CoT) prompting for arithmetic reasoning in large language models
  (LLMs). The authors propose an approach using GPT-4 to automatically identify feed-forward
  (FF) neurons related to arithmetic reasoning in Llama2.
---

# An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs

## Quick Facts
- arXiv ID: 2406.12288
- Source URL: https://arxiv.org/abs/2406.12288
- Authors: Daking Rai; Ziyu Yao
- Reference count: 31
- Primary result: Neuron activation patterns can explain why certain CoT prompts are more effective for arithmetic reasoning in LLMs, revealing that reasoning neurons are necessary but not sufficient

## Executive Summary
This paper investigates neuron activation as a lens to explain chain-of-thought (CoT) prompting effectiveness for arithmetic reasoning in large language models. The authors propose an approach using GPT-4 to automatically identify feed-forward neurons related to arithmetic reasoning in Llama2. They analyze how different CoT prompt components affect the activation of these reasoning neurons and find that neuron activation patterns correlate with model performance. The study reveals that while reasoning neuron activation is necessary for effective arithmetic reasoning, it is not sufficient, as similar activation patterns can appear in both high and low accuracy scenarios.

## Method Summary
The authors use GPT-4 to automatically identify reasoning-related neurons in Llama2 by analyzing projected vocabulary tokens from feed-forward layer updates. They then conduct experiments varying CoT prompt components (equations vs no equations, correct vs incorrect labels) and measure the activation patterns of identified reasoning neurons. The method involves greedy decoding on a single GPU to discover neurons, followed by systematic ablation experiments to test neuron importance. They correlate neuron activation metrics (frequency and coefficient strength) with model performance on arithmetic reasoning tasks to explain prior observations about CoT effectiveness.

## Key Results
- Reasoning neuron activation patterns correlate with model performance on arithmetic reasoning tasks
- Equations in CoT prompts are critical because they activate arithmetic operation neurons more strongly
- Reasoning neurons are necessary but not sufficient for effective arithmetic reasoning (similar activation patterns can yield different accuracies)
- GPT-4 can effectively identify reasoning neurons by analyzing projected vocabulary tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning neurons are necessary but not sufficient for arithmetic reasoning in LLMs.
- Mechanism: Neuron activation patterns correlate with model performance, but the same patterns can appear in biased reasoning paths (e.g., when using OOD labels).
- Core assumption: Neuron activation reflects the presence of reasoning-relevant concepts during generation.
- Evidence anchors:
  - [abstract] "Our results reveal that examining the activation of FF neurons in response to different CoT prompts can provide valuable insights into why certain CoT prompts are more effective in eliciting arithmetic reasoning capabilities in LLMs."
  - [section 6.4] "Although CoT with incorrect OOD labels has lower accuracy than CoT with correct labels (16.83% vs 7.58%), they show a similar number of reasoning neuron activations (1119 vs 1087) and similar average coefficients (2.51 vs 2.50)."
  - [corpus] Found 25 related papers; no direct citations yet; mechanism supported indirectly by related CoT reasoning studies.
- Break condition: If neuron activation patterns diverge strongly between high- and low-accuracy prompts without corresponding performance differences.

### Mechanism 2
- Claim: Equations in CoT prompts are critical because they activate arithmetic operation neurons more strongly.
- Mechanism: Presence of equations increases activation frequency and coefficient strength of neurons associated with addition, subtraction, multiplication, and division.
- Core assumption: Equations explicitly cue the model to perform arithmetic operations.
- Evidence anchors:
  - [section 6.1] "Looking into its activation pattern, we found the CoT prompt without equations... activates fewer reasoning neurons overall, 842 activations, compared to the CoT prompt with equations (CoT), 1119 activations."
  - [section 6.1] "Furthermore, we observed a decrease in both the number of activated neurons for individual concepts and their corresponding average coefficients across all categories."
  - [corpus] Related work on CoT effectiveness but no direct neuron-level evidence in corpus.
- Break condition: If CoT prompts without equations still achieve comparable accuracy through alternative activation pathways.

### Mechanism 3
- Claim: GPT-4 can effectively identify reasoning neurons by analyzing projected vocabulary tokens.
- Mechanism: GPT-4 classifies neurons based on top-promoted vocabulary tokens, automating a process previously done manually.
- Core assumption: Projected tokens are interpretable indicators of the concepts a neuron promotes.
- Evidence anchors:
  - [section 3] "Our experimental results demonstrate the high effectiveness of utilizing GPT-4 for this purpose."
  - [section 3] "For instance, the 'L21N7027' neuron... projects with high coefficients to tokens such as '+', 'U+4e0e', '&', 'and', 'U+acfc', 'plus'... and GPT-4 reasonably classified it as a neuron that promotes 'Arithmetic Addition'."
  - [corpus] No direct corpus evidence; mechanism relies on internal validation in the paper.
- Break condition: If GPT-4 misclassifies neurons with high frequency or if manual validation contradicts classifications.

## Foundational Learning

- Concept: Feed-forward (FF) layer sub-updates and their projection to vocabulary space.
  - Why needed here: Understanding how neurons promote concepts through vocabulary projections is central to identifying reasoning neurons.
  - Quick check question: How does a neuron's coefficient influence its contribution to token prediction in transformers?

- Concept: Chain-of-Thought (CoT) prompting and its components.
  - Why needed here: The study analyzes how different CoT components (equations, textual explanations) affect neuron activation.
  - Quick check question: What is the difference between CoT with and without equations in terms of model behavior?

- Concept: Neuron activation patterns and their correlation with model performance.
  - Why needed here: The paper's core insight is that activation patterns explain prior observations about CoT effectiveness.
  - Quick check question: Can two prompts with different accuracies show similar neuron activation patterns?

## Architecture Onboarding

- Component map: Transformer-based LLM with FF layers → Neuron identification via GPT-4 → Activation analysis → Performance correlation
- Critical path: Prompt generation → Neuron activation measurement → Performance evaluation → Interpretation of results
- Design tradeoffs: Manual neuron discovery vs automated GPT-4 classification (speed vs potential accuracy)
- Failure signatures: Similar neuron activation patterns across prompts with vastly different accuracies; random noise ablation affecting performance
- First 3 experiments:
  1. Replicate CoT ablation studies to validate prior observations on Llama2-7B
  2. Apply neuron discovery algorithm to identify reasoning neurons in Llama2-7B
  3. Perform random noise ablation on identified reasoning neurons to test their importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reasoning neurons interact with attention mechanisms during multi-step reasoning?
- Basis in paper: [inferred] The paper notes that their neuron-level analysis is limited because it doesn't consider interactions among neurons or other LLM components like attention modules
- Why unresolved: The study focuses solely on feed-forward neurons without examining how they coordinate with attention heads or other components during reasoning
- What evidence would resolve it: Systematic analysis of attention head activation patterns during reasoning, and how these patterns correlate with reasoning neuron activity

### Open Question 2
- Question: What is the minimum set of reasoning neurons required for effective arithmetic reasoning?
- Basis in paper: [explicit] The authors perform random noise ablation showing that corrupting reasoning neurons causes significant performance drop, but note this doesn't establish sufficiency
- Why unresolved: While the paper shows reasoning neurons are necessary, it doesn't determine if all discovered neurons are required, or if a smaller subset would suffice
- What evidence would resolve it: Targeted ablation experiments systematically removing subsets of reasoning neurons to identify the minimal critical set

### Open Question 3
- Question: How do reasoning neurons generalize across different LLM architectures?
- Basis in paper: [explicit] The authors note they only tested on Llama2-7B and found Llama3-8B to be "highly sensitive to random noise ablation" requiring different approaches
- Why unresolved: The preliminary study on Llama3-8B showed different sensitivity patterns, suggesting reasoning neurons may behave differently across architectures
- What evidence would resolve it: Systematic discovery and ablation of reasoning neurons across multiple LLM families and sizes to identify common patterns and architecture-specific differences

## Limitations
- The study focuses only on Llama2-7B, limiting generalizability to other architectures and model sizes
- The relationship between neuron activation and actual reasoning capabilities remains somewhat indirect
- The mechanism relies on GPT-4's interpretation of projected tokens without independent manual validation

## Confidence

| Claim | Confidence |
|-------|------------|
| Reasoning neurons are necessary for arithmetic reasoning | High |
| Reasoning neuron activation is not sufficient for effective reasoning | Medium |
| GPT-4 can effectively identify reasoning neurons | Medium |
| Equations in CoT prompts are critical for activating reasoning neurons | Medium |

## Next Checks

1. Replicate the CoT ablation studies on Llama2-7B to validate prior observations about the importance of equations and correct labels
2. Apply the neuron discovery algorithm using GPT-4 to identify reasoning neurons in a different LLM architecture (e.g., Mistral or GPT-2)
3. Perform systematic ablation experiments removing subsets of identified reasoning neurons to determine the minimal critical set for arithmetic reasoning