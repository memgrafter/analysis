---
ver: rpa2
title: 'Just KIDDIN: Knowledge Infusion and Distillation for Detection of INdecent
  Memes'
arxiv_id: '2411.12174'
source_url: https://arxiv.org/abs/2411.12174
tags:
- knowledge
- multimodal
- meme
- graph
- toxicity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework called KID-VLM to improve toxic
  meme detection by integrating knowledge distillation from large vision-language
  models and knowledge infusion from ConceptNet. The approach extracts relevant subgraphs
  from ConceptNet based on meme captions, combines them with distilled multimodal
  features via graph neural networks, and uses a gating mechanism for fusion.
---

# Just KIDDIN: Knowledge Infusion and Distillation for Detection of INdecent Memes

## Quick Facts
- arXiv ID: 2411.12174
- Source URL: https://arxiv.org/abs/2411.12174
- Reference count: 11
- Achieved 1.1% AUC, 7% F1, and 35% recall improvements over baselines on Hateful Memes and HarMeme datasets

## Executive Summary
This paper introduces KID-VLM, a framework that enhances toxic meme detection by combining knowledge distillation from large vision-language models with knowledge infusion from ConceptNet. The approach extracts relevant subgraphs from ConceptNet based on meme captions, encodes them with graph neural networks, and fuses the resulting context with multimodal features via a gating mechanism. Evaluated on Hateful Memes and HarMeme datasets, KID-VLM shows significant improvements in toxicity detection while providing interpretable reasoning paths through explicit knowledge graph integration.

## Method Summary
KID-VLM extracts visual and textual features using CLIP encoders, generates captions with LLaVA, and constructs joint working graphs from ConceptNet using relevance scoring. The framework employs knowledge distillation through consistency loss between student and teacher model embeddings, encodes the knowledge graph using R-GCN or GAT, and combines multimodal and graph representations using gated fusion. The model predicts toxicity using a sigmoid activation function trained with BCE loss and consistency loss.

## Key Results
- Achieved 1.1% AUC improvement over state-of-the-art baselines
- Demonstrated 7% F1 score improvement in toxicity detection
- Showed 35% recall improvement, indicating better identification of toxic content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge infusion from ConceptNet improves contextual understanding for toxicity detection
- Mechanism: Extracts relevant subgraphs from ConceptNet based on meme captions and LVLM-generated captions, then uses graph neural networks to encode relational context
- Core assumption: The relational knowledge in ConceptNet captures the nuanced contextual connections needed to understand toxic content in memes
- Evidence anchors: [abstract], [section]
- Break condition: If ConceptNet lacks relevant entities for the meme's cultural context or if the extracted subgraph contains too much noise relative to useful information

### Mechanism 2
- Claim: Knowledge distillation from LVLMs transfers implicit contextual knowledge to smaller models
- Mechanism: Uses consistency loss between student model's aligned embeddings and teacher model's embeddings to align internal representations
- Core assumption: The larger teacher model captures implicit contextual patterns that are valuable for toxicity detection
- Evidence anchors: [abstract], [section]
- Break condition: If the teacher model's implicit knowledge doesn't generalize well to the student model's architecture or if the distillation process overfits to the teacher's specific patterns

### Mechanism 3
- Claim: Gated fusion effectively combines multimodal and graph-based representations
- Mechanism: Uses sigmoid gating to learn optimal weighting between multimodal and graph representations
- Core assumption: Both multimodal and graph representations contain complementary information for toxicity detection
- Evidence anchors: [section], [abstract]
- Break condition: If one representation type dominates the other consistently, making the gating mechanism redundant

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: The model needs to understand both visual and textual components of memes simultaneously
  - Quick check question: How does the model ensure that the visual and textual features are properly aligned for joint reasoning?

- Concept: Knowledge graph reasoning
  - Why needed here: The model needs to capture relational context beyond what's explicitly stated in the meme
  - Quick check question: How does the model determine which ConceptNet entities are most relevant to a given meme?

- Concept: Knowledge distillation
  - Why needed here: The model needs to leverage the implicit knowledge captured by larger models while maintaining efficiency
  - Quick check question: What loss function ensures that the student model learns the teacher's implicit contextual patterns?

## Architecture Onboarding

- Component map: Image/text → CLIP features → Caption generation → Knowledge subgraph → GNN encoding → Fusion → Toxicity prediction
- Critical path: Image/text → CLIP features → Caption generation → Knowledge subgraph → GNN encoding → Fusion → Toxicity prediction
- Design tradeoffs:
  - Using frozen CLIP encoders vs. fine-tuning for efficiency
  - Hop 1 vs. Hop 2 expansion for subgraph size vs. computational cost
  - Roberta vs. MiniLM for relevance scoring accuracy vs. speed
- Failure signatures:
  - Poor performance on memes requiring cultural context not present in ConceptNet
  - Degradation when teacher model's implicit knowledge doesn't align with dataset characteristics
  - Overfitting when node count in subgraphs is too high relative to dataset size
- First 3 experiments:
  1. Compare performance with and without ConceptNet knowledge infusion on a small validation set
  2. Test different fusion mechanisms (gated, multiplicative, bilinear) to identify optimal combination strategy
  3. Evaluate impact of different node counts (250, 500, 750) on subgraph construction for relevance vs. noise tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the KID-VLM framework perform on toxicity detection datasets outside of Hateful Memes and HarMeme, particularly those with different cultural contexts or languages?
- Basis in paper: [inferred] The paper demonstrates KID-VLM's performance on two specific datasets (Hateful Memes and HarMeme) but does not test its generalizability to other toxicity detection datasets or cultural contexts.
- Why unresolved: The paper focuses on benchmark datasets commonly used in multimodal toxicity detection research, but does not explore whether the framework's performance would hold across diverse datasets representing different cultural contexts, languages, or toxicity types.
- What evidence would resolve it: Experimental results showing KID-VLM's performance on a diverse range of toxicity detection datasets from different cultural contexts, languages, or domains (e.g., Twitter datasets, Reddit comments, multilingual toxicity datasets).

### Open Question 2
- Question: What is the computational overhead introduced by incorporating ConceptNet subgraphs and graph neural networks compared to baseline models, and how does this impact real-time deployment feasibility?
- Basis in paper: [explicit] The paper acknowledges that incorporating graph-based methods increases computational complexity, which may hinder scalability to larger datasets or real-time applications.
- Why unresolved: While the paper mentions computational complexity as a limitation, it does not provide quantitative measurements of the additional computational overhead introduced by the ConceptNet integration and graph neural networks compared to baseline models.
- What evidence would resolve it: Detailed computational analysis comparing inference times, memory usage, and GPU requirements between KID-VLM and baseline models, along with scalability tests on larger datasets.

### Open Question 3
- Question: How does the model handle memes that contain sarcasm, irony, or other forms of figurative language that may contradict the literal interpretation of visual and textual elements?
- Basis in paper: [inferred] The paper discusses the importance of understanding nuanced and context-dependent toxicity but does not explicitly address how the model handles figurative language or rhetorical devices like sarcasm and irony.
- Why unresolved: While KID-VLM incorporates external knowledge and distilled representations to enhance contextual understanding, the paper does not demonstrate or discuss the model's ability to correctly interpret memes that rely on sarcasm, irony, or other forms of figurative language that may contradict their literal visual and textual content.
- What evidence would resolve it: Experimental results on datasets specifically containing sarcastic or ironic memes, along with qualitative analysis of the model's reasoning paths for such examples, would demonstrate whether KID-VLM can correctly identify toxicity in memes that use figurative language.

## Limitations
- Performance gains may be overstated as they lack statistical significance testing
- Framework's effectiveness depends heavily on ConceptNet's coverage of cultural and contextual knowledge
- Computational overhead from knowledge graph construction and GNN processing may limit real-time deployment

## Confidence

- Knowledge infusion mechanism: Medium
- Knowledge distillation mechanism: Medium
- Overall performance claims: Medium

## Next Checks

1. Conduct ablation studies with statistical significance testing to determine whether the performance gains are additive, synergistic, or dominated by a single mechanism
2. Test model robustness on memes requiring cultural context outside ConceptNet's coverage to assess generalization limits
3. Evaluate the computational overhead of knowledge graph construction and GNN processing relative to the performance benefits across different dataset scales