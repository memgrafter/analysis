---
ver: rpa2
title: 'Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented
  Generation Systems'
arxiv_id: '2407.08275'
source_url: https://arxiv.org/abs/2407.08275
tags:
- similarity
- gte-large
- retrieval
- embedding
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates similarity between embedding models in RAG
  systems using two approaches: direct embedding comparison via CKA and retrieval
  result comparison via Jaccard and rank similarity. Experiments on 19 models across
  5 datasets show that while CKA identifies family-based clusters, retrieval similarity
  at small k values (relevant for RAG) exhibits high variance and low overlap, especially
  in larger datasets.'
---

# Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems

## Quick Facts
- **arXiv ID**: 2407.08275
- **Source URL**: https://arxiv.org/abs/2407.08275
- **Reference count**: 40
- **Primary result**: CKA identifies model family clusters but fails to predict retrieval performance; highest similarity cluster comprises bge-large, UAE-Large, and mxbai-embed-large models

## Executive Summary
This paper addresses the challenge of selecting appropriate embedding models for Retrieval Augmented Generation (RAG) systems beyond traditional benchmark metrics. The authors propose a comprehensive evaluation framework using two complementary approaches: Centered Kernel Alignment (CKA) for direct embedding comparison and retrieval-based metrics (Jaccard similarity and rank similarity) for practical performance assessment. The study evaluates 19 embedding models across five diverse datasets, revealing that while CKA successfully identifies model family-based clusters, retrieval similarity at small k values (relevant for RAG) exhibits high variance and low overlap, particularly in larger datasets. This finding suggests that embedding model selection remains a non-trivial task despite observed clustering patterns, as structural similarities don't necessarily translate to equivalent retrieval performance in practical RAG applications.

## Method Summary
The authors employ a two-pronged evaluation approach to assess embedding model similarity. First, they use Centered Kernel Alignment (CKA) to measure direct similarity between embedding models by comparing their internal representations across the same datasets. Second, they implement retrieval-based comparison using Jaccard similarity and rank similarity metrics, evaluating how models retrieve overlapping passages at small k values (1-10) relevant for RAG systems. The study tests 19 embedding models (including OpenAI, open-source, and fine-tuned variants) across five datasets: NQ, HotpotQA, FiQA, SciDocs, and AQANews. By combining these approaches, the researchers aim to bridge the gap between theoretical embedding similarity and practical retrieval performance, addressing the limitation that current benchmarks often fail to capture the nuances important for RAG applications.

## Key Results
- CKA successfully identifies family-based clustering (OpenAI, open-source, fine-tuned models) but doesn't predict retrieval performance differences
- Retrieval similarity at small k values shows high variance and low overlap across models, especially in larger datasets
- The bge-large, UAE-Large, and mxbai-embed-large models form the highest similarity cluster
- Mistral emerges as the most similar open-source alternative to OpenAI models, though actual retrieval overlap remains modest (≤0.6 Jaccard similarity)

## Why This Works (Mechanism)
The study works by recognizing that embedding models, despite achieving similar benchmark scores, can have fundamentally different internal representations and retrieval behaviors. CKA captures structural similarity in the embedding space, while retrieval-based metrics measure practical overlap in results. The low similarity scores at small k values reveal that models make different retrieval decisions even when their embeddings appear structurally similar, which is critical for RAG systems where the quality of retrieved passages directly impacts generation output.

## Foundational Learning

**Centered Kernel Alignment (CKA)**: A similarity metric that measures the alignment between representations of different models by comparing their Gram matrices. Why needed: To capture structural similarity in embedding spaces beyond simple vector distance. Quick check: CKA values range from 0 (completely dissimilar) to 1 (identical), with family-based clustering expected.

**Jaccard Similarity for Retrieval**: Measures the overlap between retrieved passage sets from different models at specific k values. Why needed: To quantify practical retrieval agreement relevant for RAG systems. Quick check: Jaccard = |A ∩ B| / |A ∪ B|, where A and B are retrieved sets from two models.

**Rank Similarity**: Evaluates the consistency of document rankings between models. Why needed: To assess whether models agree not just on which documents to retrieve, but also their relative importance. Quick check: Spearman correlation between ranked lists from different models.

**RAG System Requirements**: Small k values (1-10) are most relevant as they represent the passages actually used for generation. Why needed: Understanding that theoretical similarity doesn't translate to practical utility in constrained retrieval scenarios. Quick check: Evaluate how retrieval decisions at k=5 affect final answer quality.

## Architecture Onboarding

**Component Map**: Data -> Embedding Models -> CKA Analysis -> Retrieval Comparison -> Similarity Assessment -> Model Selection

**Critical Path**: Dataset preprocessing -> Embedding generation -> CKA computation -> Retrieval at small k -> Similarity metric calculation -> Clustering analysis

**Design Tradeoffs**: Direct embedding comparison (CKA) vs. retrieval-based evaluation - theoretical similarity vs. practical performance. The study prioritizes practical relevance by focusing on small k values despite increased variance.

**Failure Signatures**: High CKA scores but low retrieval similarity indicates structural similarity without functional equivalence. This suggests models may encode information differently despite similar overall architecture.

**3 First Experiments**:
1. Replicate CKA analysis on a subset of models to verify family-based clustering patterns
2. Test retrieval similarity at varying k values (1, 5, 10, 50) to observe variance patterns
3. Apply the framework to a new domain-specific dataset to assess generalizability

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- CKA captures structural similarity but fails to predict retrieval-specific performance differences
- High variance in retrieval similarity at small k values suggests metrics may not reliably predict practical performance
- Observed clustering patterns don't necessarily translate to equivalent end-to-end RAG performance

## Confidence
- Clustering findings based on CKA: **Medium** confidence (aligns with model families but practical implications uncertain)
- bge-large/UAE-Large/mxbai-embed-large similarity cluster: **Medium** confidence (modest actual retrieval overlap observed)
- Mistral as OpenAI alternative: **Medium** confidence (statistically significant but practically limited similarity)

## Next Checks
1. Evaluate end-to-end RAG performance (including answer quality) of the most similar model clusters identified
2. Test stability of retrieval similarity metrics across different dataset sizes and domains
3. Investigate hybrid approaches combining multiple similarity metrics or incorporating domain-specific fine-tuning to improve model selection reliability