---
ver: rpa2
title: 'SynthSOD: Developing an Heterogeneous Dataset for Orchestra Music Source Separation'
arxiv_id: '2409.10995'
source_url: https://arxiv.org/abs/2409.10995
tags:
- dataset
- music
- midi
- separation
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of orchestral music source separation
  by introducing a large-scale, heterogeneous synthetic dataset called SynthSOD, designed
  to overcome the scarcity of clean, bleed-free multitrack recordings for training
  supervised models. The dataset is generated using high-quality soundfonts and a
  musically motivated pipeline that introduces realistic variations in tempo, dynamics,
  and articulations across MIDI files from the Symbolic Orchestra Dataset.
---

# SynthSOD: Developing an Heterogeneous Dataset for Orchestra Music Source Separation

## Quick Facts
- arXiv ID: 2409.10995
- Source URL: https://arxiv.org/abs/2409.10995
- Reference count: 40
- One-line primary result: SynthSOD dataset enables training of orchestral source separation models that outperform existing synthetic datasets on both synthetic and real-world recordings

## Executive Summary
This paper addresses the challenge of orchestral music source separation by introducing a large-scale, heterogeneous synthetic dataset called SynthSOD, designed to overcome the scarcity of clean, bleed-free multitrack recordings for training supervised models. The dataset is generated using high-quality soundfonts and a musically motivated pipeline that introduces realistic variations in tempo, dynamics, and articulations across MIDI files from the Symbolic Orchestra Dataset. This approach ensures diversity and representativeness while maintaining musical coherence. Experiments demonstrate that models trained on SynthSOD outperform those trained on the smaller EnsembleSet in both synthetic and real-world conditions, highlighting the dataset's utility. However, the paper also identifies the need for domain adaptation techniques to improve generalization to real recordings. The code and dataset are publicly available, enabling further research in orchestral music source separation.

## Method Summary
The method involves creating a synthetic orchestral music dataset by first preprocessing MIDI files from the Symbolic Orchestra Dataset to conform to the General MIDI standard and filtering out non-orchestral instruments. Random annotations for tempo, dynamics, and articulations are then generated based on expert-guided probability distributions to ensure musical coherence and diversity. These MIDI files are synthesized using the BBC Symphony Orchestra soundfont library within the Reaper DAW, creating over 47 hours of audio. The resulting dataset is used to train the X-UMX model, which is evaluated using SDR metrics on both synthetic and real-world orchestral recordings.

## Key Results
- Models trained on SynthSOD achieve higher SDR scores on synthetic test sets compared to those trained on EnsembleSet
- SynthSOD demonstrates improved performance on real-world recordings (URMP and Operation Beethoven) compared to EnsembleSet
- The heterogeneous design with varied dynamics, tempi, and articulations contributes to better generalization across different musical contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed musically motivated random annotations improve generalization to real recordings by introducing controlled variability that mimics real performance practices.
- Mechanism: The pipeline generates random tempo intervals, dynamic intervals, and articulations based on expert-guided probability distributions, creating a diverse training set that better reflects the variability in real orchestral performances.
- Core assumption: The expert-defined parameter ranges and articulation probabilities accurately represent realistic orchestral performance variations.
- Evidence anchors:
  - [abstract] "The dataset is generated using high-quality soundfonts and a musically motivated pipeline that introduces realistic variations in tempo, dynamics, and articulations"
  - [section] "we employ a custom weighting function for each available articulation in the BBCSO sound font library for each instrument that models the prevalence of specific articulations based on real-world musical practices, ensuring that common articulations are more likely to occur than rarer ones"
  - [corpus] Weak evidence - the corpus doesn't provide specific validation of this mechanism
- Break condition: If the expert-defined parameters don't accurately reflect real performance practices, or if the synthesizer doesn't properly implement the varied articulations.

### Mechanism 2
- Claim: The large-scale synthetic dataset enables training of supervised models for orchestral music source separation despite the scarcity of real multitrack recordings.
- Mechanism: By synthesizing over 47 hours of orchestral music using high-quality soundfonts and a diverse MIDI dataset, the SynthSOD dataset provides sufficient training data to train supervised source separation models for the orchestral domain.
- Core assumption: The synthesized data is sufficiently realistic to enable model training, even if not perfect for direct real-world application.
- Evidence anchors:
  - [abstract] "the challenge of extracting similarly sounding sources from orchestra recordings has not been extensively explored, largely due to a scarcity of comprehensive and clean multitrack datasets"
  - [section] "The baseline model obtained good separation results for most of the instruments when evaluated with synthetic data"
  - [corpus] Weak evidence - the corpus mentions related datasets but doesn't directly validate this specific mechanism
- Break condition: If the synthesized data quality is too poor to provide meaningful training signals, or if the dataset size is still insufficient despite being larger than alternatives.

### Mechanism 3
- Claim: The heterogeneous dataset design with varied dynamics, tempi, and articulations enables better generalization across different musical contexts compared to homogeneous datasets.
- Mechanism: By systematically varying musical parameters across the dataset rather than keeping them constant, the model learns to handle diverse musical scenarios rather than overfitting to specific patterns.
- Core assumption: Musical variability in training data leads to better generalization than uniform data.
- Evidence anchors:
  - [abstract] "SynthSOD, developed using a set of simulation techniques to create a realistic, musically motivated, and heterogeneous training set comprising different dynamics, natural tempo changes, styles, and conditions"
  - [section] "In SynthSOD, we propose that the total number of tempo intervals per MIDI file to be randomly determined based on the duration of the piece"
  - [corpus] Weak evidence - the corpus doesn't provide specific validation of this mechanism
- Break condition: If the variability introduced is too extreme and unrealistic, causing the model to learn incorrect patterns, or if the benefits of heterogeneity don't outweigh the complexity it adds.

## Foundational Learning

- Concept: MIDI file structure and General MIDI standard
  - Why needed here: The dataset creation process starts with MIDI files that need to be standardized and annotated before synthesis
  - Quick check question: What is the range of MIDI velocity values, and how does it map to musical dynamics?

- Concept: Music source separation evaluation metrics
  - Why needed here: The paper evaluates separation quality using SDR (Signal-to-Distortion Ratio), requiring understanding of what this metric measures
  - Quick check question: What does SDR measure in source separation, and what would be considered a good SDR value for orchestral music?

- Concept: Audio synthesis with soundfonts
  - Why needed here: The dataset is created by synthesizing MIDI files using the BBC Symphony Orchestra soundfont library
  - Quick check question: What are the key differences between synthesized and recorded audio that might affect source separation model performance?

## Architecture Onboarding

- Component map: MIDI preprocessing pipeline (fixing to General MIDI standard, filtering) -> Annotation generation module (tempo, dynamics, articulation variations) -> Synthesis engine (using BBCSO soundfont with Reaper DAW) -> Evaluation framework (X-UMX model training and SDR evaluation)
- Critical path: MIDI preprocessing → Annotation generation → Synthesis → Model training → Evaluation
- Design tradeoffs: Large synthetic dataset provides more training data but may not generalize well to real recordings; heterogeneous annotations improve generalization but add complexity; using a single Decca Tree mix simplifies distribution but limits acoustic variability
- Failure signatures: Poor separation performance on real recordings indicates domain adaptation issues; imbalanced instrument representation in training data leads to poor performance on underrepresented instruments; excessive randomness in annotations creates unrealistic training examples
- First 3 experiments:
  1. Train X-UMX model on SynthSOD Decca Tree signals and evaluate on the synthetic test set to establish baseline performance
  2. Train X-UMX model on SynthSOD close mic signals with simple reverb applied and evaluate on the synthetic test set to test acoustic simulation
  3. Fine-tune the model trained on SynthSOD using a small amount of real orchestral recordings and evaluate on URMP to test domain adaptation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the generalization of models trained on synthesized orchestral music to real-world recordings be improved?
- Basis in paper: [explicit] The paper identifies that models trained on SynthSOD perform poorly when evaluated on real recordings like URMP and Operation Beethoven, suggesting the need for domain adaptation techniques.
- Why unresolved: Current synthesizers like Spitfire BBCSO use a limited number of recordings per note, limiting diversity even with increased dataset duration.
- What evidence would resolve it: Experiments demonstrating improved SDRs on real recordings after applying domain adaptation techniques such as fine-tuning or using diverse synthesizers.

### Open Question 2
- Question: What is the impact of using different synthesizers on the diversity and quality of the SynthSOD dataset?
- Basis in paper: [explicit] The paper mentions that resynthesizing MIDI files with different synthesizers could enhance dataset diversity, but this has not been explored.
- Why unresolved: The current dataset relies solely on the Spitfire BBCSO library, which may not capture the full range of orchestral sounds.
- What evidence would resolve it: Comparative studies showing performance differences in MSS models trained on datasets synthesized with multiple libraries versus a single library.

### Open Question 3
- Question: How do musically motivated random annotations affect the performance of MSS models compared to manually created annotations?
- Basis in paper: [explicit] The paper introduces a method for generating annotations and compares its effectiveness to models trained without annotations or with original human-made annotations.
- Why unresolved: While the proposed method shows promise, its long-term effectiveness and potential improvements are not fully explored.
- What evidence would resolve it: Detailed analysis of SDR improvements across various MSS models and real-world datasets when trained with musically motivated annotations.

## Limitations

- The real-world evaluation is limited to only 6 pieces total, with SDR improvements ranging from 0.1 to 2.6 dB across instruments
- The paper acknowledges that direct training on SynthSOD alone yields poor results on real recordings, requiring domain adaptation techniques not fully explored
- Current synthesizers use a limited number of recordings per note, limiting diversity even with increased dataset duration

## Confidence

- Claim: SynthSOD improves performance on synthetic test sets compared to EnsembleSet -> **High**
- Claim: SynthSOD demonstrates improved performance on real-world recordings compared to EnsembleSet -> **Medium**
- Claim: Musically motivated random annotations improve generalization to real recordings -> **Low**

## Next Checks

1. **Systematic domain adaptation evaluation**: Compare SDR performance on URMP when using different domain adaptation strategies (fine-tuning, adversarial training, feature normalization) on SynthSOD versus training from scratch on real data.

2. **Ablation study on annotation diversity**: Train models with progressively simpler versions of SynthSOD (fixed tempo, single dynamic level, limited articulations) to quantify the contribution of each musical variation dimension to separation performance.

3. **Cross-dataset generalization test**: Evaluate models trained on SynthSOD on other real orchestral datasets (if available) or different sections of URMP to assess whether improvements generalize beyond the specific pieces used in evaluation.