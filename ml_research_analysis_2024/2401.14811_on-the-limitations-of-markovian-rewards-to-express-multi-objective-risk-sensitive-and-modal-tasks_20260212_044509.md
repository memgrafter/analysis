---
ver: rpa2
title: On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive,
  and Modal Tasks
arxiv_id: '2401.14811'
source_url: https://arxiv.org/abs/2401.14811
tags:
- reward
- function
- tasks
- then
- markovian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the expressivity of scalar, Markovian reward
  functions in reinforcement learning (RL), focusing on three classes of tasks: multi-objective
  RL, risk-sensitive RL, and modal RL. The authors derive necessary and sufficient
  conditions for when these tasks can be expressed using scalar, Markovian rewards.'
---

# On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks

## Quick Facts
- arXiv ID: 2401.14811
- Source URL: https://arxiv.org/abs/2401.14811
- Authors: Joar Skalse; Alessandro Abate
- Reference count: 32
- Key outcome: This paper analyzes the expressivity of scalar, Markovian reward functions in reinforcement learning (RL), focusing on three classes of tasks: multi-objective RL, risk-sensitive RL, and modal RL. The authors derive necessary and sufficient conditions for when these tasks can be expressed using scalar, Markovian rewards. They prove that most tasks in these classes cannot be expressed this way, demonstrating limitations in the expressivity of standard reward functions. The paper also introduces modal tasks as a new class and outlines potential approaches for solving some of these problems using specialized RL algorithms.

## Executive Summary
This paper investigates the fundamental limitations of scalar, Markovian reward functions in expressing complex reinforcement learning tasks. The authors focus on three classes of tasks that are challenging to express with standard reward functions: multi-objective RL, risk-sensitive RL, and modal RL. They derive necessary and sufficient conditions for when these tasks can be expressed using scalar, Markovian rewards and prove that most tasks in these classes cannot be expressed this way. The paper introduces modal tasks as a new class and outlines potential approaches for solving some of these problems using specialized RL algorithms.

## Method Summary
The authors employ a theoretical approach to analyze the expressivity of scalar, Markovian reward functions. They define three classes of tasks (multi-objective, risk-sensitive, and modal) and derive necessary and sufficient conditions for when these tasks can be expressed using scalar, Markovian rewards. The proofs involve mathematical reasoning about Markov Decision Processes (MDPs) and reward functions. The authors also introduce modal tasks as a new class and discuss potential approaches for solving some of these problems using specialized RL algorithms.

## Key Results
- Most multi-objective, risk-sensitive, and modal tasks cannot be expressed using scalar, Markovian rewards
- Necessary and sufficient conditions for reward expressivity are derived
- Modal tasks are introduced as a new class of reinforcement learning problems
- Potential approaches for solving some of these problems using specialized RL algorithms are outlined

## Why This Works (Mechanism)
The paper's theoretical analysis demonstrates that certain complex reinforcement learning tasks cannot be adequately expressed using scalar, Markovian reward functions due to their inherent limitations. The mechanism behind this limitation lies in the inability of scalar, Markovian rewards to capture the nuanced requirements of multi-objective, risk-sensitive, and modal tasks. These tasks often require consideration of multiple objectives simultaneously, risk preferences, or specific state visitation patterns, which cannot be fully encoded in a single, state-dependent reward signal.

## Foundational Learning

1. Markov Decision Processes (MDPs)
   - Why needed: MDPs form the theoretical foundation for reinforcement learning and are used to model the decision-making problems analyzed in the paper.
   - Quick check: Can you explain the components of an MDP (states, actions, transition probabilities, rewards) and how they relate to reinforcement learning?

2. Reward Functions in RL
   - Why needed: Understanding the role and limitations of reward functions is crucial for grasping the paper's main arguments about expressivity.
   - Quick check: How do reward functions influence agent behavior in reinforcement learning, and what are the potential pitfalls of poorly designed reward functions?

3. Multi-Objective Optimization
   - Why needed: Multi-objective RL is one of the three main classes of tasks analyzed in the paper.
   - Quick check: What are the challenges in optimizing for multiple objectives simultaneously, and how do they differ from single-objective optimization?

4. Risk-Sensitive Decision Making
   - Why needed: Risk-sensitive RL is another key class of tasks examined in the paper.
   - Quick check: How does risk sensitivity affect decision-making in reinforcement learning, and what are some common approaches to incorporating risk preferences into RL algorithms?

5. Modal Tasks in RL
   - Why needed: Modal tasks are introduced as a new class of reinforcement learning problems in this paper.
   - Quick check: What distinguishes modal tasks from other types of RL problems, and why might they be challenging to express using standard reward functions?

## Architecture Onboarding

Component Map:
- MDP Environment -> Agent -> Scalar, Markovian Reward Function -> Value Function
- (Multi-objective, Risk-sensitive, or Modal Task) -> (Complex Reward Structure) -> (Inability to Express with Scalar, Markovian Rewards)

Critical Path:
The critical path in this analysis involves defining the task class, constructing the MDP, and proving the impossibility of expressing the task using scalar, Markovian rewards. This process requires careful mathematical reasoning about the properties of the task and the limitations of scalar, Markovian rewards.

Design Tradeoffs:
The paper implicitly highlights a tradeoff between the simplicity and generality of scalar, Markovian reward functions and their ability to express complex task requirements. While scalar, Markovian rewards are easy to implement and widely used, they may be insufficient for capturing the nuances of certain reinforcement learning problems.

Failure Signatures:
The main failure signature identified in the paper is the inability to find a scalar, Markovian reward function that induces the desired optimal policy for certain multi-objective, risk-sensitive, or modal tasks. This failure manifests as a mismatch between the optimal policy under the proposed reward function and the intended optimal behavior for the task.

Three First Experiments:
1. Implement a simple multi-objective RL problem and attempt to express it using a scalar, Markovian reward function. Compare the optimal policy under this reward function with the intended optimal behavior.
2. Design a risk-sensitive RL task and try to encode the risk preferences using a scalar, Markovian reward. Analyze the resulting policy and its risk profile.
3. Create a modal RL task that requires specific state visitation patterns and attempt to express it using scalar, Markovian rewards. Evaluate the ability of the resulting policy to satisfy the modal constraints.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis is primarily theoretical and does not include empirical validation of the proposed expressivity gaps
- The practical implications of these limitations for real-world RL applications remain unclear
- The paper focuses on discrete MDPs, and it is uncertain whether the results directly extend to continuous state and action spaces

## Confidence
- High: The theoretical claims about the inability to express certain tasks using scalar, Markovian rewards
- Medium: The practical significance of these limitations for real-world RL applications

## Next Checks
1. Implement empirical experiments to demonstrate the practical impact of the identified expressivity limitations on standard RL benchmarks.
2. Extend the theoretical analysis to continuous MDPs and compare the results with the discrete case.
3. Investigate whether existing RL algorithms that handle non-Markovian or vector-valued rewards can effectively address the identified limitations in practice.