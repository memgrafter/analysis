---
ver: rpa2
title: Towards 3D Acceleration for low-power Mixture-of-Experts and Multi-Head Attention
  Spiking Transformers
arxiv_id: '2412.05540'
source_url: https://arxiv.org/abs/2412.05540
tags:
- spiking
- expert
- attention
- design
- buffer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first 3D hardware architecture for spiking
  Mixture-of-Experts (MoE) transformers with Multi-Head Attention (MHA), addressing
  the lack of dedicated hardware support for these brain-inspired models. The proposed
  architecture leverages face-to-face (F2F) 3D integration with memory-on-logic and
  logic-on-logic stacking to create spatially stackable circuitry.
---

# Towards 3D Acceleration for low-power Mixture-of-Experts and Multi-Head Attention Spiking Transformers

## Quick Facts
- arXiv ID: 2412.05540
- Source URL: https://arxiv.org/abs/2412.05540
- Reference count: 31
- Key outcome: First 3D hardware architecture for spiking MoE transformers with MHA, demonstrating 3%-5.1% frequency improvement, 39%-41% area reduction, 26.9%-29% memory latency reduction, and up to 14.4% power reduction versus 2D CMOS

## Executive Summary
This paper introduces the first 3D hardware architecture for spiking Mixture-of-Experts (MoE) transformers with Multi-Head Attention (MHA). The design leverages face-to-face 3D integration with memory-on-logic and logic-on-logic stacking to create spatially stackable circuitry. By mapping spiking experts and attention heads to modularized 3D two-tier acceleration cores, the architecture efficiently explores spatial and temporal parallelism while enabling weight reuse. The proposed 3D accelerators demonstrate significant improvements in energy efficiency and latency compared to conventional 2D CMOS integration.

## Method Summary
The paper proposes a 3D hardware architecture for spiking MoE transformers with MHA, using face-to-face 3D integration with memory-on-logic and logic-on-logic stacking. The design maps spiking experts and attention heads to modularized 3D two-tier acceleration cores, enabling parallel distributed processing and weight reuse. The architecture uses kernel-fused parallel processing within each modularized spiking expert core to reduce data movement and improve computation density. The 3D design is implemented using commercial 28nm PDK and compared against a 2D baseline through physical synthesis and PPA analysis.

## Key Results
- 3%-5.1% increase in effective frequency compared to 2D CMOS integration
- 39%-41% area reduction achieved through 3D stacking
- 26.9%-29% memory access latency reduction for both MHA and MoE workloads
- Up to 14.4% power reduction demonstrated for spiking MoE and MHA workloads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Face-to-face (F2F) 3D integration with memory-on-logic and logic-on-logic stacking reduces interconnect wirelength and improves memory access latency.
- Mechanism: Vertically stacking memory layers closer to computation cores shortens critical signal paths, directly reducing latency and power for memory accesses.
- Core assumption: The vertical interconnect pitch in F2F bonding (0.5-1 µm) is small enough to meaningfully shorten wirelength without introducing excessive thermal or yield penalties.
- Evidence anchors:
  - [abstract] states "significant optimization of energy efficiency and latency compared to conventional 2D CMOS integration."
  - [section V.B] shows memory access latency reductions of 15%-30% and power reductions of 26.9%-29% for both MHA and MoE workloads.
- Break condition: If the vertical interconnect pitch is too large, or if thermal management overhead outweighs the wirelength benefits, the latency and power gains would diminish.

### Mechanism 2
- Claim: Modularized 3D two-tier spiking expert cores enable parallel distributed processing by mapping spiking experts and attention heads to spatially stackable acceleration cores.
- Mechanism: By partitioning experts and attention heads across separate 3D tiers, the architecture exploits both spatial and temporal parallelism. Each expert core processes its assigned workload independently, while the router and dispatchers manage inter-expert communication.
- Core assumption: The workload can be cleanly partitioned into modular expert units that operate independently without excessive synchronization overhead.
- Evidence anchors:
  - [abstract] notes "efficiently exploring spatial and temporal parallelism weight reuse within modularized spiking experts."
  - [section III.B] describes assembling modularized Spiking Expert (SE) cores and placing two sharable expert-weight Global Buffers between them, with a centralized spiking activation GLB.
- Break condition: If expert workloads are too interdependent, or if synchronization overhead dominates, the parallelism benefits would erode.

### Mechanism 3
- Claim: Kernel-fused parallel processing within each modularized spiking expert core reduces data movement and improves computation density.
- Mechanism: By fusing synaptic integration, membrane potential accumulation, and spike generation into a single systolic array pipeline, the architecture avoids intermediate data transfers between separate functional units.
- Core assumption: The systolic array can efficiently handle the fused operations without becoming a bottleneck, and the spike generators can access integration results via dedicated readout ports.
- Evidence anchors:
  - [section III.A] outlines the five core processing steps and notes that steps ➁➂➃ are intra-expert computations handled within each expert.
  - [section III.B] describes the systolic PE array performing kernel-fused operations of ➁+➂+➃ within each SE core.
- Break condition: If the fused operations require more cycles than a pipelined approach, or if the integration-stationary approach limits throughput, the density gains would be lost.

## Foundational Learning

- Concept: Spiking Neural Networks (SNNs) and leaky integrate-and-fire (LIF) neuron models
  - Why needed here: The accelerator is designed specifically for spiking MoE transformers, which rely on binary spike-based activations and temporal coding for ultra-low energy consumption.
  - Quick check question: How does the LIF model generate a spike, and what happens to the membrane potential afterward?

- Concept: Mixture-of-Experts (MoE) and conditional computation
  - Why needed here: MoE layers route input tokens to different experts based on learned gating scores, enabling model scalability without scaling computation proportionally.
  - Quick check question: What is the role of the routing network in MoE, and how does it determine which expert processes which token?

- Concept: 3D integration and face-to-face (F2F) bonding
  - Why needed here: The 3D stacking of memory-on-logic and logic-on-logic layers is the key enabler for reduced wirelength, lower latency, and higher area efficiency compared to 2D designs.
  - Quick check question: What are the main differences between monolithic 3D and F2F bonding, and why is F2F chosen for this design?

## Architecture Onboarding

- Component map:
  - Top tier: Spiking activation GLB, weight GLBs (GLB A/B), spiking generators, token routers, attention dispatchers
  - Bottom tier: Systolic PE arrays, local buffers (activation, weight, synaptic integration), Q/K/V buffers, spike buffers
  - Inter-tier connections: Vertical extractable ports for routing scores, integration results, and weights

- Critical path:
  1. Load pre-synaptic activation and routing weights from GLBs to bottom tier
  2. Compute expert scores and route tokens to appropriate SE cores
  3. Preload expert weights and perform kernel-fused synaptic integration, accumulation, and spike generation
  4. Merge outputs and write through to activation GLB
  5. For MHA: load Q/K/V, partition features, dispatch to attention cores, compute attention maps and weighted integration, generate spikes, merge outputs

- Design tradeoffs:
  - 3D vs 2D: 3D offers reduced wirelength and memory access latency, but introduces thermal and yield complexity
  - Modularization vs integration: Modular spiking experts enable parallelism but require careful synchronization and routing logic
  - Kernel fusion vs modularity: Fusing operations reduces data movement but may limit flexibility and increase pipeline depth

- Failure signatures:
  - High memory access latency or power: Could indicate suboptimal 3D partitioning or excessive inter-tier communication
  - Low effective frequency: May point to bottlenecks in the systolic array or spike generator pipeline
  - Poor expert utilization: Suggests routing logic or workload partitioning is ineffective

- First 3 experiments:
  1. Benchmark memory access latency and power for a single MoE and MHA layer on both 2D and 3D implementations to confirm the claimed reductions
  2. Measure expert utilization and synchronization overhead as the number of experts scales up, to validate parallel distributed processing
  3. Profile effective frequency and area usage across different 3D stacking configurations (e.g., varying the number of tiers or buffer placements) to identify optimal layout

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the scalability limits of 3D integrated spiking MoE transformers in terms of the number of experts and attention heads?
- Basis in paper: [explicit] The paper mentions that MoE typically distributes experts across multiple GPUs but does not address the scalability limits when implementing these architectures in 3D integrated circuits.
- Why unresolved: The paper focuses on demonstrating improvements with 4-modularized expert systems but does not explore the upper bounds of scalability or the point at which performance gains plateau.
- What evidence would resolve it: Experimental results showing performance metrics (e.g., latency, power, area) for 3D accelerators with varying numbers of experts and attention heads beyond 4, up to a point where diminishing returns are observed.

### Open Question 2
- Question: How does the 3D architecture handle dynamic load balancing and routing efficiency when the distribution of tokens to experts becomes highly skewed?
- Basis in paper: [inferred] The paper discusses the routing mechanism for assigning tokens to experts but does not address scenarios where certain experts receive disproportionately more tokens, potentially leading to bottlenecks.
- Why unresolved: Load balancing is critical for maintaining efficiency in distributed systems, yet the paper does not explore strategies for handling imbalanced workloads in the 3D design.
- What evidence would resolve it: Simulations or analytical models demonstrating the performance impact of skewed token distributions and the effectiveness of load balancing techniques in the 3D architecture.

### Open Question 3
- Question: What are the thermal management challenges in 3D integrated spiking MoE transformers, and how do they impact long-term reliability?
- Basis in paper: [inferred] The paper highlights the area and power efficiency benefits of 3D integration but does not discuss thermal dissipation or reliability concerns associated with high-density stacking.
- Why unresolved: 3D integration can exacerbate heat dissipation issues, which are critical for maintaining performance and reliability, especially in neuromorphic systems operating continuously.
- What evidence would resolve it: Thermal modeling and reliability testing data showing temperature distributions, heat dissipation strategies, and long-term performance stability under sustained workloads.

## Limitations
- The paper lacks detailed micro-architectural specifications for the reconfigurable PE design and systolic array dimensions, making precise replication difficult
- No experimental data is provided on thermal management overhead in the 3D integration, which could offset some performance gains
- The workload partitioning assumptions for modular experts are not validated with large-scale MoE models (only 4 experts tested)

## Confidence
- **High confidence**: 3D integration benefits for memory access latency and power reduction (supported by multiple PPA comparisons in section V.B)
- **Medium confidence**: Modularization benefits for parallel distributed processing (architecture described but not experimentally validated for scaling)
- **Low confidence**: Kernel-fusion benefits for computation density (mechanism inferred but not directly measured)

## Next Checks
1. Measure expert utilization and synchronization overhead as the number of experts scales from 4 to 64, to validate parallel distributed processing scalability
2. Profile thermal gradients and power density in the 3D stack using thermal simulation tools to quantify thermal management overhead
3. Implement and measure the routing score computing array and systolic PE array dimensions in RTL to verify the claimed effective frequency improvements