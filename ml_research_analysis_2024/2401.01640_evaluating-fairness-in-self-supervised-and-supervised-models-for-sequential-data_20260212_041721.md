---
ver: rpa2
title: Evaluating Fairness in Self-supervised and Supervised Models for Sequential
  Data
arxiv_id: '2401.01640'
source_url: https://arxiv.org/abs/2401.01640
tags:
- fairness
- supervised
- learning
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of self-supervised learning
  (SSL) on fairness in human-centric sequential data. It systematically compares SSL
  models with supervised counterparts, focusing on fine-tuning strategies and their
  effects on fairness and learned representations.
---

# Evaluating Fairness in Self-supervised and Supervised Models for Sequential Data

## Quick Facts
- arXiv ID: 2401.01640
- Source URL: https://arxiv.org/abs/2401.01640
- Reference count: 10
- Primary result: SSL models can achieve performance on par with supervised methods while significantly enhancing fairness, with up to 27% increase in fairness accompanied by only 1% loss in performance

## Executive Summary
This study investigates the impact of self-supervised learning (SSL) on fairness in human-centric sequential data, comparing SSL models with supervised counterparts through systematic evaluation of fine-tuning strategies and their effects on fairness and learned representations. The research demonstrates that SSL models can achieve performance parity with supervised methods while significantly enhancing fairness metrics, particularly in healthcare applications where data scarcity is a concern. The findings underscore SSL's potential in high-stakes domains and emphasize the importance of assessing fairness alongside performance metrics in machine learning applications.

## Method Summary
The study uses MIMIC-III clinical timeseries data (17 clinical variables, 31 million events, 42.2K ICU stays) to predict in-hospital mortality, comparing SSL models (SimCLR-based with 3-layer CNN encoder) against supervised baselines through gradual freezing fine-tuning strategies. Models are evaluated using AUC-ROC for performance and Error Rate Ratio for fairness across protected attributes (gender, age, ethnicity, religion, language, insurance type), with representation similarity analyzed using Centered Kernel Alignment (CKA) to identify demographic group differences in learned representations.

## Key Results
- SSL models achieve up to 27% increase in fairness (Error Rate Ratio) with only 1% loss in performance compared to supervised methods
- Gradual fine-tuning with frozen encoder layers enables effective balance between fairness and performance
- CKA analysis reveals greater representation similarity across demographic groups in SSL models, particularly in early layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised learning reduces model bias because pre-training occurs without human-annotated labels that may encode demographic prejudices.
- Mechanism: SSL uses pretext tasks (e.g., contrastive learning) to derive representations purely from data structure, avoiding label-induced skew.
- Core assumption: The unlabeled dataset itself is not systematically biased along protected attributes.
- Evidence anchors:
  - [abstract] "SSL models may avoid such pitfalls due to their pre-training without (potentially) biased human annotations"
  - [section] "SSL exploitation is promising to explore extensive unlabeled data, effectively complementing small, labeled domain datasets"
- Break condition: If the unlabeled dataset underrepresents or misrepresents certain demographic groups, SSL may still inherit structural bias.

### Mechanism 2
- Claim: Gradual fine-tuning strategies control the balance between learned generic representations and task-specific supervision, optimizing fairness.
- Mechanism: Freezing different encoder layers during fine-tuning preserves SSL-derived invariance while allowing limited supervision to adapt to downstream labels.
- Core assumption: Intermediate SSL representations retain useful cross-demographic invariance that is not destroyed by partial supervision.
- Evidence anchors:
  - [abstract] "We employ a gradual freezing strategy, balancing the impact of the pre-trained encoder and the downstream labels"
  - [section] "We experiment with different freezing combinations, as seen in Figure 1"
- Break condition: Overfitting to supervised labels during fine-tuning may reintroduce demographic bias if the training set is imbalanced.

### Mechanism 3
- Claim: Learned representations from SSL diverge more slowly across demographic groups than supervised models, reducing group-level disparities.
- Mechanism: Representation similarity (CKA) analysis shows that SSL maintains higher similarity across subgroups in early layers, supporting equitable decision surfaces.
- Core assumption: Higher representation similarity correlates with reduced disparate error rates across groups.
- Evidence anchors:
  - [abstract] "we utilize Centered Kernel Alignment (CKA) as a similarity index... we also condition CKA on protected attributes to identify differences in representation similarity across demographic groups"
  - [section] "we notice greater similarity in learned representations for males than for females"
- Break condition: If downstream tasks require demographic-specific adaptations, enforcing high similarity could degrade performance for certain groups.

## Foundational Learning

- Concept: Contrastive learning and pretext tasks
  - Why needed here: SSL models rely on contrasting augmented views of the same sample to learn generalizable features without labels.
  - Quick check question: What loss function is typically used to train contrastive SSL models like SimCLR?

- Concept: Evaluation metrics beyond accuracy (e.g., Error Rate Ratio)
  - Why needed here: Fairness requires measuring error disparities across protected groups, not just overall performance.
  - Quick check question: How does Error Rate Ratio differ from Equal Opportunity Difference in fairness assessment?

- Concept: Representation similarity measures (e.g., CKA)
  - Why needed here: Comparing latent spaces of SSL vs. supervised models across demographic groups reveals structural bias differences.
  - Quick check question: What property of neural representations does CKA measure, and why is it preferred over linear regression-based similarity?

## Architecture Onboarding

- Component map: Data augmentation module → 3-layer CNN encoder → 2-layer MLP projection head → NT-Xent contrastive loss (pre-training) → Frozen/unfrozen encoder layers → classification head → supervised loss (fine-tuning)
- Critical path: Pre-train SSL model → Freeze/unfreeze layers → Fine-tune with downstream labels → Evaluate fairness and performance
- Design tradeoffs:
  - More frozen layers preserve fairness but may limit performance gains.
  - Less frozen layers risk reintroducing bias from supervised labels.
  - Tradeoff between representation generality and task specificity.
- Failure signatures:
  - Large fairness gaps persisting despite SSL pre-training → dataset bias or inadequate fine-tuning strategy.
  - Performance collapse when too many layers are frozen → insufficient adaptation to downstream task.
- First 3 experiments:
  1. Train fully supervised baseline and SSL model with all layers frozen; compare AUC-ROC and ERR.
  2. Gradually unfreeze one encoder layer at a time; measure impact on fairness vs. performance trade-off.
  3. Compute CKA similarity conditioned on protected attributes; identify which layers show greatest subgroup divergence.

## Open Questions the Paper Calls Out
None

## Limitations
- Claims about SSL improving fairness carry Medium confidence due to narrow scope of tested domains (primarily healthcare)
- The 27% fairness improvement figure is based on a single dataset (MIMIC-III), limiting generalizability
- Unknown impact of SSL on intersectional fairness (multiple overlapping protected attributes)

## Confidence

- SSL fairness benefits in healthcare: Medium
- Gradual fine-tuning as optimal strategy: Medium
- Representation similarity correlating with fairness: Low-Medium

## Next Checks

1. Test SSL models on additional sequential datasets (e.g., financial transactions, educational records) to verify domain transferability
2. Conduct intersectional fairness analysis by evaluating models across combinations of protected attributes
3. Perform temporal validation by retraining models on earlier MIMIC-III data and testing on later periods to assess temporal robustness