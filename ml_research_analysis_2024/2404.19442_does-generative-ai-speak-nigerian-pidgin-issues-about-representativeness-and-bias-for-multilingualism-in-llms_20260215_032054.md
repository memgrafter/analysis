---
ver: rpa2
title: 'Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness
  and Bias for Multilingualism in LLMs'
arxiv_id: '2404.19442'
source_url: https://arxiv.org/abs/2404.19442
tags:
- genre
- naija
- wikipedia
- english
- first
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights the bias of generative AI models towards
  one genre of Nigerian Pidgin (Naija BBC genre) over another (Naija Wikipedia genre),
  despite both genres having similar speaker populations. The authors introduce WARRI,
  a new MT evaluation dataset that includes parallel sentences in both BBC and Wikipedia
  genres of Naija, along with English translations.
---

# Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs

## Quick Facts
- arXiv ID: 2404.19442
- Source URL: https://arxiv.org/abs/2404.19442
- Authors: David Ifeoluwa Adelani; A. Seza Doğruöz; Iyanuoluwa Shode; Anuoluwapo Aremu
- Reference count: 20
- This paper highlights the bias of generative AI models towards one genre of Nigerian Pidgin (Naija BBC genre) over another (Naija Wikipedia genre), despite both genres having similar speaker populations.

## Executive Summary
This paper investigates the representativeness and bias of generative AI models towards different genres of Nigerian Pidgin (Naija). The authors introduce WARRI, a new MT evaluation dataset that includes parallel sentences in both BBC and Wikipedia genres of Naija, along with English translations. Through statistical analysis and MT experiments, they demonstrate that Naija Wikipedia genre is underrepresented in generative AI systems, with models performing significantly worse on this genre compared to the BBC genre. The authors attribute this bias to the higher availability of Naija BBC data on the internet and argue that this leads to language technologies that are not inclusive of all Naija speakers.

## Method Summary
The paper collects Naija data from BBC and Wikipedia genres, creates a parallel English dataset through bilingual translation, and conducts statistical analyses to compare linguistic differences between genres. It evaluates MT models (M2M-100 fine-tuned on MAFAND) and LLMs (GPT-4-TURBO and Llama 2) on both genres using BLEU and ChrF++ metrics. The study compares performance across zero-shot and few-shot prompting settings to assess the effectiveness of in-context learning for genre adaptation.

## Key Results
- Naija Wikipedia genre consistently shows lower lexical overlap with English compared to BBC genre across all n-grams
- Fine-tuned MT models perform 24.3 points worse on Wikipedia genre compared to BBC genre (ChrF++)
- Few-shot prompting provides minimal improvement (+2.3 to +2.7) for Wikipedia genre adaptation
- WARRI dataset reveals systematic bias in generative AI toward BBC genre despite similar speaker populations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical and structural differences between Naija BBC and Wikipedia genres cause biased model performance
- Mechanism: The two written genres have distinct vocabulary choices and word orders that diverge significantly from English, making them non-representative of each other
- Core assumption: Generative AI models are trained primarily on data that reflects the BBC genre's linguistic patterns
- Evidence anchors:
  - [abstract] "we prove that these two genres do not represent each other (i.e., there are linguistic differences in word order and vocabulary) and Generative AI operates only based on Naija written in the BBC genre"
  - [section] "Naija Wikipedia genre consistently has a lower Jaccard similarity score with its parallel English corpus for all n-grams" and "it takes more than twice edit-distance to transform the English sentences to Naija Wikipedia genre than to Naija BBC genre"
- Break condition: If the two genres were linguistically similar or if training data included balanced representation of both genres, the bias would diminish

### Mechanism 2
- Claim: Limited parallel training data in diverse genres leads to genre-specific model bias
- Mechanism: Models fine-tuned on MAFAND (which likely contains more BBC-style sentences) perform well on BBC genre but poorly on Wikipedia genre due to domain mismatch
- Core assumption: The MAFAND dataset composition reflects the linguistic patterns of the BBC genre more than Wikipedia genre
- Evidence anchors:
  - [abstract] "we also find that Generative AI models (e.g., GPT-4-TURBO and LLAMA 2 13B) are biased towards the BBC genre (favored by more educated groups in Nigeria)"
  - [section] "adapting MAFAND MT model to BBC genre gave an impressive result in both single-way (76.7 ChrF++) and multi-way parallel (83 ChrF++)" but "performance on Wikipedia genre is much worse (−24.3 drop in ChrF++)"
- Break condition: If MAFAND contained balanced representation of both genres or if separate genre-specific models were trained, performance disparity would reduce

### Mechanism 3
- Claim: Few-shot prompting has limited effectiveness in teaching models new genre styles
- Mechanism: Even with 1-5 examples, models struggle to adopt Wikipedia genre patterns while maintaining semantic accuracy
- Core assumption: LLMs have strong prior biases toward the BBC genre that few examples cannot overcome
- Evidence anchors:
  - [abstract] "it is hard to teach LLMs with few examples"
  - [section] "The boost in performance is very small when Naija Wikipedia genre examples are provided. The boost is only +2.3 and +2.7 when one example and five examples are provided"
- Break condition: If more extensive fine-tuning or larger prompt examples were used, the model might better learn Wikipedia genre patterns

## Foundational Learning

- Concept: Representativeness in corpus design
  - Why needed here: Understanding why sampling from multiple genres is crucial for avoiding bias in language models
  - Quick check question: If you only collect data from one genre, what linguistic features might your model miss when applied to other genres?

- Concept: Domain adaptation and transfer learning
  - Why needed here: Explains why models trained on one genre perform poorly on another despite both being "Naija"
  - Quick check question: If a model performs well on news domain but poorly on Wikipedia domain, what does this suggest about the training data composition?

- Concept: Zero-shot vs few-shot learning effectiveness
  - Why needed here: Demonstrates the limitations of in-context learning for teaching new linguistic patterns
- Quick check question: If adding examples improves performance by only 2-3 points, what does this suggest about the model's prior biases?

## Architecture Onboarding

- Component map: WARRI dataset creation → Statistical analysis (lexical overlap, Levenshtein distance) → MT model training (MAFAND fine-tuning) → LLM prompting experiments → Evaluation
- Critical path: Data collection and annotation → Statistical validation of genre differences → Model training and evaluation → Analysis of bias patterns
- Design tradeoffs: Creating multi-way parallel data ensures fair comparison but requires professional translation; using few-shot learning is faster but less effective than full fine-tuning
- Failure signatures: Large performance gaps between genres, minimal improvement from few-shot examples, semantic meaning loss when attempting Wikipedia style
- First 3 experiments:
  1. Compute lexical overlap metrics (Jaccard similarity, Levenshtein distance) between English and each Naija genre
  2. Fine-tune M2M-100 on MAFAND and evaluate on WARRI test sets for both genres
  3. Prompt GPT-4 and Llama 2 with zero, one, and five examples for each genre and compare performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific impact of BBC genre dominance on language technology users who primarily use or are more comfortable with the Wikipedia genre of Nigerian Pidgin?
- Basis in paper: [explicit] The paper highlights that the BBC genre is favored by more educated Nigerians, whereas the Wikipedia genre is closer to spoken Naija and used by a diversity of speakers.
- Why unresolved: The paper does not provide empirical evidence or case studies demonstrating how this bias affects actual users of the Wikipedia genre.
- What evidence would resolve it: User studies or surveys showing the practical challenges faced by speakers of the Wikipedia genre when interacting with language technologies biased towards the BBC genre.

### Open Question 2
- Question: How can the representativeness of different genres in low-resource languages like Nigerian Pidgin be improved in generative AI systems?
- Basis in paper: [explicit] The authors call for more research to address the issue of underrepresentation of the Wikipedia genre in generative AI systems.
- Why unresolved: The paper does not propose specific methodologies or strategies to ensure balanced representation of different genres in AI systems.
- What evidence would resolve it: Research or case studies that successfully integrate and balance multiple genres of a low-resource language in generative AI systems, showing improved inclusivity and accessibility.

### Open Question 3
- Question: What are the long-term sociolinguistic implications of generative AI systems predominantly using the BBC genre of Nigerian Pidgin over the Wikipedia genre?
- Basis in paper: [inferred] The paper discusses the linguistic and social differences between the two genres and suggests that the dominance of one genre over the other could lead to language technologies that are not inclusive of all Naija speakers.
- Why unresolved: The paper does not explore the broader sociolinguistic impacts of this genre bias on language preservation, evolution, or community identity.
- What evidence would resolve it: Longitudinal studies examining the sociolinguistic trends in Nigerian Pidgin usage, particularly focusing on how generative AI influences language practices and community dynamics over time.

## Limitations
- The analysis relies on a relatively small parallel corpus (WARRI) with only 2,000 sentences, which may not fully capture the diversity of Nigerian Pidgin across all genres and domains.
- The study focuses on written genres (BBC and Wikipedia) rather than spoken varieties, potentially missing important linguistic variations.
- The evaluation is limited to English-Naija translation tasks, which may not reflect the full capabilities or biases of generative AI models in other NLP applications.

## Confidence
- **High confidence**: The statistical analysis demonstrating lexical and structural differences between BBC and Wikipedia genres is robust, supported by multiple metrics (Jaccard similarity, Levenshtein distance) showing consistent patterns across n-grams.
- **Medium confidence**: The attribution of model bias to training data composition is plausible but not definitively proven, as other factors like model architecture and pre-training objectives could also contribute to the observed performance gaps.
- **Medium confidence**: The claim that few-shot prompting has limited effectiveness for genre adaptation is supported by the data, but the small performance improvements (+2.3 to +2.7) could potentially be improved with different prompting strategies or larger example sets.

## Next Checks
1. Expand corpus validation: Test model performance across additional Naija genres (e.g., social media, literature, spoken transcripts) to determine if the BBC-Wikipedia bias extends to other linguistic domains and whether the observed bias is systematic or genre-specific.

2. Controlled training experiments: Create balanced training datasets with equal representation of BBC and Wikipedia genres and retrain models to empirically test whether the performance gap diminishes, isolating the effect of training data composition from other potential confounding factors.

3. Cross-linguistic comparison: Replicate the analysis framework with other low-resource languages that have multiple written genres (e.g., Swahili, Hindi) to determine whether this genre bias phenomenon is specific to Nigerian Pidgin or represents a more general challenge in multilingual NLP.