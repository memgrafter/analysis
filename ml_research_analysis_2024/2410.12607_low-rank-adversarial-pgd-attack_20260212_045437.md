---
ver: rpa2
title: Low-Rank Adversarial PGD Attack
arxiv_id: '2410.12607'
source_url: https://arxiv.org/abs/2410.12607
tags:
- adversarial
- attacks
- attack
- low-rank
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes LoRa-PGD, a low-rank variant of the projected
  gradient descent (PGD) adversarial attack. The key observation is that PGD-generated
  perturbations often have a low-rank structure, meaning they predominantly affect
  only a subset of the singular values of the original image.
---

# Low-Rank Adversarial PGD Attack

## Quick Facts
- arXiv ID: 2410.12607
- Source URL: https://arxiv.org/abs/2410.12607
- Authors: Dayana Savostianova; Emanuele Zangrando; Francesco Tudisco
- Reference count: 40
- Key outcome: LoRa-PGD achieves comparable adversarial attack performance to standard PGD while using significantly less memory through low-rank tensor decomposition

## Executive Summary
The paper introduces LoRa-PGD, a low-rank variant of the Projected Gradient Descent (PGD) adversarial attack that leverages the observation that PGD-generated perturbations often exhibit low-rank structure. By decomposing the perturbation into a low-rank tensor representation, LoRa-PGD significantly reduces memory requirements while maintaining competitive attack performance. The method is particularly effective when evaluated using the nuclear norm metric, which measures perturbations in the spectral domain, though it performs comparably to standard PGD under traditional l2 norm evaluation.

## Method Summary
LoRa-PGD operates by representing adversarial perturbations using a low-rank tensor decomposition, specifically focusing on the singular value decomposition of the perturbation matrix. The algorithm iteratively updates a set of low-rank basis vectors and coefficients rather than the full perturbation matrix, dramatically reducing memory footprint. The attack maintains the core PGD framework of iterative gradient-based updates with projection back to the feasible region, but applies this to the compressed low-rank representation. The rank of the decomposition is a hyperparameter that controls the trade-off between attack effectiveness and computational efficiency.

## Key Results
- LoRa-PGD achieves comparable l2-norm performance to full-rank PGD while using significantly less memory
- Under nuclear norm evaluation, LoRa-PGD outperforms standard PGD, suggesting more efficient perturbation of the image's spectral content
- Experiments on CIFAR-10 and ImageNet demonstrate that lower-rank attacks (down to 10-20% relative rank) maintain effectiveness while providing substantial memory savings

## Why This Works (Mechanism)
LoRa-PGD exploits the observation that adversarial perturbations generated by standard PGD tend to concentrate their energy in a small number of singular values and corresponding singular vectors. This low-rank structure means that most of the perturbation's effect can be captured by a small subset of the image's principal components. By working directly in this compressed representation, the method avoids redundant computations on components that contribute minimally to the adversarial effect, achieving efficiency gains without sacrificing attack strength.

## Foundational Learning

**Singular Value Decomposition (SVD)**
- Why needed: Forms the mathematical foundation for representing images and perturbations in a low-rank format
- Quick check: Verify that most singular values of PGD perturbations decay rapidly, confirming low-rank structure

**Projected Gradient Descent (PGD)**
- Why needed: Provides the baseline adversarial attack framework that LoRa-PGD modifies
- Quick check: Ensure PGD convergence properties are maintained in the low-rank space

**Nuclear Norm**
- Why needed: Alternative metric for measuring perturbation magnitude in the spectral domain
- Quick check: Compare l2 vs nuclear norm results to understand different perturbation characteristics

**Tensor Decomposition**
- Why needed: Enables efficient representation and manipulation of low-rank perturbations
- Quick check: Validate that reconstruction error remains low for chosen rank parameters

## Architecture Onboarding

**Component Map**
Input Image -> Low-rank PGD Attack -> Adversarial Example -> Model Evaluation

**Critical Path**
Image preprocessing → Low-rank decomposition initialization → Iterative PGD updates in low-rank space → Reconstruction → Model prediction

**Design Tradeoffs**
- Higher rank → Better attack effectiveness but increased memory usage
- Lower rank → Greater efficiency but potential loss in attack strength
- Nuclear norm vs l2 norm → Different perturbation objectives with varying effectiveness

**Failure Signatures**
- Rank too low: Attack fails to achieve target confidence or success rate
- Improper initialization: Slow convergence or suboptimal final perturbations
- Numerical instability: Decomposition breakdown during iterative updates

**First 3 Experiments**
1. Measure singular value decay of PGD perturbations to confirm low-rank structure
2. Compare attack success rates across different rank thresholds (10%, 20%, 30%, 40%, 50%)
3. Evaluate memory usage differences between LoRa-PGD and standard PGD across multiple datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LoRa-PGD scale with increasing relative rank percentages (e.g., 10%, 20%, 30%, 40%, 50%) in terms of both robust accuracy and memory usage?
- Basis in paper: [explicit] The paper mentions that LoRa-PGD shows performance comparable to full-rank PGD given an equal l2 Frobenius norm budget and outperforms it under an equal nuclear norm budget. It also states that lower-rank attacks tend to be more effective.
- Why unresolved: The paper does not provide a detailed analysis of how LoRa-PGD's performance scales with increasing relative rank percentages. It only mentions that lower-rank attacks are more effective but does not quantify the trade-off between performance and memory usage at different rank levels.
- What evidence would resolve it: A comprehensive study showing the robust accuracy and memory usage of LoRa-PGD at different relative rank percentages (e.g., 10%, 20%, 30%, 40%, 50%) compared to full-rank PGD.

### Open Question 2
- Question: Can LoRa-PGD be effectively applied to adversarial training techniques that are currently considered state-of-the-art, and how does it impact their performance?
- Basis in paper: [explicit] The paper mentions that future research will explore benchmarking LoRa-PGD within the latest adversarial training techniques to further validate its effectiveness and practicality.
- Why unresolved: The paper does not provide any empirical evidence of LoRa-PGD's performance when integrated into state-of-the-art adversarial training techniques. It only suggests that this could be a future direction.
- What evidence would resolve it: Empirical results showing the impact of using LoRa-PGD in state-of-the-art adversarial training techniques, comparing the robust accuracy and computational efficiency against traditional PGD-based methods.

### Open Question 3
- Question: How does the initialization strategy (random, transfer, warm-up) affect the convergence speed and final performance of LoRa-PGD compared to full-rank PGD?
- Basis in paper: [explicit] The paper discusses three initialization strategies (random, transfer, warm-up) and notes that they did not affect the robust accuracy performance of PGD but had a noticeable impact on LoRa-PGD results.
- Why unresolved: The paper does not provide a detailed comparison of how different initialization strategies affect the convergence speed and final performance of LoRa-PGD relative to full-rank PGD.
- What evidence would resolve it: A study comparing the convergence speed and final robust accuracy of LoRa-PGD using different initialization strategies (random, transfer, warm-up) against full-rank PGD with the same initializations.

## Limitations

- The claim that LoRa-PGD "outperforms" standard PGD in nuclear norm metrics requires careful interpretation, as this measures a different perturbation space than the l2 norm typically used for adversarial robustness evaluation
- Memory efficiency gains are demonstrated empirically but lack theoretical bounds on the trade-off between rank approximation and attack success rate
- Experiments focus primarily on white-box attacks, with limited discussion of black-box transfer capabilities or robustness to detection mechanisms

## Confidence

**Major Claim Confidence:**
- Low-rank observation in PGD perturbations: **High** - well-supported by empirical evidence and aligns with prior work on image perturbation structure
- Memory efficiency claims: **Medium** - supported by experiments but lacking comprehensive theoretical analysis
- Attack effectiveness across metrics: **Medium** - results are convincing but metric selection affects interpretation

## Next Checks

1. Test LoRa-PGD's transfer attack success rate against defended models to evaluate black-box effectiveness
2. Conduct ablation studies varying rank thresholds to establish the relationship between rank approximation and attack success rate
3. Compare LoRa-PGD's effectiveness against recently proposed sparse and structured adversarial attack methods on the same benchmarks