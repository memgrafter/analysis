---
ver: rpa2
title: '2BP: 2-Stage Backpropagation'
arxiv_id: '2405.18047'
source_url: https://arxiv.org/abs/2405.18047
tags:
- memory
- training
- schedule
- backward-p2
- pipeline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Pipeline parallelism for training large neural networks is often\
  \ bottlenecked by automatic differentiation tools, leading to excessive idle compute\
  \ time. This paper introduces 2-stage backpropagation (2BP), which splits the backward\
  \ pass into two stages\u2014computing gradients with respect to layer outputs and\
  \ parameters separately\u2014to maximize accelerator utilization."
---

# 2BP: 2-Stage Backpropagation

## Quick Facts
- arXiv ID: 2405.18047
- Source URL: https://arxiv.org/abs/2405.18047
- Authors: Christopher Rae; Joseph K. L. Lee; James Richings
- Reference count: 40
- Primary result: 2BP achieves 1.10x to 1.70x throughput improvements by splitting backward propagation into two stages

## Executive Summary
Pipeline parallelism for training large neural networks is often bottlenecked by automatic differentiation tools, leading to excessive idle compute time. This paper introduces 2-stage backpropagation (2BP), which splits the backward pass into two stages—computing gradients with respect to layer outputs and parameters separately—to maximize accelerator utilization. The method was tested on transformer, BERT, Mamba, and ResNet models using various pipelining schedules, achieving throughput improvements ranging from 1.10x to 1.70x. While 2BP significantly reduces idle time, it increases peak memory usage by 1.02x to 2.67x depending on the model and schedule.

## Method Summary
2BP splits the backward propagation step into two separate stages: backward-p1 computes gradients with respect to layer outputs (∂L/∂zl), and backward-p2 computes gradients with respect to layer parameters (∂L/∂wl). By delaying backward-p2 computation, the system can interleave computation and communication, reducing idle GPU time. The method uses micro-batching and gradient concatenation during backward-p2 to improve GPU utilization. 2BP was implemented using PyTorch with custom backward functions and tested with GPipe, 1F1B-1, and 1F1B-2 pipelining schedules on transformer, BERT, Mamba, and ResNet models.

## Key Results
- Achieved throughput improvements of 1.10x to 1.70x across different models and pipelining schedules
- Reduced bubble ratio by up to 4x in certain configurations
- Increased peak memory usage by 1.02x to 2.67x depending on model architecture and schedule
- Performance gains slightly decrease when scaling to more GPUs due to increased communication overhead

## Why This Works (Mechanism)

### Mechanism 1
Splitting backward propagation into backward-p1 and backward-p2 stages allows the system to interleave computation and communication, reducing idle GPU time. By computing gradients with respect to layer outputs (backward-p1) earlier and deferring the gradients with respect to layer parameters (backward-p2), accelerators can start working on the next micro-batch during the idle time that would normally occur in synchronous pipeline parallelism.

### Mechanism 2
Concatenating micro-batch gradients during backward-p2 reduces the number of backward operations and improves GPU utilization. Instead of calling backward-p2 separately for each micro-batch, activations and intermediate derivatives are concatenated over the batch dimension, allowing a single backward-p2 call per layer. This leverages the GPU's SIMT architecture for better compute occupancy.

### Mechanism 3
Delaying backward-p2 reduces memory pressure on earlier GPUs by allowing them to release activation memory sooner. Since backward-p2 is not needed until later in the pipeline, earlier GPUs can free up memory allocated for activations after completing backward-p1, potentially reducing peak memory usage compared to immediate backward-p2 computation.

## Foundational Learning

- Concept: Pipeline parallelism and micro-batching
  - Why needed here: Understanding how pipeline parallelism splits models across GPUs and uses micro-batches to overlap computation is essential to grasp why 2BP can reduce idle time.
  - Quick check question: In a 4-GPU pipeline with GPipe and micro-batching, how many micro-batches are typically processed concurrently to hide communication latency?

- Concept: Reverse mode automatic differentiation
  - Why needed here: 2BP relies on splitting the backward pass, which is fundamentally based on how reverse mode AD computes gradients via the chain rule.
  - Quick check question: In reverse mode AD, why is it more memory-efficient to compute gradients starting from the loss rather than from the parameters?

- Concept: CUDA streams and parallel kernel execution
  - Why needed here: The paper mentions the possibility of parallelizing backward-p2 calls using CUDA streams, which is relevant for optimizing the delayed gradient computation.
  - Quick check question: How do CUDA streams enable overlapping of memory operations and kernel execution on NVIDIA GPUs?

## Architecture Onboarding

- Component map: Model definition (PyTorch modules with custom forward, backward-p1, and backward-p2 methods) -> Pipeline scheduler (GPipe, 1F1B-1, or 1F1B-2 logic) -> Communication layer (NCCL-based all-reduce and point-to-point transfers) -> Memory manager (Activation and intermediate derivative storage) -> Optimizer (Standard applied after gradient computation)

- Critical path: 1. Forward pass through pipeline stages 2. Backward-p1 pass to compute ∂L/∂zl 3. (Optional) Concatenation of micro-batch gradients 4. Backward-p2 pass to compute ∂L/∂wl 5. All-reduce of parameter gradients 6. Optimizer step

- Design tradeoffs:
  - Memory vs. throughput: 2BP can reduce idle time but increases peak memory usage due to storing intermediate derivatives
  - Communication vs. computation: Interleaving backward-p2 during idle time reduces bubble time but may increase communication if micro-batch sizes are small
  - Complexity vs. flexibility: Manual backward implementation provides control but requires maintaining custom backward functions for each layer type

- Failure signatures:
  - OOM errors during backward-p2 due to accumulated intermediate derivatives
  - Reduced throughput if backward-p1 takes longer than the available idle time
  - Load imbalance if backward-p1 and backward-p2 times are highly asymmetric across layers

- First 3 experiments:
  1. Benchmark 2BP vs. standard backpropagation on a simple transformer with GPipe and 4 GPUs, measuring throughput and memory usage
  2. Test 2BP with 1F1B-1 and 1F1B-2 schedules on ResNet-152 to observe memory scaling and throughput gains
  3. Profile GPU compute occupancy with and without micro-batch concatenation during backward-p2 to quantify the impact of this optimization

## Open Questions the Paper Calls Out

### Open Question 1
Does intermediate derivative checkpointing reduce memory consumption without significantly impacting performance?
- Basis in paper: The paper suggests investigating intermediate derivative checkpointing as a method to decrease memory consumption caused by storing intermediate derivatives.
- Why unresolved: This is proposed as future work and has not been experimentally validated.
- What evidence would resolve it: Experimental results comparing training time and memory usage with and without intermediate derivative checkpointing.

### Open Question 2
Can CUDA graphs improve parallel execution of backward-p2 operations?
- Basis in paper: The paper mentions that CUDA graphs may be an alternative method to achieve further parallelization during backward-p2 calls, which they intend to explore in the future.
- Why unresolved: This is identified as a potential future direction but has not been implemented or tested.
- What evidence would resolve it: Performance benchmarks comparing backward-p2 execution times with CUDA streams versus CUDA graphs.

### Open Question 3
How does 2BP affect the ability to overlap communication with computation in data parallelism?
- Basis in paper: The paper states that since gradients are not calculated until the delayed computation during backward-p2, it will be significantly harder to fully overlap communication with computation.
- Why unresolved: This is acknowledged as a challenge but not experimentally investigated.
- What evidence would resolve it: Performance comparisons of data parallel training with and without 2BP, measuring communication overhead and overall throughput.

## Limitations
- Increased peak memory usage (1.02x to 2.67x) due to storing intermediate derivatives and delayed backward-p2 calls
- Performance gains slightly decrease when scaling to more GPUs due to increased communication overhead
- Difficulty in fully overlapping communication with computation in data parallelism scenarios

## Confidence

**High Confidence**: The core mechanism of splitting backward propagation into two stages is well-founded in reverse mode automatic differentiation theory. The performance improvements measured on the tested models and schedules are likely reproducible.

**Medium Confidence**: The memory usage analysis and throughput calculations are reasonable but based on theoretical models rather than comprehensive empirical validation. The scalability claims beyond 4 GPUs need further verification.

**Low Confidence**: The practical applicability to real-world training workloads, especially those involving complex data pipelines, heterogeneous GPU configurations, or non-standard model architectures.

## Next Checks

1. **Memory scaling experiment**: Measure peak memory usage of 2BP versus standard backpropagation across models of increasing size (from 100M to 10B parameters) to validate the 1.02x-2.67x memory increase claim and identify architectural patterns that minimize memory overhead.

2. **Real-world workload validation**: Test 2BP on actual training datasets (not randomly generated) with realistic batch sizes and data loading pipelines to assess whether the reported throughput gains (1.10x-1.70x) persist under practical conditions.

3. **Concatenation optimization ablation**: Create a version of 2BP without micro-batch concatenation during backward-p2 and measure the performance difference to quantify the specific contribution of this optimization to the overall throughput improvement.