---
ver: rpa2
title: Generative Language Models with Retrieval Augmented Generation for Automated
  Short Answer Scoring
arxiv_id: '2408.03811'
source_url: https://arxiv.org/abs/2408.03811
tags:
- answer
- question
- reference
- unseen
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a retrieval-augmented generation approach
  for automated short answer scoring using generative language models. The method
  combines a fine-tuned information retrieval system with vector databases to retrieve
  semantically similar training responses, which are then used as context for generative
  language models to score new responses.
---

# Generative Language Models with Retrieval Augmented Generation for Automated Short Answer Scoring

## Quick Facts
- **arXiv ID**: 2408.03811
- **Source URL**: https://arxiv.org/abs/2408.03811
- **Authors**: Zifan Wang; Christopher Ormerod
- **Reference count**: 40
- **Primary result**: RAG-based GLM system significantly outperforms existing methods on SemEval 2013 dataset

## Executive Summary
This paper introduces a retrieval-augmented generation approach for automated short answer scoring using generative language models. The system combines a fine-tuned information retrieval component with vector databases to retrieve semantically similar training responses, which are then used as context for generative language models to score new responses. Evaluated on the SemEval 2013 dataset, the approach significantly outperforms existing methods on both SCIENTSBANK 3-way and 2-way tasks, demonstrating the potential of combining retrieval and generation techniques for automated short answer scoring.

## Method Summary
The method involves fine-tuning a pre-trained Sentence Transformer model using the training data, implementing both question-specific and global training strategies. The system creates vector databases of training responses, retrieves top-K similar responses during inference, and composes prompts with these examples for generative language models. The approach uses Claude 2, Claude 3 Haiku, and Claude 3 Sonnet models with optimized prompt templates generated by Claude Prompt Generator. The system is evaluated using accuracy (ACC), macro average F1 score (M-F1), and weighted average F1 score (W-F1) on the SemEval-2013 dataset.

## Key Results
- RAG-based GLM system achieves significantly higher accuracy and F1 scores than existing methods (COMET, ETS, SOFTCARDINALITY, BERT, XLNET)
- Fine-tuned IR with optimized prompts improves performance across all evaluation scenarios
- Question-specific IR training strategy provides better results than global training approach

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning the information retrieval (IR) component with question-specific data significantly improves scoring accuracy. By training separate IR backbones for each question using labeled answer pairs from the training set, the system learns to retrieve semantically similar responses that are contextually relevant to the specific question being evaluated. This approach assumes that semantic similarity between student responses varies meaningfully across different questions, requiring specialized embedding models for optimal retrieval.

### Mechanism 2
Using RAG with retrieved examples improves GLM performance by providing context-specific scoring examples. The IR system retrieves K similar training responses for each test response, which are then included in the prompt to the GLM. This gives the model concrete examples of how similar responses were scored, benefiting from concrete examples when performing scoring tasks, especially when the scoring criteria are nuanced or context-dependent.

### Mechanism 3
Optimizing prompt templates using Claude Prompt Generator significantly improves GLM scoring performance. The Claude Prompt Generator refines initial prompt drafts into well-structured templates that clearly define the task, input/output fields, and scoring criteria, resulting in more consistent and accurate judgments. The structure and clarity of prompts significantly impacts GLM performance on scoring tasks.

## Foundational Learning

- **Vector embeddings and semantic similarity**: Why needed here - The IR system relies on converting text to vectors and measuring cosine similarity to retrieve relevant responses. Quick check question: How does cosine similarity between two vectors relate to semantic similarity between their corresponding texts?

- **Fine-tuning language models**: Why needed here - Both the IR backbone (sentence transformer) and potentially the GLM require fine-tuning for optimal performance on the specific scoring task. Quick check question: What is the difference between pretraining and fine-tuning a language model?

- **Retrieval-augmented generation (RAG)**: Why needed here - The core innovation combines retrieval of similar examples with generation of scores using a GLM. Quick check question: In what way does RAG differ from standard prompting approaches for language models?

## Architecture Onboarding

- **Component map**: Training Phase: Fine-tune IR backbone → Build vector database of training responses → Inference Phase: Embed query → Retrieve top-K similar responses → Compose prompt with examples → Generate score via GLM → Supporting: Prompt optimization tool (Claude Prompt Generator)

- **Critical path**: Embedding → Retrieval → Prompt composition → GLM scoring

- **Design tradeoffs**: Question-specific IR models provide better accuracy but require more storage and computation; larger K values provide more context but increase prompt size and processing time; using more sophisticated GLMs improves accuracy but increases cost

- **Failure signatures**: Low similarity scores in retrieval → Poor examples provided to GLM; GLM outputs non-standard responses → Prompt template needs adjustment; performance degrades on unseen questions → Need more diverse training data or different IR strategy

- **First 3 experiments**: 1) Baseline: Pretrained IR + basic prompt vs fine-tuned IR + optimized prompt; 2) Ablation: Test impact of removing RAG (just use question + reference + new answer); 3) Scale test: Vary K (number of retrieved examples) to find optimal balance between accuracy and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
How can the effectiveness of RAG be quantified when using different percentages of test data for the vector database? The paper mentions an experiment where different fractions of the test set were used to build the vector database for RAG, but does not provide a comprehensive analysis of how these different percentages affect performance across all scenarios.

### Open Question 2
What are the specific limitations of using RAG for automated essay scoring (AES) compared to short answer scoring? The discussion section mentions that RAG might not be suitable for AES due to the qualitative nature of holistic rubrics in essay evaluation, but does not provide a detailed comparison or analysis.

### Open Question 3
How can the performance of DSPy-style prompts be improved to match or exceed that of Claude Prompt Generator-style prompts? The ablation study shows that Claude Prompt Generator-style prompts outperform DSPy-style prompts in all scenarios, but does not explore ways to enhance the DSPy-style prompts or combine their strengths.

## Limitations

- The study lacks ablation studies to isolate the contribution of each component (RAG, fine-tuning, prompt optimization)
- The effectiveness of question-specific IR training versus global training is asserted but not empirically validated through direct comparison
- The reliance on Claude Prompt Generator introduces a black-box component whose specific optimizations are not fully transparent or reproducible

## Confidence

- **High Confidence**: The baseline performance improvements over existing methods are well-supported by the experimental results on the SemEval-2013 dataset
- **Medium Confidence**: The mechanism by which fine-tuned IR improves performance is plausible but lacks direct experimental validation
- **Low Confidence**: The specific contribution of the Claude Prompt Generator to performance gains is difficult to assess without access to the tool

## Next Checks

1. **Ablation Study**: Run controlled experiments comparing the full RAG system against direct GLM scoring without retrieved examples, retrieval without fine-tuning, and fine-tuned retrieval without RAG to isolate the contribution of each component.

2. **Generalization Test**: Evaluate the system on a different dataset (e.g., ASAP or other ASAS benchmarks) to verify that the performance gains are not specific to the SemEval-2013 dataset.

3. **Prompt Engineering Analysis**: Conduct a systematic study of prompt structure variations while holding other components constant, comparing manually engineered prompts against Claude Prompt Generator outputs to quantify the specific contribution of the automated prompt optimization tool.