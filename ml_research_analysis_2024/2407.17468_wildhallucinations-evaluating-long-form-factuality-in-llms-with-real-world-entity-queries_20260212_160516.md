---
ver: rpa2
title: 'WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World
  Entity Queries'
arxiv_id: '2407.17468'
source_url: https://arxiv.org/abs/2407.17468
tags:
- entities
- wild
- llms
- factscore
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WildHallucinations evaluates LLM factuality on real-world entity
  queries from user-chatbot interactions. It extracts 7,917 entities from WildChat,
  builds knowledge sources via web search, and automatically fact-checks LLM generations
  using the FACTSCORE pipeline.
---

# WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries

## Quick Facts
- arXiv ID: 2407.17468
- Source URL: https://arxiv.org/abs/2407.17468
- Reference count: 40
- 52% of real-world entities lack Wikipedia pages, reflecting diverse user knowledge needs

## Executive Summary
WildHallucinations evaluates the factuality of large language models on real-world entity queries extracted from user-chatbot interactions. The benchmark extracts 7,917 entities from the WildChat dataset and builds knowledge sources via web search for fact-checking. Using the FACTSCORE pipeline, it automatically evaluates 15 models on long-form entity descriptions, finding that 52% of entities lack Wikipedia pages, reflecting diverse user knowledge needs. Across models, factuality varies by domain and entity frequency, with geographic and computing domains easiest and people/finance hardest. Retrieval-augmented models show only slight reductions in hallucinations, indicating that web access alone doesn't eliminate factual errors.

## Method Summary
The evaluation pipeline extracts entities from WildChat using GPT-3.5 for initial proper noun identification, refined with GPT-4o to remove ambiguous entities. For each entity, it builds a knowledge source by collecting top 10 web pages via Google Custom Search API, cleaning HTML/CSS tags and removing invalid URLs. The 15 evaluated models are prompted to generate descriptive texts about each entity. The FACTSCORE pipeline then decomposes these generations into atomic facts and verifies them against the knowledge source using entailment, producing WILD FACTSCORE (percentage of supported atomic facts) and WILD FACTSCORE-STRICT (1 if all facts correct, 0 otherwise) metrics.

## Key Results
- 52% of entities lack Wikipedia pages, requiring broader web search for knowledge sources
- GPT-4o and GPT-3.5 achieve highest factuality scores across all models
- Geographic and computing entities show highest factuality, while people and finance entities show lowest
- Retrieval-augmented models reduce hallucinations slightly but don't eliminate them

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Entity-based evaluation reduces ambiguity in factuality assessment compared to open-ended question answering.
- **Mechanism:** The pipeline isolates a single entity, constructs a focused knowledge source, and uses FACTSCORE to decompose and verify atomic facts. This confines the task to discrete, verifiable claims.
- **Core assumption:** Entity-focused prompts yield clear, fact-checkable statements that map directly to web evidence.
- **Evidence anchors:** [abstract] "We prompt them to generate descriptive texts about each entity. We then identify hallucinations in these generated descriptions using the FACTSCORE [20], an automatic fact-checking method for free-text generations." [section] "We posit that mastering the entities included in user queries is a prerequisite for delivering factual responses."
- **Break condition:** If an entity has multiple meanings or lacks sufficient web coverage, the evaluation loses precision or coverage.

### Mechanism 2
- **Claim:** Real-world user queries expand domain diversity beyond Wikipedia-centric benchmarks.
- **Mechanism:** Entities are mined from WildChat, which captures user interests in domains like computing, finance, and culture. 52% lack Wikipedia pages, so the knowledge source relies on broader web search.
- **Core assumption:** Real user queries reflect diverse, non-Wikipedia knowledge needs, and the web contains adequate coverage for these entities.
- **Evidence anchors:** [abstract] "Notably, half of these real-world entities do not have associated Wikipedia pages." [section] "We use the Google Custom Search JSON API... scrape these web pages, exclude invalid and paywalled URLs..."
- **Break condition:** If the web search fails to retrieve relevant pages for an entity, that entity is dropped, reducing coverage.

### Mechanism 3
- **Claim:** Retrieval-augmented generation reduces hallucinations but does not eliminate them.
- **Mechanism:** RAG models have web access; disabling retrieval increases hallucinations, but even with retrieval some hallucinations persist, indicating generation errors or contradictory evidence.
- **Core assumption:** Web retrieval improves factual grounding, but model generation quality still determines factuality.
- **Evidence anchors:** [abstract] "Finally, given the same base models, adding a retrieval component only slightly reduces hallucinations but does not eliminate hallucinations." [section] "We find that, without access to the web, all the models except Sonar-Large exhibit more frequent hallucinations."
- **Break condition:** If the model's generation step misinterprets or misuses retrieved evidence, hallucinations remain despite retrieval.

## Foundational Learning

- **Concept:** Automatic fact-checking via atomic claim decomposition and entailment verification.
  - **Why needed here:** Manual fact-checking is infeasible at scale; FACTSCORE provides reproducible, automated evaluation.
  - **Quick check question:** What are the three steps of the FACTSCORE pipeline?
- **Concept:** Perplexity as a proxy for entity rarity.
  - **Why needed here:** Entity frequency affects model performance; rare entities may be harder to recall accurately.
  - **Quick check question:** How is perplexity computed for an entity without context in this work?
- **Concept:** Abstention detection in LLM outputs.
  - **Why needed here:** Models may refuse to answer; distinguishing abstention from hallucination is critical for correct metric calculation.
  - **Quick check question:** What patterns indicate model abstention in this evaluation pipeline?

## Architecture Onboarding

- **Component map:** Entity extraction -> Knowledge source construction (web scraping) -> Prompt generation -> LLM inference -> FACTSCORE decomposition -> Entailment verification -> Metric aggregation
- **Critical path:** Web scraping -> FACTSCORE evaluation; delays here block downstream metrics
- **Design tradeoffs:** Using broader web search increases coverage but introduces noise and contradictions; limiting to Wikipedia would improve precision but reduce domain diversity
- **Failure signatures:** Low WILD FACTSCORE-STRICT indicates retrieval errors, generation errors, or entailment misclassifications; high abstention rates suggest model uncertainty on certain entities
- **First 3 experiments:**
  1. Compare FACTSCORE output for a known factual vs. hallucinated generation on the same entity
  2. Measure abstention rate difference when retrieval is enabled vs. disabled for Command R
  3. Correlate entity perplexity with WILD FACTSCORE-STRICT to confirm rarity impact

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do retrieval-augmented models compare to standard models in terms of hallucination rates when evaluated on a multilingual dataset?
- **Basis in paper:** [inferred] The paper focuses on English entities and suggests future work should include multilingual evaluation.
- **Why unresolved:** The paper does not provide data or analysis on multilingual entities or models.
- **What evidence would resolve it:** Conduct experiments evaluating retrieval-augmented and standard models on a diverse set of multilingual entities, comparing hallucination rates.

### Open Question 2
- **Question:** What impact does the inclusion of non-Wikipedia sources have on the factuality scores of LLMs across different domains?
- **Basis in paper:** [explicit] The paper notes that 52% of entities lack Wikipedia pages and that LLMs perform worse on non-Wikipedia knowledge.
- **Why unresolved:** The paper does not explore the specific impact of non-Wikipedia sources on factuality scores across various domains.
- **What evidence would resolve it:** Analyze factuality scores for entities with and without Wikipedia pages across different domains to determine the impact of source diversity.

### Open Question 3
- **Question:** How does the frequency of entities affect the hallucination rates of LLMs, and does this relationship vary across different model sizes?
- **Basis in paper:** [explicit] The paper discusses entity frequency and its impact on factuality, noting that rare entities lead to decreased performance.
- **Why unresolved:** The paper does not explore how entity frequency impacts hallucination rates differently across various model sizes.
- **What evidence would resolve it:** Evaluate hallucination rates for entities of varying frequencies across different model sizes to identify patterns and differences.

## Limitations

- The FACTSCORE pipeline's accuracy depends on implementation details not fully specified in the paper
- Web search coverage for entities without Wikipedia pages may be incomplete or contradictory
- The evaluation cannot distinguish between generation errors and retrieval failures when hallucinations occur

## Confidence

- **Confidence level:** Medium
- The approach addresses a real gap in long-form factuality evaluation using real-world entities
- Results depend on the FACTSCORE pipeline's accuracy, which we cannot fully validate without access to its implementation
- Web search quality for entities without Wikipedia pages may introduce noise or contradictions affecting measurements

## Next Checks

1. Manually verify a sample of atomic facts identified as unsupported by FACTSCORE to assess pipeline accuracy
2. Compare factuality scores across different search engine configurations to measure robustness to web search variability
3. Analyze the correlation between entity perplexity and factuality scores to confirm that rare entities are indeed harder to generate factually