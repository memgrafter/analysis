---
ver: rpa2
title: 'AKEW: Assessing Knowledge Editing in the Wild'
arxiv_id: '2402.18909'
source_url: https://arxiv.org/abs/2402.18909
tags:
- editing
- facts
- knowledge
- unstructured
- triplets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AKEW (Assessing Knowledge Editing in the
  Wild), a new practical benchmark for knowledge editing in language models. Unlike
  current evaluation strategies that rely on structured facts, AKEW evaluates knowledge
  editing using unstructured texts (e.g., news articles, Wikipedia pages) as knowledge
  updates, better reflecting real-world scenarios.
---

# AKEW: Assessing Knowledge Editing in the Wild

## Quick Facts
- arXiv ID: 2402.18909
- Source URL: https://arxiv.org/abs/2402.18909
- Authors: Xiaobao Wu; Liangming Pan; William Yang Wang; Anh Tuan Luu
- Reference count: 15
- Primary result: A new benchmark evaluating knowledge editing using unstructured texts (news articles, Wikipedia pages) rather than structured facts, revealing significant performance declines for state-of-the-art methods

## Executive Summary
This paper introduces AKEW (Assessing Knowledge Editing in the Wild), a practical benchmark for knowledge editing in language models that evaluates performance using unstructured texts as knowledge updates. Unlike current evaluation strategies relying on structured facts, AKEW uses Wikipedia summaries and LLM-generated content to create datasets with counterfactual and real-world updates. Experiments with state-of-the-art methods (ROME, MEMIT, IKE, MeLLo) show significant performance declines when editing with unstructured facts compared to structured facts, even when using extracted triplets from unstructured texts. These findings highlight the challenges of practical knowledge editing and emphasize the need for improved methods to handle unstructured facts.

## Method Summary
AKEW evaluates knowledge editing using unstructured texts as knowledge updates instead of structured facts. The benchmark includes three datasets: COUNTER FACT (counterfactual updates), MQUAKE-CF (multi-hop counterfactual updates), and WIKI UPDATE (real-world updates). These datasets are constructed using Wikipedia summaries and LLM-generated content. Knowledge editing methods (FT, LoRA, ROME, MEMIT, IKE, MeLLo) are evaluated on these datasets using editing accuracy and multi-hop accuracy metrics. The study compares performance across structured facts, unstructured facts, and extracted triplets to assess the challenges of practical knowledge editing.

## Key Results
- Knowledge editing methods show significant performance declines when using unstructured facts compared to structured facts
- Even extracted triplets from unstructured texts provide limited improvement in editing performance
- In-context learning methods (IKE, MeLLo) perform relatively better on unstructured facts by reasoning with background knowledge
- Real-world updates prove more challenging than counterfactual updates for knowledge editing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unstructured Knowledge Editing (UKE) better reflects real-world knowledge editing by directly editing with unstructured texts instead of structured facts.
- Mechanism: UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, fulfilling realistic requirements.
- Core assumption: Knowledge updates commonly emerge in unstructured texts like news articles and Wikipedia pages in real-world scenarios.
- Evidence anchors:
  - [abstract] "However, its current evaluations deviate significantly from practice: their knowledge updates solely consist of structured facts derived from meticulously crafted datasets, instead of practical sources -- unstructured texts like news articles..."
  - [section] "Different from existing studies, we propose a more practical and challenging benchmark, Unstructured Knowledge Editing (UKE). It is motivated by the reality that knowledge updates are generally conveyed through unstructured texts, such as news articles and Wikipedia pages, instead of well-curated structured facts."
  - [corpus] Weak evidence - corpus only shows related papers but no direct evidence about real-world knowledge update practices.
- Break condition: If knowledge updates in practice are predominantly in structured formats or if the cost of processing unstructured texts outweighs the benefits of responsiveness.

### Mechanism 2
- Claim: UKE poses more challenges to knowledge editing methods due to the implicit and complex nature of unstructured facts.
- Mechanism: Unstructured facts are more implicit and complex than structured facts, requiring higher capabilities of knowledge editing methods.
- Core assumption: Unstructured facts contain multiple subjects, objects, and relations, some useful and others noisy, making them more challenging to process.
- Evidence anchors:
  - [abstract] "Unlike previous explicit structured facts, knowledge updates are generally buried within unstructured facts."
  - [section] "The UKE benchmark poses more challenges to knowledge editing for the following reasons: (i) Unstructured facts are more implicit. Unlike previous explicit structured facts, knowledge updates are generally buried within unstructured facts. (ii) Unstructured facts are more complex. A structured fact contains one subject, one object, and their relation. In contrast, an unstructured fact potentially incorporates multiple subject, objects, and their relations, where some are useful and others are noisy."
  - [corpus] Weak evidence - corpus does not provide specific evidence about the complexity of unstructured facts compared to structured facts.
- Break condition: If knowledge editing methods can be easily adapted to handle implicit and complex information in unstructured texts.

### Mechanism 3
- Claim: In-context learning methods perform relatively better on UKE by retrieving related facts stored in memory for editing.
- Mechanism: In-context learning methods convert knowledge editing into an open-book question-answering task with knowledge updates as an auxiliary corpus.
- Core assumption: In-context learning methods can effectively retrieve relevant facts from memory and reason based on them to edit.
- Evidence anchors:
  - [abstract] "Tables 3 and 4 show that the performance declines on unstructured facts of IKE and MeLLo are relatively slighter compared to others. For example, IKE drops by 17% while the runner-up by 79% on COUNTER FACT. The reason lies in that instead of directly injecting facts into language models, these in-context learning methods alternatively adjust their answers by reasoning with unstructured facts as background knowledge."
  - [section] "These results underscore the challenge of our UKE benchmark for knowledge editing. (2) In-context learning methods perform relatively better on UKE. Tables 3 and 4 show that the performance declines on unstructured facts of IKE and MeLLo are relatively slighter compared to others."
  - [corpus] Weak evidence - corpus does not provide specific evidence about the performance of in-context learning methods on unstructured facts.
- Break condition: If the retrieval success rates of in-context learning methods become critical or if storing facts in memory requires regular maintenance and faces challenges in handling growing volumes of new facts.

## Foundational Learning

- Concept: Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date without massive retraining.
  - Why needed here: Understanding the goal of knowledge editing is essential to appreciate the significance of UKE as a more practical benchmark.
  - Quick check question: What is the primary objective of knowledge editing in language models?

- Concept: Structured facts are triplets with subjects, relations, and objects, while unstructured facts are texts like news articles and Wikipedia pages.
  - Why needed here: Distinguishing between structured and unstructured facts is crucial to understand the difference between current evaluation strategies and UKE.
  - Quick check question: How do structured facts differ from unstructured facts in the context of knowledge editing?

- Concept: In-context learning methods preserve model parameters and retrieve new facts in memory for in-context learning.
  - Why needed here: Understanding in-context learning methods is important to grasp why they perform relatively better on UKE compared to other methods.
  - Quick check question: What is the key characteristic of in-context learning methods in knowledge editing?

## Architecture Onboarding

- Component map: Datasets (COUNTER FACT, MQUAKE-CF, WIKI UPDATE) -> Knowledge editing methods (FT, LoRA, ROME, MEMIT, IKE, MeLLo) -> Evaluation metrics (editing accuracy, multi-hop accuracy)
- Critical path: 1) Construct datasets with unstructured facts, 2) Evaluate knowledge editing methods on these datasets, 3) Analyze the performance and challenges of UKE
- Design tradeoffs: UKE offers a more practical evaluation of knowledge editing but poses more challenges due to the implicit and complex nature of unstructured facts. In-context learning methods perform relatively better but require regular maintenance of the fact memory.
- Failure signatures: Performance declines on unstructured facts compared to structured facts, difficulties in handling real-world updates, and limited effects of extracted triplets
- First 3 experiments:
  1. Evaluate a knowledge editing method on the UKE benchmark with unstructured facts as edits and compare the performance to structured facts
  2. Analyze the error types and proportions when using extracted triplets as edits for a knowledge editing method on UKE
  3. Investigate the retrieval accuracy of an in-context learning method on unstructured facts and identify the factors contributing to performance declines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do knowledge editing methods perform on unstructured facts when the knowledge updates are more complex (e.g., longer articles, multiple updates in a single text)?
- Basis in paper: [inferred] The paper discusses that unstructured facts are more complex and implicit compared to structured facts, and that this complexity poses challenges for knowledge editing methods.
- Why unresolved: The experiments in the paper focus on relatively simple unstructured facts (e.g., Wikipedia summaries, generated paragraphs). It remains unclear how well current methods can handle more complex real-world scenarios with longer articles or multiple updates.
- What evidence would resolve it: Conducting experiments with more complex unstructured facts, such as longer news articles or scientific papers, would provide insights into the scalability and limitations of current knowledge editing methods.

### Open Question 2
- Question: Can knowledge editing methods be improved by incorporating retrieval techniques that are more robust to noisy and inaccurate extracted triplets?
- Basis in paper: [explicit] The paper mentions that extracted triplets from unstructured facts can be inaccurate and noisy, and that this negatively impacts the performance of in-context learning methods like IKE and MeLLo.
- Why unresolved: While the paper highlights the problem of noisy triplets, it does not explore potential solutions or modifications to retrieval techniques that could mitigate this issue.
- What evidence would resolve it: Investigating and comparing different retrieval techniques, such as those that incorporate semantic similarity or entity linking, could help determine if more robust retrieval methods can improve knowledge editing performance with extracted triplets.

### Open Question 3
- Question: How does the performance of knowledge editing methods vary across different types of unstructured facts (e.g., news articles, scientific papers, social media posts)?
- Basis in paper: [inferred] The paper mentions that unstructured facts can come from various sources like news articles and Wikipedia pages. However, the experiments primarily focus on Wikipedia-style paragraphs.
- Why unresolved: The paper does not provide a comprehensive analysis of how knowledge editing methods perform across different types of unstructured facts, which could have varying levels of complexity, noise, and implicit information.
- What evidence would resolve it: Conducting experiments with a diverse set of unstructured fact sources, including news articles, scientific papers, and social media posts, would reveal the strengths and weaknesses of different knowledge editing methods in handling various real-world scenarios.

## Limitations

- The study only tests two language model sizes (6B and 7B parameters), limiting generalizability across different model scales
- The quality and accuracy of extracted triplets from unstructured texts is not thoroughly analyzed, which may contribute to performance declines
- The comparison between UKE and structured fact editing is confounded by the additional preprocessing steps required for unstructured facts

## Confidence

High confidence: The observation that knowledge editing performance decreases when using unstructured facts compared to structured facts is well-supported by the experimental results.

Medium confidence: The claim that UKE better reflects real-world knowledge editing scenarios, while intuitively appealing, lacks direct empirical validation.

Low confidence: The specific performance degradation percentages (e.g., "IKE drops by 17% while the runner-up by 79%") are difficult to verify without access to the exact dataset splits and implementation details.

## Next Checks

1. **Scale validation**: Test UKE across a broader range of language model sizes (1B, 3B, 13B, 30B parameters) to determine if the observed performance patterns hold across different model scales, which would strengthen generalizability claims.

2. **Extraction quality audit**: Conduct a detailed error analysis of the triplet extraction pipeline by having human annotators rate the quality of extracted triplets from unstructured texts and correlate extraction quality with editing performance to isolate whether performance issues stem from extraction or editing methods.

3. **Real-world deployment study**: Survey 20+ organizations that maintain knowledge bases in production to document their actual knowledge update formats, update frequencies, and editing workflows, then compare these patterns against the UKE assumptions to validate or refine the benchmark's practical relevance.