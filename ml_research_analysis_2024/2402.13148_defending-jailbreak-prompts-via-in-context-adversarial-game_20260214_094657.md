---
ver: rpa2
title: Defending Jailbreak Prompts via In-Context Adversarial Game
arxiv_id: '2402.13148'
source_url: https://arxiv.org/abs/2402.13148
tags:
- i255
- defense
- jailbreak
- prompts
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the In-Context Adversarial Game (ICAG), a
  novel approach to defending large language models (LLMs) against jailbreak attacks
  without fine-tuning. ICAG uses an iterative adversarial game between attack and
  defense agents, dynamically extending defenses by refining prompts and extracting
  insights.
---

# Defending Jailbreak Prompts via In-Context Adversarial Game

## Quick Facts
- arXiv ID: 2402.13148
- Source URL: https://arxiv.org/abs/2402.13148
- Authors: Yujun Zhou; Yufei Han; Haomin Zhuang; Kehan Guo; Zhenwen Liang; Hongyan Bao; Xiangliang Zhang
- Reference count: 40
- Key outcome: ICAG reduces jailbreak success rates by an average of 7.99% compared to best baseline methods

## Executive Summary
This paper introduces the In-Context Adversarial Game (ICAG), a novel approach to defending large language models (LLMs) against jailbreak attacks without fine-tuning. ICAG uses an iterative adversarial game between attack and defense agents, dynamically extending defenses by refining prompts and extracting insights. The attack agent refines jailbreak prompts using successful attack patterns, while the defense agent generates safety-enhancing system prompts through reflection and insight extraction. Experiments show ICAG reduces jailbreak success rates by an average of 7.99% compared to the best baseline methods across multiple attack scenarios and defense LLMs.

## Method Summary
ICAG implements an iterative adversarial game where an attack agent attempts to jailbreak the target LLM while a defense agent generates safety-enhancing system prompts. Both agents use in-context learning without parameter updates. The attack agent refines jailbreak prompts using AutoDAN and insight extraction from successful attacks. The defense agent employs reflection and insight extraction to generate system prompts that enhance safety. An evaluator LLM assesses outputs for harm or over-defensiveness. The process iterates for 10 rounds, with the defense agent's final system prompt serving as the defense mechanism. ICAG is evaluated across multiple LLMs including GPT-3.5-Turbo, Llama-3-8B-Instruct, Vicuna-1.5-7B, and Mistral-7B-Instruct.

## Key Results
- ICAG reduces jailbreak success rates by an average of 7.99% compared to best baseline methods
- Strong transferability with only 2.86% average increase in jailbreak success rate when applied to different LLMs
- Maintains MMLU accuracy while significantly reducing over-defense rates compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICAG reduces jailbreak success rates by dynamically adapting both attack and defense strategies through iterative adversarial learning.
- Mechanism: The attack agent refines failed jailbreak prompts using insights extracted from successful prompts, while the defense agent generates safety-enhancing system prompts through reflection and insight extraction. This creates a feedback loop where both agents improve iteratively, leading to convergence on more robust defenses.
- Core assumption: The iterative adversarial game between attack and defense agents can converge to a state where defenses are strong enough to significantly reduce jailbreak success rates.
- Evidence anchors:
  - [abstract] "Experiments show ICAG reduces jailbreak success rates by an average of 7.99% compared to the best baseline methods across multiple attack scenarios and defense LLMs."
  - [section] "The JSR curves of the other two models present a close tendency. We skip them due to the space limit. The results show a significant decline and convergence in JSR after implementing ICAG defenses."
  - [corpus] Weak evidence - no direct mention of adversarial game convergence in related papers, but multiple papers discuss iterative defense improvement.
- Break condition: If the adversarial game fails to converge within reasonable iterations, or if one agent dominates completely preventing meaningful feedback for the other.

### Mechanism 2
- Claim: ICAG achieves transferability across different LLMs without requiring model-specific fine-tuning.
- Mechanism: The system prompts generated through the adversarial game contain generalizable safety instructions that can be applied to different LLMs, reducing the need for model-specific defenses.
- Core assumption: Safety instructions generated through adversarial game learning capture fundamental defensive principles that apply across different LLM architectures.
- Evidence anchors:
  - [abstract] "Additionally, ICAG demonstrates strong transferability, with only a 2.86% average increase in jailbreak success rate when applied to different LLMs."
  - [section] "We train ICAG on a specific defense LLM, then apply the derived system prompts to other models, assessing their efficacy across all mentioned attacks."
  - [corpus] Weak evidence - only one related paper mentions transferability but focuses on different mechanisms.
- Break condition: If different LLMs have fundamentally different safety alignment mechanisms that make general instructions ineffective.

### Mechanism 3
- Claim: In-context learning through adversarial games can achieve defense improvements without computationally expensive fine-tuning.
- Mechanism: By using LLMs as agents in an adversarial game, ICAG generates defensive knowledge through interaction rather than parameter updates, making it applicable to closed-source models.
- Core assumption: LLMs can effectively reason about and generate defensive strategies through prompt-based interactions without parameter modification.
- Evidence anchors:
  - [abstract] "ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents."
  - [section] "To address these limitations, we leverage adversarial games to dynamically extend knowledge for defending against jailbreak attacks using in-context learning, without cumbersome retraining."
  - [corpus] Weak evidence - related papers focus on fine-tuning approaches rather than in-context adversarial learning.
- Break condition: If LLMs lack sufficient reasoning capability to generate effective defensive strategies through prompt-based interactions alone.

## Foundational Learning

- Concept: Adversarial training in deep learning
  - Why needed here: Provides the theoretical foundation for using adversarial games to improve model robustness against attacks
  - Quick check question: How does adversarial training in traditional deep learning differ from the in-context adversarial approach used in ICAG?

- Concept: In-context learning and prompt engineering
  - Why needed here: Essential for understanding how ICAG generates and refines prompts without fine-tuning model parameters
  - Quick check question: What are the key differences between in-context learning and fine-tuning when it comes to adapting model behavior?

- Concept: Agent learning and multi-agent systems
  - Why needed here: Central to understanding how ICAG uses two LLM agents in an adversarial game to improve defenses
  - Quick check question: How does the interaction between attack and defense agents in ICAG differ from single-agent reinforcement learning approaches?

## Architecture Onboarding

- Component map:
  Attack Agent -> Attack Prompt Refinement -> Defense Agent -> System Prompt Generation -> Evaluator LLM -> Over-defensiveness Check

- Critical path: Initial prompts → Attack agent refinement → Defense agent reflection → System prompt generation → Evaluation → Iteration

- Design tradeoffs:
  - Computational cost vs. defense effectiveness (more iterations improve defense but increase computation)
  - Generalization vs. specificity (broader system prompts may be less effective than model-specific fine-tuning)
  - Speed vs. thoroughness (faster iterations may miss important defensive insights)

- Failure signatures:
  - JSR not decreasing over iterations (adversarial game not converging)
  - Over-defensiveness increasing significantly (defense becoming too restrictive)
  - Attack agent consistently dominating (defense not learning effectively)

- First 3 experiments:
  1. Run ICAG for 1 iteration on a simple model and verify JSR reduction compared to baseline
  2. Test transferability by applying system prompts from one model to another and measuring performance drop
  3. Compare ICAG performance against fine-tuning approaches on the same models and attack types

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The approach relies on closed-source LLMs for attack and defense agents, creating reproducibility challenges
- Evaluation focuses primarily on quantitative metrics without deep qualitative analysis of why certain system prompts are more effective
- Performance depends heavily on the reasoning capabilities of the underlying LLM, with no exploration of how it varies across different model architectures

## Confidence
- High Confidence: The core claim that ICAG reduces jailbreak success rates by 7.99% on average is well-supported by the experimental results
- Medium Confidence: The transferability claim (2.86% average increase in JSR) is demonstrated but limited to a small set of model pairs
- Low Confidence: The assertion that in-context adversarial learning can match or approach fine-tuning effectiveness lacks comparative analysis with fine-tuned baselines

## Next Checks
1. Track the JSR reduction across each iteration of ICAG to verify that the adversarial game converges rather than oscillating or plateauing prematurely
2. Apply ICAG-derived system prompts to a broader range of LLM architectures to validate transferability beyond the tested models
3. Conduct human evaluation of system prompts generated through ICAG to identify common defensive patterns and assess whether they represent genuinely novel safety strategies