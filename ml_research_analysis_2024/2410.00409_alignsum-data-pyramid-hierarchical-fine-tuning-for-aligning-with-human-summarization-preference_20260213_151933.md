---
ver: rpa2
title: 'AlignSum: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization
  Preference'
arxiv_id: '2410.00409'
source_url: https://arxiv.org/abs/2410.00409
tags:
- data
- human
- summarization
- fine-tuning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the issue of Pre-trained Language Models (PLMs)
  underperforming in human evaluations despite strong automatic scores, due to low-quality
  fine-tuning data and limited high-quality human-annotated datasets. The proposed
  solution, AlignSum, introduces a Data Pyramid with extractive, abstractive, and
  human-annotated summary data, followed by Gaussian Resampling to normalize summary
  lengths and a two-stage hierarchical fine-tuning process.
---

# AlignSum: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference

## Quick Facts
- arXiv ID: 2410.00409
- Source URL: https://arxiv.org/abs/2410.00409
- Authors: Yang Han; Yiming Wang; Rui Wang; Lu Chen; Kai Yu
- Reference count: 15
- Primary result: AlignSum enables BART-Large to surpass 175B GPT-3 in both automatic (ROUGE, BERTScore) and human evaluations for summarization alignment

## Executive Summary
AlignSum addresses the gap between strong automatic evaluation scores and poor human evaluation performance in pre-trained language models for summarization. The framework introduces a three-tier Data Pyramid (extractive, abstractive, and human-annotated data), Gaussian resampling for length distribution normalization, and a two-stage hierarchical fine-tuning process. Experiments on CNN/DailyMail and BBC XSum datasets demonstrate that AlignSum significantly improves alignment with human summarization preferences, enabling smaller models to outperform much larger ones in both automatic and human evaluations.

## Method Summary
AlignSum proposes a novel framework for aligning PLMs with human summarization preferences through a Data Pyramid construction and two-stage hierarchical fine-tuning. The approach first builds a three-tier data pyramid with extractive, abstractive, and human-annotated summary data, then applies Gaussian resampling to normalize summary lengths across all data sources. The two-stage fine-tuning process begins with generic data (extractive + abstractive) to establish foundational summarization capabilities, followed by preference data (human-annotated) to align with human writing preferences. This hierarchical approach prevents the dilution of preference signals by large amounts of generic data.

## Key Results
- BART-Large with AlignSum surpasses 175B GPT-3 in both automatic (ROUGE, BERTScore) and human evaluations
- Two-stage hierarchical fine-tuning with Data Pyramid construction improves summarization alignment with human preferences
- Gaussian resampling effectively normalizes summary length distributions across different data sources
- DP enhances performance by introducing greater diversity, as ED effectively identifies key sentences, while AD struggles to do so

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical fine-tuning reduces information loss from high-entropy preference data
- Mechanism: Two-stage training where generic data is learned first, then preference data is applied, preventing the dilution of preference signals by large generic datasets
- Core assumption: High-quality preference data has higher entropy than generic data, and mixed fine-tuning causes preference signals to be overwhelmed
- Evidence anchors:
  - [abstract]: "training directly with these disparate distributions can result in overly long or short summaries"
  - [section 2.3]: "high-entropy data, which is crucial for alignment, can be interfered with by information from a large amount of low-entropy data"
  - [corpus]: Weak - only 0 citations, but the theoretical framework is internally consistent

### Mechanism 2
- Claim: Gaussian resampling aligns summary length distributions across data sources
- Mechanism: Length distributions from extractive, abstractive, and human-annotated data are normalized to follow a Gaussian distribution matching the human-annotated target
- Core assumption: Length mismatch between data sources causes inconsistent summary generation, and Gaussian resampling effectively removes outliers
- Evidence anchors:
  - [section 2.2]: "we utilize Gaussian Resampling to adjust the generated summary lengths to approximate the target length"
  - [section 2.2]: "With a 95% probability, the confidence interval for the token length distribution is [µ − 2σ, µ + 2σ]"
  - [corpus]: Weak - only 0 citations, but the statistical reasoning is sound

### Mechanism 3
- Claim: Data pyramid construction provides diverse, multi-level supervision signals
- Mechanism: Bottom-to-top data construction with extractive (easy, large), abstractive (medium, small), and human-annotated (hard, little) data creates a comprehensive training signal
- Core assumption: Different data types capture different aspects of summarization quality, and combining them provides better supervision than any single source
- Evidence anchors:
  - [section 2.1]: "we design a bottom-to-up data construction method Data Pyramid (DP), which consists of three components: extractive data, abstractive data, and human-annotated data"
  - [section 4.4]: "DP enhances performance by introducing greater diversity, as ED effectively identifies key sentences, while AD struggles to do so"
  - [corpus]: Weak - only 0 citations, but the multi-source approach is well-established in other domains

## Foundational Learning

- Concept: Length distribution normalization
  - Why needed here: Different data sources have vastly different summary length distributions, causing model instability
  - Quick check question: If extractive summaries average 55 tokens and human-annotated average 64 tokens with σ=17, what length range would Gaussian resampling keep?

- Concept: Hierarchical fine-tuning strategy
  - Why needed here: Small amounts of high-quality preference data can be overwhelmed by large amounts of generic data in mixed training
  - Quick check question: What's the key difference between training on [X, Y, Z] sequentially versus mixing all three datasets together?

- Concept: Entropy-based data prioritization
  - Why needed here: Human-annotated data has higher entropy (more information per sample) than automatically generated data
  - Quick check question: If human-annotated data has 100 samples with high entropy and extractive data has 200k samples with low entropy, which should be trained on first for optimal preference alignment?

## Architecture Onboarding

- Component map:
  - Data pyramid construction (3 tiers: extractive, abstractive, human-annotated)
  - Gaussian resampling module (length distribution normalization)
  - Two-stage hierarchical fine-tuning pipeline (generic → preference)
  - Length filtering pre-processing

- Critical path:
  1. Build data pyramid with extractive and abstractive data generation
  2. Apply Gaussian resampling to all data
  3. Stage 1: Fine-tune on extractive + abstractive data
  4. Stage 2: Fine-tune on human-annotated data
  5. Evaluate on test set

- Design tradeoffs:
  - Gaussian resampling vs. length-based filtering: Resampling preserves more data while filtering can be more aggressive
  - Two-stage vs. mixed fine-tuning: Two-stage preserves preference signals but requires more training time
  - Data pyramid completeness vs. simplicity: More tiers provide better coverage but increase complexity

- Failure signatures:
  - Poor length control: Model generates summaries that are consistently too long or too short
  - Preference signal loss: Performance on human evaluation degrades despite good automatic metrics
  - Overfitting to generic data: Model performs well on automatic metrics but poorly on human evaluation

- First 3 experiments:
  1. Implement single-stage fine-tuning with all data mixed together to establish baseline performance
  2. Add Gaussian resampling and compare length distribution statistics
  3. Implement two-stage fine-tuning and measure improvement in human evaluation metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AlignSum change when applied to non-news datasets or other languages?
- Basis in paper: [inferred] The authors acknowledge limitations regarding dataset diversity, noting that experiments were limited to CNN/DailyMail and BBC XSum due to the scarcity of high-quality preference data. They express willingness to extend their method to more datasets if available.
- Why unresolved: The study's scope is constrained by the availability of datasets with rewritten versions reflecting human preferences. The generalizability of AlignSum to other domains or languages remains untested.
- What evidence would resolve it: Experiments applying AlignSum to diverse datasets (e.g., legal, medical, social media) and multiple languages, comparing performance metrics (ROUGE, BERTScore) and human evaluations against baselines.

### Open Question 2
- Question: What is the impact of different levels of abstraction in the abstractive data (AD) on the final summarization performance?
- Basis in paper: [inferred] The paper discusses scaling AD but finds that increasing its amount does not improve performance, possibly due to a mismatch between AD generated by large LLMs and the test set. The role of abstraction level in AD is not explicitly explored.
- Why unresolved: The study focuses on the quantity of AD rather than its quality or level of abstraction. The relationship between abstraction level and alignment with human preferences is not investigated.
- What evidence would resolve it: Controlled experiments varying the abstraction level of AD (e.g., highly abstract vs. moderately abstract summaries) while keeping other factors constant, measuring impact on automatic and human evaluation metrics.

### Open Question 3
- Question: How does AlignSum perform compared to other alignment techniques, such as reinforcement learning from human feedback (RLHF) or direct preference optimization (DPO)?
- Basis in paper: [inferred] The authors propose AlignSum as a novel framework for aligning PLMs with human summarization preferences. However, they do not compare its performance against other state-of-the-art alignment techniques.
- Why unresolved: The study focuses on validating AlignSum's effectiveness but does not benchmark it against alternative alignment methods that could achieve similar or better results.
- What evidence would resolve it: Comparative experiments evaluating AlignSum against RLHF, DPO, and other alignment techniques on the same datasets, using identical evaluation metrics and human assessments to determine relative performance.

## Limitations
- Gaussian Resampling effectiveness depends on the assumption that human-annotated data follows a normal distribution, which may not hold for all summarization tasks or domains
- The two-stage hierarchical fine-tuning requires careful hyperparameter tuning for each stage, and the paper doesn't provide comprehensive sensitivity analysis
- The data pyramid construction relies on extractive and abstractive data generation methods that aren't detailed in the paper, leaving implementation ambiguity

## Confidence
- **High Confidence**: The core hypothesis that hierarchical fine-tuning prevents preference signal dilution is well-supported by the theoretical framework and experimental results
- **Medium Confidence**: The Gaussian resampling approach for length normalization is statistically reasonable, but its practical effectiveness depends on the underlying data distribution assumptions
- **Low Confidence**: The specific implementation details for constructing the data pyramid (exact extractive methods, abstractive generation approaches) are underspecified, making exact replication challenging

## Next Checks
1. Test whether human-annotated summary lengths actually follow a Gaussian distribution across different domains and summarization tasks using Kolmogorov-Smirnov or similar tests
2. Systematically vary the transition point between Stage 1 and Stage 2 fine-tuning to determine optimal stage separation and assess robustness to hyperparameter choices
3. Compare Gaussian resampling against simpler length-based filtering approaches on the same datasets to quantify the specific contribution of the Gaussian method versus basic length constraints