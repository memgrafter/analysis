---
ver: rpa2
title: 'aiXcoder-7B: A Lightweight and Effective Large Language Model for Code Processing'
arxiv_id: '2410.13187'
source_url: https://arxiv.org/abs/2410.13187
tags:
- code
- aixcoder-7b
- llms
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces aiXcoder-7B, a lightweight yet highly effective
  LLM for code completion. It addresses the trade-off between model size and inference
  efficiency, achieving state-of-the-art performance on multiple code completion benchmarks
  while maintaining only 7 billion parameters.
---

# aiXcoder-7B: A Lightweight and Effective Large Language Model for Code Processing

## Quick Facts
- arXiv ID: 2410.13187
- Source URL: https://arxiv.org/abs/2410.13187
- Reference count: 40
- Outperforms larger models on code completion benchmarks while maintaining only 7 billion parameters

## Executive Summary
This paper introduces aiXcoder-7B, a lightweight yet highly effective LLM for code completion that addresses the trade-off between model size and inference efficiency. The model employs a multi-objective training approach incorporating Next-Token Prediction, Fill-In-the-Middle, and a novel Structured Fill-In-the-Middle (SFIM) objective that leverages syntax tree structures. Through extensive training on 1.2 trillion tokens with diverse data sampling strategies, aiXcoder-7B achieves state-of-the-art performance on multiple code completion benchmarks while maintaining a compact 7 billion parameter footprint.

## Method Summary
aiXcoder-7B uses a Transformer decoder architecture with 32 layers, 4096 hidden size, and grouped query attention. The training combines three objectives: Next-Token Prediction (NTP), Fill-In-the-Middle (FIM), and Structured Fill-In-the-Middle (SFIM). SFIM incorporates syntax tree parsing to select non-root, non-leaf nodes as spans, ensuring syntactically complete code snippets. The model is trained on 1.2 trillion tokens using diverse data sampling strategies including content similarity, path similarity, inter-file dependency, and random sampling. Training was conducted on 128 A100 40GB GPUs with a batch size of 512 and learning rate of 1e-5.

## Key Results
- Achieves 54.9% Pass@1 on HumanEval, outperforming CodeLlama-34B (48.2%) and StarCoder2-15B (46.3%)
- Surpasses seven LLMs with similar sizes and four larger models across multiple benchmarks
- Demonstrates better code conciseness and consistency with human coding styles
- Gained 2,226 GitHub stars as of January 2025

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-objective training with SFIM improves accuracy by training on complete code snippets rather than incomplete spans.
- Mechanism: SFIM parses code into syntax trees, selects non-leaf non-root nodes, and ensures the span ends at a code line boundary. This enforces models to predict syntactically complete code units, aligning with developer expectations.
- Core assumption: Complete code snippets are more representative of actual developer needs than arbitrary incomplete spans.
- Evidence anchors:
  - [abstract]: "We employ three training objectives, one of which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers the syntax structures in code and effectively improves the performance of LLMs for code."
  - [section III-C]: Describes the selection of non-root, non-leaf nodes from syntax trees and ensuring span ends at code line boundaries.
- Break condition: If the syntax tree parsing fails frequently or the selected spans do not match common coding patterns, the training signal becomes noisy and degrades performance.

### Mechanism 2
- Claim: Diverse data sampling strategies improve cross-file context understanding by simulating realistic multi-file coding scenarios.
- Mechanism: Four sampling strategies (content similarity, path similarity, inter-file dependency, random) are applied with probabilities (30%, 30%, 30%, 10%). This ensures models see related files together, mimicking real-world cross-file code completion tasks.
- Core assumption: Cross-file relationships in code repositories are important for understanding context and dependencies.
- Evidence anchors:
  - [abstract]: "Diverse data sampling strategies... consider inter-file relationships and enhance the capability of LLMs in understanding cross-file contexts."
  - [section III-B]: Algorithm 1 implements the four sampling strategies and describes their purpose.
- Break condition: If the similarity metrics used are inaccurate or if the probability distribution does not reflect real coding scenarios, the model may overfit to irrelevant file pairs.

### Mechanism 3
- Claim: Extensive high-quality training data enables the model to learn a broad distribution of code patterns.
- Mechanism: 1.2 trillion unique tokens are processed through rigorous data collection pipeline (crawling, cleaning, deduplication, quality checking, PII removal). This volume and quality ensure the model sees diverse, representative code.
- Core assumption: Larger and higher-quality datasets lead to better generalization and performance.
- Evidence anchors:
  - [abstract]: "Extensive high-quality data... enables aiXcoder-7B to learn a broad distribution of code."
  - [section II]: Details the pipeline and mentions 1.2 trillion unique tokens.
- Break condition: If the data collection pipeline introduces bias (e.g., over-represents certain languages or styles), the model's generalization may suffer despite the large volume.

## Foundational Learning

- Concept: Syntax trees and code structure
  - Why needed here: SFIM relies on parsing code into syntax trees to select meaningful spans. Understanding this is essential for grasping how SFIM improves code completion.
  - Quick check question: What is the purpose of selecting non-root, non-leaf nodes from a syntax tree in SFIM?

- Concept: Cross-file dependencies and context
  - Why needed here: The data sampling strategies aim to improve cross-file context understanding. Knowing how files relate in a repository is key to understanding this mechanism.
  - Quick check question: How do the four sampling strategies in Algorithm 1 simulate real-world cross-file code completion scenarios?

- Concept: Data deduplication and quality control
  - Why needed here: The paper emphasizes rigorous data cleaning and deduplication. Understanding these processes is important for appreciating the quality of the training data.
  - Quick check question: Why is near deduplication performed after exact deduplication, and what threshold is used to consider files near-duplicate?

## Architecture Onboarding

- Component map: Data collection -> multi-objective training (NTP, FIM, SFIM) -> model training -> evaluation on benchmarks
- Critical path: Data collection → multi-objective training (NTP, FIM, SFIM) → model training → evaluation on benchmarks
- Design tradeoffs: Smaller model (7B) vs larger models (15B, 34B) - trade-off between inference speed and accuracy. The paper shows aiXcoder-7B achieves better accuracy despite smaller size.
- Failure signatures: Poor cross-file context understanding (low performance on CrossCodeEval), over-generation (code longer than references), failure to parse syntax trees (SFIM ineffective).
- First 3 experiments:
  1. Evaluate SFIM vs FIM on a small code completion benchmark to measure impact of syntax tree-based span selection.
  2. Test different probability distributions for the four sampling strategies to find optimal cross-file context understanding.
  3. Compare model performance with and without the extensive data pipeline to quantify the impact of data quality and quantity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal model size for balancing inference efficiency and code completion accuracy?
- Basis in paper: [explicit] The paper demonstrates that aiXcoder-7B achieves state-of-the-art performance on multiple benchmarks while maintaining 7 billion parameters, outperforming larger models like CodeLlama-34B and StarCoder2-15B.
- Why unresolved: The paper only compares aiXcoder-7B against specific model sizes (7B, 13B, 15B, 34B) and does not systematically explore the relationship between model size and performance across a broader range of sizes.
- What evidence would resolve it: Systematic experiments comparing model performance across a wider range of sizes (e.g., 1B, 3B, 10B, 20B, 50B) while controlling for other variables.

### Open Question 2
- Question: How does Structured Fill-In-the-Middle (SFIM) compare to other code structure-aware training objectives?
- Basis in paper: [explicit] The paper introduces SFIM as a novel training objective that considers syntax structures in code and claims it improves performance, but does not compare it against other potential structure-aware approaches.
- Why unresolved: The paper only demonstrates SFIM's effectiveness against standard FIM and NTP, without exploring alternative ways to incorporate code structure into training objectives.
- What evidence would resolve it: Comparative experiments between SFIM and alternative structure-aware training methods (e.g., abstract syntax tree-based objectives, control flow graph objectives).

### Open Question 3
- Question: What is the optimal sampling strategy for cross-file context in code completion?
- Basis in paper: [explicit] The paper proposes four new sampling strategies (file content similarity, file path similarity, inter-file dependency, and random sampling) but does not systematically compare their relative effectiveness or determine optimal weights for each strategy.
- Why unresolved: The paper uses fixed probabilities (30%, 30%, 30%, 10%) for the four sampling strategies without exploring whether these are optimal or if different weights would yield better performance.
- What evidence would resolve it: Experiments testing different probability distributions across the sampling strategies and analyzing their individual and combined effects on model performance.

## Limitations

- Limited ablation studies: The paper lacks experiments comparing SFIM against other span selection strategies, making it difficult to isolate the specific contribution of syntax tree-based selection.
- Data leakage risk: The paper does not explicitly confirm that evaluation datasets were excluded from training, which could inflate performance metrics.
- Missing implementation details: Critical details about SFIM implementation, including exact syntax tree parsing and span selection rules, are not specified.

## Confidence

- **High Confidence**: Model architecture specifications (32 layers, 4096 hidden size, RoPE, GQA) and general training pipeline (multi-objective training with NTP, FIM, SFIM) are clearly described and technically sound.
- **Medium Confidence**: Claims about outperforming larger models while maintaining better conciseness are supported by benchmarks but could be influenced by data leakage or evaluation methodology.
- **Low Confidence**: Specific mechanisms by which SFIM improves code completion accuracy lack sufficient experimental validation, and the exact impact of the extensive data pipeline is not isolated through ablation studies.

## Next Checks

1. **Ablation study for SFIM effectiveness**: Implement a controlled experiment comparing aiXcoder-7B with variants trained using different span selection strategies (random spans, heuristic-based spans, SFIM). Measure Pass@1 scores on HumanEval to isolate the impact of syntax tree-based span selection versus other training objectives.

2. **Data leakage verification**: Conduct a thorough audit of all training data to verify that none of the evaluation datasets (HumanEval, MBPP, CrossCodeEval, etc.) appear in the training corpus. Implement exact string matching and semantic similarity checks to ensure no overlap between training and evaluation sets.

3. **Cross-file context ablation**: Train two model variants - one using only random file sampling and another using the full diverse data sampling strategy. Evaluate both on CrossCodeEval to quantify the specific contribution of the similarity-based sampling strategies to cross-file context understanding.