---
ver: rpa2
title: 'Causal Inference with Large Language Model: A Survey'
arxiv_id: '2409.09822'
source_url: https://arxiv.org/abs/2409.09822
tags:
- causal
- llms
- arxiv
- tasks
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews the application of large language
  models (LLMs) to causal inference tasks across different levels of causation. The
  study systematically categorizes existing research by tasks, methods, datasets,
  and evaluations, highlighting both the potential and limitations of using LLMs for
  causal reasoning.
---

# Causal Inference with Large Language Model: A Survey

## Quick Facts
- arXiv ID: 2409.09822
- Source URL: https://arxiv.org/abs/2409.09822
- Reference count: 18
- Key outcome: Comprehensive survey of LLM applications to causal inference across association, intervention, and counterfactual levels, highlighting effectiveness in pairwise causal discovery and attribution tasks with GPT-4 achieving high accuracy on benchmarks.

## Executive Summary
This survey systematically reviews the application of large language models to causal inference tasks, categorizing research across different levels of causation and methodologies. The study demonstrates that LLMs, particularly GPT-4, show promising performance in pairwise causal discovery and causal attribution tasks, leveraging domain knowledge, common sense reasoning, and natural language explanations. While highlighting significant potential, the paper also identifies key limitations including hallucinations, inconsistent performance across model sizes, and challenges in verifying genuine causal reasoning versus pattern matching.

## Method Summary
The survey categorizes existing research on LLM-based causal inference by tasks (pairwise discovery, full graph discovery, causal effect estimation, attribution, counterfactuals, and explanations), methods (prompting strategies, fine-tuning, traditional causal method integration, knowledge augmentation), datasets (clinical text, legal cases, general domain knowledge), and evaluation metrics. The methodology involves systematic literature review and synthesis of empirical results from various benchmarks including CEPairs, CLADDER, and domain-specific datasets, with a focus on understanding how different approaches perform across the causal inference task hierarchy.

## Key Results
- GPT-4 achieves high accuracy on pairwise causal discovery and causal attribution benchmarks
- Chain-of-Thought prompting significantly improves LLM performance on causal reasoning tasks
- Knowledge augmentation via RAG and external knowledge bases enhances domain-specific causal inference
- Performance degrades substantially with smaller model sizes, raising scalability concerns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs enhance causal inference by leveraging domain knowledge embedded in their training data
- Mechanism: LLMs can extract and apply domain-specific causal relationships from large-scale text corpora, reducing dependence on human experts
- Core assumption: Training data contains sufficient domain-specific causal knowledge that can be retrieved through appropriate prompting
- Evidence anchors:
  - [abstract] "Domain knowledge. Traditional causal methods focus on numerical data, but domain knowledge is crucial in fields like medicine for identifying causal relationships. LLMs can extract this knowledge from large-scale text, reducing dependence on human experts for causal inference."
  - [section 1.3] "Domain knowledge. Traditional causal methods focus on numerical data, but domain knowledge is crucial in fields like medicine for identifying causal relationships. LLMs can extract this knowledge from large-scale text, reducing dependence on human experts for causal inference."
  - [corpus] Weak evidence - corpus neighbors don't specifically address domain knowledge extraction from LLMs
- Break Condition: If training data lacks sufficient domain-specific causal relationships or if prompting strategies fail to retrieve relevant knowledge

### Mechanism 2
- Claim: LLMs improve causal reasoning through common sense reasoning capabilities
- Mechanism: LLMs can capture human common sense patterns that aid in identifying causal relationships across diverse contexts
- Core assumption: Training data includes sufficient everyday causal reasoning examples that models can generalize from
- Evidence anchors:
  - [abstract] "Common sense. LLMs can capture human common sense, which aids causal reasoning across contexts. For instance, legal cases require logic, and common sense often identifies abnormal events as causes (Kıcıman et al., 2023)."
  - [section 1.3] "Common sense. LLMs can capture human common sense, which aids causal reasoning across contexts. For instance, legal cases require logic, and common sense often identifies abnormal events as causes (Kıcıman et al., 2023)."
  - [corpus] No direct evidence - corpus neighbors don't specifically address common sense reasoning for causal inference
- Break Condition: If common sense patterns are insufficient or context-specific, leading to incorrect causal inferences

### Mechanism 3
- Claim: LLMs enable explainable causal inference through natural language explanations
- Mechanism: LLMs can provide intuitive, human-understandable explanations for causal reasoning processes, making complex concepts accessible
- Core assumption: LLMs have sufficient reasoning capabilities to articulate causal explanations in natural language
- Evidence anchors:
  - [abstract] "Explainable causal inference. LLMs can provide tools for more intuitive, natural language-based explanations of causal reasoning, making complex concepts more accessible and enhancing user interaction with causal inference results."
  - [section 1.3] "Explainable causal inference. LLMs can provide tools for more intuitive, natural language-based explanations of causal reasoning, making complex concepts more accessible and enhancing user interaction with causal inference results."
  - [corpus] Weak evidence - corpus neighbors don't specifically address explainability in causal inference
- Break Condition: If LLM explanations are superficial or incorrect, undermining trust in causal reasoning

## Foundational Learning

- Concept: Causal Ladder (Association, Intervention, Counterfactuals)
  - Why needed here: Understanding the hierarchy of causal reasoning tasks helps in selecting appropriate LLM strategies and evaluating model performance across different levels of complexity
  - Quick check question: What distinguishes counterfactual reasoning from intervention in terms of causal inference complexity?

- Concept: Structural Causal Models (SCM)
  - Why needed here: SCM provides the mathematical framework for representing causal relationships that LLMs need to reason about
  - Quick check question: How does an SCM represent the causal relationship between variables using structural equations?

- Concept: Prompt Engineering Strategies
  - Why needed here: Different prompting approaches (ICL, CoT, EF) significantly impact LLM performance on causal tasks
  - Quick check question: How does Chain-of-Thought prompting differ from basic prompting in terms of eliciting reasoning from LLMs?

## Architecture Onboarding

- Component map:
  - LLM backbone (various sizes and models) -> Prompting interface (basic, ICL, CoT, causality-specific) -> Knowledge augmentation layer (RAG, external knowledge bases) -> Evaluation framework (benchmarks, metrics) -> Tool integration layer (traditional causal methods, APIs)

- Critical path: Data → Prompt construction → LLM inference → Output processing → Evaluation
- Design tradeoffs:
  - Model size vs. computational cost
  - Prompt complexity vs. inference time
  - Knowledge augmentation vs. model autonomy
  - Generalization vs. task-specific optimization
- Failure signatures:
  - Inconsistent answers across similar prompts
  - Hallucinations of non-existent causal relationships
  - Failure to generalize beyond training distribution
  - Sensitivity to prompt phrasing
- First 3 experiments:
  1. Evaluate basic LLM performance on pairwise causal discovery using CEPairs dataset with simple prompts
  2. Test Chain-of-Thought prompting improvement on causal effect estimation tasks using CLADDER benchmark
  3. Implement knowledge augmentation via RAG and measure performance improvement on domain-specific causal attribution tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning techniques significantly improve LLM performance in causal inference tasks compared to prompting strategies alone?
- Basis in paper: [explicit] The paper discusses fine-tuning as a promising approach, with a specific example showing a fine-tuned Mistral model achieving 90% accuracy in pairwise causal discovery compared to baseline models.
- Why unresolved: While the paper provides promising results, it does not extensively compare the effectiveness of fine-tuning versus prompting strategies across different causal inference tasks and model sizes.
- What evidence would resolve it: Comprehensive experiments comparing fine-tuned LLMs against prompting-based approaches across various causal tasks, datasets, and model sizes, measuring performance gains and computational costs.

### Open Question 2
- Question: How can we effectively address the hallucination problem in LLMs when performing causal inference tasks?
- Basis in paper: [explicit] The paper identifies hallucinations as a common issue in LLM-generated causal reasoning, leading to misleading conclusions and self-inconsistent answers.
- Why unresolved: The paper mentions the problem but does not provide concrete solutions or evaluation metrics for detecting and mitigating hallucinations in causal inference contexts.
- What evidence would resolve it: Development and validation of techniques to detect, quantify, and reduce hallucinations in causal reasoning tasks, along with benchmarks to measure improvement in reliability and consistency.

### Open Question 3
- Question: What are the key factors that determine the effectiveness of knowledge augmentation in enhancing LLM performance for domain-specific causal inference tasks?
- Basis in paper: [explicit] The paper discusses knowledge augmentation approaches like RAG and API access to expert systems, showing improved performance in domain-specific tasks, but notes the difficulty in collecting high-quality causal knowledge sources.
- Why unresolved: The paper does not systematically investigate which types of knowledge sources, augmentation methods, or domain characteristics most significantly impact performance.
- What evidence would resolve it: Empirical studies comparing different knowledge augmentation strategies (context vs. tool augmentation) across multiple domains, measuring performance gains relative to knowledge source quality, relevance, and integration methods.

## Limitations

- Black-box nature of LLMs makes it difficult to verify genuine causal reasoning versus pattern matching
- Inconsistent performance across different prompting strategies and model sizes
- Current evaluation metrics may not adequately capture the nuances of causal reasoning, potentially overestimating model capabilities

## Confidence

**High Confidence:** The survey's systematic categorization of LLM applications across the causal inference task hierarchy (association, intervention, counterfactuals) is well-supported by existing literature and provides a clear framework for understanding current capabilities.

**Medium Confidence:** Claims about LLMs' effectiveness in pairwise causal discovery and causal attribution tasks are supported by benchmark results, but these may be influenced by dataset biases and may not generalize to real-world scenarios with complex, interdependent causal structures.

**Low Confidence:** Predictions about future improvements through domain knowledge integration and specialized causality models remain speculative, as the survey lacks concrete evidence of successful implementations in practice.

## Next Checks

1. **Cross-dataset Generalization Test:** Evaluate top-performing LLM approaches on multiple, diverse causal inference datasets to verify whether benchmark performance translates to real-world robustness, particularly testing for sensitivity to domain shift and distribution changes.

2. **Mechanism Verification Study:** Design controlled experiments to distinguish between genuine causal reasoning and pattern matching by introducing counterfactual scenarios that require true causal understanding rather than memorized associations.

3. **Human Evaluation Benchmark:** Conduct systematic human evaluations comparing LLM-generated causal explanations with expert judgments to assess the quality and reliability of explainability features, focusing on cases where LLM explanations may be superficially plausible but causally incorrect.