---
ver: rpa2
title: Game-Theoretic Unlearnable Example Generator
arxiv_id: '2401.17523'
source_url: https://arxiv.org/abs/2401.17523
tags:
- training
- data
- unlearnable
- game
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Game Unlearnable Example (GUE), a novel data
  poisoning attack method that formulates the problem as a nonzero sum Stackelberg
  game. The authors prove the existence of game equilibria under both normal and adversarial
  training settings, showing that the equilibrium gives the most powerful poison attack
  in terms of minimizing test accuracy.
---

# Game-Theoretic Unlearnable Example Generator

## Quick Facts
- arXiv ID: 2401.17523
- Source URL: https://arxiv.org/abs/2401.17523
- Reference count: 7
- Key outcome: Game-theoretic approach achieves 13.25% accuracy on CIFAR-10 and 8.35% on CIFAR-100, outperforming existing methods

## Executive Summary
This paper introduces Game Unlearnable Example (GUE), a novel data poisoning attack method that formulates the problem as a nonzero sum Stackelberg game. The authors prove the existence of game equilibria under both normal and adversarial training settings, showing that the equilibrium gives the most powerful poison attack in terms of minimizing test accuracy. GUE directly solves the game equilibrium using a first-order algorithm and employs an autoencoder-like generative network as the poison attacker. Comprehensive experiments demonstrate that GUE effectively poisons models in various scenarios, including adversarial training.

## Method Summary
GUE formulates data poisoning as a Stackelberg game where the attacker (leader) perturbs training data to minimize classifier accuracy, while the classifier (follower) updates parameters to maximize accuracy. The method uses a U-Net-based autoencoder as the poison generator and ResNet-18 as the classifier. Training employs a bi-level optimization approach using the BOME algorithm with a surrogate loss function to avoid gradient explosion. The generator learns to produce perturbations that generalize across the training set and potentially to new data.

## Key Results
- Achieves test accuracy as low as 13.25% on CIFAR-10 and 8.35% on CIFAR-100
- Effectively poisons models under both standard and adversarial training
- Demonstrates strong generalization, working well when trained on only 60% of the training set
- Outperforms existing unlearnable example attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Stackelberg game formulation ensures the attacker's perturbation minimizes the classifier's test accuracy more effectively than bi-level optimization.
- Mechanism: By modeling the attacker as leader and classifier as follower, the game-theoretic approach guarantees a Stackelberg equilibrium where the attacker's poison strategy yields the lowest possible test accuracy among all classifiers in the hypothesis space.
- Core assumption: The existence of a Stackelberg equilibrium under the given loss functions and constraints.
- Evidence anchors:
  - [abstract]: "We prove the existence of game equilibria under both normal and adversarial training settings, showing that the equilibrium gives the most powerful poison attack in terms of minimizing test accuracy."
  - [section]: "Theorem 3. Under Assumptions 1 and 2, the unlearnable example game G has a Stackelberg equilibrium."
- Break condition: If the loss functions do not satisfy continuity or compactness assumptions, or if the hypothesis space is not well-defined.

### Mechanism 2
- Claim: The autoencoder-like generative network can generalize poison perturbations to unseen data.
- Mechanism: The generator learns to produce sample-wise perturbations that are effective across the training set and can be applied to new data through simple forward propagation.
- Core assumption: The generator captures the underlying distribution of effective perturbations rather than memorizing specific training examples.
- Evidence anchors:
  - [abstract]: "the poison generator can generalize to unseen data well."
  - [section]: "When a generator gω is well trained on a given training set, it costs only forward propagation to generate poisons for images in the training set and potentially for future images that may be added to the training set."
- Break condition: If the training set is too small or unrepresentative, the generator may overfit and fail to generalize.

### Mechanism 3
- Claim: The surrogate loss function Lsur provides a convergent optimization target for computing the Stackelberg equilibrium.
- Mechanism: Lsur is upper-bounded and equivalent to cross-entropy loss, avoiding gradient explosion and providing a clear convergence criterion.
- Core assumption: Lsur maintains the adversarial properties needed to poison the model while being mathematically tractable.
- Evidence anchors:
  - [section]: "To tackle this issue, we instead leverage a surrogate loss function which is upper-bounded and is equivalent to cross-entropy loss for solving the game."
  - [section]: "Proposition 5. Assume that the data have K classes... Lsur is upper-bounded: Lsur(x, y; θ) ≤ − log(K − 1)."
- Break condition: If Lsur loses its equivalence to cross-entropy loss or fails to maintain the adversarial properties.

## Foundational Learning

- Concept: Stackelberg games and Nash equilibrium
  - Why needed here: The attack is formulated as a Stackelberg game, requiring understanding of game-theoretic equilibria.
  - Quick check question: What is the difference between a Stackelberg equilibrium and a Nash equilibrium?

- Concept: Bi-level optimization
  - Why needed here: The problem is transformed into a bi-level optimization problem, which is then solved using first-order methods.
  - Quick check question: How does bi-level optimization differ from standard optimization?

- Concept: Adversarial training and its defenses
  - Why needed here: The attack must be effective against both standard and adversarial training methods.
  - Quick check question: What is the primary goal of adversarial training?

## Architecture Onboarding

- Component map:
  - Classifier: ResNet-18 (or other architectures)
  - Generator: U-Net-based autoencoder
  - Loss functions: Cross-entropy, surrogate loss, adversarial loss
  - Optimizer: SGD for classifier, SGD for generator, Adam for inner loop

- Critical path:
  1. Initialize classifier and generator
  2. Sample mini-batch from training set
  3. Compute classifier updates via T-step gradient descent
  4. Update generator and classifier parameters using BOME algorithm
  5. Repeat for specified number of epochs

- Design tradeoffs:
  - Tradeoff between poison radius and model performance
  - Choice of loss function (cross-entropy vs. surrogate loss)
  - Number of training epochs and batch size

- Failure signatures:
  - Generator overfitting to training data (poor generalization)
  - Gradient explosion during optimization
  - Ineffective poisoning against adversarial training

- First 3 experiments:
  1. Train GUE on CIFAR-10 with standard training, evaluate clean test accuracy
  2. Train GUE on CIFAR-100 with adversarial training, evaluate clean test accuracy
  3. Test generalizability by training generator on 60% of data, evaluate on full test set

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions.

## Limitations
- Limited evaluation to CIFAR-10 and CIFAR-100 datasets
- Single model architecture (ResNet-18) tested
- No comparison with other bi-level optimization methods

## Confidence
- Stackelberg equilibrium guarantees: **High**
- Generator generalizability: **Medium** (depends heavily on training data quality and quantity)
- Surrogate loss function effectiveness: **High** (based on optimization theory)

## Next Checks
1. Verify that the surrogate loss Lsur maintains stable gradients during optimization compared to cross-entropy loss, particularly for extreme poison perturbations.
2. Test the generator's ability to generalize when trained on varying percentages of the dataset (10%, 30%, 60%) and evaluate performance degradation as training data decreases.
3. Conduct controlled experiments comparing GUE's effectiveness against standard training vs. adversarial training with different perturbation radii (ε values) to quantify robustness claims.