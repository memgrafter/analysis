---
ver: rpa2
title: Efficient Multi-Agent Collaboration with Tool Use for Online Planning in Complex
  Table Question Answering
arxiv_id: '2412.20145'
source_url: https://arxiv.org/abs/2412.20145
tags:
- table
- action
- mact
- observation
- thought
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MACT introduces a multi-agent collaboration framework with tool
  use for table question answering that achieves state-of-the-art performance without
  fine-tuning or closed-source models. The framework combines a planning agent for
  iterative planning, a coding agent for code generation, and tools including Python
  interpreter, calculator, and Wikipedia search.
---

# Efficient Multi-Agent Collaboration with Tool Use for Online Planning in Complex Table Question Answering

## Quick Facts
- arXiv ID: 2412.20145
- Source URL: https://arxiv.org/abs/2412.20145
- Reference count: 28
- Primary result: MACT achieves state-of-the-art performance on three out of four TQA benchmarks without fine-tuning or closed-source models

## Executive Summary
MACT introduces a multi-agent collaboration framework that combines a planning agent for iterative planning and a coding agent for code generation, supported by tools including Python interpreter, calculator, and Wikipedia search. The framework achieves state-of-the-art performance on table question answering benchmarks without requiring fine-tuning or closed-source models. Experiments show MACT outperforms previous SoTA systems on three out of four datasets and matches GPT-4 performance on two benchmarks, while an efficiency optimization module saves up to 33% of iterations without performance loss.

## Method Summary
MACT employs a two-agent system where a planning agent generates iterative plans and a coding agent translates instructions into executable code. The framework operates through five stages: action generation, action selection using self-consistency, tool selection/code generation, observation computation, and memory state update. Tools include Python interpreter for table operations, calculator for numerical reasoning, and Wikipedia search for factual knowledge retrieval. The system uses open-weight models (Qwen-2 72B for planning, CodeLLaMA-34B for coding) and implements an efficiency optimization module that shortcuts confident predictions based on self-consistency scores.

## Key Results
- MACT outperforms previous state-of-the-art systems on three out of four TQA benchmarks
- Achieves comparable results to GPT-4 on two benchmarks using only open-weight models without fine-tuning
- Efficiency optimization saves up to 33% of iterations without performance degradation
- The framework demonstrates effectiveness of specialized agents and tool use for complex reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent collaboration with specialized agents outperforms single-agent approaches in complex TQA
- Mechanism: Different agents (planning vs coding) focus on their strengths - the planning agent decomposes questions while the coding agent handles code generation and execution
- Core assumption: Different reasoning types (multi-step vs multi-category) benefit from specialized expertise
- Evidence anchors:
  - [abstract]: "MACT outperforms previous SoTA systems on three out of four benchmarks" and "The planning agent performs online planning, i.e., it generates a plan iteratively. This breaks down complex problems and helps to address multi-step reasoning. The coding agent and the tool set assist with generating faithful intermediate results."
  - [section]: "Our experiments on four TQA benchmarks show that MACT outperforms previous SoTA systems on three out of four benchmarks. It achieves comparable results to GPT-4 on two benchmarks even when using only open-weight models without any fine-tuning."
  - [corpus]: Weak evidence - related papers discuss multi-agent collaboration but don't specifically compare specialized vs single agents for TQA
- Break condition: When agents interfere with each other's work or when the coordination overhead exceeds the benefit of specialization

### Mechanism 2
- Claim: Tool use ensures faithful execution of operations that LLMs struggle with
- Mechanism: Tools like Python interpreter, calculator, and Wikipedia search provide reliable execution for retrieval, calculation, and knowledge lookup
- Core assumption: LLMs are unreliable for precise numerical operations and factual retrieval
- Evidence anchors:
  - [abstract]: "The coding agent and the tool set assist with generating faithful intermediate results" and "We use similar tools. Inspired by Shinn et al. (2023), we further add Wikipedia search as an additional tool to assist questions requiring factual knowledge."
  - [section]: "LLMs have been shown to be ineffective in retrieving information from long tables (Zhou et al., 2024) and carrying out numerical reasoning (Imani et al., 2023). Making use of tools can ensure faithful results of these operations."
  - [corpus]: Weak evidence - related papers mention tool use but don't quantify the reliability improvement specifically
- Break condition: When tool integration adds latency that outweighs accuracy benefits, or when tools become unreliable (e.g., API failures)

### Mechanism 3
- Claim: Online iterative planning with efficiency optimization balances performance and computational cost
- Mechanism: The framework iteratively refines plans based on previous results, with an optimization module that shortcuts confident predictions
- Core assumption: Complex problems benefit from iterative refinement, but simple problems can be answered directly
- Evidence anchors:
  - [abstract]: "We propose an efficiency optimization component that serves as a shortcut for directly outputting an answer in the first iteration. Whether the answer is output directly depends on the confidence of Mp, which we approximate by the degree of self-consistency of its estimated predictions"
  - [section]: "The iterative nature of MACT can lead to a higher upper-bound of LLM calls. However, it also allows for solving more complex problems, making the approach more tailored to real-life requirements" and "we introduce a hyper-parameter α ∈ [0..1]. If the degree of self-consistency...is larger than α * k (high degree of SC), Mp outputs this most frequent answer"
  - [corpus]: Weak evidence - related papers discuss iterative planning but don't specifically address efficiency optimization for TQA
- Break condition: When the confidence threshold α is set too high (missing optimization opportunities) or too low (incorrect shortcuts)

## Foundational Learning

- Concept: Online iterative planning vs global planning
  - Why needed here: Understanding why MACT uses iterative planning (refining steps based on previous results) rather than global planning (single upfront plan)
  - Quick check question: What's the key difference between online iterative planning and online global planning, and why does MACT prefer the former?

- Concept: Self-consistency as a selection mechanism
  - Why needed here: The framework uses self-consistency to select the most promising action from multiple generated options
  - Quick check question: How does self-consistency work as an action selection strategy, and what's its main advantage over other selection methods?

- Concept: Multi-agent collaboration patterns
  - Why needed here: Understanding how the planning and coding agents coordinate through five stages (action generation, selection, tool selection, observation computation, memory update)
  - Quick check question: What are the five core stages of collaboration between planning and coding agents in MACT?

## Architecture Onboarding

- Component map: Planning agent (Mp) -> Action generation -> Action selection -> Tool selection/code generation -> Tool execution/observation computation -> Memory state (S) update -> Repeat or Finish
- Critical path: Question → Planning agent generates action → Action selection → Tool selection/code generation → Tool execution/observation computation → Memory update → Repeat or Finish
- Design tradeoffs:
  - Specialized agents vs single agent: Better performance but more complex coordination
  - Multiple code samples (k=5) vs single sample: More robust but higher cost
  - Efficiency optimization vs full execution: Faster but potentially less accurate
- Failure signatures:
  - Invalid code from coding agent
  - Incorrect action selection leading to wrong reasoning path
  - Tool failures (API issues, execution errors)
  - Confidence threshold set incorrectly causing premature shortcuts
- First 3 experiments:
  1. Run MACT on a simple WTQ instance without efficiency optimization to observe full iterative process
  2. Compare MACT with and without the coding agent on a calculation-heavy question
  3. Test different values of α in the efficiency optimization module on a mixed-difficulty dataset

## Open Questions the Paper Calls Out
- How does MACT's performance scale with different sizes of planning and coding agents, and what is the optimal size ratio between them?
- How does MACT perform on multi-table reasoning tasks where information needs to be aggregated across multiple tables?
- How does the efficiency optimization module's α parameter affect the balance between computational cost and accuracy across different reasoning complexity levels?

## Limitations
- Evaluation is limited to single-table settings due to scarcity of multi-table reasoning datasets
- Implementation details of the efficiency optimization module and confidence threshold mechanism remain unclear
- Performance gains may be attributed to specific model choices rather than purely architectural advantages

## Confidence
- **High confidence**: The core multi-agent collaboration framework with specialized planning and coding agents is well-specified and the performance claims on three out of four benchmarks appear well-supported by the reported results
- **Medium confidence**: The effectiveness of tool use for faithful execution is plausible but the paper doesn't provide quantitative evidence comparing tool-assisted vs. non-tool-assisted performance on specific operations
- **Low confidence**: The claimed efficiency optimization benefits (33% iteration savings) lack transparency in how the confidence threshold is determined and validated

## Next Checks
1. **Implementation Verification**: Implement the efficiency optimization module with different α values and measure both accuracy retention and iteration reduction on a subset of WTQ to verify the 33% savings claim
2. **Ablation Study**: Compare MACT performance with and without each component (planning agent, coding agent, Wikipedia tool) on calculation-heavy and knowledge-heavy questions to quantify individual contributions
3. **Cost Analysis**: Measure average iterations, total LLM calls, and token usage across all datasets to validate the claimed computational efficiency improvements and assess real-world applicability