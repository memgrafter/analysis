---
ver: rpa2
title: Value Alignment from Unstructured Text
arxiv_id: '2408.10392'
source_url: https://arxiv.org/abs/2408.10392
tags:
- data
- values
- preference
- synthetic
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a method to align large language models (LLMs)
  with values implicit or explicit in unstructured text documents, without relying
  on costly human-curated data. It uses synthetic data generation: a teacher model
  extracts diverse questions and grounded answers from document chunks for supervised
  fine-tuning (SFT), and also generates contrast pairs of faithful/unfaithful answers
  for preference optimization (DPO).'
---

# Value Alignment from Unstructured Text
## Quick Facts
- arXiv ID: 2408.10392
- Source URL: https://arxiv.org/abs/2408.10392
- Reference count: 14
- One-line primary result: Introduces automated synthetic data generation for aligning LLMs to values implicit or explicit in unstructured text, outperforming human-curated baselines on two test documents.

## Executive Summary
This paper presents a fully automated pipeline for aligning large language models with values expressed in unstructured text documents, eliminating the need for costly human-curated datasets. The approach uses a teacher LLM to generate synthetic questions and answers from document chunks for supervised fine-tuning (SFT), and creates contrast pairs for direct preference optimization (DPO). Tested on Mistral-7B-Instruct with two documents (business conduct guidelines and the Universal Declaration of Human Rights), the method achieves higher win rates in human judgments and better BLEU, ROUGE, and BERTScore scores than baselines. The approach is scalable and can quickly align models to any document's values without human intervention.

## Method Summary
The paper introduces an automated pipeline for aligning large language models (LLMs) with values implicit or explicit in unstructured text documents, without relying on costly human-curated data. It uses synthetic data generation: a teacher model extracts diverse questions and grounded answers from document chunks for supervised fine-tuning (SFT), and also generates contrast pairs of faithful/unfaithful answers for preference optimization (DPO). Evaluated on Mistral-7B-Instruct, the method outperforms baselines (vanilla fine-tuning, RAG, SFT alone) on both a business conduct guideline and the Universal Declaration of Human Rights. DPO-aligned models achieve win rates above 0.6 on paired judgments and better BLEU, ROUGE, and BERTScore scores than baselines. RAG integration surprisingly underperforms, suggesting conflict between parametric and non-parametric memory. The approach is fully automated, requires no human intervention, and can align models to any document's values quickly.

## Key Results
- DPO-aligned models achieve win rates above 0.6 on paired human judgments for value alignment
- Models outperform vanilla fine-tuning, RAG, and SFT-only baselines on both business conduct and human rights documents
- RAG integration underperforms, suggesting potential conflicts between parametric and non-parametric memory

## Why This Works (Mechanism)
The method works by using a teacher LLM to automatically extract relevant questions and generate both faithful and unfaithful answer pairs from document chunks. This synthetic data captures the document's values without human intervention. SFT trains the model on accurate answers, while DPO fine-tunes it to prefer faithful responses over unfaithful ones. The combined approach ensures the model not only knows the correct answers but also understands what constitutes faithful representation of the document's values.

## Foundational Learning
- Supervised Fine-Tuning (SFT): Why needed - teaches the model the correct answers to value-related questions. Quick check - verify the model can accurately answer questions about document content.
- Direct Preference Optimization (DPO): Why needed - ensures the model prefers faithful answers over unfaithful ones. Quick check - test if the model chooses faithful responses in pairwise comparisons.
- Synthetic Data Generation: Why needed - eliminates costly human data annotation. Quick check - confirm generated questions cover diverse aspects of the document.

## Architecture Onboarding
- Component Map: Teacher LLM -> Question/Answer Generator -> SFT Dataset -> DPO Dataset -> Fine-tuned Model
- Critical Path: Document chunking -> Question/answer generation -> SFT training -> DPO training -> Evaluation
- Design Tradeoffs: Automated generation trades off potential teacher bias for scalability and cost reduction
- Failure Signatures: Poor performance on RAG suggests parametric/non-parametric memory conflicts
- First Experiments:
  1. Test the pipeline on diverse value documents beyond business and human rights
  2. Vary teacher model (e.g., Claude 3.5 Sonnet) to assess generation bias impact
  3. Evaluate safety and adversarial robustness on edge cases

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to two short documents and one base model, restricting generalizability
- RAG underperformance not explained; possible conflicts between memory types remain speculative
- Synthetic data quality not independently verified; teacher model bias not assessed

## Confidence
- Core methodology (SFT + DPO from synthetic data): High
- RAG integration findings: Medium
- Safety and real-world applicability claims: Low

## Next Checks
1. Test the pipeline on a diverse set of documents and values (e.g., privacy, sustainability, cultural norms) to assess robustness and generalization.
2. Conduct ablation studies varying the teacher model (e.g., Claude 3.5 Sonnet) to quantify the impact of generation bias.
3. Perform safety and adversarial robustness evaluations, including edge-case and ambiguous value scenarios, to establish practical reliability.