---
ver: rpa2
title: 'Give me Some Hard Questions: Synthetic Data Generation for Clinical QA'
arxiv_id: '2412.04573'
source_url: https://arxiv.org/abs/2412.04573
tags:
- questions
- clinical
- data
- question
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores generating Clinical QA data using LLMs in
  a zero-shot setting to address the scarcity of annotated clinical data. The authors
  propose two prompting strategies to improve question generation: 1) instructing
  the model to generate questions that do not overlap with the input context, and
  2) summarizing the input record using a predefined schema to scaffold question generation.'
---

# Give me Some Hard Questions: Synthetic Data Generation for Clinical QA

## Quick Facts
- arXiv ID: 2412.04573
- Source URL: https://arxiv.org/abs/2412.04573
- Reference count: 40
- Primary result: Proposed methods generate more challenging clinical QA questions, achieving 65.0 F1 and 76.8 Reference Overlap on RadQA

## Executive Summary
This paper addresses the scarcity of annotated clinical QA data by proposing LLM-based synthetic data generation methods. The authors identify that naive prompting often produces overly simple questions that don't reflect clinical complexity. They introduce two prompting strategies: "No Overlap" (instructing models to avoid lexical overlap with input context) and "Summarization" (using structured summaries to scaffold question generation). Experiments on RadQA and MIMIC-QA datasets demonstrate significant improvements in QA performance metrics, with the best method achieving 4.4 F1 and 6.0 RO points improvement over direct instruction baselines.

## Method Summary
The approach generates synthetic QA pairs using LLMs in a zero-shot setting. The "No Overlap" method instructs the model to create questions that don't overlap with the input context, forcing deeper semantic understanding. The "Summarization" method uses a predefined schema to structure the input document before question generation. After generating questions, answers are distilled from the LLM, with unanswerable questions naturally emerging. The synthetic data is then used to fine-tune BioClinicalRoBERTa models, which are evaluated on test sets using F1, EM, and Reference Overlap metrics.

## Key Results
- "No Overlap" prompting consistently outperforms baseline methods
- Combining "Summarization" with "No Overlap" achieves best results (65.0 F1, 76.8 RO on RadQA)
- Generated questions significantly improve fine-tuning performance over direct instruction
- Synthetic data generation achieves strong results with only 64 sampled documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instructing the LLM to generate questions that do not overlap with the input context increases the proportion of challenging questions.
- Mechanism: By explicitly telling the model to avoid using words from the input context, the generated questions require deeper semantic understanding rather than surface-level matching, leading to more informative and challenging questions.
- Core assumption: The model's instruction-following capability is strong enough to adhere to the "no overlap" constraint while still generating relevant and coherent questions.
- Evidence anchors:
  - [abstract]: "We find that naive prompting often results in easy questions that do not reflect the complexity of clinical scenarios. To address this, we propose two prompting strategies: 1) instructing the model to generate questions that do not overlap with the input context..."
  - [section]: "Our proposed 'No Overlap' consistently outperforms other baselines. Combining it with 'Summarization' further boosts performance, achieving the best results in both F1 and RO."
- Break condition: If the model's instruction-following capability degrades or if the "no overlap" constraint is too strict, leading to irrelevant or incoherent questions.

### Mechanism 2
- Claim: Summarizing the input record using a predefined schema scaffolds question generation by focusing on relevant content.
- Mechanism: The summarization step abstracts the input context into a structured summary using a predefined schema, which guides the model towards generating questions based on the essential aspects a doctor would consider, reducing the impact of extraneous details.
- Core assumption: The predefined schema effectively captures the high-level attributes of interest to doctors when formulating questions.
- Evidence anchors:
  - [abstract]: "2) summarizing the input record using a predefined schema to scaffold question generation."
  - [section]: "The summarization step abstracts the input context into a structured summary using a predefined schema, which includes high-level attributes covering the essential aspects a doctor would consider when formulating questions."
- Break condition: If the predefined schema is incomplete or not aligned with the types of questions doctors actually ask, leading to less effective question generation.

### Mechanism 3
- Claim: The distillation approach, where answers are distilled from the LLM after question generation, naturally leads to unanswerable questions.
- Mechanism: By first generating questions and then extracting answers from the LLM, questions that cannot be answered from the input document naturally emerge as unanswerable, increasing the diversity of question types.
- Core assumption: The LLM's ability to accurately extract answers from the input document is sufficient to identify unanswerable questions.
- Evidence anchors:
  - [abstract]: "We then distill answers from them, with unanswerable questions emerging during the second step."
  - [section]: "we adopt a distillation approach: we first generate questions using LLMs and then distill answers from them, with unanswerable questions emerging during the second step."
- Break condition: If the LLM's answer extraction capability is not accurate enough, leading to incorrect classification of answerable vs. unanswerable questions.

## Foundational Learning

- Concept: Instruction-following in LLMs
  - Why needed here: The proposed methods rely heavily on the LLM's ability to follow instructions to generate questions that do not overlap with the input context and to summarize the input using a predefined schema.
  - Quick check question: How well does the LLM follow instructions to generate questions that meet specific criteria (e.g., no overlap with input context)?

- Concept: Summarization techniques
  - Why needed here: The summarization step is crucial for guiding question generation by abstracting the input context into a structured format that focuses on relevant content.
  - Quick check question: How effectively can the LLM summarize the input context using a predefined schema to capture the essential aspects of interest?

- Concept: Question generation and answer extraction
  - Why needed here: The core task of generating questions and extracting answers from the input document is fundamental to the proposed methods.
  - Quick check question: How accurately can the LLM generate questions and extract answers from the input document, especially for complex clinical scenarios?

## Architecture Onboarding

- Component map: Input document → Summarization (optional) → Question generation → Answer extraction → Fine-tuning the Clinical QA model

- Critical path: Input document → Summarization → Question generation → Answer extraction → Fine-tuning the Clinical QA model

- Design tradeoffs:
  - Using a predefined schema for summarization vs. a more flexible approach
  - The number of questions generated per document vs. the number of documents used
  - The complexity of the instruction given to the LLM vs. the risk of generating irrelevant questions

- Failure signatures:
  - Generated questions are too simple or do not reflect the complexity of clinical scenarios
  - The LLM fails to follow the "no overlap" instruction or the summarization schema
  - Answer extraction is not accurate, leading to incorrect classification of answerable vs. unanswerable questions

- First 3 experiments:
  1. Evaluate the LLM's instruction-following capability by asking it to generate questions that do not overlap with the input context.
  2. Assess the effectiveness of the predefined schema for summarization by comparing the generated summaries with human-annotated summaries.
  3. Test the answer extraction capability of the LLM by providing it with generated questions and evaluating the extracted answers against the gold answers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of synthetic answers compare to human-annotated answers across different medical specialties and document types?
- Basis in paper: [explicit] The paper states "human-annotated answers remain essential for optimal performance" and finds a persistent gap between synthetic and gold answers even with 803 training documents.
- Why unresolved: The study only examines radiology reports and clinical notes, limiting generalizability to other medical specialties like pathology, cardiology, or oncology.
- What evidence would resolve it: Systematic comparison of synthetic vs. human-annotated answers across diverse medical document types and specialties, measuring performance differences.

### Open Question 2
- Question: What is the optimal balance between generating more QA pairs per document versus including more diverse documents in the training set?
- Basis in paper: [explicit] Figure 2 shows that increasing QA pairs per document (5→10→20) provides diminishing returns once 64 documents are reached, suggesting document diversity matters more.
- Why unresolved: The study only tests up to 803 documents (the full RadQA training set) and doesn't explore what happens when training data exceeds this amount.
- What evidence would resolve it: Scaling experiments with synthetic data exceeding the original training set size, testing different ratios of QA pairs per document versus number of unique documents.

### Open Question 3
- Question: How does the performance of Clinical QA systems degrade when tested on documents from different institutions or time periods than those used for synthetic data generation?
- Basis in paper: [inferred] The study uses documents from MIMIC-III, a single institutional dataset, raising questions about generalizability to other healthcare systems with different documentation practices.
- Why unresolved: The synthetic data generation process may capture institution-specific language patterns and practices that don't generalize.
- What evidence would resolve it: Cross-institutional evaluation of Clinical QA models trained on synthetic data from one institution and tested on real data from different institutions or time periods.

## Limitations

- The approach depends heavily on LLM instruction-following capability, which may vary across models and context complexity
- Evaluation is limited to two specific datasets (RadQA and MIMIC-QA), raising generalizability concerns to other clinical domains
- The distillation approach for answer extraction assumes LLM accuracy in identifying unanswerable questions without independent validation
- The predefined summarization schema's effectiveness for diverse clinical specialties beyond radiology remains untested

## Confidence

- **High confidence**: The claim that the "No Overlap" and "Summarization" prompting strategies improve QA performance metrics (F1 and RO) on the tested datasets
- **Medium confidence**: The assertion that these improvements translate to clinically meaningful question difficulty, as evaluation metrics measure technical performance rather than clinical reasoning complexity
- **Medium confidence**: The generalizability of results to other clinical domains or question types, given the evaluation is limited to radiology reports and discharge summaries

## Next Checks

1. **Instruction-following robustness test**: Systematically evaluate how well different LLM models (including smaller or open-source variants) follow the "No Overlap" instruction across varying context lengths and complexity levels, measuring both success rate and quality of generated questions.

2. **Schema generalizability validation**: Apply the summarization schema