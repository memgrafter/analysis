---
ver: rpa2
title: Provable Interactive Learning with Hindsight Instruction Feedback
arxiv_id: '2404.09123'
source_url: https://arxiv.org/abs/2404.09123
tags:
- learning
- agent
- instruction
- where
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies interactive learning from hindsight instruction
  feedback, where an agent learns to follow instructions by generating responses and
  receiving suitable instructions for those responses from a teacher. The key insight
  is that providing hindsight instructions is easier than providing expert demonstrations
  or rewards.
---

# Provable Interactive Learning with Hindsight Instruction Feedback

## Quick Facts
- arXiv ID: 2404.09123
- Source URL: https://arxiv.org/abs/2404.09123
- Authors: Dipendra Misra; Aldo Pacchiano; Robert E. Schapire
- Reference count: 40
- Primary result: Introduces LORIL algorithm achieving O(√T) regret in hindsight instruction learning without dependence on response space size

## Executive Summary
This paper introduces a novel framework for interactive learning from hindsight instruction feedback, where an agent learns to follow instructions by generating responses and receiving suitable instructions for those responses from a teacher. The key insight is that providing hindsight instructions is computationally easier than providing expert demonstrations or rewards, as it shifts the cognitive burden from the teacher to a structured labeling problem. The authors prove a lower bound showing worst-case regret scales polynomially with response space size, then introduce a low-rank setting where the teacher's instruction-response distribution admits a low-rank decomposition.

## Method Summary
The LORIL algorithm operates in a hindsight instruction setting where an agent observes context st and instruction xt, then generates response yt. The teacher provides hindsight instruction x't ~ P(X|yt,st). LORIL uses maximum likelihood estimation to learn the teacher model f⋆ and incorporates an elliptic bonus for exploration. The algorithm selects responses using the policy πt(x,s) = arg maxy∈Y [ˆft(x)⊤g⋆(y,s) + bt(y,s)], where the elliptic bonus bt(y,s) = C' · (√log(t|F|/δ) + √λtB) · ||g⋆(y,s)||bΣt⁻¹ provides optimism under uncertainty. This bonus ensures efficient exploration without exhaustive enumeration of the response space.

## Key Results
- LORIL achieves O(√T) regret bound that scales with the intrinsic rank d rather than exponential sizes of original spaces
- In synthetic task, LORIL achieves 12.3% smaller regret than next best baseline
- In image classification task, LORIL achieves 4.8% less regret than baselines even when low-rank assumption is violated

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Providing hindsight instruction feedback is computationally and informationally easier than providing expert demonstrations or rewards because it shifts the cognitive burden from the teacher to a structured labeling problem.
- **Mechanism**: In the hindsight instruction setting, the teacher observes the agent's generated response and provides the most suitable instruction for that response. This reverses the labeling problem, allowing non-expert teachers to provide feedback without needing to understand the agent's action space or dynamics.
- **Core assumption**: Instructions in natural language are easier for humans to provide than detailed trajectories or scalar rewards, and the teacher model P(X|Y,S) can be modeled as a conditional distribution that is learned over time.
- **Evidence anchors**:
  - [abstract] states: "This hindsight labeling of instruction is often easier to provide than providing expert supervision of the optimal response which may require expert knowledge or can be impractical to elicit."
  - [section 1] explains: "In contrast, a non-expert user can easily provide an instruction for the red trajectory in Figure 1."
  - [corpus] provides related works on interactive learning and feedback mechanisms.
- **Break condition**: If the instruction space X becomes too large or complex, or if the teacher cannot effectively map responses to instructions, the cognitive advantage disappears.

### Mechanism 2
- **Claim**: The low-rank decomposition of the teacher model P(X|Y,S) = f⋆(x)⊤g⋆(y,s) enables learning without dependence on the size of the response or instruction space, only on the intrinsic rank d.
- **Mechanism**: By assuming the teacher's distribution admits a low-rank decomposition, the algorithm LORIL can use function approximation to represent f⋆ and g⋆ in a d-dimensional space, where d << |X|, |Y|, |S|. This allows the algorithm to achieve O(√T) regret that scales with d rather than the exponential sizes of the original spaces.
- **Core assumption**: The true teacher model P(X|Y,S) can be approximated by a low-rank matrix decomposition, and the agent has access to the embedding function g⋆.
- **Evidence anchors**:
  - [section 4] defines the low-rank teacher model: "We assume that there exists f⋆ : X → Rd and g⋆ : Y → Rd such that ∀s ∈ S, x ∈ X, y ∈ Y, P(x | y, s) = f⋆(x)⊤g⋆(y, s)"
  - [section 5] proves the regret bound: "Reg(T) = O(B√T d log(1 + T B) + √T d log(T|F|/δ) log(1 + T B))"
  - [corpus] shows related work on low-rank models in bandits and RL.
- **Break condition**: If the true teacher model does not admit a low-rank structure, or if g⋆ is unknown and cannot be learned, the algorithm's theoretical guarantees no longer hold.

### Mechanism 3
- **Claim**: The elliptic bonus bt(y,s) = C' · (√log(t|F|/δ) + √λtB) · ||g⋆(y,s)||bΣt⁻¹ provides optimism under uncertainty that enables efficient exploration without exhaustive enumeration of the response space.
- **Mechanism**: The elliptic bonus is computed using the inverse of the covariance matrix bΣt, which captures information about historical data. This bonus ensures that the algorithm explores responses that are both novel (high ||g⋆(y,s)||bΣt⁻¹) and uncertain (high estimation error). The bonus scales as O(√d) rather than O(|Y|), making exploration tractable even for exponentially large response spaces.
- **Core assumption**: The feature space induced by g⋆ has bounded norm (||g⋆(y,s)|| ≤ B) and the covariance matrix bΣt is positive definite with bounded eigenvalues.
- **Evidence anchors**:
  - [section 4] introduces the elliptic bonus: "The quantity bt(y,s) can be viewed as a bonus in Equation 2 and is known as elliptic bonus in the literature"
  - [section 5] shows the bonus computation: "bt(y,s) = C' · (√log(t|F|/δ) + √λtB) · ||g⋆(y,s)||bΣt⁻¹"
  - [section 6.2] demonstrates that LORIL outperforms baselines even when the low-rank assumption is violated.
- **Break condition**: If the feature space g⋆ is not bounded or if the covariance matrix becomes ill-conditioned, the elliptic bonus may not provide proper exploration incentives.

## Foundational Learning

- **Concept**: Contextual bandits and reinforcement learning
  - **Why needed here**: The hindsight instruction setting combines elements of both contextual bandits (choosing responses based on context and instruction) and reinforcement learning (learning from feedback over time). Understanding the differences between these frameworks is crucial for analyzing the unique challenges of this setting.
  - **Quick check question**: What is the key difference between the feedback in contextual bandits and the hindsight instruction setting described in this paper?

- **Concept**: Low-rank matrix decomposition and function approximation
  - **Why needed here**: The theoretical analysis relies on the assumption that the teacher model admits a low-rank decomposition. Understanding how low-rank approximations enable generalization in high-dimensional spaces is essential for grasping why the algorithm achieves regret bounds independent of |X|, |Y|, and |S|.
  - **Quick check question**: Why does assuming a low-rank structure for P(X|Y,S) allow the algorithm to avoid dependence on the size of the response space |Y|?

- **Concept**: Optimism in the face of uncertainty and exploration-exploitation tradeoff
  - **Why needed here**: LORIL implements the principle of optimism under uncertainty through the elliptic bonus. Understanding how optimism-based algorithms balance exploration and exploitation is crucial for analyzing the algorithm's regret guarantees and implementation.
  - **Quick check question**: How does the elliptic bonus in LORIL ensure that the algorithm explores sufficiently while still exploiting known good responses?

## Architecture Onboarding

- **Component map**: Teacher model P(X|Y,S) -> Agent policy πt -> Embedding function g⋆ -> Model class F -> Covariance matrix bΣt
- **Critical path**:
  1. World presents context st and instruction xt
  2. Agent computes MLE estimate ˆft using historical data
  3. Agent computes elliptic bonus bt(y,s) using bΣt⁻¹
  4. Agent selects response yt = arg max_y [ˆft(xt)⊤g⋆(y,st) + bt(y,st)]
  5. Teacher provides hindsight instruction x't ~ P(X|yt,st)
  6. Agent updates historical data and covariance matrix

- **Design tradeoffs**:
  - Using function approximation (low-rank assumption) vs. tabular methods: Function approximation enables generalization but requires stronger assumptions
  - Computing elliptic bonus vs. simpler exploration strategies: Elliptic bonus provides theoretical guarantees but requires matrix inversion
  - Known vs. unknown g⋆: Known g⋆ simplifies the algorithm but may not be realistic; unknown g⋆ requires additional representation learning

- **Failure signatures**:
  - Poor performance on tasks where the teacher model does not admit low-rank structure
  - Numerical instability when the covariance matrix bΣt becomes ill-conditioned
  - Slow convergence if the embedding function g⋆ does not capture relevant information

- **First 3 experiments**:
  1. Implement LORIL on the synthetic task with known low-rank structure to verify theoretical guarantees
  2. Test LORIL on the image selection task with natural language instructions to evaluate practical performance
  3. Compare LORIL against ϵ-greedy and greedy baselines on both tasks to demonstrate the benefit of elliptic exploration

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the main text.

## Limitations
- The low-rank assumption is restrictive and may not hold in real-world applications, limiting the algorithm's theoretical guarantees when violated
- The algorithm requires either known or learnable embedding functions g⋆, which may be difficult to obtain in practice, especially for natural language instructions
- Computational complexity grows with d² due to matrix inversion, making the algorithm potentially impractical for high-rank settings

## Confidence
- High confidence in Mechanism 1 (cognitive advantage of hindsight instruction feedback) - supported by intuitive reasoning and related literature
- Medium confidence in Mechanism 2 (low-rank decomposition enabling generalization) - theoretically sound but relies on strong assumptions
- Medium confidence in Mechanism 3 (elliptic bonus ensuring efficient exploration) - mathematically rigorous but may not translate perfectly to practice

## Next Checks
1. Evaluate LORIL on a real-world dataset with known instruction-response distributions to test performance when the low-rank assumption is violated
2. Conduct ablation studies removing the elliptic bonus to quantify its contribution to performance improvements
3. Test scalability by increasing d and measuring computational time and regret to identify practical limits