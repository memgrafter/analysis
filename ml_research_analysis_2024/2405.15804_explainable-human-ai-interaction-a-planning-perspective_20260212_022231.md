---
ver: rpa2
title: 'Explainable Human-AI Interaction: A Planning Perspective'
arxiv_id: '2405.15804'
source_url: https://arxiv.org/abs/2405.15804
tags:
- plan
- robot
- human
- will
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This book provides a comprehensive overview of explainable human-AI
  interaction through planning, focusing on how AI agents can effectively collaborate
  with humans by considering their mental models. The authors address the challenge
  of making AI systems interpretable by introducing three key interpretability measures:
  explicability, legibility, and predictability.'
---

# Explainable Human-AI Interaction: A Planning Perspective

## Quick Facts
- arXiv ID: 2405.15804
- Source URL: https://arxiv.org/abs/2405.15804
- Reference count: 0
- Primary result: Comprehensive framework for explainable human-AI interaction through planning, introducing explicability, legibility, and predictability measures

## Executive Summary
This book presents a comprehensive framework for explainable human-AI interaction through planning, addressing the challenge of making AI systems interpretable in human collaboration contexts. The authors introduce three key interpretability measures—explicability, legibility, and predictability—that enable AI agents to effectively collaborate with humans by considering their mental models. The framework provides methods for generating explicable behavior through both model-based and model-free approaches, with explanations serving as a model reconciliation strategy to bridge the gap between AI agent models and human mental models.

## Method Summary
The authors propose a planning-based approach to explainable AI that centers on three interpretability measures: explicability (ensuring agent behavior is understandable), legibility (ensuring behavior clearly communicates agent intent), and predictability (ensuring behavior can be anticipated by humans). They present two main approaches for generating explicable behavior: model-based methods that leverage the agent's planning model to generate explanations, and model-free methods that learn explicable behavior patterns. The model reconciliation strategy serves as a key mechanism for resolving discrepancies between the AI agent's model and the human's mental model through explanations.

## Key Results
- Introduction of a unified framework for explainable human-AI interaction through planning
- Development of three interpretability measures (explicability, legibility, predictability) as foundational concepts
- Presentation of model-based and model-free approaches for generating explicable behavior
- Introduction of explanations as a model reconciliation strategy for bridging AI and human mental models
- Application of techniques to decision support systems and human-robot interaction scenarios

## Why This Works (Mechanism)
The framework works by aligning AI decision-making processes with human cognitive models through structured explanations. By treating explanations as a form of model reconciliation, the approach addresses the fundamental mismatch between how AI agents and humans represent and reason about the world. The planning perspective ensures that explanations are not just post-hoc justifications but are integrated into the agent's decision-making process from the start.

## Foundational Learning
- **Explicability**: Making AI behavior understandable to humans - needed to ensure humans can follow the agent's reasoning; quick check: can humans accurately describe why the agent took a particular action?
- **Legibility**: Ensuring behavior clearly communicates intent - needed to prevent misinterpretation of agent actions; quick check: do humans correctly infer the agent's goals from its behavior?
- **Predictability**: Enabling anticipation of agent behavior - needed for effective human-AI coordination; quick check: can humans accurately predict the agent's next actions?
- **Model reconciliation**: Resolving differences between AI and human models - needed to bridge the gap between computational and human reasoning; quick check: do explanations successfully update human mental models?
- **Planning-based explanation**: Integrating explanations into decision-making - needed for coherent and timely explanations; quick check: are explanations provided at decision points rather than as afterthoughts?

## Architecture Onboarding
**Component map**: Human Mental Model <- Model Reconciliation -> AI Planning Model -> Action Selection -> Behavior Execution
**Critical path**: Human query/observation → Model reconciliation process → Explanation generation → Human understanding update → Improved collaboration
**Design tradeoffs**: Real-time performance vs. explanation quality, computational complexity vs. explanation comprehensiveness, model accuracy vs. interpretability
**Failure signatures**: Poor explanations lead to misaligned expectations, model reconciliation failures result in persistent misunderstandings, computational overhead causes delayed responses
**First experiments**: 1) Test explicability measures with simple planning tasks, 2) Evaluate model reconciliation effectiveness in controlled scenarios, 3) Compare model-based vs. model-free approaches on benchmark problems

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the available summary, but several areas warrant further investigation including the scalability of explanation methods to complex domains, the effectiveness of model reconciliation in dynamic environments, and the development of standardized metrics for evaluating explanation quality.

## Limitations
- Lack of detailed performance metrics and validation results across diverse domains
- Absence of specific algorithmic implementations and comparative effectiveness analysis
- No concrete case studies or empirical evaluations demonstrating real-world applications

## Confidence
- Theoretical framework and interpretability measures: **High**
- Planning-based approach to explainability: **Medium**
- Model reconciliation strategy: **Medium**
- Application scenarios and empirical validation: **Low**

## Next Checks
1. Request access to empirical evaluation data comparing the effectiveness of model-based versus model-free approaches across multiple domains
2. Examine case studies or user studies demonstrating the practical implementation of model reconciliation in real-time human-AI collaboration scenarios
3. Investigate the computational overhead and scalability of the proposed explainability methods in complex, dynamic environments