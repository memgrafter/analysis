---
ver: rpa2
title: 'Mind the Gap: Examining the Self-Improvement Capabilities of Large Language
  Models'
arxiv_id: '2412.02674'
source_url: https://arxiv.org/abs/2412.02674
tags:
- verification
- arxiv
- self-improvement
- generation
- cot-s
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies self-improvement in LLMs by generating synthetic
  data, verifying its quality, and distilling from filtered generations. The central
  metric is the generation-verification gap (GV-Gap), defined as the performance improvement
  from reweighting generations by self-verification scores.
---

# Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models
## Quick Facts
- arXiv ID: 2412.02674
- Source URL: https://arxiv.org/abs/2412.02674
- Authors: Yuda Song; Hanlin Zhang; Carson Eisenach; Sham Kakade; Dean Foster; Udaya Ghai
- Reference count: 40
- Primary result: Models can self-improve when verification is stable, but performance gains saturate after 2-3 rounds due to declining diversity

## Executive Summary
This work systematically studies self-improvement in large language models through synthetic data generation, verification, and distillation. The central metric is the generation-verification gap (GV-Gap), which measures performance improvement from reweighting generations by self-verification scores. Experiments across multiple model families reveal that stable verification methods enable monotonic scaling of self-improvement capability with model size, while iterative self-improvement faces limitations from declining generation diversity. The findings establish both the potential and fundamental constraints of model self-improvement.

## Method Summary
The study uses a rejection sampling framework where models generate multiple responses per prompt, apply verification methods to score each response, filter based on thresholds, and fine-tune on the filtered data. Verification methods include Multiple Choice (MC), Chain-of-Thought scoring (CoT-Score/Binary), and tournament-based evaluation. Iterative self-improvement involves multiple rounds of this process. The study examines various model families (Qwen, Llama, Yi) and tasks (GSM8K, MATH, Natural Question, Sudoku) with specific sampling parameters (p=0.9, t=0.7, max length=512, 4-shot in-context).

## Key Results
- With stable verification (e.g., CoT-S), relative GV-Gap scales monotonically with pre-training FLOPs
- Cross-verification effectiveness increases with verifier capability and decreases with generator capability
- Iterative self-improvement saturates after 2-3 rounds due to declining generation diversity
- Most models cannot self-improve on tasks beyond their inherent reasoning abilities

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Stable verification enables self-improvement scaling
- Mechanism: When verification methods like Chain-of-Thought-Score remain stable across model sizes, the relative generation-verification gap (GV-Gap) increases monotonically with pre-training FLOPs, indicating improved self-improvement capability
- Core assumption: Verification methods produce consistent, reliable scores across different model scales
- Evidence anchors:
  - [abstract] "With proper verification method (e.g., CoT-S), the relative generation-verification gap scales monotonically with the model pre-training flops"
  - [section] "With certain verification methods (such as CoT-Score), the relative gap grows monotonically with the pre-training flops"
  - [corpus] Weak - no corpus papers directly address this scaling relationship
- Break condition: Verification methods become unstable or produce noisy scores as models scale, breaking the monotonic relationship

### Mechanism 2
- Claim: Cross-verification effectiveness depends on relative model capabilities
- Mechanism: When different models are used for generation and verification, the GV-gap increases with verifier capability and decreases with generator capability, suggesting optimal configurations exist
- Core assumption: Larger models have better verification capabilities than smaller models
- Evidence anchors:
  - [abstract] "in cross-verification (i.e., using different models for generation and verification), the GV-gap increases with verifier capability and decreases with generator capability"
  - [section] "gap(f, g) increases as the model capacity of the verifier model g increases. On the other hand, fix a verifier model g, gap(f, g) decreases as the model capacity of the generator model f increases"
  - [corpus] Weak - corpus contains related papers on self-improvement but none specifically on cross-verification scaling
- Break condition: When verifier and generator capabilities become too similar, the gap disappears and cross-verification becomes ineffective

### Mechanism 3
- Claim: Iterative self-improvement degrades generation diversity over time
- Mechanism: Each iteration of self-improvement filters generations based on verification scores, causing the model to converge on similar answer patterns and reducing effective diversity
- Core assumption: Self-improvement process amplifies common patterns while eliminating rare but correct responses
- Evidence anchors:
  - [abstract] "the effective diversity degrades during the iterative self-improvement"
  - [section] "when k is large, pass@k decreases with the number of iterations, indicating that the diversity of the generations is reduced through the self-improvement process"
  - [corpus] Weak - corpus papers discuss self-improvement but don't specifically address diversity degradation in iterative processes
- Break condition: When diversity drops below a critical threshold, the model can no longer improve and may even degrade in performance

## Foundational Learning
- Concept: Generation-verification gap (GV-Gap)
  - Why needed here: GV-Gap is the central metric for measuring self-improvement capability and understanding when models can improve themselves
  - Quick check question: If a model has 80% accuracy and improves to 85% after verification-based filtering, what is the GV-Gap percentage?

- Concept: Relative generation-verification gap
  - Why needed here: Relative GV-Gap accounts for baseline performance differences across tasks and models, making it more suitable for scaling analysis
  - Quick check question: Why might absolute GV-Gap be misleading when comparing models with different baseline accuracies?

- Concept: Rejection sampling in self-improvement
  - Why needed here: Understanding how filtered datasets are created is crucial for implementing and analyzing self-improvement experiments
  - Quick check question: In rejection sampling, what happens to the weight function when the verification score falls below the threshold?

## Architecture Onboarding
- Component map: Generation -> Verification -> Filtering -> Distillation -> Evaluation
- Critical path:
  1. Generate N responses per prompt using base model
  2. Apply verification method to score each response
  3. Filter responses using threshold-based selection
  4. Fine-tune base model on filtered data
  5. Evaluate improvement on test set
- Design tradeoffs:
  - Generation diversity vs. verification accuracy: Higher temperature increases diversity but may reduce verification reliability
  - Verification method choice: MC is faster but less stable than CoT methods
  - Threshold selection: Higher thresholds yield better quality but smaller training sets
  - Iterative rounds: More rounds may improve performance but reduce diversity
- Failure signatures:
  - Negative or near-zero GV-Gap indicates verification failure
  - Saturation of accuracy improvement across iterations
  - Rapid diversity reduction in generation outputs
  - Inconsistent scaling patterns across model families
- First 3 experiments:
  1. Verify that base model shows non-trivial GV-Gap on GSM8K with CoT-Score verification
  2. Test cross-verification by having smaller model verify larger model's outputs
  3. Run one iteration of self-improvement and measure accuracy change and diversity impact

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the generation-verification gap (GV-Gap) scaling relationship with pre-training FLOPs generalize to other model families and tasks beyond GSM8K and MATH?
- Basis in paper: [explicit] The paper observes that with certain verification methods (like CoT-Score), the relative GV-Gap increases monotonically with pre-training FLOPs for Qwen-1.5, Qwen-2, Llama-2, Llama-3, Llama-3.1, and Yi-1.5 models on GSM8K and MATH tasks.
- Why unresolved: The paper only examines a limited set of model families and tasks. It's unclear if the observed scaling relationship is universal or specific to these models and tasks.
- What evidence would resolve it: Testing the same scaling analysis on a broader range of model families (e.g., other open-source and proprietary models) and diverse tasks (e.g., code generation, reasoning, factual question answering) to determine if the monotonic relationship holds consistently.

### Open Question 2
- Question: What is the optimal verification method configuration (e.g., model size, compute budget) for cross-improvement that balances accuracy gains with computational cost?
- Basis in paper: [explicit] The paper shows that in cross-verification, the GV-Gap increases with verifier capability and decreases with generator capability, but also notes that using the largest model as the verifier might be suboptimal due to computational costs.
- Why unresolved: While the paper identifies trends in how GV-Gap changes with model capacity in cross-verification, it does not determine the compute-optimal configuration that maximizes efficiency.
- What evidence would resolve it: Conducting a systematic study varying verifier model sizes and compute budgets to identify the configuration that achieves the best accuracy-to-compute ratio for cross-improvement across different tasks.

### Open Question 3
- Question: Can the decline in effective diversity during iterative self-improvement be mitigated, and if so, what strategies are most effective?
- Basis in paper: [explicit] The paper observes that effective diversity, measured by pass@k, decreases over iterative self-improvement rounds, potentially due to convergence on incorrect answers, and suggests this as a significant obstacle.
- Why unresolved: The paper identifies the problem of declining diversity but does not explore potential solutions or strategies to counteract this effect.
- What evidence would resolve it: Experimenting with techniques such as diversity-promoting sampling methods, regularization during model updates, or dynamic thresholding to maintain or enhance diversity throughout iterative self-improvement and measuring their impact on sustained performance gains.

## Limitations
- Verification stability across model scales is assumed but not theoretically explained
- The diversity degradation mechanism is observed but not fully characterized in terms of thresholds or solutions
- Cross-verification optimal configurations are identified as trends but not systematically optimized for computational efficiency

## Confidence
- High confidence: GV-Gap definition and measurement, basic iterative self-improvement methodology, inability to self-improve on tasks beyond reasoning capabilities
- Medium confidence: Monotonic scaling relationship between relative GV-Gap and pre-training FLOPs, general trend of diversity degradation during iterative improvement
- Low confidence: Theoretical explanations for verification stability enabling scaling, optimal cross-verification configurations, exact diversity loss thresholds

## Next Checks
1. Test the scaling relationship by comparing GV-Gap across multiple model sizes within the same family on identical tasks, ensuring verification methods maintain consistent scoring behavior across scales.

2. Systematically vary the relative capabilities of generator and verifier models in cross-verification experiments to map the precise relationship between capability differences and gap size.

3. Measure generation diversity (e.g., n-gram diversity, embedding distance) at each iteration of self-improvement to quantify the exact rate and impact of diversity reduction on final performance.