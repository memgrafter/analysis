---
ver: rpa2
title: Transferable Adversarial Attacks against ASR
arxiv_id: '2411.09220'
source_url: https://arxiv.org/abs/2411.09220
tags:
- attacks
- adversarial
- speech
- attack
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the vulnerability of black-box ASR models
  to transferable adversarial attacks, which are crucial for real-world security.
  The authors propose three novel approaches: Speech-Aware Gradient Optimization (SAGO)
  that leverages voice activity detection to focus perturbations on speech segments,
  Momentum Iterative Fast Gradient Sign Method (MI-FGSM) that stabilizes gradient
  updates through momentum terms, and Variance Tuning Momentum Iterative Fast Gradient
  Sign Method (VMI-FGSM) that additionally tunes gradient variance.'
---

# Transferable Adversarial Attacks against ASR

## Quick Facts
- arXiv ID: 2411.09220
- Source URL: https://arxiv.org/abs/2411.09220
- Reference count: 40
- Key outcome: Proposed attacks achieve up to 100% WER in white-box and 20-65% in black-box scenarios while remaining imperceptible

## Executive Summary
This paper addresses the vulnerability of black-box ASR models to transferable adversarial attacks, a critical security concern for real-world applications. The authors propose three novel approaches: Speech-Aware Gradient Optimization (SAGO) that leverages voice activity detection to focus perturbations on speech segments, Momentum Iterative Fast Gradient Sign Method (MI-FGSM) that stabilizes gradient updates through momentum terms, and Variance Tuning Momentum Iterative Fast Gradient Sign Method (VMI-FGSM) that additionally tunes gradient variance. They implement these as time-domain attacks using a differentiable feature extractor. Experiments on five ASR models (Whisper-tiny, Whisper-base, S2T-small, S2T-medium, S2T-large) using two datasets (LJ-Speech and LibriSpeech) show that the proposed attacks significantly outperform baseline noise-based attacks and achieve high word error rates (up to 100% in white-box scenarios, and 20-65% in black-box scenarios), demonstrating their effectiveness and transferability across different models. The adversarial samples remain imperceptible to human listeners while causing severe degradation in ASR performance.

## Method Summary
The authors develop three transferable adversarial attack methods for ASR systems. SAGO uses voice activity detection to focus gradient updates on speech segments, reducing noise in non-speech regions. MI-FGSM incorporates momentum terms to stabilize gradient updates and escape local minima. VMI-FGSM extends MI-FGSM by adding variance tuning to further stabilize the optimization path. All three attacks operate in the time domain using a differentiable feature extractor that converts raw audio to 80-dimensional log filterbank features, enabling end-to-end gradient computation. The attacks are evaluated on five ASR models using LJ-Speech and LibriSpeech datasets, with SNR constraints of 30dB and 35dB to maintain audio quality.

## Key Results
- SAGO, MI-FGSM, and VMI-FGSM significantly outperform baseline PGD and noise-based attacks
- VMI-FGSM achieves the highest transferability with up to 100% WER in white-box and 20-65% in black-box scenarios
- SAGO shows particularly strong performance on S2T models, while MI-FGSM excels on Whisper models
- All attacks maintain audio quality with SNR constraints of 30dB and 35dB

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Voice activity detection (VAD) masking focuses perturbations on speech segments that ASR models prioritize, increasing transferability.
- Mechanism: By computing a VAD mask M that identifies speech portions of the input audio, the attack filters gradient updates to only those regions (M ⊙ ▽ϵLadv). This reduces noise in non-speech segments while emphasizing gradients in speech segments that ASR models depend on most.
- Core assumption: ASR models assign higher importance to speech segments than non-speech segments for transcription accuracy.
- Evidence anchors:
  - [abstract] "which forces mistranscription with minimal impact on human imperceptibility through voice activity detection rule"
  - [section II-B1] "The correlation stems from the fact that the commencement of speech segments within an utterance usually corresponds with the detection of speech initiation by the VAD system"
  - [corpus] Weak evidence - no direct corpus support for VAD importance in ASR models
- Break condition: If ASR model performance is equally sensitive to perturbations in non-speech segments, or if the model uses different segmentation strategies that conflict with VAD output.

### Mechanism 2
- Claim: Momentum-based gradient updates stabilize the optimization path, avoiding poor local minima and improving transferability.
- Mechanism: MI-FGSM adds a cumulative momentum term G to the gradient update (▽ϵLadv + G), while VMI-FGSM further tunes the gradient with variance reduction (V(▽ϵLadv) + G). This smooths the optimization trajectory and helps escape local minima.
- Core assumption: ASR model loss landscapes contain local minima that can trap simple gradient descent, and smoothing the gradient path improves cross-model transferability.
- Evidence anchors:
  - [section II-B2] "By incorporating a momentum term into ASR attacks, our MI-FGSM aims to stabilize update directions, aiding in escaping from local minima during iterations"
  - [section II-B2] "VMI-FGSM additionally tunes the current gradient with the gradient variance in the neighborhood of the previous data point to stabilize the update direction"
  - [corpus] No direct corpus evidence for momentum effectiveness in ASR transfer attacks
- Break condition: If the loss landscape is smooth enough that momentum provides negligible benefit, or if variance tuning introduces instability.

### Mechanism 3
- Claim: Differentiable feature extraction enables time-domain attacks that bypass non-differentiable preprocessing steps, improving gradient flow to the input.
- Mechanism: A differentiable feature extractor converts raw audio to 80-dimensional log filterbank features, allowing gradients to backpropagate all the way to the input audio waveform. This eliminates the need for separate feature-space attacks.
- Core assumption: ASR models can accept features computed by a differentiable extractor as valid input, and the gradient through this extractor accurately reflects how input perturbations affect the final loss.
- Evidence anchors:
  - [section II-C] "we propose to use a differentiable feature extractor to obtain 80-dimension log filterback features for three proposed attacks as ASR model input features"
  - [section II-C] "This enables gradients to backpropagate all the way to the input audio through the feature extractor, facilitating a time-domain attack"
  - [corpus] No corpus evidence for differentiable feature extractor effectiveness in ASR transfer attacks
- Break condition: If the differentiable feature extractor introduces approximation errors that misalign gradients with the true model behavior.

## Foundational Learning

- Concept: Automatic Speech Recognition (ASR) pipeline and loss functions
  - Why needed here: Understanding how ASR models convert audio to text and how adversarial loss functions target transcription errors is essential for designing effective attacks.
  - Quick check question: What components make up a typical ASR model, and how does the loss function measure transcription accuracy?

- Concept: Voice Activity Detection (VAD) and its relationship to speech segmentation
  - Why needed here: VAD is the core mechanism for identifying speech segments that SAGO targets, so understanding how VAD works and its limitations is crucial.
  - Quick check question: How does VAD distinguish between speech and non-speech segments, and what are common failure modes?

- Concept: Gradient-based optimization and adversarial attack methods
  - Why needed here: All three proposed attacks rely on gradient optimization techniques, so understanding projected gradient descent, momentum methods, and variance tuning is essential.
  - Quick check question: How do momentum terms and variance tuning affect gradient optimization convergence and stability?

## Architecture Onboarding

- Component map: Raw audio → Differentiable feature extractor → VAD mask computation → Gradient computation → Perturbation update → Projection → Repeat for N iterations → Adversarial audio output

- Critical path: Raw audio → Differentiable feature extraction → VAD mask computation → Gradient computation → Perturbation update → Projection → Repeat for N iterations → Adversarial audio output

- Design tradeoffs:
  - SAGO vs. MI-FGSM/VMI-FGSM: SAGO targets speech segments specifically but may miss important non-speech context; momentum methods are more general but less targeted
  - Differentiable vs. non-differentiable feature extraction: Differentiable enables end-to-end optimization but may introduce approximation errors
  - SNR constraints: Higher SNR (lower noise) preserves audio quality but may reduce attack effectiveness

- Failure signatures:
  - SAGO fails when VAD mask incorrectly identifies speech segments
  - Momentum methods fail when gradients are already smooth or when variance tuning destabilizes updates
  - Differentiable feature extraction fails when approximation errors accumulate

- First 3 experiments:
  1. Implement SAGO with simple VAD (energy-based) and test on Whisper-tiny with LJ-Speech dataset
  2. Add momentum term to SAGO and compare transfer performance to baseline PGD
  3. Replace differentiable feature extractor with non-differentiable version and measure impact on attack effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SAGO compare to other transferable attack methods across different ASR model architectures beyond the five models tested?
- Basis in paper: [explicit] The paper tests SAGO against five ASR models but suggests its effectiveness may vary across different architectures, particularly noting better performance on S2T models compared to Whisper models.
- Why unresolved: The paper only evaluates SAGO on five specific ASR models, leaving uncertainty about its effectiveness on other architectures or larger models.
- What evidence would resolve it: Testing SAGO on a broader range of ASR models including different architectures, sizes, and training datasets would provide conclusive evidence.

### Open Question 2
- Question: What is the optimal balance between attack strength (WER) and human imperceptibility for real-world applications?
- Basis in paper: [explicit] The paper notes that adversarial samples remain imperceptible to humans while causing severe degradation in ASR performance, but doesn't quantify the perceptual thresholds or trade-offs.
- Why unresolved: The paper demonstrates imperceptibility subjectively but doesn't provide objective measures or explore the trade-off between attack strength and human detection.
- What evidence would resolve it: User studies measuring detection thresholds combined with WER analysis across varying perturbation magnitudes would establish optimal parameters.

### Open Question 3
- Question: How do the proposed transferable attacks perform against defense mechanisms specifically designed for ASR systems?
- Basis in paper: [inferred] The paper focuses on attack effectiveness but doesn't evaluate performance against existing or novel ASR defenses, which is crucial for understanding practical security implications.
- Why unresolved: While the paper demonstrates successful attacks, it doesn't address how these attacks might be mitigated or how robust current defenses are against them.
- What evidence would resolve it: Testing the proposed attacks against various defense mechanisms and measuring performance degradation would provide insights into attack robustness.

## Limitations

- The VAD implementation details are underspecified, with only vague mention of "cepstral power measurement" without exact parameters or validation
- The variance tuning function V(·) in VMI-FGSM is mentioned but not mathematically defined, preventing complete verification
- Claims about human imperceptibility lack supporting evidence - no user studies or perceptual metrics are provided

## Confidence

**High Confidence**: The experimental methodology is sound - using established datasets (LJ-Speech, LibriSpeech), multiple ASR models (Whisper, S2T variants), and appropriate evaluation metrics (WER capped at 100%). The time-domain attack approach with differentiable feature extraction is technically feasible and well-grounded in existing adversarial attack literature.

**Medium Confidence**: The core attack mechanisms (SAGO, MI-FGSM, VMI-FGSM) are conceptually valid and the reported improvements over baselines are plausible. However, implementation details for key components (VAD specifics, variance tuning formula) are missing, preventing complete verification of the methods as described.

**Low Confidence**: The claims about imperceptibility to human listeners lack supporting evidence. While the paper mentions this as a goal, no human studies or perceptual metrics are provided to substantiate this claim. The transferability results across models are promising but the black-box evaluation setup could be more rigorously defined.

## Next Checks

1. **VAD Implementation Validation**: Implement the VAD component with different threshold values and measure its accuracy on the LJ-Speech and LibriSpeech datasets. Compare the resulting speech masks against ground truth speech segments to quantify how VAD errors might impact SAGO's effectiveness.

2. **Variance Tuning Function Implementation**: Define and implement the variance tuning function V(·) based on reasonable assumptions about gradient variance computation in the neighborhood of previous data points. Compare VMI-FGSM performance with and without variance tuning to isolate its contribution to attack effectiveness.

3. **Human Perceptual Evaluation**: Conduct a small-scale human listening test with the generated adversarial examples at both SNR levels (30dB and 35dB). Have listeners rate audio quality and identify any noticeable artifacts to verify the claimed imperceptibility, particularly for the more effective attacks that achieve high WER.