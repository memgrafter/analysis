---
ver: rpa2
title: Estimating the Causal Effects of Natural Logic Features in Transformer-Based
  NLI Models
arxiv_id: '2404.02622'
source_url: https://arxiv.org/abs/2404.02622
tags:
- causal
- which
- context
- effect
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work uses causal analysis to evaluate how well transformer-based
  NLI models use intermediate semantic features like monotonicity and lexical relations.
  By constructing intervention sets over a structured NLI subtask, the authors estimate
  total causal effects (sensitivity to relevant changes) and direct causal effects
  (robustness to irrelevant changes) for context and word-pair features.
---

# Estimating the Causal Effects of Natural Logic Features in Transformer-Based NLI Models

## Quick Facts
- arXiv ID: 2404.02622
- Source URL: https://arxiv.org/abs/2404.02622
- Authors: Julia Rozanova; Marco Valentino; AndrÃ© Freitas
- Reference count: 0
- Primary result: High-performing NLI models often underperform on targeted causal metrics, indicating benchmark accuracy doesn't guarantee proper semantic reasoning

## Executive Summary
This work introduces a causal analysis framework to evaluate how well transformer-based NLI models use intermediate semantic features like monotonicity and lexical relations. By constructing intervention sets over a structured NLI subtask, the authors estimate total causal effects (sensitivity to relevant changes) and direct causal effects (robustness to irrelevant changes) for context and word-pair features. The results reveal that models achieving high accuracy on standard benchmarks often fail to properly leverage desired semantic features, instead exploiting heuristics like word frequency patterns.

Fine-tuning on a specialized HELP dataset improves both robustness to irrelevant surface changes and sensitivity to important semantic changes, though at the cost of some benchmark performance. The study demonstrates that causal analysis provides a more nuanced understanding of model behavior than accuracy metrics alone, highlighting that models may learn spurious correlations rather than genuine semantic reasoning capabilities.

## Method Summary
The authors construct a causal analysis framework using a structured NLI subtask (NLI-XY) where context monotonicity and word-pair relations are explicitly labeled. They create intervention sets by systematically modifying contexts or word pairs while holding other variables constant, then measure how model predictions change to estimate total causal effects (TCE) and direct causal effects (DCE). TCE measures sensitivity to relevant semantic changes, while DCE quantifies robustness to irrelevant surface form changes. Models are evaluated on these causal metrics and compared to standard benchmark performance, with additional experiments using HELP dataset fine-tuning to improve semantic feature utilization.

## Key Results
- Models achieving high benchmark accuracy often show low sensitivity to relevant semantic changes and high sensitivity to irrelevant surface changes
- Fine-tuning on HELP dataset improves robustness to irrelevant word-pair changes and sensitivity to context monotonicity changes
- Benchmark performance often trades off against proper semantic reasoning capability
- Models exhibit biases favoring upward-monotone contexts and ignoring negation markers in downward-monotone contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models exhibit bias in favor of upward-monotone contexts due to over-reliance on lexical heuristics
- Mechanism: The causal diagram shows textual surface form (S) has a direct path to model prediction (Y), bypassing intended intermediate variable M (monotonicity), allowing models to use word frequency patterns
- Core assumption: Surface form is spuriously correlated with labels in training data
- Evidence anchors: [abstract] "reinforces previously suspected biases from a causal perspective, including biases in favour of upward-monotone contexts and ignoring the effects of negation markers" and [section] "models may end up using accidental correlations output labels, such as the frequency of certain words in a corpus"

### Mechanism 2
- Claim: Direct causal effects (DCE) quantify model robustness to irrelevant changes in input surface form
- Mechanism: Interventions change surface form while keeping semantic feature constant; DCE measures prediction changes when they shouldn't occur
- Core assumption: Semantic features are only relevant variables; surface form changes causing prediction changes indicate failure
- Evidence anchors: [section] "quantify instead the direct impact of irrelevant surface changes... and compare them to total causal effects" and [section] "The lower this DCE, the higher the model robustness to irrelevant word pair surface form changes"

### Mechanism 3
- Claim: Total causal effects (TCE) measure model sensitivity to relevant semantic changes
- Mechanism: Interventions change semantic feature while keeping surface form constant; TCE measures prediction changes when semantic feature changes
- Core assumption: Model should only change prediction when semantic feature changes, not due to surface form
- Evidence anchors: [section] "The higher the specified TCE, the greater the model's sensitivity to word pair insertion changes affecting the gold label"

## Foundational Learning

- Concept: Causal diagrams and do-calculus
  - Why needed here: The causal diagram identifies confounders and desired vs. undesired reasoning paths
  - Quick check question: What is the difference between a confounder and a mediator in a causal diagram?

- Concept: Intervention sets and counterfactuals
  - Why needed here: Intervention sets systematically change input variables to estimate causal effects by observing model prediction changes
  - Quick check question: How does an intervention set differ from a simple dataset split?

- Concept: Natural logic and monotonicity
  - Why needed here: NLI subtask uses natural logic where context monotonicity and word-pair relations determine entailment
  - Quick check question: What is the difference between upward and downward monotonicity in natural logic?

## Architecture Onboarding

- Component map: NLI-XY dataset -> Intervention generator -> Transformer models -> Effect estimator -> Benchmark comparator

- Critical path: 1. Load NLI-XY dataset 2. Generate intervention sets (I0-I3) 3. Run models on intervention sets 4. Compute TCE and DCE 5. Compare to benchmark scores

- Design tradeoffs: Structured subtask vs. full NLI task (simpler interventions but may not generalize), HELP fine-tuning (improves robustness but hurts benchmarks), behavioral vs. internal probing measures

- Failure signatures: Low TCE (ignores semantic features), High DCE (relies on surface form cues), Low TCE and high DCE (uses heuristics and ignores semantic features)

- First 3 experiments: 1. Compare TCE and DCE for baseline model on NLI-XY dataset 2. Fine-tune model on HELP dataset and recompute TCE and DCE 3. Analyze differences between baseline and HELP-fine-tuned models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does HELP dataset's improvement in context monotonicity reasoning translate to other semantic features beyond word-pair relations?
- Basis in paper: [explicit] Authors note HELP improves both context monotonicity handling and robustness to irrelevant word-pair changes, but question whether this extends to other semantic features
- Why unresolved: Study focuses specifically on monotonicity and word-pair relations without exploring generalization to other semantic reasoning tasks
- What evidence would resolve it: Testing models fine-tuned on HELP across broader semantic reasoning benchmarks would show whether improvements are feature-specific or general

### Open Question 2
- Question: Are observed biases in NLI models (e.g., favoring upward-monotone contexts) causal or merely correlational?
- Basis in paper: [inferred] Authors identify model biases like ignoring negation in downward-monotone contexts, but causal analysis quantifies effects without distinguishing causal mechanisms from spurious correlations
- Why unresolved: Causal diagrams show feature effects but don't distinguish whether biases stem from learned causal structures or dataset artifacts
- What evidence would resolve it: Controlled experiments varying context monotonicity independently of other features could reveal whether biases persist when confounding factors are removed

### Open Question 3
- Question: How do different model architectures (e.g., Transformers vs. other neural models) differ in sensitivity to irrelevant surface changes?
- Basis in paper: [explicit] Authors compare BERT-like models but don't explore whether architecture choice affects direct causal effects
- Why unresolved: Study focuses on transformer-based models, leaving open whether architectural differences influence robustness to irrelevant changes
- What evidence would resolve it: Comparing DCEs across diverse architectures (LSTMs, CNNs, or symbolic models) would reveal whether transformers are uniquely susceptible to surface-form heuristics

## Limitations
- Causal analysis framework relies on strong assumptions about independence of semantic features and surface forms in intervention sets
- Generalizability from structured NLI-XY subtask to full NLI tasks remains unclear
- Limited exploration of alternative fine-tuning strategies beyond HELP dataset

## Confidence
- High confidence: Core finding that high benchmark accuracy doesn't guarantee proper semantic reasoning
- Medium confidence: Effectiveness of HELP dataset fine-tuning for improving robustness and sensitivity
- Low confidence: Claim that all high benchmark performance stems from heuristic exploitation rather than true semantic understanding

## Next Checks
1. Test whether semantic features (monotonicity, lexical relations) and surface forms are independent in NLI-XY dataset by measuring their correlation across intervention sets
2. Apply TCE/DCE framework to subset of full NLI datasets (SNLI, MNLI) to assess generalization beyond structured subtask
3. Compare HELP dataset fine-tuning to alternative methods (adversarial training, data augmentation) to determine if similar improvements can be achieved through different approaches