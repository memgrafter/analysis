---
ver: rpa2
title: 'Encoder vs Decoder: Comparative Analysis of Encoder and Decoder Language Models
  on Multilingual NLU Tasks'
arxiv_id: '2406.13469'
source_url: https://arxiv.org/abs/2406.13469
tags:
- language
- decoder
- datasets
- text
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the ScandEval benchmark to evaluate both encoder
  and decoder language models on multilingual NLU tasks across eight Germanic languages.
  The authors introduce a few-shot evaluation methodology for decoder models and implement
  a mean rank score aggregation method that accounts for statistical significance
  and magnitude differences between models.
---

# Encoder vs Decoder: Comparative Analysis of Encoder and Decoder Language Models on Multilingual NLU Tasks

## Quick Facts
- arXiv ID: 2406.13469
- Source URL: https://arxiv.org/abs/2406.13469
- Reference count: 25
- Primary result: Encoder models achieve significantly better NLU performance than decoder models despite having orders of magnitude fewer parameters

## Executive Summary
This paper extends the ScandEval benchmark to evaluate both encoder and decoder language models on multilingual NLU tasks across eight Germanic languages. The authors introduce a few-shot evaluation methodology for decoder models and implement a mean rank score aggregation method that accounts for statistical significance and magnitude differences between models. Through experiments on named entity recognition, sentiment classification, linguistic acceptability, and question answering tasks, they find that encoder models achieve significantly better overall NLU performance than decoder models despite having orders of magnitude fewer parameters.

## Method Summary
The study evaluates encoder and decoder language models on multilingual NLU tasks using a few-shot evaluation methodology. Encoder models use traditional training on task-specific datasets, while decoder models are evaluated using few-shot examples (8 for NER, 12 for sentiment and linguistic acceptability, 4 for QA) with 10 iterations and bootstrapped test splits. Results are aggregated using a mean rank score method that applies one-tailed Welch's t-test for statistical significance and standard deviation normalization to preserve magnitude differences across heterogeneous datasets and languages.

## Key Results
- Encoder models achieve significantly better NLU performance than decoder models despite having orders of magnitude fewer parameters
- Decoder models show strong bias toward question answering tasks, following a different performance distribution pattern in UMAP space
- Mean rank score aggregation method provides robust comparison across tasks and languages while preserving magnitude differences

## Why This Works (Mechanism)

### Mechanism 1
Encoder models use bidirectional attention and masked language modeling objectives that capture richer contextual information for downstream tasks, while decoder models are optimized for autoregressive generation. The evaluation methodology accurately measures NLU performance and the few-shot evaluation framework provides fair comparison between architectures. Break condition: If the few-shot evaluation methodology introduces systematic bias or the NLU tasks don't represent full language understanding capabilities.

### Mechanism 2
The autoregressive pre-training objective of decoder models creates representations suited for next-token prediction in question answering contexts, leading to better performance on these tasks. The architecture-specific task preferences observed in UMAP analysis reflect fundamental differences in how encoders and decoders represent linguistic information. Break condition: If the observed bias is actually due to instruction-tuning effects or UMAP analysis doesn't capture meaningful task relationships.

### Mechanism 3
Mean rank score aggregation uses normalized rank scores with statistical significance testing to account for different metric ranges and variances while preserving magnitude differences between models. The statistical tests and normalization factors provide meaningful comparisons across heterogeneous datasets and languages. Break condition: If normalization factors vary too widely making comparisons meaningless, or if statistical tests are too conservative.

## Foundational Learning

- Concept: Few-shot evaluation methodology
  - Why needed here: Allows fair comparison between encoder and decoder models by treating few-shot examples analogously to training examples for encoder models
  - Quick check question: How does the few-shot approach account for differences in how encoder and decoder models are typically evaluated?

- Concept: UMAP dimensionality reduction
  - Why needed here: Reveals architecture-specific patterns in model performance across different tasks by capturing both global and local structure
  - Quick check question: What does it mean when encoder and decoder models follow different "paths" in UMAP space?

- Concept: Statistical significance testing in model comparison
  - Why needed here: Ensures that reported performance differences are meaningful and not due to random variation in evaluation
  - Quick check question: Why use Welch's t-test instead of a standard t-test for comparing model performances?

## Architecture Onboarding

- Component map: ScandEval framework consists of dataset loading modules for 8 Germanic languages, prompt generation templates for converting NLU tasks to text-to-text format, evaluation runners that handle both encoder and decoder models, and aggregation modules that compute mean rank scores with statistical testing
- Critical path: Data loading → Prompt generation → Model inference (few-shot) → Score aggregation → Leaderboard generation
- Design tradeoffs: Few-shot evaluation provides fair comparison but requires careful prompt engineering; mean rank score preserves magnitude differences but adds computational overhead; supporting both architectures requires handling different output formats
- Failure signatures: Inconsistent performance across languages suggests dataset quality issues; large variance in few-shot scores indicates prompt sensitivity; extreme rank scores suggest statistical test problems
- First 3 experiments:
  1. Run encoder and decoder models on a single dataset with varying few-shot examples to validate the evaluation methodology
  2. Compare mean rank score aggregation with simpler methods (mean score, mean rank) on synthetic data with known differences
  3. Test UMAP analysis on a small set of models to verify it reveals meaningful task relationships before scaling to full leaderboard

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the architectural differences between encoder and decoder models lead to the observed task-specific performance differences, particularly for question answering tasks?
- Basis in paper: [explicit] The paper observes that decoder models show strong bias toward question answering tasks even without instruction tuning, and that decoder models follow a different performance distribution pattern in UMAP space compared to encoder models.
- Why unresolved: The paper identifies the phenomenon but does not investigate the underlying architectural reasons for why decoder models excel at question answering while performing poorly on NER and linguistic acceptability tasks.
- What evidence would resolve it: Comparative analysis of attention mechanisms, positional encoding effects, or pre-training objectives that explain the architectural basis for these task-specific biases.

### Open Question 2
- Question: What causes the anomalous performance of decoder models on the English SST5 sentiment classification dataset compared to other sentiment classification tasks?
- Basis in paper: [explicit] The paper notes that generative models perform substantially better on the English SST5 dataset compared to other sentiment classification datasets, and speculates about possible dataset leakage or ease differences.
- Why unresolved: The paper identifies the discrepancy but does not conduct a systematic investigation into whether this is due to dataset leakage, differences in task difficulty, or other factors.
- What evidence would resolve it: Analysis of pre-training data overlap with SST5 test data, comparative difficulty assessment across sentiment datasets, or ablation studies on dataset characteristics.

### Open Question 3
- Question: How does the mean rank score aggregation method compare to other ranking-based metrics in terms of stability and sensitivity to new model additions across different leaderboard sizes?
- Basis in paper: [inferred] The paper introduces a novel mean rank score method that addresses limitations of existing aggregation methods, but does not compare its performance characteristics against alternatives.
- Why unresolved: The paper validates the method's properties but does not empirically compare its behavior to other ranking metrics under various conditions.
- What evidence would resolve it: Controlled experiments adding models to leaderboards of varying sizes, sensitivity analysis of score changes, and comparison with established ranking aggregation methods.

## Limitations
- Weak direct support from corpus evidence for specific mechanisms proposed
- Single-dimensional UMAP projections may oversimplify complex architecture-task relationships
- Mean rank score method's practical effectiveness depends on assumptions about score distributions

## Confidence
- Encoder vs decoder performance comparison (High): Experimental methodology clearly specified with reproducible steps
- Architecture-specific task preferences (Medium): UMAP analysis reveals patterns but interpretation relies on single-dimensional projections
- Mean rank score aggregation method (Medium): Theoretical justification sound but depends on statistical test behavior assumptions

## Next Checks
1. Replicate the UMAP analysis on a subset of models and tasks to verify that architecture-specific "paths" emerge consistently and meaningfully relate to known task characteristics
2. Test the sensitivity of few-shot evaluation scores to prompt engineering variations to quantify the robustness of the fair comparison methodology
3. Compare mean rank score aggregation results with alternative methods (simple mean scores, mean ranks without magnitude preservation) on controlled synthetic datasets to validate the claimed advantages