---
ver: rpa2
title: 'Toward Informal Language Processing: Knowledge of Slang in Large Language
  Models'
arxiv_id: '2404.02323'
source_url: https://arxiv.org/abs/2404.02323
tags:
- slang
- language
- sentences
- drama
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the evaluation of large language models' (LLMs)
  knowledge of slang, an informal language form common in daily conversations and
  online media. The authors construct a high-quality, publicly accessible dataset
  from movie subtitles, containing slang usages with annotations for definitions,
  paraphrases, and demographic sources.
---

# Toward Informal Language Processing: Knowledge of Slang in Large Language Models

## Quick Facts
- **arXiv ID**: 2404.02323
- **Source URL**: https://arxiv.org/abs/2404.02323
- **Reference count**: 40
- **Primary result**: Evaluates LLMs on slang detection and demographic source identification using a new dataset

## Executive Summary
This paper addresses the evaluation of large language models' (LLMs) knowledge of slang, an informal language form common in daily conversations and online media. The authors construct a high-quality, publicly accessible dataset from movie subtitles, containing slang usages with annotations for definitions, paraphrases, and demographic sources. They evaluate LLMs on two core tasks: slang detection and identification of regional and historical sources of slang. The study finds that while GPT models like GPT-4 perform well in zero-shot settings, smaller BERT-like models finetuned on the dataset achieve comparable performance. Finetuning GPT-3.5 further improves results, surpassing strong zero-shot baselines. The dataset enables both evaluation and finetuning, serving as a valuable benchmark for informal language processing.

## Method Summary
The authors create the OpenSubtitles-Slang dataset from movie subtitles, containing 7,488 slang sentences and 17,512 non-slang sentences with annotations for definitions, paraphrases, and demographic sources (region, age). They evaluate BERT, RoBERTa, XLNet, and GPT models on slang detection (sentence/word level) and source identification tasks. BERT-like models are finetuned on the dataset, while GPT models are evaluated in zero-shot and finetuned settings using prompt engineering. Performance is measured using F1 scores for detection, accuracy for classification, and probability ratios for semantic analysis.

## Key Results
- Fine-tuned GPT-3.5 outperforms zero-shot GPT-4 on slang detection (90.1 F1 vs. 84.0 F1 sentence-level)
- BERT-like models finetuned on the dataset achieve comparable performance to GPT models
- GPT models encode demographic knowledge of slang through distributional patterns in pretraining data
- BERT and RoBERTa outperform XLNet on slang detection, likely due to bidirectional context modeling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Slang detection accuracy increases when models are fine-tuned on task-specific data rather than relying on zero-shot inference alone.
- **Mechanism**: The fine-tuning process allows the model to adjust its weights to better distinguish slang-specific linguistic features (e.g., part-of-speech shifts, semantic nuance) that are not captured in general pretraining.
- **Core assumption**: Slang tokens carry distinct distributional and contextual patterns that are not sufficiently represented in the pretraining corpus.
- **Evidence anchors**:
  - [abstract]: "smaller BERT-like models finetuned on our dataset achieve comparable performance" and "finetuning GPT-3.5 further improves results, surpassing strong zero-shot baselines."
  - [section]: Table 2 shows that GPT-3.5 finetuned outperforms its zero-shot counterpart in both sentence-level (90.1 F1 vs. 84.0 F1) and word-level (77.8 F1 vs. 64.5 F1) slang detection.
  - [corpus]: Weak, as the paper does not directly compare internal representation shifts before and after fine-tuning.
- **Break condition**: If the pretraining corpus already contains enough slang examples with the same demographic and contextual distribution as the test set, fine-tuning may yield marginal gains.

### Mechanism 2
- **Claim**: Zero-shot GPT models encode slang demographic knowledge (region, age) through learned distributional patterns.
- **Mechanism**: During large-scale pretraining, the model encounters slang in varied linguistic contexts and implicitly learns associations between slang usage and demographic indicators (e.g., region-specific vocabulary, era-specific terms).
- **Core assumption**: Slang usage co-occurs predictably with demographic markers in the pretraining data, enabling zero-shot demographic inference.
- **Evidence anchors**:
  - [abstract]: "GPT-4 achieve good performance in a zero-shot setting" for identifying regional and historical sources of slang.
  - [section]: Figure 4 shows GPT-3.5 finetuned achieves the highest accuracy but GPT-4 zero-shot performs competitively, indicating pretrained knowledge.
  - [corpus]: Weak; the paper does not detail the demographic distribution of slang in pretraining data.
- **Break condition**: If slang usage is sparse or poorly annotated in pretraining data, zero-shot demographic inference will degrade.

### Mechanism 3
- **Claim**: BERT-like models perform better on slang detection than XLNet, likely due to architectural differences in handling informal language.
- **Mechanism**: BERT and RoBERTa's bidirectional context modeling better captures the semantic contrast between slang and literal terms in natural sentences, while XLNet's autoregressive approach may struggle with such nuances.
- **Core assumption**: Slang meanings often depend on bidirectional context (e.g., "blazing" meaning "excellent" vs. "burning"), which is better captured by masked language models.
- **Evidence anchors**:
  - [section]: Table 2 shows BERT (81.6 F1) and RoBERTa (84.2 F1) outperform XLNet (64.6 F1) in sentence-level slang detection.
  - [abstract]: Not explicitly mentioned; inferred from results.
  - [corpus]: Weak; no ablation of architectural components is provided.
- **Break condition**: If slang meanings are more context-independent or if the dataset favors left-to-right contextual dependencies, XLNet might close the gap.

## Foundational Learning

- **Concept**: Zero-shot vs. fine-tuned model performance
  - Why needed here: To understand the relative importance of task-specific adaptation versus pretrained knowledge in informal language processing.
  - Quick check question: If a model achieves high zero-shot accuracy on slang detection, does it still benefit from fine-tuning? (Answer: Yes, as shown by GPT-3.5 finetuned outperforming its zero-shot counterpart.)

- **Concept**: Demographic signal extraction from text
  - Why needed here: Slang is tightly coupled with user identity (region, age), so models must learn to associate linguistic cues with demographic metadata.
  - Quick check question: How might a model infer a speaker's region from the use of "blazing" vs. "excellent"? (Answer: By learning co-occurrence patterns of slang terms with region-specific contexts in pretraining data.)

- **Concept**: Distributional semantics and token probability
  - Why needed here: Probing model confidence in slang vs. literal paraphrases reveals whether models treat slang as a distinct semantic category or as a rare literal sense.
  - Quick check question: If a model assigns higher probability to "blazing" than "excellent" in the same context, what does that indicate? (Answer: The model is more confident in predicting slang usage, suggesting learned distributional knowledge.)

## Architecture Onboarding

- **Component map**: OpenSubtitles-Slang dataset -> BERT/RoBERTa/XLNet/GPT models -> Slang detection and source identification tasks -> F1 scores, accuracy, probability ratios

- **Critical path**:
  1. Load and preprocess slang dataset with annotations (definitions, paraphrases, metadata).
  2. For BERT-like models: Implement edge probing classifiers; for GPT models: Construct prompts for zero-shot inference.
  3. Train/finetune models on slang detection and source identification tasks.
  4. Evaluate performance on held-out test set; analyze demographic disparities.
  5. Probe token probabilities and embeddings to interpret semantic knowledge.

- **Design tradeoffs**:
  - Masked LMs (BERT/RoBERTa) vs. Autoregressive LMs (GPT): Masked LMs better capture bidirectional context but cannot generate text; GPT can generate but may lack fine-grained context understanding.
  - Zero-shot vs. fine-tuned: Zero-shot leverages pretraining but may miss task-specific cues; fine-tuning improves accuracy but requires labeled data.
  - Tokenization: Single-token slang terms yield cleaner probability comparisons; multi-token slang complicates interpretation.

- **Failure signatures**:
  - Low precision in word-level detection: Model struggles with sequence labeling or confuses slang with rare words.
  - High variance in demographic accuracy: Dataset imbalance or weak demographic signal in pretraining data.
  - Inconsistent probability ratios: Tokenization artifacts or model instability in generating slang vs. literal terms.

- **First 3 experiments**:
  1. **Zero-shot slang detection with GPT-4**: Prompt GPT-4 to classify sentences as containing slang or not; measure accuracy and compare with BERT/RoBERTa fine-tuned baselines.
  2. **Fine-tuning GPT-3.5 on slang detection**: Train GPT-3.5 on OpenSubtitles-Slang dataset; evaluate improvement over zero-shot performance and analyze failure cases.
  3. **Model confidence probing**: For a subset of slang terms, compare GPT-3's token probabilities for slang vs. literal paraphrases; analyze discrepancies by region and time period.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GPT-4's lack of finetuning access significantly limit our understanding of slang knowledge across all large language models?
- Basis in paper: [explicit] "the lack of access to internal layers of GPT hinders the comparison of intermediate representations in LLMs" and "we can only analyze probability values from GPT-3 as OpenAI no longer provides access to those values in newer generation models like GPT-3.5 and GPT-4"
- Why unresolved: The paper can only analyze GPT-4's zero-shot performance without finetuning, preventing comparison of how finetuning affects slang knowledge across different model generations.
- What evidence would resolve it: Direct access to finetuning GPT-4 with the OpenSub-Slang dataset, allowing comparison of pre-trained vs finetuned performance on slang detection and source identification tasks.

### Open Question 2
- Question: How do other demographic factors beyond region and age (such as gender, ethnicity, or socioeconomic status) affect slang detection and source identification performance?
- Basis in paper: [inferred] The paper notes "slang is highly reflective of a user's social identity" and discusses fairness implications, but only evaluates region and age demographics.
- Why unresolved: The study only considers region and age as demographic variables, leaving unexplored how other social identity factors that influence slang usage might affect model performance.
- What evidence would resolve it: Extending the OpenSub-Slang dataset with annotations for additional demographic factors and evaluating model performance across these new dimensions.

### Open Question 3
- Question: Does the frequency of slang exposure during pre-training primarily drive GPT models' superior slang knowledge compared to BERT-like models?
- Basis in paper: [explicit] "GPT models are no more biased compared to earlier BERT-based models" and the finding that GPT-3 assigns higher probabilities to slang, suggesting it treats slang as additional conventional senses.
- Why unresolved: The paper observes GPT's better performance but doesn't definitively establish whether this stems from frequency-based learning or deeper semantic understanding of slang.
- What evidence would resolve it: Controlled experiments manipulating slang frequency in training data while keeping semantic content constant, then measuring downstream slang processing performance.

## Limitations

- The study does not directly compare internal representation shifts before and after fine-tuning, making it difficult to attribute performance gains to specific mechanisms.
- The demographic distribution of slang in pretraining data is not detailed, limiting confidence in zero-shot demographic inference claims.
- The paper lacks ablation experiments on architectural components, particularly for understanding why BERT-like models outperform XLNet on slang detection.

## Confidence

**High Confidence**: The finding that fine-tuning improves slang detection performance over zero-shot baselines is well-supported by direct comparisons in Table 2 showing GPT-3.5 finetuned outperforming zero-shot on both sentence-level (90.1 F1 vs. 84.0 F1) and word-level (77.8 F1 vs. 64.5 F1) tasks.

**Medium Confidence**: The claim that zero-shot GPT models encode slang demographic knowledge through learned distributional patterns is plausible but not directly verified. While GPT-4 shows competitive zero-shot performance on source identification (Figure 4), the paper doesn't analyze the demographic distribution of slang in pretraining data.

**Low Confidence**: The assertion that BERT-like models perform better than XLNet specifically due to architectural differences in handling informal language lacks direct evidence. The paper shows performance differences (BERT 81.6 F1 vs. XLNet 64.6 F1) but doesn't conduct ablation studies or architectural analysis to support this claim.

## Next Checks

1. **Analyze pretraining data demographics**: Examine the distribution of slang terms across regions and time periods in the pretraining corpora to verify whether zero-shot demographic inference is supported by actual data patterns.

2. **Conduct architectural ablation study**: Compare BERT, RoBERTa, and XLNet performance while controlling for model size, training data, and fine-tuning procedures to isolate the effect of bidirectional context modeling on slang detection.

3. **Track representation changes during fine-tuning**: Use probing classifiers or embedding similarity measures to quantify how slang-related representations evolve during fine-tuning compared to pretraining, providing direct evidence for the mechanism of improvement.