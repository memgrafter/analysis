---
ver: rpa2
title: Efficient user history modeling with amortized inference for deep learning
  recommendation models
arxiv_id: '2412.06924'
source_url: https://arxiv.org/abs/2412.06924
tags:
- inference
- user
- history
- amortized
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the high latency cost of Transformer-based
  user history modeling in deep learning recommendation systems (DLRM). The authors
  compare two early fusion methods for integrating candidate items into user history
  sequences: concatenating the candidate to each history item versus appending it
  to the end of the sequence.'
---

# Efficient user history modeling with amortized inference for deep learning recommendation models

## Quick Facts
- arXiv ID: 2412.06924
- Source URL: https://arxiv.org/abs/2412.06924
- Authors: Lars Hertel; Neil Daftary; Fedor Borisyuk; Aman Gupta; Rahul Mazumder
- Reference count: 17
- One-line primary result: Appending candidate items to user history sequences with cross-attention enables amortized inference, reducing latency by 30% in LinkedIn's Feed and Ads systems while maintaining prediction accuracy comparable to concatenation methods.

## Executive Summary
This paper addresses the computational inefficiency of Transformer-based user history modeling in deep learning recommendation systems (DLRM). The authors propose a method that appends candidate items to the end of user history sequences rather than concatenating them to each history item, enabling a reformulation of amortized inference algorithms. This approach maintains similar prediction accuracy to concatenation while significantly reducing inference latency through processing all candidates simultaneously rather than individually. The method is validated on both public datasets and production LinkedIn systems, demonstrating practical efficiency gains without sacrificing recommendation quality.

## Method Summary
The authors compare two early fusion methods for integrating candidate items into user history sequences: concatenation (pairing each history item with the candidate) versus appending (adding candidates to the sequence end). They show that appending with cross-attention performs comparably to concatenation while enabling amortized inference through a reformulation of the M-FALCON algorithm. This reformulation allows processing all m candidates simultaneously by reshaping the computation from m × (n + 1) × d to 1 × (n + m) × d format, reducing FLOPS complexity and achieving 30% latency reduction in production systems.

## Key Results
- Appending with cross-attention achieves comparable engagement prediction accuracy to concatenation on MovieLens 20M, Amazon Books, Goodbooks, and Netflix datasets
- Amortized inference reduces latency by 30% compared to non-amortized inference in LinkedIn's Feed and Ads ranking systems
- The computational complexity is reduced from O(ℓmnd² + ℓmn²d) to O(ℓ(n+m)d² + ℓ(n+m)²d), with greater benefits as sequence length increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention in the appending method prevents the candidate item from attending to history items, enabling amortized inference.
- Mechanism: By separating the candidate's query projections (W_q [H1,...,Hn,C]) from the history's key/value projections (W_k [H1,...,Hn], W_v [H1,...,Hn]), the candidate cannot attend to other candidates or history items can attend to the candidate but not vice versa.
- Core assumption: The cross-attention formulation preserves the necessary information flow for accurate predictions while enabling the mathematical rearrangement needed for amortization.
- Evidence anchors:
  - [abstract]: "Using the latter method, allows us to reformulate the recently proposed amortized history inference algorithm M-FALCON [13] for the case of DLRM models."
  - [section]: "Under cross-attention described in Equation (3), the candidate outputs [C1,...,Cm] are equivalent to those from regular inference."
- Break condition: If the cross-attention formulation fails to preserve prediction accuracy compared to concatenation, or if the mathematical equivalence between regular and amortized inference breaks down.

### Mechanism 2
- Claim: Amortized inference reduces computational complexity by processing all candidates simultaneously rather than separately.
- Mechanism: Instead of processing m candidates individually (each requiring n history computations), all m candidates are appended to the history sequence and processed once, reducing FLOPS from O(ℓmnd² + ℓmn²d) to O(ℓ(n+m)d² + ℓ(n+m)²d).
- Core assumption: The Transformer architecture can process variable-length sequences efficiently, and the reshaping operations preserve the necessary information structure.
- Evidence anchors:
  - [abstract]: "amortization reduces latency by 30% compared to non-amortized inference."
  - [section]: "We can see that n + m < nm for any numbers larger than two. Furthermore, while the ratio of the two is constant as ℓ grows, it increases linearly as n grows."
- Break condition: If the overhead of reshaping operations or parallel processing negates the theoretical complexity savings, or if the increased sequence length causes memory or performance issues.

### Mechanism 3
- Claim: The appending method with cross-attention learns different attention patterns compared to concatenation, potentially leading to better generalization.
- Mechanism: Concatenation creates pairwise attention between each history item and the candidate (similar to DIN), while appending with cross-attention allows history items to attend to each other and the candidate separately, enabling information propagation across sequence steps.
- Core assumption: The different attention patterns learned by each method result in complementary strengths, with appending potentially better capturing sequential dependencies.
- Evidence anchors:
  - [section]: "The attention activations for appending (Figure 2 bottom) show different attention patterns for every query index with a diagonal indicating items attending to themselves. This indicates that information is propagated across sequence steps."
  - [section]: "In this case the model does not necessarily need to communicate information across sequence steps."
- Break condition: If the different attention patterns learned by appending do not provide any performance advantage, or if the sequential dependencies captured are not relevant to the recommendation task.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how Transformers process sequences and how attention works is crucial for grasping the difference between concatenation and appending methods.
  - Quick check question: How does multi-head attention compute attention scores, and what is the role of query, key, and value projections?

- Concept: Amortized inference and its relationship to computational complexity
  - Why needed here: The core contribution of this paper is leveraging amortized inference to reduce latency, which requires understanding the computational trade-offs involved.
  - Quick check question: Given a sequence length n and m candidates, what is the theoretical reduction in FLOPS when using amortized inference instead of regular inference?

- Concept: Early fusion in recommendation systems
  - Why needed here: Early fusion is the mechanism by which candidate items are integrated with user history, and understanding its variants is key to comparing concatenation vs. appending.
  - Quick check question: What is the difference between early fusion and late fusion in recommendation systems, and why might early fusion be preferred for certain tasks?

## Architecture Onboarding

- Component map: User history encoder (Transformer) → MLP → Prediction layer. Candidate items can be concatenated to each history item or appended to the sequence. Cross-attention enables amortized inference.
- Critical path: User history sequence → Transformer encoding → Candidate-specific output → MLP prediction. The critical path is the Transformer encoding, which is optimized through amortization.
- Design tradeoffs: Concatenation may learn pairwise interactions better but doesn't enable amortization. Appending with cross-attention enables amortization but may require different hyperparameter tuning.
- Failure signatures: Performance degradation when using amortization (indicating the mathematical equivalence doesn't hold), increased latency despite amortization (indicating overhead issues), or memory errors with long sequences.
- First 3 experiments:
  1. Compare prediction accuracy of concatenation vs. appending with cross-attention on a small public dataset.
  2. Benchmark latency of regular vs. amortized inference with varying sequence lengths and candidate counts.
  3. Analyze attention patterns of both methods on example sequences to understand what each learns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does appending with cross-attention consistently outperform concatenating across all recommendation domains and sequence lengths?
- Basis in paper: [explicit] The paper shows that appending with cross-attention performs comparably to concatenating on four public datasets and two internal datasets (LinkedIn Feed and Ads), but does not test across a broader range of domains or longer sequence lengths.
- Why unresolved: The experiments were limited to specific datasets and sequence lengths. Different recommendation domains (e.g., music, travel) and longer sequences might reveal performance differences not captured in the current study.
- What evidence would resolve it: Systematic experiments across diverse recommendation domains with varying sequence lengths, comparing both methods on multiple engagement metrics.

### Open Question 2
- Question: What is the optimal balance between sequence length and the number of candidates (m) for maximizing the benefits of amortized inference?
- Basis in paper: [inferred] The theoretical analysis shows benefits increase with sequence length, and the CPU benchmark shows benefits at m=512, but the paper doesn't systematically explore the trade-off between these parameters.
- Why unresolved: The relationship between sequence length, number of candidates, and amortized inference benefits wasn't thoroughly explored. The optimal configuration may vary based on hardware constraints and use case.
- What evidence would resolve it: Empirical studies varying both sequence length and candidate count across different hardware configurations, measuring latency, memory usage, and prediction accuracy.

### Open Question 3
- Question: How do different attention mechanisms (beyond cross-attention) affect the performance of appending-based early fusion and its compatibility with amortized inference?
- Basis in paper: [explicit] The paper uses cross-attention for appending to enable amortized inference, but doesn't explore alternative attention mechanisms like multi-query attention or sparse attention.
- Why unresolved: Cross-attention was chosen for compatibility with amortized inference, but other attention mechanisms might offer better performance or efficiency trade-offs.
- What evidence would resolve it: Comparative experiments testing various attention mechanisms (multi-query, sparse, linear, etc.) with appending-based early fusion, measuring both prediction accuracy and inference efficiency.

### Open Question 4
- Question: What are the theoretical limitations of amortized inference when the candidate set becomes very large (e.g., thousands of items)?
- Basis in paper: [inferred] The theoretical complexity analysis assumes a fixed number of candidates, and the benchmarks use m=512, but don't explore scenarios with very large candidate sets.
- Why unresolved: The paper demonstrates benefits at moderate candidate counts but doesn't address scalability limits or when regular inference might become preferable.
- What evidence would resolve it: Theoretical analysis and empirical benchmarks exploring amortized inference performance with increasingly large candidate sets, identifying breaking points where regular inference becomes more efficient.

## Limitations
- The paper relies on empirical demonstration rather than formal mathematical proof of the equivalence between regular and amortized inference under cross-attention
- Limited disclosure of specific hardware configurations and model architectures makes independent validation of the 30% latency reduction challenging
- The comparison focuses primarily on computational efficiency while treating concatenation and appending as functionally equivalent, potentially overlooking nuanced differences in learned representations

## Confidence
- Claim that appending with cross-attention achieves similar prediction accuracy to concatenation: **High** - supported by multiple public dataset experiments and attention pattern analysis
- Claim that amortized inference reduces latency by 30% in production systems: **Medium** - reported results but limited experimental details provided for independent verification
- Claim that the mathematical formulation enables exact equivalence between regular and amortized inference: **Low-Medium** - demonstrated empirically but not formally proven

## Next Checks
1. Implement a formal mathematical proof showing the exact conditions under which regular and amortized inference produce equivalent outputs under cross-attention formulation
2. Conduct controlled latency benchmarks on publicly available hardware using standardized batch sizes and sequence lengths to verify the claimed computational efficiency gains
3. Perform ablation studies varying the number of candidates (m) and history length (n) to determine the break-even point where amortization stops providing benefits, and test this across different Transformer architectures and attention variants