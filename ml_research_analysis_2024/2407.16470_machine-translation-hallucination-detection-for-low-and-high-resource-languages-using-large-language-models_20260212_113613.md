---
ver: rpa2
title: Machine Translation Hallucination Detection for Low and High Resource Languages
  using Large Language Models
arxiv_id: '2407.16470'
source_url: https://arxiv.org/abs/2407.16470
tags:
- hallucination
- text
- translation
- detection
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting hallucinations
  in machine translation, particularly for low-resource languages. The authors evaluate
  sentence-level hallucination detection using large language models (LLMs) and semantic
  similarity within multilingual embeddings across 16 language directions.
---

# Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models

## Quick Facts
- arXiv ID: 2407.16470
- Source URL: https://arxiv.org/abs/2407.16470
- Authors: Kenza Benkirane; Laura Gongas; Shahar Pelles; Naomi Fuchs; Joshua Darmon; Pontus Stenetorp; David Ifeoluwa Adelani; Eduardo Sánchez
- Reference count: 31
- Primary result: LLMs achieve state-of-the-art hallucination detection performance across both high and low-resource languages, with Llama3-70B outperforming previous methods by up to 0.16 MCC for HRLs

## Executive Summary
This paper addresses the challenge of detecting hallucinations in machine translation, particularly for low-resource languages. The authors evaluate sentence-level hallucination detection using large language models (LLMs) and semantic similarity within multilingual embeddings across 16 language directions. They compare eight LLMs with different prompt variations and four embedding-based methods. The study finds that LLMs can achieve performance comparable to or better than state-of-the-art models, despite not being explicitly trained for MT tasks. Specifically, Llama3-70B outperforms previous methods by up to 0.16 MCC for high-resource languages, while Claude Sonnet is best for low-resource languages with an improvement of 0.03 MCC.

## Method Summary
The study evaluates hallucination detection using both LLM-based and embedding-based approaches across 16 language directions. For LLMs, eight different models were tested with three prompt variations and two Chain-of-Thought options, with optimal configurations selected using MCC on a DE↔EN validation set. Embedding methods used cosine similarity between source and translated text embeddings from SONAR, OpenAI text-embedding-3-large, Cohere Embed v3, and Mistral embed. The HalOmi dataset provided 2,558 test sentence pairs with binary classification for hallucination detection (No Hallucination vs. Hallucination), evaluated using MCC, F1-score, precision-recall AUC, and other metrics.

## Key Results
- Llama3-70B surpasses the previous best performing model, BLASER-QE, by +5 points MCC for low-resource languages, with an MCC of 0.43
- For high-resource languages, Llama3-70B improves over the baseline by 16 points MCC (0.63), with 10 out of 12 evaluated methods outperforming BLASER-QE
- Claude Sonnet is the best performing LLM for low-resource languages, improving previous methods by 0.03 MCC
- Embedding-based methods remain competitive for high-resource contexts, outperforming more sophisticated models in five out of eight translation directions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs outperform BLASER-QE on hallucination detection for HRLs due to their ability to leverage general reasoning and linguistic understanding beyond semantic similarity.
- Mechanism: LLMs use prompt-guided reasoning to evaluate semantic alignment and detect information unrelated to the source text, while BLASER-QE relies solely on cosine similarity in embedding space.
- Core assumption: Semantic similarity alone is insufficient for capturing subtle hallucinations that introduce unrelated content but still share some semantic features.
- Evidence anchors:
  - [abstract]: "LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task."
  - [section 3]: "Llama3-70B surpasses the previous best performing model, BLASER-QE, by +5 points, with an MCC of 0.43. For HRLs, 10 out of 12 evaluated methods outperform BLASER-QE (0.46), with Llama3-70B greatly improving over the baseline by 16 points (0.63)."
  - [corpus]: "Large Language Models are transforming NLP for a variety of tasks. However, how LLMs perform NLP tasks for low-resource languages (LRLs) is less explored."

### Mechanism 2
- Claim: Embedding-based methods remain competitive for HRLs because their multilingual training data includes diverse language pairs and scripts, enabling robust cross-lingual similarity measures.
- Mechanism: SONAR and other embedding spaces encode semantic relationships that transfer across languages, allowing detection of severe hallucinations through distance metrics.
- Core assumption: The training corpus for embeddings is sufficiently diverse and representative of HRL translation patterns.
- Evidence anchors:
  - [abstract]: "embedding-based methods have also demonstrated superior performance over the current SOTA in high resource contexts."
  - [section 3]: "simple embedding-based methods display competitive capabilities, outperforming more sophisticated models in five out of eight translation directions."
  - [corpus]: "Multilingual NMT is a viable solution for translating low-resource languages (LRLs) when data from high-resource languages (HRLs) from the same language family is available."

### Mechanism 3
- Claim: LLMs show better performance on LRLs when the model has been trained on diverse linguistic patterns and can generalize reasoning capabilities beyond resource-level biases.
- Mechanism: Claude Sonnet's architecture and training allow it to detect hallucinations in LRLs by applying general linguistic reasoning rather than relying on language-specific patterns.
- Core assumption: LLMs trained on diverse data can generalize reasoning capabilities across resource levels, though performance varies by language family and script.
- Evidence anchors:
  - [abstract]: "for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC."
  - [section 3]: "for LRLs, Claude Sonnet is the best performing model, improving previous methods by a smaller difference."
  - [corpus]: "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs"

## Foundational Learning

- Concept: Matthews Correlation Coefficient (MCC)
  - Why needed here: MCC provides a balanced metric for evaluating binary classification performance when class distributions are imbalanced, which is crucial for hallucination detection where No Hallucination examples typically dominate.
  - Quick check question: Why is MCC preferred over accuracy when evaluating hallucination detection on imbalanced datasets?

- Concept: Multilingual embeddings and semantic similarity
  - Why needed here: Understanding how embedding spaces capture cross-lingual semantic relationships is essential for interpreting why embedding-based methods work for HRLs but struggle with LRLs.
  - Quick check question: What property of multilingual embeddings makes them effective for detecting hallucinations in high-resource languages?

- Concept: Prompt engineering and chain-of-thought reasoning
  - Why needed here: LLMs' performance heavily depends on prompt design, and understanding how to structure prompts for hallucination detection is crucial for replicating and improving results.
  - Quick check question: How does including Chain-of-Thought reasoning in prompts affect LLM performance on hallucination detection tasks?

## Architecture Onboarding

- Component map: Source text → Embedding calculation or LLM inference → Cosine similarity or classification → Threshold application or label assignment → Performance evaluation
- Critical path: Source text → Embedding calculation or LLM inference → Cosine similarity or classification → Threshold application or label assignment → Performance evaluation
- Design tradeoffs: LLMs offer superior reasoning but are computationally expensive and potentially less reliable on LRLs, while embeddings are faster but less nuanced in detection capability.
- Failure signatures:
  - Low MCC with high precision but low recall indicates conservative models missing hallucinations
  - Random performance on specific language pairs suggests fundamental limitations with script or resource level
  - Consistent underperformance on non-English centric translations reveals bias in model training
- First 3 experiments:
  1. Run embedding-based detection on a balanced subset of EN↔DE data to establish baseline performance and threshold selection
  2. Test all eight LLMs on the same EN↔DE subset with their optimal prompts to compare against embedding baselines
  3. Evaluate top-performing LLM and embedding method on a challenging LRL pair (e.g., EN→MN) to identify resource-level limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hallucination detection performances differ when evaluated across diverse low-resource languages with varying linguistic characteristics (e.g., morphological complexity, script type)?
- Basis in paper: [inferred] The paper notes that "no single model uniformly excels across all translation directions" and highlights variability in LRL scenarios, particularly for non-English centric translations.
- Why unresolved: The study primarily focuses on a fixed set of 16 language directions and does not systematically analyze how linguistic features of low-resource languages impact detection performance.
- What evidence would resolve it: A systematic evaluation across a broader range of low-resource languages with diverse linguistic characteristics, measuring detection performance against linguistic feature variables.

### Open Question 2
- Question: What is the impact of model size and architecture on hallucination detection performance, particularly for low-resource languages?
- Basis in paper: [inferred] The paper compares several LLMs and embeddings but does not provide a detailed analysis of how model size and architecture specifically affect performance in low-resource settings.
- Why unresolved: The study includes a variety of models but does not isolate the effects of model size and architecture on detection performance for different resource levels.
- What evidence would resolve it: A controlled study varying model size and architecture while keeping other factors constant, and evaluating their impact on hallucination detection across high and low-resource languages.

### Open Question 3
- Question: How can hallucination detection methods be improved to handle non-English centric translations more effectively?
- Basis in paper: [explicit] The paper notes that "six out of our fourteen methods and BLASER-QE exhibit scores close to random guessing" for the YO→ES translation direction, indicating a challenge in non-English centric settings.
- Why unresolved: The study identifies the challenge but does not explore specific strategies or improvements for handling non-English centric translations.
- What evidence would resolve it: Development and evaluation of novel approaches or adaptations specifically designed to enhance hallucination detection in non-English centric translation scenarios.

## Limitations
- Performance significantly degrades on non-English centric translations (ES↔YO, YO→ES), with MCC scores near random levels
- The distinction between high-resource and low-resource languages may be overly simplistic, with more nuanced resource-level gradients
- Optimal prompt and threshold selection on DE↔EN validation set may not generalize across all language pairs

## Confidence
- High Confidence: LLMs outperform or match state-of-the-art embedding methods on hallucination detection for high-resource languages
- Medium Confidence: Claude Sonnet is the best LLM for low-resource language hallucination detection
- Low Confidence: Embedding-based methods remain competitive for high-resource contexts

## Next Checks
1. Test the best-performing LLM (Llama3-70B for HRLs, Claude Sonnet for LRLs) and embedding method on an independent hallucination detection dataset with different language pairs and distribution to verify generalization beyond the HalOmi dataset.
2. Systematically evaluate all methods on language pairs where neither source nor target is English (e.g., ES→FR, DE→IT) to quantify the EN-centric bias and understand performance degradation patterns.
3. Conduct a controlled experiment varying training data size for a single language pair to empirically determine the resource threshold where LLMs transition from underperforming to outperforming embedding methods, providing clearer guidance for method selection.