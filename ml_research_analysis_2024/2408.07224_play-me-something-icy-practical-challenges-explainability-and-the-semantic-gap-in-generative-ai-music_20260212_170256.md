---
ver: rpa2
title: 'Play Me Something Icy: Practical Challenges, Explainability and the Semantic
  Gap in Generative AI Music'
arxiv_id: '2408.07224'
source_url: https://arxiv.org/abs/2408.07224
tags:
- tools
- music
- generative
- these
- creative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This pictorial examines practical challenges in text-to-audio and
  text-to-music generative AI tools from an explainable AI perspective. The authors
  identify a fundamental "semantic gap" between abstract musical concepts and language-based
  prompts, making precise musical control difficult.
---

# Play Me Something Icy: Practical Challenges, Explainability and the Semantic Gap in Generative AI Music

## Quick Facts
- arXiv ID: 2408.07224
- Source URL: https://arxiv.org/abs/2408.07224
- Authors: Jesse Allison; Drew Farrar; Treya Nash; Carlos Román; Morgan Weeks; Fiona Xue Ju
- Reference count: 13
- Primary result: Text-to-audio tools suffer from a fundamental "semantic gap" between abstract musical concepts and language-based prompts, making precise musical control difficult and undermining trust in AI music generation.

## Executive Summary
This pictorial examines practical challenges in text-to-audio and text-to-music generative AI tools from an explainable AI perspective. The authors identify a fundamental "semantic gap" between abstract musical concepts and language-based prompts, making precise musical control difficult. Through experimental use of commercial tools, they find that opaque training processes, limited user control, and lack of transparency about data sources undermine trust and creative utility. While some tools produce quick results suitable for beginners, they often fail to align with musicians' artistic visions. The authors suggest improvements including clearer methodology documentation, interfaces that leverage musicians' expertise, and transparency about training data sources.

## Method Summary
The authors experimented with commercial text-to-audio and text-to-music tools including Loudly, MusicGen, Music Generate, Stable Audio, and Suno. They varied prompts based on musical semantics, descriptors, complexity, genre, and style. Each author individually tested different prompt styles and inputs, then held regular meetings to share observations using design thinking methodologies like sketching and mental mapping. The study evaluated tools based on prompt creation, control, usability, understandability, explainability of AI processes, and overall aesthetic effectiveness of results.

## Key Results
- Text-based tools inherently struggle to capture abstract musical concepts due to the semantic gap between language and music
- Lack of transparency about training data sources and processes erodes user trust and limits creative adoption
- Current interfaces that generate full songs from single prompts conflict with musicians' iterative creative processes
- Opaque AI processes make it difficult for users to understand or control how their prompts translate into musical outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-to-audio tools inherently suffer from a semantic gap between linguistic prompts and musical concepts.
- Mechanism: The language used in prompts is abstract and culturally variable, while music involves precise technical elements (pitch, rhythm, timbre) that are difficult to express in words. This mismatch causes outputs to deviate from user intent.
- Core assumption: Musical concepts cannot be fully captured by natural language without significant loss of precision or cultural specificity.
- Evidence anchors: [abstract] "One of the challenges we have identified that is not explicitly addressed by these tools is the inherent semantic gap in using text-based tools to describe something as abstract as music." [section] "Unlike other forms of data, such as text or images, many musical elements defy precise and well-defined description. This causes a ‘semantic gap’."

### Mechanism 2
- Claim: Opaque training data sources and processes erode user trust and hinder creative experimentation.
- Mechanism: Without transparency about where data comes from, how it's labeled, or what the model was trained on, users fear plagiarism, unauthorized use, or misaligned outputs. This limits adoption among professional musicians.
- Core assumption: Artists require ethical clarity and provenance information to feel safe using AI tools for creative work.
- Evidence anchors: [abstract] "While tools may not need to explain their process in detail, transparency in training data and intended use cases could lead to more obvious cause and effect, and thus usability." [section] "Most of the music tools we looked at do not reveal where their datasets come from. How can users trust these tools amidst the fear that their creative property will be stolen..."

### Mechanism 3
- Claim: Current interfaces that generate full songs from single prompts conflict with musicians' iterative creative processes.
- Mechanism: Musicians typically refine music incrementally (adjusting harmony, rhythm, instrumentation). Tools that output entire songs without granular control force users to restart from scratch, breaking workflow continuity.
- Core assumption: Professional music creation relies on iterative refinement rather than one-shot generation.
- Evidence anchors: [abstract] "The gap between the user control of generative AI music tools and their input methods reduces their potential for practical use... when the user inputs 'a dark, icy section' the output could be in the form of a pop song... What part of the input said that the result should be a pop song?" [section] "As some have discussed, the rigidity of current AI generative content (AIGC) tools does not mesh well with the iterative, trial-and-error creative process typical of many artists."

## Foundational Learning

- Concept: Semantic gap in cross-modal AI
  - Why needed here: The paper's central argument is that text-to-audio systems cannot bridge the semantic gap between language and music. Understanding this concept is essential to evaluate tool limitations.
  - Quick check question: Why is describing "icy" music more difficult than describing an "icy" image?

- Concept: Explainable AI (XAI) in creative domains
  - Why needed here: The authors argue that explainability is crucial for trust and usability in music tools. Engineers need to understand how XAI principles apply differently in artistic contexts.
  - Quick check question: How might transparency about training data improve a musician's willingness to use an AI tool?

- Concept: Iterative vs. one-shot creative workflows
  - Why needed here: The paper contrasts AI tools' one-shot generation with musicians' iterative refinement. This distinction shapes interface and control design decisions.
  - Quick check question: What workflow advantage does adjusting individual musical parameters have over regenerating entire compositions?

## Architecture Onboarding

- Component map: Text prompt → Semantic interpretation layer → Music generation model → Audio output
- Critical path: User prompt → Semantic mapping → Generation → Output delivery. Any failure in semantic mapping directly impacts usability.
- Design tradeoffs:
  - Full-song generation (fast, less control) vs. modular generation (slow, more control)
  - Opaque models (simpler UI, less trust) vs. transparent models (complex UI, more trust)
  - General-purpose prompts (broad appeal) vs. domain-specific vocabularies (precision)
- Failure signatures:
  - Outputs consistently miss user intent → Semantic gap too wide
  - Users abandon tools after few attempts → Trust/explainability issues
  - Professional musicians don't adopt → Lack of control/iterative capability
- First 3 experiments:
  1. Test prompt variation: "icy" vs. "cold" vs. "frosty" to measure semantic consistency
  2. Implement parameter sliders for tempo/dynamics without regeneration
  3. Create mock transparency panel showing dataset sources and labeling methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific user interface designs would best bridge the semantic gap between musical concepts and text-based prompts?
- Basis in paper: [explicit] The paper discusses how current text-to-audio tools fail to capture musical ideas due to the semantic gap between language and music, and suggests that interfaces leveraging musicians' expertise could help.
- Why unresolved: While the paper identifies the problem and suggests general improvements, it doesn't propose specific interface designs or evaluate what types of visual/musical representations would most effectively help users translate their musical intentions into prompts.
- What evidence would resolve it: User studies comparing different interface approaches (visual programming, notation-based, hybrid text-visual systems) measuring both usability and quality of musical output alignment with user intent.

### Open Question 2
- Question: How does the lack of transparency about training data sources impact user trust and creative adoption of generative AI music tools?
- Basis in paper: [explicit] The authors note that most tools don't reveal where their datasets come from, raising concerns about plagiarism and artistic integrity, and that transparency about data sources would make tools "more usable, explainable, and trustable."
- Why unresolved: The paper asserts this relationship but doesn't provide empirical evidence of how transparency (or lack thereof) affects user behavior, adoption rates, or creative confidence.
- What evidence would resolve it: Surveys or experiments measuring user trust and creative adoption across tools with varying levels of data transparency, including actual usage patterns and creative output quality.

### Open Question 3
- Question: What training data characteristics (size, diversity, labeling methodology) produce the most musically coherent and creatively useful outputs?
- Basis in paper: [explicit] The authors call for transparency about "the size of datasets, and how the data is being labeled" but don't investigate what specific characteristics make training data effective.
- Why unresolved: While the paper identifies the need for transparency about training data, it doesn't explore which data characteristics actually matter for musical quality or creative utility.
- What evidence would resolve it: Comparative studies of tools trained on datasets with systematically varied characteristics (size, genre diversity, labeling precision) measuring output coherence, creativity, and user satisfaction.

## Limitations
- Limited empirical testing with insufficient methodological detail
- Small sample of tools tested without systematic comparison
- Lack of quantitative metrics for evaluating outputs
- No validation from actual musicians or creators

## Confidence
- High confidence: The identification of the semantic gap as a fundamental challenge in text-to-audio systems
- Medium confidence: Claims about transparency and trust issues
- Low confidence: The assertion that current tools fundamentally conflict with musicians' iterative creative processes

## Next Checks
1. Conduct structured user studies with 10-15 musicians using the same tools, measuring both output quality and workflow satisfaction through standardized metrics.
2. Implement controlled experiments varying prompt specificity (abstract vs. technical) to quantify the semantic gap's impact on output relevance.
3. Develop a transparency rating system for AI music tools based on published dataset sources, training methodologies, and licensing information, then validate its correlation with user trust and adoption.