---
ver: rpa2
title: Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed
  Weight Directions and its Application to Effective Model Pruning
arxiv_id: '2402.10639'
source_url: https://arxiv.org/abs/2402.10639
tags:
- adapters
- adapter
- performance
- mixing
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the in-domain generalizability of mixing
  domain-specific adapters, a mechanism proposed for efficient adaptation of large
  language models. Through comprehensive experiments on 13 diverse classification
  datasets and three adapter methods, the authors demonstrate that mixing adapters
  across different domains leads to performance degradation, even on in-domain examples.
---

# Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning

## Quick Facts
- arXiv ID: 2402.10639
- Source URL: https://arxiv.org/abs/2402.10639
- Authors: Tuc Nguyen; Thai Le
- Reference count: 40
- Key outcome: Mixing domain-specific adapters leads to performance degradation, with strong negative correlation between sign differences in adapter weights and generalizability; greedy mixing and pruning achieve competitive performance with up to 90% sparsity.

## Executive Summary
This study investigates the in-domain generalizability of mixing domain-specific adapters, a mechanism proposed for efficient adaptation of large language models. Through comprehensive experiments on 13 diverse classification datasets and three adapter methods, the authors demonstrate that mixing adapters across different domains leads to performance degradation, even on in-domain examples. They find a strong negative correlation between the fraction of sign differences in adapter weights and the generalizability of the resulting mixture. This observation enables a greedy adapter mixing strategy and effective model pruning, achieving competitive performance with up to 90% sparsity. The study provides critical insights into the inner workings of adapter mixtures and offers practical guidance for their deployment in real-world applications.

## Method Summary
The study trains 13 domain-specific adapters using BERT-base and RoBERTa-large models with Houlsby, Pfeiffer, and LoRA adapter methods on diverse classification datasets. All possible adapter mixtures are created through weight averaging, and in-domain accuracy is evaluated on clean and adversarial examples. The Fraction of Sign Difference (FSD) is computed between adapter weights to analyze sign conflicts. A greedy mixing strategy based on minimal FSD is implemented, and magnitude pruning guided by FSD analysis is applied to create sparse adapters. The performance of mixed sparse adapters is compared to mixed dense adapters to evaluate the effectiveness of the pruning approach.

## Key Results
- Mixing adapters across different domains leads to significant performance degradation, even on in-domain examples.
- Strong negative correlation exists between the fraction of sign differences in adapter weights and the generalizability of the resulting mixture.
- Greedy adapter mixing based on minimizing FSD leads to competitive performance compared to empirical upper-bound accuracy.
- Pruning up to 90% of adapter weights before mixing improves performance by reducing sign conflicts.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixing adapters with opposing weight signs leads to "forgotten knowledge" and performance degradation.
- Mechanism: When adapter weights with opposite signs are averaged, their contributions to the model's output can cancel each other out, leading to a loss of the specific knowledge encoded in each adapter.
- Core assumption: The direction of weight signs is critical for preserving the task-specific knowledge learned by each adapter.
- Evidence anchors:
  - [abstract] "strong negative correlation between the fraction of sign differences in adapter weights and the generalizability of the resulting mixture."
  - [section] "mixing adapter weights of conflicting signs can result in 'forgotten knowledge', and lead to performance degradation."
  - [corpus] Weak - no direct mention of weight sign cancellation in the related papers.
- Break condition: If the model architecture changes such that weight signs are not critical for knowledge encoding (e.g., if a different activation function or normalization is used).

### Mechanism 2
- Claim: Pruning redundant weights in adapters before mixing reduces the fraction of weight sign conflicts and improves performance.
- Mechanism: By removing less important weights (those with smaller magnitudes), the remaining weights are more likely to have consistent signs across adapters, reducing the chance of cancellation when mixed.
- Core assumption: Many weights in adapters are redundant and do not significantly contribute to task performance.
- Evidence anchors:
  - [section] "pruning up to 90% parameters of adapter weight does not lead to performance degradation."
  - [section] "mixing adapters with 80% or 90% sparsity consistently achieved better performance than the upper-bound empirical accuracy achieved when mixing their dense versions."
  - [corpus] Weak - related papers discuss pruning but not in the context of adapter mixing.
- Break condition: If the pruned weights are actually critical for task performance (i.e., if the pruning is too aggressive).

### Mechanism 3
- Claim: Greedy adapter mixing based on minimizing fraction of weight sign difference (FSD) leads to better generalizability than random mixing.
- Mechanism: By selecting adapters with minimal FSD for mixing, the resulting mixture is less likely to suffer from sign cancellation and thus preserves more task-specific knowledge.
- Core assumption: FSD is a good proxy for the potential performance degradation when mixing adapters.
- Evidence anchors:
  - [section] "there is a strong negative correlation between the FSD and the generalizability."
  - [section] "Greedy Adapter Mixing resulted in very competitive performance compared with empirical upper-bound accuracy."
  - [corpus] Weak - no direct mention of greedy mixing strategies in the related papers.
- Break condition: If the FSD does not accurately reflect the actual performance impact of mixing (e.g., if other factors like weight magnitude are more important).

## Foundational Learning

- Concept: Parameter-efficient fine-tuning methods (e.g., adapters, LoRA)
  - Why needed here: The study focuses on mixing domain-specific adapters, so understanding how these methods work is crucial.
  - Quick check question: What is the main advantage of using adapters over full fine-tuning?

- Concept: Adversarial attacks on NLP models (e.g., TextFooler, FGSM)
  - Why needed here: The study evaluates the robustness of adapter mixtures under adversarial attacks, so understanding these attack methods is important.
  - Quick check question: How does the FGSM attack generate adversarial examples?

- Concept: Neural network pruning
  - Why needed here: The study proposes using pruning to improve adapter mixing, so understanding pruning techniques is relevant.
  - Quick check question: What is the main goal of neural network pruning?

## Architecture Onboarding

- Component map: Pre-trained language model (e.g., BERT, RoBERTa) + domain-specific adapters (e.g., Houlsby, Pfeiffer, LoRA)
- Critical path: 1) Train domain-specific adapters, 2) Compute FSD between adapters, 3) Select adapters for mixing based on FSD, 4) Mix selected adapters, 5) Evaluate performance
- Design tradeoffs: Balancing the number of adapters to mix (more adapters = more diverse knowledge but higher risk of sign conflicts) vs. the sparsity of adapters (more pruning = less sign conflicts but risk of losing important weights)
- Failure signatures: Significant performance drop when mixing adapters, especially on domains with high FSD; poor robustness under adversarial attacks
- First 3 experiments:
  1. Train individual adapters on different domains and evaluate their performance on clean data.
  2. Mix adapters with high FSD and evaluate the performance drop compared to individual adapters.
  3. Apply pruning to adapters before mixing and compare the performance to mixing dense adapters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different adapter architectures (Houlsby, Pfeiffer, LoRA) influence the relationship between FSD and generalizability when mixing adapters?
- Basis in paper: [explicit] The paper uses three adapter methods but does not analyze their individual effects on FSD and performance degradation.
- Why unresolved: The study reports similar trends across different adapter methods but does not quantify the differences in FSD-magnitude-generalizability relationships between them.
- What evidence would resolve it: Comparative analysis showing FSD and performance degradation curves for each adapter method across the same task combinations.

### Open Question 2
- Question: What is the optimal number of adapters to mix for maximizing in-domain performance without significant degradation?
- Basis in paper: [inferred] The paper suggests mixing up to three adapters for competitive performance but does not determine the optimal number through systematic analysis.
- Why unresolved: The study provides empirical observations but does not perform a rigorous optimization to find the precise number of adapters that balances performance and efficiency.
- What evidence would resolve it: A comprehensive study varying the number of mixed adapters and measuring the trade-off between performance and computational cost.

### Open Question 3
- Question: How does the proposed FSD-based pruning method compare to other advanced pruning techniques like SynFlow or GraSP?
- Basis in paper: [explicit] The paper mentions that future work includes investigating the applicability of findings to other pruning approaches.
- Why unresolved: The study only evaluates one classic pruning method and does not compare its effectiveness to other state-of-the-art techniques.
- What evidence would resolve it: Experimental results comparing FSD-based pruning with SynFlow, GraSP, and other advanced pruning methods in terms of performance and efficiency.

## Limitations

- The analysis focuses primarily on classification tasks with relatively small domain shifts, leaving uncertainty about performance on more complex tasks or domains with larger semantic differences.
- The weight sign mechanism, while supported by strong empirical correlations, remains somewhat heuristic - we cannot definitively prove that sign cancellation is the sole mechanism behind performance degradation.
- The pruning approach shows promising results but raises questions about the nature of "redundant" weights - what appears redundant for one domain mixture might be critical for another.

## Confidence

- Adapter mixing performance degradation: High
- FSD correlation with performance: High
- Greedy mixing strategy effectiveness: Medium
- Pruning benefits: Medium
- Sign cancellation mechanism: Low-Medium

## Next Checks

1. Test adapter mixing across more diverse task types (generation, QA, reasoning) to assess generalizability beyond classification.
2. Conduct ablation studies isolating weight magnitude effects from sign effects to better understand the relative importance of each factor.
3. Evaluate whether alternative weight combination strategies (weighted averaging, learned mixing) can outperform simple sign-aware mixing.