---
ver: rpa2
title: 'Towards a Psychology of Machines: Large Language Models Predict Human Memory'
arxiv_id: '2403.05152'
source_url: https://arxiv.org/abs/2403.05152
tags:
- https
- page
- generativeartificialintelligencepredictshumanmemory
- memory
- cognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored whether generative AI models can predict human
  memory performance, using ChatGPT to rate garden-path sentences with varying contexts.
  The results showed that ChatGPT's relatedness ratings closely matched human participants,
  and its memorability ratings effectively predicted human memory outcomes.
---

# Towards a Psychology of Machines: Large Language Models Predict Human Memory

## Quick Facts
- **arXiv ID**: 2403.05152
- **Source URL**: https://arxiv.org/abs/2403.05152
- **Reference count**: 40
- **Primary result**: ChatGPT's relatedness and memorability ratings closely matched human participants and effectively predicted human memory performance for garden-path sentences.

## Executive Summary
This study explores whether generative AI models can predict human memory performance using ChatGPT to rate garden-path sentences with varying contexts. The results show that ChatGPT's relatedness ratings closely matched those of human participants, and its memorability ratings effectively predicted human memory outcomes. The findings suggest that LLMs can model aspects of human cognition and serve as valuable tools in psychological research, supporting the proposed field of machine psychology.

## Method Summary
The study used 45 garden-path sentences with fitting and unfitting context sentences. Human participants (n=85) rated relatedness and completed a surprise memory test, while ChatGPT (GPT-4) provided 100 responses per prompt for relatedness and memorability ratings. Statistical analysis used mixed-effects models to examine the interaction between context-relatedness and memory performance.

## Key Results
- ChatGPT's relatedness ratings closely matched those of human participants.
- ChatGPT's memorability ratings effectively predicted human memory performance.
- Both LLM and human data revealed that higher relatedness in the unfitting context condition was associated with better memory performance.

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT can model human memory performance in language-based tasks by capturing context-dependent semantic relatedness. The LLM processes garden-path sentences with varying contextual cues and generates relatedness and memorability ratings that correlate with human performance, reflecting shared probabilistic cue integration. Core assumption: Contextual semantic processing in LLMs mirrors probabilistic retrieval processes in human memory. Break condition: If LLMs fail to show sensitivity to context manipulation or the correlation with human ratings drops below a meaningful threshold.

### Mechanism 2
Contextual alignment between fitting/unfitting sentences modulates both LLM and human comprehension and subsequent memory. When the preceding sentence semantically aligns with the garden-path sentence (fitting), comprehension is facilitated and memory is enhanced; misalignment (unfitting) requires more effortful integration, making memory more sensitive to incremental relatedness gains. Core assumption: Comprehension effort and memory encoding are tightly coupled, and both are influenced by prior context. Break condition: If human and LLM relatedness ratings diverge significantly in one context condition or if memory performance is not enhanced by fitting context.

### Mechanism 3
LLMs can serve as valid experimental proxies in psychological research due to their alignment with human cognitive processes in controlled linguistic tasks. By reproducing human-like patterns of relatedness judgments and memory predictions, LLMs extend the methodological toolkit available to psychologists, enabling scalable, controlled studies. Core assumption: LLMs, despite lacking human-like memory mechanisms, can capture essential statistical regularities in human cognition. Break condition: If LLM predictions fail to generalize beyond the specific stimulus set or if robustness checks with synonyms or model variations break the alignment.

## Foundational Learning

- **Concept**: Probabilistic cue integration in memory retrieval
  - Why needed here: The study's core mechanism relies on how both humans and LLMs use contextual cues to predict memory outcomes; understanding cue integration is key to interpreting the results.
  - Quick check question: What is the difference between a context that facilitates memory encoding versus one that requires more effortful integration?

- **Concept**: Garden-path sentences and syntactic ambiguity
  - Why needed here: The experimental stimuli exploit temporary ambiguity; understanding how these sentences are processed is crucial for interpreting LLM and human responses.
  - Quick check question: Why do garden-path sentences pose a challenge for both human and machine language processing?

- **Concept**: Signal detection theory and d'
  - Why needed here: Memory performance is measured using d', which corrects for response bias; knowing how this metric works is essential for interpreting the results.
  - Quick check question: How does d' differ from simple accuracy, and why is it important in this context?

## Architecture Onboarding

- **Component map**: Human data collection (Prolific) -> PsychoPy presentation -> Memory testing -> LLM rating generation (ChatGPT API) -> Statistical analysis (mixed-effects models) -> Interpretation
- **Critical path**: Prompt construction → LLM rating generation → Human relatedness and memory testing → Statistical analysis (mixed-effects models) → Interpretation of context-relatedness-memory interaction
- **Design tradeoffs**: Using zero-shot prompts for simplicity vs. fine-tuning for higher precision; selecting a fixed set of garden-path sentences vs. creating new ones to avoid performative issues
- **Failure signatures**: LLM ratings not correlating with human ratings; context manipulation not affecting either LLM or human responses; memory predictions failing in the surprise test
- **First 3 experiments**:
  1. Test LLM predictions with a new set of garden-path sentences to confirm generalizability.
  2. Vary prompt phrasing or synonyms to check robustness of ratings.
  3. Compare GPT-4 predictions with a newer model (e.g., GPT-4 Turbo) to assess model dependence.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How generalizable are the findings to other LLM architectures beyond GPT-4, such as BERT or newer transformer models?
- **Basis in paper**: The authors note they used GPT-4 and are confident newer models would perform similarly, but do not empirically test this
- **Why unresolved**: The study only tests GPT-4, leaving uncertainty about whether the predictive ability extends to other LLM architectures or future models
- **What evidence would resolve it**: Direct comparison studies using multiple LLM architectures (BERT, GPT-3, Claude, etc.) on the same memory prediction task

### Open Question 2
- **Question**: What are the underlying mechanisms by which LLMs predict human memory performance without possessing human-like memory systems?
- **Basis in paper**: The authors propose a stochastic reasoning framework but acknowledge LLMs lack human memory mechanisms, creating a theoretical gap
- **Why unresolved**: The paper demonstrates correlation between LLM predictions and human memory but does not explain the cognitive or computational mechanisms enabling this
- **What evidence would resolve it**: Neurocognitive studies comparing attention patterns, retrieval cues, and processing mechanisms between humans and LLMs during similar tasks

### Open Question 3
- **Question**: How does the performativity problem (LLMs potentially being trained on the exact stimuli used) affect the validity of using LLMs as psychological research tools?
- **Basis in paper**: The authors acknowledge this limitation, noting their stimuli were compiled from diverse sources in the literature rather than created independently
- **Why unresolved**: The study cannot determine whether observed predictive ability stems from genuine cognitive modeling or prior exposure to similar examples during training
- **What evidence would resolve it**: Experiments using entirely novel stimuli created specifically for the study, with control conditions testing LLM responses to known vs. unknown sentence types

## Limitations

- The analysis uses a fixed set of 45 garden-path sentences, and it is unclear whether the observed alignment between LLM and human responses would generalize to different linguistic stimuli or semantic domains.
- The prompts used to elicit ratings from ChatGPT are only described at a high level, leaving open the possibility that small changes in phrasing or prompt construction could affect the results.
- While the study reports high correlation between human and LLM relatedness ratings, it does not fully address whether the underlying cognitive mechanisms are truly aligned or if the alignment is merely statistical.

## Confidence

- **LLMs can model human memory performance in language-based tasks**: Medium confidence
- **Contextual alignment modulates comprehension and memory in both humans and LLMs**: Medium confidence
- **LLMs can serve as valid experimental proxies in psychological research**: Low confidence

## Next Checks

1. **Test generalizability with new stimuli**: Use a different set of garden-path sentences (e.g., from another corpus) to confirm that LLM predictions hold beyond the original stimulus set.
2. **Check robustness to prompt variation**: Repeat the LLM rating task using alternative phrasings or synonyms for the same sentences to assess whether the outputs are stable or sensitive to prompt construction.
3. **Compare model versions**: Run the same prompts on a newer or alternative LLM (e.g., GPT-4 Turbo) to determine if the observed alignment is model-dependent.