---
ver: rpa2
title: Prosody-Driven Privacy-Preserving Dementia Detection
arxiv_id: '2407.03470'
source_url: https://arxiv.org/abs/2407.03470
tags:
- speaker
- dementia
- features
- speech
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses privacy concerns in dementia detection from
  speech by proposing a method to anonymize speaker embeddings while preserving their
  utility for disease detection. The core idea is to leverage domain knowledge of
  dementia-relevant prosodic features and disentangle them from speaker identity information
  in embeddings.
---

# Prosody-Driven Privacy-Preserving Dementia Detection

## Quick Facts
- arXiv ID: 2407.03470
- Source URL: https://arxiv.org/abs/2407.03470
- Authors: Dominika Woszczyk; Ranya Aloufi; Soteris Demetriou
- Reference count: 0
- One-line primary result: Achieves speaker recognition F1-score of 0.01% while maintaining dementia detection F1-score of 74% on ADReSS dataset

## Executive Summary
This work addresses privacy concerns in dementia detection from speech by proposing a method to anonymize speaker embeddings while preserving their utility for disease detection. The core idea is to leverage domain knowledge of dementia-relevant prosodic features and disentangle them from speaker identity information in embeddings. The proposed approach uses adversarial learning and mutual information-guided shuffling, training on an auxiliary dataset without requiring dementia labels. Experiments show the method achieves speaker recognition F1-score of 0.01% while maintaining dementia detection F1-score of 74% on the ADReSS dataset, with similar performance on ADReSSo. The approach outperforms traditional adversarial training and preserves speech naturalness for text-to-speech synthesis.

## Method Summary
The proposed approach leverages domain knowledge to disentangle prosody features relevant to dementia from speaker embeddings without relying on a dementia classifier. It employs two methods: adversarial learning and mutual information-guided shuffling. The adversarial approach trains a model to minimize reconstruction loss for dementia-related prosodic features while maximizing loss for speaker-related features. The shuffling method computes mutual information between embedding dimensions and dementia labels, preserving high-MI dimensions while randomizing low-MI ones. The disentanglement model is trained on the LibriSpeech dataset (100 hours from 251 speakers) without dementia labels, then applied to dementia detection datasets (ADReSS and ADReSSo). Prosodic features extracted include articulation rate, pause counts/lengths, f0, and mean energy using Parselmouth and Praat scripts.

## Key Results
- Achieves speaker recognition F1-score of 0.01% while maintaining dementia detection F1-score of 74% on ADReSS dataset
- Outperforms traditional adversarial training baselines (ADV SPK AD+AD achieves 60% F1-score)
- Maintains speech naturalness with MOSNet scores of 3.5, SI-SDR of 12.65 dB, and WER of 17.6% in zero-shot TTS evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial learning with prosody regressors can disentangle speaker identity from dementia-relevant prosodic features.
- Mechanism: The model is trained to minimize reconstruction loss for dementia-related prosodic features (speech rate, pause count, etc.) while maximizing the loss for speaker-related prosodic features. This forces the model to retain information useful for dementia detection but strip speaker-identifying details.
- Core assumption: Prosodic features that are strongly linked to dementia are weakly linked to speaker identity, or can be separated through adversarial training.
- Evidence anchors:
  - [abstract] "leverage domain knowledge to disentangle prosody features relevant to dementia from speaker embeddings without relying on a dementia classifier."
  - [section 2.3] "The proposed adversarial model is based on domain adversarial training [21] which aims to create domain-invariant latent spaces by maximising the domain discriminator's confusion while minimizing the task-specific loss."
  - [corpus] Weak evidence; only general privacy-preserving models found, no prosody-specific adversarial models in corpus.
- Break condition: If the same prosodic features are equally important for both speaker identity and dementia, the adversarial approach cannot cleanly separate them without harming dementia detection performance.

### Mechanism 2
- Claim: Mutual information-guided shuffling can selectively preserve dementia-relevant embedding dimensions while randomizing speaker-revealing ones.
- Mechanism: Compute mutual information between each embedding dimension and the dementia label, then shuffle dimensions with low mutual information (less related to dementia) to destroy speaker information while keeping high-MI dimensions intact.
- Core assumption: The mutual information between embedding dimensions and the dementia label is distinct from mutual information with speaker identity, allowing clean separation.
- Evidence anchors:
  - [abstract] "propose a feature selection approach based on mutual information to identify important features relevant to dementia while perturbing less relevant features to lower speaker recognition accuracy."
  - [section 2.4] "compute the mutual information between the distribution of dimension of the speaker embeddings and the distribution of the dementia label across the dataset."
  - [corpus] Weak evidence; only general mutual information applications found, no specific MI-based speaker anonymization in corpus.
- Break condition: If dementia and speaker identity share the same embedding dimensions with high mutual information for both, shuffling cannot preserve one without destroying the other.

### Mechanism 3
- Claim: Training on a larger auxiliary dataset (LibriSpeech) without dementia labels can produce generalizable disentanglement models.
- Mechanism: By training prosody disentanglement on a large, speaker-rich dataset without dementia labels, the model learns generalizable prosodic feature extraction that transfers to dementia detection tasks.
- Core assumption: Prosodic feature extraction patterns learned from general speech transfer effectively to dementia-specific tasks without requiring domain-specific training data.
- Evidence anchors:
  - [abstract] "trained on an auxiliary dataset without requiring dementia labels."
  - [section 3.1] "We train our disentanglement model on the LibriSpeech dataset [23], a corpus of read English speech."
  - [corpus] No direct evidence; corpus focuses on privacy preservation but not on transfer learning from auxiliary datasets.
- Break condition: If dementia-specific prosodic patterns are sufficiently different from general speech patterns, transfer learning from auxiliary datasets will fail to capture relevant features.

## Foundational Learning

- Concept: Adversarial training for feature disentanglement
  - Why needed here: The core technique relies on adversarial learning to separate speaker identity from dementia-relevant features
  - Quick check question: What is the key difference between standard adversarial training and the domain adversarial training used here?

- Concept: Mutual information as a feature selection metric
  - Why needed here: The shuffling approach uses mutual information to identify which embedding dimensions to preserve vs randomize
  - Quick check question: How does mutual information differ from correlation when measuring feature relevance?

- Concept: Prosodic feature extraction and their relation to speech disorders
  - Why needed here: Understanding which prosodic features (speech rate, pause patterns, etc.) are indicative of dementia is crucial for the domain knowledge component
  - Quick check question: Which prosodic features are most commonly affected in neurodegenerative diseases like dementia?

## Architecture Onboarding

- Component map:
  ECAPA-TDNN -> Prosody regressors -> Dementia classifier (eval only) -> Speaker classifier (eval only) -> Zero-shot TTS systems

- Critical path: Embedding extraction → prosody disentanglement (adversarial or shuffling) → dementia classification evaluation

- Design tradeoffs:
  - More prosodic features increase dementia detection accuracy but may compromise privacy
  - Larger auxiliary datasets improve disentanglement generalization but increase training complexity
  - Simpler shuffling approaches trade off some performance for easier implementation

- Failure signatures:
  - Speaker F1-score remains high (>10%) despite anonymization attempts
  - Dementia detection F1-score drops significantly (>15%) after anonymization
  - TTS synthesis quality degrades noticeably (high WER, low MOSNet)

- First 3 experiments:
  1. Baseline: Run ECAPA-TDNN embeddings through dementia classifier, measure original F1-scores
  2. Adversarial baseline: Train adversarial model with speaker classifier on dementia dataset, compare performance
  3. Feature selection: Compute mutual information between embedding dimensions and dementia labels, identify top features for shuffling approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed prosody disentanglement approach generalize to other medical conditions beyond dementia?
- Basis in paper: [explicit] The authors mention that the approach can be extended to other attributes or health conditions, but this remains unexplored in the current work.
- Why unresolved: The paper only demonstrates the approach on dementia detection, and no experiments or validation are provided for other conditions.
- What evidence would resolve it: Testing the prosody disentanglement method on datasets for other medical conditions (e.g., Parkinson's disease, COVID-19 detection) and comparing performance to baseline methods.

### Open Question 2
- Question: What is the optimal balance between privacy and utility across different privacy settings for various user needs?
- Basis in paper: [explicit] The authors emphasize the need to offer various privacy settings to balance the trade-off between privacy and utility, but do not explore this systematically.
- Why unresolved: The paper only presents one fixed configuration and does not investigate how different levels of privacy preservation affect utility across different scenarios.
- What evidence would resolve it: Systematic experiments varying the strength of privacy measures and measuring corresponding utility metrics across multiple datasets and use cases.

### Open Question 3
- Question: How robust is the disentanglement approach against a white-box adversary who knows the internal workings of the anonymization system?
- Basis in paper: [explicit] The authors note that future work will explore a stronger adversary in a white-box setting, indicating this remains an open question.
- Why unresolved: The current experiments only evaluate against a black-box adversary, leaving the vulnerability of the system to more sophisticated attacks unknown.
- What evidence would resolve it: Testing the anonymized embeddings against white-box adversaries with full knowledge of the disentanglement architecture and parameters, measuring re-identification success rates.

## Limitations

- The approach relies heavily on domain expertise for prosodic feature selection and may not generalize to other diseases or languages
- Adversarial training assumes clean separation between speaker and dementia-relevant prosody, but these features may be inherently entangled
- The mutual information shuffling approach showed slightly worse privacy-utility trade-offs than adversarial training

## Confidence

- **High confidence** in the experimental setup and baseline comparisons, as the methodology follows established privacy-preserving ML practices and uses standard evaluation metrics.
- **Medium confidence** in the generalizability of the approach beyond dementia detection, as the method is specifically tailored to known dementia-relevant prosodic features and may not transfer to other conditions without significant adaptation.
- **Low confidence** in the long-term privacy guarantees, as the approach hasn't been tested against future speaker identification techniques or adaptive adversarial attacks that could exploit subtle patterns in the anonymized embeddings.

## Next Checks

1. **Cross-disease validation**: Test the disentanglement approach on speech datasets for other neurological conditions (Parkinson's, multiple sclerosis) to verify if the prosodic feature selection and disentanglement techniques generalize beyond dementia.

2. **Adversarial robustness testing**: Evaluate the anonymized embeddings against advanced speaker identification techniques, including transfer learning attacks and adaptive adversaries that could exploit residual patterns in the embeddings.

3. **Linguistic generalization**: Validate the approach on non-English speech datasets to assess whether the prosodic feature extraction and disentanglement techniques transfer across languages, particularly for tonal languages where prosodic features may carry different information.