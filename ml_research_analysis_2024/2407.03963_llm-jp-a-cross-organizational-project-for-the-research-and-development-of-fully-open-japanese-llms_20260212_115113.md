---
ver: rpa2
title: 'LLM-jp: A Cross-organizational Project for the Research and Development of
  Fully Open Japanese LLMs'
arxiv_id: '2407.03963'
source_url: https://arxiv.org/abs/2407.03963
tags:
- japanese
- corpus
- dataset
- llm-jp
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-jp is a large-scale collaborative project in Japan that aims
  to develop open-source, high-quality Japanese large language models (LLMs). Over
  1,500 researchers from academia and industry participated.
---

# LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs

## Quick Facts
- arXiv ID: 2407.03963
- Source URL: https://arxiv.org/abs/2407.03963
- Reference count: 40
- Over 1,500 researchers from academia and industry collaborated to develop open-source Japanese LLMs

## Executive Summary
LLM-jp is a large-scale collaborative project in Japan that aims to develop open-source, high-quality Japanese large language models (LLMs). Over 1,500 researchers from academia and industry participated. The project released two model suites (v1.0 and v2.0), each with 13B-parameter LLMs and fine-tuned variants, trained on Japanese-focused corpora totaling 270B-255B tokens. Key contributions include developing a Japanese tokenizer, constructing curated training datasets, and releasing evaluation tools like llm-jp-eval and safety datasets such as AnswerCarefully. The project's models achieved competitive performance in Japanese language tasks and demonstrated improvements over prior Japanese LLMs, particularly after fine-tuning. Ongoing work targets a 175B-parameter model and expanded safety evaluations.

## Method Summary
The project follows a collaborative, open-source approach to develop Japanese LLMs. The methodology involves constructing large-scale Japanese-focused corpora (270B-255B tokens), developing specialized tokenizers for Japanese, English, and code, pre-training 13B-parameter models using Megatron-DeepSpeed/Megatron-LM, fine-tuning with Japanese instruction datasets and safety data, and evaluating using task-specific benchmarks. The process emphasizes transparency and reproducibility, with all artifacts publicly released under non-commercial licenses.

## Key Results
- Released two model suites (v1.0 and v2.0) with 13B-parameter LLMs and fine-tuned variants
- Achieved competitive performance in Japanese language tasks, outperforming previous Japanese LLMs
- Demonstrated improvements in instruction-following and safety alignment through fine-tuning with Japanese datasets
- Developed comprehensive evaluation frameworks including llm-jp-eval and safety datasets like AnswerCarefully

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The open collaboration model enables rapid scaling of human resources and computational capacity.
- Mechanism: By removing organizational barriers and making all artifacts open-source, the project attracts over 1,500 participants who contribute to corpus building, model training, and evaluation. This crowd-sourcing of expertise and compute accelerates development cycles.
- Core assumption: Contributors will sustain participation when outputs are fully open and credit is shared.
- Evidence anchors:
  - [abstract] "more than 1,500 participants from academia and industry are working together"
  - [section 1] "LLM-jp started in May 2023 with the objective of developing Japanese LLMs on our own" and "we opted for complete transparency and decided to make everything openly available"
- Break condition: If contributions dry up due to lack of incentives or if commercialization conflicts arise over open access.

### Mechanism 2
- Claim: Pre-training on a large, high-quality, Japanese-focused corpus improves model performance on Japanese language tasks.
- Mechanism: The Corpus Building WG constructs datasets totaling 270B-255B tokens with a 50-50 Japanese-English mix, then filters and curates them to remove noise and low-quality content. This focused data distribution helps the model learn nuanced Japanese linguistic patterns.
- Core assumption: A balanced bilingual corpus with strong Japanese representation offsets the typical English bias in LLMs.
- Evidence anchors:
  - [section 2.2] "we constructed the corpus v1 consisting of over 260B tokens" and "130B Japanese, 130B English, and 10B code tokens"
  - [section 2.3] "we extracted Japanese documents from the entire Common Crawl" and applied deduplication/filtering
- Break condition: If corpus quality degrades over time or if domain-specific Japanese data is underrepresented.

### Mechanism 3
- Claim: Fine-tuning with Japanese instruction data and safety datasets improves both task performance and alignment.
- Mechanism: The Fine-tuning and Evaluation WG creates Japanese instruction datasets (jaster, databricks-dolly-15k, oasst1) and safety datasets (AnswerCarefully), then applies supervised fine-tuning and DPO. This step tunes the model to follow instructions and avoid harmful outputs in Japanese cultural context.
- Core assumption: Japanese instruction data is necessary because direct translation from English leads to poorer alignment.
- Evidence anchors:
  - [section 5.2.1] "We constructed three types of Japanese instruction data: jaster, databricks-dolly-15k... and OpenAssistant Conversations Dataset"
  - [section 5.2.2] "we improved the instruction-following ability by refining the instruction-tuning data and adding Direct Preference Optimization (DPO)"
- Break condition: If safety datasets are too small or culturally mismatched, leading to false sense of security.

## Foundational Learning

- Concept: Tokenization and vocabulary design for multilingual models
  - Why needed here: The tokenizer v2.2 uses 96,864 tokens with language-specific partitions (30k Japanese, 20k English, 10k code) to balance efficiency and coverage. Understanding how subword tokenization affects downstream performance is critical.
  - Quick check question: Why did the team increase vocabulary size from 50k to 100k between v1.0 and v2.0?

- Concept: Pre-training scaling laws and data requirements
  - Why needed here: The project follows Chinchilla scaling law (~20 tokens per parameter) to determine corpus size. Knowing how model size, data size, and compute interact is essential for planning.
  - Quick check question: How many tokens were used to train the 13B-parameter models in v1.0 and v2.0?

- Concept: Evaluation benchmarks for multilingual LLMs
  - Why needed here: The team uses llm-jp-eval (task-specific) and Japanese Vicuna QA / MT-Bench (open-ended) to assess performance. Understanding the trade-offs between these methods is key for interpreting results.
  - Quick check question: Why do fine-tuned models sometimes score lower on llm-jp-eval than base models?

## Architecture Onboarding

- Component map: Corpus Building WG → Tokenizer v2.2 → Pre-training corpus v2 → Model Building WG (Megatron-LM) → Pre-trained model v2.0 → Fine-tuning and Evaluation WG (SFT/DPO) → Fine-tuned models (v2.0-M/N/O) → Safety WG (AnswerCarefully) → Evaluation pipelines (llm-jp-eval, Japanese Vicuna QA, MT-Bench)
- Critical path: Corpus construction → Tokenizer design → Pre-training → Fine-tuning → Safety alignment → Evaluation
- Design tradeoffs:
  - Larger vocabulary (100k vs 50k) improves tokenization efficiency but increases model size and training time
  - Balanced Japanese/English mix improves Japanese performance but may reduce English capability compared to English-centric models
  - Open collaboration accelerates progress but requires careful IP and licensing management
- Failure signatures:
  - Loss spikes or divergence during pre-training indicate batch size or learning rate issues
  - Poor evaluation scores on Japanese-specific tasks suggest corpus filtering problems or tokenizer mismatch
  - Safety violations in AnswerCarefully evaluation reveal insufficient fine-tuning or dataset bias
- First 3 experiments:
  1. Replicate tokenizer v2.2 training on a small Japanese corpus subset to verify tokenization quality
  2. Run a short pre-training run (1B tokens) on corpus v2 with a 1.3B-parameter model to validate pipeline
  3. Fine-tune the 1.3B model with jaster data and evaluate on a subset of llm-jp-eval to check instruction-following ability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of fine-tuning and evaluation frameworks be comprehensively analyzed and compared across different datasets and model architectures?
- Basis in paper: [explicit] The paper mentions that the current fine-tuning and evaluation frameworks are incomplete and their comprehensive analysis is still untouched.
- Why unresolved: The paper acknowledges that there is not much difference between full parameter tuning and LoRA tuning in the evaluation of llm-jp-eval, but a large difference is observed in the Japanese Vicuna QA benchmark. This suggests that a detailed analysis of the effects of instruction datasets and fine-tuning methods, as well as the effectiveness of evaluation methods, is needed.
- What evidence would resolve it: Conducting a systematic study comparing the performance of different fine-tuning methods (e.g., full parameter tuning, LoRA tuning) and evaluation frameworks on a diverse set of tasks and datasets. This could involve developing standardized evaluation protocols and conducting extensive experiments to assess the strengths and weaknesses of each approach.

### Open Question 2
- Question: How can the safety of Japanese LLMs be effectively evaluated and improved, considering cultural and linguistic nuances?
- Basis in paper: [explicit] The paper highlights the importance of addressing LLM safety in the context of model transparency and cultural differences. It mentions ongoing efforts to create safety evaluation datasets that consider the quality of medical communication, regulatory requirements, and cultural perspectives.
- Why unresolved: The paper acknowledges that the perception of risk is culturally sensitive and that existing evaluation frameworks may not adequately capture the nuances of Japanese culture and language. Additionally, the paper mentions that the current safety evaluation metrics are difficult to apply consistently across different risk categories.
- What evidence would resolve it: Developing culturally and linguistically sensitive safety evaluation metrics and datasets that accurately reflect the risks and concerns of Japanese society. This could involve conducting extensive human evaluations with diverse annotators to ensure the consistency and reliability of safety ratings. Additionally, exploring the use of cross-cultural studies to identify potential biases and vulnerabilities in Japanese LLMs.

### Open Question 3
- Question: What is the optimal ratio of Japanese to English tokens in the training corpus for Japanese LLMs?
- Basis in paper: [explicit] The paper mentions that the mixing ratio of Japanese and English in their corpora is set at 50-50, but further study is needed on the mixing ratio and the size of the corpora.
- Why unresolved: The paper acknowledges that the optimal mixing ratio for Japanese LLMs is not yet determined. While a 50-50 ratio is used, it is unclear whether this is the most effective approach or if a different ratio would lead to better performance.
- What evidence would resolve it: Conducting a series of experiments training Japanese LLMs with different ratios of Japanese to English tokens in the training corpus. Evaluating the performance of these models on various tasks and benchmarks would provide insights into the optimal mixing ratio for achieving the best results.

## Limitations

- Lack of detailed implementation specifications for critical components including exact filtering thresholds for corpus construction, specific hyperparameters for model parallelism and learning rate scheduling, and complete safety evaluation methodologies
- Evaluation framework relies heavily on self-constructed benchmarks (llm-jp-eval) without independent verification, potentially limiting generalizability
- Safety assessment may not capture the full spectrum of potential risks in real-world deployment scenarios despite innovative AnswerCarefully dataset

## Confidence

- **High Confidence**: The technical feasibility of developing Japanese LLMs through large-scale pre-training on Japanese-focused corpora, supported by the successful release of both v1.0 and v2.0 model suites
- **Medium Confidence**: Claims about performance improvements over existing Japanese LLMs, as evaluation relies on benchmark comparisons that may not fully represent real-world usage scenarios
- **Medium Confidence**: Safety improvements through AnswerCarefully dataset fine-tuning, given the limited scope of safety evaluation compared to commercial models with extensive red-teaming

## Next Checks

1. Replicate the corpus construction pipeline using publicly available Japanese datasets (Wikipedia, Common Crawl) with documented filtering parameters to verify the 255B-270B token corpus quality claims
2. Conduct independent evaluation of v2.0 models using established multilingual benchmarks (MMLU, HellaSwag) to validate performance claims beyond llm-jp-eval
3. Perform adversarial safety testing on v2.0 models using diverse Japanese-language prompts from multiple cultural perspectives to assess the robustness of AnswerCarefully fine-tuning