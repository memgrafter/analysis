---
ver: rpa2
title: Training Differentially Private Ad Prediction Models with Semi-Sensitive Features
arxiv_id: '2401.15246'
source_url: https://arxiv.org/abs/2401.15246
tags:
- features
- training
- dataset
- privacy
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies differentially private (DP) model training with
  semi-sensitive features, where some features are known to the attacker and do not
  need protection, while others and the label must be protected by DP. The authors
  propose a hybrid algorithm that first applies a label-DP mechanism (randomized response)
  to protect the labels, then trains a DP-SGD model on all features.
---

# Training Differentially Private Ad Prediction Models with Semi-Sensitive Features

## Quick Facts
- arXiv ID: 2401.15246
- Source URL: https://arxiv.org/abs/2401.15246
- Reference count: 40
- One-line primary result: Hybrid DP training with semi-sensitive features outperforms both full DP and label-DP baselines on ad prediction datasets

## Executive Summary
This paper introduces a novel approach for training differentially private (DP) machine learning models in scenarios where some features are known to the attacker and don't require privacy protection. The proposed hybrid algorithm combines a label-DP phase with randomized response and a subsequent DP-SGD phase, strategically splitting the privacy budget between protecting labels and training with sensitive features. The method demonstrates improved utility over traditional DP approaches across three real ad prediction datasets.

## Method Summary
The hybrid algorithm operates in two phases: first applying randomized response with privacy budget ε1 to protect labels while training a truncated model on non-sensitive features, then using DP-SGD with privacy budget ε2 to train on all features. The privacy budget is split as ε1 = min{0.6·ε, 3} and ε2 = ε - ε1. The approach is evaluated on three ad prediction datasets (Criteo Attribution, Criteo Display Ads, and a proprietary pCVR dataset) with features split into sensitive and non-sensitive categories. Models use embedding layers for categorical features followed by dense layers, trained with RMSprop/Adam/Yogi optimizers and cosine learning rate decay.

## Key Results
- The hybrid approach achieves up to 30% lower AUC loss than the best baseline on the Criteo Attribution dataset
- Outperforms both DP-SGD on all features and label-DP on only non-sensitive features across all three real ad prediction datasets
- Demonstrates the effectiveness of strategically splitting the privacy budget between label protection and feature training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid approach improves over both DP-SGD and label-DP baselines by strategically splitting the privacy budget between protecting labels and training with sensitive features.
- Mechanism: By first applying a label-DP mechanism (randomized response) to protect the labels, then training a DP-SGD model on all features, the hybrid approach achieves better utility than either baseline alone.
- Core assumption: The privacy budget split (ε1 := min {0.6 · ε, 3} and ε2 := ε − ε1) is optimal for balancing label protection and feature training.
- Evidence anchors:
  - [abstract] The hybrid approach outperforms both DP-SGD on all features and label-DP on only non-sensitive features across three real ad prediction datasets.
  - [section] "The main algorithm Hybrid uses a different split between the two phases. Given a privacy budget of (ε, δ), the precise split as (ε1, 0) and (ε2, δ) between the two phases has a significant impact on the model's utility."
  - [corpus] Weak - the corpus papers discuss related DP concepts but do not directly address the semi-sensitive features setting or the hybrid approach.
- Break condition: If the optimal privacy budget split changes significantly with different datasets or model architectures, the hybrid approach may no longer outperform the baselines.

### Mechanism 2
- Claim: The label-DP phase reduces the need for noise addition in the DP-SGD phase, leading to better model utility.
- Mechanism: By applying randomized response to protect the labels in the first phase, the model can learn from the noisy labels without requiring as much noise addition in the subsequent DP-SGD phase.
- Core assumption: The randomized response mechanism provides sufficient label protection while allowing the model to learn effectively from the noisy labels.
- Evidence anchors:
  - [section] "In this phase, we train the truncated model Fw◦,wc(·) for one or more epochs of mini-batch SGD using noisy labels obtained by applying RRε1, which returns ((x◦i, ˆyi))i∈[n], i.e., a dataset where the x•i's are removed and the labels are randomized."
  - [section] "By Proposition 3, this phase satisfies (ε1, 0)-DP."
  - [corpus] Weak - the corpus papers do not directly address the specific mechanism of label protection through randomized response and its impact on subsequent DP-SGD training.
- Break condition: If the randomized response mechanism does not provide sufficient label protection or if the model cannot effectively learn from the noisy labels, the hybrid approach may not improve upon the baselines.

### Mechanism 3
- Claim: The hybrid approach allows for more efficient use of the privacy budget compared to treating all features as sensitive or discarding sensitive features entirely.
- Mechanism: By only protecting the labels and sensitive features in the first phase and then training on all features in the second phase, the hybrid approach avoids the need to protect all features throughout the entire training process.
- Core assumption: The sensitive features can be effectively learned from without requiring full DP protection, while still maintaining privacy for the labels and sensitive features.
- Evidence anchors:
  - [abstract] "In this setting, a subset of the features is known to the attacker (and thus need not be protected) while the remaining features as well as the label are unknown to the attacker and should be protected by the DP guarantee."
  - [section] "This task interpolates between training the model with full DP (where the label and all features should be protected) or with label DP (where all the features are considered known, and only the label should be protected)."
  - [corpus] Weak - the corpus papers do not directly address the specific mechanism of efficiently using the privacy budget by only protecting certain features and the label.
- Break condition: If the sensitive features cannot be effectively learned from without full DP protection, or if the privacy guarantees for the labels and sensitive features are not sufficient, the hybrid approach may not improve upon the baselines.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP is the core privacy guarantee that the hybrid approach aims to achieve while maintaining model utility.
  - Quick check question: What is the definition of (ε, δ)-DP, and how does it relate to the privacy guarantees of the hybrid approach?

- Concept: Randomized Response
  - Why needed here: Randomized response is the label-DP mechanism used in the first phase of the hybrid approach to protect the labels.
  - Quick check question: How does randomized response satisfy (ε, 0)-DP, and why is it used to protect the labels in the hybrid approach?

- Concept: DP-SGD
  - Why needed here: DP-SGD is the algorithm used in the second phase of the hybrid approach to train the model on all features while maintaining DP guarantees.
  - Quick check question: How does DP-SGD achieve (ε, δ)-DP, and how does it differ from standard SGD in terms of privacy protection?

## Architecture Onboarding

- Component map: Model architecture: Fw(x◦, x•) := fwc(gw◦(x◦) ◦ hw•(x•)) -> Truncated model: Fw◦,wc(x◦) := fwc(gw◦(x◦) ◦ 0) -> Label-DP phase: Randomized response applied to labels -> DP-SGD phase: DP-SGD training on all features -> Privacy budget split: ε1 := min {0.6 · ε, 3} and ε2 := ε − ε1

- Critical path:
  1. Apply randomized response to labels with privacy budget ε1
  2. Train truncated model on non-sensitive features and noisy labels
  3. Train full model on all features using DP-SGD with privacy budget ε2
  4. Evaluate model utility and privacy guarantees

- Design tradeoffs:
  - Privacy vs. utility: Increasing ε1 improves label protection but may reduce the effectiveness of the DP-SGD phase.
  - Model complexity: The hybrid approach requires training two separate models, which may increase computational cost.
  - Feature sensitivity: The effectiveness of the hybrid approach depends on the sensitivity of the features and the ability to learn from them without full DP protection.

- Failure signatures:
  - If the model utility is significantly worse than the baselines, it may indicate that the privacy budget split is suboptimal or that the model cannot effectively learn from the noisy labels.
  - If the privacy guarantees are not met, it may indicate that the DP-SGD phase is not adding enough noise or that the randomized response mechanism is not providing sufficient label protection.

- First 3 experiments:
  1. Implement the hybrid approach on a small dataset with known feature sensitivities and evaluate the model utility and privacy guarantees compared to the baselines.
  2. Vary the privacy budget split (ε1 and ε2) and evaluate the impact on model utility and privacy guarantees to determine the optimal split.
  3. Implement the hybrid approach on a larger, more complex dataset and compare the results to the baselines in terms of model utility and privacy guarantees.

## Open Questions the Paper Calls Out

- Question: How would alternative label-DP mechanisms beyond randomized response perform in the label-DP phase of hybrid training, particularly for small ε values?
  - Basis in paper: [explicit] The paper concludes with a future direction suggesting exploration of label-DP primitives beyond randomized response, especially ones that perform better for smaller ε.
  - Why unresolved: The current work only evaluates randomized response as the label-DP mechanism, leaving the performance of alternatives unexplored.
  - What evidence would resolve it: Empirical comparisons of hybrid training using different label-DP mechanisms (e.g., those from Ghazi et al. 2021 or Malek Esmaeili et al. 2021) across various ε values and datasets.

- Question: What is the precise utility gap between "DP with semi-sensitive features" and "DP with a public feature set" as defined by Krichene et al. (2023)?
  - Basis in paper: [explicit] The paper identifies this as an interesting direction for future work, noting that Krichene et al.'s setting assumes the adversary knows the set of values a subset of features takes but not the correspondence between examples and specific values.
  - Why unresolved: The current work does not compare against the "DP with a public feature set" setting, leaving the utility gap unquantified.
  - What evidence would resolve it: Direct empirical comparison of models trained under both settings using identical datasets and privacy budgets.

- Question: How does the performance of the hybrid algorithm change when the label itself is non-sensitive rather than semi-sensitive?
  - Basis in paper: [explicit] The paper mentions this as a variant of DP training with semi-sensitive features that would be interesting to explore.
  - Why unresolved: All experiments assume the label is sensitive and requires protection, so performance under non-sensitive label assumptions is unknown.
  - What evidence would resolve it: Experiments applying the hybrid approach with non-sensitive labels to determine if it maintains its advantage over baselines in this variant setting.

## Limitations
- The optimal privacy budget split may vary depending on the specific dataset and model architecture, limiting the generalizability of the approach.
- The paper does not address potential adversarial attacks or robustness of the approach against more sophisticated threat models.
- Extensive empirical validation across a diverse set of datasets and model architectures is lacking.

## Confidence
The confidence in the proposed hybrid approach is **Medium** based on the reported empirical results. The main claims about the hybrid approach outperforming baselines have **Medium** confidence, but further validation is needed to assess the approach's robustness and generalizability.

## Next Checks
1. Evaluate the hybrid approach on a broader range of datasets, including those with different feature distributions and sensitivities, to assess its generalizability.
2. Conduct ablation studies to determine the impact of varying the privacy budget split (ε1 and ε2) on model utility and privacy guarantees.
3. Investigate the robustness of the hybrid approach against potential adversarial attacks, such as feature inference or membership inference attacks, to assess its security guarantees.