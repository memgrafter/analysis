---
ver: rpa2
title: Mixture of Cache-Conditional Experts for Efficient Mobile Device Inference
arxiv_id: '2412.00099'
source_url: https://arxiv.org/abs/2412.00099
tags:
- cache
- experts
- expert
- routing
- cache-prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of deploying Mixture of Experts
  (MoE) models on memory-constrained devices, where expert weights must be cached
  in DRAM while the rest reside in slower flash storage. The authors introduce a cache-aware
  routing strategy that modifies router logits to favor experts already in DRAM, improving
  cache hit rates during sequential token generation.
---

# Mixture of Cache-Conditional Experts for Efficient Mobile Device Inference

## Quick Facts
- arXiv ID: 2412.00099
- Source URL: https://arxiv.org/abs/2412.00099
- Reference count: 40
- Primary result: Reduces cache miss rates by over 50% with <0.1% accuracy loss on mobile MoE models

## Executive Summary
This paper addresses the challenge of deploying Mixture of Experts (MoE) models on memory-constrained devices where expert weights must be cached in DRAM while the rest reside in slower flash storage. The authors introduce a cache-aware routing strategy that modifies router logits to favor experts already in DRAM, improving cache hit rates during sequential token generation. Experiments on four MoE architectures show their method reduces cache miss rates by over 50% with negligible impact on perplexity (0.1%-3%) and downstream task accuracy (<0.1%). On-device evaluations on mobile devices demonstrate up to 2× speedups in token generation throughput compared to standard LRU caching.

## Method Summary
The proposed method manipulates router logits by adding a bias to cached experts' logits, increasing their selection probability while maintaining the model's original routing behavior when possible. A single hyperparameter λ controls the trade-off between cache efficiency and predictive performance, allowing interpolation between baseline LRU performance (λ=0) and fully cache-driven selection (λ=1). The approach is training-free, model-agnostic, and works by leveraging the observation that MoE models exhibit low sensitivity to variations in lower-ranked expert selections. During inference, a running average of logit ranges is used to scale the bias appropriately across different layers and tokens.

## Key Results
- Reduces cache miss rates by over 50% compared to baseline LRU caching
- Maintains negligible impact on perplexity (0.1%-3%) and downstream task accuracy (<0.1%)
- Achieves up to 2× speedups in token generation throughput on mobile devices
- Surpasses theoretical cache hit rate bounds by allowing controlled flexibility in expert selection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MoE models can tolerate deviations in expert selection without significant performance loss.
- **Mechanism:** Lower-ranked experts (beyond the top 2) have minimal impact on model performance when swapped with cached experts.
- **Core assumption:** The router's selection is not always optimal, and there is redundancy in the lower-ranked expert choices.
- **Evidence anchors:**
  - [section] "Our findings indicate that these models exhibit low sensitivity to variations in the selection of lower-weighted experts, suggesting that lossy routing strategies can be implemented without substantial degradation in overall performance."
  - [section] "Replacing the second-ranked expert also causes noticeable degradation, but the model remains functional. Beyond this, in granular MoEs (with a larger number of smaller active experts), the model becomes highly resilient to expert selection changes."
  - [corpus] "Weak - corpus contains related work on caching strategies but does not directly validate the tolerance to expert swapping."

### Mechanism 2
- **Claim:** Cache locality can be improved by manipulating router logits to favor cached experts.
- **Mechanism:** Adding a bias to the logits of cached experts increases their probability of selection, improving cache hit rates.
- **Core assumption:** The logit range is relatively stable and can be estimated to scale the bias appropriately.
- **Evidence anchors:**
  - [section] "We use a Cache-Prior that directly manipulates the router logitsz =G(x) to increase the probability of selecting experts already present in cache."
  - [section] "By utilizing the true range of weights, our Cache-Prior becomes adaptive, allowing a single hyperparameter to yield distinct effects across different layers and tokens."
  - [corpus] "Weak - corpus contains related work on caching strategies but does not directly validate the logit manipulation approach."

### Mechanism 3
- **Claim:** The trade-off between cache efficiency and predictive performance can be controlled via a scaling parameter.
- **Mechanism:** The parameter λ controls the strength of the bias towards cached experts, allowing for a tunable balance between cache hit rate and model accuracy.
- **Core assumption:** There is a predictable relationship between the bias strength and the resulting cache hit rate and model accuracy.
- **Evidence anchors:**
  - [section] "This scaling factor allows our method to interpolate between the original routing (λ= 0, equivalent to baseline cache hit rate) and a fully cache-driven selection (λ= 1, potentially zero cache misses, but higher perplexity)."
  - [section] "Our approach introduces a controlled trade-off between accuracy and latency, enabling more flexible resource management."
  - [corpus] "Weak - corpus contains related work on caching strategies but does not directly validate the tunable trade-off approach."

## Foundational Learning

- **Concept:** Mixture of Experts (MoE) models
  - **Why needed here:** Understanding the MoE architecture is crucial for grasping how expert selection and caching work.
  - **Quick check question:** How does the MoE architecture differ from traditional dense models, and what are the implications for memory usage?

- **Concept:** Cache locality and hit rates
  - **Why needed here:** Improving cache locality is the primary goal of the proposed method, so understanding these concepts is essential.
  - **Quick check question:** What factors influence cache hit rates, and how can they be optimized?

- **Concept:** Router logits and expert selection
  - **Why needed here:** The proposed method manipulates router logits to influence expert selection, so understanding this mechanism is crucial.
  - **Quick check question:** How do router logits determine expert selection, and what are the implications of manipulating them?

## Architecture Onboarding

- **Component map:** Input token -> Router network -> Expert selection (modified by Cache-Prior) -> Expert execution (from cache or flash) -> Output token
- **Critical path:** Token generation involves routing network computation, expert selection, and expert execution, with cache hits reducing latency by avoiding flash storage access
- **Design tradeoffs:** Balancing cache hit rate and model accuracy via the λ parameter; choosing between different routing strategies (Max Rank, Cumulative Probability Threshold, Cache-Prior); selecting appropriate cache size based on available memory and model size
- **Failure signatures:** Low cache hit rates indicating poor cache locality; significant performance degradation when deviating from the router's top-ranked experts; unstable logit ranges leading to ineffective logit manipulation
- **First 3 experiments:**
  1. Evaluate the sensitivity of the MoE model to expert swapping to determine the tolerance for deviations from the router's selection
  2. Compare the performance of different routing strategies (Max Rank, Cumulative Probability Threshold, Cache-Prior) on a small dataset to identify the most effective approach
  3. Tune the λ parameter to find the optimal balance between cache hit rate and model accuracy for a specific deployment scenario

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. The analysis focuses on evaluating the proposed method's performance and comparing it against baseline approaches.

## Limitations

- The training-free claim may be overstated as it relies on a running average of logit ranges that must be estimated during inference
- The method's effectiveness on non-autoregressive tasks remains unclear, as evaluation focuses exclusively on sequential token generation
- The claim of surpassing theoretical cache hit rate bounds requires careful scrutiny as it may conflate different optimization objectives

## Confidence

**High Confidence:** The experimental results showing 50%+ reduction in cache miss rates and the basic mechanism of logit manipulation are well-supported by the evaluation data.

**Medium Confidence:** The assertion that the method is "model-agnostic" and works across different MoE architectures needs more validation beyond the four tested models.

**Low Confidence:** The claim about surpassing theoretical optimal cache eviction bounds is questionable as it may rely on assumptions about expert selection tolerance that don't hold for all architectures.

## Next Checks

1. **Cross-architecture generalization test:** Evaluate the method on additional MoE architectures beyond the four tested to verify true model-agnostic performance and identify any architecture-specific limitations.

2. **Dynamic input distribution stability:** Test the running average logit range estimation (∆avg) across varying input types and distributions to quantify performance degradation when encountering out-of-distribution prompts.

3. **Non-autoregressive task evaluation:** Apply the cache-aware routing strategy to non-sequential inference tasks (parallel decoding, batch processing) to determine if the expert selection tolerance generalizes beyond autoregressive generation.