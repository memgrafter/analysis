---
ver: rpa2
title: 'Beyond Scalars: Concept-Based Alignment Analysis in Vision Transformers'
arxiv_id: '2412.06639'
source_url: https://arxiv.org/abs/2412.06639
tags:
- concept
- alignment
- concepts
- representations
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a concept-based approach to analyzing representational
  alignment in vision transformers (ViTs). Traditional alignment measures yield a
  single scalar value, obscuring distinctions between common and unique features in
  different representations.
---

# Beyond Scalars: Concept-Based Alignment Analysis in Vision Transformers

## Quick Facts
- arXiv ID: 2412.06639
- Source URL: https://arxiv.org/abs/2412.06639
- Reference count: 40
- Key outcome: Concept-based alignment analysis reveals universal vs. unique features in ViTs better than scalar measures

## Executive Summary
This paper introduces a concept-based approach to analyzing representational alignment in vision transformers (ViTs). Traditional alignment measures yield a single scalar value, obscuring distinctions between common and unique features in different representations. To address this, the authors define concepts as nonlinear manifolds in feature space and use density-based clustering (HDBSCAN) on UMAP embeddings to discover them. They measure alignment between concept proximity scores using a generalized Rand index, partitioned into pairwise concept distances. Experiments on four ViTs trained with varying supervision reveal that increased supervision correlates with reduced semantic structure in learned representations. The concept-based method outperforms linear baselines in a sanity check and provides fine-grained insights into universal vs. unique concepts across models.

## Method Summary
The method extracts hidden layer activations from pre-trained ViT models, applies UMAP dimensionality reduction to embed these activations into a lower-dimensional space, and uses HDBSCAN clustering to discover concept manifolds. These concepts are described by soft membership scores rather than hard assignments. Alignment between representations is measured using a generalized Rand index on concept proximity scores, which is then partitioned into contributions from individual concept pairs. The approach is evaluated on four ViTs (FS, CLIP, DINO, MAE) trained with different supervision levels using ImageNet-1k.

## Key Results
- Concept-based alignment (CBA) provides fine-grained insights beyond single scalar alignment measures
- Increased supervision correlates with reduced semantic structure in learned ViT representations
- The method outperforms linear baselines (PCA, NLMCD) in a sanity check for alignment analysis
- Analysis reveals universal concepts shared across models and unique concepts specific to individual representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concept discovery via nonlinear manifolds captures latent representation structure better than linear baselines for alignment analysis.
- Mechanism: The method uses UMAP to embed high-dimensional activations into a lower-dimensional space preserving local geometry, then applies HDBSCAN to discover density-based clusters representing concept manifolds. These manifolds are described by soft concept membership scores rather than hard assignments or linear projections.
- Core assumption: The manifold hypothesis holds for vision transformer activations, meaning data concentrate on low-dimensional manifolds embedded in high-dimensional space.
- Evidence anchors:
  - [abstract] "define concepts as the most general structure they can possibly form - arbitrary manifolds"
  - [section] "According to the manifold hypothesis... many datasets... can be described in terms of a few underlying latent factors... concentrated on a (potentially disconnected) low-dimensional manifold"
  - [corpus] "Understanding Inter-Concept Relationships in Concept-Based Models" (weak evidence for inter-concept relationship analysis)

### Mechanism 2
- Claim: Partitioning the generalized Rand index into pairwise concept distances provides fine-grained alignment insights beyond single scalar measures.
- Mechanism: The method computes a pseudo-metric between fuzzy clusterings (concept proximity scores) and decomposes this into contributions from individual concept pairs. This reveals which specific concepts are shared or unique between representations.
- Core assumption: The generalized Rand index with appropriate distance metrics preserves meaningful alignment information when partitioned into concept-level distances.
- Evidence anchors:
  - [abstract] "measure distances between concept proximity scores of two representations, we use a generalized Rand index and partition it for alignment between pairs of concepts"
  - [section] "We choose this measure because dcross(P, Q) is a pseudo-metric satisfying desirable properties... ease interpretation"
  - [corpus] "Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation" (weak evidence for statistical rigor in concept interpretation)

### Mechanism 3
- Claim: Soft clustering with HDBSCAN provides more nuanced concept membership than hard clustering or linear methods.
- Mechanism: HDBSCAN's soft clustering produces membership probabilities for each concept, allowing features to belong to multiple concepts with varying degrees. This contrasts with hard clustering or linear projection methods that force binary or one-dimensional membership.
- Core assumption: Concept membership in neural representations is inherently fuzzy rather than binary, and the data supports probabilistic membership estimation.
- Evidence anchors:
  - [section] "We leverage soft clustering with HDBSCAN based on the condensed tree... We now have a fuzzy clustering P{ϕ} = {P(ϕ0), . . . ,P(ϕN )}"
  - [section] "This approach contrasts with previous concept assignment paradigms... which often rely on hard clustering"
  - [corpus] "ASCENT-ViT: Attention-based Scale-aware Concept Learning Framework for Enhanced Alignment in Vision Transformers" (weak evidence for attention-based concept learning)

## Foundational Learning

- Concept: Manifold hypothesis
  - Why needed here: The entire concept discovery approach relies on the assumption that high-dimensional activations lie on low-dimensional manifolds.
  - Quick check question: Can you explain why assuming data lies on manifolds embedded in high-dimensional space is useful for dimensionality reduction and clustering?

- Concept: Fuzzy clustering and soft membership
  - Why needed here: The method uses HDBSCAN to produce probabilistic concept memberships rather than hard assignments, which is crucial for the alignment measurement.
  - Quick check question: What's the difference between soft clustering (like HDBSCAN's approach) and hard clustering, and why might soft clustering be more appropriate for concept discovery?

- Concept: Generalized Rand index and pseudo-metrics
  - Why needed here: The alignment measurement between concept proximity scores uses a generalized Rand index that satisfies pseudo-metric properties, enabling meaningful partitioning.
  - Quick check question: What properties must a distance measure satisfy to be considered a pseudo-metric, and why are these important for comparing fuzzy clusterings?

## Architecture Onboarding

- Component map: Feature extraction -> UMAP embedding -> HDBSCAN clustering -> Soft membership computation -> Alignment measurement
- Critical path: Feature extraction → UMAP embedding → HDBSCAN clustering → Concept membership computation → Alignment measurement
- Design tradeoffs:
  - UMAP parameters (n_neighbors, min_dist, n_components) vs. embedding quality vs. computational cost
  - HDBSCAN parameters (min_cluster_size, min_samples) vs. cluster granularity vs. noise handling
  - Concept count vs. interpretability vs. alignment resolution
  - Sampling rate vs. statistical reliability vs. computational feasibility
- Failure signatures:
  - High RMSE between original and embedded distances → Poor embedding quality
  - Low DBCV (density-based clustering validity) → Poor clustering quality
  - High noise rate in HDBSCAN → Insufficient data density or inappropriate parameters
  - Low robustness between runs → Unstable concept discovery
- First 3 experiments:
  1. Run concept discovery on a single model (e.g., FS) with default UMAP/HDBSCAN parameters and visualize the concept atlas to verify basic functionality
  2. Compare alignment results using NLMCD concepts vs. PCA/linear baselines on a simple model pair to validate the sanity check
  3. Analyze intra-model alignment across layers for one model to observe the nucleation process and concept evolution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the concept structure in ViT representations change when trained on datasets other than ImageNet-1k, particularly those with more diverse or domain-specific content?
- Basis in paper: [inferred] The paper uses ImageNet-1k for concept discovery and alignment analysis, but notes that the limited variability of ImageNet-1k might obscure the meaning of concepts (e.g., a concept representing a color but only dog patches of that color are present).
- Why unresolved: The study is constrained to ImageNet-1k, so it's unclear how concept discovery and alignment would generalize to other datasets with different characteristics or domain-specific features.
- What evidence would resolve it: Conducting concept-based alignment analysis on ViTs trained on diverse datasets (e.g., COCO, medical imaging datasets, satellite imagery) and comparing the resulting concept structures and alignment patterns.

### Open Question 2
- Question: What is the impact of varying the number of tokens sampled for concept discovery on the quality and interpretability of the discovered concepts and subsequent alignment analysis?
- Basis in paper: [explicit] The paper mentions that the number of input samples is restricted computationally by UMAP and HDBSCAN, and that high noise rates may be due to insufficiently dense sampling, i.e., thorough sampling of noisy regions could result in concept clusters.
- Why unresolved: The paper uses a fixed 25% subset of ImageNet-1k for concept discovery but doesn't explore how different sampling rates affect the results. The trade-off between computational cost and sampling density remains unclear.
- What evidence would resolve it: Systematically varying the sampling rate (e.g., 10%, 25%, 50%, 100%) and evaluating the impact on DBCV scores, noise rates, concept interpretability, and alignment analysis robustness across different ViT models and layers.

### Open Question 3
- Question: How do different density-based clustering algorithms (e.g., OPTICS, DBSCAN) compare to HDBSCAN in terms of concept discovery quality and alignment analysis performance in ViT representations?
- Basis in paper: [inferred] The paper uses HDBSCAN for density-based clustering due to its ability to handle clusters of varying densities and robust handling of noise, but doesn't compare it to other density-based clustering methods.
- Why unresolved: While HDBSCAN is shown to work well, there's no empirical comparison to alternative density-based clustering algorithms that might offer different trade-offs in terms of concept discovery and alignment quality.
- What evidence would resolve it: Implementing and comparing HDBSCAN with other density-based clustering algorithms (e.g., OPTICS, DBSCAN) on the same ViT representations, evaluating clustering validity indices (DBCV, Silhouette score), concept interpretability, and alignment analysis results across different models and layers.

## Limitations
- The method assumes the manifold hypothesis holds for ViT representations without empirical validation
- Hyperparameter choices for UMAP and HDBSCAN significantly impact results but lack sensitivity analysis
- Limited exploration of how sampling rate affects concept discovery quality and alignment analysis

## Confidence
- **High confidence**: The basic concept discovery pipeline (UMAP → HDBSCAN → concept proximity scores) works as described
- **Medium confidence**: The claim that increased supervision correlates with reduced semantic structure is supported by experiments but could have alternative explanations
- **Low confidence**: The assertion that the method "outperforms linear baselines" needs more rigorous comparison with specific alternative approaches

## Next Checks
1. Perform ablation studies varying UMAP and HDBSCAN hyperparameters to establish robustness of concept discovery across parameter choices
2. Compare alignment results using the proposed method against alternative concept discovery approaches (e.g., supervised concept learning or different manifold learning techniques)
3. Validate the manifold hypothesis assumption by testing whether discovered concepts maintain their structure when varying the number of samples and checking for concept stability across different random seeds