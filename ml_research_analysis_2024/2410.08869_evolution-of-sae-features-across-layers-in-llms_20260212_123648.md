---
ver: rpa2
title: Evolution of SAE Features Across Layers in LLMs
arxiv_id: '2410.08869'
source_url: https://arxiv.org/abs/2410.08869
tags:
- features
- feature
- related
- similarity
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes how Sparse Autoencoder (SAE) features evolve
  across transformer layers by measuring statistical relationships between adjacent-layer
  features. They construct feature graphs using four similarity measures (Pearson
  correlation, Jaccard similarity, sufficiency, and necessity) and apply community
  detection algorithms to find semantically related subgraphs.
---

# Evolution of SAE Features Across Layers in LLMs

## Quick Facts
- **arXiv ID**: 2410.08869
- **Source URL**: https://arxiv.org/abs/2410.08869
- **Reference count**: 40
- **Primary result**: Analysis of how Sparse Autoencoder (SAE) features evolve across transformer layers, finding 50-80% "pass-through" features and quasi-boolean relationships

## Executive Summary
This paper investigates how Sparse Autoencoder (SAE) features evolve across layers in GPT-2-small by measuring statistical relationships between adjacent-layer features. The authors construct feature graphs using four similarity measures (Pearson correlation, Jaccard similarity, sufficiency, and necessity) and apply community detection algorithms to identify semantically related subgraphs. They find that a significant portion of features are "passed through" from previous layers, some features can be expressed as quasi-boolean combinations of previous features, and features often become more specialized in later layers. The study also identifies instances of incorrect autointerp explanations by finding high-similarity feature pairs with inconsistent explanations.

## Method Summary
The authors compute pairwise similarity measures between SAE features across adjacent layers in GPT-2-small using activations from 10M tokens of The Pile dataset. They calculate four similarity metrics (Pearson correlation, Jaccard similarity, sufficiency, and necessity) for each feature pair, creating similarity matrices for 11 layer pairs. The similarity graphs are then analyzed using Leiden community detection to find semantically coherent feature groups. The analysis is validated through ablation experiments to test causal relationships, and the results are made available through a public visualization interface.

## Key Results
- 50-80% of features show high Pearson correlation (>0.95) with next-layer features, indicating "pass-through" behavior
- Some features can be expressed as quasi-boolean combinations (AND/OR gates) of previous features based on high necessity/sufficiency scores
- Features often become more specialized in later layers, with general upstream features evolving into downstream features detecting the same concepts in more specific contexts
- The authors identify incorrect autointerp explanations by finding high-similarity feature pairs with inconsistent explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A significant portion of SAE features (50-80%) are "passed through" from previous layers without substantial transformation
- Mechanism: Residual connections allow features to propagate through multiple layers while maintaining their semantic meaning, as evidenced by high Pearson correlation (>0.95) between adjacent-layer features
- Core assumption: Features with high statistical similarity between layers represent semantically equivalent concepts that persist through the residual stream
- Evidence anchors:
  - [abstract] "a considerable amount of features are passed through from a previous layer"
  - [section 3.1] "more than half of the features are appearing and disappearing at each layer"
  - [corpus] Weak evidence - corpus lacks papers specifically measuring pass-through features
- Break condition: If the residual stream structure changes dramatically between layers, or if SAEs in different layers learn incompatible feature representations

### Mechanism 2
- Claim: Some features can be expressed as quasi-boolean combinations (AND/OR gates) of previous features
- Mechanism: When multiple upstream features have very high necessity/sufficiency scores with a downstream feature, the downstream feature fires only when specific combinations of upstream features are active
- Core assumption: High necessity/sufficiency values (e.g., ≥0.999) approximate logical implications between feature activations
- Evidence anchors:
  - [abstract] "some features can be expressed as quasi-boolean combinations of previous features"
  - [section 3.2] Tables showing AND/OR relationships between features
  - [corpus] Weak evidence - corpus lacks papers specifically exploring boolean combinations of SAE features
- Break condition: If the sparsity assumption breaks down (too many features co-activating) or if the similarity measures don't capture true causal relationships

### Mechanism 3
- Claim: Features often become more specialized in later layers
- Mechanism: General upstream features detecting concepts in multiple contexts evolve into downstream features that detect the same concepts in more specific contexts
- Core assumption: The model learns to decompose general concepts into more specialized sub-concepts as information flows through layers
- Evidence anchors:
  - [abstract] "some features become more specialized in later layers"
  - [section 3.4] "we can trace 'general' upstream features which detect a token in several contexts to features which appear to 'specialize' in detecting that same token in mutually exclusive contexts"
  - [corpus] Weak evidence - corpus lacks papers specifically documenting feature specialization across layers
- Break condition: If feature specialization doesn't follow a clear pattern, or if SAEs fail to capture the granularity of concept decomposition

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs) and their role in mechanistic interpretability
  - Why needed here: The paper builds entirely on SAE features as the basis for analyzing cross-layer relationships
  - Quick check question: What is the primary purpose of using SAEs on transformer activations?

- Concept: Correlation vs. causation in feature relationships
  - Why needed here: The paper acknowledges that similarity measures show correlation but attempts to validate causal relationships through ablation studies
  - Quick check question: Why can't correlation measures alone prove that one feature causes another to fire?

- Concept: Community detection algorithms (Louvain, Leiden)
  - Why needed here: Used to find semantically meaningful subgraphs in the feature similarity graph
  - Quick check question: What is the main difference between Louvain and Leiden community detection algorithms?

## Architecture Onboarding

- Component map: tokens → model activations → SAE features → pairwise similarity measures → graph visualization
- Critical path: Computing similarity matrices for 11 layer pairs with 4 similarity measures each, requiring ~100GB of raw data processing
- Design tradeoffs: Choosing between statistical measures (Pearson, Jaccard, necessity, sufficiency) versus geometric measures (cosine similarity of decoder weights)
- Failure signatures: High NaN rates in similarity matrices indicate insufficient co-activation data; low similarity values suggest features aren't meaningfully related
- First 3 experiments:
  1. Replicate the pass-through feature analysis by computing the percentage of features with Pearson correlation >0.95 to next-layer features
  2. Test the AND/OR gate hypothesis by finding downstream features with exactly two upstream neighbors having high necessity/sufficiency scores
  3. Validate community detection results by checking if intra-community feature cosine similarity exceeds 0.75

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How accurately do statistical correlation measures predict true causal relationships between SAE features across layers?
- Basis in paper: [explicit] The authors acknowledge their approach measures correlation, not causation, and perform ablation studies showing correlation weakly correlates with causal effect
- Why unresolved: The ablation results show correlation is only a weak predictor of causation, and the authors note that redundancy between upstream features can mask true causal effects
- What evidence would resolve it: Systematic ablation studies varying the number of simultaneously ablated features, or direct causal inference methods applied to SAE features

### Open Question 2
- Question: What mathematical mechanism underlies feature specialization where general upstream features become more specific in downstream layers?
- Basis in paper: [explicit] The authors observe features becoming more specialized in later layers but note they "do not have a theory describing how feature specialization occurs"
- Why unresolved: The paper only observes this phenomenon qualitatively through examples but doesn't provide a mathematical explanation for how the specialization emerges from the network's computations
- What evidence would resolve it: Analysis of SAE decoder weight changes between layers, or mathematical characterization of how residual stream representations evolve to support specialization

### Open Question 3
- Question: What is the optimal strategy for SAE dictionary design across layers to minimize redundancy while maintaining representation power?
- Basis in paper: [explicit] The authors find that 50-80% of features are "passed through" layers unchanged, suggesting potential inefficiency in current SAE designs
- Why unresolved: The paper identifies the phenomenon but doesn't explore architectural modifications to SAEs that could reduce this redundancy or improve efficiency
- What evidence would resolve it: Experiments comparing performance of layer-tied SAEs versus independent SAEs, or analysis of reconstruction error when removing passed-through features

### Open Question 4
- Question: How does the choice of similarity measure affect the quality and interpretability of detected feature communities?
- Basis in paper: [explicit] The authors compare four similarity measures (Pearson, Jaccard, necessity, sufficiency) and note differences in interpretability, but don't systematically evaluate which produces the most semantically meaningful communities
- Why unresolved: While the paper shows examples of communities from different measures, it doesn't provide a rigorous comparison of community quality across measures
- What evidence would resolve it: Human evaluation of community interpretability across different similarity measures, or quantitative metrics comparing community coherence across measures

## Limitations

- The study relies on statistical similarity measures that show correlation but may not capture true causal relationships between features
- The analysis is limited to GPT-2-small with a specific SAE implementation, raising questions about generalizability to other architectures
- The boolean combination hypothesis depends on interpreting extreme necessity/sufficiency values as logical implications, which may not reflect genuine causal relationships

## Confidence

**High confidence**: The finding that 50-80% of features show high Pearson correlation (>0.95) between adjacent layers is well-supported by the data and aligns with known residual connection behavior in transformers. The feature graph construction methodology and visualization interface are technically sound.

**Medium confidence**: The identification of AND/OR gate relationships between features requires careful interpretation of necessity/sufficiency scores. While the methodology is rigorous, the leap from statistical correlation to logical implication needs more validation across different SAE implementations.

**Low confidence**: Claims about feature specialization across layers are suggestive but not definitively proven. The evidence shows correlation between general and specialized features, but the causal mechanism of how features become more specialized through the network remains unclear.

## Next Checks

1. **Cross-implement validation**: Replicate the pass-through feature analysis using a different SAE implementation (e.g., OpenAI's dense SAEs) on the same GPT-2-small model to verify that the 50-80% pass-through rate is not implementation-specific.

2. **Causal intervention study**: Design ablation experiments that directly test the AND/OR gate hypothesis by systematically activating/deactivating upstream features and measuring the impact on downstream feature activation, rather than relying solely on necessity/sufficiency scores.

3. **Temporal stability analysis**: Track how feature similarity patterns evolve when running the same analysis on different subsets of the dataset or at different points during model pretraining to assess whether observed relationships are stable features of the model's semantic representation or artifacts of the specific training data distribution.