---
ver: rpa2
title: The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks
arxiv_id: '2405.08886'
source_url: https://arxiv.org/abs/2405.08886
tags:
- adversarial
- prediction
- training
- conformal
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the uncertainty quantification of deep
  learning models under standard adversarial attacks using conformal prediction (CP).
  The authors find that CP fails to produce informative prediction sets for non-adversarially
  trained models under strong adversarial attacks, highlighting the importance of
  adversarial training for CP.
---

# The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks

## Quick Facts
- arXiv ID: 2405.08886
- Source URL: https://arxiv.org/abs/2405.08886
- Reference count: 40
- Adversarial training improves CP performance but increases prediction set sizes; proposed AT-UR method reduces this trade-off

## Executive Summary
This paper investigates how conformal prediction (CP) performs under adversarial attacks on deep learning models. The authors demonstrate that CP fails to provide meaningful uncertainty quantification when applied to non-adversarially trained models under strong adversarial attacks, as prediction sets become nearly as large as the entire label space. While adversarial training (AT) improves CP coverage, it often leads to larger prediction sets, compromising efficiency. To address this, the paper proposes an uncertainty-reducing adversarial training (AT-UR) method that combines entropy minimization with a novel Beta-weighting loss based on True Class Probability Ranking (TCPR). Theoretical analysis shows the Beta-weighting loss bounds prediction set size, and extensive experiments validate that AT-UR effectively reduces prediction set sizes while maintaining adversarial robustness across multiple datasets and AT baselines.

## Method Summary
The paper proposes AT-UR, a method that combines entropy minimization and Beta-weighting loss based on TCPR to improve CP efficiency under adversarial attacks. The approach builds upon three AT baselines (AT, TRADES, MART) by adding two components: an entropy minimization regularization term that encourages confident predictions, and a Beta-weighting loss that penalizes uncertain predictions more heavily based on their TCPR. The Beta-weighting function uses parameters a=1.1 and b∈{3.0,4.0,5.0} to shape the penalty curve. The method is evaluated using Adaptive Prediction Sets (APS) conformal prediction under AutoAttack and PGD100 attacks across four image classification datasets (CIFAR10, CIFAR100, Caltech256, CUB200) with adversarially pre-trained ResNet50 models.

## Key Results
- CP fails to produce informative prediction sets for non-adversarially trained models under strong attacks (APS close to number of classes)
- AT-UR methods (AT-Beta, AT-Beta-EM) achieve significant PSS reduction compared to standard AT baselines while maintaining coverage
- The Beta-weighting loss provides an upper bound for prediction set size at the population level, with theoretical validation

## Why This Works (Mechanism)
The proposed AT-UR method works by explicitly optimizing for prediction set efficiency during adversarial training. The entropy minimization encourages the model to make confident predictions on adversarial examples, while the Beta-weighting loss penalizes uncertain predictions more heavily based on their TCPR ranking. This dual approach ensures that the model not only remains robust to adversarial attacks but also produces more confident predictions that translate to smaller, more efficient prediction sets under CP. The TCPR-based weighting is particularly effective because it directly targets the ranking of class probabilities that determines prediction set boundaries in CP.

## Foundational Learning
**Conformal Prediction (CP)**: A framework for uncertainty quantification that produces prediction sets with guaranteed coverage probability. Needed because standard neural network predictions lack calibrated uncertainty estimates. Quick check: Verify CP coverage on clean data before adversarial testing.

**Adversarial Training (AT)**: Training procedure that improves model robustness by incorporating adversarial examples during training. Needed because standard models are vulnerable to small input perturbations. Quick check: Confirm robust accuracy under PGD attacks.

**True Class Probability Ranking (TCPR)**: Ranking of class probabilities where the true class probability is used as a reference point. Needed to identify which predictions are most uncertain for the Beta-weighting loss. Quick check: Validate TCPR calculation by checking that true class probability is correctly identified.

**Beta-weighting Loss**: A loss function that applies different penalties based on TCPR using a Beta distribution shape. Needed to emphasize reducing uncertainty for the most problematic predictions. Quick check: Plot the weighting function to verify it increases with TCPR.

**Adaptive Prediction Sets (APS)**: A specific CP method that constructs prediction sets based on the cumulative probability of classes. Needed for practical CP implementation under adversarial settings. Quick check: Verify APS prediction sets cover the true class with the desired probability.

## Architecture Onboarding

Component Map: ResNet50 -> AT Framework -> Entropy Minimization -> Beta-weighting Loss -> CP Evaluation

Critical Path: The most important computational path is from the adversarial example generation through the AT framework to the final loss computation. The Beta-weighting loss calculation depends on TCPR, which requires the model's output probabilities to be computed and sorted. The entropy minimization term is computed in parallel with the main AT loss but contributes to the final gradient update.

Design Tradeoffs: The main tradeoff is between adversarial robustness and CP efficiency. Stronger AT typically improves coverage but increases prediction set sizes. The Beta-weighting parameters (a=1.1, b∈{3.0,4.0,5.0}) control the aggressiveness of uncertainty reduction, with higher b values placing more emphasis on reducing uncertainty for the most confident predictions.

Failure Signatures: If prediction sets remain large despite AT-UR, check that the Beta-weighting parameters are correctly implemented and that TCPR calculation properly identifies the true class probability index. If coverage drops below the target, verify that the entropy minimization strength (λEM=0.3) is not too aggressive.

First Experiments: 1) Run standard AT baseline to establish baseline coverage and PSS, 2) Implement AT-EM variant to test entropy minimization effect in isolation, 3) Compare AT-Beta with different b parameters to find optimal setting for each dataset.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed Beta-weighting loss behave in the context of other regularization techniques, such as label smoothing or mixup, when combined with adversarial training?
The paper briefly mentions that label smoothing and AT-EM have different effects on prediction entropy, but does not explore their combination or compare them with the Beta-weighting loss in a systematic way. The paper does not provide a comprehensive comparison of different regularization techniques and their impact on CP-efficiency when combined with adversarial training. A thorough empirical study comparing the CP-efficiency of models trained with various combinations of regularization techniques, including label smoothing, mixup, and the proposed Beta-weighting loss, under different adversarial attacks and datasets would resolve this question.

### Open Question 2
Can the theoretical bound provided in Theorem 5.1 be further tightened or improved to provide a more accurate estimate of the prediction set size?
The paper presents a theoretical bound for the expected size of CP prediction sets, but it is acknowledged that the bound might not be tight and could potentially be improved. The theoretical analysis in the paper focuses on establishing the connection between the Beta-weighting loss and the prediction set size, but does not explore ways to tighten or improve the bound. A more refined theoretical analysis that either tightens the existing bound or provides a new bound with a smaller constant, along with empirical validation of the improved bound's accuracy in predicting the prediction set size, would resolve this question.

### Open Question 3
How does the proposed AT-UR method perform in the context of more complex and realistic adversarial attacks, such as adaptive attacks or attacks that specifically target the CP mechanism?
The paper primarily focuses on the performance of AT-UR under standard adversarial attacks, such as PGD and AutoAttack, but does not explore its behavior under more sophisticated or adaptive attacks. The paper does not provide an in-depth analysis of the robustness of AT-UR against advanced adversarial attacks that might exploit the CP mechanism or adapt to the model's defenses. An extensive empirical study evaluating the performance of AT-UR under a variety of adaptive and CP-specific adversarial attacks, comparing it with other state-of-the-art defense methods, and analyzing the effectiveness of different attack strategies against AT-UR would resolve this question.

## Limitations
- Theoretical analysis relies on simplifying assumptions about Beta distribution parameters and has index notation ambiguity
- Experimental validation uses relatively small ResNet50 models rather than modern larger architectures
- Comparison between AT methods conducted within original hyperparameter settings rather than systematically tuned

## Confidence
- Theoretical analysis of Beta-weighting loss: Medium - sound in principle but with noted index ambiguity
- Empirical results on prediction set size reduction: High - consistent across multiple datasets and baselines
- Claims about AT-UR superiority over standard AT methods: Medium - results show improvement but hyperparameter sensitivity is unclear

## Next Checks
1. Verify the Beta-weighting function implementation with both 0-based and 1-based indexing to confirm which yields correct TCPR ordering
2. Conduct ablation studies on the entropy minimization strength (λEM) across different values to assess sensitivity
3. Extend experiments to larger model architectures (e.g., ResNet101, WideResNet) to validate scalability of the findings