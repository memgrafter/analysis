---
ver: rpa2
title: 'MAVEN-Fact: A Large-scale Event Factuality Detection Dataset'
arxiv_id: '2407.15352'
source_url: https://arxiv.org/abs/2407.15352
tags:
- event
- factuality
- annotation
- events
- maven-fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAVEN-FACT, a large-scale event factuality
  detection dataset based on the MAVEN dataset. MAVEN-FACT includes factuality annotations
  of 112,276 events, making it the largest EFD dataset.
---

# MAVEN-Fact: A Large-scale Event Factuality Detection Dataset

## Quick Facts
- arXiv ID: 2407.15352
- Source URL: https://arxiv.org/abs/2407.15352
- Reference count: 40
- Introduces MAVEN-FACT, a dataset with 112,276 event factuality annotations, making it the largest EFD dataset to date.

## Executive Summary
This paper introduces MAVEN-FACT, a large-scale dataset for event factuality detection built upon the MAVEN event dataset. The dataset includes 112,276 events annotated with five factuality categories (CT+, CT-, PS+, PS-, Uu) and supporting evidence words. The authors demonstrate that MAVEN-FACT is challenging for both conventional fine-tuned models and large language models (LLMs). They also show that incorporating event arguments and relations helps fine-tuned models but not LLMs, and preliminarily explore using factuality information to mitigate event-related hallucinations in LLMs.

## Method Summary
The paper presents a comprehensive approach to event factuality detection using the MAVEN-FACT dataset. They employ LLM pre-annotation to reduce human annotation costs, followed by human verification. The dataset includes five factuality categories and supporting evidence annotations. Models evaluated include fine-tuned EFD models (BERT, RoBERTa, DMBERT variants, GenEFD) and LLMs with in-context learning (Mistral 7B, LLAMA 3, GPT-3.5, GPT-4). The paper also explores the impact of event arguments and relations on detection performance and investigates applications in hallucination mitigation.

## Key Results
- MAVEN-FACT is the largest EFD dataset with 112,276 events, making it challenging for both fine-tuned models and LLMs
- Fine-tuned models benefit from event arguments and relations, but LLMs with in-context learning do not
- Incorporating event factuality information helps mitigate event-related hallucinations in LLMs
- Supporting evidence prediction performance is significantly lower than factuality detection, with ~30% of errors from only supporting word prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-annotating CT+ events with an LLM significantly reduces human annotation cost while maintaining high data quality.
- Mechanism: Most events are factual (CT+), so using GPT-3.5 to pre-classify events as CT+ vs non-CT+ allows filtering out the majority of events that don't require human annotation. The chain-of-thought prompting approach improves recall of non-CT+ events to 97.4%, ensuring minimal noise in the pre-annotated CT+ set.
- Core assumption: The distribution of factuality classes in the dataset is skewed heavily toward CT+, making automated pre-annotation of CT+ events highly effective.
- Evidence anchors:
  - [abstract]: "Due to most events (exceeds 80%) being factual, we can endeavor to pre-annotate them automatically."
  - [section]: "We simplify the event factuality detection task into a binary classification problem... binary classification is generally simpler than multi-classification."
- Break condition: If the factuality distribution is more balanced or if the LLM's recall for non-CT+ events drops significantly, the pre-annotation approach would introduce too much noise and fail.

### Mechanism 2
- Claim: Incorporating event arguments and relations improves factuality detection performance for fine-tuned models but not for LLMs with in-context learning.
- Mechanism: Fine-tuned models can leverage the explicit information about event arguments (like "time") and relations (like "causes") as additional features, learning correlations between these elements and factuality. LLMs with few-shot prompting don't effectively utilize this additional information and may even perform worse due to prompt sensitivity.
- Core assumption: Fine-tuned models can learn generalizable patterns from event structure features, while few-shot LLMs are sensitive to prompt formatting and may not extract relevant information from additional context.
- Evidence anchors:
  - [section]: "For fine-tuned EFD models, DMRoBERTa and GenEFD, the experimental results generally align with our expectations, where the introduction of event arguments or relations tends to boost factuality detection performance."
  - [section]: "For LLMs with in-context learning, introducing additional information tends to worsen performance... This suggests that few-shot in-context learning might introduce some biases instead of generalizable patterns."
- Break condition: If the event arguments and relations are noisy or if the model architecture changes to better handle in-context learning with additional information.

### Mechanism 3
- Claim: Providing explicit event factuality information to LLMs reduces event-related hallucinations in knowledge-intensive question answering.
- Mechanism: Event-related hallucinations occur when LLMs make incorrect inferences about event factuality. By explicitly providing the factuality label (either ground truth or model-predicted), the LLM can better constrain its reasoning and avoid incorrect conclusions about events.
- Core assumption: Hallucinations in event-related QA stem primarily from incorrect assessment of event factuality, and explicit factuality labels can correct this deficiency.
- Evidence anchors:
  - [abstract]: "we preliminarily study a potential application case of event factuality detection in mitigating event-related hallucination (Huang et al., 2023b), and find that incorporating event factuality can help mitigate hallucination in LLMs."
  - [section]: "We can observe that adding factuality information (Oracle setting) significantly improves the accuracy of LLMs, i.e., reducing the hallucination rate."
- Break condition: If hallucinations stem from other sources beyond event factuality (e.g., entity linking, temporal reasoning) or if the factuality labels themselves are incorrect.

## Foundational Learning

- Concept: Event Factuality Detection (EFD)
  - Why needed here: Understanding the task definition and label taxonomy (CT+, CT-, PS+, PS-, Uu) is fundamental to working with MAVEN-FACT and designing appropriate models.
  - Quick check question: What are the five factuality classes in MAVEN-FACT and what do they represent in terms of polarity and modality?

- Concept: Chain-of-Thought Prompting
  - Why needed here: The paper uses CoT prompting for both data annotation and LLM experiments, so understanding this technique is crucial for replicating or extending the work.
  - Quick check question: How does chain-of-thought prompting differ from direct prompting in the context of event factuality detection?

- Concept: Supporting Evidence Prediction
  - Why needed here: MAVEN-FACT includes supporting word annotations, and the paper evaluates models on predicting these words, making this concept important for understanding the dataset's unique features.
  - Quick check question: Why is predicting supporting words for factuality more challenging than predicting factuality itself?

## Architecture Onboarding

- Component map: MAVEN-FACT dataset (events, arguments, relations, factuality labels) -> Fine-tuned EFD models (BERT, RoBERTa, DMBERT variants, GenEFD) -> LLM approaches (Mistral 7B, LLAMA 3, GPT-3.5, GPT-4) with in-context learning -> Analysis of task interaction and applications
- Critical path: Data annotation (LLM pre-annotation + human verification) -> Dataset construction -> Model training/evaluation -> Analysis of task interaction and applications
- Design tradeoffs: The paper trades annotation cost for data quality by using LLM pre-annotation with human verification, and trades model performance for explainability by requiring supporting word predictions
- Failure signatures: Poor performance on non-CT+ classes indicates dataset challenges; worse performance with added arguments/relations for LLMs suggests prompt sensitivity; high error rates in supporting word prediction reveal limitations in model interpretability
- First 3 experiments:
  1. Replicate the baseline EFD model results (BERT+CLS, RoBERTa+CLS, DMBERT, DMRoBERTa, GenEFD) on the MAVEN-FACT test set to verify dataset difficulty
  2. Test whether adding event arguments and relations improves factuality detection for fine-tuned models following the paper's methodology
  3. Evaluate whether providing event factuality information to an LLM reduces hallucinations in a simple QA task using a subset of MAVEN-FACT documents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of event arguments and relations impact the performance of large language models (LLMs) in event factuality detection, and why do these models not benefit from this additional information?
- Basis in paper: [explicit] The paper states that while adding event arguments and relations enhances the performance of fine-tuned EFD models, it does not benefit LLMs with in-context learning.
- Why unresolved: The paper provides a hypothesis that few-shot in-context learning might introduce biases instead of generalizable patterns, but this is not conclusively proven.
- What evidence would resolve it: Conducting experiments with many-shot in-context learning or fine-tuning LLMs on the MAVEN-FACT dataset could provide insights into whether the lack of benefit is due to the learning approach or the models' inherent limitations.

### Open Question 2
- Question: What are the potential reasons for the significant drop in performance when using the chain-of-thought (CoT) prompting method for LLMs in event factuality detection, particularly for CT- and PS- categories?
- Basis in paper: [explicit] The paper shows that while CoT improves LLMs' performance, the results are still significantly lower than those of fine-tuned models, especially for CT- and PS- categories.
- Why unresolved: The paper does not explore the reasons behind the drop in performance for specific categories or why the CoT method is less effective for LLMs in this task.
- What evidence would resolve it: Analyzing the CoT outputs to identify common errors or patterns in the reasoning process could help understand the limitations of this approach for LLMs in event factuality detection.

### Open Question 3
- Question: How effective are the supporting evidence predictions in improving the reliability of event factuality detection models, and what are the main sources of error in these predictions?
- Basis in paper: [explicit] The paper evaluates the supporting evidence prediction task and finds that the performance is significantly lower than event factuality detection, with about 30% of errors coming from only supporting word prediction.
- Why unresolved: The paper does not provide a detailed analysis of the types of errors in supporting evidence predictions or how these errors impact the overall reliability of the models.
- What evidence would resolve it: Conducting a more granular error analysis to categorize the types of errors in supporting evidence predictions and their impact on model reliability would provide insights into how to improve this aspect of event factuality detection.

### Open Question 4
- Question: What are the implications of using event factuality detection to mitigate event-related hallucinations in large language models, and how can this approach be further improved?
- Basis in paper: [explicit] The paper presents a preliminary study showing that incorporating event factuality information can help mitigate event-related hallucinations in LLMs.
- Why unresolved: The paper does not explore the broader implications of this approach or provide detailed strategies for improving its effectiveness in real-world applications.
- What evidence would resolve it: Conducting extensive experiments with different types of event-related hallucinations and exploring various methods for integrating event factuality information into LLMs could provide insights into the broader applicability and effectiveness of this approach.

## Limitations
- The LLM pre-annotation approach lacks transparency in prompt templates and validation procedures
- The differential effects of event arguments/relations on fine-tuned vs. LLM models are supported by results but lack detailed methodological transparency
- The hallucination mitigation application case is only preliminarily explored with limited empirical validation

## Confidence

- **High Confidence**: Dataset construction methodology and basic task definitions are well-documented and reproducible. Evaluation metrics and experimental setup for fine-tuned models are clearly specified.
- **Medium Confidence**: LLM annotation pipeline and differential effects of event arguments/relations are supported by results but lack detailed methodological transparency.
- **Low Confidence**: Hallucination mitigation application case is only preliminarily explored with limited empirical validation and unclear generalization potential.

## Next Checks

1. **Detailed Error Analysis**: Perform a comprehensive breakdown of model failures across the five factuality classes to understand whether poor performance on non-CT+ categories stems from dataset bias, model limitations, or annotation inconsistencies.

2. **Prompt Sensitivity Study**: Systematically vary the format and content of prompts when adding event arguments and relations to LLMs to quantify the sensitivity and identify optimal prompt structures that might overcome the observed performance degradation.

3. **Hallucination Mitigation Validation**: Extend the hallucination mitigation study beyond the preliminary results by testing across multiple domains, using diverse QA tasks, and comparing against alternative hallucination mitigation strategies to establish the robustness and general applicability of event factuality information.