---
ver: rpa2
title: 'It Couldn''t Help But Overhear: On the Limits of Modelling Meta-Communicative
  Grounding Acts with Supervised Learning'
arxiv_id: '2405.01139'
source_url: https://arxiv.org/abs/2405.01139
tags:
- dialogue
- linguistics
- association
- computational
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that supervised learning models trained on human\
  \ dialogue data act as overhearers, not participants, missing essential grounding\
  \ acts needed for true understanding. By analyzing clarification request (CR) decisions\
  \ in a multimodal collaborative task, the authors show low agreement between overhearers\
  \ and original speakers (F1\u22480.51), as well as among overhearers themselves\
  \ (Krippendorff\u2019s \u03B1\u22480.02, pairwise Cohen\u2019s \u03BA\u22480.18),\
  \ suggesting human grounding acts lack consistent patterns."
---

# It Couldn't Help But Overhear: On the Limits of Modelling Meta-Communicative Grounding Acts with Supervised Learning

## Quick Facts
- arXiv ID: 2405.01139
- Source URL: https://arxiv.org/abs/2405.01139
- Reference count: 31
- Key outcome: Supervised learning models trained on dialogue data miss essential grounding acts, acting as overhearers rather than participants, with low agreement on clarification requests (F1≈0.51, κ≈0.18) indicating human grounding lacks consistent patterns

## Executive Summary
This paper argues that supervised learning models trained on human dialogue data are fundamentally limited because they can only observe dialogue after grounding has occurred, missing the essential interactive processes that establish mutual understanding. The authors demonstrate this by analyzing clarification request decisions in a multimodal collaborative task, finding low agreement between overhearers and original speakers (F1≈0.51) and among overhearers themselves (Krippendorff's α≈0.02, pairwise Cohen's κ≈0.18). These findings suggest that grounding acts in dialogue lack consistent patterns that can be learned from data alone, pointing to the need for more interactive, multi-step learning paradigms that allow models to participate in dialogue rather than just overhear it.

## Method Summary
The authors analyze human agreement on clarification request decisions using the CoDraw multimodal collaborative dialogue dataset. Three annotators acted as overhearers, viewing dialogue context (1-3 previous turns), current reconstructed scene state, and available clip art gallery to decide whether to request clarification and what to request. Their decisions were compared against the original speaker's decisions using binary F1 score, Krippendorff's alpha, and pairwise Cohen's kappa. Surface form similarity was measured using BLEU scores and cosine similarity of sentence embeddings. The analysis focuses on 90 instances from the dataset to evaluate the variability in human grounding acts.

## Key Results
- Overhearer models achieve only F1≈0.51 agreement with original speakers on clarification request decisions
- Agreement among overhearers themselves is extremely low (Krippendorff's α≈0.02, pairwise Cohen's κ≈0.18)
- BLEU scores for clarification request surface forms are approximately 0.11
- Sentence embedding cosine similarity for clarification requests is approximately 0.36-0.38

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overhearer models lack access to grounding acts that are essential for true understanding
- Mechanism: Human dialogue participants jointly tailor understanding through grounding acts like clarification requests, while overhearers only passively consume the product of this process
- Core assumption: The process of understanding differs fundamentally between addressees and overhearers
- Evidence anchors:
  - [abstract] "Overhearers are deprived of the privilege of performing grounding acts and can only conjecture about intended meanings"
  - [section 2] "overhearers only passively consume the product of that process (Schober and Clark, 1989)"
- Break condition: If grounding acts can be fully reconstructed from observable patterns in dialogue data

### Mechanism 2
- Claim: Data-driven supervised learning models trained on human dialogue data act as overhearers, missing essential grounding acts
- Mechanism: The prevailing end-to-end deep learning methods rely on supervised learning from samples of human behavior, treating instances as local point-wise estimates rather than sequences, limiting their ability to capture the interactive nature of grounding
- Core assumption: Static datasets of human observations do not contain enough information to define human-like clarification request policies
- Evidence anchors:
  - [abstract] "supervised learning models trained on human dialogue data act as overhearers, not participants, missing essential grounding acts needed for true understanding"
  - [section 1] "The prevailing end-to-end deep learning methods commonly rely on supervised learning (SL) from a sample of human behaviour"
- Break condition: If multi-step interactive learning paradigms can effectively capture grounding mechanisms

### Mechanism 3
- Claim: Human grounding acts like clarification requests show low consistency among overhearers
- Mechanism: The variability in human decisions for requesting clarification indicates that these acts lack consistent patterns that can be learned from data alone
- Core assumption: Human decisions to request clarification depend on mutual understanding and the current situation, making them highly context-dependent and variable
- Evidence anchors:
  - [abstract] "analyzing clarification request (CR) decisions in a multimodal collaborative task, the authors show low agreement between overhearers and original speakers (F1≈0.51)"
  - [section 5] "the average binary F1 of overhearers with respect to the original decision was .51" and "Krippendorff's α was 0.10 and the mean pairwise Cohen's κ was 0.18"
- Break condition: If consistent patterns in grounding acts can be identified across different contexts

## Foundational Learning

- Concept: Grounding in dialogue
  - Why needed here: Understanding that grounding is the process by which conversation participants establish mutual understanding is crucial for grasping why overhearer models fail
  - Quick check question: What is the difference between how addressees and overhearers process dialogue according to Schober and Clark (1989)?

- Concept: Supervised learning limitations
  - Why needed here: Recognizing that supervised learning treats instances as local point-wise estimates rather than sequences is key to understanding why it fails to capture grounding
  - Quick check question: What are the three concerns Rieser and Lemon (2011) flagged about supervised approaches for learning dialogue strategies?

- Concept: Reinforcement learning for dialogue
  - Why needed here: Understanding that reinforcement learning provides a framework for multi-step interactive learning is essential for grasping potential solutions
  - Quick check question: What is the main limitation of reinforcement learning approaches for dialogue according to Schatzmann et al. (2005)?

## Architecture Onboarding

- Component map:
  - Data collection module (overhearing paradigm)
  - Annotation and analysis component
  - Supervised learning model (overhearer)
  - Evaluation framework (overhearing experiments)
  - Multi-step interactive learning module (potential solution)

- Critical path:
  1. Data collection through overhearing
  2. Annotation of grounding acts
  3. Training of supervised model
  4. Evaluation as overhearer
  5. Recognition of limitations
  6. Transition to multi-step learning

- Design tradeoffs:
  - Overhearing vs. participation in data collection
  - Supervised learning vs. reinforcement learning
  - Static datasets vs. interactive environments
  - Single-step vs. multi-step learning paradigms

- Failure signatures:
  - Low agreement between overhearers and original speakers
  - Low agreement among overhearers themselves
  - Models achieving performance similar to overhearers
  - Inability to capture context-dependent grounding acts

- First 3 experiments:
  1. Replicate the clarification request analysis with a larger sample to confirm low agreement among overhearers
  2. Implement a simple reinforcement learning model for clarification requests and compare performance to supervised baseline
  3. Design an interactive dialogue environment where models can participate in grounding acts rather than just observing them

## Open Questions the Paper Calls Out
The paper identifies the general problem of overhearing limitations but doesn't specify which particular aspects of common ground are most challenging to infer. It mentions that the current analysis is from a pilot study using the CoDraw dataset and suggests further standardized experiments with larger samples. The authors advocate for moving from one-off supervised learning to multi-step models that participate in dialogues, mentioning reinforcement learning as a possibility, but don't explore specific alternative approaches or their effectiveness.

## Limitations
- Analysis based on a single dataset (CoDraw) and specific task domain (collaborative scene reconstruction)
- Findings may not generalize to all dialogue contexts or types of grounding acts
- Treatment of alternative learning paradigms is suggestive rather than demonstrated

## Confidence
- Empirical evidence for low inter-annotator agreement is strong (F1≈0.51, κ≈0.18)
- Generalization to all grounding acts requires additional validation across different dialogue types
- Confidence in core claims is Medium

## Next Checks
1. Replicate the agreement analysis on a different collaborative dialogue dataset (e.g., task-oriented dialogue corpora) to test generalizability of the low agreement findings across domains.

2. Implement and evaluate a simple reinforcement learning model for clarification requests on the CoDraw dataset, measuring whether it achieves higher agreement with human decisions than the supervised baseline.

3. Design an experiment where a model can actively participate in grounding acts rather than only observing them, measuring the difference in understanding quality compared to overhearer models.