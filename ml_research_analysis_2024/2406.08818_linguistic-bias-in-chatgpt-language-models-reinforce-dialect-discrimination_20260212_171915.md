---
ver: rpa2
title: 'Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination'
arxiv_id: '2406.08818'
source_url: https://arxiv.org/abs/2406.08818
tags:
- english
- varieties
- annotate
- responses
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines linguistic bias in ChatGPT across ten English
  varieties, including Standard American and British English and eight minoritized
  varieties (African American, Indian, Irish, Jamaican, Kenyan, Nigerian, Scottish,
  and Singaporean English). The researchers prompted GPT-3.5 Turbo and GPT-4 with
  text by native speakers of each variety and analyzed responses through linguistic
  feature annotation and native speaker evaluation.
---

# Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination

## Quick Facts
- arXiv ID: 2406.08818
- Source URL: https://arxiv.org/abs/2406.08818
- Authors: Eve Fleisig; Genevieve Smith; Madeline Bossi; Ishita Rustagi; Xavier Yin; Dan Klein
- Reference count: 40
- Primary result: GPT models default to standard English varieties and exhibit significant bias against minoritized varieties in stereotyping, demeaning content, comprehension, and condescension

## Executive Summary
This study examines linguistic bias in ChatGPT across ten English varieties, finding that models consistently default to standard varieties and produce more problematic responses for minoritized dialects. Using both linguistic feature annotation and native speaker evaluations, the research demonstrates that GPT-3.5 and GPT-4 generate responses with significantly more stereotyping (19% worse), demeaning content (25% worse), and lack of comprehension (9% worse) for non-standard varieties. GPT-4 shows improvements in comprehension, warmth, and friendliness but exhibits increased stereotyping, particularly when imitating input dialects.

## Method Summary
The study employed two complementary approaches: feature annotation and native speaker evaluation. For feature annotation, researchers prompted GPT-3.5 and GPT-4 with native speaker text from ten English varieties and annotated the outputs for linguistic features of the input variety. For native speaker evaluation, participants rated model responses across nine harm-related dimensions including stereotyping, demeaning content, comprehension, naturalness, condescension, warmth, friendliness, and respect. The study tested both standard varieties (American and British English) and eight minoritized varieties (African American, Indian, Irish, Jamaican, Kenyan, Nigerian, Scottish, and Singaporean English).

## Key Results
- GPT models default to standard varieties, with responses to minoritized varieties showing 19% more stereotyping and 25% more demeaning content
- When prompted to imitate input dialects, models produce even more stereotyping (+9%) and comprehension issues (-6%)
- GPT-4 improves on GPT-3.5 in comprehension (+10%), warmth (+6%), and friendliness (+6%), but exhibits increased stereotyping (+18%)
- Minoritized varieties experience significantly worse performance across comprehension (9% worse), condescension (15% worse), and stereotyping (19% worse)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models default to standard varieties of English in responses, with minoritized varieties experiencing significantly more stereotyping, demeaning content, lack of comprehension, and condescension.
- Mechanism: The models are trained on large corpora that are predominantly composed of standard English varieties, leading to a bias in output generation. When prompted with non-standard varieties, the models either default to standard forms or generate content that reflects stereotypes associated with those varieties.
- Core assumption: The training data is not balanced across English varieties, with standard varieties being overrepresented.
- Evidence anchors:
  - [abstract] "We find that the models default to 'standard' varieties of English; based on evaluation by native speakers, we also find that model responses to non-'standard' varieties consistently exhibit a range of issues: stereotyping (19% worse than for 'standard' varieties), demeaning content (25% worse), lack of comprehension (9% worse), and condescending responses (15% worse)."
  - [section 5.1] "GPT-3.5 responses to minoritized varieties are rated as worse than responses to standard varieties on most axes. On average, responses to minoritized varieties were rated as 25% more demeaning and 19% more stereotyping."

### Mechanism 2
- Claim: When GPT-3.5 is prompted to imitate the input dialect, its responses exacerbate stereotyping and lack of comprehension.
- Mechanism: The model's attempt to imitate non-standard varieties may lead to the reinforcement of stereotypes and a decrease in the ability to comprehend the input, as it relies on learned patterns from the training data that may not accurately represent the nuances of the target dialect.
- Core assumption: The model's understanding of non-standard varieties is based on simplified or stereotypical representations in the training data.
- Evidence anchors:
  - [abstract] "We also find that if these models are asked to imitate the writing style of prompts in non-'standard' varieties, they produce text that exhibits lower comprehension of the input and is especially prone to stereotyping."
  - [section 5.1] "Comparing GPT-3.5 responses with and without prompting to imitate the input style, we find that when imitating the input, comprehension decreases across all varieties (-6% for all varieties; -6% for minoritized varieties specifically) and stereotyping increases across all varieties (+9% for all varieties; +10% for minoritized varieties)."

### Mechanism 3
- Claim: GPT-4 improves on GPT-3.5 in terms of comprehension, warmth, and friendliness but exhibits increased stereotyping.
- Mechanism: GPT-4's architecture allows it to better imitate the features of non-standard varieties, leading to improved comprehension and user experience. However, this ability also comes with an increased tendency to generate stereotypical content, possibly due to the model's reliance on learned patterns that are not fully representative of the target dialects.
- Core assumption: The improvements in GPT-4's architecture do not address the underlying biases in the training data.
- Evidence anchors:
  - [abstract] "GPT-4 improves on GPT-3.5 in terms of comprehension, warmth, and friendliness, but also exhibits a marked increase in stereotyping (+18%)."
  - [section 5.1] "Comparing the responses in which the model is asked to imitate the style of the input, we see that imitative responses from GPT-4 are rated as better than imitative responses from GPT-3.5 in terms of comprehension (+10% for all varieties; +10% for minoritized varieties), warmth (+6% for all varieties; +7% for minoritized varieties), and friendliness (+6% for all varieties; +6% for minoritized varieties)."

## Foundational Learning

- Concept: Linguistic discrimination and dialect bias
  - Why needed here: Understanding the context and implications of linguistic bias in language models is crucial for interpreting the study's findings and their impact on users of non-standard English varieties.
  - Quick check question: What are some examples of linguistic discrimination, and how might they manifest in language model outputs?

- Concept: Training data bias and its impact on model outputs
  - Why needed here: Recognizing how imbalanced training data can lead to biased outputs is essential for grasping the mechanisms behind the observed disparities in model performance across English varieties.
  - Quick check question: How does the composition of training data influence the behavior of language models, particularly in terms of linguistic bias?

- Concept: Evaluation methods for language models
  - Why needed here: Familiarity with the methods used to assess language model outputs, such as native speaker evaluations and linguistic feature annotation, is necessary for understanding how the study's conclusions were drawn.
  - Quick check question: What are some common approaches to evaluating the performance and fairness of language models, and what are their strengths and limitations?

## Architecture Onboarding

- Component map: Input processing layer -> Attention mechanisms -> Generation layer -> Output filtering
- Critical path: Input text -> Feature extraction -> Contextual embedding generation -> Response generation -> Output evaluation
- Design tradeoffs: Balancing accurate representation of non-standard varieties against risk of reinforcing stereotypes vs. model complexity and generalization ability
- Failure signatures: Increased stereotyping and demeaning content for minoritized varieties, decreased comprehension of input text, exacerbated biases when imitating non-standard dialects
- First 3 experiments:
  1. Evaluate the impact of balanced training data on model outputs across English varieties
  2. Test the effectiveness of fine-tuning techniques in reducing linguistic bias in model responses
  3. Assess the influence of different evaluation methods on the detection and mitigation of linguistic discrimination

## Open Questions the Paper Calls Out

- Question: What is the correlation between feature retention rates and actual training data availability for each dialect?
  - Basis in paper: Explicit - The paper notes that "the retention rate for minoritized varieties correlates with estimated maximum speaker population" but acknowledges this is "due to the lack of reliable estimates for the amount of available training data"
  - Why unresolved: The authors used population estimates as a proxy for training data availability, but actual training data volumes per dialect are unknown
  - What evidence would resolve it: Direct measurement of dialect representation in ChatGPT's training corpus

- Question: How does ChatGPT's performance vary across different types of minoritized English varieties (e.g., creoles vs. L1 vs. L2 varieties)?
  - Basis in paper: Inferred - The paper includes Jamaican English (creole), Indian English (L2), and Nigerian English (L2) but doesn't systematically compare performance across these categories
  - Why unresolved: The paper groups all minoritized varieties together in analysis without distinguishing between different linguistic origins
  - What evidence would resolve it: Comparative analysis of ChatGPT's performance across different types of English varieties

- Question: Does GPT-4's increased stereotyping for minoritized varieties represent genuine improvement or just better mimicry?
  - Basis in paper: Explicit - The paper states GPT-4 "improves on GPT-3.5 in terms of comprehension, warmth, and friendliness, but also exhibits a marked increase in stereotyping (+18%)"
  - Why unresolved: The paper doesn't distinguish whether the stereotyping increase is due to better understanding of the dialect or more effective reproduction of stereotypes
  - What evidence would resolve it: Analysis of whether the increased stereotyping content is more nuanced/faithful to the dialect or more harmful/broader in scope

## Limitations

- The study examines a limited set of ten English varieties with relatively small sample sizes for some dialects
- Evaluation relies on subjective native speaker judgments, which may introduce cultural bias in harm assessments
- Analysis is limited to written text prompts and responses, potentially missing spoken dialect discrimination patterns
- Only two GPT model versions are tested, leaving uncertainty about other language models' performance

## Confidence

- **High confidence**: GPT models default to standard varieties and show worse performance on comprehension, demeaning content, and condescension for minoritized varieties
- **Medium confidence**: GPT-4's improved comprehension but increased stereotyping requires careful interpretation as improvements may be offset by significant stereotyping increase
- **Medium confidence**: The mechanism linking training data imbalance to output bias is plausible but inferred rather than directly measured

## Next Checks

1. Analyze the composition of training data across different English varieties to quantify representation imbalances and correlate with observed model biases
2. Test whether similar bias patterns emerge for non-English varieties and dialects to determine if this is language-specific or architecture-wide issue
3. Implement fine-tuning or prompt engineering techniques designed to reduce dialect discrimination and measure their effectiveness across the same harm dimensions used in this study