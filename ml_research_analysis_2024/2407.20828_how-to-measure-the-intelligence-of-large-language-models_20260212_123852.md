---
ver: rpa2
title: How to Measure the Intelligence of Large Language Models?
arxiv_id: '2407.20828'
source_url: https://arxiv.org/abs/2407.20828
tags:
- intelligence
- llms
- language
- human
- qualitative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues for a dual framework to measure LLM intelligence,
  separating quantitative intelligence (data storage and retrieval capabilities) and
  qualitative intelligence (reasoning and novel problem-solving). The authors propose
  that while LLMs can vastly exceed human quantitative knowledge due to massive training
  data, their qualitative reasoning abilities show only gradual improvement despite
  exponential model growth.
---

# How to Measure the Intelligence of Large Language Models?

## Quick Facts
- arXiv ID: 2407.20828
- Source URL: https://arxiv.org/abs/2407.20828
- Reference count: 17
- Primary result: LLMs may exceed human quantitative intelligence but show only gradual qualitative reasoning improvements

## Executive Summary
This paper proposes a dual framework for measuring LLM intelligence, separating quantitative capabilities (data storage and retrieval) from qualitative abilities (reasoning and novel problem-solving). The authors argue that while LLMs can vastly exceed human quantitative knowledge due to massive training data, their qualitative reasoning abilities improve only gradually despite exponential model growth. Current evaluation paradigms like MMLU focus on quantitative measures but lack standardized methods for assessing qualitative intelligence. The paper suggests that truly emergent "super-human" AI may be unlikely with current learning paradigms, as LLMs are fundamentally constrained by their human-derived training data.

## Method Summary
The paper proposes a theoretical framework rather than a specific implementation. It suggests developing separate assessment modules for quantitative intelligence (knowledge retrieval across diverse domains) and qualitative intelligence (reasoning, novel problem-solving, strategic actions). The framework would use standardized benchmarks for quantitative measures while developing new protocols for qualitative assessment, with unified reporting that presents both dimensions separately. Implementation details and validation procedures are not specified.

## Key Results
- LLMs can exceed human quantitative intelligence due to ability to store and retrieve vastly more information
- Qualitative reasoning improvements show only gradual progress despite exponential growth in model size and training data
- Current evaluation metrics are insufficient for measuring emergent properties, particularly qualitative intelligence assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can exceed human quantitative intelligence due to their ability to store and retrieve vastly more information than any individual human.
- Mechanism: LLMs trained on massive internet-scale datasets can store and access more diverse knowledge than humans, enabling superior performance on quantitative knowledge retrieval tasks.
- Core assumption: The training data contains a representative sample of human knowledge across multiple domains.
- Evidence anchors: [abstract] "current models likely already can fall back on more information than any human being"; [section] "A reasonable conversation with an LLM is possible in dozens of languages and may cover topics from knitting tips via ancient literature to quantum physics"
- Break condition: If the training data is biased, incomplete, or contains systematic errors, the quantitative intelligence measurement becomes unreliable.

### Mechanism 2
- Claim: Qualitative intelligence improvements in LLMs show only gradual progress despite exponential growth in model size and training data.
- Mechanism: While quantitative intelligence scales with data size, qualitative reasoning abilities (novel problem-solving, analysis, judgment) improve at a much slower rate due to fundamental limitations in current learning paradigms.
- Core assumption: The current self-supervised learning paradigm has inherent limitations for developing true reasoning capabilities.
- Evidence anchors: [abstract] "their qualitative reasoning abilities show only gradual improvement despite exponential model growth"; [section] "comparing the results in reasoning tests of specialized models... the improvements in terms of qualitative measures are only gradual"
- Break condition: If new learning paradigms or architectures are developed that better capture reasoning capabilities.

### Mechanism 3
- Claim: Current evaluation metrics are insufficient for measuring emergent properties of LLMs, particularly for qualitative intelligence assessment.
- Mechanism: Standard benchmarks like MMLU focus on quantitative measures but lack standardized methods for assessing reasoning, novel problem-solving, and other qualitative intelligence characteristics.
- Core assumption: Emergent properties in LLMs can only be reliably detected through comprehensive evaluation frameworks that separately address quantitative and qualitative intelligence.
- Evidence anchors: [abstract] "current evaluation paradigms like MMLU focus on quantitative measures but lack standardized methods for assessing qualitative intelligence"; [section] "However, it does not explicitly differ between knowledge retrievel and problem solving capabilities"
- Break condition: If standardized qualitative intelligence assessment frameworks are developed and widely adopted.

## Foundational Learning

- Concept: Separation of quantitative vs qualitative intelligence
  - Why needed here: The paper argues that conflating these two types of intelligence leads to incomplete and potentially misleading assessments of LLM capabilities
  - Quick check question: Can you identify which aspects of LLM performance relate to knowledge storage vs reasoning abilities?

- Concept: Emergent properties in complex systems
  - Why needed here: The paper discusses whether LLMs can exhibit truly emergent "super-human" intelligence properties that go beyond their training data
  - Quick check question: What distinguishes emergent behavior from simple scaling of existing capabilities?

- Concept: Benchmark design and evaluation methodology
  - Why needed here: The paper emphasizes the need for new evaluation frameworks that can properly assess both quantitative and qualitative intelligence separately
  - Quick check question: How would you design a test that distinguishes between knowledge retrieval and genuine reasoning?

## Architecture Onboarding

- Component map: Quantitative assessment modules -> Qualitative assessment modules -> Unified reporting system
- Critical path: Designing and implementing reliable qualitative intelligence assessment methods, as this is currently the weakest area with no standardized approaches
- Design tradeoffs: Quantitative assessment can leverage existing benchmarks and automated scoring, while qualitative assessment requires human evaluation, debate protocols, or novel automated methods that may be less reliable but more insightful
- Failure signatures: Common failure modes include conflating quantitative and qualitative metrics, over-relying on existing benchmarks that don't capture reasoning abilities, and failing to account for training data contamination in qualitative assessments
- First 3 experiments:
  1. Implement a large-scale quantitative assessment using MMLU or similar benchmarks to establish baseline knowledge retrieval capabilities
  2. Design a small-scale qualitative assessment using debate protocols or novel problem-solving tasks to test reasoning abilities
  3. Create a comparative analysis tool that visualizes the gap between quantitative and qualitative performance to identify areas for improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What standardized framework could effectively measure qualitative intelligence in LLMs, distinguishing reasoning capabilities from mere data retrieval?
- Basis in paper: [explicit] The paper explicitly states that "there is still no standardized paradigm for evaluating intelligence for LLMs" and calls for "a gold standard for qualitative assessments of intelligence."
- Why unresolved: Current evaluation methods like MMLU focus on quantitative measures and don't explicitly differentiate between knowledge retrieval and problem-solving capabilities. The paper notes that while tools like RCTs and Chatbot Arena provide some qualitative evaluation, these are not standardized.
- What evidence would resolve it: Development and validation of a standardized testing framework that can reliably distinguish between LLMs' ability to retrieve information versus their capacity for novel reasoning, with demonstrated reliability across different model architectures and training paradigms.

### Open Question 2
- Question: Can current LLMs demonstrate truly emergent qualitative intelligence that exceeds human reasoning capabilities by orders of magnitude, or are they fundamentally limited by their human-derived training data?
- Basis in paper: [explicit] The authors argue that "the hypothetical qualitative improvement over humans would still be on a comparable scale as the whole training basis inherently relies on human thoughts and language from which the model can not escape."
- Why unresolved: While LLMs show impressive capabilities, the paper suggests they may be constrained by their training data's human origins. The authors question whether language alone is sufficient for acquiring broader cognitive capabilities associated with intelligence.
- What evidence would resolve it: Demonstration of LLM reasoning capabilities that consistently and significantly exceed the best human reasoning across diverse domains, or conversely, proof that all qualitative improvements are bounded by the cognitive limitations of the training data's human authors.

### Open Question 3
- Question: What are the critical differences between quantitative and qualitative intelligence growth in LLMs, and at what point (if any) does qualitative intelligence plateau despite continued quantitative growth?
- Basis in paper: [explicit] The paper observes that "comparing the results in reasoning tests of specialized models, such as the first transformer models with their successor to current LLMs, the improvements in terms of qualitative measures are only gradual" despite exponential increases in model size and training data.
- Why unresolved: The relationship between quantitative growth (data size, model parameters) and qualitative improvement (reasoning ability) is not well understood. The paper suggests a potential decoupling between these two forms of intelligence growth.
- What evidence would resolve it: Empirical data showing the correlation (or lack thereof) between quantitative metrics (training data size, model parameters) and qualitative metrics (reasoning test performance) across multiple model generations, with identification of any plateaus or inflection points in qualitative improvement.

## Limitations

- The paper proposes a theoretical framework without providing specific implementation details or validation procedures
- The boundary between knowledge retrieval and reasoning is not clearly defined, making standardized assessment challenging
- Data contamination from LLMs potentially seeing evaluation benchmarks during training undermines confidence in performance measurements

## Confidence

**High Confidence Claims**:
- LLMs can store and retrieve more information than individual humans (supported by scale of training data)
- Current evaluation paradigms inadequately assess qualitative intelligence (well-documented in literature)

**Medium Confidence Claims**:
- Qualitative intelligence improvements are only gradual despite model scaling (observational evidence, but hard to measure)
- True "super-human" emergent intelligence is unlikely with current paradigms (logical inference from training data limitations)

**Low Confidence Claims**:
- Specific quantitative vs qualitative performance gaps (lacking systematic measurement)
- Proposed framework effectiveness (not empirically validated)

## Next Checks

1. **Quantitative Baseline Validation**: Systematically test 5-10 LLMs across standardized knowledge domains using curated question sets designed to minimize reasoning requirements, then compare performance to human expert baselines to verify quantitative intelligence claims.

2. **Qualitative Assessment Protocol Development**: Create and pilot test a standardized qualitative intelligence assessment using controlled novel problem-solving tasks with blinded human evaluation, measuring inter-rater reliability and correlation with model scale.

3. **Emergent Property Detection Framework**: Design an experiment to test whether LLMs exhibit capabilities that cannot be explained by their training data alone, using time-series analysis of knowledge domains to identify truly novel insights versus memorized information.