---
ver: rpa2
title: 'Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning'
arxiv_id: '2410.08146'
source_url: https://arxiv.org/abs/2410.08146
tags:
- policy
- base
- prover
- reward
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how to define process rewards for multi-step\
  \ reasoning in large language models to improve both test-time search and online\
  \ reinforcement learning. The key insight is that process rewards should measure\
  \ progress\u2014specifically, the change in the likelihood of producing a correct\
  \ response before and after taking each step\u2014rather than just correctness or\
  \ relevance of individual steps."
---

# Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning

## Quick Facts
- arXiv ID: 2410.08146
- Source URL: https://arxiv.org/abs/2410.08146
- Authors: Amrith Setlur; Chirag Nagpal; Adam Fisch; Xinyang Ggen; Jacob Eisenstein; Rishabh Agarwal; Alekh Agarwal; Jonathan Berant; Aviral Kumar
- Reference count: 36
- Primary result: Process advantage verifiers (PAVs) improve beam search accuracy by >8% and 1.5-5× compute efficiency over outcome reward models, with 5-6× sample efficiency gains in online RL

## Executive Summary
This paper introduces process advantage verifiers (PAVs) to improve reasoning in large language models by measuring progress rather than just correctness. The key insight is that process rewards should measure the change in likelihood of producing a correct response before and after each step, corresponding to step-level advantages under a prover policy distinct from the base policy. PAVs are trained to predict these advantages, enabling more effective test-time search and online reinforcement learning. Empirically, PAVs achieve over 8% better accuracy and 1.5-5× greater compute efficiency compared to outcome reward models (ORMs), and provide 5-6× sample efficiency gains in online RL with over 6% accuracy improvement.

## Method Summary
The authors formalize reasoning as an MDP where each step is an action, and define process rewards as advantages under a prover policy distinct from the base policy. They train PAVs to predict these advantages using data collected from seed rollouts and partial rollouts to estimate Q-values. During test-time search, they use effective rewards combining outcome rewards and PAV predictions. For online RL, they optimize the base policy using policy gradients with these dense rewards. The prover policy is chosen as a Best-of-K policy (K>1) to ensure it's complementary to the base policy, enabling effective progress measurement.

## Key Results
- Beam search with PAVs achieves >8% better accuracy and 1.5-5× greater compute efficiency compared to outcome reward models
- Online RL with PAVs provides 5-6× sample efficiency gains and over 6% accuracy improvement over ORMs
- Process advantage verifiers trained to predict step-level advantages under prover policies enable both test-time and online RL improvements
- Optimal prover policy strength depends on base policy strength; Best-of-4 often optimal for medium-strength policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Process rewards should measure progress—specifically, the change in the likelihood of producing a correct response before and after taking each step—rather than just correctness or relevance of individual steps.
- Mechanism: The advantage function under a prover policy captures the marginal improvement in expected final correctness contributed by a step. This contrasts with Q-values, which mix state value and action value, obscuring the individual contribution of each step. During beam search, using advantage-based rewards allows the algorithm to explore prefixes that show progress toward the correct answer rather than only exploiting high-likelihood states.
- Core assumption: A prover policy can be chosen that is complementary to the base policy, i.e., it distinguishes steps in a way that is aligned enough to provide useful learning signals but distinct enough to reveal progress not captured by the base policy itself.
- Evidence anchors:
  - [abstract] The authors state that process rewards should measure progress: a change in the likelihood of producing a correct response before and after taking the step, corresponding to step-level advantages in RL.
  - [section 3.1] The authors provide a concrete example showing that using Q-values in beam search can retain negative-progress steps, whereas advantages explicitly measure progress and thus enable better exploration.
  - [corpus] Weak or missing; only tangentially related papers are found in the corpus.
- Break condition: If the prover policy becomes too strong (e.g., BoK with very large K), it can succeed from any prefix, making advantages near zero and thus uninformative. Conversely, if the prover is too weak, advantages will also be near zero.

### Mechanism 2
- Claim: A complementary prover policy—one that is not too misaligned with the base policy but can distinguish actions taken by the base policy—boosts improvement over using only outcome rewards.
- Mechanism: The policy gradient update with effective reward Qπ + αAμ leverages advantages from the prover to encourage the base policy to explore actions that make progress, even if the current rollout is incorrect. The authors formalize this with a lower bound on policy improvement that grows with the variance of Aμ and decreases with misalignment between Aμ and Aπ.
- Core assumption: The prover policy is complementary to the base policy, meaning its advantages have high variance under the base policy and are not too negatively correlated with the base policy's own advantages.
- Evidence anchors:
  - [section 3.4] The authors prove a lower bound on policy improvement that depends on the variance of Aμ and the alignment between Aμ and Aπ.
  - [section 3.3] Empirical results in a didactic planted subsequence task show that RL with effective rewards from a prover with K=10 vastly outperforms using only Qπ or using too weak/strong provers.
  - [corpus] Weak; related works are about process reward models but not about complementary prover policies.
- Break condition: If the prover is too strong (K very large) or too weak, advantages become uninformative (near zero), and the effective reward collapses to just Qπ.

### Mechanism 3
- Claim: Training process advantage verifiers (PAVs) to predict advantages under a prover policy enables both more compute-efficient test-time search and more sample-efficient online RL.
- Mechanism: PAVs approximate the advantage function Aμ, which is used as dense rewards in RL or as part of the effective reward in beam search. This dense supervision accelerates learning by directing exploration toward steps that make progress, whereas outcome rewards alone provide only sparse feedback at the end of a rollout.
- Core assumption: PAVs can be trained effectively from data collected by sampling from the prover and partial rollouts, and that these approximations are accurate enough to guide policy improvement.
- Evidence anchors:
  - [abstract] PAVs are shown to yield over 8% better accuracy and 1.5-5× greater compute efficiency in beam search compared to outcome reward models (ORMs), and 5-6× sample efficiency gains in online RL.
  - [section 4.1] Empirical results demonstrate that beam search with PAVs improves Pass@N performance by >8% and achieves 1.5-5× better compute efficiency versus ORMs.
  - [section 5] RL with PAVs improves sample efficiency by 5-6× and raw accuracy by >6% over ORM-RL.
  - [corpus] Weak; only tangentially related papers are found.
- Break condition: If the PAV approximation is poor (e.g., due to insufficient training data or model capacity), the effective reward will be inaccurate, potentially harming performance.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation of reasoning traces
  - Why needed here: The paper models each reasoning step as an action in an MDP with deterministic dynamics, allowing the use of RL concepts like Q-values, value functions, and advantages to design process rewards.
  - Quick check question: In the reasoning trace MDP, what is the state at step h, and what is the action taken at that state?

- Concept: Advantage function in RL
  - Why needed here: The advantage Aπ(s,a) = Qπ(s,a) - Vπ(s) measures the relative improvement in expected return from taking action a in state s versus the average return from that state, which the authors use as the definition of "progress" for process rewards.
  - Quick check question: How does the advantage function differ from the Q-value, and why is this distinction important for designing process rewards?

- Concept: Policy gradient and natural policy gradient methods
  - Why needed here: The authors use natural policy gradient updates to incorporate process rewards into the base policy training, and they provide theoretical analysis of when such updates lead to improvement.
  - Quick check question: In the natural policy gradient update, what role does the Fisher information matrix play, and how does it affect the direction of the policy update?

## Architecture Onboarding

- Component map:
  Base policy π -> Prover policy μ (Best-of-K) -> PAV (predicts Aμ) -> Effective reward (Qπ + α·PAV) -> Beam search/RL

- Critical path:
  1. Sample reasoning traces from base policy π
  2. Collect seed rollouts and partial rollouts from prover μ to estimate Qμ and Qπ
  3. Train PAV to predict Aμ = Qμ - Vμ
  4. For test-time search: Run beam search using effective reward (Qπ + α·PAV)
  5. For online RL: Optimize π using policy gradient with effective reward as per-step reward

- Design tradeoffs:
  - Choice of prover policy K: Too small → weak supervision; too large → uninformative advantages. Empirically Bo4 is often optimal.
  - α (mixing coefficient): Controls the balance between outcome and process rewards. Must be tuned per model size.
  - PAV training data ratio (nmc/ncov): More coverage early on, more label accuracy later. First-pit strategy can help coverage.

- Failure signatures:
  - PAV predicts near-zero advantages everywhere → beam search and RL revert to behavior similar to ORM-only
  - Base policy learns degenerate outputs (e.g., "REPHRASE THE PROBLEM") → indicates using Qμ instead of Aμ in rewards
  - RL training diverges or plateaus → check PAV quality, α value, or KL regularization strength

- First 3 experiments:
  1. Train a PAV for a small base policy using Bo2 and Bo4 provers; evaluate beam search accuracy and compute efficiency
  2. Run online RL with PAV rewards initialized from an RFT checkpoint; compare sample efficiency and final accuracy to ORM-RL
  3. Sweep α values for beam search; plot accuracy vs. α to find optimal mixing ratio for a given base policy

## Open Questions the Paper Calls Out

- Question: How should we automatically design the optimal prover policy for a sequence of base policy iterates during online RL?
  - Basis in paper: [inferred] The paper discusses that the choice of prover policy significantly impacts the effectiveness of process advantage verifiers (PAVs) and suggests that using Best-of-K policies (corresponding to the base policy) as provers induces better exploration. However, it also notes that the optimal prover policy might change as the base policy improves during training.
  - Why unresolved: The paper mentions that while it can compute the right-hand side of the theoretical result in Theorem 3.1 to understand whether a given prover policy will improve a fixed base policy, it is unclear how to automatically design a flexible class of optimal prover policies for a sequence of base policy iterates. The paper suggests that simultaneously optimizing the prover and the base policy in a two-player game might be an approach, but this remains an open question.
  - What evidence would resolve it: Experimental results comparing the performance of base policies trained with different strategies for selecting or updating the prover policy (e.g., fixed prover, dynamically updated prover, or simultaneous optimization) would provide evidence for the best approach.

- Question: Can we extend the approach of using advantages under a prover policy to settings where we do not train verifiers but directly estimate advantages using Monte Carlo rollouts during online RL or search?
  - Basis in paper: [explicit] The paper states that fitting errors in the learned process advantage verifier (PAV) can upper bound the performance of the method. It suggests that if the approach could run rollouts from prover policies during online RL or search to estimate advantages without training verifiers, it would circumvent these fitting errors.
  - Why unresolved: The paper acknowledges that extending the approach to this setup is a good avenue for future work but does not provide experimental results or a detailed methodology for doing so.
  - What evidence would resolve it: Experimental results comparing the performance of the proposed method (using trained PAVs) with a version that directly estimates advantages using Monte Carlo rollouts during online RL or search would demonstrate the feasibility and potential benefits of this approach.

- Question: How do the findings of this paper translate to other domains beyond mathematical reasoning, such as code generation or natural language understanding tasks?
  - Basis in paper: [inferred] The paper focuses on improving mathematical reasoning in large language models using process advantage verifiers (PAVs). While the methodology and theoretical insights might be applicable to other domains, the paper does not provide experimental results or analysis for domains beyond mathematics.
  - Why unresolved: The paper's experiments are limited to mathematical reasoning tasks, and it does not explore the generalizability of the findings to other domains.
  - What evidence would resolve it: Experimental results applying the proposed method to other domains, such as code generation or natural language understanding tasks, and comparing the performance gains with those observed in mathematical reasoning would demonstrate the generalizability of the approach.

## Limitations

- The empirical validation is confined to math word problems from the MATH dataset, limiting generalizability to other reasoning domains
- The theoretical analysis assumes access to true Q-values and advantage functions, which must be estimated from finite samples in practice
- The optimal choice of prover policy (Best-of-K) depends heavily on the base policy strength, with no fully automated method for selection
- PAV training requires careful balance of coverage versus accuracy in Monte Carlo estimation, with sensitivity to this trade-off not thoroughly explored

## Confidence

- **High Confidence**: The empirical claim that PAVs improve beam search accuracy by 8% and compute efficiency by 1.5-5× is well-supported by the experimental results across multiple model scales (Gemma 2B, 9B, 27B)
- **Medium Confidence**: The theoretical claim that complementary prover policies yield better policy improvement is supported by the lower bound proof, but practical estimation of Q-values may reduce real-world benefit
- **Low Confidence**: The claim that PAVs provide 5-6× sample efficiency gains in online RL is based on a single run per condition without statistical significance tests

## Next Checks

1. Replicate the online RL experiments with multiple random seeds and report statistical significance to confirm the claimed 5-6× sample efficiency gains
2. Test the PAV approach on a non-math reasoning dataset (e.g., logical reasoning or commonsense QA) to assess domain generalizability
3. Conduct an ablation study on the mixing coefficient α across different base policy strengths to determine if the optimal value is truly model-dependent or follows a predictable pattern