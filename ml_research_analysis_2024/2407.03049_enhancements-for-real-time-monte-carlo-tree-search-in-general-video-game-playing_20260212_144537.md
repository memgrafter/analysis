---
ver: rpa2
title: Enhancements for Real-Time Monte-Carlo Tree Search in General Video Game Playing
arxiv_id: '2407.03049'
source_url: https://arxiv.org/abs/2407.03049
tags:
- game
- mcts
- games
- tree
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents eight enhancements for Monte-Carlo Tree Search
  (MCTS) in General Video Game Playing (GVGP), where agents must play a variety of
  unknown real-time video games. The enhancements include Progressive History, N-Gram
  Selection Technique, Tree Reuse, Breadth-First Tree Initialization, Loss Avoidance,
  Novelty-Based Pruning, Knowledge-Based Evaluations, and Deterministic Game Detection.
---

# Enhancements for Real-Time Monte-Carlo Tree Search in General Video Game Playing

## Quick Facts
- arXiv ID: 2407.03049
- Source URL: https://arxiv.org/abs/2407.03049
- Reference count: 30
- Primary result: Eight enhancements improved MCTS win rate from 31.0% to 48.4% in GVGP games

## Executive Summary
This paper presents eight enhancements for Monte-Carlo Tree Search (MCTS) specifically designed for General Video Game Playing (GVGP) where agents must play unknown real-time video games. The enhancements include Progressive History, N-Gram Selection Technique, Tree Reuse, Breadth-First Tree Initialization, Loss Avoidance, Novelty-Based Pruning, Knowledge-Based Evaluations, and Deterministic Game Detection. When combined, these enhancements increased average win percentage from 31.0% to 48.4%, approaching the performance level of the best agents from the 2015 GVG-AI competition (52.4% win rate). Most enhancements showed statistically significant improvements in win percentages when applied individually, demonstrating their effectiveness in improving MCTS performance in GVGP environments.

## Method Summary
The method involves implementing eight enhancements to standard MCTS and evaluating them individually and in combination across 60 games from the GVG-AI framework. The enhancements modify different phases of the MCTS cycle: Progressive History and N-Gram Selection Technique bias selection toward historically successful actions, Loss Avoidance modifies backpropagation to avoid pessimism from losses, Knowledge-Based Evaluations improve state evaluation using distance-based heuristics, and Novelty-Based Pruning reduces search space by eliminating redundant paths. The combined system was evaluated against baseline MCTS and benchmark agents using win percentage as the primary metric, with 750 runs per game set for the enhancement combinations and 1000 runs for benchmark comparisons.

## Key Results
- Combined enhancements increased win percentage from 31.0% to 48.4% across 60 games
- Performance approaches benchmark agents (SOLMCTS, IW(1), YBC RIBER) at 52.4% win rate
- Tree Reuse with decay factor 0.6 showed significant improvement (p < 0.05) in win percentages
- Loss Avoidance roughly halved average MCTS simulations per tick in the Frogs game

## Why This Works (Mechanism)

### Mechanism 1
Progressive History (PH) and N-Gram Selection Technique (NST) bias MCTS toward playing actions and action sequences that performed well in earlier simulations. Both techniques store and reuse historical performance data conditioned on the avatar's current position, influencing selection and play-out steps to favor successful action patterns. This works because the value of an action depends significantly on the avatar's current position in the game state.

### Mechanism 2
Knowledge-Based Evaluations (KBE) improve state evaluation when standard scores are insufficient. KBE computes pathfinding distances to different object types and applies learned weights to reward or penalize moving toward those objects, based on observed score changes from past transitions. This works because changes in game score are often caused by specific collision events, and proximity to certain object types correlates with positive outcomes.

### Mechanism 3
Loss Avoidance (LA) prevents MCTS from repeatedly exploring paths leading to immediate losses in single-player games. When a play-out ends in a loss, LA searches sibling nodes for a better outcome and backpropagates only that better evaluation, keeping MCTS optimistic about potentially recoverable states. This works because single-player games often allow reactive recovery from dangerous situations, so initial pessimism from losses is often misleading.

## Foundational Learning

- Concept: Monte-Carlo Tree Search (MCTS) fundamentals
  - Why needed here: All enhancements build on or modify the standard MCTS cycle of selection, expansion, play-out, and backpropagation.
  - Quick check question: Can you describe how UCB1 balances exploration and exploitation in the selection step?

- Concept: General Video Game Playing (GVGP) constraints
  - Why needed here: Enhancements are specifically designed to handle the unknown, real-time, and potentially nondeterministic nature of GVGP games.
  - Quick check question: What makes MCTS particularly suitable for GVGP compared to domain-specific heuristics?

- Concept: Novelty-Based Pruning (NBP) mechanics
  - Why needed here: NBP is a novel contribution that prunes redundant search paths using novelty tests adapted from Iterated Width to MCTS.
  - Quick check question: How does NBP define the neighborhood N(s) for a state s in the MCTS context?

## Architecture Onboarding

- Component map: MCTS core engine -> Enhancement modules (PH, NST, TR, BFTI, LA, NBP, KBE, DGD) -> Forward model interface -> Evaluation function (base + KBE) -> Novelty test system (for NBP)
- Critical path: Simulation loop → Selection (with PH/NST) → Play-out (with LA) → Backpropagation (with KBE) → Move selection (with NBP)
- Design tradeoffs:
  - Memory vs. performance: Storing historical data (PH, NST) and caching states (BFTI) increases memory use but improves decision quality.
  - Computational overhead: LA and NBP add per-simulation cost but can reduce overall search space.
  - Generalization vs. specialization: DGD treats deterministic and nondeterministic games differently, optimizing for each but requiring detection overhead.
- Failure signatures:
  - Poor performance on games where action-value is independent of avatar position (PH/NST fail)
  - Overly optimistic evaluations leading to repeated losses (LA too aggressive)
  - Excessive pruning of viable paths (NBP threshold too low)
- First 3 experiments:
  1. Implement BFTI with Safety Prepruning and measure win percentage and loss delay in games with many immediate loss states.
  2. Add KBE to baseline MCTS and evaluate performance in games where score changes are rare or subtle.
  3. Combine PH and NST with TR and assess win percentage improvement and computational overhead.

## Open Questions the Paper Calls Out

### Open Question 1
How would the proposed enhancements perform in competitive two-player games with adversarial elements? The paper discusses that Loss Avoidance (LA) is unlikely to work well in adversarial games because a high concentration of losses in a subtree typically indicates that an opposing player has more options to win and is likely in a stronger position.

### Open Question 2
What is the optimal decay factor γ for Tree Reuse in different game categories or difficulty levels? The paper shows that TR with γ ∈ {0.4, 0.6, 1.0} significantly improves win percentage, but only tested six specific values and did not explore optimal settings for different game types.

### Open Question 3
How would the enhancements perform if the pathfinding algorithm in Knowledge-Based Evaluations could account for all object types that block movement or enable teleportation? The paper states that the pathfinding algorithm only considers wall objects as obstacles, and mentions that many games contain other objects that block movement or portals for teleportation, but these are not taken into account.

### Open Question 4
What is the computational overhead of combining all enhancements versus the performance gains in resource-constrained environments? While the paper shows performance improvements, it does not provide a comprehensive analysis of the computational trade-offs or how the enhancements scale with different processing budgets.

### Open Question 5
How would the proposed enhancements perform if adapted for continuous action spaces instead of discrete actions? All enhancements are designed for discrete action selection, and their applicability to continuous control domains (e.g., robotic control, autonomous driving) remains unexplored.

## Limitations

- Some enhancements may have limited applicability in specific game contexts where action values are position-independent
- Knowledge-Based Evaluations depend on learning rate parameters that may require tuning for different game domains
- The evaluation primarily focuses on win percentages without detailed analysis of computational overhead or memory requirements

## Confidence

- High confidence: The core finding that combining enhancements improves MCTS performance from 31.0% to 48.4% win rate is well-supported by experimental results across 60 games.
- Medium confidence: Individual enhancement effectiveness varies significantly by game type, with some showing statistically significant improvements while others show minimal impact.
- Low confidence: The optimal parameter settings (like the TR decay factor of 0.6) may not generalize beyond the tested game corpus.

## Next Checks

1. Test the combined enhancement system on additional game domains outside the GVG-AI framework to assess generalization capability.
2. Conduct ablation studies to quantify the marginal contribution of each enhancement when others are present, helping identify which components provide the most value.
3. Measure and analyze computational overhead and memory usage for the full enhancement system to evaluate practical real-time applicability.