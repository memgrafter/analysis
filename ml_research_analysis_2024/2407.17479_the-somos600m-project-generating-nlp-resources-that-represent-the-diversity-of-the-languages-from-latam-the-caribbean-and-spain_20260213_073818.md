---
ver: rpa2
title: 'The #Somos600M Project: Generating NLP resources that represent the diversity
  of the languages from LATAM, the Caribbean, and Spain'
arxiv_id: '2407.17479'
source_url: https://arxiv.org/abs/2407.17479
tags:
- spanish
- language
- corpus
- espa
- para
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Somos600M Project addresses the scarcity of Spanish and co-official
  language resources for instruction-tuning large language models (LLMs) and establishing
  a standard evaluation leaderboard. The project created a large instruction dataset
  of 2.3 million examples across 18 domains through a hackathon and dataset collection
  campaign, along with 22 evaluation datasets for the first open generative LLM leaderboard.
---

# The #Somos600M Project: Generating NLP resources that represent the diversity of the languages from LATAM, the Caribbean, and Spain

## Quick Facts
- arXiv ID: 2407.17479
- Source URL: https://arxiv.org/abs/2407.17479
- Authors: María Grandury
- Reference count: 40
- Key outcome: Created 2.3M instruction examples across 18 domains and 22 evaluation datasets for Spanish LLM instruction-tuning and leaderboard

## Executive Summary
The #Somos600M Project addresses the critical shortage of Spanish and co-official language resources for instruction-tuning large language models and establishing evaluation standards. Through a community-driven hackathon and dataset collection campaign, the project generated 2.3 million instruction examples across 18 domains, along with 22 evaluation datasets forming the first open generative LLM leaderboard in Spanish. Translation and validation efforts covered major English benchmarks including 60% of ARC-C, 15% of HellaSwag, 15% of MMLU, and 100% of DIBT prompts. These resources enable instruction-tuning and evaluation of LLMs in Spanish, Catalan, and Euskera, advancing NLP representation for the 600 million Spanish speakers worldwide.

## Method Summary
The project employed a community-driven approach combining a hackathon for synthetic instruction generation across 18 domains, dataset collection campaigns, and community translation/validation of existing English benchmarks. Native Spanish speakers validated machine-translated datasets to improve accuracy and cultural relevance. All resources were made openly available on Hugging Face, with the intention of enabling external entities to fine-tune LLMs using techniques like QLoRA or LoRA. The method prioritizes diversity and inclusivity by engaging interdisciplinary teams and representing various Spanish dialects and co-official languages.

## Key Results
- Generated 2.3 million instruction examples across 18 domains through community hackathon
- Created 22 evaluation datasets including translated versions of ARC-C (60%), HellaSwag (15%), MMLU (15%), and DIBT (100%)
- Established first open generative LLM leaderboard for Spanish and co-official languages
- Resources enable instruction-tuning of LLMs in Spanish, Catalan, and Euskera

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Community-driven, open hackathon model produces diverse, large-scale instruction datasets in under-resourced languages.
- Mechanism: Distributing dataset creation tasks across interdisciplinary teams increases coverage of domain-specific knowledge and linguistic varieties while lowering individual workload.
- Core assumption: Diverse participation (technical and linguistic backgrounds) yields higher quality and broader topic coverage than centralized data collection.
- Evidence anchors:
  - [abstract] "The project created a large instruction dataset of 2.3 million examples across 18 domains through a hackathon and dataset collection campaign"
  - [section 4.1] "We highlight the high number of countries represented in the chosen topics (e.g., Colombian Aeronautical Regulation, Refugee Legal Assistance, Peruvian Constitution, international traditional recipes), as well as the project on Guarani culture"
  - [corpus] Strong: 2.3M examples across 18 domains with explicit geographic and topic diversity documented.
- Break condition: If participation drops significantly, domain coverage may shrink and quality could degrade due to lack of peer review and validation.

### Mechanism 2
- Claim: Translation and community validation of existing English datasets extends evaluation coverage for Spanish LLMs.
- Mechanism: Native speaker validation of machine-translated datasets improves accuracy and cultural relevance, enabling robust benchmarking.
- Core assumption: Native speaker review identifies errors that automatic translation tools miss, especially in idiomatic or domain-specific contexts.
- Evidence anchors:
  - [abstract] "Translation and validation efforts covered 60% of ARC-C, 15% of HellaSwag, 15% of MMLU, and 100% of DIBT prompts"
  - [section 3.3] "In collaboration with Hugging Face and Argilla, we launched a community effort for native Spanish speakers to validate these translations"
  - [corpus] Strong: Quantitative coverage metrics (60% ARC-C, 15% HellaSwag, 15% MMLU, 100% DIBT) explicitly documented.
- Break condition: If validation participation is concentrated among a small group (observed 10% of individuals doing most work), bias and fatigue could compromise data quality.

### Mechanism 3
- Claim: Open-source, community-owned resources lower barriers to entry and accelerate NLP development for under-resourced languages.
- Mechanism: Public datasets and leaderboards provide shared evaluation standards, reducing duplication of effort and enabling comparison across models and institutions.
- Core assumption: Availability of open, high-quality datasets and leaderboards incentivizes external entities to build and fine-tune LLMs in Spanish and co-official languages.
- Evidence anchors:
  - [abstract] "These resources enable instruction-tuning and evaluation of LLMs in Spanish, Catalan, and Euskera, advancing NLP representation for the 600 million Spanish speakers worldwide"
  - [section 6] "The generated resources are open; we invite entities with greater computing power to use them for training (with our support, if desired) high-quality LLMs that are open, inclusive, and native"
  - [corpus] Moderate: Open datasets exist but evidence of external adoption and fine-tuning is not yet documented.
- Break condition: If compute or funding barriers remain too high, or if the open resources are not well-advertised, adoption by external entities may remain limited.

## Foundational Learning

- Concept: Instruction-tuning and zero-shot learning in LLMs
  - Why needed here: The project's core goal is to create datasets that enable fine-tuning LLMs to follow instructions, improving zero-shot capabilities.
  - Quick check question: What is the difference between pre-training and instruction-tuning in the context of LLM development?

- Concept: Dataset annotation and validation best practices
  - Why needed here: The project relies on community annotation and validation to ensure dataset quality, especially for translated evaluation sets.
  - Quick check question: Why is native speaker validation important for machine-translated NLP datasets, and what risks arise if it is skipped?

- Concept: Cross-lingual and dialectal variation in NLP
  - Why needed here: Spanish has many dialects and co-official languages; models must be evaluated across these to ensure fairness and usability.
  - Quick check question: How might dialectal variation in Spanish impact the performance of a general-purpose Spanish LLM?

## Architecture Onboarding

- Component map: Dataset generation (hackathon) -> Dataset collection (campaigns) -> Dataset validation (community review) -> Leaderboard creation (benchmark aggregation) -> Model fine-tuning (QLoRA/instruction tuning) -> Open publishing (Hugging Face)
- Critical path: Generate diverse instruction datasets → Validate and clean datasets → Aggregate evaluation benchmarks → Publish open resources → Enable external fine-tuning and evaluation
- Design tradeoffs: Open, community-driven creation maximizes diversity but risks inconsistency; centralized curation ensures quality but limits coverage and inclusivity
- Failure signatures: Low annotation participation → incomplete validation; low diversity in hackathon topics → narrow dataset coverage; poor documentation → low external adoption
- First 3 experiments:
  1. Fine-tune a small Spanish LLM (e.g., 1B-3B parameters) using the #Somos600M instruction dataset and evaluate on the new leaderboard
  2. Compare performance of machine-translated vs. community-validated versions of English benchmarks on Spanish models
  3. Conduct a pilot study with external partners to assess barriers and incentives for adopting the open resources for LLM development

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the #Somos600M Project ensure the representativeness of Spanish dialects and co-official languages in the instruction dataset as it scales up?
- Basis in paper: [explicit] The paper mentions the goal to create an inclusive dataset that represents various Spanish dialects and co-official languages, and the intention to scale up the collection campaign.
- Why unresolved: The current dataset includes a good mix of Spanish dialects and co-official languages, but as the dataset grows, maintaining this diversity will be challenging. The paper does not provide specific strategies for ensuring representativeness in future iterations.
- What evidence would resolve it: A detailed plan outlining specific strategies for engaging with diverse linguistic communities, setting quotas for different dialects and languages, and implementing quality control measures to ensure balanced representation.

### Open Question 2
- Question: What are the most effective methods for translating and validating evaluation datasets to ensure high-quality multilingual benchmarks?
- Basis in paper: [explicit] The paper discusses the translation and validation efforts for English evaluation datasets, highlighting the involvement of native Spanish speakers and the challenges faced in the process.
- Why unresolved: While the paper provides insights into the translation and validation process, it does not offer a comprehensive analysis of the most effective methods or best practices for creating high-quality multilingual benchmarks.
- What evidence would resolve it: A comparative study evaluating different translation and validation methods, including human translation, machine translation with human post-editing, and crowd-sourcing, to determine the most effective approach for creating high-quality multilingual benchmarks.

### Open Question 3
- Question: How can the #Somos600M Project effectively measure and address biases in the instruction dataset and evaluation benchmarks?
- Basis in paper: [explicit] The paper mentions the intention to include evaluations of ethical aspects, such as biases, in the generative LLM leaderboard, but does not provide specific details on how to measure or address these biases.
- Why unresolved: Addressing biases in NLP datasets is a complex challenge, and the paper does not offer concrete strategies for measuring and mitigating biases in the #Somos600M resources.
- What evidence would resolve it: A comprehensive framework for measuring and addressing biases in NLP datasets, including specific metrics, tools, and methodologies for identifying and mitigating biases in both the instruction dataset and evaluation benchmarks.

## Limitations
- The community validation process relies heavily on a small group of contributors (10% of participants doing most work), raising concerns about potential bias and fatigue
- While the datasets exist and are open, there is no documented evidence of their actual use in LLM fine-tuning or external adoption
- The paper lacks detailed quality metrics for the instruction datasets and comprehensive validation studies

## Confidence

**High confidence**: The project successfully created and publicly released the instruction datasets and evaluation benchmarks as described. The hackathon structure and community participation metrics are well-documented.

**Medium confidence**: The claim that these resources will enable instruction-tuning and evaluation of LLMs in Spanish and co-official languages. While the datasets exist and are open, evidence of their actual use in model development is not yet available.

**Low confidence**: The assertion that the community-driven approach will consistently produce high-quality, diverse datasets without significant quality control challenges. The paper documents the process but lacks comprehensive quality metrics and validation studies.

## Next Checks
1. Conduct a controlled experiment fine-tuning a small Spanish LLM (1B-3B parameters) using the #Somos600M instruction dataset, evaluating on the new leaderboard, and comparing results to baseline models.

2. Perform a detailed quality analysis comparing machine-translated versus community-validated versions of English benchmarks, measuring error rates and cultural relevance.

3. Survey external NLP practitioners and organizations to assess actual barriers and incentives for adopting the #Somos600M resources in LLM development projects.