---
ver: rpa2
title: Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream Allocation
  in Feed
arxiv_id: '2412.10381'
source_url: https://arxiv.org/abs/2412.10381
tags:
- live
- stream
- learning
- sl-mgac
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Supervised Learning-enhanced Multi-Group
  Actor Critic (SL-MGAC) algorithm to optimize live stream allocation in a short video
  and live stream mixed recommendation system. The algorithm addresses the challenges
  of high variance in user behavior and live stream supply by introducing a Multi-Group
  State Decomposition (MG-SD) module to partition users into distinct groups based
  on their historical live stream activity.
---

# Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream Allocation in Feed

## Quick Facts
- arXiv ID: 2412.10381
- Source URL: https://arxiv.org/abs/2412.10381
- Reference count: 40
- Primary result: Novel SL-MGAC algorithm optimizes live stream allocation, achieving 2.616% increase in live stream DAU and 7.431% increase in watch time while maintaining platform constraints.

## Executive Summary
This paper introduces a novel Supervised Learning-enhanced Multi-Group Actor Critic (SL-MGAC) algorithm to optimize live stream allocation in short video and live stream mixed recommendation systems. The algorithm addresses challenges of high variance in user behavior and live stream supply by introducing Multi-Group State Decomposition to partition users into distinct groups and incorporating supervised learning-enhanced critic frameworks. Experimental results demonstrate significant improvements in user engagement with live streams while satisfying platform-level constraints on app usage duration and user retention.

## Method Summary
The SL-MGAC algorithm combines multi-group state decomposition, supervised learning-enhanced critic frameworks, and layer normalization to optimize live stream allocation. Users are partitioned into K groups based on historical live stream activity, with separate processing for each group. The critic network uses a reward prediction network (RPN) and Q residual network (QRN) trained via multi-task learning on normalized watch-time proportions. Distribution discretization and layer normalization further stabilize training. The method is evaluated through offline policy evaluation and online A/B testing.

## Key Results
- SL-MGAC significantly outperforms baseline methods in maximizing user engagement with live streams
- Online A/B tests show 2.616% increase in live stream DAU and 7.431% increase in live stream watch time
- Platform constraints on app usage duration and user retention are maintained while improving engagement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-group state decomposition reduces variance in Q-value predictions by isolating user segments with distinct live stream preferences.
- Mechanism: Users are partitioned into ùêæ groups based on their cumulative live stream watch time over 3 weeks. Separate MLP layers per group process state representations, reducing the need for the network to generalize across dissimilar behaviors.
- Core assumption: User behavior variance is high enough that a single policy cannot model all groups effectively, and group-level patterns are stable over time.
- Evidence anchors:
  - [section] "we categorize users intoùêæ disjoint groups according to their activity level of the live stream, which is determined by the cumulative watch time of the live stream during the past 3 weeks."
  - [section] "we find that SL-MGAC without the MG-SD module tends to be more unstable during training, with a larger variance in its Q-values."
- Break condition: Group boundaries become obsolete if user preferences shift rapidly or if the partitioning criteria no longer capture meaningful behavioral differences.

### Mechanism 2
- Claim: Supervised learning-enhanced critic learning stabilizes training by constraining temporal difference error accumulation.
- Mechanism: The critic network is split into a reward prediction network (RPN) and a Q residual network (QRN). The RPN is trained via multi-task learning to predict normalized watch-time proportions, while the QRN learns the residual. This combination reduces the variance of predicted rewards and improves Q-value accuracy.
- Core assumption: The variance in raw reward signals is large enough to destabilize critic learning, and supervised prediction of normalized rewards can provide a stable training signal.
- Evidence anchors:
  - [section] "we seamlessly introduce supervised learning... into the critic learning procedure... the variance of ùõø is much smaller than that of the original watch time ùë¶."
  - [section] "reward learning with the discretization of watch time distributions can be viewed as a novel variance reduction technique."
- Break condition: If the normalized reward prediction becomes inaccurate or if the RPN-QRN split introduces significant bias in Q-value estimates.

### Mechanism 3
- Claim: Layer normalization prevents divergence in critic learning by stabilizing neural tangent kernel responses to input perturbations.
- Mechanism: Layer normalization is applied to the inputs of both actor and critic networks, reducing sensitivity to input variations and maintaining stable gradients during training.
- Core assumption: The critic network's self-excitation pattern can cause Q-value explosion without normalization, and layer normalization is sufficient to mitigate this effect.
- Evidence anchors:
  - [section] "normalization techniques, such as Layer Normalization [4] can be utilized to alleviate divergence and instability problems."
  - [section] "From the proof in Appendix D of [45], we know that for any input x and any direction v, if a network applies Layer Normalization to the input, then we have ùëòNTK (x, x + ùúÜv) ‚â§ ùê∂."
- Break condition: If layer normalization's stabilizing effect is overwhelmed by other sources of instability, such as extreme reward variance or highly non-stationary data distributions.

## Foundational Learning

- Concept: Constrained Markov Decision Process (CMDP)
  - Why needed here: The live stream allocation problem must satisfy platform-level constraints on app usage duration and user retention while maximizing long-term engagement.
  - Quick check question: How does the Lagrange multiplier approach transform a CMDP into an unconstrained MDP in this context?

- Concept: Temporal Difference (TD) Error
  - Why needed here: TD error accumulation in critic learning can lead to divergence and model collapse; understanding this helps explain the need for variance reduction techniques.
  - Quick check question: Why does the TD error become unstable when the target Q-value increases faster than the current Q-value?

- Concept: Multi-task Learning
  - Why needed here: Supervised reward learning is integrated with critic learning via multi-task networks to stabilize Q-value estimation and reduce variance.
  - Quick check question: How does predicting normalized watch-time proportions help stabilize the overall critic learning process?

## Architecture Onboarding

- Component map: User & live stream feature extraction ‚Üí Multi-group state decomposition (actor & critic) ‚Üí Actor network with Q-normalization ‚Üí Critic network (RPN + QRN) ‚Üí Supervised reward learning ‚Üí Layer normalization ‚Üí Online exploration (ùúñ-greedy)
- Critical path: State embedding ‚Üí MG-SD ‚Üí Critic RPN ‚Üí Critic QRN ‚Üí Q-value prediction ‚Üí Actor decision ‚Üí Online serving
- Design tradeoffs: Separating RPN and QRN reduces variance but adds model complexity; MG-SD improves stability but requires periodic group reassignment; layer normalization adds computation but prevents divergence.
- Failure signatures: Large Q-value variance indicates MG-SD is insufficient; policy collapse (always injecting live streams) suggests ùúÜ or Q-normalization needs adjustment; divergence in critic loss points to insufficient normalization or reward prediction error.
- First 3 experiments:
  1. Train SL-MGAC without MG-SD and measure Q-value variance and policy stability.
  2. Remove supervised learning from the critic and observe impact on TD error accumulation.
  3. Disable layer normalization and check for divergence or model collapse during training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SL-MGAC model perform in scenarios with even more extreme fluctuations in user behavior and live stream supply than observed in the short video & live stream mixed recommendation system?
- Basis in paper: [inferred] The paper mentions that RL models often struggle with high variance in user behavior and live stream supply, but doesn't test SL-MGAC under even more extreme conditions.
- Why unresolved: The experiments were conducted in a specific mixed recommendation system, and it's unclear how the model would generalize to scenarios with higher variance.
- What evidence would resolve it: Testing SL-MGAC in environments with artificially increased variance or in different recommendation systems with known high variability.

### Open Question 2
- Question: What is the long-term impact of the SL-MGAC model on user retention beyond the 5-day A/B test period?
- Basis in paper: [inferred] The online A/B test only ran for 5 days, but the model is designed to optimize long-term user engagement.
- Why unresolved: The short duration of the A/B test doesn't capture potential long-term effects on user behavior and retention.
- What evidence would resolve it: Conducting extended A/B tests over several weeks or months to observe sustained effects on user retention and engagement.

### Open Question 3
- Question: How does the performance of SL-MGAC compare to other state-of-the-art RL algorithms that have been specifically designed to handle high-variance environments?
- Basis in paper: [explicit] The paper mentions that traditional RL algorithms often face divergence and instability problems in high-variance environments, but doesn't compare SL-MGAC to newer algorithms designed for such scenarios.
- Why unresolved: The comparison is limited to existing methods, and newer algorithms might offer better performance in high-variance settings.
- What evidence would resolve it: Benchmarking SL-MGAC against recently developed RL algorithms like Muesli or other methods designed for stability in high-variance environments.

## Limitations

- User grouping stability may become problematic as user preferences shift rapidly between reassignment periods
- Computational overhead may not scale efficiently to platforms with millions of concurrent users or strict latency requirements
- Lagrange multiplier approach for constraints may not transfer directly to platforms with different constraint types or business models

## Confidence

- **High Confidence**: Multi-Group State Decomposition reduces Q-value variance and improves stability; layer normalization's stabilizing effect is well-grounded
- **Medium Confidence**: Supervised learning-enhanced critic learning shows promise but relies on assumptions about normalized reward prediction stability
- **Medium Confidence**: Online A/B test improvements are significant but should be interpreted within specific platform context

## Next Checks

1. Implement longitudinal study tracking user group membership to quantify how frequently users cross group boundaries and assess impact on policy performance when groups become stale.

2. Conduct systematic ablation study removing RPN component while keeping QRN, and vice versa, to isolate specific contribution of supervised reward learning to variance reduction and training stability.

3. Apply SL-MGAC framework to different recommendation domain with distinct constraint types to evaluate generalizability of Lagrange multiplier approach and identify necessary adaptations.