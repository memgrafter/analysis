---
ver: rpa2
title: 'Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic
  Dialect'
arxiv_id: '2409.17912'
source_url: https://arxiv.org/abs/2409.17912
tags:
- darija
- dataset
- arabic
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Atlas-Chat, the first large language models
  (LLMs) developed for dialectal Arabic, specifically Moroccan Arabic (Darija). To
  address the lack of resources for this low-resource language, the authors constructed
  the Darija-SFT-Mixture dataset of 458K instruction samples by consolidating existing
  resources, creating novel datasets manually and synthetically, and translating English
  instructions with strict quality control.
---

# Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect

## Quick Facts
- arXiv ID: 2409.17912
- Source URL: https://arxiv.org/abs/2409.17912
- Authors: Guokan Shang, Hadi Abdine, Yousef Khoubrane, Amr Mohamed, Yassine Abbahaddou, Sofiane Ennadir, Imane Momayiz, Xuguang Ren, Eric Moulines, Preslav Nakov, Michalis Vazirgiannis, Eric Xing
- Reference count: 40
- Primary result: Introduces Atlas-Chat, the first LLMs for Moroccan Arabic dialect (Darija), achieving superior performance on Darija tasks through instruction tuning on a 458K sample dataset

## Executive Summary
This paper introduces Atlas-Chat, the first large language models (LLMs) specifically developed for Moroccan Arabic dialect (Darija). The authors address the challenge of low-resource languages by constructing the Darija-SFT-Mixture dataset of 458K instruction samples through consolidation of existing resources, manual and synthetic data creation, and translation of English instructions with strict quality control. They fine-tuned Gemma 2 base models (2B, 9B, 27B) on this dataset using LoRA, achieving superior performance in following Darija instructions and standard NLP tasks. Atlas-Chat outperforms both state-of-the-art and Arabic-specialized LLMs like LLaMa, Jais, and AceGPT, with Atlas-Chat-9B showing a 13% performance boost over a larger 13B model on DarijaMMLU. The authors also conducted experimental analysis of different fine-tuning strategies and base model choices, making all resources publicly accessible.

## Method Summary
The authors developed Atlas-Chat by first constructing the Darija-SFT-Mixture dataset containing 458K instruction samples, combining existing Darija resources, manually and synthetically created datasets, and translated English instructions with strict quality control. They then fine-tuned Gemma 2 base models (2B, 9B, 27B) using LoRA with rank 256 and alpha 128, training for 3 epochs with learning rate 5e-5 and warmup ratio 3%. The models were evaluated on custom Darija benchmarks (DarijaMMLU, DarijaHellaSwag, DarijaAlpacaEval, DarijaBench) and compared against baseline models. The approach emphasizes parameter-efficient fine-tuning and careful dataset curation to address the low-resource nature of Darija.

## Key Results
- Atlas-Chat models (2B, 9B, 27B) exhibit superior ability in following Darija instructions and performing standard NLP tasks
- Atlas-Chat-9B achieves a 13% performance boost over a larger 13B model on DarijaMMLU
- LoRA fine-tuning proved more effective than full fine-tuning, preventing catastrophic forgetting
- Models outperform state-of-the-art and Arabic-specialized LLMs like LLaMa, Jais, and AceGPT on Darija benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality, native Darija instruction data significantly improves model performance in following Darija instructions
- Mechanism: By consolidating existing Darija resources, creating novel datasets manually and synthetically, and translating English instructions with stringent quality control, the authors constructed a comprehensive instruction dataset (Darija-SFT-Mixture) that exposes the model to diverse Darija language patterns and contexts
- Core assumption: The quality and diversity of instruction data directly correlates with the model's ability to understand and generate Darija
- Evidence anchors:
  - [abstract]: "Atlas-Chat-2B, 9B, and 27B models, fine-tuned on the dataset, exhibit superior ability in following Darija instructions and performing standard NLP tasks"
  - [section]: "By combining these different sources, we aimed to enhance the model's ability to understand and generate Darija across various contexts"
  - [corpus]: Weak evidence - The corpus provides related papers but no direct evidence for this specific mechanism
- Break condition: If the quality of Darija instruction data is poor or insufficient, the model's performance in following Darija instructions would degrade

### Mechanism 2
- Claim: Fine-tuning on an instruction-tuned model yields significantly higher scores than fine-tuning on a base model
- Mechanism: The authors investigated the performance differences between fine-tuning on an instruction-tuned Gemma 2 model versus a base model, finding that continual fine-tuning of instruction-tuned models resulted in significantly higher scores on their dataset
- Core assumption: Models that have already been instruction-tuned have learned representations that are more conducive to further fine-tuning for instruction following
- Evidence anchors:
  - [section]: "Results indicate that the latter, with Low-Rank Adaptation (LoRA), proved to be more effective, whereas full fine-tuning resulted in catastrophic forgetting"
  - [abstract]: "Atlas-Chat models, fine-tuned from the Gemma 2 models on our instruction dataset, exhibit superior ability in Darija"
  - [corpus]: Weak evidence - The corpus provides related papers but no direct evidence for this specific mechanism
- Break condition: If the base model is already highly capable or the instruction-tuning process is not effective, the advantage of fine-tuning on an instruction-tuned model may diminish

### Mechanism 3
- Claim: Parameter-efficient fine-tuning approaches (LoRA) are more effective than full fine-tuning for adapting LLMs to low-resource languages
- Mechanism: The authors compared full fine-tuning and parameter-efficient approaches, finding that LoRA was more effective and helped maintain more diverse generations while better maintaining the base model's performance on tasks outside the target domain
- Core assumption: LoRA introduces a desirable form of regularization that prevents catastrophic forgetting and maintains model diversity
- Evidence anchors:
  - [section]: "Results indicate that the latter, with Low-Rank Adaptation (LoRA), proved to be more effective, whereas full fine-tuning resulted in catastrophic forgetting"
  - [abstract]: "Atlas-Chat-2B, 9B, and 27B models, fine-tuned on the dataset, exhibit superior ability in Darija"
  - [corpus]: Weak evidence - The corpus provides related papers but no direct evidence for this specific mechanism
- Break condition: If the task requires significant changes to the model's architecture or if the base model is not well-suited for the target domain, LoRA may not be sufficient

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: To adapt large language models to follow specific instructions in Darija, a low-resource language
  - Quick check question: What is the primary difference between instruction tuning and traditional fine-tuning approaches?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: To efficiently adapt large language models to Darija without requiring full fine-tuning, which can lead to catastrophic forgetting
  - Quick check question: How does LoRA differ from traditional fine-tuning approaches in terms of parameter updates?

- Concept: Dataset construction and curation
  - Why needed here: To create a comprehensive instruction dataset for Darija that includes native data, synthetic data, and translated data
  - Quick check question: What are the key considerations when constructing a dataset for a low-resource language like Darija?

## Architecture Onboarding

- Component map: Gemma 2 base models (2B, 9B, 27B) -> LoRA fine-tuning -> Atlas-Chat models
- Critical path: Data preparation → LoRA fine-tuning → Evaluation on Darija benchmarks
- Design tradeoffs: Using LoRA allows for efficient adaptation but may limit the model's capacity to learn new representations compared to full fine-tuning
- Failure signatures: Poor performance on Darija tasks may indicate issues with data quality, LoRA configuration, or base model selection
- First 3 experiments:
  1. Compare performance of LoRA fine-tuning vs full fine-tuning on a small subset of the dataset
  2. Evaluate the impact of different LoRA rank values on model performance
  3. Test the model's ability to handle zero-shot and few-shot scenarios in Darija tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Atlas-Chat model perform on tasks outside of the evaluated benchmarks, such as creative writing or code generation?
- Basis in paper: [inferred] The paper mentions that the model can handle various NLP tasks, but doesn't explicitly evaluate its performance on creative writing or code generation.
- Why unresolved: The evaluation focuses on discriminative and generative tasks within the DarijaBench, DarijaMMLU, and DarijaHellaSwag benchmarks. These benchmarks may not cover the full range of capabilities that the model possesses.
- What evidence would resolve it: Conducting additional evaluations on creative writing tasks (e.g., story generation, poetry) and code generation tasks (e.g., writing simple functions, explaining code snippets) would provide insights into the model's performance on these specific domains.

### Open Question 2
- Question: How does the performance of Atlas-Chat models vary with different fine-tuning strategies and base model choices?
- Basis in paper: [explicit] The paper mentions that they conducted an experimental analysis of various fine-tuning strategies and base model choices.
- Why unresolved: The paper provides results comparing different fine-tuning strategies and base models, but doesn't delve into the specific reasons behind the performance differences.
- What evidence would resolve it: A detailed analysis of the impact of different fine-tuning strategies (e.g., LoRA vs. full fine-tuning) and base model choices (e.g., Gemma 2 vs. Jais) on model performance would shed light on the factors contributing to the observed differences.

### Open Question 3
- Question: How does the Atlas-Chat model handle long-form conversations and maintain context over multiple turns?
- Basis in paper: [inferred] The paper mentions that the model was fine-tuned on a dataset containing multi-turn samples, but doesn't explicitly evaluate its ability to maintain context in long conversations.
- Why unresolved: The evaluation focuses on single-turn tasks and doesn't assess the model's ability to engage in coherent and context-aware conversations over multiple turns.
- What evidence would resolve it: Conducting evaluations on long-form conversations, where the model is asked to maintain context and coherence over multiple turns, would provide insights into its conversational capabilities.

## Limitations

- The evaluation framework relies heavily on self-constructed benchmarks, which may not fully capture real-world usage scenarios
- The translation quality of English instructions to Darija, despite claimed "stringent quality control," remains difficult to verify without independent assessment
- The comparison with other Arabic models is constrained by the availability of public benchmarks and standardized evaluation protocols for dialectal Arabic

## Confidence

**High Confidence**: The claim that Atlas-Chat models outperform baseline models on self-constructed Darija benchmarks is supported by the reported metrics and methodology. The dataset construction process and fine-tuning approach are well-documented and reproducible.

**Medium Confidence**: The assertion that Atlas-Chat achieves a 13% performance boost over larger models requires further validation through independent benchmarking and statistical significance testing. The effectiveness of LoRA over full fine-tuning is supported by results but could benefit from more extensive ablation studies.

**Low Confidence**: The claim of "superior ability in following Darija instructions" in real-world scenarios lacks independent verification. The quality and representativeness of the synthetic data generation process, while described, cannot be fully evaluated without access to the generated samples.

## Next Checks

1. **Independent Benchmark Validation**: Have the models evaluated on externally developed Darija benchmarks and real-world user interaction data to verify generalization beyond self-constructed tests.

2. **Statistical Significance Analysis**: Perform comprehensive statistical tests (e.g., paired t-tests) on the performance differences between Atlas-Chat and baseline models across multiple random seeds and training runs to establish confidence in the claimed improvements.

3. **Ablation Study on Data Components**: Conduct systematic ablation studies removing different components of the Darija-SFT-Mixture dataset (native, synthetic, translated) to quantify their individual contributions to model performance and identify potential over-reliance on any single data source.