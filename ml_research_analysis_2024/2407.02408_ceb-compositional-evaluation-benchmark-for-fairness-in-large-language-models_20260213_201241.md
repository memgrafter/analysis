---
ver: rpa2
title: 'CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models'
arxiv_id: '2407.02408'
source_url: https://arxiv.org/abs/2407.02408
tags:
- bias
- datasets
- social
- llms
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CEB, a Compositional Evaluation Benchmark
  designed to assess fairness and bias in large language models (LLMs). Existing bias
  evaluation efforts are limited by scope and incompatible metrics, hindering comprehensive
  and comparable assessments.
---

# CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models

## Quick Facts
- arXiv ID: 2407.02408
- Source URL: https://arxiv.org/abs/2407.02408
- Reference count: 40
- Authors: Song Wang; Peng Wang; Tong Zhou; Yushun Dong; Zhen Tan; Jundong Li
- One-line primary result: CEB provides a comprehensive framework for evaluating LLM bias across multiple dimensions, revealing significant variations in model performance

## Executive Summary
This paper introduces CEB (Compositional Evaluation Benchmark), a novel framework designed to address the fragmented landscape of LLM bias evaluation. The authors identify that existing bias evaluation efforts are limited by scope and incompatible metrics, hindering comprehensive and comparable assessments. CEB proposes a compositional taxonomy that characterizes bias evaluation datasets along three dimensions: bias type (stereotyping and toxicity), social group (age, gender, race, religion), and task (recognition, selection, continuation, conversation, classification). Based on this taxonomy, the authors constructed new evaluation datasets covering previously unexplored configurations, resulting in a total of 11,004 samples.

The experimental results demonstrate that bias levels vary significantly across different dimensions and models. For instance, GPT models consistently outperform other models in recognition and selection tasks, while smaller models like Llama2 show higher rates of refusing to answer (RtA) sensitive questions. The findings provide insights into the strengths and weaknesses of different LLMs in handling various types of bias, offering guidance for developing targeted bias mitigation methods.

## Method Summary
CEB addresses bias evaluation fragmentation by creating a compositional taxonomy that maps existing datasets and identifies gaps across three dimensions: bias type (stereotyping and toxicity), social group (age, gender, race, religion), and task (recognition, selection, continuation, conversation, classification). The authors processed existing datasets and generated new samples using GPT-4 to cover unexplored configurations, resulting in 11,004 total samples. Evaluation uses unified metrics: Micro-F1 for recognition/selection tasks, GPT-4 bias scoring for continuation/conversation tasks, and fairness metrics (Demographic Parity, Equalized Odds, Unfairness Score) for classification tasks. The framework was applied to evaluate multiple LLMs including GPT-3.5, GPT-4, Llama2 variants, Llama3, and Mistral.

## Key Results
- GPT models consistently outperform other models in recognition and selection tasks across all social groups
- Smaller models like Llama2 show higher rates of refusing to answer (RtA) sensitive questions, particularly for race and religion
- Bias levels vary significantly across different dimensions, with task type and social group being major factors
- GPT-4 serves as a reliable bias evaluator with general alignment to human evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The compositional taxonomy allows unified evaluation across diverse datasets by mapping them to a common framework
- Mechanism: Datasets are characterized along three dimensions (bias type, social group, task), enabling consistent evaluation metrics to be applied across previously incompatible datasets
- Core assumption: Different bias evaluation datasets can be meaningfully mapped to a shared taxonomy without losing essential distinctions
- Evidence anchors:
  - [abstract] "Based on this taxonomy, the authors constructed new evaluation datasets covering previously unexplored configurations"
  - [section] "By rigorously characterizing each collected dataset with a configuration, we propose to establish evaluation metrics applicable across these datasets"
  - [corpus] "Average neighbor FMR=0.448" indicates moderate relatedness to existing fairness benchmarks
- Break condition: If datasets cannot be adequately mapped to the three dimensions, the unified evaluation approach fails

### Mechanism 2
- Claim: GPT-4 serves as a reliable bias evaluator, providing scalable alternative to human evaluation
- Mechanism: GPT-4 is used to score bias in generated content, with human evaluations showing general alignment with GPT-4 scores
- Core assumption: GPT-4's performance as an evaluator is not significantly compromised by its own biases
- Evidence anchors:
  - [abstract] "We conduct additional human evaluations for scoring... Humans are generally aligned with GPT-4"
  - [section] "This suggests that GPT-4 could serve as a viable and reliable tool for evaluating bias in generated content"
  - [corpus] "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs" shows similar approaches exist
- Break condition: If GPT-4's inherent biases systematically skew evaluation results away from human judgment

### Mechanism 3
- Claim: Creating datasets for unexplored configurations fills critical evaluation gaps in bias assessment
- Mechanism: The taxonomy reveals numerous configurations not covered by existing datasets, which are then constructed using GPT-4 to generate appropriate samples
- Core assumption: GPT-4 can generate high-quality bias evaluation samples for previously unexplored configurations
- Evidence anchors:
  - [abstract] "we develop a comprehensive evaluation strategy for the bias in LLMs" and "datasets covering previously unexplored configurations"
  - [section] "we craft novel evaluation datasets that cover a wide range of configurations"
  - [corpus] "FairI Tales: Evaluation of Fairness in Indian Contexts" shows expansion into new cultural contexts
- Break condition: If GPT-4-generated samples do not adequately represent the intended bias configurations or introduce new biases

## Foundational Learning

- Concept: Taxonomy design and dimensional analysis
  - Why needed here: The core innovation relies on creating a structured framework that can categorize diverse bias datasets
  - Quick check question: Can you identify what three dimensions are used to characterize each dataset in CEB?

- Concept: Evaluation metric standardization
  - Why needed here: The unified approach requires consistent metrics across different dataset types
  - Quick check question: How does CEB handle evaluation metrics differently for recognition/selection tasks versus continuation/conversation tasks?

- Concept: LLM-based data augmentation
  - Why needed here: GPT-4 is used extensively to create new evaluation datasets and process existing ones
  - Quick check question: What role does GPT-4 play in transforming BBQ dataset samples for CEB's Recognition and Selection tasks?

## Architecture Onboarding

- Component map:
  - Taxonomy Engine: Maps datasets to three-dimensional configurations
  - Dataset Processor: Transforms existing datasets and generates new ones using GPT-4
  - Evaluation Engine: Applies appropriate metrics based on task type
  - Result Aggregator: Compiles and visualizes bias evaluation results across configurations
  - Human Evaluation Module: Validates GPT-4 evaluation quality

- Critical path: Taxonomy mapping → Dataset processing → Evaluation execution → Result aggregation → Analysis

- Design tradeoffs: Unified metrics simplify comparison but may lose task-specific nuances; GPT-4 generation enables coverage but introduces potential bias; human validation ensures quality but limits scalability.

- Failure signatures: Inconsistent mappings across dimensions; evaluation results that diverge significantly from human judgment; high refuse-to-answer rates from LLMs; configuration coverage gaps.

- First 3 experiments:
  1. Map 3-4 existing bias datasets to the taxonomy and verify consistent evaluation results
  2. Generate 10 samples for an unexplored configuration using GPT-4 and validate quality
  3. Run a pilot evaluation comparing GPT-4 bias scores against human evaluations on a small sample set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are alternative bias mitigation strategies compared to the compositional taxonomy approach in reducing biases across different social groups and task types?
- Basis in paper: [explicit] The paper mentions that existing bias mitigation techniques focus on specific aspects and employ inconsistent metrics, and proposes CEB to address these limitations
- Why unresolved: The paper focuses on evaluating biases but does not directly compare the effectiveness of different mitigation strategies
- What evidence would resolve it: Experimental results comparing the performance of various bias mitigation techniques on CEB datasets across different social groups and task types

### Open Question 2
- Question: What is the impact of model size and architecture on the ability to handle biases across different social groups and task types?
- Basis in paper: [explicit] The paper experiments with various LLMs with different sizes and observes varying performance levels across tasks and social groups
- Why unresolved: The paper does not provide a detailed analysis of how model size and architecture specifically influence bias handling capabilities
- What evidence would resolve it: A comprehensive study comparing the performance of LLMs with varying sizes and architectures on CEB datasets, focusing on bias detection and mitigation

### Open Question 3
- Question: How do cultural contexts influence the manifestation and evaluation of biases in LLMs?
- Basis in paper: [inferred] The paper acknowledges that cultural contexts may influence bias evaluation but does not explore this aspect in depth
- Why unresolved: The paper primarily focuses on general bias evaluation without considering the impact of cultural differences
- What evidence would resolve it: Experiments evaluating LLM biases across different cultural contexts and comparing the results to identify cultural influences on bias manifestation and evaluation

## Limitations
- The paper relies heavily on GPT-4 for both dataset generation and evaluation, creating potential circularity
- Limited coverage of non-Western social groups and contexts may reduce generalizability
- The unified metrics approach may miss nuanced task-specific aspects of bias

## Confidence
- High confidence: Compositional taxonomy design and dataset coverage
- Medium confidence: GPT-4 evaluation mechanism reliability
- Low confidence: Comparative claims between different LLMs across cultural boundaries

## Next Checks
1. Conduct blind human evaluation on a stratified sample of 200+ responses across different LLM models to verify GPT-4 scoring accuracy
2. Test the CEB framework with a non-English dataset to assess cross-cultural applicability
3. Run ablation studies removing GPT-4-generated samples to measure their impact on overall evaluation results