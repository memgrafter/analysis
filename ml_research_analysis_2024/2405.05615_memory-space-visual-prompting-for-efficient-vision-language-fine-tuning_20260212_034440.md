---
ver: rpa2
title: Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning
arxiv_id: '2405.05615'
source_url: https://arxiv.org/abs/2405.05615
tags:
- visual
- gid00001
- language
- gid00032
- gid00047
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Memory-Space Visual Prompting (MemVP), a novel
  approach for efficiently fine-tuning large vision-language (VL) models. Unlike existing
  methods that integrate visual prompts into the input space of language models, MemVP
  treats visual prompts as additional knowledge stored in the memory space of language
  models, specifically within the Feed-Forward Network (FFN) weights.
---

# Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning

## Quick Facts
- **arXiv ID**: 2405.05615
- **Source URL**: https://arxiv.org/abs/2405.05615
- **Reference count**: 10
- **Primary result**: MemVP achieves superior performance on VL tasks with 23%-44% of VL-PET's FLOPs and outperforms fully fine-tuned models on ScienceQA.

## Executive Summary
This paper introduces Memory-Space Visual Prompting (MemVP), a novel parameter-efficient fine-tuning method for vision-language models. Unlike existing approaches that inject visual prompts into the input space, MemVP embeds visual knowledge directly into the Feed-Forward Network (FFN) weights of language models, treating them as key-value memory. This approach significantly reduces computational complexity during training and inference while maintaining or improving performance across various VL tasks. Experiments demonstrate MemVP's effectiveness on multiple language models including BART, T5, and LLaMA, achieving state-of-the-art results with remarkable efficiency gains.

## Method Summary
MemVP modifies the standard vision-language fine-tuning approach by inserting visual prompts into the FFN weights of language models rather than concatenating them to the input sequence. Visual features from a frozen vision encoder are projected through a lightweight FC layer, position-embedded, and concatenated into the key-value matrices of FFN blocks. Only the projector and position embeddings are fine-tuned, while the language model parameters remain frozen except for these additions. This design maintains the original input length, dramatically reducing computational overhead compared to input-space visual prompting methods while effectively retrieving visual knowledge through the FFN's key-value mechanism.

## Key Results
- MemVP outperforms VL-PET by 0.4% average across VQAv2, GQA, and COCO Captions while using only 23%-44% of the FLOPs
- On ScienceQA with LLaMA-7B, MemVP achieves 94.45% accuracy, surpassing other PEFT methods and even a fully fine-tuned VL-pretrained model
- Training and inference speed improvements are significant, with reduced memory usage due to shorter input sequences
- MemVP demonstrates consistent performance improvements across BART, T5, and LLaMA architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MemVP reduces input sequence length by embedding visual prompts into FFN weights rather than concatenating them into the input
- Mechanism: Visual prompts are projected via a lightweight FC layer, position-embedded, and concatenated into the key-value matrices of the FFN blocks, bypassing the quadratic complexity of long input sequences
- Core assumption: FFNs in language models act as key-value memory and can store additional visual knowledge without degrading performance
- Evidence anchors: [abstract] FFN acts as "key-value memory" for visual knowledge injection; [section] Dai et al. (2022) edit knowledge in FFN by replacing rows; [corpus] No direct evidence, relies on cited works
- Break condition: If FFN weight matrices are too small to accommodate visual prompt entries without excessive parameter overhead or performance degradation

### Mechanism 2
- Claim: By keeping input length unchanged, MemVP dramatically reduces FLOPs compared to input-space visual prompting
- Mechanism: FFN complexity grows linearly with hidden dimension while attention complexity grows quadrillion with input length; MemVP only increases FFN hidden size, not input length
- Core assumption: Attention dominates computational cost, so reducing input length is more effective than modest FFN expansion
- Evidence anchors: [section] Detailed FLOPs calculation showing 4ndL vs 4nd(6d + n + 2L) complexity; [section] Analysis that d >> n and n > L in most VL cases; [corpus] Weak evidence, no direct corpus comparison
- Break condition: If n becomes large enough that 4ndL ~ 4nd(6d + n + 2L), the advantage diminishes

### Mechanism 3
- Claim: Visual knowledge retrieval through FFN keys/values aligns with the model's internal knowledge storage, enabling effective task performance
- Mechanism: Textual queries interact with visual keys/values in FFN blocks, effectively retrieving visual knowledge in the same way factual knowledge is retrieved
- Core assumption: The model can learn to route visual queries to visual keys effectively during fine-tuning
- Evidence anchors: [section] Text tokens show high similarity with related visual knowledge entries; [section] Different input words retrieve different visual knowledge entries; [corpus] No direct corpus evidence, based on internal experiments
- Break condition: If textual queries fail to match visual keys due to poor projector design or position embedding, retrieval fails and performance drops

## Foundational Learning

- Concept: Feed-Forward Network (FFN) as Key-Value Memory
  - Why needed here: MemVP relies on the assumption that FFN weights store knowledge that can be augmented with visual prompts
  - Quick check question: How does the FFN in a transformer layer map an input token to its output, and why is it considered a memory?

- Concept: Parameter-Efficient Fine-Tuning (PEFT) methods
  - Why needed here: MemVP is a PEFT technique; understanding LoRA, adapters, and VL-Adapter is essential to compare efficiency gains
  - Quick check question: What is the main computational bottleneck in input-space visual prompting that MemVP aims to solve?

- Concept: Vision-Language (VL) model architectures
  - Why needed here: MemVP modifies how visual prompts are incorporated; knowing standard architectures helps understand the innovation
  - Quick check question: In typical VL models, how are image features projected into the language model's input space?

## Architecture Onboarding

- Component map: Vision Encoder -> Visual Projector -> Position Embedding -> FFN Blocks -> Language Model -> LoRA (optional)

- Critical path:
  1. Extract image features from frozen vision encoder
  2. Project to visual prompts (shared projector)
  3. Add position embedding to visual prompts
  4. Concatenate into FFN key/value matrices
  5. Fine-tune projector and position embeddings only

- Design tradeoffs:
  - Memory vs. speed: Longer position embeddings increase parameters but allow richer visual knowledge
  - Projector complexity: Single FC vs. deeper projector balances expressivity vs. efficiency
  - GLU vs. standard FFN: GLU allows more sophisticated gating but complicates insertion logic

- Failure signatures:
  - Performance collapse: Visual prompts not retrievable → check projector dimension, position embedding quality
  - Training instability: Large λ scaling factor → reduce λ or clip gradients
  - Memory overflow: Too many visual prompt entries → reduce position embedding length

- First 3 experiments:
  1. Compare FLOPs and latency on VQAv2 between MemVP and VL-PET with 256 visual tokens
  2. Ablation: Insert visual prompts only in keys vs. only in values to test retrieval importance
  3. Projector ablation: Single FC vs. deeper projector on ScienceQA to measure accuracy vs. parameter trade-off

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas warrant further investigation based on the experimental findings and limitations discussed.

## Limitations

- Limited empirical validation of core assumptions about FFNs acting as key-value memory, relying heavily on prior work rather than direct experimental evidence
- Theoretical FLOPs analysis not comprehensively validated with experimental measurements across different model scales and tasks
- Visual instruction tuning results lack comprehensive baseline comparisons with other strong PEFT methods under identical training conditions

## Confidence

- **High confidence**: MemVP's architectural design and implementation details are clearly specified with mathematically sound theoretical efficiency gains
- **Medium confidence**: Performance claims on standard VL benchmarks are supported by experimental results, but superiority varies significantly across tasks and model sizes
- **Low confidence**: Claims about visual instruction tuning effectiveness and superiority over fully fine-tuned models require additional validation given limited comparative experiments

## Next Checks

1. **Experimental FLOPs validation**: Measure and compare actual training and inference FLOPs for MemVP versus input-space visual prompting methods across multiple VL tasks and model scales to verify theoretical efficiency claims

2. **Knowledge retrieval ablation study**: Systematically evaluate the importance of visual knowledge retrieval by ablating either keys or values in the FFN blocks, and assess performance degradation to confirm that retrieval mechanism is critical for MemVP's success

3. **Visual instruction tuning baseline expansion**: Replicate visual instruction tuning experiments with additional PEFT baselines and conduct ablation studies on projector architecture and LoRA rank to better understand performance factors