---
ver: rpa2
title: '"Patriarchy Hurts Men Too." Does Your Model Agree? A Discussion on Fairness
  Assumptions'
arxiv_id: '2408.00330'
source_url: https://arxiv.org/abs/2408.00330
tags:
- fairness
- fair
- sensitive
- assumptions
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the implicit assumptions in fair machine learning
  models, particularly regarding how bias is introduced into data. The authors examine
  eight common fairness assumptions and prove that they all require the biasing process
  to be monotonic - meaning the probability of a positive outcome must increase or
  stay constant as the fair probability increases.
---

# "Patriarchy Hurts Men Too." Does Your Model Agree? A Discussion on Fairness Assumptions

## Quick Facts
- arXiv ID: 2408.00330
- Source URL: https://arxiv.org/abs/2408.00330
- Reference count: 22
- Key outcome: The paper proves that common fairness assumptions in ML models require monotonic bias processes, which may not reflect real-world discrimination patterns.

## Executive Summary
This paper critically examines the implicit assumptions in fair machine learning models, revealing that many common fairness approaches assume bias operates monotonically across sensitive groups. The authors demonstrate that this assumption implies bias only occurs between different groups, not within them - a simplification that may not capture the complexity of real-world discrimination. Through theoretical analysis of eight common fairness assumptions, the paper shows they all require monotonic biasing processes, which could make current fair ML models inadequate when bias is non-monotonic. The authors argue for explicit statement of bias assumptions by practitioners and suggest that current models may need significant revision to handle the true complexity of unfairness in data.

## Method Summary
The paper employs a theoretical analysis framework examining eight common fairness assumptions used in machine learning. The authors develop a fair world framework where they model decisions as functions of fair probabilities (pf) that are then transformed into unfair probabilities (pu) through biasing processes. They analyze whether these biasing processes can be represented as monotonic functions of fair scores, dependent solely on sensitive attributes. The analysis proves that all examined fairness assumptions implicitly require monotonicity in the biasing process, either following an "Affirmative Action" or "Double Standard" pattern. The paper uses mathematical proofs and logical argumentation to demonstrate the implications of these assumptions and their potential limitations.

## Key Results
- All eight examined fairness assumptions require the biasing process to be monotonic
- Monotonicity implies bias only occurs between different sensitive groups, not within them
- Current fair ML models that frequently rerank individuals within the same sensitive group contradict this assumption
- Practitioners must explicitly state their assumptions about bias behavior for models to be interpretable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit fairness assumptions require the biasing process to be monotonic.
- Mechanism: Fairness models often assume that improving fairness for one group will not worsen outcomes for another, which implies a monotonic relationship between fair and unfair probabilities.
- Core assumption: The biasing process can be modeled as a monotonic function of the fair scores.
- Evidence anchors:
  - [abstract] "we are already assuming that the biasing process is a monotonic function of the fair scores, dependent solely on the sensitive attribute."
  - [section 5.2] "All the presented fairness assumptions imply either the 'Affirmative Action' assumption or the 'Double Standard' one."
- Break condition: When bias affects individuals within the same sensitive group, violating monotonicity.

### Mechanism 2
- Claim: Current fair ML models may be inadequate when bias is non-monotonic.
- Mechanism: If the biasing process is non-monotonic, applying standard fairness constraints can actually worsen fairness by reranking individuals within the same group.
- Core assumption: Bias can be captured by simple monotonic transformations.
- Evidence anchors:
  - [abstract] "the biasing process is a monotonic function on each sensitive group"
  - [section 5.2] "The 'No Harm, No Foul' assumption states that decisions can be unfair only when compared between different sensitive groups."
- Break condition: When discrimination occurs within sensitive groups, not just between them.

### Mechanism 3
- Claim: Practitioners must explicitly state their assumptions about bias behavior.
- Mechanism: Without explicit assumptions, fairness models may operate under conflicting or contradictory principles, leading to suboptimal outcomes.
- Core assumption: Fairness is subjective and context-dependent.
- Evidence anchors:
  - [abstract] "practitioners must explicitly state their assumptions about bias behavior"
  - [section 7] "Despite what some old proverbs may say, assuming is not wrong, as long as we are explicit about it."
- Break condition: When model behavior is inconsistent with stated fairness goals.

## Foundational Learning

- Concept: Pareto order and fairness measures
  - Why needed here: Understanding how decisions are compared and ranked is crucial for analyzing fairness assumptions.
  - Quick check question: What does it mean for one decision to be Pareto non-inferior to another?

- Concept: Fair world framework
  - Why needed here: This framework provides the theoretical foundation for analyzing bias and fairness in ML models.
  - Quick check question: How does the fair world framework differ from classical fairness definitions?

- Concept: Biasing and debiasing processes
  - Why needed here: Understanding how fair probabilities are transformed into unfair ones is key to analyzing fairness assumptions.
  - Quick check question: What is the relationship between the biasing process and the debiasing process?

## Architecture Onboarding

- Component map: Data preprocessing and feature extraction -> Fair world probability modeling (pf and pu) -> Decision function (d) -> Fairness measure calculation -> Model comparison and selection
- Critical path: Data preprocessing → Fair world modeling → Decision function → Fairness evaluation → Model selection
- Design tradeoffs:
  - Model complexity vs. interpretability
  - Computational efficiency vs. fairness accuracy
  - Assumption specificity vs. model generalizability
- Failure signatures:
  - Inconsistent model behavior within sensitive groups
  - Poor performance on fairness metrics
  - Unexpected model rankings
- First 3 experiments:
  1. Test model behavior on synthetic data with known monotonic bias
  2. Evaluate model performance on data with non-monotonic bias
  3. Compare model outcomes across different fairness assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the monotonicity assumption break down in real-world datasets where bias operates differently across subgroups?
- Basis in paper: [explicit] The paper explicitly states that current fair ML models assume monotonic bias, but argues this assumption often doesn't reflect reality, citing examples where bias affects individuals within the same sensitive group differently.
- Why unresolved: The paper provides theoretical proofs about the consequences of the monotonicity assumption but doesn't empirically demonstrate how frequently or severely this assumption fails in practice.
- What evidence would resolve it: Empirical studies analyzing real-world datasets to quantify how often bias violates monotonicity within sensitive groups, along with case studies showing the impact of this violation on model fairness.

### Open Question 2
- Question: What alternative mathematical frameworks could better capture non-monotonic bias patterns while maintaining computational tractability?
- Basis in paper: [inferred] The paper concludes that when bias is non-monotonic, current models may be inadequate, implying a need for new frameworks, but doesn't propose specific alternatives.
- Why unresolved: The paper identifies the problem with current assumptions but doesn't explore what mathematical structures could replace monotonicity while still enabling practical fair ML solutions.
- What evidence would resolve it: Development and validation of new fair ML frameworks that can handle non-monotonic bias, along with proofs showing their theoretical properties and empirical results demonstrating improved fairness outcomes.

### Open Question 3
- Question: Under what conditions does within-group reranking in fair ML models indicate model failure versus a necessary consequence of addressing complex bias?
- Basis in paper: [explicit] The paper discusses research showing that many fair models rerank individuals within sensitive groups, and argues this behavior contradicts the monotonicity assumption, but doesn't clearly delineate when this is problematic versus acceptable.
- Why unresolved: The paper establishes that within-group reranking contradicts certain fairness assumptions but doesn't provide criteria for determining when such reranking is justified versus when it indicates a flawed model.
- What evidence would resolve it: A taxonomy of bias scenarios distinguishing when within-group reranking is a necessary feature of fair models versus a bug, supported by case studies and formal proofs.

## Limitations
- The analysis assumes binary sensitive attributes, which may not capture intersectional discrimination
- Real-world bias processes may be more complex than the theoretical monotonic/non-monotonic dichotomy
- The paper focuses on theoretical implications rather than empirical validation on real-world datasets

## Confidence
- **High Confidence**: The identification of implicit monotonic assumptions in fairness models (Mechanism 1)
- **Medium Confidence**: The claim that current models are inadequate for non-monotonic bias (Mechanism 2)
- **Medium Confidence**: The recommendation for explicit assumption stating (Mechanism 3)

## Next Checks
1. Test the monotonic assumption on real-world datasets with documented non-monotonic bias patterns, such as gender discrimination in hiring where bias varies by position level.
2. Develop and evaluate a prototype fairness model that explicitly handles non-monotonic bias within sensitive groups.
3. Conduct user studies with practitioners to assess how explicit assumption stating affects fairness model selection and implementation in real ML pipelines.