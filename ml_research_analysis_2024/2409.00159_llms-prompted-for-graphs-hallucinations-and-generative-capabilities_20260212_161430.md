---
ver: rpa2
title: 'LLMs Prompted for Graphs: Hallucinations and Generative Capabilities'
arxiv_id: '2409.00159'
source_url: https://arxiv.org/abs/2409.00159
tags:
- graph
- graphs
- llms
- output
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the ability of large language models (LLMs)\
  \ to recite and generate graphs. For recitation, the study assesses how accurately\
  \ LLMs reproduce well-known graphs like the Karate Club and Les Mis\xE9rables graphs,\
  \ identifying graph hallucinations as errors in topology."
---

# LLMs Prompted for Graphs: Hallucinations and Generative Capabilities

## Quick Facts
- **arXiv ID:** 2409.00159
- **Source URL:** https://arxiv.org/abs/2409.00159
- **Reference count:** 0
- **Primary result:** LLMs exhibit significant variations in graph recitation and generation capabilities, with notable hallucinations and limited structural fidelity in random graph generation.

## Executive Summary
This study investigates large language models' (LLMs) ability to recite and generate graphs, focusing on graph hallucinations and structural accuracy. Using metrics like graph edit distance and spectral distance, the research evaluates LLMs' performance on tasks such as reproducing well-known graphs (e.g., Karate Club) and generating Erdös-Rényi random graphs. Results show significant variations in LLM performance, with some models producing highly inaccurate graphs. The study also explores correlations between graph-based hallucination metrics and existing benchmarks, highlighting the potential for graph-based evaluation methods while underscoring limitations in LLMs' generative capabilities for structured data.

## Method Summary
The study evaluates LLMs by prompting them to recite and generate graphs, using structured requests for edge lists. For recitation, well-known graphs (Karate Club, Les Misérables, Graph Atlas #50) are used as ground truth. For generation, Erdös-Rényi random graphs are requested. Outputs are parsed into NetworkX graph objects, and metrics like graph edit distance, spectral distance, and degree sequence L2 distance are computed to quantify topological differences. The study also tests degree distribution accuracy for random graph generation using χ² tests against binomial distributions.

## Key Results
- LLMs show significant variations in graph recitation accuracy, with models like dbrx-instruct and gpt4o performing well, while others like qwen2-72B-Instruct produce highly inaccurate graphs.
- Most models struggle to generate statistically accurate Erdös-Rényi graphs, with performance varying significantly across node counts and edge probabilities.
- Weak but meaningful correlations (Spearman rank correlation of 0.2-0.3) are found between graph-based hallucination metrics and established text-based hallucination benchmarks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Graph-structured queries can extract O(n²) structured bits per LLM prompt, far exceeding the 1-2 bits from MCQ benchmarks.
- **Mechanism:** A single request for a graph yields an edge list; parsing this list into a graph object provides node and edge counts plus adjacency relations, each a bit of structured information.
- **Core assumption:** The LLM can return structured data (edge lists) in a parseable format.
- **Evidence anchors:**
  - [abstract] "a single request to obtain a n node graph from a LLM yields O(n2) structured information bits"
  - [section 1] "requesting the standard ‘Les Misérables’ graph is much more efficient than issuing queries like ‘do Jean Valljean and Cosette both appear in a chapter?’"
  - [corpus] "Found 25 related papers" (corpus signal shows relevant graph-LLM work exists)
- **Break condition:** LLM refuses to return structured lists or outputs non-standard formats that regex parsing cannot extract.

### Mechanism 2
- **Claim:** Topological error metrics (e.g., edit distance, spectral distance) correlate weakly but meaningfully with established hallucination benchmarks.
- **Mechanism:** By averaging graph edit distances over a small set of ground-truth graphs, we obtain a scalar hallucination score that correlates with larger MCQ-based hallucination leaderboards.
- **Core assumption:** Hallucination in graph recitation shares statistical signatures with textual hallucination.
- **Evidence anchors:**
  - [section 3] "Spearman rank correlation of 0.3" (RQ1) and "Spearman correlation of 0.2" (RQ2) with the Hallucination Leaderboard
  - [section 2.2] "t-SNE representation" shows model families spread in embedding space
  - [corpus] "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework" (relevant metric work)
- **Break condition:** If correlation falls below ~0.1, the metric loses discriminative power.

### Mechanism 3
- **Claim:** LLM temperature modulates the trade-off between syntactic correctness and structural fidelity in random graph generation.
- **Mechanism:** Higher temperature increases output diversity but also incoherence; low temperature yields deterministic, often correct syntax but potentially less random topology.
- **Evidence anchors:**
  - [section 4.3] "increasing the temperature interestingly improves the success rate γ" for degree-distribution test
  - [section 4.2] "σ(M,T,n,p)" defined as fraction of syntactically correct answers vs temperature T
  - [corpus] "Can LLMs generate random numbers?" (Hopkins et al.) shows temperature effects on uniformity
- **Break condition:** If σ drops below 0.2, outputs become too noisy to parse.

## Foundational Learning

- **Concept:** Graph edit distance
  - Why needed here: To quantify topological differences between ground-truth and LLM-generated graphs.
  - Quick check question: If two graphs differ by one edge addition, what is their edit distance? *(Answer: 1)*
- **Concept:** Spectral distance (eigenvalue-based)
  - Why needed here: Provides a scalable alternative to edit distance for larger graphs.
  - Quick check question: If two graphs have identical degree sequences but different edge arrangements, will their spectral distances differ? *(Answer: Yes)*
- **Concept:** Degree distribution hypothesis testing
  - Why needed here: To evaluate whether LLM-generated graphs follow Erdős–Rényi binomial degree distributions.
  - Quick check question: What distribution do node degrees follow in a true Erdős–Rényi graph? *(Answer: Binomial)*

## Architecture Onboarding

- **Component map:** Prompt generator → LLM API/Platform → Response parser (regex) → Graph builder (NetworkX) → Metric calculator (edit/spectral/degree test) → Aggregator (rankings, correlations)
- **Critical path:**
  1. Issue graph prompt
  2. Parse edge list
  3. Build graph object
  4. Compute topology metrics
  5. Compare to ground truth
- **Design tradeoffs:**
  - Use regex for parsing (fast, brittle) vs. structured schema parsing (robust, slower)
  - Small prompt set (efficient, less reliable) vs. large benchmark set (expensive, more reliable)
- **Failure signatures:**
  - High σ but low γ → syntactically correct but structurally inaccurate outputs
  - Low σ across all models → parsing logic or prompt format issue
  - Negative correlation with leaderboard → metric capturing different error types
- **First 3 experiments:**
  1. Vary prompt wording (Direct vs. Chain-of-Thought) and measure impact on σ and γ.
  2. Sweep temperature T from 0.0 to 2.0 and record changes in syntactic correctness and spectral distance.
  3. Replace regex parsing with a small Python script that calls LLM’s structured output API, compare success rates.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of LLMs in generating random graphs scale with the number of nodes and edge probability in the Erdös-Rényi model?
- **Basis in paper:** [explicit] The paper notes that performance varies significantly with different Erdös-Rényi parameters (n, p) and that all models struggle with certain parameter pairs, such as (n=15, p=0.5).
- **Why unresolved:** The paper only tests a limited set of parameter pairs and does not explore the full parameter space or scaling behavior.
- **What evidence would resolve it:** Systematic testing of LLMs across a wide range of node counts and edge probabilities to identify patterns or thresholds in performance.

### Open Question 2
- **Question:** Can LLMs be fine-tuned or prompted to reduce hallucinations when reciting or generating graphs?
- **Basis in paper:** [inferred] The paper identifies significant hallucinations in both graph recitation and generation tasks but does not explore mitigation strategies.
- **Why unresolved:** The study focuses on characterizing hallucinations rather than addressing them, leaving open the question of whether targeted interventions could improve performance.
- **What evidence would resolve it:** Experiments comparing standard LLMs to those fine-tuned or prompted specifically for graph tasks, measuring reductions in hallucination rates.

### Open Question 3
- **Question:** How do LLMs' graph generation capabilities compare to specialized graph neural networks (GNNs) or other graph generation algorithms?
- **Basis in paper:** [inferred] The paper evaluates LLMs' ability to generate Erdös-Rényi graphs but does not compare their performance to dedicated graph generation methods.
- **Why unresolved:** The study focuses on emergent LLM capabilities but does not benchmark them against established graph generation techniques.
- **What evidence would resolve it:** Direct comparisons of LLM-generated graphs to those produced by GNNs or classical algorithms, using metrics like degree distribution accuracy and structural similarity.

## Limitations
- **Ground Truth Dependency:** All hallucination measurements depend on having exact reference graphs, which may themselves contain errors.
- **Prompt Fragility:** The study uses fixed prompts without exploring variations, potentially limiting generalizability.
- **Scale Constraints:** Graph edit distance computation scales poorly (O(n³)), limiting analysis to small graphs.

## Confidence
- **High Confidence:** The core methodology of using structured graph outputs as information extraction is sound, supported by the O(n²) information density claim.
- **Medium Confidence:** The correlation findings between graph-based and text-based hallucination metrics, though weak, suggest potential for graph-based evaluation methods.
- **Low Confidence:** The generative capability results, particularly the temperature effects, need replication across more models and graph sizes to establish robustness.

## Next Checks
1. **Prompt Robustness Test:** Systematically vary prompt formulations (direct request, chain-of-thought, example-based) and measure impact on σ (syntactic correctness) and γ (structural fidelity) across 5-10 models.
2. **Correlation Replication:** Expand the graph set to 10-15 well-known graphs and recompute correlations with the Hallucination Leaderboard to test statistical significance and stability of the 0.2-0.3 Spearman range.
3. **Temperature Sweep Validation:** For each model, conduct a comprehensive temperature sweep (0.0 to 2.0 in 0.2 increments) measuring all three metrics (σ, edit distance, degree distribution fit) to characterize the full trade-off surface.