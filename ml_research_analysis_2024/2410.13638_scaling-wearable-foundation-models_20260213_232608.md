---
ver: rpa2
title: Scaling Wearable Foundation Models
arxiv_id: '2410.13638'
source_url: https://arxiv.org/abs/2410.13638
tags:
- data
- sensor
- scaling
- wearable
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether scaling laws apply to wearable sensor
  data by training a multimodal foundation model, LSM, on up to 40 million hours of
  physiological and behavioral data from over 165,000 individuals. LSM is built using
  a masked autoencoder approach and evaluated on generative tasks (imputation, interpolation,
  extrapolation) and discriminative tasks (exercise and activity recognition).
---

# Scaling Wearable Foundation Models

## Quick Facts
- arXiv ID: 2410.13638
- Source URL: https://arxiv.org/abs/2410.13638
- Authors: Girish Narayanswamy; Xin Liu; Kumar Ayush; Yuzhe Yang; Xuhai Xu; Shun Liao; Jake Garrison; Shyam Tailor; Jake Sunshine; Yun Liu; Tim Althoff; Shrikanth Narayanan; Pushmeet Kohli; Jiening Zhan; Mark Malhotra; Shwetak Patel; Samy Abdel-Ghaffar; Daniel McDuff
- Reference count: 40
- Key outcome: Scaling compute, data, and model parameters improves wearable sensor model performance by 16-23% in generative tasks and 27-29% in discriminative accuracy.

## Executive Summary
This study investigates whether scaling laws apply to wearable sensor data by training a multimodal foundation model, LSM, on up to 40 million hours of physiological and behavioral data from over 165,000 individuals. Using a masked autoencoder approach, LSM is evaluated on both generative tasks (imputation, interpolation, extrapolation) and discriminative tasks (exercise and activity recognition). The results demonstrate that scaling compute, data size, and model parameters leads to substantial improvements in performance, with power-law relationships observed across all three dimensions. LSM outperforms baseline methods significantly, establishing the potential of scaling wearable sensor models for real-world health applications.

## Method Summary
The study trains a multimodal foundation model using a masked autoencoder approach on wearable sensor data. The model processes 26 sensor signals (features) across 300-minute windows using random masking with 80% ratio as the pretraining task. Training uses Vision Transformer (ViT) backbone with AdamW optimizer on Google v5e TPUs. The dataset consists of up to 40 million hours of heart rate, heart rate variability, electrodermal activity, accelerometer, skin temperature, and altimeter data from over 165,000 people. Downstream evaluation is performed via linear probing and fine-tuning on discriminative tasks.

## Key Results
- LSM improves generative task performance by 16-23% (MAE reduction) for interpolation and extrapolation compared to baselines
- LSM achieves 27-29% higher accuracy on exercise detection and activity recognition discriminative tasks
- Power-law scaling relationships are empirically validated for compute, data size, and model parameters in wearable sensor models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling compute, data, and model parameters predictably improves wearable sensor model performance via power-law relationships.
- Mechanism: The study empirically validates that test loss follows a power-law relationship with compute (L = aC^b + c), data size, and model parameters. As these resources increase, model performance improves monotonically until saturation, driven by the model's ability to learn generalizable representations from larger and more diverse datasets.
- Core assumption: Scaling laws observed in language and vision domains also apply to wearable sensor data, despite differences in data structure and modality.
- Evidence anchors:
  - [abstract]: "Our results establish the scaling laws of LSM with respect to compute, data size, and model parameters, leading to substantial performance gains on generative imputation, interpolation and extrapolation as well as downstream discriminative tasks."
  - [section]: "Power law behavior has been observed across various domains... In this work, we take a step further and investigate the scaling behavior of training foundation models for multimodal wearable sensor data."
  - [corpus]: Weak. The corpus includes related papers on wearable sensor foundation models but does not directly discuss scaling laws or power-law relationships. Evidence is missing for direct comparison.
- Break condition: Saturation effects emerge when model capacity exceeds the informational complexity of the dataset, or when pretraining tasks become insufficient to leverage additional data.

### Mechanism 2
- Claim: Random masking as a pretraining task enables effective generative and discriminative capabilities in wearable sensor models.
- Mechanism: By randomly masking patches across time and sensor modalities, the model learns to reconstruct missing values, which improves its ability to impute, interpolate, and extrapolate sensor data. This generative pretraining also transfers to discriminative tasks like activity recognition through learned representations.
- Core assumption: Masked input modeling is more effective than contrastive methods for wearable sensor data due to the generative capabilities it offers and the nature of missing data in wearables.
- Evidence anchors:
  - [abstract]: "Our primary pretraining objective is to optimize the masked signal reconstruction loss... Our results establish the scaling laws of LSM for tasks such as imputation, interpolation and extrapolation..."
  - [section]: "We posit that defining generative tasks in the training of wearable sensor models may not only result in learned representations that are useful for downstream classification tasks, but also produce models that can impute missing or incomplete data (interpolate) and extrapolate future sensor values (forecast)."
  - [corpus]: Weak. The corpus mentions related foundation models but does not discuss masking strategies or their effectiveness for generative tasks in wearables.
- Break condition: If masking ratios are too low (insufficient learning signal) or too high (loss of context), or if the pretraining task does not align with downstream task requirements.

### Mechanism 3
- Claim: Scaling the number of subjects and wearable data hours per subject together maximizes model generalization and performance.
- Mechanism: Increasing both subject diversity and data volume per subject provides the model with inter-subject variability and intra-subject activity patterns, enabling better generalization across diverse populations and activities.
- Core assumption: Total number of hours is more important than total number of subjects when the total data volume is held constant, but both dimensions are needed to fully leverage model capacity.
- Evidence anchors:
  - [abstract]: "Using a dataset of up to 40 million hours of in-situ heart rate, heart rate variability, electrodermal activity, accelerometer, skin temperature, and altimeter per-minute data from over 165,000 people..."
  - [section]: "When training using the same total number of wearable signal hours, reducing the number of subjects (but drawing more hours per subject) can yield similar performance. This suggests that total number of hours rather than number of subjects drives gains."
  - [corpus]: Weak. The corpus does not provide evidence on the relative importance of subject count versus data volume per subject for model performance.
- Break condition: If either dimension is scaled in isolation without sufficient balance, the model may overfit to specific subjects or fail to capture diverse activity patterns.

## Foundational Learning

- Concept: Power-law scaling relationships in deep learning
  - Why needed here: Understanding that model performance improves predictably with compute, data, and model size is crucial for designing efficient scaling strategies and allocating resources effectively.
  - Quick check question: If doubling the model parameters while keeping data and compute constant results in a 15% improvement in test loss, what would you expect if you double all three resources simultaneously?

- Concept: Masked autoencoder pretraining
  - Why needed here: The masked autoencoder approach is the core mechanism enabling both generative capabilities (imputation, interpolation, extrapolation) and discriminative performance (activity recognition) in wearable sensor models.
  - Quick check question: How does random masking across time and sensor modalities help the model learn representations that transfer to both generative and discriminative downstream tasks?

- Concept: Multimodal time-series data processing
  - Why needed here: Wearable sensor data consists of multiple correlated physiological and behavioral signals sampled at different rates, requiring specialized preprocessing and model architectures to capture temporal and cross-modal dependencies.
  - Quick check question: Why is it important to consider both the temporal axis (time steps) and feature axis (sensor modalities) when designing patch sizes for the masked autoencoder?

## Architecture Onboarding

- Component map: Data preprocessing → Random masking → Encoder processing → Decoder reconstruction → Loss calculation (MSE) → Backpropagation. For downstream tasks, embeddings from the encoder are either linearly probed or fine-tuned for specific classification objectives.
- Critical path: The core architecture uses a Vision Transformer (ViT) encoder-decoder backbone, processing 26 sensor signals across 300-minute windows with random 80% masking as the pretraining task.
- Design tradeoffs: Larger models and more data improve performance but increase computational cost and risk overfitting. The choice of masking ratio, patch size, and sensor signal order affects learning efficiency and downstream task performance. Balancing subject count and data volume per subject is crucial for generalization.
- Failure signatures: Saturation in scaling laws indicates model capacity exceeding dataset complexity or pretraining task limitations. Poor performance on long-range interpolation/extrapolation suggests difficulty capturing temporal dependencies. Failure to generalize across subjects indicates overfitting to specific activity patterns.
- First 3 experiments:
  1. Validate that random masking with 80% ratio outperforms other masking strategies (structured, temporal, sensor) on interpolation and extrapolation tasks using a small dataset.
  2. Test the impact of different patch sizes (e.g., 10x5 vs. 10x1 vs. 10x10) on reconstruction quality and computational efficiency.
  3. Compare linear probing versus fine-tuning on downstream activity recognition tasks to determine the most sample-efficient approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between the number of subjects and the amount of data per subject for training wearable foundation models?
- Basis in paper: [explicit] The paper states that "total number of hours rather than number of subjects drives gains" but also notes that "scaling both dimensions—subjects and hours—together is essential to fully leverage the model's capacity."
- Why unresolved: The paper suggests that both subject diversity and data volume are important, but does not provide a clear formula or method for determining the optimal balance between the two.
- What evidence would resolve it: Experiments comparing models trained on different combinations of subject numbers and data hours per subject, measuring performance on various downstream tasks.

### Open Question 2
- Question: How can missing data in wearable sensor streams be handled more effectively during pretraining and downstream tasks?
- Basis in paper: [explicit] The paper acknowledges that "handling missing data in both pretraining and downstream tasks remains an open question" and mentions that their current approach uses imputation, which may introduce biases.
- Why unresolved: The paper recognizes the importance of handling missing data but does not propose a specific method for doing so, noting that missing data in wearables often correlates with real-world events.
- What evidence would resolve it: Development and testing of models that can naturally handle missing data without imputation, and comparison of their performance to traditional imputation methods on downstream tasks.

### Open Question 3
- Question: What is the optimal patch size for processing multimodal sensor data in foundation models?
- Basis in paper: [explicit] The paper discusses the importance of patch size in their ablation studies, stating that "a moderate patch size of 10 minutes by 5 features is what we select" but also notes that "both dimensions, of time and features, share unique correlations and dependencies along their corresponding axis."
- Why unresolved: While the paper identifies a patch size that works well for their specific model and task, it does not explore whether this is optimal across different model architectures or downstream applications.
- What evidence would resolve it: Systematic comparison of different patch sizes across various model architectures and downstream tasks, measuring both performance and computational efficiency.

## Limitations
- Potential saturation effects at extreme scales not fully characterized
- Assumption that scaling laws from language/vision domains directly transfer to wearable sensor data
- Limited investigation of how scaling interacts with specific downstream task requirements or domain shifts

## Confidence
- High: Power-law scaling relationships between compute/data/model size and performance are empirically validated
- Medium: Generalizability of findings beyond specific sensor modalities and populations studied
- Medium: Claim that random masking is superior to contrastive methods for wearables

## Next Checks
1. Test scaling law predictions at 2-4x larger scales to identify saturation points and validate power-law exponents across different resource dimensions.
2. Compare masked autoencoder pretraining against contrastive learning approaches on identical datasets to directly evaluate the claimed superiority for generative tasks.
3. Evaluate model performance across diverse populations (different age groups, health conditions, geographic regions) to assess generalization beyond the primary dataset.