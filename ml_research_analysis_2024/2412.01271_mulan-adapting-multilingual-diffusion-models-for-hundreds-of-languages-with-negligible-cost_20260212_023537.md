---
ver: rpa2
title: 'MuLan: Adapting Multilingual Diffusion Models for Hundreds of Languages with
  Negligible Cost'
arxiv_id: '2412.01271'
source_url: https://arxiv.org/abs/2412.01271
tags:
- multilingual
- languages
- generation
- text
- mulan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present MuLan, a lightweight multilingual adapter for
  image diffusion models that enables generation in over 110 languages using only
  English training data. The key insight is that text encoders pre-trained on large-scale
  noisy multilingual image-text pairs provide efficient multilingual semantic alignment.
---

# MuLan: Adapting Multilingual Diffusion Models for Hundreds of Languages with Negligible Cost

## Quick Facts
- arXiv ID: 2412.01271
- Source URL: https://arxiv.org/abs/2412.01271
- Reference count: 29
- Primary result: Enables generation in over 110 languages using only English training data with CLIP similarity scores of 39.57 for English and 39.61 for other languages

## Executive Summary
MuLan presents a lightweight multilingual adapter for image diffusion models that enables generation across hundreds of languages using only English training data. The key innovation leverages pre-trained multilingual text encoders that naturally align different languages around image representations through contrastive learning. By training a small adapter (<20M parameters) while freezing both the multilingual text encoder and diffusion model, MuLan achieves cost-effective multilingual generation that matches or exceeds previous multilingual T2I models.

## Method Summary
MuLan uses a plug-and-play adapter architecture that bridges a frozen multilingual text encoder with a frozen diffusion model. The adapter is trained on 17M English image-text pairs for approximately 12 hours on 8 A100 GPUs. Different adapter architectures are employed for different diffusion models - simple MLPs for Pixart-α and transformer-based designs for SD models. The approach achieves multilingual generation through the pre-existing semantic alignment in the multilingual text encoder's vector space, eliminating the need for multilingual training data.

## Key Results
- Achieves CLIP similarity scores of 39.57 for English and 39.61 for other languages
- Trained in only 12 hours on 8 A100 GPUs using 17M English image-text pairs
- Successfully generates images in over 110 languages including low-resource languages
- Integrates seamlessly with community tools like LoRA, LCM, ControlNet, and IP-Adapter

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual text encoders trained on large-scale noisy multilingual image-text pairs provide efficient multilingual semantic alignment for text-to-image generation.
- Mechanism: The multilingual text encoder, pre-trained on diverse multilingual image-text pairs using contrastive learning, naturally aligns different languages in a shared vector space around the image representation. This alignment enables the adapter to project multilingual text embeddings into the same space as the image decoder without requiring multilingual training data.
- Core assumption: Semantic alignment of different languages around images in the multilingual text encoder's vector space is sufficient to enable zero-shot multilingual image generation when combined with a frozen image decoder.
- Evidence anchors:
  - [abstract] "leveraging text encoders pre-trained on widely available, noisy Internet image-text pairs significantly enhances data efficiency in text-to-image (T2I) generation across multiple languages"
  - [section 3.2] "Image-Centered Alignment. In the image-centered alignment approach, CLIP maximizes the similarity between positive text-image pairs and minimizes it for negative pairs through contrastive learning. This training uses text-image pairs, and when the text includes multiple languages, the language encoder aligns different languages in the vector space naturally around the image"
  - [corpus] Weak evidence. Related papers focus on multilingual evaluation and retrieval, but none directly address multilingual diffusion model training with noisy image-text pairs.
- Break condition: If the multilingual text encoder's alignment around images does not generalize well to unseen concepts or if the adapter cannot properly bridge the semantic gap between the text and image spaces.

### Mechanism 2
- Claim: Training only a lightweight adapter while freezing both the multilingual text encoder and diffusion model achieves cost-effective multilingual generation.
- Mechanism: By freezing the pre-trained multilingual text encoder and image diffusion model, the adapter only needs to learn the mapping between their respective feature spaces. This significantly reduces training cost compared to fine-tuning the entire model or training from scratch with multilingual data.
- Core assumption: The pre-trained multilingual text encoder and diffusion model have compatible feature spaces that can be bridged with a relatively simple adapter architecture.
- Evidence anchors:
  - [abstract] "a lightweight language adapter with fewer than 20M parameters, trained alongside a frozen text encoder and image diffusion model"
  - [section 3.2] "we propose MuLan. This model incorporates a lightweight language adapter L′ that bridges a multilingual-aligned language model with a visual generator, enabling generalization to multiple languages after training on a small amount of English text-to-image generation data"
  - [section 4.2] "InternVL-MuLan-SD15 requires just 0.5×8 GPU-days, and InternVL-MuLan-PixArt only 2×8 GPU-days"
- Break condition: If the adapter architecture is insufficient to bridge the semantic gap, or if the frozen components have drifted too far from the training distribution.

### Mechanism 3
- Claim: The adapter can be designed differently for different diffusion model architectures (e.g., MLP for Pixart-α, transformer for SD models) while maintaining effectiveness.
- Mechanism: The adapter architecture is matched to the specific requirements of each diffusion model's text encoder interface. For Pixart-α, a simple MLP can project InternVL outputs, while for SD models, a transformer with learnable queries better handles the complex text conditioning.
- Core assumption: Different diffusion model architectures have different text encoding requirements, and the adapter can be specialized to each architecture without losing multilingual generalization.
- Evidence anchors:
  - [section 3.2] "We adopt different adapter designs for different diffusion models. In detail, we can achieve good results using a simple MLP architecture for Pixart-α (Chen et al., 2023a). However, we find MLP could not properly deal with SD models (Rombach et al., 2022; Podell et al., 2023). Instead, we choose to use one layer encoder-decoder transformer with a set of learnable queries for extracting embeddings from InternVL outputs"
  - [section 4.1] "For SD 1.5, we train the adapter for 50k steps at the resolution of 512×512, and for SD 2.1 we adjust the resolution to 768×768. For SDXL, we first train the adapter for 100k steps at the resolution of 512 ×512 and finetune it for another 1k steps at the resolution of 1024×1024"
- Break condition: If the adapter architecture cannot be adapted to new diffusion model architectures, or if the specialization reduces multilingual generalization.

## Foundational Learning

- Concept: Contrastive learning for multimodal representation
  - Why needed here: The multilingual text encoder's ability to align different languages around images comes from contrastive learning on image-text pairs. Understanding this mechanism is crucial for appreciating why the adapter approach works.
  - Quick check question: How does contrastive learning on image-text pairs enable a text encoder to naturally align different languages in a shared vector space?

- Concept: Diffusion model architecture and text conditioning
  - Why needed here: The adapter bridges between a multilingual text encoder and a diffusion model's text conditioning mechanism. Understanding how text is encoded and used in diffusion models is essential for designing effective adapters.
  - Quick check question: What is the role of text conditioning in diffusion models, and how does it differ between architectures like SD and Pixart-α?

- Concept: Adapter modules and parameter-efficient fine-tuning
  - Why needed here: The core innovation is using a small adapter instead of fine-tuning the entire model. Understanding adapter architectures and their design principles is crucial for implementing and extending this approach.
  - Quick check question: What are the key design considerations when creating an adapter module to bridge between two pre-trained models with different feature spaces?

## Architecture Onboarding

- Component map: Multilingual text encoder -> Adapter -> Diffusion model
- Critical path: Text prompt → Multilingual text encoder → Adapter → Diffusion model → Generated image
  The adapter is the critical component that enables multilingual generation by bridging the frozen multilingual text encoder and diffusion model.

- Design tradeoffs:
  - Adapter complexity vs. training efficiency: More complex adapters may achieve better performance but require more training resources
  - Adapter specialization vs. generalization: Specialized adapters for each diffusion model architecture may perform better but reduce code reusability
  - Frozen components vs. fine-tuning: Freezing pre-trained components saves training cost but may limit adaptation to specific domains

- Failure signatures:
  - Poor performance on non-English languages indicates adapter is not properly bridging the semantic gap
  - Degradation in English performance suggests the adapter is over-specializing to multilingual inputs
  - Training instability or divergence indicates the adapter architecture is incompatible with the frozen components

- First 3 experiments:
  1. Verify the multilingual text encoder's zero-shot performance on non-English prompts without any adapter
  2. Train the adapter on a small subset of English data and evaluate performance on both English and non-English prompts
  3. Compare different adapter architectures (MLP vs. transformer) on the same diffusion model to identify optimal design

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact contribution of different multilingual semantic alignment methods (Image-Centered vs Language-Centered) to the final performance of MuLan?
- Basis in paper: [explicit] The paper explicitly compares different alignment methods and shows Image-Centered Alignment outperforms Language-Centered Alignment significantly.
- Why unresolved: The paper shows performance differences but doesn't provide a detailed ablation study or explain the specific mechanisms that make Image-Centered Alignment more effective.
- What evidence would resolve it: A comprehensive ablation study isolating the contributions of each alignment method, including controlled experiments varying alignment strength and analyzing the impact on different language families.

### Open Question 2
- Question: How does MuLan's performance scale when trained with different proportions of English data (beyond the 1/1024 threshold tested)?
- Basis in paper: [explicit] The paper tests down to 1/1024 of the dataset but notes the model maintains "relatively strong performance" without specifying the exact performance degradation curve.
- Why unresolved: The paper provides limited data points on the scaling relationship between training data size and performance, leaving uncertainty about the optimal data efficiency sweet spot.
- What evidence would resolve it: A detailed scaling study with multiple data fraction points between 1/1024 and full dataset, measuring performance on various language families and quantifying the trade-off between data efficiency and quality.

### Open Question 3
- Question: What are the specific limitations of MuLan when handling low-resource languages versus mainstream languages, and how can these be addressed?
- Basis in paper: [inferred] The paper mentions MuLan performs well on low-resource languages compared to alternatives but doesn't analyze specific failure modes or linguistic features that challenge the model.
- Why unresolved: The paper demonstrates superiority over baselines but doesn't investigate which linguistic properties (morphological complexity, script differences, etc.) pose challenges or what architectural modifications could improve performance.
- What evidence would resolve it: Systematic testing across languages with varying linguistic features, identifying specific failure patterns, and evaluating targeted architectural modifications (e.g., script-specific preprocessing, morphological analysis) to improve low-resource language performance.

### Open Question 4
- Question: How does MuLan's multilingual capability affect the generation of culturally-specific visual concepts that may not have direct translations?
- Basis in paper: [inferred] The paper demonstrates multilingual generation but doesn't investigate how well culturally-specific concepts are preserved across languages, particularly when direct translations may not exist.
- Why unresolved: The paper focuses on quantitative metrics like CLIP scores but doesn't examine qualitative aspects of cultural preservation in generated images.
- What evidence would resolve it: Human evaluation studies comparing culturally-specific concept preservation across languages, including cases where direct translations are unavailable, and analyzing how prompt structure affects cultural concept transfer.

## Limitations

- Data dependency assumptions: The approach critically depends on the quality and coverage of the multilingual text encoder's pre-training data, which may not adequately represent specialized or culturally-specific concepts.
- Evaluation scope: The evaluation focuses on CLIP similarity scores and standard benchmarks but does not thoroughly examine the quality and cultural appropriateness of generated images for non-English languages.
- Adapter architecture generality: While the paper demonstrates success with different adapter designs, the approach may not generalize to all diffusion architectures, suggesting potential brittleness when adapting to new models.

## Confidence

**High Confidence**: The core claim that training a lightweight adapter with frozen components achieves cost-effective multilingual generation is well-supported by the empirical results, with strong evidence from training efficiency and performance metrics.

**Medium Confidence**: The claim about different adapter architectures being necessary for different diffusion models is supported by the paper's results, but the underlying reasons are not fully explored, leaving uncertainty about whether this is a general principle.

**Low Confidence**: The assumption that semantic alignment in the text encoder's vector space automatically translates to appropriate visual representations for all languages is not directly validated, as the paper does not investigate whether aligned languages produce culturally appropriate or conceptually accurate images.

## Next Checks

1. **Cross-lingual concept consistency test**: Evaluate whether the same concept prompts in different languages produce semantically equivalent images by conducting human evaluations across multiple language pairs to test whether semantic alignment translates to visual consistency.

2. **Out-of-distribution language robustness**: Test the model on languages not represented in the pre-training data of the multilingual text encoder to determine the true zero-shot capability and identify the boundaries of language coverage.

3. **Adapter architecture transfer study**: Systematically test the same adapter architecture (MLP and transformer) across different diffusion models to determine whether the architecture choices are truly model-specific or if simpler adapters could work across architectures with appropriate tuning.