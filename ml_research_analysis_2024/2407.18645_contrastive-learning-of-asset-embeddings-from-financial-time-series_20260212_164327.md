---
ver: rpa2
title: Contrastive Learning of Asset Embeddings from Financial Time Series
arxiv_id: '2407.18645'
source_url: https://arxiv.org/abs/2407.18645
tags:
- learning
- asset
- financial
- contrastive
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel contrastive learning framework for
  generating asset embeddings from financial time series data. The approach leverages
  pairwise similarity of asset returns over rolling subwindows to create positive
  and negative samples, using a statistical sampling strategy based on hypothesis
  testing to address the noisy nature of financial data.
---

# Contrastive Learning of Asset Embeddings from Financial Time Series

## Quick Facts
- arXiv ID: 2407.18645
- Source URL: https://arxiv.org/abs/2407.18645
- Reference count: 37
- One-line primary result: Novel contrastive learning framework generates asset embeddings that outperform baselines on industry sector classification (F1=0.69, accuracy=69%) and portfolio optimization (19.1% volatility reduction)

## Executive Summary
This paper introduces a novel contrastive learning framework for generating asset embeddings from financial time series data. The approach leverages pairwise similarity of asset returns over rolling subwindows to create positive and negative samples, using a statistical sampling strategy based on hypothesis testing to address the noisy nature of financial data. Various contrastive loss functions are explored to learn a discriminative representation space. Experiments on real-world datasets demonstrate the effectiveness of the learned embeddings on industry sector classification and portfolio optimization tasks, significantly outperforming existing baselines.

## Method Summary
The framework computes pairwise return similarity between assets across multiple rolling subwindows, counting co-occurrences among top-k similar assets to build a co-occurrence matrix. A hypothesis test of proportions filters noise by identifying statistically significant co-occurrence patterns, generating positive samples (assets frequently co-occurring) and negative samples (assets rarely co-occurring). Three contrastive loss functions are explored: individual sigmoid (treats each pair independently), aggregate sigmoid (uses average embeddings), and hybrid sigmoid-softmax (combines individual positives with aggregated negatives). The framework is trained on 611 U.S. stocks from 2000-2018 using sliding windows of length 22 with stride 5, embedding dimension 16, and evaluated on industry sector classification and portfolio optimization tasks.

## Key Results
- Industry sector classification: F1-score of 0.69 and accuracy of 69% using sigmoid-softmax loss
- Portfolio optimization: 19.1% reduction in average volatility compared to Pearson correlation baseline
- Individual sigmoid loss performs best for portfolio optimization, while sigmoid-softmax excels at sector classification
- Contrastive learning significantly outperforms Pearson correlation baseline on both tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise return similarity across rolling subwindows drives embedding quality
- Mechanism: The framework computes similarity between assets over many overlapping subwindows and counts co-occurrences among top-k similar assets. Assets with frequent co-occurrence across windows are sampled as positives, while those with rare co-occurrence are sampled as negatives. This repeated exposure to similarity patterns in different market regimes allows the model to learn stable, discriminative embeddings.
- Core assumption: Return similarity co-occurrence over time is a meaningful signal of asset relationship that generalizes across market regimes
- Evidence anchors:
  - [abstract] "Our approach leverages the similarity of asset returns over many subwindows to generate informative positive and negative samples"
  - [section 3.2.1] "Cáµ¢,â±¼ represents the number of sliding windows in which asset ð‘Žâ±¼ appears among the top-ð‘˜ most similar assets to ð‘Žáµ¢"
- Break condition: If the market regime shifts dramatically (e.g., structural breaks) and similarity patterns change fundamentally, the co-occurrence statistics may become outdated and embeddings may degrade

### Mechanism 2
- Claim: Statistical hypothesis testing filters noise in financial time series
- Mechanism: The framework uses a hypothesis test of proportions to distinguish co-occurrence patterns that are statistically significant from random chance. By computing p-values for each asset pair and using thresholds to select positives and negatives, the model focuses on meaningful relationships while avoiding noise from spurious correlations common in financial data.
- Core assumption: Financial data noise can be statistically separated from meaningful co-occurrence patterns using proportion tests
- Evidence anchors:
  - [section 3.2.2] "We consider the null hypothesis ð»â‚€: ð‘áµ¢,â±¼ â‰¤ ð‘â‚€, where ð‘áµ¢,â±¼ is the probability of asset ð‘Žâ±¼ co-occurring with asset ð‘Žáµ¢"
  - [section 3.2.2] "To generate positive samples for asset ð‘Žáµ¢, we only sample assets ð‘Žâ±¼ that have a p-value below a threshold ð›¼â‚š"
- Break condition: If the noise level in financial data increases significantly (e.g., during extreme volatility events), the p-value thresholds may become too conservative and fail to capture meaningful relationships

### Mechanism 3
- Claim: Different contrastive loss functions capture different aspects of asset relationships
- Mechanism: The framework explores three loss functions - individual sigmoid (treats each positive/negative independently), aggregate sigmoid (considers average embeddings), and hybrid sigmoid-softmax (combines individual positives with aggregated negatives). Each loss captures different relationship structures, with individual sigmoid performing best for portfolio optimization and sigmoid-softmax excelling at sector classification.
- Core assumption: The optimal loss function depends on the downstream task's requirements for relationship representation
- Evidence anchors:
  - [section 3.3] "We explore three loss functions to learn the asset embeddings by maximizing the similarity between an anchor asset and its positive samples while minimizing the similarity between the anchor asset and its negative samples"
  - [section 4.2] "Our contrastive learning approach using the sigmoid-softmax loss achieves the best performance across all metrics, with an F1-score of 0.69 and an accuracy of 69%"
  - [section 4.3] "Our contrastive learning approach with the individual sigmoid loss achieves the lowest average volatility of 19.1%"
- Break condition: If a new downstream task requires a different relationship structure not captured by any of the three loss functions, the framework may need additional loss formulations

## Foundational Learning

- Concept: Statistical hypothesis testing (proportion tests)
  - Why needed here: The framework uses hypothesis testing to distinguish meaningful asset co-occurrence from random chance in noisy financial data
  - Quick check question: If an asset appears as a top-5 similar asset to another asset in 30 out of 100 windows, and the expected random co-occurrence is 5/100, what is the null hypothesis being tested?

- Concept: Time series similarity measures
  - Why needed here: The framework requires computing similarity between asset return subsequences to build the co-occurrence matrix
  - Quick check question: What are the trade-offs between using Pearson correlation versus dynamic time warping for measuring similarity between financial time series?

- Concept: Contrastive learning principles
  - Why needed here: The framework learns embeddings by pulling together similar assets and pushing apart dissimilar ones through different loss functions
  - Quick check question: Why is having a diverse set of negative samples important in contrastive learning, and how does the framework ensure this diversity?

## Architecture Onboarding

- Component map: Data preprocessing -> Co-occurrence matrix computation -> Statistical sampling -> Contrastive loss computation -> Embedding update -> Regularization
- Critical path: Co-occurrence matrix computation -> Statistical sampling -> Contrastive loss computation -> Embedding update (these steps directly impact embedding quality)
- Design tradeoffs: Window size vs. computational cost (larger windows capture more context but increase computation), threshold sensitivity (more conservative thresholds reduce noise but may miss relationships), loss function selection (task-specific performance varies)
- Failure signatures: Poor downstream task performance (embeddings not capturing meaningful relationships), high variance in results (insufficient sampling or unstable training), slow convergence (inappropriate learning rate or window parameters)
- First 3 experiments:
  1. Verify co-occurrence matrix computation by checking that assets with known relationships (e.g., from sector labels) have higher co-occurrence counts
  2. Test statistical sampling by visualizing p-value distributions and ensuring the sampling regions capture expected assets
  3. Compare loss function performance on a small validation task before full training to identify the best loss for the target application

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed contrastive learning framework perform on non-financial time series data?
- Basis in paper: [explicit] The paper mentions that the proposed techniques are applicable to a wide range of non-financial time series and suggests exploring the effectiveness of the contrastive learning framework in these domains as a promising avenue for future research.
- Why unresolved: The paper focuses on financial time series data and does not provide experimental results or analysis on non-financial time series data.
- What evidence would resolve it: Conducting experiments on non-financial time series data, such as sensor data, medical data, or climate data, and comparing the performance of the proposed framework with baseline methods would provide evidence to resolve this question.

### Open Question 2
- Question: How does the choice of similarity measure between returns subsequences affect the quality of the learned asset embeddings?
- Basis in paper: [explicit] The paper mentions that the choice of the similarity function can be adapted to the specific characteristics of the financial time series data, such as Pearson correlation, dynamic time warping, or domain-specific measures. However, the paper only uses Pearson correlation as the similarity measure in the experiments.
- Why unresolved: The paper does not explore the impact of different similarity measures on the quality of the learned embeddings or provide a comparison of different similarity measures.
- What evidence would resolve it: Conducting experiments with different similarity measures, such as dynamic time warping or domain-specific measures, and comparing the performance of the learned embeddings on downstream tasks would provide evidence to resolve this question.

### Open Question 3
- Question: How does the proposed framework scale with the number of assets and the length of the time series?
- Basis in paper: [inferred] The paper mentions that the scalability of the pairwise similarity computation over multiple windows can become intractable as the number of assets and the length of the time series grow. However, it also suggests that recent research has shown that this type of time series similarity calculation can scale without issue due to hardware utilization and new algorithms.
- Why unresolved: The paper does not provide experimental results or analysis on the scalability of the proposed framework with respect to the number of assets and the length of the time series.
- What evidence would resolve it: Conducting experiments with varying numbers of assets and different lengths of time series, and measuring the computational time and memory requirements of the proposed framework, would provide evidence to resolve this question.

## Limitations

- Framework performance may degrade during structural breaks or regime shifts in financial markets when co-occurrence statistics become outdated
- Statistical sampling approach assumes noise follows predictable patterns that may not hold during extreme volatility events
- Optimal loss function is task-dependent, suggesting no universal best approach for all financial applications

## Confidence

- High confidence in the mechanism linking co-occurrence statistics to embedding quality (supported by clear formulas and implementation details)
- Medium confidence in the statistical sampling approach (theoretically sound but sensitive to parameter choices)
- Medium confidence in task-specific performance claims (strong results but limited comparison to more recent methods)

## Next Checks

1. Test embedding stability across different market regimes by computing performance metrics before and after major market events (2008 crisis, COVID-2020) to identify potential degradation patterns
2. Perform ablation studies on the statistical sampling parameters (pâ‚€ threshold, Î±â‚š significance level) to quantify their impact on embedding quality and downstream task performance
3. Evaluate the learned embeddings on additional financial tasks beyond sector classification and portfolio optimization, such as anomaly detection or risk forecasting, to assess generalizability