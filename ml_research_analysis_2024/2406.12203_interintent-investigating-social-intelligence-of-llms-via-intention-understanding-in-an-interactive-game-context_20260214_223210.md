---
ver: rpa2
title: 'InterIntent: Investigating Social Intelligence of LLMs via Intention Understanding
  in an Interactive Game Context'
arxiv_id: '2406.12203'
source_url: https://arxiv.org/abs/2406.12203
tags:
- intention
- intentions
- players
- game
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces INTER INTENT, a novel framework for evaluating\
  \ large language models' (LLMs) social intelligence through intention understanding\
  \ in the Avalon social deduction game. The framework maps four dimensions of social\
  \ intelligence\u2014situational awareness, self-regulation, self-awareness, and\
  \ theory of mind\u2014to specific game tasks: intention selection, intention following,\
  \ intention summarization, and intention guessing."
---

# InterIntent: Investigating Social Intelligence of LLMs via Intention Understanding in an Interactive Game Context

## Quick Facts
- arXiv ID: 2406.12203
- Source URL: https://arxiv.org/abs/2406.12203
- Reference count: 40
- GPT-3.5 achieves 87.5% accuracy in intention selection while GPT-4 reaches 88.8%, but both lag humans by 20% in intention guessing tasks

## Executive Summary
This paper introduces INTERINTENT, a novel framework for evaluating large language models' social intelligence through intention understanding in the Avalon social deduction game. The framework maps four dimensions of social intelligence—situational awareness, self-regulation, self-awareness, and theory of mind—to specific game tasks: intention selection, intention following, intention summarization, and intention guessing. Experiments with GPT-3.5 and GPT-4 reveal that while both models perform well in selecting intentions, they struggle significantly with intention guessing, trailing human performance by 20%. GPT-4 achieves human-level performance in intention summarization (83.8% F1), while GPT-3.5 reaches 69.5% F1. The study demonstrates a strong correlation between intention understanding and game performance, particularly for loyal players at a disadvantage, highlighting the importance of intention comprehension in social intelligence evaluation.

## Method Summary
The INTERINTENT framework evaluates LLMs' social intelligence by mapping four social intelligence dimensions to intention-related tasks in Avalon gameplay. Models engage in first-order reasoning about their own intentions, select from predefined intention categories, perform second-order reasoning about how others might interpret their statements, modify intentions accordingly, and generate speaking outputs. Human annotators identify impactful intentions from 2,440 GPT-3.5 and 350 GPT-4 game samples, focusing on intentions with strong positive or negative associations with game outcomes. The framework evaluates models across four tasks: intention selection accuracy, intention following success, intention summarization F1 scores, and intention guessing accuracy.

## Key Results
- GPT-3.5 achieves 87.5% accuracy in intention selection while GPT-4 reaches 88.8%
- GPT-4 achieves human-level performance in intention summarization (83.8% F1), GPT-3.5 reaches 69.5% F1
- Models trail human performance by 20% in intention guessing tasks
- Strong correlation exists between intention understanding and game performance, especially for loyal players at a disadvantage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intention understanding in dynamic game contexts can reveal social intelligence dimensions not captured by static benchmarks.
- Mechanism: The framework maps social intelligence components (situational awareness, self-regulation, self-awareness, theory of mind) to specific intention-related tasks in a multi-round, conversational game environment. This provides a dynamic testbed with clear goals and ongoing feedback loops.
- Core assumption: Intentions are a meaningful and measurable proxy for underlying social intelligence capabilities.
- Evidence anchors: [abstract] "intention understanding plays a critical role in games" and "game performance correlates with intention understanding"; [section] "comprehension of one's and others' intentions plays a critical role in games"

### Mechanism 2
- Claim: Separating intention tasks (selection, following, summarization, guessing) enables fine-grained diagnosis of social intelligence deficits.
- Mechanism: Each task isolates a different social intelligence component, allowing researchers to identify whether LLMs struggle with understanding their own intentions, following others', or inferring unstated intentions.
- Core assumption: Different social intelligence components map cleanly to distinct intention tasks without overlap.
- Evidence anchors: [abstract] "four dimensions of social intelligence... linked to specific game tasks: intention selection, intention following, intention summarization, and intention guessing"; [section] "Each dimension is linked to a specific game task"

### Mechanism 3
- Claim: Iterative intention refinement during gameplay improves both model reasoning and evaluation validity.
- Mechanism: The framework includes first-order reasoning, intention selection, second-order reasoning about others' interpretations, intention modification, and refinement contemplation, creating a feedback loop that mimics real social interaction.
- Core assumption: Second-order reasoning and intention modification are necessary for authentic social intelligence in dynamic contexts.
- Evidence anchors: [section] "players engage in second-order reasoning to evaluate how their statements might be interpreted by others, allowing them to reconsider and adjust their initial intentions"

## Foundational Learning

- Concept: Theory of Mind (ToM)
  - Why needed here: ToM is central to understanding others' intentions, which is a key evaluation dimension in the framework.
  - Quick check question: Can you explain the difference between first-order and second-order theory of mind reasoning?

- Concept: Intention detection and classification
  - Why needed here: The framework relies on categorizing player utterances into predefined intention types for evaluation.
  - Quick check question: How would you design a system to automatically classify player statements into the seven intention categories used in Avalon?

- Concept: Game theory and social deduction mechanics
  - Why needed here: Understanding the strategic elements of Avalon is crucial for interpreting model behavior and intention choices.
  - Quick check question: In Avalon, what strategic considerations differentiate how loyal players versus evil players approach team selection?

## Architecture Onboarding

- Component map: Game engine -> LLM interface -> Intention classifier -> Evaluation module -> Annotation interface
- Critical path:
  1. Initialize game state and player roles
  2. Generate first-order reasoning prompt for each player
  3. Collect intention selections from players
  4. Generate second-order reasoning prompt
  5. Allow intention modification
  6. Generate speaking output based on refined intentions
  7. Execute game action (vote, quest, etc.)
  8. Repeat until game conclusion
  9. Extract contexts for intention summarization and guessing evaluation
  10. Compute evaluation metrics

- Design tradeoffs:
  - Fixed vs. dynamic intention sets: Fixed sets enable consistent evaluation but may miss emergent intentions
  - Single vs. multiple response sampling: Multiple samples improve robustness but increase cost
  - Context inclusion in ToM tasks: More context may help but also introduces noise and complexity

- Failure signatures:
  - Low intention selection accuracy across all models suggests fundamental misunderstanding of game context
  - High intention selection but low intention following indicates gap between understanding and execution
  - Strong correlation between intention scores and game performance validates the framework's effectiveness

- First 3 experiments:
  1. Run baseline games with random intention selection to establish minimum performance thresholds
  2. Compare GPT-3.5 vs. GPT-4 performance across all four evaluation dimensions
  3. Test human vs. model performance on intention guessing tasks to establish upper bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the intention selection and following correlate with long-term game outcomes across multiple games?
- Basis in paper: [explicit] The paper states that intention understanding correlates with game performance, particularly for loyal players at a disadvantage.
- Why unresolved: The correlation analysis is limited to individual games or rounds rather than tracking performance across multiple games to establish a more robust relationship.
- What evidence would resolve it: Longitudinal studies tracking intention selection and following across multiple games with larger sample sizes, measuring win rates and quest success rates over time.

### Open Question 2
- Question: What is the impact of intention modification (self-correction) on game performance and social intelligence?
- Basis in paper: [inferred] The framework mentions intention modification as a potential component but does not evaluate its effectiveness or impact on outcomes.
- Why unresolved: The study focuses on static intention selection and following without examining how dynamic intention modification during gameplay affects social intelligence and game results.
- What evidence would resolve it: Experimental comparison between models that can modify intentions versus those that cannot, measuring differences in game performance and social intelligence metrics.

### Open Question 3
- Question: How do different instruction tuning methods affect LLM performance in intention understanding tasks?
- Basis in paper: [explicit] The paper mentions using instruction tuning to improve LLM performance in following intentions but does not explore different tuning methods or their comparative effectiveness.
- Why unresolved: The study uses a single approach to instruction tuning without comparing alternative methods or determining optimal training strategies for intention-guided gameplay.
- What evidence would resolve it: Systematic comparison of various instruction tuning techniques on the same models, measuring performance in intention selection, following, summarization, and guessing tasks.

## Limitations
- Limited model diversity with only GPT-3.5 and GPT-4 tested, raising questions about generalizability across different LLMs
- Annotation methodology concerns due to lack of inter-annotator agreement metrics and specific criteria for identifying "impactful intentions"
- Intention categorization rigidity using seven predefined categories may not capture the full complexity of social reasoning in dynamic game contexts

## Confidence
- High Confidence: The correlation between intention understanding and game performance, particularly for loyal players at a disadvantage
- Medium Confidence: The framework's ability to map social intelligence dimensions to specific game tasks
- Low Confidence: The claim that the framework can "effectively evaluate the social intelligence of LLMs" as a general proposition

## Next Checks
1. Cross-game validation: Apply the INTERINTENT framework to a different social deduction game (e.g., Among Us or Secret Hitler) to test whether the four-dimensional mapping holds across game mechanics and whether performance patterns replicate.

2. Model diversity testing: Evaluate additional LLM architectures (Claude, LLaMA, PaLM) and smaller models to determine whether the observed performance patterns are specific to OpenAI's models or represent broader limitations in current language model social intelligence.

3. Annotation reliability assessment: Conduct inter-annotator agreement studies with multiple human raters classifying intentions from the same game contexts, and perform sensitivity analysis by varying the threshold for "impactful intentions" to assess robustness of evaluation metrics.