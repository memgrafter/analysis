---
ver: rpa2
title: 7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based Reinforcement
  Learning Enhancement
arxiv_id: '2412.06845'
source_url: https://arxiv.org/abs/2412.06845
tags:
- arxiv
- data
- training
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Moxin 7B, a fully open-source large language
  model developed in accordance with the Model Openness Framework to promote transparency,
  reproducibility, and innovation. The authors pre-train the base model on a carefully
  curated dataset mix and enhance it with supervised fine-tuning, direct preference
  optimization, and reinforcement learning via Group Relative Policy Optimization
  to improve reasoning capabilities.
---

# 7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement

## Quick Facts
- arXiv ID: 2412.06845
- Source URL: https://arxiv.org/abs/2412.06845
- Reference count: 40
- This paper introduces Moxin 7B, a fully open-source large language model developed in accordance with the Model Openness Framework to promote transparency, reproducibility, and innovation.

## Executive Summary
This paper presents Moxin 7B, a fully open-source large language model developed following the Model Openness Framework to promote transparency and reproducibility in AI research. The authors pre-train the base model on 2 trillion tokens using a carefully curated dataset mix, then enhance it through supervised fine-tuning, direct preference optimization, and reinforcement learning via Group Relative Policy Optimization (GRPO). They also develop a vision-language model using the base as a backbone. Experimental results show the Moxin models achieve strong performance across zero-shot, few-shot, chain-of-thought, and vision-language evaluations, often matching or outperforming leading models of similar size, demonstrating the effectiveness of their open and transparent training approach.

## Method Summary
The Moxin 7B development follows a three-phase pre-training approach spanning 2 trillion tokens, starting with base pre-training on web data (SlimPajama and DCLM-BASELINE), followed by extended context pre-training with context length increased from 2K to 4K tokens, and finally capability enhancement with instruction and reasoning data. Post-training involves supervised fine-tuning (SFT) on Tülu 3 datasets, direct preference optimization (DPO), and reinforcement learning with Group Relative Policy Optimization (GRPO) following DeepSeek R1 methodology. The vision-language model combines the language backbone with DINOv2 and SigLIP visual encoders using the Prismatic VLMs framework. The entire pipeline uses Colossal-AI for distributed training and DeepScaleR for RL fine-tuning.

## Key Results
- Moxin 7B achieves competitive performance on standard benchmarks including HellaSwag, Winogrande, PIQA, ARC, and MMLU
- The Moxin Reasoning model demonstrates superior performance in chain-of-thought evaluations on MATH-500, AMC, Minerva Math, and OlympiadBench
- The Moxin VLM shows strong performance on DocVQA with DINOv2 achieving 83.5 F1 score compared to SigLIP's 75.3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-source training data and code enable full reproducibility and independent innovation
- Mechanism: Releasing pre-training code, configurations, datasets, and checkpoints allows researchers to replicate and extend the model development process
- Core assumption: Access to complete training materials is necessary for meaningful scientific progress in LLM development
- Evidence anchors:
  - [abstract] "We release the pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints"
  - [section] "Our open-source LLM has released pre-training code and configurations, training and fine-tuning data, and intermediate and final checkpoints"
  - [corpus] Weak - corpus focuses on related papers rather than data/code transparency
- Break condition: If training data or code quality is insufficient, reproducibility claims fail despite openness

### Mechanism 2
- Claim: Reinforcement learning with Group Relative Policy Optimization (GRPO) improves reasoning capabilities in small LLMs
- Mechanism: GRPO provides efficient policy optimization by comparing group performance relative to a baseline, enabling effective reasoning skill development without massive computational resources
- Core assumption: Small models can benefit from RL-based reasoning enhancement when properly configured
- Evidence anchors:
  - [abstract] "use Group Relative Policy Outcome (GRPO), an efficient and effective reinforcement learning algorithm following DeepSeek R1, to finetune our model"
  - [section] "we further adopt the RL techniques in DeepSeek R1, i.e., GRPO to finetune our model with RL"
  - [corpus] Weak - related papers don't provide direct evidence about GRPO effectiveness
- Break condition: If reward signal is poorly designed or sparse, RL training may not converge or could lead to reward hacking

### Mechanism 3
- Claim: Multi-stage pre-training with increasing context length and capability-specific data improves model performance
- Mechanism: Progressive training from 2K to 4K context length, followed by capability enhancement data, allows the model to learn fundamental patterns before tackling complex reasoning tasks
- Core assumption: Sequential skill development through staged training is more effective than monolithic approaches
- Evidence anchors:
  - [section] "The pre-training of Moxin 7B spans over 2 trillion tokens and is executed in three distinct phases: Base Pre-training... Extended Context Pre-training... Capability Enhancement"
  - [abstract] "After pre-training the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data"
  - [corpus] Missing - no corpus evidence about staged pre-training approaches
- Break condition: If capability data is insufficient or poorly curated, the enhancement phase may not provide meaningful improvements

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF) and its variants
  - Why needed here: Understanding how GRPO differs from standard RLHF is crucial for grasping the model's post-training approach
  - Quick check question: What distinguishes Group Relative Policy Optimization from standard Proximal Policy Optimization in terms of reward computation?

- Concept: Mixture of Experts (MoE) and sparse model architectures
  - Why needed here: The paper references Mixtral as a model that uses MoE, which is relevant context for understanding architectural choices
  - Quick check question: How does a MoE architecture with 7B total parameters but only 12B active parameters during inference differ from a dense 7B model?

- Concept: Vision-language model architecture and feature fusion
  - Why needed here: The VLM development section describes combining DINOv2 and SigLIP features with the language model
  - Quick check question: What are the complementary strengths of DINOv2 (self-supervised) versus SigLIP (contrastive) visual representations?

## Architecture Onboarding

- Component map: Pre-training pipeline (data curation → model training → checkpointing) → Post-training pipeline (SFT → DPO → GRPO) → VLM pipeline (visual backbone → projector → LLM)
- Critical path: Pre-training → Base model → SFT → DPO → GRPO → Reasoning model, with parallel VLM development using the base model as backbone
- Design tradeoffs: 36-layer architecture vs computational efficiency, FP16 precision vs numerical stability, sliding window attention vs full attention
- Failure signatures: Poor zero-shot performance indicates pre-training issues, weak reasoning performance suggests GRPO configuration problems, VLM performance issues may stem from visual backbone or fusion
- First 3 experiments:
  1. Reproduce base model pre-training with provided code and compare validation loss curves to expected patterns
  2. Run SFT fine-tuning on a small subset of Tülu 3 data to verify training pipeline functionality
  3. Test GRPO implementation on a simplified reasoning task to validate reward signal processing

Assumption: The provided code and configurations are functional and compatible with current dependencies. Assumption: Training data quality is sufficient for the claimed performance improvements. Assumption: Hardware requirements are met for the specified training procedures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Moxin 7B compare to proprietary models of similar size when evaluated on reasoning tasks, and what specific architectural or training innovations contribute to this performance?
- Basis in paper: [explicit] The paper states that Moxin Reasoning model "achieves superior performance in CoT evaluations" and "demonstrates the effectiveness of RL for small LLMs such as 7B models"
- Why unresolved: While the paper shows competitive performance against other open-source models, direct comparisons with proprietary models of similar size on reasoning tasks are not provided, making it difficult to assess the true impact of their innovations
- What evidence would resolve it: Comprehensive benchmarking results comparing Moxin Reasoning model performance against proprietary models like GPT-4o mini or Claude 3 Haiku on the same reasoning tasks

### Open Question 2
- Question: What is the optimal balance between general web data and curated capability-specific data in the pre-training mixture for achieving strong performance across diverse tasks?
- Basis in paper: [explicit] The paper discusses incorporating "instruction-based or high-quality data during the pre-training stage to enhance these abilities" and mentions using "OpenMathInstruct [114] and NuminaMath [115] for mathematical reasoning" but does not provide ablation studies
- Why unresolved: The paper mentions experimenting with capability enhancement data but does not provide systematic analysis of how different proportions of curated data affect overall model performance
- What evidence would resolve it: Ablation studies showing performance across different ratios of general web data to curated capability-specific data, with detailed analysis of trade-offs

### Open Question 3
- Question: How does the long-context handling capability (32K tokens) impact the model's performance on tasks requiring extensive context compared to models with shorter context windows?
- Basis in paper: [explicit] The paper describes implementing "Grouped-Query Attention (GQA)" and "Sliding Window Attention (SWA)" to support 32K context length but does not provide empirical comparisons
- Why unresolved: While the architecture supports long-context processing, the paper does not demonstrate whether this capability translates to measurable performance improvements on relevant tasks
- What evidence would resolve it: Comparative evaluations of Moxin models against baseline models with shorter context windows on tasks specifically designed to require long-context understanding (e.g., document QA, multi-document summarization)

### Open Question 4
- Question: What are the specific contributions of each post-training phase (SFT, DPO, GRPO) to the final performance of the Moxin Reasoning model, and how do these contributions vary across different task categories?
- Basis in paper: [explicit] The paper describes a multi-stage post-training pipeline but does not provide ablation studies isolating the impact of each phase
- Why unresolved: The paper presents a complete post-training pipeline but does not quantify how much each component (SFT, DPO, GRPO) contributes to final performance or whether certain components are more critical for specific task types
- What evidence would resolve it: Detailed ablation studies showing performance with different combinations of post-training phases, broken down by task category (reasoning, language understanding, coding, etc.)

## Limitations

- The evaluation methodology relies heavily on standard benchmarks that may not fully capture practical utility of open-source models in real-world applications
- The vision-language model evaluation is limited to a single downstream task (DocVQA), which may not be representative of general VLM capabilities
- GRPO implementation details are referenced from DeepSeek R1 without full specification, making independent verification challenging

## Confidence

**High Confidence**: The pre-training methodology and three-phase approach are well-documented and follow established practices. The use of standard datasets (SlimPajama, Tülu 3) and frameworks (Colossal-AI, DeepScaleR) provides strong confidence in the technical implementation.

**Medium Confidence**: The reasoning enhancement claims through GRPO are supported by benchmark improvements, but the exact implementation details and hyperparameter tuning remain partially unspecified. The performance comparisons to other models assume fair benchmarking conditions.

**Low Confidence**: The vision-language model evaluation lacks breadth, with only one downstream task reported. The architectural claims about DINOv2 and SigLIP complementarity are stated but not empirically validated within the paper.

## Next Checks

1. **Independent GRPO Implementation**: Reimplement the Group Relative Policy Optimization algorithm from the DeepSeek R1 paper specifications and verify that it produces the claimed reasoning improvements on a standardized math benchmark. This would validate whether the GRPO methodology itself is responsible for the performance gains rather than specific implementation details.

2. **Cross-Validation of Vision-Language Performance**: Evaluate the Moxin VLM on multiple vision-language benchmarks (including image captioning, visual question answering, and cross-modal retrieval) beyond DocVQA to establish the general capability claims. This would determine if the model's VLM performance is robust across different task types.

3. **Reproducibility Test of Base Model**: Using only the released pre-training code, configurations, and publicly available datasets, attempt to reproduce the base model's training loss curves and validation performance. This would directly test the "fully open-source" claim and identify any missing implementation details or dataset requirements.