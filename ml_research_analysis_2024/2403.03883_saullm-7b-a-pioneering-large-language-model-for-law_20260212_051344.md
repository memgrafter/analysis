---
ver: rpa2
title: 'SaulLM-7B: A pioneering Large Language Model for Law'
arxiv_id: '2403.03883'
source_url: https://arxiv.org/abs/2403.03883
tags:
- legal
- arxiv
- language
- preprint
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SaulLM-7B, the first large language model
  specifically designed for the legal domain. It extends the Mistral 7B architecture
  with continued pretraining on a 30B token English legal corpus, followed by instruction
  fine-tuning using both generic and legal datasets.
---

# SaulLM-7B: A pioneering Large Language Model for Law

## Quick Facts
- arXiv ID: 2403.03883
- Source URL: https://arxiv.org/abs/2403.03883
- Reference count: 15
- Primary result: State-of-the-art 0.61 balanced accuracy on LegalBench-Instruct, outperforming other open-source 7B models by 4-6 points

## Executive Summary
This paper introduces SaulLM-7B, the first large language model specifically designed for the legal domain. Built on the Mistral 7B architecture, it achieves state-of-the-art performance through continued pretraining on a 30B token English legal corpus followed by instruction fine-tuning using both generic and legal datasets. The model demonstrates significant improvements on LegalBench-Instruct (11% relative improvement over best open-source instruct models) and Legal-MMLU tasks. The authors also provide a refined version of LegalBench for better evaluation of legal LLMs and release the model under MIT License.

## Method Summary
SaulLM-7B extends the Mistral 7B architecture through a two-stage training approach. First, continued pretraining on a 30B token legal corpus (combining sources like FreeLaw, EDGAR, EuroParl, and court transcripts) adapts the model's representations to legal syntax and terminology. Second, instruction fine-tuning leverages both generic datasets (SlimOrca, Meta Math QA) and legal instruction datasets to enhance the model's ability to follow legal-specific instructions. The training uses DeepSpeed with Flash attention and incorporates replay data from general datasets to prevent catastrophic forgetting.

## Key Results
- Achieves 0.61 balanced accuracy on LegalBench-Instruct, outperforming Mistral-Instruct and Llama2 by 4-6 absolute points
- Consistently outperforms baseline models on Legal-MMLU tasks with absolute improvements of 3-9 points
- Shows lower perplexity across all legal document categories (laws, contracts, court cases, regulations, agreements) with reduced variance
- Demonstrates ability to identify relevant legal information in long documents without additional training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued pretraining on a large, domain-specific corpus improves legal task performance by adapting token representations to legal syntax and terminology
- Mechanism: The continued pretraining performs a "domain shift" of the embedding space from general-purpose to legal-specific distributions
- Core assumption: Legal language has sufficiently distinct token statistics and context patterns from general language
- Evidence anchors:
  - [abstract] "Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens."
  - [section 6.3] "SaulLM-7B, consistently outperforms Mistral-7B across all categories, exhibiting lower average perplexity scores with reduced variance."

### Mechanism 2
- Claim: Instruction fine-tuning on legal datasets enhances the model's ability to follow legal-specific instructions and perform legal reasoning tasks
- Mechanism: Fine-tuning on both generic and legal instruction datasets teaches the model to parse and respond to legal prompts with appropriate legal terminology
- Core assumption: Legal tasks require not just knowledge of legal terms but also the ability to interpret specific instruction formats
- Evidence anchors:
  - [abstract] "Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks."
  - [section 6.1] "SaulLM-7B-Instruct establishes a new state-of-the-art on the LegalBench-Instruct benchmark, with an average score of 0.61."

### Mechanism 3
- Claim: The combination of continued pretraining and legal instruction fine-tuning creates a synergistic effect where the model has both domain knowledge and instruction-following capabilities
- Mechanism: Continued pretraining provides the foundation of legal knowledge, while instruction fine-tuning teaches application in response to specific prompts
- Core assumption: Legal domain knowledge and instruction-following capabilities are complementary skills that reinforce each other
- Evidence anchors:
  - [section 6.1] "SaulLM-7B-Instruct largely outperforms generic Instruct models on tasks that most require legal-specific knowledge."

## Foundational Learning

- Concept: Domain adaptation through continued pretraining
  - Why needed here: Legal language has unique terminology, syntax, and document structures that differ significantly from general language
  - Quick check question: What is the primary difference between standard pretraining and continued pretraining in the context of domain adaptation?

- Concept: Instruction fine-tuning for task-specific capabilities
  - Why needed here: Legal professionals need models that can not only understand legal text but also respond appropriately to specific legal queries
  - Quick check question: How does instruction fine-tuning differ from standard fine-tuning in terms of the types of datasets used?

- Concept: Benchmark evaluation for specialized domains
  - Why needed here: General benchmarks may not adequately capture the nuances of legal reasoning
  - Quick check question: Why might standard accuracy metrics be insufficient for evaluating legal language models?

## Architecture Onboarding

- Component map: Mistral 7B architecture -> Continued pretraining on 30B token legal corpus -> Instruction fine-tuning on legal and generic datasets -> Evaluation on LegalBench-Instruct and Legal-MMLU -> Model release
- Critical path: Collect and clean legal corpus (30B tokens) → Continued pretraining on legal corpus → Instruction fine-tuning on legal and generic instructions → Evaluation on LegalBench-Instruct and Legal-MMLU → Model release
- Design tradeoffs: Larger legal corpus improves performance but increases training cost; synthetic legal instructions are cheaper to generate but may lack real-world diversity; MIT License enables commercial use but requires attribution
- Failure signatures: Poor performance on legal tasks indicates insufficient pretraining data quality or quantity; inability to follow instructions suggests instruction fine-tuning issues; high perplexity on legal documents indicates inadequate domain adaptation
- First 3 experiments:
  1. Evaluate perplexity of SaulLM-7B vs. Mistral-7B on legal document subsets to verify domain adaptation effectiveness
  2. Test instruction-following capabilities on a small set of legal queries to validate the instruction fine-tuning approach
  3. Compare performance on LegalBench-Instruct tasks with and without legal instruction fine-tuning to measure its impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can legal-specific DPO (Direct Preference Optimization) be designed to improve performance on legal tasks?
- Basis in paper: The paper mentions that "DPO-aligned models tend to underperform their instruction-tuned counterparts" and suggests "an interesting research direction would be to explore how legal-specific DPO can help."
- Why unresolved: While the paper identifies the potential of DPO for legal tasks, it does not explore this avenue due to scope limitations.

### Open Question 2
- Question: What is the optimal balance between legal and general instruction data for instruction tuning?
- Basis in paper: The paper shows that adding legal instructions to generic instruction fine-tuning boosts performance, but the optimal ratio is not explored.
- Why unresolved: The paper only tests two extremes (generic only vs. generic + legal) without exploring intermediate ratios.

### Open Question 3
- Question: How does the performance of SaulLM-7B compare to larger legal LLMs (e.g., 13B or 70B parameter models)?
- Basis in paper: The paper only compares SaulLM-7B to other 7B models, but doesn't explore how it would perform against larger models.
- Why unresolved: The computational resources required for larger models may be prohibitive, but the performance comparison remains untested.

## Limitations
- The evaluation relies heavily on the newly introduced LegalBench-Instruct benchmark, which may not fully capture the diversity of real-world legal tasks
- Limited evaluation on practical legal workflows that would demonstrate real-world utility
- Performance comparisons only with other 7B models, not with larger legal LLMs

## Confidence
- **High confidence**: The claim that continued pretraining on a 30B token legal corpus improves perplexity on legal documents is well-supported by the perplexity analysis
- **Medium confidence**: The assertion that SaulLM-7B establishes state-of-the-art performance on LegalBench-Instruct is based on comparisons with other 7B models, but the benchmark itself may have limitations
- **Medium confidence**: The claim of 4-6 absolute point improvements over baseline models is supported by the reported results, though more extensive comparisons would strengthen this

## Next Checks
1. Test SaulLM-7B on practical legal workflows such as contract analysis, legal research assistance, or document summarization with human evaluation
2. Evaluate the model's performance on legal tasks from jurisdictions not represented in the training corpus to assess generalization
3. Conduct stress tests with intentionally challenging legal prompts that require nuanced reasoning and identification of edge cases