---
ver: rpa2
title: 'Casablanca: Data and Models for Multidialectal Arabic Speech Recognition'
arxiv_id: '2410.04527'
source_url: https://arxiv.org/abs/2410.04527
tags:
- speech
- dialects
- arabic
- dialect
- casablanca
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Casablanca, a large-scale community-driven dataset
  for Arabic dialect speech recognition covering eight dialects (Algerian, Egyptian,
  Emirati, Jordanian, Mauritanian, Moroccan, Palestinian, and Yemeni) with 48 hours
  of manually transcribed data. The dataset includes annotations for transcription,
  speaker gender, dialect, and code-switching.
---

# Casablanca: Data and Models for Multidialectal Arabic Speech Recognition

## Quick Facts
- **arXiv ID**: 2410.04527
- **Source URL**: https://arxiv.org/abs/2410.04527
- **Reference count**: 26
- **Primary result**: Casablanca dataset presents 48 hours of Arabic dialect speech across 8 varieties; baseline experiments show WER 63-101% and CER 28-58% for state-of-the-art multilingual ASR models, indicating poor generalization to unseen Arabic dialects.

## Executive Summary
This paper introduces Casablanca, a large-scale community-driven dataset for Arabic dialect speech recognition covering eight dialects with 48 hours of manually transcribed data. The dataset includes annotations for transcription, speaker gender, dialect, and code-switching, addressing the lack of resources for low-resource and zero-resource Arabic dialects. Baseline experiments reveal that state-of-the-art multilingual models achieve high error rates on these dialects, indicating poor generalization to unseen Arabic varieties. The authors fine-tune Whisper models on Modern Standard Arabic and specific dialects, achieving better but still suboptimal performance. Code-switching evaluation reveals limitations in handling mixed Arabic-Latin text, highlighting the need for more robust modeling approaches.

## Method Summary
The Casablanca dataset was created through a community-driven effort, collecting 48 hours of speech data across eight Arabic dialects (Algerian, Egyptian, Emirati, Jordanian, Mauritanian, Moroccan, Palestinian, and Yemeni). The dataset includes manual transcriptions, speaker gender annotations, dialect labels, and code-switching annotations. Baseline experiments evaluate zero-shot performance of multilingual ASR models (Whisper, SeamlessM4T, MMS) on all dialects, followed by fine-tuning Whisper models on Modern Standard Arabic (MSA), Egyptian, and Moroccan data. The authors also fine-tune Whisper-v3 on combined Casablanca training data for the Algerian dialect and evaluate code-switching segments with and without transliteration. Text preprocessing includes resampling audio to 16kHz and normalizing text by retaining specific symbols, removing diacritics/hamzas/maddas, and converting numerals.

## Key Results
- State-of-the-art multilingual ASR models achieve high WER (63-101%) and CER (28-58%) on Casablanca dialects
- Fine-tuning Whisper on MSA or specific dialects improves performance but still shows significant gaps compared to in-domain data
- Code-switching evaluation reveals limitations in handling mixed Arabic-Latin text
- Low-resource dialects (Emirati, Yemeni, Mauritanian) show particularly poor performance due to limited training data

## Why This Works (Mechanism)
The Casablanca dataset provides diverse, manually transcribed speech data across eight Arabic dialects, enabling evaluation of Arabic ASR systems on low-resource and zero-resource varieties. By including code-switching annotations and gender information, the dataset captures the complexity of real-world Arabic speech. The fine-tuning experiments demonstrate that domain adaptation improves performance, though significant gaps remain, highlighting the need for more robust modeling approaches and larger datasets.

## Foundational Learning
- **Multilingual ASR models**: Pre-trained models that can handle multiple languages, needed for zero-shot evaluation across Arabic dialects. Quick check: Verify model architecture and supported languages.
- **Dialect identification**: Automatic detection of Arabic dialect from speech, needed for dataset annotation and evaluation. Quick check: Assess dialect classification accuracy on Casablanca data.
- **Code-switching**: Mixing of Arabic and Latin scripts in speech, needed for realistic evaluation scenarios. Quick check: Analyze frequency and patterns of code-switching in the dataset.
- **Fine-tuning**: Adapting pre-trained models to specific dialects or domains, needed for improving ASR performance. Quick check: Compare fine-tuned model performance to zero-shot baselines.
- **Text preprocessing**: Normalizing text by removing diacritics, hamzas, and madda, needed for consistent evaluation. Quick check: Verify preprocessing pipeline implementation.
- **WER/CER metrics**: Standard evaluation metrics for ASR, needed for quantifying performance. Quick check: Confirm correct computation of WER and CER on test data.

## Architecture Onboarding

**Component Map:**
Data Collection -> Annotation -> Preprocessing -> Model Evaluation -> Fine-tuning

**Critical Path:**
Data Collection (48 hours across 8 dialects) → Manual Transcription → Text Preprocessing (resampling, normalization) → Zero-shot Evaluation (Whisper, SeamlessM4T, MMS) → Fine-tuning (MSA, Egyptian, Moroccan) → Code-switching Analysis

**Design Tradeoffs:**
- Community-driven collection enables diversity but may introduce variability in recording quality
- Manual transcription ensures accuracy but limits dataset size
- Text preprocessing improves consistency but may lose dialect-specific information
- Zero-shot evaluation tests generalization but doesn't leverage domain adaptation

**Failure Signatures:**
- High WER/CER on low-resource dialects indicates insufficient training data
- Poor code-switching performance suggests limitations in handling mixed scripts
- Gender imbalance may bias model performance on gender-sensitive tasks

**3 First Experiments:**
1. Evaluate baseline multilingual models on each dialect separately to identify performance patterns
2. Fine-tune Whisper on MSA data and evaluate on all dialects to assess domain adaptation benefits
3. Analyze code-switching segments with and without transliteration to understand script handling challenges

## Open Questions the Paper Calls Out
- **Open Question 1**: How does the performance of Whisper-Casablanca compare to models fine-tuned on dialect-specific data when evaluated on other Arabic dialects not included in Casablanca? The paper only evaluates Whisper-Casablanca on the Algerian dialect, leaving its generalization capabilities to other dialects unexplored.
- **Open Question 2**: What is the impact of gender imbalance in the dataset on the performance of ASR models, particularly for gender-sensitive tasks? The paper acknowledges male dominance across dialects but doesn't empirically assess how this affects ASR performance.
- **Open Question 3**: How does the preprocessing pipeline affect the performance of ASR models on code-switched data, and can alternative preprocessing methods improve results? The paper applies a specific preprocessing pipeline but doesn't explore alternative approaches or their impact on code-switched data performance.

## Limitations
- Small dataset size (48 hours total) limits model training and evaluation
- High error rates on low-resource dialects indicate insufficient data for these varieties
- Code-switching evaluation reveals limitations in handling mixed Arabic-Latin text
- Gender imbalance in the dataset may bias model performance on gender-sensitive tasks

## Confidence
- **High confidence**: Dataset creation methodology and basic evaluation framework are clearly specified; core claim about poor multilingual ASR performance on Arabic dialects is well-supported
- **Medium confidence**: Comparative performance analysis between fine-tuning strategies is reasonable but could benefit from additional ablation studies; code-switching evaluation methodology is sound but transliteration approach may not fully address underlying challenges
- **Low confidence**: Generalization claims for low-resource dialects are limited by small data availability; paper doesn't extensively explore transfer learning or data augmentation strategies

## Next Checks
1. Replicate baseline experiments using exact test/train/dev splits and preprocessing pipeline to verify reported WER/CER values across all eight dialects
2. Conduct ablation studies to determine contribution of different fine-tuning strategies (MSA, dialect-specific, multilingual) to performance improvements, particularly for low-resource dialects
3. Evaluate alternative transliteration and normalization strategies for code-switching segments to assess whether high error rates are primarily due to mixed-script nature or other factors