---
ver: rpa2
title: KV-weights are all you need for skipless transformers
arxiv_id: '2404.12362'
source_url: https://arxiv.org/abs/2404.12362
tags:
- transformer
- figure
- attention
- arxiv
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to reduce the number of weights in
  skipless transformer models, specifically extending the existing approach to multi-query
  and grouped-query attention architectures. The key idea is to mathematically eliminate
  the query and post-attention projection linear layers by merging them with preceding
  layers, resulting in a reduction of 2d^2 weights per transformer block.
---

# KV-weights are all you need for skipless transformers

## Quick Facts
- arXiv ID: 2404.12362
- Source URL: https://arxiv.org/abs/2404.12362
- Authors: Nils Graef
- Reference count: 25
- Removes 2d² weights per transformer block, achieving 15% weight savings in Mistral-7B

## Executive Summary
This paper presents a method to reduce weights in skipless transformer models by mathematically eliminating query and post-attention projection linear layers through merging with preceding layers. The approach extends the existing weight reduction technique from multi-head attention to multi-query and grouped-query attention architectures used in popular LLMs. For Mistral-7B, this achieves 15% weight reduction and a 1.17x speedup in inference for batch size 1 systems limited by memory bandwidth.

## Method Summary
The method merges the query projection (Q_i) with the previous layer's output (O_{i-1}) and the post-attention projection (P_i) with the FFN input (M_i), effectively eliminating two linear layers per transformer block. The merged matrices M*_i = P_i * M_i and O*_i-1 * Q_i replace the original transformations while maintaining mathematical equivalence. This requires that Q_i, K_i, and V_i matrices are invertible (non-singular), which is "extremely rare" for random square matrices. The approach reduces total weights by 2d² per block and is applicable to MHA, MQA, and GQA architectures.

## Key Results
- Removes 2d² weights per transformer block, totaling 15% weight reduction in Mistral-7B
- Achieves 1.17x speedup in inference for batch size 1 systems limited by memory bandwidth
- Extends weight reduction from MHA to MQA and GQA architectures used in popular LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merging Q and P linear layers with preceding layers eliminates 2d² weights per transformer block while maintaining mathematical equivalence
- Mechanism: By combining the query projection (Q_i) with the previous layer's output (O_{i-1}) and the post-attention projection (P_i) with the FFN input (M_i), the redundant weight matrices are eliminated. The merged matrices M*_i = P_i * M_i and O*_i-1 * Q_i effectively replace two separate linear transformations with single equivalent ones.
- Core assumption: The Q_i and V_i matrices must be invertible (non-singular) for the mathematical equivalence to hold
- Evidence anchors:
  - [abstract] "The key idea is to mathematically eliminate the query and post-attention projection linear layers by merging them with preceding layers, resulting in a reduction of 2d² weights per transformer block."
  - [section] "This requires that Qi,K i,V i are invertible (i.e. nonsingular). It is extremely rare that a square matrix with random values is not invertible [15]"
  - [corpus] Weak evidence - no direct corpus papers discussing this specific merging mechanism, though related to tensor product attention approaches
- Break condition: The mechanism breaks if Q_i or V_i become singular (determinant = 0), which the paper notes is "extremely rare" for random square matrices

### Mechanism 2
- Claim: The weight reduction enables 1.17x speedup in inference for batch size 1 systems limited by memory bandwidth
- Mechanism: By removing 15% of total weights in Mistral-7B, the memory bandwidth bottleneck during autoregressive next-token generation is alleviated, allowing faster data transfer between memory and computation units
- Core assumption: The system is memory-bandwidth limited rather than compute-limited, which is typical for batch size 1 inference
- Evidence anchors:
  - [abstract] "leading to a 1.17x speedup in inference for batch size 1 systems limited by memory bandwidth"
  - [section] "For a batch 1 system that is limited by memory bandwidth, these 15% weight savings can speed up inference by 1.17x during the autoregressive next-token-generation phase"
  - [corpus] Weak evidence - no direct corpus papers measuring this specific speedup, though related to MLKV and cache reduction approaches
- Break condition: The speedup disappears if the system becomes compute-bound or if batch sizes increase significantly, changing the bottleneck from memory to computation

### Mechanism 3
- Claim: The approach extends from MHA to MQA and GQA architectures used in popular LLMs
- Mechanism: The mathematical framework that works for standard multi-head attention can be generalized to multi-query and grouped-query attention by adjusting the dimensional relationships (e = d for MHA, e = d/n_heads for MQA, e = d·n_kv_heads/n_heads for GQA)
- Core assumption: The dimensional relationships between embedding space and attention heads can be preserved while eliminating the same linear layers
- Evidence anchors:
  - [abstract] "The paper presents a method to reduce the number of weights in skipless transformer models, specifically extending the existing approach to multi-query and grouped-query attention architectures"
  - [section] "He and Hofmann [1] detailed a skipless transformer without the V and P (post-attention projection) linear layers, which reduces the total number of weights. However, this scheme is only applicable to MHA (multi-head attention), but not for MQA (multi-query attention) and GQA (grouped-query attention)"
  - [corpus] Moderate evidence - related papers on MLKV and grouped query attention support the relevance of these architectures
- Break condition: The mechanism fails if the dimensional constraints cannot be maintained or if the specific attention patterns of MQA/GQA require the eliminated matrices for correctness

## Foundational Learning

- Concept: Matrix invertibility and singular matrices
  - Why needed here: The approach requires Q_i, K_i, and V_i matrices to be invertible to enable the weight merging operations
  - Quick check question: What mathematical condition must be satisfied for a square matrix to be invertible, and why is this condition "extremely rare" to violate for random matrices?

- Concept: Linear layer fusion and weight sharing
  - Why needed here: Understanding how two consecutive linear transformations can be mathematically equivalent to a single merged transformation is fundamental to the weight reduction approach
  - Quick check question: Given two linear layers with weights W1 and W2, what is the equivalent single-layer weight matrix, and under what conditions is this equivalent to the original two-layer structure?

- Concept: Attention mechanism dimensional relationships
  - Why needed here: Different attention schemes (MHA, MQA, GQA) have different dimensional relationships between embedding dimension, number of heads, and key/value dimensions that affect how the weight reduction applies
  - Quick check question: How do the dimensional relationships e = d, e = d/n_heads, and e = d·n_kv_heads/n_heads differ between MHA, MQA, and GQA respectively, and why does this matter for the proposed approach?

## Architecture Onboarding

- Component map:
  - Original architecture: Q-linear, K-linear, V-linear, attention mechanism, P-linear, FFN with M and O matrices
  - Modified architecture: Merged matrices M*_i = P_i * M_i and O*_i-1 * Q_i (or K_i, V_i), eliminating Q and P layers
  - Key dimensions: d (embedding), e (attention dimension), f (FFN hidden dimension), n_heads, n_kv_heads

- Critical path:
  1. Input embedding through previous block output O_{i-1}
  2. Query projection (merged with O_{i-1})
  3. Key and value projections
  4. Attention computation
  5. Post-attention projection (merged with FFN input M_i)
  6. FFN computation
  7. Output projection O_i

- Design tradeoffs:
  - Weight reduction vs. architectural complexity: The merged approach reduces parameters but requires careful dimensional management
  - Memory bandwidth vs. compute: Weight reduction benefits memory-bound systems but may increase compute requirements for matrix inversions
  - Generalization vs. specialization: The approach works across MHA, MQA, and GQA but requires different dimensional treatments

- Failure signatures:
  - Singular matrix errors during training indicating Q_i, K_i, or V_i matrices have become non-invertible
  - Numerical instability in merged matrix computations due to accumulated floating-point errors
  - Performance degradation if the system is not actually memory-bandwidth limited

- First 3 experiments:
  1. Implement the weight merging for a single transformer block with random initialization and verify numerical equivalence through forward pass comparison
  2. Test the approach on a small language modeling task with MHA, then extend to MQA and GQA configurations, measuring both parameter count and inference latency
  3. Create a controlled experiment varying batch size from 1 to larger values to identify the exact point where memory bandwidth limitation shifts to compute limitation, validating the speedup claims

## Open Questions the Paper Calls Out
- Future work should investigate whether removing P and Q (or K or V) is also beneficial for transformers with normalization and skip connections

## Limitations
- The approach is only validated on skipless transformers without normalization layers or skip connections
- The 1.17x speedup claim lacks direct empirical validation and depends on unstated assumptions about hardware characteristics
- Numerical stability of merged matrices over long sequences and training epochs hasn't been demonstrated

## Confidence
- High Confidence: The mathematical framework for merging linear layers is sound and the weight reduction calculation (2d² per block) is correct
- Medium Confidence: The claim that singular matrices are "extremely rare" for random initialization is theoretically reasonable but lacks empirical validation across training epochs
- Low Confidence: The specific 1.17x speedup claim lacks direct empirical validation and depends heavily on unstated assumptions about hardware characteristics and workload patterns

## Next Checks
- Monitor Q_i, K_i, and V_i matrices throughout full training of a small transformer to empirically verify that singular matrices don't occur, and measure any numerical drift in the merged matrices O*_{i-1} and M*_i over many layers and training steps
- Implement the approach on multiple hardware platforms (GPU, CPU, different memory bandwidths) and measure actual inference latency across batch sizes 1-64 to validate whether the 1.17x speedup holds or if the memory-bandwidth bottleneck assumption breaks down at different scales
- Apply the weight reduction technique to transformers with rotary embeddings, ALiBi, or other attention variants, and test on models beyond Mistral-7B (different sizes, different attention head configurations) to establish the true scope of applicability and identify any architectural constraints