---
ver: rpa2
title: The sampling complexity of learning invertible residual neural networks
arxiv_id: '2411.05453'
source_url: https://arxiv.org/abs/2411.05453
tags:
- neural
- function
- networks
- network
- residual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the sample complexity of learning invertible
  residual neural networks (i-ResNets) and invertible convolutional residual neural
  networks (i-Conv-ResNets) in the uniform norm. It shows that determining these networks
  from point samples suffers from the curse of dimensionality, meaning the number
  of samples needed grows exponentially with input dimension.
---

# The sampling complexity of learning invertible residual neural networks

## Quick Facts
- arXiv ID: 2411.05453
- Source URL: https://arxiv.org/abs/2411.05453
- Authors: Yuanyuan Li; Philipp Grohs; Philipp Petersen
- Reference count: 40
- This paper shows that learning invertible residual neural networks (i-ResNets) and invertible convolutional residual neural networks (i-Conv-ResNets) suffers from the curse of dimensionality, requiring exponentially many samples in the input dimension for uniform norm approximation.

## Executive Summary
This paper studies the fundamental sample complexity of learning invertible residual neural networks (i-ResNets) and invertible convolutional residual neural networks (i-Conv-ResNets) from point samples. The authors prove that these architectures suffer from the curse of dimensionality, meaning the number of samples required to approximate functions within these networks grows exponentially with input dimension. The key insight is that residual connections and invertibility do not overcome the complexity barriers encountered with simpler feedforward architectures when measured in the uniform norm.

## Method Summary
The authors construct lower bounds on the optimal error by designing a specially crafted hat function that can be incorporated into residual blocks while maintaining the required Lipschitz constraint for invertibility. They create a large set of functions by placing translates of this hat function on a grid, ensuring that a limited number of samples cannot distinguish between these functions. This indistinguishability forces any learning algorithm to make large approximation errors. The proof involves careful counting of grid points and analysis of the distance between functions differing by these hat functions.

## Key Results
- The minimal number of samples required to approximate i-ResNets within accuracy ε scales as ε^(-d), where d is the input dimension
- The same exponential dependence on dimension holds for i-Conv-ResNets, showing that convolutional layers don't alleviate the curse
- These lower bounds apply to all practical learning algorithms including stochastic gradient descent
- The results demonstrate that residual network architecture and invertibility do not overcome complexity barriers encountered with simpler feedforward architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual connections and invertibility do not overcome the curse of dimensionality in uniform norm approximation.
- Mechanism: The paper shows that any practical algorithm (including SGD) needs exponentially many samples in the input dimension to approximate i-ResNets or i-Conv-ResNets within a given uniform accuracy. This is because the residual architecture and invertibility constraints still allow function sets that are too rich, requiring a lower bound on the number of samples that scales as m^(-1/d).
- Core assumption: The residual network architecture and invertibility do not reduce the approximation complexity compared to simpler feedforward architectures when measured in the uniform norm.
- Evidence anchors:
  - [abstract] "Our main result shows that the residual neural network architecture and invertibility do not help overcome the complexity barriers encountered with simpler feedforward architectures."
  - [section 4.1] "Theorem 21... errM C m (U, L p ( [0, 1]d, Rd)) ≥ Cm − 1 d − 1 p" where the lower bound depends exponentially on input dimension d.
  - [corpus] Weak evidence: no direct corpus neighbor papers provide supporting mechanisms.
- Break condition: If the hypothesis set is restricted more severely than just invertible residual networks (e.g., bounded weights and Lipschitz constants as in the theorem), or if a different norm (e.g., L1) is used, the curse might be mitigated.

### Mechanism 2
- Claim: The hat function construction enables proving lower bounds by creating indistinguishable function pairs.
- Mechanism: The paper constructs a hat function that is compactly supported and Lipschitz with constant less than 1, allowing it to be inserted into i-ResNet/i-Conv-ResNet blocks without violating invertibility. By placing many such functions on a grid, the paper creates a large set of functions that cannot be distinguished by a limited number of samples, forcing any algorithm to make large errors.
- Core assumption: The hat function can be represented within the i-ResNet/i-Conv-ResNet architecture (feedforward or convolutional) while maintaining the required Lipschitz constraint.
- Evidence anchors:
  - [section 3] "We put the hat function Θv z,M,c in a residual neural network block, forming a local disturbance on the first variable" and "Let v = ± 1, c = 1 6dM and Θ v z,M,c be the hat function for the i-ResNet block."
  - [section 4.1] "Lemma 20... establishes the distance between functions differing by addition/subtraction of the hat function" which is used to prove the lower bound.
  - [corpus] Weak evidence: no direct corpus neighbor papers provide supporting mechanisms.
- Break condition: If the hat function cannot be implemented in the architecture (e.g., due to incompatible activation functions or layer constraints), the lower bound construction fails.

### Mechanism 3
- Claim: The support structure of the hat function and its translates creates a covering argument for the lower bound.
- Mechanism: The hat function has a bounded support. By placing many such functions on a fine grid (with spacing ~1/M), the paper ensures that a limited number of samples cannot cover all supports. Functions differing by these hat functions produce identical outputs on the sampled points but differ significantly elsewhere, forcing large approximation errors.
- Core assumption: The number of grid points (and thus candidate functions) grows exponentially with dimension, while the number of samples grows only polynomially, creating an indistinguishability gap.
- Evidence anchors:
  - [section 4.1] "Let Γ = f (x∗ ) + ( 1 M − 1 2C2 )·⃗1 + G ⊂ f (x∗ ) + [ − 1 2C2 , 1 2C2 ]d" and "# ˜Γ ≥ #Γ − n ≥ m" showing the grid construction.
  - [section 4.1] "The number of elements in Γ is 3 m ≤ #Γ = (⌈ (3m) 1 d ⌉)d ≤ 2d ·3m" demonstrating exponential growth with dimension.
  - [corpus] Weak evidence: no direct corpus neighbor papers provide supporting mechanisms.
- Break condition: If the support of the hat function could be made smaller without violating the Lipschitz constraint, or if a different sampling strategy could cover all supports with fewer samples, the lower bound might not hold.

## Foundational Learning

- Concept: Lipschitz continuity and its role in invertibility
  - Why needed here: The paper requires residual functions to have Lipschitz constant less than 1 to ensure invertibility of the i-ResNet. This constraint is crucial for the hat function construction and the overall proof.
  - Quick check question: Why must Lip(Gℓ) < 1 for each residual function Gℓ in an i-ResNet to guarantee invertibility?

- Concept: Curse of dimensionality in uniform norm approximation
  - Why needed here: The paper's main result is that i-ResNets and i-Conv-ResNets still suffer from the curse of dimensionality when approximating functions in the uniform norm. Understanding this concept is key to interpreting the lower bounds.
  - Quick check question: What does it mean for a learning problem to "suffer from the curse of dimensionality" in terms of sample complexity?

- Concept: Translation equivariance in convolutional neural networks
  - Why needed here: The paper distinguishes between i-ResNets (using feedforward layers) and i-Conv-ResNets (using convolutional layers). Understanding translation equivariance is crucial for why the hat function needs different variants for each architecture.
  - Quick check question: Why can't the hat function for i-ResNet blocks be directly represented as a convolutional layer?

## Architecture Onboarding

- Component map: Input layer (d-dimensional) -> Hidden layers (2 feedforward or 3 convolutional) -> Residual blocks (identity + hat function) -> Output layer (d-dimensional)

- Critical path: The hat function must be correctly implemented in the residual block, maintaining the Lipschitz constraint (<1) to preserve invertibility. The hat function's support and scaling must be carefully chosen based on the number of samples m and input dimension d.

- Design tradeoffs: Using smaller hat functions (smaller c, larger M) allows more functions to be placed on the grid, strengthening the lower bound proof but potentially making the network harder to train. Using larger hat functions makes the network easier to train but weakens the lower bound.

- Failure signatures: If the hat function's Lipschitz constant exceeds 1, the network becomes non-invertible. If the hat function's support is too large relative to the grid spacing, the covering argument in the proof fails. If the hat function cannot be represented within the chosen architecture (feedforward vs. convolutional), the lower bound construction is invalid.

- First 3 experiments:
  1. Implement a simple i-ResNet with a single residual block containing the hat function. Verify invertibility by checking that the network can be inverted numerically.
  2. Generate a grid of functions as in the proof (varying the hat function's position and sign) and test whether a simple learning algorithm (e.g., least squares) can distinguish them with a limited number of samples.
  3. Measure the approximation error of the i-ResNet on a test function as the number of samples varies, comparing against the theoretical lower bound m^(-1/d).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the curse of dimensionality in learning i-ResNets and i-Conv-ResNets be avoided by modifying the residual network architecture or adding additional constraints?
- Basis in paper: [explicit] The paper demonstrates that even with invertible residual neural networks and convolutional residual neural networks, the curse of dimensionality persists. The authors state, "Our main result shows that the residual neural network architecture and invertibility do not help overcome the complexity barriers encountered with simpler feedforward architectures."
- Why unresolved: While the paper proves lower bounds for these specific architectures, it does not explore whether alternative modifications or constraints could circumvent the curse of dimensionality.
- What evidence would resolve it: Constructing new residual network architectures or constraints that provably achieve better sample complexity bounds, particularly those that avoid the exponential scaling with input dimension.

### Open Question 2
- Question: What is the impact of using alternative activation functions instead of ReLU on the sample complexity of learning invertible residual neural networks?
- Basis in paper: [inferred] The paper focuses on ReLU activation functions for the hat function and residual network blocks. However, the authors do not investigate how different activation functions might affect the curse of dimensionality.
- Why unresolved: The analysis is specific to ReLU, and the paper does not provide insights into the behavior of other activation functions.
- What evidence would resolve it: Analyzing the sample complexity of learning i-ResNets with various activation functions (e.g., Leaky ReLU, Tanh, Sigmoid) and comparing the results to the ReLU case.

### Open Question 3
- Question: Can the optimal error bounds for learning i-ResNets and i-Conv-ResNets be improved for specific target functions or function classes?
- Basis in paper: [explicit] The paper establishes lower bounds on the optimal error for general i-ResNets and i-Conv-ResNets. However, the authors do not explore whether these bounds can be tightened for specific target functions or function classes.
- Why unresolved: The analysis focuses on worst-case scenarios, and the paper does not investigate the sample complexity for structured or well-behaved target functions.
- What evidence would resolve it: Deriving upper bounds on the optimal error for specific target functions or function classes (e.g., smooth functions, sparse functions) and comparing them to the general lower bounds.

## Limitations

- The lower bound construction relies on a specific hat function architecture that may not capture all practical scenarios
- The proof assumes idealized conditions including perfect invertibility and doesn't account for optimization challenges in training these networks
- The results focus on the uniform norm, which is particularly challenging; different norms might yield different complexity behaviors

## Confidence

- High confidence: The core theoretical framework and lower bound proofs are mathematically rigorous and internally consistent
- Medium confidence: The extension from i-ResNets to i-Conv-ResNets follows logically but involves additional technical details that require careful verification
- Medium confidence: The claim that these results apply to all practical learning algorithms, including SGD, follows from the minimax nature of the lower bounds but requires empirical validation

## Next Checks

1. Implement the hat function construction and verify its properties (support, Lipschitz constant, bounds on function values) numerically for both i-ResNet and i-Conv-ResNet architectures
2. Test whether the lower bound construction holds under practical constraints like finite precision and approximate invertibility
3. Evaluate the approximation error of i-ResNets/i-Conv-ResNets on benchmark functions with varying sample sizes to empirically verify the m^(-1/d) scaling predicted by the theory