---
ver: rpa2
title: Learning Metadata-Agnostic Representations for Text-to-SQL In-Context Example
  Selection
arxiv_id: '2410.14049'
source_url: https://arxiv.org/abs/2410.14049
tags:
- query
- question
- select
- table
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of selecting optimal in-context
  demonstrations for large language models (LLMs) on Text-to-SQL tasks. The proposed
  MARLO method learns metadata-agnostic embeddings that align natural language questions
  with SQL queries based on query structure, rather than database metadata.
---

# Learning Metadata-Agnostic Representations for Text-to-SQL In-Context Example Selection

## Quick Facts
- **arXiv ID**: 2410.14049
- **Source URL**: https://arxiv.org/abs/2410.14049
- **Reference count**: 40
- **Primary result**: MARLO achieves 2.9 percentage points average improvement in execution accuracy over generic embedding models on Text-to-SQL tasks

## Executive Summary
This paper addresses the challenge of selecting optimal in-context demonstrations for large language models on Text-to-SQL tasks. The proposed MARLO method learns metadata-agnostic embeddings that align natural language questions with SQL queries based on query structure rather than database metadata. This enables retrieval of structurally and semantically relevant demonstrations across different domains. When used for demonstration selection, MARLO outperforms generic embedding models by an average of 2.9 percentage points in execution accuracy on the Spider benchmark.

## Method Summary
MARLO learns metadata-agnostic embeddings for natural language questions and SQL queries to improve in-context learning demonstration selection. The method uses a semi-asymmetric bi-encoder architecture with a shared pre-trained transformer backbone and separate dense layers for questions and queries. Training is performed on the Spider benchmark using a custom similarity metric (QED) that focuses on structural SQL keywords rather than table/column names. The QED score is used as weak supervision to fine-tune the encoder with cosine similarity loss. For inference, the trained encoder retrieves top-k demonstrations by cosine similarity between test exemplars and candidate question-query pairs.

## Key Results
- MARLO achieves 2.9 percentage points average improvement in execution accuracy over generic embedding models on Spider benchmark
- Outperforms the next best method by 0.8 percentage points while maintaining significantly lower inference latency
- Question embeddings prove more expressive than query embeddings, leading to greater unique demonstration diversity during retrieval

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuned embeddings align natural language questions and SQL queries based on query structure rather than database metadata.
- **Mechanism**: The MARLO model uses a novel similarity metric (QED) that focuses on structural SQL keywords rather than table/column names, enabling retrieval of demonstrations that share structural intent across domains.
- **Core assumption**: Structural similarity in SQL queries correlates with semantic similarity in user intent, regardless of specific domain entities.
- **Evidence anchors**:
  - [abstract]: "Our technique, dubbed MARLO - Metadata-Agnostic Representation Learning for Text-tO-SQL - uses query structure to model querying intent without over-indexing on underlying database metadata"
  - [section]: "MARLO focuses on query structure (rather than metadata specifics) to represent the intent of each question more accurately"
  - [corpus]: Weak - corpus doesn't provide specific validation of this mechanism
- **Break condition**: If structural similarity doesn't correlate with user intent (e.g., complex queries with similar structure but different meanings), or if QED metric fails to capture meaningful structural differences.

### Mechanism 2
- **Claim**: Semi-asymmetric bi-encoder architecture with shared backbone and separate dense layers enables better alignment than symmetric or fully asymmetric alternatives.
- **Mechanism**: Parameter sharing in the backbone benefits from pre-trained transformer transfer learning, while separate dense layers provide regularization during alignment, creating more expressive representations.
- **Core assumption**: The semi-asymmetric architecture balances transfer learning benefits with task-specific alignment requirements better than other architectures.
- **Evidence anchors**:
  - [section]: "we hypothesize that parameter sharing in the backbone benefits from the transfer learning abilities of pre-trained transformers and the separate embedding heads acts as a form of regularization during alignment"
  - [section]: "we find the semi-asymmetric architecture is more capable of learning expressive and aligned representations than the symmetric or asymmetric alternatives"
  - [corpus]: Weak - corpus doesn't provide specific validation of this architectural claim
- **Break condition**: If the semi-asymmetric architecture doesn't consistently outperform other architectures across different datasets or tasks.

### Mechanism 3
- **Claim**: Question embeddings are more expressive than query embeddings for demonstration selection, leading to better unique demonstration diversity.
- **Mechanism**: Natural language questions can be phrased in many ways, creating richer semantic representations, while SQL queries have more rigid syntax, making question embeddings more informative for retrieval.
- **Core assumption**: The diversity in natural language expression translates to better semantic coverage in embeddings compared to structured SQL representations.
- **Evidence anchors**:
  - [section]: "Table 2c shows that selecting demonstrations for a given test question based on its semantic similarity to candidate questions or queries results in comparable performance on EX†. Nevertheless, question embeddings are more expressive than their query counterparts as the former leads to a greater number of unique demonstrations selected during the evaluation (R)"
  - [section]: "Intuitively, a natural language question can be written in many more ways than its corresponding SQL query, which explains why their respective embeddings are aligned but not equally expressive"
  - [corpus]: Weak - corpus doesn't provide specific validation of this claim
- **Break condition**: If query embeddings consistently outperform question embeddings in certain domains or if the expressiveness advantage disappears with more complex queries.

## Foundational Learning

- **Concept**: Cosine similarity as a metric for embedding alignment
  - Why needed here: The loss function uses cosine similarity to measure alignment between question and query embeddings, requiring understanding of how this metric works in embedding spaces
  - Quick check question: If two embeddings have cosine similarity of 0.9, are they considered more or less similar than embeddings with cosine similarity of 0.7?

- **Concept**: Contrastive learning and metric learning
  - Why needed here: MARLO uses a custom similarity metric (QED) for weak supervision, which builds on principles from contrastive learning where similar items should be closer in embedding space
  - Quick check question: In contrastive learning, should similar items have embeddings that are closer together or farther apart in the embedding space?

- **Concept**: Siamese network architectures
  - Why needed here: The bi-encoder architecture is a type of Siamese network, requiring understanding of parameter sharing strategies and how they affect representation learning
  - Quick check question: What's the key difference between symmetric and asymmetric Siamese networks in terms of parameter sharing?

## Architecture Onboarding

- **Component map**: Input → Transformer → Pooling → Dense layer → Cosine similarity loss → Parameter update
- **Critical path**: Input → Transformer → Pooling → Dense layer → Cosine similarity loss → Parameter update
- **Design tradeoffs**:
  - Semi-asymmetric vs. symmetric/asymmetric architectures
  - Fixed vs. dynamic output dimensions
  - QED-based vs. binary labels for supervision
  - Question vs. query embeddings for retrieval
- **Failure signatures**:
  - Poor alignment: High cosine similarity between unrelated question-query pairs
  - Overfitting: Excellent performance on training data but poor generalization
  - Embedding collapse: All embeddings converging to similar values
  - Domain bias: Embeddings capturing database-specific metadata instead of structural intent
- **First 3 experiments**:
  1. Train a baseline symmetric bi-encoder and compare performance to MARLO on Spider-dev
  2. Test QED vs. random labels to validate the effectiveness of the similarity metric
  3. Compare question vs. query embeddings for demonstration selection on a small subset of Spider

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. However, based on the discussion and results, several important questions arise:
- How does MARLO perform on other complex structured output tasks beyond Text-to-SQL, such as code generation or text-to-image synthesis?
- What is the impact of different SQL dialects on MARLO's performance, and how can the method be adapted to handle dialect-specific syntax?
- How does the choice of the QED score threshold impact MARLO's performance, and what is the optimal threshold for different datasets or tasks?

## Limitations
- The method relies heavily on the QED metric's effectiveness in capturing meaningful structural similarity across domains
- The semi-asymmetric architecture's superiority over alternatives is asserted but not rigorously validated against all possible configurations
- The metadata-agnostic approach may have limited effectiveness for complex queries where database schema understanding is crucial for semantic interpretation

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| MARLO's execution accuracy improvements on Spider benchmark (2.9 percentage points average gain) | High |
| Superiority of semi-asymmetric architecture over alternatives | Medium |
| QED metric's ability to capture meaningful structural similarity | Medium |
| Question embeddings being more expressive than query embeddings for retrieval | Medium |

## Next Checks
1. Test MARLO's performance on out-of-domain SQL tasks (e.g., WikiSQL, academic datasets) to validate metadata-agnostic generalization claims
2. Conduct ablation studies comparing semi-asymmetric architecture against fully symmetric and asymmetric variants across multiple Text-to-SQL benchmarks
3. Evaluate QED metric's correlation with human judgment of structural similarity by conducting a small-scale annotation study on randomly sampled question-query pairs from different domains