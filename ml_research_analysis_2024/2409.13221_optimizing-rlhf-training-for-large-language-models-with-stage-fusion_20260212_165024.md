---
ver: rpa2
title: Optimizing RLHF Training for Large Language Models with Stage Fusion
arxiv_id: '2409.13221'
source_url: https://arxiv.org/abs/2409.13221
tags:
- training
- generation
- pipeline
- stage
- rlhf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RLHFuse is a training system for Reinforcement Learning from Human
  Feedback (RLFH) that addresses low GPU utilization caused by data skewness in generation
  and pipeline bubbles in training. The key innovation is stage fusion, which splits
  RLHF tasks into finer-grained subtasks and overlaps their execution.
---

# Optimizing RLHF Training for Large Language Models with Stage Fusion

## Quick Facts
- arXiv ID: 2409.13221
- Source URL: https://arxiv.org/abs/2409.13221
- Authors: Yinmin Zhong, Zili Zhang, Bingyang Wu, Shengyu Liu, Yukun Chen, Changyi Wan, Hanpeng Hu, Lei Xia, Ranchen Ming, Yibo Zhu, Xin Jin
- Reference count: 40
- RLHFuse achieves up to 3.7× higher throughput compared to state-of-the-art systems

## Executive Summary
RLHFuse is a training system for Reinforcement Learning from Human Feedback (RLHF) that addresses low GPU utilization caused by data skewness in generation and pipeline bubbles in training. The key innovation is stage fusion, which splits RLHF tasks into finer-grained subtasks and overlaps their execution. By migrating long-tailed samples to dedicated GPUs and using a fused pipeline schedule that co-locates Actor and Critic model training, RLHFuse eliminates bottlenecks in both generation and training stages.

## Method Summary
RLHFuse implements stage fusion through two complementary techniques: data-aware inter-stage fusion and model-aware intra-stage fusion. For generation-inference stages, it splits tasks into sample-level subtasks and migrates long-tailed samples to dedicated GPUs when remaining samples fall below threshold Rt, enabling concurrent execution. For training, it breaks tasks into micro-batches and uses a bidirectional pipeline schedule to co-locate Actor and Critic model training, filling pipeline bubbles. The system uses simulated annealing to generate near-optimal fused pipeline schedules that minimize both latency and peak activation memory.

## Key Results
- Up to 3.7× higher throughput compared to state-of-the-art RLHF training systems
- Effective elimination of bottlenecks in both generation (long-tail samples) and training (pipeline bubbles) stages
- Achieves theoretical lower bound most of the time for fused pipeline schedules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHFuse splits generation and inference tasks into sample-level subtasks, enabling inter-stage fusion to overlap execution and mitigate the long-tail bottleneck.
- Mechanism: When the number of remaining samples in generation falls below threshold Rt, long-tailed samples are migrated to dedicated GPUs. Freed resources launch inference tasks early, overlapping inference with generation of long-tailed samples.
- Core assumption: Computation of generation and inference is independent for different samples, allowing sample-level parallelism without breaking data dependencies.
- Evidence anchors:
  - [abstract] "splits them into sample-level subtasks, enabling efficient inter-stage fusion to overlap the execution of generation and inference stages, thus mitigating the original generation bottleneck dominated by long-tailed samples."
  - [section 4.1] "The dependency between the two stages is predicated on the sample level, implying that once a sample completes its generation stage, it can seamlessly advance to the inference stage."
  - [corpus] No direct corpus evidence found supporting sample-level independence; this appears to be an original assumption.
- Break condition: If generation and inference have interdependencies at the sample level (e.g., requiring global normalization across samples), the independence assumption fails.

### Mechanism 2
- Claim: RLHFuse breaks training tasks into micro-batches and fuses them across Actor and Critic models using a bidirectional pipeline schedule, filling pipeline bubbles.
- Mechanism: Actor and Critic training subtasks are co-located and scheduled in opposite pipeline directions. The fused schedule interleaves forward/backward passes to fill bubbles created by each model's pipeline dependencies.
- Core assumption: Actor and Critic models are independent during training, so their micro-batches can be interleaved without violating synchronous training semantics.
- Evidence anchors:
  - [abstract] "breaks them into subtasks of micro-batches and performs intra-stage fusion to concurrently execute these subtasks in the training stage with a fused pipeline schedule, effectively mitigating the pipeline bubbles."
  - [section 5.1] "the Actor and Critic models are trained independently. Inspired by the bi-directional pipeline schedule...we can break the two training tasks into subtasks of micro-batches and co-locate these subtasks to mutually fill the pipeline bubbles."
  - [corpus] No direct corpus evidence found supporting independence of Actor/Critic training; this appears to be an original insight.
- Break condition: If Actor and Critic training have shared weight updates or dependencies (e.g., joint optimization), the independence assumption fails.

### Mechanism 3
- Claim: RLHFuse uses simulated annealing to generate near-optimal fused pipeline schedules that minimize both latency and peak activation memory.
- Mechanism: Simulated annealing searches over valid pipeline schedules by swapping adjacent subtasks. It optimizes for minimal finish time while respecting data dependencies and memory constraints.
- Core assumption: The search space of valid pipeline schedules is large but tractable, and simulated annealing can find schedules close to the theoretical lower bound.
- Evidence anchors:
  - [section 5.2] "we use simulated annealing...to search for a better solution. The motivation for using simulated annealing is that we have a huge search space with many variables."
  - [section 7.3] "our approach outperforms the baseline methods and achieves the theoretical lower bound most of the time."
  - [corpus] No direct corpus evidence found for simulated annealing in RLHF; this appears to be an original approach.
- Break condition: If the schedule search space is too constrained or if the energy function poorly approximates actual runtime, simulated annealing may not find good solutions.

## Foundational Learning

- Concept: Pipeline parallelism and pipeline bubbles
  - Why needed here: Understanding how pipeline bubbles form (N-1 bubbles for N stages with 1F1B scheduling) is critical to grasping why intra-stage fusion helps.
  - Quick check question: In a 4-stage pipeline with 4 micro-batches using 1F1B scheduling, what percentage of time are GPUs idle?

- Concept: Data parallelism vs tensor parallelism vs pipeline parallelism
  - Why needed here: RLHFuse chooses different parallel strategies for different tasks; understanding these strategies is key to understanding the system's design choices.
  - Quick check question: If you have a model too large for a single GPU, which parallel strategy (DP, TP, or PP) would you use to scale to multiple GPUs while minimizing communication?

- Concept: Autoregressive generation and key-value cache
  - Why needed here: The generation stage's memory-bandwidth bound nature and key-value cache memory usage are crucial for understanding the migration mechanism.
  - Quick check question: Why does LLM generation require maintaining a key-value cache for each token position, and how does this affect memory usage?

## Architecture Onboarding

- Component map:
  - Generation instances: Handle Actor model autoregressive generation, split into sample-level subtasks
  - Inference instances: Handle Ref, Critic, and Reward model forward passes
  - Training instances: Handle Actor and Critic model training with fused pipeline schedule
  - Migration coordinator: Monitors generation progress and triggers sample migration
  - Schedule generator: Uses simulated annealing to create fused pipeline schedules

- Critical path:
  1. Actor generates samples (prefiling + decoding)
  2. Generated samples migrate to inference instances when long-tail threshold reached
  3. Inference instances process samples in parallel with remaining generation
  4. Training instances process Actor and Critic micro-batches with fused schedule
  5. Updated Actor weights redistributed for next iteration

- Design tradeoffs:
  - Migration threshold Rt: Early migration frees resources but may overload remaining generation instances; late migration misses overlap opportunity
  - Migration mechanism: Transfer key-value cache (fast but network-heavy) vs regenerate (slow but network-light)
  - Schedule search time: Simulated annealing can take minutes but produces near-optimal schedules that save seconds per iteration

- Failure signatures:
  - High migration overhead: Network bandwidth becomes bottleneck, migration time exceeds overlap gains
  - Memory OOM: Migration destination GPUs lack sufficient memory for key-value caches of long-tailed samples
  - Schedule invalidation: Model sizes or parallel configurations change mid-training, requiring new schedules

- First 3 experiments:
  1. Measure baseline iteration time with serial generation→inference→training on a small model (13B) with short max length (512 tokens)
  2. Enable inter-stage fusion with varying Rt values (10%, 20%, 30% of batch size) and measure overlap gains
  3. Enable intra-stage fusion on a medium model (33B) and compare training stage time against serial execution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RLHFuse perform under highly imbalanced workload distributions where long-tailed samples become even more extreme?
- Basis in paper: [inferred] The paper demonstrates effectiveness with long-tailed distributions but doesn't test extreme imbalances or show performance degradation points
- Why unresolved: The paper shows RLHFuse handles typical long-tailed distributions well, but doesn't establish boundaries for its effectiveness or test scenarios with extremely skewed workloads
- What evidence would resolve it: Experimental results showing RLHFuse performance with progressively more extreme workload imbalances, particularly cases where Rt threshold becomes insufficient

### Open Question 2
- Question: What is the impact of inter-stage fusion migration overhead on training efficiency when using less ideal network topologies than RDMA?
- Basis in paper: [explicit] The paper mentions "high-bandwidth RDMA connections" and negligible migration overhead, but doesn't explore performance with standard Ethernet or other network configurations
- Why unresolved: The paper assumes optimal network conditions for migration but real-world deployments may have less ideal networking, potentially affecting the viability of inter-stage fusion
- What evidence would resolve it: Comparative performance results of RLHFuse with different network configurations, including standard Ethernet, InfiniBand, and RDMA, showing migration overhead scaling

### Open Question 3
- Question: How does RLHFuse scale when training multimodal models that require three-way fusion between different model types (e.g., text, image, and audio)?
- Basis in paper: [inferred] The paper mentions multimodal scenarios briefly in context of intra-stage fusion but doesn't explore three-way model fusion or provide experimental results
- Why unresolved: The current intra-stage fusion algorithm is designed for two models, and the paper doesn't address how the algorithm would extend to three or more models with different characteristics
- What evidence would resolve it: Experimental results showing RLHFuse performance with three or more heterogeneous models, including fusion schedule generation and performance comparison with serial execution

## Limitations
- Mechanism 1's sample-level independence assumption requires empirical validation across different model architectures and prompt types
- Mechanism 2's Actor/Critic independence assumption may not hold for all RLHF variants, particularly those with joint optimization objectives
- Migration mechanism's benefits are contingent on network bandwidth availability and the relative costs of data transfer versus regeneration

## Confidence

| Mechanism | Confidence Level | Rationale |
|-----------|------------------|-----------|
| Inter-stage fusion (sample-level parallelism) | Medium | Sample-level independence assumption is reasonable but requires empirical validation |
| Intra-stage fusion (Actor/Critic co-training) | Medium | Independence assumption is plausible but may not hold for all RLHF variants |
| Simulated annealing for schedule optimization | High | Well-established algorithm with sound optimization formulation |

## Next Checks

1. Profile actual generation traces to measure the distribution of sample completion times and verify that sample-level independence holds across different model architectures and prompt types

2. Implement a controlled experiment comparing migration overhead (network transfer time) versus regeneration time for varying sample lengths and key-value cache sizes to determine optimal migration strategy

3. Benchmark the simulated annealing schedule generator against simpler heuristics (e.g., greedy schedule, round-robin) to quantify the actual performance gains from the more complex optimization approach