---
ver: rpa2
title: Structured Extraction of Real World Medical Knowledge using LLMs for Summarization
  and Search
arxiv_id: '2412.15256'
source_url: https://arxiv.org/abs/2412.15256
tags:
- disease
- knowledge
- patient
- data
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a method for extracting and structuring medical
  knowledge from electronic health records using large language models to create patient-specific
  knowledge graphs. The approach addresses the challenge of rare disease identification
  where standard diagnostic codes are insufficient or nonexistent.
---

# Structured Extraction of Real World Medical Knowledge using LLMs for Summarization and Search

## Quick Facts
- arXiv ID: 2412.15256
- Source URL: https://arxiv.org/abs/2412.15256
- Reference count: 40
- Key outcome: LLM-based extraction achieves HPO mapping F1 score of 0.678 for rare disease patient identification

## Executive Summary
This work develops a method for extracting and structuring medical knowledge from electronic health records using large language models to create patient-specific knowledge graphs. The approach addresses the challenge of rare disease identification where standard diagnostic codes are insufficient or nonexistent. By extracting phenotypic features from clinical notes and mapping them to established ontologies like HPO, the system enables natural language-based patient search rather than rigid ontological hierarchies. Testing on a 33.6M patient dataset demonstrates the method's effectiveness in identifying Dravet syndrome patients through ICD-10 codes and phenotypic patterns, while also discovering potential Beta-propeller protein-associated neurodegeneration (BPAN) cases without ground truth labels.

## Method Summary
The method employs large language models for entity extraction from unstructured clinical notes, mapping extracted clinical features to standardized ontologies including HPO, MeSH, SNOMED-CT, and RxNORM. The system uses dynamic few-shot prompting techniques to improve biomedical named entity recognition, retrieving contextually similar examples from training data. Patient knowledge graphs are constructed by representing patients as nodes connected to extracted clinical features, enabling natural language-based patient search. The approach was tested on EHR data from 33.6M patients, with fine-tuned LLM models achieving HPO mapping F1 scores of 0.678 through validation against known rare disease cases.

## Key Results
- HPO mapping F1 score of 0.678 achieved through fine-tuned LLM models
- Successful identification of Dravet syndrome patients using ICD-10 codes and phenotypic patterns
- Discovery of potential BPAN cases without ground truth labels through phenotypic feature extraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can extract phenotypic features from unstructured clinical notes and map them to standardized ontologies like HPO with reasonable accuracy
- Mechanism: LLMs leverage their contextual understanding capabilities to identify clinical features in free text and match them to ontology terms through semantic similarity
- Core assumption: Clinical notes contain sufficient phenotypic information that can be reliably extracted and mapped to ontology terms
- Evidence anchors: "Using confirmed Dravet syndrome ICD10 codes as ground truth, we employ LLM-based entity extraction to characterize patients in grounded ontologies"
- Break condition: If clinical notes lack sufficient phenotypic detail or use non-standard terminology that LLMs cannot parse

### Mechanism 2
- Claim: Patient knowledge graphs enable natural language-based patient search rather than rigid ontological hierarchies
- Mechanism: By representing patients as nodes with multiple connected clinical features, the system can match patients based on semantic similarity rather than exact code matches
- Core assumption: Phenotypic features extracted from clinical notes provide sufficient discriminative power for patient matching
- Evidence anchors: "knowledge graphs with generalized language capability allow for data to be extracted using natural language rather than being constrained by the exact terminology or hierarchy of existing ontologies"
- Break condition: If extracted phenotypic features are too sparse or noisy to enable meaningful patient matching

### Mechanism 3
- Claim: Dynamic few-shot prompting significantly improves LLM performance on biomedical named entity recognition tasks
- Mechanism: By retrieving contextually similar examples from the training set, LLMs can better understand the specific terminology and context of biomedical text
- Core assumption: Biomedical text has sufficient structure that similar examples can provide useful context for entity recognition
- Evidence anchors: "Dynamic few-shot retrieves the top 5 most similar abstracts via cosine similarity of the text embeddings"
- Break condition: If the retrieved examples are not sufficiently similar or the task requires highly specialized knowledge not captured in the examples

## Foundational Learning

- Concept: Knowledge Graph structure and terminology
  - Why needed here: Understanding how patients, clinical features, and relationships are represented as nodes and edges is fundamental to grasping the system architecture
  - Quick check question: In a patient knowledge graph, what would be the relationship between a patient node and a symptom node?

- Concept: Biomedical ontologies (HPO, SNOMED-CT, ICD-10)
  - Why needed here: The system relies on mapping extracted clinical features to standardized ontology terms for interoperability and semantic search
  - Quick check question: What is the primary purpose of the Human Phenotype Ontology in clinical practice?

- Concept: Large Language Model capabilities and limitations
  - Why needed here: Understanding both the strengths (contextual understanding) and weaknesses (hallucinations, sensitivity to prompt formatting) of LLMs is crucial for effective system design
  - Quick check question: What is a common failure mode of LLMs when extracting structured information from text?

## Architecture Onboarding

- Component map: Data Ingestion -> Text Processing -> Entity Extraction -> Ontology Mapping -> Knowledge Graph Construction -> Search Interface -> Validation Layer
- Critical path: Clinical note → LLM extraction → HPO mapping → Knowledge graph → Patient search
- Design tradeoffs:
  - LLM size vs. inference cost: Larger models provide better accuracy but are more expensive to run
  - Fine-tuning vs. few-shot: Fine-tuning provides better performance but reduces flexibility for new ontologies
  - Graph complexity vs. query performance: More detailed graphs enable better matching but slow down search
- Failure signatures:
  - Low recall in HPO extraction: Clinical notes lack sufficient phenotypic detail
  - High false positives in entity recognition: LLM overgeneralizes from few-shot examples
  - Slow search performance: Knowledge graph becomes too dense with connections
- First 3 experiments:
  1. Run entity extraction on a small sample of clinical notes and manually verify HPO mappings
  2. Test patient search with known ground truth cases to measure precision and recall
  3. Benchmark different LLM models (Qwen2.5 vs Llama3.1) on the same extraction task to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-based HPO extraction compare to human expert annotation in terms of precision, recall, and clinical utility?
- Basis in paper: The paper discusses LLM-based HPO extraction but doesn't provide direct comparison to human expert performance, instead comparing against other automated methods
- Why unresolved: The paper lacks a head-to-head comparison with human expert annotation, which would provide a gold standard benchmark for clinical utility and accuracy
- What evidence would resolve it: A study comparing LLM-extracted HPO terms against expert-annotated ground truth data from the same patient cohort, measuring both accuracy and clinical decision-making impact

### Open Question 2
- Question: What is the long-term generalization capability of fine-tuned LLM models for HPO extraction across evolving medical ontologies and new disease presentations?
- Basis in paper: The paper notes that narrow fine-tuning degrades generalization and that the HPO dataset only contains 184 unique phenotypes out of over 13,000
- Why unresolved: The paper raises concerns about model generalizability but doesn't provide empirical evidence of performance degradation over time or across diverse clinical scenarios
- What evidence would resolve it: Longitudinal studies tracking model performance across multiple years with new HPO terms, different medical specialties, and varying clinical documentation styles

### Open Question 3
- Question: How does the dynamic few-shot prompting approach scale computationally and what are its limitations with very large patient datasets?
- Basis in paper: The paper mentions dynamic few-shot prompting but doesn't discuss computational costs or scalability challenges with millions of patients
- Why unresolved: While the paper demonstrates effectiveness of dynamic few-shot prompting, it doesn't address practical implementation challenges for large-scale deployment
- What evidence would resolve it: Performance benchmarks showing computational time, memory usage, and accuracy trade-offs when scaling from hundreds to millions of patient records, including analysis of diminishing returns

## Limitations
- HPO mapping F1 score of 0.678 indicates significant room for improvement in entity extraction accuracy
- System performance heavily depends on quality and completeness of clinical notes, which may vary across healthcare systems
- BPAN case identification performed without ground truth validation, making it difficult to assess false positive rates

## Confidence
- High confidence: The general approach of using LLMs for entity extraction and ontology mapping is technically sound and builds on established methods
- Medium confidence: The specific performance metrics (F1=0.678) are plausible given the complexity of the task, but would benefit from independent validation
- Low confidence: The BPAN discovery claims require ground truth validation to establish true positive rates

## Next Checks
1. **Ground truth validation of BPAN cases**: Independently verify the identified BPAN cases through medical record review or genetic testing to establish true positive and false positive rates for novel disease discovery

2. **Cross-site performance evaluation**: Test the system on clinical notes from multiple healthcare systems with different documentation practices to assess generalizability and identify potential data quality issues

3. **Error analysis of HPO mappings**: Conduct detailed manual review of failed entity extractions and incorrect HPO mappings to identify systematic failure patterns and improve the extraction pipeline