---
ver: rpa2
title: Efficient Multi-Objective Neural Architecture Search via Pareto Dominance-based
  Novelty Search
arxiv_id: '2407.20656'
source_url: https://arxiv.org/abs/2407.20656
tags:
- uni00000013
- uni00000011
- search
- novelty
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Pareto Dominance-based Novelty Search (PDNS)
  for multi-objective neural architecture search (NAS), addressing the limitations
  of objective-driven approaches that can lead to premature convergence and bias in
  training-free metric-based search. PDNS introduces a novelty score calculated using
  multiple training-free performance and complexity metrics (synflow, jacov, snip,
  and FLOPs/parameters) to guide a genetic algorithm, while maintaining an elitist
  archive of non-dominated architectures.
---

# Efficient Multi-Objective Neural Architecture Search via Pareto Dominance-based Novelty Search

## Quick Facts
- arXiv ID: 2407.20656
- Source URL: https://arxiv.org/abs/2407.20656
- Authors: An Vo; Ngoc Hoang Luong
- Reference count: 40
- Primary result: PDNS achieves comparable results to training-based NAS methods with 51√ó lower computational cost (2.02 vs 104.44 GPU hours on NAS-Bench-101)

## Executive Summary
This paper introduces Pareto Dominance-based Novelty Search (PDNS) for multi-objective neural architecture search (NAS), addressing the limitations of objective-driven approaches that can lead to premature convergence and bias in training-free metric-based search. PDNS uses a novelty score calculated from multiple training-free performance and complexity metrics (synflow, jacov, snip, and FLOPs/parameters) to guide a genetic algorithm while maintaining an elitist archive of non-dominated architectures. The method achieves significant improvements in search efficiency and approximation front quality compared to conventional MOENAS methods while maintaining low computational costs.

## Method Summary
PDNS is a multi-objective NAS method that uses novelty search guided by training-free metrics to prevent premature convergence and promote diversity in the search space. The method maintains an elitist archive of non-dominated architectures and computes novelty scores based on Euclidean distances in a descriptor space formed by multiple training-free metrics and complexity measures. A genetic algorithm uses these novelty scores to guide the search toward less-explored regions while maintaining high-performing, low-complexity architectures. The approach is evaluated on NAS-Bench-101, NAS-Bench-1Shot1, and NAS-Bench-201 benchmarks.

## Key Results
- PDNS significantly outperforms conventional MOENAS methods in IGD+ and Hypervolume metrics across all tested benchmarks
- Achieves comparable performance to training-based NAS methods while requiring 51√ó less computational time (2.02 vs 104.44 GPU hours on NAS-Bench-101)
- Maintains strong performance on CIFAR-100 and ImageNet16-120 transfer tasks while using much less computational resources
- Multi-training-free metric variant (MTF-PDNS) consistently outperforms single-metric variants across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Novelty search guided by training-free metrics prevents premature convergence on local optima by prioritizing exploration of diverse architectures
- Mechanism: The novelty score, calculated as the mean Euclidean distance in a descriptor space of synflow, jacov, snip, and complexity metrics, drives the GA to explore regions with fewer previously visited architectures
- Core assumption: The descriptor space (training-free metrics + complexity) is sufficiently informative to differentiate architectures meaningfully and correlate with actual performance
- Evidence anchors:
  - [abstract]: "promotes population diversity by utilizing a novelty score calculated based on multiple training-free performance and complexity metrics, thereby yielding a broader exploration of the search space"
  - [section 2.3]: "The novelty score ùúÇ (x) for an individual x can be calculated as follows: ùúÇ (x) = 1/ùëÅ Œ£·µ¢ ùõø(ùúô(x), ùúô(a·µ¢)) ... This aims to encourage the algorithm's exploration of less-visited parts of the descriptor space, rewarding individuals with unique descriptors"
  - [corpus]: Weak or missing - neighbors focus on differentiable or evolution-based methods, not novelty search
- Break condition: If the training-free metrics are too biased or poorly correlated with actual performance, the novelty score will guide the search toward irrelevant regions

### Mechanism 2
- Claim: Using an elitist archive of non-dominated architectures ensures the search maintains a set of high-quality trade-offs while computing novelty scores
- Mechanism: The elitist archive stores architectures that are not dominated in the multi-objective space (performance vs. complexity). The novelty score is computed relative to this archive, ensuring that novel but poor architectures are not prioritized
- Core assumption: Non-dominated architectures in the elitist archive are representative of the Pareto front and provide a meaningful reference for novelty calculation
- Evidence anchors:
  - [abstract]: "maintaining an elitist archive of non-dominated architectures" and "Our method only utilizes archives to calculate novelty scores without directly optimizing the metrics"
  - [section 3.1]: "An elitist archive A is a subset of architectures that only contains non-dominated architectures... It is updated dynamically after each generation and evaluation of new architectures"
  - [corpus]: Weak or missing - neighbors discuss multi-objective NAS but not elitist archives for novelty
- Break condition: If the archive becomes too large or dominated by similar architectures, novelty scores may lose discriminative power

### Mechanism 3
- Claim: Combining multiple training-free metrics (synflow, jacov, snip) in the novelty score calculation mitigates individual metric biases and provides a more robust exploration signal
- Mechanism: Each metric has different biases (e.g., synflow biases towards larger cells), but their combination in the descriptor space balances these out, leading to a more accurate novelty assessment
- Core assumption: The combination of metrics provides complementary information that, when normalized and combined, yields a meaningful novelty signal
- Evidence anchors:
  - [section 2.2]: "Many studies have been conducted recently to effectively employ training-free metrics for NAS... Using multiple training-free metrics is a suggested way to increase NAS performance, while mitigating the bias of each training-free metric"
  - [section 3.1]: "In our method, the elitist archive A serves as a memory of the search process, storing non-dominated architectures and serving as a component to compute the novelty score"
  - [corpus]: Weak or missing - neighbors do not discuss multi-metric novelty search
- Break condition: If metrics are highly correlated or redundant, combining them adds little value and increases computation

## Foundational Learning

- Concept: Multi-objective optimization and Pareto dominance
  - Why needed here: The method searches for architectures balancing performance and complexity, requiring Pareto dominance to identify non-dominated solutions
  - Quick check question: Given two architectures with (accuracy=90%, params=1M) and (accuracy=85%, params=0.5M), which one dominates the other?

- Concept: Training-free metrics for neural architecture performance estimation
  - Why needed here: The novelty score relies on synflow, jacov, and snip to estimate architecture quality without training, enabling rapid search
  - Quick check question: What is the main advantage of using training-free metrics like synflow or snip in NAS?

- Concept: Genetic algorithms and novelty search
  - Why needed here: The search is driven by a GA that uses novelty scores instead of direct fitness, requiring understanding of both GA operators and novelty search principles
  - Quick check question: In novelty search, what is the novelty score of an individual relative to a population?

## Architecture Onboarding

- Component map:
  Population initialization ‚Üí Training-free metric evaluation ‚Üí Elitist archive update ‚Üí Offspring generation (crossover/mutation) ‚Üí Training-free metric evaluation ‚Üí Novelty score calculation ‚Üí Selection ‚Üí Repeat

- Critical path:
  1. Initialize population with random architectures
  2. Evaluate each with synflow, jacov, snip, and FLOPs/params
  3. Update elitist archive with non-dominated architectures
  4. Generate offspring via crossover and mutation
  5. Evaluate offspring with same metrics
  6. Update elitist archive
  7. Compute novelty score for all individuals
  8. Select next generation based on novelty scores
  9. Repeat until termination

- Design tradeoffs:
  - Single vs. multiple training-free metrics: Multiple metrics reduce bias but increase computation slightly
  - Archive size: Larger archives provide better novelty discrimination but increase computation
  - Normalization: Essential for fair metric comparison; poor normalization skews novelty scores

- Failure signatures:
  - Search converges to similar architectures despite high novelty scores ‚Üí Metrics not discriminating enough
  - Search explores irrelevant regions ‚Üí Training-free metrics poorly correlated with actual performance
  - Archive becomes dominated by few architectures ‚Üí Diversity maintenance failing

- First 3 experiments:
  1. Run MTF-PDNS on NAS-Bench-101 with default settings; verify IGD+ and Hypervolume improve over MOENAS-synflow baseline
  2. Test with only one training-free metric (e.g., synflow) to observe impact on search quality vs. computation
  3. Vary population size (10, 20, 50) to find sweet spot between exploration and computation cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of different combinations of training-free metrics (beyond synflow, jacov, and snip) affect the performance and novelty search effectiveness of PDNS?
- Basis in paper: [inferred] The paper mentions that "Future works could further explore the potential of combining other training-free metrics" and that MTF-PDNS uses three specific training-free metrics, but doesn't explore alternatives
- Why unresolved: The paper only experiments with three specific training-free metrics and doesn't investigate how different combinations might impact performance, limiting understanding of the metric selection's importance
- What evidence would resolve it: Systematic experiments comparing PDNS performance using various combinations of different training-free metrics (e.g., adding or replacing metrics like Fisher-Rao or Eigenvalue-based measures) on the same benchmarks, measuring IGD+, Hypervolume, and search efficiency

### Open Question 2
- Question: What is the optimal balance between novelty and fitness in the novelty score calculation for different NAS search spaces?
- Basis in paper: [explicit] The paper states "However, designing a novelty score requires task-specific knowledge as we must find a way to strike a balance between the novelty and the quality of architectures" and discusses their sign function approach to adjust novelty scores
- Why unresolved: While the paper proposes a specific sign function approach to balance novelty and fitness, it doesn't explore how different weightings or alternative balancing mechanisms might perform across various search space complexities and sizes
- What evidence would resolve it: Comparative experiments testing different novelty score formulations with varying emphasis on novelty vs. fitness (e.g., different weight parameters, alternative distance metrics, or adaptive weighting schemes) across multiple NAS benchmarks of varying complexity

### Open Question 3
- Question: How does the size of the elitist archive affect the exploration-exploitation balance and overall performance of PDNS?
- Basis in paper: [inferred] The paper describes maintaining an "elitist archive" for novelty score calculations but doesn't investigate how its size impacts performance, despite noting that "the novelty score is also calculated based on the current elitist archive A, which means it adapts over time as the archive evolves"
- Why unresolved: The paper uses a fixed archive size implicitly but doesn't analyze the impact of archive size on the search process, exploration of new regions, or the quality of the final approximation front
- What evidence would resolve it: Experiments varying the elitist archive size across different search spaces while measuring the impact on IGD+, Hypervolume, convergence speed, and the diversity of architectures discovered, comparing small vs. large archives and their effects on exploration vs. exploitation

## Limitations

- Reliance on training-free metrics introduces uncertainty in correlation with actual trained performance across different datasets and architectures
- Elitist archive management strategy is not fully specified, particularly regarding how dominated architectures are removed and tie-breaking rules
- Normalization procedure for training-free metrics is not explicitly detailed, which could significantly impact novelty score calculation

## Confidence

- **High confidence** in the multi-objective optimization framework and the use of Pareto dominance for maintaining an elitist archive
- **Medium confidence** in the effectiveness of combining multiple training-free metrics for novelty search, as this is a novel approach with limited validation
- **Low confidence** in the generalizability of the method to architectures and datasets beyond those tested, given the strong dependence on training-free metric correlations

## Next Checks

1. **Metric Correlation Validation**: Measure the Pearson correlation coefficients between training-free metrics (synflow, jacov, snip) and actual trained performance across multiple NAS benchmarks to quantify the reliability of the novelty search signal

2. **Archive Management Analysis**: Track the size and composition of the elitist archive over generations to ensure it maintains diversity and does not become dominated by similar architectures. Test different archive management strategies

3. **Transferability Testing**: Evaluate the transferability of architectures discovered by PDNS on CIFAR-10 to completely different datasets (e.g., CIFAR-100, ImageNet16-120) to assess the robustness of the search method across domains