---
ver: rpa2
title: 'Repetition Neurons: How Do Language Models Produce Repetitions?'
arxiv_id: '2410.13497'
source_url: https://arxiv.org/abs/2410.13497
tags:
- repetition
- neurons
- language
- figure
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces repetition neurons, a type of skill neuron
  responsible for the repetition problem in text generation tasks. These neurons become
  progressively more activated as repetition continues, indicating they perceive repetition
  as a task of copying previous context.
---

# Repetition Neurons: How Do Language Models Produce Repetitions?

## Quick Facts
- **arXiv ID:** 2410.13497
- **Source URL:** https://arxiv.org/abs/2410.13497
- **Reference count:** 17
- **Primary result:** Repetition neurons, when deactivated, can reduce repetitive text generation by up to 25%, with roughly 30% of repetition problems attributed to these neurons.

## Executive Summary
This paper identifies a specific type of skill neuron called "repetition neurons" that contribute to the repetition problem in language model text generation. These neurons progressively activate more strongly as repetition continues, perceiving repetition as a task of copying previous context. The authors identify these neurons by comparing activation values before and after the onset of repetition in texts generated by pre-trained models. Experiments demonstrate that deactivating these neurons can reduce repetition by up to 25%, while activating them increases repetitive outputs. The findings suggest that approximately 30% of the repetition problem can be attributed to these specific neurons, offering a potential mechanism for controlling repetitive text generation.

## Method Summary
The authors generate 1,000 repetitive texts for each model using greedy decoding with temperature=1.0, filtering for texts containing 10-gram repetition appearing three times at equal intervals within 100 tokens. They calculate activation values for each neuron in normal and repetition ranges (30 tokens before/after repetition onset), compute the difference (Δn) between these ranges, and identify the top K neurons with the largest Δn as repetition neurons. The intervention involves deactivating these neurons by setting activation values to 0.0 or activating them by adding 1.0, then measuring the effect on repetition reduction/increase and perplexity changes.

## Key Results
- Repetition neurons progressively activate more strongly as repetition continues, indicating they perceive repetition as copying previous context
- Deactivating repetition neurons can reduce repetition by up to 25%, while activating them increases repetitive outputs
- The intervention shows consistent patterns across three English models and one Japanese model, suggesting similar underlying mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Repetition neurons are skill neurons that progressively activate more strongly as repetition continues, perceiving repetition as a task of copying previous context.
- **Core assumption:** The activation values of repetition neurons increase with the continuation of repetition, indicating they are responsible for the repetition problem.
- **Evidence:** Figure 1 shows average activation values of top repetition neurons increasing as repetition continues; no direct evidence in corpus papers.

### Mechanism 2
- **Claim:** Deactivating repetition neurons can reduce the repetition problem by suppressing the output probabilities of repeated tokens.
- **Core assumption:** Repetition neurons are responsible for triggering the repetition problem, and their deactivation will lead to a reduction in repetitive outputs.
- **Evidence:** Experiments show up to 25% reduction in repetition when deactivating repetition neurons; Figure 4 demonstrates effectiveness compared to random neuron deactivation.

### Mechanism 3
- **Claim:** Activating repetition neurons can increase the likelihood of generating repetitive outputs by encouraging the model to copy previous contexts.
- **Core assumption:** Repetition neurons play a role in generating repetitive outputs, and their activation will lead to an increase in repetitive outputs.
- **Evidence:** Activating repetition neurons increases repetitive samples; Figure 5 shows increase in repetitive outputs with more activated neurons.

## Foundational Learning

- **Concept:** Transformer language models and their feed-forward networks
  - **Why needed:** Understanding the architecture is crucial for identifying and manipulating repetition neurons
  - **Quick check:** What are the main components of a transformer language model, and how do they contribute to text generation?

- **Concept:** Activation functions and neuron activation values
  - **Why needed:** Identification and manipulation rely on comparing activation values before and after repetition onset
  - **Quick check:** How do activation functions determine neuron output, and how can changes in activation values indicate a neuron's role?

- **Concept:** In-context learning and task vectors
  - **Why needed:** The paper suggests repetition neurons perceive repetition as copying context, similar to in-context learning
  - **Quick check:** How does in-context learning work, and what are task vectors, and how do they relate to repetition neurons?

## Architecture Onboarding

- **Component map:** Transformer layers (feed-forward networks) -> Activation functions -> Neuron activation values -> Text generation process
- **Critical path:** Identify repetition neurons by comparing activation values -> Deactivate/activate identified neurons -> Measure reduction/increase in repetitive outputs
- **Design tradeoffs:** Balancing repetition reduction with model performance maintenance; determining optimal number of neurons to intervene; considering language-specific nature
- **Failure signatures:** Deactivation causes significant performance degradation; activation doesn't increase repetition; intervention has no effect
- **First 3 experiments:** 1) Identify repetition neurons by comparing activation values, 2) Deactivate identified neurons and evaluate reduction, 3) Activate identified neurons and evaluate increase

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What causes the remaining 70% of repetition problems not attributed to repetition neurons?
- **Basis:** The paper states deactivating repetition neurons reduces repetition by up to 25%, suggesting roughly 30% is caused by these neurons, but doesn't investigate other factors in detail.
- **Resolution needed:** A comprehensive study identifying and quantifying other neural mechanisms or factors contributing to repetition.

### Open Question 2
- **Question:** Are repetition neurons language-specific, and if so, how do their functions differ across languages?
- **Basis:** The paper observes that LLM-jp-1.8B's perplexity increases substantially when repetition neurons are deactivated, unlike English models.
- **Resolution needed:** A systematic comparison of repetition neurons across multiple languages, including activation patterns and functional roles.

### Open Question 3
- **Question:** How do repetition neurons behave differently in larger versus smaller language models of the same architecture?
- **Basis:** The paper compares Gemma-2B and Gemma-7B, finding repetition neurons in Gemma-7B are mainly in the last layer versus intermediate layers in Gemma-2B.
- **Resolution needed:** A detailed investigation of how model scale affects the emergence, distribution, and function of repetition neurons.

## Limitations
- The intervention methodology using greedy decoding may not translate to other decoding strategies like sampling or beam search
- The study only examines three English models and one Japanese model without comprehensive cross-linguistic validation
- The mechanism explaining how 30% of repetition stems from these neurons remains somewhat vague

## Confidence
- **High:** The existence of repetition neurons and their progressive activation patterns (supported by Figure 1)
- **Medium:** The effectiveness of deactivation/activation interventions (up to 25% reduction, though absolute impact unclear)
- **Medium:** The claim that roughly 30% of repetition problems are attributable to these neurons (based on relative effect size)

## Next Checks
1. Test the intervention across multiple decoding strategies (sampling, beam search) to verify robustness beyond greedy decoding
2. Conduct cross-linguistic validation on non-English models to determine if repetition neurons are language-specific or universal
3. Measure downstream task performance degradation when deactivating repetition neurons to assess practical viability of the intervention