---
ver: rpa2
title: Explaining Large Language Models Decisions Using Shapley Values
arxiv_id: '2404.01332'
source_url: https://arxiv.org/abs/2404.01332
tags:
- shapley
- prompt
- llms
- value
- flight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses Shapley values to interpret LLM behavior and quantify
  token contributions to model decisions. The method treats prompt tokens as players
  in a cooperative game, computing each token's marginal contribution to the output
  probability.
---

# Explaining Large Language Models Decisions Using Shapley Values

## Quick Facts
- arXiv ID: 2404.01332
- Source URL: https://arxiv.org/abs/2404.01332
- Authors: Behnam Mohammadi
- Reference count: 0
- Primary result: Shapley values reveal that LLM decisions are disproportionately influenced by low-information tokens rather than semantically important content, raising concerns about using LLMs as human proxies in marketing research.

## Executive Summary
This paper presents a novel approach using Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. The method treats prompt tokens as players in a cooperative game, computing each token's marginal contribution to the output probability. Two applications demonstrate the approach: a discrete choice experiment revealing disproportionate influence of low-information tokens ("token noise") and a framing effect experiment showing that apparent cognitive biases are artifacts of token noise rather than genuine cognitive processing.

## Method Summary
The methodology converts natural language prompts into structured Jinja templates with replaceable fields, models the LLM as a function mapping prompt vectors to probabilities, and estimates Shapley values using a moving average Monte Carlo approach. The method calculates each token's marginal contribution to choice probabilities by averaging over all possible orderings of token inclusion, weighted by the number of permutations that create each coalition. This allows identification of key tokens that significantly impact model decisions and reveals phenomena like token noise effects where LLM decisions are disproportionately influenced by tokens providing minimal informative content.

## Key Results
- LLM decisions in discrete choice experiments are disproportionately influenced by low-information tokens rather than semantically important content like prices and flight times
- Apparent framing effects in LLMs are artifacts of token noise rather than genuine cognitive processing
- Token noise causes unpredictable probability shifts that cannot be systematically corrected, casting doubt on LLM reliability as human proxies in marketing research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shapley values can quantify token contributions to LLM outputs by treating tokens as players in a cooperative game.
- Mechanism: Each token in a prompt is treated as a player in a cooperative game. The LLM's output probability serves as the "payoff" function. Shapley values calculate each token's marginal contribution to this payoff by averaging over all possible orderings of token inclusion, weighted by the number of permutations that create each coalition.
- Core assumption: The LLM can be modeled as a linear function of token attributions, even though the underlying model is highly nonlinear.
- Evidence anchors:
  - [abstract] "This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output."
  - [section] "Our proposed approach, based on Shapley values from cooperative game theory, addresses precisely this challenge... This approach allows us to identify key tokens that significantly impact the model's decisions."
  - [corpus] Weak - related papers focus on Shapley values for general ML interpretability but don't specifically address token-level attribution in LLMs.
- Break condition: If token contributions are highly nonlinear or interactive in ways that cannot be captured by marginal contributions, the Shapley value approximation breaks down.

### Mechanism 2
- Claim: The "token noise" effect occurs because LLMs are disproportionately influenced by low-information tokens rather than semantically important content.
- Mechanism: The Shapley value analysis reveals that tokens with minimal semantic content (articles, prepositions, "flight") have disproportionately high attribution scores compared to tokens containing critical decision information (prices, travel times). This indicates the LLM's decision-making is being driven by superficial token patterns rather than genuine semantic understanding.
- Core assumption: High Shapley values indicate genuine importance in the decision-making process.
- Evidence anchors:
  - [abstract] "This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output."
  - [section] "Our findings show otherwise. This token noise effect, combined with the sensitivity to format-level changes like newline characters, casts serious doubt on whether the LLM's outputs truly reflect an understanding of the semantic content."
  - [corpus] Weak - no direct corpus evidence for token noise in LLMs specifically.
- Break condition: If high Shapley values for low-information tokens reflect genuine processing needs (e.g., grammatical parsing) rather than noise, the token noise interpretation breaks down.

### Mechanism 3
- Claim: Apparent cognitive biases in LLMs (like framing effects) are artifacts of token noise rather than genuine cognitive processing.
- Mechanism: When a prompt is framed positively (e.g., "Costs only $600"), the LLM's apparent sensitivity to framing is actually due to the Shapley value of the word "only" being high. By optimizing the prompt to reduce token noise, the apparent framing effect can be eliminated, suggesting it was not a genuine cognitive bias.
- Core assumption: Genuine cognitive biases would persist across prompt optimizations, while token noise effects would be sensitive to prompt structure.
- Evidence anchors:
  - [abstract] "the Shapley value method can uncover what we term 'token noise' effects, a phenomenon where LLM decisions are disproportionately influenced by tokens providing minimal informative content."
  - [section] "However, as LLMs are fundamentally autoregressive models trained on extensive datasets, their responses may also reflect learned statistical associations rather than genuine cognitive processes."
  - [corpus] Weak - related papers discuss cognitive biases in LLMs but don't use Shapley values to distinguish between genuine biases and token noise artifacts.
- Break condition: If the framing effect persists even after token noise optimization, or if the optimization itself introduces other artifacts, the token noise interpretation of the framing effect breaks down.

## Foundational Learning

- Concept: Cooperative game theory and Shapley value calculation
  - Why needed here: The entire methodology relies on treating tokens as players in a cooperative game and calculating their Shapley values to determine contribution to the LLM output.
  - Quick check question: Can you explain why Shapley values satisfy efficiency, symmetry, additivity, and null player properties in simple 3-player examples?

- Concept: Prompt vectorization and template-based prompt generation
  - Why needed here: The method requires converting natural language prompts into structured templates with fields that can be systematically varied to calculate marginal contributions.
  - Quick check question: Given a simple prompt, can you create a Jinja template and explain how different field substitutions would generate different prompt variations?

- Concept: LLM as function mapping and baseline/reference point selection
  - Why needed here: The methodology models the LLM as a function from prompts to probabilities and requires choosing appropriate baseline/reference points for Shapley value calculation.
  - Quick check question: Why is the choice of reference vector (using " _ " for absent fields) important for calculating Shapley values in single-prompt scenarios?

## Architecture Onboarding

- Component map: Prompt preprocessing -> Template creation -> Shapley value computation -> Analysis and visualization
- Critical path: Prompt → Template → Shapley calculation → Analysis
  The most critical component is the Shapley value estimation algorithm, as it directly determines the quality of the attribution results.
- Design tradeoffs:
  - Template granularity vs. computational cost: Finer templates (more fields) provide more precise attribution but exponentially increase computation.
  - Monte Carlo iterations vs. accuracy: More iterations improve Shapley value estimates but increase runtime.
  - Reference point choice: Different reference vectors can yield different interpretations of token contributions.
- Failure signatures:
  - High variance in Shapley estimates across runs suggests insufficient Monte Carlo iterations.
  - Unusually high or low attribution scores for certain tokens may indicate template design issues.
  - Computational timeouts suggest the template has too many fields or the LLM interface is inefficient.
- First 3 experiments:
  1. Implement a simple prompt template with 3-4 fields and verify Shapley values sum to the difference between full prompt probability and baseline.
  2. Test the method on a known linear model (like logistic regression) to verify Shapley values match analytical expectations.
  3. Apply the method to a simple LLM prompt and visualize the attribution distribution to identify any obvious token noise patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features beyond "only" in framing prompts trigger token noise effects in LLM decision-making?
- Basis in paper: [explicit] The paper demonstrates that changing "Flight" to "Option" and "Costs" to "Priced at" eliminates apparent framing effects, suggesting other token-level features may drive spurious biases
- Why unresolved: The paper identifies token noise as a general phenomenon but doesn't systematically test which specific token categories (function words, punctuation, etc.) have the strongest impact across different prompt types
- What evidence would resolve it: Controlled experiments varying different token categories (function words, punctuation, capitalization) while keeping semantic content constant would identify which features most strongly influence LLM decisions

### Open Question 2
- Question: How do different LLM architectures (transformers vs alternatives) vary in their susceptibility to token noise effects?
- Basis in paper: [inferred] The paper uses only Llama-2 models and notes their method is "model-agnostic" but doesn't test other architectures like GPT, Claude, or non-transformer models
- Why unresolved: The analysis is limited to Llama-2 variants; it's unclear whether token noise is an inherent LLM property or specific to transformer architectures
- What evidence would resolve it: Testing the same Shapley value analysis across multiple model families (GPT, Claude, BERT, etc.) would reveal whether token noise is architecture-dependent

### Open Question 3
- Question: Can token noise effects be systematically predicted and corrected rather than just mitigated through prompt optimization?
- Basis in paper: [explicit] The paper notes that token noise causes "unpredictable" probability shifts and that "we cannot systematically correct for this type of noise"
- Why unresolved: The paper identifies the problem but only offers reactive solutions (prompt optimization) rather than predictive models for token noise
- What evidence would resolve it: Developing a meta-model that predicts token noise contributions based on token properties (frequency, position, semantic category) would enable proactive correction rather than trial-and-error prompt engineering

## Limitations

- The token noise interpretation may be conflating genuine processing requirements (e.g., grammatical parsing) with spurious influences
- Findings are based on a single LLM architecture and may not generalize across different model families or sizes
- The computational complexity of the moving average Monte Carlo approach limits scalability to longer or more complex prompts

## Confidence

- **High confidence**: The Shapley value methodology for token attribution is technically sound and mathematically well-established
- **Medium confidence**: The identification of token noise effects is plausible but requires additional validation across multiple models and tasks
- **Low confidence**: The claim that apparent cognitive biases are artifacts of token noise rather than genuine cognitive processing is the weakest claim

## Next Checks

1. **Cross-model validation**: Apply the same methodology to multiple LLM architectures (GPT-4, Claude, open-source models) using identical prompts. Compare whether token noise patterns persist across models or are specific to the tested architecture.

2. **Control experiment with synthetic data**: Create prompts where the correct answer is deterministically tied to specific token values (e.g., price comparisons). Apply the Shapley method to verify that semantically important tokens receive the highest attributions.

3. **Human comparison study**: Have human subjects complete the same discrete choice and framing tasks. Compare human decision patterns and sensitivity to prompt modifications against LLM outputs.