---
ver: rpa2
title: Human-Readable Programs as Actors of Reinforcement Learning Agents Using Critic-Moderated
  Evolution
arxiv_id: '2410.21940'
source_url: https://arxiv.org/abs/2410.21940
tags:
- learning
- program
- programs
- genetic
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to learn a program as the policy of
  a reinforcement learning agent, combining TD3 with Genetic Programming. The method
  uses TD3 critics to compute gradients of program quality and steer the genetic algorithm
  to optimize the program during training.
---

# Human-Readable Programs as Actors of Reinforcement Learning Agents Using Critic-Moderated Evolution

## Quick Facts
- arXiv ID: 2410.21940
- Source URL: https://arxiv.org/abs/2410.21940
- Reference count: 40
- Key outcome: Method combines TD3 with Genetic Programming to learn programs as RL policies, using critic gradients to optimize programs during training, achieving high sample efficiency and interpretable policies.

## Executive Summary
This paper introduces a novel approach that combines Deep Deterministic Policy Gradient (DDPG) with Genetic Programming (GP) to learn interpretable programs as reinforcement learning policies. The method uses TD3 critics to compute gradients of program quality and steer the genetic algorithm during training, directly optimizing programs for high rewards rather than distilling from black-box policies. Experiments in a simple gridworld environment demonstrate that the approach produces explainable programs with policy quality comparable to vanilla TD3 while achieving significantly higher sample efficiency than pure genetic programming methods.

## Method Summary
The method combines TD3 with Genetic Programming where programs are evolved using gradients from TD3 critics. Programs are represented as sequences of real-valued genes that map to operators through stochastic selection. During training, TD3 critics evaluate and improve actions, which are then used as targets for program optimization via genetic algorithms. The approach directly optimizes programs for high rewards rather than approximating a black-box policy, and leverages critic estimates for sample efficiency.

## Key Results
- Learned programs demonstrate interpretability while maintaining policy quality comparable to vanilla TD3
- Achieved significantly higher sample efficiency compared to pure genetic programming approaches
- Successfully navigated a simple gridworld environment with continuous state and action spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method produces explainable programs without sacrificing final policy quality by directly optimizing programs for high rewards rather than distilling a black-box policy.
- Mechanism: Uses TD3 critics to compute gradients of program quality with respect to actions predicted by the program, steering genetic algorithm optimization during training.
- Core assumption: TD3 critics can provide meaningful gradients that lead to improved actions when backpropagated through the program.
- Evidence anchors: [abstract] "This approach directly optimizes the program for high rewards rather than distilling a black-box policy" and [section] "Our approach builds the program during training, as opposed to after the fact."

### Mechanism 2
- Claim: The method achieves high sample-efficiency compared to pure genetic programming approaches.
- Mechanism: Leverages TD3 critics to compute program quality instead of performing environment rollouts, eliminating the need for numerous environment interactions to calculate fitness functions.
- Core assumption: TD3 critics maintain accurate value estimates throughout training, serving as reliable proxies for environment interaction.
- Evidence anchors: [abstract] "Also, our approach leverages the TD3 critics to achieve high sample-efficiency" and [section] "The fitness function on a batch of states S for an individual gi in the population is the mean-squared error (MSE) between program actions pgi(S) = A and improved actions A* from the critic."

### Mechanism 3
- Claim: The program representation and execution design creates a smooth optimization landscape for genetic programming.
- Mechanism: Programs are represented as sequences of real-valued genes with stochastic interpretation of operators, where sampling operators based on gene values plus uniform noise creates gradual changes in program behavior.
- Core assumption: Stochastic operator selection and multiple execution averaging effectively smooth the fitness landscape, preventing abrupt changes that would create local optima.
- Evidence anchors: [section] "To smoothen the optimization landscape, we introduce stochasticity in the mapping from real value to operators" and [section] "Slowly changing the value of genes now has the effect of slowly changing that average value."

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The method builds on reinforcement learning foundations where agents interact with environments modeled as MDPs, requiring understanding of states, actions, rewards, and policies.
  - Quick check question: What is the difference between the state transition probability P a ss′ and the reward function Ra ss′ in an MDP formulation?

- Concept: Actor-Critic methods
  - Why needed here: The approach extends TD3, an actor-critic algorithm, by replacing the neural network actor with a genetic programming-based program.
  - Quick check question: How does TD3 use multiple critics to reduce value overestimation, and why is this important for the critic-moderated evolution approach?

- Concept: Genetic Programming fundamentals
  - Why needed here: The method uses genetic programming to evolve programs, requiring knowledge of population initialization, selection, crossover, mutation, and fitness evaluation in the context of program synthesis.
  - Quick check question: What is the role of the fitness function in genetic programming, and how does the critic-moderated approach differ from traditional fitness evaluation methods?

## Architecture Onboarding

- Component map: Environment -> TD3 Critic Network -> Program Actor -> Evolution Module -> Interaction Loop
- Critical path: Environment interaction → Critic training → Program evaluation → Gradient computation → Genetic algorithm optimization → Program update → Next environment interaction
- Design tradeoffs:
  - Program complexity vs. interpretability: Simpler programs are more interpretable but may have limited expressiveness
  - Evolution frequency vs. computational cost: More frequent evolution improves program quality but increases computational overhead
  - Critic reliance vs. exploration: Heavy reliance on critics may reduce exploration diversity
- Failure signatures:
  - Programs producing constant outputs indicate loss of state dependence
  - Rapid fitness improvement followed by plateau suggests local optima
  - Critic Q-value divergence indicates training instability
- First 3 experiments:
  1. Implement a basic TD3 agent without genetic programming to establish baseline performance and verify environment interaction
  2. Implement simple genetic programming with random program initialization and basic fitness evaluation to verify evolution mechanics
  3. Integrate TD3 critics with genetic programming, starting with simple programs and gradually increasing complexity while monitoring sample efficiency and policy quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the program representation affect the optimization landscape and search efficiency?
- Basis in paper: [explicit] The paper discusses using real-valued genes to represent programs and mentions smoothing the optimization landscape to avoid plateaus.
- Why unresolved: The paper uses a simple set of operators and real-valued genes, but does not explore alternative representations like tree-based or graph-based structures.
- What evidence would resolve it: Comparative experiments using different program representations (e.g., tree-based, graph-based) to measure their impact on optimization landscape smoothness and search efficiency.

### Open Question 2
- Question: What is the impact of different operator sets on the interpretability and performance of the generated programs?
- Basis in paper: [explicit] The paper notes that the selection of operators is domain-dependent and that a more suitable set of primitives would benefit interpretability.
- Why unresolved: The paper uses a fixed set of operators without exploring how different sets might affect the resulting programs' interpretability and performance.
- What evidence would resolve it: Experiments comparing program performance and interpretability using various operator sets tailored to different domains.

### Open Question 3
- Question: How does the proposed method scale to more complex environments with higher-dimensional state and action spaces?
- Basis in paper: [inferred] The experiments are conducted in a simple gridworld environment, and the paper does not address scalability to more complex tasks.
- Why unresolved: The paper focuses on a simple environment and does not provide evidence of the method's effectiveness in more complex scenarios.
- What evidence would resolve it: Experiments applying the method to environments with higher-dimensional state and action spaces, such as robotic control or video games.

## Limitations
- Limited experimental validation only in a simple gridworld environment
- No direct comparison with pure genetic programming baselines for sample efficiency claims
- Uncertainty about whether critic-guided evolution scales to more complex environments

## Confidence
- Medium: The core mechanism of using TD3 critics to guide genetic programming evolution - supported by theoretical reasoning but limited experimental validation
- Low: Sample efficiency claims compared to pure genetic methods - no direct comparison data provided
- Medium: Program interpretability without sacrificing performance - demonstrated only in a simple environment

## Next Checks
1. Test the method in more complex environments (e.g., continuous control tasks) to verify critic-guided evolution scales beyond gridworld scenarios
2. Compare sample efficiency against pure genetic programming baselines using identical program representations and fitness evaluation methods
3. Analyze the learned programs to verify they remain interpretable and state-dependent rather than collapsing to constant outputs or trivial policies