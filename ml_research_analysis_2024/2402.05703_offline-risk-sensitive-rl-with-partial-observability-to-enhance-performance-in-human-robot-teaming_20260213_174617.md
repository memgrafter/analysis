---
ver: rpa2
title: Offline Risk-sensitive RL with Partial Observability to Enhance Performance
  in Human-Robot Teaming
arxiv_id: '2402.05703'
source_url: https://arxiv.org/abs/2402.05703
tags:
- policy
- pomdp
- data
- human
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for offline risk-sensitive reinforcement
  learning with partial observability in human-robot teaming. The core idea is to
  learn a POMDP model from a fixed dataset of human-robot interactions, incorporating
  model uncertainty via a Bayesian framework.
---

# Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming

## Quick Facts
- arXiv ID: 2402.05703
- Source URL: https://arxiv.org/abs/2402.05703
- Reference count: 40
- Primary result: Proposed robust POMDP policy led to statistically significant higher mission scores than the random policy used to collect the dataset.

## Executive Summary
This paper addresses the challenge of offline risk-sensitive reinforcement learning with partial observability in human-robot teaming scenarios. The method learns a POMDP model from a fixed dataset of human-robot interactions, incorporating model uncertainty via a Bayesian framework. Policies are computed for the learned POMDP and risk-sensitive selection is performed among candidates using offline policy evaluation and Value-at-Risk. Experiments in a simulated robot teleoperation environment with 26 participants showed that the proposed approach led to statistically significant higher mission scores compared to the random policy used to collect the dataset, demonstrating generalization across diverse participants.

## Method Summary
The proposed method learns a POMDP model from a fixed dataset of human-robot interactions, where the human state (e.g., performance, cognitive load) is not directly observable. High-dimensional physiological and behavioral features (HR, HRV, gaze, robot actions) are mapped to discrete observations using trained classifiers. These observations are used to define the POMDP observation function and Bayesian prior. Model uncertainty is incorporated via Dirichlet posteriors over observation functions, and policies are computed for the learned POMDP using different discount factors. Risk-sensitive policy selection is performed by evaluating policies on sampled POMDP models using Monte Carlo rollout and selecting by Value-at-Risk at risk-level 0.5.

## Key Results
- Proposed robust POMDP policy led to statistically significant higher mission scores than the random policy used to collect the dataset
- The approach demonstrated generalization across diverse participants in the validation experiments
- Improved risk-sensitive metrics compared to MDP-based methods, with VaR at 0.5 effectively capturing the desired risk-sensitive criterion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offline risk-sensitive policy selection is enabled by Bayesian uncertainty modeling of POMDP dynamics.
- Mechanism: The method learns an initial trivial POMDP from a limited dataset, then uses Dirichlet posteriors over observation functions to sample multiple POMDP models. Each candidate policy is evaluated on these sampled models using Monte Carlo rollout, and the Value-at-Risk at risk-level 0.5 selects the most robust policy.
- Core assumption: Dirichlet posteriors over observation functions adequately represent model uncertainty given sparse data.
- Evidence anchors:
  - [abstract]: "we propose a method to incorporate model uncertainty, thus enabling risk-sensitive sequential decision-making."
  - [section 4]: "Non-normalized confusion matrices updated Dirichlet priors expressing observation model uncertainty. Diverse observation functions were sampled from these posteriors."
  - [corpus]: Weak evidence; no direct literature found on Dirichlet-based uncertainty in offline POMDPs.
- Break condition: If the confusion matrix priors are not representative of true uncertainty (e.g., poor classifier performance or class imbalance), the sampled models may not cover the true dynamics.

### Mechanism 2
- Claim: Partial observability is handled by learning observation functions from high-dimensional physiological and behavioral features via classifiers.
- Mechanism: Raw multimodal features (HR, HRV, gaze fixations, robot actions) are mapped to discrete observations (e.g., "performant" vs "non-performant") using trained classifiers. These observations are then used to define the POMDP observation function and Bayesian prior.
- Core assumption: The trained classifiers generalize across diverse participants and accurately reflect the hidden performance state.
- Evidence anchors:
  - [section 4]: "We opted to train four Extra Tree Classifiers... to map high-dimensional observations into a low-dimensional variable, whose two possible entries are performant and non-performant."
  - [section 5.2]: "The Spearman's rank correlation reveals a significant positive correlation between β and the global mission score."
  - [corpus]: Weak evidence; classifier-based dimensionality reduction in POMDPs is not common in the literature.
- Break condition: If classifiers are overfit or fail to generalize (e.g., due to inter-subject variability), the observation function will misrepresent the true state, leading to poor policy performance.

### Mechanism 3
- Claim: Risk-sensitive policy selection is achieved by evaluating policies on sampled POMDP models and selecting by Value-at-Risk.
- Mechanism: For each sampled POMDP, multiple trajectories are simulated to estimate the distribution of returns for each policy. The Value-at-Risk at risk-level 0.5 (median return) is used to select the policy that performs well even under model uncertainty.
- Core assumption: The Value-at-Risk at 0.5 captures the desired risk-sensitive criterion and the sampled models cover plausible dynamics.
- Evidence anchors:
  - [section 4]: "The Value-at-Risk [50] at risk level q, i.e. the value of the q-order quantile, set to 0.5, guided robust policy selection."
  - [section 5.2]: "The MDP policy maintains the manual mode... With the POMDP policy, the robot's mode depends on user state estimation."
  - [corpus]: Weak evidence; VaR-based policy selection in offline POMDPs is not standard.
- Break condition: If the sample size of POMDP models is too small or the quantile is not representative of true risk preferences, the selected policy may not be truly risk-sensitive.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The human state (e.g., performance, cognitive load) is not directly observable; only noisy physiological/behavioral features are available.
  - Quick check question: What is the difference between an MDP and a POMDP in terms of state observability?

- Concept: Offline Reinforcement Learning (ORL)
  - Why needed here: The system must learn from a fixed dataset of human-robot interactions without further interaction, which is expensive and potentially unsafe.
  - Quick check question: How does ORL differ from online RL in terms of data collection and policy evaluation?

- Concept: Bayesian Uncertainty Estimation
  - Why needed here: Limited data makes the POMDP model uncertain; Bayesian methods quantify this uncertainty to enable risk-sensitive decisions.
  - Quick check question: What role do Dirichlet posteriors play in modeling uncertainty over discrete observation functions?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Classifier training -> POMDP model learning -> Policy computation -> Risk-sensitive selection -> Validation

- Critical path:
  1. Data preprocessing → classifier training
  2. Classifier outputs → observation function priors
  3. POMDP learning (EM) → candidate policies (SARSOP)
  4. Policy evaluation on sampled models → VaR-based selection
  5. Policy deployment → validation experiments

- Design tradeoffs:
  - Classifier choice: Extra Trees vs Random Forests (reduced variance, slight bias increase)
  - Discount factor set: Larger set increases policy diversity but computational cost
  - Number of sampled models: More models better uncertainty coverage but slower selection

- Failure signatures:
  - Low classifier balanced accuracy → poor observation function → belief updates do not track true state
  - High variance in policy evaluation → insufficient sampled models or too few rollouts
  - Policy selection does not improve over random → VaR quantile not representative or models not diverse

- First 3 experiments:
  1. Train classifiers on a subset of data; evaluate balanced accuracy and confusion matrices; check for class imbalance
  2. Sample 10 POMDP models; evaluate one candidate policy on each; plot return distributions; check VaR consistency
  3. Deploy selected policy in a pilot with 5 participants; record performance, belief trajectories, and subjective feedback; compare to random policy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method scale with larger and more diverse datasets?
- Basis in paper: [inferred] The paper acknowledges data representativeness and scarcity as major concerns, suggesting that larger and more diverse datasets could improve performance.
- Why unresolved: The experiments were conducted with a relatively small dataset (72 mission recordings from 18 participants). It's unclear how the method would perform with more data.
- What evidence would resolve it: Conducting experiments with significantly larger and more diverse datasets, and comparing the performance of the proposed method to other baselines.

### Open Question 2
- Question: Can the proposed method effectively handle continuous state and action spaces?
- Basis in paper: [inferred] The paper uses a discrete POMDP representation, but many real-world problems involve continuous state and action spaces. It's unclear how the method would generalize to such cases.
- Why unresolved: The experiments were conducted in a simulated environment with discrete states and actions. The method's applicability to continuous problems remains untested.
- What evidence would resolve it: Extending the method to handle continuous state and action spaces, and evaluating its performance on benchmark continuous control problems.

### Open Question 3
- Question: How sensitive is the proposed method to the choice of discount factor and other hyperparameters?
- Basis in paper: [explicit] The paper mentions that different discount factors were used to generate diverse policies, and the best one was selected based on risk-sensitive criteria. However, the impact of other hyperparameters is not discussed.
- Why unresolved: The paper only explores a limited range of discount factors and does not provide a comprehensive sensitivity analysis.
- What evidence would resolve it: Conducting a thorough sensitivity analysis by varying multiple hyperparameters and evaluating their impact on the method's performance and robustness.

## Limitations
- Classifier accuracy may not generalize across diverse participants due to inter-subject variability in physiological and behavioral features
- Small validation sample size (26 participants) limits generalizability and may not capture broader population diversity
- Moderate balanced accuracy (67-75%) of classifiers may lead to suboptimal observation functions and belief updates

## Confidence
- **High**: The method's core components (POMDP learning, Bayesian uncertainty, risk-sensitive selection) are correctly described and implemented
- **Medium**: The experimental results show improvement over random policy, but the small sample size and moderate classifier performance limit generalizability
- **Low**: The claim that the approach generalizes across diverse participants is weakly supported by the limited validation data and lack of cross-validation

## Next Checks
1. **Classifier Robustness**: Evaluate classifier performance on held-out data from each participant to assess inter-subject variability and generalization
2. **Uncertainty Coverage**: Increase the number of sampled POMDP models and assess whether the VaR-based policy selection remains consistent across different model sets
3. **Risk Preference Sensitivity**: Test the method with different VaR quantiles (e.g., 0.25, 0.75) to determine if the selected policy changes significantly, indicating sensitivity to risk preferences