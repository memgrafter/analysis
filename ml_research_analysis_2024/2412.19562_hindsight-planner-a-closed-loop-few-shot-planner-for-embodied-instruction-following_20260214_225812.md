---
ver: rpa2
title: 'Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied Instruction
  Following'
arxiv_id: '2412.19562'
source_url: https://arxiv.org/abs/2412.19562
tags:
- task
- planner
- fridge
- rollout
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a closed-loop few-shot planner for embodied
  instruction following by treating the task as a POMDP. The approach uses an actor-critic
  framework with an adaptation module and a novel hindsight method to handle out-of-distribution
  states and sparse rewards.
---

# Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied Instruction Following

## Quick Facts
- **arXiv ID**: 2412.19562
- **Source URL**: https://arxiv.org/abs/2412.19562
- **Reference count**: 40
- **Primary result**: Achieves 25.51% and 18.77% success rates on ALFRED test seen and unseen splits, representing 60% and 39% improvements over previous few-shot baselines

## Executive Summary
This paper addresses the challenge of few-shot embodied instruction following by treating the task as a Partially Observable Markov Decision Process (POMDP) and proposing a closed-loop planning approach. The authors introduce Hindsight Planner, which combines an actor-critic framework with a novel hindsight method to handle out-of-distribution states and sparse rewards. The method demonstrates state-of-the-art performance under few-shot assumptions, achieving significant improvements over previous baselines and approaching the performance of full-shot supervised agents for the first time in this domain.

## Method Summary
The approach formulates embodied instruction following as a POMDP where agents must infer hidden states from observations and plan actions accordingly. The method employs an actor-critic framework where the actor generates action sequences and the critic evaluates them. A key innovation is the hindsight method that estimates OOD states by looking back at historical trajectories, allowing the agent to adapt to states not seen during training. The adaptation module enables efficient few-shot learning by leveraging demonstration trajectories, while the closed-loop structure allows for real-time replanning as new observations arrive. This combination addresses the sparse reward problem and enables better generalization to unseen environments compared to previous few-shot approaches.

## Key Results
- Achieves 25.51% success rate on ALFRED test seen split (60% improvement over previous baseline)
- Achieves 18.77% success rate on ALFRED test unseen split (39% improvement over previous baseline)
- First few-shot agent to approach and surpass full-shot supervised agent performance

## Why This Works (Mechanism)
The method works by reframing embodied instruction following as a POMDP, which naturally captures the partial observability inherent in these tasks. The actor-critic framework provides a stable learning foundation, while the hindsight method specifically addresses the challenge of OOD states that frequently occur in novel environments. By leveraging historical trajectories to estimate these states, the agent can better generalize beyond its training distribution. The closed-loop structure enables dynamic replanning as new observations arrive, which is crucial for handling the sequential and partially observable nature of embodied tasks.

## Foundational Learning

**POMDP Formulation**: Why needed - captures partial observability where agents can only observe partial information about the true state. Quick check - verify the state space is indeed partially observable by checking observation history vs. true state correlation.

**Actor-Critic Framework**: Why needed - provides stable policy learning by separating action selection (actor) from value estimation (critic). Quick check - monitor actor-critic loss ratio to ensure neither component dominates training.

**Hindsight Learning**: Why needed - addresses the sparse reward problem by enabling learning from failed trajectories. Quick check - verify hindsight trajectories actually connect to successful states in the demonstration data.

**Adaptation from Demonstrations**: Why needed - enables few-shot learning by leveraging existing successful trajectories. Quick check - measure adaptation speed by tracking success rate improvement over demonstration exposures.

## Architecture Onboarding

**Component Map**: Observation Encoder -> Hindsight State Estimator -> Actor Network -> Critic Network -> Action Selection -> Environment -> Next Observation

**Critical Path**: The core execution flow processes observations through the encoder, estimates current state using hindsight, generates actions via the actor, evaluates them with the critic, and updates based on environment feedback. The hindsight estimator is critical as it bridges the gap between observed and true states.

**Design Tradeoffs**: The closed-loop design trades computational efficiency for better handling of partial observability. The hindsight method adds complexity but enables better OOD state handling. The actor-critic approach balances stability with expressiveness compared to pure value-based or policy-based methods.

**Failure Signatures**: Common failure modes include getting stuck in local optima due to sparse rewards, poor generalization to unseen object configurations, and breakdown of the hindsight estimator in highly novel environments. The method may also struggle with very long-horizon tasks where error accumulation becomes significant.

**First Experiments**:
1. Run a single demonstration trajectory through the system to verify all components connect correctly
2. Test the hindsight estimator's ability to recover states from partial observations
3. Validate the closed-loop replanning responds appropriately to observation changes

## Open Questions the Paper Calls Out
None

## Limitations
- Struggles with long-horizon tasks and complex manipulation sequences, particularly in unseen environments
- Performance gap between seen (25.51%) and unseen (18.77%) splits indicates incomplete generalization
- Reliance on ALFRED's synthetic demonstrations may not translate well to real-world scenarios with scarce optimal demonstrations

## Confidence

**Major Claim Confidence:**
- **High Confidence**: The 60% and 39% improvements over previous few-shot baselines are well-supported by experimental results on ALFRED
- **Medium Confidence**: The claim of approaching full-shot supervised agent performance is supported but should be viewed cautiously given the still relatively low absolute success rates
- **Medium Confidence**: The effectiveness of the hindsight method for handling OOD states is demonstrated, but could benefit from more granular evaluation

## Next Checks
1. Conduct ablation studies specifically isolating the contribution of the hindsight method versus the actor-critic framework to better understand which components drive the improvements
2. Test the approach on a more diverse set of instruction-following benchmarks beyond ALFRED to assess generalization across different task types and environments
3. Implement a real-world deployment or simulation with noise and imperfect demonstrations to validate the method's robustness outside the controlled ALFRED environment