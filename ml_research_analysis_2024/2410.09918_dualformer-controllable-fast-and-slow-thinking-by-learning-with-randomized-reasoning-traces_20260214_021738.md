---
ver: rpa2
title: 'Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized
  Reasoning Traces'
arxiv_id: '2410.09918'
source_url: https://arxiv.org/abs/2410.09918
tags:
- wall
- dualformer
- mode
- reasoning
- maze
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Dualformer, a Transformer model that integrates
  fast and slow reasoning modes by training on randomized reasoning traces with strategic
  trace dropping. During training, different parts of the reasoning traces are selectively
  dropped to mimic human shortcuts in thinking, enabling the model to operate in fast,
  slow, or auto modes at inference.
---

# Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces

## Quick Facts
- arXiv ID: 2410.09918
- Source URL: https://arxiv.org/abs/2410.09918
- Reference count: 40
- Primary result: Dualformer outperforms baselines on maze navigation and Sokoban tasks, achieving 97.6% optimal rate in slow mode with 45.5% fewer reasoning steps than Searchformer

## Executive Summary
Dualformer introduces a novel approach to integrate fast and slow reasoning modes in a single Transformer model by training on randomized reasoning traces with strategic trace dropping. The model learns to generate solutions with varying levels of reasoning detail, controlled by simple control tokens at inference time. Experiments on maze navigation and Sokoban tasks demonstrate that Dualformer outperforms baseline approaches across all modes, achieving high optimal rates while using fewer reasoning steps in slow mode and maintaining strong performance in fast mode.

## Method Summary
Dualformer uses an encoder-decoder Transformer architecture trained on randomized reasoning traces where different parts of A* search traces are selectively dropped during training. The model implements four levels of trace dropping (D1-D4) that progressively remove reasoning components like close clauses, cost tokens, create clauses, or entire traces. This structured randomization enables the model to operate in fast, slow, or auto modes at inference, where auto mode automatically adjusts reasoning depth based on problem complexity. The approach is evaluated on maze navigation and Sokoban tasks, with additional experiments showing generalization to LLM fine-tuning for math reasoning.

## Key Results
- Achieves 97.6% optimal rate in slow mode with 45.5% fewer reasoning steps than Searchformer
- Maintains 80% optimal rate in fast mode versus 30% for solution-only models
- Generates more diverse reasoning traces than baselines, with 9.05-25.77 unique feasible paths vs 1.52-7.60 for baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured trace dropping during training enables Dualformer to learn both fast and slow reasoning modes in a single model
- Mechanism: By selectively dropping parts of the A* search trace at training time, the model learns to generate solutions with or without reasoning steps based on a simple control token
- Core assumption: The structure of A* search traces is hierarchical and predictable, allowing systematic removal of reasoning components without losing task semantics
- Evidence anchors: [abstract]: "by training on randomized reasoning traces, where different parts of the traces are strategically dropped during training"; [section 3]: Describes four levels of trace dropping (D1-D4)

### Mechanism 2
- Claim: Randomized trace dropping strategies improve plan diversity by preventing memorization of single optimal paths
- Mechanism: Random sampling of different dropping strategies for each training example forces the model to learn multiple solution pathways rather than memorizing specific search traces
- Core assumption: The non-deterministic A* implementation in training data provides sufficient diversity that the model can exploit when trace dropping creates varied training examples
- Evidence anchors: [abstract]: "Moreover, Dualformer produces more diverse reasoning traces than Searchformer"; [section 4.1.1]: Shows Dualformer generates 9.05-25.77 unique feasible paths vs 1.52-7.60 for baselines

### Mechanism 3
- Claim: Auto mode enables context-aware switching between fast and slow thinking based on problem difficulty
- Mechanism: When no control token is provided, Dualformer samples from its learned distribution of fast/slow responses, automatically engaging slower thinking for more complex problems
- Core assumption: The model can implicitly learn to associate problem complexity with appropriate reasoning mode through the randomized training objective
- Evidence anchors: [section 4.1.3]: "As the wall density increases, indicating a (likely) more challenging maze, the proportion of slow-mode paths also increases"; [figure 4.1]: Shows increased slow-mode usage with higher wall density

## Foundational Learning

- **Tokenization and sequence modeling in encoder-decoder transformers**
  - Why needed here: Dualformer operates on tokenized sequences representing maze problems, A* traces, and solutions; understanding tokenization is essential for modifying the model
  - Quick check question: How does the model distinguish between prompt, trace, and solution tokens in the input sequence?

- **A* search algorithm and heuristic search**
  - Why needed here: The training data consists of A* search traces, and understanding the algorithm's structure is crucial for designing effective trace dropping strategies
  - Quick check question: What information is contained in create vs close clauses of A* search traces?

- **Randomization techniques in training (dropout, masking)**
  - Why needed here: The core innovation involves randomized trace dropping, which builds on understanding of how random masking affects model learning
  - Quick check question: How does trace dropping differ from standard token masking in terms of what is dropped and why?

## Architecture Onboarding

- **Component map**: Tokenized maze description + search trace + solution → encoder-decoder transformer with rotary positional embeddings → decoder autoregressive generation → output parsing for path extraction
- **Critical path**: Input tokenization → encoder processing → decoder autoregressive generation → output parsing for path extraction
- **Design tradeoffs**: Smaller model size (15M/46M) vs performance; randomized training vs standard supervised learning; single model vs separate fast/slow models
- **Failure signatures**: Model generates invalid paths (through walls); reasoning traces are too long/short; poor performance in either fast or slow mode
- **First 3 experiments**:
  1. Verify tokenization correctly represents maze problems and solutions
  2. Test basic generation with complete traces to ensure model learns from full data
  3. Evaluate fast mode performance with plan control token to confirm mode switching works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Dualformer's performance scale when trained on larger datasets and more complex environments beyond maze navigation and Sokoban?
- Basis in paper: [inferred] The paper mentions that future work could investigate whether the approach helps models scale to more complex tasks, suggesting this has not been thoroughly explored
- Why unresolved: The experiments were limited to specific grid-based navigation and planning tasks with fixed model sizes (15M and 46M parameters), without exploring scaling to larger models or more complex domains
- What evidence would resolve it: Systematic experiments scaling Dualformer to larger model sizes (e.g., 100M+ parameters), testing on more complex planning domains like robotics manipulation or multi-agent scenarios

### Open Question 2
- Question: What is the theoretical explanation for why structured trace dropping strategies improve both reasoning accuracy and efficiency in Dualformer?
- Basis in paper: [inferred] The paper demonstrates empirically that trace dropping works but does not provide a formal theoretical analysis of why this technique is effective
- Why unresolved: The paper relies on empirical observations and analogies to human System 1/System 2 thinking but lacks formal mathematical justification for why selectively dropping trace components improves learning outcomes
- What evidence would resolve it: Theoretical analysis showing how trace dropping affects gradient updates, formal proofs about information retention in training data, or empirical studies measuring information entropy changes during training

### Open Question 3
- Question: How does Dualformer's auto mode determine when to switch between fast and slow thinking, and can this decision-making process be made more interpretable?
- Basis in paper: [explicit] The paper shows that Dualformer automatically engages more slow thinking as task difficulty increases, but does not explain the internal mechanism or decision criteria
- Why unresolved: While the paper demonstrates that Dualformer can automatically select modes, it does not analyze the internal attention patterns, token generation probabilities, or other indicators that trigger mode switching
- What evidence would resolve it: Analysis of attention weights during auto mode generation, visualization of decision boundaries, or controlled experiments manipulating difficulty cues to understand what triggers mode selection

## Limitations
- Limited evidence of generalization to more complex, real-world reasoning problems beyond grid-world tasks
- Effectiveness depends heavily on specific trace dropping probabilities and strategies
- Diversity metrics focus on unique path generation rather than semantic diversity or solution quality variation

## Confidence
- **High Confidence**: Dualformer outperforms baselines in optimal rate and solution quality across all three modes; auto mode effectively adapts reasoning depth; structured trace dropping enables controllable fast/slow thinking
- **Medium Confidence**: Dualformer generates more diverse reasoning traces than Searchformer; approach generalizes to LLM fine-tuning for math reasoning; trace dropping strategies effectively simulate human-like reasoning shortcuts
- **Low Confidence**: Claims about handling complex, real-world reasoning problems; generalization to domains significantly different from grid-world navigation; long-term effectiveness on tasks requiring deep, sustained reasoning

## Next Checks
1. Conduct ablation study on trace dropping probabilities (p0-p4) to determine robustness to hyperparameter variations
2. Evaluate Dualformer on broader range of reasoning tasks (planning, logical inference, multi-step decision making) to assess cross-domain generalization
3. Perform detailed analysis of auto mode performance across wider range of problem complexities with more nuanced difficulty gradients