---
ver: rpa2
title: 'Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy
  Risk'
arxiv_id: '2403.09450'
source_url: https://arxiv.org/abs/2403.09450
tags:
- fine-tuning
- data
- private
- privacy
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals a new privacy risk in diffusion models, where
  fine-tuning with manipulated data can amplify the generative privacy risk. The authors
  demonstrate that various fine-tuning strategies, including concept-injection methods
  (DreamBooth and Textual Inversion) and
---

# Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk

## Quick Facts
- arXiv ID: 2403.09450
- Source URL: https://arxiv.org/abs/2403.09450
- Reference count: 40
- Authors: Zhangheng Li; Junyuan Hong; Bo Li; Zhangyang Wang
- Primary result: Fine-tuning diffusion models on synthetic data can amplify membership inference and data extraction attacks

## Executive Summary
This paper reveals a new privacy risk in diffusion models where fine-tuning with manipulated data can amplify generative privacy leakage. The authors demonstrate that various fine-tuning strategies, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient tuning (LoRA, Hypernetwork), can significantly increase the success of membership inference attacks and data extraction when the fine-tuning data is synthetically generated from target domain prompts. The phenomenon, termed "Shake-to-Leak" (S2L), shows that the fine-tuning process can force models into local optima that encode private domain information, making membership inference 5.4% more effective in AUC terms. The research highlights the need for new protective measures in diffusion model deployment and fine-tuning APIs.

## Method Summary
The study evaluates privacy risks in diffusion models through membership inference attacks (MIA) and data extraction attacks after fine-tuning. The method involves generating synthetic private sets by prompting a pre-trained Stable Diffusion model with target domain prompts (e.g., "The face of [Celebrity Name]"), then fine-tuning the model using various methods including DreamBooth, Textual Inversion, LoRA, and Hypernetwork. The privacy leakage is measured using MIA AUC, TPR@1%FPR, number of extracted examples with l2-distance < 0.15, and CLIP-RP score. The authors test these attacks on 40 celebrity domains from CelebA and evaluate different fine-tuning strategies to understand which amplify privacy risks most effectively.

## Key Results
- Fine-tuning on synthetic data similar to target domains amplifies MIA AUC by 5.4% absolute improvement
- Parameter-efficient fine-tuning methods (DreamBooth+LoRA) show higher privacy leakage than dense fine-tuning
- Gaussian random perturbation of small models can amplify privacy leakage without prior knowledge
- DreamBooth and Textual Inversion combined with LoRA achieve the highest extraction precision (over 88% for some domains)

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on synthetic data similar to target domain amplifies overfitting to private training examples. The diffusion model, when fine-tuned on synthetic data generated from the same prompts that define private domains, is pushed into local optima that strongly encode the distribution of those domains. This overfit representation makes membership inference and data extraction easier because the model's outputs are more discriminative with respect to the target domain. The synthetic data carries statistical patterns of the private domain due to conditioning on the same prompt. If the synthetic data does not match the private domain distribution (e.g., unrelated prompts), fine-tuning will not cause amplification.

### Mechanism 2
Parameter-efficient fine-tuning methods amplify privacy leakage more effectively than dense fine-tuning. By selectively updating only a subset of parameters (e.g., LoRA, Hypernetwork, embeddings), these methods concentrate the optimization pressure on specific model components that are most relevant to the target domain. This targeted adaptation increases the model's sensitivity to the domain's patterns, enhancing attack success. The chosen parameter subsets are sufficient to encode domain-specific features while maintaining generalization to the prompt. If too few parameters are updated (e.g., near-zero), amplification disappears.

### Mechanism 3
Gaussian random perturbation of small models can amplify privacy leakage without prior knowledge. For models with fewer parameters, random perturbations can accidentally move parameters into local optima that encode private domain information, increasing attack success. The likelihood increases as model size decreases because local optima are closer to global optima. Local optima corresponding to private domains are densely packed in parameter space for small models. For large models like SD-v1-1, random perturbations reduce leakage rather than amplify it.

## Foundational Learning

- **Concept: Diffusion model training objective and denoising process**
  - Why needed here: Understanding how diffusion models generate images conditioned on prompts is essential to grasp why synthetic data can mimic private domains.
  - Quick check question: What is the role of the noise schedule in the denoising process?

- **Concept: Membership inference attack (MIA) in generative models**
  - Why needed here: S2L's effectiveness is measured by MIA AUC improvement; understanding how MIA works on diffusion models is key.
  - Quick check question: How does SecMI use the l2 distance loss to infer membership?

- **Concept: Parameter-efficient fine-tuning (LoRA, Hypernetwork, DreamBooth)**
  - Why needed here: S2L integrates with these methods; knowing their mechanics explains why some amplify leakage more than others.
  - Quick check question: What is the difference between updating full weights vs. low-rank matrices in LoRA?

## Architecture Onboarding

- **Component map:** Pre-trained diffusion model -> Synthetic data generation -> Fine-tuning (DreamBooth/DI/LoRA/Hypernetwork) -> MIA and data extraction attacks
- **Critical path:** 1) Generate synthetic private set P using target prompts on the pre-trained model, 2) Fine-tune the model using chosen method on P, 3) Evaluate privacy leakage via MIA and data extraction
- **Design tradeoffs:** Dense vs. parameter-efficient fine-tuning (dense gives broader adaptation but less amplification; PEFT concentrates adaptation and increases leakage), Model scale (small models amplify leakage via random perturbations; large models require targeted fine-tuning)
- **Failure signatures:** No increase in MIA AUC after S2L (synthetic data does not match private domain), Decreased MIA AUC (fine-tuning caused catastrophic forgetting), Low data extraction precision (model does not overfit to private patterns despite MIA gain)
- **First 3 experiments:** 1) Replicate Table I: run S2L with DreamBooth+LoRA on SD-v1-1 and measure MIA AUC vs. baseline, 2) Test S2L with varying LoRA rank to confirm fewer parameters â†’ higher leakage, 3) Apply Gaussian attack on SDsm1 (small model) to verify amplification without prior knowledge

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the effectiveness of Shake-to-Leak (S2L) vary across different model architectures and sizes beyond Stable Diffusion?
  - Basis in paper: The paper primarily focuses on Stable Diffusion v1-1 but mentions exploring down-scaled models (SDsm1 and SDsm2) to understand the impact of model size on S2L effectiveness.
  - Why unresolved: The study's scope is limited to Stable Diffusion and its scaled-down versions, leaving the generalizability of S2L across various model architectures and sizes unexplored.
  - What evidence would resolve it: Systematic experiments evaluating S2L across a wide range of diffusion model architectures (e.g., DDPM, ADM, and their variants) and scales, including those trained on different datasets.

- **Open Question 2:** What are the long-term privacy implications of S2L when diffusion models are continuously fine-tuned with manipulated data over extended periods?
  - Basis in paper: The paper discusses the immediate effects of S2L but does not address the cumulative impact of repeated fine-tuning with manipulated data on the model's privacy risks over time.
  - Why unresolved: The study focuses on single instances of fine-tuning and does not explore the dynamics of privacy leakage as the model undergoes multiple rounds of fine-tuning with S2L.
  - What evidence would resolve it: Longitudinal studies tracking the privacy risks of diffusion models subjected to multiple rounds of S2L fine-tuning, including analyses of membership inference and data extraction capabilities over time.

- **Open Question 3:** How can we develop effective defenses against S2L that preserve the utility of diffusion models for legitimate fine-tuning tasks?
  - Basis in paper: The paper mentions the need for new protective measures but does not propose specific defense mechanisms against S2L.
  - Why unresolved: While the paper identifies the problem and its severity, it does not delve into the technical details of potential defenses.
  - What evidence would resolve it: Research demonstrating the effectiveness of proposed defense mechanisms (e.g., differential privacy techniques, secure fine-tuning APIs) in preventing S2L attacks while maintaining the diffusion models' ability to generate high-quality images.

## Limitations

- The fidelity of synthetic data to true private distribution is not empirically validated, introducing uncertainty about whether observed amplification is due to genuine overfitting or artifacts of synthetic generation.
- Lack of corpus evidence for proposed mechanisms, especially Gaussian perturbation amplification, means these explanations are speculative rather than established.
- The paper does not address potential defenses or whether S2L amplification persists across different model architectures beyond Stable Diffusion.

## Confidence

- **High confidence**: Empirical observation that parameter-efficient fine-tuning methods show higher MIA AUC and extraction success than dense fine-tuning (supported by ablation studies in Tables I and II).
- **Medium confidence**: Mechanism that synthetic data generated from target prompts amplifies overfitting is plausible but not definitively proven (lacks direct validation of synthetic data distribution matching).
- **Low confidence**: Gaussian random perturbation mechanism for small models lacks corpus support and is based solely on paper's own experiments with reduced model sizes.

## Next Checks

1. Validate synthetic data fidelity by comparing generated SP Sets to real CelebA samples using quantitative metrics (FID, CLIP similarity) to confirm they capture private domain characteristics before fine-tuning.

2. Test mechanism independence by applying S2L to a different generative model family (e.g., GANs or autoregressive models) to determine whether amplification is specific to diffusion models or a general phenomenon.

3. Evaluate defense effectiveness by applying differential privacy during fine-tuning or regularization techniques and measuring whether they mitigate the amplification effect observed with S2L.