---
ver: rpa2
title: 'LokiLM: Technical Report'
arxiv_id: '2407.07370'
source_url: https://arxiv.org/abs/2407.07370
tags:
- lokilm
- training
- language
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LokiLM is a 1.4B parameter language model trained on 500B tokens
  using multi-teacher knowledge distillation and high-quality data filtering. It achieves
  state-of-the-art performance among models under 1.5B parameters on standard benchmarks,
  outperforming larger models despite fewer training tokens.
---

# LokiLM: Technical Report

## Quick Facts
- arXiv ID: 2407.07370
- Source URL: https://arxiv.org/abs/2407.07370
- Authors: Justin Kiefel; Shrey Shah
- Reference count: 10
- Key outcome: 1.4B parameter model trained on 500B tokens achieves state-of-the-art performance among models under 1.5B parameters on standard benchmarks

## Executive Summary
LokiLM is a 1.4B parameter language model trained on 500B tokens using multi-teacher knowledge distillation and high-quality data filtering. It achieves state-of-the-art performance among models under 1.5B parameters on standard benchmarks, outperforming larger models despite fewer training tokens. However, it exhibits poor truthfulness and hallucinations, scoring low on the TruthfulQA benchmark. The model is not publicly released due to these issues. Key architectural optimizations include FlashAttention-2, ALiBi, RMSNorm, and SwiGLU activations.

## Method Summary
LokiLM employs a decoder-only Transformer architecture with 24 layers, 32 attention heads, and a hidden dimension of 2048. The model is trained on 500B tokens of web-scraped content, filtered through a multi-stage pipeline including semantic deduplication and density-based pruning, resulting in 250B filtered tokens. Training uses 8 NVIDIA A100 GPUs with 8-bit precision and FSDP, incorporating multi-teacher knowledge distillation every fourth batch using GPT-4, Mistral 7B, and Llama 2 13B. The model includes architectural optimizations like FlashAttention-2, ALiBi, RMSNorm, and SwiGLU activations, trained for 2 epochs.

## Key Results
- State-of-the-art performance among models under 1.5B parameters on ARC-C, HellaSwag, MMLU, Winogrande, and GSM8K benchmarks
- Achieves higher average score than larger models trained on significantly more tokens
- Scores only 11.3% on TruthfulQA benchmark, indicating significant truthfulness and hallucination issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-teacher knowledge distillation improves performance of small models by leveraging diverse teacher outputs.
- Mechanism: LokiLM uses GPT-4, Mistral 7B, and Llama 2 13B as teacher models to generate alternative correct sequences during training, which acts as regularization and helps the student model capture more complex knowledge representations.
- Core assumption: Student models can effectively learn from multiple teacher models even when there is a significant size gap.
- Evidence anchors:
  - [section] "We incorporate knowledge distillation [Hinton et al., 2015, Gu et al., 2023] every fourth training batch using GPT-4 [OpenAI et al., 2023], Mistral 7B [Jiang et al., 2023], and Llama 2 13B [Touvron et al., 2023b] as teacher models. Student models often struggle to represent the complexity when there is a significant size gap [Cho and Hariharan, 2019], so we avoid using only large models."
  - [corpus] Weak evidence - no specific corpus results on distillation effectiveness provided.
- Break condition: If the student model cannot effectively represent the knowledge from teacher models due to architecture limitations or if teacher models provide conflicting information.

### Mechanism 2
- Claim: High-quality data filtering and deduplication improves model performance despite fewer training tokens.
- Mechanism: LokiLM uses semantic deduplication (SemDeDup), density-based pruning, and transformer-based classifiers to filter training data, removing redundant or low-quality content while preserving diversity.
- Core assumption: Removing redundant and low-quality data while maintaining diversity leads to better generalization than simply increasing token count.
- Evidence anchors:
  - [section] "Our filtering pipeline incorporates various techniques to refine the dataset. To enhance data quality and diversity, we employ methods such as SemDeDup [Abbas et al., 2023] for semantic deduplication, semantic density-based pruning using Self-Supervised-Prototypes Pruning [Sorscher et al., 2023], and Transformer-based classifiers similar to those described by Gunasekar et al. [2023]."
  - [abstract] "LokiLM is trained using multi-teacher knowledge distillation and high-quality training data to achieve benchmark results competitive with larger models trained on significantly more tokens."
- Break condition: If the filtering process removes too much data, reducing coverage of important knowledge domains, or if the quality metrics used don't correlate with downstream performance.

### Mechanism 3
- Claim: Architectural optimizations enable efficient training and improved performance in small models.
- Mechanism: LokiLM incorporates FlashAttention-2 for faster attention computation, ALiBi for better positional information without positional embeddings, RMSNorm for training stability, and SwiGLU activations for learning complex representations.
- Core assumption: These architectural modifications provide measurable improvements in training efficiency and model quality.
- Evidence anchors:
  - [section] "LokiLM uses a standard decoder-only Transformer architecture [Vaswani et al., 2023] with 24 layers, 32 attention heads, and a hidden dimension of 2048. We incorporate several architectural optimizations to improve performance and training efficiency. FlashAttention-2 [Dao, 2023] is used to accelerate the multi-headed attention computation, while Attention with Linear Bias (ALiBi) [Press et al., 2022] replaces positional embeddings to better capture positional information. RMSNorm [Zhang and Sennrich, 2019] and SwiGLU activations [Shazeer, 2020] are employed to stabilize training and learn more complex representations [Zhang et al., 2024b], respectively."
  - [corpus] Weak evidence - no specific corpus results on individual architectural modifications provided.
- Break condition: If the architectural optimizations introduce instability or if the complexity added doesn't translate to measurable performance gains.

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: LokiLM uses multi-teacher knowledge distillation to transfer knowledge from larger models (GPT-4, Mistral 7B, Llama 2 13B) to the smaller 1.4B parameter model.
  - Quick check question: How does multi-teacher distillation differ from single-teacher distillation, and what are the benefits and risks of using multiple teachers?

- Concept: Data filtering and deduplication
  - Why needed here: LokiLM employs multiple filtering techniques (SemDeDup, density-based pruning, transformer-based classifiers) to ensure high-quality training data despite using web-scraped content.
  - Quick check question: What are the tradeoffs between data quality and data quantity in language model training, and how do different filtering approaches affect model performance?

- Concept: Transformer architecture optimizations
  - Why needed here: LokiLM incorporates FlashAttention-2, ALiBi, RMSNorm, and SwiGLU activations to improve training efficiency and model performance.
  - Quick check question: How do these specific architectural optimizations (FlashAttention-2, ALiBi, RMSNorm, SwiGLU) improve upon standard transformer implementations?

## Architecture Onboarding

- Component map: Data filtering pipeline -> Model training with knowledge distillation -> Architectural optimizations -> Evaluation and benchmarking
- Critical path: Data filtering → Model training with knowledge distillation → Architecture optimizations → Evaluation and benchmarking
- Design tradeoffs: Smaller model size (1.4B) vs. performance, fewer training tokens (500B) vs. data quality, multiple teacher models vs. complexity
- Failure signatures:
  - Loss spikes during training requiring checkpoint rollback
  - Poor performance on truthfulness benchmarks (TruthfulQA)
  - Difficulty in distinguishing incremental improvements due to variance in results
  - Hallucinations and generation of false information
- First 3 experiments:
  1. Compare model performance with and without knowledge distillation to quantify its impact
  2. Test different data filtering thresholds to find the optimal balance between data quality and quantity
  3. Evaluate the impact of individual architectural optimizations (e.g., ALiBi vs. positional embeddings) on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can deterministic evaluation techniques be developed to reliably distinguish between architectural improvements and random performance fluctuations in language model training?
- Basis in paper: [explicit] The paper explicitly identifies the challenge of distinguishing genuine architectural improvements from random fluctuations due to small performance margins compared to variance in results.
- Why unresolved: Current evaluation methodologies lack the precision to separate true architectural improvements from statistical noise, making it difficult to iterate effectively during model development.
- What evidence would resolve it: A systematic evaluation framework that demonstrates consistent differentiation between architectural variants with statistical significance, validated across multiple model families and scales.

### Open Question 2
- Question: What specific training modifications or architectural changes could effectively mitigate the truthfulness issues observed in LokiLM while maintaining its strong performance on reasoning tasks?
- Basis in paper: [explicit] The paper identifies LokiLM's poor performance on TruthfulQA benchmark and its tendency to generate false information, while maintaining strong reasoning capabilities.
- Why unresolved: Current approaches to improving truthfulness often compromise performance on other tasks, and the relationship between architectural choices and truthfulness remains unclear.
- What evidence would resolve it: An ablation study showing specific modifications that improve truthfulness scores without degrading performance on other benchmarks, with qualitative analysis of generated outputs.

### Open Question 3
- Question: How does the knowledge distillation process from multiple teacher models contribute to both the performance gains and truthfulness issues in LokiLM?
- Basis in paper: [explicit] The paper notes that knowledge distillation from GPT-4, Mistral 7B, and Llama 2 13B contributed to LokiLM's performance but may have amplified biases and misconceptions.
- Why unresolved: The specific mechanisms by which knowledge distillation affects different aspects of model behavior (performance vs. truthfulness) are not well understood.
- What evidence would resolve it: Controlled experiments comparing models trained with different combinations of teacher models, with detailed analysis of how each teacher contributes to various capabilities and failure modes.

## Limitations

- Model exhibits significant truthfulness issues, scoring only 11.3% on TruthfulQA benchmark
- Lack of public release prevents independent verification of benchmark results and investigation of truthfulness problems
- Insufficient analysis of why knowledge distillation from supposedly truthful teacher models resulted in poor truthfulness scores

## Confidence

**High confidence**: Technical specifications of model architecture and training setup are clearly specified and reproducible. Benchmark results showing LokiLM as state-of-the-art among models under 1.5B parameters on standard tasks appear reliable.

**Medium confidence**: Effectiveness of multi-teacher knowledge distillation approach and high-quality data filtering pipeline. While techniques are described, individual component contributions lack empirical evidence.

**Low confidence**: Interpretation that LokiLM's poor truthfulness is solely due to "generalization and complexity" of training data. Paper does not adequately explore alternative explanations.

## Next Checks

1. Replicate the knowledge distillation ablation study: Train identical models with single-teacher vs. multi-teacher distillation to quantify the specific contribution of using multiple teachers and investigate whether this approach introduces truthfulness issues.

2. Analyze the filtered dataset composition: Examine the actual content distribution of the 250B filtered tokens to determine if specific domains or types of content (particularly those prone to misinformation) dominate the training data, which could explain the truthfulness failures.

3. Test architectural optimization impact: Conduct controlled experiments isolating each architectural modification (FlashAttention-2, ALiBi, RMSNorm, SwiGLU) to determine which, if any, contribute to the truthfulness problems or could be modified to improve factual accuracy while maintaining performance.