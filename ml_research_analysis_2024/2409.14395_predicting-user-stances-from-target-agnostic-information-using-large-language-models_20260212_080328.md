---
ver: rpa2
title: Predicting User Stances from Target-Agnostic Information using Large Language
  Models
arxiv_id: '2409.14395'
source_url: https://arxiv.org/abs/2409.14395
tags:
- stance
- tweets
- user
- target-agnostic
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explores the use of large language models (LLMs) to
  predict user stances on targets using only target-agnostic social media posts. Two
  strategies are tested: direct user-level prediction and aggregated tweet-level predictions.'
---

# Predicting User Stances from Target-Agnostic Information using Large Language Models

## Quick Facts
- arXiv ID: 2409.14395
- Source URL: https://arxiv.org/abs/2409.14395
- Reference count: 17
- Primary result: LLMs predict user stances from target-agnostic posts with performance approaching stance detection using target-specific posts when given sufficient data (10-20 tweets).

## Executive Summary
This study investigates whether large language models (LLMs) can predict user stances on specific targets using only target-agnostic social media postsâ€”content that doesn't explicitly mention the target. Two strategies are tested: direct user-level prediction (passing all user tweets at once) and aggregated tweet-level predictions (passing tweets individually and combining results). LLMs outperform traditional machine learning methods and achieve comparable performance to stance detection using target-specific posts when provided with sufficient target-agnostic data. Post-hoc analyses suggest that LLMs leverage both surface-level signals (target-relevant keywords) and user-level features (moral values) encoded in target-agnostic posts to make accurate predictions.

## Method Summary
The study uses target-agnostic tweets from 1,000 Twitter users collected between April and December 2020, covering three topics: Donald Trump in the 2020 election, wearing masks during COVID-19, and racial equality. Each user had 50-150 target-agnostic tweets. Two LLM-based strategies were tested: ULSP-LLM (passing all user tweets at once) and ULSP-LLM (pooled) (passing tweets individually and aggregating predictions). Non-LLM methods (TF-IDF + logistic regression, SBERT + logistic regression, SBERT + random forest) were also used for comparison. Models were evaluated on a 500-user test set, with performance improving as the number of tweets per user increased.

## Key Results
- LLMs outperform traditional ML methods for stance prediction from target-agnostic posts
- ULSP-LLM (user-level prediction) performs best overall, while ULSP-LLM (pooled) shows systematic biases
- Performance converges to stance detection levels with 10-20 target-agnostic tweets per user
- Post-hoc analysis reveals moral foundation differences between users with opposing stances on racial equality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs leverage moral foundation inferences from target-agnostic posts to predict stance.
- Mechanism: LLMs encode user-level moral values through extended Moral Foundation Dictionary (eMFD) scores extracted from concatenated target-agnostic tweets, allowing stance prediction even without explicit target references.
- Core assumption: Moral value systems inferred from past tweets are predictive of stance on new, unseen targets.
- Evidence anchors:
  - [abstract] "Post-hoc analyses further hint at the usefulness of target-agnostic posts in providing relevant information to LLMs through the presence of both surface-level (e.g., target-relevant keywords) and user-level features (e.g., encoding users' moral values)."
  - [section] "we further conducted a post-hoc analysis to examine if users who differed in their stance on a given topic reflected systematic differences in individual-level factors (e.g., moral considerations) as inferred from their target-agnostic tweets... supporters of 'Racial Equality' scored lower on these three dimensions, reflecting a lower tendency to generate content associated with cruelty, injustice, and betrayal compared to their counterparts who oppose 'Racial Equality'."

### Mechanism 2
- Claim: Target-agnostic tweets contain implicit topical relevance that LLMs can detect.
- Mechanism: Even without explicit target mentions, target-agnostic tweets may include keywords or concepts related to the target (e.g., tweets about conservative values predicting stance on Donald Trump).
- Core assumption: Target-agnostic tweets contain latent topical signals that correlate with stance.
- Evidence anchors:
  - [abstract] "Post-hoc analyses further hint at the usefulness of target-agnostic posts in providing relevant information to LLMs through the presence of both surface-level (e.g., target-relevant keywords)"
  - [section] "We observed a significant negative relationship in the 'care.vice,' 'loyalty.vice,' and 'fairness.vice' scores of target-agnostic tweets and stance on 'Racial Equality'."

### Mechanism 3
- Claim: LLMs achieve stance prediction parity with stance detection through sufficient volume of target-agnostic data.
- Mechanism: As the number of target-agnostic posts increases, LLMs accumulate enough contextual information to make accurate stance predictions comparable to stance detection using target-specific posts.
- Core assumption: Information about user stance accumulates with each additional target-agnostic post.
- Evidence anchors:
  - [abstract] "the performance of stance prediction quickly converges to the performance of stance detection as we increase the number of target-agnostic posts provided to the model for stance prediction."
  - [section] "the performance of ULSP - LLM eventually caught up to that of stance detection given sufficient tweets (10 or 20), demonstrating that LLMs can successfully carry out stance prediction given a relatively small number of target-agnostic tweets (10 or 20), at a level that matches stance detection when target-specific tweets are available."

## Foundational Learning

- Concept: Target-agnostic vs. target-specific tweets
  - Why needed here: Understanding the distinction is critical for grasping the novelty of predicting stance without explicit target mentions
  - Quick check question: What is the key difference between target-agnostic and target-specific tweets in the context of stance detection?

- Concept: Zero-shot learning
  - Why needed here: The study uses zero-shot methods, which don't require training on labeled data for each new target
  - Quick check question: How does zero-shot stance prediction differ from traditional supervised stance detection?

- Concept: Moral Foundation Theory
  - Why needed here: The study uses moral foundation scores (eMFD) to analyze user-level differences in stance
  - Quick check question: What are the five moral foundations described in Moral Foundation Theory, and how might they relate to stance prediction?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Feature extraction (TF-IDF, SBERT) -> LLM prompting -> Prediction aggregation -> Evaluation

- Critical path:
  1. Collect target-agnostic tweets for each user
  2. Prompt LLM with user tweets and stance target
  3. Aggregate LLM predictions (either user-level or tweet-level)
  4. Evaluate performance against ground truth stance labels

- Design tradeoffs:
  - User-level vs. pooled prediction strategy: User-level may capture more context but could be limited by prompt length; pooled allows more granular analysis but may lose context
  - Number of tweets: More tweets provide more information but increase computational cost and may introduce noise

- Failure signatures:
  - Poor performance on certain stance targets (e.g., "Donald Trump" vs. "Wearing Masks")
  - Large performance gap between user-level and pooled prediction strategies
  - Lack of improvement with increasing number of tweets

- First 3 experiments:
  1. Compare user-level vs. pooled prediction strategies on a small subset of data
  2. Test performance with varying numbers of target-agnostic tweets (e.g., 5, 10, 20, 50)
  3. Evaluate the impact of including/excluding target-relevant keywords in the analysis

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to three stance targets from a single dataset, constraining generalizability
- Performance varies substantially across different stance targets
- Cannot establish causation between moral value encoding and stance prediction accuracy
- Focus on English-language tweets from a specific time period (2020)

## Confidence
**High Confidence**: The core finding that LLMs can predict user stances from target-agnostic posts with performance approaching that of stance detection using target-specific posts, when provided with sufficient data (10-20 tweets).

**Medium Confidence**: The claim that moral foundation inferences and implicit topical relevance are the primary mechanisms enabling target-agnostic stance prediction.

**Low Confidence**: The assertion that this approach can be reliably applied to new, unseen topics beyond the three tested.

## Next Checks
1. **Cross-Domain Validation**: Test the approach on stance targets from different domains (e.g., climate change, healthcare policy, technology adoption) and cultural contexts to assess generalizability beyond the political and pandemic-related topics studied.

2. **Ablation on Implicit Signals**: Conduct systematic ablation studies to isolate the contribution of moral foundation signals, target-relevant keywords, and other linguistic features to stance prediction accuracy, using techniques like partial input masking or feature permutation importance.

3. **Novel Topic Generalization**: Evaluate performance on stance targets that were not represented in the target-agnostic training data at all, to test whether the approach truly enables stance prediction for genuinely new topics versus topics that share linguistic or conceptual overlap with the training data.