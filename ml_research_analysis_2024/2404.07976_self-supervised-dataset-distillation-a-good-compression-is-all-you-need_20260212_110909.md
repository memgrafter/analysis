---
ver: rpa2
title: 'Self-supervised Dataset Distillation: A Good Compression Is All You Need'
arxiv_id: '2404.07976'
source_url: https://arxiv.org/abs/2404.07976
tags:
- dataset
- data
- distillation
- self-supervised
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a self-supervised dataset distillation approach
  that achieves state-of-the-art performance by leveraging self-supervised pretraining
  to generate more informative synthetic data. The method addresses the challenge
  of compressing large datasets into compact synthetic versions while preserving essential
  information.
---

# Self-supervised Dataset Distillation: A Good Compression Is All You Need

## Quick Facts
- arXiv ID: 2404.07976
- Source URL: https://arxiv.org/abs/2404.07976
- Reference count: 40
- Self-supervised dataset distillation approach achieves state-of-the-art performance on CIFAR-100, Tiny-ImageNet, and ImageNet-1K

## Executive Summary
This work introduces a self-supervised dataset distillation framework that leverages self-supervised pretraining to generate more informative synthetic data. The approach addresses the challenge of compressing large datasets into compact synthetic versions while preserving essential information. By utilizing self-supervised learning, the method amplifies supervision signals and improves scalability compared to traditional supervised methods. Extensive experiments demonstrate superior performance, surpassing previous state-of-the-art methods by significant margins.

## Method Summary
The method involves three main phases: (1) pretraining a backbone model using self-supervised learning objectives, (2) applying linear probing to align the pretrained model to the target dataset without fine-tuning the backbone, and (3) generating synthetic data using cross-entropy loss and batch normalization matching loss with the pretrained model. The framework is validated by training models on synthetic data and evaluating them on the original validation set.

## Key Results
- Outperforms previous state-of-the-art methods by significant margins on CIFAR-100, Tiny-ImageNet, and ImageNet-1K
- Demonstrates positive correlation between model size and performance
- Shows superior cross-architecture generalization capabilities
- Achieves better scalability compared to traditional supervised methods

## Why This Works (Mechanism)

### Mechanism 1
- Self-supervised pretraining amplifies the variance of BN statistics, which makes loss signals larger and more informative for data synthesis.
- Core assumption: Larger variance in BN statistics correlates with better data synthesis performance.
- Evidence: Observations that supervised pretraining causes BN mean/variance distributions to flatten, while self-supervised pretraining preserves larger variance.

### Mechanism 2
- Self-supervised pretraining provides better generalization across architectures and tasks than supervised pretraining.
- Core assumption: Self-supervised representations generalize better than supervised ones for downstream tasks.
- Evidence: The framework's effectiveness across different model architectures and its superior cross-architecture generalization.

### Mechanism 3
- Linear probing after self-supervised pretraining preserves informative BN distributions better than fine-tuning.
- Core assumption: Linear probing preserves BN statistics better than fine-tuning.
- Evidence: The approach keeps the backbone frozen while focusing on learning the classifier, effectively separating intermediate feature distributions from higher-level semantic alignment.

## Foundational Learning

- Concept: Batch Normalization statistics and their role in training dynamics
  - Why needed here: The paper's core mechanism relies on BN statistics variance affecting synthesis quality
  - Quick check: How do BN statistics change during supervised vs self-supervised pretraining, and why does this matter for data synthesis?

- Concept: Self-supervised learning objectives and their effects on feature representations
  - Why needed here: The approach depends on understanding how different pretraining objectives affect learned representations
  - Quick check: What are the key differences between contrastive and supervised pretraining in terms of feature space geometry?

- Concept: Dataset distillation optimization objectives and their relationship to model informativeness
  - Why needed here: The framework's effectiveness depends on understanding how compression affects subsequent learning
  - Quick check: How does the quality of synthetic data relate to the compression method used during pretraining?

## Architecture Onboarding

- Component map: Pretraining phase -> Recovery phase -> Validation phase
- Critical path: 1) Pretrain backbone with self-supervised objective, 2) Apply linear probing for dataset alignment, 3) Generate synthetic data using BN statistics matching, 4) Validate synthetic data quality
- Design tradeoffs: Pretraining budget vs synthesis quality, model size vs performance, BN statistics matching vs other objectives
- Failure signatures: Poor synthesis quality, architecture-specific issues, scaling problems
- First 3 experiments: 1) Compare BN statistics variance between supervised and self-supervised pretraining, 2) Test linear probing vs fine-tuning for preserving BN distributions, 3) Validate cross-architecture performance of synthetic data from different pretraining methods

## Open Questions the Paper Calls Out

### Open Question 1
- How does the specific architecture of the self-supervised pretraining model affect the informativeness of BN statistics for dataset distillation?
- Basis: The paper shows ViT-Tiny achieves significantly higher accuracy compared to SRe2L
- Why unresolved: The paper demonstrates architecture's importance but doesn't analyze why certain architectures perform better
- Resolution evidence: Controlled study comparing BN statistics informativeness across diverse architectures with identical pretraining objectives

### Open Question 2
- What is the theoretical relationship between the magnitude of BN statistic variance and the quality of generated synthetic data?
- Basis: The paper proves SSL models have higher entropy in BN statistics but doesn't establish a direct quantitative link
- Why unresolved: Establishes statistical difference but doesn't empirically or theoretically quantify how variance translates to synthetic data quality
- Resolution evidence: Regression analysis correlating BN variance values with downstream dataset distillation metrics

### Open Question 3
- Can the SC-DD framework be extended to handle continual learning scenarios with incremental updates?
- Basis: The paper focuses on static dataset distillation but the approach could theoretically adapt to new data distributions
- Why unresolved: Doesn't explore incremental updates or lifelong learning scenarios
- Resolution evidence: Experiments demonstrating SC-DD's performance on sequential dataset distillation tasks with gradually introduced new classes

## Limitations
- Claims about BN statistics variance correlation with synthesis quality lack direct empirical validation
- Mechanism connecting self-supervised pretraining to better synthetic data quality remains partially theoretical
- Limited systematic evaluation across diverse architectures

## Confidence

| Claim | Confidence |
|-------|------------|
| Superior performance on CIFAR-100, Tiny-ImageNet, and ImageNet-1K | High |
| BN statistics variance mechanism | Medium |
| Cross-architecture generalization | Medium |

## Next Checks
1. Conduct ablation studies varying BN statistics variance independently to isolate its effect on synthesis quality
2. Test synthetic data performance across a wider range of model architectures including non-convolutional networks
3. Perform controlled experiments comparing self-supervised pretraining on different dataset sizes to disentangle BN statistics preservation effects from pretraining data volume