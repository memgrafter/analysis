---
ver: rpa2
title: Cogs in a Machine, Doing What They're Meant to Do -- The AMI Submission to
  the WMT24 General Translation Task
arxiv_id: '2410.03381'
source_url: https://arxiv.org/abs/2410.03381
tags:
- translation
- data
- machine
- sentence
- icelandic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the AMI team's submission to the WMT24 General\
  \ Translation Task for English\u2192Icelandic. The system uses an ensemble of four\
  \ Transformer models (Base, Basedeep, Big, Bigdeep) and a grammatical error correction\
  \ model."
---

# Cogs in a Machine, Doing What They're Meant to Do -- the AMI Submission to the WMT24 General Translation Task

## Quick Facts
- arXiv ID: 2410.03381
- Source URL: https://arxiv.org/abs/2410.03381
- Reference count: 40
- Primary result: AMI team achieved chrF score of 58.4 on WMT21 test set for English→Icelandic translation

## Executive Summary
This paper presents the AMI team's submission to the WMT24 General Translation Task for English→Icelandic. The system uses an ensemble of four Transformer models (Base, Basedeep, Big, Bigdeep) and a grammatical error correction model. Key components include aggressive data filtering to remove noisy sentence pairs and the use of synthetic data generated by the ALMA-R 13B LLM, which significantly improved translation quality. The final system achieved a chrF score of 58.4 on the WMT21 test set. The approach demonstrates that traditional Transformer models remain competitive for MT when combined with careful data curation and LLM-generated synthetic data.

## Method Summary
The AMI system uses a multi-stage pipeline: aggressive filtering of parallel corpora using LaBSE, NMTScore, and other heuristics; training four Transformer models of different sizes on filtered data; generating synthetic data with ALMA-R 13B LLM; ensemble decoding with 5-beam search hypotheses per model; COMET KIWI reranking to select best candidates; and final grammatical error correction with ByT5. The system processes 60 million sentence pairs through careful filtering, combines human-translated and synthetic data, and uses ensemble methods with reranking to achieve high-quality translations.

## Key Results
- Achieved chrF score of 58.4 on WMT21 test set
- Synthetic data generated by ALMA-R 13B LLM significantly improved translation capability
- Aggressive filtering of noisy sentence pairs improved model quality
- Ensemble approach with COMET KIWI reranking outperformed individual models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggressive filtering improves model quality by removing noisy sentence pairs that introduce translation errors.
- Mechanism: Noisy training data containing mistranslations, misalignments, and untranslated segments degrades translation quality; aggressive filtering removes these problematic pairs before training.
- Core assumption: Filtering reduces noise without removing too much useful training data, maintaining sufficient data volume for effective learning.
- Evidence anchors:
  - [abstract] "aggressively filtering out sentence pairs that may detrimentally affect the quality of our system's output"
  - [section 3] "Khayrallah and Koehn (2018) show that incorrect translations, untranslated target text, misalignments, and other noisy segments in training data can have a detrimental effect on NMT systems"
  - [corpus] Weak - corpus shows related papers on causality and counterfactual explanations, not directly on filtering mechanisms
- Break condition: Over-filtering removes too many useful sentence pairs, reducing training data below critical mass needed for model convergence.

### Mechanism 2
- Claim: Synthetic data generated by LLM (ALMA-R 13B) significantly improves translation capability by providing high-quality parallel examples.
- Mechanism: LLM-generated translations provide additional training examples that capture linguistic patterns not present in limited human-translated data, expanding the effective training corpus size and diversity.
- Core assumption: LLM-generated synthetic data maintains sufficient quality standards when filtered, and the additional examples provide meaningful learning signals beyond existing parallel data.
- Evidence anchors:
  - [abstract] "A part of the synthetic data is generated using an LLM, and we find that it increases the translation capability of our system significantly"
  - [section 3.3] "We use the 13B parameter ALMA-R model to translate English sentences from Newscrawl to Icelandic and Icelandic texts from the Icelandic Gigaword Corpus to English"
  - [corpus] Weak - corpus shows LLM applications in translation but not specifically synthetic data generation
- Break condition: LLM-generated data quality degrades below filtering thresholds, or synthetic examples introduce systematic translation biases.

### Mechanism 3
- Claim: Ensemble approach with multiple Transformer models and reranking improves translation quality by combining complementary strengths of different model architectures.
- Mechanism: Different Transformer architectures (Base, Big, deeper variants) capture different linguistic patterns; combining their outputs and selecting best candidates through COMET KIWI reranking produces superior translations compared to any single model.
- Core assumption: Different models have complementary strengths and weaknesses, and the reranking model can effectively identify the best translation from the ensemble pool.
- Evidence anchors:
  - [abstract] "Our system comprises four translation models and a grammar correction model"
  - [section 4] "Using the WMT21 test set we experiment with an ensemble approach, using COMET KIWI to select the best sentence out of 20 hypotheses made by the four models"
  - [section 4.1] "Using each of our four models, we generate five translation hypotheses... resulting in a total of 20 candidates"
  - [corpus] Weak - corpus shows ensemble approaches in other contexts but not specifically for MT reranking
- Break condition: Models become too similar in performance, reducing complementary benefits, or reranking model fails to consistently select high-quality translations.

## Foundational Learning

- Concept: Parallel corpus filtering and quality assessment
  - Why needed here: The system relies on carefully curated training data, requiring understanding of filtering techniques and quality metrics
  - Quick check question: What are the key differences between LaBSE, NMTScore, and W AScore for sentence pair filtering?

- Concept: Transformer architecture and model scaling
  - Why needed here: The system uses multiple Transformer variants (Base, Big, deep versions) with different configurations
  - Quick check question: How do model depth (number of layers) and width (model dimension) affect translation quality and computational requirements?

- Concept: Synthetic data generation and evaluation
  - Why needed here: The system incorporates LLM-generated synthetic data as a key training component
  - Quick check question: What are the trade-offs between using LLM-generated synthetic data versus human-translated parallel data?

## Architecture Onboarding

- Component map: Four Transformer models (Base, Basedeep, Big, Bigdeep) → Beam search generation (5 hypotheses each) → COMET KIWI reranking → Grammar correction (ByT5) → Final translation selection
- Critical path: Data preprocessing → Model training → Translation generation → Post-processing → Evaluation
- Design tradeoffs: Computational efficiency vs. translation quality, aggressive filtering vs. data retention, model diversity vs. consistency
- Failure signatures: Low chrF scores indicate filtering problems or model quality issues; slow inference suggests inefficient model configurations; inconsistent translations across models indicate training instability
- First 3 experiments:
  1. Train single Base Transformer on filtered ParIce data and evaluate chrF on WMT21 test set
  2. Add LLM-generated synthetic data to training and compare chrF improvement
  3. Implement ensemble with two models and COMET KIWI reranking, measure chrF gain over single model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal combination of LaBSE, LASER, and NMTScore thresholds for filtering noisy sentence pairs in English-Icelandic MT training data?
- Basis in paper: [explicit] The authors mention they use individual cutoff values for each score but suggest using a classifier to combine all metrics for optimal results
- Why unresolved: The paper only states they used LaBSE threshold of 0.8 and NMTScore threshold of 0.4 without exploring optimal combinations or classifier-based approaches
- What evidence would resolve it: Systematic experiments testing different threshold combinations and classifier-based approaches on validation data, measuring their impact on final translation quality

### Open Question 2
- Question: How does the quality of synthetic data generated by LLMs compare to human-translated data for low-resource language pairs like English-Icelandic?
- Basis in paper: [explicit] The authors found that ALMA-R 13B synthetic data significantly improved translation quality, but they don't compare it directly to human-translated data quality
- Why unresolved: The paper doesn't include a direct comparison between LLM-generated synthetic data and human-translated parallel data in terms of translation quality impact
- What evidence would resolve it: Comparative experiments training models with equivalent amounts of human-translated vs LLM-generated synthetic data, measuring final translation quality

### Open Question 3
- Question: What is the impact of including bilingual lexicon data using token-pair training on MT model performance for English-Icelandic?
- Basis in paper: [explicit] The authors experimented with token-pair training from a bilingual lexicon but found it negatively affected output, without investigating why
- Why unresolved: The paper doesn't explore potential reasons for the negative impact or alternative approaches to incorporating lexicon data
- What evidence would resolve it: Controlled experiments testing different lexicon integration methods, analysis of which types of lexicon entries cause problems, and investigation of preprocessing steps to make lexicon data more compatible with MT training

## Limitations
- Filtering methodology relies on threshold-based decisions without sensitivity analysis
- LLM-generated synthetic data quality assessment limited to filtering metrics rather than direct human evaluation
- Ensemble effectiveness assumption of complementary strengths not empirically validated

## Confidence
- High confidence: The ensemble approach with COMET KIWI reranking improving translation quality
- Medium confidence: The effectiveness of aggressive filtering in removing noisy data
- Medium confidence: The LLM-generated synthetic data significantly improving translation capability

## Next Checks
1. **Filter threshold sensitivity analysis**: Systematically vary LaBSE and NMTScore thresholds to identify optimal filtering parameters and quantify the trade-off between data retention and noise reduction
2. **LLM synthetic data quality evaluation**: Conduct human evaluation of a random sample of LLM-generated translations to assess whether filtered synthetic data maintains sufficient quality standards beyond automated metrics
3. **Model diversity validation**: Analyze pairwise model similarity scores and correlation of COMET KIWI rankings across models to empirically verify that the ensemble components have complementary strengths rather than redundant capabilities