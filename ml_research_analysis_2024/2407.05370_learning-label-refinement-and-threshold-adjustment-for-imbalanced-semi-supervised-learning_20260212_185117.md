---
ver: rpa2
title: Learning Label Refinement and Threshold Adjustment for Imbalanced Semi-Supervised
  Learning
arxiv_id: '2407.05370'
source_url: https://arxiv.org/abs/2407.05370
tags:
- learning
- seval
- class
- data
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of semi-supervised learning
  (SSL) in imbalanced scenarios, where pseudo-labels generated by existing methods
  tend to be biased towards the majority class, leading to amplified class imbalance.
  The authors propose SEVAL, a method that optimizes pseudo-label refinement and threshold
  adjustment using a small, separate labeled dataset (validation set).
---

# Learning Label Refinement and Threshold Adjustment for Imbalanced Semi-Supervised Learning

## Quick Facts
- arXiv ID: 2407.05370
- Source URL: https://arxiv.org/abs/2407.05370
- Reference count: 16
- Authors: Zeju Li; Ying-Qiu Zheng; Chen Chen; Saad Jbabdi
- Primary result: SEVAL achieves higher accuracy and more balanced pseudo-labels in imbalanced SSL scenarios

## Executive Summary
This paper addresses the challenge of semi-supervised learning (SSL) in imbalanced scenarios, where pseudo-labels generated by existing methods tend to be biased towards the majority class, leading to amplified class imbalance. The authors propose SEVAL, a method that optimizes pseudo-label refinement and threshold adjustment using a small, separate labeled dataset (validation set). SEVAL learns class-specific offsets to refine pseudo-label logits and class-specific thresholds to select high-quality pseudo-labels. The method is theoretically grounded and does not rely on uncalibrated model confidence or heuristic strategies. Experiments on CIFAR-10-LT, CIFAR-100-LT, STL-10-LT, and Semi-Aves show that SEVAL outperforms state-of-the-art SSL methods, achieving higher accuracy and more balanced pseudo-labels. The code is publicly available.

## Method Summary
SEVAL addresses imbalanced SSL by learning class-specific offsets (π) to refine pseudo-label logits and class-specific thresholds (τ) to select high-quality pseudo-labels using a small validation set. The method partitions labeled training data into X' (for SSL) and V (for parameter learning), then sequentially estimates optimal parameters through curriculum learning with exponential moving average updates. This approach improves pseudo-label quality by aligning them with the expected test distribution and selecting samples based on precision rather than recall, resulting in better trade-offs between quantity and quality of pseudo-labels.

## Key Results
- SEVAL outperforms state-of-the-art SSL methods on CIFAR-10-LT, CIFAR-100-LT, STL-10-LT, and Semi-Aves datasets
- Achieves higher accuracy and more balanced pseudo-labels compared to existing methods
- Demonstrates effectiveness with as few as 200 labeled samples in extreme imbalance scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SEVAL improves pseudo-label quality by learning class-specific offsets that refine decision boundaries using a small labeled validation set.
- Mechanism: Instead of relying on uncalibrated model confidence or heuristic strategies, SEVAL estimates optimal logit offsets π that align pseudo-labels with the expected test distribution. The offsets are learned by minimizing cross-entropy loss on a validation set V, making the adjustment task-specific and data-driven.
- Core assumption: The validation set V has the same class-conditional likelihood as the unlabeled data, enabling it to act as a proxy for the test distribution.
- Evidence anchors:
  - [abstract] SEVAL learns class-specific offsets to refine pseudo-label logits and class-specific thresholds to select high-quality pseudo-labels.
  - [section 4.1] π∗ = arg minπ 1/K Σ H(yi, σ(zV i − log π)), optimizing refinement using holdout data.
  - [corpus] SEVAL does not rely on uncalibrated model confidence or heuristic strategies.

### Mechanism 2
- Claim: SEVAL selects high-quality pseudo-labels by learning class-specific thresholds that prioritize precision over recall.
- Mechanism: SEVAL optimizes thresholds τc so that selected samples per class achieve the same accuracy t, rather than relying on maximum class probability which estimates recall. This ensures a better trade-off between quantity and quality of pseudo-labels.
- Core assumption: Precision is a better indicator of pseudo-label quality than recall for threshold selection, and class-wise precision can be approximated using the validation set.
- Evidence anchors:
  - [abstract] SEVAL learns class-specific thresholds to select high-quality pseudo-labels.
  - [section 3.3] Lemma 4: A better threshold τ for choosing effective pseudo-labels should be derived from class-wise Precision, instead of class-wise Recall.
  - [section 5.6] SEVAL learns τc to be low for classes that have high Precision, improving overall accuracy.

### Mechanism 3
- Claim: SEVAL's sequential curriculum learning of π and τ stabilizes the refinement and thresholding process, improving generalization.
- Mechanism: SEVAL first partitions the labeled training data into X' and V', learns π and τ from V' using exponential moving average to update parameters smoothly over L curriculum steps, and then proceeds with standard SSL using the learned curriculum.
- Core assumption: Learning a smooth curriculum over multiple iterations using exponential moving average prevents overfitting to the validation set and improves stability.
- Evidence anchors:
  - [section 4.3] π(l) = ηππ(l−1) + (1 − ηπ)π(l)∗ and τ (l) = ητ τ (l−1) + (1 − ητ)τ (l)∗ update parameters with momentum decay.
  - [section 5.4.1] SEVAL consistently delivers a positive Gain(iter) throughout training iterations, indicating stable improvement.
  - [section 5.5.2] SEVAL consistently identifies similar π and τ even with varied numbers of validation samples, showing data-efficiency.

## Foundational Learning

- Concept: Long-tailed learning and class imbalance
  - Why needed here: The paper addresses class imbalance in semi-supervised learning, where pseudo-labels tend to be biased towards the majority class.
  - Quick check question: What is the imbalance ratio γ and how does it affect pseudo-label generation in SSL?

- Concept: Pseudo-labeling and threshold-based selection
  - Why needed here: Pseudo-labels are central to semi-supervised learning; threshold adjustment ensures only high-confidence samples are used for training.
  - Quick check question: How does maximum class probability relate to recall and precision in threshold selection?

- Concept: Model calibration and uncertainty estimation
  - Why needed here: The paper highlights that uncalibrated model confidence leads to unreliable pseudo-labels; calibration is critical for effective threshold adjustment.
  - Quick check question: Why is precision preferred over recall for deriving thresholds in SEVAL?

## Architecture Onboarding

- Component map:
  - Input: Labeled training data X, unlabeled data U, validation set V
  - Core: Network f for classification, SEVAL parameter estimation (π, τ)
  - Output: Refined pseudo-labels and adjusted thresholds for SSL training

- Critical path:
  1. Partition X into X' and V'
  2. Estimate π∗ and τ ∗ using V' (Algorithm 1)
  3. Apply curriculum learning to update π and τ over L steps
  4. Train SSL model using refined pseudo-labels and adjusted thresholds

- Design tradeoffs:
  - Using a validation set V adds a small overhead but significantly improves pseudo-label quality.
  - Sequential optimization of π and τ vs joint optimization: sequential is simpler and more stable.
  - Class-specific vs global thresholds: class-specific adapts better to imbalance but increases complexity.

- Failure signatures:
  - Pseudo-labels remain biased towards majority class: likely π not well learned or V distribution mismatch.
  - High variance in performance: thresholds τ may be unstable due to small or imbalanced V.
  - No improvement over baseline: curriculum length L too short or momentum decay ηπ, ητ too high.

- First 3 experiments:
  1. Run SEVAL-PL on CIFAR-10-LT with γl = γu = 100, n1 = 500, compare accuracy to DARP and FlexMatch.
  2. Test SEVAL threshold adjustment by visualizing class-wise Precision vs learned τc on CIFAR-10-LT.
  3. Stress test SEVAL with minimal labeled data (n1 = 200) on CIFAR-10-LT, assess robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SEVAL's performance scale with different types of label distribution shifts between training and test data beyond simple prior shifts?
- Basis in paper: [explicit] The paper assumes PX (X|Y ) = PT (X|Y ) and only considers prior shifts in Proposition 1 and Corollary 2
- Why unresolved: The theoretical analysis only covers cases where class conditional distributions remain the same but priors differ. Real-world scenarios may involve more complex distribution shifts
- What evidence would resolve it: Systematic experiments testing SEVAL's performance under various types of label distribution shifts (covariate shift, concept drift, etc.) and comparing it to methods that explicitly handle such shifts

### Open Question 2
- Question: What is the optimal curriculum length L for SEVAL across different datasets and imbalance scenarios?
- Basis in paper: [inferred] The paper mentions using curriculum learning with parameter L but doesn't provide systematic analysis of how L affects performance
- Why unresolved: While the paper demonstrates SEVAL works well, it doesn't explore how the length of the curriculum impacts effectiveness or provide guidelines for selecting L
- What evidence would resolve it: Extensive experiments varying L across multiple datasets and imbalance ratios, along with analysis of the trade-off between computational cost and performance improvement

### Open Question 3
- Question: Can SEVAL's framework be extended to handle open-set semi-supervised learning where unlabeled data contains classes not present in the labeled set?
- Basis in paper: [explicit] The paper mentions Semi-Aves as a dataset with open-set data but doesn't propose modifications to SEVAL's framework to handle this case
- Why unresolved: The current SEVAL framework assumes all unlabeled data belongs to classes present in the labeled set, making it unclear how it would handle open-set scenarios
- What evidence would resolve it: Modified version of SEVAL that incorporates open-set detection mechanisms and experiments demonstrating its effectiveness on datasets with significant open-set components

## Limitations
- The method requires a separate validation set, which may not always be available in true SSL scenarios
- Performance depends on the validation set being representative of the test distribution
- The sequential curriculum approach may not scale well to very large datasets or many classes

## Confidence
- Mechanism 1 (logit refinement): High - well-supported by theory and experiments
- Mechanism 2 (threshold adjustment): Medium - strong theoretical grounding but empirical validation could be deeper
- Overall method effectiveness: High - consistent improvements across multiple benchmarks

## Next Checks
1. Test SEVAL with varying validation set sizes to quantify data-efficiency requirements
2. Evaluate performance when validation set distribution differs from test set
3. Compare SEVAL against joint optimization approaches for π and τ