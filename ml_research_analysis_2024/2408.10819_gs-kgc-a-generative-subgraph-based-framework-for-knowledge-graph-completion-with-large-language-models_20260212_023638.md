---
ver: rpa2
title: 'GS-KGC: A Generative Subgraph-based Framework for Knowledge Graph Completion
  with Large Language Models'
arxiv_id: '2408.10819'
source_url: https://arxiv.org/abs/2408.10819
tags:
- gs-kgc
- knowledge
- llms
- language
- neighbors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes GS-KCG, a novel generative framework for knowledge
  graph completion (KGC) using large language models (LLMs). Unlike previous KGC methods
  that focus on ranking candidate entities, GS-KCG leverages subgraph information,
  including negatives and neighbors, to improve the accuracy of LLM predictions.
---

# GS-KGC: A Generative Subgraph-based Framework for Knowledge Graph Completion with Large Language Models

## Quick Facts
- arXiv ID: 2408.10819
- Source URL: https://arxiv.org/abs/2408.10819
- Reference count: 40
- One-line primary result: GS-KGC achieves up to 9.3% improvement in Hits@3 on ICEWS14 dataset

## Executive Summary
GS-KGC is a novel generative framework for knowledge graph completion (KGC) that leverages large language models (LLMs) by reframing KGC as a question-answering task. Unlike traditional KGC methods that rank candidate entities, GS-KGC uses subgraph information including negatives and neighbors to enhance LLM predictions. The framework is designed to discover new triples within knowledge graphs and predict facts beyond existing data. Experiments on four datasets demonstrate significant improvements over existing text-based and LLM-based methods.

## Method Summary
The GS-KGC framework reframes KGC as a question-answering task, utilizing subgraph information to improve LLM predictions. It incorporates negatives to encourage broader answer generation and neighbors to provide contextual reasoning support. The approach differs from previous KGC methods by focusing on generative capabilities rather than ranking-based candidate selection. The framework aims to discover new triples and predict facts not present in the existing knowledge graph structure.

## Key Results
- Achieves up to 9.3% improvement in Hits@3 on ICEWS14 dataset
- Outperforms existing text-based and LLM-based methods on four KGC datasets
- Demonstrates strong potential for enhancing KGC tasks through generative LLM capabilities

## Why This Works (Mechanism)
The framework leverages the generative capabilities of LLMs by incorporating subgraph information that includes negative examples and neighbor entities. Negatives help the model generate a broader range of potential answers rather than being constrained to top-ranked candidates, while neighbors provide contextual information that supports reasoning. By reframing KGC as question-answering, the model can better understand the relational context and discover new facts beyond the existing knowledge graph structure.

## Foundational Learning
- **Knowledge Graph Completion (KGC)**: Task of predicting missing triples in a knowledge graph; needed to understand the problem domain and evaluation metrics.
- **Subgraph Information**: Local graph structures around target entities; needed to provide context for LLM reasoning and improve prediction accuracy.
- **Negative Sampling**: Including incorrect examples during training; needed to prevent model from being overly confident and encourage broader answer generation.
- **Question-Answering Framework**: Reframing KGC as QA tasks; needed to leverage LLM's natural language understanding capabilities for structured prediction tasks.
- **Generative vs Ranking Approaches**: Moving from entity ranking to direct generation; needed to utilize LLM's full generative potential rather than treating them as classifiers.
- **Neighbor Entities**: Related entities in the knowledge graph; needed to provide contextual support for reasoning about relationships.

## Architecture Onboarding
- **Component Map**: Question -> Subgraph Construction -> LLM Generation -> Answer Parsing -> Triple Prediction
- **Critical Path**: The subgraph construction phase is critical as it determines the quality of input to the LLM; poor subgraph selection will degrade performance regardless of LLM capability.
- **Design Tradeoffs**: Balancing subgraph size (larger provides more context but increases computational cost and potential noise) versus focusing on most relevant neighbors.
- **Failure Signatures**: Poor performance on sparse graphs where neighbor information is limited, or when negative sampling strategy fails to provide useful contrast.
- **First Experiments**: 1) Test baseline LLM performance without subgraph information to establish baseline improvement, 2) Evaluate impact of different negative sampling strategies, 3) Test neighbor selection methods (distance-based vs random) on performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to four KGC datasets with single LLM architecture, restricting generalizability
- No systematic exploration of subgraph size, negative sampling strategy, or neighbor selection impacts
- Missing comparison against recent graph neural network approaches that use structural information
- No ablation studies on relative contributions of negatives versus neighbors
- Computational costs and inference latency not addressed for real-world deployment

## Confidence
- **High confidence**: Framework achieves state-of-the-art performance on tested datasets
- **Medium confidence**: Integration of negatives and neighbors drives improvements (not conclusively demonstrated)
- **Low confidence**: Effectiveness generalizes across different KGC datasets, LLM architectures, and knowledge graph structures

## Next Checks
1. Conduct ablation studies to quantify individual and combined contributions of negative sampling and neighbor inclusion to model performance
2. Evaluate framework on additional KGC datasets and with different LLM architectures to test generalizability
3. Compare GS-KGC against recent GNN-based KGC models using subgraph information to isolate LLM versus structural encoding impacts