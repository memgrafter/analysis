---
ver: rpa2
title: On the Problem of Text-To-Speech Model Selection for Synthetic Data Generation
  in Automatic Speech Recognition
arxiv_id: '2407.21476'
source_url: https://arxiv.org/abs/2407.21476
tags:
- data
- training
- speech
- decoder
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of selecting the best text-to-speech
  (TTS) model for generating synthetic data to train automatic speech recognition
  (ASR) systems. The authors compare five different TTS decoder architectures (Transformer,
  NAR LSTM, AR LSTM, Glow-TTS, and Grad-TTS) in the context of synthetic data generation
  for CTC-based ASR training.
---

# On the Problem of Text-To-Speech Model Selection for Synthetic Data Generation in Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2407.21476
- Source URL: https://arxiv.org/abs/2407.21476
- Authors: Nick Rossenbach; Ralf SchlÃ¼ter; Sakriani Sakti
- Reference count: 0
- Primary result: Autoregressive decoding outperforms non-autoregressive decoding for synthetic data generation in CTC-based ASR training

## Executive Summary
This paper addresses the challenge of selecting optimal text-to-speech (TTS) models for generating synthetic training data for automatic speech recognition (ASR) systems. The authors systematically compare five different TTS decoder architectures (Transformer, NAR LSTM, AR LSTM, Glow-TTS, and Grad-TTS) in the context of synthetic data generation for CTC-based ASR training. Their empirical results show that autoregressive decoding consistently outperforms non-autoregressive decoding, with the AR LSTM decoder achieving the best performance (28.4% WER on clean test data, 14.9% on other test data). The study also reveals that current TTS models may be under-fitting training data, suggesting that larger models could yield better results for synthetic data generation.

## Method Summary
The authors train five TTS models with identical encoder and duration predictor components but different decoder architectures (Transformer, NAR LSTM, AR LSTM, Glow-TTS, Grad-TTS) on LibriSpeech train-clean-100. They use these models to generate synthetic speech from train-clean-360 transcriptions, then train CTC-based ASR systems on the synthetic data. Performance is evaluated using word error rate (WER) on real LibriSpeech test sets, along with NISQA MOS for naturalness and synthetic WER for recognizability. The study introduces a method to quantify TTS generalization by varying conditions under which synthetic training data is generated.

## Key Results
- AR LSTM decoder achieved the best ASR performance (28.4% WER on clean test, 14.9% on other test)
- NAR decoding consistently underperformed AR decoding for synthetic data generation
- NISQA MOS and synthetic WER showed no clear correlation with final ASR performance
- TTS models demonstrated high generalization abilities across different synthetic data generation conditions

## Why This Works (Mechanism)

### Mechanism 1
Autoregressive decoding produces more effective synthetic training data for ASR than non-autoregressive decoding because AR decoding generates speech output frame-by-frame, conditioning each prediction on previous outputs. This captures temporal dependencies more accurately, leading to spectrograms with richer high-frequency detail and clearer phoneme boundaries, which are critical for ASR training.

### Mechanism 2
TTS systems underfit current training data, and larger models could yield better ASR performance because TTS models trained on limited data memorize rather than generalize. This results in synthetic data that closely mimics training conditions but fails to capture broader speech variability needed for robust ASR.

### Mechanism 3
Metrics like NISQA MOS and synthetic WER are poor predictors of ASR training effectiveness because these metrics capture naturalness and recognizability in isolation but fail to account for the specific characteristics needed for ASR training, such as consistent phoneme pronunciation and minimal high-frequency noise.

## Foundational Learning

- Concept: CTC (Connectionist Temporal Classification) loss and architecture
  - Why needed here: The paper uses CTC-based ASR systems, which require understanding how alignment between input frames and output labels is handled without explicit segmentation.
  - Quick check question: How does CTC handle variable-length alignments between speech frames and phoneme sequences without requiring pre-segmented training data?

- Concept: Text-to-speech synthesis pipeline (encoder, duration prediction, decoder)
  - Why needed here: The paper compares different TTS decoder architectures while keeping other components constant, requiring understanding of how each component contributes to final output quality.
  - Quick check question: What role does the duration predictor play in controlling the temporal characteristics of synthesized speech, and how might this impact ASR training?

- Concept: Spectrogram analysis and acoustic features
  - Why needed here: The paper discusses differences in spectrogram characteristics between decoder types and their impact on ASR performance, requiring ability to interpret these visualizations.
  - Quick check question: What acoustic features in a spectrogram (e.g., high-frequency content, phoneme boundary clarity) are most critical for effective ASR training?

## Architecture Onboarding

- Component map: TTS encoder -> Duration predictor -> Decoder (varies) -> Vocoder (Griffin & Lim) -> ASR frontend -> Conformer encoder -> CTC decoder with LM -> WER evaluation
- Critical path: TTS model training -> Synthetic data generation -> ASR training -> WER evaluation on real test sets
- Design tradeoffs:
  - Model size vs. training time: Larger TTS models may improve synthetic data quality but increase computational costs significantly
  - Decoder architecture choice: AR decoding provides better temporal modeling but slower inference compared to NAR
  - Training data conditions: Using different text/speaker combinations tests generalization but may introduce domain mismatch
- Failure signatures:
  - High synthetic WER but low NISQA MOS: Indicates good naturalness but poor phoneme pronunciation accuracy
  - Low synthetic WER but high final WER: Suggests synthetic data captures text well but lacks acoustic variability
  - Minimal difference between training conditions: Indicates model overfitting rather than true generalization
- First 3 experiments:
  1. Reproduce the baseline AR LSTM results on LibriSpeech to establish performance expectations
  2. Test a single NAR decoder variant (e.g., Transformer decoder) to confirm the AR vs NAR performance gap
  3. Generate synthetic data with shuffled speakers only (condition b) to isolate the impact of speaker variability on ASR performance

## Open Questions the Paper Calls Out

### Open Question 1
How do large-scale TTS models perform compared to smaller models when generating synthetic data for ASR training, particularly on constrained datasets like LibriSpeech 100h?
- Basis in paper: The authors suggest that current TTS models may be under-fitting and that larger models could yield better results.
- Why unresolved: The paper only used relatively small TTS models and did not explore the performance of large-scale models on this task.
- What evidence would resolve it: Experiments comparing ASR performance when using synthetic data generated by large-scale TTS models versus smaller models, trained on the same datasets.

### Open Question 2
What metrics can reliably predict the utility of synthetic TTS data for ASR training, beyond naturalness and recognizability?
- Basis in paper: The authors found no clear relation between NISQA MOS, sWER, and final ASR performance, indicating that current metrics are not reliable predictors.
- Why unresolved: Existing metrics like NISQA MOS and sWER do not correlate well with ASR performance, suggesting other factors are at play.
- What evidence would resolve it: Development and validation of new metrics that correlate strongly with ASR performance when trained on synthetic data, potentially incorporating aspects like speaker diversity or temporal variability.

### Open Question 3
How does the performance of different TTS decoder architectures vary when generating synthetic data for ASR training across different languages and domains?
- Basis in paper: The study was conducted on a single dataset (LibriSpeech) and language (English), raising questions about generalizability.
- Why unresolved: The paper's results are limited to one dataset and language, and it's unclear if the findings hold for other languages or domains.
- What evidence would resolve it: Comparative studies of TTS decoder architectures for synthetic data generation in multiple languages and domains, measuring ASR performance across these scenarios.

## Limitations
- Limited evaluation scope: Only one ASR architecture (CTC-based Conformer) and one dataset (LibriSpeech) were tested
- Suboptimal waveform synthesis: Uses Griffin-Lim rather than neural vocoders, which may not reflect current best practices
- Indirect evidence for model capacity: The underfitting claim is based on synthetic data performance gaps rather than explicit model size experiments

## Confidence

- High confidence: Autoregressive decoding outperforms non-autoregressive decoding for synthetic data generation in this specific CTC-based ASR setup
- Medium confidence: Current TTS models underfit training data and could benefit from larger architectures
- Low confidence: NISQA MOS and synthetic WER poorly predict ASR training effectiveness

## Next Checks

1. **Replicate with neural vocoder**: Repeat the experiments using a neural vocoder (e.g., HiFi-GAN) instead of Griffin-Lim to verify if the AR vs NAR performance gap persists with higher-quality audio output.

2. **Cross-domain generalization**: Test the same decoder comparison on a non-LibriSpeech dataset (e.g., TED-LIUM or Common Voice) to assess whether the findings generalize beyond the original domain.

3. **Alternative ASR architectures**: Evaluate the synthetic data generated by each decoder on an attention-based encoder-decoder ASR system to determine if the AR decoding advantage extends beyond CTC-based models.