---
ver: rpa2
title: Graph-tree Fusion Model with Bidirectional Information Propagation for Long
  Document Classification
arxiv_id: '2410.02930'
source_url: https://arxiv.org/abs/2410.02930
tags:
- document
- long
- information
- classification
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel Graph-Tree Fusion Model to tackle
  the challenges of long document classification, particularly addressing issues related
  to token limits and hierarchical relationships. The approach combines syntax trees
  for sentence encodings and document graphs for document encodings, utilizing Tree
  Transformers and Graph Attention Networks (GATs) to capture both local and global
  dependencies.
---

# Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification

## Quick Facts
- arXiv ID: 2410.02930
- Source URL: https://arxiv.org/abs/2410.02930
- Reference count: 16
- Introduces a novel Graph-Tree Fusion Model that combines syntax trees and document graphs with bidirectional information propagation for long document classification

## Executive Summary
This paper presents a novel approach to long document classification that addresses the token limit constraints of traditional transformer models. The Graph-Tree Fusion Model combines syntax tree structures for sentence-level encoding with document graph representations for document-level encoding, enabling effective handling of arbitrarily long contexts. The model employs Tree Transformers for parsing syntax trees and Graph Attention Networks (GATs) for document graphs, connected through a bidirectional information propagation mechanism that allows contextual information to flow both from words to sentences to document, and vice versa. This architecture effectively captures both local dependencies within sentences and global relationships across the entire document.

The proposed approach demonstrates significant improvements over existing methods across multiple long document classification tasks, including binary, multi-class, and multi-label classification scenarios. The model successfully handles documents of varying lengths without being constrained by token limits, addressing a critical limitation in current transformer-based approaches. Through comprehensive experiments, the authors show that their method achieves superior accuracy and macro-F1 scores compared to state-of-the-art baselines, validating the effectiveness of the graph-tree fusion architecture with bidirectional propagation for long document understanding.

## Method Summary
The paper introduces a novel Graph-Tree Fusion Model that addresses the challenges of long document classification by combining syntax trees and document graphs with bidirectional information propagation. The methodology involves parsing each sentence into a syntax tree using Tree Transformer models to capture local dependencies and syntactic relationships. Simultaneously, the entire document is represented as a graph where sentences are nodes connected based on semantic similarity or structural relationships, processed using Graph Attention Networks (GATs). A key innovation is the bidirectional information propagation mechanism that allows contextual information to flow from word-level to sentence-level to document-level representations, and then back down to refine local representations. This approach enables the model to handle arbitrarily long documents without token limitations while capturing both fine-grained syntactic structures and broader document-level semantics.

## Key Results
- Achieves significant improvements in accuracy and macro-F1 scores across multiple long document classification tasks including binary, multi-class, and multi-label classification
- Effectively handles arbitrarily long contexts without being constrained by token limit issues that affect traditional transformer models
- Outperforms existing state-of-the-art methods on long document classification benchmarks, demonstrating the effectiveness of the graph-tree fusion architecture with bidirectional propagation

## Why This Works (Mechanism)
The model works by creating a hierarchical representation of documents that preserves both local syntactic structures and global semantic relationships. The syntax tree encoding captures fine-grained word-level dependencies and grammatical structures within sentences through Tree Transformers, while the document graph encoding captures broader relationships between sentences and document-level semantics through GATs. The bidirectional information propagation mechanism is crucial as it allows information to flow from the bottom-up (words to sentences to document) to establish context, and then from top-down (document to sentences to words) to refine local representations based on global context. This dual flow ensures that local representations are informed by global document semantics while maintaining the syntactic precision of the syntax tree structure.

## Foundational Learning
- Tree Transformers (why needed: to parse syntax trees and capture local syntactic dependencies; quick check: can the model effectively handle nested grammatical structures and parse tree hierarchies)
- Graph Attention Networks (why needed: to process document graph representations and capture global semantic relationships; quick check: does the GAT layer properly aggregate information from neighboring sentences in the document graph)
- Bidirectional Information Propagation (why needed: to enable context flow from word-to-sentence-to-document and vice versa; quick check: is information flowing correctly in both directions and improving representation quality at each level)

## Architecture Onboarding
Component map: Document -> Document Graph (GAT) <-> Bidirectional Propagation <-> Syntax Tree (Tree Transformer) -> Sentences -> Words

Critical path: Document graph encoding through GATs provides global context, which flows through bidirectional propagation to syntax tree encoding, where local syntactic structures are refined by global information, producing final document representations for classification.

Design tradeoffs: The architecture trades computational complexity for expressive power, as maintaining both syntax trees and document graphs with bidirectional propagation increases parameter count and inference time compared to simpler transformer approaches. However, this complexity enables handling of arbitrarily long documents and captures richer hierarchical relationships.

Failure signatures: Potential failure modes include: 1) Inadequate syntax tree parsing leading to poor local representations, 2) Document graph construction that fails to capture meaningful relationships between sentences, 3) Bidirectional propagation that either overwhelms local information with global context or fails to effectively integrate the two, resulting in representations that are either too local or too global.

First experiments: 1) Test syntax tree parsing accuracy on standard constituency parsing benchmarks to validate the Tree Transformer component, 2) Evaluate document graph construction quality by measuring semantic similarity preservation between connected sentences, 3) Validate bidirectional information flow by examining attention weights to ensure proper information distribution between levels.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Computational efficiency for very long documents is not adequately addressed, with unclear scalability beyond 10,000 tokens
- Limited ablation studies that comprehensively analyze the individual contributions of syntax tree encoding, document graph encoding, and bidirectional propagation components
- Experiments primarily focus on documents up to 5,000 tokens, leaving uncertainty about performance on truly long documents

## Confidence
High confidence in: The architectural design and its ability to handle token limitations through the fusion of syntax trees and document graphs. The experimental setup and baseline comparisons appear methodologically sound.

Medium confidence in: The effectiveness of bidirectional information propagation mechanism, as the paper provides limited analysis of how information flows through the different encoding levels and its specific contribution to performance gains.

Low confidence in: The computational efficiency claims and scalability to extremely long documents, as these aspects are not empirically validated in the paper.

## Next Checks
1. Conduct comprehensive ablation studies that isolate the contribution of each component (syntax tree encoding, document graph encoding, bidirectional propagation) with statistical significance testing to quantify their individual impacts on performance.

2. Evaluate the model's performance and computational efficiency on documents exceeding 10,000 tokens to validate scalability claims and identify any practical limitations.

3. Compare the model's performance against recent transformer variants specifically designed for long sequences (such as Longformer or BigBird) on identical datasets to establish relative effectiveness.