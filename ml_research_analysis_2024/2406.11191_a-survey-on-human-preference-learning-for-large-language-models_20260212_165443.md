---
ver: rpa2
title: A Survey on Human Preference Learning for Large Language Models
arxiv_id: '2406.11191'
source_url: https://arxiv.org/abs/2406.11191
tags:
- preference
- human
- learning
- feedback
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews human preference learning methods
  for aligning large language models (LLMs) with human intentions. It categorizes
  feedback sources (direct human, model-generated, and inductive bias-based), formats
  (relative relations, absolute properties, and combinations), and preference modeling
  approaches (numerical explicit, natural language explicit, and implicit).
---

# A Survey on Human Preference Learning for Large Language Models

## Quick Facts
- arXiv ID: 2406.11191
- Source URL: https://arxiv.org/abs/2406.11191
- Reference count: 40
- Key outcome: Comprehensive survey reviewing human preference learning methods for aligning large language models with human intentions, categorizing feedback sources, formats, modeling approaches, usage methods, and evaluation frameworks while identifying key challenges and future directions.

## Executive Summary
This survey provides a systematic review of human preference learning methods for aligning large language models (LLMs) with human intentions. The authors categorize feedback sources (direct human, model-generated, and inductive bias-based), feedback formats (relative relations, absolute properties, and combinations), and preference modeling approaches (numerical explicit, natural language explicit, and implicit). The work details preference usage methods including reinforcement learning from human feedback (RLHF), supervised fine-tuning on preferred outputs, preference-guided contrastive learning, and preference-conditioned fine-tuning. The survey also covers evaluation approaches and identifies key challenges including pluralistic preference learning, scalable oversight, language-agnostic alignment, multi-modal complement, comprehensive assessment, and deceptive alignment research.

## Method Summary
The survey synthesizes existing research on human preference learning for LLM alignment through comprehensive literature review and systematic categorization. The methodology involves collecting and organizing preference learning approaches from various sources, classifying them based on feedback collection mechanisms, data formats, modeling techniques, and usage methods. The authors examine the theoretical foundations of each approach while providing practical insights into their implementation and effectiveness. The survey structure follows a logical progression from feedback collection to preference modeling to usage methods to evaluation, creating a comprehensive framework for understanding how human preferences are introduced into LLMs.

## Key Results
- Preference learning can effectively align LLMs with human intentions by optimizing model outputs according to feedback reflecting human preferences
- Different feedback formats (relative relations, absolute properties, combinations) capture complementary aspects of preference information
- Preference modeling approaches (numerical explicit, natural language explicit, implicit) create generalizable preference signals from raw feedback data
- Multiple preference usage methods (RLHF, SFT, contrastive learning, conditioning) can be applied to align foundation LLMs
- Key challenges include pluralistic preference learning, scalable oversight, and language-agnostic alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human preference learning aligns LLMs by optimizing model outputs according to feedback that reflects human preferences and intentions
- Mechanism: Preference signals (numerical, natural language, or implicit) are used to adjust foundation LLM parameters through reinforcement learning, supervised fine-tuning, contrastive learning, or conditioning
- Core assumption: The preference feedback accurately represents human intentions and can be generalized from the collected data
- Evidence anchors:
  - [abstract] "Human preference learning can effectively align LLMs with human intentions by optimizing LLMs according to feedback information on their outputs that reflects the preferences and thus specifies the intentions of humans"
  - [section] "Preference modeling aims to obtain preference models from preference feedback data. While it is viable to directly apply the collected preference feedback as usable preference signals, a typical practice is to learn preference models with generalization ability as proxies of human preferences"
  - [corpus] Weak - no direct corpus evidence on generalization ability
- Break condition: If preference feedback doesn't generalize or becomes outdated due to evolving human preferences

### Mechanism 2
- Claim: LLMs can simulate human feedback to create scalable preference signals
- Mechanism: Powerful commercial LLMs (e.g., GPT-4) are prompted to provide preference judgments or scores, reducing dependency on expensive human labelers
- Core assumption: LLMs trained on human preferences can accurately simulate human judgment at scale
- Evidence anchors:
  - [section] "LLMs instructed with appropriate prompts are capable of providing human preference information. What differs from the above is that instructed LLMs as preference models aim to directly provide generalizable preference signals at scale"
  - [corpus] Moderate - corpus contains related work "RLAIF: Scaling reinforcement learning from human feedback with ai feedback"
- Break condition: If simulated feedback quality degrades or LLM hallucinations introduce systematic bias

### Mechanism 3
- Claim: Different feedback formats capture different aspects of human preference information
- Mechanism: Relative relations (pairwise comparisons, rankings) are easy to collect but less informative; absolute properties (numerical scores, binary labels, feedback texts) provide more detail but are harder to collect
- Core assumption: The information captured by each format is complementary and can be combined effectively
- Evidence anchors:
  - [section] "The feedback formats adopted by most works on preference learning include relations between several responses and properties within each response. Besides, the combined formats of these categories are also feasible"
  - [corpus] Moderate - corpus contains related work on combining formats "Rrhf: Rank responses to align language models with human feedback"
- Break condition: If combining formats introduces inconsistency or if one format becomes dominant over others

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is a core preference usage method that maximizes expected reward scores of LLM outputs
  - Quick check question: What are the three stages of the typical RLHF scheme?

- Concept: Preference Modeling
  - Why needed here: Preference modeling creates generalizable preference signals from raw feedback data
  - Quick check question: What are the three major categories of preference modeling approaches?

- Concept: Contrastive Learning in Preference Optimization
  - Why needed here: Contrastive learning methods increase generation probabilities of preferred outputs while decreasing less preferred ones
  - Quick check question: What is the key difference between raw log probability contrast and normalized log probability contrast?

## Architecture Onboarding

- Component map:
  - Feedback collection system (human, model, inductive bias sources) -> Feedback processing pipeline (format parsing, validation) -> Preference modeling module (reward models, critics, implicit models) -> Preference usage module (RLHF, SFT, contrastive learning, conditioning) -> Evaluation framework (open-form benchmarks, automatic metrics, qualitative analysis)

- Critical path: Feedback collection → Preference modeling → Preference usage → Evaluation
- Design tradeoffs:
  - Quality vs. scalability in feedback collection
  - Information density vs. collection difficulty in feedback formats
  - Numerical vs. natural language preference signals
  - RL vs. non-RL preference usage methods

- Failure signatures:
  - Reward hacking (model exploits reward signal loopholes)
  - Overfitting to preference feedback
  - Preference collapse (model ignores nuanced preferences)
  - Evaluation metric misalignment

- First 3 experiments:
  1. Implement pairwise comparison feedback collection and basic reward modeling
  2. Compare RLHF vs. supervised fine-tuning on preferred outputs
  3. Test combining numerical scores with feedback texts for preference modeling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively model and satisfy pluralistic human preferences in LLMs while maintaining zero-shot generalizability?
- Basis in paper: [explicit] "However, it is still worth studying how to simultaneously model general human preferences and preferences of different groups with zero-shot generalizability, satisfying diverse and subtle preferences inferred from human queries."
- Why unresolved: Current methods either focus on general preferences or specific groups, but lack approaches that can dynamically adapt to inferred preferences from queries without extensive fine-tuning.
- What evidence would resolve it: Development and validation of a preference modeling framework that can accurately capture and satisfy diverse user preferences through zero-shot inference, demonstrated on benchmark datasets with varied user groups.

### Open Question 2
- Question: What are the most effective approaches for scalable oversight of superhuman LLMs that can maintain alignment with human intentions?
- Basis in paper: [explicit] "when AI systems such as LLMs are more capable than humans, new approaches that scale up the ability of human supervision are required to align them with human intentions, namely scalable oversight"
- Why unresolved: Current human feedback methods become less effective as models surpass human capabilities, and existing scalable oversight proposals lack empirical validation.
- What evidence would resolve it: Comparative studies showing that new scalable oversight methods (like recursive reward modeling or amplification) can maintain alignment quality as model capabilities exceed human levels, measured against established benchmarks.

### Open Question 3
- Question: Can we develop language-agnostic alignment methods that eliminate performance disparities across different languages in LLMs?
- Basis in paper: [explicit] "However, existing studies demonstrated that current LLMs are mostly more capable of reasoning in certain languages such as English... Further research can be conducted on mitigating or eliminating the overall capability gap in languages for LLMs by language-agnostic LLM alignment."
- Why unresolved: Current alignment approaches show significant performance variations across languages, and no comprehensive solutions have been demonstrated to eliminate these gaps.
- What evidence would resolve it: Demonstrated reduction or elimination of performance disparities across languages in LLM alignment tasks, validated through multilingual benchmark evaluations showing comparable performance across language families.

## Limitations
- The survey does not provide quantitative comparisons of method effectiveness due to the heterogeneous nature of existing studies
- Limited discussion of computational costs and scalability challenges across different preference learning approaches
- The survey does not address potential safety risks of preference learning, such as reward hacking or preference manipulation

## Confidence
- High confidence in comprehensive categorization of preference learning methods and their applications to LLM alignment
- Medium confidence in practical effectiveness of different preference learning approaches due to lack of empirical validation across methods
- Medium confidence in scalability and computational cost assessments of preference learning approaches

## Next Checks
1. Conduct systematic benchmarking of RLHF versus alternative preference usage methods across multiple tasks and preference formats
2. Evaluate the robustness of preference models to feedback noise and distribution shifts over time
3. Test the scalability limits of different preference collection methods by comparing human, model, and hybrid approaches across varying data volumes