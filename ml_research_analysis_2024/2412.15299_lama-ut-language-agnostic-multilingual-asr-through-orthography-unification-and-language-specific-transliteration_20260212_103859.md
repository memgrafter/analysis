---
ver: rpa2
title: 'LAMA-UT: Language Agnostic Multilingual ASR through Orthography Unification
  and Language-Specific Transliteration'
arxiv_id: '2412.15299'
source_url: https://arxiv.org/abs/2412.15299
tags:
- languages
- universal
- transcription
- language
- language-specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LAMA-UT, a language-agnostic multilingual
  ASR pipeline that achieves strong performance across over 100 languages without
  relying on language-specific modules. The approach unifies orthographic features
  into Romanized form and leverages a universal converter (LLM) to perform language-specific
  transliteration.
---

# LAMA-UT: Language Agnostic Multilingual ASR through Orthography Unification and Language-Specific Transliteration

## Quick Facts
- arXiv ID: 2412.15299
- Source URL: https://arxiv.org/abs/2412.15299
- Authors: Sangmin Lee; Woo-Jin Chung; Hong-Goo Kang
- Reference count: 9
- Primary result: Achieves 45% relative error reduction vs Whisper using only 0.1% of training data

## Executive Summary
This paper introduces LAMA-UT, a language-agnostic multilingual ASR pipeline that achieves strong performance across over 100 languages without relying on language-specific modules. The approach unifies orthographic features into Romanized form and leverages a universal converter (LLM) to perform language-specific transliteration. The pipeline achieves a relative error reduction of 45% compared to Whisper and matches the performance of MMS while being trained on only 0.1% of Whisper's training data.

## Method Summary
LAMA-UT uses a two-stage approach: first, a universal transcription generator (wav2vec2.0-XLS-R encoder + classification layer) produces Romanized transcriptions from audio using CTC loss; second, a frozen LLM performs language-specific transliteration on these Romanized outputs. The system is trained on 680 hours of FLEURS corpus data (0.1% of Whisper's training data) and evaluated on both seen languages (FLEURS) and unseen languages (CommonVoice 17.0). Key innovations include orthography unification through Romanization and leveraging LLMs for language-specific transliteration without language-specific modules.

## Key Results
- 45% relative error reduction compared to Whisper across 100+ languages
- Matches MMS performance while using only 0.1% of Whisper's training data
- Strong generalization to unseen languages (CommonVoice 17.0) with comparable performance to zero-shot ASR methods
- GPT-4o-mini shows highest performance among tested LLM sizes (8B, 70B, and GPT-4o-mini)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Romanization reduces orthographic complexity while preserving phonetic alignment across languages
- Mechanism: By converting diverse writing systems into Latin script, the model learns a unified orthographic representation that maps directly to shared phonetic features
- Core assumption: Phonetic characteristics across human languages are constrained by anatomical vocal tract limitations, making a common representation possible
- Evidence anchors:
  - [abstract]: "we focused on reducing orthographic complexity by unifying diverse orthographic systems into a consistent format, approximating phonetic features across multiple languages"
  - [section]: "Prior research (Taguchi and Chiang 2024) has empirically demonstrated that the primary obstacle in multilingual ASR is the orthographic complexity across languages"
- Break condition: When languages have fundamentally incompatible phonetic structures or when Romanization loses critical phonological distinctions

### Mechanism 2
- Claim: LLM-based transliteration can convert universal Romanized transcriptions back to language-specific forms without language-specific modules
- Mechanism: Frozen LLMs leverage their multilingual pretraining to perform transliteration as a text-to-text transformation task
- Core assumption: LLMs have sufficient multilingual and multitask understanding from pretraining to handle transliteration
- Evidence anchors:
  - [abstract]: "we regard the transformation from universal transcription to language-specific transcription as a transliteration task by leveraging a universal converter"
  - [section]: "we focused on the versatility of LLMs which excel in multilingual and multitask benchmarks due to extensive training on diverse text data"
- Break condition: When transliteration requires language-specific knowledge not present in pretraining data

### Mechanism 3
- Claim: Minimal training data (0.1% of Whisper) suffices due to effective orthography unification and universal converter
- Mechanism: Romanization provides a compact, phonetically-aligned representation that reduces the effective vocabulary size and learning complexity
- Core assumption: The unified representation captures sufficient language-agnostic features for ASR
- Evidence anchors:
  - [abstract]: "Our pipeline achieves a relative error reduction rate of 45% when compared to Whisper and performs comparably to MMS, despite being trained on only 0.1% of Whisper's training data"
- Break condition: When languages require fine-grained phonetic distinctions that Romanization cannot capture

## Foundational Learning

- Concept: International Phonetic Alphabet (IPA) vs Romanization tradeoffs
  - Why needed here: Understanding why Romanization was chosen over IPA despite IPA's phonetic precision
  - Quick check question: What are the three main challenges with using IPA for this pipeline?

- Concept: Prompt engineering strategies for LLM-based transliteration
  - Why needed here: The performance heavily depends on effective prompt design
  - Quick check question: Which prompting strategy showed the highest performance and why might sequential reasoning fail?

- Concept: Connectionist Temporal Classification (CTC) loss in speech recognition
  - Why needed here: The universal transcription generator uses CTC loss for training
  - Quick check question: How does CTC handle variable-length input-output mappings in speech recognition?

## Architecture Onboarding

- Component map: Audio Encoder (wav2vec2.0-XLS-R) → Classification Layer → Romanized Transcription → Universal Converter (LLM) → Language-Specific Transcription
- Critical path: Audio → Universal Transcription → Language-Specific Transcription
- Design tradeoffs:
  - Romanization vs IPA: Simpler tokenization vs phonetic precision
  - Frozen LLM vs fine-tuning: Zero-shot generalization vs task-specific optimization
  - Minimal data vs performance: Efficiency vs potential accuracy gains from more data
- Failure signatures:
  - High CER in non-Latin scripts: Indicates transliteration issues
  - Language misidentification: LLM not recognizing target language
  - Word repetition in output: Model confusion or generation issues
- First 3 experiments:
  1. Test Romanization conversion accuracy on diverse language samples
  2. Evaluate LLM transliteration with ground truth Romanized transcriptions
  3. Compare few-shot vs zero-shot prompting performance on held-out languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the LAMA-UT pipeline's performance scale with increasing model size, particularly for low-resource languages, and what is the optimal model size for balancing performance and computational efficiency?
- Basis in paper: [explicit] The paper compares LLaMA-8B, LLaMA-70B, and GPT-4o-mini, noting that GPT-4o-mini performs best, but does not explore intermediate sizes or scaling effects
- Why unresolved: The paper only tests three fixed model sizes without exploring a range of sizes or analyzing the scaling relationship between model capacity and performance, especially for low-resource languages
- What evidence would resolve it: Experiments training LAMA-UT with models of varying sizes (e.g., 3B, 13B, 30B) and analyzing the performance curves for both seen and unseen languages, particularly focusing on low-resource languages

### Open Question 2
- Question: How does the LAMA-UT pipeline's performance compare to state-of-the-art multilingual ASR models when trained on datasets larger than 680 hours, and what is the impact on generalization to unseen languages?
- Basis in paper: [explicit] The paper highlights that LAMA-UT is trained on only 0.1% of Whisper's data (680 hours) and achieves strong performance, but does not explore the effects of scaling the training data
- Why unresolved: The paper focuses on demonstrating performance with minimal data but does not investigate whether further scaling the dataset improves performance or generalization, particularly for unseen languages
- What evidence would resolve it: Training LAMA-UT on datasets of increasing size (e.g., 1k, 10k, 100k hours) and evaluating performance on both seen and unseen languages, comparing results to models trained on larger datasets like Whisper and MMS

### Open Question 3
- Question: What are the limitations of using Romanization as the intermediate representation for languages with complex phonological systems, such as tonal languages or those with significant phonetic distinctions not captured by Latin script?
- Basis in paper: [inferred] The paper discusses the advantages of Romanization over IPA but does not address potential limitations for languages with complex phonological features that may not be adequately represented by Romanization
- Why unresolved: The paper does not explore how well Romanization captures phonetic nuances in languages with tones, pitch accents, or other features that may not be easily represented in Latin script, potentially limiting the pipeline's effectiveness for such languages
- What evidence would resolve it: Comparative experiments evaluating LAMA-UT's performance on tonal languages (e.g., Mandarin, Vietnamese) versus non-tonal languages, and analyzing the impact of Romanization on transcription accuracy for these languages

## Limitations

- Limited evaluation of truly unseen languages beyond CommonVoice 17.0
- No exploration of fine-tuning vs frozen LLM performance for transliteration
- Potential issues with languages that have complex phonological systems not well-represented by Romanization

## Confidence

- High confidence: Orthography unification through Romanization effectively reduces complexity and enables cross-linguistic generalization
- Medium confidence: LLM-based transliteration performs well with prompting but optimal approach (fine-tuning vs zero-shot) remains unclear
- Low confidence: Scalability to 100+ languages and performance on languages with complex phonological systems

## Next Checks

1. Ablation study on Romanization quality: Systematically compare different Romanization methods (Uroman vs manual transliteration) across languages with varying orthographic complexity to quantify the impact on ASR performance

2. LLM fine-tuning experiment: Compare frozen LLM performance against fine-tuned variants on the transliteration task to establish whether the current zero-shot approach is optimal or represents a practical compromise

3. Zero-resource language evaluation: Test the system on truly unseen languages (zero-shot across all components) to validate the claim of language-agnostic generalization beyond the CommonVoice evaluation set