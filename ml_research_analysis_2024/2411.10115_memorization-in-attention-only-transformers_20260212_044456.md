---
ver: rpa2
title: Memorization in Attention-only Transformers
arxiv_id: '2411.10115'
source_url: https://arxiv.org/abs/2411.10115
tags:
- memorization
- scaling
- attention
- sequence
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes new theoretical and empirical results on
  the memorization capacity of attention-only Transformers (AoTs). Theoretically,
  the authors prove that a one-layer AoT with H heads can memorize Hd h + d associations
  (sequence-token pairs), improving prior work which required limited context windows.
---

# Memorization in Attention-only Transformers

## Quick Facts
- arXiv ID: 2411.10115
- Source URL: https://arxiv.org/abs/2411.10115
- Reference count: 40
- Primary result: Attention-only Transformers can memorize Hd h + d associations with one layer, improving prior work limited to context windows.

## Executive Summary
This paper establishes new theoretical and empirical results on the memorization capacity of attention-only Transformers (AoTs). The authors prove that a one-layer AoT with H heads can memorize Hd h + d associations, extending prior work that required limited context windows. They also introduce the novel concept of approximate memorization of probability distributions and provide bounds on KL-divergence performance. Empirically, they demonstrate that memorization capacity scales linearly with the number of heads H and quadratically with head dimension dh. The results show AoTs are better optimized for memorization compared to MLP-based Transformers under parameter constraints, advancing understanding of how attention mechanisms store information.

## Method Summary
The paper evaluates memorization capacity through theoretical analysis and controlled synthetic experiments. For theory, they construct proofs showing exact memorization of Hd h + d sequence-token associations and approximate memorization of probability distributions with bounded KL-divergence error. For experiments, they generate synthetic bigram datasets with 50 tokens and uniform prior, train AoTs and MLP-based Transformers for 26 epochs with Adam optimizer (learning rate 0.1→0.05), and measure next-token prediction accuracy across 5 random seeds. The comparison between architectures uses equal parameter counts while varying H, dh, and embedding dimension d to establish scaling laws.

## Key Results
- AoTs can exactly memorize Hd h + d sequence-token associations with one layer, improving prior work limited to context windows
- Memorization capacity scales linearly with number of heads H and quadratically with head dimension dh (C(N)Hd²h + C'(d,N))
- Under parameter constraints, AoTs achieve better memorization scaling than MLP-based Transformers
- Approximate memorization of probability distributions is bounded by KL-divergence error depending on ||WE||²

## Why This Works (Mechanism)

### Mechanism 1
Attention-only Transformers can exactly memorize Hd h + d sequence-token associations with one layer. The attention layer constructs sequence embeddings E(t1:S) = e(tS) + posS + WOWV A(t1:S) and maps them via a full-rank unembedding matrix to logits. The linear system E(t1:S) = WOWV A(t1:S) is solvable when WV A has rank ≥ T0 = Hd h + d, achieved by designing attention patterns A(t1:S) with sufficient rank.

Core assumption: The family of attention patterns {A(t1:S)} has rank ≥ min(T0, Hd) for some choice of embeddings and attention matrices.

Evidence anchors: [abstract] states the new proof extends memorization to any context size, improving prior work that required limited context windows. [section 4] shows the proof constructs WV A with full rank by choosing W h
QK and embeddings so the family of attention patterns has rank ≥ min(T0, Hd).

Break condition: If the attention patterns cannot achieve the required rank (e.g., embedding dimension too small or attention heads insufficiently expressive), the linear system becomes unsolvable and exact memorization fails.

### Mechanism 2
Attention-only Transformers can approximate memorization of probability distributions with KL-divergence bounded by the divergence of the best sequence encoder plus a term depending on ||WE||². The Transformer implements a sequence encoder fW,E(t1:S) = W E(t1:S) with E as above. By choosing W = WU and solving E = WOWV A, the Transformer's output approximates the optimal sequence encoder fW,E.

Core assumption: The sequence encoder achieving minimal divergence fW,E can be approximated by the attention layer's output within the stated bound.

Evidence anchors: [section 4] proves that for any distribution π, the Transformer can approximate the best sequence encoder up to an error depending on ||WE||². [abstract] introduces the concept of approximate memorization of distributions and provides bounds on KL-divergence performance.

Break condition: If ||WE||² grows too large (e.g., due to poor approximation of fW,E by the attention layer), the KL-divergence error term dominates and approximation quality degrades.

### Mechanism 3
Under Assumption 1 (unique next token with probability 1), an attention-only Transformer can achieve perfect accuracy for a uniform prior over token sequences. The Transformer maps each sequence to the correct next token by constructing embeddings that separate sequences in logit space. The accuracy bound in Corollary 2 shows that the Transformer can achieve accuracy ≥ 1/N + (1 - 1/N)(Hd h + d)/T0.

Core assumption: The prior distribution over token sequences is uniform, so accuracy directly measures associative memorization capacity.

Evidence anchors: [section 5] describes experiments training AoTs on bigrams with uniform prior and measuring accuracy as a proxy for associative memory. [abstract] mentions demonstrating that proposed bounds accurately reflect the true memorization capacity of language models.

Break condition: If the prior distribution is non-uniform, accuracy no longer directly measures memorization capacity, and the bound may not hold.

## Foundational Learning

- Concept: Linear algebra (matrix rank, singular value decomposition, linear systems)
  - Why needed here: The proofs rely heavily on constructing matrices with specific ranks and solving linear systems to achieve exact or approximate memorization.
  - Quick check question: Can you explain why the rank of WV A must be ≥ T0 for the linear system E = WOWV A to be solvable?

- Concept: Probability theory (KL-divergence, conditional distributions, entropy)
  - Why needed here: The paper introduces and analyzes memorization in terms of approximating probability distributions, using KL-divergence as the metric.
  - Quick check question: What is the difference between exact memorization (0 entropy conditionals) and approximate memorization (full support conditionals) in this context?

- Concept: Attention mechanisms in Transformers (multi-head attention, attention patterns, softmax)
  - Why needed here: The paper focuses on attention-only Transformers, so understanding how attention heads compute weighted averages of token embeddings is crucial.
  - Quick check question: How does the choice of W h
QK affect the attention pattern and the resulting weighted average of token embeddings?

## Architecture Onboarding

- Component map: Input tokens → embeddings + positional encodings → multi-head attention → residual addition → unembedding → logits for next token

- Critical path:
  1. Token sequence → embeddings (e(t) + pos)
  2. Attention layer → attention output A(t1:S)
  3. Residual addition → E(t1:S) = e(tS) + posS + WOWV A(t1:S)
  4. Unembedding → logits for next token
  5. Softmax → probability distribution over next tokens

- Design tradeoffs:
  - Embedding dimension d vs. head dimension dh: Larger d allows more expressive embeddings but increases parameter count; larger dh increases attention expressiveness but may lead to overfitting.
  - Number of heads H vs. memorization capacity: More heads increase memorization capacity linearly but also increase parameter count.
  - Exact vs. approximate memorization: Exact memorization requires more parameters but guarantees perfect accuracy; approximate memorization uses fewer parameters but allows some error.

- Failure signatures:
  - Low accuracy: May indicate insufficient memorization capacity (too few heads or small head dimension) or optimization issues (learning rate too high/low).
  - High KL-divergence: May indicate poor approximation of the target distribution or insufficient embedding dimension to capture the distribution's complexity.
  - Unstable training: May indicate learning rate issues or exploding/vanishing gradients in deep attention layers.

- First 3 experiments:
  1. Train a one-layer AoT on bigrams with uniform prior and varying H, measuring accuracy to verify linear scaling in H.
  2. Train a one-layer AoT with fixed H and varying dh, measuring accuracy to verify quadratic scaling in dh.
  3. Compare the accuracy scaling of a one-layer AoT with an MLP-based Transformer with the same number of parameters to verify that AoTs are better optimized for memorization under parameter constraints.

## Open Questions the Paper Calls Out

### Open Question 1
What is the exact scaling relationship between the number of attention heads (H) and the memorization capacity when the head dimension (dh) is not fixed, and how does this compare to the theoretical bound of Hd h + d? The paper presents experimental results showing linear scaling in H when dh is fixed, but the authors note that the constant term C(N) in the empirical scaling law C(N)Hd²h + C'(d,N) is much larger than the theoretical bound, and the scaling in dh appears quadratic when H is fixed. Additional experiments with a wider range of H and dh values, combined with a more rigorous theoretical analysis of the constant term C(N), could clarify the exact scaling relationship.

### Open Question 2
How does the memorization capacity of attention-only transformers (AoTs) compare to that of transformers with MLP layers when both models have the same number of parameters and are trained under optimal conditions? The paper compares AoTs to MLP-based transformers with equal parameters and finds that AoTs have better memorization scaling under optimization constraints, but the authors note that in large language models (LLMs), this constraint does not hold, and the greater memorization capacity of MLPs could explain why they store facts and associations. Experiments comparing AoTs and MLP-based transformers with multiple layers and residual connections, trained under optimal conditions, would provide a more accurate assessment of their memorization capacities in the context of LLMs.

### Open Question 3
Can the concept of approximate memorization of distributions be extended to other loss functions beyond KL-divergence, and what would be the implications for the memorization capacity of transformers? The paper introduces the concept of approximate memorization of distributions and provides bounds on KL-divergence performance. The authors mention that the choice of loss function affects the approximation, and a different first-order approximation would be obtained if optimizing for total variation loss instead of KL-divergence. Theoretical analysis and experiments comparing the memorization capacity of transformers under different loss functions, such as cross-entropy, mean squared error, or total variation loss, would reveal the impact of the loss function on memorization.

## Limitations

- Theoretical results rely on strong assumptions (unique next token with probability 1, uniform priors) that may not hold in practical language modeling tasks
- Empirical validation uses synthetic bigram datasets with uniform priors, which may not generalize to realistic language modeling where token dependencies are more complex
- Comparison between AoTs and MLP-based Transformers is limited to one-layer architectures, not exploring how layer interactions affect memorization dynamics

## Confidence

**High Confidence:** The theoretical proof of exact memorization capacity (Hd h + d) under the stated assumptions is mathematically rigorous and the linear algebra arguments are sound. The experimental demonstration of scaling laws (linear in H, quadratic in dh) for synthetic bigrams is clearly presented and reproducible.

**Medium Confidence:** The approximate memorization bounds and KL-divergence analysis provide useful theoretical insights, but the practical relevance depends on factors not fully explored in the paper (e.g., achievable values of ||WE||², behavior under non-uniform priors). The comparison showing AoTs better optimized for memorization than MLPs under parameter constraints is supported by experiments but limited to simple architectures.

**Low Confidence:** Generalization of results to natural language tasks, deeper architectures, and non-uniform distributions remains largely unvalidated. The paper does not provide evidence that the theoretical memorization capacity translates to practical performance improvements in real-world language modeling scenarios.

## Next Checks

1. **Distribution Sensitivity Test:** Evaluate memorization capacity and accuracy under non-uniform priors and multi-token distributions (violating Assumption 1) to understand robustness to more realistic language modeling conditions.

2. **Depth Scaling Analysis:** Extend experiments to multi-layer AoTs and MLP-based Transformers to investigate how layer interactions affect memorization capacity and whether the one-layer theoretical bounds remain predictive.

3. **Practical Efficiency Benchmark:** Compare AoTs and MLP-based Transformers not just on memorization capacity but also on training time, inference speed, and parameter efficiency for language modeling tasks on real datasets like WikiText or LAMBADA.