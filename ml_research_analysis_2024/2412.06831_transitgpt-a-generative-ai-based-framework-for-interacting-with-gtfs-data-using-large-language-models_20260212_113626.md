---
ver: rpa2
title: 'TransitGPT: A Generative AI-based framework for interacting with GTFS data
  using Large Language Models'
arxiv_id: '2412.06831'
source_url: https://arxiv.org/abs/2412.06831
tags:
- gtfs
- arxiv
- code
- data
- feed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TransitGPT, a framework that uses Large Language
  Models (LLMs) to answer natural language queries about General Transit Feed Specification
  (GTFS) data. The framework guides LLMs to generate Python code that extracts and
  manipulates GTFS data, which is then executed on a server.
---

# TransitGPT: A Generative AI-based framework for interacting with GTFS data using Large Language Models

## Quick Facts
- arXiv ID: 2412.06831
- Source URL: https://arxiv.org/abs/2412.06831
- Authors: Saipraneeth Devunuri; Lewis Lehe
- Reference count: 7
- Primary result: Framework that uses LLMs to answer natural language queries about GTFS data with up to 93% accuracy

## Executive Summary
TransitGPT is a framework that enables users to interact with General Transit Feed Specification (GTFS) data using natural language queries through Large Language Models (LLMs). The system guides LLMs to generate Python code that extracts and manipulates GTFS data, which is then executed on a server to provide results. By leveraging prompt engineering and dynamic few-shot examples, TransitGPT achieves high accuracy without requiring fine-tuning or access to actual GTFS feeds during training. The framework significantly improves accessibility to transit data for users without extensive GTFS or programming knowledge.

## Method Summary
TransitGPT works by guiding LLMs to generate Python code that extracts and manipulates GTFS data relevant to user queries, which is then executed on a server where the GTFS feed is stored. The framework uses prompt engineering with dynamic few-shot examples and error handling mechanisms to achieve high accuracy. It was evaluated using GPT-4o and Claude-3.5-Sonnet on a benchmark of 100 tasks across five diverse transit agencies (SFMTA, MBTA, CUMTD, DART, CTA). The evaluation compared baseline (zero-shot, no retries) and TransitGPT+ (dynamic few-shot examples, error handling) configurations, measuring task accuracy rate (α), token usage (T), and time taken (∆t).

## Key Results
- TransitGPT achieved task accuracy rates up to 93% in the enhanced configuration with dynamic few-shot examples and error handling
- The framework successfully answered a wide range of queries, from simple data retrieval to complex calculations and interactive visualizations
- Evaluation across five diverse transit agencies demonstrated the framework's generalizability to different GTFS data structures and geographic regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework successfully bridges the gap between natural language queries and GTFS data manipulation by using LLMs to generate executable Python code
- Mechanism: TransitGPT leverages the code generation capabilities of LLMs to translate natural language queries into Python code that can extract and manipulate GTFS data. This code is then executed on a server where the GTFS feed is stored, allowing users to interact with complex transit data without needing extensive GTFS or programming knowledge
- Core assumption: LLMs can reliably generate accurate Python code for data manipulation tasks when provided with appropriate prompts and guidance
- Evidence anchors:
  - [abstract] "The framework guides LLMs to generate Python code that extracts and manipulates GTFS data relevant to a query, which is then executed on a server where the GTFS feed is stored."
  - [section] "TransitGPT works by guiding LLMs to generate Python code that extracts and manipulates GTFS data relevant to a query, which is then executed on a server where the GTFS feed is stored."
  - [corpus] Weak - No direct evidence in related papers specifically about using LLMs for GTFS data manipulation, but there is related work on using LLMs for code generation in other domains
- Break condition: If the generated code contains errors or is unable to handle the complexity of GTFS data relationships, the framework would fail to provide accurate results

### Mechanism 2
- Claim: The framework achieves high accuracy by using a combination of prompt engineering, dynamic few-shot examples, and error handling mechanisms
- Mechanism: TransitGPT employs a large system prompt that guides the LLM in generating appropriate code. It also uses dynamic few-shot examples, selecting the most relevant examples based on the user's query. If the generated code encounters errors, the framework employs an error handling mechanism that provides feedback to the LLM, allowing it to correct and retry the code generation
- Core assumption: LLMs can learn from dynamic few-shot examples and improve their code generation performance based on feedback
- Evidence anchors:
  - [abstract] "The LLMs that produce the code are guided entirely by prompts, without fine-tuning or access to the actual GTFS feeds."
  - [section] "To identify the three ideal query/response pairs that are most similar to the user's query, we use a technique called TF-IDF (Term Frequency-Inverse Document Frequency) to compute 'similarity scores' between each query and example."
  - [corpus] Weak - While there is evidence of using few-shot prompting in other LLM applications, there is no specific evidence in the corpus about using dynamic few-shot examples for GTFS data manipulation
- Break condition: If the LLM is unable to learn from the few-shot examples or if the error handling mechanism fails to guide the LLM towards correct code generation, the accuracy of the framework would decrease

### Mechanism 3
- Claim: The framework enhances accessibility and usability of transit data by allowing users to interact with it using natural language, without requiring extensive GTFS or programming knowledge
- Mechanism: TransitGPT provides a chatbot interface where users can ask questions about transit systems in natural language. The framework then translates these questions into Python code, executes it on the GTFS data, and provides the results in a human-readable format. This eliminates the need for users to understand the complexities of GTFS data structure or write code themselves
- Core assumption: Users can effectively communicate their information needs in natural language, and the framework can accurately interpret and fulfill these requests
- Evidence anchors:
  - [abstract] "It can accomplish a wide range of tasks, including data retrieval, calculations, and interactive visualizations, without requiring users to have extensive knowledge of GTFS or programming."
  - [section] "TransitGPT can answer questions that a rider might have, such as: 'When does the last Orange bus arrive at University and Victor on a Tuesday?' It can also answer system-level questions such as: 'Which routes have headway shorter than 15 minutes on the weekend?'"
  - [corpus] Weak - No direct evidence in related papers about using natural language interfaces for GTFS data, but there is evidence of using LLMs for information retrieval in other domains
- Break condition: If users' natural language queries are too ambiguous or if the framework misinterprets the intent, it would fail to provide accurate or useful results

## Foundational Learning

- Concept: General Transit Feed Specification (GTFS)
  - Why needed here: Understanding GTFS is crucial for interpreting the structure and content of transit data that TransitGPT interacts with
  - Quick check question: What are the main components of a GTFS feed, and how are they related to each other?

- Concept: Large Language Models (LLMs) and Prompt Engineering
  - Why needed here: Knowledge of how LLMs work and how to effectively prompt them is essential for understanding and potentially improving TransitGPT's code generation capabilities
  - Quick check question: How do few-shot examples influence an LLM's performance, and what is dynamic few-shot prompting?

- Concept: Python Programming and Data Manipulation Libraries
  - Why needed here: Understanding Python and libraries like pandas, numpy, and geopandas is necessary for comprehending the code generated by TransitGPT and potentially debugging or extending its functionality
  - Quick check question: How can pandas DataFrames be used to manipulate and analyze GTFS data?

## Architecture Onboarding

- Component map:
  User Interface (Streamlit app) -> Moderation LLM (GPT-4o-mini) -> Main LLM (GPT-4o or Claude-3.5-Sonnet) -> Sandbox (Python execution environment) -> GTFS Feeds (preprocessed and stored on server) -> Summary LLM (GPT-4o-mini) -> Results displayed to user

- Critical path:
  1. User inputs query in natural language
  2. Moderation LLM checks query relevance
  3. Main LLM generates Python code based on query and system prompt
  4. Code is executed in the sandbox with the selected GTFS feed
  5. Summary LLM formats the results for human consumption
  6. Results are displayed to the user

- Design tradeoffs:
  - Using pre-processed GTFS feeds vs. loading raw feeds on-demand
  - Relying on prompt engineering vs. fine-tuning LLMs for specific tasks
  - Executing code on a remote server vs. client-side execution
  - Providing detailed error messages vs. generic error handling

- Failure signatures:
  - Moderation step incorrectly blocks relevant queries or allows irrelevant ones
  - Main LLM generates incorrect or non-functional Python code
  - Code execution times out or encounters errors that cannot be resolved
  - Summary LLM fails to accurately represent the results or introduces errors
  - GTFS feed preprocessing introduces errors or misses important data

- First 3 experiments:
  1. Test the framework with a simple query that requires basic data retrieval from a single GTFS file
  2. Evaluate the error handling mechanism by intentionally providing a query that would generate erroneous code
  3. Assess the accuracy of the framework by comparing its results to manually generated answers for a set of benchmark queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of TransitGPT vary when applied to GTFS feeds from different geographic regions or transit agencies with varying data quality and complexity?
- Basis in paper: [inferred] The paper evaluates TransitGPT on five diverse GTFS feeds (SFMTA, MBTA, CUMTD, DART, CTA) but does not explicitly compare performance across geographic regions or data quality levels
- Why unresolved: The paper focuses on demonstrating TransitGPT's effectiveness rather than conducting a detailed regional or data quality analysis
- What evidence would resolve it: A comprehensive study comparing TransitGPT's performance across GTFS feeds from different regions and agencies with varying data quality, using standardized metrics

### Open Question 2
- Question: What are the limitations of TransitGPT in handling real-time GTFS-RT data, and how can the framework be extended to incorporate such data effectively?
- Basis in paper: [explicit] The paper explicitly states that TransitGPT is limited to GTFS Static data and does not draw information from GTFS Realtime feeds
- Why unresolved: The paper does not explore potential methods for extending the framework to handle real-time data or discuss the challenges involved
- What evidence would resolve it: Research demonstrating the integration of GTFS-RT data into the TransitGPT framework, including accuracy and performance evaluations

### Open Question 3
- Question: How does the performance of TransitGPT compare to traditional GTFS analysis tools in terms of accuracy, efficiency, and user-friendliness for various types of queries?
- Basis in paper: [inferred] While the paper evaluates TransitGPT's performance on a benchmark dataset, it does not directly compare it to traditional GTFS analysis tools
- Why unresolved: The paper focuses on demonstrating TransitGPT's capabilities rather than conducting a comparative analysis with existing tools
- What evidence would resolve it: A head-to-head comparison of TransitGPT and traditional GTFS analysis tools across a range of queries, using standardized metrics for accuracy, efficiency, and user experience

## Limitations
- The framework's effectiveness depends heavily on the quality of prompt engineering and the LLMs' ability to generate accurate Python code
- The evaluation is based on a limited benchmark of 100 tasks and may not capture edge cases or more complex queries requiring deeper semantic reasoning
- The reliance on pre-processed GTFS feeds means the system may struggle with real-time data or feeds with unusual structures

## Confidence

- Mechanism 1 (Code generation from natural language): Medium - supported by abstract but lacks detailed implementation evidence
- Mechanism 2 (Prompt engineering and dynamic few-shot examples): Low - described but not thoroughly validated
- Mechanism 3 (Accessibility through natural language interface): Medium - demonstrated but with limited user testing

## Next Checks

1. Test the framework with edge cases in GTFS data, such as routes with unusual stop patterns or trips with complex timing relationships, to evaluate its robustness
2. Conduct a user study with transit agency staff and riders to assess the practical usability and accuracy of natural language queries in real-world scenarios
3. Evaluate the framework's performance on real-time GTFS-RT data to determine if it can handle dynamic transit information beyond static schedule data