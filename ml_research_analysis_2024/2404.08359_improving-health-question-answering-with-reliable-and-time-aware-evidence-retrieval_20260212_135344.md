---
ver: rpa2
title: Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval
arxiv_id: '2404.08359'
source_url: https://arxiv.org/abs/2404.08359
tags:
- evidence
- documents
- question
- health
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the impact of retrieval settings on the performance
  of an open-domain question-answering (QA) system for health questions. The authors
  modify parameters such as the number of retrieved documents, sentences extracted,
  and the publication year and citation count of evidence sources.
---

# Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval

## Quick Facts
- arXiv ID: 2404.08359
- Source URL: https://arxiv.org/abs/2404.08359
- Reference count: 25
- Key outcome: Retrieving fewer, more recent, and highly cited documents improves health QA macro F1 score by up to 10%

## Executive Summary
This paper investigates how retrieval settings impact the performance of open-domain question answering systems for health-related queries. Using a retrieve-then-read pipeline with BM25 document retrieval, SPICED sentence selection, and DeBERTa-v3 NLI-based answer prediction, the authors systematically vary the number of retrieved documents and sentences, as well as filter by publication year and citation count. Experiments across four biomedical and health datasets demonstrate that reducing noise through fewer, more recent, and highly cited evidence sources significantly improves answer accuracy, with macro F1 scores improving by up to 10%. The findings emphasize the importance of time-aware evidence retrieval and highlight challenges around evidence disagreement and user interpretability.

## Method Summary
The study employs a retrieve-then-read pipeline for health question answering. First, BM25 retrieves the top k documents from a corpus of 20.6 million PubMed abstracts. If sentence-level evidence is used, the top j sentences are selected from these documents using the SPICED model based on semantic similarity to the question. Each evidence unit is then classified by a DeBERTa-v3 model fine-tuned for natural language inference, and a majority vote determines the final yes/no answer. The experiments systematically vary k (1, 5, 10, 15, 20, 50, 100), j (1, 3, 5, 10, 15, 20), and filter evidence by publication year and citation count to measure their impact on macro F1 score.

## Key Results
- Retrieving fewer documents (k=20 optimal) improves macro F1 score by up to 10% by reducing noise in the evidence
- Time-aware retrieval favoring recent (2020+) and highly cited documents improves accuracy by up to 10%
- Sentence-level retrieval with SPICED outperforms full document retrieval for most datasets
- Optimal settings vary by dataset: k=20 documents and j=5 sentences for HealthFC-3, j=3 for HealthFC-2, k=20 for BioASQ-7b, and k=15 for TREC-Health

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing the number of retrieved documents improves QA performance by decreasing noise in the evidence.
- Mechanism: Fewer documents mean higher precision in the retrieved set, reducing irrelevant or conflicting information passed to the reader module.
- Core assumption: The retriever's top-ranked documents are more relevant than lower-ranked ones, and noise outweighs the benefit of additional coverage.
- Evidence anchors:
  - [abstract] "cutting down on the amount of retrieved documents ... can improve the final macro F1 score up to 10%"
  - [section] "For all four datasets, the worst performance was when retrieving the highest amount of documents (50 and 100) and slowly increased towards the lower values."
  - [corpus] Weak; the study uses PubMed abstracts but does not analyze individual document relevance distributions.
- Break condition: If the top-ranked documents miss critical evidence, performance will degrade regardless of reduced noise.

### Mechanism 2
- Claim: Time-aware retrieval (favoring recent and highly cited documents) improves QA performance by aligning evidence with current scientific consensus.
- Mechanism: Recent publications are more likely to reflect up-to-date findings, and highly cited papers indicate community validation, both improving answer accuracy.
- Core assumption: Gold labels in the datasets were derived using current evidence, so recent citations better match the ground truth.
- Evidence anchors:
  - [abstract] "favoring more recent and highly cited documents can improve the final macro F1 score up to 10%"
  - [section] "The more recent the selected documents were, the more accurate the answers ... the better the reputation of a paper (more citations), it could average median std.dev."
  - [corpus] Weak; citation counts come from Semantic Scholar API, but the study does not validate citation quality or recency trends.
- Break condition: If the most recent papers contain preliminary or controversial findings, relying on them may introduce error.

### Mechanism 3
- Claim: Selecting a small number of highly relevant sentences from abstracts improves QA performance compared to using full documents.
- Mechanism: Sentence-level retrieval focuses the reader on the most semantically relevant evidence, reducing irrelevant context.
- Core assumption: Abstracts contain concise, relevant sentences that directly address the question, making sentence-level selection effective.
- Evidence anchors:
  - [section] "we first retrieve the top 20 most similar abstracts ... all abstracts are split into sentences ... the top j most similar sentences to the question q ... are chosen"
  - [section] "In fact, for datasets HealthFC-3 and BioASQ, the effect is rather opposite ... The increased amount of knowledge in the bigger corpus of sentences helped the performance."
  - [corpus] Weak; sentence relevance depends on SPICED model performance, which is not benchmarked against human relevance judgments.
- Break condition: If relevant information is spread across multiple sentences or documents, sentence-level filtering may discard necessary context.

## Foundational Learning

- Concept: Open-domain QA pipeline structure
  - Why needed here: Understanding retriever-reader separation is critical to interpreting how retrieval changes affect final performance.
  - Quick check question: What happens if you swap the retriever but keep the reader fixed?

- Concept: Macro-averaged F1 vs. micro-averaged F1
  - Why needed here: Macro averaging treats all classes equally, which is important when class imbalance exists in health QA datasets.
  - Quick check question: Why might macro F1 be preferred over micro F1 for evaluating health QA?

- Concept: Evidence disagreement in scientific literature
  - Why needed here: The study highlights conflicting studies; understanding how disagreement affects QA systems is key to interpreting results.
  - Quick check question: How might evidence disagreement be modeled beyond simple majority voting?

## Architecture Onboarding

- Component map:
  - Retriever: BM25 (document-level) → SPICED (sentence-level)
  - Reader: DeBERTa-v3 NLI model
  - Evidence filtering: Metadata-based (year, citations)
  - Aggregation: Majority voting

- Critical path:
  1. Query → BM25 retrieval (top k documents)
  2. If sentence-level: split → SPICED similarity → top j sentences
  3. Reader predicts label for each evidence unit
  4. Majority vote → final label

- Design tradeoffs:
  - Precision vs. recall in retrieval: BM25 chosen for precision over recall.
  - Document vs. sentence evidence: Full documents give context but more noise; sentences are focused but may lose context.
  - Metadata filtering: Recency and citations improve quality but may introduce bias toward established research.

- Failure signatures:
  - Low recall in retriever → missing critical evidence
  - High variance in reader predictions → inconsistent evidence quality
  - Majority voting ties → ambiguous evidence

- First 3 experiments:
  1. Vary top k documents (1, 5, 10, 15, 20, 50, 100) → measure macro F1
  2. Vary top j sentences (1, 3, 5, 10, 15, 20) → measure macro F1
  3. Filter by publication year (≥ 2020, ≥ 2018, ≥ 2015, etc.) → measure macro F1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of health QA systems vary when incorporating additional metadata parameters beyond publication year and citation count, such as study type (e.g., randomized controlled trial, systematic review) or author expertise?
- Basis in paper: [explicit] The paper mentions that including the strength of evidence, such as different types of studies, could be an important factor in improving the reliability of answers.
- Why unresolved: The paper only explored the impact of publication year and citation count as metadata parameters. The effect of other metadata parameters on QA performance remains unexplored.
- What evidence would resolve it: Conducting experiments that incorporate various metadata parameters and measuring their impact on QA performance would provide insights into the effectiveness of these parameters.

### Open Question 2
- Question: How can evidence disagreement and label variation be effectively modeled and incorporated into health QA systems to improve their reliability and interpretability?
- Basis in paper: [explicit] The paper acknowledges the challenge of evidence disagreement and suggests that future work should focus on modeling human label variation and uncertainty in answering questions with diverse evidence documents.
- Why unresolved: The paper used majority voting to determine the final answer, which discards information about evidence disagreement and label variation. Developing methods to effectively model and incorporate this uncertainty is an open challenge.
- What evidence would resolve it: Developing and evaluating methods that can model and incorporate evidence disagreement and label variation into health QA systems would provide insights into improving their reliability and interpretability.

### Open Question 3
- Question: How can health QA systems be enhanced to provide more interpretable and user-friendly results, such as displaying different evidence documents, highlighting important sections, and showing prediction probabilities?
- Basis in paper: [explicit] The paper mentions that end users would appreciate making the results more interpretable, including displaying different evidence documents, highlighting important sections, and showing prediction probabilities.
- Why unresolved: The paper did not explore methods to enhance the interpretability and user-friendliness of health QA systems. Developing techniques to provide more interpretable and user-friendly results is an open challenge.
- What evidence would resolve it: Conducting experiments that incorporate methods to enhance the interpretability and user-friendliness of health QA systems, such as displaying different evidence documents, highlighting important sections, and showing prediction probabilities, would provide insights into improving their usability.

## Limitations

- The study's findings depend heavily on the quality of the BM25 retriever and SPICED sentence similarity model, neither of which is extensively validated against human judgments
- The use of citation counts as a proxy for paper quality assumes citation behavior accurately reflects scientific validity, which may not hold for controversial or emerging research areas
- The analysis focuses primarily on macro F1 scores without examining the distribution of errors across different question types or difficulty levels

## Confidence

- **High confidence**: The general finding that fewer retrieved documents improve performance due to reduced noise is well-supported by the consistent experimental results across all four datasets
- **Medium confidence**: The time-aware retrieval benefits (recent and highly cited documents) are supported by the data, but the assumption that gold labels align with current evidence is not explicitly validated
- **Low confidence**: The optimal values for retrieval parameters (k=20 documents, j=5 sentences) may be dataset-specific and may not generalize to other domains or question types

## Next Checks

1. **Cross-dataset validation**: Test whether the optimal retrieval settings (k=20, j=5) generalize to additional health QA datasets beyond the four used in this study
2. **Human evaluation**: Conduct expert review of retrieved evidence to verify that top-ranked documents and sentences are actually relevant to the questions
3. **Temporal validation**: Analyze whether answers based on recent literature remain accurate over time by comparing predictions against updated gold standards or expert consensus