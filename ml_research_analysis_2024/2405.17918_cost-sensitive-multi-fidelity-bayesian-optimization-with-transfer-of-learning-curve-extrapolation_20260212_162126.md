---
ver: rpa2
title: Cost-Sensitive Multi-Fidelity Bayesian Optimization with Transfer of Learning
  Curve Extrapolation
arxiv_id: '2405.17918'
source_url: https://arxiv.org/abs/2405.17918
tags:
- step
- regret
- normalized
- learning
- configurations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses cost-sensitive multi-fidelity Bayesian optimization
  (BO) for hyperparameter optimization (HPO), where the goal is to maximize a user-defined
  utility function that balances performance and computational cost. The authors propose
  a novel BO framework with a utility-based acquisition function and stopping criterion,
  which dynamically selects configurations expected to maximize future utility and
  automatically terminates BO around the optimal trade-off.
---

# Cost-Sensitive Multi-Fidelity Bayesian Optimization with Transfer of Learning Curve Extrapolation

## Quick Facts
- arXiv ID: 2405.17918
- Source URL: https://arxiv.org/abs/2405.17918
- Authors: Dong Bok Lee; Aoxuan Silvia Zhang; Byungjoo Kim; Junhyeon Park; Juho Lee; Sung Ju Hwang; Hae Beom Lee
- Reference count: 40
- Key outcome: This paper addresses cost-sensitive multi-fidelity Bayesian optimization (BO) for hyperparameter optimization (HPO), where the goal is to maximize a user-defined utility function that balances performance and computational cost. The authors propose a novel BO framework with a utility-based acquisition function and stopping criterion, which dynamically selects configurations expected to maximize future utility and automatically terminates BO around the optimal trade-off. They improve sample efficiency by incorporating transfer learning into a learning curve extrapolation method based on Prior-Fitted Networks (PFNs), trained with existing LC datasets using a mixup strategy. Experiments on three LC benchmarks show that their method significantly outperforms previous multi-fidelity and transfer-BO baselines, achieving better cost-performance trade-offs.

## Executive Summary
This paper introduces a novel Bayesian optimization framework that addresses the challenge of cost-sensitive hyperparameter optimization by incorporating utility-based acquisition functions and stopping criteria. The method leverages transfer learning to improve the sample efficiency of learning curve extrapolation, enabling better performance with fewer function evaluations. Through extensive experiments on three learning curve benchmarks, the authors demonstrate significant improvements over existing multi-fidelity and transfer-based BO methods, achieving superior cost-performance trade-offs.

## Method Summary
The method proposes a cost-sensitive multi-fidelity Bayesian optimization framework that maximizes user-defined utility functions balancing performance and computational cost. It uses Prior-Fitted Networks (PFNs) trained on existing learning curve datasets with a mixup strategy for efficient learning curve extrapolation. The framework includes a utility-based acquisition function that selects configurations expected to maximize future utility improvement, and a stopping criterion that terminates optimization when further improvements are unlikely to justify the computational cost. The approach is evaluated on three learning curve benchmarks, showing significant improvements over existing multi-fidelity and transfer-based BO methods.

## Key Results
- The proposed method achieves significantly better cost-performance trade-offs compared to previous multi-fidelity and transfer-BO baselines on three LC benchmarks
- Transfer learning with mixup strategy improves sample efficiency of learning curve extrapolation, enabling better performance with fewer function evaluations
- The utility-based acquisition function and stopping criterion successfully balance exploration and exploitation while automatically terminating BO around the optimal trade-off

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The utility-based acquisition function improves BO efficiency by dynamically balancing exploration and exploitation based on user-defined trade-offs.
- **Mechanism:** Instead of maximizing expected improvement in raw performance, the acquisition function maximizes expected improvement in utility, which incorporates both cost and performance. This allows the BO to shift from exploration to exploitation as costs accumulate.
- **Core assumption:** Users can define a meaningful utility function that captures their cost-performance preferences.
- **Evidence anchors:**
  - [abstract]: "This utility function, combined with our novel acquisition function and stopping criterion, allows us to dynamically choose for each BO step b > 1 the best configuration that we expect to maximally improve the utility in future"
  - [section]: "Instead of maximizing the expected improvement of validation performance y, we maximize the expected improvement of utility."
  - [corpus]: Weak - no direct mention of utility-based acquisition in related papers, but multi-fidelity BO papers mention cost-awareness.
- **Break condition:** If the utility function poorly reflects true user preferences, the BO will make suboptimal configuration choices.

### Mechanism 2
- **Claim:** The stopping criterion prevents unnecessary BO iterations by terminating when utility improvements no longer justify costs.
- **Mechanism:** The criterion compares the normalized regret of utility to a threshold that depends on the probability of future improvement. When this probability is low, the threshold is low, encouraging early stopping.
- **Core assumption:** The probability of improvement can be reasonably estimated from the LC extrapolator.
- **Evidence anchors:**
  - [abstract]: "automatically stop the BO around the maximum utility"
  - [section]: "We propose to stop when the following criterion is satisfied at each BO step b > 1"
  - [corpus]: Weak - related papers mention early stopping but not with utility-based criteria.
- **Break condition:** If the LC extrapolator is inaccurate, the stopping criterion may trigger too early or too late.

### Mechanism 3
- **Claim:** Transfer learning with mixup strategy improves sample efficiency by leveraging existing LC datasets.
- **Mechanism:** The LC extrapolator is trained on augmented data created by mixing across datasets and configurations, allowing it to generalize better from limited BO observations.
- **Core assumption:** LC datasets from different tasks share transferable patterns that can be exploited through mixup.
- **Evidence anchors:**
  - [abstract]: "we improve the sample efficiency of existing learning curve (LC) extrapolation methods with transfer learning"
  - [section]: "we propose to use Prior Fitted Networks (PFNs) for LC extrapolation... we use transfer learning, i.e., train PFNs with the existing LC datasets"
  - [corpus]: Moderate - some papers mention transfer learning in BO but not specifically with mixup strategy.
- **Break condition:** If the LC datasets are too dissimilar from the target task, transfer learning may hurt rather than help.

## Foundational Learning

- **Concept:** Bayesian Optimization (BO) fundamentals
  - Why needed here: The paper builds on BO framework for hyperparameter optimization
  - Quick check question: What is the role of the surrogate function in BO?

- **Concept:** Learning Curve (LC) extrapolation
  - Why needed here: LC extrapolation is crucial for the freeze-thaw BO framework used
  - Quick check question: How does LC extrapolation enable dynamic configuration selection?

- **Concept:** Transfer learning in ML
  - Why needed here: Transfer learning is used to improve sample efficiency of the LC extrapolator
  - Quick check question: What are the main challenges in applying transfer learning to BO?

## Architecture Onboarding

- **Component map:**
  - Utility function: User-defined function balancing cost and performance
  - Acquisition function: Selects next configuration based on expected utility improvement
  - Stopping criterion: Determines when to terminate BO based on utility regret
  - LC extrapolator: Predicts future learning curves using transfer-learned PFNs
  - Mixup strategy: Data augmentation technique for training LC extrapolator

- **Critical path:**
  1. User defines utility function
  2. LC extrapolator is trained on existing datasets using mixup
  3. BO begins with initial observations
  4. At each step: LC extrapolation → utility improvement estimation → configuration selection
  5. Check stopping criterion
  6. Terminate when criterion is met

- **Design tradeoffs:**
  - Utility function complexity vs. ease of specification
  - Transfer learning data augmentation vs. training time
  - LC extrapolator accuracy vs. computational efficiency
  - Stopping criterion sensitivity vs. premature termination risk

- **Failure signatures:**
  - Poor configuration selection → check utility function and acquisition implementation
  - Inaccurate LC predictions → check transfer learning training and mixup strategy
  - Premature stopping → check stopping criterion parameters and LC extrapolator accuracy
  - Slow convergence → check utility function and acquisition function implementation

- **First 3 experiments:**
  1. Implement utility function and verify it correctly balances cost and performance
  2. Train LC extrapolator on synthetic LC data and test extrapolation accuracy
  3. Run BO on a simple benchmark with known optimal configuration to verify convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we learn the utility function from data instead of requiring users to predefine it?
- Basis in paper: [explicit] The paper states "we assume that the utility function is predefined by a user" and suggests this as a limitation.
- Why unresolved: The authors acknowledge this is a limitation but do not propose methods for learning the utility function from user data or behavior.
- What evidence would resolve it: Experimental results showing a learned utility function that matches or exceeds the performance of predefined utility functions across different user preferences and tasks.

### Open Question 2
- Question: How does the LC extrapolator's performance scale with the size of training datasets?
- Basis in paper: [explicit] The authors note that PFNs "require relatively a large Transformer architecture [43] as well as huge amounts of training examples for good generalization performance [2]," and mention the risk of overfitting when training with finite datasets.
- Why unresolved: While the authors propose mixup as a data augmentation strategy, they do not empirically study how the extrapolator's performance changes with dataset size or compare it to other transfer learning approaches.
- What evidence would resolve it: Systematic experiments varying the size of LC datasets used for training, measuring extrapolation accuracy and BO performance across different dataset sizes.

### Open Question 3
- Question: What are the theoretical guarantees for the proposed stopping criterion?
- Basis in paper: [explicit] The authors propose a stopping criterion based on normalized regret and probability of improvement, but acknowledge it is an approximation and do not provide theoretical analysis.
- Why unresolved: The stopping criterion is based on heuristics and approximations rather than theoretical bounds on utility or regret.
- What evidence would resolve it: Theoretical analysis proving bounds on regret or utility when using the proposed stopping criterion, or empirical evidence showing its performance across a wide range of utility functions and problem settings.

## Limitations

- The utility function formulation requires careful calibration of the cost parameter α, which the paper provides limited guidance on choosing effectively
- The stopping criterion thresholds appear arbitrary without justification for their selection or guidance on adaptation for different scenarios
- The transfer learning approach assumes significant overlap between LC datasets and target tasks, which may not hold in practice, potentially leading to negative transfer

## Confidence

**High confidence**: The core framework combining utility-based acquisition with LC extrapolation is well-established in the BO literature. The mathematical formulation of the acquisition function and stopping criterion is clearly presented and internally consistent.

**Medium confidence**: The effectiveness of the mixup strategy for transfer learning is supported by experimental results but lacks theoretical justification for why this specific data augmentation approach would work well across diverse LC datasets.

**Low confidence**: The practical utility of the method depends heavily on the user's ability to specify an appropriate utility function and cost parameter α, which is not adequately addressed in the paper.

## Next Checks

1. **Ablation study on cost parameter α**: Systematically vary α across multiple orders of magnitude to understand its impact on BO performance and determine if there are robust ranges for different types of problems.

2. **Cross-dataset transfer learning evaluation**: Test the LC extrapolator on tasks from datasets not seen during training to assess the generality of the transfer learning approach and identify conditions where it succeeds or fails.

3. **Stopping criterion sensitivity analysis**: Evaluate how the BO performance varies with different threshold values (β, γ) to establish guidelines for parameter selection and understand the trade-offs between premature stopping and unnecessary computation.