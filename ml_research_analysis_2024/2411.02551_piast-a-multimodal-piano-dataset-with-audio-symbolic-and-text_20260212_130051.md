---
ver: rpa2
title: 'PIAST: A Multimodal Piano Dataset with Audio, Symbolic and Text'
arxiv_id: '2411.02551'
source_url: https://arxiv.org/abs/2411.02551
tags:
- music
- piano
- dataset
- audio
- midi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PIAST, a multimodal piano dataset featuring
  audio, symbolic (MIDI), and text annotations. The dataset includes 9,673 YouTube
  tracks (PIAST-YT) and 2,023 expert-annotated tracks (PIAST-AT), with piano-specific
  taxonomy covering genre, emotion, mood, and style.
---

# PIAST: A Multimodal Piano Dataset with Audio, Symbolic and Text

## Quick Facts
- arXiv ID: 2411.02551
- Source URL: https://arxiv.org/abs/2411.02551
- Reference count: 14
- Primary result: PIAST dataset with 9,673 YouTube tracks (PIAST-YT) and 2,023 expert-annotated tracks (PIAST-AT), showing MIDI outperforms audio for piano music tasks

## Executive Summary
This paper introduces PIAST, a multimodal piano dataset featuring audio, symbolic (MIDI), and text annotations. The dataset includes 9,673 YouTube tracks (PIAST-YT) and 2,023 expert-annotated tracks (PIAST-AT), with piano-specific taxonomy covering genre, emotion, mood, and style. The dataset supports music tagging and retrieval tasks, with experiments showing that MIDI outperforms audio across both tasks, achieving ROC-AUC scores of 84.82 and 85.69 respectively. Pre-training on the large PIAST-YT dataset improves performance, demonstrating its value for piano music research. The dataset addresses the scarcity of piano-focused multimodal resources, offering potential applications in music retrieval, generation, analysis, and classification.

## Method Summary
The PIAST dataset was constructed through a two-stage framework. First, 9,673 YouTube piano tracks were collected and processed to generate audio, transcribed MIDI, and text annotations. A subset of 2,023 tracks received expert annotations using a piano-specific taxonomy. For pre-training, contrastive loss was applied to learn joint embeddings across audio, MIDI, and text modalities using the PIAST-YT dataset. In the transfer learning stage, pre-trained encoders served as frozen feature extractors for downstream music tagging and retrieval tasks on PIAST-AT, with performance evaluated using ROC-AUC and PR-AUC metrics.

## Key Results
- MIDI representations achieved higher ROC-AUC scores (84.82 and 85.69) than audio for both music-to-tag and tag-to-music retrieval tasks
- Pre-training on PIAST-YT dataset significantly improved downstream classification performance on PIAST-AT
- The piano-specific taxonomy enabled comprehensive coverage of musical expressions across genre, emotion, mood, and style dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal pre-training with contrastive loss improves downstream classification performance.
- Mechanism: The model learns shared representations across audio, MIDI, and text modalities during pre-training, enabling better generalization to the smaller annotated dataset (PIAST-AT) during transfer learning.
- Core assumption: Contrastive loss effectively aligns embeddings across modalities, and the large PIAST-YT dataset provides sufficient diversity for meaningful pre-training.
- Evidence anchors:
  - [abstract] "Pre-training on the large PIAST-YT dataset improves performance, demonstrating its value for piano music research."
  - [section 3.1] "We applied contrastive loss to maximize similarity between corresponding pairs (audio-text or MIDI-text) while minimizing similarity with in-batch negative samples."

### Mechanism 2
- Claim: MIDI representations outperform audio representations for piano music tasks.
- Mechanism: MIDI captures precise pitch, timing, and performance dynamics that are crucial for piano-specific tasks, while audio may contain noise and interference from recording conditions.
- Core assumption: The transcription process accurately converts audio to MIDI, preserving essential musical information.
- Evidence anchors:
  - [abstract] "MIDI outperforms audio across both tasks, achieving ROC-AUC scores of 84.82 and 85.69 respectively."
  - [section 2.2.1] "The piano audio files were transcribed to performance MIDI using an automatic piano transcription model."

### Mechanism 3
- Claim: The piano-specific taxonomy captures diverse musical expressions better than general music taxonomies.
- Mechanism: By including genre, emotion, mood, and style tags specifically curated for piano music, the dataset enables more precise and relevant music retrieval and classification.
- Core assumption: The expert-curated taxonomy accurately represents the range of piano music expressions and is suitable for the target MIR tasks.
- Evidence anchors:
  - [abstract] "Utilizing a piano-specific taxonomy of semantic tags, we collected 9,673 tracks from YouTube and added human annotations for 2,023 tracks by music experts."
  - [section 2.1] "To encompass and precisely define the range of expressions possible in solo piano music, we constructed a comprehensive taxonomy considering genre, emotion, mood, and style tags."

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: It's the core mechanism for learning aligned representations across audio, MIDI, and text modalities during pre-training.
  - Quick check question: What is the primary objective of contrastive loss in multimodal pre-training?

- Concept: Music transcription
  - Why needed here: Accurate conversion of audio to MIDI is essential for creating symbolic representations that capture piano-specific performance characteristics.
  - Quick check question: What information does MIDI capture that audio alone may not preserve?

- Concept: Music information retrieval (MIR) tasks
  - Why needed here: Understanding the specific challenges and requirements of music tagging and retrieval tasks is crucial for evaluating the dataset's effectiveness.
  - Quick check question: What are the key differences between music-to-tag and tag-to-music retrieval tasks?

## Architecture Onboarding

- Component map: Data collection pipeline (YouTube scraping, filtering) -> Transcription pipeline (audio→MIDI conversion) -> Annotation pipeline (expert labeling) -> Pre-training framework (multimodal contrastive learning) -> Transfer learning framework (downstream classification) -> Evaluation framework (ROC-AUC, PR-AUC metrics)

- Critical path: Data collection → Transcription → Annotation → Pre-training → Transfer learning → Evaluation

- Design tradeoffs:
  - Larger PIAST-YT dataset vs. quality of PIAST-AT annotations
  - Automatic transcription vs. manual MIDI creation
  - Broader genre coverage vs. deeper subgenre representation

- Failure signatures:
  - Poor performance on genre tags may indicate taxonomy issues
  - Low agreement among annotators suggests unclear tag definitions
  - Large performance gap between audio and MIDI may indicate transcription quality issues

- First 3 experiments:
  1. Evaluate pre-training effectiveness by comparing models trained with and without PIAST-YT
  2. Assess modality contributions by training models using only audio, only MIDI, or both
  3. Analyze tag-wise performance to identify taxonomy strengths and weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of music tagging and retrieval tasks vary when using the PIAST-YT dataset for pre-training versus using other large-scale music datasets like Million Song Dataset or ECALS?
- Basis in paper: [explicit] The paper compares pre-training with PIAST-YT to a supervised model trained exclusively on PIAST-AT, showing improved performance with pre-training. However, it does not compare against other large-scale music datasets.
- Why unresolved: The paper only evaluates pre-training effectiveness using PIAST-YT, leaving open whether other datasets could provide similar or better performance improvements.
- What evidence would resolve it: Experimental results comparing pre-training on PIAST-YT against pre-training on other large-scale music datasets for the same tagging and retrieval tasks.

### Open Question 2
- Question: What is the impact of including sheet music or lead sheets as an additional modality in the PIAST dataset on music generation and analysis tasks?
- Basis in paper: [inferred] The paper mentions potential applications including "text-based music generation" and "music analysis" but only includes audio, MIDI, and text annotations. Sheet music could provide additional structural information.
- Why unresolved: The current dataset lacks sheet music representation, which could potentially enhance music generation and analysis capabilities that rely on symbolic musical structure.
- What evidence would resolve it: Comparative experiments showing performance differences in music generation and analysis tasks when using PIAST with and without sheet music annotations.

### Open Question 3
- Question: How does the performance of the PIAST dataset's tagging and retrieval tasks change when using the consensus degree information among annotators (n=3, n=2, n=1) as weighted features in the classification models?
- Basis in paper: [explicit] The paper mentions that the PIAST-AT dataset contains consensus degree information among annotators and generates hierarchical captions based on agreement levels, but does not explore using this information as features in classification models.
- Why unresolved: The dataset includes valuable information about annotator agreement that could potentially improve model performance if properly leveraged, but this approach has not been tested.
- What evidence would resolve it: Experimental results comparing model performance when incorporating consensus degree information as weighted features versus using the tags without consensus information.

## Limitations
- The transcription quality from audio to MIDI is critical but not independently verified, creating potential bottlenecks in downstream performance
- The piano-specific taxonomy, while comprehensive, may have limited generalizability to other instruments or broader musical contexts
- The evaluation focuses on standard MIR metrics without exploring the dataset's utility for more complex tasks like music generation or detailed performance analysis

## Confidence
- High confidence: The dataset construction methodology, taxonomy development, and basic experimental setup are clearly described and reproducible
- Medium confidence: The reported performance differences between audio and MIDI representations are plausible but depend heavily on the quality of the transcription process, which is not fully detailed
- Medium confidence: The effectiveness of pre-training on PIAST-YT is demonstrated, but the ablation studies are limited to comparing with random initialization rather than exploring other pre-training strategies

## Next Checks
1. Conduct an independent evaluation of the automatic piano transcription quality to establish the reliability of the MIDI representations
2. Perform cross-validation with additional piano datasets to test the generalizability of the taxonomy and model performance
3. Extend experiments to include more diverse downstream tasks (e.g., expressive performance analysis, style transfer) to assess the dataset's broader research potential