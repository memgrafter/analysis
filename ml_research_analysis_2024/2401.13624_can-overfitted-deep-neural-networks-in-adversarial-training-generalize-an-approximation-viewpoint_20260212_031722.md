---
ver: rpa2
title: Can overfitted deep neural networks in adversarial training generalize? --
  An approximation viewpoint
arxiv_id: '2401.13624'
source_url: https://arxiv.org/abs/2401.13624
tags:
- adversarial
- training
- generalization
- robust
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the theoretical understanding of whether overfitted
  deep neural networks in adversarial training can still generalize well. The authors
  focus on the phenomenon of robust overfitting, where adversarial training on over-parameterized
  networks achieves almost zero training error but fails to generalize robustly to
  test data.
---

# Can overfitted deep neural networks in adversarial training generalize? -- An approximation viewpoint

## Quick Facts
- arXiv ID: 2401.13624
- Source URL: https://arxiv.org/abs/2401.13624
- Authors: Zhongjie Shi; Fanghui Liu; Yuan Cao; Johan A. K. Suykens
- Reference count: 40
- Primary result: Proves existence of infinitely many overfitted DNNs that achieve both small adversarial training error and good robust generalization error under certain conditions

## Executive Summary
This paper addresses whether overfitted deep neural networks in adversarial training can still generalize well. The authors focus on robust overfitting, where adversarial training achieves near-zero training error but fails to generalize robustly to test data. Through theoretical analysis, they prove that over-parameterized DNNs can achieve both small adversarial training error and good robust generalization error under specific conditions regarding data quality, well-separated classes, and perturbation levels.

The core finding is that robust overfitting can be avoided with sufficient model capacity, which depends on the smoothness of the target function. However, the authors acknowledge that a robust generalization gap is inevitable. This analysis provides mathematical foundations for understanding robustness in DNNs from an approximation viewpoint, demonstrating that overfitted models are not necessarily poor performers in adversarial settings.

## Method Summary
The authors employ a theoretical construction approach to prove the existence of infinitely many adversarial training classifiers on over-parameterized deep neural networks. They establish conditions under which these classifiers can achieve arbitrarily small adversarial training error while maintaining good robust generalization error. The method involves analyzing the approximation capabilities of over-parameterized DNNs and deriving bounds on generalization error based on data quality, class separation, and perturbation constraints. For regression tasks, they extend their analysis to show that overfitted DNNs with linear over-parameterization can achieve optimal rates of convergence for standard generalization error.

## Key Results
- For classification, there exist infinitely many adversarial training classifiers on over-parameterized DNNs that achieve both arbitrarily small adversarial training error and good robust generalization error
- Linear over-parameterization (slightly larger than sample size) is sufficient to ensure this existence if the target function is smooth enough
- For regression, overfitted DNNs with linear over-parameterization can achieve almost optimal rates of convergence for standard generalization error

## Why This Works (Mechanism)
The theoretical framework demonstrates that over-parameterized deep neural networks have sufficient capacity to simultaneously fit training data perfectly while maintaining generalization properties. The key mechanism is that the approximation error decreases with model capacity, allowing the network to learn representations that are both adversarially robust and generalizable. The smoothness of the target function plays a crucial role in determining how much over-parameterization is needed, as smoother functions require less capacity to approximate well.

## Foundational Learning
- **Adversarial Training**: Training neural networks to be robust against adversarial perturbations by including adversarial examples in the training process. Needed to understand the problem context of robust overfitting.
- **Over-parameterization**: When the number of parameters in a neural network exceeds the number of training samples. Quick check: Verify that the network has more parameters than training samples.
- **Robust Generalization Error**: The difference between the model's performance on clean test data versus adversarial test data. Quick check: Compare clean accuracy versus adversarial accuracy on test set.
- **Approximation Theory**: Mathematical framework for analyzing how well functions can be approximated by neural networks. Quick check: Ensure the target function satisfies smoothness conditions.
- **VC Dimension and Generalization Bounds**: Theoretical tools for bounding generalization error based on model complexity. Quick check: Verify that generalization bounds scale appropriately with model capacity.

## Architecture Onboarding
- **Component Map**: Input Data -> DNN Model -> Adversarial Training -> Classifier/Regressor -> Generalization Error
- **Critical Path**: Data Quality/Well-separated Classes → Model Capacity → Perturbation Level → Training Error → Generalization Error
- **Design Tradeoffs**: Model capacity vs. generalization gap, smoothness assumptions vs. practical applicability, perturbation strength vs. robustness
- **Failure Signatures**: When data is not well-separated, when target function is not smooth enough, when perturbation levels exceed theoretical bounds
- **First Experiments**: 1) Vary model capacity while keeping data fixed to observe effect on generalization gap 2) Test different smoothness levels of target functions 3) Compare results across different perturbation norms (L∞, L2)

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Theoretical results rely heavily on specific assumptions about data quality, well-separated classes, and perturbation levels that may not hold in practical scenarios
- The claim that linear over-parameterization is sufficient depends critically on the smoothness of the target function, which is not always known or controllable in practice
- The distinction between achieving "good robust generalization error" versus "zero robust generalization error" is not clearly quantified

## Confidence
- High Confidence: The mathematical construction proving existence of infinitely many adversarial training classifiers that achieve both small training error and good robust generalization error
- Medium Confidence: The claim that linear over-parameterization is sufficient, depending on smoothness assumptions
- Low Confidence: The practical implications of avoiding robust overfitting while accepting an inevitable robust generalization gap

## Next Checks
1. Empirical validation on standard benchmark datasets (CIFAR-10, CIFAR-100, ImageNet) to verify whether theoretical bounds translate to actual performance improvements when varying model capacity and smoothness constraints
2. Investigation of how results change when relaxing the well-separated classes assumption using datasets with overlapping or ambiguous class boundaries
3. Systematic study of the relationship between perturbation level bounds and generalization performance across different perturbation norms (L∞, L2, etc.) to determine whether theoretical results hold across various adversarial attack scenarios