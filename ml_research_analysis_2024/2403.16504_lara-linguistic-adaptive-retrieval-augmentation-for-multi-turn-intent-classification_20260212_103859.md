---
ver: rpa2
title: 'LARA: Linguistic-Adaptive Retrieval-Augmentation for Multi-Turn Intent Classification'
arxiv_id: '2403.16504'
source_url: https://arxiv.org/abs/2403.16504
tags:
- intent
- multi-turn
- each
- single-turn
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LARA, a linguistic-adaptive retrieval-augmented
  framework designed to improve multi-turn intent classification accuracy across six
  languages. LARA leverages a fine-tuned smaller model to identify candidate intents,
  retrieves semantically similar single-turn examples, and employs in-context learning
  with large language models to enhance context understanding.
---

# LARA: Linguistic-Adaptive Retrieval-Augmentation for Multi-Turn Intent Classification
## Quick Facts
- arXiv ID: 2403.16504
- Source URL: https://arxiv.org/abs/2403.16504
- Reference count: 11
- Primary result: Achieves state-of-the-art performance on multi-turn intent classification across six languages using only single-turn training data

## Executive Summary
This paper introduces LARA, a linguistic-adaptive retrieval-augmented framework for multi-turn intent classification. The method leverages a fine-tuned smaller model to identify candidate intents, retrieves semantically similar single-turn examples, and employs in-context learning with large language models to enhance context understanding. By using only single-turn training data, LARA eliminates the need for costly multi-turn dataset annotation while achieving significant performance improvements across multiple languages.

## Method Summary
LARA employs a three-stage approach to multi-turn intent classification. First, a fine-tuned smaller model identifies candidate intents from the conversation history. Second, it retrieves semantically similar single-turn examples from the training corpus based on these candidates. Third, it uses in-context learning with a large language model, providing the retrieved examples alongside the conversation history to improve classification accuracy. The framework is designed to be language-agnostic and practical for real-time applications due to reduced inference time during in-context learning.

## Key Results
- Improves average accuracy by 3.67% over single-turn classifiers
- Outperforms selective concatenation baselines by 3.67%
- Demonstrates strong adaptability across six languages
- Eliminates need for multi-turn dataset annotation
- Reduces inference time during in-context learning for practical deployment

## Why This Works (Mechanism)
The retrieval-augmented approach addresses the core challenge of context understanding in multi-turn conversations by providing the model with semantically relevant single-turn examples that capture intent patterns. By identifying candidate intents first, the system narrows the search space for retrieval, making the process more efficient and focused. The in-context learning mechanism allows the large language model to leverage these retrieved examples as additional context, effectively bridging the gap between single-turn training data and multi-turn inference scenarios.

## Foundational Learning
- **Intent Classification**: Understanding user goals from conversational context; needed to frame the problem; quick check: verify classification accuracy on single-turn data
- **Multi-turn Conversations**: Context dependency across dialogue turns; needed to justify the approach; quick check: analyze conversation history length impact
- **Retrieval-Augmentation**: Using external knowledge sources during inference; needed for performance gains; quick check: measure retrieval precision and recall
- **In-context Learning**: Prompting LLMs with examples for task adaptation; needed for leveraging retrieved examples; quick check: test with varying numbers of examples
- **Cross-lingual Adaptation**: Applying models across multiple languages; needed for generalization claims; quick check: compare performance across language families
- **Efficient Inference**: Balancing accuracy with computational cost; needed for practical deployment; quick check: measure inference time per turn

## Architecture Onboarding
- **Component Map**: Fine-tuned model -> Intent candidate identification -> Semantic retrieval -> In-context learning -> Intent classification
- **Critical Path**: The retrieval and in-context learning stages are critical, as errors in candidate identification propagate through the pipeline and poor retrieval quality directly impacts classification performance
- **Design Tradeoffs**: Single-turn training data vs. multi-turn accuracy, retrieval precision vs. computational efficiency, and model size vs. inference speed
- **Failure Signatures**: Poor retrieval quality leading to irrelevant examples, candidate selection errors causing retrieval of incorrect intent examples, and in-context learning degradation with insufficient or noisy examples
- **First Experiment 1**: Ablation study removing the retrieval component to quantify its contribution
- **First Experiment 2**: Testing with varying numbers of retrieved examples (k=1,3,5,10) to find optimal in-context learning size
- **First Experiment 3**: Evaluating performance degradation with increasing conversation history length (2, 5, 10+ turns)

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope may not capture real-world conversational complexity where context evolves unpredictably
- Methodology's reliance on a "fine-tuned smaller model" for candidate intent identification lacks specification of architecture, training details, or error rates
- Absence of detailed ablations for each component makes it difficult to isolate which elements drive improvements
- Claim of being "language-agnostic" remains unverified for typologically diverse or low-resource languages

## Confidence
- Major claims: Medium - Core retrieval-augmentation mechanism appears sound with measurable performance improvements, but lacks component-level ablations and comprehensive error analysis
- Language adaptability: Medium - Tested across six languages but not verified for typologically diverse or low-resource language families
- Practical deployment claims: Medium - Reduced inference time demonstrated but real-world scalability not fully evaluated

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of candidate intent selection, semantic retrieval, and in-context learning to overall performance
2. Test the system on datasets with longer conversation histories (10+ turns) and evaluate performance degradation with increasing context length
3. Evaluate the retrieval component's precision by measuring how often the top-k retrieved examples actually belong to the correct intent class, particularly for ambiguous or rare intents