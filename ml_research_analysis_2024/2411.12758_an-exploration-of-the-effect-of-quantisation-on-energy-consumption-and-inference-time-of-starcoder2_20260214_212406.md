---
ver: rpa2
title: An exploration of the effect of quantisation on energy consumption and inference
  time of StarCoder2
arxiv_id: '2411.12758'
source_url: https://arxiv.org/abs/2411.12758
tags:
- energy
- consumption
- pass
- quantisation
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates quantization and pruning techniques to
  reduce energy consumption in code Large Language Models (LLMs) during inference,
  focusing on StarCoder2 models. Using model compression methods like 4-bit and 8-bit
  quantization, and pruning of final layers, the research evaluates energy usage and
  accuracy trade-offs.
---

# An exploration of the effect of quantisation on energy consumption and inference time of StarCoder2

## Quick Facts
- arXiv ID: 2411.12758
- Source URL: https://arxiv.org/abs/2411.12758
- Authors: Pepijn de Reus; Ana Oprescu; Jelle Zuidema
- Reference count: 40
- Primary result: Quantization increased energy consumption by 19%-75% while pruning reduced energy usage but impaired performance

## Executive Summary
This study investigates quantization and pruning techniques to reduce energy consumption in code Large Language Models (LLMs) during inference, focusing on StarCoder2 models. Using model compression methods like 4-bit and 8-bit quantization, and pruning of final layers, the research evaluates energy usage and accuracy trade-offs. Quantization reduced throughput, increasing energy consumption by 19%-75%, while pruning decreased accuracy without significant energy savings. Results indicate that quantization methods optimized for hardware could enhance efficiency. The study highlights challenges in balancing model compression with energy and performance trade-offs, suggesting future research on hardware-optimized quantization.

## Method Summary
The research applied 4-bit and 8-bit quantization using bitsandbytes library and pruned final layers of StarCoder2-3B and StarCoder2-7B models. Models were evaluated on HumanEval+ benchmark using EvalPlus framework for pass@1 scoring. Energy consumption was measured using CodeCarbon with NVIDIA SMI on Snellius supercomputer. Experiments compared energy usage, inference time, and accuracy across different compression methods and model sizes. The study systematically removed layers from the end of models and applied quantization to assess trade-offs between compression, energy efficiency, and model performance.

## Key Results
- 8-bit quantization increased energy consumption by 19%-75% due to throughput reduction despite lower per-token energy
- 4-bit quantization showed more efficient results with 19%-43% energy increase compared to 8-bit
- Pruning final layers reduced energy consumption but significantly impaired model accuracy, with large confidence intervals indicating high variability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization increases energy consumption because lower-bit formats slow down throughput more than they reduce per-token energy.
- Mechanism: Quantized models must perform more operations per token during dequantization, reducing throughput. Even though the per-second energy is lower, the increased inference time leads to higher total energy usage.
- Core assumption: The speedup from lower per-token energy is outweighed by the slowdown from dequantization and GPU inefficiency with integer math.
- Evidence anchors:
  - [abstract] "Using model compression methods like 4-bit and 8-bit quantization, and pruning of final layers, the research evaluates energy usage and accuracy trade-offs. Quantization reduced throughput, increasing energy consumption by 19%-75%..."
  - [section] "Our data shows that 8-bit quantisation leads to the highest increase, from 19%-75%. 4-bit quantisation seems more efficient and increases by 19%-43%. As we expected a reduction in energy consumption, this result was unexpected. Table IV clarifies the higher energy consumption of quantisation because quantised models require more time."
  - [corpus] Weak. Only general references to quantization efficiency in speech recognition and edge deployment; no specific mechanism for why quantization can increase energy in code LLMs.
- Break condition: If hardware provides fast integer-matrix multiplication or if the quantization method is optimized for the specific GPU architecture, the throughput slowdown may not occur and energy could decrease.

### Mechanism 2
- Claim: Pruning reduces energy consumption by removing computation from the model's forward pass.
- Mechanism: Removing the last layers eliminates the matrix multiplications and activations for those layers, so each inference requires fewer FLOPs and thus less energy.
- Core assumption: The last layers contribute the most to inference cost and removing them proportionally reduces energy use without introducing compensating inefficiencies.
- Evidence anchors:
  - [abstract] "Conversely, pruning reduces energy usage but impairs performance."
  - [section] "Figure 8 shows the energy consumption of Experiment 3. Apart from the large confidence interval, we see a trend in which removing layers results in reduced energy consumption for all models."
  - [corpus] Weak. General pruning surveys exist, but none directly measure energy reduction in code LLMs specifically.
- Break condition: If pruning introduces load imbalance or forces the remaining layers to work harder (e.g., more attention heads activated), the energy savings may be negated.

### Mechanism 3
- Claim: Quantization can maintain accuracy if the model has sufficient capacity and the quantization is done carefully.
- Mechanism: 8-bit quantization can preserve the dynamic range of weights closely enough that the model's learned representations remain intact, leading to similar accuracy on benchmarks.
- Core assumption: The original model's weights are not too extreme in magnitude, and the quantization granularity is fine enough to capture essential variations.
- Evidence anchors:
  - [abstract] "The study highlights challenges in balancing model compression with energy and performance trade-offs..."
  - [section] "We empirically show that in some cases the accuracy for quantised models can match that of original models, reinforcing the claims from [35]."
  - [corpus] Weak. General quantization papers discuss accuracy preservation, but none directly tie this to code LLMs with the same precision.
- Break condition: If the model's weights have large dynamic ranges or the quantization granularity is too coarse, accuracy will degrade even if energy increases.

## Foundational Learning

- Concept: Energy measurement in AI systems (power plug vs. software-based methods)
  - Why needed here: The study compares energy consumption across different model configurations, requiring understanding of how energy is measured and the limitations of each method.
  - Quick check question: What are the main differences between hardware-based and software-based energy measurement, and why might software methods be preferred in a research setting?

- Concept: Model quantization and its impact on computation
  - Why needed here: Quantization is the primary compression method studied, and understanding how it changes matrix operations and throughput is essential to interpreting results.
  - Quick check question: How does quantizing weights from FP32 to INT8 affect the number of bits processed per matrix multiplication, and what hardware considerations might influence the net energy impact?

- Concept: Pruning in transformer models and its effect on accuracy
  - Why needed here: Pruning is the second compression method studied, and understanding which layers are most critical for performance is key to interpreting the accuracy-energy trade-off.
  - Quick check question: Why might pruning the last layers of a transformer model be less harmful to accuracy than pruning earlier layers, and what architectural role do final layers typically play?

## Architecture Onboarding

- Component map: StarCoder2 model -> Quantization/Pruning -> Inference -> Energy Measurement -> Accuracy Evaluation
- Critical path:
  1. Load full-sized StarCoder2 model.
  2. Apply quantization (4-bit or 8-bit) or prune layers.
  3. Run inference on HumanEval+ benchmark.
  4. Record energy, time, and accuracy metrics.
  5. Repeat for statistical significance.
- Design tradeoffs:
  - Use of bitsandbytes for ease of quantization vs. potentially better throughput with other methods.
  - Shared computing environment (Slurm cluster) introduces node-to-node variability in energy measurements.
  - Evaluation on code-specific benchmarks (pass@1) vs. more general NLP metrics.
- Failure signatures:
  - Large confidence intervals in energy measurements suggest interference from other users or hardware variability.
  - Accuracy drop after quantization may indicate quantization granularity is too coarse for the model's weight distribution.
  - Pruning to zero accuracy indicates critical layers were removed; model becomes non-functional.
- First 3 experiments:
  1. Run the original StarCoder2-3B model on a small subset of HumanEval+ and record baseline energy, time, and pass@1.
  2. Apply 8-bit quantization to StarCoder2-3B and repeat the same inference, comparing metrics.
  3. Prune one layer from StarCoder2-3B, run inference, and record the changes in energy and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hardware-optimized quantization methods reduce energy consumption in code LLM inference without compromising accuracy?
- Basis in paper: [explicit] The authors suggest future work on hardware-optimized quantization to enhance efficiency with minimal loss in accuracy, noting that quantization methods optimized for GPU work could reduce runtime and consequently energy consumption.
- Why unresolved: Current quantization methods, like bitsandbytes, show increased energy consumption due to lower throughput, suggesting that existing methods are not optimized for hardware efficiency.
- What evidence would resolve it: Empirical studies comparing energy consumption and accuracy of hardware-optimized quantization methods (e.g., GPU-specific) against generic methods on code LLMs like StarCoder2.

### Open Question 2
- Question: What is the impact of targeted pruning strategies on the energy consumption and accuracy of code LLMs compared to naive pruning of the last layers?
- Basis in paper: [inferred] The authors note that more advanced pruning strategies could lead to a more balanced trade-off between size and performance, suggesting that current naive pruning methods are insufficient.
- Why unresolved: The study found that pruning layers reduced energy consumption but significantly impaired performance, indicating a need for more sophisticated pruning techniques.
- What evidence would resolve it: Comparative analysis of energy consumption and accuracy between targeted pruning strategies and naive pruning on code LLMs like StarCoder2.

### Open Question 3
- Question: How do different parallelization techniques affect the energy efficiency of quantized and pruned code LLMs during inference?
- Basis in paper: [inferred] The authors mention that techniques such as multi-threading or parallelization were not used in their experiments, suggesting potential improvements in energy efficiency.
- Why unresolved: The study did not explore parallelization techniques, leaving their impact on energy efficiency of compressed models unexplored.
- What evidence would resolve it: Experiments measuring energy consumption and throughput of quantized and pruned code LLMs using various parallelization techniques.

## Limitations

- The study cannot explain why 4-bit quantization sometimes shows lower energy consumption than 8-bit despite slower throughput, suggesting measurement noise or hardware-specific factors.
- Energy measurement methodology using CodeCarbon with NVIDIA SMI has known limitations for capturing short-duration GPU workloads and may not accurately reflect true energy costs.
- Pruning experiments show reduced energy consumption but with large confidence intervals, indicating high variability that isn't fully characterized.

## Confidence

**High Confidence:** The core finding that quantization increases energy consumption by 19%-75% is well-supported by direct measurements across multiple model sizes. The relationship between pruning and reduced energy consumption is also clearly demonstrated, though with high variance.

**Medium Confidence:** The explanation that quantization increases energy due to throughput reduction is plausible but incomplete. The unexplained case where 4-bit shows lower energy than 8-bit despite slower throughput creates uncertainty about the underlying mechanisms.

**Low Confidence:** The accuracy results for quantized models matching original models are presented without sufficient statistical analysis. The study claims accuracy preservation in some cases but doesn't quantify how often this occurs or under what conditions.

## Next Checks

1. **Quantization Method Validation:** Test alternative quantization libraries (e.g., AutoGPTQ, exllama) alongside bitsandbytes to determine if the energy consumption patterns are consistent across implementations or specific to the quantization method used.

2. **Hardware-Optimized Quantization:** Evaluate the same quantization techniques on different GPU architectures (e.g., A100 vs H100) to determine if the energy consumption patterns are hardware-dependent and identify which architectures benefit from quantization.

3. **Extended Pruning Analysis:** Conduct systematic pruning experiments that remove layers from different positions in the model (not just final layers) and measure both energy consumption and accuracy to identify optimal pruning strategies for energy efficiency.