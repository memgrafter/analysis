---
ver: rpa2
title: 'HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning'
arxiv_id: '2407.15680'
source_url: https://arxiv.org/abs/2407.15680
tags:
- hallucination
- haloquest
- image
- question
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'HaloQuest addresses hallucination in vision-language models (VLMs)
  by introducing a challenging benchmark dataset with both real and synthetic images.
  The dataset includes 7.7K examples across three hallucination-triggering categories:
  false premises, insufficient context, and visually challenging questions.'
---

# HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning

## Quick Facts
- **arXiv ID**: 2407.15680
- **Source URL**: https://arxiv.org/abs/2407.15680
- **Reference count**: 40
- **Primary result**: Current VLMs achieve below 36% accuracy on HaloQuest, demonstrating significant hallucination issues

## Executive Summary
HaloQuest addresses hallucination in vision-language models (VLMs) by introducing a challenging benchmark dataset with both real and synthetic images. The dataset includes 7.7K examples across three hallucination-triggering categories: false premises, insufficient context, and visually challenging questions. A key innovation is the use of synthetic images generated via prompt-based methods, enabling large-scale dataset creation with diverse and unusual visual content. Experiments show that current VLMs achieve below 36% accuracy on HaloQuest, demonstrating significant hallucination issues, while fine-tuning on HaloQuest reduces hallucination rates while maintaining performance on standard tasks.

## Method Summary
The method involves creating a dataset of 7.7K examples using both real images from Open Images and synthetic images generated through prompt-based methods. A machine-human-in-the-loop pipeline is used for question generation, where human annotators filter images and craft hallucination-triggering questions, followed by filtering using high-performing VQA models and human review. An LLM-based Auto-Eval mechanism evaluates VLM responses using a Langfun schema that extracts main points from both responses and ground truth. The dataset is then used to fine-tune VLMs to reduce hallucination rates while maintaining performance on standard tasks.

## Key Results
- Current VLMs achieve below 36% accuracy on HaloQuest, demonstrating significant hallucination issues
- Synthetic images correlate highly (r=0.97) with real images for benchmarking purposes
- Auto-Eval mechanism correlates strongly (r=0.99) with human evaluation
- Fine-tuning on HaloQuest reduces hallucination rates while maintaining performance on standard tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The synthetic image generation approach enables creation of visually challenging scenarios that real-world datasets cannot provide at scale.
- **Mechanism**: Prompt-based synthetic images circumvent the limitations of real-world datasets by generating unusual, complex, and abstract scenes that trigger hallucination in VLMs.
- **Core assumption**: Synthetic images can effectively simulate the visual complexity and diversity needed to test VLM robustness against hallucination.
- **Evidence anchors**: [abstract] "A novel idea from HaloQuest is to leverage synthetic images, apart from real ones, to enable dataset creation at scale." [section] "Utilizing prompt-based synthetic images circumvents this constraint, offering a cost-effective and scalable solution."
- **Break condition**: If synthetic images fail to correlate with real images for benchmarking purposes (though experiments show r=0.97 correlation).

### Mechanism 2
- **Claim**: The machine-human-in-the-loop pipeline creates high-quality, challenging examples that target specific VLM weaknesses.
- **Mechanism**: Human annotators filter images for interest and comprehensibility, then craft questions designed to trigger hallucination, followed by filtering of question-answer pairs using high-performing VQA models and human review.
- **Core assumption**: Human judgment can identify interesting yet comprehensible images and craft questions that effectively probe VLM weaknesses.
- **Evidence anchors**: [section] "These images could include scenes that defy real-world physics or logic. However, the images must be coherent, artifact-free and understandable by humans." [section] "This process leads to a dataset composed of challenging, high-quality examples."
- **Break condition**: If the filtering process removes too many examples or fails to maintain sufficient dataset size for effective training.

### Mechanism 3
- **Claim**: The Auto-Eval mechanism provides nuanced, open-ended evaluation that correlates strongly with human judgment.
- **Mechanism**: An LLM (Gemini) evaluates VLM responses using a Langfun schema that extracts main points from both responses and ground truth, then determines agreement.
- **Core assumption**: An LLM can accurately extract and compare the main points of VLM responses and ground truth answers to determine correctness.
- **Evidence anchors**: [abstract] "we propose a novel Auto-Eval mechanism that is highly correlated with human raters (r=0.99) for evaluating VLMs." [section] "This Auto-Eval system allows for nuanced, open-ended evaluation of model responses."
- **Break condition**: If the Auto-Eval mechanism fails to capture nuanced differences in responses or becomes less accurate as VLM capabilities advance.

## Foundational Learning

- **Concept: Hallucination in VLMs**
  - Why needed here: Understanding what hallucination means in the context of vision-language models is fundamental to grasping why HaloQuest was developed.
  - Quick check question: What distinguishes hallucination in VLMs from hallucination in text-only language models?

- **Concept: Multimodal reasoning**
  - Why needed here: The paper focuses on advancing multimodal reasoning capabilities, so understanding how vision and language are integrated is crucial.
  - Quick check question: How do vision-language models process and integrate information from both visual and textual inputs?

- **Concept: Dataset curation methodologies**
  - Why needed here: HaloQuest uses specific techniques for dataset creation including synthetic image generation and human-in-the-loop filtering, which require understanding of modern data curation practices.
  - Quick check question: What are the key considerations when designing a dataset specifically to test for model weaknesses?

## Architecture Onboarding

- **Component map**: Image collection pipeline (real + synthetic images) -> Question generation pipeline (human + LLM) -> Data filtering mechanism (VQA models + human annotators) -> Auto-Eval system (LLM-based evaluation) -> Training pipeline (fine-tuning on HaloQuest data)

- **Critical path**: 1. Image collection and filtering 2. Question generation and filtering 3. Auto-Eval implementation 4. Fine-tuning experiments 5. Zero-shot evaluation

- **Design tradeoffs**: Real vs. synthetic images: Synthetic images enable scale and diversity but may have slight performance discrepancies (r=0.97 correlation with real images). Human vs. automated evaluation: Human evaluation is gold standard but Auto-Eval enables scale. Question difficulty: More challenging questions provide better benchmarks but may reduce dataset size.

- **Failure signatures**: Low correlation between synthetic and real image performance. Poor agreement between Auto-Eval and human raters. Insufficient reduction in hallucination rates after fine-tuning. Degradation in performance on standard reasoning tasks after HaloQuest training.

- **First 3 experiments**: 1. Zero-shot evaluation on HaloQuest to establish baseline hallucination rates 2. Correlation analysis between synthetic and real image performance 3. Auto-Eval vs. human evaluation comparison to validate the automatic evaluation system

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but based on the discussion, several implicit questions emerge regarding the dataset's scope, evaluation methodology, and the generalizability of findings to broader VLM applications.

## Limitations

- Dataset size (7.7K examples) may be insufficient for comprehensive VLM training and evaluation
- Slight performance discrepancies between synthetic and real images suggest potential blind spots in synthetic image coverage
- Auto-Eval mechanism may not capture all nuanced failure modes as VLM capabilities evolve

## Confidence

- **High Confidence**: The effectiveness of synthetic images for large-scale dataset creation and the strong correlation with real images for benchmarking purposes.
- **Medium Confidence**: The robustness of the machine-human-in-the-loop pipeline for generating high-quality hallucination-triggering examples.
- **Medium Confidence**: The ability of fine-tuning on HaloQuest to reduce hallucination rates while maintaining performance on standard tasks.

## Next Checks

1. Conduct ablation studies varying the ratio of synthetic to real images to determine optimal composition for hallucination detection.
2. Perform temporal validation by testing Auto-Eval against human judgment across multiple VLM generations to ensure continued reliability.
3. Expand the dataset with additional examples in the hallucination-triggering categories to verify whether current size is sufficient for effective fine-tuning.