---
ver: rpa2
title: Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented
  Generation
arxiv_id: '2402.18150'
source_url: https://arxiv.org/abs/2402.18150
tags:
- llms
- retrieved
- info-rag
- texts
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INFO-RAG is an unsupervised training method for large language
  models (LLMs) that improves their ability to use retrieved information in retrieval-augmented
  generation (RAG). The key insight is to treat LLMs as "Information Refiners" that
  can consistently integrate knowledge from retrieved texts and model parameters to
  generate more concise, accurate, and complete outputs.
---

# Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2402.18150
- Source URL: https://arxiv.org/abs/2402.18150
- Authors: Shicheng Xu; Liang Pang; Mo Yu; Fandong Meng; Huawei Shen; Xueqi Cheng; Jie Zhou
- Reference count: 11
- One-line primary result: INFO-RAG improves LLaMA2's RAG performance by 9.39% relative points on average across 11 datasets in zero-shot settings

## Executive Summary
INFO-RAG introduces an unsupervised training method that reframes LLMs as "Information Refiners" capable of consistently producing positive information gain from retrieved texts regardless of their quality. The method addresses the fundamental gap between pre-training objectives (prefix language modeling) and RAG objectives (using retrieved information as reference) by training LLMs on three unsupervised tasks: Select and Copy, Correct and Complete, and Contextual Stimulation. Using Wikipedia data and LoRA fine-tuning, INFO-RAG demonstrates significant improvements across 11 datasets spanning 7 task categories while maintaining base model capabilities and showing advantages in in-context learning and robustness to retrieval quality.

## Method Summary
INFO-RAG constructs unsupervised training data from Wikipedia by extracting consecutive sentences and creating three scenarios: retrieved texts contain all knowledge, retrieved texts are incomplete/incorrect, or retrieved texts don't contain answers. The method uses LoRA fine-tuning to train LLaMA2 models on three tasks with specific masking strategies for each scenario, mixing them in 20%/40%/40% proportions. The training teaches LLMs to extract, verify, and supplement knowledge rather than simply conditioning generation on retrieved texts, with ColBERTv2 and SCODE-R providing top-K passages for text and code tasks respectively.

## Key Results
- INFO-RAG improves LLaMA2's zero-shot RAG performance by 9.39% relative points on average across 11 datasets
- The method demonstrates advantages in in-context learning and robustness to retrieval quality
- INFO-RAG avoids catastrophic forgetting while enhancing RAG capabilities, performing close to original LLaMA2 on MMLU

## Why This Works (Mechanism)

### Mechanism 1
INFO-RAG reframes LLMs from passive information consumers to active "Information Refiners" that can consistently integrate knowledge from retrieved texts and model parameters. The three unsupervised tasks teach models to extract, verify, and supplement knowledge regardless of retrieved text quality. This works because LLMs have sufficient internal knowledge to verify and correct information, and can learn to consistently extract relevant knowledge from complex contexts. The method may fail if models lack sufficient internal knowledge or if retrieved texts are too noisy for reliable verification.

### Mechanism 2
INFO-RAG addresses the fundamental gap between prefix language modeling and RAG usage by teaching LLMs to treat retrieved information as reference material rather than prefix continuation. The training data construction creates scenarios where retrieved texts are incomplete, incorrect, or insufficient, forcing the model to learn information refinement rather than simple continuation. This approach assumes the pre-training objective gap is the primary cause of RAG performance issues. The method may break if other factors like retrieval quality dominate performance, or if the gap isn't the primary issue.

### Mechanism 3
Unsupervised Wikipedia training provides sufficient diversity and scale to teach information refinement across multiple RAG tasks without catastrophic forgetting. The method uses Wikipedia's diverse content to simulate various RAG scenarios, trains with LoRA to maintain base model capabilities, and mixes tasks to ensure generalization. This assumes Wikipedia provides sufficient task diversity and that LoRA fine-tuning prevents catastrophic forgetting. The approach may fail if Wikipedia lacks sufficient diversity for real-world RAG scenarios or if LoRA proves insufficient for task-specific adaptation.

## Foundational Learning

- **Prefix Language Modeling vs. RAG Objectives**: Understanding the mismatch between how LLMs are trained (treating all input as prefix) versus how they should use retrieved information in RAG (as reference material) is crucial for grasping INFO-RAG's approach. Quick check: What's the key difference between minimizing NLL of the entire input sequence versus minimizing NLL of the question-answer subsequence when retrieved texts are present?

- **Information Gain in Different RAG Scenarios**: INFO-RAG defines three specific scenarios (all knowledge in retrieved texts, incomplete/incorrect retrieved texts, and no answer in retrieved texts) and expects different types of information gain in each. Quick check: In Scenario 2 where retrieved texts contain incomplete or incorrect knowledge, what three types of information gain should the LLM produce?

- **Unsupervised Training Data Construction**: INFO-RAG's effectiveness relies on cleverly constructed training data from Wikipedia that simulates various RAG scenarios without requiring manual annotation. Quick check: How does INFO-RAG simulate the scenario where retrieved texts contain incomplete or incorrect knowledge using Wikipedia data?

## Architecture Onboarding

- **Component map**: Wikipedia documents -> Sentence extraction -> Task assignment (3 scenarios) -> Data construction (masking/replacement) -> LoRA fine-tuning on LLaMA2 -> ColBERTv2/SCODE-R retrieval -> Evaluation on 11 datasets

- **Critical path**: 1) Extract consecutive sentences from Wikipedia documents 2) Randomly assign each sample to one of three scenarios 3) Apply scenario-specific transformations (masking, replacement, elimination) 4) Alternate between tasks using LoRA fine-tuning 5) Test zero-shot performance on diverse RAG tasks

- **Design tradeoffs**: Unsupervised training vs. supervised fine-tuning (generalization vs. task-specific optimization), Wikipedia data vs. task-specific data (scalability vs. domain specificity), LoRA fine-tuning vs. full fine-tuning (computational efficiency vs. performance ceiling), three-task mix vs. single-task focus (balanced capability vs. specialized optimization)

- **Failure signatures**: Performance degradation on base tasks (catastrophic forgetting), overfitting to Wikipedia-style content, insufficient improvement in noisy retrieval scenarios, task imbalance where certain INFO-RAG tasks dominate training

- **First 3 experiments**: 1) Baseline evaluation: Measure RAG performance of base LLaMA2 on all 11 datasets 2) Single-task evaluation: Train with only one of the three tasks and compare to baseline 3) Mixed-task evaluation: Train with all three tasks in 20%/40%/40% proportions and measure performance improvement

## Open Questions the Paper Calls Out

### Open Question 1
How does INFO-RAG's performance scale with larger LLM parameter sizes beyond 13B? The authors tested only 7B and 13B models due to computing resource limitations and plan to address this in future work. What evidence would resolve it: Training and evaluating INFO-RAG on multiple LLM sizes (e.g., 30B, 65B, 175B) and comparing the percentage performance improvement across these scales.

### Open Question 2
What is the optimal mix ratio of the three training tasks for different task types? The authors use a fixed mix ratio of 20%/40%/40% but don't explore whether different task categories might benefit from different ratios. What evidence would resolve it: Systematic experiments varying the mix ratios for different task categories and measuring performance to identify optimal ratios for each type.

### Open Question 3
How does INFO-RAG's performance change when using different retrieval methods beyond ColBERTv2 and BM25? The authors show INFO-RAG works with both ColBERTv2 and BM25 but only test these two methods. What evidence would resolve it: Comprehensive evaluation of INFO-RAG across multiple retrieval architectures and methods to determine its generalizability to different retrieval paradigms.

## Limitations

- The unsupervised Wikipedia-based training approach may not generalize well to specialized domains or languages beyond English
- LoRA-based fine-tuning may impose performance ceilings compared to full fine-tuning approaches, particularly for larger model sizes
- The method's reliance on retrieved texts as input means poor retrieval quality could still limit performance regardless of the LLM's refinement capabilities

## Confidence

**High Confidence**: The fundamental insight that LLMs struggle with retrieved information because pre-training objectives differ from RAG objectives is well-supported by the theoretical framework and experimental results across 11 datasets in 7 task categories.

**Medium Confidence**: The effectiveness of the three unsupervised training tasks in teaching information refinement is supported by experimental results, but specific task designs and their relative contributions could benefit from more ablation studies.

**Low Confidence**: The assertion that INFO-RAG completely avoids catastrophic forgetting while significantly improving RAG performance requires more extensive testing on non-RAG benchmarks, and scalability to much larger models remains unproven.

## Next Checks

1. **Domain Transfer Validation**: Test INFO-RAG's performance on specialized domains (medical, legal, technical documentation) that differ significantly from Wikipedia-style content to validate generalization claims and identify potential limitations in domain adaptation.

2. **Retrieval Quality Sensitivity Analysis**: Systematically vary retrieval quality (using different retrievers, k values, and query formulations) to determine whether INFO-RAG maintains its information refinement advantages across the full spectrum of retrieval scenarios, including highly noisy or irrelevant retrievals.

3. **Ablation Study on Task Contributions**: Conduct detailed ablation experiments where each of the three training tasks is individually removed or modified in proportion to determine their relative importance and identify potential task redundancies or synergies.