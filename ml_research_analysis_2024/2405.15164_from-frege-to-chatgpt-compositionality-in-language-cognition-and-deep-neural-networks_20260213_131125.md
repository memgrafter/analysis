---
ver: rpa2
title: 'From Frege to chatGPT: Compositionality in language, cognition, and deep neural
  networks'
arxiv_id: '2405.15164'
source_url: https://arxiv.org/abs/2405.15164
tags:
- compositionality
- neural
- language
- compositional
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper surveys the challenge of compositionality\u2014how human-like\
  \ language and cognition combine familiar elements into novel constructions\u2014\
  in deep neural networks (DNNs), a topic that has been debated since Fodor and Pylyshyn\
  \ (1988) argued that DNNs lack the structural mechanisms to replicate compositional\
  \ generalization. It reviews historical context, operationalizing compositionality\
  \ as the ability to generalize systematically to out-of-distribution (o.o.d.) data,\
  \ and evaluates standard DNNs on tasks like SCAN, which test whether models can\
  \ combine unseen words with learned rules."
---

# From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks

## Quick Facts
- arXiv ID: 2405.15164
- Source URL: https://arxiv.org/abs/2405.15164
- Authors: Jacob Russin; Sam Whitman McGrath; Danielle J. Williams
- Reference count: 21
- Standard DNNs fail on compositional generalization; metalearning and LLMs with prompting achieve near-perfect performance

## Executive Summary
This paper surveys the challenge of compositionality—combining familiar elements into novel constructions—in deep neural networks, a topic debated since Fodor and Pylyshyn (1988) argued DNNs lack structural mechanisms for compositional generalization. Initial results showed DNNs fail on tasks like SCAN that test systematic generalization to out-of-distribution data. However, recent advances using metalearning and large-scale pretraining (e.g., LLMs) can induce in-context learning algorithms capable of compositional generalization without explicit symbolic structures. These findings suggest DNNs can acquire compositionality through learned inductive biases rather than fixed architectures, challenging classical critiques and offering new perspectives on human cognition.

## Method Summary
The paper reviews approaches to compositional generalization in DNNs, focusing on metalearning transformers and large-scale pretraining in LLMs. Metalearning involves training on a distribution of compositional tasks to learn an inner-loop in-context learning algorithm that generalizes compositionally. Large-scale pretraining on next-word prediction over vast text corpora creates pressure to develop in-context learning capabilities. Evaluation involves in-context learning on held-out compositional tasks like SCAN with compositional splits. Prompt engineering strategies like chain-of-thought and least-to-most prompting can guide the in-context learning algorithm toward compositional solutions.

## Key Results
- Standard DNNs trained on SCAN fail dramatically on compositional splits (e.g., leaving out "jump" from training)
- Metalearning transformers achieve near-perfect performance on compositional generalization tasks
- LLMs with prompting strategies show significant improvements on compositional reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Metalearning transforms out-of-distribution generalization into in-distribution learning from the perspective of the inner-loop algorithm.
- **Mechanism**: During metalearning, the network is trained on a distribution of compositional tasks. This causes it to learn an inner-loop (in-context) learning algorithm that can generalize compositionally. From the perspective of the outer-loop (in-weight) algorithm, each task is sampled from the same distribution, so it can learn to support compositional behavior even though the original distribution is non-compositional.
- **Core assumption**: The training distribution during metalearning contains enough compositional generalization tasks to induce an inductive bias for compositionality in the inner-loop algorithm.
- **Evidence anchors**:
  - [abstract]: "metalearning... offers a novel perspective on how neural networks like LLMs... can come to reproduce the behavioral signatures of compositionality"
  - [section]: "metalearning creates a scenario where two kinds of generalization can coexist simultaneously: the IWL algorithm is simply generalizing i.i.d., while the ICL algorithm is capable of generalizing o.o.d."
  - [corpus]: Found 25 related papers, but no specific compositionality studies in the corpus to anchor this mechanism directly.
- **Break condition**: If the metalearning dataset lacks sufficient compositional generalization tasks, the inner-loop algorithm will not develop compositionality.

### Mechanism 2
- **Claim**: Large-scale pretraining on unstructured text induces an emergent in-context learning capability that supports compositional generalization.
- **Mechanism**: When transformers are trained on next-word prediction over vast text corpora, they encounter many instances where successful prediction requires composing novel concepts. This creates pressure to develop an in-context learning algorithm that can handle compositional generalization, even though the training objective itself is purely statistical.
- **Core assumption**: The distribution of text in the training corpus contains enough instances of compositional constructions that the model must learn to compose concepts in context to perform well.
- **Evidence anchors**:
  - [abstract]: "LLM pretraining can be understood as a kind of metalearning, and can thereby equip DNNs with compositional generalization abilities"
  - [section]: "large-scale pretraining in LLMs... can endow neural network models with sophisticated in-context learning abilities, and that these abilities can in some cases capture compositional generalization behaviors"
  - [corpus]: Weak evidence - corpus contains related papers but no direct studies showing pretraining induces compositionality.
- **Break condition**: If the training corpus lacks compositional constructions, the model will not develop compositional capabilities.

### Mechanism 3
- **Claim**: Prompt engineering can activate and guide the in-context learning algorithm toward compositional generalization.
- **Mechanism**: The way a model is prompted determines which aspects of its in-context learning algorithm are activated. Chain-of-thought and least-to-most prompting provide explicit scaffolding that helps the model apply its compositional capabilities to novel problems by breaking them down into manageable steps.
- **Core assumption**: The in-context learning algorithm has compositional capabilities that can be activated through appropriate prompting strategies.
- **Evidence anchors**:
  - [abstract]: "LLMs... especially with prompting strategies, achieve near-perfect performance on compositional tasks"
  - [section]: "prompting methods, called 'chain-of-thought' prompting... and 'least-to-most' prompting... have been shown to improve performance on many reasoning tasks"
  - [corpus]: No direct corpus evidence for prompting effects on compositionality.
- **Break condition**: If prompting strategies don't align with how the model's in-context learning algorithm works, performance gains will not materialize.

## Foundational Learning

- **Concept**: Out-of-distribution vs in-distribution generalization
  - Why needed here: The paper frames compositionality as requiring o.o.d. generalization, while the proposed solutions convert this to i.i.d. learning from the perspective of the outer-loop algorithm
  - Quick check question: If a model is trained only on sequences like "A B" and "B C", can it correctly generate "A C" without being explicitly trained on this composition? Why or why not?

- **Concept**: Inner-loop vs outer-loop learning algorithms
  - Why needed here: The paper distinguishes between the IWL algorithm that trains weights and the ICL algorithm that operates in forward pass to handle new tasks
  - Quick check question: In a metalearning setup, what is frozen during evaluation - the weights or the forward pass computation?

- **Concept**: Inductive biases and their origins
  - Why needed here: The paper argues that compositionality can emerge from learned inductive biases rather than being built into architecture
  - Quick check question: If two neural networks have identical architectures but are trained on different data distributions, will they necessarily learn the same inductive biases?

## Architecture Onboarding

- **Component map**: Transformer base architecture -> Metalearning/pretraining applied -> Frozen weights during evaluation -> In-context learning algorithm handles new tasks
- **Critical path**: 1) Metalearning/pretraining phase - train on task distribution, 2) Evaluation phase - freeze weights, provide in-context examples, 3) In-context processing - ICL algorithm composes solutions from examples
- **Design tradeoffs**: Metalearning provides explicit control over task distribution but requires curated datasets. Large-scale pretraining is more scalable but less controllable. Prompt engineering is lightweight but may not always work.
- **Failure signatures**: Poor performance on compositional tasks despite success on standard benchmarks suggests the ICL algorithm hasn't learned compositionality. Sudden drops in performance with slight task variations indicate reliance on shallow heuristics rather than true composition.
- **First 3 experiments**:
  1. Train a transformer on SCAN task with random train/test split to confirm baseline performance
  2. Replicate compositional generalization test by excluding "jump" from training (except "jump" → JUMP) and evaluate on test set with "jump twice" etc.
  3. Train a metalearning transformer on distribution of compositional tasks and evaluate on held-out compositional generalization tasks using in-context learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do metalearning and large-scale pretraining in LLMs influence the emergence of compositional generalization in neural networks?
- Basis in paper: [explicit] The paper discusses how metalearning and large-scale pretraining can endow neural networks with sophisticated in-context learning abilities, enabling compositional generalization behaviors.
- Why unresolved: While the paper suggests that these methods can facilitate compositional generalization, the exact mechanisms by which they influence the emergence of these behaviors remain unclear.
- What evidence would resolve it: Detailed studies comparing the compositional generalization abilities of networks trained with and without metalearning or large-scale pretraining, alongside analyses of the learned representations, would help clarify the influence of these methods.

### Open Question 2
- Question: What role do neural mechanisms play in human compositionality, and how do they compare to those in neural networks?
- Basis in paper: [explicit] The paper suggests that metalearning models can inform our understanding of human compositionality, particularly regarding neural mechanisms and development.
- Why unresolved: The paper highlights the potential for neural networks to model human compositionality but does not fully explore the specific neural mechanisms involved in humans.
- What evidence would resolve it: Comparative studies of neural activity in humans and neural networks during compositional tasks, along with developmental studies, would provide insights into the neural mechanisms underlying compositionality.

### Open Question 3
- Question: How does the distribution of tasks in metalearning and pretraining datasets affect the compositional generalization abilities of neural networks?
- Basis in paper: [explicit] The paper discusses how metalearning and pretraining can impart inductive biases that facilitate compositional generalization, but the impact of task distribution on these biases is not fully explored.
- Why unresolved: While the paper suggests that the distribution of tasks can influence the emergence of compositional generalization, the specific effects of different task distributions are not well understood.
- What evidence would resolve it: Experiments manipulating the distribution of tasks in metalearning and pretraining datasets, along with analyses of the resulting compositional generalization abilities, would help clarify the role of task distribution.

## Limitations

- The paper relies on the assumption that training distributions contain sufficient compositional generalization tasks, but this is not empirically validated
- The mechanism by which large-scale pretraining on unstructured text induces compositionality remains somewhat speculative
- The theoretical framing of metalearning converting o.o.d. to i.i.d. generalization is logically sound but requires broader empirical validation

## Confidence

- **High confidence**: The empirical finding that standard DNNs fail on compositional generalization tasks like SCAN, and that metalearning approaches can achieve near-perfect performance on these same tasks.
- **Medium confidence**: The claim that large-scale pretraining can induce compositionality through emergent in-context learning capabilities, based primarily on observational evidence rather than controlled experiments isolating the mechanism.
- **Medium confidence**: The theoretical framing of metalearning as converting o.o.d. generalization into i.i.d. learning from the perspective of the outer-loop algorithm, which is logically sound but requires empirical validation across diverse compositional tasks.

## Next Checks

1. **Corpus Analysis**: Analyze the training corpora of large LLMs to quantify the distribution of compositional constructions versus non-compositional patterns, directly testing whether sufficient compositional examples exist to explain the emergence of compositionality.

2. **Controlled Ablation Study**: Compare metalearning transformers trained on compositional vs non-compositional task distributions to isolate whether the compositional generalization ability specifically depends on the presence of compositional tasks in the training distribution.

3. **Cross-Architecture Transfer**: Test whether compositionality learned through metalearning in one architecture (e.g., transformer) transfers to different architectures (e.g., RNN, CNN) when frozen weights are transferred, helping to distinguish between learned algorithms versus architecture-specific features.