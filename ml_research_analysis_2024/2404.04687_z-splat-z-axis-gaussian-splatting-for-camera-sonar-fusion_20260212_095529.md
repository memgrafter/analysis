---
ver: rpa2
title: 'Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion'
arxiv_id: '2404.04687'
source_url: https://arxiv.org/abs/2404.04687
tags:
- splatting
- camera
- gaussian
- scene
- sonar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the 'missing cone' problem in 3D Gaussian splatting
  when only limited camera viewpoints are available, resulting in poor depth-axis
  reconstruction. The proposed method extends Gaussian splatting to incorporate depth-resolved
  sonar data (echosounder and forward-looking sonar) by introducing z-axis splatting
  operations that capture depth information orthogonal to camera measurements.
---

# Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion

## Quick Facts
- arXiv ID: 2404.04687
- Source URL: https://arxiv.org/abs/2404.04687
- Reference count: 40
- The method extends Gaussian splatting with z-axis operations to fuse camera and depth-resolved sonar data, achieving 5 dB PSNR improvement and 60% lower Chamfer distance compared to camera-only approaches.

## Executive Summary
Z-Splat addresses the 'missing cone' problem in 3D Gaussian splatting when limited camera viewpoints are available, which results in poor depth-axis reconstruction. The proposed method introduces z-axis splatting operations that capture depth information orthogonal to camera measurements by incorporating depth-resolved sonar data. By jointly optimizing RGB and sonar losses, the approach reconstructs both photometric and geometric properties. Evaluated across simulated, emulated, and real hardware datasets, the fusion approach demonstrates significant improvements in novel view synthesis and 3D geometry reconstruction.

## Method Summary
The method extends traditional Gaussian splatting by introducing z-axis splatting operations that operate perpendicular to the camera measurement plane. This enables the fusion of depth-resolved sonar data (from echosounders and forward-looking sonar) with RGB camera inputs. The approach jointly optimizes photometric consistency for RGB data and geometric consistency for sonar depth measurements, creating a unified representation that captures both appearance and structure. The z-axis operations specifically address the missing cone problem where limited viewpoints leave large regions of depth space poorly constrained.

## Key Results
- Achieves 5 dB improvement in PSNR compared to camera-only Gaussian splatting
- Reduces Chamfer distance by 60% in 3D geometry reconstruction
- Successfully reconstructs both photometric and geometric properties across simulated, emulated, and real hardware datasets

## Why This Works (Mechanism)
The method works by introducing z-axis splatting operations that capture depth information orthogonal to traditional camera measurements. By fusing depth-resolved sonar data with RGB inputs, the approach fills the "missing cone" of information that occurs when limited camera viewpoints leave depth space poorly constrained. The joint optimization of RGB and sonar losses ensures both photometric fidelity and geometric accuracy are preserved in the unified 3D representation.

## Foundational Learning
- Gaussian splatting fundamentals: Core technique for 3D reconstruction from 2D images using point-based representations
  - Why needed: Provides the baseline framework that Z-Splat extends
  - Quick check: Understanding how Gaussian primitives are placed, scaled, and optimized

- Sonar depth resolution: How echosounders and forward-looking sonar provide depth measurements
  - Why needed: Z-Splat specifically leverages depth-resolved sonar data
  - Quick check: Understanding sonar data formats and depth measurement principles

- Missing cone problem: The geometric limitation where limited viewpoints leave large regions of 3D space poorly constrained
  - Why needed: Primary motivation for Z-Splat's z-axis extension
  - Quick check: Visualizing how sparse camera views create reconstruction gaps

- Multi-modal fusion: Techniques for combining data from different sensor modalities
  - Why needed: Z-Splat jointly optimizes RGB and sonar data
  - Quick check: Understanding loss functions that can handle heterogeneous data

- Depth-based loss functions: How geometric distances and depth consistency are measured
  - Why needed: Sonar optimization requires depth-specific metrics
  - Quick check: Comparing photometric vs geometric loss formulations

## Architecture Onboarding

**Component Map:** Camera RGB frames -> RGB encoder -> RGB loss -> 3D Gaussian representation <- Sonar depth frames -> Sonar encoder -> Sonar loss

**Critical Path:** Sensor input → Multi-modal encoder → Joint optimization → Z-axis splatting → 3D reconstruction

**Design Tradeoffs:** The method trades increased computational complexity and memory requirements for improved depth reconstruction accuracy. The z-axis operations add significant overhead compared to standard Gaussian splatting but enable reconstruction in regions where camera-only approaches fail.

**Failure Signatures:** Poor performance when sonar data is noisy, sparse, or missing in certain regions; degraded results in environments with varying turbidity or acoustic properties; limited effectiveness with moving objects or changing scene geometry.

**First Experiments:**
1. Evaluate performance across multiple underwater environments with varying turbidity, depth, and lighting conditions to assess robustness
2. Conduct ablation studies comparing z-axis splatting with alternative fusion approaches (depth completion, multi-view stereo) on identical datasets
3. Measure computational overhead and memory requirements of z-axis operations compared to standard Gaussian splatting with profiling on embedded hardware targets

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the limitations section implies several areas requiring further investigation, including real-world deployment readiness, performance in dynamic environments, and computational optimization for embedded systems.

## Limitations
- Evaluation based on limited datasets with unclear sample size and diversity across environmental conditions
- Assumes availability of high-quality, depth-resolved sonar data without addressing noisy or sparse sonar measurements
- Computational complexity and memory requirements for z-axis operations not quantified
- Real-world applicability to underwater environments with varying turbidity and acoustic properties not demonstrated

## Confidence

| Claim | Confidence |
|-------|------------|
| Technical feasibility of z-axis splatting extension | High |
| Quantitative performance improvements (5 dB PSNR, 60% lower Chamfer) | Medium |
| Real-world deployment readiness | Low |

## Next Checks

1. Evaluate performance across multiple underwater environments with varying turbidity, depth, and lighting conditions to assess robustness
2. Conduct ablation studies comparing z-axis splatting with alternative fusion approaches (depth completion, multi-view stereo) on identical datasets
3. Measure computational overhead and memory requirements of z-axis operations compared to standard Gaussian splatting with profiling on embedded hardware targets