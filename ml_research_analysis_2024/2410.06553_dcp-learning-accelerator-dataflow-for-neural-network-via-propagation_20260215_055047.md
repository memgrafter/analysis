---
ver: rpa2
title: 'DCP: Learning Accelerator Dataflow for Neural Network via Propagation'
arxiv_id: '2410.06553'
source_url: https://arxiv.org/abs/2410.06553
tags:
- dataflow
- accelerator
- accelerators
- data
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DCP, a data-centric method to automatically
  find optimal dataflow configurations for DNN hardware accelerators. DCP encodes
  dataflow and DNN layer configurations into a unified coding space, trains a neural
  predictor to estimate hardware metrics (latency, energy), and then uses gradient-based
  optimization to efficiently search for optimal dataflow configurations.
---

# DCP: Learning Accelerator Dataflow for Neural Network via Propagation

## Quick Facts
- arXiv ID: 2410.06553
- Source URL: https://arxiv.org/abs/2410.06553
- Authors: Peng Xu; Wenqi Shao; Mingyu Ding; Ping Luo
- Reference count: 20
- Key outcome: DCP achieves 98.5-99.9% EDP reduction on MobileNet-V2, ResNet101, and Vision Transformer compared to state-of-the-art accelerators

## Executive Summary
This paper presents DCP, a data-centric method that automatically finds optimal dataflow configurations for DNN hardware accelerators. DCP encodes dataflow and DNN layer configurations into a unified coding space, trains a neural predictor to estimate hardware metrics (latency, energy), and uses gradient-based optimization to efficiently search for optimal configurations. The method achieves significant improvements over existing approaches, reducing energy-delay-product (EDP) by up to 99.9% on tested models while being 3-6x faster than conventional search methods.

## Method Summary
DCP builds a benchmark dataset by encoding DNN layers and dataflows into unified codes, then trains a neural predictor using this data to estimate hardware metrics. The predictor consists of a 4-layer self-attention encoder and multiple prediction heads for different metrics. During optimization, DCP applies gradient descent on the dataflow codes to minimize target metrics, with options for layer-level, model-level, or multi-objective optimization. The method uses integer rounding and clipping to ensure hardware constraints are satisfied.

## Key Results
- Reduces EDP by 98.5% on MobileNet-V2, 99.5% on ResNet101, and 99.9% on Vision Transformer compared to Eyeriss baseline
- Achieves 3-6x faster search time than conventional methods
- Successfully generalizes to unseen hardware configurations in zero-shot and few-shot settings
- Outperforms hand-crafted dataflow designs and other search methods across all tested models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DCP efficiently searches optimal dataflow configurations in seconds without human effort
- Mechanism: Uses encoding scheme to translate dataflow and DNN layer configurations into unified coding space, trains neural predictor to estimate hardware metrics, applies gradient-based optimization to update dataflow codes
- Core assumption: Neural predictor accurately maps unified dataflow code to hardware metrics and generalizes to unseen configurations
- Evidence: [abstract] "DCP encodes dataflow and DNN layer configurations into a unified coding space, trains a neural predictor to estimate hardware metrics... and then uses gradient-based optimization to efficiently search for optimal dataflow configurations."

### Mechanism 2
- Claim: DCP optimizes dataflow at both layer-level and model-level simultaneously
- Mechanism: Accumulates gradients across all layers within a model (model-level) or optimizes each layer individually (layer-level) to find best dataflow configuration
- Core assumption: Model-level optimization by accumulating gradients across layers yields better overall performance than layer-level optimization
- Evidence: [section 4.3] "Model-level Dataflow Propagation... DCP can search for the best dataflow that is optimized for all layers within a DNN model simultaneously."

### Mechanism 3
- Claim: DCP optimizes dataflow for multiple objectives simultaneously
- Mechanism: Uses weighted average of gradients from multiple hardware metrics (latency, energy, EDP) for back-propagation
- Core assumption: Weighted average of gradients leads to Pareto-optimal solution balancing all target metrics
- Evidence: [section 4.3] "Multiple Objective Propagation... uses the weighted average of the gradients of target metrics to perform the back-propagation."

## Foundational Learning

- Concept: Hardware accelerator architecture and dataflow design
  - Why needed: Understanding how dataflow affects data reuse patterns, parallelism, and memory hierarchy is crucial for designing efficient neural network accelerators
  - Quick check: What are the three main factors that describe a dataflow in DNN accelerators?

- Concept: Neural network architecture and layer configurations
  - Why needed: DCP needs to understand relationship between different DNN layer configurations and their optimal dataflow requirements
  - Quick check: How do different DNN architectures (MobileNet, ResNet, ViT) typically differ in their layer configurations?

- Concept: Machine learning and optimization techniques
  - Why needed: DCP uses neural predictors and gradient-based optimization, requiring understanding of training predictors and applying gradient descent to optimize complex systems
  - Quick check: What are advantages of using gradient-based optimization over traditional search methods for large design spaces?

## Architecture Onboarding

- Component map: DNN Layer Code (7D) -> Encoder (4-layer self-attention) -> Prediction Heads -> Hardware Metrics
- Critical path: 1) Build benchmark dataset with dataflow and DNN layer configurations 2) Train neural predictor on benchmark data 3) Use predictor to estimate hardware metrics 4) Apply gradient-based optimization to update dataflow codes 5) Evaluate optimized dataflow performance
- Design tradeoffs: Encoding scheme complexity vs. search space size, predictor accuracy vs. training time, layer-level vs. model-level optimization granularity, single vs. multiple objective optimization
- Failure signatures: Predictor cannot accurately estimate hardware metrics, optimization gets stuck in local optima, gradient updates violate hardware constraints, model-level optimization conflicts with layer-level requirements
- First 3 experiments: 1) Train predictor on small subset of benchmark data and evaluate prediction accuracy 2) Optimize dataflow for single layer and verify performance improvement 3) Optimize dataflow for entire model and compare with hand-crafted solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DCP's performance scale with increasingly complex DNN architectures beyond tested ResNet101, MobileNet-V2, and ViT models?
- Basis: Paper tests three representative models but does not explore performance on more complex or diverse architectures
- Why unresolved: Focuses on these three models for evaluation, leaving scalability question open
- What evidence would resolve it: Empirical results on broader range of DNN architectures including more complex models

### Open Question 2
- Question: What are limitations of DCP when applied to hardware platforms with significantly different memory hierarchies or processing element configurations?
- Basis: Paper discusses generalization to unseen hardware settings but does not detail performance on platforms with drastically different configurations
- Why unresolved: Mentions generalization but does not explore boundaries of DCP's adaptability to vastly different hardware setups
- What evidence would resolve it: Testing on variety of hardware platforms with diverse memory hierarchies and PE configurations

### Open Question 3
- Question: How does computational overhead of training neural predictor in DCP compare to runtime efficiency gains achieved during inference?
- Basis: Paper mentions predictor training is "almost negligible" but lacks detailed comparison of training overhead versus runtime efficiency
- Why unresolved: Highlights efficiency but lacks quantitative analysis of trade-off between training time and runtime performance
- What evidence would resolve it: Detailed analysis comparing time and computational resources required for training versus efficiency gains during inference

### Open Question 4
- Question: Can DCP be effectively integrated with other hardware optimization techniques, such as pruning or quantization, to further enhance DNN accelerator performance?
- Basis: Paper focuses on dataflow optimization but does not explore integration with other optimization techniques like pruning or quantization
- Why unresolved: While DCP optimizes dataflow, potential synergies with other optimization methods remain unexplored
- What evidence would resolve it: Experimental results demonstrating combined effects of DCP with pruning or quantization on hardware performance

## Limitations
- Generalization capability to hardware configurations beyond training data range is not fully validated
- Encoding scheme scalability to more complex dataflow designs with additional parameters is unclear
- Comprehensive analysis of trade-off between search time and optimization quality is lacking
- Limited comparison against alternative learning-based approaches in detail

## Confidence

- High confidence: DCP's ability to optimize dataflow configurations using gradient-based search on unified coding space
- Medium confidence: Claims about zero-shot and few-shot generalization
- Medium confidence: Model-level optimization superiority over layer-level optimization

## Next Checks

1. Test DCP's predictor on hardware configurations with PE counts and memory sizes that are 2-3Ã— larger than those in the training set to validate true zero-shot generalization capabilities
2. Compare DCP's optimization quality against random search and evolutionary algorithms on the same benchmark datasets to quantify the advantage of gradient-based optimization
3. Evaluate the impact of different encoding scheme complexities (e.g., varying the dimensionality of dataflow representation) on both predictor accuracy and optimization effectiveness