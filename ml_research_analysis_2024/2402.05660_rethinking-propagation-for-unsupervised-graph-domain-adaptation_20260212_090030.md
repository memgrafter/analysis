---
ver: rpa2
title: Rethinking Propagation for Unsupervised Graph Domain Adaptation
arxiv_id: '2402.05660'
source_url: https://arxiv.org/abs/2402.05660
tags:
- graph
- domain
- source
- target
- propagation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of graph neural network (GNN)
  propagation in unsupervised graph domain adaptation. Through empirical analysis,
  it finds that stacking propagation layers on target graphs while removing them from
  source graphs significantly improves adaptation performance.
---

# Rethinking Propagation for Unsupervised Graph Domain Adaptation

## Quick Facts
- arXiv ID: 2402.05660
- Source URL: https://arxiv.org/abs/2402.05660
- Authors: Meihan Liu; Zeyu Fang; Zhen Zhang; Ming Gu; Sheng Zhou; Xin Wang; Jiajun Bu
- Reference count: 40
- Key outcome: Asymmetric propagation architecture (A2GNN) improves unsupervised graph domain adaptation by 1.8% Macro-F1

## Executive Summary
This paper investigates the role of graph neural network (GNN) propagation in unsupervised graph domain adaptation. Through empirical analysis, it finds that stacking propagation layers on target graphs while removing them from source graphs significantly improves adaptation performance. The authors propose A2GNN, an asymmetric architecture with one transformation layer on source graphs and multiple propagation layers on target graphs. Theoretical analysis shows this design tightens the generalization bound. Experiments on citation and social network datasets demonstrate A2GNN outperforms state-of-the-art methods, achieving up to 1.8% improvement in Macro-F1 scores.

## Method Summary
The authors propose A2GNN (Asymmetric Aggregation Graph Neural Network) to address unsupervised graph domain adaptation by treating source and target domains asymmetrically. The method uses a single transformation layer on the source graph to preserve source features while applying multiple propagation layers on the target graph to leverage its structure. This design choice is motivated by empirical observations showing that source graph propagation can be detrimental to adaptation performance. The architecture consists of a shared feature transformation module, followed by domain-specific aggregation modules with different propagation depths. Theoretical analysis demonstrates that this asymmetric design provides tighter generalization bounds compared to symmetric approaches.

## Key Results
- A2GNN achieves up to 1.8% improvement in Macro-F1 scores compared to state-of-the-art methods
- Removing propagation layers from source graphs while adding them to target graphs consistently improves adaptation performance
- The asymmetric architecture provides theoretical generalization bound improvements over symmetric approaches

## Why This Works (Mechanism)
The effectiveness of asymmetric propagation stems from the fundamental difference in how source and target domains should be processed during adaptation. Source graphs contain labeled data but may have different structural patterns than the target domain, making aggressive propagation potentially harmful by amplifying domain-specific noise. Target graphs, being unlabeled, benefit from deeper propagation to capture local structural patterns that aid in semi-supervised learning. By applying minimal transformation to source graphs, the model preserves discriminative features while using multiple propagation layers on target graphs to exploit its structural information for better feature representation.

## Foundational Learning
1. **Graph Domain Adaptation** - Transfer learning across graph-structured data with different distributions
   *Why needed*: Enables knowledge transfer when labeled data is scarce in target domains
   *Quick check*: Can be validated by measuring domain divergence between source and target graphs

2. **Graph Neural Network Propagation** - Message passing mechanism that aggregates neighbor information
   *Why needed*: Core operation that captures local graph structure in node representations
   *Quick check*: Depth of propagation directly affects receptive field size

3. **Asymmetric Architecture Design** - Domain-specific processing with different model depths
   *Why needed*: Addresses domain-specific characteristics that symmetric approaches cannot capture
   *Quick check*: Can be validated through ablation studies comparing symmetric vs asymmetric designs

## Architecture Onboarding

**Component Map**
Source Graph -> Feature Transformation (1 layer) -> Shared Representation -> Target Graph -> Multiple Propagation Layers -> Classification

**Critical Path**
1. Shared feature transformation on source features
2. Asymmetric propagation (1 layer source, multiple layers target)
3. Domain discriminator for adversarial alignment

**Design Tradeoffs**
- Fewer parameters on source domain reduces overfitting to source-specific patterns
- Deeper target propagation improves feature learning but increases computational cost
- Single transformation layer preserves source discriminative information

**Failure Signatures**
- Performance degradation when domain shift is minimal (symmetric approaches may be sufficient)
- Over-smoothing in target graphs with very deep propagation
- Insufficient feature transformation leading to poor source feature utilization

**First Experiments**
1. Compare performance with varying propagation depths on target graph (1-5 layers)
2. Test different numbers of source transformation layers (0-3 layers)
3. Evaluate with and without adversarial domain alignment component

## Open Questions the Paper Calls Out
None

## Limitations
- The empirical analysis shows asymmetric propagation improves performance, but the underlying mechanisms remain incompletely understood
- The study focuses on citation networks and one social network, limiting generalizability to other graph types
- The performance gains of 1.8% Macro-F1 represent relatively modest improvements over existing methods

## Confidence

**High Confidence**: The empirical observation that removing propagation layers from source graphs while adding them to target graphs improves performance is well-supported by experimental results across multiple datasets.

**Medium Confidence**: The theoretical analysis connecting asymmetric architecture to tighter generalization bounds is sound but relies on assumptions about domain divergence that may not hold in all practical scenarios.

**Low Confidence**: The claim about fundamental insights into GNN generalization capabilities for cross-domain adaptation overstates the findings, as the study is primarily empirical without deep mechanistic understanding of why asymmetric propagation works.

## Next Checks
1. Test A2GNN on diverse graph types beyond citation networks, including molecular graphs and biological networks, to assess generalizability across different graph structures and node feature distributions.

2. Conduct experiments with varying degrees of domain shift (measured through domain divergence metrics) to understand when asymmetric propagation is most beneficial and when it may underperform standard approaches.

3. Perform ablation studies specifically isolating the effects of feature transformation versus propagation asymmetry to better understand which component drives the performance improvements.