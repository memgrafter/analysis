---
ver: rpa2
title: Towards Holistic Disease Risk Prediction using Small Language Models
arxiv_id: '2408.06943'
source_url: https://arxiv.org/abs/2408.06943
tags:
- data
- different
- sources
- multimodal
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified multimodal framework for simultaneous
  prediction of 12 distinct disease-related tasks using small language models (SLMs).
  By integrating data from multiple sources (time series, text, and images), the framework
  leverages joint-fusion of modality-specific embeddings into a shared token space
  via regularized overcomplete autoencoders.
---

# Towards Holistic Disease Risk Prediction using Small Language Models

## Quick Facts
- arXiv ID: 2408.06943
- Source URL: https://arxiv.org/abs/2408.06943
- Reference count: 31
- Unified multimodal framework predicting 12 disease-related tasks using small language models

## Executive Summary
This paper presents a unified multimodal framework for simultaneous prediction of 12 distinct disease-related tasks using small language models (SLMs). The framework integrates data from multiple sources (time series, text, and images) and leverages joint-fusion of modality-specific embeddings into a shared token space via regularized overcomplete autoencoders. The approach is evaluated using two SLM variants—Gemma-2B and Phi-3-mini-4k—on the MIMIC-IV dataset, demonstrating competitive performance particularly in disease detection tasks.

The study addresses the challenge of label imbalance through class-balanced and asymmetric loss functions while showing that joint training of multiple tasks significantly outperforms isolated training and single-source baselines. Although the multimodal SLM approach does not surpass task-specific XGBoost models across all tasks, it demonstrates the potential of SLMs for multimodal reasoning in healthcare settings.

## Method Summary
The framework employs a multimodal architecture that processes time series data, clinical text, and medical images through modality-specific encoders. These embeddings are then fused into a shared token space using regularized overcomplete autoencoders, which learn to map diverse input modalities into a common representation suitable for small language models. Two SLM variants (Gemma-2B and Phi-3-mini-4k) are fine-tuned on this unified representation to predict 12 distinct disease-related tasks simultaneously. The joint-fusion approach allows the model to learn shared representations across modalities while maintaining task-specific predictions, with training optimized using both class-balanced and asymmetric loss functions to handle label imbalance in the MIMIC-IV dataset.

## Key Results
- Joint training approach significantly outperforms isolated training and single-source baselines
- Gemma-2B and Phi-3-mini-4k SLMs demonstrate competitive performance in disease detection tasks
- Multimodal SLM approach shows mixed results compared to task-specific XGBoost models
- Regularized overcomplete autoencoders effectively fuse modality-specific embeddings into shared token space

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to leverage complementary information across multiple data modalities while maintaining computational efficiency through small language models. By jointly training on 12 disease-related tasks, the model learns shared representations that capture complex relationships between different disease indicators and clinical features. The regularized overcomplete autoencoders serve as a bridge between heterogeneous data sources, transforming diverse inputs into a unified representation space that SLMs can process effectively. This multimodal fusion approach enables the model to capture richer contextual information than single-modality approaches while the use of SLMs provides a computationally efficient alternative to larger transformer models.

## Foundational Learning
1. **Multimodal Learning**: Integrating information from multiple data sources (time series, text, images) to improve prediction accuracy
   - Why needed: Healthcare data is inherently multimodal, with patient information distributed across various formats
   - Quick check: Verify that all three data modalities are properly preprocessed and aligned temporally

2. **Overcomplete Autoencoders**: Neural networks with more hidden units than inputs, used for learning robust representations
   - Why needed: Enables learning of non-linear transformations that capture complex relationships between modalities
   - Quick check: Monitor reconstruction loss during training to ensure effective encoding/decoding

3. **Class-Balanced Loss Functions**: Techniques to address imbalanced datasets by adjusting loss weights
   - Why needed: Medical datasets often have severe class imbalance, particularly for rare diseases
   - Quick check: Compare performance metrics across different loss weighting strategies

4. **Small Language Models**: Compact transformer-based models optimized for efficiency while maintaining performance
   - Why needed: Provides computational efficiency compared to large language models while retaining reasoning capabilities
- Quick check: Evaluate inference time and memory usage compared to baseline models

## Architecture Onboarding

**Component Map**: Medical Data Sources -> Modality-Specific Encoders -> Regularized Overcomplete Autoencoders -> Shared Token Space -> Small Language Models -> 12 Disease Prediction Tasks

**Critical Path**: The most critical sequence is Medical Data Sources → Modality-Specific Encoders → Regularized Overcomplete Autoencoders → Shared Token Space, as this multimodal fusion pipeline directly impacts the quality of input representations for the SLMs.

**Design Tradeoffs**: The framework trades potential performance gains from larger models for computational efficiency through SLMs. The choice of regularized overcomplete autoencoders over simpler fusion methods prioritizes learning complex non-linear relationships at the cost of increased model complexity. Joint training of 12 tasks enables knowledge sharing but may introduce interference between unrelated tasks.

**Failure Signatures**: Poor performance may manifest as: (1) High reconstruction loss in autoencoders indicating failed modality fusion, (2) Degraded performance on individual tasks suggesting task interference, (3) Overfitting on MIMIC-IV data indicating poor generalization, (4) Class imbalance causing systematic bias toward majority classes despite loss weighting.

**First 3 Experiments**:
1. Ablation study removing one modality at a time to quantify individual contributions
2. Comparison of different autoencoder regularization techniques (L1, L2, dropout variations)
3. Evaluation of alternative SLM architectures (different model sizes, pretraining strategies)

## Open Questions the Paper Calls Out
The paper identifies several key uncertainties regarding the generalizability of the framework beyond the MIMIC-IV dataset. There is limited understanding of whether the computational overhead of multimodal fusion is justified across all task types, given that the approach only outperforms task-specific models in disease detection while underperforming in other areas. The study also lacks extensive ablation analyses to determine the individual value contributions of each modality (time series, text, images) to the combined prediction task.

## Limitations
- Mixed performance compared to task-specific XGBoost models, with superiority only demonstrated in disease detection tasks
- Limited evaluation to MIMIC-IV dataset raises concerns about generalizability to other healthcare settings
- Absence of detailed ablation studies makes it difficult to assess the individual value of each modality
- Comparison with only two SLM variants limits conclusions about SLM performance in multimodal healthcare settings

## Confidence
- **High confidence**: Experimental methodology and implementation details are clearly described with MIMIC-IV providing solid reproducibility foundation
- **Medium confidence**: Comparative performance results are reliable for tested tasks and models but may not generalize beyond specific context
- **Low confidence**: Claims about multimodal fusion superiority are not fully substantiated given mixed results across different task types

## Next Checks
1. Conduct cross-institutional validation using a different EHR dataset to assess generalizability beyond MIMIC-IV
2. Perform detailed ablation studies to quantify individual contribution of each modality (time series, text, images)
3. Compare multimodal SLM approach with alternative fusion techniques (attention-based, cross-modal attention, or late fusion) to validate regularized overcomplete autoencoder choice