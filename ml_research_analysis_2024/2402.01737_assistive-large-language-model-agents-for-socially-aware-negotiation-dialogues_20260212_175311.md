---
ver: rpa2
title: Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues
arxiv_id: '2402.01737'
source_url: https://arxiv.org/abs/2402.01737
tags:
- price
- seller
- dialogue
- remediation
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the problem of assisting conversational
  agents in achieving both task-oriented and social goals during business negotiations.
  The core method introduces a socially intelligent remediator agent that intervenes
  in simulated negotiations to correct language violating social norms, thereby improving
  negotiation outcomes.
---

# Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues

## Quick Facts
- arXiv ID: 2402.01737
- Source URL: https://arxiv.org/abs/2402.01737
- Reference count: 40
- Primary result: Novel label-free in-context learning method with value impact scoring significantly improves negotiation success rates and social goal achievement

## Executive Summary
This work introduces a socially intelligent remediator agent that intervenes in simulated business negotiations to correct language violating social norms, thereby improving both task-oriented and social outcomes. The authors propose a novel tuning-free and label-free in-context learning approach with a value impact scoring function to select high-quality exemplars for the remediator. Experiments across three negotiation topics demonstrate that the remediator significantly improves negotiation success rates, deal prices, and achievement of social goals compared to baseline models.

## Method Summary
The approach uses a remediator agent in multi-party negotiation dialogues to identify and correct social norm violations. It employs a novel value impact scoring function to evaluate exemplar quality based on simulated negotiation outcomes, combined with hierarchical traversal with early pruning to efficiently select optimal exemplar sets. The method is label-free, using silver annotations from GPT-3.5 to generate remediations for norm violations, which are then evaluated through simulated negotiations to create value impact scores for exemplar selection.

## Key Results
- Up to 4% improvement in negotiation success rate compared to baseline models
- 1.5% increase in deal price achieved through social norm adherence
- 3% enhancement in social goal achievement (trust and relationship metrics)
- Value impact scoring method outperforms random exemplar selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Value impact scoring selects exemplars that improve both negotiation outcomes and social goal achievement
- Mechanism: Value impact calculates the difference in simulated negotiation outcomes when using a particular exemplar versus not using it, capturing both task-oriented (deal price, success rate) and social (trust, relationship) metrics
- Core assumption: Simulated negotiations accurately reflect real negotiation dynamics and the impact of social norm violations
- Evidence anchors:
  - [abstract]: "We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL exemplars for the remediator, where we propose a novel select criteria, called value impact, to measure the quality of the negotiation outcomes"
  - [section]: "We define the value of y′s wrt the silver remediation ys for an remediation point xs as, Vz(y′s) := Epsim(h′>s|y′s,xs,h<s) · R(h<s, xs, y′s, h′>s) − Epsim(h>s|ys,xs,h<s) · R(h<s, xs, ys, h>s)"
  - [corpus]: Weak - no direct citations found for value impact methodology specifically, though related work exists on negotiation outcomes and LLM-based agents
- Break condition: If simulated negotiations don't accurately model real negotiation dynamics, or if the reward function doesn't capture the true value of social norm adherence

### Mechanism 2
- Claim: Early pruning hierarchical traversal efficiently finds near-optimal exemplar combinations without exhaustive search
- Mechanism: The algorithm builds exemplar sets incrementally, pruning branches where adding lower-ranked exemplars consistently reduces value impact, reducing search space from exponential to manageable
- Core assumption: Exemplars with higher individual value impact are more likely to contribute positively to combined sets
- Evidence anchors:
  - [abstract]: "We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL exemplars for the remediator"
  - [section]: "Our goal is to combine ICL examples from SIN IT and SCAN D sequentially using a hierarchical traversal algorithm... The computational complexity of hierarchical traversal is O(|S′| · |Ds| · |SIN IT | · |SCAN D|)"
  - [corpus]: Weak - no direct citations for hierarchical traversal with early pruning for ICL exemplar selection, though related to optimization algorithms
- Break condition: If exemplar interactions are non-linear or synergistic in ways that early pruning misses, or if the heuristic of replacing elements sequentially doesn't capture optimal combinations

### Mechanism 3
- Claim: Label-free learning with silver annotations enables effective remediation without expensive human annotation
- Mechanism: Uses GPT-3.5 to generate silver remediations for norm violations, then evaluates their effectiveness through simulated negotiations to create value impact scores for exemplar selection
- Core assumption: Silver annotations from GPT-3.5 are sufficiently accurate to serve as training signals for the remediator
- Evidence anchors:
  - [abstract]: "We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL exemplars for the remediator"
  - [section]: "We view the remediations annotated by human as gold annotations, while those labeled by GPT4 as silver annotations. We combine silver annotations and gold annotations into a high-quality silver annotation set D"
  - [corpus]: Weak - no direct citations for silver annotation methodology in negotiation remediation, though related to LLM-based annotation approaches
- Break condition: If silver annotations contain systematic biases or errors that propagate through the value impact scoring and exemplar selection

## Foundational Learning

- Concept: In-context learning (ICL) mechanics
  - Why needed here: Understanding how exemplars in prompts influence LLM behavior is crucial for grasping the remediator's operation
  - Quick check question: What are the three components of a typical ICL prompt structure in this work?

- Concept: Multi-agent simulation and reinforcement learning concepts
  - Why needed here: The value impact scoring relies on simulating negotiations to evaluate exemplar quality
  - Quick check question: How does the reward function R(d) combine task-oriented and social goal metrics?

- Concept: Natural language processing and social norm detection
  - Why needed here: The remediator must identify and correct social norm violations in text
  - Quick check question: What are the key components of the norm violation definition provided in this work?

## Architecture Onboarding

- Component map: Buyer LLM agent -> Seller LLM agent -> Remediator LLM agent (intervenes when norm violations detected) -> Moderator agent (terminates dialogue) -> GPT-3.5 evaluator (assesses trust and relationship metrics)

- Critical path: Norm violation occurs → remediator generates remediation → dialogue continues with modified text → negotiation outcomes evaluated → value impact calculated for exemplar selection

- Design tradeoffs: Label-free approach vs. potential accuracy loss from silver annotations; computational cost of hierarchical traversal vs. quality of exemplar selection; use of multiple LLMs vs. system complexity

- Failure signatures: Low success rates in negotiations despite remediation; inconsistent remediation quality across different topics; high computational overhead for exemplar selection

- First 3 experiments:
  1. Test remediator performance on a single negotiation topic with and without exemplar selection
  2. Compare value impact exemplar selection against random exemplar selection
  3. Evaluate the impact of different M values in hierarchical traversal on exemplar quality and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the remediator's effectiveness vary across different languages beyond Chinese and English, particularly for languages with fewer restrictions on model outputs?
- Basis in paper: [explicit] The authors acknowledge testing only in bilingual Chinese and English environments and plan to extend research to other languages with fewer restrictions.
- Why unresolved: The current study focuses specifically on Chinese and English, leaving uncertainty about performance in other linguistic contexts.
- What evidence would resolve it: Experimental results showing remediation effectiveness across multiple languages with varying degrees of model output restrictions.

### Open Question 2
- Question: How does increasing the size of the training dataset for supervised fine-tuning (SFT) impact the remediator's performance compared to the in-context learning (ICL) approach?
- Basis in paper: [explicit] The authors conducted experiments with additional training data but found no significant improvement, suggesting potential overfitting due to limited dialogue diversity.
- Why unresolved: The experiments used only 1000 additional dialogues, which may not be sufficient to overcome the overfitting issue or reveal potential performance gains.
- What evidence would resolve it: Results from experiments with substantially larger and more diverse training datasets, comparing SFT performance against the ICL approach.

### Open Question 3
- Question: What is the optimal value of M in the hierarchical traversal algorithm for selecting ICL demonstrations, balancing search space and performance improvement?
- Basis in paper: [explicit] The authors tested M=1 and M=5 but found M=2 to be most cost-effective, though the optimal value remains unclear.
- Why unresolved: The study only tested a limited range of M values, and the impact on different negotiation topics or scenarios is unknown.
- What evidence would resolve it: Comprehensive experiments testing various M values across different negotiation scenarios, measuring both performance and computational efficiency.

## Limitations

- Silver annotation quality relies on GPT-3.5's ability to accurately identify and correct social norm violations, but validation against human judgment is limited
- Value impact scoring assumes simulated negotiation outcomes accurately reflect real-world negotiation dynamics and appropriate reward function design
- Hierarchical traversal with early pruning may miss optimal exemplar combinations when interactions are non-linear or synergistic

## Confidence

- **High confidence**: The core methodology of using a remediator agent to correct social norm violations and improve negotiation outcomes is well-established and the experimental design is sound
- **Medium confidence**: The value impact scoring method for exemplar selection is theoretically justified but relies on several assumptions about simulation accuracy and reward function design
- **Medium confidence**: The label-free approach using silver annotations is practical but its effectiveness depends on the quality of LLM-generated remediations

## Next Checks

1. Conduct human evaluation studies to validate the accuracy of silver annotations against ground truth for social norm violations across different negotiation topics
2. Perform ablation studies comparing value impact exemplar selection against alternative methods (random selection, diversity-based selection) to isolate the contribution of the proposed scoring mechanism
3. Test the remediator's performance in multi-turn negotiations with complex social dynamics to evaluate robustness beyond the three topics studied