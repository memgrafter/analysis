---
ver: rpa2
title: A comprehensive interpretable machine learning framework for Mild Cognitive
  Impairment and Alzheimer's disease diagnosis
arxiv_id: '2412.09376'
source_url: https://arxiv.org/abs/2412.09376
tags:
- features
- feature
- were
- methods
- alzheimer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an interpretable machine learning framework\
  \ for diagnosing Mild Cognitive Impairment (MCI) and Alzheimer\u2019s disease (AD)\
  \ using volumetric MRI measurements and genetic data. The framework addresses class\
  \ imbalance through ensemble learning with one-versus-one decomposition and employs\
  \ multiple interpretability methods (SHAP, LIME, Gini index, counterfactual explanations)\
  \ to explain model predictions."
---

# A comprehensive interpretable machine learning framework for Mild Cognitive Impairment and Alzheimer's disease diagnosis

## Quick Facts
- arXiv ID: 2412.09376
- Source URL: https://arxiv.org/abs/2412.09376
- Reference count: 37
- Key outcome: Ensemble learning framework achieving 87.5% balanced accuracy and 90.8% F1-score for MCI/AD diagnosis using interpretable methods

## Executive Summary
This paper presents an interpretable machine learning framework for diagnosing Mild Cognitive Impairment (MCI) and Alzheimer's disease (AD) using volumetric MRI measurements and genetic data. The framework addresses the multiclass classification problem with class imbalance through ensemble learning with one-versus-one decomposition and multiple interpretability methods including SHAP, LIME, Gini index, and counterfactual explanations. The approach identifies key brain regions (hippocampus, amygdala, lateral ventricles) and genetic markers (ApoE, CLNK, MS4A6A genes) associated with disease diagnosis, providing both accurate predictions and clinically interpretable explanations.

## Method Summary
The framework uses volumetric MRI measurements and genetic SNP data from 1463 subjects (449 CN, 740 MCI, 274 AD) from the ADNI database. It addresses class imbalance through ensemble learning with Bagging and one-versus-one decomposition, training multiple classifiers (SVM, RF, XGBoost, etc.) on balanced subsets. The interpretability component combines attribution-based methods (SHAP, LIME, Gini) with counterfactual explanations to provide robust feature importance rankings and necessity/sufficiency assessments. The unified framework generates counterfactual examples to evaluate which features are truly necessary and sufficient for predictions.

## Key Results
- Achieved 87.5% balanced accuracy and 90.8% F1-score for MCI/AD diagnosis
- Identified hippocampus, amygdala, and lateral ventricles as most important brain regions
- Found ApoE, CLNK, and MS4A6A genes as significant genetic markers
- Demonstrated that necessity and sufficiency scores vary between counterfactual generation methods (Permute Attack vs DiCE)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble learning with one-versus-one decomposition effectively addresses both multiclass classification and class imbalance in MCI/AD diagnosis.
- Mechanism: The Bagging ensemble method splits the majority MCI class into two equal halves, training two separate OVO classifiers on each subset. This reduces variance while maintaining class balance in each training subset.
- Core assumption: Splitting the majority class and applying OVO decomposition maintains sufficient representation of each class relationship while reducing overfitting.
- Evidence anchors: [abstract] "The existing class imbalance is addressed by an ensemble learning approach"; [section] "To tackle the problem of class imbalance, an ensemble learning strategy based on the Bagging method... was deployed"

### Mechanism 2
- Claim: Combining multiple interpretability methods (SHAP, LIME, Gini, counterfactuals) provides more robust explanations than any single method alone.
- Mechanism: Different interpretability methods capture complementary aspects of model behavior - SHAP provides global feature importance, LIME offers local explanations, counterfactuals show feature necessity/sufficiency, and Gini measures tree-based feature importance.
- Core assumption: Each interpretability method captures unique aspects of feature influence, and their combination provides more complete understanding than individual methods.
- Evidence anchors: [abstract] "A unification method combining SHAP with counterfactual explanations assesses the interpretability techniques' robustness"; [section] "various interpretability methods, including attribution-based and counterfactual-based approaches... provide human-friendly explanations"

### Mechanism 3
- Claim: Volumetric MRI measurements combined with genetic SNP data provide complementary information for MCI/AD diagnosis.
- Mechanism: MRI features capture structural brain changes while genetic SNPs provide molecular risk factors, creating a multimodal feature space that captures different aspects of disease pathology.
- Core assumption: Structural brain changes and genetic variations are independent but complementary risk factors for MCI/AD progression.
- Evidence anchors: [abstract] "The dataset used comprises volumetric measurements from brain MRI and genetic data"; [section] "leverages a combination of MRI volumetric measurements... together with 54 AD related Single Nucleotide Polymorphisms (SNPs)"

## Foundational Learning

- Concept: One-vs-One (OVO) decomposition for multiclass classification
  - Why needed here: The problem involves three classes (CN, MCI, AD), requiring decomposition into binary subproblems
  - Quick check question: How many binary classifiers are needed for a 3-class OVO decomposition?
  - Answer: 3 binary classifiers (CN vs MCI, CN vs AD, MCI vs AD)

- Concept: Shapley values and feature attribution
  - Why needed here: To quantify each feature's contribution to model predictions and identify important brain regions and genetic markers
  - Quick check question: What property of Shapley values makes them suitable for feature attribution in healthcare?
  - Answer: They provide fair attribution by considering all possible feature coalitions

- Concept: Counterfactual explanations and necessity/sufficiency
  - Why needed here: To assess whether identified features are not just important but also necessary and sufficient for diagnosis
  - Quick check question: What's the difference between feature necessity and sufficiency in this context?
  - Answer: Necessity means feature is required for prediction; sufficiency means feature alone can cause prediction

## Architecture Onboarding

- Component map: Data preprocessing → Bagging ensemble with OVO → Multiple classifiers (SVM, RF, XGBoost, etc.) → Interpretability methods → Unification framework

- Critical path: Data loading → Preprocessing (z-score normalization) → Bagging split → OVO decomposition → Classifier training → Cross-validation → Interpretability analysis → Unification

- Design tradeoffs:
  - Model complexity vs interpretability: More complex models may achieve better accuracy but reduce interpretability
  - Feature selection: Including all features vs dimensionality reduction - impacts both performance and interpretability
  - Counterfactual method choice: Permute Attack vs DiCE affects necessity/sufficiency calculations

- Failure signatures:
  - High variance in cross-validation scores → potential overfitting or class imbalance issues
  - Interpretability methods producing contradictory results → model may be capturing spurious correlations
  - Necessity scores near zero → features may not be truly important despite high SHAP values

- First 3 experiments:
  1. Train SVM with OVO on preprocessed data, evaluate balanced accuracy and F1-score
  2. Apply SHAP and Gini importance to identify top features, compare results
  3. Generate counterfactual examples using Permute Attack, calculate feature modification frequencies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the classification performance change if additional modalities like cognitive assessments were included in the model?
- Basis in paper: [inferred] The authors mention that "The inclusion of additional modalities, such as cognitive assessments and the evaluation of their contribution to the models' performance will also be considered" in the discussion section.
- Why unresolved: The study only used volumetric MRI measurements and genetic data (SNPs), leaving open the question of how cognitive assessments might affect performance.
- What evidence would resolve it: Running the same classification framework with cognitive assessment data added to the existing features and comparing performance metrics (balanced accuracy, F1-score) with the current model.

### Open Question 2
- Question: How do the necessity and sufficiency scores vary when using different counterfactual generation methods beyond Permute Attack and DiCE?
- Basis in paper: [explicit] The authors note that "The observed differences in the obtained necessity and sufficiency results between Permute Attack and DiCE could be related to the methods' distinct approaches to generating counterfactuals" and suggest exploring "further techniques for unifying the results of the interpretability methods."
- Why unresolved: Only two counterfactual methods were compared, and the authors acknowledge that different methods may yield different results.
- What evidence would resolve it: Applying additional counterfactual generation methods (such as Counterfactual with Constraint (CCH) or Growing Spheres) and comparing their necessity and sufficiency scores with those obtained from Permute Attack and DiCE.

### Open Question 3
- Question: How does the model perform on data from different clinical centers or populations outside the ADNI dataset?
- Basis in paper: [inferred] The study used data exclusively from the ADNI database, and the authors mention generalizability concerns in the discussion.
- Why unresolved: The model's performance and interpretability results were obtained using a single dataset, raising questions about external validity.
- What evidence would resolve it: Testing the trained model on MRI and genetic data from other clinical studies or populations and comparing performance metrics and feature importance rankings.

## Limitations

- The interpretability framework's unified necessity/sufficiency metrics lack validation against clinical expert knowledge
- The study only compared two counterfactual generation methods, limiting understanding of method variability
- External validity is unknown as the model was trained and tested exclusively on ADNI dataset

## Confidence

- Ensemble learning effectiveness: Medium confidence (limited ablation studies)
- Interpretability unification framework: Low confidence (no validation of method agreement)
- Multimodal feature complementarity: Low confidence (no single-modality comparison)

## Next Checks

1. Compare necessity/sufficiency metrics from unified framework with clinical expert knowledge to assess practical interpretability
2. Conduct ablation studies testing OVO + Bagging against alternative imbalance handling methods (SMOTE, class weighting)
3. Validate whether top-ranked features by different interpretability methods show consistent biological plausibility across validation cohorts