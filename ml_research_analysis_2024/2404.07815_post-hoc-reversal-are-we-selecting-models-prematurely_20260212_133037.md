---
ver: rpa2
title: 'Post-Hoc Reversal: Are We Selecting Models Prematurely?'
arxiv_id: '2404.07815'
source_url: https://arxiv.org/abs/2404.07815
tags:
- post-hoc
- naive
- selection
- reversal
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a phenomenon called "post-hoc reversal" where
  the performance ranking of trained models can flip after applying post-hoc transforms
  like temperature scaling, ensembling, or stochastic weight averaging. The authors
  show this occurs especially in high-noise settings and can prevent double descent
  and loss-error mismatch.
---

# Post-Hoc Reversal: Are We Selecting Models Prematurely?

## Quick Facts
- arXiv ID: 2404.07815
- Source URL: https://arxiv.org/abs/2404.07815
- Authors: Rishabh Ranjan; Saurabh Garg; Mrigank Raman; Carlos Guestrin; Zachary Lipton
- Reference count: 40
- Key outcome: Post-hoc transforms can reverse model performance rankings, and using post-hoc metrics for selection significantly improves performance (over 2x improvement in some cases, over 1.5x MMLU improvement on LLM instruction tuning)

## Executive Summary
This paper identifies a phenomenon called "post-hoc reversal" where the performance ranking of trained models can flip after applying post-hoc transforms like temperature scaling, ensembling, or stochastic weight averaging. The authors show this occurs especially in high-noise settings and can prevent double descent and loss-error mismatch. Based on these findings, they propose "post-hoc selection" - using post-hoc metrics to select model checkpoints rather than base metrics. This approach significantly improves performance, with over 2x improvement over naive selection in some cases, and over 1.5x MMLU improvement on an LLM instruction tuning dataset.

## Method Summary
The authors train base models on various datasets including CIFAR-10/100 with injected label noise, FMoW satellite imagery, and LLM instruction tuning data. They then apply post-hoc transforms (temperature scaling, ensembling, stochastic weight averaging) to these trained models and compare performance rankings before and after transformation. The key insight is that post-hoc selection - choosing models based on transformed performance rather than base performance - can yield significantly better results, especially in noisy settings where traditional selection fails.

## Key Results
- Post-hoc reversal occurs when model performance rankings flip after applying transforms like temperature scaling, ensembling, or SWA
- Post-hoc selection significantly outperforms naive selection, with over 2x improvement in some cases and over 1.5x MMLU improvement on LLM instruction tuning
- Post-hoc reversal can prevent double descent phenomena and mitigate loss-error mismatch seen in base models
- The effect is most pronounced in high-noise settings where traditional selection strategies fail

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc transforms reverse performance trends by suppressing the influence of mislabeled examples while reinforcing patterns from clean examples.
- Mechanism: Models continue learning generalizable patterns from clean examples even as they overfit on mislabeled ones. Post-hoc transforms (SWA, ensembling, TS) exploit differences in learning dynamics between these example typesâ€”clean examples are learned early and stably, while mislabeled examples cause more prediction fluctuations. The transforms suppress mislabeled influence and amplify clean patterns, reversing performance trends.
- Core assumption: Post-hoc transforms can distinguish and differentially treat clean vs mislabeled examples based on their learning dynamics.
- Evidence anchors:
  - [abstract]: "Post-hoc reversal suggests a path to alleviate them. Noise can arise not only from labeling errors, but also from inherent uncertainty in the prediction task... Indeed, severe performance degradation has limited multi-epoch training of large language models (LLMs) [81]. Here too, post-hoc reversal reveals a promising path for sustained performance improvements over longer training."
  - [section]: "The core intuition for post-hoc reversal is that models continue to learn generalizable patterns from clean examples, even when spurious patterns learnt from mislabeled examples worsen the overall performance. Post-hoc transforms exploit differences in the learning dynamics of clean and mislabeled examples [42] to reinforce the influence of the former, while suppressing that of the latter."
  - [corpus]: No direct corpus evidence found for this specific mechanism of differential treatment based on learning dynamics.

### Mechanism 2
- Claim: Temperature scaling (TS) mitigates loss-error mismatch by downscaling overconfident logits.
- Mechanism: Once a neural net fits a training example, cross-entropy loss can be reduced by upscaling output layer weights, making the model overconfident. For mislabeled examples, this leads to worse loss on similar test instances while error remains unaffected. TS fixes this by downscaling logits, aligning loss and error curves.
- Core assumption: TS can effectively reduce overconfidence without harming classification accuracy.
- Evidence anchors:
  - [abstract]: "Preliminary analyses suggest that these transforms induce reversal by suppressing the influence of mislabeled examples, exploiting differences in their learning dynamics from those of clean examples."
  - [section]: "TS fixes this by downscaling the logits. Indeed, one finds that the temperature (as obtained with a held-out set) increases with epochs (Fig. 8)."
  - [corpus]: No direct corpus evidence found for the specific claim about TS mitigating loss-error mismatch through overconfidence reduction.

### Mechanism 3
- Claim: Ensembling and SWA delay catastrophic overfitting by improving fitting of clean examples and reducing memorization of mislabeled ones.
- Mechanism: Models learn generalizable patterns from clean examples and spurious patterns from mislabeled ones. Ensembling and SWA improve fitting of clean examples and reduce memorization of mislabeled ones. When this overturns the dominance of spurious patterns, we observe reversal.
- Core assumption: Ensembling and SWA can effectively reduce the impact of mislabeled examples on the final model.
- Evidence anchors:
  - [abstract]: "Post-hoc reversal can also prevent the appearance of double descent and mitigate mismatches between test loss and test error seen in base models."
  - [section]: "Models learn generalizable patterns from clean examples, and spurious patterns from mislabeled ones. The latter causes overfitting. When noise is low, the former dominates and overfitting is benign. Otherwise, overfitting is catastrophic. Ensembling and SWA improve fitting of clean examples, and reduce memorization of mislabeled ones."
  - [corpus]: No direct corpus evidence found for this specific mechanism of delayed catastrophic overfitting through differential treatment of clean vs mislabeled examples.

## Foundational Learning

- Concept: Learning dynamics of clean vs mislabeled examples
  - Why needed here: Understanding how post-hoc transforms exploit differences in learning dynamics between clean and mislabeled examples is crucial for grasping the mechanism of post-hoc reversal.
  - Quick check question: Can you explain how the learning dynamics of clean and mislabeled examples differ, and how post-hoc transforms exploit these differences?

- Concept: Cross-entropy loss and calibration
  - Why needed here: Understanding how temperature scaling works to mitigate loss-error mismatch requires knowledge of cross-entropy loss and model calibration.
  - Quick check question: How does temperature scaling affect the cross-entropy loss and model calibration?

- Concept: Ensemble methods and model averaging
  - Why needed here: Understanding how ensembling and SWA work to improve model performance and delay overfitting requires knowledge of ensemble methods and model averaging techniques.
  - Quick check question: Can you explain the difference between ensembling and SWA, and how they each contribute to improved model performance?

## Architecture Onboarding

- Component map:
  - Base models -> Post-hoc transforms (Temperature Scaling, Ensembling, SWA) -> Performance evaluation
  - Selection strategy: Naive selection (based on base model performance) vs post-hoc selection (based on post-transform performance)

- Critical path:
  1. Train base models on the dataset
  2. Apply post-hoc transforms (TS, ensembling, SWA) to the base models
  3. Evaluate performance of base models and post-transform models
  4. Select models based on post-transform performance (post-hoc selection)

- Design tradeoffs:
  - Post-hoc selection vs naive selection: Post-hoc selection can lead to better performance by exploiting post-hoc reversal, but may require additional computation for applying transforms
  - Choice of post-hoc transforms: Different transforms (TS, ensembling, SWA) may be more or less effective depending on the dataset and model architecture

- Failure signatures:
  - Post-hoc reversal not occurring: If the dataset is extremely clean or well-regularized, post-hoc reversal may not be observed
  - Post-hoc selection not improving performance: If the post-hoc transforms are not effective for the given dataset and model architecture, post-hoc selection may not lead to better performance

- First 3 experiments:
  1. Replicate the post-hoc reversal phenomenon on a noisy dataset (e.g., CIFAR-10 with noisy labels) by comparing base model performance to post-transform model performance
  2. Compare naive selection and post-hoc selection strategies on the same dataset to demonstrate the benefits of post-hoc selection
  3. Investigate the impact of different post-hoc transforms (TS, ensembling, SWA) on model performance and post-hoc reversal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does post-hoc reversal manifest in different types of noisy data beyond label noise, such as uncertainty in the prediction target or insufficient information in the input context?
- Basis in paper: The authors explicitly state that noise can arise from labeling errors, inherent uncertainty in the prediction task, and insufficient information in the input context. They demonstrate post-hoc reversal on label noise but acknowledge the broader applicability.
- Why unresolved: The paper primarily focuses on label noise in CIFAR-N datasets. While the authors mention other sources of noise, they don't provide extensive empirical evidence on how post-hoc reversal manifests in these scenarios.
- What evidence would resolve it: Empirical studies on diverse datasets exhibiting different types of noise (aleatoric uncertainty, epistemic uncertainty, annotation errors) would demonstrate the generalizability of post-hoc reversal across various noise patterns.

### Open Question 2
- Question: What is the theoretical explanation for why post-hoc transforms induce reversal by suppressing the influence of mislabeled examples?
- Basis in paper: The authors provide a preliminary explanation that post-hoc transforms exploit differences in the learning dynamics of clean and mislabeled examples, but they acknowledge that a deeper theoretical understanding is needed.
- Why unresolved: The paper focuses on empirical observations and provides initial intuitions. A rigorous theoretical framework explaining the mechanism behind post-hoc reversal is lacking.
- What evidence would resolve it: Developing a theoretical model that captures the learning dynamics of clean and mislabeled examples, and how post-hoc transforms interact with these dynamics to induce reversal, would provide a comprehensive explanation.

### Open Question 3
- Question: How does post-hoc reversal impact the scaling laws of large language models and other large-scale models?
- Basis in paper: The authors briefly mention the potential impact on scaling laws but don't explore it in depth. They focus on the immediate practical implications of post-hoc reversal for model development.
- Why unresolved: The paper's experiments are primarily on smaller-scale models and datasets. The effects of post-hoc reversal on the scaling behavior of large models remain unexplored.
- What evidence would resolve it: Conducting experiments on large language models and analyzing how post-hoc reversal affects their performance as they scale in size and are trained on more data would provide insights into the relationship between post-hoc reversal and scaling laws.

## Limitations
- The proposed mechanisms rely heavily on the assumption that post-hoc transforms can effectively distinguish and differentially treat clean versus mislabeled examples based on their learning dynamics
- Empirical validation of post-hoc reversal mechanisms across diverse noise patterns and model architectures remains limited
- The specific claim that temperature scaling "fixes" loss-error mismatch through overconfidence reduction needs more rigorous empirical support

## Confidence

- **High Confidence**: The empirical observation of post-hoc reversal across multiple datasets and transforms (CIFAR-10/100-N, FMoW, LLM instruction tuning)
- **Medium Confidence**: The mechanism explanation linking post-hoc reversal to differential treatment of clean vs mislabeled examples
- **Low Confidence**: The specific claim that temperature scaling "fixes" loss-error mismatch through overconfidence reduction without broader empirical validation

## Next Checks

1. **Mechanism Validation**: Design experiments to directly measure whether post-hoc transforms indeed distinguish between clean and mislabeled examples by analyzing their effect on examples with known labels versus injected noise

2. **Cross-Domain Generalization**: Test post-hoc selection on additional domains beyond image classification and language modeling, particularly in scientific domains where noise patterns differ significantly from CIFAR or LLM datasets

3. **Robustness to Transform Choice**: Systematically evaluate how different combinations and sequences of post-hoc transforms affect the reversal phenomenon to determine if certain transform compositions are more effective at exploiting learning dynamics differences