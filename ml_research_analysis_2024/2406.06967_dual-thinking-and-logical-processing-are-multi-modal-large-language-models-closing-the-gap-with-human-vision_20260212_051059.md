---
ver: rpa2
title: Dual Thinking and Logical Processing -- Are Multi-modal Large Language Models
  Closing the Gap with Human Vision ?
arxiv_id: '2406.06967'
source_url: https://arxiv.org/abs/2406.06967
tags:
- human
- processing
- vision
- visual
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the need for better understanding of dual thinking
  in human vision and its relationship to deep learning models. It introduces a novel
  adversarial dataset designed to study the differences between fast, intuitive and
  slower, logical processing in vision.
---

# Dual Thinking and Logical Processing -- Are Multi-modal Large Language Models Closing the Gap with Human Vision ?

## Quick Facts
- arXiv ID: 2406.06967
- Source URL: https://arxiv.org/abs/2406.06967
- Reference count: 40
- Primary result: Multi-modal LLMs have made significant progress in correcting errors from intuitive processing but still struggle with finer logical reasoning, particularly involving sub-components

## Executive Summary
This paper explores the dual thinking framework in human vision and compares it with deep learning models, introducing a novel adversarial dataset to study the differences between fast intuitive and slower logical visual processing. Through psychophysical experiments with 100 participants and evaluation of 11 multi-modal LLMs and segmentation models, the study reveals that while segmentation models exhibit errors similar to human intuitive processing, recent multi-modal LLMs have made tremendous progress in resolving logical errors but still lack fine-grained reasoning capabilities. The research highlights the importance of integrating logical processing for robust AI systems in safety-critical applications and provides a framework for qualitative analysis of model behavior.

## Method Summary
The study introduces the Human Confusion Dataset containing 1000 images with confusing instances from MS-COCO classes, annotated for Gestalt principles, logical errors, and specific masks. Psychophysical experiments were conducted with 100 participants who answered structured questions about figure-ground relationships, object counts, size differences, and impossible patterns. An automatic evaluation algorithm using IoU thresholds (73% for instance recognition, 25% for non-instance confusion, 70% for instance confusion) was developed to assess model outputs at scale. The evaluation compared 11 multi-modal LLMs (including GPT-4o and Llama 3.2) and segmentation models from the MMDetection framework against human performance on intuitive vs. logical processing errors.

## Key Results
- Multi-modal LLMs achieved 97.6% accuracy on figure-ground errors compared to 93.3% for segmentation models, showing significant improvement in logical processing
- Both humans and models struggled with count mismatch errors, with humans achieving only 87.6% accuracy and segmentation models 78.7%
- Segmentation models exhibited texture bias and focus on details, similar to human intuitive processing errors, while multi-modal LLMs showed better logical reasoning capabilities
- Human vision uses shape-based grouping for intuitive processing while deep learning models show texture bias, explaining similar error patterns between humans and segmentation models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dual thinking in human vision involves fast intuitive processing followed by slower logical processing that can correct errors from the first stage.
- **Mechanism**: The paper introduces an adversarial dataset that forces human participants to experience both intuitive and logical stages, revealing that logical processing corrects errors missed in intuitive processing. Segmentation models exhibit similar errors to intuitive human processing, while multi-modal LLMs show improved logical processing but still struggle with fine-grained reasoning.
- **Core assumption**: Human visual processing has two distinct stages, and the dataset captures images where these stages differ in inference.
- **Evidence anchors**:
  - [abstract] "Our psychophysical studies show the presence of multiple inferences in rapid succession, and analysis of errors shows that the early stopping of visual processing can result in missing relevant information."
  - [section] "Our study shows that logical processing was nascent in segmentation models, and multi-modal LLMs in recent years have made tremendous progress in resolving many logical errors"
  - [corpus] Weak evidence for this specific mechanism; corpus papers focus on general multimodal reasoning rather than dual thinking framework specifically.

### Mechanism 2
- **Claim**: Shape-based grouping is a primary strategy in intuitive human visual processing, while texture-based grouping dominates in deep learning segmentation models.
- **Mechanism**: The paper observes that human vision prioritizes shape over texture for grouping sub-components during intuitive processing, whereas segmentation models show texture bias and focus on details rather than overall structure. This explains why segmentation models make similar errors to intuitive human processing.
- **Core assumption**: Shape and texture serve different roles in human vs. deep learning visual processing.
- **Evidence anchors**:
  - [section] "Our study observes that human vision uses a top-down approach similar to Gestalt theory, focusing on overall structure and shape, compared to deep learning models that focus more on details"
  - [section] "In general, the texture bias in deep learning models helped segmentation models perform slightly better on figure-ground errors"
  - [corpus] Limited direct evidence; corpus papers focus on general shape/texture biases rather than the specific dual processing framework.

### Mechanism 3
- **Claim**: Logical processing in human vision can detect and correct errors by evaluating the validity, integrity, and coherence of perceived objects using basic factual knowledge.
- **Mechanism**: The paper shows that human vision assesses whether grouped components form valid objects by checking for impossible patterns, size mismatches, and count errors. Multi-modal LLMs have made progress on these logical errors but still struggle with fine-grained reasoning like size differences.
- **Core assumption**: Human vision uses logical evaluation of object properties to detect errors that intuitive processing misses.
- **Evidence anchors**:
  - [abstract] "analysis of errors shows that the early stopping of visual processing can result in missing relevant information"
  - [section] "Human vision can distinguish instances where objects of different sizes appear together with the shape of a single object in the second stage of processing"
  - [corpus] Weak evidence for this specific mechanism; corpus papers focus on general reasoning capabilities rather than visual logical processing.

## Foundational Learning

- **Concept: Dual thinking framework in cognitive psychology**
  - Why needed here: The paper builds on Kahneman's System 1/System 2 framework to understand human visual processing
  - Quick check question: Can you explain the difference between fast intuitive and slow logical processing in human cognition?

- **Concept: Gestalt principles of visual perception**
  - Why needed here: The paper uses Gestalt principles (similarity, proximity, continuity, figure-ground, amodal closure) to understand how humans group visual elements
  - Quick check question: How do Gestalt principles explain why humans might group unrelated elements together in certain images?

- **Concept: Multi-modal large language model architecture**
  - Why needed here: The paper compares human visual processing to MLLMs and VLMs, requiring understanding of how these models process visual information
  - Quick check question: What are the key architectural differences between vision-language models and traditional vision-only models?

## Architecture Onboarding

- **Component map**: Human Confusion Dataset -> Psychophysical experiment framework -> Automatic evaluation algorithm -> Multi-modal LLM models (GPT-4o, Llama 3.2) -> Segmentation models (YOLACT, Swin-Transformer)

- **Critical path**: Data collection → Dataset annotation → Human psychophysical trials → Model evaluation → Analysis of model behavior → Comparison of human vs. model performance

- **Design tradeoffs**: The paper uses an adversarial dataset that may not generalize to standard benchmarks, but provides deeper insights into reasoning mechanisms. Automatic evaluation trades some accuracy for scalability across multiple models.

- **Failure signatures**: If models perform well on this dataset but poorly on standard benchmarks, or if human participants don't report experiencing both intuitive and logical processing stages, the dual thinking framework may not be valid.

- **First 3 experiments**:
  1. Run the automatic evaluation algorithm on a subset of 50 images to validate threshold parameters against manual annotations
  2. Conduct pilot psychophysical trials with 10 participants to test question clarity and image selection
  3. Compare model performance on the Human Confusion Dataset versus a standard dataset like MS-COCO to establish baseline differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Human Confusion Dataset be extended to create domain-specific adversarial examples for safety-critical applications like autonomous driving or facial recognition systems?
- Basis in paper: [explicit] The paper mentions that the dataset could be extended to specific domains to assess edge cases relevant to those applications.
- Why unresolved: The current dataset is general-purpose and does not include domain-specific scenarios such as occluded vehicles, unusual lighting conditions, or facial spoofing techniques.
- What evidence would resolve it: Developing and testing domain-specific extensions of the dataset, followed by evaluating model performance on these tailored adversarial examples in real-world safety-critical systems.

### Open Question 2
- Question: How do the logical processing mechanisms in human vision differ from the reasoning strategies used by large language models (LLMs) when resolving perceptual ambiguities?
- Basis in paper: [explicit] The paper discusses that while LLMs have made progress in resolving logical errors, they still lack fine-grained reasoning abilities and may rely on learned patterns rather than true logical inference.
- Why unresolved: The paper highlights differences in reasoning but does not fully characterize the underlying mechanisms or compare them directly with human cognitive processes.
- What evidence would resolve it: Neurocognitive studies comparing human brain activity during visual reasoning tasks with LLM processing patterns, along with systematic evaluations of LLM reasoning on structured logical tasks.

### Open Question 3
- Question: Can segmentation models be enhanced to incorporate sub-component formation and logical reasoning similar to human vision, and how would this impact their performance on ambiguous or adversarial images?
- Basis in paper: [inferred] The paper notes that segmentation models lack sub-component structure and rely on probabilistic grouping, leading to errors that human vision corrects through logical processing.
- Why unresolved: Current segmentation models do not integrate the iterative refinement and error-checking mechanisms observed in human visual processing.
- What evidence would resolve it: Developing and testing segmentation architectures that explicitly model sub-components and incorporate logical verification steps, followed by benchmarking their performance on the Human Confusion Dataset.

## Limitations
- The study relies on a relatively small psychophysical sample (100 participants) which may not capture the full diversity of human visual processing
- The automatic evaluation algorithm using IoU thresholds may introduce false positives/negatives compared to semantic understanding
- The adversarial dataset design may not generalize well to real-world scenarios or standard computer vision benchmarks

## Confidence
- **High Confidence**: The existence of intuitive vs. logical processing stages in human vision (supported by established cognitive psychology literature and the psychophysical experiments)
- **Medium Confidence**: The claim that multi-modal LLMs have made significant progress in logical processing compared to segmentation models (supported by empirical results but dependent on the specific dataset and evaluation methodology)
- **Low Confidence**: The assertion that shape-based grouping is the primary strategy in intuitive human processing while texture-based grouping dominates in segmentation models (supported by limited direct evidence and some contradictory findings in the corpus)

## Next Checks
1. **Cross-dataset validation**: Evaluate the same models on a standard benchmark dataset (e.g., MS-COCO) to determine whether the observed differences between human and model performance generalize beyond the adversarial Human Confusion Dataset.

2. **Human model comparison refinement**: Conduct a controlled experiment where humans and models are presented with identical prompts and questions to isolate whether performance differences stem from processing capabilities or input differences.

3. **Error type analysis replication**: Manually annotate a subset of model outputs (50-100 images) to validate the automatic evaluation algorithm's classifications and quantify false positive/negative rates for each error type.