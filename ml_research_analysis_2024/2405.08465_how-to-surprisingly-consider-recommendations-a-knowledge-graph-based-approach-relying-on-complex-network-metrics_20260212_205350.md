---
ver: rpa2
title: How to Surprisingly Consider Recommendations? A Knowledge-Graph-based Approach
  Relying on Complex Network Metrics
arxiv_id: '2405.08465'
source_url: https://arxiv.org/abs/2405.08465
tags:
- recommendations
- items
- user
- metrics
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a knowledge-graph-based approach to introduce
  surprising elements into recommender systems by reranking items based on complex
  network metrics. The authors hypothesize that metrics like betweenness centrality,
  when computed on user profile subgraphs within a larger catalog knowledge graph,
  correlate with the degree of surprise in recommendations.
---

# How to Surprisingly Consider Recommendations? A Knowledge-Graph-based Approach Relying on Complex Network Metrics

## Quick Facts
- **arXiv ID:** 2405.08465
- **Source URL:** https://arxiv.org/abs/2405.08465
- **Reference count:** 29
- **Primary result:** Introduces a knowledge-graph-based re-ranking approach using betweenness centrality to introduce surprising elements into recommender systems while maintaining reasonable accuracy

## Executive Summary
This study proposes a knowledge-graph-based approach to introduce surprising elements into recommender systems by reranking items based on complex network metrics. The authors hypothesize that metrics like betweenness centrality, when computed on user profile subgraphs within a larger catalog knowledge graph, correlate with the degree of surprise in recommendations. Using two datasets (LastFM and Netflix), they demonstrate that reranking items based on network metrics such as node/edge counts, degree measures, and particularly betweenness centrality in ascending order leads to more unexpected and diverse recommendations compared to baseline approaches. The method introduces configurable levels of surprise while maintaining reasonable accuracy, as measured by novelty, diversity, and nDCG metrics.

## Method Summary
The approach constructs a catalog knowledge graph from item metadata and user interactions, extracts user profile subgraphs, and uses a base recommender system to generate initial recommendations. These recommendations are then re-ranked based on their impact on graph metrics (node/edge counts, degree measures, betweenness centrality) when added to the user's subgraph. The re-ranking layer sorts items by these metric impacts, with betweenness centrality in ascending order shown to produce the most surprising recommendations while maintaining reasonable accuracy.

## Key Results
- Betweenness centrality re-ranking in ascending order produces more unexpected and diverse recommendations
- The approach maintains reasonable accuracy (nDCG) while increasing novelty and diversity
- Node/edge count metrics provide a computationally cheaper alternative to betweenness centrality
- Configurable surprise levels can be achieved through different re-ranking strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Betweenness centrality computed on user profile subgraphs correlates with surprise in recommendations
- Mechanism: Items that, when added to a user's subgraph, increase network decentralization (lower betweenness HHI) tend to be more unexpected and diverse
- Core assumption: Surprise correlates with items that connect otherwise disconnected parts of the user's knowledge graph
- Evidence anchors:
  - [abstract] "reranking items based on network metrics such as node/edge counts, degree measures, and particularly betweenness centrality in ascending order leads to more unexpected and diverse recommendations"
  - [section 5] "recommendations sorted by betweenness in ascending order of the associated HHI exhibit high Unexpectedness and Diversity"

### Mechanism 2
- Claim: Re-ranking recommendations based on graph metric impact introduces configurable surprise levels
- Mechanism: The re-ranking layer evaluates how each candidate item would change structural graph metrics when added to the user's subgraph, then sorts items by this impact
- Core assumption: Structural changes to the user's subgraph reflect semantic novelty and surprise in recommendations
- Evidence anchors:
  - [abstract] "Our study explores whether network-level metrics on KGs can influence the degree of surprise in recommendations"
  - [section 3] "determine the impact of including that item and its KG-informed neighborhood on the user's subgraph through predetermined graph metrics"

### Mechanism 3
- Claim: Network metrics can replace or augment traditional similarity-based recommendation approaches
- Mechanism: By treating user profiles as subgraphs and measuring metric impact, the system can surface items that wouldn't be recommended by similarity alone
- Core assumption: Traditional RS approaches that prioritize globally popular items miss opportunities for serendipitous discovery
- Evidence anchors:
  - [abstract] "Existing approaches lack ways of introducing unexpectedness into recommendations, prioritizing globally popular items over exposing users to unforeseen items"
  - [section 1] "We argue that users may profit from recommendations that present a surprise element"

## Foundational Learning

- Concept: Knowledge Graphs and Ontology Modeling
  - Why needed here: The approach relies on constructing KGs from user interaction data and metadata to capture relational information between items
  - Quick check question: How would you represent the relationship between a music track, its artist, and its genre in a KG using standard ontologies?

- Concept: Graph Centrality Measures
  - Why needed here: Betweenness centrality, degree measures, and other network metrics are the core mechanism for determining recommendation surprise
  - Quick check question: What's the difference between betweenness centrality and degree centrality, and why might betweenness be more relevant for measuring surprise?

- Concept: Recommender System Evaluation Metrics
  - Why needed here: The paper uses novelty, diversity, and nDCG to evaluate whether the re-ranking introduces surprise without sacrificing relevance
  - Quick check question: How would you design an experiment to measure whether increased diversity in recommendations actually improves user satisfaction?

## Architecture Onboarding

- Component map: Catalog KG construction -> User profile subgraph extraction -> Base RS integration -> Graph metric computation engine -> Re-ranking layer -> Evaluation pipeline

- Critical path:
  1. Build catalog KG from item metadata
  2. Extract user profile subgraphs
  3. Get base recommendations from existing RS
  4. For each recommendation, compute impact on graph metrics
  5. Re-rank based on metric values
  6. Evaluate using surprise metrics

- Design tradeoffs:
  - Betweenness centrality is computationally expensive for large subgraphs vs. simpler metrics like node/edge counts
  - Configurable surprise levels vs. automatic optimization for user preferences
  - Rich KG construction vs. minimal KG that's faster to process

- Failure signatures:
  - Low nDCG scores indicate the re-ranking is moving too far from relevant items
  - High computational costs suggest need for subgraph truncation or metric simplification
  - Poor correlation between metric changes and user-perceived surprise

- First 3 experiments:
  1. Compare nDCG@10 for betweenness-based re-ranking vs. base recommendations on Netflix dataset
  2. Measure diversity and unexpectedness on LFM-1b dataset for ascending vs. descending betweenness sorting
  3. Test node/edge count re-ranking as a computationally cheaper alternative to betweenness on both datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does betweenness centrality specifically correlate with different dimensions of surprise (novelty, unexpectedness, diversity) in recommendations?
- Basis in paper: [explicit] The authors state that betweenness centrality correlates with different dimensions of surprise and that reranking by betweenness leads to more unexpected and surprising recommendations
- Why unresolved: The paper demonstrates correlation but doesn't provide a detailed theoretical explanation of why betweenness centrality specifically captures these surprise dimensions better than other metrics
- What evidence would resolve it: A study comparing the mathematical relationship between betweenness centrality values and specific surprise metrics, potentially using synthetic graphs to isolate the effect

### Open Question 2
- Question: How scalable is the betweenness centrality computation for real-time recommendation systems with large user profiles?
- Basis in paper: [inferred] The authors note that calculating betweenness may not be computationally feasible in constrained environments, especially on large profile subgraphs
- Why unresolved: The paper identifies this as a limitation but doesn't explore optimization techniques or provide performance benchmarks for different profile sizes
- What evidence would resolve it: Empirical studies measuring computation time for betweenness centrality on various profile sizes, and evaluation of approximation algorithms or sampling techniques

### Open Question 3
- Question: How would cross-domain recommendations (e.g., music tracks appearing in movies) affect the surprise metrics and user satisfaction?
- Basis in paper: [explicit] The authors mention this as future work, noting that their catalog KGs only contain intra-domain concepts and that cross-domain linking could enable new types of recommendations
- Why unresolved: The study is limited to single-domain datasets (LastFM and Netflix titles), preventing analysis of cross-domain effects
- What evidence would resolve it: Experiments using KGs with cross-domain links and user studies measuring satisfaction with cross-domain surprising recommendations

## Limitations

- Computational complexity of betweenness centrality may limit scalability for large user profiles
- Reliance on high-quality metadata for KG construction, which may not always be available
- Evaluation based on proxy metrics rather than direct user studies measuring surprise perception

## Confidence

- Low confidence in the scalability of betweenness centrality for large user subgraphs
- Medium confidence that the catalog KG construction adequately captures semantic relationships
- Medium confidence that betweenness centrality correlates with user-perceived surprise

## Next Checks

1. **Computational scaling analysis**: Measure execution time for betweenness centrality computation on user subgraphs ranging from 10 to 1000 items to establish practical limits and identify threshold points where alternative metrics become necessary.

2. **User study validation**: Conduct A/B testing where real users rate recommendations generated with different re-ranking strategies (betweenness ascending, descending, and baseline) on surprise, relevance, and satisfaction dimensions to verify that metric-based surprise aligns with user perception.

3. **KG robustness testing**: Systematically remove 20-80% of edges from the catalog KG and measure the impact on re-ranking performance to determine the minimum connectivity requirements for the approach to function effectively.