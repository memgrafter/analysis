---
ver: rpa2
title: Accelerating Diffusion Transformers with Token-wise Feature Caching
arxiv_id: '2410.05317'
source_url: https://arxiv.org/abs/2410.05317
tags:
- tokens
- cache
- caching
- toca
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ToCa, a token-wise feature caching method for
  accelerating diffusion transformers. The method identifies that different tokens
  exhibit varying sensitivities to feature caching in terms of temporal redundancy
  and error propagation.
---

# Accelerating Diffusion Transformers with Token-wise Feature Caching

## Quick Facts
- **arXiv ID**: 2410.05317
- **Source URL**: https://arxiv.org/abs/2410.05317
- **Reference count**: 40
- **Key outcome**: 2.36× and 1.93× acceleration on OpenSora and PixArt-α respectively with almost no drop in generation quality

## Executive Summary
This paper proposes ToCa, a token-wise feature caching method that accelerates diffusion transformers by identifying tokens with varying sensitivities to caching based on temporal redundancy and error propagation. The method introduces four scores to select optimal tokens for caching without additional computation costs, and enables different caching ratios for layers of different depths and types. Extensive experiments demonstrate ToCa's effectiveness across image and video generation tasks, achieving significant acceleration while maintaining generation quality.

## Method Summary
ToCa is a training-free approach that uses four scores (s1: self-attention influence, s2: cross-attention entropy, s3: cache frequency tracking, s4: spatial uniformity) to select tokens for caching in each layer. The method applies different caching ratios based on layer type (self-attention: full computation, cross-attention and MLP: proportional to compute cost) and depth (higher ratios for deeper layers). Feature caching is initialized at the first timestep, tokens are selected and computed at subsequent timesteps, and caches are updated after token computation.

## Key Results
- Achieves 2.36× acceleration on OpenSora (video generation) with minimal quality loss
- Achieves 1.93× acceleration on PixArt-α (image generation) while maintaining FID-30k scores
- Demonstrates effectiveness across multiple model architectures including PixArt-α, OpenSora, and DiT

## Why This Works (Mechanism)

### Mechanism 1
Different tokens exhibit varying sensitivities to feature caching in terms of temporal redundancy and error propagation. Tokens with higher temporal redundancy (similar features across timesteps) and lower error propagation impact can be safely cached without significant quality degradation. The core assumption is that feature distance between adjacent timesteps and error propagation patterns are consistent enough to form a reliable scoring system.

### Mechanism 2
Four scores (s1, s2, s3, s4) can effectively identify tokens suitable for caching without additional computation costs. s1 measures influence to other tokens via self-attention weights, s2 measures influence on control ability via cross-attention entropy, s3 prevents over-caching via frequency tracking, and s4 ensures spatial uniformity. The core assumption is that attention weights and cache frequency can serve as reliable proxies for token importance without requiring additional forward passes.

### Mechanism 3
Different caching ratios for layers of different depths and types optimize the trade-off between acceleration and quality. Deeper layers receive higher caching ratios due to better temporal redundancy, while self-attention layers cache all tokens to prevent error propagation. The core assumption is that the relationship between layer depth/type and caching suitability is consistent across different models and tasks.

## Foundational Learning

- **Temporal redundancy in diffusion models**: Understanding why feature caching works at all - tokens often have similar features across adjacent timesteps. Quick check: What is the primary reason feature caching can accelerate diffusion models without significant quality loss?

- **Attention mechanisms in transformers**: Self-attention and cross-attention weights are used to compute the caching scores. Quick check: How do self-attention weights indicate a token's influence on other tokens?

- **Error propagation in neural networks**: Understanding how errors introduced by caching can propagate through the network and affect final output quality. Quick check: Why does caching a token in an early layer potentially have a larger impact than caching a token in a later layer?

## Architecture Onboarding

- **Component map**: Input tokens → Self-attention layers → Cross-attention layers → MLP layers → Output; Cache storage for intermediate features; Scoring system (s1, s2, s3, s4) for token selection; Layer-specific caching ratio controllers (rl, rtype)

- **Critical path**: Token feature computation → Cache score calculation → Token selection → Feature caching/reuse → Cache update

- **Design tradeoffs**: Higher caching ratios provide more acceleration but risk quality degradation; More sophisticated scoring increases selection accuracy but adds computational overhead; Layer-specific caching ratios optimize performance but increase implementation complexity

- **Failure signatures**: Significant FID score degradation; Loss of text-image alignment (CLIP score drop); Visual artifacts in generated images/videos; Inconsistent results across different random seeds

- **First 3 experiments**: 1) Baseline comparison: Run original model without caching to establish performance metrics; 2) Uniform caching test: Apply equal caching ratio across all tokens and layers to measure quality impact; 3) Score ablation study: Test each caching score (s1, s2, s3, s4) individually to understand their individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of hyperparameters (N, R, λ values) in ToCa affect the trade-off between acceleration and generation quality across different diffusion transformer architectures? The paper shows hyperparameter sensitivity for DiT-XL/2 but does not demonstrate how these hyperparameters generalize across different model families or provide a systematic method for selecting optimal hyperparameters for new architectures.

### Open Question 2
What is the theoretical limit of acceleration achievable with token-wise feature caching before generation quality becomes unacceptably degraded? The paper demonstrates up to 2.36× acceleration on OpenSora with minimal quality loss, but does not establish theoretical bounds or explore the fundamental limits of this approach.

### Open Question 3
How does ToCa perform when applied to diffusion transformers with different attention mechanisms or architectural variations? The paper evaluates ToCa on three diffusion transformer models but does not explore its performance on architectures with different attention mechanisms (e.g., linear attention, sparse attention) or structural variations.

## Limitations

- The generalizability of ToCa across different diffusion transformer architectures remains uncertain, as experiments are limited to three specific models (PixArt-α, OpenSora, DiT)
- The interaction between layer-specific caching ratios and different model architectures is not thoroughly explored
- The claim of "no additional computation costs" is questionable given the complexity of the scoring system

## Confidence

- **High Confidence**: The fundamental observation that different tokens exhibit varying sensitivities to feature caching is well-supported by empirical evidence showing up to 2.5× differences in feature distance between adjacent timesteps. The acceleration metrics (2.36× and 1.93×) are clearly reported with specific FLOPs calculations.

- **Medium Confidence**: The four-score selection mechanism shows theoretical soundness, but the paper lacks ablation studies demonstrating each score's individual contribution to overall performance. The claim that this approach works "without additional computation costs" is questionable given the complexity of the scoring system.

- **Low Confidence**: The generalizability of ToCa across different diffusion transformer architectures remains uncertain, as experiments are limited to three specific models. The interaction between layer-specific caching ratios and different model architectures is not thoroughly explored.

## Next Checks

1. **Ablation Study**: Conduct controlled experiments to isolate the contribution of each caching score (s1, s2, s3, s4) by testing them individually and in different combinations. This would validate whether all four scores are necessary and identify which ones provide the most significant performance gains.

2. **Cross-Architecture Testing**: Apply ToCa to at least two additional diffusion transformer architectures beyond the three tested (PixArt-α, OpenSora, DiT), including both image and video generation models with different attention mechanisms and layer configurations. Compare performance consistency across architectures.

3. **Computational Overhead Analysis**: Implement a detailed profiling study to measure the actual computational overhead of the scoring system, including attention weight extraction, entropy calculation, and cache frequency tracking. Compare this overhead against the claimed "no additional computation costs" to provide a complete efficiency picture.