---
ver: rpa2
title: Performance Control in Early Exiting to Deploy Large Models at the Same Cost
  of Smaller Ones
arxiv_id: '2412.19325'
source_url: https://arxiv.org/abs/2412.19325
tags:
- accuracy
- layers
- confidence
- exit
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a new perspective on early exiting, demonstrating
  that larger models deployed with early exiting can achieve higher performance than
  smaller models while maintaining similar computational costs. The authors propose
  Performance Control Early Exiting (PCEE), a method that enables accuracy thresholding
  by using the average accuracy of samples with similar confidence levels from a held-out
  validation set, rather than relying on data point's confidence.
---

# Performance Control in Early Exiting to Deploy Large Models at the Same Cost of Smaller Ones

## Quick Facts
- arXiv ID: 2412.19325
- Source URL: https://arxiv.org/abs/2412.19325
- Reference count: 30
- This work demonstrates that larger models with early exiting can achieve higher performance than smaller models while maintaining similar computational costs.

## Executive Summary
This paper introduces Performance Control Early Exiting (PCEE), a novel approach that enables accuracy thresholding in early exiting models by using average accuracy from validation set reliability diagrams rather than sample confidence. PCEE provides better control over performance than standard confidence-based approaches, allowing larger models to achieve higher accuracy while maintaining similar computational costs to smaller models. Experiments on image classification benchmarks show that PCEE consistently achieves higher accuracy than baselines while maintaining accuracy above the set threshold.

## Method Summary
PCEE replaces confidence-based exiting with accuracy-based decisions using reliability diagrams constructed from validation data. For each exit layer, the method bins confidence scores from the validation set and calculates the average accuracy per bin. During inference, when a sample reaches an exit layer, its confidence score is used to look up the corresponding accuracy estimate from the reliability diagram. If this accuracy estimate meets or exceeds a predefined threshold δ, the model exits early with that prediction; otherwise, it continues to the next layer. This approach provides direct control over the minimum achievable accuracy while maintaining computational efficiency.

## Key Results
- PCEE consistently achieves higher accuracy than confidence-based baselines across CIFAR-10, CIFAR-100, and ImageNet-1K
- Larger models with PCEE achieve 6% error at 26M FLOPs versus 7.4% for smaller models at the same computational cost
- PCEE maintains accuracy above the specified threshold δ for all tested configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models with early exiting can achieve higher accuracy at lower computational cost than smaller models.
- Mechanism: Deep architectures allow easier samples to be classified correctly at early layers while reserving later layers for harder samples, reducing average compute.
- Core assumption: The model's intermediate representations improve monotonically with depth for a sufficient fraction of samples.
- Evidence anchors:
  - [abstract] "larger models deployed with EE can achieve higher performance than smaller models while maintaining similar computational costs"
  - [section] "The large model with PCEE achieves a prediction error of around 6% using 2 layers (approximately 26 × 10^6 FLOPs) on average. In contrast, the smaller model utilizing the same amount of FLOPs has a higher level of prediction error (about 7.4%)"
  - [corpus] Weak/no direct evidence on compute-accuracy scaling in early-exiting literature; relies on this paper's own experiments.
- Break condition: If intermediate layers do not generalize well for easy samples, or if accuracy does not improve monotonically with depth.

### Mechanism 2
- Claim: Using average accuracy from validation set (PCEE) provides better control over performance than confidence-based thresholding.
- Mechanism: Reliability diagrams map confidence scores to average accuracy per bin; exit decisions are made based on whether the bin's accuracy meets the desired threshold, not the sample's raw confidence.
- Core assumption: The confidence-to-accuracy mapping learned from validation data generalizes to test data.
- Evidence anchors:
  - [abstract] "enables accuracy thresholding by basing decisions not on a data point's confidence but on the average accuracy of samples with similar confidence levels from a held-out validation set"
  - [section] "PCEE... offers a simple yet computationally efficient alternative that provides control over performance... Unlike confidence, accuracy is an indicator of the actual model performance"
  - [corpus] No strong corpus support; method is novel and contrasts with existing confidence thresholding.
- Break condition: If validation and test distributions differ significantly, or if confidence is poorly correlated with accuracy in a way that varies across layers.

### Mechanism 3
- Claim: Early exiting can reduce "destructive overthinking" by allowing correct predictions from intermediate layers even when the final layer misclassifies.
- Mechanism: If an intermediate exit layer's prediction matches the ground truth but the final layer's does not, early exit avoids the drop in accuracy caused by the last layer's misclassification.
- Core assumption: Intermediate classifiers can be accurate even if the final classifier is not, due to regularization or representational differences.
- Evidence anchors:
  - [section] "This surprising result can happen when intermediate layers predict the correct label while the last layer does not, known as destructive overthinking (Kaya et al., 2018). This suggests that early exiting (EE) may have a regularizing effect"
  - [corpus] No direct corpus evidence provided; relies on cited work (Kaya et al., 2018) for the phenomenon.
- Break condition: If the final layer is always at least as accurate as intermediate layers, or if intermediate layers overfit to simpler patterns.

## Foundational Learning

- Concept: Calibration of neural network confidence estimates.
  - Why needed here: Early exiting relies on confidence thresholds; miscalibration causes premature exits and degraded accuracy.
  - Quick check question: If a model assigns 80% confidence to 100 predictions, how many should be correct if it is perfectly calibrated?

- Concept: Reliability diagrams and Expected Calibration Error (ECE).
  - Why needed here: PCEE uses reliability diagrams to map confidence to accuracy; understanding ECE helps assess calibration quality.
  - Quick check question: What does it mean if a bin in a reliability diagram lies above the y=x line?

- Concept: Overconfidence in deep networks.
  - Why needed here: Overconfident intermediate layers trigger premature exits; understanding this motivates PCEE's accuracy-based approach.
  - Quick check question: How does increasing model depth typically affect ECE?

## Architecture Onboarding

- Component map:
  - Exit layers (one per intermediate layer) produce logits for classification
  - Confidence scores (softmax max or entropy) are computed from exit layer outputs
  - Reliability diagrams (learned from validation data) map confidence to accuracy per bin
  - Decision logic compares accuracy from reliability diagram to threshold δ to determine exit

- Critical path:
  1. Input passes through layer L_i
  2. Exit layer E_i produces logits and confidence c_i
  3. Look up accuracy estimate a_i from reliability diagram D_i for c_i
  4. If a_i ≥ δ, exit and output prediction; else continue to next layer

- Design tradeoffs:
  - Using a single global accuracy threshold δ simplifies tuning but may not adapt to layer-specific behavior
  - Smoothing accuracy per bin (PCEE-WS) reduces variance but adds a hyperparameter (H)
  - Post-hoc calibration (TS) can improve performance but adds training overhead

- Failure signatures:
  - Model consistently exits early with low accuracy: likely overfitted validation set or distribution shift
  - Model never exits early: threshold δ too high or reliability diagram inaccurate
  - High variance in accuracy across seeds: bin sizes in reliability diagram too small; increase smoothing or bins

- First 3 experiments:
  1. Train a model with intermediate exit layers; plot reliability diagrams for each layer to check calibration trends
  2. Apply PCEE with δ set to desired accuracy; measure average layers used and final accuracy; compare to confidence baseline
  3. Vary δ to generate accuracy/compute tradeoff curve; confirm controllability property (accuracy ≥ δ for all thresholds)

## Open Questions the Paper Calls Out

- The paper doesn't explicitly call out open questions in the text provided.

## Limitations

- Computational savings comparison between large and small models is somewhat idealized, not accounting for overhead from additional exit layers
- Method assumes validation data represents test distribution well enough for reliability diagrams to generalize
- Smoothing parameter H for PCEE-WS is mentioned without clear justification or sensitivity analysis

## Confidence

- **High confidence**: The core mechanism of using reliability diagrams for accuracy-based exiting is technically sound and well-implemented
- **Medium confidence**: The claim about "destructive overthinking" regularization effect is plausible but not rigorously demonstrated
- **Medium confidence**: The comparative advantage over baselines is shown but limited to specific architectures and datasets

## Next Checks

1. Conduct ablation studies on the smoothing parameter H across different dataset sizes to understand its sensitivity
2. Test PCEE under domain shift conditions where validation and test distributions differ to evaluate reliability diagram generalization
3. Measure actual wall-clock inference time (not just FLOPs) to quantify real-world computational savings when using PCEE