---
ver: rpa2
title: 'Large Language Models in Mental Health Care: a Scoping Review'
arxiv_id: '2401.02984'
source_url: https://arxiv.org/abs/2401.02984
tags:
- mental
- health
- language
- llms
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review provides the first comprehensive analysis of large
  language models (LLMs) in mental health care. It identified diverse applications
  of LLMs in mental health, including diagnosis, therapy, and patient engagement enhancement.
---

# Large Language Models in Mental Health Care: a Scoping Review

## Quick Facts
- **arXiv ID**: 2401.02984
- **Source URL**: https://arxiv.org/abs/2401.02984
- **Reference count**: 40
- **Key outcome**: This review provides the first comprehensive analysis of large language models (LLMs) in mental health care. It identified diverse applications of LLMs in mental health, including diagnosis, therapy, and patient engagement enhancement. However, key challenges remain in data availability and reliability, nuanced handling of mental states, and effective evaluation methods. While LLMs show promise in improving accuracy and accessibility, significant gaps exist in clinical applicability and ethical considerations. The review emphasizes the need for robust datasets, standardized evaluations, and interdisciplinary collaboration to address current limitations and fully realize the potential of LLMs in mental health care.

## Executive Summary
This scoping review systematically examines the applications, challenges, and gaps in using large language models (LLMs) for mental health care. Analyzing 34 studies published between October 2019 and December 2022, the review identifies diverse applications of LLMs in mental health, including diagnosis, therapy, and patient engagement enhancement. However, significant challenges persist in data availability and reliability, nuanced handling of mental states, and effective evaluation methods. The review highlights the need for robust datasets, standardized evaluations, and interdisciplinary collaboration to address current limitations and fully realize the potential of LLMs in mental health care.

## Method Summary
This systematic scoping review followed PRISMA guidelines to identify relevant studies using specific keywords across six databases (PubMed, Web of Science, Google Scholar, arXiv, medRxiv, PsyArXiv). The search focused on LLM models developed after T5 (2019) and published between October 2019 and December 2022. GPT-4 was employed for initial article screening with Cohen's Kappa of ~0.9024, followed by manual review of 34 studies. Studies were categorized into four main areas: Dataset and Benchmark, Model Development and Fine-tuning, Application and Evaluation, and Ethics, Privacy, and Safety considerations.

## Key Results
- LLMs demonstrate promise in improving accuracy and accessibility of mental health care through applications in diagnosis, therapy, and patient engagement
- Fine-tuning existing LLMs with mental health-specific data is the predominant approach, with instruction fine-tuning being the most common technique
- Significant challenges remain in data availability, evaluation methods, and ethical considerations for clinical deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can effectively support mental health care tasks like diagnosis, therapy, and patient engagement.
- Mechanism: LLMs leverage their natural language processing capabilities to analyze text data from sources like electronic health records, social media, and therapy sessions, providing insights into patient behaviors and experiences. They can also engage in conversational interactions, offering a relatable means for individuals to articulate their emotional states.
- Core assumption: LLMs can accurately interpret the nuances of human language and mental states.
- Evidence anchors:
  - [abstract]: "LLMs showed promise in improving accuracy and accessibility"
  - [section]: "LLMs, as a class of algorithms, have demonstrated remarkable capabilities in NLP. Their skillset aligns closely with the requirements for mental health-related tasks"
  - [corpus]: "Average neighbor FMR=0.406, indicating moderate relevance of related papers to the core topic"
- Break condition: If LLMs fail to accurately interpret the nuances of human language and mental states, their effectiveness in mental health care tasks would be compromised.

### Mechanism 2
- Claim: Fine-tuning existing LLMs with mental health-specific data can improve their performance in mental health care tasks.
- Mechanism: By fine-tuning general LLMs on mental health-specific texts, such as social media data and therapy session transcripts, the models can acquire domain knowledge and evolve into dedicated mental health-focused LLMs.
- Core assumption: Fine-tuning can effectively inject mental health knowledge into LLMs without causing significant performance degradation.
- Evidence anchors:
  - [section]: "Acknowledging the high cost and extensive time required to train LLMs from scratch, existing studies have uniformly adopted fine-tuning existing models on mental health data"
  - [section]: "All the studies that involved fine-tuning adopted the instruction fine-tuning (IFT) technique"
  - [corpus]: "Found 25 related papers, suggesting a growing body of research on LLMs in mental health care"
- Break condition: If fine-tuning leads to overfitting or loss of general language understanding, the performance of the LLMs in mental health care tasks may suffer.

### Mechanism 3
- Claim: A combination of human and automated evaluations is necessary to comprehensively assess the performance of LLMs in mental health care tasks.
- Mechanism: Automated metrics like BLEU, ROUGE, and F1 scores can evaluate language proficiency and mental health applicability, while human assessments can provide insights into aspects like empathy, relevance, and helpfulness.
- Core assumption: Human assessments can capture aspects of LLM performance that automated metrics may miss.
- Evidence anchors:
  - [section]: "Among the 34 articles reviewed, 19 employed a combination of human and automated evaluations"
  - [section]: "Human evaluation is thus frequently used across studies"
  - [corpus]: "Found 25 related papers, indicating ongoing research on evaluating LLMs in mental health care"
- Break condition: If human assessments are inconsistent or biased, they may not provide reliable insights into LLM performance.

## Foundational Learning

- Concept: Natural Language Processing (NLP)
  - Why needed here: Understanding the basics of NLP is crucial for comprehending how LLMs can be applied in mental health care tasks.
  - Quick check question: What are the key components of an NLP system, and how do they work together to process and generate human language?

- Concept: Mental Health Conditions and Symptoms
  - Why needed here: Familiarity with various mental health conditions and their symptoms is essential for developing and evaluating LLMs in mental health care.
  - Quick check question: What are the main diagnostic criteria for depression, anxiety, and other common mental health disorders?

- Concept: Ethical Considerations in AI and Mental Health
  - Why needed here: Understanding the ethical implications of using LLMs in mental health care is crucial for ensuring responsible development and deployment.
  - Quick check question: What are the key ethical concerns when using AI in mental health, and how can they be addressed?

## Architecture Onboarding

- Component map: Data collection -> Model development (fine-tuning) -> Evaluation (automated + human) -> Deployment
- Critical path: Collect high-quality mental health data → Fine-tune LLMs on this data → Evaluate models using appropriate metrics → Deploy in clinical settings
- Design tradeoffs: Large, general-purpose LLMs vs. smaller, domain-specific models (better language understanding vs. efficiency and easier fine-tuning)
- Failure signatures: Biased or inaccurate outputs, lack of empathy in conversations, poor performance on mental health tasks, data quality issues
- First 3 experiments:
  1. Evaluate pre-trained LLM performance on mental health task using BLEU and F1 scores
  2. Fine-tune LLM on mental health dataset, compare performance to pre-trained model using same metrics
  3. Conduct human evaluation of fine-tuned model's outputs, focusing on empathy, relevance, and helpfulness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal approach to evaluating LLM-generated text for mental health applications, given the current limitations of automated metrics and the lack of a standardized human evaluation framework?
- Basis in paper: [explicit] The review highlights the inadequacy of existing automated metrics like BLEU, ROUGE, and BERT-Score in capturing the nuances of mental health dialogues, and notes the absence of a unified human evaluation framework.
- Why unresolved: Current metrics are designed for general language tasks and fail to account for the specific requirements of mental health care, such as empathy and clinical relevance. The lack of standardization in human evaluation leads to inconsistent and incomparable results across studies.
- What evidence would resolve it: Development and validation of a comprehensive evaluation framework specifically tailored for LLM-generated text in mental health contexts, incorporating both automated and human evaluation components that capture empathy, clinical accuracy, and contextual understanding.

### Open Question 2
- Question: How can LLMs be effectively integrated into clinical workflows while ensuring patient privacy, safety, and ethical standards are maintained?
- Basis in paper: [explicit] The review emphasizes the need for robust data protection, adherence to regulations like HIPAA and GDPR, and continuous monitoring of LLM performance to prevent harmful content generation and ensure clinical efficacy.
- Why unresolved: The integration of LLMs into clinical settings involves complex challenges related to data security, algorithmic bias, and the potential for over-reliance on AI. Balancing the benefits of LLM assistance with the need for human oversight and ethical considerations requires further investigation.
- What evidence would resolve it: Implementation and evaluation of LLM integration frameworks that incorporate privacy-preserving techniques, bias detection mechanisms, and clear guidelines for human-AI collaboration in clinical decision-making.

### Open Question 3
- Question: What is the most effective approach to fine-tuning LLMs for mental health applications, considering the limitations of current datasets and the need for specialized training data?
- Basis in paper: [explicit] The review notes that current fine-tuning efforts primarily rely on dialogue and social media data, which may not accurately represent clinical diagnoses and lack diversity in user populations. It also highlights the need for high-quality data that accurately defines mental health conditions.
- Why unresolved: The scarcity of large, diverse, and clinically validated datasets for mental health applications hinders the development of robust and generalizable LLMs. Fine-tuning on biased or unrepresentative data can lead to inaccurate diagnoses and ineffective interventions.
- What evidence would resolve it: Creation and evaluation of large-scale, diverse, and clinically validated datasets for mental health applications, along with the development of fine-tuning techniques that address data limitations and ensure the transfer of knowledge from training data to real-world clinical scenarios.

## Limitations

- Small sample size of 34 studies may not capture the full landscape of LLM applications in mental health care
- Limited insights into model capabilities due to focus on fine-tuning rather than training from scratch
- Ethical considerations section lacks concrete implementation guidelines

## Confidence

- **High Confidence**: The systematic methodology following PRISMA guidelines and the use of GPT-4 for initial screening with high inter-rater agreement (Cohen's Kappa ~0.9024) provide strong confidence in the review's comprehensiveness and rigor.
- **Medium Confidence**: The identification of diverse LLM applications and challenges in mental health care is well-supported, but the limited number of studies and focus on fine-tuning techniques reduces confidence in generalizability.
- **Low Confidence**: The practical implications for clinical deployment and specific ethical frameworks remain underdeveloped, requiring further research and validation.

## Next Checks

1. Conduct a meta-analysis of the 34 studies to quantify the effect sizes of LLM performance improvements across different mental health tasks and evaluation metrics.
2. Perform a sensitivity analysis on the screening process by having multiple independent reviewers evaluate a subset of studies to validate the GPT-4 screening accuracy and refine inclusion criteria.
3. Develop a standardized evaluation framework that combines automated metrics with validated human assessment protocols to ensure consistent and reliable evaluation of LLM performance in mental health care applications.