---
ver: rpa2
title: 'BRAVE: Broadening the visual encoding of vision-language models'
arxiv_id: '2404.07204'
source_url: https://arxiv.org/abs/2404.07204
tags:
- vision
- visual
- arxiv
- encoders
- brave
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of limited visual encoding capabilities
  in vision-language models (VLMs), which can lead to issues like "blindness" to certain
  image features and visual hallucinations. The authors propose BRAVE (Broadening
  the visual encoding of vision-language models), a method that consolidates features
  from multiple frozen vision encoders into a more versatile representation for VLMs.
---

# BRAVE: Broadening the visual encoding of vision-language models

## Quick Facts
- arXiv ID: 2404.07204
- Source URL: https://arxiv.org/abs/2404.07204
- Authors: Oğuzhan Fatih Kar; Alessio Tonioni; Petra Poklukar; Achin Kulshrestha; Amir Zamir; Federico Tombari
- Reference count: 40
- Key outcome: State-of-the-art performance on captioning and VQA benchmarks while reducing visual hallucinations and using fewer trainable parameters

## Executive Summary
This paper addresses the fundamental limitation of vision-language models (VLMs) that rely on single vision encoders, which can lead to "blindness" to certain image features and visual hallucinations. The authors propose BRAVE (Broadening the visual encoding of vision-language models), a method that consolidates features from multiple frozen vision encoders into a more versatile representation. By using a multi-encoder querying transformer (MEQ-Former) to efficiently combine visual features from an arbitrary number of vision encoders with different inductive biases, BRAVE achieves state-of-the-art performance across a broad range of captioning and visual question answering benchmarks while requiring significantly fewer trainable parameters than existing methods.

## Method Summary
BRAVE uses a multi-encoder querying transformer (MEQ-Former) to combine visual features from multiple frozen vision encoders into a compressed, task-aligned representation for a frozen language model. The MEQ-Former learns to cross-attend to concatenated visual features from different encoders using learnable queries and a textual prompt, resampling them into a compact visual representation that serves as a soft prompt for the language model. The method is pre-trained on the WebLI dataset and fine-tuned on downstream tasks, achieving superior performance while training only ~116M parameters (about 1% of the total VLM parameters).

## Key Results
- Achieves state-of-the-art performance on COCO captioning (134.2 CIDEr), NoCaps, VQAv2, OKVQA, GQA, VizWiz-QA, MMVP, and POPE benchmarks
- Reduces visual hallucinations and improves robustness against out-of-distribution inputs compared to single-encoder VLMs
- Requires significantly fewer trainable parameters (116M) than existing methods while maintaining compressed representation

## Why This Works (Mechanism)

### Mechanism 1
Consolidating diverse visual features from multiple encoders into a single fixed-length representation improves robustness and reduces hallucinations. The MEQ-Former learns to cross-attend to concatenated visual features from different encoders, resampling them into a compressed, task-aligned prompt for the frozen LM. This bypasses the need to forward all encoders at inference time while still leveraging their complementary strengths. The core assumption is that different encoders capture complementary visual cues, yielding a more complete representation than any single encoder.

### Mechanism 2
Using a frozen LM with a learned bridging module (MEQ-Former) reduces trainable parameters while maintaining or improving performance. The MEQ-Former acts as a bottleneck, resampling high-dimensional concatenated visual features into a compact prompt that the frozen LM can interpret. Only ~116M parameters are trained versus billions in full fine-tuning. The core assumption is that the LM's pre-trained knowledge is sufficient for downstream tasks if given appropriately aligned visual features.

### Mechanism 3
Pre-training the MEQ-Former with encoder dropout encourages it to learn to use all encoders rather than relying on a single one. During pre-training, each encoder's features are randomly masked with 20% probability, forcing the MEQ-Former to develop a robust strategy for combining available encoders. The core assumption is that encoder dropout acts as a regularizer, preventing the MEQ-Former from overfitting to a single encoder's features.

## Foundational Learning

- Concept: Vision encoders trained on different objectives and data produce complementary visual representations.
  - Why needed here: Understanding that no single encoder dominates all tasks motivates the multi-encoder design.
  - Quick check question: If CLIP and DINOv2 are both trained on image-text pairs but with different objectives, what complementary visual cues might each capture?

- Concept: Cross-attention vs self-attention in transformer layers and their computational complexity.
  - Why needed here: The MEQ-Former uses cross-attention to efficiently resample concatenated features, avoiding the quadratic cost of self-attention over many tokens.
  - Quick check question: Why does concatenating visual features and using cross-attention reduce computation compared to self-attention over all tokens?

- Concept: Frozen LM adaptation through prompt engineering and bridging modules.
  - Why needed here: The MEQ-Former's output must be in the LM's input space; understanding how soft prompts work is critical for implementation.
  - Quick check question: How does the MEQ-Former's fixed-length output serve as a "soft visual prompt" for the frozen LM?

## Architecture Onboarding

- Component map: Vision Encoders (frozen) -> MEQ-Former -> Frozen Language Model
- Critical path: 
  1. Load and preprocess images through all vision encoders
  2. Linearly project and concatenate visual features (1223×1408 → 1223×1408)
  3. MEQ-Former cross-attends to concatenated features using learnable queries and text prompt
  4. Project MEQ-Former output to LM embedding space
  5. Feed to frozen LM for generation or classification
- Design tradeoffs:
  - More encoders → better coverage but higher memory/computation during feature extraction
  - Larger MEQ-Former → better fusion capacity but more trainable parameters
  - Dropout during pre-training → better robustness but may slow convergence
- Failure signatures:
  - Training instability: Check encoder dropout rate and learning rate
  - Poor downstream performance: Verify visual features are properly projected and concatenated
  - Memory errors: Monitor feature extraction and concatenation steps
- First 3 experiments:
  1. Ablation: Remove one encoder at a time to measure performance drop and robustness
  2. Encoder dropout: Vary dropout probability (0%, 10%, 20%, 30%) during pre-training
  3. Resolution scaling: Compare 224×224 vs 336×336 input resolution on COCO captioning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of vision encoder affect the performance of VLMs on different downstream tasks?
- Basis in paper: The authors conducted a comprehensive benchmark of several vision encoders with different inductive biases on various VQA and captioning tasks.
- Why unresolved: The authors found that there is no single encoder achieving consistent top performance across tasks, and encoders with different biases can perform surprisingly similarly. However, the specific impact of each encoder on different tasks was not fully explored.
- What evidence would resolve it: A more detailed analysis of the performance of each encoder on individual tasks, including a breakdown of their strengths and weaknesses, would help understand the impact of the choice of vision encoder on VLM performance.

### Open Question 2
- Question: How does the number of vision encoders used in BRAVE affect its performance and efficiency?
- Basis in paper: The authors used five vision encoders in their main results, but did not explore the impact of using more or fewer encoders.
- Why unresolved: While the authors showed that BRAVE outperforms single-encoder VLMs, it is unclear how the number of encoders affects its performance and efficiency.
- What evidence would resolve it: Experiments comparing the performance and efficiency of BRAVE with different numbers of encoders would help determine the optimal number of encoders for a given task or resource constraint.

### Open Question 3
- Question: How does BRAVE compare to other methods that combine multiple vision encoders, such as LLaVa-MoF and SPHINX?
- Basis in paper: The authors mentioned these methods in the related work section but did not directly compare their performance to BRAVE.
- Why unresolved: While the authors showed that BRAVE outperforms single-encoder VLMs, it is unclear how it compares to other methods that combine multiple encoders.
- What evidence would resolve it: A direct comparison of BRAVE to LLaVa-MoF and SPHINX on the same benchmarks used in the paper would help determine the relative strengths and weaknesses of these approaches.

## Limitations

- The assumption that different encoders capture truly complementary visual cues lacks direct empirical validation through ablation studies
- The method's performance ceiling with a frozen LM is unknown, as the paper doesn't compare against fine-tuning the LM
- Claims about hallucination reduction rely on indirect robustness tests rather than direct hallucination detection metrics

## Confidence

- High confidence: The MEQ-Former architecture design, parameter efficiency claims (116M trainable parameters), and baseline performance comparisons are well-documented and reproducible
- Medium confidence: The state-of-the-art benchmark results are convincing, but depend on specific encoder combinations and pre-training procedures that may not generalize across different LM scales or domains
- Low confidence: Claims about hallucination reduction and encoder complementarity require more direct evidence beyond indirect robustness tests

## Next Checks

1. **Encoder ablation study**: Systematically remove each encoder individually during both pre-training and fine-tuning to quantify their individual contributions and test the complementarity assumption

2. **LM fine-tuning comparison**: Implement a version that fine-tunes the LM alongside MEQ-Former to establish the true performance ceiling and validate the frozen LM design choice

3. **Hallucination detection**: Apply established hallucination detection metrics (e.g., text-image consistency scores) and conduct controlled experiments with known hallucination-inducing inputs to directly measure the claimed reduction