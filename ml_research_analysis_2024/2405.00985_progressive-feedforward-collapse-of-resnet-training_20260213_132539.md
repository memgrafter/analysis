---
ver: rpa2
title: Progressive Feedforward Collapse of ResNet Training
arxiv_id: '2405.00985'
source_url: https://arxiv.org/abs/2405.00985
tags:
- features
- collapse
- resnet
- data
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the geometry of intermediate layers in
  ResNet and proposes the progressive feedforward collapse (PFC) conjecture, which
  states that the degree of collapse increases during forward propagation. The authors
  derive a transparent model for well-trained ResNet based on the geodesic curve assumption,
  showing that ResNet with weight decay approximates the geodesic curve in Wasserenstein
  space at the terminal phase.
---

# Progressive Feedforward Collapse of ResNet Training

## Quick Facts
- arXiv ID: 2405.00985
- Source URL: https://arxiv.org/abs/2405.00985
- Reference count: 40
- Primary result: The degree of collapse increases during forward propagation in ResNet, with intermediate layers following a geodesic curve in Wasserstein space

## Executive Summary
This paper investigates the geometry of intermediate layers in ResNet training and proposes the progressive feedforward collapse (PFC) conjecture, which states that the degree of collapse increases during forward propagation. The authors derive a transparent model for well-trained ResNet based on the geodesic curve assumption, showing that ResNet with weight decay approximates the geodesic curve in Wasserstein space at the terminal phase. They empirically demonstrate that PFC metrics monotonically decrease across depth on various datasets and propose a new surrogate model, multilayer unconstrained feature model (MUFM), to better understand both neural collapse and PFC.

## Method Summary
The paper proposes a framework to analyze progressive feedforward collapse in ResNet training by extending neural collapse analysis to intermediate layers. The method involves training ResNet architectures on various image datasets, computing PFC metrics at each layer, and comparing results with predictions under the geodesic curve assumption. A new surrogate model, MUFM, is introduced to connect intermediate layers to the final collapse geometry using optimal transport regularization. The framework is validated through extensive experiments on multiple datasets including MNIST, Fashion MNIST, CIFAR10, STL10, and CIFAR100.

## Key Results
- PFC metrics monotonically decrease across depth on various datasets under the geodesic curve assumption
- ResNet with weight decay approximates the geodesic curve in Wasserstein space at the terminal phase
- MUFM solutions are inconsistent with neural collapse but more concentrated relative to input data
- Progressive feedforward collapse metrics (PFC1, PFC2, PFC3) show consistent patterns across different architectures and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ResNet with weight decay approximates the geodesic curve in Wasserstein space at the terminal phase of training
- Mechanism: The geodesic curve assumption states that forward propagation in ResNet follows a straight line in feature space, induced by the optimal transport map between input data distribution and target class mean distribution
- Core assumption: The Benamou-Brenier formula establishes that minimizing the squared Wasserstein distance is equivalent to minimizing the path energy, which is upper bounded by weight decay in ResNet
- Evidence anchors:
  - [abstract] "ResNet with weight decay approximates the geodesic curve in Wasserstein space at the terminal phase"
  - [section II-C] "Gai and Zhang [26] proved that the weight decay is an upper bound of the above energy in DNNs"
  - [corpus] Weak evidence - no direct citations to geodesic curve assumption in neighbor papers

### Mechanism 2
- Claim: Progressive feedforward collapse metrics monotonically decrease across depth under the geodesic curve assumption
- Mechanism: When features move along a straight line between input data and final simplex ETF, the variability collapse metric decreases because within-class variance contracts linearly while between-class variance expands, and the simplex ETF distance decreases because the line path optimally reduces distance to the target geometry
- Core assumption: The optimal transport cost between input and final features is sufficiently small, ensuring the geodesic path provides monotonic improvement
- Evidence anchors:
  - [abstract] "the metrics of PFC indeed monotonically decrease across depth on various datasets"
  - [section III-B] "under the geodesic curve assumption, PFC metrics monotonically decrease across depth at the terminal phase"
  - [corpus] Weak evidence - neighbor papers discuss neural collapse but not monotonic progression across layers

### Mechanism 3
- Claim: The multilayer unconstrained feature model (MUFM) connects intermediate layers to the final collapse geometry while maintaining alignment with input data
- Mechanism: MUFM uses optimal transport regularization between consecutive layers, which serves as a lower bound for weight decay and forces intermediate features to lie along the geodesic path while the final layer is optimized under MSE loss
- Core assumption: The optimal transport regularizer can effectively replace weight decay while preserving the geometric progression from data to simplex ETF
- Evidence anchors:
  - [abstract] "MUFM... connecting intermediate layers by an optimal transport regularizer"
  - [section III-C] "it is reasonable to replace the weight decay on former layers with the optimal transport regularizer"
  - [corpus] No direct evidence - neighbor papers do not discuss MUFM specifically

## Foundational Learning

- Concept: Wasserstein space and optimal transport
  - Why needed here: The paper's theoretical framework relies on understanding how distributions of features evolve through layers and how minimizing transport cost induces geometric collapse
  - Quick check question: What is the Benamou-Brenier formula and how does it relate to minimizing Wasserstein distance?

- Concept: Neural collapse geometry
  - Why needed here: Understanding the final layer phenomenon of features collapsing to class means and forming simplex ETF is essential for interpreting how intermediate layers progress toward this geometry
- Quick check question: What are the four properties of neural collapse (NC1-NC4) and how do they define the final layer geometry?

- Concept: Geodesic curves in metric spaces
  - Why needed here: The paper's key assumption that ResNet follows a geodesic curve requires understanding what geodesic curves are and why they represent optimal paths between distributions
  - Quick check question: In what sense is a geodesic curve the "shortest path" between two distributions in Wasserstein space?

## Architecture Onboarding

- Component map:
  Input data distribution → ResNet blocks → Intermediate features → Final layer features → Classifier
  Weight decay regularization applied across all layers
  Optimal transport regularizer connecting consecutive layers in MUFM
  Loss function (cross-entropy or MSE) applied at final layer

- Critical path:
  1. Initialize ResNet with random weights
  2. Forward propagate through all layers to compute features
  3. Compute PFC metrics at each layer
  4. Backpropagate gradients through all layers
  5. Update weights using SGD with momentum
  6. Repeat until convergence

- Design tradeoffs:
  - Using weight decay vs optimal transport regularizer: Weight decay is simpler but less interpretable geometrically; optimal transport provides theoretical guarantees but adds computational complexity
  - Network depth vs PFC progression: Deeper networks show more pronounced collapse but may reach effective depth where further collapse is minimal
  - Regularization strength: Too little prevents collapse; too much causes premature convergence

- Failure signatures:
  - PFC metrics not monotonic across depth: Indicates violation of geodesic curve assumption or poor transport cost properties
  - Effective depth reached too early: Suggests network architecture too deep or regularization too strong
  - MUFM solutions not aligned with data: Indicates optimal transport regularizer coefficient too large

- First 3 experiments:
  1. Train a simple 3-layer ResNet on MNIST and plot PFC metrics across layers to verify monotonic decrease
  2. Vary the optimal transport regularizer coefficient in MUFM and observe trade-off between data alignment and simplex ETF proximity
  3. Remove weight decay and observe impact on PFC progression and final layer collapse geometry

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does the geodesic curve assumption break down for ResNet, and how would this affect the PFC metrics?
- Basis in paper: The authors assume ResNet with weight decay approximates the geodesic curve in Wasserstein space at the terminal phase, but this is an assumption rather than a proven fact
- Why unresolved: The authors provide empirical evidence for PFC metrics decreasing monotonically under this assumption, but do not prove the assumption holds for all cases or explore when it might fail
- What evidence would resolve it: Experiments varying weight decay, network depth, or dataset characteristics to identify when the assumption breaks down, along with theoretical analysis of the continuity equation's validity

### Open Question 2
- Question: How does the choice of loss function (cross-entropy vs. MSE) impact the convergence behavior and the degree of collapse in the MUFM model?
- Basis in paper: The authors use MSE loss in their MUFM experiments but mention cross-entropy in the model formulation, and they note that different loss functions could affect the results
- Why unresolved: The paper only provides empirical results for MSE loss in MUFM, leaving the impact of cross-entropy on the trade-off between data and simplex ETF unexplored
- What evidence would resolve it: Numerical experiments solving MUFM with both loss functions and comparing the collapse metrics and alignment with data across different regularization strengths

### Open Question 3
- Question: Can the PFC conjecture be extended to other deep learning architectures beyond ResNet, such as Transformers or Vision Transformers?
- Basis in paper: The authors focus on ResNet and derive their results based on the geodesic curve assumption specific to ResNet's residual connections, but do not explore other architectures
- Why unresolved: While the authors mention this as future work, they do not provide theoretical or empirical evidence for whether PFC holds in architectures without residual connections or with different forward propagation dynamics
- What evidence would resolve it: Experiments measuring PFC metrics across different architectures, along with theoretical analysis of how their forward propagation dynamics relate to optimal transport or geodesic curves

## Limitations

- The geodesic curve assumption lacks direct empirical validation and relies primarily on theoretical connections between weight decay and Wasserstein distance
- The effectiveness of the MUFM model depends critically on tuning the optimal transport regularizer coefficient, which may not be straightforward in practice
- The analysis is primarily focused on ResNet architectures and may not generalize to other deep learning models without further investigation

## Confidence

- High confidence: The empirical observation that PFC metrics monotonically decrease across depth is well-supported by experiments on multiple datasets
- Medium confidence: The theoretical framework connecting weight decay to Wasserstein geodesic approximation is plausible but relies on assumptions that need more rigorous testing
- Medium confidence: The MUFM model's effectiveness in bridging data distribution and simplex ETF geometry shows promise but requires optimization of the regularizer coefficient λ

## Next Checks

1. **Verify geodesic curve assumption**: Train ResNet with varying weight decay strengths and measure the actual Wasserstein distance between consecutive feature distributions to test if weight decay truly bounds the transport cost
2. **Test MUFM optimization**: Systematically sweep the optimal transport regularizer coefficient λ across multiple orders of magnitude and evaluate how well solutions balance alignment with input data versus proximity to simplex ETF
3. **Generalize across architectures**: Apply the PFC analysis framework to different network architectures (e.g., DenseNet, Vision Transformers) to determine if the progressive collapse phenomenon is architecture-specific or more general