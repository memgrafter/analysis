---
ver: rpa2
title: On the Robustness of Kernel Goodness-of-Fit Tests
arxiv_id: '2408.05854'
source_url: https://arxiv.org/abs/2408.05854
tags:
- test
- robust
- kernel
- where
- tests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of robustness in existing kernel goodness-of-fit
  tests by proposing the first robust kernel goodness-of-fit test that controls Type-I
  error against mild perturbations of the model. The key idea is to use kernel Stein
  discrepancy balls as the uncertainty set, which naturally accommodates common perturbation
  models like Huber contamination and density-band models.
---

# On the Robustness of Kernel Goodness-of-Fit Tests

## Quick Facts
- arXiv ID: 2408.05854
- Source URL: https://arxiv.org/abs/2408.05854
- Reference count: 40
- Primary result: First robust kernel goodness-of-fit test that controls Type-I error against mild perturbations using KSD balls as uncertainty sets.

## Executive Summary
This paper addresses a critical gap in kernel goodness-of-fit (GOF) testing by developing the first robust kernel GOF test that maintains Type-I error control under model perturbations. The authors show that existing kernel Stein discrepancy (KSD) tests with stationary kernels are fundamentally non-robust due to unbounded Stein kernels, while tilted kernels provide only qualitative robustness. Their proposed solution uses KSD balls as uncertainty sets, which naturally accommodates common perturbation models like Huber contamination and density-band models. The robust KSD test achieves both qualitative and quantitative robustness, maintaining calibration while retaining non-trivial power through appropriate thresholding strategies.

## Method Summary
The authors propose a robust KSD test that uses kernel Stein discrepancy balls as uncertainty sets to control Type-I error under mild model perturbations. The method involves tilting stationary kernels with appropriate weighting functions to bound the Stein kernel, preventing unbounded growth that causes lack of robustness. The test statistic is defined as ∆θ(Xn) = max(0, D(Xn) - θ), where θ is chosen to contain the uncertainty set of interest. A bootstrap procedure provides valid decision thresholds under the composite null hypothesis. The approach encompasses various contamination models including Huber contamination and density-band models, and is validated through extensive numerical experiments on synthetic and real datasets.

## Key Results
- Standard KSD tests with stationary kernels are not robust due to unbounded Stein kernels
- Tilted-KSD tests achieve qualitative but not quantitative robustness
- Proposed robust KSD test controls Type-I error under composite null hypotheses while maintaining non-trivial power
- Numerical experiments confirm effectiveness on contaminated Gaussian models, RBMs, KEF models, and galaxy velocity data

## Why This Works (Mechanism)

### Mechanism 1
- Tilted kernels with appropriate weighting functions bound the Stein kernel, preventing unbounded growth that causes lack of robustness.
- The weighting function w(x) is chosen such that w(x) · ||sp(x)||² is bounded, counteracting the growth of the score function. This transforms the kernel into k(x,x') = w(x)h(x-x')w(x'), where h is a stationary kernel.
- Core assumption: The score function sp(x) grows at most root-exponentially, and w(x) decays sufficiently slowly to counteract this growth without losing test power.

### Mechanism 2
- KSD balls as uncertainty sets provide a natural way to control Type-I error against mild perturbations while maintaining non-trivial power.
- The robust KSD test uses a test statistic ∆θ(Xn) = max(0, D(Xn) - θ), where θ is chosen such that the KSD ball BKSD(P;θ) contains the uncertainty set of interest (e.g., Huber contamination models).
- Core assumption: The radius θ can be chosen to control the desired type of contamination, and the bootstrap procedure provides a valid decision threshold.

### Mechanism 3
- The bootstrap procedure used in the robust KSD test provides a valid decision threshold even under the composite null hypothesis.
- The bootstrap procedure approximates the distribution of the P-KSD between the empirical measure and the population measure, which is related to the robust KSD test statistic through Hoeffding's decomposition.
- Core assumption: The Stein kernel is bounded, ensuring the P-KSD is well-defined and satisfies the conditions for the bootstrap approximation to be valid.

## Foundational Learning

- **Kernel Stein Discrepancy (KSD)**: The core discrepancy measure used in the robust KSD test. Understanding its properties, computation, and relationship to the score function is essential for understanding the test's behavior.
  - Quick check: What is the relationship between the KSD and the score function of the model? How is the KSD computed using the V-statistic estimator?

- **Tilted Kernels**: Used to bound the Stein kernel and ensure robustness. Understanding how they are constructed and their properties is crucial for implementing the robust KSD test.
  - Quick check: How is a tilted kernel constructed from a stationary kernel and a weighting function? What conditions must the weighting function satisfy to ensure the Stein kernel is bounded?

- **Robustness in Hypothesis Testing**: The paper addresses the lack of robustness in existing KSD tests and proposes a robust test. Understanding the concepts of qualitative and quantitative robustness, and how they are formalized using uncertainty sets, is essential for understanding the problem being solved.
  - Quick check: What is the difference between qualitative and quantitative robustness in hypothesis testing? How are uncertainty sets used to formalize robustness?

## Architecture Onboarding

- **Component map**: Input data samples → Model reference P with score function → Kernel (stationary or tilted) → Test statistic ∆θ(Xn) → Bootstrap threshold qB,1-α(Xn) → Decision (reject/fail to reject)

- **Critical path**:
  1. Compute the score function of the model P
  2. Choose an appropriate kernel (stationary or tilted)
  3. Compute the V-statistic estimate of the KSD D(Xn)
  4. Choose the uncertainty radius θ based on the desired robustness
  5. Compute the robust KSD test statistic ∆θ(Xn)
  6. Compute the bootstrap quantile qB,1-α(Xn)
  7. Compare ∆θ(Xn) to qB,1-α(Xn) and make a decision

- **Design tradeoffs**:
  - Kernel choice: Stationary kernels are simpler but not robust; tilted kernels are robust but require careful choice of weighting function
  - Uncertainty radius θ: Larger θ provides more robustness but may reduce power; smaller θ may not control the desired type of contamination
  - Bootstrap method: Weighted bootstrap is used in this work; wild bootstrap may be more flexible but requires different theoretical justification

- **Failure signatures**:
  - If the score function grows too fast or the weighting function decays too quickly, the Stein kernel may still be unbounded, causing the test to fail
  - If θ is not chosen appropriately, the test may not be well-calibrated against the desired type of contamination
  - If the bootstrap procedure fails to provide a valid decision threshold, the test may not control the Type-I error correctly

- **First 3 experiments**:
  1. Verify that the tilted kernel bounds the Stein kernel by computing up(x,x) for different choices of w(x) and comparing to the stationary kernel case
  2. Test the robust KSD test on a contaminated Gaussian model with known contamination ratio and compare the rejection rate to the nominal level α
  3. Compare the power of the robust KSD test to the standard KSD test on a model deviation that is not a contamination (e.g., mean shift)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the robust KSD test maintain calibration when using unbounded Stein kernels for models with bounded score functions (e.g., t-distributions or super-Laplacian distributions)?
- **Basis in paper**: [inferred] The paper discusses that existing KSD tests with stationary kernels are not robust due to unbounded Stein kernels, but tilted kernels can provide qualitative robustness. However, the robustness to bounded score functions with unbounded kernels is not explicitly explored.
- **Why unresolved**: The paper primarily focuses on sub-Gaussian models with unbounded score functions and does not address the scenario where the score function is bounded but the Stein kernel is unbounded.
- **What evidence would resolve it**: Empirical or theoretical results demonstrating the performance of the robust KSD test with unbounded kernels on models with bounded score functions.

### Open Question 2
- **Question**: How does the choice of the weighting function w(x) in the tilted kernel affect the trade-off between robustness and test power in high-dimensional settings?
- **Basis in paper**: [explicit] The paper mentions that the choice of w(x) is crucial for balancing robustness and power, but it primarily focuses on low-dimensional examples and suggests b = 1/2 for sub-Gaussian models.
- **Why unresolved**: The paper does not provide a systematic study of the impact of different weighting functions on test performance in high-dimensional settings, where the curse of dimensionality may further complicate the trade-off.
- **What evidence would resolve it**: A comprehensive analysis of the performance of robust KSD tests with various weighting functions across different dimensions and model complexities.

### Open Question 3
- **Question**: Can the robust KSD test be extended to handle non-i.i.d. data, such as time series or spatial data, while maintaining its robustness properties?
- **Basis in paper**: [inferred] The paper mentions that extension to non-independent cases could potentially be made following the approach in Chérief-Abdellatif & Alquier (2022), but it does not provide a detailed discussion or results.
- **Why unresolved**: The paper focuses on i.i.d. data and does not explore the challenges and potential solutions for adapting the robust KSD test to non-i.i.d. data structures.
- **What evidence would resolve it**: Theoretical and empirical results demonstrating the robustness of the KSD test when applied to non-i.i.d. data, such as time series or spatial data.

## Limitations
- Theoretical framework requires score functions to grow at most root-exponentially, limiting applicability to certain models
- Robust test's power depends heavily on appropriate choice of uncertainty radius θ, with no universal selection method provided
- General applicability to complex high-dimensional models remains to be thoroughly tested beyond synthetic examples

## Confidence
- **High Confidence**: The proof that standard KSD tests with stationary kernels are not robust due to unbounded Stein kernels, and that tilted kernels with bounded Stein kernels achieve qualitative robustness
- **Medium Confidence**: The claim that tilted-KSD tests achieve qualitative but not quantitative robustness, as this depends on the specific weighting function and contamination model
- **Medium Confidence**: The proposed robust KSD test's ability to control Type-I error across various contamination models, as this is primarily validated through simulations rather than theoretical bounds

## Next Checks
1. Test the robust KSD test on a broader range of contamination models, including those with unbounded score functions, to verify the root-exponential growth assumption
2. Investigate the impact of the uncertainty radius θ on test power and develop a data-driven method for its selection in different contamination scenarios
3. Apply the robust KSD test to a high-dimensional real-world dataset with known model misspecification to assess its practical utility beyond synthetic examples