---
ver: rpa2
title: How Do Transformers "Do" Physics? Investigating the Simple Harmonic Oscillator
arxiv_id: '2405.17209'
source_url: https://arxiv.org/abs/2405.17209
tags:
- linear
- methods
- harmonic
- oscillator
- intermediates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformers model the simple harmonic
  oscillator (SHO), a fundamental physical system. The authors develop criteria for
  identifying when transformers use specific numerical methods by analyzing the encoding
  of intermediate quantities in the hidden states.
---

# How Do Transformers "Do" Physics? Investigating the Simple Harmonic Oscillator

## Quick Facts
- **arXiv ID:** 2405.17209
- **Source URL:** https://arxiv.org/abs/2405.17209
- **Reference count:** 40
- **Key outcome:** Transformers use the matrix exponential method to model simple harmonic oscillator trajectories, with stronger evidence for undamped cases.

## Executive Summary
This paper investigates how transformers model the simple harmonic oscillator (SHO), a fundamental physical system. The authors develop criteria for identifying when transformers use specific numerical methods by analyzing the encoding of intermediate quantities in the hidden states. Applying these criteria to linear regression and SHO tasks, they find strong evidence that transformers use the matrix exponential method to model SHO trajectories. For the undamped SHO, the matrix exponential intermediate explains the most variance in hidden states and allows successful interventions, providing causal evidence for its use. While results are less clear for the damped SHO, the framework developed can extend to other linear and nonlinear systems. This work takes a step towards understanding the "world models" that transformers build by analyzing the computational methods they use to represent physical systems.

## Method Summary
The authors train transformers of varying sizes (L=1-5 layers, H=2-32 hidden size) on synthetic data for linear regression and SHO tasks. They develop a framework to identify numerical methods by analyzing how intermediates are encoded in hidden states using linear and nonlinear probes, as well as reverse probing for causal interventions. For linear regression, they examine encoding of the slope parameter w. For SHO, they test whether transformers use linear multistep methods, Taylor expansion, or matrix exponential by analyzing the encoding of corresponding intermediates. They apply four criteria: (1) linear encoding of intermediates, (2) R2 of Taylor probes, (3) reverse probe success, and (4) intervention effectiveness. The methodology includes training on 5000 sequences of 65 timesteps for undamped SHO and 5000 sequences of 32 timesteps for damped cases, using the Adam optimizer with epochs=20000, lr=10^-3, batchsize=64.

## Key Results
- Transformers show strong encoding of the matrix exponential intermediate for undamped SHO, explaining the most variance in hidden states
- Reverse probing interventions successfully manipulate model behavior when matrix exponential intermediates are well-encoded
- Linear regression models show correlation between performance and intermediate encoding quality, supporting the lottery ticket hypothesis
- Results for damped SHO are less conclusive, with intermediates explaining less variance and suggesting possible use of alternative methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers encode intermediates using linear or nonlinear representations depending on model size.
- Mechanism: Larger models have more "lottery tickets" in their increased capacity to find a "winning" representation of intermediate quantities, leading to stronger encoding.
- Core assumption: Model capacity correlates with the ability to represent intermediate quantities needed for computation.
- Evidence anchors:
  - [abstract] "We find that these models generalize to out-of-distribution test data...but we focus on investigating intermediates on in-distribution training data."
  - [section 2.3] "We attribute the stronger encoding of w in larger models to the 'lottery ticket hypothesis' - larger models have more 'lottery tickets' in their increased capacity to find a 'winning' representation of w"
  - [corpus] "Weak correlation between neighbor FMR and paper relevance"
- Break condition: If the lottery ticket hypothesis doesn't hold or if smaller models can represent intermediates through alternative mechanisms.

### Mechanism 2
- Claim: The quality of intermediate encoding correlates with model performance, enabling in-context learning.
- Mechanism: When intermediates are better represented, the model can more effectively use them in computations, leading to improved predictions and generalization.
- Core assumption: Better intermediate representation directly enables more effective use in downstream computations.
- Evidence anchors:
  - [abstract] "We also link the performance of models to their encoding of w and use it as an explanation for in-context learning."
  - [section 2.3] "We find that better performing models generally have stronger encodings of w"
  - [section 2.4] "The ability of the best performing models to in-context learn is highly correlated with their encoding of w"
- Break condition: If performance gains come from other sources unrelated to intermediate encoding quality.

### Mechanism 3
- Claim: Interventions on hidden states using reverse probes provide strong causal evidence for method usage.
- Mechanism: By replacing hidden states with those predicted from intermediates and observing predictable changes in output, we can demonstrate that the model uses these intermediates in computation.
- Core assumption: If a model uses an intermediate in computation, we can predictably manipulate its behavior by changing the intermediate representation.
- Evidence anchors:
  - [abstract] "Can we intervene on hidden states to produce predictable outcomes?"
  - [section 2.4] "For models where we identified a quadratic representation of w, we see that w = 0.5, −0.5 are both represented in the observed intervention."
  - [section 3.3] "We attempt to make w′ = 0.5 for all series, and then measure the observed ˆw from the models' outputs"
- Break condition: If interventions fail to produce predictable outcomes even when intermediates are well-encoded.

## Foundational Learning

- Concept: Linear regression as a testbed for understanding intermediates
  - Why needed here: Linear regression provides a simpler setting to develop and validate criteria for detecting method usage through intermediate analysis before applying to more complex systems.
  - Quick check question: Why is linear regression with y = wx a relevant testbed for studying SHO modeling?

- Concept: Numerical methods for solving differential equations
  - Why needed here: Understanding methods like matrix exponential, Taylor expansion, and linear multistep methods is essential for hypothesizing what computational approaches transformers might use.
  - Quick check question: What are the key differences between matrix exponential and Taylor expansion methods for solving linear ODEs?

- Concept: Probing techniques (linear, nonlinear, reverse probing)
  - Why needed here: Different probing methods are required to detect various ways intermediates might be encoded in hidden states, and reverse probing enables causal interventions.
  - Quick check question: How does reverse probing differ from standard probing, and why is it important for establishing causality?

## Architecture Onboarding

- Component map: Data generation -> Model training -> Probing analysis -> Intervention experiments -> Method identification
- Critical path: Train transformers on synthetic SHO data → Apply probing analysis to detect intermediate encoding → Use reverse probing for causal interventions → Identify numerical methods based on encoding patterns and intervention success
- Design tradeoffs: Simpler models (fewer layers, smaller hidden size) are more interpretable but may not encode intermediates well; complex models encode better but are harder to analyze.
- Failure signatures: Poor intermediate encoding despite strong performance suggests alternative computational methods; successful interventions that don't match predictions indicate incomplete understanding of the method.
- First 3 experiments:
  1. Train a small transformer (L=1, H=2) on linear regression and test if w is encoded using linear and Taylor probes.
  2. Train a larger transformer (L=3, H=8) on SHO and use reverse probes to see if matrix exponential intermediates explain hidden state variance.
  3. Perform intervention experiments on an SHO model by replacing hidden states with those predicted from matrix exponential intermediates and observe output changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do transformers use the same numerical methods to model other types of oscillatory systems beyond the simple harmonic oscillator?
- Basis in paper: [explicit] The authors state that their framework can extend to high-dimensional linear systems and nonlinear systems.
- Why unresolved: The paper only demonstrates results for the simple harmonic oscillator (SHO), both undamped and damped cases. While the authors claim their framework can extend to other systems, they do not provide evidence or experiments to support this claim.
- What evidence would resolve it: Experiments applying the same methodology (analyzing encoding of intermediate quantities in hidden states) to other oscillatory systems like the driven harmonic oscillator or coupled oscillators would provide evidence for or against the generalizability of the framework.

### Open Question 2
- Question: Why do transformers show stronger evidence of using the matrix exponential method for the undamped SHO compared to the damped SHO?
- Basis in paper: [explicit] The authors note that their intermediate analysis performs much more poorly on the underdamped case than the undamped case.
- Why unresolved: The paper provides several hypotheses (natural decay requiring less "understanding", more data to encode) but does not definitively explain the discrepancy.
- What evidence would resolve it: Further experiments varying damping factors, analyzing the evolution of encodings with context length, and comparing against synthetic data could help determine which hypothesis is correct or if another explanation is needed.

### Open Question 3
- Question: Are there other numerical methods beyond the linear multistep, Taylor expansion, and matrix exponential methods that transformers use to model SHO trajectories?
- Basis in paper: [inferred] The authors note that the intermediates explain so little of the hidden states even when combined for the damped case, leading them to hypothesize that transformers may be using a novel numerical method or another known method outside of their proposed hypothesis space.
- Why unresolved: The paper only considers three specific numerical methods from the literature. Transformers could be using a different numerical method or a combination of methods not considered in this study.
- What evidence would resolve it: Analyzing the residuals between predicted and actual SHO trajectories, or using techniques like symbolic regression on the transformer's hidden states, could potentially uncover alternative numerical methods being used.

## Limitations
- Method identification confidence varies by system complexity, with damped SHO results being less conclusive than undamped cases
- Probing methodology may miss alternative computational strategies transformers employ beyond the proposed hypothesis space
- Data generation specifics (parameter distributions) aren't fully specified, potentially affecting reproducibility and generalizability

## Confidence
- **High confidence**: Framework for identifying numerical methods through intermediate analysis is methodologically sound; causal evidence from intervention experiments on undamped SHO is compelling
- **Medium confidence**: Identification of matrix exponential for undamped SHO is well-supported but could benefit from additional validation; correlation between model capacity and intermediate encoding follows established patterns
- **Low confidence**: Conclusions about damped SHO modeling are tentative due to weaker evidence and potential confounding factors; framework extension to nonlinear systems remains speculative

## Next Checks
1. **Cross-architecture validation**: Test whether smaller transformer variants (LSTMs, MLPs) trained on identical SHO tasks show similar intermediate encoding patterns, helping determine whether observed phenomena are architecture-specific or more general properties of sequence models learning physical dynamics.

2. **Parameter space expansion**: Generate SHO training data with systematically varied parameter ranges (particularly for damped cases) to test whether method identification robustness depends on the breadth of training distribution, and whether transformers switch methods when operating outside their training distribution.

3. **Nonlinear system testbed**: Apply the framework to a simple nonlinear oscillator (e.g., Duffing oscillator) to empirically test whether the probing and intervention methodology generalizes beyond linear systems, and whether transformers employ fundamentally different computational strategies for nonlinear dynamics.