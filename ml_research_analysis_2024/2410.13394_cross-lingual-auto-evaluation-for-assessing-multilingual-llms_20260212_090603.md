---
ver: rpa2
title: Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs
arxiv_id: '2410.13394'
source_url: https://arxiv.org/abs/2410.13394
tags:
- score
- response
- evaluation
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Cross Lingual Auto Evaluation (CIA) Suite
  for assessing multilingual LLMs. It addresses the challenge of evaluating non-English
  machine-generated text by proposing a reference-based, cross-lingual evaluation
  framework.
---

# Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs

## Quick Facts
- arXiv ID: 2410.13394
- Source URL: https://arxiv.org/abs/2410.13394
- Authors: Sumanth Doddapaneni; Mohammed Safi Ur Rahman Khan; Dilip Venkatesh; Raj Dabre; Anoop Kunchukuttan; Mitesh M. Khapra
- Reference count: 40
- The CIA Suite introduces reference-based cross-lingual evaluation for multilingual LLMs, using English references to evaluate responses in six target languages.

## Executive Summary
This paper addresses the critical challenge of evaluating non-English machine-generated text by introducing the Cross Lingual Auto Evaluation (CIA) Suite. The framework uses English reference answers to evaluate responses in six target languages (Bengali, German, French, Hindi, Telugu, and Urdu) through a novel reference-based approach. A human-annotated test set (RECON) with 500 prompts and corresponding human judgment scores was developed to benchmark the evaluation framework. The proposed HERCULE model, fine-tuned on the INTEL dataset, demonstrates strong alignment with human judgments across all languages, outperforming both zero-shot approaches and large proprietary models.

## Method Summary
The CIA Suite introduces a reference-based evaluation framework that bridges the gap between English reference answers and non-English responses. The core approach involves translating English reference answers into target languages and using them to evaluate machine-generated responses. The framework employs a two-stage evaluation process: first, the HERCULE model fine-tuned on the INTEL dataset provides reference-based scoring, and second, cross-lingual semantic alignment ensures evaluation quality. The evaluation uses Cohen's Kappa scores to measure agreement with human judgments, with scores above 0.70 indicating strong alignment. The framework demonstrates effectiveness in both supervised and zero-shot settings across multiple language families.

## Key Results
- HERCULE model achieves Cohen's Kappa scores above 0.70 for most languages, demonstrating strong alignment with human judgments
- Outperforms zero-shot approaches and large proprietary models (GPT-4, Claude) on the RECON test set
- Shows effectiveness in zero-shot evaluation on unseen languages with 50 prompts per language

## Why This Works (Mechanism)
The CIA Suite works by establishing a reference-based evaluation framework that leverages English reference answers to evaluate non-English responses. The key mechanism involves fine-tuning the HERCULE model on the INTEL dataset, which contains aligned English and target language pairs. This fine-tuning enables the model to understand cross-lingual semantic equivalence and provide accurate evaluation scores. The framework's effectiveness stems from its ability to capture semantic meaning across languages rather than relying on direct translation, allowing for more nuanced evaluation of machine-generated text in multiple languages.

## Foundational Learning
1. **Cross-lingual semantic alignment** - Understanding how meaning transfers across languages is crucial for accurate evaluation. Quick check: Verify that English references maintain equivalent meaning when translated to target languages.
2. **Reference-based evaluation** - Using reference answers as ground truth for scoring responses requires careful consideration of semantic equivalence. Quick check: Ensure reference answers cover diverse response patterns and language-specific nuances.
3. **Cohen's Kappa for inter-rater reliability** - This statistical measure quantifies agreement between automated and human evaluations. Quick check: Calculate Kappa scores across multiple language pairs to ensure consistent evaluation quality.
4. **Zero-shot evaluation capability** - The ability to evaluate languages without specific training data is essential for scalability. Quick check: Test zero-shot performance on languages from different families than training languages.
5. **Fine-tuning strategies for multilingual models** - Proper fine-tuning on aligned datasets enables cross-lingual understanding. Quick check: Monitor training loss and validation performance across all target languages.

## Architecture Onboarding

**Component Map:** HERCULE model (fine-tuned on INTEL) -> Cross-lingual alignment module -> Scoring engine -> Cohen's Kappa validation

**Critical Path:** Input prompt/response pair → HERCULE evaluation → Cross-lingual semantic matching → Final score generation

**Design Tradeoffs:** The framework prioritizes semantic accuracy over literal translation fidelity, which improves evaluation quality but requires more complex fine-tuning. The use of English references simplifies data collection but introduces potential bias in cross-lingual evaluation.

**Failure Signatures:** Poor performance on languages with limited training data, degradation in evaluation quality for longer responses, and potential semantic misalignment in language pairs with significant structural differences.

**3 First Experiments:** 1) Evaluate the framework on a new language family (e.g., Slavic languages) to test generalizability. 2) Compare performance between literal translation-based and semantic alignment-based evaluation approaches. 3) Test the framework's ability to handle code-mixed responses that combine multiple languages.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the limitations section implies several areas requiring further investigation, including expanding language coverage, improving zero-shot performance, and validating the framework on longer, more complex responses.

## Limitations
- The evaluation framework relies on English reference answers for non-English responses, introducing uncertainty about semantic equivalence across languages
- Limited to six target languages from three language families, constraining generalizability claims
- Zero-shot performance on unseen languages is based on small sample sizes (50 prompts per language), potentially not capturing full evaluation complexity

## Confidence
- Framework effectiveness across languages: Medium confidence - Strong human alignment results demonstrated, but limited to six languages with potential sampling bias
- HERCULE model superiority: Medium confidence - Outperforms baselines on RECON test set, but evaluation constrained to single annotated dataset
- Zero-shot capability: Low confidence - Preliminary results on unseen languages show promise but lack statistical robustness due to small sample sizes

## Next Checks
1. Expand language coverage to 10+ additional languages spanning diverse language families to test generalizability beyond initial six languages
2. Conduct detailed linguistic analysis to verify that English reference answers maintain semantic equivalence when used to evaluate non-English responses across different language pairs
3. Test the framework's effectiveness on longer, more complex responses (500+ words) to assess whether evaluation quality degrades with response length