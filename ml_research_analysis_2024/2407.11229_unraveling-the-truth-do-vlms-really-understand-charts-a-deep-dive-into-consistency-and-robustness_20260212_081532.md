---
ver: rpa2
title: 'Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency
  and Robustness'
arxiv_id: '2407.11229'
source_url: https://arxiv.org/abs/2407.11229
tags:
- chart
- charts
- question
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the performance and robustness of current visual
  language models (VLMs) in chart question answering (CQA). The authors evaluate state-of-the-art
  VLMs and multimodal large language models (MLLMs) on carefully curated datasets,
  assessing their ability to handle varying chart and question complexities and their
  robustness to visual perturbations.
---

# Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness

## Quick Facts
- **arXiv ID**: 2407.11229
- **Source URL**: https://arxiv.org/abs/2407.11229
- **Reference count**: 20
- **Key outcome**: This paper analyzes the performance and robustness of current visual language models (VLMs) in chart question answering (CQA), revealing significant performance variations based on question and chart types, with models struggling particularly with complex questions and perturbed charts.

## Executive Summary
This paper investigates the performance and robustness of state-of-the-art Visual Language Models (VLMs) and Multimodal Large Language Models (MLLMs) in chart question answering (CQA) tasks. Through extensive experiments on carefully curated datasets, the study evaluates models' abilities to handle varying chart and question complexities, as well as their robustness to visual perturbations. The findings reveal that while models demonstrate promising performance on standard chart datasets, they struggle significantly when faced with complex questions, non-annotated charts, and various visual perturbations. The study provides detailed insights into model strengths and weaknesses, identifying areas for improvement and proposing future research directions for building more reliable CQA systems.

## Method Summary
The researchers evaluated state-of-the-art VLMs and MLLMs on two datasets: ChartQA and RobustCQA. ChartQA contains charts, questions, and underlying tables, while RobustCQA includes perturbed charts generated using various perturbation types. The evaluation used zero-shot Chain-of-Thought prompting and a relaxed accuracy metric. Models were assessed across different chart and question complexities, with particular focus on performance variations when handling visual perturbations. The study categorized charts and questions by complexity and tested models' responses to perturbations such as annotations, color changes, and chart type variations.

## Key Results
- Model performance significantly degrades when handling non-annotated charts, dropping from 54% to 40% accuracy on average
- Certain perturbations consistently improve performance (like annotations), while others cause substantial accuracy drops
- Models show clear bias toward simple charts and questions, with complex combinations resulting in performance decreases of up to 20-25%
- Top-performing models still experience substantial accuracy drops when faced with visual variations, highlighting robustness issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model performance drops significantly when faced with non-annotated charts due to reliance on visual recognition rather than true understanding.
- Mechanism: Models trained on annotated charts learn to extract data points based on explicit labels and markers. When these annotations are removed, models struggle to accurately interpret the underlying data, relying instead on visual cues that may be ambiguous or misleading.
- Core assumption: Models primarily learn to map visual features to answers during training, rather than developing a deep understanding of chart semantics.
- Evidence anchors:
  - [abstract] "Although Multimodal Large Language Models (MLLMs) have demonstrated increasingly impressive performance in chart understanding, most of them exhibit alarming hallucinations and significant performance degradation when handling non-annotated charts."
  - [section] "Our results reveal a significant performance degradation for most models when confronted with perturbations. While performance generally decreases across all models, some exhibit more drastic drops."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.471, average citations=0.0." (Weak corpus evidence for this specific claim)

### Mechanism 2
- Claim: Different perturbation types have varying effects on model performance, with some consistently helping and others consistently harming accuracy.
- Mechanism: Certain visual modifications, such as adding annotations or improving font size, provide additional cues that aid model interpretation. Conversely, complex modifications like log scales or stacked charts introduce ambiguity that confuses the models.
- Core assumption: Models are sensitive to specific visual features and can leverage helpful cues while being misled by confusing ones.
- Evidence anchors:
  - [section] "Our experiments highlighted several perturbations that improved model performance. Across all models, annotated data points consistently boosted accuracy."
  - [section] "Our analysis reveals that while models demonstrate promising performance on standard chart datasets, they struggle with robustness when faced with visual perturbations."
  - [corpus] "Average neighbor FMR=0.471" (Moderate corpus evidence supporting the general sensitivity of models to perturbations)

### Mechanism 3
- Claim: Models exhibit a bias towards simple charts and questions, with performance significantly decreasing for complex combinations.
- Mechanism: Simple charts and questions require less complex reasoning and data extraction, tasks that current models are better equipped to handle. Complex charts and questions demand multi-step reasoning and integration of information from multiple sources, which poses a greater challenge.
- Core assumption: Models have limitations in their reasoning capabilities and struggle with tasks that require integrating information from multiple sources or performing complex calculations.
- Evidence anchors:
  - [abstract] "Our analysis reveals significant performance variations based on question and chart types, highlighting both strengths and weaknesses of current models."
  - [section] "For the same chart type, models consistently perform better on simple questions compared to complex questions."
  - [corpus] "Found 25 related papers (using 8)" (Limited corpus evidence specifically addressing the bias towards simple charts and questions)

## Foundational Learning

- Concept: Visual Language Understanding (VLU)
  - Why needed here: Understanding the field of VLU is crucial for grasping the context of chart question answering and the challenges involved in training models to interpret visual data.
  - Quick check question: What are the key components of a VLU system, and how do they contribute to tasks like chart question answering?

- Concept: Multimodal Learning
  - Why needed here: Chart question answering involves integrating information from both visual (charts) and textual (questions and answers) modalities, making multimodal learning a fundamental concept.
  - Quick check question: How do multimodal models process and fuse information from different modalities, and what are the challenges in training such models?

- Concept: Data Extraction and Reasoning
  - Why needed here: Effective chart question answering requires accurate data extraction from charts and the ability to reason about the extracted information to answer questions correctly.
  - Quick check question: What are the different approaches to data extraction from charts, and how do models perform reasoning tasks based on the extracted data?

## Architecture Onboarding

- Component map: Vision Encoder -> Text Encoder -> Fusion Module -> Reasoning Module -> Output Module
- Critical path:
  1. Vision Encoder processes the chart image.
  2. Text Encoder processes the question and answer text.
  3. Fusion Module combines the visual and textual features.
  4. Reasoning Module performs the necessary reasoning steps.
  5. Output Module generates the final answer.

- Design tradeoffs:
  - Model size vs. performance: Larger models generally perform better but require more computational resources.
  - Pre-training data vs. fine-tuning data: The quality and diversity of pre-training data can significantly impact model performance on downstream tasks.
  - Interpretability vs. accuracy: More interpretable models may sacrifice some accuracy, while highly accurate models may be less interpretable.

- Failure signatures:
  - Inability to extract data from non-annotated charts.
  - Struggles with complex charts and questions.
  - Sensitivity to visual perturbations.
  - Reliance on visual recognition rather than true understanding.

- First 3 experiments:
  1. Evaluate model performance on a dataset of annotated and non-annotated charts to assess the impact of annotations on accuracy.
  2. Test model robustness to different types of visual perturbations, such as color changes, chart type variations, and font size adjustments.
  3. Analyze model performance on simple versus complex charts and questions to identify strengths and weaknesses in handling different levels of complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do models perform on pie and doughnut charts, pyramid and funnel charts, as well as radar charts?
- Basis in paper: [inferred] The paper mentions these chart types as limitations, stating they were excluded from analysis due to metadata limitations and the complexity of adapting data for chart representation.
- Why unresolved: The study did not include these chart types in their analysis, leaving their performance on these specific chart types unexplored.
- What evidence would resolve it: Testing the models on datasets containing these chart types and comparing their performance with other chart types.

### Open Question 2
- Question: How does the performance of models vary across different languages beyond English?
- Basis in paper: [inferred] The paper acknowledges the limitation of only covering English data and mentions that models are developed and evaluated on a wide variety of languages.
- Why unresolved: The study only used English data, and the models' performance on charts in other languages remains unknown.
- What evidence would resolve it: Evaluating the models on chart datasets in multiple languages and analyzing their performance across different languages.

### Open Question 3
- Question: How do models perform when visual captions present in the original charts are included in the perturbed charts?
- Basis in paper: [inferred] The paper mentions that inconsistent metadata of the original dataset sometimes lacked visual captions present in the original charts, which could not be replicated in the perturbed charts.
- Why unresolved: The study could not identify attributes pertaining to chart elements due to the lack of visual captions, potentially limiting the models' understanding of the charts.
- What evidence would resolve it: Creating perturbed charts with visual captions and assessing the models' performance with and without captions.

## Limitations

- The study only evaluated models on English data, limiting understanding of multilingual performance
- Specific chart types (pie, doughnut, pyramid, funnel, radar) were excluded due to metadata limitations
- Inconsistent metadata in the original dataset sometimes lacked visual captions, potentially limiting model understanding

## Confidence

- **High Confidence**: The observation that models perform significantly worse on complex questions and perturbed charts is well-supported by experimental evidence across multiple models and perturbation types.
- **Medium Confidence**: The claim that models rely more on visual recognition than true understanding is plausible but requires further investigation to definitively establish the underlying mechanisms.
- **Low Confidence**: The assertion that specific perturbation types consistently help or harm performance may be influenced by the particular models and datasets used, and may not generalize to all VLMs.

## Next Checks

1. **Dataset Diversity**: Evaluate model performance on a broader range of chart datasets, including those with different chart types, styles, and levels of complexity, to assess the generalizability of the findings.
2. **Perturbation Robustness**: Conduct a more extensive analysis of model robustness to visual perturbations, including a wider variety of perturbation types and magnitudes, to better understand the factors that affect model performance.
3. **Interpretability Analysis**: Investigate the internal representations and decision-making processes of VLMs when handling chart question answering tasks, using techniques such as attention visualization and feature attribution, to gain insights into their strengths and weaknesses.