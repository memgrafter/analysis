---
ver: rpa2
title: Explainable Metric Learning for Deflating Data Bias
arxiv_id: '2407.04866'
source_url: https://arxiv.org/abs/2407.04866
tags:
- learning
- metric
- image
- segments
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new explainable metric learning framework,
  Hierarchical Explainable Metric Learning (HEML), to provide human-understandable
  similarity measurements between images. The core idea is to break down images into
  semantic segments using a SegFormer, train individual deep metric learning models
  for each segment, and progressively combine segments in a tree structure while refining
  similarity metrics at each level.
---

# Explainable Metric Learning for Deflating Data Bias

## Quick Facts
- arXiv ID: 2407.04866
- Source URL: https://arxiv.org/abs/2407.04866
- Authors: Emma Andrews; Prabhat Mishra
- Reference count: 31
- Primary result: Reduces memory requirements from 18-20 GB to 2-3 GB while achieving comparable accuracy

## Executive Summary
This paper introduces Hierarchical Explainable Metric Learning (HEML), a novel framework for creating human-understandable similarity metrics between images. The approach breaks images into semantic segments using SegFormer, trains individual metric learning models for each segment, and progressively combines them in a hierarchical tree structure. HEML addresses the challenge of data bias in image classification by providing interpretable similarity measurements that can be used to generate new samples for bias reduction. The framework achieves state-of-the-art accuracy while dramatically reducing memory requirements, making it practical for real-world deployment.

## Method Summary
HEML operates through a bottom-up training approach where semantic segments are first created using SegFormer segmentation. Individual deep metric learning models are trained on each segment, then progressively combined by averaging weights from constituent parts. At each hierarchical level, models are trained to learn similarity metrics specific to combined segments. For inference, a metric tree is constructed using SNR distance to visualize similarity breakdowns at each level. The framework supports multiple loss functions including Triplet Margin, Angular, Multiple Similarity, NTXent, SNR, Proxy Anchor, and Sub-Center ArcFace. This progressive refinement enables both accurate similarity measurement and human-interpretable explanations of why images are similar or different.

## Key Results
- Achieves Precision@1 scores ranging from 66.8% to 90.0% across three datasets
- Reduces memory requirements from 18-20 GB to 2-3 GB compared to A VSL
- Maintains comparable accuracy to state-of-the-art methods while providing explainable similarity metrics
- Successfully generates new samples to reduce bias in training datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Breaking images into semantic segments and training separate models for each enables human-understandable similarity comparisons.
- Mechanism: SegFormer creates pixel-level semantic regions that correspond to human-perceived features (hair, eyes, nose). Individual metric learning models are trained on these segments, preserving human interpretability. When segments are combined hierarchically, the similarity scores at each level can be explained by examining the contribution of individual segments.
- Core assumption: Semantic segments created by SegFormer align with human perceptual features and are meaningful for similarity measurement.
- Evidence anchors:
  - [abstract] "The basic idea is to break down the images into their semantic segments, train a metric learning model for each segment, and progressively combine the segments to reconstruct the original image."
  - [section] "Our approach enables a more human-understandable similarity measurement between two images based on the semantic segments within it"
- Break condition: If SegFormer segments don't align with human perception, or if segment boundaries are too fine-grained to be meaningful, the explainability benefit disappears.

### Mechanism 2
- Claim: The bottom-up training approach where combined segment models average weights from their constituent parts improves overall accuracy.
- Mechanism: Lower-level models trained on individual segments capture fine-grained feature similarities. When segments combine, averaging their weights provides a good initialization for the next level, allowing the model to learn how these features interact. This progressive refinement creates a hierarchical similarity metric.
- Core assumption: Averaging weights from constituent segment models provides a reasonable starting point for combined segment training.
- Evidence anchors:
  - [section] "If the current segment is a combination of two prior segments, then the models are loaded with the weighted average of the individual segment weights that were previously trained."
  - [section] "Models trained at lower levels contribute to the training of higher levels, iteratively refining the weights in a bottom-up manner"
- Break condition: If weight averaging doesn't provide a good initialization, training may diverge or fail to converge properly at higher levels.

### Mechanism 3
- Claim: The metric tree construction with SNR distance provides interpretable similarity measurements at each hierarchical level.
- Mechanism: After training, the system constructs a tree where each node represents a segment or combined segment. SNR distance is computed between corresponding segments of two images, creating an interpretable similarity breakdown. Lower SNR distances indicate higher similarity for that segment.
- Core assumption: SNR distance is an appropriate metric for comparing embeddings and provides meaningful interpretability.
- Evidence anchors:
  - [section] "To aid in the explanation, we construct a visualization of the similarity between two images at each level within the semantic reconstruction."
  - [section] "We use the distance metric as our main explainability component. By default, tree composition uses Signal-to-Noise Ratio (SNR) as the primary distance metric"
- Break condition: If SNR doesn't correlate well with human perception of similarity, or if the distance values don't provide meaningful discrimination, the interpretability benefit is lost.

## Foundational Learning

- Concept: Semantic segmentation
  - Why needed here: SegFormer breaks images into human-perceivable regions, which is the foundation for explainable similarity measurement
  - Quick check question: What are the advantages of using SegFormer over other segmentation approaches for this application?

- Concept: Metric learning fundamentals
  - Why needed here: Understanding how embeddings are created and how similarity is measured is crucial for grasping HEML's approach
  - Quick check question: How does triplet margin loss differ from other metric learning loss functions?

- Concept: Hierarchical model training
  - Why needed here: The bottom-up training approach with weight averaging is key to HEML's methodology
  - Quick check question: What are the benefits and drawbacks of averaging weights from multiple models versus training from scratch?

## Architecture Onboarding

- Component map:
  SegFormer -> Trunk models -> Embedder models -> Triplet miner -> Metric tree

- Critical path:
  1. Segment images using SegFormer
  2. Train individual segment models bottom-up with weight averaging
  3. Construct metric tree for inference
  4. Query similarity between two images at each level

- Design tradeoffs:
  - Memory vs. Explainability: HEML uses 2-3GB vs 18-20GB for A VSL, trading some runtime overhead for significant memory savings
  - Granularity vs. Interpretability: More segments provide finer explainability but increase complexity
  - Training time vs. Accuracy: Bottom-up training is slower but achieves comparable accuracy

- Failure signatures:
  - Poor segmentation quality from SegFormer (check segmentation output)
  - Training instability at higher levels (monitor loss curves during combined segment training)
  - SNR distances not correlating with human perception (validate with user studies)

- First 3 experiments:
  1. Verify SegFormer segmentation quality on CelebA faces
  2. Train individual segment models and check if learned embeddings make sense
  3. Test combined segment training with weight averaging to ensure it converges properly

## Open Questions the Paper Calls Out
None

## Limitations
- The paper assumes SegFormer semantic segments perfectly align with human perceptual features without empirical validation through user studies
- Weight averaging as initialization for combined segments is a heuristic approach whose optimality is not demonstrated
- SNR distance as the interpretability metric is chosen without comparative analysis against other distance metrics

## Confidence

| Claim | Confidence |
|-------|------------|
| Memory efficiency (2-3GB vs 18-20GB) | High |
| Accuracy claims | Medium |
| Explainability claims | Low |

## Next Checks
1. Conduct human perception studies where participants rate image similarity and compare their judgments against HEML's segment-based similarity breakdown to validate the explainability claim
2. Perform ablation studies testing alternative initialization strategies for combined segment training (e.g., random initialization, transfer learning from pre-trained models) to evaluate the optimality of the weight averaging approach
3. Test HEML's bias reduction capabilities on datasets with known demographic biases (e.g., gender or racial bias in face recognition) and measure improvements using established bias metrics like Equalized Odds or Demographic Parity