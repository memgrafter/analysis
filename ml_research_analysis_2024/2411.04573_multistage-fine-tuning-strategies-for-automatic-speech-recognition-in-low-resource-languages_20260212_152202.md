---
ver: rpa2
title: Multistage Fine-tuning Strategies for Automatic Speech Recognition in Low-resource
  Languages
arxiv_id: '2411.04573'
source_url: https://arxiv.org/abs/2411.04573
tags:
- malasar
- fine-tuning
- languages
- speech
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multistage fine-tuning strategy to improve
  automatic speech recognition (ASR) performance for low-resource languages using
  OpenAI's Whisper model. The approach addresses the challenge of building ASR systems
  for languages with limited digital resources by sequentially adapting the model
  across linguistically similar languages.
---

# Multistage Fine-tuning Strategies for Automatic Speech Recognition in Low-resource Languages

## Quick Facts
- arXiv ID: 2411.04573
- Source URL: https://arxiv.org/abs/2411.04573
- Reference count: 24
- Primary result: Multistage fine-tuning strategy achieved 51.9% WER on Malasar ASR, a 4.5% absolute reduction from direct fine-tuning

## Executive Summary
This paper presents a multistage fine-tuning strategy to improve automatic speech recognition (ASR) performance for low-resource languages using OpenAI's Whisper model. The approach addresses the challenge of building ASR systems for languages with limited digital resources by sequentially adapting the model across linguistically similar languages. The method was tested on the Malasar language, a Dravidian language spoken by approximately ten thousand people in the Western Ghats of South India, which faces critical challenges for technological intervention due to its lack of a native script and absence of digital or spoken data resources.

The researchers created a spoken Malasar corpus paired with transcription in Tamil script, a closely related major language, and first built an intermediate Tamil ASR before fine-tuning on Malasar data. This multistage fine-tuning strategy achieved a word error rate (WER) of 51.9%, representing a 4.5% absolute reduction compared to direct fine-tuning on Malasar data alone. Further WER reduction to 47.3% was achieved through punctuation removal in post-processing. The results demonstrate the effectiveness of sequential multistage fine-tuning combined with targeted post-processing as a scalable strategy for ASR system development in low-resource languages, especially where linguistic similarities can be leveraged to bridge gaps in training data.

## Method Summary
The researchers developed a two-stage fine-tuning approach using OpenAI's Whisper model for the low-resource Malasar language. First, they fine-tuned the Whisper model on Tamil data to create an intermediate representation, then further fine-tuned this intermediate model on 4 hours 36 minutes of Malasar audio data. The Malasar corpus was transcribed in Tamil script and split into 70% training, 10% validation, and 20% test sets. Both Whisper Small and Whisper Medium models were evaluated, with the latter showing better performance. Punctuation removal was applied during evaluation to normalize formatting inconsistencies between reference and predicted transcriptions.

## Key Results
- Multistage fine-tuning achieved 51.9% WER on Malasar ASR, representing a 4.5% absolute reduction from direct fine-tuning
- Further WER reduction to 47.3% was achieved through punctuation removal in post-processing
- Whisper Medium model outperformed Whisper Small in the multistage fine-tuning approach
- The approach demonstrates effectiveness for low-resource languages where linguistic similarities can be leveraged

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multistage fine-tuning leverages transfer learning from a linguistically related language to improve ASR performance in low-resource settings.
- Mechanism: The model first adapts to a high-resource language with similar phonetic and lexical properties, creating an intermediate representation that is then fine-tuned on the target language data. This sequential adaptation reduces the learning burden compared to direct fine-tuning on scarce target data.
- Core assumption: Tamil and Malasar share sufficient linguistic similarities (lexical, phonetic) that pretraining on Tamil will transfer effectively to Malasar.
- Evidence anchors:
  - [abstract] "leveraging higher data availability for Tamil annotated speech. This intermediate model is subsequently fine-tuned on Malasar data, allowing for more effective ASR adaptation despite limited resources."
  - [section] "The multistage fine-tuning strategy demonstrated significant improvements over direct fine-tuning on Malasar data alone, achieving a word error rate (WER) of 51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning method."
- Break condition: If the linguistic similarity between Tamil and Malasar is overestimated, the transfer may introduce irrelevant patterns or noise, reducing rather than improving performance.

### Mechanism 2
- Claim: Punctuation removal as a post-processing normalization step significantly reduces WER by eliminating non-linguistic inconsistencies.
- Mechanism: Since the corpus consists of biblical text with varied punctuation, removing punctuation from both reference and predicted transcriptions before WER calculation eliminates formatting differences that would otherwise inflate error rates.
- Core assumption: Punctuation differences between reference and predictions are not meaningful linguistic errors and should not contribute to WER.
- Evidence anchors:
  - [abstract] "Further a WER reduction to 47.3% was achieved through punctuation removal in post-processing, which addresses formatting inconsistencies that impact evaluation."
  - [section] "In this work, we employed a novel multistage sequential fine-tuning approach using the Whisper model... to leverage linguistic similarities between Malasar and Tamil... This intermediate transfer learning configuration yielded notable improvements in recognition accuracy over the zeroshot and direct target fine-tuning approaches."
- Break condition: If punctuation carries important prosodic or grammatical information that affects speech recognition quality, removing it could mask genuine performance issues.

### Mechanism 3
- Claim: The multistage approach reduces the effective data requirement for the target language by first building a robust intermediate model.
- Mechanism: By first training on abundant Tamil data, the model develops general speech representation capabilities that can be efficiently adapted to Malasar with minimal target language data, effectively multiplying the utility of limited resources.
- Core assumption: The amount of Malasar data available is insufficient for direct training but sufficient for fine-tuning an already adapted model.
- Evidence anchors:
  - [abstract] "The multistage fine-tuning strategy demonstrated significant improvements over direct fine-tuning on Malasar data alone, achieving a word error rate (WER) of 51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning method."
  - [section] "This intermediate model is subsequently fine-tuned on Malasar data, allowing for more effective ASR adaptation despite limited resources."
- Break condition: If the target language data is too limited (less than needed for fine-tuning), the multistage approach may overfit to noise in the small dataset.

## Foundational Learning

- Concept: Transfer learning and its effectiveness based on linguistic similarity
  - Why needed here: Understanding why Tamil→Malasar transfer works requires knowledge of how neural models transfer representations across related languages
  - Quick check question: What linguistic features would need to be shared between Tamil and Malasar for effective transfer learning?

- Concept: Word Error Rate (WER) calculation and its sensitivity to normalization
  - Why needed here: The paper shows punctuation removal significantly affects WER, demonstrating the importance of consistent evaluation metrics
  - Quick check question: How does WER calculation differ when comparing raw vs. punctuation-normalized transcriptions?

- Concept: Self-supervised learning in speech representation (Wav2vec2, Whisper architecture)
  - Why needed here: Understanding the foundation of pretrained models helps explain why they can be effectively fine-tuned on low-resource languages
  - Quick check question: What distinguishes encoder-only models like Wav2vec2 from encoder-decoder models like Whisper in terms of ASR capabilities?

## Architecture Onboarding

- Component map: Whisper transformer-based encoder-decoder architecture → Intermediate Tamil fine-tuning stage → Malasar target fine-tuning stage → Punctuation normalization post-processing → WER evaluation
- Critical path: Data collection (Malasar + Tamil) → Model selection (Whisper) → Stage 1: Tamil fine-tuning → Stage 2: Malasar fine-tuning → Evaluation with punctuation normalization
- Design tradeoffs: Larger models (Whisper Medium) show better performance but require more computational resources; punctuation normalization improves WER but may mask certain types of errors
- Failure signatures: High WER with no improvement from multistage approach suggests linguistic similarity assumptions are invalid; inconsistent WER reductions across different normalization techniques indicate evaluation metric issues
- First 3 experiments:
  1. Run zeroshot evaluation on Malasar to establish baseline performance
  2. Implement direct fine-tuning on Malasar to measure improvement from task-specific adaptation
  3. Test punctuation normalization on reference and predicted transcriptions to quantify its impact on evaluation metrics

## Open Questions the Paper Calls Out
None

## Limitations
- The linguistic similarity between Tamil and Malasar is asserted but not empirically validated beyond both being Dravidian languages
- The corpus size of 4 hours 36 minutes, while substantial for an endangered language, may still be insufficient for robust model training
- Punctuation removal normalization may mask genuine performance issues related to prosody or grammatical structure
- The study focuses on a single low-resource language pair, limiting generalizability to other language families or writing systems

## Confidence
- **High confidence**: The methodology for multistage fine-tuning and punctuation normalization is sound and well-documented
- **Medium confidence**: The claim that Tamil-Malasar linguistic similarity enables effective transfer learning is reasonable but requires further validation
- **Medium confidence**: The WER improvements (4.5% absolute reduction from direct fine-tuning, additional 4.6% from punctuation normalization) are statistically significant but may be influenced by evaluation normalization choices

## Next Checks
1. Conduct a controlled experiment with a language pair of known but lower similarity to Tamil (e.g., Kannada or Telugu) to quantify the relationship between linguistic distance and transfer learning effectiveness
2. Implement cross-validation with different punctuation schemes (removing only certain punctuation marks vs. all) to determine which normalization approach best reflects actual recognition performance without masking genuine errors
3. Test the multistage approach on a synthetic low-resource scenario by artificially limiting data from a high-resource language and measuring how the two-stage process compares to direct fine-tuning as data scarcity increases