---
ver: rpa2
title: 'InFoBench: Evaluating Instruction Following Ability in Large Language Models'
arxiv_id: '2401.03601'
source_url: https://arxiv.org/abs/2401.03601
tags:
- gpt-4
- each
- instruction
- generated
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Decomposed Requirements Following Ratio
  (DRFR), a new metric for evaluating Large Language Models' (LLMs) ability to follow
  instructions. DRFR breaks down complex instructions into simpler criteria, enabling
  a detailed analysis of LLMs' compliance with various aspects of tasks.
---

# InFoBench: Evaluating Instruction Following Ability in Large Language Models

## Quick Facts
- arXiv ID: 2401.03601
- Source URL: https://arxiv.org/abs/2401.03601
- Reference count: 40
- Authors: Yiwei Qin; Kaiqiang Song; Yebowen Hu; Wenlin Yao; Sangwoo Cho; Xiaoyang Wang; Xuansheng Wu; Fei Liu; Pengfei Liu; Dong Yu
- Key outcome: Introduces DRFR metric and INFOBENCH benchmark for evaluating LLM instruction-following ability

## Executive Summary
This paper addresses the critical need for better evaluation methods for Large Language Models' (LLMs) instruction-following capabilities. The authors introduce the Decomposed Requirements Following Ratio (DRFR), a novel metric that breaks down complex instructions into simpler, evaluable criteria. They also present INFOBENCH, a comprehensive benchmark containing 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories. The study compares traditional scoring methods with DRFR and explores different annotation sources, demonstrating that GPT-4 can serve as a cost-effective and reliable automatic evaluator.

## Method Summary
The methodology centers on decomposing complex instructions into atomic binary questions that can be objectively evaluated. The DRFR metric calculates instruction-following performance by measuring compliance with each decomposed criterion. The INFOBENCH benchmark was manually curated with diverse instructions spanning various domains and constraint types. The evaluation process uses multi-turn prompts to maintain context when evaluating responses across multiple criteria. The study compares annotation sources including human experts, crowd-sourced workers, and GPT-4 to determine cost-effectiveness and reliability.

## Key Results
- DRFR shows higher reliability than traditional Direct Scoring methods for evaluating instruction-following ability
- GPT-4 demonstrates superior performance as an automatic evaluator, balancing accuracy with cost-effectiveness
- LLMs exhibit highest performance on Content and Style constraints, with lowest performance on Number and Linguistic constraints
- All evaluated models show significant room for improvement in complex instruction-following scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DRFR's decomposed approach increases annotator agreement by simplifying evaluation criteria
- **Mechanism:** Breaking complex instructions into atomic binary questions makes each criterion objectively evaluable, reducing ambiguity
- **Core assumption:** Individual sub-criteria can be answered with binary (YES/NO) without losing essential information
- **Evidence anchors:** Abstract states DRFR enables detailed analysis of compliance with various aspects of tasks; Section 3.1 explains granular evaluation of individual requirements
- **Break condition:** If sub-criteria become too granular and lose contextual meaning, or if binary questions cannot capture nuanced requirements

### Mechanism 2
- **Claim:** GPT-4 serves as an effective automatic evaluator due to its ability to process multi-turn prompts and maintain context
- **Mechanism:** Sequential, multi-turn prompt structure allows GPT-4 to evaluate each decomposed question while retaining context from previous answers
- **Core assumption:** GPT-4 can maintain consistent evaluation standards across multiple decomposed questions for the same instruction
- **Evidence anchors:** Section 3.2 describes the multi-turn prompt approach; Section 4 reveals significant gaps in LLM ability to follow instructions perfectly
- **Break condition:** If GPT-4's evaluation consistency degrades over longer instruction sets or context retention fails across multi-turn prompts

### Mechanism 3
- **Claim:** Constraint categorization enables targeted model improvement by identifying specific weakness patterns
- **Mechanism:** Labeling decomposed questions with constraint types reveals which categories models struggle with most, guiding focused development
- **Core assumption:** Different constraint types represent distinct capabilities that can be independently improved
- **Evidence anchors:** Section 4 shows performance patterns across constraint types; Section A.6 reveals lowest performance on Number and Linguistic constraints
- **Break condition:** If constraint categories overlap significantly or improvement in one category doesn't transfer to others

## Foundational Learning

- **Concept:** Instruction decomposition and atomic evaluation criteria
  - **Why needed here:** Understanding how complex instructions can be broken down into evaluable components is fundamental to grasping the DRFR methodology
  - **Quick check question:** Given the instruction "Write a 500-word essay on climate change impacts with three main arguments," what are three potential decomposed criteria?

- **Concept:** Multi-turn prompt engineering
  - **Why needed here:** Essential for understanding how GPT-4 is used as an automatic evaluator and how context is maintained across evaluation questions
  - **Quick check question:** Why might sequential multi-turn prompting be more effective than asking all decomposed questions simultaneously?

- **Concept:** Constraint-based evaluation frameworks
  - **Why needed here:** Critical for understanding how the evaluation system categorizes and analyzes model performance across different types of requirements
  - **Quick check question:** How does constraint categorization help identify specific areas for model improvement?

## Architecture Onboarding

- **Component map:** Instruction Bank -> Decomposition Engine -> Constraint Classifier -> Evaluation Layer (DRFR metric) -> Annotation Pipeline (human/GPT-4) -> Result Aggregator

- **Critical path:** 1. Instruction selection from bank 2. Generation of decomposed questions 3. Assignment of constraint labels 4. Model response generation 5. Evaluation using DRFR metric 6. Results aggregation and analysis

- **Design tradeoffs:** Granularity vs. completeness in decomposition, Cost vs. accuracy in annotation sources, Automation vs. human oversight in evaluation, Constraint specificity vs. evaluation efficiency

- **Failure signatures:** Low inter-annotator agreement (>0.4), High proportion of "UNKNOWN" responses in AMT annotations, Inconsistent evaluation results across different LLM evaluators, Pattern of failures in specific constraint categories

- **First 3 experiments:** 1. Compare DRFR scores with direct scoring on a small subset (50 instructions) to validate metric reliability 2. Test different prompt structures for GPT-4 evaluation to optimize accuracy 3. Analyze performance patterns across constraint categories to identify model weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we address the scalability limitations of the INFOBENCH dataset, given its manual creation process?
- Basis in paper: From the "Limitations" section stating "The size of our dataset presents another limitation" and "The manual nature of our instruction writing process limits our capacity to scale this dataset significantly"
- Why unresolved: The paper identifies the need for automated methods but does not provide a solution or prototype
- What evidence would resolve it: Development and demonstration of an automated system capable of generating diverse, high-quality instructions and decomposed questions at scale, with validation showing the generated content maintains the same quality as the current manual dataset

### Open Question 2
- Question: How can the evaluation protocol be extended to incorporate factors like truthfulness and harmlessness that are currently not considered?
- Basis in paper: From the "Limitations" section stating "Several crucial factors, such as truthfulness and harmlessness... were not considered in this study"
- Why unresolved: The paper acknowledges these factors as important but does not propose methods for integrating them into the current framework
- What evidence would resolve it: Proposal and implementation of a modified DRFR metric or additional evaluation criteria that specifically assess LLM outputs for truthfulness and harmlessness, along with experimental results demonstrating effectiveness

### Open Question 3
- Question: What are the underlying reasons for GPT-4's superior performance in annotation tasks compared to other models and human annotators?
- Basis in paper: From the "Experiment of Annotation Sources" section showing GPT-4's high accuracy and cost-effectiveness in annotation tasks
- Why unresolved: While the paper demonstrates GPT-4's effectiveness, it does not explore the reasons behind its superior performance or compare its reasoning process to humans
- What evidence would resolve it: Detailed analysis of GPT-4's annotation process, including comparison of its reasoning steps to human annotators, and investigation into whether GPT-4's training data or architecture contribute to its annotation capabilities

## Limitations

- The manual nature of instruction writing and question decomposition limits dataset scalability
- The evaluation protocol does not consider crucial factors like truthfulness and harmlessness
- The 500-instruction scope may not represent all possible instruction types and complexity levels

## Confidence

- **High confidence:** The decomposition methodology and constraint categorization framework are well-defined and consistently applied
- **Medium confidence:** The comparison between annotation sources (human vs. GPT-4) shows promising results but requires further validation across different domains
- **Medium confidence:** The identified performance patterns across constraint categories are supported by data but may vary with different instruction sets

## Next Checks

1. Conduct cross-domain validation by testing the benchmark with instructions from different fields to ensure generalizability
2. Perform ablation studies to determine the optimal granularity level for instruction decomposition
3. Compare DRFR results with task-completion outcomes in real-world applications to validate practical relevance