---
ver: rpa2
title: On Zero-Shot Counterspeech Generation by LLMs
arxiv_id: '2403.14938'
source_url: https://arxiv.org/abs/2403.14938
tags:
- counterspeech
- type
- generation
- speech
- gpt-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive analysis of Large Language
  Models (LLMs) in zero-shot settings for counterspeech generation, the first of its
  kind. The authors investigate the performance of four LLMs (GPT-2, DialoGPT, ChatGPT,
  and FlanT5) on four datasets, comparing their performance and analyzing the impact
  of model size and prompting strategies.
---

# On Zero-Shot Counterspeech Generation by LLMs

## Quick Facts
- arXiv ID: 2403.14938
- Source URL: https://arxiv.org/abs/2403.14938
- Authors: Punyajoy Saha; Aalok Agrawal; Abhik Jana; Chris Biemann; Animesh Mukherjee
- Reference count: 0
- Key outcome: ChatGPT outperforms other LLMs in zero-shot counterspeech generation with 120% improvement in counterspeech quality and 35% in argument quality, though with 35% lower readability

## Executive Summary
This paper presents the first comprehensive analysis of Large Language Models in zero-shot settings for counterspeech generation, evaluating four models (GPT-2, DialoGPT, ChatGPT, and FlanT5) across four datasets. The authors investigate how model size and prompting strategies affect performance, finding that ChatGPT consistently outperforms other models across multiple metrics including generational quality, counterspeech quality, and argument quality. They also propose and evaluate three prompting strategies (manual, frequency-based, and cluster-centered) to guide type-specific counterspeech generation, with manual prompts generally performing best except for affiliation-type counterspeech.

## Method Summary
The study evaluates four LLMs (GPT-2, DialoGPT, ChatGPT, and FlanT5) in zero-shot settings using four hate speech datasets (CONAN, CONAN-MT, Reddit, Gab). The authors propose three prompting strategies—manual, frequency-based, and cluster-centered—to guide type-specific counterspeech generation. Models generate responses using top-k and top-p sampling with specified parameters. Generated outputs are evaluated using multiple metrics including GLEU, METEOR, BLEURT for generation quality, counterspeech quality, argument quality, counterargument quality, toxicity, readability (Flesch Reading Ease), and type-specific classification precision.

## Key Results
- ChatGPT achieves 120% improvement in counterspeech quality and 35% improvement in argument quality compared to other models
- Larger model sizes show 17% improvement in generation quality but 25% increase in toxicity
- Manual prompting strategies generally outperform frequency-based and cluster-centered approaches for most counterspeech types
- Cluster-centered prompts perform best for affiliation-type counterspeech for GPT-2 and DialoGPT, while frequency-based prompts work best for the same type with FlanT5 and ChatGPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot counterspeech generation by LLMs can be improved through targeted prompting strategies that guide the model to produce type-specific counterspeech.
- Mechanism: Prompts act as explicit instructions that constrain the model's output to match desired counterspeech types (e.g., facts, humor, denouncing). Different prompting strategies provide varying levels of guidance.
- Core assumption: The model's pretraining data contains sufficient knowledge about counterspeech types to generate them when given appropriate prompts, even without fine-tuning.
- Evidence anchors: [abstract] "we propose three different prompting strategies... and analyse the impact of such strategies on the performance of the models"; [section] "We propose three prompting strategies, namely manual, frequency based, and cluster centered... These prompts help us in controlling the type of the counterspeech generated"
- Break condition: If the model's pretraining data lacks exposure to counterspeech patterns, prompting strategies will fail to produce coherent type-specific responses.

### Mechanism 2
- Claim: Larger model sizes generally improve counterspeech generation quality but increase toxicity risks.
- Mechanism: Increased model capacity allows better language understanding and generation capabilities, enabling more sophisticated counterspeech. However, larger models may also amplify biases or harmful patterns present in training data.
- Core assumption: Model performance scales predictably with parameter count for this task, and toxicity increases are manageable.
- Evidence anchors: [abstract] "our analysis shows that there is an improvement in generation quality for two datasets (17%), however the toxicity increase (25%) with increase in model size"; [section] "With the increase of the size of the models (both for DialogGPT and GPT-2), there is an increase in toxicity in responses by 44%, 25%, and 30% for CONAN-MT, Reddit, and Gab respectively"
- Break condition: If toxicity increases disproportionately to quality gains, larger models become counterproductive.

### Mechanism 3
- Claim: ChatGPT demonstrates emergent capabilities in counterspeech generation that smaller models lack.
- Mechanism: The RLHF fine-tuning process in ChatGPT creates superior alignment with human preferences for counterspeech tasks, enabling better performance across multiple metrics.
- Core assumption: The RLHF process successfully transfers to counterspeech generation despite being trained primarily for general conversation.
- Evidence anchors: [abstract] "ChatGPT are much better at generating counter speech than other models across all metrics"; [section] "ChatGPT outperforms all other models... in terms of generational metrics... counterspeech quality and argument quality improves by 120% and 35% respectively"
- Break condition: If ChatGPT's advantages are task-specific rather than general capabilities, performance may degrade on specialized counterspeech requirements.

## Foundational Learning

- Concept: Zero-shot learning in language models
  - Why needed here: The paper evaluates LLMs without fine-tuning, requiring understanding of how models can perform tasks they weren't explicitly trained for
  - Quick check question: What distinguishes zero-shot from few-shot prompting, and why might zero-shot be preferred for counterspeech generation?

- Concept: Prompt engineering strategies
  - Why needed here: Three different prompting approaches are proposed and evaluated, requiring understanding of how prompt design affects model output
  - Quick check question: How do manual, frequency-based, and cluster-centered prompts differ in their approach to guiding model generation?

- Concept: Counterspeech typology
  - Why needed here: The evaluation involves classifying generated text into specific counterspeech types, requiring knowledge of what constitutes each type
  - Quick check question: What are the six counterspeech types used in the evaluation, and how might they differ in their persuasive approaches?

## Architecture Onboarding

- Component map: Hate speech text → Prompt strategy selector → LLM (GPT-2/DialoGPT/FlanT5/ChatGPT) → Generated counterspeech → Evaluation pipeline (multiple metrics)
- Critical path: Hate speech + prompt → LLM generation → Type classification → Quality evaluation
- Design tradeoffs: Model size vs toxicity, prompt complexity vs effectiveness, evaluation comprehensiveness vs computational cost
- Failure signatures: High toxicity scores, poor type classification accuracy, low generation metric scores, readability degradation
- First 3 experiments:
  1. Compare zero-shot performance of all four models on a small subset of hate speech without any prompts to establish baseline capabilities
  2. Test manual prompting strategy across all models for one counterspeech type to validate prompt effectiveness
  3. Evaluate toxicity scaling across model sizes using the same prompts to quantify the size-toxicity tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompting strategies impact the quality and diversity of generated counterspeech across different types of hate speech?
- Basis in paper: [explicit] The paper proposes three prompting strategies (manual, frequency-based, and cluster-centered) and evaluates their impact on categorical counterspeech generation.
- Why unresolved: While the paper provides insights into the effectiveness of each strategy for specific types of counterspeech, a comprehensive analysis across various hate speech categories and datasets is needed to draw definitive conclusions.
- What evidence would resolve it: Conducting a large-scale study with diverse hate speech datasets and evaluating the generated counterspeech using a combination of automatic and human evaluation metrics for each prompting strategy.

### Open Question 2
- Question: Can the proposed prompting strategies be adapted to generate counterspeech that addresses implicit biases and microaggressions?
- Basis in paper: [inferred] The paper focuses on countering explicit hate speech, but implicit biases and microaggressions are also prevalent in online discourse and require effective counterspeech strategies.
- Why unresolved: The current prompting strategies may not be well-suited for addressing the subtle and nuanced nature of implicit biases and microaggressions, which require a deeper understanding of context and intent.
- What evidence would resolve it: Developing and evaluating prompting strategies specifically designed to identify and counter implicit biases and microaggressions, and testing their effectiveness in generating appropriate counterspeech.

### Open Question 3
- Question: How can the effectiveness of generated counterspeech be measured beyond traditional evaluation metrics like BLEU and ROUGE?
- Basis in paper: [explicit] The paper mentions the limitations of using BLEU and ROUGE for evaluating counterspeech quality and suggests the need for alternative evaluation methods.
- Why unresolved: Existing evaluation metrics may not capture the nuances of counterspeech, such as its ability to change attitudes, promote empathy, and foster constructive dialogue.
- What evidence would resolve it: Developing and validating new evaluation metrics that assess the impact of counterspeech on the audience, including measures of attitude change, empathy, and engagement.

### Open Question 4
- Question: How can the proposed methods be scaled to handle the vast amount of hate speech generated on social media platforms in real-time?
- Basis in paper: [inferred] The paper focuses on generating counterspeech for individual instances of hate speech, but the scale of the problem requires efficient and scalable solutions.
- Why unresolved: The current methods may not be computationally efficient enough to handle the real-time processing of large volumes of hate speech data on social media platforms.
- What evidence would resolve it: Developing and evaluating methods that can efficiently process and generate counterspeech for large-scale hate speech data, potentially using techniques like parallel processing, distributed computing, and model optimization.

## Limitations

- The findings may not generalize beyond the four specific datasets used (CONAN, CONAN-MT, Reddit, Gab) to other hate speech contexts or cultural settings
- The study relies heavily on automated metrics and LLM-as-a-judge approaches, which may not fully capture real-world effectiveness of counterspeech
- The temporal advantage of ChatGPT's RLHF fine-tuning may be temporary as other models receive similar training or as the model updates over time

## Confidence

**High confidence** (supported by direct evidence and reasonable mechanism):
- ChatGPT outperforms other models across all metrics in the tested scenarios
- Larger model sizes improve generation quality but increase toxicity
- Manual prompting strategies generally outperform frequency-based and cluster-centered approaches for most counterspeech types

**Medium confidence** (supported by evidence but with mechanistic gaps):
- The three prompting strategies effectively guide type-specific counterspeech generation
- The observed tradeoffs between generation quality and toxicity are consistent across datasets
- Zero-shot settings can produce viable counterspeech without fine-tuning

**Low confidence** (extrapolations or limited evidence):
- The specific percentage improvements (120% counterspeech quality, 35% argument quality) are robust across different evaluation contexts
- The cluster-centered prompting strategy's superiority for affiliation type counterspeech extends beyond the tested models
- The readability degradation (-35%) represents a fundamental limitation rather than a parameter tuning opportunity

## Next Checks

**Check 1: Cross-dataset validation** - Test the three best-performing model-prompt combinations (ChatGPT with manual prompts, GPT-2 with cluster-centered prompts for affiliation, and FlanT5 with frequency-based prompts for affiliation) on an entirely new hate speech dataset from a different platform or cultural context. Compare performance drops to assess generalizability limits.

**Check 2: Human evaluation validation** - Conduct a small-scale human evaluation (n=50 responses) comparing automated metrics against human judgments of counterspeech effectiveness, particularly for the 120% quality improvement claim. Focus on whether automated toxicity detection aligns with human perceptions of harmful content.

**Check 3: Temporal robustness check** - Replicate the core findings using the most recent versions of the models (GPT-4, updated ChatGPT, newer DialoGPT variants) to determine if the observed performance patterns and tradeoffs remain stable as models evolve. This validates whether the RLHF advantage and size-toxicity relationship are persistent phenomena.