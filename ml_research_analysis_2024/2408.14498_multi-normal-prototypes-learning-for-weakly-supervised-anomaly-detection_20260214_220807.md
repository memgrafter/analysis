---
ver: rpa2
title: Multi-Normal Prototypes Learning for Weakly Supervised Anomaly Detection
arxiv_id: '2408.14498'
source_url: https://arxiv.org/abs/2408.14498
tags:
- anomaly
- detection
- normal
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses weakly supervised anomaly detection, where
  limited labeled anomalies and abundant unlabeled data are available. The key insight
  is that normal data often consists of multiple subgroups, making single-prototype
  approaches insufficient.
---

# Multi-Normal Prototypes Learning for Weakly Supervised Anomaly Detection

## Quick Facts
- arXiv ID: 2408.14498
- Source URL: https://arxiv.org/abs/2408.14498
- Authors: Zhijin Dong, Hongzhi Liu, Boyuan Ren, Weimin Xiong, Zhonghai Wu
- Reference count: 10
- Primary result: Achieves 0.642 average AUC-PR, significantly outperforming state-of-the-art baselines (0.489-0.271) on 15 diverse datasets

## Executive Summary
This paper addresses weakly supervised anomaly detection where limited labeled anomalies and abundant unlabeled data are available. The key insight is that normal data often consists of multiple subgroups, making single-prototype approaches insufficient. The authors propose a reconstruction-based multi-normal prototypes learning framework that learns multiple prototypes to represent normal data subgroups using deep embedding clustering and contrastive learning. A novel weighting scheme estimates the likelihood of unlabeled samples being normal during training, improving robustness to anomaly contamination.

## Method Summary
The method learns multiple prototypes to represent different normal data subgroups through a reconstruction-based framework. It uses an encoder-decoder structure with deep embedding clustering and contrastive learning to capture the multi-modal nature of normal data. A weighting scheme assigns higher importance to samples likely to be normal during training, mitigating contamination from anomalies in unlabeled data. The unified anomaly scoring module integrates reconstruction error, latent representation, and prototype similarity through a learned MLP to produce final anomaly scores.

## Key Results
- Achieves average AUC-PR of 0.642 compared to 0.489-0.271 for baselines across 15 diverse datasets
- Outperforms state-of-the-art methods including AVID, CSI, DAGMM, and Deep SAD
- Demonstrates strong generalization to unseen anomalies with minimal performance degradation as labeled anomaly ratios decrease

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-prototype learning captures the diversity of normal data better than single-prototype approaches
- Mechanism: Normal data is modeled as multi-modal distribution with multiple prototypes representing different subgroups, learned through deep embedding clustering and contrastive learning
- Core assumption: Normal data consists of multiple categories or subgroups rather than clustering around a single prototype
- Evidence anchors:
  - [abstract] "Most of the existing methods assume the normal sample data clusters around a single central prototype while the real data may consist of multiple categories or subgroups."
  - [section] "We assume the normal sample data may consist of multiple categories or subgroups, and propose to learn multi-normal prototypes to represent them with deep embedding clustering and contrastive learning."
  - [corpus] Weak evidence - related papers focus on single-prototype or generic anomaly detection but don't specifically address multi-modal normal distributions
- Break condition: If normal data actually clusters around a single prototype or if the number of true subgroups is unknown and poorly estimated

### Mechanism 2
- Claim: Weighting scheme for unlabeled samples reduces contamination from anomalies
- Mechanism: Samples are weighted by their similarity to learned normal prototypes using a sigmoid function, giving higher weights to samples likely to be normal
- Core assumption: Unlabeled data inevitably contains some anomalies that need to be downweighted during training
- Evidence anchors:
  - [abstract] "we propose a method to estimate the likelihood of each unlabeled sample being normal during model training, which can help to learn more efficient data encoder and normal prototypes for anomaly detection."
  - [section] "we estimate the likelihood of each unlabeled sample being normal based on the multi-normal prototypes, guiding the training process to mitigate the impact of contaminated anomalies in the unlabeled data."
  - [corpus] Weak evidence - corpus papers mention contamination but don't detail specific weighting mechanisms
- Break condition: If anomaly contamination is too severe (e.g., >50% of unlabeled data) or if the similarity metric fails to distinguish normal from anomalous samples

### Mechanism 3
- Claim: Unified anomaly scoring integrates reconstruction error and prototype similarity
- Mechanism: Anomaly score combines reconstruction error, latent representation, and maximum prototype similarity through a learned MLP with sigmoid output
- Core assumption: Both reconstruction error and prototype similarity provide complementary information for anomaly detection
- Evidence anchors:
  - [section] "To detect anomalies with consideration of both the reconstruction error ei = l(xi, ˆxi) and the multi-normal prototype information, we design a unified anomaly scoring module."
  - [section] "The concatenated vector forms the input to the unified anomaly score evaluator φ to get an anomaly score"
  - [corpus] No direct evidence in corpus - this appears to be a novel integration approach
- Break condition: If either reconstruction error or prototype similarity becomes uninformative (e.g., due to poor encoder training or inadequate prototype learning)

## Foundational Learning

- Concept: Deep embedding clustering
  - Why needed here: To initialize and refine multiple prototypes that represent different normal subgroups
  - Quick check question: What clustering loss function is used to align prototypes with normal data clusters?

- Concept: Contrastive learning
  - Why needed here: To push normal samples close to at least one prototype while pushing anomalies away from all prototypes
  - Quick check question: How does the contrastive loss ensure separation between normal and anomalous samples?

- Concept: Weakly supervised learning with contaminated data
  - Why needed here: To handle the practical constraint of limited labeled anomalies and unlabeled data containing unknown anomalies
  - Quick check question: What mechanism is used to mitigate the impact of unlabeled anomalies during training?

## Architecture Onboarding

- Component map: Encoder-Decoder (reconstruction) → Deep Embedding Clustering → Contrastive Learning → Prototype Weighting → Unified Scoring
- Critical path: Data → Encoder → Prototype similarity → Weight calculation → Reconstruction loss → Clustering loss → Contrastive loss → Anomaly score
- Design tradeoffs: Multi-prototype vs single-prototype (better representation but higher complexity), weighting vs no weighting (robustness vs simplicity)
- Failure signatures: Poor prototype initialization, inadequate separation between normal and anomalous samples, reconstruction errors not distinguishing anomalies
- First 3 experiments:
  1. Verify prototype initialization by clustering normal data and checking prototype diversity
  2. Test weighting scheme effectiveness by measuring weight distribution for known normal vs anomalous samples
  3. Validate anomaly scoring by comparing scores for synthetically generated normal vs anomalous samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's performance scale with increasingly larger numbers of normal prototypes beyond what was tested?
- Basis in paper: [inferred] The sensitivity analysis (Section 4.6) only tested up to 10 prototypes, showing performance fluctuations. The paper states "too many or too few prototypes can significantly degrade performance" but doesn't explore the upper bounds.
- Why unresolved: The paper only tested prototype numbers from 1-10, leaving uncertainty about optimal scaling for datasets with more complex normal distributions.
- What evidence would resolve it: Experiments testing 20+ prototypes on diverse datasets, particularly those with known multi-modal normal distributions, would clarify scalability limits.

### Open Question 2
- Question: Would replacing the current reconstruction architecture with more advanced variants like Variational Autoencoders (VAEs) provide significant performance gains?
- Basis in paper: [explicit] The conclusion section mentions this as future work: "For the reconstruction learning component, we could also consider using more advanced variants like Variational AutoEncoders (VAEs) to enhance feature extraction capabilities."
- Why unresolved: The authors acknowledge this possibility but haven't tested it, leaving uncertainty about the potential performance improvement from more sophisticated reconstruction models.
- What evidence would resolve it: Direct comparison of the proposed method with a VAE-based reconstruction module on the same benchmark datasets would quantify the performance difference.

### Open Question 3
- Question: How would the method perform when extended from tabular data to other data types like images or time series?
- Basis in paper: [explicit] The conclusion states "Future work could explore extending our method to other types of data, such as images, to further validate its versatility and effectiveness."
- Why unresolved: The method was only validated on tabular datasets, so its effectiveness on other data modalities remains unknown despite the authors' suggestion that it could work.
- What evidence would resolve it: Implementing the method for image or time series data (e.g., using CNN encoders/decoders for images or RNN-based architectures for time series) and testing on appropriate benchmarks would demonstrate cross-domain applicability.

## Limitations

- The effectiveness of the weighting scheme may break down in high-dimensional spaces where distance metrics become less meaningful
- Performance claims depend heavily on specific datasets and experimental setup without comprehensive sensitivity analysis across different contamination levels
- The method was only validated on tabular datasets, leaving uncertainty about effectiveness on other data types like images or time series

## Confidence

**High Confidence:** The core insight that normal data consists of multiple subgroups rather than clustering around a single prototype is well-supported by empirical results across all tested datasets.

**Medium Confidence:** The significant improvement over baselines (0.642 vs 0.489-0.271 AUC-PR) may depend on specific experimental conditions and hasn't been tested across varying contamination levels.

**Low Confidence:** The weighting scheme's effectiveness relies on similarity metrics that may fail in high-dimensional spaces where distance measures become less meaningful.

## Next Checks

1. **Prototype Sensitivity Analysis:** Systematically vary the number of normal prototypes k and measure the impact on AUC-PR to determine the robustness of the method to prototype initialization and selection.

2. **Contamination Robustness Test:** Evaluate performance across different levels of anomaly contamination in unlabeled data (e.g., 5%, 10%, 25%, 50%) to validate the effectiveness of the weighting scheme under varying contamination scenarios.

3. **Cross-Dataset Transferability:** Test the method's generalization by training on one dataset and evaluating on another to assess whether learned prototypes transfer effectively across different domains and data distributions.