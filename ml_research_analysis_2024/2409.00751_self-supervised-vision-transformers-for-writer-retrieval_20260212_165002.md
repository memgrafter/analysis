---
ver: rpa2
title: Self-Supervised Vision Transformers for Writer Retrieval
arxiv_id: '2409.00751'
source_url: https://arxiv.org/abs/2409.00751
tags:
- writer
- vlad
- retrieval
- tokens
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel self-supervised Vision Transformer
  (ViT) approach for writer retrieval, achieving state-of-the-art results on historical
  and modern handwriting datasets. The method extracts local features from ViT patch
  tokens containing handwriting, aggregates them using VLAD encoding, and employs
  cosine distance for retrieval with optional reranking.
---

# Self-Supervised Vision Transformers for Writer Retrieval

## Quick Facts
- arXiv ID: 2409.00751
- Source URL: https://arxiv.org/abs/2409.00751
- Authors: Tim Raven; Arthur Matei; Gernot A. Fink
- Reference count: 40
- Primary result: Self-supervised ViT achieves state-of-the-art mAP of 83.1% on Historical-WI, 95.0% on HisIR19, and 98.6% on CVL without fine-tuning

## Executive Summary
This paper introduces a novel self-supervised Vision Transformer (ViT) approach for writer retrieval that achieves state-of-the-art performance on both historical and modern handwriting datasets. The method extracts local features from ViT patch tokens containing handwriting, aggregates them using VLAD encoding, and employs cosine distance for retrieval with optional reranking. Trained without labels, the ViT outperforms CNN-based and handcrafted features, demonstrating robustness across diverse datasets.

## Method Summary
The method employs a ViT-small/16 model trained via AttMask self-supervised learning on binarized handwriting images. During inference, the model processes document images in 224x224 windows with stride 224, extracting patch tokens from regions containing sufficient handwriting (foreground tokens). These tokens are aggregated using VLAD encoding with a codebook of 100 clusters, followed by power normalization, L2 normalization, and PCA dimensionality reduction to 384 dimensions. Retrieval is performed using cosine distance, with optional reranking via kRNN, Graph-based, or SGR methods.

## Key Results
- Achieves 83.1% mAP on Historical-WI, 95.0% mAP on HisIR19, and 98.6% mAP on CVL
- Outperforms CNN-based methods (ResNet-18, ConvNet) and handcrafted features on all datasets
- Foreground token extraction with VLAD aggregation provides +8.7 mAP improvement over class token with sum-pooling on Historical-WI
- Attention-guided masking during training improves robustness compared to random masking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised Vision Transformer extracts more discriminative local handwriting features than class token alone.
- Mechanism: By masking the most attended patches during training (AttMask), the model learns to reconstruct important regions, forcing deeper understanding of local structure. During inference, foreground patch tokens capture these learned local details, while class tokens average over all patches and lose locality.
- Core assumption: Handwriting style is better encoded by local structural cues than global image averages.
- Evidence anchors:
  - [abstract] "We show that extracting local foreground features is superior to using the ViT's class token in the context of writer retrieval."
  - [section] "We find that for aggregating local features using VLAD, retaining the local information of the patch tokens is crucial... many ViT patches may only contain background information, contributing little to the analysis of handwriting characteristics."
- Break condition: If handwriting contains very uniform, low-detail strokes, class token aggregation might suffice and locality filtering may lose useful signal.

### Mechanism 2
- Claim: VLAD aggregation of filtered foreground tokens outperforms sum-pooling and yields state-of-the-art retrieval.
- Mechanism: VLAD encodes residuals between each token and its nearest centroid, preserving fine-grained local differences across many tokens. Sum-pooling collapses these differences into a single average, losing discriminative detail. Filtering to foreground tokens reduces background noise before VLAD.
- Core assumption: Writer retrieval benefits from preserving residual differences rather than just mean pooling.
- Evidence anchors:
  - [abstract] "We show that encoding these features with a Vector of Locally Aggregated Descriptors (VLAD) [...] enhances performance notably compared to using the class token with either sum-pooling or VLAD."
  - [section] "A likely explanation for this is the low number of class features extracted compared to patch tokens."
- Break condition: If number of foreground tokens per document is very small, VLAD may overfit to limited samples and sum-pooling could perform comparably.

### Mechanism 3
- Claim: Self-distillation with attention-guided masking produces robust, generalizable features without labeled data.
- Mechanism: The teacher's self-attention map identifies the most informative patches; masking them forces the student to learn representations that do not rely on a few key regions, improving robustness to variations. Exponential moving average of student weights stabilizes training.
- Core assumption: Attention-guided masking yields richer, more generalizable features than random masking.
- Evidence anchors:
  - [section] "AttMask [20] introduces a novel masking strategy that increases the complexity of feature reconstruction compared to the original iBOT masking, aiming to generate a more robust feature space."
  - [section] "The final self-attention map generated by the teacher is used to mask the most attended input patches from the student."
- Break condition: If attention maps are noisy or misleading, masking based on them may degrade learning more than random masking.

## Foundational Learning

- Concept: Vision Transformer patch tokenization and positional embeddings
  - Why needed here: ViT splits image into patches and linear embeddings; understanding patch arrangement and embedding size is critical for feature extraction and foreground filtering.
  - Quick check question: Given a 224x224 image and patch size 16x16, how many patches are produced? (Answer: 196)

- Concept: VLAD encoding and codebook clustering
  - Why needed here: VLAD requires clustering local descriptors into centroids; knowing how k-means minibatch works and how residuals are aggregated is essential for interpreting aggregation performance.
  - Quick check question: If codebook size C=100 and final PCA dimension D=384, what is the raw VLAD dimension before PCA? (Answer: 38,400)

- Concept: Self-supervised learning objectives (contrastive, masked image modeling)
  - Why needed here: The training pipeline uses AttMask (a variant of MIM + self-distillation); understanding how masked patches are predicted from unmasked ones is key to grasping why the model generalizes without labels.
  - Quick check question: In self-distillation, what role does the teacher's exponential moving average of student weights play? (Answer: Stabilizes training by providing consistent target representations)

## Architecture Onboarding

- Component map: Input pipeline (binary image → regular grid windows → patch tokenization → [CLS] + patch sequence) → Feature extractor (ViT-small/16 trained via AttMask) → Filtering (foreground token extraction based on pixel count threshold tfg) → Aggregation (VLAD codebook → power/l2 normalization → PCA) → Retrieval (cosine distance + optional reranking)
- Critical path: Window extraction → ViT forward pass → foreground token filtering → VLAD encoding → retrieval scoring
- Design tradeoffs:
  - Window stride (Seval): smaller stride increases recall but quadruples computation
  - tfg threshold: higher removes more background but risks discarding useful faint strokes
  - PCA dimensionality (D): higher preserves detail but increases memory; too low loses discriminative info
- Failure signatures:
  - mAP plateaus despite more training → attention masking not helping or overfitting
  - Top1 much lower than mAP → model ranks correct writer high but not first
  - Performance drops sharply when removing reranking → local features insufficient alone
- First 3 experiments:
  1. Run inference with tfg=0 (all tokens) vs tfg=10 (filtered) to confirm foreground filtering improves VLAD.
  2. Replace AttMask with DINO to check if attention-guided masking is essential.
  3. Test sum-pooling of class token vs VLAD of foreground tokens to reproduce baseline claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the ViT feature extractor compare to CNNs on modern datasets beyond CVL, such as IAM or Rimes?
- Basis in paper: [inferred] The paper mentions CVL performance but does not compare to other modern datasets or CNNs.
- Why unresolved: The paper focuses on historical datasets and only briefly tests on CVL without broader comparison.
- What evidence would resolve it: Evaluations on additional modern datasets with direct comparisons to CNN-based methods.

### Open Question 2
- Question: What is the impact of different masking strategies (e.g., block vs. hint) on the robustness of the ViT feature extractor for writer retrieval?
- Basis in paper: [explicit] The paper evaluates masking strategies in Table 2 but does not explore their robustness across diverse datasets or conditions.
- Why unresolved: The evaluation is limited to specific datasets and does not test generalization.
- What evidence would resolve it: Experiments testing masking strategies on varied datasets and under different noise conditions.

### Open Question 3
- Question: Can the VLAD codebook size be optimized dynamically based on the dataset characteristics to improve retrieval performance?
- Basis in paper: [inferred] The paper uses a fixed codebook size (C=100) without exploring adaptive sizing.
- Why unresolved: The paper does not investigate the relationship between codebook size and dataset complexity.
- What evidence would resolve it: Experiments comparing fixed vs. adaptive codebook sizes across multiple datasets.

## Limitations
- Limited external validation of the specific mechanisms (foreground token filtering, VLAD aggregation, attention-guided masking) beyond within-paper ablation studies
- Fixed foreground token extraction threshold (tfg=10) without exploration of dataset-specific optimization
- Lack of comparison to other self-supervised ViT methods like DINO or MoCo beyond a single mention

## Confidence
- **High confidence**: The overall performance claims (mAP scores on three datasets) and the general superiority over CNN-based methods
- **Medium confidence**: The specific mechanisms (foreground token filtering, VLAD aggregation, attention-guided masking) due to limited external validation
- **Low confidence**: Claims about the robustness to diverse datasets beyond the three tested benchmarks

## Next Checks
1. **Cross-dataset generalization test**: Train on Historical-WI and evaluate directly on HisIR19 and CVL without fine-tuning to verify the claimed zero-shot transfer capability
2. **Ablation on foreground filtering threshold**: Systematically vary tfg from 0 to 50 and measure impact on mAP to determine optimal threshold and sensitivity
3. **Comparison with alternative self-supervised methods**: Replace AttMask with DINO training while keeping all other components identical to isolate the contribution of attention-guided masking