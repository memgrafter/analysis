---
ver: rpa2
title: '1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?'
arxiv_id: '2406.14721'
source_url: https://arxiv.org/abs/2406.14721
tags:
- language
- llms
- knowledge
- answer
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of inconsistent multilingual performance
  in Large Language Models (LLMs), where identical queries yield different answers
  across languages. The proposed method enhances LLM multilingual performance by detecting
  low-resource knowledge queries, selecting the most suitable target language, and
  integrating or replacing answers accordingly.
---

# 1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?

## Quick Facts
- arXiv ID: 2406.14721
- Source URL: https://arxiv.org/abs/2406.14721
- Reference count: 25
- Key outcome: The proposed method significantly improves LLM multilingual performance by detecting low-resource knowledge queries, selecting optimal target languages, and integrating/replacing answers, reducing language performance disparities across six LLMs and five datasets.

## Executive Summary
This paper addresses the problem of inconsistent multilingual performance in Large Language Models, where identical queries yield different answers across languages. The authors propose a novel method that enhances LLM multilingual performance by detecting low-resource knowledge queries, selecting the most suitable target language, and integrating or replacing answers accordingly. The approach leverages cross-lingual knowledge transfer to improve both overall performance and fairness across languages, demonstrating significant improvements on six popular LLMs and five bilingual datasets.

## Method Summary
The proposed method consists of three main components: a low-resource knowledge detector that identifies queries underrepresented in the source language, a target language selection process that determines the most suitable language for answering the query, and an answer replacement/integration strategy that combines or replaces the original answer with the translated answer. The method is evaluated on six popular LLMs (ChatGPT, GPT-4, ChatGLM3, Yi-34b, Qwen-turbo, Llama3-Chinese) using five bilingual datasets (English and Chinese), with improvements measured using LLM-as-a-Judge for answer correctness and reduction in cross-lingual performance gaps.

## Key Results
- The proposed method significantly improves overall LLM performance across six models and five datasets
- Cross-lingual performance disparities are substantially reduced, enhancing fairness across languages
- Both answer replacement and integration strategies show significant improvements, with ablation studies confirming each component's contribution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The low-resource knowledge detector successfully identifies queries where multilingual knowledge gaps exist.
- **Mechanism**: The detector is trained on a labeled dataset distinguishing common sense queries from language-specific and low-resource queries. When a query is classified as low-resource, it triggers the translation pipeline to fetch knowledge from other languages.
- **Core assumption**: Queries that are under-represented in the pretraining corpus of a specific language will be accurately classified as low-resource by the trained detector.
- **Evidence anchors**:
  - [abstract] "This approach incorporates a low-resource knowledge detector specific to a language"
  - [section 3.2] "We train a classifier for each source language to identify the low-resource query for that language"
  - [corpus] Weak: The corpus contains papers on cross-lingual knowledge transfer, but none directly validate the specific low-resource detector mechanism proposed here.
- **Break condition**: If the detector's training data does not adequately represent the distribution of low-resource queries in real-world usage, it will fail to identify knowledge gaps accurately.

### Mechanism 2
- **Claim**: The target language selection module chooses the language most likely to provide accurate knowledge for a low-resource query.
- **Mechanism**: The LLM itself is prompted to select the target language based on the query's content. This selection is then used to translate the query and generate the answer in that language before translating back.
- **Core assumption**: The LLM has sufficient knowledge about its own training corpus distribution to select the most appropriate language for a given query.
- **Evidence anchors**:
  - [section 3.4] "Different LLMs may select different target languages due to their pretraining corpus"
  - [section 4.3] "The language selection module only adds a small inference cost while significantly improving the model performance"
  - [corpus] Weak: The corpus mentions cross-lingual alignment and multilingual models, but doesn't provide direct evidence for LLMs selecting optimal languages for knowledge retrieval.
- **Break condition**: If the LLM's knowledge about its own training corpus is incomplete or biased, it may select suboptimal languages, reducing the effectiveness of the approach.

### Mechanism 3
- **Claim**: Answer replacement and integration effectively combine knowledge from multiple languages to produce superior responses.
- **Mechanism**: For replacement, the translated answer fully replaces the original. For integration, the LLM combines both answers using a prompt that prioritizes one language while incorporating elements from the other.
- **Core assumption**: LLMs can effectively integrate information from multiple language sources when guided by appropriate prompts.
- **Evidence anchors**:
  - [section 3.5] "We also explore the integration of answers in the scenario of open-ended question answering"
  - [section 4.3] "Both replacement and integration methods significantly enhanced the LLMs' performance across most datasets"
  - [corpus] Weak: The corpus contains general information about multilingual models and answer integration, but lacks specific validation of this approach.
- **Break condition**: If the answers from different languages contain conflicting factual information that cannot be reconciled, the integration process may produce confusing or incorrect responses.

## Foundational Learning

- **Concept**: Cross-lingual knowledge transfer
  - Why needed here: Understanding how knowledge in one language can be leveraged to improve performance in another is fundamental to this approach.
  - Quick check question: Can you explain why a model might answer a question about Chinese history better when prompted in Chinese versus English?

- **Concept**: Low-resource language detection
  - Why needed here: The core innovation relies on accurately identifying when a query involves knowledge that is underrepresented in the source language's training data.
  - Quick check question: What features might you use to train a classifier to distinguish low-resource queries from common knowledge queries?

- **Concept**: Prompt engineering for multilingual tasks
  - Why needed here: The approach heavily relies on carefully crafted prompts to guide the LLM in selecting languages and integrating answers across languages.
  - Quick check question: How would you prompt an LLM to choose between multiple languages when answering a query?

## Architecture Onboarding

- **Component map**: Query → Low-resource detector → (if low-resource) Language selector → Translator → LLM → Translator → (if integration) Answer integrator → Final answer
- **Critical path**: Query → Low-resource detector → (if low-resource) Language selector → Translator → LLM → Translator → (if integration) Answer integrator → Final answer
- **Design tradeoffs**:
  - Accuracy vs. latency: The low-resource detector adds minimal overhead but the translation and multi-inference steps increase latency
  - Model-specific vs. universal: The language selector uses model-specific knowledge, making it less portable across different LLMs
  - Replacement vs. integration: Replacement is simpler but integration can produce better results when answers are complementary
- **Failure signatures**:
  - Low-resource detector misclassifies queries → unnecessary translation overhead or missed knowledge gaps
  - Language selector chooses wrong language → degraded answer quality
  - Translation errors propagate → incorrect final answers
  - Integration conflicts unresolved → confusing or incorrect responses
- **First 3 experiments**:
  1. Test the low-resource detector's accuracy on a held-out test set of labeled queries
  2. Evaluate the language selector's performance by checking if it chooses the same language as a human expert would
  3. Compare answer quality using replacement vs. integration strategies on a small multilingual dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several questions arise regarding the generalizability and limitations of the approach.

## Limitations
- The low-resource knowledge detector's accuracy depends heavily on the quality and representativeness of its training data
- The approach introduces additional latency through translation and multi-inference steps
- The language selection process may be model-specific, limiting portability across different LLMs

## Confidence
- **High confidence**: The experimental results showing improved overall performance and reduced language disparities across six LLMs and five datasets are well-documented and reproducible.
- **Medium confidence**: The three proposed mechanisms (low-resource detection, language selection, answer integration) are theoretically sound but require more detailed validation of their individual contributions.
- **Low confidence**: The specific implementation details of the low-resource detector and language selection prompts are not fully specified, making exact reproduction challenging.

## Next Checks
1. **Low-resource detector validation**: Create a held-out test set of labeled queries and measure the detector's precision and recall in identifying low-resource queries across multiple languages.
2. **Language selection accuracy**: Have human experts evaluate whether the LLM's language selections align with optimal choices for a diverse set of low-resource queries.
3. **Answer integration quality**: Test the integration process on queries with known conflicting information across languages to assess how well the approach handles contradictions and produces coherent responses.