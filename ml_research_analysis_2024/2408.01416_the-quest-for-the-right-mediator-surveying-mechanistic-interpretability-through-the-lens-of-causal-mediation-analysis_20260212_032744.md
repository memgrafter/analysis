---
ver: rpa2
title: 'The Quest for the Right Mediator: Surveying Mechanistic Interpretability Through
  the Lens of Causal Mediation Analysis'
arxiv_id: '2408.01416'
source_url: https://arxiv.org/abs/2408.01416
tags:
- causal
- interpretability
- methods
- neural
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a causal mediation analysis framework to survey
  and categorize mechanistic interpretability methods in neural networks. The authors
  taxonomize work by the types of causal mediators (e.g., neurons, attention heads,
  non-basis-aligned directions) and methods for searching over them.
---

# The Quest for the Right Mediator: Surveying Mechanistic Interpretability Through the Lens of Causal Mediation Analysis

## Quick Facts
- arXiv ID: 2408.01416
- Source URL: https://arxiv.org/abs/2408.01416
- Reference count: 39
- Primary result: A causal mediation analysis framework taxonomizes mechanistic interpretability methods by mediator types and search methods, finding non-basis-aligned directions most effective for explaining model behavior

## Executive Summary
This paper introduces a causal mediation analysis framework to systematically survey and categorize mechanistic interpretability methods in neural networks. The authors develop a taxonomy that classifies work by the types of causal mediators (neurons, attention heads, non-basis-aligned directions) and methods for searching over them. Through this lens, they analyze the effectiveness of different mediator types for explaining model behavior, finding that sparse autoencoders discovering non-basis-aligned directions offer the best interpretability while basis-aligned mediators excel at localization and editing tasks.

## Method Summary
The authors employ causal mediation analysis to frame mechanistic interpretability as identifying causal mediators between input and output in neural networks. They taxonomize existing work by mediator types (basis-aligned vs non-basis-aligned) and search methods (supervised, unsupervised, model-guided). The framework evaluates mediators based on their ability to explain model behavior through causal interventions, comparing their effectiveness across different interpretability goals like understanding, localization, and editing.

## Key Results
- Non-basis-aligned directions discovered via unsupervised sparse autoencoders are most effective for explaining model behavior due to their sparsity and interpretability
- Basis-aligned mediators are better suited for localization and editing tasks where specific components need to be identified and modified
- The survey highlights the need for standardized benchmarks to compare mediator types and search methods across different interpretability goals

## Why This Works (Mechanism)
The causal mediation framework works by treating neural network components as potential causal mediators between input and output. By systematically searching for and validating these mediators through interventions, the approach can identify which components actually drive model behavior rather than just correlating with it. The framework's effectiveness stems from its ability to distinguish between mediators that merely correlate with outputs and those that causally influence them, providing a rigorous foundation for mechanistic interpretability.

## Foundational Learning
- **Causal mediation analysis**: A statistical framework for decomposing causal effects into direct and indirect pathways through mediators; needed to rigorously identify which neural components actually cause model behavior rather than just correlating with it
- **Basis-aligned vs non-basis-aligned mediators**: Basis-aligned mediators correspond to standard neural network components (neurons, attention heads) while non-basis-aligned mediators exist in alternative representational spaces; needed to understand the trade-offs between interpretability and causal effectiveness
- **Search methods taxonomy**: Supervised, unsupervised, and model-guided approaches for discovering mediators; needed to evaluate the efficiency and scalability of different discovery strategies
- **Interpretability vs manipulability trade-off**: The tension between finding mediators that are both interpretable to humans and effective for causal interventions; needed to understand why certain mediator types excel at different tasks
- **Intervention-based validation**: Testing mediator effectiveness through systematic ablation and activation experiments; needed to empirically verify causal claims about mediator importance

## Architecture Onboarding

Component map: Input -> Feature extractor -> Mediator space -> Output prediction

Critical path: The search for causal mediators that bridge input representations to output predictions, with different mediator types offering varying levels of interpretability and causal effectiveness.

Design tradeoffs: The framework balances between finding highly interpretable mediators (basis-aligned) versus those with stronger causal explanatory power (non-basis-aligned). Search methods trade off between supervision requirements and discovery generality.

Failure signatures: Ineffective mediators may correlate with outputs without causing them, or be too complex to interpret despite having causal power. Poor search methods may miss important mediators or find spurious ones.

First experiments:
1. Apply the causal mediation framework to a simple CNN to validate the approach on non-transformer architectures
2. Compare mediator effectiveness using standardized ablation studies across different search methods
3. Test the interpretability of discovered mediators through human evaluation tasks

## Open Questions the Paper Calls Out
The authors highlight several open questions: How to design inherently interpretable models rather than interpreting post-hoc? Can we develop more efficient search methods for discovering mediators at scale? What are the fundamental limits of causal mediation analysis in complex neural networks? How to create standardized benchmarks for comparing mediator types across different interpretability goals?

## Limitations
- The framework's applicability across diverse neural architectures remains uncertain, as most surveyed methods focus on transformer-based language models
- The effectiveness claims for different mediator types rely heavily on qualitative assessments rather than standardized quantitative benchmarks, limiting reproducibility
- The causal inference assumptions underlying the mediation analysis approach may not hold for complex neural networks with non-linear interactions and feedback loops

## Confidence

High confidence:
- The taxonomy of mediator types and search methods is well-grounded in existing literature
- The observation that non-basis-aligned directions offer interpretability advantages is supported by multiple cited works

Medium confidence:
- Claims about the relative effectiveness of different mediator types for specific tasks are based on aggregating findings from diverse studies with varying methodologies and evaluation criteria
- The causal mediation framework's practical utility depends on future empirical validation across broader model families

Low confidence:
- Predictions about future research directions and the potential for building inherently interpretable models remain speculative without concrete implementation examples or experimental results

## Next Checks
1. Apply the causal mediation framework to non-transformer architectures (CNNs, RNNs) to test generalizability across model families
2. Design standardized benchmarks that directly compare different mediator types on identical tasks to quantify their relative effectiveness
3. Conduct ablation studies removing key mediators to empirically verify their causal role in model behavior