---
ver: rpa2
title: Biomedical Large Languages Models Seem not to be Superior to Generalist Models
  on Unseen Medical Data
arxiv_id: '2408.13833'
source_url: https://arxiv.org/abs/2408.13833
tags:
- llama
- instruct
- biomedical
- chat
- openbiollm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study challenges the assumption that fine-tuning large language
  models on biomedical data improves their performance in clinical tasks. Evaluating
  both biomedical and general-purpose LLMs on clinical case challenges and various
  NLP tasks, the researchers found that general-purpose models like Llama-3-70B-Instruct
  often outperformed their biomedical counterparts.
---

# Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data

## Quick Facts
- arXiv ID: 2408.13833
- Source URL: https://arxiv.org/abs/2408.13833
- Reference count: 31
- General-purpose LLMs often outperform biomedical LLMs on clinical tasks despite domain-specific fine-tuning

## Executive Summary
This study challenges the assumption that fine-tuning large language models on biomedical data improves their performance in clinical tasks. Evaluating both biomedical and general-purpose LLMs on clinical case challenges and various NLP tasks, the researchers found that general-purpose models like Llama-3-70B-Instruct often outperformed their biomedical counterparts. The results suggest that biomedical fine-tuning may not provide expected benefits and could potentially lead to reduced performance, highlighting the need for more rigorous evaluation frameworks in healthcare AI.

## Method Summary
The study compared biomedical and general-purpose LLMs on clinical case challenges (NEJM and JAMA datasets) and CLUE benchmark tasks using GPU-based evaluation with Hugging Face transformers and vLLM for compatible models. Models were tested with consistent inference parameters across all tasks, with biomedical models evaluated against their general-purpose counterparts. The evaluation included accuracy metrics for case challenges, text generation metrics (ROUGE/BERTScore), and F1-scores for coding and entity recognition tasks.

## Key Results
- Llama-3-70B-Instruct achieved 65% accuracy on JAMA case challenges, slightly outperforming OpenBioLLM-70B's 66%
- Smaller biomedical models like OpenBioLLM-8B (18%) significantly underperformed compared to Llama-3-8B-Instruct (57%)
- Similar trends observed across other clinical tasks, with general-purpose models showing better performance in text generation, question answering, and coding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalist LLMs outperform biomedical LLMs on clinical tasks due to better cross-domain generalization.
- Mechanism: Generalist models trained on vast web data likely include substantial biomedical information, enabling them to reason across domains without the domain-specific bias introduced by fine-tuning.
- Core assumption: The biomedical knowledge required for clinical tasks is sufficiently represented in general web data.
- Evidence anchors:
  - [abstract]: "Llama 3 was trained on an unprecedented 15 trillion tokens of data... nearly all freely available biomedical texts on the internet are included in the training data"
  - [section]: "The broader knowledge base could enable more flexible reasoning and better generalization to novel tasks"
  - [corpus]: Weak - neighbor papers focus on domain-specific adaptations but don't directly test this generalization claim
- Break condition: If biomedical knowledge requires proprietary or specialized datasets not available on the web, this mechanism fails.

### Mechanism 2
- Claim: Fine-tuning causes catastrophic forgetting, degrading general knowledge needed for clinical reasoning.
- Mechanism: Continued pretraining on biomedical data overwrites weights representing general knowledge, while the biomedical knowledge added is redundant with what's already learned.
- Core assumption: General knowledge is more valuable for clinical reasoning than domain-specific knowledge.
- Evidence anchors:
  - [abstract]: "fine-tuning LLMs to biomedical data may not provide the expected benefits and may potentially lead to reduced performance"
  - [section]: "fine-tuning process for biomedical models might inadvertently introduce biases or overly narrow the models' focus"
  - [corpus]: Missing - no direct evidence about catastrophic forgetting in the neighbor papers
- Break condition: If clinical tasks require deep domain-specific knowledge not present in general training data.

### Mechanism 3
- Claim: Biomedical LLMs have higher hallucination rates due to overfitting and domain-specific biases.
- Mechanism: Domain-specific fine-tuning creates models that confidently generate incorrect or fabricated information when faced with novel scenarios outside their training distribution.
- Core assumption: Generalist models have better calibration and uncertainty handling across domains.
- Evidence anchors:
  - [abstract]: "Our findings suggest that fine-tuning LLMs on biomedical data may not provide the expected benefits and may potentially lead to reduced performance"
  - [section]: "The superior performance of general-purpose models in this aspect is particularly intriguing"
  - [corpus]: Weak - hallucination is mentioned but not systematically evaluated in neighbor papers
- Break condition: If biomedical LLMs are specifically trained with hallucination mitigation techniques.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding why fine-tuning degrades performance is central to interpreting the results
  - Quick check question: What happens to a model's weights when it's fine-tuned on new data after initial training?

- Concept: Evaluation methodology and benchmark contamination
  - Why needed here: The study emphasizes using "benchmarks specifically chosen to be likely outside the fine-tuning datasets"
  - Quick check question: How can you verify that test data wasn't present in training data?

- Concept: Retrieval-augmented generation (RAG) as alternative
  - Why needed here: The paper suggests RAG may be more effective than fine-tuning for enhancing biomedical capabilities
  - Quick check question: How does RAG differ from fine-tuning in terms of knowledge integration?

## Architecture Onboarding

- Component map: Model loading -> Prompt formatting -> Inference -> Metric calculation -> Result aggregation
- Critical path: Model loading → Prompt formatting → Inference → Metric calculation → Result aggregation
- Design tradeoffs: Using vLLM for larger models provides speed but requires more complex GPU setup; simpler models run faster on single GPUs
- Failure signatures: Incoherent outputs from biomedical models suggest prompt formatting issues; poor performance relative to baselines suggests catastrophic forgetting or overfitting
- First 3 experiments:
  1. Run Llama-3-70B-Instruct on a single JAMA case to verify baseline performance
  2. Compare OpenBioLLM-70B performance with and without the correct chat template
  3. Test a small biomedical model on a simple MedNLI example to verify it produces valid outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of biomedical fine-tuning lead to catastrophic forgetting of general knowledge in LLMs?
- Basis in paper: [explicit] The paper suggests that biomedical fine-tuning may cause catastrophic forgetting, with evidence that models undergoing continued pretraining (like BioMistral-7B) showed more performance decrease compared to those with only supervised fine-tuning.
- Why unresolved: The paper notes insufficient data to definitively conclude the mechanisms behind catastrophic forgetting in biomedical fine-tuning and calls for further research.
- What evidence would resolve it: Comparative studies isolating different fine-tuning methods (supervised vs. continued pretraining) while controlling for other variables, along with detailed analysis of knowledge retention across general and biomedical domains.

### Open Question 2
- Question: Under what specific conditions, if any, does biomedical fine-tuning provide performance benefits over general-purpose models?
- Basis in paper: [inferred] The paper observes that performance differences vary by task, with biomedical models potentially showing advantages in highly specialized tasks requiring deep domain knowledge (e.g., ICD coding).
- Why unresolved: The study provides limited evidence due to task diversity and suggests the relationship is nuanced, requiring more rigorous, task-specific evaluation frameworks.
- What evidence would resolve it: Systematic evaluation of biomedical LLMs across a wide range of tasks with varying levels of domain specificity, comparing fine-tuned models against general-purpose models while controlling for dataset size and composition.

### Open Question 3
- Question: How can retrieval-augmented generation be optimized to enhance biomedical capabilities of LLMs without compromising general knowledge?
- Basis in paper: [explicit] The authors suggest retrieval-augmented generation as an alternative approach worth exploring, citing recent studies showing promising results.
- Why unresolved: While the paper mentions RAG as a potential solution, it doesn't provide specific implementation details or comparative performance data against fine-tuning approaches.
- What evidence would resolve it: Empirical studies comparing RAG-enhanced biomedical LLMs against both fine-tuned and general-purpose models across multiple clinical tasks, with analysis of knowledge retention and task-specific performance.

## Limitations

- The study tested only three biomedical models compared to three general-purpose models, which may not represent the full landscape of biomedical LLMs
- The paper suggests biomedical models may have higher hallucination rates but lacks direct empirical validation through hallucination detection methods
- Evaluation focuses on accuracy metrics without deeper analysis of reasoning processes or error types

## Confidence

- **High confidence**: The empirical observation that general-purpose models often outperform biomedical models on the specific clinical benchmarks tested, particularly for larger model sizes
- **Medium confidence**: The broader claim that fine-tuning on biomedical data may not provide expected benefits and could potentially degrade performance, as this conclusion is based on a limited sample of models and tasks
- **Low confidence**: The speculation about catastrophic forgetting and hallucination rates in biomedical models, which are mentioned as potential mechanisms but not empirically validated in the study

## Next Checks

1. Conduct systematic hallucination analysis on both biomedical and general-purpose models using established hallucination detection methods across the same clinical tasks to verify if biomedical models indeed have higher hallucination rates
2. Expand the evaluation to include additional biomedical models (e.g., BioGPT, BioMedLM, PubMedBERT) and diverse task types to determine if the performance patterns generalize beyond the current sample
3. Perform ablation studies by fine-tuning general-purpose models on biomedical data with different strategies (full fine-tuning, LoRA, RAG) to isolate the specific factors that may cause performance degradation in biomedical models