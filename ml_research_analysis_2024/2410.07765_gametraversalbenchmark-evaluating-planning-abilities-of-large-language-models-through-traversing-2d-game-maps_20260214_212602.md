---
ver: rpa2
title: 'GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models
  Through Traversing 2D Game Maps'
arxiv_id: '2410.07765'
source_url: https://arxiv.org/abs/2410.07765
tags:
- llms
- uni00000013
- objective
- agent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GameTraversalBenchmark (GTB), a novel benchmark
  designed to evaluate the planning capabilities of large language models (LLMs) through
  2D grid-based game map traversal tasks. GTB consists of diverse game maps with multiple
  objectives, where LLMs must generate action sequences to navigate from their starting
  position to specified target coordinates while minimizing steps and generation errors.
---

# GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps

## Quick Facts
- arXiv ID: 2410.07765
- Source URL: https://arxiv.org/abs/2410.07765
- Authors: Muhammad Umair Nasir; Steven James; Julian Togelius
- Reference count: 40
- Primary result: GPT-4-Turbo achieved 44.97% on GTB_Score, while o1 achieved 67.84%

## Executive Summary
This paper introduces GameTraversalBenchmark (GTB), a novel benchmark designed to evaluate the planning capabilities of large language models (LLMs) through 2D grid-based game map traversal tasks. GTB consists of diverse game maps with multiple objectives, where LLMs must generate action sequences to navigate from their starting position to specified target coordinates while minimizing steps and generation errors. The benchmark employs a composite scoring system (GTB_Score) that accounts for path length, generation mistakes, and proximity to objectives. Evaluation results show that while state-of-the-art LLMs can generate reasonable path lengths, they often fail to produce the correct actions, suggesting limitations in their ability to internally construct and reason about game state representations.

## Method Summary
The benchmark uses 150 diverse 2D grid-based game maps generated by Word2World, each containing multiple objectives represented as coordinates within the map. LLMs receive the current game state (grid, agent position, objectives, walkable tiles) and must generate complete action sequences in a zero-shot evaluation setting, meaning they cannot re-generate actions after errors. The GTB_Score combines path length optimization, generation error minimization, and proximity to objectives through a composite reward system that normalizes performance across all levels. The evaluation measures both the quality of generated paths and the frequency of generation mistakes, providing insights into LLMs' planning abilities versus their token generation capabilities.

## Key Results
- GPT-4-Turbo achieved the highest GTB_Score of 44.97% among tested models
- The large reasoning model o1 achieved 67.84%, indicating GTB remains challenging for current models
- LLMs showed reasonable path lengths but high generation error rates, suggesting planning limitations
- The benchmark revealed that state-of-the-art LLMs struggle to construct internal game state representations for effective planning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate 2D grid-based maps using character sequences that represent walkable tiles, objectives, and agent positions, enabling planning evaluation in a domain less likely to be memorized during training.
- Mechanism: The Word2World algorithm first places environment tiles (alphanumeric characters) in a 2D grid, then adds game characters (agent, objectives), and iteratively refines placement to generate coherent maps. These character sequences are interpretable by LLMs as game states.
- Core assumption: LLMs trained on natural language can understand and reason about abstract character sequences representing game environments when provided appropriate context and instructions.
- Evidence anchors:
  - [abstract]: "a benchmark consisting of diverse 2D grid-based game maps... represented as a string of characters that depicts a 2D grid-based map"
  - [section]: "Word2World is an LLM-based game design system that creates a story and narrative for the game by extracting useful information... It places alphanumeric characters for the environment of the world in a 2D grid-based map"
  - [corpus]: Weak - related papers focus on LLM evaluation in games but don't specifically address the Word2World generation mechanism
- Break condition: If LLMs cannot interpret character-based map representations as meaningful game states, the benchmark loses validity.

### Mechanism 2
- Claim: The composite scoring system (GTB_Score) effectively measures LLM planning ability by combining path length optimization, generation error minimization, and proximity to objectives.
- Mechanism: GTB_Score normalizes rewards across all levels using the difference between achieved reward and theoretical minimum/maximum, accounting for both path efficiency and generation quality. Generation errors are tracked separately to identify planning vs. execution failures.
- Core assumption: A single composite metric can capture multiple dimensions of planning performance (path optimization, error reduction, objective proximity) in a balanced way.
- Evidence anchors:
  - [abstract]: "a composite scoring system (GTB_Score) that accounts for path length, generation mistakes, and proximity to objectives"
  - [section]: "GTB_Score = 1/M * Σ[(R(m) - LLM(m)_PL - ε(m) - R(m)_min)/(R(m)_max - R(m)_min)]"
  - [corpus]: Weak - related work mentions game benchmarks but doesn't discuss composite scoring approaches
- Break condition: If the normalization formula disproportionately weights one component (e.g., path length vs. errors), the score may not accurately reflect true planning ability.

### Mechanism 3
- Claim: Zero-shot evaluation prevents LLMs from leveraging training data memorization, ensuring the benchmark tests genuine planning capabilities rather than pattern matching.
- Mechanism: LLMs receive only the current game state and must generate complete action sequences without access to previous attempts or solutions, forcing them to plan from scratch for each objective.
- Core assumption: The 2D grid-based game maps are sufficiently novel that LLMs cannot rely on memorized patterns from training data to succeed.
- Evidence anchors:
  - [abstract]: "we introduce the GameTraversalBenchmark (GTB), which consists of a dataset of diverse maps"
  - [section]: "Zero-shot means that the LLM agent will have to generate action sequences in one go. If it makes a generation error, it is not given the previous actions to re-generate the actions"
  - [corpus]: Weak - related papers discuss LLM evaluation but don't specifically address zero-shot planning evaluation
- Break condition: If LLMs can successfully complete tasks by pattern matching or copying from training data rather than genuine planning, the benchmark loses validity.

## Foundational Learning

- Concept: A* pathfinding algorithm
  - Why needed here: The benchmark uses A* to compute optimal paths for scoring LLM performance, establishing a baseline for comparison
  - Quick check question: What is the primary heuristic function used in A* pathfinding for grid-based navigation?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: LLMs are based on transformer architecture, and understanding how attention mechanisms process sequential information is crucial for understanding their planning limitations
  - Quick check question: How does the self-attention mechanism in transformers enable processing of the character-based map representations?

- Concept: Reinforcement learning vs. next-token prediction
  - Why needed here: The paper discusses the debate over whether LLMs can plan, which relates to the fundamental difference between RL-based planning and token prediction
  - Quick check question: What is the key difference between how reinforcement learning agents and LLMs approach sequential decision-making tasks?

## Architecture Onboarding

- Component map: Map generation pipeline (Word2World) -> Benchmark evaluation engine -> LLM integration interface -> Scoring calculation module -> Error tracking system -> Visualization tools

- Critical path: Map generation → LLM evaluation loop → Action sequence generation → Simulation → Reward calculation → Score aggregation

- Design tradeoffs:
  - Map complexity vs. evaluation tractability: More complex maps provide better planning tests but require more computational resources
  - Action granularity: Fine-grained actions (e.g., move_up, move_down) provide precise control but may require more tokens to describe paths
  - Evaluation frequency: More frequent evaluation points provide better error analysis but increase computational overhead

- Failure signatures:
  - High generation errors with reasonable path lengths: Indicates LLMs understand spatial relationships but struggle with action generation
  - Random action sequences: Suggests LLMs cannot construct internal map representations
  - Consistently short paths with poor accuracy: Indicates LLMs recognize path length constraints but fail to plan effectively

- First 3 experiments:
  1. Test baseline performance on simple 5×10 maps with single objectives to establish minimum viable performance
  2. Compare zero-shot vs. one-shot performance to quantify the benefit of additional context
  3. Evaluate different prompt formulations to identify optimal instruction design for the benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs improve their planning abilities through fine-tuning on GTB, and if so, how much improvement can be expected?
- Basis in paper: [explicit] The authors state, "For future work, we would like to see how LLMs of all scales perform when they are fine-tuned on GTB. We would like to see how much smaller LLMs can improve upon fine-tuning, and after fine-tuning, how much such fine-tuning will generalise."
- Why unresolved: The paper does not provide any results or analysis on the effects of fine-tuning LLMs on GTB.
- What evidence would resolve it: Experiments comparing the performance of LLMs on GTB before and after fine-tuning, with different model sizes and architectures.

### Open Question 2
- Question: Can LLMs be trained to generate the state representation for GTB, rather than just receiving it as input?
- Basis in paper: [explicit] The authors mention, "Once we have LLMs that can perform well on GTB, we would like to extend the work by letting LLMs generate the state representation as well."
- Why unresolved: The paper focuses on evaluating LLMs' planning abilities given a pre-defined state representation, not their ability to generate it.
- What evidence would resolve it: Experiments where LLMs are trained to generate the state representation for GTB, and their performance is compared to LLMs that receive the state representation as input.

### Open Question 3
- Question: How can the difficulty of GTB be increased to further challenge LLMs' planning abilities?
- Basis in paper: [explicit] The authors discuss potential extensions, including "The 2D game maps are static, which is still challenging but can be increased in difficulty, such as moving non-player characters, enemies that may attack, tiles that may end in terminating conditions etc."
- Why unresolved: The current version of GTB uses static maps and a limited action space, which may not fully capture the complexity of real-world planning tasks.
- What evidence would resolve it: Experiments comparing the performance of LLMs on GTB with and without the proposed extensions, such as dynamic maps and a larger action space.

## Limitations

- The benchmark cannot definitively distinguish between planning deficits versus token prediction failures or prompt comprehension issues
- The Word2World generation mechanism may produce maps that still align with patterns seen in textual corpora, potentially allowing partial memorization
- The composite scoring system lacks validation that its normalization formula appropriately balances path optimization against error reduction

## Confidence

- Mechanism 1: Low confidence - The character-based map representation system's novelty and interpretability by LLMs is not sufficiently validated
- Mechanism 2: Medium confidence - The composite scoring system is theoretically sound but lacks empirical validation of its balance
- Mechanism 3: Medium confidence - Zero-shot evaluation prevents memorization but may be too restrictive for longer path sequences

## Next Checks

1. **Error type classification**: Implement detailed logging to distinguish between syntactic generation errors, semantic errors (invalid moves), and strategic errors (poor path choices) to better understand failure modes.

2. **One-shot vs. zero-shot comparison**: Evaluate the same models with one-shot learning (providing successful examples) to determine whether performance improvements stem from genuine planning learning versus pattern matching.

3. **Human baseline comparison**: Have human participants complete a subset of the same tasks to establish whether the benchmark difficulty is appropriately calibrated and to validate that observed LLM failures represent genuine planning limitations rather than benchmark design flaws.