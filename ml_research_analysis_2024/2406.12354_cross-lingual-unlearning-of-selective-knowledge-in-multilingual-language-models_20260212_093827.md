---
ver: rpa2
title: Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models
arxiv_id: '2406.12354'
source_url: https://arxiv.org/abs/2406.12354
tags:
- unlearning
- language
- multilingual
- original
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for machine unlearning in
  multilingual language models, addressing the limitation that unlearning in one language
  does not transfer to others. The proposed method employs an adaptive unlearning
  scheme that assigns language-dependent weights based on a multilingual teacher model's
  performance, allowing selective forgetting across languages while maintaining overall
  model performance.
---

# Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models

## Quick Facts
- arXiv ID: 2406.12354
- Source URL: https://arxiv.org/abs/2406.12354
- Authors: Minseok Choi; Kyunghyun Min; Jaegul Choo
- Reference count: 15
- This paper introduces a novel approach for machine unlearning in multilingual language models, addressing the limitation that unlearning in one language does not transfer to others.

## Executive Summary
This paper addresses a critical limitation in multilingual language models: the lack of cross-lingual transfer in machine unlearning. While unlearning can remove targeted knowledge in one language, this forgetting does not automatically propagate to other languages. The authors propose an adaptive unlearning scheme that employs language-dependent weights based on a multilingual teacher model's confidence, enabling selective forgetting across languages while maintaining overall model performance. Experiments demonstrate significant improvements in memorization accuracy and perplexity metrics across various languages compared to existing unlearning methods.

## Method Summary
The proposed method employs an adaptive unlearning scheme that assigns language-dependent weights to address varying language performances in multilingual models. A multilingual teacher model guides the unlearning process through knowledge distillation, with the student model learning from the teacher when confident and independently otherwise. The approach uses random sampling of languages during training for efficiency, achieving comparable performance to sequential unlearning. The method minimizes KL divergence between the original and unlearned models on retained data to ensure the forgotten samples appear as if they never existed, while maintaining the model's general capabilities.

## Key Results
- The proposed framework significantly outperforms existing unlearning methods on multilingual parallel datasets
- Memorization Accuracy (MA) and Perplexity (PPL) show substantial improvements across various languages
- The method achieves as much time-efficiency as unlearning a single language, compared to sequential unlearning approaches
- Cross-lingual unlearning effectiveness is demonstrated, showing that forgetting in one language can transfer to others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adaptive weighting scheme based on teacher confidence improves multilingual unlearning by allowing the student model to learn from the teacher when confident and independently otherwise.
- Mechanism: The method calculates a language-dependent weight (κ) based on the teacher's confidence in token sequences. When the teacher's confidence is high, the student learns from the teacher using KL divergence loss; when low, the student learns independently using language modeling loss.
- Core assumption: The teacher model's confidence accurately reflects its capability in a given language, and this confidence can be effectively used to determine whether to learn from the teacher or independently.
- Evidence anchors: [abstract] "our method employs an adaptive unlearning scheme that assigns language-dependent weights to address different language performances of multilingual language models"
- Break condition: If the teacher model's confidence is not a reliable indicator of its actual performance in a language, or if the adaptive weighting fails to properly balance learning from the teacher versus independent learning.

### Mechanism 2
- Claim: Random sampling of languages during training enables efficient multilingual unlearning comparable to single-language unlearning.
- Mechanism: Instead of unlearning one language at a time, the method randomly samples languages for token sequences in both forget and retain sets during training.
- Core assumption: Random sampling of languages during training provides sufficient exposure to all languages to achieve effective unlearning across the multilingual model.
- Evidence anchors: [section] "we employ Dz f and Dz r where z is randomly sampled from Z during training. We demonstrate in §5 that this approach achieves comparable performance to unlearning one language at a time, with significantly improved efficiency"
- Break condition: If random sampling fails to provide adequate representation of low-resource languages or if the sampling introduces bias that prevents effective unlearning in certain languages.

### Mechanism 3
- Claim: Maintaining distribution differences between forgotten and retained data ensures the model behaves as if forgotten data never existed.
- Mechanism: The method uses KL divergence to align the unlearned model's distribution on retained data with the original model's distribution.
- Core assumption: Minimizing KL divergence between the original and unlearned models on retained data effectively preserves the model's knowledge while allowing for selective forgetting of targeted information.
- Evidence anchors: [section] "This involves adjusting the model so that its performance on retained data aligns closely with the original model as if the forgotten samples never existed"
- Break condition: If the KL divergence minimization fails to adequately preserve the model's knowledge on retained data, or if it introduces unintended side effects that affect the model's general capabilities.

## Foundational Learning

- Concept: Cross-lingual transfer in multilingual language models
  - Why needed here: Understanding how knowledge transfers (or doesn't transfer) between languages is crucial for addressing the core problem that unlearning in one language doesn't automatically transfer to others
  - Quick check question: Why does unlearning in one language not automatically transfer to other languages in multilingual models?

- Concept: Knowledge distillation and teacher-student frameworks
  - Why needed here: The method relies on a multilingual teacher model to guide the unlearning process, requiring understanding of how teacher models can effectively transfer knowledge to student models
  - Quick check question: How does the teacher model's confidence in different languages affect the student model's learning process?

- Concept: Machine unlearning objectives and metrics
  - Why needed here: The method builds on existing unlearning techniques and uses specific metrics (MA, PPL) to evaluate success, requiring understanding of how these metrics work and what they measure
  - Quick check question: What is the difference between Memorization Accuracy (MA) and Perplexity (PPL) in evaluating unlearning performance?

## Architecture Onboarding

- Component map: Original multilingual language model (teacher) - frozen weights -> Student model - learns to forget targeted information -> Language-dependent weighting module - calculates κ based on teacher confidence -> Loss computation module - combines forgetting loss, retaining loss, and language modeling loss -> Sampling module - randomly selects languages for training

- Critical path: 1. Initialize student model with teacher weights 2. For each training step: - Sample language z from available languages - Compute teacher confidence κ for sampled language - Calculate adaptive loss combining forgetting and retaining components - Update student model parameters 3. Evaluate performance on forget set and test set

- Design tradeoffs: Adaptive vs. fixed weighting: Adaptive weighting allows for language-specific optimization but adds complexity; Random vs. sequential language sampling: Random sampling is more efficient but may introduce variance in learning; Teacher model selection: Using the same model as teacher provides consistency but may limit the range of knowledge that can be forgotten

- Failure signatures: High MA on forget set but degraded PPL on test set indicates over-aggressive unlearning; Low MA on forget set but also low performance on test set suggests insufficient retention; Language-specific failures where certain languages show high MA despite overall low performance may indicate sampling bias

- First 3 experiments: 1. Compare adaptive weighting with fixed κ values (0.1, 0.5, 0.9) to demonstrate the superiority of the adaptive approach 2. Test different language sampling strategies (random vs. sequential) to evaluate efficiency vs. effectiveness tradeoffs 3. Vary the number of samples to forget (32, 64, 96, 128) to assess scalability of the approach

## Open Questions the Paper Calls Out

None

## Limitations

- The empirical validation is limited to 10 languages from FLORES-200 and 9 from BLMAMA-53, which may not capture the full diversity of multilingual model performance
- The effectiveness of random language sampling across truly low-resource languages remains unverified
- The adaptive weighting mechanism's sensitivity to teacher model selection is not thoroughly explored

## Confidence

**High Confidence**: The adaptive unlearning framework's general approach and the use of KL divergence for distribution alignment are well-established techniques.

**Medium Confidence**: The claim that random language sampling achieves comparable performance to sequential unlearning while improving efficiency is supported by experimental results but lacks extensive ablation studies.

**Low Confidence**: The generalizability of the method to extremely low-resource languages and the robustness of the teacher confidence metric across different model architectures have not been sufficiently validated.

## Next Checks

1. **Cross-Architecture Teacher Validation**: Test the adaptive weighting scheme with different teacher models (e.g., mBERT, XLM-R) to verify that the confidence-based approach generalizes beyond the specific multilingual teacher used in the experiments.

2. **Low-Resource Language Stress Test**: Evaluate the method on languages with fewer than 1,000 training examples to assess whether random sampling adequately represents these languages and whether the adaptive weighting compensates for the teacher's reduced confidence.

3. **Long-Term Retention Analysis**: Conduct experiments tracking model performance over extended periods and across multiple fine-tuning tasks to verify that the unlearning remains effective and that retained knowledge is preserved without catastrophic forgetting.