---
ver: rpa2
title: 'Auto-PRE: An Automatic and Cost-Efficient Peer-Review Framework for Language
  Generation Evaluation'
arxiv_id: '2410.12265'
source_url: https://arxiv.org/abs/2410.12265
tags:
- llms
- evaluation
- arxiv
- gpt-4
- auto-pre
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes an automatic LLM evaluation framework, Auto-PRE,
  which is inspired by the peer review process in academia. Unlike previous methods,
  Auto-PRE selects evaluator LLMs automatically based on three core traits: consistency,
  pertinence, and self-confidence.'
---

# Auto-PRE: An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation

## Quick Facts
- **arXiv ID**: 2410.12265
- **Source URL**: https://arxiv.org/abs/2410.12265
- **Reference count**: 37
- **Primary result**: Achieves state-of-the-art LLM evaluation performance at lower cost using automated filtering of evaluator LLMs based on consistency, self-confidence, and pertinence traits.

## Executive Summary
Auto-PRE introduces an automated framework for selecting evaluator LLMs based on three core traits: consistency (filtering positional bias), self-confidence (filtering difficulty perception), and pertinence (filtering relevance vs. superficial quality bias). Unlike previous methods requiring human annotations, Auto-PRE automatically qualifies evaluators through an automated exam system. Extensive experiments on summary generation, question-answering, and dialogue tasks show superior performance compared to baselines like ChatEval and PRE, while being more cost-efficient due to its automated selection process.

## Method Summary
Auto-PRE automatically selects evaluator LLMs through three filtering stages: consistency filtering (testing positional bias by comparing rankings before/after answer position swaps), self-confidence filtering (testing difficulty perception by comparing confidence on easy vs. hard questions), and pertinence filtering (testing relevance discrimination by comparing relevant-but-low-quality vs. irrelevant-but-high-quality answers). The framework uses 100 instances from three datasets (Xsum, NF_CATS, DailyDialog) with 7 LLMs generating answers. Human annotations serve as ground truth for validation. The method requires no human annotations during evaluator selection, making it fully automated and cost-efficient.

## Key Results
- Achieves state-of-the-art accuracy in LLM evaluation across three tasks while reducing costs compared to manual annotation methods
- Consistency filtering removes positional bias, improving evaluator reliability by eliminating LLMs with position-dependent preferences
- Combining all three selection methods (consistency, self-confidence, pertinence) yields better results than using any single method individually

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency filtering improves evaluator reliability by removing LLMs with position-dependent preferences.
- Mechanism: Evaluators are tested by asking them to rank two answers, then swapping answer order and testing again. Only LLMs maintaining consistent rankings pass.
- Core assumption: Positional bias is a systematic flaw in LLMs that correlates with poor evaluation performance.
- Evidence anchors:
  - [abstract] "Unlike previous approaches that rely on human annotations, Auto-PRE automatically selects evaluator LLMs based on their inherent traits including consistency, self-confidence, and pertinence."
  - [section 3.1] "Suppose there are a question Q and two different answers Y1 and Y2. Then the input to the candidate LLM L should include the tuple (Q, Y1, Y2), and L is required to provide a preference relation T1 for Y1 and Y2. Subsequently, swap the positions of Y1 and Y2... If T1 and T2 are the same, the candidate L is considered to maintain consistent output."
  - [corpus] Weak evidence - no direct citations for positional bias correlation with evaluation quality.
- Break condition: If positional bias is task-dependent or varies by model architecture, consistency filtering may exclude otherwise competent evaluators.

### Mechanism 2
- Claim: Self-confidence filtering selects evaluators that can accurately gauge their own difficulty assessment.
- Mechanism: Evaluators face easy and hard questions with the same task format. Those showing higher confidence on easy questions pass.
- Core assumption: Self-confidence correlates with evaluation accuracy and task difficulty perception is objective.
- Evidence anchors:
  - [abstract] "self-confidence... correspond to the instruction, content, and response stages, respectively"
  - [section 3.2] "we can find the following three important traits: Consistency, Self-Confidence and Pertinence" and "we convert the probability of outputting a specific token into the uncertainty of the output"
  - [corpus] Weak evidence - no direct citations for self-confidence correlation with evaluation quality.
- Break condition: If confidence estimation methods are prompt-sensitive or confidence doesn't correlate with actual performance, filtering becomes unreliable.

### Mechanism 3
- Claim: Pertinence filtering removes evaluators biased by superficial answer qualities rather than relevance.
- Mechanism: Evaluators compare relevant-but-low-quality answers against irrelevant-but-high-quality answers. Those correctly preferring relevance pass.
- Core assumption: Ability to distinguish relevance from superficial quality correlates with evaluation accuracy.
- Evidence anchors:
  - [abstract] "pertinence... correspond to the instruction, content, and response stages, respectively"
  - [section 3.3] "we can select evaluator LLMs based on whether the candidate LLMs can distinguish between the pertinence of the answer to the given question and its superficial quality"
  - [section 3.3] "we generate answers that are highly pertinent to given questions but of lower superficial quality as RA: Relevant Answers and answers that are less pertinent but of higher superficial quality as IA"
  - [corpus] Weak evidence - no direct citations for pertinence correlation with evaluation quality.
- Break condition: If pertinence assessment varies by task domain or evaluator bias patterns differ from assumed, filtering becomes unreliable.

## Foundational Learning

- Concept: LLM evaluation bias patterns
  - Why needed here: Understanding how LLMs systematically prefer certain outputs (e.g., verbose answers, same-origin models) is crucial for designing effective filtering mechanisms.
  - Quick check question: What are the three main types of evaluation bias identified in LLM-as-judge research?

- Concept: Automatic qualification exams
  - Why needed here: The framework uses automated testing rather than human annotations to select evaluators, requiring understanding of how to design effective LLM-based exams.
  - Quick check question: How does Auto-PRE's qualification exam differ from PRE's human-annotated approach?

- Concept: Multi-trait evaluation
  - Why needed here: Auto-PRE combines consistency, self-confidence, and pertinence filtering - understanding how these traits interact is key to proper implementation.
  - Quick check question: Why might combining all three selection methods yield better results than using any single method?

## Architecture Onboarding

- Component map:
  - Question Generator -> Answer Generator -> Consistency Filter -> Confidence Filter -> Pertinence Filter -> Aggregator -> Cost Calculator

- Critical path:
  1. Generate question set
  2. Generate answer set using candidate LLMs
  3. Apply consistency filter
  4. Apply self-confidence filter
  5. Apply pertinence filter
  6. Aggregate passing evaluators' results
  7. Output final evaluation scores

- Design tradeoffs:
  - Open-source vs. closed-source LLMs: Open-source are free but may require more filtering; closed-source are costly but often more capable
  - Number of evaluators: More evaluators improve robustness but increase cost linearly
  - Prompt sensitivity: Self-confidence extraction is highly prompt-dependent, requiring careful prompt design
  - Filter thresholds: Stringent thresholds improve quality but reduce evaluator pool size

- Failure signatures:
  - High variance in evaluation results across different evaluator combinations
  - Systematic preference for certain answer characteristics (e.g., length, formatting)
  - Performance degradation when evaluator pool composition changes
  - Cost overruns due to excessive evaluator selection iterations

- First 3 experiments:
  1. Baseline consistency test: Run positional bias test on all candidate LLMs to establish baseline filtering effectiveness
  2. Confidence calibration: Test self-confidence extraction across different prompt strategies to find most reliable approach
  3. Pertinence discrimination: Evaluate pertinence filter performance with different L1/L2 LLM combinations to optimize selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Auto-PRE vary with different combinations of evaluator LLMs selected through the three traits (consistency, self-confidence, pertinence)?
- Basis in paper: [explicit] The paper discusses three selection methods based on these traits and their individual contributions to performance, but does not explore all possible combinations.
- Why unresolved: The paper only tests variants using each trait individually and all three together, leaving a gap in understanding the optimal combination.
- What evidence would resolve it: Conducting experiments with all possible combinations of the three selection methods and comparing their performance would provide insights into the optimal configuration.

### Open Question 2
- Question: Can the self-confidence selection method be generalized to tasks beyond open-ended questions, such as those with fixed outputs or multiple-choice questions?
- Basis in paper: [inferred] The paper focuses on open-ended questions and uses a specific task format for self-confidence selection, which may not be directly applicable to other task types.
- Why unresolved: The paper does not explore the applicability of the self-confidence method to different task formats, leaving its generalizability unclear.
- What evidence would resolve it: Testing the self-confidence selection method on a variety of task formats and evaluating its effectiveness would determine its generalizability.

### Open Question 3
- Question: How do different prompt strategies impact the performance of the pertinence selection method, and can a universal prompt strategy be identified?
- Basis in paper: [explicit] The paper discusses the impact of prompt strategies on the pertinence method and notes that smaller parameterized LLMs are more affected, but does not identify a universal strategy.
- Why unresolved: The paper does not provide a comprehensive analysis of all possible prompt strategies or determine if a single strategy works best across different LLMs and tasks.
- What evidence would resolve it: Experimenting with a wide range of prompt strategies and analyzing their effects on various LLMs and tasks would help identify a universal or optimal strategy.

## Limitations
- The three selection criteria lack strong empirical validation for correlation with evaluation quality
- Self-confidence extraction method is highly prompt-sensitive and may not generalize across different model architectures
- Performance may not generalize beyond the three tested tasks (summary generation, question-answering, dialogue)

## Confidence
- **High confidence**: Framework architecture and experimental methodology are well-specified; cost-efficiency claims are supported by concrete token usage calculations; superiority over baselines is demonstrated across multiple metrics
- **Medium confidence**: Effectiveness of individual selection methods is supported by experimental results, but theoretical justification for why these traits predict evaluation quality remains weak
- **Low confidence**: Claim that combining all three selection methods yields optimal results is based on ablation studies, but underlying reasons for observed improvements are not fully explained

## Next Checks
1. **Correlation validation study**: Conduct experiments to directly measure the correlation between consistency, self-confidence, pertinence scores and actual evaluation accuracy across different model families and tasks
2. **Prompt sensitivity analysis**: Systematically vary prompt strategies for self-confidence extraction and measure the impact on evaluator selection and overall performance to quantify prompt sensitivity
3. **Cross-task generalization test**: Apply Auto-PRE to at least two additional language generation tasks (e.g., code generation, creative writing) to validate the framework's generalizability beyond the three tested tasks