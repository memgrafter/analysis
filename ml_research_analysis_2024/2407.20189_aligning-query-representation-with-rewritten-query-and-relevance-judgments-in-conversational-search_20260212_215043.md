---
ver: rpa2
title: Aligning Query Representation with Rewritten Query and Relevance Judgments
  in Conversational Search
arxiv_id: '2407.20189'
source_url: https://arxiv.org/abs/2407.20189
tags:
- query
- conversational
- search
- representation
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of context-dependent query understanding
  in conversational search, where historical context is lengthy and noisy. The proposed
  method, QRACDR, aligns query representations by leveraging both rewritten queries
  and relevance judgments to train a better query encoder.
---

# Aligning Query Representation with Rewritten Query and Relevance Judgments in Conversational Search

## Quick Facts
- arXiv ID: 2407.20189
- Source URL: https://arxiv.org/abs/2407.20189
- Reference count: 40
- The proposed method, QRACDR, achieves 16.2% and 1.2% relative gains in NDCG@3 over LeCoRe on TopiOCQA and QReCC, respectively.

## Executive Summary
This paper addresses the challenge of context-dependent query understanding in conversational search, where historical context is lengthy and noisy. The proposed method, QRACDR, aligns query representations by leveraging both rewritten queries and relevance judgments to train a better query encoder. It uses Mean Square Error (MSE) loss to minimize the distance between session queries, rewritten queries, and relevant documents, while also incorporating negative samples. Evaluated on eight datasets across various conversational and ad-hoc search settings, QRACDR consistently outperforms state-of-the-art baselines.

## Method Summary
QRACDR fine-tunes an ANCE-based conversational dense retriever using MSE loss to align session query representations with rewritten queries and relevant documents. The method combines two MSE alignment losses (session query to rewritten query, and session query to relevant document) with optional contrastive learning using hard negatives. During training, the session query encoder is fine-tuned while rewritten query and document encoders remain frozen. The approach aims to push session query representations into an "aligned area" where they are closer to both rewritten queries and relevant documents in the embedding space.

## Key Results
- QRACDR achieves 16.2% and 1.2% relative gains in NDCG@3 over LeCoRe on TopiOCQA and QReCC, respectively
- The method consistently outperforms state-of-the-art baselines across eight datasets including TopiOCQA, QReCC, and CAsT-19/20/21
- Incorporating both rewritten queries and relevance judgments is more effective than using either alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query representation alignment pushes the session query representation closer to both rewritten query and relevant document representations in the embedding space.
- Mechanism: Two MSE losses create a "balanced anchor point" that the session query representation is pulled toward.
- Core assumption: There exists an "aligned area" in the representation space where session queries should fall, which is around the intersection of lines connecting rewritten query and relevant document representations.
- Evidence anchors:
  - [abstract] "The key idea is to align the query representation with those of rewritten queries and relevant documents."
  - [section 3.2.1] "An ideal conversational dense retriever is expected to encode the session query representation closer to its relevant documents"
  - [section 3.2.2] "We assume a hyper-sphere Sε with radius ε exists in the learned representation space"

### Mechanism 2
- Claim: Incorporating hard negatives with contrastive learning improves the model's ability to distinguish between relevant and irrelevant documents.
- Mechanism: MSE loss with hard negatives pushes session query representation away from negative samples, while contrastive learning provides adaptive gradient updates.
- Core assumption: The adaptive learning mechanism in contrastive learning is more effective than static MSE for handling negative samples.
- Evidence anchors:
  - [abstract] "The proposed model – Query Representation Alignment Conversational Dense Retriever, QRACDR, is tested on eight datasets, including various settings in conversational search and ad-hoc search."
  - [section 4.3] "To incorporate the hard negative, we further add one more MSE loss to move the session query representation away from the hard negative"
  - [section 4.3] "The coefficient could dynamically adjust the weight on d−, which enable the adaptive penalty on negative samples"

### Mechanism 3
- Claim: Combining rewritten query supervision with relevance judgments provides complementary information that improves conversational search performance.
- Mechanism: Rewritten queries provide explicit search intent that helps guide the model, while relevance judgments provide direct feedback on retrieval quality; together they create a more complete supervision signal.
- Core assumption: The explicit search intent in rewritten queries remains useful even when combined with relevance judgment signals.
- Evidence anchors:
  - [abstract] "we leverage both rewritten queries and relevance judgments in the conversational search data to train a better query representation model"
  - [section 3.2.1] "The explicit search intent contained in the rewritten query, which could be addressed by the initial ad-hoc search retriever, becomes a crucial feature to achieve the query representation alignment"
  - [section 6.3.1] "Removing any information leads to performance degradation, indicating the usefulness of training the model with each information"

## Foundational Learning

- Concept: Representation alignment in neural embedding spaces
  - Why needed here: The method relies on pushing query representations into specific regions of the embedding space relative to other representations
  - Quick check question: How does minimizing MSE between two representations affect their relative positions in the embedding space?

- Concept: Contrastive learning and hard negative mining
  - Why needed here: The method combines MSE-based alignment with contrastive learning that uses hard negatives for better discrimination
  - Quick check question: What is the difference between static and adaptive gradient updates when incorporating negative samples?

- Concept: Conversational query rewriting and its limitations
  - Why needed here: The method leverages rewritten queries as supervision, so understanding their role and limitations is crucial
  - Quick check question: Why might two-stage rewrite-then-search approaches be suboptimal compared to end-to-end methods?

## Architecture Onboarding

- Component map: Session query encoder (fine-tuned ANCE) -> MSE alignment losses -> Contrastive learning with hard negatives -> Fine-tuned model
- Critical path:
  1. Concatenate context and current query to form session query
  2. Encode session query, rewritten query, and relevant document
  3. Apply MSE losses for alignment between session query and both rewritten query/relevant document
  4. Apply contrastive learning with hard negatives
  5. Backpropagate through session query encoder only

- Design tradeoffs:
  - Using MSE vs. cosine similarity for alignment
  - Including hard negatives in MSE vs. only contrastive learning
  - Freezing rewritten query and document encoders vs. fine-tuning them
  - Training with only rewritten queries vs. both rewritten queries and relevance judgments

- Failure signatures:
  - Training loss decreases but retrieval performance doesn't improve (alignment may be pushing representations to wrong areas)
  - Model overfits to training data (too much reliance on specific rewritten queries)
  - Hard negatives don't improve performance (poor negative mining strategy)

- First 3 experiments:
  1. Train with only the base MSE alignment (MSE between session query and rewritten query + MSE between session query and relevant document) and compare to baseline Conv-ANCE
  2. Add hard negatives to the base alignment and evaluate impact on performance
  3. Combine MSE alignment with contrastive learning and test on both TopiOCQA and QReCC datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size and quality of the aligned area in the representation space vary with different dimensions of the embedding space?
- Basis in paper: [explicit] The paper defines the aligned area as a hyper-spherical cap and mentions that the probability of landing in this area decays exponentially with respect to the dimension of the representation space.
- Why unresolved: The paper provides a theoretical upper bound for the ratio of the measure of the aligned area to the sphere, but it does not provide empirical evidence or a detailed analysis of how the size and quality of the aligned area change with different embedding dimensions.
- What evidence would resolve it: Empirical studies showing the size and quality of the aligned area for different embedding dimensions, and how this affects the performance of the QRACDR model.

### Open Question 2
- Question: What is the impact of using different types of negative samples (e.g., hard negatives, in-batch negatives) on the performance of QRACDR?
- Basis in paper: [explicit] The paper mentions that incorporating negative samples with suitable mechanisms, such as adaptive negative learning in contrastive learning, helps achieve better performance. However, it does not provide a detailed analysis of the impact of different types of negative samples.
- Why unresolved: The paper does not provide a comprehensive comparison of the impact of different types of negative samples on the performance of QRACDR.
- What evidence would resolve it: Experimental results comparing the performance of QRACDR using different types of negative samples, such as hard negatives, in-batch negatives, and randomly sampled negatives.

### Open Question 3
- Question: How does the performance of QRACDR vary across different conversational search datasets with varying levels of complexity and topic-shift phenomena?
- Basis in paper: [explicit] The paper evaluates QRACDR on eight datasets, including TopiOCQA and QReCC, which have different levels of complexity and topic-shift phenomena. However, it does not provide a detailed analysis of how the performance of QRACDR varies across these datasets.
- Why unresolved: The paper does not provide a comprehensive analysis of the performance of QRACDR across different conversational search datasets with varying levels of complexity and topic-shift phenomena.
- What evidence would resolve it: Experimental results showing the performance of QRACDR on a wide range of conversational search datasets with varying levels of complexity and topic-shift phenomena, and a detailed analysis of how the performance varies across these datasets.

## Limitations
- The paper does not specify the exact negative sampling strategy used for hard negative mining, which could significantly impact the effectiveness of the contrastive learning component
- The weighting between different MSE loss terms is not clearly defined, making it difficult to reproduce the exact training dynamics
- While results show consistent improvements across eight datasets, the paper does not provide ablation studies on different dataset characteristics to understand when the alignment approach is most beneficial

## Confidence
- High confidence in the core mechanism of using MSE loss to align session query representations with rewritten queries and relevant documents
- Medium confidence in the contribution of contrastive learning with hard negatives, due to unspecified negative sampling details
- Medium confidence in the overall effectiveness claims, as the paper demonstrates improvements but lacks detailed analysis of failure cases or limitations

## Next Checks
1. **Ablation study on loss components**: Test the model with only rewritten query alignment, only relevance judgment alignment, and their combination to quantify the contribution of each supervision signal

2. **Analysis of negative mining quality**: Implement different negative sampling strategies (in-batch negatives, BM25 hard negatives, etc.) and evaluate their impact on retrieval performance to understand the importance of the negative mining component

3. **Robustness testing on noisy rewritten queries**: Evaluate the model's performance when rewritten queries contain errors or are completely unaligned with relevance judgments to understand the break condition for the alignment mechanism