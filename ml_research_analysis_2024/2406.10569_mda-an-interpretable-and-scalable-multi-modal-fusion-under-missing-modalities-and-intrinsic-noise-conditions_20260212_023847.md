---
ver: rpa2
title: 'MDA: An Interpretable and Scalable Multi-Modal Fusion under Missing Modalities
  and Intrinsic Noise Conditions'
arxiv_id: '2406.10569'
source_url: https://arxiv.org/abs/2406.10569
tags:
- modalities
- missing
- multi-modal
- modality
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MDA, a multi-modal fusion framework that
  addresses three key challenges: missing modalities, intrinsic noise, and interpretability
  in multi-modal learning. MDA employs a continuous attention mechanism that dynamically
  allocates weights to different modalities based on their relevance to the final
  decision, reducing attention to missing modalities, noisy data, or low-correlation
  information.'
---

# MDA: An Interpretable and Scalable Multi-Modal Fusion under Missing Modalities and Intrinsic Noise Conditions

## Quick Facts
- arXiv ID: 2406.10569
- Source URL: https://arxiv.org/abs/2406.10569
- Reference count: 40
- Primary result: MDA achieves state-of-the-art performance on multi-modal classification with interpretability, handling missing modalities and noise

## Executive Summary
This paper introduces MDA, a multi-modal fusion framework designed to address missing modalities, intrinsic noise, and interpretability challenges in multi-modal learning. MDA employs a continuous attention mechanism that dynamically allocates weights to different modalities based on their relevance to the final decision, effectively reducing attention to missing modalities, noisy data, or low-correlation information. The framework is evaluated on four datasets including a gastrointestinal disease dataset, Derm7pt, MM-IMDb, and avMNIST, achieving state-of-the-art performance across all tasks. MDA demonstrates robustness to missing modalities (up to 80%) and intrinsic noise (up to 20%), while providing interpretable insights into modality contributions that align with clinical diagnostic standards.

## Method Summary
MDA is a multi-modal fusion framework that uses continuous attention to dynamically weight modalities during fusion. The approach employs self-attention modules for each modality type (CNN for images, BERT for text, MFCCs+SA for audio) to enhance intra-modal feature quality. Continuous attention is then applied across modalities through recursive attention maps, allowing the model to scale efficiently to many modalities without combinatorial complexity. The framework is trained with cross-entropy loss and Adam optimizer, with uni-modal parameters frozen during multi-modal training. Missing modalities and intrinsic noise are simulated during training to improve robustness. The model outputs interpretable attention weights that reveal each modality's contribution to final predictions.

## Key Results
- Achieved 98.9% accuracy on the gastrointestinal disease dataset with interpretable attention weights
- Outperformed existing methods on Derm7pt with 85.7% accuracy for disease diagnosis
- Demonstrated robustness to missing modalities (maintaining high accuracy with up to 80% missing) and intrinsic noise (up to 20%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous attention dynamically reduces weights for missing modalities, low-correlation data, and noisy inputs.
- Mechanism: The MDA map ΘN−1i is computed recursively through dot-product attention, allowing each modality to integrate information from all others. When a modality is missing or noisy, its contribution to the attention map diminishes, effectively filtering it out of the final decision.
- Core assumption: Modalities are conditionally independent given the task, so attention from others can approximate the missing modality's signal.
- Evidence anchors:
  - [abstract] "MDA can reduce attention to low-correlation data, missing modalities, or modalities with inherent noise"
  - [section 2.4] "Θei = ReLU(Qi·(Ke)T/√d)·Ve,i for e=1, ReLU(Θe−1i·(Ke)T/√d)·Ve,i for 1<e≤N−1"
  - [corpus] Weak support; no direct mention of continuous attention in neighbor papers.
- Break condition: If modalities are strongly coupled, the recursive attention cannot properly approximate missing signals, leading to biased weights.

### Mechanism 2
- Claim: Self-attention on individual modalities before fusion improves intra-modal feature quality.
- Mechanism: Each modality's latent features are passed through a self-attention module (Eq. 2, Eq. 3, Eq. 4), allowing the model to reweight internal features based on their relevance before fusion.
- Core assumption: Internal redundancy or noise within a modality can be reduced by self-attention, improving the signal passed to cross-modal attention.
- Evidence anchors:
  - [section 2.1] "We introduce self-attention to enable deeper feature exploration, enhancing cross-modal fusion"
  - [section 2.2] "BERT learns contextual representations of words or subwords in a text by using a self-attention mechanism"
  - [corpus] No direct evidence in neighbors; assumption based on standard NLP/ML practice.
- Break condition: If self-attention is over-parameterized relative to the data, it may overfit and degrade generalization.

### Mechanism 3
- Claim: Linear attention relationships scale efficiently to many modalities without combinatorial complexity.
- Mechanism: MDA constructs attention maps incrementally (Θei), avoiding pairwise attention matrices between all modalities. This keeps complexity O(N) rather than O(N²).
- Core assumption: Sequential attention accumulation preserves enough context for fusion decisions.
- Evidence anchors:
  - [abstract] "MDA allows for infinite expansion without being limited to handling only two or three modalities"
  - [section 2.4] "MDA allows for infinite expansion without being limited to handling only two or three modalities, and the network complexity remains consistently at a low level"
  - [corpus] Weak; neighbor papers mention multi-modal but do not detail scalability mechanisms.
- Break condition: If modality interactions are highly non-linear, sequential attention may miss important cross-modal dependencies.

## Foundational Learning

- Concept: Attention mechanisms in neural networks
  - Why needed here: Core to understanding how MDA weights modalities dynamically
  - Quick check question: How does scaled dot-product attention differ from additive attention?

- Concept: Self-supervised learning via pre-trained models (BERT, CNNs)
  - Why needed here: MDA relies on frozen/unfrozen feature extractors for each modality
  - Quick check question: What is the difference between fine-tuning and feature extraction in transfer learning?

- Concept: Multi-task learning with shared representations
  - Why needed here: MDA trains separate uni-modal classifiers but fuses them; understanding joint training dynamics is key
  - Quick check question: How does freezing uni-modal parameters during multi-modal training affect gradient flow?

## Architecture Onboarding

- Component map:
  - Uni-modal feature extractors (CNN for images, BERT for text, MFCCs+SA for audio)
  - Self-attention modules per modality
  - Continuous attention fusion (MDA module)
  - Multi-modal classifier head
  - Loss functions: cross-entropy for each uni-modal task and the fused task

- Critical path:
  1. Forward pass through each modality extractor
  2. Self-attention applied per modality
  3. MDA computes attention weights across modalities
  4. Weighted fusion fed into multi-modal classifier
  5. Losses computed and backpropagated (uni-modal params frozen)

- Design tradeoffs:
  - Simplicity vs expressiveness: Linear attention vs full pairwise attention
  - Training stability: Freezing uni-modal encoders vs joint fine-tuning
  - Interpretability: MDA weights vs black-box fusion

- Failure signatures:
  - If one modality dominates weights regardless of input → attention module not learning
  - If performance drops sharply with missing modalities → attention not adapting
  - If training loss diverges → potential over-parameterization or learning rate issues

- First 3 experiments:
  1. Verify uni-modal encoders produce reasonable features on held-out validation set
  2. Test MDA with synthetic missing modalities (zero out features) and check attention redistribution
  3. Compare MDA weights vs ground-truth clinical priors on the gastrointestinal dataset to validate interpretability claims

## Open Questions the Paper Calls Out

- Question: How does the MDA framework perform when applied to modalities beyond the three types (image, text, audio) used in the study, particularly in scenarios with a significantly larger number of heterogeneous modalities?
  - Basis in paper: [explicit] The paper claims MDA is scalable to an arbitrary number of modalities, but only demonstrates results with three modalities.
  - Why unresolved: The scalability claim needs empirical validation with more than three modalities to confirm the framework's generalizability and performance.
  - What evidence would resolve it: Testing MDA on datasets with 4+ modalities and comparing its performance and computational efficiency against other multi-modal fusion methods.

- Question: What is the long-term impact of MDA's interpretability features on clinical decision-making, particularly for pathologies without established diagnostic criteria?
  - Basis in paper: [explicit] The paper claims MDA's interpretability aligns with clinical diagnostic standards and could serve as a reference for pathologies lacking established criteria, but does not provide long-term clinical validation.
  - Why unresolved: While initial results show alignment with clinical standards, the real-world impact on diagnostic accuracy and standardization over time is not addressed.
  - What evidence would resolve it: Longitudinal clinical studies comparing MDA-assisted diagnoses with traditional methods across various pathologies, particularly those without established criteria.

- Question: How does MDA's performance compare to human experts in diagnosing complex multi-modal medical cases, especially when dealing with intrinsic noise and missing modalities?
  - Basis in paper: [inferred] The paper demonstrates MDA's robustness to noise and missing data, but does not compare its diagnostic accuracy to human experts in challenging scenarios.
  - Why unresolved: The paper focuses on comparing MDA to other computational methods but does not establish a benchmark against human diagnostic performance.
  - What evidence would resolve it: Head-to-head comparisons between MDA and experienced clinicians on identical multi-modal medical cases with varying levels of noise and missing data.

## Limitations

- Scalability to many modalities remains untested beyond the three modalities demonstrated
- Lack of ablation studies isolating the contribution of self-attention versus continuous attention
- Clinical interpretability claims lack external validation from clinical experts

## Confidence

- High: MDA's effectiveness on the four evaluated datasets
- Medium: MDA's robustness to missing modalities and noise
- Low: Clinical interpretability claims without external validation

## Next Checks

1. Ablation study: Remove self-attention modules and test if continuous attention alone achieves similar performance
2. Scalability test: Evaluate MDA on datasets with 5+ modalities to verify O(N) complexity claims
3. External validation: Have clinicians review MDA attention weights on the gastrointestinal dataset to confirm clinical interpretability claims