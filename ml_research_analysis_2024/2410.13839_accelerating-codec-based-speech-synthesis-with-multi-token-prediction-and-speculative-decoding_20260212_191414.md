---
ver: rpa2
title: Accelerating Codec-based Speech Synthesis with Multi-Token Prediction and Speculative
  Decoding
arxiv_id: '2410.13839'
source_url: https://arxiv.org/abs/2410.13839
tags:
- speech
- tokens
- heads
- decoding
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accelerating codec-based speech
  synthesis systems while maintaining high speech quality. The core method involves
  predicting multiple tokens per inference step using multiple prediction heads and
  incorporating a Viterbi-based speculative decoding technique to select the optimal
  sequence of generated tokens.
---

# Accelerating Codec-based Speech Synthesis with Multi-Token Prediction and Speculative Decoding

## Quick Facts
- **arXiv ID**: 2410.13839
- **Source URL**: https://arxiv.org/abs/2410.13839
- **Reference count**: 40
- **Primary result**: 4-5x speedup in time-per-token for codec-based TTS with minimal quality degradation (8.7% WER for USLM model)

## Executive Summary
This paper introduces a method to accelerate codec-based speech synthesis systems by predicting multiple tokens per inference step using parallel prediction heads, combined with Viterbi-based speculative decoding to select the optimal token sequence. The approach achieves 4-5x reduction in time-per-token compared to baseline models while maintaining or even improving speech quality metrics like WER and MOS. The method addresses the fundamental trade-off between inference speed and quality in auto-regressive speech synthesis by amortizing the computational cost of self-attention across multiple tokens and using a probabilistic decoding algorithm to maintain coherence.

## Method Summary
The method trains auto-regressive (AR) and non-auto-regressive (NAR) modules separately on the LibriTTS dataset. The AR module uses multiple prediction heads to generate n tokens in parallel during inference, reducing time complexity from O(L) to O(L/n + α). A Viterbi-based speculative decoding algorithm then selects the optimal sequence from the parallel predictions by modeling token transitions as a Markov chain. The NAR module handles the remaining layers of codec tokens iteratively. The system uses SpeechTokenizer for audio token extraction and is trained for 20 epochs on a single A6000 GPU.

## Key Results
- Time-per-token reduced by 4-5x compared to baseline models
- USLM with speculative decoding achieved 8.7% WER (best among all models tested)
- Minimal quality trade-off or improvement in speech intelligibility
- Achieved fastest inference speed while maintaining competitive quality metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multiple prediction heads reduce inference time per token from O(L) to O(L/n + α)
- **Mechanism**: Parallel prediction of n tokens amortizes self-attention and feed-forward computation costs
- **Core assumption**: Overhead α from additional heads is small compared to savings from reducing inference steps
- **Evidence**: [abstract] linear reduction in synthesis time with more heads; [section III.B] complexity analysis O(L/n + α)
- **Break condition**: If α becomes comparable to L/n, speedup advantage disappears

### Mechanism 2
- **Claim**: Viterbi-based speculative decoding selects optimal token sequence from multiple heads
- **Mechanism**: Finds most probable sequence using transition probabilities between consecutive tokens
- **Core assumption**: Markov assumption (each token depends only on previous token) holds for speech
- **Evidence**: [abstract] Viterbi algorithm selects optimal sequence; [section III.C] Markov chain modeling
- **Break condition**: If Markov assumption violated or transition matrix poorly represents speech patterns

### Mechanism 3
- **Claim**: Multiple token prediction improves content consistency despite faster inference
- **Mechanism**: Simultaneous prediction captures broader context patterns improving accuracy
- **Core assumption**: Combined effect of multi-token prediction and Viterbi decoding improves quality metrics
- **Evidence**: [abstract] minimal quality trade-off or improvement; [section IV.B] USLM achieves 8.7% WER
- **Break condition**: If increasing heads beyond 4-5 degrades quality significantly or Viterbi overhead becomes prohibitive

## Foundational Learning

- **Concept**: Auto-regressive (AR) sequence generation
  - **Why needed**: Entire acceleration method builds on AR architecture where each token predicted from previous tokens
  - **Quick check**: What is time complexity of generating sequence of length L with standard AR inference?

- **Concept**: Transformer decoder architecture and self-attention
  - **Why needed**: Understanding how multiple heads share same self-attention computation across tokens
  - **Quick check**: How does computational complexity of self-attention scale with sequence length?

- **Concept**: Markov chains and Viterbi algorithm
  - **Why needed**: Speculative decoding relies on modeling token transitions as Markov process
  - **Quick check**: What is time complexity of Viterbi algorithm for finding most likely sequence of length T with V possible states?

## Architecture Onboarding

- **Component map**: Text prompt + acoustic prompt → AR module (multiple heads → Viterbi) → NAR module → Waveform
- **Critical path**: Text → AR module (multiple heads → Viterbi) → NAR module → Waveform
- **Design tradeoffs**:
  - Speed vs quality: More heads = faster inference but potentially lower quality without Viterbi
  - Memory vs speed: Larger transition matrices enable better Viterbi decoding but consume more memory
  - Simplicity vs performance: Separate AR/NAR modules vs unified architecture
- **Failure signatures**:
  - Degraded speech quality: Check if WER/CER metrics increasing with more heads
  - No speedup: Verify overhead α not dominating computation time
  - Viterbi computation too slow: Check if reduced transition matrix still too large for hardware
- **First 3 experiments**:
  1. Baseline measurement: Run standard AR inference on LibriTTS test-clean, record time-per-token, WER, MOS
  2. Single head multi-token: Implement 2-head multi-token prediction without Viterbi, compare speed/quality
  3. Full system with Viterbi: Add Viterbi decoding, evaluate quality metrics across different head counts (2, 4, 6, 8)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does computational overhead of Viterbi-based speculative decoding scale with increasing vocabulary size and number of prediction heads?
- **Basis**: Paper states complexity is O(n * m²) after reducing transition matrix dimensions
- **Why unresolved**: No empirical data on how overhead scales with varying vocabulary sizes and head counts
- **What evidence resolves it**: Empirical measurements of inference time and memory usage for different vocabulary sizes and head counts

### Open Question 2
- **Question**: What is impact of Viterbi-based speculative decoding on diversity of generated speech outputs?
- **Basis**: Paper mentions higher top-k values increase diversity but lower quality, suggesting trade-off
- **Why unresolved**: Paper doesn't explore how Viterbi algorithm influences output diversity
- **What evidence resolves it**: Analysis of diversity metrics (entropy, uniqueness) with and without Viterbi decoding

### Open Question 3
- **Question**: Can proposed method be effectively integrated with audio tokenizers other than SpeechTokenizer?
- **Basis**: Paper states exploring compatibility with more tokenizers is promising future work
- **Why unresolved**: Method only tested with SpeechTokenizer, no results for other tokenizers
- **What evidence resolves it**: Experimental results showing performance with other tokenizers like Encodec or WavTokenizer

## Limitations

- Architecture details for multi-head AR module not fully specified (dimensions, layers, attention mechanisms)
- Viterbi algorithm implementation specifics unclear (thresholds, complexity reduction methods)
- Quality improvements not statistically validated with significance testing
- Hardware dependency on A6000 GPUs makes results difficult to generalize

## Confidence

- **High Confidence**: Multi-token prediction with parallel heads reduces inference time by factor proportional to number of heads
- **Medium Confidence**: Viterbi-based speculative decoding maintains/improves speech quality while enabling multi-token prediction
- **Low Confidence**: Quality actually improves (not just maintains) with multi-token prediction plus Viterbi decoding

## Next Checks

1. Implement Viterbi decoding on small synthetic dataset with known transition probabilities to verify correct optimal sequence identification and measure computation time
2. Conduct ablation studies systematically varying number of prediction heads (2, 4, 6, 8) on held-out validation set, measuring WER, CER, and TPT for each configuration
3. Perform statistical significance testing on quality metrics (WER, MOS) comparing multi-token prediction with Viterbi decoding against baseline AR inference using paired t-tests or Wilcoxon signed-rank tests