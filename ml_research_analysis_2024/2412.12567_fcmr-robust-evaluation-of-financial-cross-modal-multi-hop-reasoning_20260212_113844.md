---
ver: rpa2
title: 'FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning'
arxiv_id: '2412.12567'
source_url: https://arxiv.org/abs/2412.12567
tags:
- reasoning
- table
- statement
- chart
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FCMR, a benchmark for evaluating cross-modal
  multi-hop reasoning in financial contexts. The authors address limitations in existing
  datasets like MMQA, including data contamination and insufficient three-hop reasoning
  cases.
---

# FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning

## Quick Facts
- **arXiv ID:** 2412.12567
- **Source URL:** https://arxiv.org/abs/2412.12567
- **Reference count:** 40
- **Primary result:** Introduces FCMR benchmark for cross-modal multi-hop reasoning in financial contexts; state-of-the-art models achieve only 30.4% accuracy on Hard-level tasks.

## Executive Summary
This paper introduces FCMR, a benchmark designed to evaluate cross-modal multi-hop reasoning in financial contexts. The benchmark addresses limitations in existing datasets like MMQA by using proprietary financial documents from SEC 10-K reports, reducing data contamination risk, and providing more three-hop reasoning cases. FCMR is categorized into Easy, Medium, and Hard difficulty levels based on the number of reasoning hops required. The authors also propose CMRGen, an automated framework for generating high-quality reasoning questions at significantly reduced costs compared to manual methods.

## Method Summary
The paper presents FCMR, a benchmark for cross-modal multi-hop reasoning using financial 10-K reports, tables, and charts. The CMRGen framework enables automated dataset generation through a pipeline involving input data construction from SEC EDGAR and WRDS Compustat, statement generation using LLMs, and paraphrasing with quality filtering. The benchmark categorizes problems into Easy (one-hop), Medium (two-hop), and Hard (three-hop) levels, with questions requiring integration across text, tables, and charts. Evaluation uses a zero-shot CoT setting with models like Claude 3.5 Sonnet, achieving 30.4% accuracy on Hard-level tasks.

## Key Results
- FCMR achieves 30.4% accuracy for state-of-the-art models on Hard-level three-hop reasoning tasks
- Information retrieval is identified as the critical bottleneck, with only 24.3% success rate for this stage
- Automated CMRGen framework reduces question generation cost from $0.33 to $0.004 per question while maintaining quality
- Models show 80.3% accuracy on Easy level but performance drops significantly with increased complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** FCMR reduces data contamination risk by sourcing data from SEC 10-K reports and financial statements rather than general web data.
- **Mechanism:** Using domain-specific, less commonly scraped data (financial reports) limits exposure to pretraining data, reducing memorization and ensuring models must perform actual reasoning rather than recall.
- **Core assumption:** Models trained on general web data have not extensively seen proprietary financial documents like SEC 10-K reports.
- **Evidence anchors:**
  - [abstract] "As FCMR is built using data sources from the financial domain, it is relatively free from the risk of data contamination."
  - [section 3.5] "To verify that our dataset avoids the contamination issue identified in MMQA, we replicate the contamination experiment under the same conditions… GPT-4o's performance approximates random guessing when charts are withheld."
- **Break condition:** If future models are pretrained on proprietary financial datasets, contamination risk would re-emerge.

### Mechanism 2
- **Claim:** Cross-modal three-hop reasoning tasks in FCMR require integration of text, table, and chart data, preventing single-modality shortcuts.
- **Mechanism:** Problems are designed so that all three modalities are necessary to answer correctly, with distractors based on real-world financial entities and structured difficulty levels.
- **Core assumption:** Multi-hop reasoning inherently requires combining evidence from multiple sources, and task design can prevent models from ignoring certain modalities.
- **Evidence anchors:**
  - [abstract] "problems at the Hard level require precise cross-modal three-hop reasoning and are designed to prevent the disregard of any modality."
  - [section 3.2] "We leverage GPT-4o-mini to generate text-based one-hop statements… we create table-based and chart-based one-hop statements… we combine these single-modal one-hop statements across entities to construct cross-modal two-hop statements, which are further merged to create cross-modal three-hop statements."
- **Break condition:** If models develop strategies to bypass certain modalities while still achieving correct answers, the benchmark's effectiveness diminishes.

### Mechanism 3
- **Claim:** The CMRGen framework enables automated, cost-effective dataset generation while maintaining high data quality through systematic paraphrasing and filtering.
- **Mechanism:** Automated pipeline reduces cost per question from $0.33 (MMQA) to $0.004, with two-stage paraphrasing and LLM-based filtering ensuring semantic accuracy and diversity.
- **Core assumption:** Automated generation with quality controls can match or exceed manual dataset creation in both efficiency and quality.
- **Evidence anchors:**
  - [section 3.1] "CMRGen distinguishes itself from other cross-modal multi-hop dataset generation methods with its highly automated and cost-effective pipeline. Notably, while generating a single question in MMQA incurs a cost of $0.33, our method reduces this to $0.004 per question."
  - [section 3.5] "We utilize Word Position Deviation (WPD) and Lexical Deviation (LD) metrics… to evaluate paraphrasing quality and compare these values with those from MRPC and PAWS, confirming the superiority of our paraphrasing method."
- **Break condition:** If automated generation produces repetitive patterns or fails to capture the complexity needed for genuine multi-hop reasoning.

## Foundational Learning

- **Concept:** Multi-hop reasoning across multiple modalities
  - **Why needed here:** FCMR evaluates models' ability to chain reasoning steps across text, tables, and charts, requiring understanding of how information in different formats relates
  - **Quick check question:** Given a financial statement and chart, can you identify what additional information from a text report would be needed to answer a question about company performance trends?

- **Concept:** Cross-modal information retrieval and integration
  - **Why needed here:** Models must locate relevant facts in each modality and combine them correctly; this involves understanding the strengths and limitations of different data representations
  - **Quick check question:** If a chart shows sales trends and a table shows revenue breakdown, what type of text information would you need to determine which product line contributed most to growth?

- **Concept:** Financial domain knowledge and terminology
  - **Why needed here:** FCMR uses real financial documents, requiring familiarity with concepts like balance sheets, income statements, and financial ratios to properly interpret the data
  - **Quick check question:** What is the difference between operating income and net income, and where would you find each in financial reports?

## Architecture Onboarding

- **Component map:** Text parser → Table parser → Chart parser → Planning module → Information retrieval → Information reasoning → Answer synthesis
- **Critical path:** Text → Table → Chart → Reasoning Chain → Answer
  - Models must successfully process all three modalities in sequence to arrive at correct answers
- **Design tradeoffs:**
  - Automated generation vs. manual curation (cost vs. quality)
  - Single-choice vs. multiple-choice format (simplicity vs. comprehensive reasoning assessment)
  - Open-ended vs. structured prompts (flexibility vs. consistency)
- **Failure signatures:**
  - Success on Easy level but failure on Medium/Hard suggests modality integration weaknesses
  - Consistent errors on chart interpretation indicate visual reasoning limitations
  - Pattern of skipping planning stages suggests reasoning strategy deficiencies
- **First 3 experiments:**
  1. Test baseline models on Easy level to establish modality processing capabilities
  2. Evaluate information retrieval accuracy by isolating each modality
  3. Measure cross-modal integration performance on Medium level problems

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of FCMR scale with the size of the dataset, particularly when using more than 101 companies or extending beyond the 5-year timeframe?
- **Basis in paper:** [inferred]
- **Why unresolved:** The paper only uses 101 companies over a 5-year period (2019-2023). It's unclear whether the reasoning challenges would persist or diminish with larger datasets covering longer timeframes or more diverse companies.
- **What evidence would resolve it:** Systematic experiments varying the number of companies and time periods, measuring model performance and error rates across these dimensions.

### Open Question 2
- **Question:** How do different chart types and visualization styles affect model performance on FCMR, particularly when comparing static charts versus interactive or dynamically generated visualizations?
- **Basis in paper:** [inferred]
- **Why unresolved:** While the paper uses four chart types with variations in fonts, colors, and libraries, it doesn't explore the impact of interactive elements, alternative visualization approaches, or more complex chart designs that might better reflect real-world financial analysis.
- **What evidence would resolve it:** Comparative experiments testing model performance across different visualization types, including interactive and complex multi-layered charts commonly used in financial analysis.

### Open Question 3
- **Question:** What is the relationship between model performance on FCMR and specific financial reasoning capabilities, such as understanding accounting principles, detecting anomalies in financial statements, or predicting financial trends?
- **Basis in paper:** [explicit]
- **Why unresolved:** The paper identifies that models struggle with information retrieval and reasoning but doesn't map these failures to specific financial reasoning skills or domain knowledge that could be explicitly trained or evaluated.
- **What evidence would resolve it:** Detailed error analysis correlating model failures with specific financial reasoning tasks, followed by targeted experiments testing whether financial domain training improves performance on those specific reasoning types.

## Limitations

- Data contamination risk remains despite using SEC 10-K reports, as financial news sites may discuss these documents extensively
- Automated generation quality concerns exist without extensive human evaluation comparing automated vs manual question quality
- Model performance attribution is incomplete, not fully distinguishing between retrieval failures, reasoning limitations, and modality processing issues

## Confidence

**High Confidence:** The dataset construction methodology and the fundamental claim that FCMR presents a challenging benchmark for cross-modal reasoning.

**Medium Confidence:** The contamination risk assessment and the claim that FCMR is relatively free from this issue.

**Low Confidence:** The automated generation quality claims and the specific attribution of model failures to information retrieval bottlenecks.

## Next Checks

1. **Contamination Verification:** Conduct comprehensive analysis of whether specific facts, entities, or numerical values from the 10-K reports appear in publicly available pretraining data sources, particularly financial news and business publications.

2. **Generation Quality Assessment:** Implement blind comparison study where human evaluators rate quality, complexity, and reasoning requirements of questions generated by CMRGen versus manually curated questions, without knowing their source.

3. **Modality-Specific Failure Analysis:** Design experiments that systematically isolate each modality's contribution to reasoning performance, using ablation studies to determine whether failures are truly due to retrieval issues or other factors like visual processing or arithmetic reasoning capabilities.