---
ver: rpa2
title: SnapE -- Training Snapshot Ensembles of Link Prediction Models
arxiv_id: '2408.02707'
source_url: https://arxiv.org/abs/2408.02707
tags:
- training
- prediction
- snape
- learning
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SnapE, a method for training snapshot ensembles
  of link prediction models in knowledge graphs. Unlike traditional ensemble methods
  that require training multiple models, SnapE trains an ensemble of models at the
  same cost as training a single model by using cyclic learning rates and storing
  snapshots of the model during training.
---

# SnapE -- Training Snapshot Ensembles of Link Prediction Models

## Quick Facts
- arXiv ID: 2408.02707
- Source URL: https://arxiv.org/abs/2408.02707
- Authors: Ali Shaban; Heiko Paulheim
- Reference count: 40
- Primary result: SnapE trains snapshot ensembles of link prediction models at the same cost as single models, achieving 10-200% gains in Hits@10 across four datasets.

## Executive Summary
This paper introduces SnapE, a method for training snapshot ensembles of link prediction models in knowledge graphs. Unlike traditional ensemble methods that require training multiple models, SnapE trains an ensemble of models at the same cost as training a single model by using cyclic learning rates and storing snapshots of the model during training. The paper proposes a novel negative sampling method that uses previous snapshots to generate adversarial samples for the next training cycle. Experiments on four datasets with four base models show that SnapE consistently outperforms single model approaches while maintaining the same training time and memory budget.

## Method Summary
SnapE trains a single model with cyclic learning rate schedules, storing snapshots at learning rate minima to create an ensemble. It introduces an extended negative sampling method that uses previous snapshots to generate adversarial examples for subsequent training cycles. The approach combines predictions from selected snapshots using weighted averaging, Borda rank aggregation, or simple averaging. The method claims to achieve ensemble performance gains within the same computational budget as single models by sharing training epochs across snapshots.

## Key Results
- SnapE consistently outperforms single model approaches across all tested datasets and models
- Performance gains range from 10-200% in Hits@10 depending on dataset and model
- Extended negative sampling yields better results than standard negative sampling in most cases
- Using a subset of snapshots often provides sufficient performance while reducing prediction time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cyclic learning rate schedules enable snapshot ensembles to capture diverse model behaviors without extra training time.
- Mechanism: By cycling the learning rate between a high value (enabling exploration) and a low value (allowing convergence to local minima), the model is saved at each low point. These snapshots collectively form an ensemble with varied decision boundaries.
- Core assumption: Diverse snapshots trained at different learning rate minima will make uncorrelated errors, improving ensemble robustness.
- Evidence anchors:
  - [abstract]: "They are known to yield more robust predictions by creating a set of diverse base models."
  - [section 3]: "Each time the learning rate schedule reaches a local minimum, a snapshot of the model is stored."
- Break condition: If the learning rate cycles too rapidly or too slowly, snapshots may become too similar or fail to converge, reducing ensemble diversity.

### Mechanism 2
- Claim: Using previous snapshot models to generate negative examples guides the ensemble to correct earlier mistakes, improving performance.
- Mechanism: After storing a snapshot, it is used to predict corrupted triples. The highest-scoring negatives (likely false positives) are added as adversarial examples for the next training cycle, focusing subsequent snapshots on correcting past errors.
- Core assumption: Models can benefit from focusing on mistakes made by previous snapshots, similar to boosting.
- Evidence anchors:
  - [section 3]: "The idea is similar to boosting [10], where an ensemble of models is trained in a sequence, with each model focusing on the mistakes made by the previous ones."
  - [section 3]: "The highest scoring negative< s, p, o′ > or < s ′, p, o > is then added as a negative example."
- Break condition: If snapshots are too similar or the negative sampling is ineffective, adversarial examples may not provide useful gradients for improvement.

### Mechanism 3
- Claim: SnapE achieves ensemble performance gains within the same computational budget as single models by sharing training epochs across snapshots.
- Mechanism: Instead of training m independent models for N epochs each (total mN epochs), SnapE trains one model for N epochs with m snapshots taken during cyclic learning rate phases, maintaining the same total epoch count.
- Core assumption: The diversity gained from cyclic training and snapshot storage compensates for not training fully independent models.
- Evidence anchors:
  - [abstract]: "Unlike traditional ensemble methods that require training multiple models, SnapE trains an ensemble of models at the same cost as training a single model."
  - [section 2]: "Unlike our approach, they train m models in N epochs each, which requires a total of N × m epochs, whereas we train m models in N epochs in total."
- Break condition: If snapshots do not capture sufficient diversity, the ensemble may not outperform a single well-trained model despite the shared training budget.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGEs) and Link Prediction
  - Why needed here: SnapE is a training method specifically for KGE models used in link prediction tasks on knowledge graphs.
  - Quick check question: What is the primary challenge in training KGE models for link prediction compared to standard supervised learning?

- Concept: Ensemble Methods and Diversity
  - Why needed here: The core benefit of SnapE relies on creating diverse base models whose errors are uncorrelated, improving overall prediction quality.
  - Quick check question: Why is model diversity critical for the success of ensemble methods in machine learning?

- Concept: Learning Rate Schedules and Optimization Dynamics
  - Why needed here: SnapE uses cyclic learning rate schedules to generate snapshots; understanding how learning rates affect convergence and exploration is essential.
  - Quick check question: How does a cyclic learning rate schedule differ from a standard decaying learning rate schedule in terms of model exploration?

## Architecture Onboarding

- Component map: Base KGE model -> Learning rate scheduler -> Negative sampler -> Snapshot storage -> Prediction combiner
- Critical path:
  1. Initialize base KGE model with chosen hyperparameters
  2. Apply cyclic learning rate schedule during training
  3. At each learning rate minimum, store a snapshot
  4. Use previous snapshots (if enabled) to generate adversarial negative samples
  5. After training, combine predictions from selected snapshots
- Design tradeoffs:
  - Snapshot frequency vs. model diversity: More snapshots can increase diversity but also storage and prediction time.
  - Extended vs. standard negative sampling: Extended sampling may improve performance but adds complexity and computational overhead.
  - Number of cycles and warmup: Deferred scheduling may improve snapshot quality but reduces the total number of snapshots.
- Failure signatures:
  - Low correlation between snapshots: Indicates insufficient diversity; ensemble may not outperform single models.
  - Prediction time much higher than baseline: Suggests too many snapshots are being used or combination method is inefficient.
  - No performance gain over baseline: Could indicate poor snapshot timing, ineffective negative sampling, or inappropriate learning rate schedule.
- First 3 experiments:
  1. Train a baseline single model (e.g., TransE with d=64) on a small dataset (e.g., DBpedia50) and record Hits@10, training time, and prediction time.
  2. Apply SnapE with standard negative sampling, CCA scheduler, and simple averaging on the same model and dataset; compare performance and timings.
  3. Repeat experiment 2 with extended negative sampling enabled; measure impact on Hits@10 and model correlation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of negative sampling strategy (standard vs. extended) impact the performance of SnapE across different knowledge graph datasets?
- Basis in paper: [explicit] The paper proposes an extended negative sampler that uses previous snapshots to generate adversarial samples, and an ablation study shows that the extended negative sampler yields better results in most cases.
- Why unresolved: The impact of the negative sampling strategy on performance is not fully explored, and the reasons for the observed differences are not fully explained.
- What evidence would resolve it: Additional experiments comparing the performance of SnapE with different negative sampling strategies on a wider range of knowledge graph datasets, and analysis of the generated negative samples to understand their impact on model training.

### Open Question 2
- Question: How does the selection of snapshots for the ensemble affect the prediction performance and computational efficiency of SnapE?
- Basis in paper: [explicit] The paper mentions that it is often sufficient or even beneficial to only use a subset of the snapshots, which in turn lowers the prediction time.
- Why unresolved: The optimal strategy for selecting snapshots for the ensemble is not fully explored, and the trade-offs between prediction performance and computational efficiency are not fully characterized.
- What evidence would resolve it: Experiments comparing the performance of SnapE with different snapshot selection strategies, and analysis of the computational costs associated with each strategy.

### Open Question 3
- Question: Can the SnapE framework be effectively applied to other knowledge graph embedding methods and downstream tasks beyond link prediction?
- Basis in paper: [explicit] The paper suggests that it would be interesting to see how the idea of snapshot ensembles can be carried over to different kinds of embedding and graph learning methods, like R-GCNs or walk-based methods, and other downstream tasks, such as node classification.
- Why unresolved: The applicability of SnapE to other knowledge graph embedding methods and downstream tasks is not fully explored, and the potential benefits and limitations are not fully characterized.
- What evidence would resolve it: Experiments applying SnapE to other knowledge graph embedding methods and downstream tasks, and analysis of the performance gains and computational costs compared to standard training methods.

## Limitations

- Experimental validation limited to four datasets and four base models, restricting generalizability
- Ablation study could be more comprehensive in isolating individual contribution of cyclic learning rates vs. adversarial negative sampling
- No evaluation on larger, more complex knowledge graphs to verify scalability claims

## Confidence

- Core SnapE mechanism (cyclic learning rates with snapshot storage): **High** - well-established from prior work
- Extended negative sampling contribution and performance benefits: **Medium** - impact not fully isolated from other factors
- Generalizability across knowledge graph structures and embedding architectures: **Low** - limited experimental coverage

## Next Checks

1. **Diversity Analysis**: Measure pairwise correlation coefficients between all snapshot models for each experiment. Values should consistently fall below 0.9 to validate the diversity claim that underpins ensemble effectiveness.

2. **Ablation on Negative Sampling**: Run experiments comparing standard vs. extended negative sampling while keeping all other factors constant (same snapshots, same combination method) to isolate the performance contribution of the adversarial sampling approach.

3. **Scalability Testing**: Evaluate SnapE on larger, more complex knowledge graphs (e.g., Wikidata5M or YAGO3-10) to verify that the computational efficiency claims hold at scale and that ensemble benefits persist with increased graph complexity.