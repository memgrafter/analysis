---
ver: rpa2
title: 'Time series classification with random convolution kernels: pooling operators
  and input representations matter'
arxiv_id: '2409.01115'
source_url: https://arxiv.org/abs/2409.01115
tags:
- self-rocket
- time
- number
- accuracy
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SelF-Rocket, a new approach for time series
  classification (TSC) based on MiniRocket. Unlike existing methods that use fixed
  pooling operators and input representations, SelF-Rocket dynamically selects the
  best combination during training through a wrapper-based feature selection module.
---

# Time series classification with random convolution kernels: pooling operators and input representations matter

## Quick Facts
- arXiv ID: 2409.01115
- Source URL: https://arxiv.org/abs/2409.01115
- Reference count: 26
- Primary result: Introduces SelF-Rocket, a dynamic feature selection method for time series classification that achieves state-of-the-art accuracy while using fewer features (9,996 or 19,992 vs 50,000) and offering faster prediction times

## Executive Summary
This paper introduces SelF-Rocket, a novel approach for time series classification that builds upon MiniRocket by dynamically selecting the optimal combination of input representations (raw time series, first-order difference, or both) and pooling operators (PPV, GMP, MPV, MIPV, LSPV) during training. Unlike existing methods that use fixed combinations, SelF-Rocket employs a wrapper-based feature selection module to identify the best IR-PO pair for each dataset through stratified cross-validation. The method is evaluated on the UCR TSC benchmark datasets and demonstrates state-of-the-art accuracy comparable to MultiRocket while using significantly fewer features and achieving faster prediction times.

## Method Summary
SelF-Rocket generates features using various combinations of input representations and pooling operators, then employs a wrapper-based feature selection module to identify the optimal combination. The selection process involves stratified k-fold cross-validation repeated multiple times, where the combination with the highest median accuracy is chosen. A vote validation system further ensures robustness by verifying that the selected combination is supported by a sufficient percentage of voters (stratified splits). The final classification uses a Ridge classifier with the selected features. The method is implemented in two versions: SelF-Rocket-MIX (9,996 kernels × 2 IR × 5 PO = 99,960 features) and SelF-Rocket-FULL (9,996 kernels × 2 IR × 5 PO = 199,920 features).

## Key Results
- SelF-Rocket achieves state-of-the-art accuracy on the UCR TSC benchmark datasets
- Uses significantly fewer features (9,996 or 19,992) compared to MultiRocket (50,000) while maintaining comparable accuracy
- Offers faster classification prediction times due to reduced feature dimensionality
- Vote validation system significantly improves performance, especially on small datasets
- The choice of number of runs (nr) is more important than the number of features (f) for performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic selection of input representations and pooling operators improves accuracy compared to fixed combinations.
- **Mechanism:** The algorithm trains multiple mini-classifiers on different feature sets generated from various combinations of input representations and pooling operators. A wrapper-based feature selection module then selects the combination yielding the highest median accuracy across stratified splits of the training data.
- **Core assumption:** The optimal combination of input representation and pooling operator varies across datasets, and this variation can be effectively captured through cross-validation on the training set.
- **Evidence anchors:**
  - [abstract] states the method "dynamically selects the best couple of input representations and pooling operator during the training process."
  - [section 4.2] describes using stratified k-fold repeated nr times to generate new train and validation sets and selecting the combination with the highest median accuracy.
  - [corpus] does not directly provide evidence for this mechanism.
- **Break condition:** If the training data is too small or not representative of the test distribution, the wrapper-based selection may choose suboptimal combinations.

### Mechanism 2
- **Claim:** Using fewer features (9,996 or 19,992) with dynamic selection achieves accuracy comparable to methods using 50,000 fixed features.
- **Mechanism:** Instead of generating a fixed large number of features like MultiRocket (50,000), SelF-Rocket generates a smaller set of features (9,996 kernels × up to 2 input representations × up to 5 pooling operators). The feature selection module identifies the most relevant subset for each dataset, reducing computational cost while maintaining accuracy.
- **Core assumption:** A smaller, carefully selected feature set can match or exceed the performance of a larger, fixed feature set.
- **Evidence anchors:**
  - [abstract] states SelF-Rocket "offers faster classification prediction times due to using fewer features (9,996 or 19,992 vs 50,000)."
  - [section 5.2] shows SelF-Rocket achieves "state-of-the-art accuracy" comparable to MultiRocket while using fewer features.
  - [corpus] does not directly provide evidence for this mechanism.
- **Break condition:** If the feature selection module fails to identify truly relevant features, the reduced feature set may lead to lower accuracy.

### Mechanism 3
- **Claim:** Vote validation improves robustness, especially for small datasets.
- **Mechanism:** After selecting the best IR-PO combination based on median accuracy, the algorithm validates this choice by checking if it appears in the top-ranked combinations for a sufficient percentage of voters (stratified splits). If not, a default combination (PPV MIX) is used instead.
- **Core assumption:** A combination that performs well across multiple stratified splits is more likely to generalize well to unseen data.
- **Evidence anchors:**
  - [section 4.2] describes the vote validation system that checks if the selected IR-PO combination is "sufficiently supported by the voters" and uses a default if not.
  - [section 5.3.5] shows vote validation improves performance, especially for small datasets.
  - [corpus] does not directly provide evidence for this mechanism.
- **Break condition:** If the threshold for validation is set too high or too low, the system may either reject good combinations or accept poor ones.

## Foundational Learning

- **Concept:** Time Series Classification (TSC) fundamentals
  - Why needed here: Understanding the problem domain and why random convolution kernels are effective for TSC is crucial for grasping SelF-Rocket's approach.
  - Quick check question: What is the main difference between distance-based methods (like DTW) and feature-based methods (like ROCKET) for TSC?

- **Concept:** Random convolution kernels and pooling operators
  - Why needed here: SelF-Rocket builds upon MiniRocket, which uses random convolution kernels with specific pooling operators. Understanding how these work is essential.
  - Quick check question: How does the PPV (Proportion of Positive Values) pooling operator transform the output of a convolution operation?

- **Concept:** Wrapper-based feature selection
  - Why needed here: The core innovation of SelF-Rocket is its wrapper-based feature selection module. Understanding how wrapper methods work is key to understanding the algorithm.
  - Quick check question: What is the main advantage of wrapper-based feature selection over filter-based methods?

## Architecture Onboarding

- **Component map:** Feature Generation → Feature Selection Module (stratified splits, median voting, validation) → Ridge Classification

- **Critical path:** Feature Generation → Feature Selection Module (stratified splits, median voting, validation) → Ridge Classification

- **Design tradeoffs:**
  - Fewer features (9,996/19,992) vs. more features (50,000): Faster prediction but requires effective selection
  - Dynamic selection vs. fixed combination: Potentially better accuracy but adds computational overhead for selection
  - Vote validation threshold: Balances robustness vs. flexibility

- **Failure signatures:**
  - Poor accuracy on small datasets: May indicate vote validation threshold too strict or insufficient representation diversity
  - Long training times: May indicate too many combinations being evaluated or insufficient parallel processing
  - Inconsistent results across runs: May indicate random initialization issues or unstable feature selection

- **First 3 experiments:**
  1. Run SelF-Rocket on a small dataset (e.g., ECG200) with default parameters and observe the selected IR-PO combination
  2. Compare accuracy and prediction time of SelF-Rocket vs. MiniRocket on a medium-sized dataset (e.g., FaceAll)
  3. Test the impact of vote validation by running with and without it on a small dataset and comparing accuracy consistency across multiple runs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of runs (nr) and number of features (f) for SelF-Rocket across different dataset sizes?
- Basis in paper: [explicit] The paper conducts a sensitivity analysis showing that the choice of nr is more important than f for performance, and that for small datasets, vote validation is crucial.
- Why unresolved: The analysis only considers specific values (nr from 1-15, f from 2500-9996) and doesn't provide a clear formula for optimal parameter selection based on dataset characteristics.
- What evidence would resolve it: Empirical studies varying nr and f across a broader range of dataset sizes and types, identifying clear patterns or guidelines for parameter selection.

### Open Question 2
- Question: How does SelF-Rocket's performance compare to state-of-the-art methods on non-UCR datasets, particularly those with missing values or variable time series lengths?
- Basis in paper: [explicit] The paper acknowledges that UCR datasets are pre-processed and of fixed length, and that the evaluation is limited to 112 datasets without missing values or variable lengths.
- Why unresolved: The paper only evaluates on UCR datasets, which may not represent real-world data challenges.
- What evidence would resolve it: Performance evaluation on diverse datasets including those with missing values, variable lengths, and different domain characteristics.

### Open Question 3
- Question: Can the feature selection process in SelF-Rocket be accelerated without sacrificing accuracy?
- Basis in paper: [explicit] The paper mentions that the feature selection phase takes 3.71 seconds on average for the MIX version and 11.12 seconds for the FULL version, and suggests using filter-based methods instead of wrapper-based methods.
- Why unresolved: While the paper suggests potential improvements, it doesn't provide empirical evidence of their effectiveness.
- What evidence would resolve it: Comparative studies of different feature selection methods (wrapper vs. filter) implemented in SelF-Rocket, measuring both speed and accuracy trade-offs.

## Limitations

- The evaluation is limited to UCR datasets, which are pre-processed and of fixed length, potentially limiting generalizability to real-world data with missing values or variable lengths
- The computational overhead of the feature selection module is not fully characterized, making it difficult to assess the true efficiency gains
- No ablation studies are provided to isolate the individual contributions of input representation selection, pooling operator selection, and vote validation to overall performance

## Confidence

- Mechanism 1 (Dynamic selection improves accuracy): Medium
- Mechanism 2 (Fewer features with selection achieves comparable accuracy): Medium
- Mechanism 3 (Vote validation improves robustness): Medium

## Next Checks

1. Conduct an ablation study comparing SelF-Rocket's accuracy when using fixed vs. dynamically selected IR-PO combinations on a diverse set of UCR datasets to quantify the contribution of dynamic selection.

2. Measure the training time overhead of SelF-Rocket's feature selection module across different dataset sizes to determine the scalability of the approach and whether the prediction speed benefits outweigh the training costs.

3. Test the robustness of vote validation by varying the validation threshold percentage and observing how this affects accuracy consistency across multiple runs, particularly on small datasets where the paper claims the most benefit.