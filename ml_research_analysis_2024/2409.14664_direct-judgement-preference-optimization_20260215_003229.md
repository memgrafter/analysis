---
ver: rpa2
title: Direct Judgement Preference Optimization
arxiv_id: '2409.14664'
source_url: https://arxiv.org/abs/2409.14664
tags:
- response
- judge
- instruction
- responses
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces SFR-Judges, a family of generative judge\
  \ models trained to perform diverse evaluation tasks like pairwise comparison, single\
  \ rating, and classification. The authors employ direct preference optimization\
  \ (DPO) with three types of preference pairs\u2014Chain-of-Thought Critique, Standard\
  \ Judgement, and Response Deduction\u2014to enhance the judges' reasoning, judgment\
  \ accuracy, and understanding of good/bad responses."
---

# Direct Judgement Preference Optimization

## Quick Facts
- arXiv ID: 2409.14664
- Source URL: https://arxiv.org/abs/2409.14664
- Reference count: 35
- SFR-Judges achieve state-of-the-art performance on 10 out of 13 benchmarks, outperforming strong baselines like GPT-4o and specialized judge models

## Executive Summary
This work introduces SFR-Judges, a family of generative judge models trained to perform diverse evaluation tasks including pairwise comparison, single rating, and classification. The authors employ direct preference optimization (DPO) with three types of preference pairs—Chain-of-Thought Critique, Standard Judgement, and Response Deduction—to enhance the judges' reasoning, judgment accuracy, and understanding of good/bad responses. SFR-Judges demonstrate robustness to inherent biases, flexibility in adapting to various evaluation protocols, and provide actionable feedback for improving downstream models.

## Method Summary
The authors develop SFR-Judges using a teacher-student framework where a strong teacher LLM (Llama-3.1-70B-Instruct) generates preference pairs for training. The student models are trained using a combination of supervised fine-tuning and direct preference optimization with three types of preference data: Chain-of-Thought critiques (680K pairs, 70% of total), Standard judgements (15%), and Response deduction (15%). The training pipeline involves prompting the teacher model to generate evaluations, categorizing them as positive/negative based on ground truth, constructing preference pairs, and training student models with combined SFT+DPO loss across multiple evaluation task formats.

## Key Results
- SFR-Judges achieve state-of-the-art performance on 10 out of 13 benchmarks
- Outperform strong baselines including GPT-4o and specialized judge models
- Demonstrate flexibility across single rating, pairwise comparison, and classification tasks
- Show robustness to inherent biases and provide actionable feedback for model improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference optimization enables the judge to learn from both positive and negative examples, improving its ability to distinguish correct from incorrect judgements
- Mechanism: By using DPO with preference pairs (yw > yl), the model learns to increase the likelihood of correct judgements while decreasing the likelihood of incorrect ones
- Core assumption: Negative examples with incorrect judgements are informative for training, not just harmful noise
- Evidence anchors: abstract statement about learning from positive and negative data; section 3.4 noting DPO's suitability when preferred response is not satisfactory
- Break condition: If negative examples are too noisy or the model cannot distinguish meaningful patterns in incorrect judgements

### Mechanism 2
- Claim: Chain-of-Thought critiques improve the judge's reasoning capability by exposing it to detailed analytical traces
- Mechanism: Including CoT critiques in preference pairs teaches the model to generate comprehensive reasoning traces before arriving at final judgements
- Core assumption: Longer reasoning traces with critiques contain valuable signal for improving judgement quality
- Evidence anchors: abstract mention of three approaches for improving generative judge; section 3.1 describing CoT critique inclusion; section 5.4 showing best performance with all three training types
- Break condition: If CoT traces become too verbose and dilute the final judgement signal

### Mechanism 3
- Claim: Response deduction task enhances the judge's understanding of what constitutes good/bad responses
- Mechanism: By training the judge to deduce original responses from critiques and judgements, the model develops a deeper understanding of the relationship between responses and evaluations
- Core assumption: Reverse reasoning tasks help models understand evaluation tasks "in hindsight"
- Evidence anchors: abstract statement about enhancing understanding of good/bad responses; section 3.3 describing Response Deduction task; section 5.4 showing its contribution to best performance
- Break condition: If deduced responses don't provide meaningful signal for improving evaluation capabilities

## Foundational Learning

- Concept: Preference optimization (DPO)
  - Why needed here: Standard supervised fine-tuning only learns from positive examples, missing the opportunity to explicitly learn what makes judgements incorrect
  - Quick check question: What is the key difference between DPO and standard supervised fine-tuning in terms of learning from positive vs. negative examples?

- Concept: Chain-of-thought reasoning
  - Why needed here: Evaluation tasks require complex reasoning about response quality, and CoT helps expose the intermediate reasoning steps that lead to judgements
  - Quick check question: How does including CoT critiques in training data potentially improve the model's ability to reason about response quality?

- Concept: Multi-task learning across different evaluation formats
  - Why needed here: Real-world evaluation requires flexibility across single rating, pairwise comparison, and classification tasks
  - Quick check question: Why is it beneficial to train a single judge model on multiple evaluation task formats rather than separate models for each task?

## Architecture Onboarding

- Component map: Teacher LLM (Llama-3.1-70B-Instruct) -> Student LLM (8B/12B/70B) -> Three training tasks (CoT Critique, Standard Judgement, Response Deduction) -> Combined SFT+DPO loss function

- Critical path: 1. Prompt teacher model to generate evaluations for training data 2. Categorize evaluations into positive/negative based on ground truth 3. Construct three types of preference pairs 4. Train student model with combined SFT + DPO loss 5. Evaluate on diverse benchmarks

- Design tradeoffs: Model size vs. performance (8B, 12B, 70B options); Training data diversity vs. focused expertise; Chain-of-thought detail vs. signal dilution; Prompt specificity vs. generalization

- Failure signatures: High consistency but low accuracy suggests positional bias issues; Low performance on single rating tasks suggests reasoning capability gaps; Poor transfer to unseen evaluation tasks suggests overfitting to training data

- First 3 experiments: 1. Compare performance with and without CoT critiques on a pairwise benchmark 2. Test bias mitigation by evaluating on EvalBiasBench with different prompting strategies 3. Measure performance drop when removing each training task (CoT, Standard, Deduction)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SFR-Judges compare to other large language models (LLMs) on specific evaluation tasks, such as single rating, pairwise comparison, and classification, across different benchmarks?
- Basis in paper: [explicit] The paper presents results comparing SFR-Judges to other LLMs like GPT-4o and specialized judge models on various benchmarks
- Why unresolved: The paper provides a general overview of the performance comparison but doesn't delve into specific task-by-task performance differences
- What evidence would resolve it: A detailed table or figure showing the performance of SFR-Judges and other models on each evaluation task and benchmark

### Open Question 2
- Question: What is the impact of each training task (CoT critique, standard judgement, response deduction) on the overall performance of SFR-Judges, and are there specific tasks or benchmarks where one training task is more beneficial than others?
- Basis in paper: [explicit] The paper mentions that all three training tasks contribute to creating well-rounded judge models, but doesn't provide a detailed analysis of their individual impact
- Why unresolved: The paper lacks a comprehensive ablation study to isolate the effects of each training task on different evaluation tasks
- What evidence would resolve it: A detailed analysis showing the performance of SFR-Judges when trained with different combinations of the three training tasks on each evaluation task and benchmark

### Open Question 3
- Question: How does the performance of SFR-Judges vary across different model sizes (8B, 12B, 70B parameters) on various evaluation tasks and benchmarks, and are there specific tasks or benchmarks where a smaller or larger model size is more advantageous?
- Basis in paper: [explicit] The paper mentions that SFR-Judges are available in three sizes, but doesn't provide a detailed comparison of their performance across different tasks and benchmarks
- Why unresolved: The paper lacks a comprehensive analysis of the performance differences between the different model sizes on each evaluation task and benchmark
- What evidence would resolve it: A detailed table or figure showing the performance of each SFR-Judge model size on each evaluation task and benchmark

## Limitations

- Data Quality Dependency: Effectiveness heavily relies on quality of preference pairs generated by teacher LLM, with no extensive analysis of teacher model reliability
- Generalization to Novel Domains: Training data composition and domain coverage remain underspecified, limiting validation of generalization to completely unseen evaluation tasks
- Bias Mitigation Verification: Methodology for bias detection and mitigation is not detailed, with insufficient quantitative analysis of improvements on EvalBiasBench

## Confidence

- High Confidence (90%+): SFR-Judges outperform baseline models on established benchmarks (10/13 benchmarks); three-pronged training approach contributes to performance gains; preference optimization with both positive and negative examples improves evaluation capabilities
- Medium Confidence (70-90%): Model's flexibility across different evaluation protocols; claims about robustness to inherent biases; effectiveness of Response Deduction task in enhancing understanding of good/bad responses
- Low Confidence (below 70%): Claims about providing "actionable feedback" for improving downstream models (not empirically validated); generalization to completely novel evaluation tasks or domains; long-term stability and performance consistency

## Next Checks

1. **Teacher Model Error Analysis**: Conduct systematic error analysis of teacher LLM's preference pair generation to quantify rate and types of errors being propagated to student models, including both qualitative analysis of common error patterns and quantitative metrics on generation accuracy.

2. **Zero-shot Transfer Evaluation**: Test SFR-Judges on completely novel evaluation tasks from domains not represented in training data (e.g., medical diagnosis evaluation, legal document quality assessment) to measure performance degradation and identify which components of training approach contribute most to generalization.

3. **Bias Type Isolation**: Conduct targeted experiments to identify specific types of biases (position bias, style bias, domain bias) and measure how each training component (CoT, Standard, Deduction) affects different bias types, including both quantitative metrics and qualitative analysis of bias manifestations in model outputs.