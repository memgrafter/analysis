---
ver: rpa2
title: Rehearsal-free Federated Domain-incremental Learning
arxiv_id: '2405.13900'
source_url: https://arxiv.org/abs/2405.13900
tags:
- learning
- task
- prompts
- domain
- reffil
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RefFiL, a rehearsal-free federated domain-incremental
  learning framework that mitigates catastrophic forgetting by leveraging global prompt
  sharing across heterogeneous domains. It employs a client-wise domain adaptive prompt
  generator to create instance-level, fine-grained prompts with domain adaptation
  knowledge, and a global prompt learning paradigm to distribute clustered local prompts
  globally.
---

# Rehearsal-free Federated Domain-incremental Learning

## Quick Facts
- arXiv ID: 2405.13900
- Source URL: https://arxiv.org/abs/2405.13900
- Authors: Rui Sun; Haoran Duan; Jiahua Dong; Varun Ojha; Tejal Shah; Rajiv Ranjan
- Reference count: 40
- Primary result: RefFiL achieves 93.81% average accuracy and 94.82% final accuracy on Digits-five, outperforming baselines

## Executive Summary
This paper introduces RefFiL, a rehearsal-free federated domain-incremental learning framework that addresses catastrophic forgetting in heterogeneous domain settings. The method leverages global prompt sharing across distributed participants while maintaining privacy through prompt-based knowledge transfer rather than raw data sharing. By combining client-wise domain adaptive prompt generation, global prompt clustering, and domain-specific contrastive learning with temperature decay, RefFiL effectively balances domain-invariant and domain-specific knowledge learning.

## Method Summary
RefFiL is a federated learning framework designed to handle domain-incremental learning without rehearsal. It uses a client-wise domain adaptive prompt generator to create instance-level prompts with domain adaptation knowledge, which are then clustered globally using FINCH to select representative prompts. The framework employs domain-specific prompt contrastive learning with temperature decay to enhance discrimination between related and unrelated prompts. Training occurs over 30 global rounds with 20 local epochs per client using federated averaging, with evaluation metrics focusing on average and final task accuracy across four benchmark datasets.

## Key Results
- Achieves 93.81% average accuracy and 94.82% final accuracy on Digits-five dataset
- Demonstrates 3.76% improvement in average accuracy and 3.91% in final accuracy on OfficeCaltech10 compared to EWC
- Shows consistent performance across different domain introduction orders
- Maintains effectiveness on complex FedDomainNet with 48 classes and high-resolution images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Global prompt clustering mitigates domain imbalance by aggregating domain-specific knowledge without losing discriminative features.
- Mechanism: The framework clusters local prompts domain-wise using FINCH, selecting representative prompts for each class based on semantic domain characteristics.
- Core assumption: Clustering preserves essential domain-characterized features while reducing noise from imbalanced prompt distributions.
- Evidence anchors: Abstract mentions domain-specific prompt contrastive learning; section describes global prompts clustering to filter representative prompts based on semantic characteristics.
- Break condition: If clustering fails to preserve discriminative features or if domain overlap is too high for FINCH to separate effectively.

### Mechanism 2
- Claim: Client-wise domain adaptive prompt generation creates instance-level fine-grained prompts with domain adaptation knowledge.
- Mechanism: The CDAP generator uses conditional input embeddings (task IDs) to produce scaling and shifting parameters that modulate instance-level prompts.
- Core assumption: Instance-level prompts with domain adaptation knowledge can effectively capture local domain characteristics while maintaining global distinctiveness.
- Evidence anchors: Abstract states method addresses learning domain-invariant knowledge with domain-specific prompts; section describes personalized instance-level prompts incorporating domain adaptation knowledge.
- Break condition: If conditional embedding fails to capture task-specific domain characteristics or if modulation parameters become too generic.

### Mechanism 3
- Claim: Domain-specific prompt contrastive learning with temperature decay enhances the model's ability to distinguish between semantically related and unrelated prompts.
- Mechanism: The contrastive loss encourages generation of prompts closely aligned with semantically similar prompts while being distinct from unrelated prompts, with temperature decay making the loss more stringent over time.
- Core assumption: Contrastive learning with adaptive temperature can effectively separate domain-specific features while maintaining semantic relationships.
- Evidence anchors: Abstract mentions domain-specific prompt contrastive learning loss differentiates between locally generated prompts and those from other domains; section introduces contrastive learning with temperature decay to improve prompt discrimination.
- Break condition: If temperature decay becomes too aggressive, causing overfitting to specific domains or if contrastive pairs are not well-defined.

## Foundational Learning

- Concept: Federated Learning
  - Why needed here: The framework operates in a federated setting where multiple participants collaboratively train a model while keeping data decentralized.
  - Quick check question: How does federated learning differ from centralized learning in terms of data privacy and communication overhead?

- Concept: Catastrophic Forgetting
  - Why needed here: The primary challenge addressed is preventing the model from forgetting previously learned knowledge when encountering new domains.
  - Quick check question: What are the main approaches to mitigate catastrophic forgetting in continual learning scenarios?

- Concept: Domain Adaptation
  - Why needed here: The framework needs to handle data from different domains with varying distributions while maintaining domain-invariant knowledge.
  - Quick check question: How does domain adaptation differ from domain generalization in terms of training data requirements?

## Architecture Onboarding

- Component map: Input image -> Feature Extractor -> Feature Map -> CDAP Generator (with Task ID) -> Local Prompts -> Global Prompt Clustering -> Clustered Global Prompts -> Contrastive Learning; Feature Map + Local Prompts -> Classification Loss

- Critical path:
  1. Input image → Feature Extractor → Feature Map
  2. Feature Map + Task ID → CDAP Generator → Local Prompts
  3. Local Prompts → Global Prompt Clustering → Clustered Global Prompts
  4. Feature Map + Global Prompts → Contrastive Learning
  5. Feature Map + Local Prompts → Classification Loss
  6. Aggregate losses → Model update

- Design tradeoffs:
  - Memory vs Performance: Using prompts instead of raw data reduces memory requirements but may lose some fine-grained information
  - Communication vs Privacy: Global prompt sharing enables knowledge transfer but requires careful privacy considerations
  - Complexity vs Effectiveness: More sophisticated prompt generation and clustering improves performance but increases computational overhead

- Failure signatures:
  - High variance in accuracy across different domains indicates poor domain adaptation
  - Gradual decrease in accuracy on previously seen domains indicates catastrophic forgetting
  - Inconsistent performance across different domain introduction orders suggests lack of robustness

- First 3 experiments:
  1. Test CDAP generator independently on synthetic domain-shifted data to verify prompt quality
  2. Evaluate global prompt clustering on balanced vs imbalanced domain distributions
  3. Measure contrastive learning effectiveness with varying temperature decay rates on domain-separated datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RefFiL's performance compare to baseline methods when handling highly complex datasets with diverse class distributions and high-resolution images?
- Basis in paper: [explicit] The paper mentions that RefFiL demonstrated improvement even in the challenging FedDomainNet dataset with sparse data distribution across 48 classes and high-resolution images.
- Why unresolved: The paper only provides a summary of results for FedDomainNet, without detailed performance metrics or comparisons to baseline methods for this specific dataset.
- What evidence would resolve it: Detailed performance metrics and comparisons of RefFiL and baseline methods on the FedDomainNet dataset, including accuracy, precision, and recall for each class.

### Open Question 2
- Question: How does RefFiL's reliance on client local task IDs as a reference impact its performance when task IDs are not uniquely managed globally or when clients are unaware of them?
- Basis in paper: [explicit] The paper acknowledges that RefFiL's dependency on client local task IDs as a reference can become a limitation when task IDs are not uniquely managed globally or when clients are unaware of them.
- Why unresolved: The paper does not explore the impact of this limitation on RefFiL's performance or discuss potential solutions to mitigate this issue.
- What evidence would resolve it: Experimental results demonstrating the impact of inconsistent or unknown task IDs on RefFiL's performance, along with proposed solutions or modifications to handle such scenarios.

### Open Question 3
- Question: How can RefFiL be extended to handle federated learning from streaming data that involves both new domains and new classes?
- Basis in paper: [inferred] The paper mentions that federated learning from streaming data presents the additional challenge of sequentially learning from both new domains and new classes, which is not addressed by RefFiL.
- Why unresolved: The paper does not propose any extensions or modifications to RefFiL to handle this scenario, nor does it discuss the potential challenges and trade-offs involved in such an extension.
- What evidence would resolve it: A proposed extension or modification to RefFiL that enables it to handle federated learning from streaming data with both new domains and new classes, along with experimental results demonstrating its effectiveness and efficiency compared to the original RefFiL.

## Limitations

- The paper lacks detailed implementation specifics for the CDAP generator's CCDA layer and FINCH clustering process, making exact reproduction challenging
- The effectiveness of global prompt sharing in heterogeneous federated settings with highly imbalanced domains is not thoroughly validated
- Limited ablation studies on the individual contributions of each component (CDAP, GPL, DPCL) to overall performance

## Confidence

- High confidence: The general framework design and experimental methodology are well-described
- Medium confidence: The theoretical mechanisms for preventing catastrophic forgetting through prompt-based approaches
- Low confidence: The specific implementation details of the clustering and prompt generation components

## Next Checks

1. Validate the CDAP generator's ability to produce domain-adaptive prompts by testing on controlled domain-shifted datasets with known characteristics
2. Verify the effectiveness of global prompt clustering by conducting experiments with varying degrees of domain imbalance and overlap
3. Test the temperature decay schedule in DPCL by measuring its impact on both domain discrimination and semantic preservation across different decay rates