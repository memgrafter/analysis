---
ver: rpa2
title: The 20 questions game to distinguish large language models
arxiv_id: '2409.10338'
source_url: https://arxiv.org/abs/2409.10338
tags:
- questions
- pairs
- llms
- same
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of distinguishing between large
  language models (LLMs) using only black-box access and a small set of binary questions,
  with applications in auditing and intellectual property protection. The authors
  formalize the problem as finding a set of k questions that maximizes the probability
  of correctly identifying whether two LLMs are the same or different.
---

# The 20 questions game to distinguish large language models

## Quick Facts
- arXiv ID: 2409.10338
- Source URL: https://arxiv.org/abs/2409.10338
- Reference count: 0
- Key outcome: Novel heuristics halve question count needed to distinguish LLMs with 95% accuracy

## Executive Summary
This paper addresses the challenge of distinguishing between large language models using only black-box access and a small set of binary questions. The authors formalize this as finding a set of k questions that maximizes the probability of correctly identifying whether two LLMs are the same or different. They introduce two novel heuristics - Separability and Recursive Similarity - that significantly outperform a random baseline, achieving 95% accuracy with only 3 questions compared to 6 for random selection. This work has important applications in auditing and intellectual property protection for LLMs.

## Method Summary
The authors formalize LLM distinguishability as finding a set of k binary questions that maximizes correct identification between models. They establish a random baseline using questions from benchmark datasets, achieving ~95% accuracy with 6 questions. Two novel heuristics are introduced: Separability selects questions that divide models into balanced groups, while Recursive Similarity iteratively chooses questions maximally dissimilar to previous selections. The approach is evaluated on 22 HuggingFace LLMs using 4 question datasets (hellaswag, MedMCQA, mmlu, piqa) with 20k questions each, demonstrating significant improvements in question efficiency.

## Key Results
- Random baseline achieves ~95% accuracy with 6 questions from benchmark datasets
- Separability heuristic reduces required questions to 3 while maintaining 95% accuracy
- Recursive Similarity heuristic further improves efficiency through iterative dissimilarity selection
- Both heuristics effectively halve the number of questions needed compared to random baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random baseline selection achieves nearly 100% accuracy within 20 questions because most questions are sufficiently discriminative.
- Mechanism: When two LLMs are different, they are likely to answer at least one question differently in a random sample of 20 questions drawn from diverse datasets.
- Core assumption: Models are deterministic and sufficiently distinct that they will differ on a non-trivial fraction of all possible inputs.
- Evidence anchors:
  - [abstract] "baseline using a random selection of questions from known benchmark datasets, achieving an accuracy of nearly 100% within 20 questions"
  - [section] "By randomly selecting questions uniformly across all datasets, we achieve an average accuracy of 95% with 6 questions"

### Mechanism 2
- Claim: The Separability heuristic halves the number of questions needed by selecting questions that split the model space into two nearly equal groups.
- Mechanism: By choosing questions that divide models into balanced partitions, each question maximizes the expected information gain about which model is being queried.
- Core assumption: There exist questions in the benchmark datasets that naturally partition the model space in roughly balanced ways.
- Evidence anchors:
  - [section] "Separability of X ⊂ M: ∆M(X) = N( |M| − |X| − |M\X| )"
  - [section] "The Separability heuristic Asep as follows: Asep(M, Q) = q → U([0,1]) if q ∈ arg maxq∈Q ∆M(Mq) 0 else"

### Mechanism 3
- Claim: The Recursive Similarity heuristic further improves efficiency by selecting questions that are maximally dissimilar to previously chosen ones.
- Mechanism: After selecting highly separable questions, subsequent questions are chosen to minimize similarity with existing partitions, ensuring each new question provides novel information about model differences.
- Core assumption: Questions can be meaningfully compared for similarity based on how they partition the model space, and there exists diversity in the question space.
- Evidence anchors:
  - [section] "what makes it a good question set? Sampling from questions with maximum separability makes it possible for two questions to obtain the same partitions"
  - [section] "Algorithm 2 Asim: The Recursive Similarity Heuristic" with its recursive selection process

## Foundational Learning

- Concept: Binary classification with black-box access
  - Why needed here: The problem requires distinguishing between two unknown models using only their outputs to prompts, without access to internal parameters or training data
  - Quick check question: If you have two black-box functions and can only observe their outputs to your inputs, what is the minimum number of queries needed to guarantee distinguishing them if they differ on at least one input?

- Concept: Information theory and entropy maximization
  - Why needed here: The heuristics work by selecting questions that maximize information gain about model identity, which is fundamentally an entropy maximization problem
  - Quick check question: If you have a set of binary questions and want to distinguish between N possible states, what property should your questions have to minimize the number needed?

- Concept: Monte Carlo approximation for empirical probabilities
  - Why needed here: Since exact computation of model distinguishability would require checking all possible model pairs, the paper uses sampling to estimate the probability of correct discrimination
  - Quick check question: If you want to estimate the probability that two random models differ on a given question, and you have 22 models to test, how would you approximate this using pairwise comparisons?

## Architecture Onboarding

- Component map:
  - Question selection module (baseline vs heuristics)
  - Model evaluation engine (querying and recording responses)
  - Statistical analysis component (computing accuracy metrics)
  - Visualization layer (CDF plots and performance comparison)

- Critical path:
  1. Load question datasets and model collection
  2. For each heuristic: compute question scores and sort
  3. For each model pair: query sequentially until models differ
  4. Record number of questions needed and update CDF
  5. Compute accuracy metrics and generate visualizations

- Design tradeoffs:
  - Exhaustive search vs heuristic selection: Exact optimal question sets are intractable for large question pools, necessitating heuristics
  - Question diversity vs purity: More diverse question sources improve generalization but may introduce noise
  - Computational cost vs accuracy: More sophisticated heuristics provide better accuracy but require more computation

- Failure signatures:
  - Heuristic consistently selecting same questions across runs (indicates lack of diversity in question space)
  - CDF plateauing early (models are too similar to distinguish with available questions)
  - Accuracy dropping significantly on held-out model pairs (overfitting to specific model collection)

- First 3 experiments:
  1. Implement random baseline with single question dataset (e.g., hellaswag) and verify ~95% accuracy with 6 questions
  2. Implement Separability heuristic and compare performance against random baseline on same dataset
  3. Test Recursive Similarity heuristic and measure improvement over Separability alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the distinguishability approach scale to much larger sets of LLMs (e.g., thousands of models)?
- Basis in paper: [inferred] The authors mention "a generalization to wider sets of models" as future work, and only tested with 22 models.
- Why unresolved: The paper only tested with 22 models, and the optimal number of questions scales with the number of models. For much larger sets, the approach may become computationally infeasible or require different heuristics.
- What evidence would resolve it: Experimental results showing the performance of the approach on significantly larger sets of LLMs, and analysis of computational complexity.

### Open Question 2
- Question: How robust are the questioning heuristics against non-deterministic LLMs that may produce different outputs for the same input?
- Basis in paper: [explicit] The authors mention "investigating the robustness of our approach with non-deterministic models" as future work.
- Why unresolved: The paper assumes deterministic models, but many real-world LLMs have some degree of randomness in their outputs. The heuristics may not work as well if models don't always give the same answer to a question.
- What evidence would resolve it: Experiments testing the approach on non-deterministic models, and analysis of how the heuristics perform when models produce variable outputs.

### Open Question 3
- Question: Can the approach be extended to distinguish between models that are intentionally designed to be very similar (e.g., same architecture but different training data)?
- Basis in paper: [inferred] The authors suggest investigating "differentiating models that are by construction close to each other" as an interesting angle, implying this is an open challenge.
- Why unresolved: The current approach may not be able to reliably distinguish between models that are very similar in architecture and training, as they may give similar answers to most questions.
- What evidence would resolve it: Experiments showing the approach can distinguish between closely related models, and development of new heuristics specifically for this case.

## Limitations

- The deterministic assumption about LLM behavior may not hold for models with temperature-based variability
- The evaluation is based on a specific set of 22 models, limiting generalizability to broader LLM landscape
- The approach doesn't address potential adversarial questions designed to fool the distinction system

## Confidence

**High Confidence**: The baseline random selection achieving ~95% accuracy with 6 questions and the overall effectiveness of both heuristics in reducing question count while maintaining accuracy.

**Medium Confidence**: The generalizability of the heuristics across different model collections and question domains.

**Low Confidence**: The assumption that all LLMs can be meaningfully distinguished using only 20 binary questions for highly similar models.

## Next Checks

1. **Stochastic Output Testing**: Run each model multiple times with the same questions to measure response consistency and quantify the impact of non-deterministic behavior on distinguishability accuracy.

2. **Cross-Domain Generalization**: Apply the heuristics to a different domain (e.g., legal reasoning or creative writing) with a new set of 20+ models to test whether the ~3 question requirement holds across diverse applications.

3. **Adversarial Question Generation**: Develop a method to generate questions specifically designed to maximize similarity between different models, then test whether the heuristics can still achieve high accuracy when faced with such optimized queries.