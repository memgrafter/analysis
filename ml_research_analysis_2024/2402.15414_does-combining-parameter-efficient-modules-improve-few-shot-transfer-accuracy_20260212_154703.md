---
ver: rpa2
title: Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?
arxiv_id: '2402.15414'
source_url: https://arxiv.org/abs/2402.15414
tags:
- composition
- lora
- learned
- upstream
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether combining pre-trained LoRA modules
  can enhance transfer accuracy on new tasks. The authors explore two approaches:
  uniform composition, which averages upstream LoRA modules with equal weights, and
  learned composition, which learns weights for each upstream module and performs
  weighted averaging.'
---

# Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?

## Quick Facts
- arXiv ID: 2402.15414
- Source URL: https://arxiv.org/abs/2402.15414
- Reference count: 40
- One-line primary result: Combining pre-trained LoRA modules via uniform or learned composition improves few-shot transfer accuracy compared to full fine-tuning or training LoRA from scratch

## Executive Summary
This paper investigates whether combining pre-trained LoRA (Low-Rank Adaptation) modules can enhance transfer accuracy on new tasks in few-shot settings. The authors propose two approaches: uniform composition (equal-weight averaging of LoRA modules) and learned composition (learning weights for each upstream module and performing weighted averaging). Experiments on both vision and language models demonstrate that in few-shot settings, both composition methods outperform full fine-tuning and training LoRA from scratch. In full-shot settings, learned composition achieves comparable performance to regular LoRA training while using significantly fewer trainable parameters.

## Method Summary
The authors employ a three-phase approach: (1) pre-train a foundation model (ViT-base or Flan-T5), (2) fine-tune LoRA adapters on diverse upstream tasks from multiple vision and NLP datasets, and (3) adapt to downstream tasks using either uniform composition (averaging LoRA modules equally) or learned composition (learning softmax-weighted combinations of LoRA modules via gradient descent). The experiments evaluate transfer accuracy in few-shot settings (1-32 samples per class) and compare against baselines including full fine-tuning, training LoRA from scratch, and model merging techniques.

## Key Results
- Both uniform and learned composition methods outperform full fine-tuning and training LoRA from scratch in few-shot settings
- Learned composition performs comparably to regular LoRA training in full-shot settings with significantly fewer trainable parameters
- Performance improves as the number of diverse upstream LoRA modules increases
- Uniform composition provides parameter-free improvements in low-shot settings without overfitting risks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining LoRA modules improves generalization by interpolating complementary features learned on different upstream tasks
- Mechanism: Each LoRA module captures task-specific low-rank adaptations. Averaging or learning weights for these modules effectively combines complementary features, allowing the model to leverage diverse knowledge without overfitting to limited downstream data
- Core assumption: Upstream tasks are sufficiently diverse so their learned adaptations capture different, complementary features relevant to unseen tasks
- Evidence anchors: Abstract results showing composition methods outperform baselines; correlation between number of upstream modules and performance improvement
- Break condition: If upstream tasks are too similar, the combined LoRA modules provide redundant information, offering minimal or no improvement over single LoRA training

### Mechanism 2
- Claim: Learned composition improves performance by adaptively selecting the most relevant upstream LoRA modules for the downstream task
- Mechanism: The learned composition learns a weighting vector for each upstream LoRA module, dynamically assigning higher weights to upstream modules whose learned adaptations are most relevant to the downstream task
- Core assumption: The learned composition can effectively identify and prioritize relevant upstream modules based on limited downstream data
- Evidence anchors: Abstract results showing learned composition outperforms uniform in many cases; visualization of learned weights showing preference for certain upstream modules
- Break condition: If the learned composition overfits to the limited downstream data, it may assign inappropriate weights, leading to worse performance than uniform composition

### Mechanism 3
- Claim: Uniform composition improves performance by providing a stable, parameter-free baseline that still leverages diverse upstream knowledge
- Mechanism: Averaging LoRA modules with equal weights provides a simple way to combine diverse upstream knowledge without introducing any additional learnable parameters, particularly beneficial in extremely low-data regimes
- Core assumption: Averaging LoRA modules with equal weights is a reasonable approximation for combining diverse upstream knowledge, even without task-specific weighting
- Evidence anchors: Abstract highlighting uniform composition's parameter-free nature; results showing uniform composition effectiveness in low-shot settings
- Break condition: If upstream tasks are too diverse or irrelevant to the downstream task, uniform averaging might dilute useful information, leading to worse performance than using a single, highly relevant upstream LoRA module

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA is the fundamental technique used to create the parameter-efficient modules that are combined in this work
  - Quick check question: How does LoRA achieve parameter efficiency compared to full fine-tuning?

- Concept: Few-shot learning
  - Why needed here: The paper specifically focuses on the few-shot setting, where only a limited number of samples are available for the downstream task
  - Quick check question: What are the key challenges of few-shot learning, and how do they motivate the use of parameter-efficient fine-tuning techniques like LoRA?

- Concept: Transfer learning
  - Why needed here: The paper investigates the transferability of knowledge from upstream tasks to unseen downstream tasks through LoRA composition
  - Quick check question: What are the different types of transfer learning (e.g., inductive, transductive, unsupervised), and how do they relate to the few-shot transfer setting studied in this paper?

## Architecture Onboarding

- Component map: Pre-trained foundation model (Θ0) -> LoRA modules for upstream tasks (∆W1, ..., ∆WN) -> Composition methods (uniform, learned) -> Downstream task with limited samples (SK)
- Critical path: 1. Pre-train foundation model, 2. Fine-tune LoRA adapters on diverse upstream tasks, 3. Combine LoRA modules using uniform or learned composition, 4. Evaluate on few-shot downstream task
- Design tradeoffs: Uniform composition offers simplicity and parameter-free operation but less adaptability; learned composition provides potentially better performance but requires additional parameters and training
- Failure signatures: Uniform composition underperforms single LoRA when upstream tasks are too similar; learned composition underperforms uniform when overfitting to limited data; both underperform when upstream tasks are too diverse
- First 3 experiments: 1. Replicate label shift experiment on Split-CIFAR100, 2. Test effect of varying number of upstream tasks on performance, 3. Analyze learned composition weights to understand module selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the learned composition method's performance compare to model merging techniques that use Fisher-weighted averaging or Task Vectors?
- Basis in paper: The paper compares learned composition to other LoRA-based methods but does not compare it to model merging techniques that use Fisher-weighted averaging or Task Vectors
- Why unresolved: The paper focuses on the composability of LoRA modules and does not explore the relationship between learned composition and other model merging techniques
- What evidence would resolve it: Experimental results comparing learned composition to model merging techniques using Fisher-weighted averaging or Task Vectors on the same downstream tasks

### Open Question 2
- Question: How does the learned composition method's performance change when using different rank values for the low-rank decomposition matrix in LoRA?
- Basis in paper: The paper mentions that the rank of the low-rank decomposition matrix is set to 16 for all experiments, but it does not explore the effect of using different rank values
- Why unresolved: The paper does not investigate the impact of varying the rank on the learned composition method's performance
- What evidence would resolve it: Experimental results showing performance of learned composition using different rank values for the low-rank decomposition matrix in LoRA

### Open Question 3
- Question: How does the learned composition method's performance change when using different pre-trained foundation models as the base model?
- Basis in paper: The paper uses ViT-base for vision experiments and Flan-T5 large for NLP experiments, but it does not explore the effect of using different pre-trained foundation models
- Why unresolved: The paper focuses on the composability of LoRA modules and does not investigate the impact of using different pre-trained foundation models as the base model
- What evidence would resolve it: Experimental results showing performance of learned composition using different pre-trained foundation models as the base model for the same downstream tasks

## Limitations

- The evaluation focuses primarily on vision tasks with limited NLP experiments, leaving uncertainty about generalizability across domains
- The computational overhead of learned composition training is not thoroughly analyzed, particularly regarding memory requirements when loading multiple LoRA modules
- The theoretical explanations for why composition works are largely speculative and not directly supported by experimental evidence

## Confidence

**High Confidence:** Core experimental results showing both composition methods outperform baselines in few-shot settings are well-supported by presented data across multiple benchmarks

**Medium Confidence:** Claim that learned composition performs comparably to regular LoRA training in full-shot settings with fewer trainable parameters is supported but requires more extensive validation, especially for NLP tasks

**Low Confidence:** Theoretical explanations for why composition works (e.g., "interpolating complementary features") are largely speculative and not directly supported by experimental evidence

## Next Checks

1. **Architecture Generalization Test:** Validate composition methods across different model architectures (e.g., ResNet, CLIP, BERT) and scales to determine whether benefits generalize beyond ViT-base and Flan-T5-base models

2. **Upstream Task Diversity Analysis:** Systematically vary the similarity and diversity of upstream tasks to test break conditions identified in mechanisms section through controlled experiments with increasingly similar or dissimilar upstream tasks

3. **Computational Efficiency Benchmark:** Conduct detailed analysis of computational costs (training time, memory usage, number of training steps) for learned composition versus single LoRA training, including profiling during learned composition weight optimization phase