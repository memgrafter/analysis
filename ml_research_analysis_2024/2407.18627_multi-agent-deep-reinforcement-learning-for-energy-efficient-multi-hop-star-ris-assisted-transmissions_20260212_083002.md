---
ver: rpa2
title: Multi-Agent Deep Reinforcement Learning for Energy Efficient Multi-Hop STAR-RIS-Assisted
  Transmissions
arxiv_id: '2407.18627'
source_url: https://arxiv.org/abs/2407.18627
tags:
- star-ris
- agent
- star-riss
- global
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-hop STAR-RIS architecture to maximize
  energy efficiency in wireless networks. It jointly optimizes active beamforming
  at the base station and passive beamforming at STAR-RIS elements, considering their
  on-off states.
---

# Multi-Agent Deep Reinforcement Learning for Energy Efficient Multi-Hop STAR-RIS-Assisted Transmissions

## Quick Facts
- arXiv ID: 2407.18627
- Source URL: https://arxiv.org/abs/2407.18627
- Reference count: 17
- Primary result: MAGAR achieves highest energy efficiency compared to Q-learning, multi-agent DQN with global reward, and multi-agent DQN with local rewards

## Executive Summary
This paper proposes a multi-hop STAR-RIS architecture to maximize energy efficiency in wireless networks through joint optimization of active beamforming at the base station and passive beamforming at STAR-RIS elements, including their on-off states. The authors design a Multi-Agent Global and locAl deep Reinforcement learning (MAGAR) algorithm where global and local agents collaborate to improve learning performance, with the global agent periodically replacing local agents to optimize overall system performance. Simulation results demonstrate that MAGAR outperforms benchmark methods and that the proposed multi-hop STAR-RIS system achieves higher energy efficiency than mode switching based STAR-RISs, conventional RISs, and deployments without RISs/STAR-RISs.

## Method Summary
The method employs a Multi-Agent Global and locAl deep Reinforcement learning (MAGAR) algorithm with collaborative multiple agents to jointly optimize active beamforming at the base station and passive beamforming at STAR-RIS elements, considering their on-off states. The global agent periodically replaces local agents to dictate actions using aggregated global state information, while local agents learn independently between global agent executions. The algorithm uses neural network function approximators for Q-value estimation, Ç«-greedy policy for action selection, and trains on a wireless communication environment with channel models following Rician distribution. The objective is to maximize energy efficiency defined as the ratio of achievable data rate to total power consumption.

## Key Results
- MAGAR achieves highest energy efficiency compared to Q-learning, multi-agent DQN with global reward, and multi-agent DQN with local rewards
- The proposed multi-hop STAR-RIS architecture outperforms mode switching based STAR-RISs, conventional RISs, and deployments without RISs/STAR-RISs
- Energy efficiency trends show optimal performance with 2-hop STAR-RIS configuration and 4 active elements per STAR-RIS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAGAR improves energy efficiency by periodically replacing local agents with a global agent to enforce coordinated decision-making
- Mechanism: Local agents learn independently based on their own observations and rewards. The global agent periodically executes actions across all agents using aggregated global state information. This combination allows both local autonomy and global coordination
- Core assumption: Global optimization benefits outweigh the computational overhead of periodic global agent execution
- Evidence anchors:
  - [abstract] "The global agent periodically replaces local agents to optimize overall system performance."
  - [section] "The global agent periodically replaces local agents to dictate actions, thereby optimizing the overall system performance."
- Break condition: If the period Tq is too frequent, computational overhead negates the performance gains

### Mechanism 2
- Claim: Energy efficiency is maximized by jointly optimizing active beamforming at the BS and passive beamforming at STAR-RIS elements, including on-off states
- Mechanism: The algorithm learns to select which STAR-RIS elements to activate and how to configure their phase shifts and amplitudes, while simultaneously optimizing BS beamforming, all under power constraints
- Core assumption: The on-off mechanism of STAR-RIS elements significantly impacts total power consumption and energy efficiency
- Evidence anchors:
  - [abstract] "Furthermore, we investigate the impact of the on-off state of STAR-RIS elements on energy efficiency."
  - [section] "We have proposed a Multi-Agent Global and locAl deep Reinforcement learning (MAGAR) algorithm with collaborative multiple agents."
- Break condition: If power consumption from active STAR-RIS elements dominates the system, the optimization may fail to find feasible solutions

### Mechanism 3
- Claim: The proposed multi-hop STAR-RIS architecture outperforms single-hop and conventional RIS systems by creating more diverse signal paths
- Mechanism: Multiple STAR-RISs create cascaded transmission and reflection paths, enabling coverage in scenarios where single STAR-RIS or conventional RIS would fail
- Core assumption: Additional signal paths provide meaningful diversity gain that improves overall system performance
- Evidence anchors:
  - [abstract] "the proposed architecture of multi-hop STAR-RISs achieves the highest energy efficiency compared to mode switching based STAR-RISs, conventional RISs and deployment without RISs or STAR-RISs."
  - [section] "Since multiple distributed RISs without signal cooperation uniquely reflects the signals to users, the concept of multi-hop transmissions has been proposed to enlarge the coverage and connectivity."
- Break condition: If channel correlation between hops is too high, diversity gains diminish

## Foundational Learning

- Concept: Multi-Agent Reinforcement Learning
  - Why needed here: The problem involves multiple independent agents (STAR-RISs and BS) that need to coordinate to maximize a global reward
  - Quick check question: What is the difference between independent Q-learning and joint action learning in multi-agent settings?

- Concept: STAR-RIS passive beamforming with energy splitting
  - Why needed here: The STAR-RIS can both transmit and reflect signals, with coupled phase shift constraints between transmission and reflection
  - Quick check question: How does the energy conservation constraint affect the amplitude coefficients for transmission and reflection?

- Concept: Markov Decision Process formulation for communication systems
  - Why needed here: The joint beamforming problem is reformulated as an MDP to enable RL-based optimization
  - Quick check question: What are the state, action, and reward definitions for the STAR-RIS agent in this problem?

## Architecture Onboarding

- Component map:
  - Global Agent (Qtot) -> Local Agents (STAR-RISs and BS) -> Environment -> Rewards -> All Agents

- Critical path:
  1. Local agents interact with environment and update Q-values
  2. Global agent aggregates state and executes coordinated actions periodically
  3. Rewards are computed based on energy efficiency metric
  4. All agents update their Q-networks using TD learning

- Design tradeoffs:
  - Frequency of global agent execution (Tq): Higher frequency improves coordination but increases computational overhead
  - Exploration vs exploitation: Balance between discovering new strategies and exploiting known good policies
  - State/action space granularity: More detailed states/actions provide better optimization but increase learning complexity

- Failure signatures:
  - Poor convergence: May indicate insufficient exploration or inappropriate reward scaling
  - Suboptimal local agent policies: Could suggest inadequate global coordination frequency
  - High variance in performance: Might indicate unstable learning dynamics or insufficient training

- First 3 experiments:
  1. Verify convergence behavior with different Tq values while monitoring energy efficiency trends
  2. Compare energy efficiency across different on-off policies (all-on, half-on, optimized) for various STAR-RIS element counts
  3. Test scalability by increasing number of users and measuring impact on energy efficiency and convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MAGAR scale with the number of STAR-RIS elements per surface (N) beyond the tested range, and what is the theoretical limit of energy efficiency improvement?
- Basis in paper: [explicit] Fig. 4 shows energy efficiency trends with varying N, but the behavior beyond tested values is unclear
- Why unresolved: The paper only tests a limited range of N values and doesn't provide theoretical analysis of scaling limits
- What evidence would resolve it: Additional simulations with much larger N values and theoretical analysis of power consumption vs. sum rate scaling

### Open Question 2
- Question: How does the proposed multi-hop STAR-RIS architecture perform in dynamic environments with mobile users and/or changing obstacles compared to static scenarios?
- Basis in paper: [inferred] The paper assumes static user distribution and doesn't address mobility or environmental changes
- Why unresolved: All simulations use fixed user positions and static channel conditions
- What evidence would resolve it: Simulations with mobile users, time-varying channels, and dynamic obstacle placement

### Open Question 3
- Question: What is the impact of imperfect channel state information on MAGAR's performance, and how robust is the algorithm to estimation errors?
- Basis in paper: [inferred] The paper assumes perfect channel knowledge for all simulations
- Why unresolved: No analysis of performance degradation under realistic imperfect CSI conditions
- What evidence would resolve it: Simulations with noisy or outdated channel estimates and comparison of performance degradation

### Open Question 4
- Question: How does the computational complexity of MAGAR scale with the number of agents (V) and users (K), and what are the practical limitations for large-scale deployments?
- Basis in paper: [explicit] The paper mentions high complexity for traditional algorithms but doesn't analyze MAGAR's scaling behavior
- Why unresolved: No complexity analysis or discussion of scalability limitations
- What evidence would resolve it: Complexity analysis of MAGAR with varying V and K, including computational time measurements

## Limitations
- The paper lacks detailed specifications for key implementation parameters, particularly the neural network architecture for Q-value function approximation
- Computational complexity analysis is absent, leaving questions about scalability to larger networks
- The paper assumes perfect channel state information, which may not be realistic in practical deployments

## Confidence
- **High Confidence**: The fundamental mechanism of combining local autonomy with periodic global coordination is well-established in multi-agent RL literature. The energy efficiency metric definition and the basic STAR-RIS passive beamforming model are standard.
- **Medium Confidence**: The specific MAGAR algorithm design and its superiority over benchmark methods are supported by simulation results, but the exact implementation details are incomplete. The claim about multi-hop STAR-RIS architecture outperforming single-hop systems is plausible but lacks extensive validation.
- **Low Confidence**: The practical feasibility of the proposed architecture in real-world scenarios is uncertain due to missing complexity analysis and assumptions about perfect CSI.

## Next Checks
1. **Convergence Analysis**: Implement the MAGAR algorithm with varying global agent replacement periods (Tq) and systematically analyze the trade-off between coordination benefits and computational overhead. Measure convergence speed, final energy efficiency, and variance across multiple runs.

2. **Scalability Testing**: Evaluate the algorithm's performance as the number of STAR-RIS elements, users, and hops increases. Track energy efficiency, training time, and memory requirements to identify practical scalability limits.

3. **Robustness Validation**: Test the algorithm under imperfect channel state information by introducing estimation errors and delays. Compare performance degradation against the perfect CSI baseline to assess practical applicability.