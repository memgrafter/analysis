---
ver: rpa2
title: Proximal Curriculum with Task Correlations for Deep Reinforcement Learning
arxiv_id: '2405.02481'
source_url: https://arxiv.org/abs/2405.02481
tags:
- curriculum
- learning
- target
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PROCURL-TARGET, a curriculum learning strategy
  for deep RL that selects tasks balancing agent's learning potential and task similarity
  to target distribution. The method theoretically derives task selection via gradient
  alignment analysis and extends to general settings using softmax sampling.
---

# Proximal Curriculum with Task Correlations for Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.02481
- Source URL: https://arxiv.org/abs/2405.02481
- Reference count: 21
- Key outcome: PROCURL-TARGET matches or outperforms state-of-the-art baselines across five environments

## Executive Summary
This paper introduces PROCURL-TARGET, a curriculum learning strategy for deep RL that selects tasks balancing agent's learning potential and task similarity to target distribution. The method theoretically derives task selection via gradient alignment analysis and extends to general settings using softmax sampling. Experiments across five environments show PROCURL-TARGET matches or outperforms state-of-the-art baselines like SPDL, CURROT, and PLR, achieving faster convergence and higher performance, particularly in challenging target distributions. The approach requires minimal hyperparameter tuning and integrates seamlessly with deep RL frameworks.

## Method Summary
PROCURL-TARGET is a curriculum learning strategy for contextual multi-task reinforcement learning that selects tasks by maximizing the product of learning potential on current task, learning potential on target task, and task similarity. The method uses gradient alignment approximation to efficiently compute task selection without expensive policy updates. A teacher component selects tasks using softmax sampling over task pairs weighted by these three factors, while a student component (PPO agent) attempts selected tasks and updates policy parameters. The approach balances the Zone of Proximal Development principle with progression toward target distributions, requiring only the temperature parameter β for softmax sampling.

## Key Results
- Matches or outperforms state-of-the-art baselines (SPDL, CURROT, PLR, GRADIENT, IID, TARGET) across five diverse environments
- Achieves faster convergence and higher asymptotic performance, particularly on challenging target distributions
- Requires minimal hyperparameter tuning with only β needing adjustment
- Demonstrates effectiveness on environments ranging from contextual bandits to continuous control tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The curriculum strategy selects tasks that balance difficulty (ZPD) and target task similarity by maximizing the product of learning potential on current task, learning potential on target task, and task similarity.
- Mechanism: At each step, the teacher component evaluates all tasks in the training pool by computing three terms: the agent's learning potential on the current task, the agent's learning potential on the target task, and the correlation between the current and target tasks. The task with the highest product of these three terms is selected, ensuring the agent practices tasks that are neither too hard nor too easy while progressively moving toward the target distribution.
- Core assumption: Task similarity can be effectively measured by the exponential of negative squared distance in the context space.
- Evidence anchors:
  - [abstract] "balances the need for selecting tasks that are not too difficult for the agent while progressing the agent's learning toward the target distribution via leveraging task correlations."
  - [section 3.2] "The curriculum strategy involves the following quantities: A⃝ the agent's learning potential on task c, B⃝ the agent's learning potential on taskctarg, and C⃝ the correlation between the tasks c and ctarg."
- Break condition: If task similarity metric fails to capture true transferability, the curriculum may select suboptimal tasks that do not effectively progress toward the target distribution.

### Mechanism 2
- Claim: The gradient alignment approximation allows the curriculum to select tasks that maximize expected improvement in the target task performance without computing the updated policy.
- Mechanism: By approximating the expected improvement in the training objective using the first-order Taylor expansion of the target task value function, the curriculum reduces to selecting tasks that maximize the inner product between the gradient of the current task and the gradient of the target task. This approximation enables efficient task selection while maintaining theoretical justification for convergence.
- Core assumption: The first-order Taylor approximation accurately captures the relationship between task gradients and expected performance improvement.
- Evidence anchors:
  - [section 3.1] "we approximate the expected improvement in the training objective as follows: It(ct) ≈ ηt · ⟨gt(ct), gt(ctarg)⟩."
  - [section 3.1] "the natural greedy curriculum strategy in Eq. (1) can be effectively approximated by the following gradient-alignment-based curriculum strategy: ct ← arg maxc⟨gt(c), gt(ctarg)⟩."
- Break condition: If the policy gradients are poorly aligned or if the Taylor approximation breaks down due to large parameter updates, the gradient alignment may not reflect true learning potential.

### Mechanism 3
- Claim: The softmax sampling with temperature parameter β makes the curriculum selection more robust to approximation errors in value estimates.
- Mechanism: Instead of selecting the task with the maximum score deterministically, the curriculum samples tasks from a softmax distribution over the product of learning potentials and task similarity. This introduces stochasticity that prevents the curriculum from getting stuck in local optima and makes it more resilient to noisy or inaccurate value estimates from the critic network.
- Core assumption: Introducing controlled stochasticity through softmax sampling improves robustness without sacrificing convergence speed.
- Evidence anchors:
  - [section 3.2] "To make the selection more robust, we replace arg max selection with softmax selection and sample (cttarg, ct) from the distribution..."
  - [section 4.2] "P ROCURL-T ARGET is based on Eq. (4). Throughout all the experiments, we use the following choice to compute the similarity between ψ(s) and ψ(ctarg): exp (− ∥c − ctarg∥2)."
- Break condition: If the temperature parameter β is set too high or too low, the curriculum may become too random or too deterministic, respectively, losing the benefits of robust sampling.

## Foundational Learning

- Concept: Zone of Proximal Development (ZPD)
  - Why needed here: The ZPD concept ensures the curriculum selects tasks that are neither too easy (no learning) nor too hard (no progress) for the current policy, which is crucial for efficient learning in multi-task RL settings.
  - Quick check question: How does the curriculum ensure tasks are appropriately challenging for the agent's current skill level?

- Concept: Task correlation and transferability
  - Why needed here: Understanding how tasks are related allows the curriculum to leverage transfer learning by selecting tasks that share relevant features with the target task, accelerating learning progress.
  - Quick check question: What metric is used to measure the similarity between different tasks in the curriculum strategy?

- Concept: Gradient alignment in policy optimization
  - Why needed here: The gradient alignment approximation provides a computationally efficient way to estimate which tasks will provide the most learning progress toward the target task without expensive policy updates.
  - Quick check question: Why is gradient alignment between source and target tasks a good proxy for expected learning improvement?

## Architecture Onboarding

- Component map: Teacher component -> Task selection -> Student attempts task -> Value network updates -> Teacher selects next task
- Critical path: Teacher → Task selection → Student attempts task → Value network updates → Teacher selects next task
- Design tradeoffs: The curriculum trades off between computational efficiency (gradient alignment approximation) and theoretical guarantees (softmax sampling vs. deterministic selection)
- Failure signatures: Curriculum gets stuck selecting only easy tasks, agent fails to progress toward target distribution, value estimates become inaccurate leading to poor task selection
- First 3 experiments:
  1. Implement PROCURL-TARGET with a simple contextual bandit environment to verify task selection behavior
  2. Compare PROCURL-TARGET against uniform sampling baseline on PM-S:1T environment
  3. Test robustness by varying the temperature parameter β in PROCURL-TARGET on PM-S:2G environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PROCURL-TARGET scale with increasing dimensionality of the context space in sparse reward environments?
- Basis in paper: [inferred] The paper mentions this as a limitation in the concluding discussions, noting that sampling new tasks becomes challenging with high-dimensional context spaces due to the need to estimate values for all tasks in discrete sets.
- Why unresolved: The paper does not provide experimental results or theoretical analysis for high-dimensional context spaces. The authors only mention it as a potential area for future work.
- What evidence would resolve it: Experiments comparing PROCURL-TARGET performance across varying context space dimensionalities, or theoretical analysis of computational complexity scaling with dimensionality.

### Open Question 2
- Question: What is the impact of using different distance metrics over the context space on the effectiveness of PROCURL-TARGET's curriculum design?
- Basis in paper: [explicit] The paper mentions in the concluding discussions that while PROCURL-TARGET uses a simple distance measure to capture task correlation, it would be worthwhile to investigate the effects of employing different distance metrics.
- Why unresolved: The paper uses a specific distance metric (exponential of negative squared distance) but does not compare it to alternatives or analyze its sensitivity to different choices.
- What evidence would resolve it: Experiments comparing PROCURL-TARGET performance using various distance metrics (e.g., cosine similarity, learned embeddings) across different environments and task distributions.

### Open Question 3
- Question: How does PROCURL-TARGET perform when the target distribution has a complex, non-convex shape in the context space?
- Basis in paper: [inferred] While the paper demonstrates PROCURL-TARGET on various target distributions (single mode, multi-modal, uniform), it does not explicitly test highly non-convex or irregularly shaped target distributions that might present challenges for task selection strategies.
- Why unresolved: The paper's experiments focus on relatively simple target distribution shapes (Gaussian modes, uniform distributions, single points) and do not explore more complex geometries.
- What evidence would resolve it: Experiments applying PROCURL-TARGET to environments with target distributions that have non-convex, disconnected, or otherwise complex shapes in the context space.

## Limitations

- The method assumes task similarity can be effectively captured by Euclidean distance in context space, which may not hold for all problem domains
- Performance scaling with increasing dimensionality of context space remains unexplored and may present computational challenges
- The exact implementation details of value network architecture and training procedure are not fully specified

## Confidence

- Theoretical framework (High): The gradient alignment derivation and softmax sampling approach are mathematically rigorous
- Empirical performance claims (Medium): Results show consistent improvement across environments but lack detailed ablations
- Task similarity metric (Low): The exponential distance measure is a strong assumption that may not generalize

## Next Checks

1. Implement an ablation study comparing deterministic vs softmax task selection with varying β values to quantify the robustness benefit
2. Test PROCURL-TARGET on environments with non-Euclidean task similarity structures to evaluate the distance metric assumption
3. Measure the correlation between gradient alignment scores and actual learning progress to validate the approximation quality