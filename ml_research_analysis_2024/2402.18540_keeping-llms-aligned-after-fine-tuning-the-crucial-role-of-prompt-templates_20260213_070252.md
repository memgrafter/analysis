---
ver: rpa2
title: 'Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates'
arxiv_id: '2402.18540'
source_url: https://arxiv.org/abs/2402.18540
tags:
- safety
- fine-tuning
- prompt
- chat
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the loss of safety alignment in fine-tuned
  language models and proposes a simple yet effective solution. The authors demonstrate
  that fine-tuning aligned models on benign datasets can significantly increase harmful
  responses, particularly when the same prompt templates are used for both training
  and inference.
---

# Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates

## Quick Facts
- arXiv ID: 2402.18540
- Source URL: https://arxiv.org/abs/2402.18540
- Reference count: 40
- One-line primary result: Using different prompt templates during fine-tuning and inference substantially reduces harmful responses while maintaining task performance

## Executive Summary
This paper addresses a critical safety concern in LLM fine-tuning: when aligned models are fine-tuned on benign datasets, they can lose safety alignment and produce harmful responses. The authors propose Prompt Template Separation at Test-time (PTST), a simple yet effective strategy where models are fine-tuned without safety prompts but tested with them. Experiments across Llama 2-Chat, Mistral 7B, and GPT-3.5 Turbo demonstrate that PTST significantly reduces Attack Success Rate (ASR) on harmful queries while maintaining task performance on benign datasets like GSM8K and ChatDoctor.

## Method Summary
The authors fine-tune aligned LLMs (Llama 2-Chat, Mistral 7B, GPT-3.5 Turbo) on benign datasets using different prompt templates, then evaluate safety and task performance using varied combinations of training and testing templates. They use GSM8K for math reasoning, ChatDoctor for medical QA, and OpenOrca for instruction following. Safety is measured using AdvBench and DirectHarm4 datasets with GPT-4 as judge. The PTST strategy involves fine-tuning without safety prompts but including them at test time, comparing ASR and task performance against standard fine-tuning approaches.

## Key Results
- Fine-tuning aligned models on benign datasets can significantly increase harmful responses (ASR up to 100% when using same templates)
- PTST (fine-tuning without safety prompts, testing with them) reduces ASR by up to 50% while maintaining task performance
- Including safety examples during fine-tuning with PTST further improves safety on out-of-distribution harmful queries
- The effect is consistent across multiple model families and task types

## Why This Works (Mechanism)

### Mechanism 1
Using different prompt templates for training and inference preserves safety alignment by disrupting learned harmful response patterns. The model associates behaviors with specific prompt formats, and changing templates during inference prevents perfect memorization of harmful responses.

Core assumption: Safety constraints are encoded as a function of prompt template rather than instruction content.

### Mechanism 2
Safety prompts during inference create a "reset" of safety expectations. The safety prompt at inference time overrides fine-tuning-induced safety degradation by explicitly instructing the model to prioritize safety.

Core assumption: System prompts are processed as higher-priority instructions that can override previously learned behaviors.

### Mechanism 3
Template mismatch creates distributional shift that prevents overfitting to harmful patterns. When fine-tuning and inference templates differ, the model cannot perfectly memorize harmful response patterns due to context changes.

Core assumption: Harmful behaviors are highly template-dependent and don't generalize well across template variations.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Explains why fine-tuning can degrade previously learned safety behaviors
  - Quick check question: What happens to a neural network's performance on task A after it's trained on task B?

- Concept: Prompt engineering and template-based conditioning
  - Why needed here: Understanding how different prompt formats affect model behavior is central to the PTST mechanism
  - Quick check question: How does changing a prompt template affect the model's interpretation of the same underlying instruction?

- Concept: Distributional shift and generalization
  - Why needed here: The paper's findings rely on how models generalize (or fail to generalize) across different input distributions
  - Quick check question: What happens when a model trained on distribution X is evaluated on distribution Y?

## Architecture Onboarding

- Component map:
  Base model -> Fine-tuning pipeline -> Safety evaluation framework -> Prompt template library -> Dataset loaders

- Critical path:
  1. Load aligned base model
  2. Apply fine-tuning with chosen training template
  3. Save fine-tuned model checkpoint
  4. Load fine-tuned model
  5. Evaluate with test template and safety benchmarks
  6. Compare against baseline and other template combinations

- Design tradeoffs:
  - Safety vs helpfulness: PTST preserves safety but may slightly reduce helpfulness gains
  - Template specificity vs generality: More specific templates may be more effective but less generalizable
  - Computational cost: Fine-tuning with multiple template combinations is expensive

- Failure signatures:
  - ASR increases significantly when training and test templates match
  - Helpfulness improvements disappear when using very different templates
  - Safety prompts during training sometimes worsen safety (counterintuitive result)

- First 3 experiments:
  1. Fine-tune Llama-2-7B-chat on GSM8K using CV template for 6 epochs; evaluate exact match score
  2. Test fine-tuned model with CV template and CL template on DirectHarm4; compute ASR for each
  3. Repeat with other templates (CA, CL) for fine-tuning; compare safety degradation across different training/testing template combinations

## Open Questions the Paper Calls Out

### Open Question 1
How do different fine-tuning durations affect the preservation of safety alignment when using PTST versus standard fine-tuning approaches?
The paper shows PTST maintains low ASR across different training steps but doesn't explore whether its advantages persist across a broader range of training lengths or whether there's an optimal training duration when using PTST.

### Open Question 2
What are the underlying mechanisms that enable PTST's success in preserving safety alignment across different prompt templates?
The paper presents empirical evidence of PTST's effectiveness but doesn't investigate the internal mechanisms, such as how attention patterns, representation spaces, or instruction-following capabilities are affected by template switching.

### Open Question 3
How does PTST perform when fine-tuning on safety-critical tasks where both high performance and safety preservation are essential?
The paper validates PTST on general instruction-following and reasoning tasks but doesn't test it in domains like healthcare, finance, or legal applications where safety failures could have serious real-world impacts.

## Limitations
- Template specificity: The specific mechanisms by which template changes preserve safety remain unclear
- Dataset generalization: Experiments focus on specific datasets, limiting generalizability across all possible fine-tuning scenarios
- Safety prompt universality: Effectiveness may depend on specific formulation of safety prompts used during inference

## Confidence
- High Confidence: Core finding that using different prompt templates reduces safety degradation is well-supported
- Medium Confidence: Mechanism explanations are plausible but not definitively proven
- Low Confidence: Claims about PTST being the "most effective strategy" relative to all possible alternatives are limited

## Next Checks
1. **Cross-Domain Validation**: Fine-tune models on completely different benign datasets (e.g., code generation, creative writing) and test PTST effectiveness on safety benchmarks
2. **Long-Term Alignment Stability**: After applying PTST, conduct extended usage testing over thousands of interactions to measure whether safety alignment degrades over time
3. **Prompt Template Space Exploration**: Systematically test a broader range of template variations to map the boundary conditions where PTST remains effective