---
ver: rpa2
title: Derandomizing Multi-Distribution Learning
arxiv_id: '2409.17567'
source_url: https://arxiv.org/abs/2409.17567
tags:
- learning
- algorithm
- multi-distribution
- distribution
- distributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational complexity of derandomizing
  multi-distribution learning, a collaborative learning setting where the goal is
  to find a single predictor that works well across multiple unknown data distributions.
  While existing multi-distribution learning algorithms achieve near-optimal sample
  complexity, they output randomized predictors.
---

# Derandomizing Multi-Distribution Learning

## Quick Facts
- arXiv ID: 2409.17567
- Source URL: https://arxiv.org/abs/2409.17567
- Authors: Kasper Green Larsen; Omar Montasser; Nikita Zhivotovskiy
- Reference count: 9
- Key outcome: Proves computational hardness of derandomizing multi-distribution learning and provides a black-box reduction for label-consistent distributions.

## Executive Summary
This paper addresses the computational complexity of derandomizing multi-distribution learning, where the goal is to find a single predictor that works well across multiple unknown data distributions. While existing algorithms achieve near-optimal sample complexity, they output randomized predictors. The authors show that derandomizing these algorithms is computationally hard, even when efficient empirical risk minimization is available. However, they identify a structural condition (label-consistent distributions) under which a black-box reduction can convert randomized predictors into deterministic ones with minimal overhead.

## Method Summary
The paper presents two main results: First, it proves that derandomizing multi-distribution learning is computationally hard through a reduction to discrepancy minimization, showing that any algorithm producing a deterministic predictor with near-optimal error must have super-polynomial training or evaluation time, assuming BPP ≠ NP. Second, for label-consistent distributions where Di(y|x) is the same across all distributions, the authors provide an efficient black-box reduction that converts randomized multi-distribution predictors into deterministic ones with minimal overhead in sample complexity and runtime.

## Key Results
- Derandomizing multi-distribution learning is NP-hard via reduction to discrepancy minimization
- A black-box reduction can convert randomized predictors to deterministic ones for label-consistent distributions
- The derandomization algorithm uses random rounding with r-wise independent hash functions for efficient implementation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Derandomizing multi-distribution learning is computationally hard due to the need to find a single deterministic predictor that works well across all distributions simultaneously.
- Mechanism: The paper reduces the problem to discrepancy minimization, showing that distinguishing between matrices with low vs. high discrepancy is NP-hard. This reduction demonstrates that any algorithm producing a deterministic predictor with near-optimal error must have either super-polynomial training time or evaluation time.
- Core assumption: BPP ≠ NP (randomized polynomial-time algorithms cannot efficiently solve all NP problems)
- Evidence anchors:
  - [abstract]: "Through a reduction to discrepancy minimization, we show that derandomizing multi-distribution learning is computationally hard, even when ERM is computationally efficient."
  - [section 2]: "Let us now use Theorem 3 to prove our hardness result, Theorem 1. We remark that NP-hardness is formally defined in a uniform model of computation where a Turing Machine takes an encoded input on a tape and decides language membership."
  - [corpus]: Weak - corpus neighbors don't directly address computational hardness
- Break condition: If BPP = NP, or if we restrict to label-consistent distributions where Di(y|x) is the same across all distributions for any given x

### Mechanism 2
- Claim: For label-consistent distributions, derandomization becomes tractable through a black-box reduction.
- Mechanism: The algorithm identifies "heavily biased" points where the label distribution varies significantly across distributions, makes deterministic predictions for these points, and uses random rounding only for "lightly biased" points where the variance is small enough to be handled by concentration inequalities.
- Core assumption: The distributions are label-consistent (Di(y|x) = Dj(y|x) for all i, j, x)
- Evidence anchors:
  - [abstract]: "On the positive side, we identify a structural condition enabling an efficient black-box reduction, converting existing randomized multi-distribution predictors into deterministic ones."
  - [section 3]: "Remarkably, in terms of sample complexity, in the case of a single distribution, the case of deterministic labeling is almost as hard as the general agnostic case as shown in [Ben-David and Urner, 2014]."
  - [corpus]: Weak - corpus neighbors don't discuss label-consistency or derandomization
- Break condition: If distributions are not label-consistent, or if the bias threshold is set incorrectly

### Mechanism 3
- Claim: The algorithm can be implemented efficiently using r-wise independent hash functions to reduce storage and evaluation time.
- Mechanism: Instead of storing independent random choices for each point, the algorithm uses an r-wise independent hash function to simulate the random rounding process, reducing storage from O(|X|) to O(r ln(|X||Y|)) bits.
- Core assumption: The randomized multi-distribution learner outputs a distribution over a finite number of classifiers
- Evidence anchors:
  - [section 3.1]: "Instead of storing ˆf (x) for every x ∈ X \ T explicitly, the learning algorithm instead stores q and the distribution F. Given this information, we evaluate ˆf (x) by computing q(x) and letting ˆf (x) = 1 if q(x) ≤ Prf ∼ F [f (x) = 1] |Y| −1 and − 1 otherwise."
  - [section 3.1]: "Using fast multiplication algorithms, q(x) can be evaluated in time ˜O(r ln(|X ||Y|)), even when ln(|X ||Y|) bits does not fit in a machine word."
  - [corpus]: Weak - corpus neighbors don't discuss hash functions or efficient implementation
- Break condition: If r is set too small or |Y| is set incorrectly, the guarantees may fail

## Foundational Learning

- Concept: VC-dimension and its role in learning theory
  - Why needed here: The paper's complexity results depend on the VC-dimension of the hypothesis class, and the algorithms' sample complexity bounds scale with VC-dimension
  - Quick check question: If a hypothesis class has VC-dimension d, what is the maximum number of points it can shatter?

- Concept: Discrepancy minimization and its computational complexity
  - Why needed here: The hardness proof relies on reducing multi-distribution learning to discrepancy minimization, showing that distinguishing low vs. high discrepancy matrices is NP-hard
  - Quick check question: What is the relationship between discrepancy minimization and the problem of finding a coloring z that minimizes ∥Az∥∞?

- Concept: Concentration inequalities (Hoeffding's inequality)
  - Why needed here: The derandomization algorithm uses concentration bounds to argue that random rounding works for "lightly biased" points where the variance is small
  - Quick check question: Under what conditions does Hoeffding's inequality guarantee that the empirical mean is close to the true mean?

## Architecture Onboarding

- Component map: Input distributions P -> Randomized multi-distribution learner A -> Derandomization algorithm -> Deterministic classifier ˆf

- Critical path:
  1. Run black-box randomized learner A on input distributions with precision ε/2 and failure probability δ/2
  2. Draw samples from each distribution to identify heavily biased points
  3. Make deterministic predictions for heavily biased points based on majority labels
  4. Use random rounding for lightly biased points using r-wise independent hash functions
  5. Combine predictions to form final deterministic classifier

- Design tradeoffs:
  - Sample complexity vs. error tolerance: Increasing samples reduces error but increases runtime
  - r-wise independence vs. storage: Larger r gives better concentration but requires more storage
  - Threshold for "heavily biased" points: Higher thresholds reduce number of deterministic predictions but increase reliance on random rounding

- Failure signatures:
  - High error on test data despite low training error: May indicate insufficient samples or incorrect bias threshold
  - Algorithm runs out of memory: May need to increase r or use more efficient data structures
  - Runtime exceeds polynomial bounds: May need to reconsider computational model or relax requirements

- First 3 experiments:
  1. Implement derandomization algorithm for simple hypothesis class (e.g., linear classifiers) on synthetic label-consistent data
  2. Test sensitivity to bias threshold parameter by varying it and measuring error rates
  3. Benchmark storage and runtime using different values of r in the r-wise independent hash function approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a computationally efficient multi-distribution learning algorithm that outputs a deterministic predictor when the hypothesis class admits efficient empirical risk minimization?
- Basis in paper: [explicit] The paper proves that derandomizing multi-distribution learning is computationally hard, even when efficient ERM is available for the hypothesis class.
- Why unresolved: The paper shows that any multi-distribution learning algorithm producing a deterministic predictor with near-optimal error must have super-polynomial training time or evaluation time, assuming BPP ≠ NP. However, this doesn't rule out the existence of computationally inefficient aggregation approaches to construct deterministic predictors.
- What evidence would resolve it: A proof that no sample-efficient and oracle-efficient multi-distribution learning algorithm can aggregate multiple ERM predictors in polynomial time to construct deterministic predictors, or a construction of such an algorithm.

### Open Question 2
- Question: Can the restriction to finite domains in the deterministic multi-distribution learner be removed or relaxed?
- Basis in paper: [explicit] The authors view the restriction to finite domains as mild but acknowledge it as a limitation, proposing some initial ideas for extending to infinite domains.
- Why unresolved: The algorithm requires finite domains for storage and evaluation of the classifier. While the authors propose using r-wise independent hash functions to reduce storage requirements, this still requires a finite output domain.
- What evidence would resolve it: A modification of the algorithm that works for infinite domains, or a proof that no such algorithm can exist while maintaining the same sample complexity and runtime guarantees.

### Open Question 3
- Question: Is there a gap between the sample complexity achievable with deterministic versus randomized predictors in multi-distribution learning?
- Basis in paper: [explicit] The paper discusses the gap between the guarantees of deterministic and randomized classifiers, noting that even using a randomized algorithm is somewhat problematic in multi-distribution learning.
- Why unresolved: While the paper proves computational hardness of derandomization, it doesn't address whether there's a fundamental statistical difference between deterministic and randomized predictors in terms of sample complexity.
- What evidence would resolve it: An example or proof showing that no deterministic predictor can achieve the same sample complexity as the best randomized predictor for some multi-distribution learning problem, or a demonstration that such a gap doesn't exist.

## Limitations
- Computational hardness results assume BPP ≠ NP, which while widely believed, remains unproven
- The algorithm's performance on real-world non-label-consistent distributions remains untested
- The sample complexity bounds depend on unknown constants in the theoretical analysis

## Confidence

- Computational hardness reduction (Mechanism 1): High - follows from well-established complexity theory and explicit reduction
- Derandomization algorithm for label-consistent distributions (Mechanism 2): Medium - theoretical guarantees are sound but practical performance untested
- Efficient implementation via r-wise independence (Mechanism 3): Medium - theoretical storage/runtime bounds proven but constants and practical overhead unknown

## Next Checks

1. Empirical evaluation of Algorithm 1 on synthetic label-consistent distributions to verify the sample complexity bounds and error rates match theoretical predictions
2. Stress-testing the r-wise independent hash function implementation with varying r values to identify the practical tradeoff between storage and concentration guarantees
3. Testing Algorithm 1 on non-label-consistent distributions to quantify performance degradation and identify failure modes when the structural condition is violated