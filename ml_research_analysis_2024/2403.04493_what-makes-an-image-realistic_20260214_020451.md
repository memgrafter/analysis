---
ver: rpa2
title: What makes an image realistic?
arxiv_id: '2403.04493'
source_url: https://arxiv.org/abs/2403.04493
tags:
- realism
- image
- realistic
- which
- would
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of quantifying realism in generated
  data, a problem that remains poorly understood despite advancements in generative
  AI. The author argues that common approaches based on probability and typicality
  fail to capture realism because they address the wrong question: instead of the
  probability of x given P, we need the probability of P given x.'
---

# What makes an image realistic?

## Quick Facts
- arXiv ID: 2403.04493
- Source URL: https://arxiv.org/abs/2403.04493
- Reference count: 38
- Key outcome: Introduces universal critics based on randomness deficiency as a principled framework for measuring realism in generated data

## Executive Summary
This paper addresses the fundamental challenge of quantifying realism in generated data by arguing that common probability-based approaches answer the wrong question. Rather than asking "what is the probability of data x given model P," we should ask "what is the probability of model P given observed data x." Drawing from algorithmic information theory, the author introduces universal critics that measure realism through randomness deficiency, combining model probability with Kolmogorov complexity. This framework provides theoretical grounding for realism assessment and suggests new approaches to generative model evaluation and training.

## Method Summary
The paper proposes measuring realism through universal critics based on randomness deficiency U(x) = -log P(x) - K(x), where P(x) is the model's probability of data x and K(x) is its Kolmogorov complexity. Unlike traditional approaches that directly use model probabilities or adversarial training, universal critics capture how much information is needed to generate x beyond what the model already encodes. The approach connects to statistical hypothesis testing and MCMC methods while avoiding explicit adversarial training. Though K(x) is uncomputable, the framework provides theoretical insights and suggests practical approximation strategies using compression algorithms or limited-memory observers.

## Key Results
- Universal critics provide a theoretically principled framework for measuring realism by addressing the correct statistical question
- The approach connects adversarial critics, MCMC methods, and significance testing through a unified theoretical lens
- Randomness deficiency captures both model fit and data complexity, offering advantages over probability-based metrics alone

## Why This Works (Mechanism)
The paper argues that traditional probability-based realism metrics fail because they measure P(x|model) when they should measure P(model|x). This inversion matters because we care about how likely a model is to have generated observed data, not how likely the data is under the model. Universal critics based on randomness deficiency solve this by measuring the additional information needed to specify x beyond what the model already encodes. This naturally balances model fit with data complexity, avoiding pathologies where simple models get unfairly penalized for generating complex-looking data.

## Foundational Learning

**Kolmogorov Complexity**: The length of the shortest program that produces a given string. *Why needed*: Forms the theoretical foundation for measuring intrinsic data complexity. *Quick check*: Verify you understand why K(x) is uncomputable via the halting problem.

**Randomness Deficiency**: The difference between an object's length and its Kolmogorov complexity. *Why needed*: Provides the core metric for measuring how "non-random" or structured data is relative to a model. *Quick check*: Can you explain why perfectly random data has high randomness deficiency?

**Universal Prior**: A probability distribution that assigns higher probability to simpler (lower K(x)) strings. *Why needed*: Connects algorithmic information theory to probabilistic modeling. *Quick check*: Understand the relationship between universal priors and Occam's razor.

## Architecture Onboarding

**Component Map**: Universal Critic (randomness deficiency) -> Model Evaluation -> Training Feedback (optional)

**Critical Path**: Data x -> Model P(x) -> Kolmogorov Complexity K(x) -> Randomness Deficiency U(x) = -log P(x) - K(x) -> Realism Score

**Design Tradeoffs**: Exact Kolmogorov complexity is uncomputable, requiring approximations; simple models may be unfairly penalized without proper complexity normalization; the framework assumes a universal computing model that may not match human perception.

**Failure Signatures**: Overestimation of realism for data that's complex but fits the model well; underestimation for simple data from complex models; sensitivity to choice of compression algorithm when approximating K(x).

**First Experiments**:
1. Implement universal critic using standard compression algorithms (gzip, bz2, lzma) as K(x) proxies and evaluate on CIFAR-10
2. Compare universal critic scores against FID and KID on a suite of generative models with known realism levels
3. Test whether models trained with universal critic feedback generate more realistic samples than those trained with traditional losses

## Open Questions the Paper Calls Out
None

## Limitations

**Uncomputability Barrier**: Kolmogorov complexity K(x) is provably uncomputable, making exact universal critics impossible to implement directly.

**Limited Empirical Validation**: The paper provides theoretical arguments but lacks extensive empirical validation comparing universal critics to established realism metrics on real-world datasets.

**Human Perception Gap**: No evidence is provided that randomness deficiency correlates with human judgments of realism, leaving a disconnect between theoretical measures and perceptual quality.

## Confidence

**High Confidence**: The theoretical argument about asking the right statistical question (P(model|x) vs P(x|model)) is logically sound and well-articulated.

**Medium Confidence**: Connections between universal critics, adversarial methods, and MCMC are intellectually coherent but require more rigorous validation.

**Low Confidence**: Practical utility claims and performance improvements over existing methods lack sufficient empirical support.

## Next Checks

1. Implement a practical universal critic approximation using multiple compression algorithms and benchmark against FID/KID on standard generative model evaluation datasets.

2. Conduct human perception studies comparing ratings of realism with predictions from universal critics versus traditional metrics to validate correlation.

3. Train a generative model using only universal critic feedback (no adversarial training) and evaluate whether the generated samples show improved realism.