---
ver: rpa2
title: Consistency Model is an Effective Posterior Sample Approximation for Diffusion
  Inverse Solvers
arxiv_id: '2403.12063'
source_url: https://arxiv.org/abs/2403.12063
tags:
- posterior
- diffusion
- sample
- should
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of sampling from conditional distributions
  in diffusion inverse solvers, specifically addressing the challenge of using neural
  network operators that are sensitive to out-of-distribution inputs. The authors
  propose using the solution of the Probability Flow Ordinary Differential Equation
  (PF-ODE) as an effective posterior sample approximation, and implement this using
  a Consistency Model (CM) distilled from PF-ODE.
---

# Consistency Model is an Effective Posterior Sample Approximation for Diffusion Inverse Solvers

## Quick Facts
- arXiv ID: 2403.12063
- Source URL: https://arxiv.org/abs/2403.12063
- Authors: Tongda Xu; Ziran Zhu; Jian Li; Dailan He; Yuanyuan Wang; Ming Sun; Ling Li; Hongwei Qin; Yan Wang; Jingjing Liu; Ya-Qin Zhang
- Reference count: 40
- Primary result: CM-based inversion techniques significantly improve performance when operators are neural networks, particularly in semantic segmentation tasks

## Executive Summary
This paper addresses the challenge of sampling from conditional distributions in diffusion inverse solvers, particularly when neural network operators are sensitive to out-of-distribution inputs. The authors propose using the Consistency Model (CM) distilled from Probability Flow ODE (PF-ODE) as an effective posterior sample approximation. They demonstrate that CM provides samples within the support of the image distribution and implement a new family of diffusion inverse solvers using only CM. Through extensive experiments on semantic segmentation, layout estimation, and image captioning tasks, they show that their approach significantly improves consistency metrics when operators are neural networks, while also performing well for non-neural network operators.

## Method Summary
The method uses Consistency Models (CM) distilled from PF-ODE to approximate posterior samples for diffusion inverse problems. The approach works by first obtaining a sample from the posterior distribution using CM, then evaluating the operator with this sample to estimate the conditional score, and finally sampling from the posterior using the estimated conditional score. The authors implement two variants: one using single-step CM sampling with noise addition (τ=0.2) for robustness, and another using iterative CM inversion in a GAN inversion fashion with multiple steps and learning rates. The method is tested on LSUN Bedroom and LSUN Cat datasets (256x256 resolution) with various operators including segmentation, layout estimation, and image captioning models.

## Key Results
- CM-based posterior sampling significantly improves consistency metrics (mIOU, CLIP score) for neural network operators compared to existing diffusion inverse solvers
- The proposed approach achieves FID scores of 17.5 and KID scores of 0.011 for semantic segmentation tasks, outperforming baseline methods
- For non-neural network operators like down-sampling, the proposed method is comparable to existing state-of-the-art approaches while maintaining simplicity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The solution of the Probability Flow Ordinary Differential Equation (PF-ODE) provides a valid posterior sample within the support of the image distribution.
- **Mechanism:** Given an initial condition $X_t = x_t$, solving the PF-ODE yields a sample $X_0$ that lies in the support of the image distribution $p_\theta(X_0)$. This is because the PF-ODE preserves the marginal distribution of the reverse diffusion process, which matches the image distribution.
- **Core assumption:** The diffusion model $p_\theta(X_0)$ is well-learned and the score function $\nabla_{X_t} \log p(X_t)$ is accurate.
- **Evidence anchors:**
  - [abstract]: "We first demonstrate that the solution of the Probability Flow Ordinary Differential Equation (PF-ODE) with an initial value $x_t$ yields an effective posterior sample $p_\theta(X_0|X_t=x_t)$."
  - [section]: "It is known that the PF-ODE and reverse SDE in Eq. 3 have the same marginal distribution, which is the image distribution $p_\theta(X_0)$ if the score is learned perfectly [Song et al., 2020]."
  - [corpus]: No direct evidence found in related papers. This appears to be a novel contribution.
- **Break condition:** If the diffusion model is poorly learned or the score function is inaccurate, the PF-ODE solution may not lie in the support of the image distribution.

### Mechanism 2
- **Claim:** The Consistency Model (CM), distilled from PF-ODE, can approximate the PF-ODE solution effectively.
- **Mechanism:** CM is trained to approximate the one-step solution of the PF-ODE. By using CM, we can efficiently sample from the posterior $p_\theta(X_0|X_t)$ without explicitly solving the PF-ODE.
- **Core assumption:** The CM can be trained to accurately approximate the PF-ODE solution.
- **Evidence anchors:**
  - [abstract]: "Based on this observation, we adopt the Consistency Model (CM), which is distilled from PF-ODE, for posterior sampling."
  - [section]: "Fortunately, PF-ODE can be distilled by Consistency Model (CM) [Song et al., 2023c]. More specifically, CM trains a one-step neural function $g_\theta(t, x_t)$ to approximate the solution of PF-ODE $\Phi_0(x_t)$."
  - [corpus]: No direct evidence found in related papers. This appears to be a novel application of CM.
- **Break condition:** If the CM is poorly trained or overfits the operator $f(\cdot)$, the approximation may be inaccurate.

### Mechanism 3
- **Claim:** Adding small Gaussian noise to the CM output improves robustness to neural network operators.
- **Mechanism:** Neural network operators are sensitive to out-of-distribution inputs. Adding noise makes the CM output more robust to such operators by preventing overfitting.
- **Core assumption:** The neural network operator is sensitive to small perturbations in the input.
- **Evidence anchors:**
  - [abstract]: "In practice, we find CM often over-fits the operator $f(\cdot)$. ... To make $f(\cdot)$ robust, we propose to add a small Gaussian noise to the output of CM as literature in adversarial robustness [Li et al., 2019]"
  - [section]: "The CM approximated sample $x_{0|t}$ is numerically close to $y$ after passing the operator $f(\cdot)$. However, by inspecting $x_{0|t}$ visually, one often concludes that it is not aligned with $y$."
  - [corpus]: No direct evidence found in related papers. This appears to be a novel contribution.
- **Break condition:** If the noise level is too high, the CM output may deviate significantly from the true posterior sample.

## Foundational Learning

- **Concept:** Diffusion Models
  - **Why needed here:** The paper relies on diffusion models as the prior distribution for solving inverse problems.
  - **Quick check question:** What is the role of the score function in diffusion models?

- **Concept:** Conditional Score Estimation
  - **Why needed here:** The paper uses conditional score estimation to sample from the posterior distribution $p_\theta(X_0|y)$.
  - **Quick check question:** How does conditional score estimation differ from unconditional score estimation?

- **Concept:** Probability Flow Ordinary Differential Equation (PF-ODE)
  - **Why needed here:** The PF-ODE provides a valid posterior sample within the support of the image distribution.
  - **Quick check question:** What is the relationship between the PF-ODE and the reverse diffusion process?

## Architecture Onboarding

- **Component map:** Diffusion Model -> Operator f(.) -> Consistency Model (CM) -> Noise Addition
- **Critical path:** Sample from $p_\theta(X_0|X_t)$ using CM → Evaluate operator $f(\cdot)$ with the sample → Estimate conditional score → Sample from $p_\theta(X_0|y)$ using the conditional score
- **Design tradeoffs:** Using CM for posterior sampling is faster than solving the PF-ODE directly, but may be less accurate. Adding noise improves robustness but may reduce sample quality.
- **Failure signatures:** If the CM is poorly trained, the samples may not lie in the support of the image distribution. If the noise level is too high, the samples may deviate significantly from the true posterior.
- **First 3 experiments:**
  1. Evaluate the effectiveness of CM as a posterior sample approximation for a simple operator $f(\cdot)$.
  2. Compare the performance of CM with and without noise addition for a neural network operator $f(\cdot)$.
  3. Evaluate the robustness of CM to different levels of noise for a neural network operator $f(\cdot)$.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed approach perform on larger datasets or higher resolution images (e.g., 512x512 or larger) when using latent diffusion models?
- **Basis in paper:** [inferred] The paper acknowledges that all experiments were conducted on 256x256 images and diffusion models in the pixel domain, while noting that recent works have shown potential for CM on 512x512 for latent diffusion models.
- **Why unresolved:** The paper explicitly states this as a limitation and expresses interest in seeing how the approach performs empirically for large images using latent diffusion models.
- **What evidence would resolve it:** Experiments demonstrating the approach's performance on 512x512 or larger images using latent diffusion models, with quantitative metrics (FID, KID, etc.) and qualitative visual comparisons to state-of-the-art methods.

### Open Question 2
- **Question:** How sensitive is the proposed method to the choice of hyperparameters, particularly the noise level τ and learning rates for the CM inversion process?
- **Basis in paper:** [explicit] The paper mentions hyperparameters like τ=0.2 for Proposed I and ζ1, ζ2, ts, and K for Proposed II, but doesn't provide extensive sensitivity analysis.
- **Why unresolved:** While some hyperparameters are mentioned, the paper doesn't thoroughly explore how variations in these parameters affect performance across different operators and datasets.
- **What evidence would resolve it:** Comprehensive ablation studies varying τ, learning rates, and other key hyperparameters, showing their impact on consistency metrics (mIOU, CLIP score) and sample quality (FID, KID) across different operators.

### Open Question 3
- **Question:** How does the proposed approach compare to other state-of-the-art methods when operators are linear or have simpler structures?
- **Basis in paper:** [explicit] The paper mentions that for non-neural network operators like down-sampling, the Proposed I is only comparable to DPS, and states that non-neural network f(.) are not as sensitive to out-of-distribution approximations.
- **Why unresolved:** While the paper focuses on neural network operators, it doesn't provide a comprehensive comparison to specialized methods for linear operators, which may outperform the proposed approach in those cases.
- **What evidence would resolve it:** Experiments comparing the proposed approach to specialized methods for linear operators (e.g., projection-based methods) on various linear inverse problems, with quantitative metrics and runtime comparisons.

## Limitations
- The method's effectiveness for non-neural network operators is limited, performing only comparably to existing methods like DPS
- The approach has only been validated on 256x256 images in the pixel domain, with unknown performance on larger images or latent diffusion models
- The theoretical justification for why CM specifically addresses the out-of-distribution problem remains implicit rather than rigorously proven

## Confidence
- **Medium confidence:** The core claim that CM-based posterior sampling improves robustness for neural network operators has experimental support but lacks rigorous theoretical guarantees
- **Low confidence:** The assumption that CM can accurately approximate PF-ODE solutions is validated empirically but lacks theoretical bounds on approximation quality
- **Medium confidence:** The noise addition heuristic for robustness is empirically justified but lacks theoretical analysis of optimal noise levels

## Next Checks
1. **Quantitative analysis of support membership**: Measure what fraction of CM-generated samples actually lie in the support of the image distribution using density estimation techniques.
2. **Noise sensitivity study**: Systematically vary noise levels (τ) across multiple operator types to identify optimal ranges and failure thresholds.
3. **Cross-architecture generalization**: Test CM-based inversion across different diffusion model architectures to verify the approach's dependence on EDM-specific properties.