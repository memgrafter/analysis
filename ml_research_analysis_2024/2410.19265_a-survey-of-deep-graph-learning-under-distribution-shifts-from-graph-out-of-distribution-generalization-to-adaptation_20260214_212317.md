---
ver: rpa2
title: 'A Survey of Deep Graph Learning under Distribution Shifts: from Graph Out-of-Distribution
  Generalization to Adaptation'
arxiv_id: '2410.19265'
source_url: https://arxiv.org/abs/2410.19265
tags:
- graph
- learning
- distribution
- data
- shifts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of deep graph learning
  under distribution shifts, covering three main scenarios: graph OOD generalization,
  training-time graph OOD adaptation, and test-time graph OOD adaptation. It systematically
  categorizes existing methods into model-centric and data-centric approaches, offering
  insights into techniques for improving model generalizability and adaptability across
  various distribution shifts.'
---

# A Survey of Deep Graph Learning under Distribution Shifts: from Graph Out-of-Distribution Generalization to Adaptation

## Quick Facts
- arXiv ID: 2410.19265
- Source URL: https://arxiv.org/abs/2410.19265
- Reference count: 40
- One-line primary result: Comprehensive survey of deep graph learning under distribution shifts covering three main scenarios: graph OOD generalization, training-time adaptation, and test-time adaptation

## Executive Summary
This survey provides a systematic overview of deep graph learning methods that address distribution shifts across three primary scenarios: graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation. The authors categorize existing methods into model-centric approaches (invariant representation learning, causality-based learning, self-supervised learning, and regularization) and data-centric approaches (graph data augmentation, instance weighting, and distribution augmentation). The survey highlights the unique challenges of graph-structured data under distribution shifts and identifies key research directions including theoretical understanding, label-efficient adaptation, and trustworthy graph learning.

## Method Summary
The survey synthesizes existing research by organizing graph OOD learning methods into a taxonomy based on problem scenarios and solution approaches. Model-centric methods focus on improving the model's inherent ability to handle distribution shifts through techniques like invariant representation learning with adversarial training, causality-based learning with invariant subgraph extraction, graph self-supervised learning, and model regularization. Data-centric methods manipulate input data through graph data augmentation, instance weighting, and distribution augmentation. The survey also summarizes commonly used datasets and highlights future research directions, serving as a comprehensive resource for researchers entering this field.

## Key Results
- Comprehensive categorization of graph OOD learning methods into model-centric and data-centric approaches
- Systematic coverage of three primary scenarios: generalization, training-time adaptation, and test-time adaptation
- Identification of future research directions including theoretical understanding and label-efficient adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph OOD generalization is achievable by learning invariant representations that are robust to distribution shifts across environments.
- Mechanism: Methods decompose graphs into invariant and variant subgraphs, focusing learning on the invariant parts to preserve stable structure-label relationships across domains.
- Core assumption: The existence of invariant subgraphs that maintain consistent structure-label relationships across different distributions.
- Evidence anchors:
  - [abstract] "We cover three primary scenarios: graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation."
  - [section] "Invariant subgraph extraction methods divide each graph into two segments [45], [46]: an invariant subgraph, which maintains stable structure-label relationships across distributions, and a variant subgraph, which may change across different distributions."
  - [corpus] Weak corpus evidence for this specific mechanism; related surveys exist but lack direct support for invariant subgraph decomposition.
- Break condition: If no consistent invariant structure exists across environments, or if the invariant subgraph extraction fails to capture the decisive information for classification.

### Mechanism 2
- Claim: Distribution augmentation compensates for scarce training distributions by generating new distributions of graphs to facilitate robust representation learning.
- Mechanism: Methods generate new distributions by replacing variant subgraphs with those from other graphs or by using subgraph generators to create diverse augmented subgraphs while preserving graph rationale.
- Core assumption: The ability to generate meaningful new graph distributions that maintain the decisive information for the target task while introducing sufficient variance.
- Evidence anchors:
  - [abstract] "we cover three primary scenarios: graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation."
  - [section] "Distribution augmentation focuses on generating new distributions of graphs to facilitate robust representation learning [54]."
  - [corpus] Limited direct corpus evidence; survey mentions related work but no specific validation of distribution augmentation effectiveness.
- Break condition: If the generated distributions are not diverse enough or if they introduce noise that corrupts the invariant information, the augmentation becomes counterproductive.

### Mechanism 3
- Claim: Model regularization through spectral properties and knowledge distillation improves knowledge transfer under distribution shifts.
- Mechanism: Regularization strategies control spectral properties of GNNs or use knowledge distillation to transfer knowledge from source to target graphs while maintaining stability.
- Core assumption: The pre-trained model contains transferable knowledge that can be effectively distilled or that spectral regularization can constrain the model to be more transferable.
- Evidence anchors:
  - [abstract] "graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation."
  - [section] "Model Regularization. Instead of focusing on the process of learning aligned representations, some other methods achieve effective knowledge transfer under distribution shifts through model regularization strategy, including both spectral regularization [16], [137] and knowledge distillation [4], [115]."
  - [corpus] Weak corpus evidence; survey mentions these methods but lacks specific empirical support in related papers.
- Break condition: If the source and target distributions are too dissimilar, regularization may not be sufficient to bridge the gap, leading to negative transfer.

## Foundational Learning

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: Understanding GNNs is fundamental because all methods discussed operate on graph-structured data using message-passing mechanisms to learn node representations.
  - Quick check question: What is the difference between a graph convolutional layer and a graph attention layer in terms of how they aggregate information from neighbors?

- Concept: Distribution Shifts (Covariate and Concept Shifts)
  - Why needed here: The entire survey is about methods that handle distribution shifts, so understanding the types of shifts (covariate vs concept) is crucial for categorizing and evaluating different approaches.
  - Quick check question: How would a covariate shift differ from a concept shift in a citation network where papers cite each other?

- Concept: Domain Adaptation vs. Domain Generalization
  - Why needed here: The survey distinguishes between adaptation (where target data is available) and generalization (where it is not), which is central to understanding the different problem scenarios and corresponding solution strategies.
  - Quick check question: In which scenario would you have access to labeled data from the target domain during training: domain adaptation or domain generalization?

## Architecture Onboarding

- Component map: Three main problem scenarios (generalization, training-time adaptation, test-time adaptation) -> Model-centric approaches (invariant representation learning, causality-based learning, self-supervised learning, regularization) and data-centric approaches (graph data augmentation, instance weighting, distribution augmentation)
- Critical path: For a new engineer, the critical path is understanding the problem scenarios first, then learning the taxonomy, and finally diving into specific techniques within each category.
- Design tradeoffs: Model-centric approaches focus on improving the model's inherent ability but may require more architectural changes, while data-centric approaches manipulate input data which is often simpler but may have scalability issues with large graphs.
- Failure signatures: Methods fail when the core assumptions are violated (e.g., no invariant structure exists, generated distributions are poor quality, or source and target are too dissimilar for regularization to work).
- First 3 experiments:
  1. Implement a simple invariant subgraph extraction method on a small citation network dataset to verify that the method can identify and focus on invariant portions of the graph.
  2. Apply distribution augmentation techniques to a molecular graph dataset and measure the diversity of generated distributions using statistical metrics.
  3. Compare spectral regularization vs knowledge distillation for a semi-supervised node classification task with limited labeled data in the target domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we theoretically analyze and guarantee the performance of graph OOD adaptation when label functions change between source and target domains?
- Basis in paper: [explicit] The paper discusses that when there exist concept shifts in P(Y|X) or P(Y|H), the performance of previous invariant representation learning methods on target is no longer guaranteed, citing theoretical work on upper bounds.
- Why unresolved: Current theoretical analyses focus on covariate shifts and lack frameworks for handling changes in label functions in graph settings.
- What evidence would resolve it: Development of theoretical bounds and guarantees for graph OOD adaptation under concept shifts, along with empirical validation showing improved performance in scenarios with changing label functions.

### Open Question 2
- Question: What are effective techniques for label-efficient test-time graph OOD adaptation when only limited labeled data is available in the target domain?
- Basis in paper: [explicit] The paper identifies label-efficient test-time adaptation as a promising future direction, noting that with sparse labeled examples, the adaptation process may struggle to accurately reflect the underlying distribution of the target domain.
- Why unresolved: Most current test-time adaptation methods assume either full labels or completely unlabeled target data, with limited work addressing the sparse label scenario.
- What evidence would resolve it: Novel algorithms that can effectively leverage limited labeled data in conjunction with unlabeled data during test-time adaptation, validated through experiments showing improved performance over baseline methods.

### Open Question 3
- Question: How can we develop trustworthy graph learning methods that maintain fairness, explainability, and reliability under distribution shifts?
- Basis in paper: [explicit] The paper highlights trustworthy graph learning under distribution shifts as an under-explored area, noting that current metrics on trustworthiness may not be directly applicable to scenarios with distribution shifts.
- Why unresolved: Existing trustworthy graph learning research has not adequately addressed the additional challenges posed by distribution shifts in real-world applications.
- What evidence would resolve it: New theoretical frameworks and practical methods for fairness, explainability, and reliability that explicitly account for distribution shifts, validated through both theoretical analysis and empirical studies on diverse graph datasets.

## Limitations
- Primary limitation is reliance on existing literature without providing original experimental validation
- Limited empirical evidence for the effectiveness of specific mechanisms presented
- Does not address computational complexity or scalability issues for large-scale graphs

## Confidence
- Problem scenario taxonomy: High - clearly structured and comprehensive
- Mechanism 1 (Invariant subgraph extraction): Medium - theoretically sound but limited empirical validation
- Mechanism 2 (Distribution augmentation): Medium - concept well-defined but effectiveness not demonstrated
- Mechanism 3 (Model regularization): Medium - established techniques but transfer success not guaranteed

## Next Checks
1. Conduct ablation studies on a molecular property prediction dataset comparing invariant subgraph extraction with standard GNN baselines to quantify the actual performance gain from focusing on invariant structures.

2. Implement distribution augmentation on a citation network dataset and use statistical tests (e.g., Maximum Mean Discrepancy) to measure whether generated distributions meaningfully differ from the original while preserving task-relevant information.

3. Design a controlled experiment with varying degrees of distribution shift between source and target domains to empirically determine when spectral regularization outperforms knowledge distillation for knowledge transfer.