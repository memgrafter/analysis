---
ver: rpa2
title: 'XMeCap: Meme Caption Generation with Sub-Image Adaptability'
arxiv_id: '2407.17152'
source_url: https://arxiv.org/abs/2407.17152
tags:
- caption
- meme
- memes
- captions
- humor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces XMeCap, a framework for meme caption generation
  that handles both single-image and multi-image memes with different emotional contexts.
  XMeCap employs supervised fine-tuning and reinforcement learning, using a reward
  model that incorporates global and local visual-textual similarities.
---

# XMeCap: Meme Caption Generation with Sub-Image Adaptability

## Quick Facts
- arXiv ID: 2407.17152
- Source URL: https://arxiv.org/abs/2407.17152
- Authors: Yuyan Chen; Songzhou Yan; Zhihong Zhu; Zhixu Li; Yanghua Xiao
- Reference count: 40
- Key outcome: XMeCap achieves 75.85 average score for single-image memes and 66.32 for multi-image memes, exceeding the best baseline by 6.75% and 8.56% respectively on a newly constructed Chinese meme dataset.

## Executive Summary
XMeCap introduces a framework for meme caption generation that handles both single-image and multi-image memes with different emotional contexts. The approach employs supervised fine-tuning and reinforcement learning, using a reward model that incorporates global and local visual-textual similarities. XMeCap features adaptive feature extraction, including image enhancement and structured text generation using a "chain-of-humor" template. Evaluation on a newly constructed Chinese meme dataset shows XMeCap outperforms baselines with average scores of 75.85 for single-image memes and 66.32 for multi-image memes, exceeding the best baseline by 6.75% and 8.56% respectively. The method also demonstrates strong performance in multi-modal humor detection tasks, highlighting its adaptability and effectiveness in understanding humor across different meme structures and sentiments.

## Method Summary
XMeCap is a framework for meme caption generation that handles both single-image and multi-image memes. It employs supervised fine-tuning (SFT) and reinforcement learning (RL) with a reward model trained on human evaluations. The approach features adaptive feature extraction with image enhancement (AutoAugment) and structured text generation using a "chain-of-humor" template. The model computes global and token-level similarities between visual and textual features using self-attention, and uses these as supervision signals during training. The framework was evaluated on a newly constructed Chinese meme dataset with 12,320 memes, balanced across sentiment categories (self-praise, praise others, self-mockery, mockery of others).

## Key Results
- XMeCap achieves average scores of 75.85 for single-image memes and 66.32 for multi-image memes on human evaluation metrics
- The model outperforms the best baseline by 6.75% and 8.56% respectively
- Strong performance demonstrated in multi-modal humor detection tasks
- The framework shows adaptability across different meme structures and sentiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adaptive transformation layer aligns image and text features across different granularities, enabling both global context and token-level detail to inform caption generation.
- Mechanism: The framework first projects image and text features independently into a shared embedding space. Then, a self-attention mechanism computes token-wise similarities between image areas and caption words. Global attention is obtained by averaging token-level scores, and both levels are used as supervision signals during training.
- Core assumption: Token-level similarity between image regions and caption words reflects the true semantic correspondence, and averaging these provides a valid global representation.
- Evidence anchors:
  - [abstract] "factors in both global and local similarities between visuals and text"
  - [section] "use the self-attention mechanism of the LLM to calculate the token-level similarity between each area in the image and each word in the caption...To compute the global attention, denoted as S_I,j, we average the token-level attention"
  - [corpus] Weak. Corpus shows related works but no direct evidence for multi-granularity supervision effectiveness.
- Break condition: If the attention distribution is noisy or unaligned with human interpretation, the similarity signal degrades and hurts caption quality.

### Mechanism 2
- Claim: The "chain-of-humor" template provides structured scaffolding that guides the model to generate captions containing core concept, emotion, event, consequence, and humor elements, which improves both coherence and humor.
- Mechanism: After initial feature extraction, descriptions of each sub-image are transformed using a predefined template. This structured text serves as an intermediate representation before final caption generation, ensuring key humor components are present.
- Core assumption: Humor generation benefits from explicit representation of narrative components rather than free-form text generation.
- Evidence anchors:
  - [section] "these descriptions are transformed into structured text with a 'chain-of-humor' template...This template includes creating a narrative that encompasses the core concept...emotion...event...consequence...and humor elements"
  - [corpus] Weak. Related works focus on generation but do not validate structured templates specifically.
- Break condition: If the template constrains creativity too tightly, it may suppress emergent humor or lead to formulaic outputs.

### Mechanism 3
- Claim: The combination of supervised fine-tuning (SFT) and reinforcement learning (RL) aligns model outputs with both ground-truth captions and human preferences, improving informativeness, relevance, creativity, and humor.
- Mechanism: SFT minimizes token-level and global similarity losses while aligning predicted similarities with prior knowledge. RL uses a reward model trained on human-scored captions to further optimize for preferred outputs, balancing reward and KL divergence to maintain coherence with the SFT baseline.
- Core assumption: Human preference rankings correlate with the defined quality metrics (informativeness, relevance, creativity, humor) and can be captured in a reward model.
- Evidence anchors:
  - [abstract] "incorporates these multi-granularity similarities as a part of the reward signal"
  - [section] "build a reward model to align captions with human preferences...annotators are three volunteers...ranking according to the scoring criteria...apply the Pairwise Ranking Loss"
  - [corpus] Moderate. Works on humor detection and multimodal sentiment show human preference alignment is feasible, but reward model effectiveness in generation is less directly evidenced.
- Break condition: If the reward model overfits to a narrow set of human judgments, it may fail to generalize to new meme styles or cultures.

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Core to computing token-wise similarity between image regions and caption words, and to conditioning caption generation on visual content.
  - Quick check question: What role does the query-key-value decomposition play in computing attention scores between two modalities?

- Concept: Contrastive learning for alignment
  - Why needed here: Used to maximize similarity between matched image-caption pairs and minimize similarity for mismatched pairs, sharpening the embedding space.
  - Quick check question: How does a temperature parameter influence the penalty for negative pairs in contrastive loss?

- Concept: Reinforcement learning with human feedback
  - Why needed here: Allows fine-tuning the model to generate captions that match human judgments on quality, beyond strict adherence to ground truth.
  - Quick check question: Why is a KL divergence term added when using RL for caption generation?

## Architecture Onboarding

- Component map:
  - Image encoder (LMM: LLaVA-1.5-7B) -> image feature extraction
  - Text encoder (LLM: Baichuan2-7B) -> text feature extraction
  - AutoAugment -> image enhancement
  - Back-translation -> text augmentation
  - Adaptive transformation layer -> shared embedding projection + token-wise similarity
  - SFT module -> initial fine-tuning with similarity supervision
  - Reward model -> human preference scoring
  - RL module -> final fine-tuning with reward and KL loss
  - Output: Generated caption

- Critical path:
  1. Load image and detect sub-images (multi-image case)
  2. Apply AutoAugment and extract visual features
  3. Apply back-translation and extract textual features
  4. Pass features through adaptive transformation
  5. Compute global and token-wise similarities
  6. Fine-tune via SFT using similarity losses
  7. Generate captions with SFT model
  8. Score captions with reward model
  9. Fine-tune via RL using reward and KL loss
  10. Output final caption

- Design tradeoffs:
  - Multi-image vs single-image: Multi-image requires ROI detection and per-sub-image processing; adds complexity but enables cross-sub-image coherence.
  - Global vs token-level supervision: Global gives coarse alignment; token-level offers fine-grained control but risks overfitting to noisy attention.
  - Human preference vs ground truth: Human preference improves engagement; ground truth ensures factual alignment. Balancing both is critical.

- Failure signatures:
  - Captions are overly generic or unrelated -> attention alignment broken or similarity loss misweighted.
  - Captions are repetitive or formulaic -> chain-of-humor template too rigid.
  - Captions miss humor or are offensive -> reward model misgeneralized or insufficient human preference data.

- First 3 experiments:
  1. Ablation: Remove token-wise similarity loss (Lt) and retrain; measure impact on humor and relevance scores.
  2. Ablation: Remove chain-of-humor template; compare caption coherence and creativity.
  3. Ablation: Replace human-scored reward model with automatic metrics (e.g., CIDEr, METEOR); measure robustness and generalizability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does XMeCap's performance vary across different cultural contexts when generating meme captions?
- Basis in paper: [inferred] The paper mentions that future work should explore "cross-cultural adaptability of our method, understanding the subtle variations in humor across different societies" and that "integrating more sophisticated semantic analysis tools could further refine the quality of generated captions."
- Why unresolved: The current research focuses on a Chinese meme dataset and does not investigate how the model performs with memes from other cultural backgrounds or languages.
- What evidence would resolve it: Conducting experiments with meme datasets from multiple cultures and languages, comparing performance metrics across these datasets, and analyzing how cultural nuances affect caption generation quality.

### Open Question 2
- Question: What is the optimal balance between image augmentation and text augmentation techniques for maximizing meme caption quality?
- Basis in paper: [explicit] The ablation study shows that "the effects of image and text augmentation were relatively subdued, suggesting that mere alterations without changing the core content of images or captions may not substantially enhance performance."
- Why unresolved: While the paper tests the impact of removing these components, it doesn't explore different combinations or intensities of augmentation techniques to find an optimal balance.
- What evidence would resolve it: Systematic experimentation varying the types, intensities, and combinations of image and text augmentation techniques while measuring their impact on caption quality metrics.

### Open Question 3
- Question: How does XMeCap's attention mechanism contribute to understanding multi-image meme structures compared to single-image memes?
- Basis in paper: [explicit] The paper states that "the attention mechanism displays the most pronounced effect, underscoring the significance of cross-modal interaction" and demonstrates strong performance on both single and multi-image memes.
- Why unresolved: While the attention mechanism is identified as important, the paper doesn't provide detailed analysis of how the model uses attention differently when processing multi-image memes versus single-image memes.
- What evidence would resolve it: Visualizing and analyzing the attention patterns specifically for multi-image memes, comparing them to single-image attention patterns, and quantifying how these differences contribute to caption quality.

## Limitations

- The performance claims hinge on a newly constructed Chinese meme dataset with limited transparency about data diversity, preprocessing, and evaluation stability
- The "chain-of-humor" template's necessity and impact on humor quality is not empirically validated
- The reward model is trained on a small subset (1%) of human-scored captions, raising concerns about coverage and bias

## Confidence

**High Confidence**
- The overall framework design (SFT + RL with multi-granularity supervision) is technically sound and aligns with established methods in multimodal generation
- The claim that XMeCap can handle both single-image and multi-image memes is well-supported by the architectural description

**Medium Confidence**
- The reported quantitative improvements (75.85 vs. 69.10 for single-image, 66.32 vs. 61.04 for multi-image) are plausible given the method, but depend heavily on the reliability of the constructed dataset and human evaluation protocol
- The claim that XMeCap "demonstrates strong performance in multi-modal humor detection tasks" is not substantiated by results in the paper and appears to be an overstatement

**Low Confidence**
- The claim that the "chain-of-humor" template is essential for improved humor and coherence is not empirically validated
- The assertion that the reward model reliably captures human preferences for humor is speculative, given the limited annotation and lack of reward model validation

## Next Checks

1. **Ablation Study on Template Structure**: Remove the "chain-of-humor" template and retrain the model. Compare generated captions on human evaluation metrics (coherence, humor, creativity) and automatic metrics (CIDEr, METEOR) to isolate the template's contribution.

2. **Statistical Validation of Human Evaluations**: Report inter-annotator agreement (e.g., Krippendorff's alpha or Cohen's kappa) and perform statistical tests (e.g., paired t-tests) to confirm that performance improvements are significant and not due to annotator variability or chance.

3. **Reward Model Robustness Test**: Train a baseline reward model using only automatic metrics (CIDEr, BLEU, etc.) instead of human scores. Compare the final RL-tuned model's performance on human evaluation to assess whether human preference modeling provides a measurable advantage over purely automatic supervision.