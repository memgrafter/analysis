---
ver: rpa2
title: Non-autoregressive Sequence-to-Sequence Vision-Language Models
arxiv_id: '2403.02249'
source_url: https://arxiv.org/abs/2403.02249
tags:
- sequence
- tokens
- decoder
- narvl
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the slow inference speed of autoregressive
  vision-language models by proposing NARVL, a non-autoregressive sequence-to-sequence
  model that generates all output tokens in parallel. The key innovation is the Query-CTC
  (Q-CTC) loss, which marginalizes over multiple inference paths in the decoder by
  leveraging learnable query tokens, enabling joint modeling of token distributions
  rather than sequential conditional distributions.
---

# Non-autoregressive Sequence-to-Sequence Vision-Language Models

## Quick Facts
- arXiv ID: 2403.02249
- Source URL: https://arxiv.org/abs/2403.02249
- Authors: Kunyu Shi; Qi Dong; Luis Goncalves; Zhuowen Tu; Stefano Soatto
- Reference count: 40
- One-line primary result: NARVL achieves 1.4x-12.7x faster inference than autoregressive models while maintaining comparable performance on vision-language tasks

## Executive Summary
This paper introduces NARVL, a non-autoregressive sequence-to-sequence model for vision-language tasks that generates all output tokens in parallel rather than sequentially. The key innovation is the Query-CTC (Q-CTC) loss, which marginalizes over multiple inference paths in the decoder by leveraging learnable query tokens. NARVL achieves comparable performance to state-of-the-art autoregressive models across visual question answering, visual grounding, image captioning, and visual entailment tasks, while achieving significant speedups due to its parallel decoding architecture.

## Method Summary
NARVL modifies the standard Transformer encoder-decoder architecture by replacing the autoregressive decoder with a parallel decoder that takes a fixed sequence of learnable query tokens as input. The model is trained using a custom Q-CTC loss that marginalizes over multiple generation paths from the query tokens to the output sequence, combined with knowledge distillation from a pre-trained autoregressive model. During inference, all output tokens are generated simultaneously, reducing the linear complexity of sequential generation to constant-time joint inference.

## Key Results
- Achieves 36.4 BLEU-4 score on MSCOCO image captioning (vs 36.7 for autoregressive baseline)
- 89.0 accuracy on SNLI-VE visual entailment (vs 89.3 for baseline)
- 72.1 accuracy on GQA visual question answering (vs 73.1 for baseline)
- 1.4x to 12.7x faster inference speed compared to autoregressive models
- Speedups are most significant for tasks with longer output sequences (captioning, VQA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Q-CTC loss marginalizes over multiple generative paths from learnable query tokens to output tokens
- Mechanism: The loss function computes cross-entropy over all valid paths from the sequence of learnable query tokens (which have redundancy) to the ground truth output sequence, using the collapse operation B to remove blanks and merge repeated tokens
- Core assumption: Multiple valid paths exist from learnable query tokens to output tokens, and marginalizing over these paths provides better gradient signals than single-path training
- Evidence anchors:
  - [abstract]: "trained with a Query-CTC loss, that marginalizes over multiple inference paths in the decoder"
  - [section 3.2.1]: "We denote the collapsed valid sequences as ˜p(z|x; q). Before x is instantiated at inference time, zi are random variables, functions of x, which we need to marginalize in the loss"
  - [corpus]: No direct corpus evidence for Q-CTC in vision-language models specifically
- Break condition: If the collapse operation B produces ambiguous or incorrect token sequences, or if the number of learnable query tokens doesn't provide sufficient diversity for meaningful marginalization

### Mechanism 2
- Claim: Parallel decoding with learnable query tokens avoids the sequential complexity of autoregressive models
- Mechanism: The decoder takes a fixed sequence of learnable query tokens as input and generates all output tokens in parallel, rather than generating tokens one-by-one conditioned on previous outputs
- Core assumption: The learnable query tokens can encode sufficient information to generate the complete output sequence without sequential conditioning
- Evidence anchors:
  - [abstract]: "generates all output tokens in parallel" and "reducing from the linear complexity associated with the sequential generation of tokens to a paradigm of constant time joint inference"
  - [section 3.1]: "Our NARVL decoder also takes as input the output of the encoder, but it replaces the input sequence with a fixed-length and constant sequence of tokens, representing the task-specific hypothesis space"
  - [section 3.2.3]: "During the inference, for each output token, we use the text token in the vocabulary with the largest probability as prediction"
- Break condition: If the learnable query tokens cannot capture the necessary context for generating coherent sequences, or if the parallel generation fails to maintain inter-token dependencies

### Mechanism 3
- Claim: Knowledge distillation simplifies the learning task by providing deterministic targets
- Mechanism: A pre-trained autoregressive model (teacher) provides target sequences for training the non-autoregressive model, reducing output sequence variability and making the learning task more deterministic
- Core assumption: The teacher model produces more consistent and structured outputs than ground truth sequences, especially for tasks with high variability like image captioning
- Evidence anchors:
  - [section 3.2.2]: "To reduce the solution space in model training, we exploit two simple knowledge distillation mechanisms[20] in training"
  - [section 4.1]: "We observe that this knowledge distillation benefits the task of long sequence outputs such as image captioning"
  - [section 4.3]: "We found the default OFA model that uses all-candidate inference is quite slow, and we additionally benchmark and report the faster beam-search version"
- Break condition: If the teacher model produces incorrect or suboptimal outputs that mislead the student model, or if the distillation process reduces the model's ability to handle diverse outputs

## Foundational Learning

- Concept: CTC (Connectionist Temporal Classification) loss
  - Why needed here: Provides the theoretical foundation for marginalizing over multiple alignment paths between input and output sequences
  - Quick check question: What is the key difference between standard CTC loss and Query-CTC loss as described in this paper?

- Concept: Sequence-to-sequence modeling with Transformers
  - Why needed here: The architecture builds on standard Transformer encoder-decoder structure but modifies the decoder for non-autoregressive generation
  - Quick check question: How does the attention mechanism differ between autoregressive and non-autoregressive decoders?

- Concept: Knowledge distillation
  - Why needed here: Used to simplify the training process and improve performance, particularly for tasks with high output variability
  - Quick check question: What is the primary benefit of using knowledge distillation for non-autoregressive models compared to autoregressive models?

## Architecture Onboarding

- Component map:
  Encoder -> Learnable Query Tokens -> Parallel Decoder -> Output Sequence

- Critical path: Image/Text → Encoder → Learnable Query Tokens → Parallel Decoder → Output Sequence

- Design tradeoffs:
  - Query token count vs. output sequence length (need sufficient redundancy)
  - Training complexity vs. inference speed (parallel vs. sequential)
  - Model capacity vs. speed (larger models have more overhead)

- Failure signatures:
  - Poor output quality indicating learnable query tokens lack sufficient context
  - Training instability suggesting marginalization over too many paths
  - Speed degradation indicating inefficient parallel implementation

- First 3 experiments:
  1. Verify parallel generation works: Compare output quality of single parallel pass vs. sequential generation
  2. Test query token count: Vary number of learnable query tokens and measure impact on quality and speed
  3. Validate Q-CTC vs. standard CE: Train with both loss functions and compare convergence and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimization difficulty of NARVL be addressed without relying on knowledge distillation from autoregressive models?
- Basis in paper: [explicit] The paper states that "Optimization of the NAR models is difficult, and the best performance we get on Image Captioning and VQA datasets rely on distillation from an autoregressive model, which is inconvenient in practice as one needs to train two models."
- Why unresolved: Knowledge distillation requires training an additional autoregressive model, adding computational overhead and complexity to the training pipeline.
- What evidence would resolve it: Demonstrating comparable performance on captioning and VQA tasks using alternative optimization strategies such as curriculum learning, better initialization methods, or architectural modifications that improve the learning dynamics of NARVL without requiring an autoregressive teacher model.

### Open Question 2
- Question: Can the Q-CTC loss be further generalized to handle more complex output structures beyond sequences, such as hierarchical or graph-structured outputs?
- Basis in paper: [explicit] The paper introduces Q-CTC loss for sequence modeling and notes that "This innovation is key to enabling the flexibility of letting the task drive the design of the hypothesis space," suggesting potential for broader application.
- Why unresolved: The current Q-CTC formulation is designed for sequential outputs, and its extension to non-sequential, structured outputs like graphs or trees is not explored.
- What evidence would resolve it: Developing and validating a generalized Q-CTC variant that can effectively marginalize over multiple inference paths for hierarchical or graph-structured outputs, with empirical demonstrations on tasks like scene graph generation or structured prediction.

### Open Question 3
- Question: What is the theoretical limit of inference speedup achievable with NARVL compared to autoregressive models, and under what conditions?
- Basis in paper: [explicit] The paper demonstrates speedups ranging from 1.4x to 12.7x, noting that "the significant speedups are on VQA and Captioning datasets, because the length of output sequence is longer than that of the grounding task."
- Why unresolved: While empirical results show substantial speedups, a theoretical analysis of the speedup limits based on output sequence length, model architecture, and hardware characteristics is missing.
- What evidence would resolve it: A theoretical framework that models the relationship between output sequence length, parallel computation efficiency, and hardware constraints to predict the maximum achievable speedup, validated through controlled experiments across diverse tasks and hardware configurations.

## Limitations

- The Q-CTC mechanism relies on learnable query tokens that may not capture sufficient contextual information for all vision-language tasks
- Knowledge distillation approach may inherit biases from the teacher model and reduce the model's ability to handle diverse outputs
- The paper doesn't extensively validate whether the number of query tokens (set to 30) is optimal across different tasks

## Confidence

**High Confidence**: Claims about inference speed improvements (1.4x-12.7x) are well-supported by empirical measurements across multiple tasks and compared against the autoregressive baseline (OFA).

**Medium Confidence**: Claims about comparable performance to autoregressive models (e.g., 36.4 BLEU-4 on MSCOCO, 89.0 accuracy on SNLI-VE) are supported by benchmark results, but the paper doesn't extensively explore failure modes or provide detailed ablation studies on the Q-CTC loss components.

**Low Confidence**: The theoretical claims about Q-CTC's superiority in marginalizing over multiple paths lack empirical validation through controlled experiments comparing different marginalization strategies.

## Next Checks

1. **Query Token Sensitivity Analysis**: Systematically vary the number of learnable query tokens (e.g., 10, 20, 30, 40) across all tasks and measure the impact on both performance and inference speed to determine if the current setting is optimal.

2. **Ablation of Q-CTC Components**: Compare NARVL trained with standard cross-entropy loss, CTC loss, and Q-CTC loss on the same tasks to isolate the specific contribution of the marginalization mechanism versus other architectural changes.

3. **Teacher Model Dependency**: Evaluate NARVL performance when trained with different teacher models (e.g., OFA with different configurations, other autoregressive models) to assess how much the knowledge distillation component contributes to the reported performance gains.