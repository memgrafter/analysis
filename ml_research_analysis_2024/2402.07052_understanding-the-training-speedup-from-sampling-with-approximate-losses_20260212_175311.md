---
ver: rpa2
title: Understanding the Training Speedup from Sampling with Approximate Losses
arxiv_id: '2402.07052'
source_url: https://arxiv.org/abs/2402.07052
tags:
- sift
- training
- layer
- early
- approximate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of approximate loss values for
  sample selection in stochastic gradient descent (SGD) to accelerate training. The
  authors theoretically show that greedy SGD (GSGD), which selects samples with the
  largest approximate losses, can converge faster than standard SGD for smooth convex
  objectives, reaching a constant factor of the minimum loss in fewer iterations.
---

# Understanding the Training Speedup from Sampling with Approximate Losses

## Quick Facts
- arXiv ID: 2402.07052
- Source URL: https://arxiv.org/abs/2402.07052
- Authors: Rudrajit Das; Xi Chen; Bertram Ieong; Parikshit Bansal; Sujay Sanghavi
- Reference count: 40
- Key outcome: SIFT reduces BERT training time by ~13 hours (43h vs 57h) while maintaining 64% validation accuracy by selecting samples with highest approximate losses from early exiting

## Executive Summary
This paper addresses the challenge of accelerating neural network training by proposing a sample selection strategy that uses approximate loss values to identify the most informative training examples. The authors introduce Greedy SGD (GSGD), a theoretical framework showing that selecting samples with largest approximate losses can converge faster than random sampling for smooth convex objectives. They then develop SIFT, a practical implementation using early exiting in neural networks to cheaply obtain these approximate losses. Experiments demonstrate significant training time reductions on BERT and ResNet models without requiring optimized implementations.

## Method Summary
The method involves computing approximate losses for all samples in a batch using intermediate layer representations (early exiting), then selecting the top fraction (50%) of samples with highest approximate losses for full forward and backward passes. This reduces the number of backpropagation steps while maintaining training effectiveness. The approach is theoretically grounded with convergence guarantees for convex objectives and validated empirically on BERT base and modified ResNet-50 architectures.

## Key Results
- SIFT with early exit at first layer takes approximately 43 hours to reach 64% validation accuracy on BERT, compared to 57 hours for vanilla training
- The method significantly reduces the number of backpropagation steps while maintaining competitive validation performance
- Theoretical analysis shows GSGD can converge to a constant factor of the minimum loss in fewer iterations than standard SGD for smooth convex objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting samples with largest approximate losses converges faster than random sampling for smooth convex objectives
- Mechanism: Prioritizes high-loss samples which are more informative for gradient updates
- Core assumption: argmax of approximate losses equals argmax of actual losses
- Evidence anchors: Theorem 5.7 provides convergence bound for GSGD on smooth convex losses

### Mechanism 2
- Claim: Early exiting provides cheap approximate losses for sample selection
- Mechanism: Intermediate-layer representations compute approximate losses; top samples get full backpropagation
- Core assumption: Early-exit loss correlates strongly with final loss
- Evidence anchors: Theorem 6.2 quantifies probability of correctly selecting highest-loss sample

### Mechanism 3
- Claim: Converging faster to constant factor of minimum loss is practically valuable for large-scale training
- Mechanism: Trades asymptotic convergence for faster early progress toward good-enough solution
- Core assumption: Target metrics plateau before exact loss minimization
- Evidence anchors: Abstract notes converging faster to O(F*) is desirable when exact convergence is infeasible

## Foundational Learning

- Concept: Stochastic Gradient Descent (SGD) and its variants
  - Why needed here: Baseline algorithm that GSGD modifies for faster convergence
  - Quick check question: What is the update rule for vanilla SGD with step-size Î· and how does it differ from GSGD?

- Concept: Importance sampling in optimization
  - Why needed here: Positions approach relative to existing methods for reducing gradient variance
  - Quick check question: Why is exact importance sampling (sampling proportional to gradient norms) infeasible in practice?

- Concept: Early exiting in neural networks
  - Why needed here: Key mechanism for obtaining approximate losses cheaply in large models
  - Quick check question: How does early exiting reduce computational cost during inference, and how is this adapted for training in SIFT?

## Architecture Onboarding

- Component map: Data pipeline -> Model (BERT/ResNet) -> Early exit layer -> Loss computation -> Sample selection -> Full forward pass (selected samples) -> Backward pass -> Parameter update

- Critical path: 1) Forward pass through early exit layer for all samples, 2) Compute approximate losses/entropies, 3) Select top 50% samples by approximate loss, 4) Full forward pass on selected samples, 5) Backward pass and parameter update

- Design tradeoffs: 50% sampling balances speedup vs. accuracy; entropy vs. loss for selection; earlier layers cheaper but less correlated with final loss

- Failure signatures: Poor correlation between approximate and actual losses causing validation accuracy plateau; sampling overhead negating speedup; insufficient model size/dataset complexity

- First 3 experiments: 1) Implement SIFT with early exit at first layer on BERT base; compare validation accuracy and training time vs. baseline, 2) Vary fraction of samples selected (25%, 50%, 75%) to find optimal balance, 3) Test entropy-based vs. loss-based sample selection

## Open Questions the Paper Calls Out

- Question: How does SIFT performance scale with model and dataset size?
- Basis: Paper evaluates on specific BERT base and ResNet sizes but doesn't explore larger models
- Why unresolved: Only presents results on specific model/dataset sizes
- What evidence would resolve it: Experiments on larger transformers and extensive datasets

- Question: Can SIFT integrate with other sample selection schemes beyond entropy and loss?
- Basis: Paper mentions seamless integration with other schemes but only demonstrates two approaches
- Why unresolved: Only explores two specific sample selection criteria
- What evidence would resolve it: Experiments with alternative selection criteria (gradient norm, uncertainty sampling)

## Limitations

- Theoretical analysis assumes smooth convex losses, but neural networks are non-convex
- Early exiting mechanism for transformers not fully specified, making exact reproduction difficult
- Paper doesn't address potential bias from systematically selecting harder examples
- Computational overhead of approximate loss computation not quantified relative to savings

## Confidence

- Theoretical claims: High (rigorous convergence proofs for convex objectives)
- Practical implementation: Medium (empirical validation but missing details on early exiting)
- Real-world applicability: Medium (promising results but limited model diversity and lack of comparison to other methods)

## Next Checks

1. Reproduce BERT base training with SIFT: Implement early exiting mechanism and verify 13-hour speedup claim with 64% validation accuracy
2. Test correlation decay across layers: Measure correlation between intermediate and final layer losses across different early exit positions
3. Compare against diversity-based sampling: Implement diversity-aware baseline and compare validation accuracy and training time to SIFT