---
ver: rpa2
title: 'MaxMin-RLHF: Alignment with Diverse Human Preferences'
arxiv_id: '2402.08925'
source_url: https://arxiv.org/abs/2402.08925
tags:
- human
- reward
- preferences
- rlhf
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of aligning language models
  to diverse human preferences, which is critical for ensuring equitable representation
  of all user groups. The authors identify a fundamental limitation in current single-reward
  RLHF approaches: they fail to capture the rich diversity of human preferences, leading
  to bias towards majority groups and neglect of minority preferences.'
---

# MaxMin-RLHF: Alignment with Diverse Human Preferences

## Quick Facts
- arXiv ID: 2402.08925
- Source URL: https://arxiv.org/abs/2402.08925
- Reference count: 40
- Single-reward RLHF fails to capture diverse human preferences, leading to majority group bias

## Executive Summary
This paper addresses the fundamental challenge of aligning language models to diverse human preferences, which is critical for ensuring equitable representation across all user groups. Current single-reward RLHF approaches suffer from a critical limitation: they fail to capture the rich diversity of human preferences, leading to bias toward majority groups and neglect of minority preferences. To overcome this, the authors propose MaxMin-RLHF, a novel approach that learns a mixture of reward models via an Expectation-Maximization algorithm and solves a MaxMin alignment objective inspired by the Egalitarian principle in social choice theory. This ensures the language model aligns with preferences of all user groups, including minorities, without compromising majority group performance.

## Method Summary
The proposed MaxMin-RLHF algorithm consists of two key components: an Expectation-Maximization (EM) algorithm to learn a mixture of reward models from diverse preference data, and a MaxMin policy iteration approach that selects the minimum utility subpopulation and performs PPO updates to maximize the worst-case reward across all user groups. The EM algorithm initializes with multiple reward functions and iteratively assigns users to clusters based on maximum likelihood, learning distinct reward models for each user group. The MaxMin policy iteration then ensures alignment with the social utility objective by optimizing for the worst-case scenario across all groups. The approach is validated through comprehensive experiments on both small-scale (GPT-2) and large-scale (Tulu2-7B) language models, demonstrating significant improvements in win-rates and minority group accuracy.

## Key Results
- Achieves an average improvement of over 16% in win-rates compared to conventional RLHF algorithms
- Improves win-rate (accuracy) for minority groups by over 33% without compromising majority group performance
- Demonstrates robustness and fairness through experiments on both small-scale GPT-2 and large-scale Tulu2-7B language models

## Why This Works (Mechanism)
MaxMin-RLHF works by explicitly modeling the heterogeneity in human preferences through a mixture of reward models, rather than assuming a single unified preference distribution. The EM algorithm discovers distinct user groups with different preference patterns, while the MaxMin objective ensures that the learned policy performs well for all groups by optimizing the worst-case scenario. This approach directly addresses the averaging problem in single-reward RLHF, where minority preferences get washed out when combined with majority preferences.

## Foundational Learning

**Expectation-Maximization Algorithm**: Iterative method for finding maximum likelihood estimates when data has latent variables (user group membership). Needed because we don't know which user belongs to which preference group upfront. Quick check: Verify EM converges to distinct clusters that match ground truth user group labels.

**Social Choice Theory & Egalitarian Principle**: Framework for aggregating preferences from multiple individuals/groups fairly. Needed to ensure minority preferences aren't ignored in favor of majority preferences. Quick check: Verify MaxMin objective actually optimizes for worst-case group performance.

**Reward Misspecification**: When reward functions fail to capture true human preferences due to averaging or oversimplification. Needed to understand why single-reward RLHF fails with diverse preferences. Quick check: Compare single-reward vs multi-reward performance on minority group tasks.

## Architecture Onboarding

**Component Map**: Preference Data -> EM Algorithm -> Mixture of Reward Models -> MaxMin Policy Iteration -> Aligned Language Model

**Critical Path**: The EM algorithm must successfully learn distinct reward models for each user group before MaxMin policy iteration can effectively optimize for all groups. If EM fails to separate groups properly, the MaxMin objective has nothing meaningful to optimize.

**Design Tradeoffs**: The MaxMin approach guarantees fairness across groups but may sacrifice some efficiency compared to optimizing for average performance. The EM algorithm adds computational overhead but is necessary for discovering heterogeneous preferences.

**Failure Signatures**: Single reward RLHF failing to capture minority preferences due to averaging effect; EM algorithm failing to converge to distinct user groups.

**3 First Experiments**:
1. Implement EM algorithm to learn mixture of reward models from preference data
2. Implement MaxMin-RLHF policy iteration algorithm with PPO updates
3. Validate on small-scale GPT-2 experiment using IMDB dataset with sentiment and conciseness preferences

## Open Questions the Paper Calls Out
None

## Limitations
- The approach requires sufficient data from each user group to learn meaningful reward models
- Computational overhead of learning multiple reward models and running MaxMin optimization
- Performance depends on quality of preference data and ability to identify distinct user groups

## Confidence

**High confidence** in the fundamental technical claims about single-reward RLHF's limitations for diverse preference alignment, based on well-established literature on reward misspecification and social choice theory.

**Medium confidence** in the empirical validation claims due to unspecified experimental details, need for verification of baseline comparisons and statistical significance testing, and dependence on preference datasets not fully described in the abstract.

## Next Checks

1. Replicate the small-scale GPT-2 experiment with IMDB dataset to verify the 16%+ win-rate improvement over baseline RLHF, including proper statistical testing across multiple runs

2. Analyze the learned reward model mixture components to confirm they capture distinct preference clusters corresponding to different user groups

3. Test the algorithm's robustness by varying the proportion of minority vs majority preferences in the training data to see if the MaxMin objective consistently protects minority performance