---
ver: rpa2
title: 'Accelerated Smoothing: A Scalable Approach to Randomized Smoothing'
arxiv_id: '2402.07498'
source_url: https://arxiv.org/abs/2402.07498
tags:
- smoothing
- randomized
- certified
- radius
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Accelerated Smoothing, a method that replaces
  Monte Carlo sampling in randomized smoothing with a trained surrogate neural network
  to predict class-wise counts. The surrogate model is trained using Jensen-Shannon
  divergence to map inputs to the probability distribution over classes obtained via
  Monte Carlo sampling.
---

# Accelerated Smoothing: A Scalable Approach to Randomized Smoothing

## Quick Facts
- arXiv ID: 2402.07498
- Source URL: https://arxiv.org/abs/2402.07498
- Reference count: 40
- Key outcome: Replaces Monte Carlo sampling in randomized smoothing with a trained surrogate neural network, achieving nearly 600× speedup while maintaining similar certified accuracy.

## Executive Summary
This paper introduces Accelerated Smoothing, a method that replaces the computationally expensive Monte Carlo sampling in randomized smoothing with a trained surrogate neural network to predict class-wise counts. By training a ResNet-based surrogate model to predict normalized class counts using Jensen-Shannon divergence loss, the method achieves constant O(1) certification time compared to the traditional linear O(N) complexity. Experimental results on CIFAR-10 show that Accelerated Smoothing preserves certified accuracy within 5-10% of the original method while drastically reducing computation time from hours to seconds.

## Method Summary
The method involves training a surrogate neural network to predict class-wise counts that would normally be obtained via Monte Carlo sampling. The surrogate model is trained on Monte Carlo sampled class counts vectors generated from a pre-trained base classifier with Gaussian noise augmentation. During certification, the algorithm first predicts the top class using the base classifier with small n₀ samples, then uses the surrogate model to estimate counts for radius calculation via statistical bounds (Clopper-Pearson intervals). The method is model-agnostic and maintains the original smoothed classifier's predictions while accelerating certification.

## Key Results
- Achieves nearly 600× speedup in certification time compared to traditional Monte Carlo sampling
- Maintains certified accuracy within 5-10% of baseline methods across various radii (0.25 to 1.5)
- Reduces certification time from linear O(N) to constant O(1) complexity
- Experimental validation on CIFAR-10 shows consistent performance with mean variance within 5% of all samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accelerated Smoothing reduces certification time from O(N) to O(1) by training a surrogate model to predict class-wise counts.
- Mechanism: Instead of running Monte Carlo sampling with N noisy samples per input, the surrogate model h_θ(x) directly predicts the normalized class counts vector, allowing certification in a single forward pass.
- Core assumption: The surrogate model can approximate the class counts distribution well enough to preserve the ranking and probabilistic bounds required for certification.
- Evidence anchors: [abstract] "replacing Monte Carlo sampling with the training of a surrogate neural network" - [section IV.B] "we train a surrogate neural network that aims to predict the class-wise counts that is obtained via Monte Carlo sampling"

### Mechanism 2
- Claim: The surrogate model is trained using Jensen-Shannon divergence to approximate the probability distribution over classes.
- Mechanism: JS divergence is symmetric and bounded, making it suitable for training a model to match the normalized class counts vector C, which represents a probability distribution.
- Core assumption: The normalized class counts from Monte Carlo sampling approximate the true class probabilities well enough for training.
- Evidence anchors: [section IV.B] "Since we are estimating a probability distribution over classes, hence we use JS divergence as our loss function"

### Mechanism 3
- Claim: The certification algorithm only uses the surrogate model for radius calculation, not for classification, preserving the original smoothed classifier's predictions.
- Mechanism: The algorithm first uses the base classifier to predict the top class (via Monte Carlo sampling with small n₀), then uses the surrogate model only to estimate counts for radius calculation.
- Core assumption: The base classifier's prediction remains correct and matches the surrogate model's top class prediction when the surrogate is accurate.
- Evidence anchors: [section IV.C] "It is important to emphasize that we only offer the certified radius in cases where the smoothed classifier's predicted top 1 class matches the predicted top 1 class"

## Foundational Learning

- Concept: Randomized smoothing and its certification process using Monte Carlo sampling
  - Why needed here: The entire paper builds on replacing this computationally expensive process
  - Quick check question: What is the time complexity of Monte Carlo sampling in randomized smoothing and why is it problematic?

- Concept: Surrogate model training with probability distribution targets
  - Why needed here: The surrogate model must learn to predict class counts that represent probabilities
  - Quick check question: Why is Jensen-Shannon divergence appropriate for training a model to predict normalized class counts?

- Concept: Statistical hypothesis testing for certification (Clopper-Pearson intervals)
  - Why needed here: The certification still requires lower confidence bounds on class probabilities
  - Quick check question: What is the purpose of the LowerConfBound function in the certification algorithm?

## Architecture Onboarding

- Component map: Data sampling module -> Surrogate model (ResNet) -> Certification module
- Critical path: 1) Sample training data (class counts vectors) - one-time cost 2) Train surrogate model on sampled data 3) During inference: predict top class → predict counts → compute lower bound → output radius or abstain
- Design tradeoffs: Model complexity vs accuracy (larger ResNets give better accuracy but longer training), sampling size N vs surrogate accuracy (larger N gives better training data but longer sampling time), certification confidence vs computation (higher n₀ and α require more computation but fewer abstentions)
- Failure signatures: High abstention rate (surrogate model not confident or top classes mismatched), large gap from baseline ACR (surrogate model predictions significantly different from Monte Carlo), high variance in class counts (training data too noisy for stable learning)
- First 3 experiments: 1) Verify surrogate model can reproduce class counts from small validation set 2) Test certification with known correct cases to measure abstention rate 3) Compare ACR trends with varying noise levels σ to baseline method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Accelerated Smoothing perform when applied to randomized smoothing methods that use noise distributions other than Gaussian (e.g., Laplace, Uniform, or adversarial noise distributions)?
- Basis in paper: [explicit] The paper states "Our approach is noteworthy because it does not rely on any assumptions about the base classifier or the smoothing distribution" and mentions extending to other randomized smoothing approaches as future work.
- Why unresolved: Current experiments only evaluate Gaussian noise smoothing, leaving performance on other noise distributions unexplored.
- What evidence would resolve it: Experimental results comparing Accelerated Smoothing with Monte Carlo sampling across various noise distributions on benchmark datasets, showing certification accuracy and runtime trade-offs.

### Open Question 2
- Question: Can the surrogate model's uncertainty estimation be effectively integrated with Bayesian Neural Networks to dynamically adjust the number of noise samples (N₀) needed for radius certification?
- Basis in paper: [inferred] The paper discusses overestimation/underestimation errors and suggests training the model to recognize uncertainty in its outputs to determine required samples.
- Why unresolved: While proposed as future direction, no experiments or theoretical analysis are provided on implementing uncertainty estimation through BNNs.
- What evidence would resolve it: Implementation of a BNN-based surrogate model with uncertainty quantification, showing improved certification accuracy and sample efficiency.

### Open Question 3
- Question: What is the mathematical relationship between the surrogate model's prediction error and the theoretical lower bound on the certified radius, and how can this relationship be formally characterized?
- Basis in paper: [explicit] The paper acknowledges "the main issue with our methodology lies in the lack of mathematical understanding" and discusses overestimation/underestimation without providing theoretical framework.
- Why unresolved: The paper provides empirical analysis of prediction errors but lacks rigorous mathematical framework connecting surrogate errors to certification bounds.
- What evidence would resolve it: Formal proof establishing error bounds on certified radius as function of surrogate model prediction error.

## Limitations
- Critical dependency on surrogate model accuracy for reliable certification
- Lack of mathematical proof establishing bounds between prediction error and certification reliability
- Limited evaluation only on CIFAR-10 with Gaussian noise, performance on other datasets/noise distributions unknown

## Confidence
- High Confidence: Computational complexity improvement (O(N) → O(1)) and theoretical framework for surrogate-based certification
- Medium Confidence: Empirical results showing 5-10% accuracy gaps between methods, exact implementation details not fully specified
- Low Confidence: Claims about robustness to distribution shift and generalization to non-Gaussian noise, not empirically validated

## Next Checks
1. **Error-to-Abstention Mapping**: Systematically analyze how different magnitudes of class count prediction errors (5%, 10%, 15%) affect the abstention rate and certified radius accuracy across the test set.
2. **Cross-Dataset Transfer**: Train and evaluate Accelerated Smoothing on CIFAR-100 and Tiny ImageNet to assess performance degradation and robustness to dataset complexity.
3. **Noise Distribution Sensitivity**: Compare performance using Laplacian and uniform noise distributions against Gaussian noise to validate the method's robustness to noise assumptions.