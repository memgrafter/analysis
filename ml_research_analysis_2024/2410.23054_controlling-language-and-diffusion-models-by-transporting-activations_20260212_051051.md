---
ver: rpa2
title: Controlling Language and Diffusion Models by Transporting Activations
arxiv_id: '2410.23054'
source_url: https://arxiv.org/abs/2410.23054
tags:
- linear-act
- concept
- best
- transport
- strength
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Activation Transport (ACT), a general framework
  for controlling both language models and diffusion models by steering their internal
  activations using optimal transport theory. The key idea is to learn transport maps
  between activation distributions corresponding to desired and undesired behaviors,
  and apply these maps at inference time to induce or mitigate specific concepts or
  styles.
---

# Controlling Language and Diffusion Models by Transporting Activations

## Quick Facts
- arXiv ID: 2410.23054
- Source URL: https://arxiv.org/abs/2410.23054
- Reference count: 40
- Primary result: ACT framework achieves up to 7.5× toxicity reduction in LLMs and 95% style control in diffusion models

## Executive Summary
This paper introduces Activation Transport (ACT), a unified framework for controlling both language models and diffusion models by steering their internal activations using optimal transport theory. The key innovation is learning transport maps between activation distributions corresponding to desired and undesired behaviors, then applying these maps at inference time to induce or mitigate specific concepts or styles. ACT provides fine-grained control via an interpretable strength parameter λ between 0 and 1, generalizing many existing activation steering methods. The approach demonstrates effectiveness across diverse tasks including toxicity mitigation (7.5× reduction), concept induction (87% success rate), truthfulness improvement (7.8%), and style control in diffusion models (95% style presence).

## Method Summary
ACT operates by learning optimal transport maps between activation distributions from examples exhibiting target behaviors and those without. For language models, ACT extracts residual stream activations from LLaMA-family models, computes transport maps using sliced Wasserstein distance, and applies these maps at inference time with strength parameter λ. For diffusion models, ACT applies transport maps to attention and MLP activations within the U-Net architecture. The framework is trained using contrastive learning objectives that align transported activations with target distributions while preserving task performance. ACT's strength parameter enables fine-grained control between 0 (original behavior) and 1 (fully transported behavior), providing interpretable control over the degree of modification.

## Key Results
- Toxicity mitigation: Up to 7.5× reduction in toxic outputs using RealToxicityPrompts dataset
- Concept induction: 87% success rate for inducing arbitrary concepts in LLaMA models
- Truthfulness improvement: Up to 7.8% improvement in truthful responses
- Diffusion style control: 95% style presence while maintaining 80% prompt similarity
- Computational efficiency: Minimal overhead without hyperparameter tuning required

## Why This Works (Mechanism)
ACT leverages optimal transport theory to learn precise mappings between activation distributions, capturing complex nonlinear relationships that simpler steering methods miss. By operating directly on internal activations rather than outputs, ACT can intervene at the level where concepts are being processed, enabling more effective control. The use of sliced Wasserstein distance provides a computationally tractable approximation to optimal transport while maintaining theoretical guarantees about distribution alignment. The strength parameter λ allows for graded control rather than binary switches, enabling nuanced behavior modification that preserves base model capabilities while introducing targeted changes.

## Foundational Learning

**Optimal Transport Theory**: Mathematical framework for finding minimal-cost mappings between probability distributions. Why needed: Provides principled way to learn activation transformations that preserve distribution properties. Quick check: Verify that transport maps satisfy mass preservation constraints.

**Sliced Wasserstein Distance**: Efficient approximation of Wasserstein distance using projections onto random directions. Why needed: Enables tractable computation of transport maps for high-dimensional activation spaces. Quick check: Confirm convergence properties with increasing number of projections.

**Contrastive Learning**: Training approach that pulls representations of similar examples together while pushing dissimilar examples apart. Why needed: Enables learning of transport maps that distinguish between target and non-target activation distributions. Quick check: Validate that contrastive loss effectively separates desired from undesired behaviors.

**Activation Steering**: General class of methods that modify model behavior by altering internal activations. Why needed: Provides context for understanding ACT's relationship to existing approaches. Quick check: Compare ACT's transport-based approach against direct activation addition methods.

## Architecture Onboarding

**Component Map**: Input data -> Activation extraction -> Transport map learning -> Inference-time transport application -> Controlled outputs

**Critical Path**: 
1. Extract activations from model layers during training
2. Compute transport maps between positive and negative examples
3. Store transport maps for inference deployment
4. Apply transport maps to activations during inference
5. Forward pass through modified activations

**Design Tradeoffs**: 
ACT trades computational complexity of optimal transport against improved control precision. The framework requires storing transport maps for each controlled dimension, increasing memory usage but enabling fine-grained control. Using sliced Wasserstein distance provides computational efficiency at the cost of some approximation error compared to exact optimal transport solutions.

**Failure Signatures**: 
Poor transport map quality manifests as degraded base model performance or incomplete control over target behaviors. Over-regularization can cause transport maps to become identity mappings, eliminating control effects. Insufficient training data for transport learning results in unstable or ineffective control.

**First 3 Experiments**:
1. Baseline comparison: Apply ACT to toxicity control task and measure reduction versus no-intervention baseline
2. Strength parameter sweep: Vary λ from 0 to 1 and measure control strength versus base model preservation
3. Computational overhead measurement: Time ACT inference versus standard inference across different model sizes

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- Limited ablation studies comparing ACT against simpler steering methods using identical protocols
- Toxicity mitigation results depend on RealToxicityPrompts dataset with known limitations
- Generalizability claims lack systematic testing across diverse model architectures beyond LLaMA and Stable Diffusion
- Computational efficiency claims require broader empirical validation across hardware configurations

## Confidence

**High Confidence**: Core mathematical formulation using optimal transport theory is sound and well-defined. Implementation details for applying transport maps are technically correct and reproducible.

**Medium Confidence**: Experimental results demonstrating effectiveness on tested datasets and models are credible, though improvement magnitudes may be overestimated due to limited baseline comparisons.

**Low Confidence**: Generalizability claims across all model types and control objectives are not yet substantiated. Robustness claims regarding computational efficiency require broader empirical validation.

## Next Checks

1. Conduct systematic ablation studies comparing ACT against established steering methods (ILF, activation addition, conceptor-based approaches) on identical datasets using standardized evaluation protocols to quantify actual improvements.

2. Test ACT across diverse model architectures and scales (including models outside LLaMA and Stable Diffusion families) to validate generalizability claims and identify architectural constraints.

3. Implement real-time computational benchmarks across different hardware configurations and model sizes to empirically verify the "minimal computational overhead" claim and identify scaling limitations.