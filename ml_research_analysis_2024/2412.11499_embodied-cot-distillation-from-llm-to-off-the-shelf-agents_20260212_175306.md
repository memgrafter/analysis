---
ver: rpa2
title: Embodied CoT Distillation From LLM To Off-the-shelf Agents
arxiv_id: '2412.11499'
source_url: https://arxiv.org/abs/2412.11499
tags:
- embodied
- task
- distillation
- deder
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of deploying large language
  model (LLM)-based embodied task planning on capacity-limited off-the-shelf devices.
  The proposed DEDER framework decomposes the reasoning process into two tiers: a
  reasoning-policy that generates rationales using an embodied knowledge graph and
  contrastive learning, and a planning-policy that uses these rationales to generate
  executable plans.'
---

# Embodied CoT Distillation From LLM To Off-the-shelf Agents

## Quick Facts
- arXiv ID: 2412.11499
- Source URL: https://arxiv.org/abs/2412.11499
- Reference count: 33
- Key outcome: DEDER framework achieves 21.6% higher success rates and 12.3% higher goal-conditioned success rates on ALFRED benchmark compared to leading baselines

## Executive Summary
This paper addresses the challenge of deploying large language model (LLM)-based embodied task planning on capacity-limited devices. The proposed DEDER framework decomposes LLM reasoning into a two-tier hierarchy: a reasoning-policy that generates rationales using an embodied knowledge graph, and a planning-policy that uses these rationales to generate executable plans. This approach enables efficient distillation of LLM capabilities into small language models (sLMs) suitable for real-time deployment.

## Method Summary
DEDER reconstructs LLM decision-making into a two-tier hierarchy consisting of a reasoning-policy and planning-policy. The framework extracts rationales from expert trajectories using MDP-featured in-context learning with self-verification, then distills these into sLMs. An embodied knowledge graph captures environment information as triples and is updated at each planning step. Contrastive learning ensures similar situations produce similar rationales, improving consistency. The reasoning-policy uses prompted attention mechanisms to process the KG and generate task-specific rationales, while the planning-policy uses these rationales to generate executable plans.

## Key Results
- Achieves 21.6% higher success rates and 12.3% higher goal-conditioned success rates on ALFRED benchmark compared to most competitive baseline
- Outperforms leading language planning and distillation approaches including SCoTD, SCOTT, and vanilla in-context learning
- Demonstrates effective knowledge distillation from LLMs to sLMs while maintaining strong embodied reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing LLM reasoning into a two-tier hierarchy enables efficient distillation into small language models
- Mechanism: The reasoning-policy generates task-specific rationales using an embodied knowledge graph and attention mechanisms, while the planning-policy uses these rationales to generate executable plans
- Core assumption: Task-specific rationales can be effectively extracted from LLM reasoning and used to guide planning in small models
- Evidence anchors: Abstract states "decision-making process of LLM-based strategies is restructured into a hierarchy with a reasoning-policy and planning-policy"; section 4.2 describes the reasoning-policy's responsibility for inferring rationale sets

### Mechanism 2
- Claim: MDP-featured in-context learning with self-verification extracts high-quality rationales from LLM for embodied tasks
- Mechanism: The LLM is prompted with RL-specific queries and uses retrieval-augmented examples, with a self-critic function verifying if generated rationales are sufficient
- Core assumption: RL-specific queries can effectively extract MDP features necessary for embodied task planning
- Evidence anchors: Section 4.1 describes prompting LLM with pre-defined set of RL-specific queries and using LLM as self-critic function

### Mechanism 3
- Claim: Embodied knowledge graph and behavior-based contrastive learning improve rationale quality and enable single-step inference
- Mechanism: The embodied KG captures environment information as triples and is updated at each planning step, with contrastive learning ensuring consistency across similar situations
- Core assumption: Representing environment information as knowledge graph and using contrastive learning can improve quality and consistency of rationales
- Evidence anchors: Section 4.2 describes embodied KG as set of triples and use of behavior-based contrastive learning

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The paper models embodied agent environments as POMDPs, where agents have limited perception and must make decisions based on partial observations
  - Quick check question: What are the key components of a POMDP, and how do they differ from a standard MDP?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: The paper uses CoT prompting to extract reasoning processes from LLMs and distill them into small models
  - Quick check question: How does CoT prompting help LLMs solve complex problems, and what are its limitations?

- Concept: Knowledge Distillation
  - Why needed here: The paper's core approach involves distilling LLM capabilities into small language models for efficient deployment
  - Quick check question: What are the main challenges in knowledge distillation, particularly when transferring reasoning capabilities?

## Architecture Onboarding

- Component map: Rationale Dataset Construction (LLM with MDP-featured in-context learning and self-verification) -> Policy Distillation (Two-tier sLM-based policy: reasoning-policy + planning-policy) -> Deployment (Embodied KG, attention mechanisms, text generation)

- Critical path: Rationale extraction → KG prompting → Attention processing → Plan generation
  The most critical components are the rationale extraction quality and the attention mechanism in the reasoning-policy

- Design tradeoffs:
  - LLM size vs. rationale quality: Larger LLMs may extract better rationales but increase computational cost
  - KG complexity vs. inference speed: More detailed KGs improve reasoning but slow down processing
  - Attention mechanism complexity vs. inference efficiency: More sophisticated attention improves rationale quality but increases computation

- Failure signatures:
  - Poor rationale quality: Agent fails to complete tasks or makes illogical decisions
  - Slow inference: Embodied KG becomes too large or attention mechanisms are too complex
  - Inconsistent performance: Contrastive learning fails to maintain consistency across similar situations

- First 3 experiments:
  1. Test rationale extraction quality: Compare rationales generated by different LLMs (GPT2 vs. PaLM) on the same expert transitions
  2. Validate attention mechanism effectiveness: Compare performance with and without causal/gated attention in the reasoning-policy
  3. Evaluate KG impact: Measure performance and inference time with and without the embodied KG

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DEDER scale with the complexity and diversity of embodied tasks beyond those tested in ALFRED?
- Basis in paper: The paper demonstrates DEDER's effectiveness on ALFRED but doesn't explore its scalability to more complex or diverse environments
- Why unresolved: The evaluation is limited to the ALFRED benchmark, which may not encompass the full range of complexities encountered in real-world applications
- What evidence would resolve it: Testing DEDER on a wider variety of benchmarks with increasing task complexity and environmental diversity

### Open Question 2
- Question: What is the impact of using different types of sLMs (e.g., transformers vs. other architectures) on the performance and efficiency of DEDER?
- Basis in paper: The paper uses GPT2 and T5 models but doesn't explore the impact of using different sLM architectures
- Why unresolved: The choice of sLM architecture could significantly affect the reasoning and planning capabilities of DEDER
- What evidence would resolve it: Comparing DEDER's performance using various sLM architectures, such as LSTMs or other transformer variants

### Open Question 3
- Question: How does DEDER handle tasks that require long-term planning or memory, which may not be fully captured by the current embodied KG approach?
- Basis in paper: The embodied KG is updated at each planning step, but the paper doesn't discuss how DEDER handles tasks requiring extensive memory or long-term planning
- Why unresolved: Complex tasks may require the agent to remember and utilize information from earlier steps that are not directly related to the current observation
- What evidence would resolve it: Evaluating DEDER on tasks that require multi-step reasoning and long-term memory, such as puzzle-solving or strategic games

## Limitations

- External validation: Performance claims are evaluated only on the ALFRED benchmark with no evidence of generalization to other embodied reasoning tasks or robotic platforms
- Knowledge graph implementation: Critical details about KG storage, retrieval, and update mechanisms are underspecified
- Scalability concerns: The computational cost of generating rationales for each expert transition and how this scales with larger datasets is not addressed

## Confidence

- High confidence: The two-tier policy architecture (reasoning-policy + planning-policy) is well-defined and its benefits for computational efficiency are clearly articulated
- Medium confidence: The effectiveness of the embodied knowledge graph combined with contrastive learning is supported by experimental results but lacks detailed theoretical justification
- Low confidence: The claim that MDP-featured in-context learning with self-verification reliably extracts high-quality rationales is based on internal evaluation only

## Next Checks

1. **Ablation study on KG complexity**: Systematically vary the size and detail level of the embodied knowledge graph to quantify the tradeoff between rationale quality and inference speed. Measure performance degradation as KG size increases.

2. **Cross-environment generalization test**: Evaluate the distilled policies on at least one additional embodied reasoning benchmark (e.g., Habitat or RoboTHOR) to assess whether the 21.6% improvement holds across different environments and task types.

3. **Computational cost analysis**: Profile the full system (including rationale extraction, KG operations, and policy inference) on representative off-the-shelf hardware to verify that the claimed real-time performance is achievable and to identify computational bottlenecks.