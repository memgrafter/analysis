---
ver: rpa2
title: Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning
  Rate
arxiv_id: '2405.15384'
source_url: https://arxiv.org/abs/2405.15384
tags:
- learning
- tasks
- resel
- policy
- recurrent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses training instability in recurrent reinforcement
  learning (RL) for partially observable Markov decision processes (POMDPs). The authors
  identify that recurrent neural networks (RNNs) suffer from amplified output variations
  during training due to their autoregressive nature, leading to instability.
---

# Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning Rate

## Quick Facts
- arXiv ID: 2405.15384
- Source URL: https://arxiv.org/abs/2405.15384
- Authors: Fan-Ming Luo; Zuolin Tu; Zefang Huang; Yang Yu
- Reference count: 40
- This paper addresses training instability in recurrent RL for POMDPs by using lower learning rates specifically for RNN context encoders

## Executive Summary
This paper addresses a fundamental instability in training recurrent neural networks for reinforcement learning in partially observable environments. The authors identify that RNNs suffer from amplified output variations during training due to their autoregressive nature, which compounds small parameter changes across sequence steps. They propose RESeL, which uses a lower learning rate specifically for the RNN context encoder while maintaining standard rates for other network layers. This approach stabilizes RNN training without sacrificing the efficiency of multilayer perceptrons (MLPs), achieving significant performance improvements over previous recurrent RL baselines and matching state-of-the-art methods in standard MDP tasks.

## Method Summary
RESeL is an off-policy reinforcement learning algorithm that addresses training instability in recurrent neural networks by using context-encoder-specific learning rates. The method employs a Mamba-based context encoder to process observation and action history, with a lower learning rate (LRCE) for the context encoder compared to other layers (LRother). The algorithm follows the SAC framework with an ensemble-Q mechanism, using 8 MLP critics. RESeL samples full-length trajectories from the replay buffer and trains with a curriculum approach for certain tasks. The key innovation is the decoupling of learning rates between the context encoder and other network components to counteract the amplification of output variations that occurs in RNNs during training.

## Key Results
- RESeL achieves notable performance improvements over previous recurrent RL baselines in 18 POMDP tasks, including classic, meta-RL, and credit assignment scenarios
- RESeL matches or surpasses state-of-the-art RL methods in 5 MDP locomotion tasks while maintaining superior performance on POMDP tasks
- Ablation studies confirm the necessity of using distinct learning rates for the context encoder, with LRCE=1e-5 and LRother=3e-4 providing optimal results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RNN output variations amplify over time due to autoregressive feedback.
- Mechanism: Parameter changes cause small initial output shifts that compound as hidden states propagate through sequence steps, with amplification converging to a constant factor.
- Core assumption: The RNN satisfies Lipschitz continuity with hidden state contraction factor Kh ∈ [0, 1).
- Evidence anchors:
  - [abstract] "with the autoregressive property of RNNs, the output variations caused by parameter changes are amplified as the sequence length increases"
  - [section 4.2] Proposition 1 derives the amplification factor Ky/(1-Kh) for output variations
  - [corpus] Weak - no corpus papers specifically discuss RNN amplification in RL training
- Break condition: If RNN hidden states grow without bound or Kh ≥ 1, the amplification analysis fails.

### Mechanism 2
- Claim: Using lower learning rate for context encoder stabilizes training without hurting MLP efficiency.
- Mechanism: Smaller learning rate counteracts amplified RNN output variations while maintaining normal learning rate for MLPs that don't experience this amplification.
- Core assumption: MLPs don't have autoregressive output amplification, so they can tolerate higher learning rates.
- Evidence anchors:
  - [abstract] "RESeL uses a lower learning rate for context encoder than other MLP layers to ensure the stability of the former while maintaining the training efficiency of the latter"
  - [section 4.2] Explains why context-encoder-specific learning rate preserves MLP training efficiency
  - [corpus] Weak - no corpus papers discuss this specific learning rate decoupling approach
- Break condition: If MLPs unexpectedly show similar amplification behavior, or if RNNs don't require rate reduction.

### Mechanism 3
- Claim: RESeL achieves comparable performance to SOTA methods on MDP tasks while excelling on POMDP tasks.
- Mechanism: RNN context encoder provides robustness to partial observability while context-encoder-specific learning rate prevents training instability.
- Core assumption: MDP tasks benefit from the stability improvements even when full state information is available.
- Evidence anchors:
  - [abstract] "RESeL achieves notable performance improvements over previous recurrent RL baselines in POMDP tasks, and is competitive with, or even surpasses, state-of-the-art (SOTA) RL methods in MDP tasks"
  - [section 5.2] Shows RESeL matches TD7 on MuJoCo tasks while outperforming it on some environments
  - [corpus] Weak - no corpus papers directly compare RNN-based methods with state-of-the-art MDP algorithms
- Break condition: If RNN context encoder provides no benefit to MDP tasks or if training instability persists.

## Foundational Learning

- Concept: Lipschitz continuity and gradient stability in recurrent networks
  - Why needed here: Understanding why RNN parameter updates cause amplified output variations requires grasping the mathematical properties of RNN stability
  - Quick check question: Why does having Kh < 1 matter for RNN stability over long sequences?

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper addresses POMDP tasks where state information is incomplete, requiring memory-based approaches
  - Quick check question: What key challenge distinguishes POMDPs from standard MDPs that makes recurrent networks necessary?

- Concept: Off-policy reinforcement learning and actor-critic architectures
  - Why needed here: RESeL builds on SAC framework with ensemble critics, requiring understanding of off-policy RL fundamentals
  - Quick check question: How does using a replay buffer in off-policy RL enable more stable training compared to on-policy methods?

## Architecture Onboarding

- Component map: Pre-encoders (MLPs) → Context Encoder (RNN/Mamba) → MLP Policy → Ensemble Critics (8 MLP critics)
- Critical path: Observation and action history → Context embedding → Action distribution → Environment interaction → Reward feedback → Critic update → Policy update
- Design tradeoffs: Lower LRCE for stability vs. higher LRCE for faster learning; RNN complexity vs. MLP simplicity; full trajectory vs. truncated sequence training
- Failure signatures: Training divergence (NaN outputs), unstable learning curves, poor asymptotic performance, excessive GPU memory usage
- First 3 experiments:
  1. Measure output variation amplification with different learning rates on a pre-trained policy to validate Proposition 1
  2. Train on a simple POMDP task like WalkerBLT-V-v0 using full-length trajectories, monitoring RNN output variations and policy performance
  3. Compare full trajectory vs. truncated sequence training to assess distribution shift impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RNN architecture choice (GRU vs Mamba) impact the optimal learning rate ratio between the context encoder and other layers?
- Basis in paper: [inferred] The paper shows RESeL-GRU outperforms RESeL-Mamba in some tasks, suggesting different RNN architectures may require different learning rate ratios.
- Why unresolved: The paper only tests one learning rate configuration for each RNN type without exploring how the optimal ratio varies across architectures.
- What evidence would resolve it: Systematic ablation studies varying the learning rate ratio for both GRU and Mamba architectures across multiple tasks would reveal architecture-specific optimal configurations.

### Open Question 2
- Question: What is the relationship between sequence length and the optimal context encoder learning rate?
- Basis in paper: [inferred] The paper demonstrates output variation amplification increases with sequence length but doesn't explore how this affects the optimal learning rate.
- Why unresolved: While the paper shows output variation amplification, it only tests a fixed learning rate configuration without examining how the optimal rate scales with sequence length.
- What evidence would resolve it: Experiments varying sequence lengths while systematically tuning the context encoder learning rate would reveal whether a universal scaling relationship exists.

### Open Question 3
- Question: Does the context encoder learn task-specific features beyond gravity in dynamics-randomized environments?
- Basis in paper: [explicit] The paper notes "the context encoder of RESeL may extract not only gravity but also other factors that help the agent achieve higher returns."
- Why unresolved: The paper observes superior performance over methods specifically designed to extract gravity but doesn't analyze what other features the context encoder learns.
- What evidence would resolve it: Feature attribution analysis or ablation studies removing hypothesized learned features would identify what additional information the context encoder extracts.

## Limitations
- The core mechanism relies on a specific mathematical analysis of RNN amplification that may not generalize to all recurrent architectures
- Limited MDP evaluation (only 5 tasks) makes claims about MDP performance less robust
- Corpus analysis shows limited related work specifically addressing this amplification problem in RL training

## Confidence
- High confidence: The empirical demonstration that different learning rates for context encoder vs. other layers improves training stability and performance across POMDP tasks.
- Medium confidence: The theoretical analysis of RNN amplification (Proposition 1) and its practical implications for learning rate selection.
- Low confidence: The claim that RESeL provides comparable benefits for MDP tasks, given the limited MDP evaluation and lack of comparison to specialized MDP algorithms.

## Next Checks
1. Test RESeL on additional MDP tasks from different domains (Atari, Procgen) to verify the MDP performance claims across broader task distributions.
2. Conduct ablation studies comparing RESeL with different recurrent architectures (LSTM, GRU) to determine if the learning rate decoupling approach generalizes beyond Mamba.
3. Measure actual RNN output variation magnitudes during training across different learning rate configurations to empirically validate the theoretical amplification analysis.