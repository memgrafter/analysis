---
ver: rpa2
title: "Krey\xF2l-MT: Building MT for Latin American, Caribbean and Colonial African\
  \ Creole Languages"
arxiv_id: '2405.05376'
source_url: https://arxiv.org/abs/2405.05376
tags:
- languages
- creole
- language
- data
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the largest dataset for machine translation
  (MT) of Creole languages, including 14.5M unique Creole sentences with parallel
  translations. The dataset covers 41 Creole languages, with 11.6M sentences publicly
  released.
---

# Kreyòl-MT: Building MT for Latin American, Caribbean and Colonial African Creole Languages

## Quick Facts
- arXiv ID: 2405.05376
- Source URL: https://arxiv.org/abs/2405.05376
- Reference count: 40
- This work introduces the largest dataset for machine translation (MT) of Creole languages, including 14.5M unique Creole sentences with parallel translations.

## Executive Summary
This paper presents Kreyòl-MT, the largest dataset for machine translation of Creole languages, covering 41 languages with 14.5M unique Creole sentences and parallel translations. The authors provide MT models supporting all 41 languages in 172 translation directions, achieving state-of-the-art performance on a Creole language benchmark for 26 of 34 translation directions. The dataset is notably diverse in genre, surpassing previous works limited to religious texts. The models demonstrate effective cross-lingual transfer for low-resource languages and improved performance through data cleaning for higher-resource languages.

## Method Summary
The authors collected and preprocessed the largest dataset for Creole language MT to date, aggregating bitexts from multiple genres including Bible, educational, legal, narrative, and news sources. They trained multilingual MT models using the YANMTT toolkit with mBART-50 fine-tuning, employing SentencePiece tokenization. The training pipeline included data cleaning according to GILT Leaders Forum's Best Practices, with separate models trained on cleaned versus non-cleaned data. Models were evaluated using chrF and BLEU scores on test sets not publicly released.

## Key Results
- Largest dataset for Creole MT with 14.5M unique Creole sentences, 11.6M publicly released
- Models achieve state-of-the-art performance on Creole benchmark for 26 of 34 translation directions
- Cross-lingual transfer enables effective translation for low-resource languages with minimal parallel data
- Diverse genre training data improves general-domain translation performance compared to genre-specific models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse training data improves general-domain translation performance for Creole languages.
- Mechanism: Aggregating bitexts from multiple genres exposes the model to wider linguistic constructions and vocabulary, enabling better generalization to unseen domains.
- Core assumption: Linguistic diversity in training data correlates with improved performance on out-of-domain test sets.
- Evidence anchors: Abstract states diverse dataset produces model outperforming genre-specific model on its own benchmark for 26 of 34 translation directions.

### Mechanism 2
- Claim: Cross-lingual transfer enables effective translation for low-resource Creole languages with minimal parallel data.
- Mechanism: Morphosyntactic similarities with Niger-Congo languages and lexical overlap with Romance/Germanic languages allow knowledge transfer from related languages to low-resource Creole languages.
- Core assumption: Morphosyntactic and lexical similarities enable effective cross-lingual transfer in neural machine translation.
- Evidence anchors: Abstract mentions potential for cross-lingual transfer; section reports successful translation of Miskito Coast Creole with only 391 parallel sentences.

### Mechanism 3
- Claim: Cleaning training data improves translation quality for higher-resource Creole languages but may hurt performance for lower-resource, noisier corpora.
- Mechanism: Removing noise reduces variability in training data, leading to better model performance for higher-resource languages, but may remove valuable information for lower-resource languages.
- Core assumption: Noise negatively impacts translation quality, and its removal is beneficial for higher-resource languages.
- Evidence anchors: Section states models trained on cleaned data typically outperform non-cleaned data for higher resource languages, but cleaning can hurt lower-resource, noisier corpora.

## Foundational Learning

- Concept: Morphosyntactic and lexical relationships between languages
  - Why needed here: Understanding these relationships is crucial for leveraging cross-lingual transfer in low-resource machine translation.
  - Quick check question: What are the primary morphosyntactic similarities between Creole languages and Niger-Congo languages?

- Concept: Data cleaning and preprocessing techniques
  - Why needed here: Proper data cleaning is essential for improving translation quality, especially for higher-resource languages, but aggressive cleaning may hurt performance for lower-resource languages.
  - Quick check question: What are the potential drawbacks of cleaning training data for lower-resource, noisier corpora?

- Concept: Multilingual vs. bilingual model training
  - Why needed here: Choosing between multilingual and bilingual training can significantly impact translation quality, especially for low-resource languages.
  - Quick check question: How does multilingual training differ from bilingual training in terms of cross-lingual transfer and performance on low-resource languages?

## Architecture Onboarding

- Component map: Data collection -> Preprocessing -> Model training -> Evaluation -> Release
- Critical path: Data collection → Preprocessing → Model training → Evaluation → Release
- Design tradeoffs:
  - Data diversity vs. data quality: Including more genres increases diversity but may introduce noise
  - Model size vs. training time: Larger models may perform better but require more computational resources
  - Public vs. private data: Releasing more data benefits community but may involve privacy concerns
- Failure signatures:
  - Poor translation quality: May indicate issues with data quality, model architecture, or hyperparameters
  - Overfitting: May occur if model is too large or training data is too small or repetitive
  - Underfitting: May occur if model is too small or training data is insufficient
- First 3 experiments:
  1. Train baseline model on single genre (e.g., Bible translations) and evaluate on out-of-domain test sets
  2. Train multilingual model on related languages and evaluate on low-resource Creole languages
  3. Compare performance of cleaned vs. non-cleaned training data for higher-resource and lower-resource Creole languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MT models for Creole languages vary across different genres of text?
- Basis in paper: [explicit] Authors highlight dataset is more genre-diverse than previous works focused on religious texts, acknowledging increased genre diversity could impact model performance.
- Why unresolved: Authors do not provide detailed analysis of how model performance varies across different genres within their dataset.
- What evidence would resolve it: Breakdown of model performance metrics for each genre present in the dataset.

### Open Question 2
- Question: What is the potential for cross-lingual transfer between Creole languages and their phylogenetic relatives, and how does this potential vary based on linguistic distance?
- Basis in paper: [explicit] Authors mention linguistic proximity between Creole languages and phylogenetic relatives, suggesting potential for cross-lingual transfer, with examples of successful zero-shot translation.
- Why unresolved: Authors do not explore extent of cross-lingual transfer potential or investigate how it correlates with linguistic distance.
- What evidence would resolve it: Experiments comparing MT performance using models trained on related languages with varying degrees of linguistic similarity.

### Open Question 3
- Question: How do different data cleaning strategies impact the performance of MT models for Creole languages, especially for low-resource languages?
- Basis in paper: [explicit] Authors compare performance of models trained on cleaned vs. non-cleaned data, noting cleaning generally improves performance for higher-resource languages but may hurt performance for lower-resource languages.
- Why unresolved: Authors do not provide detailed analysis of trade-offs between data cleaning and model performance for different resource levels or explore alternative cleaning strategies.
- What evidence would resolve it: Comprehensive study comparing various data cleaning approaches and their impact on model performance across different resource levels.

## Limitations
- Dataset completeness: Authors acknowledge additional relevant corpora may exist but were not included due to accessibility constraints or lack of awareness.
- Evaluation constraints: Primary evaluation metric is chrF, but test sets used are not publicly available and human evaluation was not conducted.
- Data quality variability: Dataset contains both high-quality and noisier corpora, with aggressive cleaning potentially removing valuable information for lower-resource languages.

## Confidence
- High confidence: Claim that this dataset represents the largest and most diverse collection of Creole language bitexts to date.
- Medium confidence: Claim that models achieve state-of-the-art performance on Creole language benchmark for 26 of 34 translation directions.
- Medium confidence: Claim that cross-lingual transfer enables effective translation for low-resource Creole languages with minimal parallel data.

## Next Checks
1. Replicate core findings with public subsets: Train models using only the publicly released 11.6M sentence pairs and evaluate on publicly available test sets for a subset of languages.
2. Conduct human evaluation: For a representative sample of translation directions, perform human evaluation of translation quality to complement automated metrics.
3. Test cross-lingual transfer robustness: Systematically evaluate performance of multilingual versus bilingual models across the full spectrum of language resource levels.