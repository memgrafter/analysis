---
ver: rpa2
title: Automated Image Captioning with CNNs and Transformers
arxiv_id: '2412.10511'
source_url: https://arxiv.org/abs/2412.10511
tags:
- image
- captions
- training
- captioning
- cnn-rnn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic evaluation of image captioning
  models using CNNs and Transformers. The authors compare CNN-RNN, CNN-Attention,
  YOLO-CNN-Attention, ViT-Attention, and a novel hybrid ViTCNN-Attention architecture
  across MSCOCO and Flickr30k datasets.
---

# Automated Image Captioning with CNNs and Transformers

## Quick Facts
- arXiv ID: 2412.10511
- Source URL: https://arxiv.org/abs/2412.10511
- Authors: Joshua Adrian Cahyono; Jeremy Nathan Jusuf
- Reference count: 12
- Primary result: Hybrid ViTCNN-Attention model achieves best performance with BLEU-4 scores of 31.24 (MSCOCO) and 21.93 (Flickr30k)

## Executive Summary
This paper presents a systematic evaluation of image captioning models using CNNs and Transformers across five different architectures. The authors compare CNN-RNN, CNN-Attention, YOLO-CNN-Attention, ViT-Attention, and a novel hybrid ViTCNN-Attention architecture on MSCOCO and Flickr30k datasets. The hybrid model, which combines local CNN features with global ViT context, achieves the highest performance across multiple evaluation metrics, demonstrating the advantage of integrating both local and global visual information for caption generation.

## Method Summary
The study evaluates five image captioning architectures: CNN-RNN, CNN-Attention, YOLO-CNN-Attention, ViT-Attention, and a novel hybrid ViTCNN-Attention model. Each architecture follows a two-stage approach where image features are first extracted using either CNN (ResNet50, EfficientNetB0) or Transformer (ViT) backbones, then fed into a decoder for caption generation. The hybrid ViTCNN-Attention model combines features from both CNN and ViT backbones through concatenation before passing to the decoder. Models are trained using teacher forcing with cross-entropy loss on MSCOCO and Flickr30k datasets, with comprehensive hyperparameter tuning and evaluation using BLEU, METEOR, ROUGE-L, CIDEr, and SPICE metrics.

## Key Results
- Hybrid ViTCNN-Attention model achieves best performance with BLEU-4 scores of 31.24 on MSCOCO and 21.93 on Flickr30k
- ViT-based architectures outperform CNN-based models across all metrics on both datasets
- The hybrid approach combining CNN and ViT features shows consistent improvement over using either modality alone

## Why This Works (Mechanism)
The hybrid ViTCNN-Attention architecture leverages complementary strengths of CNNs and Transformers. CNNs excel at capturing local spatial features and fine-grained details through convolutional filters, while ViTs provide global context through self-attention mechanisms that capture long-range dependencies across the entire image. By concatenating features from both backbones, the model gains access to both detailed local information and holistic scene understanding, enabling more accurate and contextually appropriate caption generation. The attention decoder can then learn to weigh these complementary feature sets appropriately based on the semantic requirements of each word in the generated caption.

## Foundational Learning

**Convolutional Neural Networks (CNNs)**: Why needed - Extract local spatial features and hierarchical visual representations from images. Quick check - Can identify edges, textures, and object parts through successive convolutional layers.

**Vision Transformers (ViTs)**: Why needed - Capture global contextual relationships across entire images through self-attention mechanisms. Quick check - Can model long-range dependencies and scene-level understanding that CNNs may miss.

**Attention Mechanisms**: Why needed - Enable selective focus on relevant image regions when generating each word in the caption. Quick check - Weight different image features dynamically based on current decoding context.

**Encoder-Decoder Architecture**: Why needed - Separate visual feature extraction (encoder) from language generation (decoder) for specialized processing. Quick check - Image features flow from encoder to decoder without feedback until final output.

**Teacher Forcing**: Why needed - Accelerate training by using ground truth tokens as input during decoder training rather than generated tokens. Quick check - Decoder learns to predict next word given previous ground truth words during training.

## Architecture Onboarding

**Component Map**: Image -> Backbone (CNN/ViT) -> Feature Extraction -> Feature Concatenation (Hybrid only) -> Decoder (Attention) -> Caption

**Critical Path**: Image → Backbone → Feature Vector → Attention Decoder → Generated Caption. The bottleneck is the feature extraction stage, as it determines the quality and richness of visual information available to the decoder.

**Design Tradeoffs**: CNN backbones offer faster computation and better local feature extraction but may miss global context. ViT backbones capture better global understanding but require more computational resources and may lose fine details. The hybrid approach balances these tradeoffs but increases model complexity and computational cost.

**Failure Signatures**: Poor performance on fine-grained object recognition suggests CNN feature inadequacy. Failure to capture scene-level relationships indicates ViT limitations. Inconsistent captions across similar images may indicate insufficient attention mechanism training or feature representation issues.

**First Experiments**: 1) Test individual backbone performance on a validation set to establish baseline capabilities. 2) Evaluate attention visualization to verify the model focuses on relevant image regions during caption generation. 3) Perform ablation study removing either CNN or ViT features from the hybrid model to quantify their individual contributions.

## Open Questions the Paper Calls Out
None

## Limitations
- GPU resource constraints limited hyperparameter tuning to only 10 combinations
- Evaluation relies solely on automatic metrics without human assessment of caption quality
- Study focuses exclusively on MSCOCO and Flickr30k datasets, limiting generalizability

## Confidence
- High: Hybrid ViTCNN-Attention model outperforms other architectures on MSCOCO and Flickr30k datasets
- Medium: Combining local CNN features with global ViT context provides superior performance
- Low: Broader claims about transformer superiority over RNN approaches are not thoroughly examined

## Next Checks
1. Conduct expanded hyperparameter search across wider ranges of learning rates, batch sizes, and architectural parameters
2. Implement human evaluation study to validate whether automated metric improvements translate to perceived quality gains
3. Evaluate best-performing architectures on additional datasets from different domains to test generalizability