---
ver: rpa2
title: Self-Pluralising Culture Alignment for Large Language Models
arxiv_id: '2410.12971'
source_url: https://arxiv.org/abs/2410.12971
tags:
- culture
- question
- alignment
- data
- cultures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CultureSPA, a framework for pluralistic culture
  alignment in large language models (LLMs) by leveraging their internal cultural
  knowledge. The method generates diverse culture-related questions from seed examples,
  then collects model outputs under culture-unaware and culture-aware settings.
---

# Self-Pluralising Culture Alignment for Large Language Models

## Quick Facts
- arXiv ID: 2410.12971
- Source URL: https://arxiv.org/abs/2410.12971
- Authors: Shaoyang Xu; Yongqi Leng; Linhao Yu; Deyi Xiong
- Reference count: 36
- Primary result: Introduces CultureSPA framework that significantly improves cultural alignment across diverse cultures without compromising general capabilities

## Executive Summary
This paper introduces CultureSPA, a framework for pluralistic culture alignment in large language models (LLMs) by leveraging their internal cultural knowledge. The method generates diverse culture-related questions from seed examples, then collects model outputs under culture-unaware and culture-aware settings. Questions showing inconsistent outputs between these settings are paired with culture-aware responses to form training data. This data is used for either culture-joint or culture-specific supervised fine-tuning. Extensive experiments demonstrate that CultureSPA significantly improves cultural alignment across diverse cultures without compromising general capabilities.

## Method Summary
CultureSPA operates through a multi-stage process: First, it generates questions on various culture topics using iterative prompting with in-context examples from seed questions. Then, it collects LLM outputs in response to these questions under both culture-unaware and culture-aware settings. Questions showing inconsistent outputs between these settings are identified as culture-representative and paired with their culture-aware responses to form training data. This data is then used for either culture-joint or culture-specific supervised fine-tuning using LoRA. The framework assumes LLMs possess latent cultural knowledge that can be elicited through targeted prompting, and that output consistency indicates bias while inconsistency indicates cultural sensitivity.

## Key Results
- CultureSPA significantly improves cultural alignment across diverse cultures without compromising general capabilities
- The approach is robust to variations in data quality and quantity
- CultureSPA is orthogonal to prompt engineering techniques and effectively filters out biased responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CultureSPA activates internal cultural knowledge by comparing outputs under culture-unaware and culture-aware prompting.
- Mechanism: The method generates questions, then prompts the LLM to answer them under two settings. Questions showing inconsistent outputs between settings are deemed culture-representative and used for fine-tuning.
- Core assumption: LLMs possess latent cultural knowledge that can be elicited through targeted prompting.
- Evidence anchors:
  - [abstract] "CultureSPA first generates questions on various culture topics, then yields LLM outputs in response to these generated questions under both culture-aware and culture-unaware settings."
  - [section 4.2] "We collect culture-unaware LLM outputs as O = [ o1, o2, . . .] and culture-aware LLM outputs as Oc = [oc1, oc2, . . .] for each culture c."
- Break condition: If LLMs lack sufficient cultural knowledge or the prompting fails to elicit meaningful differences, the inconsistency detection will not yield useful culture-related instances.

### Mechanism 2
- Claim: Culture-related QA pairs collecting filters biased responses by retaining only samples with output shifts between prompting scenarios.
- Mechanism: Samples that the LLM is confident about regardless of cultural prompting are more likely to reflect biases. Samples that adapt to cultural contexts are more representative.
- Core assumption: Output consistency indicates bias, while inconsistency indicates cultural sensitivity.
- Evidence anchors:
  - [section 6.1] "Samples that the LLM is highly confident about, regardless of whether it is prompted to align to specific cultures, are more likely to reflect biases. In contrast, samples that readily adapt to specific cultural contexts are more likely to accurately represent that culture."
  - [section 6.1] "CRQPC is designed to exclude the former type of samples and retain the latter, ultimately producing better tuning data."
- Break condition: If the model's confidence does not correlate with bias, or if inconsistent outputs do not reflect true cultural differences, the filtering may remove valuable data.

### Mechanism 3
- Claim: Culture-joint SFT improves cross-cultural understanding by exposing the model to diverse cultural data simultaneously.
- Mechanism: Training on data from multiple cultures enhances the model's ability to understand relationships between cultures, leading to better cultural alignment and steerability.
- Core assumption: Exposure to multiple cultural perspectives during training improves generalization across cultures.
- Evidence anchors:
  - [section 5.4] "We hypothesize that SFT with data from various cultures enhances LLMs' ability to understand the relationships between different cultures, resulting in better cultural alignment and steerability."
  - [section 5.4] "CultureSPA (joint) outperforms CultureSPA (specific) across most data proportions."
- Break condition: If the model cannot effectively integrate multiple cultural perspectives, or if joint training dilutes specific cultural knowledge, performance may degrade.

## Foundational Learning

- Concept: Self-instructing data generation
  - Why needed here: To create diverse culture-related questions without manual annotation.
  - Quick check question: Can you explain how Self-Instruct uses in-context examples to generate new instructions?

- Concept: Culture-unaware vs. culture-aware prompting
  - Why needed here: To elicit and compare internal cultural knowledge in LLMs.
  - Quick check question: What is the key difference between culture-unaware and culture-aware prompting in the CultureSPA framework?

- Concept: Supervised fine-tuning (SFT) with LoRA
  - Why needed here: To efficiently update LLM parameters using the collected culture-related data.
  - Quick check question: Why is LoRA preferred over full fine-tuning in this context?

## Architecture Onboarding

- Component map: Seed question collection -> Diverse question generation -> Culture-unaware/aware prompting -> Inconsistency detection -> Culture-related QA pairs collecting -> Culture-joint/specific SFT -> Evaluation
- Critical path: Seed question collection -> Diverse question generation -> Culture-unaware/aware prompting -> Inconsistency detection -> Culture-related QA pairs collecting -> Culture-joint SFT -> Evaluation
- Design tradeoffs: Joint vs. specific SFT (efficiency vs. specialization), data quantity vs. quality, cross-culture thinking vs. direct prompting
- Failure signatures: Low inconsistency rates (poor knowledge elicitation), poor alignment improvement (ineffective fine-tuning), overfitting to specific cultures (loss of generalization)
- First 3 experiments:
  1. Generate 1000 questions per culture topic and verify diversity and quality.
  2. Run culture-unaware and culture-aware prompting on a small sample and check for output inconsistencies.
  3. Perform culture-joint SFT on a subset of cultures and evaluate alignment improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CultureSPA perform on underrepresented cultures not included in the WVS?
- Basis in paper: Inferred
- Why unresolved: The paper only tested 18 countries across five continents, leaving many cultures unexamined. The effectiveness of CultureSPA on cultures without WVS data remains unknown.
- What evidence would resolve it: Testing CultureSPA on cultures outside the WVS dataset and comparing alignment scores with those from WVS cultures would demonstrate its generalizability.

### Open Question 2
- Question: What is the long-term stability of cultural alignment improvements achieved through CultureSPA?
- Basis in paper: Inferred
- Why unresolved: The paper does not report on the persistence of cultural alignment improvements over time. It is unclear whether the effects of CultureSPA diminish with continued use or exposure to new data.
- What evidence would resolve it: Longitudinal studies tracking the cultural alignment scores of LLMs fine-tuned with CultureSPA over extended periods would reveal the stability of the improvements.

### Open Question 3
- Question: How does CultureSPA handle conflicting cultural values within a single culture?
- Basis in paper: Explicit
- Why unresolved: The paper does not address how CultureSPA manages situations where different subgroups within a culture hold conflicting values. It is unclear whether CultureSPA can effectively align with such internal cultural diversity.
- What evidence would resolve it: Evaluating CultureSPA on cultures known for internal value conflicts and analyzing its performance in resolving these conflicts would demonstrate its capability to handle cultural complexity.

## Limitations

- The framework assumes LLMs possess latent cultural knowledge that can be effectively elicited through prompting, which may not hold for all models.
- The filtering mechanism based on output inconsistency may not reliably distinguish between genuine cultural differences and random variation in model outputs.
- The study does not validate whether the culture-related QA pairs collecting actually removes biased responses versus simply removing consistently answered questions.

## Confidence

- **High Confidence**: The overall methodology of using culture-unaware versus culture-aware prompting to generate training data is clearly specified and technically sound. The experimental setup and evaluation metrics are well-defined.
- **Medium Confidence**: The claim that inconsistent outputs indicate cultural knowledge while consistent outputs indicate bias. This assumption drives the core filtering mechanism but lacks direct validation.
- **Low Confidence**: The effectiveness of the filtering mechanism in removing biased responses while retaining culturally representative samples. The paper asserts this works but does not provide direct evidence of bias reduction.

## Next Checks

1. **Correlation Analysis**: Verify that output consistency correlates with actual bias in LLM responses by manually annotating a sample of consistently answered questions for biased content versus randomly selected inconsistent questions.

2. **Ablation Study**: Compare CultureSPA performance against a baseline that uses all questions (not just inconsistent ones) to determine whether the filtering mechanism actually improves cultural alignment or simply reduces training data size.

3. **General Capability Testing**: Evaluate whether CultureSPA fine-tuning degrades performance on general language understanding tasks (e.g., MMLU, HELM) to ensure cultural alignment improvements do not come at the cost of general capabilities.