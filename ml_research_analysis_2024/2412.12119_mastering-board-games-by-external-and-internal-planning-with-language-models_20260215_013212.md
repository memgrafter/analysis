---
ver: rpa2
title: Mastering Board Games by External and Internal Planning with Language Models
arxiv_id: '2412.12119'
source_url: https://arxiv.org/abs/2412.12119
tags:
- kqkq
- language
- mcts
- arxiv
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates planning and reasoning capabilities of
  large language models (LLMs) in board games, proposing two approaches: external
  search, where the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations
  without external game engines, and internal search, where the model generates a
  linearized tree of search and final choice in context. The authors pre-train a multi-action-value
  (MAV) Transformer model capable of reliably tracking board states, predicting legal
  moves, and assigning values across multiple perfect-information games (Chess, Chess960,
  Connect Four, Hex) with minimal hallucinations.'
---

# Mastering Board Games by External and Internal Planning with Language Models

## Quick Facts
- arXiv ID: 2412.12119
- Source URL: https://arxiv.org/abs/2412.12119
- Reference count: 40
- One-line primary result: LLM-based agents achieve Grandmaster-level chess performance using external MCTS search with human-comparable move counts

## Executive Summary
This paper investigates planning and reasoning capabilities of large language models (LLMs) in board games, proposing two approaches: external search, where the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without external game engines, and internal search, where the model generates a linearized tree of search and final choice in context. The authors pre-train a multi-action-value (MAV) Transformer model capable of reliably tracking board states, predicting legal moves, and assigning values across multiple perfect-information games (Chess, Chess960, Connect Four, Hex) with minimal hallucinations. Evaluation shows substantial strength improvements over the base model, with external search achieving Grandmaster-level performance in chess within a human-comparable move count search budget, and internal search showing consistent improvements scaling with the search budget.

## Method Summary
The authors pre-train a multi-action-value (MAV) Transformer model on textual game data that includes state tracking, legal move prediction, and value estimation in a single output. For external search, they implement MCTS using the MAV model as both value function and transition function, with asynchronous simulations and dynamic virtual counts. For internal search, they fine-tune the MAV model on linearized search traces to enable in-context generation of search trees. The approaches are evaluated across Chess, Chess960, Connect Four, and Hex, with performance measured against game engines and other agents using Elo ratings.

## Key Results
- External MCTS with MAV model achieves Grandmaster-level chess performance within human-comparable search budgets
- Internal search shows consistent Elo improvements scaling with search budget across all tested games
- MAV model demonstrates <1% hallucination rate in state tracking and legal move prediction
- Substantial strength improvements over base model across all games and search approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MAV model functions simultaneously as a world model, value function, and policy function, eliminating the need for external game engines during inference.
- Mechanism: The model is trained on a textual game data format that includes state tracking, legal move prediction, and value estimation in a single output, allowing it to handle all aspects of game state management internally.
- Core assumption: The model can learn to reliably predict legal moves and game state transitions without hallucinating or requiring external validation.
- Evidence anchors:
  - [abstract] "capable of reliably tracking board states, predicting legal moves, and assigning values across multiple perfect-information games"
  - [section] "the MAV model is trained on examples following a flexible format which provides a strong learning signal and allows for a quality-compute tradeoff at inference time while remaining in-distribution"
  - [corpus] Weak - no direct corpus evidence for this specific multi-function capability
- Break condition: The model starts hallucinating illegal moves or fails to track game state accurately, requiring fallback to external engines.

### Mechanism 2
- Claim: External search with MCTS significantly improves LLM game-playing strength by combining learned world models with search-based planning.
- Mechanism: The MAV model serves as both the value function and transition function in MCTS, with the search algorithm using PUCT to balance exploration and exploitation while the model provides state-action values and legal moves.
- Core assumption: The MAV model's value estimates are sufficiently accurate to guide effective search, and the MCTS framework can handle the model's latency and potential hallucinations.
- Evidence anchors:
  - [abstract] "external search achieving Grandmaster-level performance in chess within a human-comparable move count search budget"
  - [section] "The external search algorithm guided by a learned world model is summarized in Algorithm 1"
  - [corpus] Weak - no direct corpus evidence for this specific MCTS integration
- Break condition: The search performance plateaus or degrades despite increasing simulation budgets, indicating the model's value function is insufficient for effective search guidance.

### Mechanism 3
- Claim: Internal search distills the search procedure directly into the LLM, allowing it to generate search trees in context without external controllers.
- Mechanism: The model is fine-tuned on linearized search traces, learning to execute search algorithms (expand, evaluate, summarize) within a single model call, scaling performance with search budget.
- Core assumption: The model can learn algorithmic reasoning patterns from search traces and execute them reliably in new contexts.
- Evidence anchors:
  - [abstract] "internal search showing consistent improvements scaling with the search budget"
  - [section] "training on tokens representing various search actions (such as expand, evaluate, summarize) and game actions"
  - [corpus] Weak - no direct corpus evidence for this specific internal search approach
- Break condition: The model fails to generate coherent search traces or the quality of search degrades with increased depth/breadth parameters.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The MAV model uses a decoder-only Transformer architecture, requiring understanding of how attention patterns enable state tracking and move prediction
  - Quick check question: How does the relative positioning of board squares in token space help the model navigate and manipulate the board?

- Concept: Reinforcement learning vs supervised learning tradeoffs
  - Why needed here: The MAV model uses supervised learning on game data rather than reinforcement learning, which has implications for generalization and performance
  - Quick check question: Why might supervised learning on game data be preferable to reinforcement learning for training chess-playing models?

- Concept: Monte Carlo Tree Search (MCTS) fundamentals
  - Why needed here: External search uses AlphaZero-style MCTS, requiring understanding of tree policies, value functions, and backup mechanisms
  - Quick check question: How does the PUCT formula balance exploration and exploitation in MCTS?

## Architecture Onboarding

- Component map:
  - MAV model: Core transformer handling state tracking, legal move prediction, and value estimation
  - External search controller: MCTS implementation using MAV for value and transition functions
  - Internal search fine-tuning: Additional training on search trace data to enable in-context search generation
  - Game-specific parsers: Convert MAV output to game actions and handle hallucinations
  - Asynchronous evaluation system: Manage batch processing of model calls during search

- Critical path:
  1. Input game state → MAV model → legal moves + values
  2. For external search: MCTS uses MAV outputs to build search tree → final move selection
  3. For internal search: MAV directly generates linearized search tree → final move selection
  4. Game engine validates move and updates state

- Design tradeoffs:
  - Single-model approach vs separate world model/value function/policy
  - Synchronous vs asynchronous MCTS implementation
  - Max scoring vs mean scoring for value representation
  - Fixed vs dynamic virtual counts in asynchronous MCTS

- Failure signatures:
  - High hallucination rate (>1%) in MAV outputs
  - Performance plateauing despite increased search budget
  - Internal search failing to generate coherent search traces
  - MAV model unable to handle out-of-distribution positions

- First 3 experiments:
  1. Baseline evaluation: Test MAV model on legal move accuracy and state tracking on OOD positions
  2. External search tuning: Compare performance of max vs mean scoring methods with varying simulation budgets
  3. Internal search scaling: Test different breadth/depth combinations to find optimal token usage vs performance tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the multi-action-value (MAV) model be effectively extended to imperfect-information games or games with hidden states?
- Basis in paper: [inferred] The paper focuses exclusively on perfect-information games (Chess, Chess960, Connect Four, Hex) and does not explore imperfect-information scenarios.
- Why unresolved: The MAV model's architecture and training methodology are specifically designed for perfect-information games, and there is no exploration of how it might handle hidden states or partial observability.
- What evidence would resolve it: Testing MAV in games like Poker or Stratego, or modifying it to incorporate belief states, would demonstrate its applicability to imperfect-information settings.

### Open Question 2
- Question: How does the performance of MAV-based agents scale with increased computational resources (e.g., simulation count, token budget)?
- Basis in paper: [explicit] The paper shows logarithmic improvements in Elo with more simulations in external search and improved playing strength with larger search budgets in internal search, but does not explore the limits of this scaling.
- Why unresolved: The experiments are conducted in a low-simulation regime, and the paper does not investigate how performance plateaus or continues to improve with significantly higher resources.
- What evidence would resolve it: Running MAV agents with orders of magnitude more simulations or tokens, and comparing their performance to traditional engines, would clarify the scalability limits.

### Open Question 3
- Question: Can the internal search approach be generalized to other domains beyond board games, such as natural language tasks or robotics?
- Basis in paper: [explicit] The authors suggest that the combination of search with domain knowledge is not specific to board games and hint at potential extensions to more general language model inference and training techniques.
- Why unresolved: The paper focuses on board games and does not provide evidence or experiments for applying internal search to other domains.
- What evidence would resolve it: Applying internal search to tasks like code generation, mathematical problem-solving, or robot path planning, and comparing its performance to traditional methods, would demonstrate its broader applicability.

## Limitations

- Evaluation scope is narrow, testing against limited opponents and search budgets without broader tournament conditions
- Lacks detailed ablation studies on critical design choices like scoring methods and virtual count adjustments
- Unclear generalization properties of internal search to new games without additional fine-tuning

## Confidence

**High Confidence Claims:**
- The MAV model architecture can track board states and predict legal moves across multiple board games with low hallucination rates (<1%)
- External MCTS using MAV models provides significant performance improvements over base models
- Internal search shows consistent improvements with increased search budgets

**Medium Confidence Claims:**
- Grandmaster-level chess performance is achieved within human-comparable search budgets
- The combination of world model, value function, and policy in a single MAV model is effective
- Asynchronous MCTS with dynamic virtual counts improves search efficiency

**Low Confidence Claims:**
- Internal search will generalize to new games without additional fine-tuning
- The MAV model can handle all edge cases in board state tracking without external validation
- The specific search budget sizes used are optimal for the computational resources invested

## Next Checks

1. **Hallucination Impact Analysis**: Conduct systematic testing to identify how different types and frequencies of hallucinations affect search performance, particularly focusing on whether certain illegal moves or state tracking errors are more detrimental than others.

2. **Scaling Limits Investigation**: Test the external and internal search approaches with substantially larger search budgets (beyond the current limits) to determine where performance plateaus occur and whether the claimed grandmaster-level performance holds under more extensive search.

3. **Cross-Game Generalization Test**: Evaluate the internal search approach on a new game variant (such as a modified version of Connect Four or a different board size) without additional fine-tuning to assess the true generalization capabilities of the learned search algorithm.