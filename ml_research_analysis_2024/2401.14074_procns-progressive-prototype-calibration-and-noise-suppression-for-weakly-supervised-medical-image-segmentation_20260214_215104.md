---
ver: rpa2
title: 'ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised
  Medical Image Segmentation'
arxiv_id: '2401.14074'
source_url: https://arxiv.org/abs/2401.14074
tags:
- segmentation
- procns
- noise
- image
- regions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes ProCNS, a novel weakly-supervised medical
  image segmentation approach that addresses the issues of representation bias and
  noise accumulation in existing methods. ProCNS comprises two synergistic modules:
  a Prototype-based Regional Spatial Affinity (PRSA) loss and an Adaptive Noise Perception
  and Masking (ANPM) module.'
---

# ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation

## Quick Facts
- arXiv ID: 2401.14074
- Source URL: https://arxiv.org/abs/2401.14074
- Reference count: 40
- Primary result: ProCNS achieves 93.74% DSC for FAZ, 88.32% DSC for ODOC, and 82.73% DSC for polyp segmentation

## Executive Summary
ProCNS introduces a novel weakly-supervised medical image segmentation approach that addresses representation bias and noise accumulation in existing methods. The framework combines a Prototype-based Regional Spatial Affinity (PRSA) loss with an Adaptive Noise Perception and Masking (ANPM) module. By leveraging multi-scale prototype representations and progressively identifying noisy regions in pseudo-labels, ProCNS significantly outperforms state-of-the-art methods across three different medical imaging tasks. The method demonstrates robust performance on foveal avascular zone, optic disc and cup, and polyp segmentation tasks using various forms of sparse annotations.

## Method Summary
ProCNS employs a two-stage training process: an initialization stage using temporal ensembling to generate initial pseudo-labels from sparse annotations, followed by a main stage that combines PRSA loss and ANPM module. The PRSA loss leverages multi-scale sample-wise prototypes to maximize intra-class compactness and inter-class separability through combined spatial and semantic affinities. The ANPM module progressively identifies and masks noisy regions within pseudo-proposals while generating specialized soft pseudo-labels for these regions. The framework is trained using a combination of pixel-wise cross-entropy loss, PRSA loss, noise-specific loss, and regularization terms, optimized through the AdamW optimizer.

## Key Results
- Achieves Dice Similarity Coefficient (DSC) of 93.74%, 88.32%, and 82.73% for FAZ, ODOC, and polyp segmentation tasks respectively
- Significantly outperforms representative state-of-the-art weakly-supervised segmentation methods
- Demonstrates robust performance across three different medical imaging modalities (sOCTA, retinal fundus, colonoscopy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prototype-based regional spatial affinity loss (PRSA) improves intra-class compactness and inter-class separability by leveraging both low-level spatial and high-level semantic pair-wise affinities.
- Mechanism: PRSA computes sample-wise prototypes at multiple scales (deep for global semantics, shallow for local spatial info) and uses cosine similarity between pixel embeddings and prototypes to form a relation matrix. This matrix is combined with both image-based spatial affinities (Gaussian kernel on intensity/location) and prototype-based semantic affinities to generate a refined prediction that maximizes affinity between spatially and semantically related pixels.
- Core assumption: Multi-scale sample-wise prototypes are more representative than batch-wise or epoch-wise prototypes for each individual image, capturing both global semantics and local spatial details necessary for accurate segmentation.
- Evidence anchors:
  - [abstract]: "The PRSA loss leverages low-level spatial and high-level semantic pair-wise affinities to maximize intra-class compactness and inter-class separability."
  - [section]: "Given an image ð‘¥ and the corresponding label ð‘¦ in a same batch (with a batch size ð‘), the deep prototypes ð‘§dpð‘ are batch-wise, signifying that images within the same batch share identical deep prototypes. The shallow prototypes ð‘§swð‘–,ð‘ are sample-wise..."
  - [corpus]: Weak evidence - no direct citation, but the mechanism aligns with prototype learning literature in few-shot and semi-supervised settings.
- Break condition: If the multi-scale prototype computation fails to capture the relevant features due to domain shift or if the affinity matrices become too noisy, the PRSA loss may degrade segmentation performance rather than improve it.

### Mechanism 2
- Claim: Adaptive noise perception and masking (ANPM) module progressively identifies and masks noisy regions in pseudo-labels, reducing erroneous interference during prototype computation.
- Mechanism: ANPM uses iterative label refinement where at each epoch it computes a reliable region mask (intersection of one-hot prediction and previous denoised label) and a noisy region mask (symmetric difference). The reliable regions are preserved for prototype computation while noisy regions are masked out, and soft pseudo-labels are generated for these noisy regions to provide additional supervision.
- Core assumption: Model predictions gradually fit correct labels before overfitting to noise, so using predictions at certain epochs to identify noisy regions is valid.
- Evidence anchors:
  - [abstract]: "Meanwhile, we propose an Adaptive Noise Perception and Masking (ANPM) module to obtain more enriched and representative prototype representations, which adaptively identifies and masks noisy regions within the pseudo proposals..."
  - [section]: "Observations by Liu et al. [30] indicate that a model's adaptation or fitting to noisy labels is gradual, wherein the model initially fits correct labels, then gradually overfits to noise."
  - [corpus]: Weak evidence - no direct citation, but the mechanism is consistent with iterative label refinement approaches in weakly supervised learning.
- Break condition: If the model predictions are too uncertain or if the noisy regions are misidentified as reliable regions, the ANPM module may propagate errors instead of reducing them.

### Mechanism 3
- Claim: Temporal ensembling strategy generates more reliable initial pseudo-labels by averaging predictions across training epochs, reducing the impact of ambiguous boundary regions.
- Mechanism: EMA is applied to model predictions during the initialization stage, where the temporal ensemble prediction at epoch t is a weighted average of the current prediction and previous ensemble. This smoothed prediction is then used to generate initial pseudo-labels by taking the argmax.
- Core assumption: Predictions for ambiguous regions fluctuate during training, and averaging over time reduces this fluctuation to produce more stable labels.
- Evidence anchors:
  - [abstract]: "Inspired by Temporal Ensembling [28], we perform exponential moving average (EMA) of the model's predictions to relieve the issue."
  - [section]: "The temporal ensemble prediction ð‘ð‘¡ at epoch ð‘¡ is defined as ð‘ð‘¡ = ð›¼ð‘ð‘¡ + (1 âˆ’ ð›¼)ð‘ð‘¡âˆ’1..."
  - [corpus]: Weak evidence - the reference to Temporal Ensembling [28] is made but not detailed in the corpus.
- Break condition: If the EMA decay rate is too high or too low, the temporal ensemble may either be too unstable or too slow to adapt, respectively, failing to generate reliable initial pseudo-labels.

## Foundational Learning

- Concept: Prototype representation learning
  - Why needed here: Medical images have ambiguous boundaries and complex structures that make pixel-wise supervision challenging. Prototypes provide a way to capture class-representative features that can guide segmentation even when dense annotations are unavailable.
  - Quick check question: How do sample-wise prototypes differ from batch-wise prototypes in their ability to handle data heterogeneity in medical images?

- Concept: Weakly supervised segmentation (WSS)
  - Why needed here: Collecting dense annotations for medical images is expensive and time-consuming, requiring expert knowledge. WSS allows training segmentation models using sparse annotations (points, scribbles, blocks) while still achieving good performance.
  - Quick check question: What are the main challenges of using sparse annotations for medical image segmentation compared to natural images?

- Concept: Noise suppression in pseudo-labels
  - Why needed here: Pseudo-labels generated from sparse annotations often contain errors, especially near boundaries and in regions with similar intensity patterns. These errors can accumulate during training and degrade model performance.
  - Quick check question: Why is it important to distinguish between reliable and noisy regions in pseudo-labels, and how does this distinction affect prototype computation?

## Architecture Onboarding

- Component map: Initialization stage (sparse annotations -> pCE loss -> temporal ensembling -> initial pseudo-labels) -> Main stage (PRSA loss + ANPM module + noise loss + pCE loss -> final predictions)

- Critical path: 1. Generate initial pseudo-labels using temporal ensembling in initialization stage 2. Compute multi-scale sample-wise prototypes from denoised labels 3. Form relation matrix using cosine similarity between pixel embeddings and prototypes 4. Calculate PRSA loss using both spatial and semantic affinities 5. Identify and mask noisy regions using ANPM 6. Generate soft pseudo-labels for noisy regions 7. Calculate additional supervision loss on noisy regions 8. Combine all losses for final training

- Design tradeoffs:
  - Sample-wise vs batch-wise prototypes: Sample-wise prototypes better handle data heterogeneity but require more computation
  - Multiple scale embeddings: Combining deep and shallow features captures both global semantics and local details but increases model complexity
  - Iterative refinement: Progressive improvement of prototypes but requires careful epoch scheduling

- Failure signatures:
  - Performance plateaus early: May indicate noisy regions are not being properly identified/masked
  - Inconsistent results across runs: Could suggest instability in prototype computation or affinity calculation
  - Poor boundary segmentation: Might indicate insufficient spatial affinity or prototype calibration

- First 3 experiments:
  1. Ablation study removing PRSA loss to verify its contribution to segmentation performance
  2. Ablation study removing ANPM to assess impact of noise suppression on prototype quality
  3. Hyperparameter sensitivity analysis for trade-off coefficients (Î»1, Î»2, Î»3, Î»4) to find optimal balance between different loss components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ProCNS vary with different types of sparse annotations (e.g., points, scribbles, blocks) across various medical image segmentation tasks?
- Basis in paper: [explicit] The paper mentions that ProCNS is evaluated on three medical image segmentation tasks involving different modalities and forms of sparse annotations.
- Why unresolved: The paper does not provide a detailed comparison of ProCNS's performance with different types of sparse annotations.
- What evidence would resolve it: A comprehensive analysis comparing ProCNS's performance with different types of sparse annotations across various medical image segmentation tasks would provide insights into the method's versatility and robustness.

### Open Question 2
- Question: Can the PRSA loss and ANPM modules be effectively applied to other weakly-supervised segmentation methods, and how would they impact the performance of these methods?
- Basis in paper: [explicit] The paper suggests that ProCNS can be flexibly utilized as a plugin for existing WSS methods.
- Why unresolved: The paper does not provide experimental results on integrating ProCNS's modules with other WSS methods.
- What evidence would resolve it: Empirical studies evaluating the integration of ProCNS's modules with various WSS methods would demonstrate their generalizability and potential for improving the performance of existing methods.

### Open Question 3
- Question: How does the performance of ProCNS change with varying levels of annotation sparsity, and is there an optimal level of annotation sparsity for achieving the best segmentation results?
- Basis in paper: [explicit] The paper mentions that the impact of annotation sparsity on ProCNS's performance is analyzed.
- Why unresolved: The paper does not provide a detailed analysis of ProCNS's performance with varying levels of annotation sparsity.
- What evidence would resolve it: A comprehensive study evaluating ProCNS's performance with different levels of annotation sparsity would reveal the method's sensitivity to annotation sparsity and identify the optimal level for achieving the best segmentation results.

## Limitations

- The framework relies on the assumption that model predictions gradually fit correct labels before overfitting to noise, which may not hold for all medical imaging tasks
- The effectiveness of the ANPM module depends on reliably identifying noisy regions through symmetric differences, which can fail when noise patterns persist across epochs
- Limited guidance is provided on how to balance multi-scale prototype computation across different medical imaging modalities

## Confidence

**High Confidence:** The overall framework architecture combining temporal ensembling, prototype-based affinity learning, and noise suppression is well-established and has been validated across multiple medical imaging tasks (FAZ, ODOC, polyp segmentation).

**Medium Confidence:** The specific implementation details of the PRSA loss and ANPM module show promise based on ablation studies, but the robustness of these components across diverse medical imaging scenarios remains to be fully established.

**Low Confidence:** The claim that ProCNS significantly outperforms all representative state-of-the-art methods requires careful scrutiny, particularly given the limited comparison to the most recent weakly-supervised segmentation approaches.

## Next Checks

1. **Cross-dataset generalization test:** Evaluate ProCNS on additional medical imaging datasets beyond the three presented to assess whether the framework generalizes across different imaging modalities and anatomical structures.

2. **Ablation of temporal assumptions:** Conduct experiments where the order of noise fitting is artificially reversed (e.g., by injecting synthetic noise patterns) to test the robustness of the ANPM module's assumptions about temporal noise accumulation.

3. **Prototype stability analysis:** Perform sensitivity analysis on prototype computation by varying the number of scales, prototype update frequencies, and affinity weightings to determine the stability boundaries of the PRSA loss across different medical imaging tasks.