---
ver: rpa2
title: 'Constructive Apraxia: An Unexpected Limit of Instructible Vision-Language
  Models and Analog for Human Cognitive Disorders'
arxiv_id: '2410.03551'
source_url: https://arxiv.org/abs/2410.03551
tags:
- lines
- horizontal
- spatial
- image
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tests 25 vision-language models on their ability to
  reproduce the Ponzo illusion, a basic spatial reasoning task. Models consistently
  failed to draw two horizontal lines correctly against a perspective background,
  mirroring deficits seen in human constructive apraxia patients.
---

# Constructive Apraxia: An Unexpected Limit of Instructible Vision-Language Models and Analog for Human Cognitive Disorders

## Quick Facts
- **arXiv ID**: 2410.03551
- **Source URL**: https://arxiv.org/abs/2410.03551
- **Reference count**: 37
- **Primary result**: 24 out of 25 tested vision-language models failed to correctly render two horizontal lines against a perspective background, revealing fundamental spatial reasoning limitations

## Executive Summary
This study systematically evaluates 25 vision-language models on a basic spatial reasoning task - the Ponzo illusion - where models must draw two horizontal lines against a perspective background. Remarkably, 24 out of 25 models failed this task, producing lines that followed the perspective instead of remaining horizontal. This systematic failure mirrors deficits seen in human constructive apraxia patients and suggests that VLMs lack explicit encoding of geometric principles and physical laws. The findings reveal a critical limitation in current VLM architectures that impacts their reliability in applications requiring precise spatial reasoning.

## Method Summary
The study tested 25 vision-language models using a standardized prompt to create the Ponzo illusion, asking models to draw two yellow horizontal lines against a perspective background of a receding road. Each model generated 50 samples per task, which were evaluated by human judges to determine if the lines were correctly rendered as horizontal. The models tested included major commercial systems like GPT-4 Vision, DALL-E 3, Midjourney v5, and Stable Diffusion XL, among others. Performance was scored as binary Pass/Fail based on whether the lines remained horizontal regardless of the perspective background.

## Key Results
- 24 out of 25 tested VLMs failed to correctly render two horizontal lines against a perspective background
- Failure pattern mirrors human constructive apraxia deficits seen in patients with parietal lobe damage
- Models consistently misinterpreted spatial instructions, producing tilted or misaligned lines that followed perspective rather than remaining horizontal
- Limitation persists across diverse architectures including GPT-4 Vision, DALL-E 3, and Midjourney v5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs fail at spatial reasoning tasks because they lack explicit encoding of geometric principles and physical laws.
- Mechanism: VLMs learn correlations between text and images but do not inherently understand spatial concepts. This results in unreliable execution of seemingly simple visual instructions.
- Core assumption: The statistical nature of VLM training does not encode geometry or physical space rules.
- Evidence anchors:
  - [abstract] "This limitation in AI systems provides a novel computational model for studying spatial cognition deficits and highlights a critical area for improvement in VLM architecture and training methodologies."
  - [section] "The models consistently misinterpreted spatial instructions, producing tilted or misaligned lines that followed the perspective of the background rather than remaining horizontal."
  - [corpus] "Weak evidence for this specific mechanism in corpus. Related papers discuss spatial reasoning benchmarks but do not directly address geometric principle encoding."
- Break condition: If VLMs are trained with explicit geometric rules or physics engines, they might overcome this limitation.

### Mechanism 2
- Claim: VLMs struggle with basic spatial tasks due to a gap between natural language understanding and visual output generation.
- Mechanism: VLMs can recognize and describe images accurately but cannot reproduce them, indicating a dissociation between visual recognition and visual production systems.
- Core assumption: There is a fundamental difference between understanding spatial relationships and executing spatial instructions.
- Evidence anchors:
  - [abstract] "The models consistently misinterpreted spatial instructions, producing tilted or misaligned lines that followed the perspective of the background rather than remaining horizontal."
  - [section] "This behavior is strikingly similar to how apraxia patients struggle to copy or construct simple figures despite intact visual perception and motor skills."
  - [corpus] "No direct evidence in corpus. The related papers focus on benchmarking spatial reasoning but do not discuss the recognition vs. production gap."
- Break condition: If VLMs are designed to bridge the gap between recognition and production, they might improve spatial task performance.

### Mechanism 3
- Claim: VLMs exhibit systematic biases in spatial understanding, similar to cognitive disorders in humans.
- Mechanism: VLMs consistently fail to execute spatial tasks in a way that mirrors deficits seen in constructive apraxia patients, suggesting a shared limitation in spatial reasoning.
- Core assumption: The limitations in VLMs are analogous to human cognitive disorders, providing insights into both artificial and human cognition.
- Evidence anchors:
  - [abstract] "Remarkably, 24 out of 25 models failed to correctly render two horizontal lines against a perspective background, mirroring the deficits seen in patients with parietal lobe damage."
  - [section] "This limitation in AI systems provides a novel computational model for studying spatial cognition deficits and highlights a critical area for improvement in VLM architecture and training methodologies."
  - [corpus] "Weak evidence in corpus. Related papers discuss spatial reasoning but do not directly compare VLM limitations to human cognitive disorders."
- Break condition: If VLMs are trained with methods that address these systematic biases, they might overcome the limitations.

## Foundational Learning

- Concept: Spatial reasoning and geometric principles
  - Why needed here: Understanding spatial relationships and geometric concepts is crucial for VLMs to accurately execute spatial tasks.
  - Quick check question: Can you explain why a horizontal line remains horizontal regardless of the perspective background?

- Concept: Vision-language model architecture
  - Why needed here: Knowledge of VLM architecture helps in identifying why these models fail at spatial reasoning tasks.
  - Quick check question: What are the key components of a VLM that contribute to its ability to process visual and textual information?

- Concept: Cognitive disorders and their analogs in AI
  - Why needed here: Understanding human cognitive disorders like constructive apraxia provides insights into the limitations of VLMs.
  - Quick check question: How do the deficits in constructive apraxia patients relate to the failures of VLMs in spatial reasoning tasks?

## Architecture Onboarding

- Component map: Vision encoder -> Language model -> Fusion layer -> Image generator
- Critical path: Text prompt interpretation -> Spatial reasoning processing -> Visual output generation
- Design tradeoffs: Balancing spatial reasoning accuracy with computational efficiency and training data requirements
- Failure signatures: Misaligned or tilted lines, incorrect object positioning, inability to reproduce complex figures
- First 3 experiments:
  1. Test the VLM's ability to draw simple geometric shapes with specific orientations.
  2. Evaluate the VLM's performance in positioning multiple objects relative to each other.
  3. Assess the VLM's capability to reproduce intricate images and scenes with multiple elements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can vision-language models be trained to overcome their spatial reasoning limitations, particularly in tasks requiring precise geometric understanding like the Ponzo illusion?
- Basis in paper: [explicit] The paper discusses the models' fundamental lack of understanding of basic geometric principles and suggests this as a critical area for improvement in VLM architecture and training methodologies.
- Why unresolved: The paper identifies the problem but does not propose specific solutions or conduct experiments to test potential training approaches that might address these spatial reasoning deficits.
- What evidence would resolve it: A series of experiments testing different training methodologies, architectural modifications, or fine-tuning approaches that successfully enable VLMs to consistently produce accurate horizontal lines and understand geometric relationships would resolve this question.

### Open Question 2
- Question: How do the spatial reasoning limitations of VLMs compare to the cognitive deficits observed in human constructive apraxia patients, and can these parallels inform both AI development and neurological understanding?
- Basis in paper: [explicit] The paper explicitly draws parallels between VLM limitations and human cognitive disorders, suggesting that studying these similarities could impact both AI development and our comprehension of human cognition.
- Why unresolved: While the paper identifies similarities, it does not quantify these parallels or explore their implications for either field in detail.
- What evidence would resolve it: Systematic comparison studies quantifying the similarities and differences between VLM spatial reasoning failures and human apraxia performance, along with investigations into how these insights could inform both AI architectures and cognitive rehabilitation strategies.

### Open Question 3
- Question: What is the underlying mechanism causing VLMs to fail at spatial reasoning tasks, and how does this relate to their training methodology and data representation?
- Basis in paper: [inferred] The paper suggests that the models' struggle stems from their training methodology, which lacks explicit encoding of physical laws and geometric principles, but does not delve into the specific mechanisms.
- Why unresolved: The paper identifies a potential cause but does not explore the technical details of how the models process spatial information or why their current approach fails.
- What evidence would resolve it: Detailed analysis of the internal representations and processing steps of VLMs when handling spatial tasks, potentially through techniques like attention visualization or feature extraction, to identify the specific breakdown points in spatial reasoning.

## Limitations

- Evaluation methodology relies on subjective human judgment without quantitative tolerance thresholds
- Unclear whether models received reference examples of the Ponzo illusion alongside text prompts
- Results may not generalize to all VLMs or future architectures beyond those tested

## Confidence

- **High Confidence**: VLMs consistently fail at basic spatial reasoning tasks involving geometric principles (measured by 24/25 models failing the Ponzo illusion task)
- **Medium Confidence**: The failure pattern mirrors human constructive apraxia deficits (supported by qualitative similarity but limited quantitative comparison)
- **Low Confidence**: Current VLM architecture cannot be modified to overcome this limitation (the study does not explore architectural modifications or training interventions)

## Next Checks

1. Conduct quantitative analysis by measuring line angles in model outputs to establish objective pass/fail thresholds and assess whether failures follow a normal distribution or cluster around specific error types.

2. Test whether providing explicit geometric instructions ("draw lines perpendicular to the y-axis") or visual examples improves model performance, isolating whether the issue is instruction interpretation or spatial reasoning.

3. Evaluate whether VLMs can correctly position multiple objects in spatial relationships (e.g., "place a red circle above a blue square") to determine if the limitation extends beyond line drawing to broader spatial reasoning capabilities.