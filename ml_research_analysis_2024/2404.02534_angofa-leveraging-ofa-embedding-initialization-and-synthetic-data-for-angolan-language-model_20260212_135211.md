---
ver: rpa2
title: 'ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan
  Language Model'
arxiv_id: '2404.02534'
source_url: https://arxiv.org/abs/2404.02534
tags:
- languages
- data
- language
- https
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces four multilingual pre-trained language models
  (PLMs) specifically finetuned for Angolan languages using a Multilingual Adaptive
  Fine-tuning (MAFT) approach. The authors improve upon existing baselines (AfroXLMR-base
  and OFA) by leveraging informed embedding initialization and synthetic data.
---

# ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model

## Quick Facts
- arXiv ID: 2404.02534
- Source URL: https://arxiv.org/abs/2404.02534
- Reference count: 13
- Primary result: Four multilingual PLMs finetuned for Angolan languages outperform baselines by 12.3 and 3.8 points respectively

## Executive Summary
This paper introduces four multilingual pre-trained language models (PLMs) specifically finetuned for Angolan languages using a Multilingual Adaptive Fine-tuning (MAFT) approach. The authors improve upon existing baselines (AfroXLMR-base and OFA) by leveraging informed embedding initialization and synthetic data. Their approach combines MAFT with OFA embedding factorization and synthetic data generated through machine translation. The resulting models, called ANGOFA, significantly outperform the baselines on a text classification task across five Angolan languages (Chokwe, Kimbundu, Kikongo, Luba-Kasai, and Umbundu).

## Method Summary
The authors developed two main variants: ANGXLMR (MAFT only) and ANGOFA (MAFT with OFA embedding factorization), both with and without synthetic data augmentation. They expanded the vocabulary from 250K to 400K tokens, implemented OFA embedding factorization using ColexNet+ embeddings, and performed MAFT fine-tuning on combined monolingual and synthetic data from the NLLB dataset. The models were evaluated on the SIB-200 text classification dataset using weighted F1 score across 7 classes.

## Key Results
- ANGOFA significantly outperforms ANGXLMR and OFA baselines on SIB-200 text classification
- Weighted F1 scores improve by 12.3 points over AfroXLMR-base and 3.8 points over OFA
- Synthetic data augmentation expands training corpus from 281.6 MB to 808.7 MB
- Best performance achieved with vocabulary expansion (250K→400K) combined with OFA initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Informed embedding initialization via OFA improves low-resource language PLM performance by preserving lexical knowledge from source models
- Mechanism: OFA factorizes the source PLM's embedding matrix into smaller matrices, representing new subword embeddings as weighted combinations of source subword embeddings using similarity scores from external multilingual embeddings
- Core assumption: Lexical knowledge encoded in source PLM embeddings transfers effectively to new languages when combined through similarity-weighted combinations
- Evidence anchors: Abstract shows 12.3 and 3.8 point improvements; section explains OFA addresses random initialization problems
- Break condition: If source model lacks relevant lexical patterns for target languages, or if similarity weights poorly align with true semantic relationships

### Mechanism 2
- Claim: Synthetic data generation through machine translation significantly expands effective training corpus for low-resource languages
- Mechanism: NLLB-600M machine translation model translates English news commentary into target Angolan languages, creating parallel data that supplements limited monolingual resources
- Core assumption: Machine-translated text preserves sufficient linguistic structure and patterns to be useful for PLM pre-training
- Evidence anchors: Abstract shows performance improvements; section discusses synthetic data generation for low-resource languages
- Break condition: If machine translation introduces significant noise or if translated patterns diverge too much from native language usage

### Mechanism 3
- Claim: Vocabulary expansion combined with informed initialization creates more comprehensive language coverage than either approach alone
- Mechanism: Expanding vocabulary from 250K to 400K tokens and using OFA to initialize new embeddings creates a more complete representation space for target languages
- Core assumption: Larger vocabulary with informed initialization captures more linguistic diversity than baseline approaches
- Evidence anchors: Abstract shows ANGOFA significantly outperforms baselines; section mentions Glot-500 vocabulary expansion approach
- Break condition: If expanded vocabulary introduces too many irrelevant tokens or if initialization fails to properly distribute weights

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the core self-supervised learning objective used in XLM-R pre-training, which ANGOFA builds upon
  - Quick check question: What percentage of tokens are typically masked during MLM training in transformer-based PLMs?

- Concept: Embedding factorization
  - Why needed here: OFA's core innovation relies on factorizing large embedding matrices into smaller components for efficient initialization
  - Quick check question: How does matrix factorization reduce parameter count while maintaining representational capacity?

- Concept: Transfer learning in multilingual contexts
  - Why needed here: ANGOFA leverages knowledge transfer from high-resource languages to improve low-resource Angolan language performance
  - Quick check question: What factors determine successful cross-lingual transfer in PLM adaptation?

## Architecture Onboarding

- Component map: Base XLM-R encoder with expanded vocabulary (250K→400K) -> OFA embedding initialization layer (factorized matrices) -> MAFT fine-tuning pipeline with monolingual and synthetic data -> Classification head for downstream evaluation

- Critical path: 1. Vocabulary expansion and tokenizer modification 2. OFA embedding initialization using external multilingual embeddings 3. MAFT fine-tuning on combined monolingual + synthetic corpus 4. Evaluation on SIB-200 text classification task

- Design tradeoffs: Larger vocabulary improves coverage but increases memory usage; OFA initialization improves performance but adds computational complexity; synthetic data increases training volume but may introduce noise

- Failure signatures: Performance degradation on held-out Angolan languages; unstable training loss during MAFT fine-tuning; large gap between training and validation performance

- First 3 experiments: 1. Baseline comparison: ANGXLMR (MAFT only) vs OFA-initialized model on SIB-200 2. Synthetic data impact: ANGXLMR with vs without synthetic data augmentation 3. Vocabulary size sensitivity: Test different vocabulary expansion sizes (300K, 400K, 500K)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does synthetic data quality or quantity have a greater impact on model performance for low-resource languages?
- Basis in paper: [inferred] The paper shows that ANG XLM-R with synthetic data performs better than AfroXLMR-base-76L, but doesn't compare different amounts or qualities of synthetic data
- Why unresolved: The study only used one synthetic data generation approach and didn't experiment with varying quantities or qualities of synthetic data
- What evidence would resolve it: Experiments comparing different amounts of synthetic data, different translation models, or different data generation strategies

### Open Question 2
- Question: What specific factors determine whether MAFT or OFA embedding initialization performs better for a given language?
- Basis in paper: [explicit] The authors note that "the specific factors contributing to ANG XLM-R's superiority over OFA, especially in the context of Luba-Kassai, raise intriguing questions"
- Why unresolved: The paper shows that MAFT can outperform OFA in some cases (Luba-Kassai), but doesn't identify the specific characteristics that determine which approach is better
- What evidence would resolve it: Systematic analysis of language characteristics (monolingual corpus size, similarity to source languages, script overlap) and their relationship to MAFT vs OFA performance

### Open Question 3
- Question: What is the optimal latent dimension size for OFA embedding factorization in multilingual models?
- Basis in paper: [explicit] The authors mention that "we opted to maintain 768 as the only latent dimension in our experiments based on insights from...and further supported by preliminary results from our own experiments"
- Why unresolved: The paper acknowledges that lower dimensions may cause information loss but only tested one dimension size (768) without exploring alternatives
- What evidence would resolve it: Systematic experiments with different latent dimension sizes and their impact on downstream task performance

### Open Question 4
- Question: How does the performance of region-specific PLMs compare to general multilingual models when evaluated on tasks beyond text classification?
- Basis in paper: [inferred] The study only evaluates on text classification (SIB-200) but shows region-specific models (ANG XLM-R) outperform general models
- Why unresolved: The evaluation is limited to one task type, so it's unclear if the advantage of region-specific models generalizes to other NLP tasks
- What evidence would resolve it: Evaluation of the same models on diverse NLP tasks like named entity recognition, question answering, and machine translation

## Limitations
- Evaluation scope limited to single downstream task (text classification on SIB-200) without validation across multiple NLP applications
- Synthetic data quality not analyzed - no validation of machine translation quality or potential noise introduction
- OFA embedding initialization sensitivity to external multilingual embeddings not examined for specific Angolan languages

## Confidence
- High Confidence: Comparative results showing ANGOFA outperforming ANGXLMR and OFA baselines are well-supported by reported F1 scores
- Medium Confidence: Mechanism explanations for why informed embedding initialization and synthetic data improve performance are plausible but lack direct empirical validation
- Low Confidence: Claim that this approach represents a general solution for low-resource language model adaptation is not well-supported due to limited evaluation scope

## Next Checks
1. Evaluate ANGOFA on at least two additional downstream tasks (e.g., named entity recognition and sentiment analysis) for the five Angolan languages to assess whether improvements extend beyond text classification

2. Conduct human evaluation of a sample of machine-translated synthetic data to measure translation quality and identify potential sources of noise or error that could affect model training

3. Systematically disable each key innovation (vocabulary expansion, OFA initialization, synthetic data) to quantify their individual contributions to the overall performance improvements