---
ver: rpa2
title: IDs for AI Systems
arxiv_id: '2406.12137'
source_url: https://arxiv.org/abs/2406.12137
tags:
- could
- systems
- instance
- system
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for AI system IDs to address the
  problem of lacking information needed to interact with AI systems. The framework
  involves assigning unique identifiers and associated attributes to instances of
  AI systems, which can aid decisions about interactions, incident investigation,
  and allocation of liability.
---

# IDs for AI Systems

## Quick Facts
- arXiv ID: 2406.12137
- Source URL: https://arxiv.org/abs/2406.12137
- Reference count: 33
- Primary result: Proposes framework for AI system IDs to enable better interaction, incident investigation, and liability allocation

## Executive Summary
This paper addresses the problem of lacking information needed to interact with AI systems, investigate incidents, and allocate liability by proposing a framework for assigning unique identifiers and associated attributes to instances of AI systems. The framework could enable parties to make informed decisions about interactions, aid incident investigation, and facilitate liability allocation. The authors argue there could be significant demand for AI IDs from governments, service providers, and users, and suggest limited experimentation with IDs in high-stakes settings.

## Method Summary
The paper proposes a framework for AI system IDs that involves assigning unique identifiers to instances of AI systems and associating attributes with these IDs. The framework relies on existing technologies like TLS/SSL for preventing tampering, digital signatures for preventing ID spoofing, and content provenance standards for preventing instance spoofing. The authors discuss potential implementation scenarios, limitations and risks, and related work on digital infrastructure for AI systems.

## Key Results
- AI system IDs could enable precise incident investigation and liability attribution by linking each instance to its incident history and ancestor/descendant relationships
- Service providers may require IDs from AI systems to prevent abuse and maintain service quality through plugins and access restrictions
- Existing technologies like digital signatures and content provenance can provide verifiability for AI system IDs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Assigning unique identifiers to AI system instances enables precise incident investigation and liability attribution.
- Mechanism: Each AI system instance gets a unique ID that links to attributes like incident history, system card, and ancestor/descendant relationships. When an incident occurs, investigators can trace the ID chain to identify the responsible instance and deployer.
- Core assumption: Deployers can reliably detect instance formation and assign unique IDs that persist across reloads and branches.
- Evidence anchors:
  - [abstract] "An investigator may not know whom to investigate when a system causes an incident"
  - [section 6.1] "Since the two branches have no necessary causal impact on each other (unless they interact with each other or modify a shared object in the world), they should have two separate IDs, with links to the ancestor ID"
  - [corpus] Weak - related papers focus on intrusion detection rather than AI instance identification
- Break condition: If deployers cannot detect instance formation (e.g., user-run systems that combine outputs from multiple deployer instances), the ID chain breaks and accountability becomes unclear.

### Mechanism 2
- Claim: Service providers will require IDs from AI systems to prevent abuse and maintain service quality.
- Mechanism: Service providers implement plugins that require IDs for AI system access. Without IDs, AI systems face rate limiting or restricted access, while humans can interact freely through CAPTCHA bypass or direct access.
- Core assumption: Service providers have sufficient market power to enforce ID requirements and AI systems cannot easily bypass these restrictions.
- Evidence anchors:
  - [section 5.2] "Service providers could develop, or encourage the development of, plugins that require IDs"
  - [section 5.2] "A potential response could be for providers to restrict the services themselves, rather than just the plugins, in the absence of an ID"
  - [corpus] Weak - related papers focus on intrusion detection systems rather than service access control for AI
- Break condition: If users can easily run their own AI systems without deployer involvement, or if CAPTCHAs become ineffective against AI systems, the incentive structure collapses.

### Mechanism 3
- Claim: ID verifiability ensures parties can trust that an ID accurately represents its associated AI instance.
- Mechanism: Existing technologies like TLS/SSL prevent tampering, digital signatures prevent ID spoofing, and content provenance standards prevent instance spoofing. This creates a verifiable chain from ID creation to consumption.
- Core assumption: The combination of existing security technologies provides sufficient protection against all three threat models (tampering, ID spoofing, instance spoofing).
- Evidence anchors:
  - [section 3.3] "To combat ID spoofing, authors could digitally sign the ID (potentially with public key ownership verified by a trusted third party)"
  - [section 3.3] "For instance spoofing, there are existing standards for binding a digital object, such as an ID, to a piece of content, such as an AI system's output"
  - [section 3.3] "Such binding could ensure that IDs are associated to particular outputs from an instance in a verifiable way"
- Break condition: If any of the underlying security technologies (TLS, digital signatures, content provenance) are compromised, the entire verifiability chain fails.

## Foundational Learning

- Concept: Instance definition and causal independence
  - Why needed here: The paper's core innovation relies on treating each user interaction session as a causally independent unit that can be uniquely identified and tracked
  - Quick check question: Why does the paper treat a chat session with regenerated responses as separate instances rather than a single instance?

- Concept: Attribute specificity levels
  - Why needed here: Understanding how information can be attached to IDs at different granularities (instance, user, system level) is crucial for designing effective ID systems
  - Quick check question: What are the three levels of specificity mentioned for incident attributes, and why would you choose different levels for different use cases?

- Concept: Threat models for ID systems
  - Why needed here: The paper's approach to verifiability depends on understanding and defending against specific attack vectors
  - Quick check question: What are the three threat models identified for ID systems, and which existing technology addresses each one?

## Architecture Onboarding

- Component map: ID Generator -> Attribute Store -> Verifiability Layer -> Access Control -> Incident Investigation Interface

- Critical path:
  1. User creates new interaction session
  2. Deployer generates unique ID and stores initial attributes
  3. System sends ID with each output (watermarked/metadata)
  4. Service provider receives request with ID and validates it
  5. Incident occurs, investigator traces ID chain to identify responsible parties

- Design tradeoffs:
  - Granularity vs. privacy: More detailed attributes enable better investigation but increase privacy risks
  - Persistence vs. storage: Longer ID retention enables better incident analysis but requires more storage
  - Verification vs. usability: Stronger verification methods increase security but may add friction to legitimate use

- Failure signatures:
  - ID spoofing detected: Multiple instances claiming the same identifier
  - Tampering detected: ID contents don't match expected format or signature verification fails
  - Instance spoofing detected: Output doesn't match the claimed ID's expected behavior patterns

- First 3 experiments:
  1. Deploy ID system for a single high-stakes service (e.g., financial transactions) with basic attribute storage
  2. Implement verifiability layer using digital signatures and test against all three threat models
  3. Conduct user study to measure privacy concerns and willingness to interact with ID-requiring systems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can IDs be implemented in decentralized settings where users run AI systems themselves without a deployer?
- Basis in paper: [explicit] The paper discusses the challenge of implementing IDs in decentralized settings and mentions the need for straightforward ID implementation methods.
- Why unresolved: The paper does not provide a concrete solution for implementing IDs in decentralized settings.
- What evidence would resolve it: Development and testing of a system that allows users to request and attach IDs to their self-run AI systems, similar to how Let's Encrypt automates HTTPS certificate creation.

### Open Question 2
- Question: What information is appropriate to attach to IDs in different contexts, and how can privacy concerns be addressed?
- Basis in paper: [explicit] The paper discusses privacy risks associated with IDs, including the potential for user identification through instance information and the need for privacy-preserving ways to perform audits.
- Why unresolved: The paper does not provide specific guidelines on what information should be included in IDs or how to address privacy concerns.
- What evidence would resolve it: Development of a framework for determining appropriate ID attributes in different contexts, along with privacy-preserving techniques for implementing IDs.

### Open Question 3
- Question: How can the broader societal consequences of introducing IDs for AI systems be studied and mitigated?
- Basis in paper: [explicit] The paper discusses potential negative consequences of IDs, such as enabling user ranking, influence over instances, and creating a separate digital channel for trusted AI systems.
- Why unresolved: The paper does not provide a methodology for studying these consequences or strategies for mitigating them.
- What evidence would resolve it: Empirical studies on the societal impacts of IDs, including user behavior, market dynamics, and power imbalances, along with proposed interventions to address negative consequences.

## Limitations

- Deployment Ecosystem Complexity: The framework assumes deployers can reliably detect and track AI system instances, but this becomes problematic for decentralized or user-run systems where multiple deployer instances might combine outputs.
- Privacy-Utility Tradeoffs: The paper does not adequately address the tension between providing sufficient information for incident investigation and protecting user privacy.
- Incentive Alignment: The assumption that market forces will naturally drive adoption of AI ID systems is speculative and not well-supported by current market dynamics.

## Confidence

**High Confidence**: The core problem identification - that current AI systems lack sufficient identification for proper interaction, investigation, and liability allocation - is well-supported by real-world incidents and stakeholder needs.

**Medium Confidence**: The technical mechanisms for ID generation, verifiability, and attribute storage are feasible based on existing technologies (TLS, digital signatures, content provenance), though implementation details remain unspecified.

**Low Confidence**: The assumption that market forces will naturally drive adoption of AI ID systems, and that service providers will have sufficient leverage to enforce requirements, is speculative and not well-supported by current market dynamics.

## Next Checks

1. **Implement prototype with real-world service provider**: Partner with a high-stakes service (e.g., financial institution) to implement basic ID system and measure adoption friction, verification success rates, and user acceptance.

2. **Conduct privacy impact assessment**: Design and run a study measuring how different levels of attribute disclosure affect both investigation effectiveness and user privacy concerns, identifying optimal disclosure policies.

3. **Test ID system resilience**: Simulate various attack scenarios (ID spoofing, tampering, instance spoofing) using the proposed verification mechanisms to identify weaknesses and required security improvements.