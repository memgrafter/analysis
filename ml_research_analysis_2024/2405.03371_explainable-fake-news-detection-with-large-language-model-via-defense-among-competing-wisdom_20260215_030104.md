---
ver: rpa2
title: Explainable Fake News Detection With Large Language Model via Defense Among
  Competing Wisdom
arxiv_id: '2405.03371'
source_url: https://arxiv.org/abs/2405.03371
tags:
- claim
- veracity
- news
- 'false'
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses explainable fake news detection by proposing
  a defense-based framework that leverages competing wisdom from raw reports. The
  core method idea involves extracting evidence from two competing parties in the
  wisdom of crowds, generating justifications via large language models (LLMs), and
  determining veracity through a defense-based inference module.
---

# Explainable Fake News Detection With Large Language Model via Defense Among Competing Wisdom

## Quick Facts
- arXiv ID: 2405.03371
- Source URL: https://arxiv.org/abs/2405.03371
- Authors: Bo Wang; Jing Ma; Hongzhan Lin; Zhiwei Yang; Ruichao Yang; Yuan Tian; Yi Chang
- Reference count: 40
- Primary result: L-Defense achieves new state-of-the-art results, with improvements of at least 4% in macro-averaged F1 score compared to the best baseline on the RAWFC dataset

## Executive Summary
This paper proposes a defense-based framework for explainable fake news detection that leverages competing wisdom from raw reports. The method extracts evidence from two competing parties in the wisdom of crowds, generates justifications via large language models (LLMs), and determines veracity through a defense-based inference module. By capturing the divergence between competing narratives rather than majority bias, the framework achieves superior performance in both fake news detection and explanation quality compared to state-of-the-art baselines.

## Method Summary
The proposed framework consists of three main components: evidence extraction, prompt-based reasoning, and defense-based inference. The evidence extraction module splits raw reports into two competing parties by assigning veracity scores to evidence sentences and ranking them separately. The prompt-based reasoning module uses LLMs to generate justifications for both false and true veracities based on the respective evidence sets. Finally, the defense-based inference module compares these competing justifications through a transformer encoder to determine the final veracity verdict. The framework effectively mitigates majority bias by capturing quality divergence rather than quantity bias.

## Key Results
- L-Defense achieves new state-of-the-art results with at least 4% improvement in macro-averaged F1 score on RAWFC dataset
- The method generates high-quality explanations, performing superior or comparable to human experts across multiple evaluation metrics
- Extensive experiments on two real-world benchmarks (RAWFC and LIAR-RAW) demonstrate consistent superiority over state-of-the-art baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting wisdom of crowds into competing parties captures quality divergence rather than quantity bias
- Mechanism: Evidence extraction assigns separate veracity scores to each evidence sentence, ranking them to form competing sets
- Core assumption: True veracity reports exhibit higher informativeness and soundness than inaccurate ones
- Evidence anchors:
  - [abstract]: "we first propose an evidence extraction module to split the wisdom of crowds into two competing parties and respectively detect salient evidences."
  - [section 3.1.2]: "we assume that the reports indicating truthfulness could exhibit higher quality of informativeness and soundness compared to those conveying inaccurate information."
- Break condition: If competing parties have similar quality evidence, defense mechanism cannot reliably determine veracity

### Mechanism 2
- Claim: LLM-generated justifications from competing evidence sets provide sufficient signal for veracity determination
- Mechanism: Prompt-based reasoning module uses LLM to generate streamlined explanations for both veracities from respective evidence sets
- Core assumption: LLM can effectively reason from evidence sets to generate explanations reflecting quality differences
- Evidence anchors:
  - [abstract]: "To gain concise insights from evidences, we then design a prompt-based module that utilizes a large language model to generate justifications by inferring reasons towards two possible veracities."
  - [section 3.2]: "Based on the respective evidences, we then design a prompt-based module to generate justifications by inferring reasons towards two possible veracities."
- Break condition: If LLM generates similar-quality explanations from both evidence sets, defense mechanism fails

### Mechanism 3
- Claim: Defense-based inference module effectively determines veracity by comparing competing justifications
- Mechanism: Transformer encoder compares two LLM-generated justifications to determine winning party
- Core assumption: Comparison reveals clear quality differences indicating correct veracity
- Evidence anchors:
  - [abstract]: "Finally, we propose a defense-based inference module to determine veracity via modeling the defense among these justifications."
  - [section 3.3]: "we develop a defense-based fake news detector. This detector aims to discern the relative strength of the two explanations from their defense, ultimately providing the veracity verdict."
- Break condition: If defense-based inference cannot distinguish between competing justifications, system cannot determine veracity

## Foundational Learning

- Concept: Evidence extraction and ranking based on multiple veracity scores
  - Why needed here: To create competing evidence sets that capture different perspectives on claim's veracity
  - Quick check question: How does assigning both false and true scores to each evidence sentence help create competing parties?

- Concept: Prompt engineering for LLM reasoning
  - Why needed here: To generate explanations that reflect quality differences between competing evidence sets
  - Quick check question: What role does the prior veracity label play in prompting LLM to generate competing explanations?

- Concept: Defense-based comparison through transformer encoding
  - Why needed here: To effectively compare competing explanations and determine which party indicates correct veracity
  - Quick check question: How does transformer encoder capture semantic relationships between competing explanations?

## Architecture Onboarding

- Component map: Claim + Raw reports → Evidence Extraction → LLM Reasoning → Defense-based Inference → Output
- Critical path: Claim → Evidence Extraction → LLM Reasoning → Defense-based Inference → Output
- Design tradeoffs:
  - Evidence extraction granularity (sentence-level vs. report-level)
  - LLM size vs. reasoning quality
  - Number of top evidences extracted (k value)
  - Trade-off between false and true evidence extraction during training (γ parameter)
- Failure signatures:
  - Similar quality of competing justifications → Classifier uncertainty
  - Poor evidence extraction → Misleading justifications
  - LLM failure to reason → Low-quality explanations
  - Training instability → Poor veracity prediction
- First 3 experiments:
  1. Test evidence extraction module alone on small dataset to verify it can create meaningfully different competing evidence sets
  2. Test LLM reasoning module with fixed evidence sets to verify it can generate quality-differentiated explanations
  3. Test defense-based inference module with pre-generated competing explanations to verify it can determine correct veracity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the defense-based framework compare to other methods in handling claims without competing evidence?
- Basis in paper: [explicit] The authors discuss that their model can be generalized to claims without competing evidence, but do not provide specific experimental results or comparisons.
- Why unresolved: The paper mentions the model's ability to handle claims without competing evidence but does not provide empirical evidence or comparisons with other methods in this scenario.
- What evidence would resolve it: Conducting experiments comparing the performance of the defense-based framework with other methods on claims without competing evidence would provide empirical evidence to resolve this question.

### Open Question 2
- Question: How does the choice of LLM (ChatGPT vs. LLaMA2) impact the quality of generated justifications and final predictions?
- Basis in paper: [explicit] The authors mention that the ChatGPT variant of their model achieves slightly superior results on RAWFC while maintaining competitiveness on LIAR-RAW, but do not provide a detailed analysis of the impact of LLM choice.
- Why unresolved: While the authors mention differences in performance between the ChatGPT and LLaMA2 variants, they do not provide a detailed analysis of how the choice of LLM impacts the quality of generated justifications and final predictions.
- What evidence would resolve it: Conducting a detailed analysis of the justifications generated by each LLM variant and comparing their impact on final predictions would provide evidence to resolve this question.

### Open Question 3
- Question: How does the model handle claims with more than two competing parties or multiple facets?
- Basis in paper: [inferred] The paper focuses on claims with two competing parties, but does not discuss how the model would handle claims with more than two parties or multiple facets.
- Why unresolved: The paper's focus on claims with two competing parties leaves open the question of how the model would handle claims with more complex competing narratives or multiple facets.
- What evidence would resolve it: Experimenting with claims that have more than two competing parties or multiple facets and analyzing the model's performance would provide evidence to resolve this question.

## Limitations
- The framework relies heavily on the assumption that splitting wisdom of crowds will reveal quality divergence, with limited corpus support for this approach
- LLM justification generation lacks specific prompt templates and detailed analysis of prompt effectiveness
- Defense-based inference module evaluation shows aggregate metrics without analyzing individual prediction uncertainty or failure cases

## Confidence
- Evidence extraction mechanism: Medium - limited corpus support for split-into-two-parties approach
- LLM justification generation: Low - lacks specific prompt details and effectiveness analysis
- Defense-based inference: Medium - aggregate performance metrics without individual prediction analysis

## Next Checks
1. Conduct controlled experiments manipulating evidence quality to test defense mechanism's ability to distinguish competing parties with subtle or absent quality differences
2. Systematically vary LLM prompts to quantify sensitivity of final detection performance to prompt design choices and identify optimal structures
3. Create detailed catalog of defense-based inference module failure cases, including examples where competing justifications have similar quality scores, to understand method limitations and failure patterns