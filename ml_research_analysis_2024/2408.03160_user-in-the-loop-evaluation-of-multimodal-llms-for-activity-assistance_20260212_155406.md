---
ver: rpa2
title: User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance
arxiv_id: '2408.03160'
source_url: https://arxiv.org/abs/2408.03160
tags:
- visual
- actions
- video
- history
- activity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates multimodal large language models (LLMs) for\
  \ vision-powered activity assistance. It compares two approaches\u2014Socratic models\
  \ (text-based visual history) and Vision-Conditioned Language Models (VCLMs, implicit\
  \ visual embeddings)\u2014on offline video anticipation tasks and a first-of-its-kind\
  \ online user study."
---

# User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance

## Quick Facts
- arXiv ID: 2408.03160
- Source URL: https://arxiv.org/abs/2408.03160
- Reference count: 40
- One-line primary result: Socratic models (text-based visual history) outperform VCLMs (implicit visual embeddings) for long-horizon activity assistance in both offline and online evaluations.

## Executive Summary
This study evaluates multimodal large language models (LLMs) for vision-powered activity assistance by comparing two approaches: Socratic models using text-based visual history and Vision-Conditioned Language Models (VCLMs) using implicit visual embeddings. The evaluation includes offline video anticipation tasks on cooking datasets and a first-of-its-kind online user study with 18 participants performing cooking activities. Results show Socratic models achieve higher task completion success rates and mean IoU than VCLMs, while offline metrics like mIoU fail to predict online performance. Grounding errors, particularly recognizing completed steps, dominate failures. The findings highlight the superiority of text-based visual history representation for long-horizon activity assistance and emphasize the need for real-world evaluation of multimodal LLMs.

## Method Summary
The study evaluates two multimodal LLM approaches for activity assistance: Socratic models using text-based visual history and VCLMs using implicit visual embeddings. Offline benchmarking uses Ego4D and CrossTask datasets with metrics like edit distance and mean IoU. Online evaluation involves 18 participants performing cooking activities while wearing an Aria device, receiving assistance from multimodal LLMs. The critical path includes video narration (LaViLa), multimodal LLMs (Llama2), and prompt templates. Socratic models encode long visual histories as text, while VCLMs use continuous visual embeddings. Task completion success rate and mean IoU measure online performance.

## Key Results
- Socratic models outperform VCLMs in offline benchmarks for long visual history tasks
- Socratic models achieve higher task completion success rates and mean IoU in online user study
- Offline metrics like mIoU fail to predict online performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-based visual history representation outperforms implicit visual embeddings for long visual history tasks.
- Mechanism: Socratic models encode long visual histories as text using video narration models, while VCLMs use continuous visual embeddings. For tasks requiring grounding of long visual histories, the explicit text representation captures coarse-level information across the entire history more effectively than the implicit embeddings from a limited set of frames.
- Core assumption: Long visual histories contain crucial coarse-level information that is difficult to capture implicitly in a limited number of visual embeddings.
- Evidence anchors:
  - [abstract] "Offline results show Socratic models outperform VCLMs in long visual history tasks"
  - [section] "Socratic approach outperforms VCLMs in tasks requiring the grounding of a long visual history"
  - [corpus] Weak evidence - no directly relevant papers found

### Mechanism 2
- Claim: Implicit visual representation benefits smaller LLMs for short to medium visual histories.
- Mechanism: VCLMs use continuous visual embeddings alongside text tokens. For short to medium visual histories, these embeddings capture fine-grained visual details that aid smaller LLMs with limited reasoning capabilities. However, larger LLMs can plan effectively with coarse-level text-based grounding alone.
- Core assumption: Smaller LLMs have limited reasoning and planning capabilities compared to larger LLMs.
- Evidence anchors:
  - [abstract] "VCLMs excel only with short histories and smaller LLMs"
  - [section] "VCLMs show competitive performance in the VPA task requiring grounding of medium visual history"
  - [corpus] Weak evidence - no directly relevant papers found

### Mechanism 3
- Claim: Offline metrics like mIoU fail to predict online performance in user-in-the-loop settings.
- Mechanism: Offline metrics evaluate action prediction accuracy against a single ground truth sequence. Online settings allow for multiple successful action sequences and require real-time replanning based on user execution. Offline metrics may be inflated because they don't account for grounding errors, planning errors, or failure to detect activity completion that are critical in online settings.
- Core assumption: Online settings have different success criteria and error modes compared to offline settings.
- Evidence anchors:
  - [abstract] "Offline metrics like mIoU fail to predict online performance"
  - [section] "We find that offline metrics such as mean Intersection over Union (mIoU) are an inflated measure of online performance"
  - [corpus] Weak evidence - no directly relevant papers found

## Foundational Learning

- Concept: Video-based action anticipation and planning
  - Why needed here: The study evaluates multimodal LLMs for activity assistance using video-based benchmarks. Understanding these concepts is crucial for designing and evaluating the models.
  - Quick check question: What is the difference between action anticipation and action planning in video-based tasks?

- Concept: Socratic models vs Vision-Conditioned Language Models (VCLMs)
  - Why needed here: The study compares these two approaches for representing visual history in multimodal LLMs. Understanding their differences is essential for interpreting the results.
  - Quick check question: How do Socratic models and VCLMs differ in their approach to grounding visual information for LLMs?

- Concept: User-in-the-loop evaluation
  - Why needed here: The study conducts an online evaluation with real users performing activities while receiving assistance from multimodal LLMs. Understanding this evaluation approach is crucial for interpreting the real-world performance of the models.
  - Quick check question: What are the key differences between offline evaluation using benchmark datasets and online evaluation with real users?

## Architecture Onboarding

- Component map: Input video → Video narration model (LaViLa) → Text-based visual history (Socratic) OR Continuous visual embeddings (VCLM) → Multimodal LLM (Llama2) → Predicted actions
- Critical path: For offline evaluation: input video → video narration model → text-based visual history → multimodal LLM → predicted actions. For online evaluation: includes additional steps for real-time video streaming, segmentation, summarization, and user interaction.
- Design tradeoffs: Using text-based visual history (Socratic) provides explicit coarse-level information but may miss fine-grained details. Using implicit visual embeddings (VCLM) captures fine-grained details but is limited by the number of frames and may miss crucial information in long histories.
- Failure signatures: Grounding errors (failing to recognize completed steps), planning errors (suggesting incorrect orderings or irrelevant actions), and failure to detect activity completion are the main failure modes observed in the study.
- First 3 experiments:
  1. Replicate the offline benchmarking experiments on LTA and VPA tasks to verify the performance differences between Socratic and VCLM approaches.
  2. Conduct a small-scale online evaluation with a few participants to validate the real-world performance and identify potential issues in the user interaction flow.
  3. Perform ablation studies to understand the impact of different visual history representation methods (e.g., narrations only vs. narrations + objects/actions) and the role of goal conditioning in the prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do VCLMs perform on long visual history tasks when using a larger number of frames (e.g., 32-64 frames) compared to the current 8-16 frame limit?
- Basis in paper: [inferred] The paper mentions that current VCLMs uniformly sample 8-16 frames from input videos, which may be ineffective for tasks requiring grounding on average >500 frames. The authors suggest that encoding long videos and aligning long videos with text tokens as needed by VCLMs would be a rich avenue for future work.
- Why unresolved: The paper does not test VCLMs with a larger number of frames, focusing instead on comparing Socratic models and VCLMs with their current architectures.
- What evidence would resolve it: An experiment comparing VCLM performance on long visual history tasks using 8-16 frames versus 32-64 frames would provide evidence for whether increasing the number of frames improves performance.

### Open Question 2
- Question: What is the impact of using different video narration models on the performance of Socratic models in online settings?
- Basis in paper: [explicit] The paper compares two video narration models, LaViLa and Blip-2, in offline settings and finds that LaViLa significantly outperforms Blip-2. However, the paper does not test different narration models in online settings.
- Why unresolved: The paper only uses LaViLa in online experiments, so the impact of using other narration models is unknown.
- What evidence would resolve it: An experiment comparing Socratic model performance in online settings using different video narration models (e.g., LaViLa vs. Blip-2) would provide evidence for the impact of narration model choice.

### Open Question 3
- Question: How do Socratic models and VCLMs perform on other types of multi-step activities beyond cooking, such as assembling furniture or performing repairs?
- Basis in paper: [explicit] The paper evaluates the models on three cooking activities (espresso latte, caprese salad, and BLT sandwich) and suggests that the findings may generalize to other multi-step activities.
- Why unresolved: The paper only tests the models on cooking activities, so their performance on other types of activities is unknown.
- What evidence would resolve it: An experiment evaluating Socratic models and VCLMs on other types of multi-step activities, such as assembling furniture or performing repairs, would provide evidence for their generalizability.

## Limitations

- Small sample size in online evaluation (18 participants) limits statistical power and generalizability
- Dataset and task specificity (cooking datasets and tasks) may not directly translate to other domains
- Limited exploration of VCLM variants and hybrid approaches that could potentially perform better

## Confidence

- High confidence: The core finding that text-based visual history representation (Socratic models) outperforms implicit visual embeddings (VCLMs) for long visual history tasks in offline benchmarks
- Medium confidence: The superiority of Socratic models in the online user study due to small sample size and potential user variability
- Medium confidence: The claim that offline metrics like mIoU fail to predict online performance based on observed discrepancies

## Next Checks

1. Scale up the online user study with a more diverse participant pool and wider range of cooking activities to validate robustness and real-world applicability
2. Investigate VCLM variants and hybrid approaches (e.g., more frames, combined text+visual representations) to compare with standard Socratic approach
3. Analyze the impact of goal conditioning and summarization through ablation studies to identify potential improvements in both Socratic and VCLM models