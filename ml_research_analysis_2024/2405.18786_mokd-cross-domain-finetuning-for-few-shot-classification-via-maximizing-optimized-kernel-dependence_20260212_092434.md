---
ver: rpa2
title: 'MOKD: Cross-domain Finetuning for Few-shot Classification via Maximizing Optimized
  Kernel Dependence'
arxiv_id: '2405.18786'
source_url: https://arxiv.org/abs/2405.18786
tags:
- mokd
- hsic
- kernel
- dependence
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of cross-domain few-shot classification,
  where the goal is to learn a classifier that can generalize well to unseen domains
  with only a few labeled examples. The authors propose a novel method called MOKD
  (Maximizing Optimized Kernel Dependence) that learns class-specific representations
  by maximizing the dependence between representations and labels, while minimizing
  the dependence among all samples.
---

# MOKD: Cross-domain Finetuning for Few-shot Classification via Maximizing Optimized Kernel Dependence

## Quick Facts
- arXiv ID: 2405.18786
- Source URL: https://arxiv.org/abs/2405.18786
- Reference count: 40
- Primary result: Achieves 67.3% average accuracy across Meta-Dataset benchmarks, outperforming state-of-the-art methods in most cases

## Executive Summary
This paper addresses cross-domain few-shot classification by proposing MOKD (Maximizing Optimized Kernel Dependence), a method that learns class-specific representations through kernelized dependence optimization. The approach uses Hilbert-Schmidt Independence Criterion (HSIC) with test power maximization to optimize kernel parameters, then maximizes dependence between representations and labels while minimizing dependence among all samples. The method demonstrates strong empirical performance on the Meta-Dataset benchmark, achieving 67.3% average accuracy across 13 datasets.

## Method Summary
MOKD is a cross-domain few-shot classification method that uses pre-trained backbone features and optimizes a linear transformation head using kernelized dependence measures. The method first selects optimal kernel bandwidths via grid search to maximize test power, then minimizes an objective function combining negative HSIC between representations and labels with positive HSIC among all samples. This framework encourages the model to learn class-specific representations while regularizing against overfitting through dependence minimization among all samples.

## Key Results
- Achieves 67.3% average accuracy across Meta-Dataset benchmarks, outperforming existing methods in most cases
- Demonstrates better data representation clusters compared to traditional NCC-based loss methods
- Shows improved generalization on unseen domains, particularly on complex datasets like Fungi
- MOKD with test power maximization (TPM) performs better than without TPM on unseen domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MOKD achieves better generalization on unseen domains by learning class-specific representations that maximize within-class similarity and minimize between-class similarity.
- Mechanism: The method optimizes kernelized representations using Hilbert-Schmidt Independence Criterion (HSIC) with test power maximization. By maximizing HSIC between representations and labels, it explicitly learns cluster structures. Minimizing HSIC among all representations acts as regularization to reduce high-variance features.
- Core assumption: Kernel HSIC with optimized bandwidth can accurately measure dependence between representations and labels in high-dimensional feature spaces.
- Evidence anchors:
  - [abstract]: "MOKD first optimizes the kernel adopted in Hilbert-Schmidt independence criterion (HSIC) to obtain the optimized kernel HSIC (opt-HSIC) that can capture the dependence more precisely."
  - [section 4]: "Specifically, MOKD first maximizes the test power of the kernels used in HSIC to improve its capability in dependence detection, and then optimizes the dependence respectively between representations and labels, and among representations with the optimized kernel HSIC measures."
  - [corpus]: Weak - no direct citations of HSIC-based methods in related papers.

### Mechanism 2
- Claim: Test power maximization of kernel HSIC increases sensitivity to dependence, leading to more accurate cluster structure discovery.
- Mechanism: The method selects kernel bandwidth parameters by maximizing the ratio of HSIC to its standard variance (test power). This optimization makes the dependence measure more sensitive to actual relationships between samples.
- Core assumption: Test power maximization via grid search over bandwidth parameters effectively increases kernel HSIC sensitivity without requiring computationally expensive gradient-based optimization.
- Evidence anchors:
  - [section 4.3]: "The maximization of test power can be performed by any optimizer, such as gradient-based optimizers. However, in practice, we perform test power maximization via selecting the optimal bandwidth from a list of candidates in the way of grid search."
  - [section 5.2]: "According to the results, we find that MOKD with test power maximization performs better. Besides, MOKD with TPM performs better on unseen domains and complicated datasets like Fungi."
  - [corpus]: Weak - no direct citations of test power maximization in HSIC-based methods.

### Mechanism 3
- Claim: Minimizing HSIC among all representations acts as effective regularization to prevent overfitting in few-shot scenarios.
- Mechanism: The objective includes a term that minimizes dependence among all sample representations, which penalizes high-variance features and common information shared across samples. This regularizes the model when labeled data is scarce.
- Core assumption: The HSIC(Z,Z) term effectively captures dependence among all representations and can be scaled appropriately to balance regularization strength.
- Evidence anchors:
  - [section 3.3]: "The HSIC(Z, Z) term measures the dependence among all data samples in the given task. In this case, we can control the dependence between sample representations via scaling HSIC(Z, Z) term."
  - [section 5.2]: "According to Fig. 3(c), when HSIC(Z, Z) is removed, the performance of MOKD drops significantly. According to Fig. 3(d) and 6, it is easy to find that the reason for the performance drop is overfitting."
  - [corpus]: Weak - no direct citations of HSIC-based regularization in few-shot learning.

## Foundational Learning

- Concept: Hilbert-Schmidt Independence Criterion (HSIC)
  - Why needed here: HSIC provides a kernel-based measure of statistical dependence between representations and labels, enabling optimization of cluster structure discovery.
  - Quick check question: What is the mathematical form of HSIC between two random variables X and Y using kernel functions k and l?

- Concept: Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: RKHS provides the theoretical foundation for kernel methods and HSIC computation, allowing nonlinear dependence measurement.
  - Quick check question: What property of RKHS ensures that HSIC equals zero if and only if the variables are mutually independent?

- Concept: Test Power Maximization
  - Why needed here: Test power maximization optimizes kernel bandwidth to increase sensitivity to actual dependence relationships, improving cluster structure discovery.
  - Quick check question: How is test power mathematically defined in terms of HSIC and its variance?

## Architecture Onboarding

- Component map: Pre-trained backbone (ResNet-18) -> Linear transformation head -> HSIC computation module -> Bandwidth selection module -> Optimization module
- Critical path: 1) Sample task and extract representations using pre-trained backbone 2) Select optimal bandwidth via grid search to maximize test power 3) Compute HSIC measures with optimized kernels 4) Optimize linear head parameters using bi-level objective 5) Evaluate classification performance
- Design tradeoffs:
  - Grid search vs gradient-based bandwidth optimization: Grid search is simpler and more stable but may miss optimal parameters
  - HSIC computation cost: O(m²) complexity may limit scalability to large datasets
  - Regularization strength (γ): Must be tuned per dataset to balance feature discrimination and overfitting prevention
- Failure signatures:
  - Poor performance on unseen domains: Indicates inadequate dependence measurement or insufficient regularization
  - Overfitting on simple datasets: Suggests γ is too high or bandwidth selection is suboptimal
  - Slow convergence: May indicate inappropriate learning rate or initialization issues
- First 3 experiments:
  1. Compare MOKD with baseline URL on a simple dataset (Omniglot) to verify basic functionality
  2. Test bandwidth selection with different grid ranges to find optimal configuration
  3. Evaluate effect of γ parameter by running with γ=0, γ=1, γ=3 on multiple datasets to observe regularization impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limits of kernel HSIC measures for cross-domain few-shot classification, and how do these limits vary across different dataset characteristics (e.g., domain shift magnitude, number of classes, image complexity)?
- Basis in paper: [inferred] The paper demonstrates that MOKD improves performance by optimizing kernel HSIC, but does not analyze the theoretical limitations of HSIC for CFC tasks or how these limitations depend on dataset properties.
- Why unresolved: The paper focuses on empirical evaluation rather than theoretical analysis of HSIC's limitations in the CFC context.
- What evidence would resolve it: Theoretical analysis proving bounds on HSIC's effectiveness for CFC, and empirical studies showing HSIC performance across datasets with varying domain shifts and complexities.

### Open Question 2
- Question: How does the choice of kernel function (e.g., Gaussian, IMQ) affect the performance of MOKD across different datasets and task settings, and are there dataset-specific kernel selection strategies that could further improve results?
- Basis in paper: [explicit] The paper compares Gaussian and IMQ kernels but only provides limited results, suggesting that kernel choice may impact performance.
- Why unresolved: The paper only briefly explores kernel type effects and does not investigate optimal kernel selection strategies for different dataset characteristics.
- What evidence would resolve it: Comprehensive experiments comparing various kernel functions across all Meta-Dataset datasets, and development of adaptive kernel selection methods based on dataset properties.

### Open Question 3
- Question: Can the MOKD framework be extended to handle more complex few-shot learning scenarios, such as few-shot domain adaptation or continual few-shot learning, and what modifications would be necessary?
- Basis in paper: [inferred] The paper focuses on static few-shot classification tasks, but the framework could potentially be adapted to more dynamic learning scenarios.
- Why unresolved: The paper does not explore extensions of MOKD beyond the basic CFC setting.
- What evidence would resolve it: Implementation and evaluation of MOKD variants for few-shot domain adaptation and continual learning, demonstrating improvements over existing methods in these settings.

## Limitations

- Scalability concerns: The O(m²) complexity of HSIC computation and grid search over bandwidth parameters may limit applicability to large-scale problems
- Limited theoretical analysis: The paper demonstrates empirical success but lacks theoretical justification for why the method works or bounds on its effectiveness
- Kernel sensitivity: Performance may depend heavily on appropriate kernel selection and bandwidth optimization, which are not fully explored

## Confidence

- HSIC-based dependence optimization claims: Medium confidence - sound theoretical framework but limited ablation studies on kernel optimization
- Scalability limitations: Low confidence - mentioned but not rigorously analyzed or benchmarked
- Generalization to unseen domains: Medium confidence - supported by Meta-Dataset results but not tested on extreme domain shifts

## Next Checks

1. **Ablation Study on Kernel Optimization**: Conduct experiments varying grid search ranges and step sizes to determine the sensitivity of MOKD performance to bandwidth selection. Compare grid search with gradient-based optimization to validate the computational efficiency claims.

2. **Scalability Analysis**: Measure actual runtime and memory usage of MOKD compared to URL on tasks of varying sizes (different shot numbers and ways). Profile the HSIC computation bottleneck and test with larger backbone architectures.

3. **Robustness to Domain Shift**: Create synthetic domain shift scenarios where the relationship between pre-training and target domains is systematically varied. Test MOKD's performance degradation as domain similarity decreases to quantify its generalization limits.