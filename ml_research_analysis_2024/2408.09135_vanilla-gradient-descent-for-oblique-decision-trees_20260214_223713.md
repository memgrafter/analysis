---
ver: rpa2
title: Vanilla Gradient Descent for Oblique Decision Trees
arxiv_id: '2408.09135'
source_url: https://arxiv.org/abs/2408.09135
tags:
- dtsemnet
- learning
- linear
- which
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of training oblique decision trees
  (DTs) by proposing a novel semantically equivalent and invertible encoding of DTs
  as neural networks, called DTSemNet. The core method idea is to use a neural network
  architecture with ReLU activation functions that is semantically equivalent to a
  DT, allowing standard gradient descent to be applied without approximations.
---

# Vanilla Gradient Descent for Oblique Decision Trees

## Quick Facts
- arXiv ID: 2408.09135
- Source URL: https://arxiv.org/abs/2408.09135
- Authors: Subrat Prasad Panda; Blaise Genest; Arvind Easwaran; Ponnuthurai Nagaratnam Suganthan
- Reference count: 40
- Primary result: DTSemNet achieves 5.5% better classification accuracy and 10% better regression performance than state-of-the-art oblique decision tree methods while reducing training time

## Executive Summary
This paper introduces DTSemNet, a novel neural network architecture that is semantically equivalent to hard oblique decision trees. By using a carefully designed network with ReLU activations and fixed weight connections, the authors enable standard gradient descent to train oblique decision trees without approximations like straight-through estimators. The method achieves superior accuracy on both classification and regression tasks while significantly reducing training time compared to existing oblique decision tree algorithms.

## Method Summary
The core innovation is encoding an oblique decision tree as a neural network where each internal node corresponds to a hidden layer node and each leaf to an output node. The architecture uses linear layers followed by ReLU activations to simulate binary decision paths, with fixed weights preserving the tree's branching structure. For classification, the network directly outputs class predictions through max pooling, while for regression, a single straight-through estimator approximation combines the tree decision with leaf-level linear regressors. Standard gradient descent optimizes all trainable weights (decision boundaries and leaf regressors) without requiring complex tree induction heuristics.

## Key Results
- DTSemNet outperforms state-of-the-art oblique decision tree methods by 5.5% on average for classification tasks
- DTSemNet achieves 10% better performance on regression tasks compared to competing methods
- DTSemNet can learn decision tree policies as efficiently as neural network policies in reinforcement learning setups with physical inputs

## Why This Works (Mechanism)

### Mechanism 1
The DTSemNet architecture is semantically equivalent to a hard oblique decision tree, allowing exact reconstruction without approximation. By using a carefully designed neural network structure with ReLU activations and fixed weight connections, the network can simulate the binary decision paths of a decision tree. Each internal node in the tree maps to a hidden layer node, and each leaf maps to an output node, ensuring that for any input, the same leaf is selected by both the network and the original tree.

### Mechanism 2
Standard gradient descent can be applied to learn the decision boundaries in the tree without needing approximations like straight-through estimators. Since the DTSemNet architecture uses only linear and ReLU layers, the gradients flow naturally through the network during backpropagation. The trainable weights correspond directly to the decision boundaries in the tree, so optimizing these weights optimizes the tree's structure.

### Mechanism 3
For regression tasks, combining the tree decision with a linear regressor at the leaves via a single STE call is more efficient and accurate than using STE at every node. In DTSemNet-regression, the network selects the correct leaf using the same semantic equivalence as in classification, then applies a linear regression at that leaf. Only the final argmax operation requires an STE approximation, reducing the number of STE calls from n (tree depth) to 1.

## Foundational Learning

- Concept: Oblique decision trees use linear combinations of features for splits rather than single-feature thresholds.
  - Why needed here: Understanding why oblique trees are more expressive than axis-aligned trees is key to appreciating the advantage of DTSemNet over traditional tree learners.
  - Quick check question: Why might an oblique split be more effective than an axis-aligned split in a rotated feature space?

- Concept: Neural network backpropagation and gradient descent.
  - Why needed here: DTSemNet relies on standard gradient descent to learn the tree parameters, so understanding how gradients flow through linear and ReLU layers is essential.
  - Quick check question: What happens to the gradient when a ReLU unit is not active (input ≤ 0)?

- Concept: Straight-through estimators (STE) for non-differentiable operations.
  - Why needed here: DTSemNet-regression uses a single STE call at the output layer, so understanding when and why STEs are used is important for grasping the architecture's efficiency.
  - Quick check question: In what scenario would you need to use an STE instead of standard backpropagation?

## Architecture Onboarding

- Component map: Input features + bias unit → Hidden1 (linear) → Hidden2 (ReLU) → Output (linear) → (optional) Final layer (max pooling) → Decision
- Critical path: Input → Hidden1 (linear) → Hidden2 (ReLU) → Output (linear) → (optional) Final layer (max pooling) → Decision
- Design tradeoffs:
  - Using ReLU instead of sign activation allows vanilla gradient descent but requires more nodes to simulate the binary logic.
  - Fixed weights in Hidden2 preserve the tree structure but reduce flexibility.
  - Single STE call in regression reduces approximation error but adds non-differentiability at the output.
- Failure signatures:
  - Vanishing gradients in deep trees despite ReLU activations.
  - Poor accuracy if the linear regressors at leaves are insufficient for the task.
  - Instability if input points lie exactly on decision boundaries (Aix + bi = 0).
- First 3 experiments:
  1. Implement DTSemNet-classification on a small synthetic dataset (e.g., 2D blobs) and verify that the predicted class matches the tree's leaf selection.
  2. Train DTSemNet-regression on a simple piecewise linear dataset and compare RMSE with a standard decision tree regressor.
  3. Replace a neural network policy with DTSemNet in a simple RL environment (e.g., CartPole) and measure performance difference.

## Open Questions the Paper Calls Out

### Open Question 1
How does DTSemNet's performance scale with increasing input dimensionality beyond 32 dimensions, particularly for high-dimensional data like images? The paper explicitly mentions that DTSemNet is unsuitable for high-dimensional inputs like images due to the difficulty DTs face with complex shapes and the need for many leaves, which negates their benefits. This remains unresolved as the paper does not provide experimental results or analysis for DTSemNet performance on high-dimensional datasets beyond tabular data and simple image data like MNIST.

### Open Question 2
What is the impact of different tree pruning and adaptive growth strategies on DTSemNet's accuracy and efficiency? The authors mention that for future work, they will consider developing differentiable methods for tree pruning and adaptive growth, indicating that current DTSemNet requires manual selection of tree height. The paper does not explore or evaluate any pruning or adaptive growth strategies for DTSemNet, leaving the potential benefits of such approaches unknown.

### Open Question 3
How does the approximation used in DTSemNet-regression (STE call to combine decisions with regression output) affect its performance compared to DTSemNet-classification, which uses no approximations? The paper notes that DTSemNet-regression uses one STE approximation, while DTSemNet-classification uses none, and this approximation sometimes hurts efficiency in regression tasks. The paper does not provide a detailed analysis of the impact of this single STE approximation on regression performance or compare it directly to the approximation-free classification approach.

## Limitations

- The fixed-weight architecture may limit generalization to complex tree structures with many internal nodes
- Single STE approximation in regression could introduce accuracy degradation in high-dimensional or noisy datasets
- Performance claims are based on benchmark datasets that may not reflect real-world deployment scenarios with streaming or non-stationary data

## Confidence

**High Confidence**: The theoretical framework for semantic equivalence between DTSemNet and hard oblique decision trees is well-established through formal proofs and clear architectural mappings. The gradient descent implementation using ReLU activations is straightforward and standard.

**Medium Confidence**: The empirical performance improvements (5.5% classification accuracy, 10% regression improvement) are promising but need validation on diverse datasets beyond the reported benchmarks. The claim of training time reduction is plausible given the elimination of complex tree induction heuristics.

**Low Confidence**: The reinforcement learning comparisons lack sufficient detail on environment complexity and policy evaluation metrics. The assertion that DTSemNet matches neural network efficiency in RL setups needs more rigorous validation across multiple environments.

## Next Checks

1. **Boundary Case Testing**: Systematically test DTSemNet on datasets where decision boundaries pass through or near training points to verify the claimed robustness to points lying exactly on decision boundaries.

2. **Deep Tree Performance**: Evaluate DTSemNet on datasets requiring deep tree structures (depth > 10) to assess gradient vanishing/exploding concerns and verify that training efficiency gains persist with increased complexity.

3. **Cross-Domain Generalization**: Test DTSemNet across diverse domains including image classification, time series forecasting, and tabular data with varying feature correlations to validate the broad applicability of the claimed accuracy improvements.