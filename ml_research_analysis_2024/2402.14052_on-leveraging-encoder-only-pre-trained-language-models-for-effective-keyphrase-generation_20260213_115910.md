---
ver: rpa2
title: On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase
  Generation
arxiv_id: '2402.14052'
source_url: https://arxiv.org/abs/2402.14052
tags:
- keyphrase
- linguistics
- computational
- association
- plms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the use of encoder-only Pre-trained Language
  Models (PLMs) for Keyphrase Generation (KPG), a task that requires predicting both
  present and absent keyphrases from documents. The authors explore three research
  questions: (1) the efficacy of encoder-only PLMs in KPG, (2) optimal architectural
  decisions for employing encoder-only PLMs in KPG, and (3) a performance comparison
  between in-domain encoder-only and encoder-decoder PLMs across varied resource settings.'
---

# On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation

## Quick Facts
- arXiv ID: 2402.14052
- Source URL: https://arxiv.org/abs/2402.14052
- Authors: Di Wu; Wasi Uddin Ahmad; Kai-Wei Chang
- Reference count: 0
- Key outcome: Encoder-only PLMs fine-tuned with prefix-LM style attention masks can effectively generate both present and absent keyphrases, outperforming general-domain seq2seq PLMs in data efficiency.

## Executive Summary
This paper investigates the use of encoder-only Pre-trained Language Models (PLMs) for Keyphrase Generation (KPG), a task that requires predicting both present and absent keyphrases from documents. The authors explore three research questions: (1) the efficacy of encoder-only PLMs in KPG, (2) optimal architectural decisions for employing encoder-only PLMs in KPG, and (3) a performance comparison between in-domain encoder-only and encoder-decoder PLMs across varied resource settings. The study reveals that while keyphrase extraction (KPE) with Conditional Random Fields (CRF) slightly excels in identifying present keyphrases, the KPG formulation with encoder-only PLMs generates a broader spectrum of keyphrase predictions, including absent keyphrases. Specifically, prefix-LM fine-tuning of encoder-only PLMs emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs.

## Method Summary
The authors fine-tune encoder-only PLMs (BERT, SciBERT, NewsBERT) for KPE using sequence labeling or sequence labeling with CRF, and for KPG using prefix-LM style attention masks or initialize an encoder-decoder architecture with encoder-only PLMs (BERT2BERT). They also fine-tune encoder-decoder PLMs (BART, SciBART, NewsBART) for KPG using the sequence generation formulation. The models are evaluated on the SciKP and KPTimes datasets using macro-averaged F1@5 and F1@M scores for present and absent keyphrases.

## Key Results
- Prefix-LM fine-tuning of encoder-only PLMs is a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs.
- BERT2BERT models with a deep encoder and shallow decoder outperform the reverse configuration for keyphrase generation.
- In-domain encoder-only PLMs (e.g., SciBERT, NewsBERT) outperform general-domain PLMs for keyphrase generation within their respective domains.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoder-only PLMs fine-tuned with prefix-LM style attention masks can effectively generate both present and absent keyphrases.
- Mechanism: By modifying the attention mask pattern to allow the decoder part of the sequence (keyphrases) to attend to tokens on their left, the model learns to generate sequences autoregressively while still leveraging the bidirectional encoding capability for the input document.
- Core assumption: The bidirectional encoding of the document provides sufficient contextual information for generating keyphrases, even without a separate decoder.
- Evidence anchors:
  - [abstract]: "Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs."
  - [section 4.2]: "Using this formulation, we fine-tune encoder-only PLMs for seq2seq keyphrase generation."
  - [corpus]: Weak - The corpus contains related papers but no direct evidence for this specific mechanism.
- Break condition: If the document context encoded bidirectionally is insufficient to capture all relevant information for generating absent keyphrases, or if the prefix-LM attention mask introduces significant training instability.

### Mechanism 2
- Claim: BERT2BERT models with a deep encoder and shallow decoder outperform the reverse configuration for keyphrase generation.
- Mechanism: A deep encoder provides rich, contextualized representations of the input document, which are then decoded by a shallow decoder into keyphrases. This allocation prioritizes understanding the document over complex decoding.
- Core assumption: Understanding the document (encoding) is more challenging and important than generating the keyphrases (decoding) for this task.
- Evidence anchors:
  - [section 5.3]: "We also identify a favorable parameter allocation towards model depth rather than width when employing encoder-decoder architectures initialized with encoder-only PLMs."
  - [section 5.3]: "Specifically, model depth should be prioritized over the width, and a deep encoder with a shallow decoder outperforms the reverse for keyphrase quality and inference latency."
  - [corpus]: Weak - No direct evidence in the corpus for this specific architectural claim.
- Break condition: If the keyphrase generation task becomes more complex (e.g., longer keyphrases, more complex relationships), requiring a deeper decoder to capture these nuances.

### Mechanism 3
- Claim: In-domain encoder-only PLMs (e.g., SciBERT, NewsBERT) outperform general-domain PLMs for keyphrase generation within their respective domains.
- Mechanism: Pre-training on domain-specific corpora allows the model to learn domain-specific vocabulary, syntax, and concepts, leading to better representations for documents in that domain.
- Core assumption: The domain-specific pre-training provides a better initialization for the downstream keyphrase generation task within that domain compared to general-domain pre-training.
- Evidence anchors:
  - [abstract]: "We conducted an evaluation of applying the encoder-only SciBERT and NewsBERT for KPG inside and outside of their pre-training domain. Compared to the general-domain BERT, both models demonstrate better in-domain performance."
  - [section 5.2]: "The strength is especially notable within-domain BERT models. On KP20k, SciBERT-G outperforms BART-base on all the metrics."
  - [corpus]: Weak - No direct evidence in the corpus for this specific domain adaptation claim.
- Break condition: If the domain-specific pre-training introduces significant bias that hinders generalization to out-of-domain data, or if the general-domain PLM is sufficiently large and diverse to capture the necessary domain knowledge.

## Foundational Learning

- Concept: Keyphrase Extraction (KPE) vs. Keyphrase Generation (KPG)
  - Why needed here: The paper explicitly distinguishes between these two tasks and their formulations. Understanding the difference is crucial for interpreting the results and the motivation for the study.
  - Quick check question: What is the key difference between KPE and KPG in terms of the keyphrases they aim to predict?
- Concept: Encoder-only vs. Encoder-decoder PLMs
  - Why needed here: The paper compares the performance of encoder-only PLMs (e.g., BERT, SciBERT) with encoder-decoder PLMs (e.g., BART) for KPG. Understanding their architectures and capabilities is essential for interpreting the findings.
  - Quick check question: What is the main architectural difference between encoder-only and encoder-decoder PLMs, and how does it relate to their typical use cases?
- Concept: Pre-training and Fine-tuning
  - Why needed here: The paper relies on pre-trained PLMs and explores different fine-tuning strategies (e.g., sequence labeling, prefix-LM, BERT2BERT). Understanding these concepts is crucial for understanding the methodology and the motivation for using pre-trained models.
  - Quick check question: What is the difference between pre-training and fine-tuning in the context of PLMs, and why is pre-training often used as a starting point for downstream tasks?

## Architecture Onboarding

- Component map: Document (title + body) -> Encoder (BERT/SciBERT/NewsBERT) -> Attention mask (prefix-LM) or Decoder (BERT2BERT) -> Sequence of keyphrases
- Critical path: Document encoding -> Keyphrase generation (via modified attention or separate decoder)
- Design tradeoffs:
  - Encoder-only vs. Encoder-decoder: Computational efficiency vs. potential performance gain
  - Prefix-LM vs. BERT2BERT: Simplicity and data efficiency vs. potentially better performance with more parameters
  - Model depth vs. width: Prioritizing understanding (encoding) over generation (decoding)
- Failure signatures:
  - Prefix-LM: Poor generation quality, inability to generate absent keyphrases
  - BERT2BERT: Overfitting, slow inference due to deep decoder
  - In-domain models: Poor generalization to out-of-domain data
- First 3 experiments:
  1. Compare prefix-LM fine-tuning of BERT with sequence labeling for KPE on KP20k.
  2. Compare BERT2BERT with different encoder-decoder depth ratios on KP20k.
  3. Compare in-domain (SciBERT, NewsBERT) vs. general-domain (BERT) models for KPG on their respective domains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do encoder-only PLMs compare to encoder-decoder PLMs in terms of generating absent keyphrases?
- Basis in paper: [explicit] The authors compare encoder-only PLMs and encoder-decoder PLMs in terms of present and absent keyphrase generation.
- Why unresolved: The paper provides some comparisons but does not give a definitive answer on whether encoder-only PLMs can match or surpass encoder-decoder PLMs in generating absent keyphrases.
- What evidence would resolve it: A comprehensive evaluation of encoder-only and encoder-decoder PLMs on a large dataset, focusing specifically on absent keyphrase generation, would provide a clearer answer.

### Open Question 2
- Question: What is the optimal parameter allocation strategy for encoder-decoder architectures initialized with encoder-only PLMs?
- Basis in paper: [explicit] The authors investigate the impact of parameter allocation on the performance of encoder-decoder architectures initialized with encoder-only PLMs.
- Why unresolved: While the paper provides some insights, it does not give a definitive answer on the optimal parameter allocation strategy.
- What evidence would resolve it: A systematic study of different parameter allocation strategies on a large dataset, considering both performance and computational efficiency, would provide a clearer answer.

### Open Question 3
- Question: How do domain-specific encoder-only PLMs perform on keyphrase generation tasks in their pre-training domain compared to general-domain PLMs?
- Basis in paper: [explicit] The authors pre-train domain-specific encoder-only PLMs (SciBERT and NewsBERT) and compare their performance on keyphrase generation tasks to general-domain PLMs.
- Why unresolved: The paper provides some results but does not give a comprehensive comparison of domain-specific and general-domain PLMs on keyphrase generation tasks.
- What evidence would resolve it: A large-scale evaluation of domain-specific and general-domain PLMs on keyphrase generation tasks in various domains would provide a clearer answer.

## Limitations

- The evaluation primarily relies on automatic metrics (F1@5, F1@M) which may not fully capture the quality and utility of generated keyphrases.
- The ablation studies on model depth/width trade-offs are conducted only on BERT2BERT configurations, limiting generalizability to other encoder-decoder architectures.
- The paper does not provide a comprehensive comparison of domain-specific and general-domain PLMs on keyphrase generation tasks across various domains.

## Confidence

- **High Confidence**: The core finding that prefix-LM fine-tuning of encoder-only PLMs is effective for KPG, supported by multiple experiments across datasets and direct comparisons with seq2seq baselines.
- **Medium Confidence**: The domain adaptation benefits of in-domain encoder-only PLMs, as the evidence is primarily based on F1 scores without extensive qualitative analysis of generated keyphrases.
- **Medium Confidence**: The architectural insight regarding depth vs. width trade-offs in BERT2BERT models, though limited to specific configurations and requiring further validation across different model scales.

## Next Checks

1. Conduct human evaluation studies comparing keyphrases generated by encoder-only PLMs versus encoder-decoder models to validate automatic metric results.
2. Extend the depth/width ablation studies to larger model scales and different encoder-decoder initialization strategies to test the generalizability of the architectural findings.
3. Test the prefix-LM approach on additional document types and domains to assess its robustness and limitations when handling diverse text structures and terminology.