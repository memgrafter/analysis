---
ver: rpa2
title: 'M-QALM: A Benchmark to Assess Clinical Reading Comprehension and Knowledge
  Recall in Large Language Models via Question Answering'
arxiv_id: '2406.03699'
source_url: https://arxiv.org/abs/2406.03699
tags:
- question
- option
- context
- mcqa
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) show promise for medical applications,
  but their clinical knowledge and comprehension remain poorly understood. This paper
  introduces M-QALM, a comprehensive benchmark of 22 clinical datasets spanning multiple-choice
  and abstractive question answering tasks across generalist and specialist medical
  domains.
---

# M-QALM: A Benchmark to Assess Clinical Reading Comprehension and Knowledge Recall in Large Language Models via Question Answering

## Quick Facts
- **arXiv ID**: 2406.03699
- **Source URL**: https://arxiv.org/abs/2406.03699
- **Reference count**: 40
- **Primary result**: Large language models show promise for medical applications but still lag behind humans and proprietary models like GPT-4, with fine-tuning on M-QALM improving performance across architectures but limited generalization to unseen datasets.

## Executive Summary
This paper introduces M-QALM, a comprehensive benchmark of 22 clinical datasets spanning multiple-choice and abstractive question answering tasks across generalist and specialist medical domains. Experiments with 15 open-source LLMs reveal that while models significantly outperform random baselines, they still lag behind humans and proprietary models like GPT-4. Fine-tuning on M-QALM improves performance across architectures and model sizes, partially compensating for weaknesses in architecture or pre-training data. However, improvements on unseen datasets are limited, with only about 60% attributable to generalization rather than memorization. Manual error analysis highlights a gap between models' abilities to recall knowledge and integrate it with context, particularly for reading comprehension and quantitative questions.

## Method Summary
The study evaluates 15 open-source LLMs on the M-QALM benchmark using both multiple-choice question answering (MCQA) and abstractive question answering (AQA) tasks. Models are fine-tuned on M-QALM training portions using parameter-efficient fine-tuning (QLora) with 4-bit quantization. Performance is measured using accuracy for MCQA and ROUGE-L, BERTScore, and METEOR for AQA. The evaluation includes both seen datasets (from M-QALM training) and unseen datasets to assess generalization. Error analysis is conducted on 100 samples to identify failure modes across different question types.

## Key Results
- Open-source LLMs significantly outperform random baselines but lag behind humans and GPT-4 on clinical QA tasks
- Fine-tuning on M-QALM improves performance across architectures and model sizes, with decoder-only models like MPT (7B) showing the largest gains (+25.6 accuracy improvement)
- Only about 60% of performance improvements on unseen datasets can be attributed to generalization rather than memorization
- Models perform significantly worse on reading comprehension questions compared to knowledge recall questions, indicating difficulty integrating knowledge with context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning on general-domain tasks improves LLMs' ability to recall and integrate medical knowledge during QA.
- Mechanism: Instruction tuning aligns model outputs to expected formats and improves representations of questions and contexts, even when the instruction data is not domain-specific.
- Core assumption: Representations learned from general instruction tuning transfer to domain-specific QA tasks.
- Evidence anchors:
  - [abstract] "uncovers success factors such as instruction tuning that lead to improved recall and comprehension."
  - [section] "instruction fine-tuned models perform better than their corresponding Base versions, despite the fact that the instruction set used for fine-tuning contains only tasks in the general domain."
  - [corpus] Weak evidence: Only general MRQA and QA datasets are available, not medical QA.
- Break condition: If medical domain requires specialized reasoning patterns not present in general instruction tuning, performance gains may not materialize.

### Mechanism 2
- Claim: Fine-tuning on M-QALM improves model performance across architectures and model sizes, partially compensating for weaknesses in architecture or pre-training data.
- Mechanism: Task-specific fine-tuning adapts model weights to the statistical patterns of medical QA, enabling better knowledge recall and integration.
- Core assumption: The fine-tuning data contains sufficient variation and coverage of medical QA patterns to be generalizable.
- Evidence anchors:
  - [abstract] "fine-tuning on M-QALM improves performance across architectures and model sizes, partially compensating for weaknesses in architecture or pre-training data."
  - [section] "Decoder-only models like MPT (7B) benefit more than others (+25.6 Accuracy improvement)."
  - [corpus] Weak evidence: Limited prior studies on fine-tuning large LLMs on medical QA datasets.
- Break condition: If fine-tuning data is too narrow or memorization-dominated, generalization to unseen datasets may fail.

### Mechanism 3
- Claim: Models struggle more with integrating knowledge with context than with recalling knowledge, particularly for reading comprehension and quantitative questions.
- Mechanism: Knowledge recall involves retrieving stored facts, while integration requires aligning retrieved knowledge with contextual cues, a more complex cognitive operation.
- Core assumption: The difficulty difference stems from the complexity of the cognitive operation, not data distribution or prompt design.
- Evidence anchors:
  - [abstract] "manual error analysis highlights a gap between models' abilities to recall knowledge and integrate it with context."
  - [section] "models perform worse on Reading Comprehension questions, suggesting that it is indeed harder to integrate necessary knowledge rather than just recalling it."
  - [corpus] Moderate evidence: Prior work on reading comprehension shows similar difficulty gaps.
- Break condition: If the prompt design favors recall over integration, the observed gap may be artifactual.

## Foundational Learning

- **Concept: Question Answering Task Formulation**
  - Why needed here: M-QALM uses both MCQA and AQA formats; understanding their differences is critical for model evaluation and design.
  - Quick check question: What is the main difference between MCQA and AQA in terms of model output?

- **Concept: Fine-tuning vs. Instruction Tuning**
  - Why needed here: The paper distinguishes between instruction tuning (general tasks) and fine-tuning (domain-specific M-QALM), both affecting model performance.
  - Quick check question: Which method adapts models to specific domains, instruction tuning or fine-tuning?

- **Concept: Evaluation Metrics for QA**
  - Why needed here: Accuracy is used for MCQA, while ROUGE-L, BERTScore, and METEOR are used for AQA; knowing their strengths/weaknesses is key for interpretation.
  - Quick check question: Which AQA metric shows the most reliable correlation with MCQA accuracy in this study?

## Architecture Onboarding

- **Component map**: Base LLMs (decoder-only: LLaMA, MPT, Falcon; encoder-decoder: Flan-T5) → Instruction Tuning → Domain-Specific Fine-tuning → Evaluation on M-QALM
- **Critical path**: Model selection → Prompt engineering → Evaluation → Error analysis → Fine-tuning strategy
- **Design tradeoffs**: Larger models perform better but are harder to fine-tune; encoder-decoder vs. decoder-only affects AQA performance; instruction tuning helps but may not suffice for domain adaptation.
- **Failure signatures**: Low accuracy on unseen datasets may indicate memorization; poor performance on reading comprehension suggests integration weaknesses; inconsistent AQA metric results hint at metric limitations.
- **First 3 experiments**:
  1. Evaluate base LLaMA2-7B on M-QALM MCQA to establish baseline.
  2. Fine-tune LLaMA2-7B on M-QALM MCQA and evaluate on both seen and unseen datasets.
  3. Compare performance of instruction-tuned vs. base models on a subset of M-QALM to test transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on clinical tasks requiring both knowledge recall and reasoning compared to tasks requiring only knowledge recall?
- Basis in paper: [explicit] The paper's error analysis shows that Reading Comprehension questions (requiring both knowledge recall and context integration) are harder than Recall questions, and that models struggle more with these.
- Why unresolved: While the paper identifies this gap, it doesn't quantify the performance difference between these task types or investigate which specific reasoning skills (e.g., inference, comparison) are most challenging.
- What evidence would resolve it: Detailed breakdown of model performance on different reasoning types (e.g., comparison, inference, application) across various clinical tasks.

### Open Question 2
- Question: Can fine-tuning on medical QA datasets improve LLM performance on tasks requiring quantitative reasoning (e.g., dosage calculations, probability)?
- Basis in paper: [explicit] The error analysis reveals that Quantitative/Arithmetic questions are the worst-performing category even after fine-tuning, suggesting a persistent weakness.
- Why unresolved: The paper only examines the effect of fine-tuning on knowledge recall and comprehension, not on quantitative reasoning abilities.
- What evidence would resolve it: Experiments comparing LLM performance on quantitative reasoning tasks before and after fine-tuning on datasets containing similar calculations.

### Open Question 3
- Question: What is the optimal balance between general instruction tuning and domain-specific fine-tuning for achieving the best clinical performance in LLMs?
- Basis in paper: [inferred] The paper shows that instruction tuning improves performance even without domain-specific data, while domain-specific fine-tuning further improves results, suggesting a potential complementarity.
- Why unresolved: The paper doesn't explore different combinations or sequences of instruction tuning and domain-specific fine-tuning to find the optimal approach.
- What evidence would resolve it: Comparative experiments testing various combinations and sequences of general instruction tuning and domain-specific fine-tuning on clinical tasks.

## Limitations

- The finding that approximately 40% of performance improvements on unseen datasets cannot be attributed to true generalization raises concerns about benchmark's ability to measure real-world deployment readiness
- Manual error analysis covers only 100 samples, which may not capture the full spectrum of model weaknesses
- The study does not investigate potential safety concerns or harmful outputs that could arise in clinical settings

## Confidence

- **High Confidence**: The core finding that instruction tuning improves performance even on domain-specific tasks, and that larger models generally outperform smaller ones, is well-supported by extensive experimental evidence across multiple model families and sizes.

- **Medium Confidence**: The claim that models struggle more with knowledge integration than recall is supported by error analysis but could be influenced by prompt design choices and dataset characteristics. The interpretation of generalization metrics (60% attributable to memorization vs. 40% to generalization) requires careful consideration of potential confounding factors.

- **Low Confidence**: The assertion that M-QALM comprehensively covers both generalist and specialist medical domains is difficult to verify given the rapidly evolving nature of medical knowledge and the potential for dataset bias. The specific claim that decoder-only models benefit more from fine-tuning than encoder-decoder models may be architecture-specific and not universally applicable.

## Next Checks

1. **Generalization Stress Test**: Conduct a systematic evaluation of model performance on completely out-of-distribution medical QA tasks not represented in M-QALM to quantify true generalization capabilities beyond the 60% estimate.

2. **Safety and Robustness Analysis**: Implement a comprehensive safety evaluation protocol to identify potentially harmful outputs, incorrect recommendations, or biased responses in clinical scenarios, addressing the critical deployment gap.

3. **Metric Correlation Study**: Perform a detailed correlation analysis between different AQA metrics (ROUGE-L, BERTScore, METEOR) and clinical expert judgments to establish which metric most reliably predicts clinically meaningful performance.