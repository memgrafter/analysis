---
ver: rpa2
title: Stratified Prediction-Powered Inference for Hybrid Language Model Evaluation
arxiv_id: '2406.04291'
source_url: https://arxiv.org/abs/2406.04291
tags:
- autorater
- where
- data
- confidence
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a stratified prediction-powered inference (StratPPI)
  method that improves confidence interval estimation by combining limited human-labeled
  data with larger amounts of data labeled by an automatic system. The key innovation
  is applying stratified sampling to the prediction-powered inference framework, allowing
  for better adaptation to heterogeneity in autorater performance across different
  data domains.
---

# Stratified Prediction-Powered Inference for Hybrid Language Model Evaluation

## Quick Facts
- arXiv ID: 2406.04291
- Source URL: https://arxiv.org/abs/2406.04291
- Reference count: 40
- One-line primary result: StratPPI improves confidence interval estimation by applying stratified sampling to prediction-powered inference, reducing interval widths by up to 35% compared to classical inference

## Executive Summary
This paper addresses the challenge of hybrid evaluation in language models, where human-labeled data is limited but larger amounts of autorater-labeled data are available. The authors propose Stratified Prediction-Powered Inference (StratPPI), which extends the prediction-powered inference framework by incorporating stratified sampling to handle heterogeneity in autorater performance across different data domains. By partitioning data into strata and applying optimal allocation strategies, StratPPI achieves tighter confidence intervals while maintaining valid coverage guarantees.

## Method Summary
StratPPI combines stratified sampling with prediction-powered inference by partitioning the input space into K non-overlapping strata based on autorater predictions or confidence scores. For each stratum, it computes a stratum-specific prediction-powered loss function using both human-labeled samples (with both human and autorater labels) and autorater-only samples. The method then aggregates these stratum-specific losses with data-dependent weights to produce a final parameter estimate. Confidence intervals are constructed using the asymptotic normality of the weighted M-estimator, with stratum-specific variance components. Optimal sample allocation across strata is determined using autorater confidence scores when available, otherwise using heuristic allocation strategies.

## Key Results
- StratPPI can reduce confidence interval widths by up to 35% compared to classical inference
- Performance improvements of 10-15% over standard prediction-powered inference when autorater performance varies across data domains
- Asymptotic coverage guarantees are maintained while achieving tighter intervals through optimal stratification and allocation
- Benefits are most pronounced when autorater performance is systematically different across identifiable partitions of the input space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stratification reduces variance when autorater performance varies across data domains
- Mechanism: By partitioning data into strata where autorater bias and variance are more homogeneous, the stratified prediction-powered loss becomes a more accurate and lower-variance estimator of the true parameter
- Core assumption: The autorater's performance is systematically different across identifiable partitions of the input space
- Evidence anchors:
  - [abstract] "StratPPI is expected to improve in cases where the performance of the autorater varies across different conditional distributions of the target data"
  - [section 4] "Splitting the space of inputs and their autorater predictions into different domains allows us to derive a more specialized form of the prediction powered loss that can better adapt to this autorater heterogeneity via stratified sampling"
  - [corpus] Weak evidence - no direct mention of variance reduction through stratification in neighboring papers
- Break condition: If autorater performance is uniform across all strata, stratification provides no benefit

### Mechanism 2
- Claim: Optimal allocation of human-labeled samples across strata minimizes overall confidence interval width
- Mechanism: By allocating more human-labeled samples to strata where the autorater has higher variance or bias, the stratified estimator achieves lower overall variance than uniform allocation
- Core assumption: We can estimate stratum-specific variances and biases from autorater confidence scores
- Evidence anchors:
  - [section 4.3] "Although the solution for ρ* is informative, it is not necessarily practical, as it depends on knowing A_w^(-1) V_k^λ k,∆,θ* A_w^(-1)"
  - [section 4.3] "To address the remaining dependence on P(X, Y ), we propose to use autorater confidence scores, assuming they are available"
  - [corpus] No direct evidence - neighboring papers don't discuss sample allocation optimization
- Break condition: If autorater confidence scores are unavailable or unreliable, optimal allocation cannot be implemented

### Mechanism 3
- Claim: The stratified approach provides asymptotic coverage guarantees while potentially achieving tighter confidence intervals
- Mechanism: The asymptotic normality of weighted M-estimators combined with stratum-specific variance components allows construction of confidence intervals with valid coverage probability
- Core assumption: Regularity conditions for M-estimation are satisfied and sample sizes grow large enough for asymptotic results to apply
- Evidence anchors:
  - [section 4.1] "We now present our main result, which is a confidence interval for θ* based on the stratified loss... as in PPI++, the minimizer of the stratified loss... has an asymptotically normal distribution with mean θ*"
  - [section A.1] "Proof. For ease of notation, we will define..." (derivation of asymptotic normality)
  - [corpus] Moderate evidence - neighboring papers mention asymptotic properties but not specifically for stratified settings
- Break condition: For small sample sizes, asymptotic guarantees may not hold

## Foundational Learning

- Concept: Prediction-Powered Inference (PPI)
  - Why needed here: StratPPI builds directly on PPI by extending it to handle stratified sampling
  - Quick check question: What is the key insight that allows PPI to combine autorater predictions with human labels while maintaining unbiased estimates?

- Concept: Stratified Sampling
  - Why needed here: The core innovation is applying stratified sampling to the PPI framework to handle heterogeneous autorater performance
  - Quick check question: How does stratified sampling differ from simple random sampling in terms of variance reduction?

- Concept: M-estimation and Asymptotic Normality
  - Why needed here: The theoretical foundation for deriving confidence intervals relies on properties of M-estimators
  - Quick check question: What regularity conditions must be satisfied for an M-estimator to be asymptotically normal?

## Architecture Onboarding

- Component map: Data inputs → Stratification logic → Stratum-specific loss computation → Weighted aggregation → Parameter estimation → Confidence interval construction
- Critical path: Stratification → Loss computation per stratum → Weighted aggregation → Parameter estimation → Confidence interval construction
- Design tradeoffs: More strata can capture heterogeneity better but increase computational complexity and parameter count; simpler stratification may miss important performance differences
- Failure signatures: If confidence intervals are unexpectedly wide, check for: (1) inappropriate stratification boundaries, (2) poor autorater confidence score calibration, or (3) insufficient human labels in high-variance strata
- First 3 experiments:
  1. Implement stratified sampling with equal allocation across strata and verify it matches PPI performance on homogeneous data
  2. Add stratum-specific weighting and test on data with known performance heterogeneity
  3. Implement heuristic allocation using autorater confidence scores and compare against optimal allocation on synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of strata K for stratified PPI in practice, and how does it scale with sample size?
- Basis in paper: [inferred] The paper assumes K is fixed and mentions that if K scales with n, a more careful treatment is required. They use K=10 in experiments.
- Why unresolved: The paper treats K as a fixed parameter and does not explore how to determine the optimal number of strata or how it should scale with n.
- What evidence would resolve it: Experiments showing the trade-off between stratification granularity and estimation variance across different sample sizes and numbers of strata.

### Open Question 2
- Question: How does StratPPI perform when human-labeled data is not available for optimal stratification, but rather has already been collected?
- Basis in paper: [explicit] The paper notes this as a limitation, stating "the human data might have already been collected, in which case the studied stratified sampling setup does not directly apply."
- Why unresolved: The paper focuses on cases where stratification can be designed in advance with control over sample allocation, but many practical scenarios involve post-stratification with fixed existing data.
- What evidence would resolve it: Experimental comparisons between pre-stratified vs post-stratified approaches when the human data allocation is fixed.

### Open Question 3
- Question: How does the performance of StratPPI vary with different stratification strategies beyond autorater predictions?
- Basis in paper: [explicit] The paper mentions they choose to focus on stratifications based on autorater predictions but notes "It turns out that the optimal ρk values can be exactly calculated" suggesting other strategies might be possible.
- Why unresolved: The paper only explores one specific stratification strategy (based on autorater predictions) and doesn't compare it to other potential stratification methods.
- What evidence would resolve it: Systematic comparison of StratPPI performance across different stratification strategies (e.g., based on input features, model uncertainty, or other metadata).

## Limitations

- The method requires either control over human data collection for optimal stratification or availability of autorater confidence scores for heuristic allocation
- Asymptotic guarantees may not hold for small sample sizes or poorly chosen stratification boundaries
- Computational complexity increases with the number of strata, creating a tradeoff between capturing heterogeneity and tractability
- Performance degrades if autorater predictions are anti-correlated with true labels in some strata

## Confidence

- High confidence in the variance reduction mechanism when autorater performance is heterogeneous across identifiable strata
- Medium confidence in the practical benefit of optimal allocation when autorater confidence scores are available
- Low confidence in asymptotic coverage guarantees for small sample sizes or complex stratification schemes

## Next Checks

1. Conduct a simulation study varying the number of strata and measuring the tradeoff between variance reduction and computational overhead
2. Test StratPPI on real-world datasets where autorater confidence scores are unavailable or unreliable to assess robustness
3. Perform sensitivity analysis on the impact of poor stratification choices (e.g., too few/many strata, uninformative boundaries) on confidence interval coverage and width