---
ver: rpa2
title: Do Large Language Models Mirror Cognitive Language Processing?
arxiv_id: '2402.18023'
source_url: https://arxiv.org/abs/2402.18023
tags:
- llms
- similarity
- llm-brain
- language
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how well Large Language Models (LLMs) mirror
  human cognitive language processing by comparing LLM text embeddings to brain fMRI
  signals using Representational Similarity Analysis (RSA). Experiments on 23 mainstream
  LLMs show that pre-training data size, model scaling, and alignment training (e.g.,
  supervised fine-tuning, reinforcement learning from human feedback) all significantly
  improve the alignment between LLMs and brain signals.
---

# Do Large Language Models Mirror Cognitive Language Processing?

## Quick Facts
- arXiv ID: 2402.18023
- Source URL: https://arxiv.org/abs/2402.18023
- Authors: Yuqi Ren; Renren Jin; Tongxuan Zhang; Deyi Xiong
- Reference count: 19
- Key outcome: This study investigates how well Large Language Models (LLMs) mirror human cognitive language processing by comparing LLM text embeddings to brain fMRI signals using Representational Similarity Analysis (RSA). Experiments on 23 mainstream LLMs show that pre-training data size, model scaling, and alignment training (e.g., supervised fine-tuning, reinforcement learning from human feedback) all significantly improve the alignment between LLMs and brain signals. Explicit prompts enhance this alignment, whereas nonsensical prompts reduce it. Sentiment analysis reveals LLMs encode positive sentiment more strongly than negative sentiment. Furthermore, LLM performance on standard benchmarks (MMLU, Chatbot Arena) strongly correlates with their brain similarity, suggesting RSA-based alignment could serve as a proxy for evaluating LLM capabilities.

## Executive Summary
This study investigates the cognitive alignment of Large Language Models (LLMs) with human brain activity during language processing using Representational Similarity Analysis (RSA) on fMRI data. The researchers systematically evaluated 23 mainstream LLMs across various model sizes and training configurations, examining how pre-training data, model scaling, alignment training, and prompting strategies affect brain-LLM similarity. The work establishes that larger models with more pre-training data and alignment training show stronger correspondence with human neural representations, while also revealing interesting patterns in how LLMs encode different types of semantic content and sentiment.

## Method Summary
The researchers employed Representational Similarity Analysis (RSA) to compare LLM text embeddings with human brain fMRI signals during language processing tasks. They analyzed 23 mainstream LLMs across various sizes and training configurations, measuring how well LLM representations align with brain activity patterns using Pearson correlation coefficients. The study systematically varied factors including pre-training data size, model scaling, alignment training methods (SFT, RLHF), and prompt types. Brain data came from fMRI recordings of participants reading text, with similarity assessed across 1,000 voxels representing language processing regions. The researchers also examined sentiment encoding patterns and benchmark performance correlations.

## Key Results
- Pre-training data size and model scaling significantly improve brain-LLM alignment, with larger models showing stronger correspondence to human neural representations
- Alignment training methods (SFT, RLHF) substantially enhance brain-LLM similarity compared to pre-trained-only models
- Explicit prompts increase brain-LLM alignment while nonsensical prompts decrease it, and LLMs encode positive sentiment more strongly than negative sentiment
- LLM benchmark performance (MMLU, Chatbot Arena) strongly correlates with brain similarity scores, suggesting RSA alignment could serve as a capability proxy

## Why This Works (Mechanism)
The alignment between LLMs and brain activity likely stems from shared optimization objectives in language processing. Both systems face similar computational challenges in mapping linguistic input to meaningful representations, suggesting convergent solutions. Pre-training on vast text corpora enables LLMs to capture distributional patterns that mirror human language experience, while alignment training refines these representations to better match human communicative preferences and pragmatic constraints.

## Foundational Learning
- **Representational Similarity Analysis (RSA)**: A technique for comparing neural representations across systems by analyzing pattern similarity rather than absolute activation values. Why needed: Allows meaningful comparison between LLM embeddings and brain activity patterns despite different representational spaces.
- **fMRI language processing**: Brain imaging technique that measures neural activity during language comprehension tasks. Why needed: Provides the ground truth human neural responses for comparison with LLM representations.
- **Alignment training (SFT, RLHF)**: Fine-tuning methods that adapt LLMs to better match human preferences and behaviors. Why needed: Critical factor that significantly improves brain-LLM alignment beyond pre-training alone.
- **Prompt sensitivity**: The degree to which LLM outputs vary based on input prompt characteristics. Why needed: Demonstrates how external factors influence brain-LLM alignment, with explicit prompts enhancing and nonsensical prompts reducing similarity.
- **Sentiment encoding**: The representation of emotional valence in LLM embeddings. Why needed: Reveals systematic biases in how LLMs process positive versus negative content relative to brain activity patterns.

## Architecture Onboarding
**Component Map**: Pre-training data → LLM architecture → Alignment training → RSA analysis -> Brain similarity measurement

**Critical Path**: Pre-training data size and model scaling form the foundation, followed by alignment training which most strongly impacts brain-LLM similarity, with prompting strategies providing fine-tuning effects.

**Design Tradeoffs**: Larger models with more pre-training data require greater computational resources but yield better brain alignment. Alignment training improves cognitive alignment but may reduce general language modeling performance. Prompt sensitivity offers control over alignment but introduces variability.

**Failure Signatures**: Poor brain-LLM alignment may indicate insufficient pre-training data, inadequate model capacity, or lack of alignment training. Reduced alignment with nonsensical prompts confirms the importance of meaningful input structure.

**First Experiments**:
1. Compare brain similarity across different model families (decoder-only vs encoder-decoder) with matched parameters and pre-training data
2. Test whether brain-LLM alignment generalizes across different fMRI experimental paradigms beyond naturalistic reading
3. Examine voxel-level brain-LLM similarity mapping to identify specific brain regions with strongest alignment

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do different alignment training methods (SFT vs RLHF vs DPO) specifically impact LLM-brain similarity, and are certain methods more effective than others?
- Basis in paper: The paper compares alignment methods but doesn't analyze their differential effects on brain similarity.
- Why unresolved: The paper groups alignment training broadly but doesn't distinguish between the effectiveness of specific methods.
- What evidence would resolve it: Comparative analysis of LLM-brain similarity scores across models trained with different alignment methods under controlled conditions.

### Open Question 2
- Question: Does LLM-brain similarity vary across different cognitive tasks or brain regions beyond the 1,000 voxels studied?
- Basis in paper: The paper focuses on general language processing but doesn't explore task-specific or regional differences.
- Why unresolved: The analysis uses a fixed voxel selection without examining whether similarity patterns differ for different cognitive functions.
- What evidence would resolve it: Comparative RSA analyses using task-specific fMRI datasets and voxel-by-voxel similarity mapping.

### Open Question 3
- Question: What is the relationship between LLM-brain similarity and the semantic content of the input text, particularly for abstract vs concrete concepts?
- Basis in paper: The paper uses 180 concepts but doesn't analyze how concept concreteness affects similarity.
- Why unresolved: The paper doesn't stratify results by semantic properties of the stimulus material.
- What evidence would resolve it: Analysis of LLM-brain similarity scores separated by concrete/abstract concept categories and controlled for other linguistic features.

### Open Question 4
- Question: How do smaller language models (below 1B parameters) compare to the 7B+ models studied in terms of brain similarity?
- Basis in paper: The paper explicitly notes that previous work studied smaller models but doesn't include them in the current analysis.
- Why unresolved: The study focuses on larger models to match current trends but doesn't establish whether the scaling relationships hold for smaller models.
- What evidence would resolve it: RSA analysis of brain similarity for a range of smaller models with controlled pre-training data and alignment training.

## Limitations
- The study cannot definitively establish whether brain-LLM alignment reflects true cognitive mirroring or merely shared linguistic competence
- RSA approach measures representational similarity but does not capture temporal dynamics or causal relationships between LLM processing and brain activity
- Focus on fMRI data from naturalistic reading may limit generalizability to other language tasks or brain measurement modalities

## Confidence
- **High Confidence**: The core finding that larger pre-training datasets and model scaling improve brain-LLM alignment is well-supported by systematic experiments across 23 models. The positive effect of alignment training (SFT, RLHF) on brain similarity is also robustly demonstrated.
- **Medium Confidence**: The claim that explicit prompts enhance brain-LLM alignment while nonsensical prompts reduce it is supported but requires replication across different prompt types and experimental conditions. The correlation between benchmark performance and brain similarity, while statistically significant, may not hold for all LLM families or evaluation metrics.
- **Low Confidence**: The interpretation of sentiment encoding differences as reflecting cognitive processing patterns is speculative and requires additional validation. The proposal of RSA-based alignment as a proxy for LLM capability evaluation needs extensive empirical testing across diverse benchmarks and model architectures.

## Next Checks
1. Conduct temporal analysis using magnetoencephalography (MEG) or electroencephalography (EEG) to examine whether brain-LLM alignment exhibits consistent time courses across different processing stages.
2. Design controlled experiments with matched positive and negative sentiment stimuli to determine whether the observed sentiment encoding differences reflect genuine cognitive biases or stimulus confounds.
3. Perform cross-linguistic validation using multilingual fMRI datasets to assess whether brain-LLM alignment patterns generalize across different language families and cultural contexts.