---
ver: rpa2
title: Evaluating Large Language Models for Public Health Classification and Extraction
  Tasks
arxiv_id: '2405.14766'
source_url: https://arxiv.org/abs/2405.14766
tags:
- health
- tasks
- public
- llms
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates Large Language Models (LLMs) for public health
  tasks involving classification and extraction from free text. It combines six externally
  annotated datasets with seven new internally annotated datasets to assess LLMs on
  tasks related to health burden, epidemiological risk factors, and public health
  interventions.
---

# Evaluating Large Language Models for Public Health Classification and Extraction Tasks

## Quick Facts
- **arXiv ID:** 2405.14766
- **Source URL:** https://arxiv.org/abs/2405.14766
- **Reference count:** 40
- **Primary result:** Llama-3.3-70B-Instruct achieves best results on 8/16 public health tasks, with significant performance variation across tasks (some >80% micro-F1, others <60%)

## Executive Summary
This paper evaluates Large Language Models (LLMs) for public health classification and extraction tasks using zero-shot in-context learning. The study combines six externally annotated datasets with seven internally annotated datasets covering health burden, epidemiological risk factors, and public health interventions. Eleven open-weight LLMs (7-123 billion parameters) are compared against GPT-4 and GPT-4o models. Llama-3.3-70B-Instruct emerges as the top-performing open-weight model, achieving the best results on 8/16 tasks. Performance varies significantly across tasks, with some achieving over 80% micro-F1 and others below 60%, highlighting both the potential and limitations of LLMs for public health applications.

## Method Summary
The study employs zero-shot in-context learning with standardized prompting across 16 public health tasks using 11 open-weight LLMs and three GPT-4 series models. Models are evaluated on six externally annotated and seven internally annotated datasets covering classification and extraction tasks. The evaluation uses 20-80 validation-test splits, greedy decoding for reproducibility, and micro-F1/macro-F1 metrics. Few-shot prompting is tested on the most challenging tasks. Model performance is ranked by mean task rank, and results are compared across parameter sizes (7-123B) and model families.

## Key Results
- Llama-3.3-70B-Instruct outperforms other open-weight models on 8/16 tasks using micro-F1 scores
- Significant performance variation exists across tasks, with some achieving >80% micro-F1 and others <60%
- GPT-4 series models show comparable performance to Llama-3.3-70B-Instruct on 11 tasks, outperforming on MMLU Genetics
- Scaling from 8B to 70B parameters yields >10 percentage point micro-F1 improvements on complex tasks like MMLU Genetics and Health Advice
- Few-shot prompting substantially improves performance on the hardest tasks, particularly Contact Classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large language models can classify and extract structured information from free text in public health domains when provided with detailed protocols or definitions.
- **Mechanism:** Models use in-context learning from zero-shot or few-shot prompts that encode domain-specific definitions and classification rules, enabling structured reasoning without fine-tuning.
- **Core assumption:** Prompt format and content sufficiently convey task semantics, and pre-training data includes related context.
- **Evidence anchors:** Llama-3.3-70B-Instruct achieves best results on 8/16 tasks; Contact Type Classification requires detailed protocol application within prompts.
- **Break condition:** Performance drops sharply if prompts don't fully encode protocols or models lack relevant pre-training exposure.

### Mechanism 2
- **Claim:** Model performance scales with parameter size for complex classification and reasoning tasks in public health.
- **Mechanism:** Larger models have more representational capacity and richer latent associations, enabling better capture of complex domain-specific patterns.
- **Core assumption:** Scaling laws hold for domain-specific tasks and larger models' increased context handling improves extraction from longer documents.
- **Evidence anchors:** Significant variation across tasks with some open-weight LLMs scoring below 60% micro-F1; scaling from Llama-3-8B to Llama-3-70B leads to >10 percentage point micro-F1 increases on complex tasks.
- **Break condition:** If tasks are simple enough that small models saturate performance, or if task complexity exceeds even large models' reasoning limits.

### Mechanism 3
- **Claim:** Few-shot prompting with carefully chosen examples substantially improves performance on tasks requiring nuanced label distinctions.
- **Mechanism:** Providing examples of edge cases and correct protocol application helps models disambiguate similar classes and reduces errors from misinterpretation.
- **Core assumption:** Examples are representative of task difficulty spectrum and models can generalize without overfitting.
- **Evidence anchors:** Few-shot prompting significantly improves performance on hardest tasks; 10-shot Contact Classification yields substantial micro-F1 improvements across models.
- **Break condition:** If examples are poorly chosen or tasks require knowledge absent from pre-training, few-shot prompting may not help and can mislead.

## Foundational Learning

- **Concept:** Zero-shot and few-shot in-context learning
  - **Why needed here:** The study evaluates models without fine-tuning, relying on prompting to convey task semantics
  - **Quick check question:** What is the difference between zero-shot and few-shot prompting in the context of this paper?

- **Concept:** Micro-F1 and macro-F1 scoring
  - **Why needed here:** These metrics quantify classification and extraction performance, accounting for class imbalance and label distribution
  - **Quick check question:** When would micro-F1 differ significantly from macro-F1 in this dataset?

- **Concept:** Prompt fragility and output consistency
  - **Why needed here:** Small changes in prompt wording can cause large performance swings; robust prompt design is critical
  - **Quick check question:** What evidence in the paper suggests that smaller models are more sensitive to prompt wording?

## Architecture Onboarding

- **Component map:** Data ingestion -> annotation pipeline -> prompt template generation -> LLM inference (vLLM/HuggingFace) -> post-processing -> evaluation (F1 metrics)
- **Critical path:** Generate prompt -> send to model API -> receive raw output -> clean and normalize output -> compute F1 vs ground truth
- **Design tradeoffs:** FP16 vs INT4 quantization (memory vs performance, limited degradation observed); Greedy decoding vs sampling (reproducibility vs potential creativity); Zero-shot vs few-shot (simplicity vs performance on hard tasks)
- **Failure signatures:** Incorrect JSON format -> post-processing error; Out-of-vocabulary labels -> false negatives in exact matching; Misinterpretation of protocol -> systematic label errors
- **First 3 experiments:** 1) Run zero-shot prompt on NCBI Disease Extraction; verify micro-F1 calculation; 2) Test few-shot prompt on Contact Classification; compare F1 improvement; 3) Evaluate INT4 vs FP16 on Llama-3.1-70B for one task; measure performance delta

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific performance gap between open-weight models and GPT-4 series models for the most challenging public health tasks, and does this gap close with few-shot or chain-of-thought prompting?
- Basis in paper: The paper compares GPT-4 series models to open-weight models on 11 tasks, finding GPT-4 series models outperform open-weight models on MMLU Genetics but are otherwise comparable to Llama-3.3-70B-Instruct.
- Why unresolved: The paper doesn't provide detailed performance breakdowns for the most difficult tasks for GPT-4 series models, nor does it test few-shot or chain-of-thought prompting on these models.
- What evidence would resolve it: Direct performance comparisons of GPT-4 series models vs. open-weight models on the hardest tasks, with and without few-shot or chain-of-thought prompting.

### Open Question 2
- Question: How does the performance of open-weight models vary across different public health sub-domains (burden, risk factors, interventions) and what are the specific factors contributing to this variation?
- Basis in paper: The paper evaluates tasks across three sub-domains but doesn't provide a detailed breakdown of model performance within each sub-domain or identify specific factors driving performance differences.
- Why unresolved: The paper reports overall micro-F1 scores but lacks granular analysis of performance within each sub-domain and doesn't explore factors like task complexity, text type, or knowledge requirements.
- What evidence would resolve it: Detailed performance metrics for each model within each sub-domain, along with an analysis of task characteristics and their impact on model performance.

### Open Question 3
- Question: What is the impact of quantization (e.g., INT-4 AWQ) on model performance for public health tasks, and how does this vary across different model architectures and task types?
- Basis in paper: The paper evaluates INT-4 AWQ quantization on three Llama 3 family models and finds limited performance degradation.
- Why unresolved: The evaluation is limited to a small set of models and tasks, and doesn't explore the impact of quantization on other model architectures or task types.
- What evidence would resolve it: A comprehensive evaluation of quantization impact across a wider range of models and tasks, including different quantization methods and model architectures.

### Open Question 4
- Question: How do public health experts perceive the utility and trustworthiness of LLM-generated outputs for real-world public health tasks, and what are the key factors influencing their adoption of this technology?
- Basis in paper: The paper focuses on automated evaluations but acknowledges the need for expert evaluation of NLG tasks and mentions potential risks like bias and output fragility.
- Why unresolved: The paper doesn't involve public health experts in the evaluation process or explore their perspectives on LLM utility and trustworthiness.
- What evidence would resolve it: Surveys or interviews with public health experts on their experiences with LLM-generated outputs, including their perceptions of accuracy, usefulness, and potential risks.

## Limitations

- **Prompt design uncertainty:** Exact prompt wording and structure for each task are not disclosed, introducing significant reproducibility concerns
- **Internal dataset quality:** Seven internally annotated datasets lack detailed characterization of inter-annotator agreement and validation procedures
- **Uneven task distribution:** The 16 tasks span public health domains unevenly, with insufficient analysis of which task characteristics most strongly predict difficulty

## Confidence

- **High Confidence:** Llama-3.3-70B-Instruct outperforms other open-weight models on 8/16 tasks is well-supported by experimental results
- **Medium Confidence:** LLMs can support public health surveillance, research, and interventions is supported by positive results but tempered by significant performance variation
- **Low Confidence:** Larger models consistently outperform smaller ones for complex public health tasks lacks strong empirical support across all task types

## Next Checks

1. **Prompt ablation study:** Systematically vary prompt wording, example selection, and instruction format for 3-4 representative tasks to quantify performance sensitivity

2. **Annotation quality assessment:** Conduct inter-annotator agreement analysis and annotation guideline validation for internally annotated datasets

3. **Task characteristic analysis:** Perform correlation analysis between task difficulty and quantifiable task features (average document length, number of classes, required reasoning steps)