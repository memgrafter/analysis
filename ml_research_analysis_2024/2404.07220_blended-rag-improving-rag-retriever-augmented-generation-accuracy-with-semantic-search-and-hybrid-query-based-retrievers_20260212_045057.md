---
ver: rpa2
title: 'Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with
  Semantic Search and Hybrid Query-Based Retrievers'
arxiv_id: '2404.07220'
source_url: https://arxiv.org/abs/2404.07220
tags:
- dataset
- accuracy
- search
- retrieval
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called 'Blended RAG' to improve the
  accuracy of RAG (Retriever-Augmented Generation) systems. The main idea is to use
  a combination of semantic search techniques, such as dense vector indexes and sparse
  encoder indexes, blended with hybrid query strategies.
---

# Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers

## Quick Facts
- arXiv ID: 2404.07220
- Source URL: https://arxiv.org/abs/2404.07220
- Authors: Kunal Sawarkar; Abhilasha Mangal; Shivam Raj Solanki
- Reference count: 14
- Key outcome: Improves RAG accuracy by blending semantic search techniques with hybrid query strategies, achieving superior results on NQ, TREC-COVID, and SQUAD datasets.

## Executive Summary
This paper introduces "Blended RAG," a method that enhances Retrieval-Augmented Generation (RAG) systems by integrating semantic search techniques with hybrid query strategies. The approach combines dense vector indexes and sparse encoder indexes, leveraging metadata and multiple data fields to improve retrieval accuracy. The authors demonstrate that their method outperforms traditional RAG systems on benchmark datasets and even surpasses fine-tuning performance in some cases.

## Method Summary
The paper proposes a progressive methodology that constructs three types of indexes—BM25 for keyword-based search, KNN for dense vector-based search, and Elastic Learned Sparse Encoder (ELSER) for sparse encoder-based semantic search. These indexes are evaluated using various hybrid query strategies across multiple fields to identify the best-performing combinations. The optimal retrievers are then integrated into a RAG pipeline using the FLAN-T5-XXL language model, resulting in improved generative question-answering performance without dataset-specific fine-tuning.

## Key Results
- Hybrid queries leveraging the Sparse Encoder with Best Fields demonstrate the highest efficacy across all index types at 78% accuracy.
- The hybrid query incorporating Sparse Encoder with Best Fields achieves a 98% top-10 retrieval accuracy.
- Blended RAG showcases enhanced performance for Generative Q&A with F1 scores higher by 50%, even without dataset-specific fine-tuning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Blending dense vector and sparse encoder indices with hybrid query strategies improves retrieval accuracy.
- Mechanism: The method combines semantic search capabilities of dense vectors (for capturing deep semantic relationships) with the precision of sparse encoders (for interpretable term relationships). Hybrid queries (e.g., Best Fields, Cross Fields) allow leveraging metadata and multiple data fields to find the most relevant documents.
- Core assumption: The combination of semantic and keyword-based search captures a more complete representation of query intent than either method alone.
- Evidence anchors:
  - [abstract]: "We propose the 'Blended RAG' method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies."
  - [section]: "Our methodology unfolds in a sequence of progressive steps... escalating to hybrid queries that amalgamate diverse search techniques across multiple fields..."
  - [corpus]: Weak evidence; corpus shows related work on hybrid retrieval but lacks direct benchmarks for this specific blended approach.
- Break condition: If datasets lack metadata or structured fields, hybrid queries lose their advantage and retrieval accuracy drops.

### Mechanism 2
- Claim: Using 'Best Fields' hybrid query strategy consistently yields the highest retrieval accuracy across datasets.
- Mechanism: Best Fields queries aggregate matching terms within a single field, prioritizing field-specific relevance. This is especially effective when documents contain rich metadata or when certain fields are more informative.
- Core assumption: Document fields are structured and metadata-rich enough that focusing on the best-matching field improves precision.
- Evidence anchors:
  - [section]: "In particular, hybrid queries that leverage the Sparse EncodeR utilizing Best Fields demonstrate the highest efficacy across all index types at 78% accuracy."
  - [section]: "Notably, the hybrid query incorporating Sparse Encoder with Best Fields demonstrates a 98% top-10 retrieval accuracy..."
  - [corpus]: Weak; no direct evidence in corpus about Best Fields superiority.
- Break condition: In unstructured datasets or those without metadata, Best Fields strategy underperforms compared to Cross Fields or Most Fields.

### Mechanism 3
- Claim: Blended Retrievers improve RAG system performance without requiring fine-tuning on target datasets.
- Mechanism: By optimizing the retriever component with semantic search and hybrid queries, the RAG system receives more relevant context for the LLM to generate answers, reducing the need for dataset-specific fine-tuning.
- Core assumption: The LLM can effectively utilize the improved context provided by the enhanced retriever to produce accurate answers in a zero-shot setting.
- Evidence anchors:
  - [abstract]: "We further extend such a 'Blended Retriever' to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance."
  - [section]: "Consequently, as shown in Table IV, our Blended RAG showcases enhanced performance for Generative Q&A with F1 scores higher by 50%, even without dataset-specific fine-tuning."
  - [corpus]: No direct evidence in corpus; related papers focus on hybrid retrieval but not zero-shot RAG performance.
- Break condition: If the LLM is too small or lacks sufficient capacity, even the best context may not yield accurate answers.

## Foundational Learning

- Concept: Semantic search vs keyword-based search
  - Why needed here: Understanding the difference is critical to grasp why blending dense and sparse indices improves accuracy.
  - Quick check question: What is the main advantage of semantic search over keyword-based search?

- Concept: Hybrid query strategies (Best Fields, Cross Fields, Most Fields, Phrase Prefix)
  - Why needed here: These strategies determine how the retriever combines multiple indices and fields to return results.
  - Quick check question: When would you prefer Best Fields over Cross Fields in a hybrid query?

- Concept: NDCG@10 and F1/EM evaluation metrics
  - Why needed here: These metrics are used to benchmark retrieval and answer generation performance in the paper.
  - Quick check question: What does NDCG@10 measure in information retrieval?

## Architecture Onboarding

- Component map: Corpus → Index (BM25, Dense Vector, Sparse Encoder) → Hybrid Query Engine → Retriever → LLM → Answer
- Critical path: Query → Hybrid Query Processing → Multi-index Retrieval → Top-k Document Selection → Context Assembly → LLM Generation
- Design tradeoffs:
  - Dense vectors: Fast indexing, slower queries, high memory (e.g., 50GB for HotPotQA)
  - Sparse encoders: Slower indexing, fast queries, lower memory (e.g., 10.5GB for HotPotQA)
  - Hybrid queries: More complex but higher accuracy when metadata is available
- Failure signatures:
  - Low retrieval accuracy despite high query relevance → Metadata missing or fields poorly defined
  - Slow query performance → Dense vector index too large; consider sparse encoder or federated search
  - Inconsistent RAG answers → Retriever returning irrelevant context; revisit hybrid query strategy
- First 3 experiments:
  1. Run baseline BM25 retrieval on a sample dataset; record NDCG@10.
  2. Replace BM25 with KNN dense vector index; compare NDCG@10 and memory usage.
  3. Add sparse encoder index with Best Fields hybrid query; measure improvement in retrieval and RAG F1 scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective metrics for evaluating Generative Q&A systems in terms of human alignment and relevance?
- Basis in paper: [explicit] The paper states that NDCG@10 scores for Retriever and F1, EM scores for RAG are commonly used metrics but are poor proxies of Generative Q&A systems for human alignment. Better metrics to evaluate the RAG system is a key area of future work.
- Why unresolved: Current metrics do not adequately capture the quality of generated answers in relation to human expectations and understanding.
- What evidence would resolve it: Development and validation of new evaluation metrics that better align with human judgment of answer quality, relevance, and coherence.

### Open Question 2
- Question: How can Blended RAG systems be optimized for datasets without metadata?
- Basis in paper: [explicit] The paper mentions that Blended Retrievers are less effective for datasets without metadata, such as CoQA, and that sparse encoder-based semantic searches still yield the most favorable outcomes.
- Why unresolved: The current Blended RAG approach relies heavily on metadata to enhance retrieval accuracy, which is not always available.
- What evidence would resolve it: Experiments and case studies demonstrating improved retrieval and generation performance using Blended RAG on metadata-poor datasets, potentially through alternative data enrichment techniques.

### Open Question 3
- Question: What are the trade-offs between sparse and dense vector indices in terms of computational efficiency and retrieval accuracy?
- Basis in paper: [explicit] The paper discusses the trade-offs between sparse and dense vector indices, noting that dense vector indexing is rapid but querying is slow, while sparse vector indexing is slower but querying is faster. It also highlights the significant difference in storage requirements.
- Why unresolved: The balance between computational efficiency and retrieval accuracy needs further exploration to determine the optimal use cases for each type of index.
- What evidence would resolve it: Comparative studies and benchmarks showing the performance and efficiency of sparse versus dense vector indices across various datasets and use cases.

## Limitations

- The paper lacks detailed hyperparameter specifications for constructing indexes and hybrid queries, making exact reproduction difficult.
- The claim of surpassing fine-tuning performance is striking but lacks direct comparative evidence against fine-tuned baselines on the same datasets.
- The computational cost implications, especially memory usage with dense vectors, are noted but not thoroughly analyzed.

## Confidence

- **High Confidence:** The general approach of blending semantic search with hybrid queries is sound and well-supported by related work in the field.
- **Medium Confidence:** The empirical results on retrieval accuracy and RAG performance are promising but limited by the lack of detailed implementation specifics.
- **Low Confidence:** The claim of surpassing fine-tuning performance and the generalizability of the approach to other domains or languages are not fully substantiated.

## Next Checks

1. **Reproduce Baseline Retrieval:** Recreate the BM25, dense vector, and sparse encoder indexes on a subset of the NQ dataset to verify the reported retrieval accuracy improvements.
2. **Ablation Study on Hybrid Queries:** Systematically test each hybrid query strategy (Best Fields, Cross Fields, Most Fields, Phrase Prefix) in isolation to determine their individual contributions to retrieval accuracy.
3. **Fine-tuning Comparison:** Implement a fine-tuned baseline on SQUAD using the same LLM (FLAN-T5-XXL) and compare its F1 score against the Blended RAG approach to validate the zero-shot superiority claim.