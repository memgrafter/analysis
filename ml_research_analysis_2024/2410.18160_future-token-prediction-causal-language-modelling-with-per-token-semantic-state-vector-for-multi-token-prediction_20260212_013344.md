---
ver: rpa2
title: Future Token Prediction -- Causal Language Modelling with Per-Token Semantic
  State Vector for Multi-Token Prediction
arxiv_id: '2410.18160'
source_url: https://arxiv.org/abs/2410.18160
tags:
- token
- tokens
- decoder
- embedding
- future
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Future Token Prediction (FTP) extends causal language model pretraining
  to predict multiple future tokens instead of just the next one. FTP uses a large
  encoder to generate per-token embeddings, which are expansively projected into a
  pseudo-sequence and attended to by a small decoder to predict the next N tokens.
---

# Future Token Prediction -- Causal Language Modelling with Per-Token Semantic State Vector for Multi-Token Prediction

## Quick Facts
- arXiv ID: 2410.18160
- Source URL: https://arxiv.org/abs/2410.18160
- Authors: Nicholas Walker
- Reference count: 27
- Key outcome: FTP models outperform GPT on topic coherence, sentence-to-prompt similarity, text classification accuracy, and coding tasks

## Executive Summary
Future Token Prediction (FTP) extends causal language modeling by predicting multiple future tokens instead of just the next one. The approach uses a large encoder to generate per-token embeddings that are expansively projected into a pseudo-sequence, which a small decoder attends to for multi-token prediction. FTP embeddings show smoother variation over text sequences and better capture topic semantics compared to standard GPT models. The method demonstrates improved topic coherence and sentence-to-prompt similarity in generated text, along with higher text classification accuracy and significantly better performance on coding tasks.

## Method Summary
FTP extends causal language model pretraining to predict multiple future tokens simultaneously. A large encoder generates per-token embeddings from the input text, which are then expansively projected into a pseudo-sequence. A small decoder attends to this pseudo-sequence to predict the next N tokens. This architecture allows FTP embeddings to vary smoothly over text sequences and better capture topic semantics compared to standard GPT models. The approach maintains causal language modeling principles while enabling richer semantic representations through multi-token prediction.

## Key Results
- FTP models show improved topic coherence and better sentence-to-prompt similarity in generated text
- Higher text classification accuracy compared to standard GPT models
- Significantly outperform GPT models on coding tasks

## Why This Works (Mechanism)
The expansively projected pseudo-sequence approach allows the decoder to attend to rich semantic representations from multiple tokens simultaneously, capturing broader context and topic information. By predicting multiple tokens at once rather than just the next token, the model learns more robust semantic relationships and smoother transitions between text segments. The large encoder-small decoder architecture enables efficient processing while maintaining semantic richness in the representations.

## Foundational Learning
1. Causal Language Modeling (CLM): Why needed - Foundation for autoregressive text generation; Quick check - Model predicts next token given previous context
2. Per-token embeddings: Why needed - Capture semantic meaning at individual token level; Quick check - Each token has unique embedding vector
3. Pseudo-sequence projection: Why needed - Enable multi-token attention in decoder; Quick check - Transformed embeddings maintain semantic relationships
4. Multi-token prediction: Why needed - Learn richer semantic patterns than next-token prediction; Quick check - Model predicts N future tokens simultaneously
5. Encoder-decoder architecture: Why needed - Separate semantic extraction from generation; Quick check - Large encoder, small decoder configuration
6. Semantic state vectors: Why needed - Represent topic and context information; Quick check - Embeddings vary smoothly over text sequences

## Architecture Onboarding
Component map: Input text -> Large Encoder -> Expansive Projection -> Pseudo-sequence -> Small Decoder -> N future tokens
Critical path: The encoder generates per-token embeddings, which are projected into pseudo-sequence space where the decoder attends to predict multiple future tokens
Design tradeoffs: Large encoder provides rich semantic representations but increases computational cost; small decoder improves efficiency but may limit capacity
Failure signatures: Poor topic coherence, abrupt semantic shifts between generated tokens, degraded performance on long-range dependencies
First experiments: 1) Test single-token prediction capability 2) Verify smooth embedding variation over text 3) Validate multi-token prediction accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns for expansively projected pseudo-sequence approach to longer sequences
- Lack of ablation studies on large encoder-small decoder configuration impact
- Coding task results lack dataset size and diversity specifications

## Confidence
- Topic coherence and sentence-to-prompt similarity improvements: Medium (qualitative assessments need quantitative validation)
- Text classification accuracy improvements: High (clear comparative metrics provided)
- Causal relationship between per-token embeddings and performance gains: Partially speculative (lacks controlled isolation experiments)

## Next Checks
1. Conduct controlled ablation studies comparing FTP with different encoder-decoder size ratios while keeping the multi-token prediction mechanism constant
2. Test FTP models on diverse and larger-scale coding datasets to verify the robustness of the coding task improvements
3. Implement quantitative metrics for topic coherence and semantic similarity to complement the qualitative assessments provided in the paper