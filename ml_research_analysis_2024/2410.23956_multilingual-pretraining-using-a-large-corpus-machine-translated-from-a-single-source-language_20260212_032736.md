---
ver: rpa2
title: Multilingual Pretraining Using a Large Corpus Machine-Translated from a Single
  Source Language
arxiv_id: '2410.23956'
source_url: https://arxiv.org/abs/2410.23956
tags:
- data
- arxiv
- language
- pretraining
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the feasibility of creating high-quality multilingual
  language models by translating a high-quality English corpus into other languages.
  The authors propose a method where they translate FineWeb-Edu, a high-quality English
  web dataset, into French, German, and Spanish, creating a balanced multilingual
  pretraining corpus called TransWeb-Edu.
---

# Multilingual Pretraining Using a Large Corpus Machine-Translated from a Single Source Language

## Quick Facts
- arXiv ID: 2410.23956
- Source URL: https://arxiv.org/abs/2410.23956
- Authors: Jiayi Wang; Yao Lu; Maurice Weber; Max Ryabinin; Yihong Chen; Raphael Tang; Pontus Stenetorp
- Reference count: 20
- Primary result: Machine-translated pretraining from a single high-quality source language matches or exceeds state-of-the-art multilingual models

## Executive Summary
This paper investigates the feasibility of creating high-quality multilingual language models by translating a high-quality English corpus into other languages. The authors propose a method where they translate FineWeb-Edu, a high-quality English web dataset, into French, German, and Spanish, creating a balanced multilingual pretraining corpus called TransWeb-Edu. They then pretrain a 1.3B-parameter model, CuatroLLM, from scratch on this translated dataset. The results show that CuatroLLM matches or outperforms state-of-the-art multilingual models like Llama3.2 and Gemma2 across five non-English reasoning tasks, despite using an order of magnitude less data. The authors also demonstrate that with additional domain-specific pretraining, CuatroLLM surpasses the state of the art in multilingual reasoning.

## Method Summary
The authors create a multilingual pretraining corpus by translating a high-quality English dataset (FineWeb-Edu) into three target languages (French, German, Spanish) using an advanced machine translation model (GPT-4o). They construct a balanced dataset called TransWeb-Edu, containing approximately 40 billion tokens across the four languages. A 1.3B-parameter model, CuatroLLM, is then pretrained from scratch on this translated corpus. The authors employ curriculum learning during pretraining, starting with English-only data before gradually introducing the translated data. They evaluate CuatroLLM on five non-English reasoning tasks and compare its performance against state-of-the-art multilingual models. Additionally, they conduct domain-specific finetuning to demonstrate improved performance on multilingual reasoning tasks.

## Key Results
- CuatroLLM matches or outperforms state-of-the-art multilingual models (Llama3.2, Gemma2) across five non-English reasoning tasks
- Achieves competitive performance despite using an order of magnitude less data than comparison models
- Domain-specific pretraining further improves CuatroLLM's performance, surpassing state-of-the-art models in multilingual reasoning

## Why This Works (Mechanism)
The approach leverages the high quality and domain relevance of the source English corpus, combined with advanced machine translation capabilities, to create a rich multilingual pretraining dataset. By starting with a well-curated English corpus and translating it into multiple languages, the method ensures consistency in content and domain across languages while benefiting from the superior quality of the source material. The use of curriculum learning during pretraining helps the model first learn general language patterns in English before adapting to the translated multilingual data, potentially leading to better cross-lingual transfer and understanding.

## Foundational Learning
- **Multilingual pretraining**: Training language models on multiple languages simultaneously to improve cross-lingual understanding and transfer. Why needed: To create models that can process and generate text in multiple languages effectively.
- **Curriculum learning**: A training strategy that presents examples in a meaningful order, typically starting with easier examples and gradually increasing difficulty. Why needed: To help the model learn foundational patterns before tackling more complex multilingual tasks.
- **Machine translation quality**: The accuracy and fluency of translated text, which directly impacts the quality of the pretraining corpus. Why needed: To ensure that the translated data maintains the semantic and stylistic properties of the source language.
- **Cross-lingual transfer**: The ability of a model trained on one language to perform well on tasks in other languages. Why needed: To demonstrate that knowledge gained from the source language can be effectively applied to target languages.

## Architecture Onboarding
- **Component map**: FineWeb-Edu (English corpus) -> Machine translation (GPT-4o) -> TransWeb-Edu (multilingual corpus) -> CuatroLLM (1.3B model) -> Evaluation tasks
- **Critical path**: High-quality English corpus translation -> Balanced multilingual corpus creation -> Curriculum-based pretraining -> Task-specific evaluation
- **Design tradeoffs**: Using a single source language simplifies corpus creation but may limit linguistic diversity; machine translation quality directly impacts downstream performance; smaller model size (1.3B) vs. larger state-of-the-art models (8B+)
- **Failure signatures**: Poor translation quality leading to noisy pretraining data; imbalanced language representation causing bias; curriculum learning misconfiguration resulting in suboptimal learning curves
- **First experiments**: 1) Compare translation quality across different MT models, 2) Test curriculum learning with varying difficulty progressions, 3) Evaluate model performance on individual languages vs. multilingual settings

## Open Questions the Paper Calls Out
- Whether the approach generalizes to other source languages beyond English, particularly those with different linguistic properties
- The performance of this method on lower-resource languages not included in the current study
- The potential impact of using different domain-specific corpora or less curated web data as the source

## Limitations
- The approach was only tested with English as the source language, leaving uncertainty about performance with other source languages
- Limited evaluation to three target languages (French, German, Spanish), constraining generalizability to other language families
- Reliance on a specific high-quality dataset (FineWeb-Edu) raises questions about performance with alternative or less curated data sources

## Confidence
- High confidence: The methodology for dataset creation and model pretraining is clearly described and reproducible
- Medium confidence: The claim that CuatroLLM matches or outperforms state-of-the-art models, given the limited comparison set and task diversity
- Medium confidence: The assertion that the approach is cost-effective and scalable, as this depends on the quality and availability of translation infrastructure

## Next Checks
1. Test the approach with additional source languages (e.g., Chinese, Arabic) to verify the hypothesis that English is not uniquely suited for this method
2. Expand evaluation to include lower-resource languages and different language families to assess generalizability
3. Compare performance using alternative translation approaches (e.g., commercial vs. open-source translation models) to determine sensitivity to translation quality