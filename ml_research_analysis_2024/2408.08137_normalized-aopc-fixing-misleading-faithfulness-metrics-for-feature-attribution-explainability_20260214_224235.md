---
ver: rpa2
title: 'Normalized AOPC: Fixing Misleading Faithfulness Metrics for Feature Attribution
  Explainability'
arxiv_id: '2408.08137'
source_url: https://arxiv.org/abs/2408.08137
tags:
- aopc
- attention
- feature
- scores
- decompx
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current faithfulness metrics for feature attribution methods are
  unreliable for cross-model comparisons due to model-specific lower and upper bounds.
  We introduce Normalized AOPC (NAOPC), which normalizes faithfulness scores to ensure
  consistent limits across all models.
---

# Normalized AOPC: Fixing Misleading Faithfulness Metrics for Feature Attribution Explainability

## Quick Facts
- arXiv ID: 2408.08137
- Source URL: https://arxiv.org/abs/2408.08137
- Reference count: 21
- Current faithfulness metrics are unreliable for cross-model comparisons due to model-specific bounds

## Executive Summary
Current faithfulness metrics for feature attribution methods suffer from model-specific lower and upper bounds, making cross-model comparisons unreliable. This paper introduces Normalized AOPC (NAOPC), which normalizes faithfulness scores to ensure consistent limits across all models. The method uses exact search or beam search to identify these bounds, enabling fair comparisons. Experiments demonstrate that NAOPC significantly alters model faithfulness rankings while preserving feature attribution method rankings, with beam search size 5 providing an optimal balance between accuracy and efficiency.

## Method Summary
The paper proposes NAOPC as a normalization approach for AOPC (Area Over the Perturbation Curve) faithfulness metrics. NAOPC addresses the fundamental issue that AOPC scores are bounded by model-specific limits, making cross-model comparisons unreliable. The method identifies the lower and upper AOPC limits through either exact search or beam search, then applies min-max normalization to produce comparable scores. The beam search approach (NAOPCbeam) offers computational efficiency with O(B · N^2) complexity while maintaining accuracy.

## Key Results
- AOPC scores are not comparable across different models due to model-specific lower and upper bounds
- NAOPC significantly changes model faithfulness rankings while preserving feature attribution method rankings
- A beam size of 5 provides accurate and efficient approximation of exact bounds
- Kendall correlation between AOPC and NAOPC beam is 0.87-0.93 for comprehensiveness and 0.43-0.72 for sufficiency

## Why This Works (Mechanism)

### Mechanism 1
AOPC scores are not comparable across different models due to model-specific lower and upper bounds. Different models have varying feature dependencies and interaction patterns, leading to different minimum and maximum possible AOPC scores for the same input. The number of features a model relies on and its feature interactions directly impact the best possible comprehensiveness and sufficiency scores.

### Mechanism 2
Normalization through NAOPC enables consistent cross-model evaluations by standardizing the score range. NAOPC applies min-max normalization to AOPC scores using the model-specific lower and upper limits, ensuring all models have the same score range. The lower and upper AOPC limits can be accurately identified for each model and input.

### Mechanism 3
NAOPCbeam provides an efficient approximation of NAOPCexact while maintaining accuracy. NAOPCbeam uses beam search to find the lower and upper AOPC limits, limiting the search space and achieving O(B · N^2) time complexity. Beam search with a reasonable beam size can approximate the exact limits without significant loss of accuracy.

## Foundational Learning

- **Min-max normalization**: Transforms AOPC scores from model-specific ranges to a standardized range for comparison. *Quick check: If a model has AOPC limits of 0.2 and 0.8, what is the NAOPC score for an AOPC of 0.5?*

- **Beam search algorithm**: Efficiently approximates the exact AOPC limits without exhaustive search. *Quick check: What is the time complexity of NAOPCbeam with beam size B and N features?*

- **Feature attribution methods**: Determine how different methods rank features and how this affects AOPC scores. *Quick check: How do gradient-based and perturbation-based attribution methods differ in their approach?*

## Architecture Onboarding

- **Component map**: Feature attribution method → AOPC score calculation → Limit identification → Normalization → Final score
- **Critical path**: Feature attribution → AOPC calculation → Limit identification → Normalization → Final score
- **Design tradeoffs**: Exact search provides accuracy but is computationally expensive; beam search is faster but may sacrifice some accuracy
- **Failure signatures**: Large discrepancies between NAOPCbeam and NAOPCexact results indicate beam search limitations; inconsistent rankings across models suggest normalization issues
- **First 3 experiments**:
  1. Compare AOPC scores across models with identical architectures to verify model-specific bounds
  2. Test NAOPCbeam with different beam sizes to find the optimal balance between speed and accuracy
  3. Evaluate the impact of normalization on model faithfulness rankings using real-world datasets

## Open Questions the Paper Calls Out

### Open Question 1
How does the accuracy of NAOPCbeam scale with beam size beyond 50, and what is the optimal beam size for balancing computational efficiency and accuracy? The paper tested beam sizes up to 50 and found that increasing beyond 5 had minimal impact, but stopped at 50 due to computational budget. This suggests potential for further investigation into larger beam sizes.

### Open Question 2
How does NAOPC perform when applied to non-text domains such as image classification or tabular data, where feature interactions and model architectures may differ significantly from NLP models? The paper focuses exclusively on NLP models (BERT and RoBERTa) and text classification tasks, noting that findings are limited to this domain.

### Open Question 3
Can NAOPC be extended to other faithfulness metrics beyond AOPC, such as sensitivity-n or decision-flip metrics, to address similar issues of model-specific bounds in those measures? The paper discusses the limitations of AOPC but mentions other faithfulness metrics in the Related Work section, suggesting potential for broader application of the normalization concept.

## Limitations
- The paper assumes that model-specific AOPC bounds can be accurately identified through exhaustive or beam search, which may not hold for extremely complex models with high-dimensional feature interactions
- The effectiveness of beam search (NAOPCbeam) is claimed but not extensively validated across diverse model architectures and datasets, potentially limiting generalizability
- The drop in correlation for sufficiency metrics (0.43-0.72) compared to comprehensiveness (0.87-0.93) suggests that NAOPC may not equally improve all faithfulness evaluations

## Confidence

- **High confidence**: Identification of AOPC's model-specific bounds problem and the need for normalization
- **Medium confidence**: Mathematical formulation of NAOPC and its theoretical correctness
- **Medium confidence**: Experimental results showing improved cross-model comparisons, though broader validation would strengthen this

## Next Checks
1. Test NAOPC on models with known feature dependencies (e.g., attention-based models) to verify if the normalization correctly captures varying levels of feature importance
2. Compare NAOPC rankings with causal-based faithfulness metrics to validate whether normalized scores align with ground-truth feature importance
3. Conduct ablation studies on beam size and search depth to quantify the tradeoff between computational efficiency and accuracy in identifying AOPC bounds