---
ver: rpa2
title: 'SiT: Symmetry-Invariant Transformers for Generalisation in Reinforcement Learning'
arxiv_id: '2406.15025'
source_url: https://arxiv.org/abs/2406.15025
tags:
- local
- graph
- attention
- global
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Symmetry-Invariant Transformers (SiTs) to
  improve generalization in reinforcement learning by leveraging graph-based symmetry
  preservation within the attention mechanism. The key innovation is Graph Symmetric
  Attention (GSA), which constrains attention weights to respect local and global
  symmetries of a 2D grid, yielding both invariant and equivariant representations.
---

# SiT: Symmetry-Invariant Transformers for Generalisation in Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.15025
- Source URL: https://arxiv.org/abs/2406.15025
- Authors: Matthias Weissenbacher; Rishabh Agarwal; Yoshinobu Kawahara
- Reference count: 40
- Key outcome: Up to 9× improvement over standard ViTs in RL tasks with better sample efficiency

## Executive Summary
SiT (Symmetry-Invariant Transformers) introduces Graph Symmetric Attention (GSA) to improve generalization in reinforcement learning by preserving graph symmetries within the attention mechanism. This approach constrains attention weights to respect local and global symmetries of 2D grids, yielding both invariant and equivariant representations. The model demonstrates significant performance improvements across multiple RL benchmarks including MiniGrid, Procgen, and Atari, while also showing strong performance on vision tasks like CIFAR-10.

## Method Summary
SiT leverages graph-based symmetry preservation through Graph Symmetric Attention (GSA), which modifies traditional self-attention by constraining attention weights via a graph topology matrix. The architecture incorporates both local and global GSA modules to capture interplay between neighborhood and global symmetries. A flip symmetry breaking layer handles dead-end situations by preserving rotation symmetry while breaking flip symmetry. The method is evaluated across multiple RL environments using different training algorithms including IMPALA, PPO with DrAC, and SAC.

## Key Results
- Up to 9× improvement over standard ViTs in RL tasks
- Better sample efficiency compared to CNNs with similar parameter counts
- Particularly effective in environments with rotational symmetry
- Offers novel alternative to data augmentation for improving generalization

## Why This Works (Mechanism)

### Mechanism 1
Graph Symmetric Attention (GSA) preserves graph symmetries by constraining attention weights through a graph topology matrix G. GSA modifies the self-attention equation by replacing the attention score matrix with a symmetrized version that is multiplied element-wise with G, ensuring only edges in the graph topology contribute to attention. This results in invariant or equivariant representations depending on the choice of G.

### Mechanism 2
SiTs incorporate equivariance to handle dead-end situations by breaking flip symmetries while preserving rotation symmetries. This is achieved through a flip symmetry breaking layer that sums over directed triangle sub-graphs, which changes orientation under flips but remains invariant under rotations. This allows the model to maintain the meaning of directions while still benefiting from rotational symmetry.

### Mechanism 3
SiTs leverage the interplay between local and global information by employing both local and global GSA modules. Local GSA is applied to specific patches or their surrounding neighborhoods, while global GSA is applied to entire image patches. This allows the model to capture both local symmetries within a neighborhood and global symmetries throughout the entire image.

## Foundational Learning

- Concept: Graph symmetries (rotations, flips, translations)
  - Why needed here: Understanding graph symmetries is crucial for designing the graph topology matrix G and implementing the GSA mechanism.
  - Quick check question: What are the different types of graph symmetries that can be modeled by a graph topology matrix G?

- Concept: Equivariance and invariance in neural networks
  - Why needed here: Understanding equivariance and invariance is essential for designing SiTs that can handle dead-end situations and generalize to new situations.
  - Quick check question: What is the difference between equivariance and invariance in neural networks, and how do they relate to the flip symmetry breaking layer in SiTs?

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding attention mechanisms is crucial for implementing the GSA mechanism and integrating it into the SiT architecture.
  - Quick check question: How does the self-attention mechanism work in transformers, and how is it modified in the GSA mechanism?

## Architecture Onboarding

- Component map: Graph Symmetric Attention (GSA) module -> Flip symmetry breaking layer -> Local and global GSA modules -> SiT architecture
- Critical path: GSA module → Flip symmetry breaking layer → Local and global GSA modules → SiT architecture
- Design tradeoffs:
  - Local vs. global attention window sizes
  - Choice of graph topology matrix G
  - Incorporation of equivariance vs. invariance
- Failure signatures:
  - Poor generalization to new or slightly different situations
  - Inability to handle dead-end situations
  - Overfitting to specific augmentations
- First 3 experiments:
  1. Implement the GSA module with a simple graph topology matrix G and test it on a small dataset.
  2. Add the flip symmetry breaking layer and test its impact on the model's ability to handle dead-end situations.
  3. Integrate the local and global GSA modules into the SiT architecture and evaluate its performance on a larger dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope focuses on relatively simple environments like MiniGrid and specific Procgen levels rather than more complex RL benchmarks
- Computational overhead of local GSA layers may limit scalability to larger, more complex environments
- Exact implementation details for graph matrix construction and symmetry breaking layers remain underspecified

## Confidence

- High confidence: The core mechanism of Graph Symmetric Attention (GSA) and its ability to preserve graph symmetries
- Medium confidence: The effectiveness of flip symmetry breaking in handling dead-end situations
- Medium confidence: Generalization benefits across diverse RL tasks given evaluation was primarily on relatively simple environments

## Next Checks

1. Implement ablation studies comparing SiT performance with and without the flip symmetry breaking layer across various dead-end scenarios to quantify its specific contribution
2. Test SiT performance on more complex RL environments with multiple symmetry types (e.g., DeepMind Control Suite tasks) to evaluate scalability
3. Conduct computational efficiency analysis comparing SiT against CNN and ViT baselines, measuring both wall-clock time and memory usage during training and inference