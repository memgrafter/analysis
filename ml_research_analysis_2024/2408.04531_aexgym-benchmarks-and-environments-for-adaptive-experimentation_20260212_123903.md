---
ver: rpa2
title: 'AExGym: Benchmarks and Environments for Adaptive Experimentation'
arxiv_id: '2408.04531'
source_url: https://arxiv.org/abs/2408.04531
tags:
- sampling
- treatment
- algorithms
- adaptive
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AExGym, a benchmark and open-source library
  for adaptive experimentation, addressing practical challenges in A/B testing that
  hinder the adoption of adaptive designs. While adaptive methods can improve statistical
  power in theory, they struggle with issues like non-stationarity, batched/delayed
  feedback, multiple outcomes/objectives, and external validity.
---

# AExGym: Benchmarks and Environments for Adaptive Experimentation

## Quick Facts
- arXiv ID: 2408.04531
- Source URL: https://arxiv.org/abs/2408.04531
- Authors: Jimmy Wang; Ethan Che; Daniel R. Jiang; Hongseok Namkoong
- Reference count: 40
- Primary result: Introduces AExGym, a benchmark and open-source library for adaptive experimentation addressing practical challenges that hinder adoption of adaptive designs.

## Executive Summary
AExGym addresses the gap between theoretical promises of adaptive experimentation and practical barriers to implementation. While adaptive methods can theoretically improve statistical power in A/B testing, they struggle with real-world challenges like non-stationarity, batched/delayed feedback, multiple outcomes/objectives, and external validity. The framework provides a unified environment using real-world datasets to evaluate algorithms under realistic constraints, modeling adaptive experiments as Markov decision processes. The library demonstrates that simpler methods like uniform sampling can outperform sophisticated algorithms in practical settings, emphasizing the need for inductive algorithmic development based on empirical benchmarking.

## Method Summary
AExGym formulates adaptive experiments as Markov decision processes where the agent selects actions (treatments) and observes outcomes under practical constraints. The framework provides modular environments using real-world datasets from domains including microcredit studies, reemployment programs, and field experiments. It implements various bandit and best-arm identification algorithms (Uniform, Linear Thompson Sampling, Linear Top-Two Thompson Sampling, Linear UCB, Linear Expected Improvement) and evaluates them across multiple objectives including simple regret, policy regret, cumulative regret, and constraint satisfaction. The library allows flexible specification of experimental settings such as non-stationarity patterns, batched feedback, and budget constraints.

## Key Results
- Adaptive algorithms often perform worse than uniform sampling under practical constraints like batched feedback and limited sampling opportunities
- Non-stationarity severely degrades performance of adaptive algorithms, with batch sizes significantly impacting effectiveness
- Multiple objective optimization reveals fundamental trade-offs between cumulative regret and policy regret that simple benchmarks miss
- Real-world datasets reveal practical barriers that synthetic benchmarks fail to capture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AExGym provides a unified framework that models practical adaptive experimentation challenges using real-world datasets, enabling more realistic algorithm evaluation than synthetic benchmarks.

## Foundational Learning

### Concept 1: Markov Decision Process formulation
- Why needed: Provides mathematical framework for modeling sequential decision-making in adaptive experiments
- Quick check: Can you identify states, actions, rewards, and transition dynamics in the experimental setting?

### Concept 2: Non-stationarity in experimental contexts
- Why needed: Real-world environments change over time, violating stationarity assumptions of many adaptive algorithms
- Quick check: Can you identify temporal patterns and drift in the provided datasets?

### Concept 3: Multiple objective optimization
- Why needed: Practical experiments often balance competing goals like power, cost, and feasibility
- Quick check: Can you formulate the Pareto frontier between cumulative and policy regret?

## Architecture Onboarding

### Component map
Data Sources -> Environment Interface -> Algorithm Module -> Evaluation Metrics -> Benchmark Suite

### Critical path
Dataset preprocessing -> Environment configuration -> Algorithm instantiation -> Episode execution -> Metric computation

### Design tradeoffs
- Real data vs synthetic: Ecological validity vs controlled experimentation
- Multiple objectives vs single metrics: Comprehensive evaluation vs clear comparisons
- Modular design vs integrated framework: Flexibility vs ease of use

### Failure signatures
- Performance degradation under batched feedback indicates insufficient exploration
- Non-stationarity effects suggest model misspecification or adaptation lag
- Multiple objective conflicts reveal fundamental tradeoffs in algorithm design

### Three first experiments
1. Site selection task with Meager microcredit data comparing uniform vs adaptive sampling under sampling constraints
2. ASOS non-stationarity experiment comparing batched contextual vs non-contextual bandit algorithms
3. Multi-objective optimization experiment balancing cumulative and policy regret on field experiment data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do adaptive algorithms outperform uniform sampling in practical experimental settings?
- Basis in paper: The authors observe that adaptive algorithms often perform worse than uniform sampling under practical constraints like batched feedback and limited sampling opportunities.
- Why unresolved: The paper demonstrates performance degradation but doesn't provide a clear characterization of when adaptive methods will succeed.
- What evidence would resolve it: Systematic analysis across diverse real-world datasets identifying specific characteristics (e.g., effect sizes, batch sizes, horizon length) that predict when adaptive algorithms will outperform uniform sampling.

### Open Question 2
- Question: How can adaptive experimentation frameworks be extended to handle non-stationary environments while maintaining theoretical guarantees?
- Basis in paper: The authors note that non-stationarity severely impacts adaptive algorithm performance but don't propose solutions that maintain both practical utility and theoretical rigor.
- Why unresolved: Current methods either sacrifice theoretical guarantees for practical robustness or remain brittle in realistic settings.
- What evidence would resolve it: Development and validation of algorithms that provably handle non-stationarity while maintaining competitive performance in real-world benchmarks.

### Open Question 3
- Question: What is the optimal balance between exploration and exploitation when multiple competing objectives exist in adaptive experiments?
- Basis in paper: The authors discuss the tension between cumulative regret and policy regret but don't provide guidance on optimal tradeoffs.
- Why unresolved: The paper illustrates the tradeoff but doesn't establish principles for navigating it in practice.
- What evidence would resolve it: Empirical studies quantifying the impact of different exploration-exploitation balances across various objective combinations and constraints.

## Limitations
- Reliance on specific real-world datasets may limit generalizability across different domains
- Coverage of non-stationarity scenarios appears constrained to the ASOS.com dataset
- Evaluation focuses primarily on performance metrics rather than implementation complexity or computational costs

## Confidence
- Claims about practical barriers to adaptive experimentation adoption: High
- Claims about the proposed benchmark's ability to capture these barriers comprehensively: Medium
- Claims about the relative performance of specific algorithms given limited experimental scope: Low

## Next Checks
1. Test AExGym with synthetic datasets that systematically vary non-stationarity patterns to validate whether observed performance degradation generalizes beyond the ASOS.com case.
2. Implement and benchmark additional adaptive algorithms (e.g., contextual bandit variants with different exploration strategies) to assess the sensitivity of conclusions to algorithm selection.
3. Conduct a computational complexity analysis comparing the proposed algorithms under the realistic constraints modeled in AExGym to understand trade-offs between statistical power and implementation feasibility.