---
ver: rpa2
title: Training Neural Networks from Scratch with Parallel Low-Rank Adapters
arxiv_id: '2402.16828'
source_url: https://arxiv.org/abs/2402.16828
tags:
- lora
- training
- learning
- parameters
- low-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of low-rank adapters for training neural
  networks from scratch. It identifies limitations of standard LoRA in this context
  and introduces LoRA-the-Explorer (LTE), a bi-level optimization algorithm that enables
  parallel training of multiple low-rank heads across computing nodes.
---

# Training Neural Networks from Scratch with Parallel Low-Rank Adapters

## Quick Facts
- arXiv ID: 2402.16828
- Source URL: https://arxiv.org/abs/2402.16828
- Reference count: 40
- Primary result: Parallel low-rank adapters (LoRA-the-Explorer) enable competitive full-model pre-training with reduced communication overhead

## Executive Summary
This paper addresses the challenge of training neural networks from scratch using low-rank adapters, a technique traditionally used only for fine-tuning. The authors identify limitations of standard LoRA in full pre-training and introduce LoRA-the-Explorer (LTE), a bi-level optimization algorithm that enables parallel training of multiple low-rank heads across computing nodes. LTE reduces the need for frequent synchronization and is competitive with standard pre-training on vision transformers using various datasets. The method leverages lower-memory devices and only depends on communicating the LoRA parameters, making it ideal for bandwidth-limited or memory-constrained training frameworks.

## Method Summary
The paper proposes LoRA-the-Explorer (LTE), a bi-level optimization algorithm that trains multiple low-rank adapters in parallel across computing nodes. Each worker is assigned a unique LoRA head trained on a different mini-batch, and these heads are merged back into the main weights after T training steps. This approach approximates full-rank model training without materializing the full parameter matrix. The method uses quantized weights for storage efficiency and adjusts LoRA scaling and learning rates for optimal pre-training performance. LTE is designed for federated-like settings where workers train independently before merging, trading off stale parameter estimates for reduced bandwidth usage.

## Key Results
- LTE achieves competitive performance with standard pre-training on vision transformers across multiple datasets
- Parallel low-rank updates approximate full-rank model training while keeping memory usage low
- Delayed merging (merge interval T=10) reduces communication overhead without severely harming convergence
- Scaling LoRA parameter scalar s and adjusting learning rate η enables better pre-training performance than standard LoRA fine-tuning settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel low-rank updates approximate full-rank model training without materializing the full parameter matrix.
- Mechanism: By assigning each worker a unique LoRA head trained on a different mini-batch, the sum of all heads' updates spans the full-rank space. Merging these updates back into the main weights enables exploration of the full parameter space while keeping memory usage low.
- Core assumption: Multiple low-rank matrices, when summed, can approximate a full-rank matrix if their span collectively covers the original rank.
- Evidence anchors:
  - [abstract] "LTE reduces the need for frequent synchronization and is competitive with standard pre-training on vision transformers"
  - [section 3.1] "we investigate the possibility of arriving at an equivalent performance by leveraging multiple low-rank adapters in parallel"
  - [corpus] Weak correlation; neighboring papers focus on LoRA fine-tuning, not full pre-training.
- Break condition: If LoRA heads collapse to the same subspace or if the number of heads × rank < original weight rank.

### Mechanism 2
- Claim: Delayed merging reduces communication overhead without severely harming convergence.
- Mechanism: Instead of merging every iteration, LoRA parameters are trained independently for T steps before synchronization. This trades off stale parameter estimates for reduced bandwidth usage.
- Core assumption: Stale estimates of LoRA parameters still provide informative updates to the main weights.
- Evidence anchors:
  - [section 3.2] "we extend and combine the ideas of local updates and model-averaging... Instead of merging every iteration, we allow the LoRA parameters to train independently of each other for a longer period"
  - [section 3.3] "Our algorithm is designed with two primary considerations... (2) parameterizing W such that it can be stored in low-precision and communicated efficiently"
  - [corpus] Weak correlation; neighboring papers focus on LoRA fine-tuning, not full pre-training.
- Break condition: If merge interval T is too large, LoRA heads diverge from the intended optimization path.

### Mechanism 3
- Claim: Scaling LoRA parameter scalar s and adjusting learning rate η enables better pre-training performance than standard LoRA fine-tuning settings.
- Mechanism: A larger s emphasizes LoRA contribution in forward pass, and with proper learning rate adjustment, avoids quadratic alignment terms from dominating updates.
- Core assumption: s can be tuned independently of learning rate to balance gradient magnitude and stability.
- Evidence anchors:
  - [section 3.4] "we found using a large value of s and a slightly lower learning rate η to work the best"
  - [section B.1] "we show that s does not linearly scale the learning rate for Adam"
  - [corpus] Weak correlation; neighboring papers focus on LoRA fine-tuning, not full pre-training.
- Break condition: If s is too large relative to learning rate, alignment terms dominate and destabilize training.

## Foundational Learning

- Concept: Low-rank matrix approximation
  - Why needed here: LoRA relies on representing weight updates as low-rank matrices; understanding when this is valid is key.
  - Quick check question: Can a rank-r matrix approximate any m×n matrix if r < min(m,n)? (Answer: Only if the target matrix is approximately low-rank.)

- Concept: Distributed training and synchronization
  - Why needed here: LTE operates in a federated-like setting where workers train independently before merging.
  - Quick check question: What is the trade-off between synchronization frequency and stale gradient effects? (Answer: More frequent sync → fresher gradients but higher communication cost.)

- Concept: Gradient noise and batch size scaling
  - Why needed here: LTE uses smaller per-head batch sizes, increasing gradient variance; understanding this is crucial for tuning.
  - Quick check question: How does gradient noise scale with batch size? (Answer: Noise inversely proportional to √batch size.)

## Architecture Onboarding

- Component map:
  Main model weights W (quantized) -> N LoRA parameter pairs (Bn, An) per layer -> Local optimizers per worker -> Parameter server or all-reduce sync mechanism -> Merge logic (averaging over N heads)

- Critical path:
  1. Initialize W and N LoRA heads
  2. Assign each worker a unique LoRA head and mini-batch
  3. Train LoRA head locally for T steps
  4. Synchronize and merge LoRA updates into W
  5. Repeat until convergence

- Design tradeoffs:
  - More heads → better rank coverage but more memory and compute per step
  - Larger T → less sync but more stale updates
  - Higher s → stronger LoRA influence but risk of instability
  - Quantized W → lower memory but possible precision loss

- Failure signatures:
  - Slow convergence → too few heads or too large T
  - Poor final accuracy → LoRA heads collapse to same subspace
  - Memory overflow → LoRA rank too high or too many heads
  - Communication bottleneck → merge frequency too high or model too large

- First 3 experiments:
  1. Linear regression with synthetic data: Test rank recovery with varying T and heads.
  2. ViT-S on ImageNet100: Compare LTE vs full-model training with default rank/r settings.
  3. Ablation on initialization: Test Kaiming vs Xavier vs Bernstein init for LoRA heads.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LTE performance scale with larger models (e.g., 10B+ parameters) and what are the communication/computation bottlenecks at that scale?
- Basis in paper: [explicit] The paper states "stress tests on larger models are essential for a comprehensive understanding of the method's scalability" and discusses hypothetical analysis but lacks empirical validation.
- Why unresolved: The paper only tests up to ViT-L (306M parameters) and does not empirically validate scaling claims for truly large models.
- What evidence would resolve it: Empirical experiments training models in the 10B+ parameter range using LTE, with detailed analysis of communication/computation bottlenecks and comparison to standard DDP.

### Open Question 2
- Question: What is the optimal strategy for determining the number of LoRA heads and rank per head dynamically during training rather than using fixed values?
- Basis in paper: [inferred] The paper mentions "whether heterogeneous parameterization of LoRA is feasible, where each LoRA head employs a variable rank r" as an open question, and discusses rank dynamics throughout training.
- Why unresolved: The paper uses fixed configurations (32 heads with rank 64) and does not explore adaptive strategies for determining these parameters.
- What evidence would resolve it: Experiments showing performance improvements when dynamically adjusting the number of heads and ranks during training, compared to fixed configurations.

### Open Question 3
- Question: How does LTE performance change when using more sophisticated merging strategies compared to simple averaging?
- Basis in paper: [explicit] The paper states "we opt for simple averaging and leave more sophisticated merging such as those used in (Karimireddy et al., 2020; Matena & Raffel, 2022; Yadav et al., 2023) for future works."
- Why unresolved: The paper only uses basic averaging for merging LoRA parameters and does not explore more advanced merging techniques.
- What evidence would resolve it: Experiments comparing LTE performance using different merging strategies (e.g., weighted averaging, model averaging techniques) and quantifying the performance gains.

### Open Question 4
- Question: What is the impact of gradient noise on LTE convergence and how can it be mitigated?
- Basis in paper: [explicit] The paper discusses "gradient noise is the primary factor contributing to slower convergence" and experiments with varying batch sizes.
- Why unresolved: While the paper identifies gradient noise as a problem and suggests larger batch sizes as a solution, it does not provide a comprehensive analysis of the noise's impact or explore other mitigation strategies.
- What evidence would resolve it: Detailed experiments measuring the relationship between gradient noise, convergence speed, and final performance across different batch sizes and optimization strategies.

## Limitations
- Limited empirical validation on larger-scale models beyond ViT-L (306M parameters)
- Simple averaging used for merging without exploring more sophisticated strategies
- No systematic ablation studies on merge interval T across different tasks and model sizes
- Claims about communication efficiency and memory savings not backed by detailed profiling

## Confidence
- **High**: The bi-level optimization formulation and parallel training structure are mathematically consistent.
- **Medium**: Empirical results show competitive performance on tested datasets, but lack comprehensive scaling and ablation studies.
- **Low**: Claims about communication efficiency and memory savings are not backed by detailed profiling or comparison with alternative distributed methods.

## Next Checks
1. **Scaling Test**: Apply LTE to a larger model (e.g., ViT-Huge or ResNet-50) and measure both accuracy and memory/communication efficiency compared to standard full-model training.
2. **Merge Interval Sensitivity**: Systematically vary T across a wider range (e.g., 1 to 50) and measure convergence speed and final accuracy to identify optimal settings per task.
3. **Rank Recovery Experiment**: On a controlled synthetic task, measure how well the sum of low-rank heads recovers the true full-rank update, varying the number of heads and their ranks.