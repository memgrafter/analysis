---
ver: rpa2
title: On the Impact of Fine-Tuning on Chain-of-Thought Reasoning
arxiv_id: '2411.15382'
source_url: https://arxiv.org/abs/2411.15382
tags:
- reasoning
- have
- bags
- must
- sold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the impact of fine-tuning on the reasoning
  capabilities of large language models, specifically focusing on Chain-of-Thought
  (CoT) reasoning. The authors fine-tune LLMs on various datasets and evaluate their
  CoT reasoning performance and faithfulness using three tests: Early Termination,
  Filler Substitution, and Paraphrasing.'
---

# On the Impact of Fine-Tuning on Chain-of-Thought Reasoning

## Quick Facts
- arXiv ID: 2411.15382
- Source URL: https://arxiv.org/abs/2411.15382
- Authors: Elita Lobo; Chirag Agarwal; Himabindu Lakkaraju
- Reference count: 40
- One-line primary result: Fine-tuning generally reduces CoT reasoning faithfulness, with smaller models more affected than larger models.

## Executive Summary
This study investigates how fine-tuning impacts Chain-of-Thought (CoT) reasoning in large language models. The authors fine-tune LLMs on various datasets and evaluate their CoT reasoning performance and faithfulness using three tests: Early Termination, Filler Substitution, and Paraphrasing. Results show that fine-tuning generally reduces CoT accuracy and faithfulness, with smaller models like Llama-3-8b-Instruct being more affected than larger models like GPT-4. Fine-tuning on non-reasoning datasets further decreases CoT faithfulness, highlighting the need for careful consideration when fine-tuning LLMs to preserve their reasoning abilities.

## Method Summary
The authors fine-tune Llama-3-8b-Instruct and GPT-4 on four datasets (MedQA, MedMCQA, CosmosQA, and GSM8K) using QLoRA (rank 16) with supervised fine-tuning. They evaluate pre-trained vs fine-tuned models on CoT reasoning accuracy and faithfulness using three faithfulness tests (Early Termination, Paraphrasing, and Filler Substitution) at different α thresholds. The study compares performance across model sizes and dataset types to understand how fine-tuning affects reasoning capabilities.

## Key Results
- Fine-tuning generally reduces CoT accuracy and faithfulness across all tested datasets
- Smaller models (Llama-3-8b-Instruct) are more affected by fine-tuning than larger models (GPT-4)
- Fine-tuning on non-reasoning datasets (MedQA, CosmosQA) further decreases CoT faithfulness
- GPT-4 shows minimal impact on CoT faithfulness due to larger capacity and superior generalization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning can reduce CoT reasoning faithfulness by altering internal reasoning pathways.
- **Mechanism:** When LLMs are fine-tuned on datasets that do not emphasize reasoning, the fine-tuning process may reinforce patterns that bypass intermediate reasoning steps, leading to reduced faithfulness.
- **Core assumption:** Fine-tuning changes internal model weights in ways that affect reasoning pathways.
- **Evidence anchors:**
  - [abstract] "fine-tuning leads to an average decrease in the faithfulness of CoT reasoning across four datasets"
  - [section] "fine-tuning LLMs on non-reasoning datasets further decreases CoT faithfulness"
- **Break condition:** If fine-tuning is done with reasoning examples or if the model is large enough to generalize without significant weight changes.

### Mechanism 2
- **Claim:** Smaller models are more susceptible to reduced CoT reasoning performance after fine-tuning.
- **Mechanism:** Smaller models have fewer parameters and less capacity to maintain both task-specific performance and general reasoning abilities, leading to greater degradation in CoT reasoning.
- **Core assumption:** Model size correlates with capacity to preserve reasoning abilities during fine-tuning.
- **Evidence anchors:**
  - [abstract] "smaller models like Llama-3-8b-Instruct being more affected than larger models like GPT-4"
  - [section] "fine-tuning on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness"
- **Break condition:** If the fine-tuning process includes explicit reasoning steps or if the model is large enough to maintain generalization.

### Mechanism 3
- **Claim:** Fine-tuning can lead to overfitting on specialized tasks, reducing generalization in reasoning.
- **Mechanism:** When fine-tuned on specific datasets, the model may overfit to the training distribution, losing the ability to generalize to unseen reasoning tasks.
- **Core assumption:** Overfitting during fine-tuning reduces the model's ability to generalize to new reasoning tasks.
- **Evidence anchors:**
  - [abstract] "fine-tuning generally reduces CoT accuracy and faithfulness"
  - [section] "fine-tuning on MedQA and CosmosQA leads to larger performance drops on the GSM8K math reasoning dataset"
- **Break condition:** If the fine-tuning process includes regularization techniques or if the model is evaluated on diverse reasoning tasks.

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT reasoning is the method used to evaluate the impact of fine-tuning on reasoning capabilities.
  - Quick check question: What is the purpose of CoT reasoning in LLMs?

- **Concept:** Fine-tuning methods (e.g., QLoRA, SFT)
  - Why needed here: Different fine-tuning methods can have varying impacts on reasoning capabilities.
  - Quick check question: How does QLoRA differ from traditional fine-tuning methods?

- **Concept:** Faithfulness metrics (e.g., Early Termination, Filler Substitution, Paraphrasing)
  - Why needed here: These metrics are used to evaluate the faithfulness of CoT reasoning after fine-tuning.
  - Quick check question: What does a high CoT Pred Match value indicate in the Early Termination test?

## Architecture Onboarding

- **Component map:** Pre-trained LLM → Fine-tuning → CoT reasoning evaluation → Faithfulness assessment
- **Critical path:** Pre-trained LLM → Fine-tuning → CoT reasoning evaluation → Faithfulness assessment
- **Design tradeoffs:** Balancing task-specific performance gains with preservation of general reasoning abilities
- **Failure signatures:** Reduced CoT accuracy, decreased faithfulness, overfitting on specialized tasks
- **First 3 experiments:**
  1. Fine-tune a small LLM on a non-reasoning dataset and evaluate CoT accuracy and faithfulness.
  2. Fine-tune a large LLM on a reasoning dataset and compare CoT performance with the pre-trained model.
  3. Use different QLoRA ranks to fine-tune an LLM and assess the impact on CoT reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning on specialized datasets affect the faithfulness of Chain-of-Thought reasoning in larger language models (e.g., GPT-4) compared to smaller models (e.g., Llama-3-8b-Instruct)?
- Basis in paper: [explicit] The paper states that fine-tuning has minimal impact on the faithfulness of CoT reasoning generated by GPT-4, likely due to its larger capacity and superior generalization, which require fewer weight adjustments for new reasoning tasks.
- Why unresolved: The paper does not provide a detailed analysis of the mechanisms behind the differences in faithfulness between larger and smaller models after fine-tuning.
- What evidence would resolve it: Comparative studies analyzing the internal mechanisms (e.g., attention head activations) of larger and smaller models during fine-tuning and their impact on CoT reasoning faithfulness.

### Open Question 2
- Question: What are the long-term effects of fine-tuning on the reasoning capabilities of large language models, particularly in terms of catastrophic forgetting and generalization to out-of-distribution tasks?
- Basis in paper: [explicit] The paper mentions that fine-tuning can lead to catastrophic forgetting, where performance on tasks outside the target domain degrades, and that smaller LLMs are more susceptible to reduced generalization after fine-tuning.
- Why unresolved: The paper does not investigate the long-term effects of fine-tuning on model performance across a wide range of tasks or the extent of catastrophic forgetting.
- What evidence would resolve it: Longitudinal studies tracking model performance on diverse tasks before and after fine-tuning, including assessments of catastrophic forgetting and generalization to unseen tasks.

### Open Question 3
- Question: How do different fine-tuning methods (e.g., QLoRA, LoRA, RLHF) impact the reasoning abilities and faithfulness of Chain-of-Thought reasoning in large language models?
- Basis in paper: [inferred] The paper uses QLoRA for fine-tuning and discusses its efficiency, but does not compare its impact on reasoning abilities and faithfulness to other fine-tuning methods.
- Why unresolved: The paper focuses on QLoRA and does not explore how other fine-tuning techniques might differently affect reasoning capabilities and CoT faithfulness.
- What evidence would resolve it: Comparative studies evaluating the effects of various fine-tuning methods on model reasoning abilities and CoT faithfulness across multiple datasets and model sizes.

## Limitations

- The study relies on faithfulness metrics that measure surface-level reasoning consistency rather than actual internal reasoning changes.
- The evaluation uses a specific fine-tuning approach (QLoRA with rank 16) and limited model sizes, which may not generalize to all fine-tuning scenarios.
- The proposed mechanisms explaining why fine-tuning reduces reasoning faithfulness lack direct evidence and remain speculative interpretations of observed correlations.

## Confidence

- **High Confidence**: The claim that fine-tuning reduces CoT reasoning faithfulness is well-supported by experimental results across multiple datasets and models.
- **Medium Confidence**: The claim that smaller models are more affected than larger models is supported by the data but requires more systematic testing across different model sizes and fine-tuning approaches to confirm.
- **Low Confidence**: The proposed mechanisms explaining why fine-tuning reduces reasoning faithfulness (changes to internal reasoning pathways, overfitting, etc.) lack direct evidence.

## Next Checks

1. **Mechanism validation**: Conduct ablation studies that systematically modify different aspects of fine-tuning (learning rate, rank, data diversity) to isolate which factors most strongly impact reasoning faithfulness, and use attention visualization or other interpretability tools to directly observe changes in reasoning pathways.

2. **Generalization testing**: Fine-tune models on diverse reasoning tasks and evaluate performance on held-out reasoning problems to distinguish between task-specific overfitting and general reasoning degradation, testing whether fine-tuning on reasoning tasks preserves faithfulness better than non-reasoning tasks.

3. **Method comparison**: Compare the impact of different fine-tuning methods (full fine-tuning, QLoRA with different ranks, SFT) on CoT reasoning to determine if the observed effects are specific to the QLoRA approach used in this study or represent a general phenomenon across fine-tuning methods.