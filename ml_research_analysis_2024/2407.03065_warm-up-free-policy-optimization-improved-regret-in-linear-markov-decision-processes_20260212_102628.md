---
ver: rpa2
title: 'Warm-up Free Policy Optimization: Improved Regret in Linear Markov Decision
  Processes'
arxiv_id: '2407.03065'
source_url: https://arxiv.org/abs/2407.03065
tags:
- lemma
- regret
- learning
- policy
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Contracted Features Policy Optimization (CFPO),
  a new algorithm that eliminates the costly pure exploration warm-up phase required
  by previous policy optimization methods in linear MDPs. CFPO uses a simple contraction
  mechanism that shrinks features based on estimation uncertainty, integrated directly
  into the policy optimization loop.
---

# Warm-up Free Policy Optimization: Improved Regret in Linear Markov Decision Processes

## Quick Facts
- **arXiv ID**: 2407.03065
- **Source URL**: https://arxiv.org/abs/2407.03065
- **Reference count**: 40
- **Primary result**: Achieves rate-optimal regret bounds of Õ(√H⁴d³K) in linear MDPs without warm-up phase

## Executive Summary
This paper introduces Contracted Features Policy Optimization (CFPO), a new algorithm that eliminates the costly pure exploration warm-up phase required by previous policy optimization methods in linear Markov Decision Processes. The key innovation is a contraction mechanism that shrinks features based on estimation uncertainty, integrated directly into the policy optimization loop. CFPO achieves improved regret bounds over previous methods while being simpler, more computationally efficient, and reward-aware.

## Method Summary
CFPO uses a contraction mechanism to avoid the warm-up phase in linear MDPs. At each epoch, the algorithm scales down features using a sigmoid function of the negative inverse covariance estimate. This contraction shrinks the MDP in high-uncertainty regions, ensuring bounded Q-values without separate exploration. The algorithm updates policies via online mirror descent over Q-value estimates with the contracted features, achieving rate-optimal regret bounds while being reward-aware and more efficient than previous approaches.

## Key Results
- Achieves Õ(√H⁴d³K) regret for adversarial losses with full information
- Achieves Õ(√H⁴d³K) regret for stochastic losses with bandit feedback
- Improves upon previous best-known bounds by a factor of √H³d
- Matches performance of value iteration methods in the stochastic setting

## Why This Works (Mechanism)

### Mechanism 1
Feature contraction eliminates the warm-up phase while maintaining bounded Q-values. The algorithm scales down features using a sigmoid of the negative inverse covariance estimate, shrinking the MDP in high-uncertainty regions. The contraction coefficients remain in [0,1], ensuring valid probability transitions and bounded losses.

### Mechanism 2
Optimism is preserved under feature contraction via a novel regret decomposition. The algorithm replaces true features with contracted features in Q-value estimates. The decomposition shows this change only affects terms that can be bounded using elliptical potential arguments and the logistic bound lemma.

### Mechanism 3
The contraction mechanism is reward-aware, unlike reward-free exploration methods. Since contraction coefficients are computed using current policy's uncertainty estimates, the algorithm naturally stops exploring inferior actions once their uncertainty is low, saving samples compared to uniform exploration.

## Foundational Learning

- **Concept: Linear Markov Decision Processes**
  - Why needed: Algorithm relies on linear structure to estimate dynamics and rewards
  - Quick check: What is the form of the transition probability in a linear MDP?

- **Concept: Policy Optimization and Online Mirror Descent**
  - Why needed: Algorithm updates policies via OMD over Q-value estimates
  - Quick check: How does entropy regularization in OMD affect the policy update step?

- **Concept: Elliptical Potential and Concentration Inequalities**
  - Why needed: Bounding sum of inverse covariance norms requires elliptical potential
  - Quick check: What is the standard bound on ∑ₖ ∥φₖ∥²(Λₖ)⁻¹ in terms of log K?

## Architecture Onboarding

- **Component map**: 
  - Epoch manager -> Feature contraction module -> Estimator -> Q-value calculator -> OMD updater -> Regret tracker

- **Critical path**: 
  1. Observe trajectory and losses
  2. Update covariance matrices Λₖ,h
  3. Compute contraction coefficients and contracted features
  4. Update estimators θₖ,h and ψₖ,h
  5. Compute Q-values and update policy
  6. Record regret contribution

- **Design tradeoffs**: 
  - Contraction instead of truncation reduces regret but requires careful covariance handling
  - Epoch-based resets simplify analysis but may slow convergence if epochs are too long
  - Full-information vs. bandit feedback changes only the reward estimator

- **Failure signatures**: 
  - Covariance matrices not growing → contraction coefficients stuck at 1 → no exploration
  - Q-values exploding → optimism not preserved → regret bound fails
  - Epochs too frequent → regret dominated by reset cost

- **First 3 experiments**: 
  1. Verify contraction coefficients stay in [0,1] and decrease with uncertainty
  2. Check that regret scales as O(√H⁴d³K) on synthetic linear MDP
  3. Compare sample efficiency against baseline with reward-free warm-up

## Open Questions the Paper Calls Out

### Open Question 1
Can the contracted features approach be extended to non-linear function approximation settings beyond linear MDPs? The paper only proves theoretical guarantees for linear MDPs and explicitly states "we are not aware of a method that resolves all simultaneously" when moving beyond linear settings.

### Open Question 2
How does the practical performance of CFPO compare to Sherman et al.'s warm-up approach in terms of sample efficiency and computational time? The paper discusses multiple advantages but doesn't provide direct empirical comparisons.

### Open Question 3
What is the optimal choice of contraction schedule beyond the fixed schedule used in the paper? The authors use a specific contraction schedule but don't explore alternatives or prove optimality of this choice.

## Limitations
- Assumes knowledge of linear MDP structure but doesn't address model misspecification or unknown features
- Contraction mechanism's numerical stability is not thoroughly analyzed
- Reward-aware nature claimed but lacks empirical validation against reward-free baselines

## Confidence

- **High confidence**: The theoretical regret bound of Õ(√H⁴d³K) and its improvement over previous bounds by a factor of √H³d
- **Medium confidence**: The mechanism by which contraction eliminates the warm-up phase while preserving optimism
- **Low confidence**: Practical performance advantages of the reward-aware contraction mechanism without empirical comparisons

## Next Checks
1. Implement numerical safeguards for contraction coefficient computation to prevent division-by-zero or extreme values when covariance matrices become ill-conditioned
2. Conduct ablation studies comparing CFPO with and without contraction against a reward-free exploration baseline
3. Test the algorithm on non-linear MDPs or with misspecified features to evaluate robustness beyond theoretical assumptions