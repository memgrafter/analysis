---
ver: rpa2
title: 'SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread
  INT4 Quantization'
arxiv_id: '2411.10958'
source_url: https://arxiv.org/abs/2411.10958
tags:
- quantization
- latexit
- attention
- zhang
- int4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SageAttention2 improves quantized attention efficiency by quantizing
  Q, K to INT4 in a per-thread granularity and eP, V to FP8. It introduces smoothing
  for Q to reduce quantization error from outliers, and uses a two-level accumulation
  strategy to mitigate FP22 accumulator precision loss in FP8 matrix multiplication.
---

# SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization

## Quick Facts
- arXiv ID: 2411.10958
- Source URL: https://arxiv.org/abs/2411.10958
- Authors: Jintao Zhang; Haofeng Huang; Pengle Zhang; Jia Wei; Jun Zhu; Jianfei Chen
- Reference count: 40
- Primary result: 3× speedup over FlashAttention2 and 4.5× over xformers on RTX4090

## Executive Summary
SageAttention2 introduces per-thread INT4 quantization for Q and K matrices and FP8 quantization for eP and V matrices to accelerate attention computation. The method employs outlier smoothing on Q and K to reduce quantization error, and implements a two-level accumulation strategy to mitigate FP22 accumulator precision loss in FP8 matrix multiplication. These techniques maintain high accuracy while achieving significant speed gains, with 3× speedup over FlashAttention2 on RTX4090 and matching FlashAttention3(fp8) speed on Hopper GPUs.

## Method Summary
SageAttention2 accelerates attention through three key innovations: per-thread INT4 quantization that eliminates dequantization overhead by aligning quantization groups with GPU thread allocation, Q and K smoothing that reduces quantization error from outliers by centering distributions, and a two-level accumulation strategy that compensates for FP22 accumulator precision loss by accumulating block-level results in FP32. The method quantizes Q and K to INT4 per-thread, eP to FP8 per-channel, and V to FP8 per-channel, then performs matrix multiplications with careful precision management.

## Key Results
- Achieves 3× speedup over FlashAttention2 and 4.5× over xformers on RTX4090
- Matches FlashAttention3(fp8) speed on Hopper GPUs with better accuracy
- Maintains negligible accuracy loss across language, image, and video generation models including Llama3.1, CogvideoX, and Stable-Diffusion3.5

## Why This Works (Mechanism)

### Mechanism 1: Per-thread quantization preserves accuracy while eliminating dequantization overhead
The per-thread quantization method exploits the mapping between GPU threads and matrix memory layout dictated by PTX mma instructions. Each GPU thread corresponds to one quantization scale for both Q and K, avoiding the need for vector dot products of multiple scales during dequantization. This works because the PTX mma.m16n8k64 instruction layout ensures that tokens corresponding to the same thread can share a quantization scale without loss of representational fidelity.

### Mechanism 2: Smoothing Q reduces quantization error from outliers
Subtracting the channel-wise mean from Q (γ(Q) = Q - Q̄) creates a centered distribution with reduced variance. This makes the largest values smaller and the overall distribution more uniform, allowing the limited INT4 range to be utilized more fully. The method works because outliers in Q are significant enough to dominate the quantization range, and removing their mean effect distributes values more evenly.

### Mechanism 3: Two-level accumulation compensates for FP22 accumulator precision loss
Each block Matmul produces results in FP22 (due to hardware constraints), which are then accumulated into an FP32 buffer. This confines precision errors to block boundaries rather than propagating them across the entire matrix multiplication. The strategy works because FP22 accumulators have sufficient precision for block-level accumulation but not for the entire operation, and the block size is small enough that FP32 accumulation can correct the errors.

## Foundational Learning

- **Concept:** Quantization granularity and its impact on accuracy
  - Why needed here: Different quantization granularities (per-tensor, per-block, per-thread, per-token) trade off between accuracy and computational overhead. Understanding this tradeoff is crucial for designing efficient attention mechanisms.
  - Quick check question: What is the main computational overhead introduced by per-token quantization that per-thread quantization avoids?

- **Concept:** Matrix multiplication accumulator precision
  - Why needed here: The precision of accumulators in matrix multiplication directly affects the accuracy of the final result. Understanding how hardware implements accumulators (e.g., FP16 vs FP32 vs FP22) is essential for designing compensation strategies.
  - Quick check question: Why does using FP32 accumulators for FP8 matrix multiplication improve accuracy compared to FP16 accumulators?

- **Concept:** Activation outlier distributions in neural networks
  - Why needed here: Outliers in activation matrices like Q and K cause quantization errors. Understanding their distribution and how to mitigate them through techniques like smoothing is crucial for maintaining accuracy during quantization.
  - Quick check question: How do channel-wise outliers in Q and K affect the quantization error when using INT4 representation?

## Architecture Onboarding

- **Component map:** Input FP16 Q, K, V → Smoothing Q and K (subtract channel means) → Quantization: INT4 per-thread Q/K, FP8 per-channel eP/V → Core computation: Two-level accumulation for FP8 eP V, INT4 QK⊤ → Postprocessing: Dequantization and output correction

- **Critical path:** The path from input matrices through smoothing, quantization, core computation (both QK⊤ and eP V), and dequantization to output. The most computationally intensive parts are the two matrix multiplications.

- **Design tradeoffs:**
  - Accuracy vs speed: Per-thread quantization vs per-token quantization (higher accuracy but potential overhead)
  - Memory vs computation: Smoothing requires additional memory but reduces quantization error
  - Hardware dependency: Two-level accumulation strategy depends on specific FP22 accumulator behavior that may vary across hardware

- **Failure signatures:**
  - Accuracy degradation: Could indicate insufficient quantization granularity, ineffective smoothing, or accumulator precision issues
  - Speed degradation: Could indicate overhead from smoothing operations or inefficient quantization implementation
  - Memory issues: Could indicate excessive memory usage from intermediate representations or smoothing buffers

- **First 3 experiments:**
  1. Benchmark accuracy degradation when switching from per-thread to per-block quantization on a representative model (e.g., CogvideoX)
  2. Measure the impact of smoothing Q on quantization accuracy across different INT4 bit widths (e.g., comparing smoothed vs unsmoothed Q in INT4 representation)
  3. Verify the FP22 accumulator behavior by testing matrix multiplication with known input values and comparing against expected results with different accumulator precisions

## Open Questions the Paper Calls Out
None

## Limitations
- Claims about per-thread quantization benefits rely on undocumented hardware behaviors of PTX mma instructions
- Lacks ablation studies to isolate individual contributions of smoothing, per-thread quantization, and two-level accumulation
- Speedup claims lack statistical significance measures or variability assessments

## Confidence

- **High confidence**: The fundamental observation that quantization can accelerate attention computation and that smoothing can reduce quantization error from outliers
- **Medium confidence**: The specific architectural innovations (per-thread quantization, two-level accumulation) are described clearly but lack sufficient empirical validation
- **Low confidence**: The hardware-specific claims about PTX mma instruction layouts and FP22 accumulator behaviors cannot be verified without access to NVIDIA's proprietary documentation

## Next Checks

1. Conduct controlled experiments comparing per-thread, per-block, and per-token quantization on identical hardware to isolate the accuracy-speed tradeoff and verify the claimed 3× speedup.

2. Measure quantization error distributions with and without Q smoothing across different outlier severities to validate the claimed improvement in INT4 utilization.

3. Implement matrix multiplication tests with controlled input values to empirically verify the FP22 accumulator behavior and quantify the precision loss that the two-level accumulation strategy compensates for.