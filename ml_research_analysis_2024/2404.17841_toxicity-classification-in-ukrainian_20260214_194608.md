---
ver: rpa2
title: Toxicity Classification in Ukrainian
arxiv_id: '2404.17841'
source_url: https://arxiv.org/abs/2404.17841
tags:
- toxic
- data
- language
- ukrainian
- toxicity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the lack of a Ukrainian toxicity classification
  corpus by creating labeled datasets through three methods: translating an English
  corpus, filtering tweets with toxic keywords, and crowdsourcing annotations. It
  investigates cross-lingual knowledge transfer approaches including Backtranslation,
  LLM prompting, and Adapter Training, comparing them with fine-tuning approaches.'
---

# Toxicity Classification in Ukrainian

## Quick Facts
- arXiv ID: 2404.17841
- Source URL: https://arxiv.org/abs/2404.17841
- Reference count: 15
- This study creates Ukrainian toxicity classification datasets through translation, keyword filtering, and crowdsourcing, achieving 99% F1 on in-domain tests with fine-tuning on crowdsourced data.

## Executive Summary
This study addresses the lack of a Ukrainian toxicity classification corpus by creating labeled datasets through three methods: translating an English corpus, filtering tweets with toxic keywords, and crowdsourcing annotations. It investigates cross-lingual knowledge transfer approaches including Backtranslation, LLM prompting, and Adapter Training, comparing them with fine-tuning approaches. The model fine-tuned on crowdsourced data achieved near-perfect performance (99% F1) on in-domain test sets and demonstrated the highest effectiveness. Backtranslation emerged as the strongest unsupervised baseline, while the translated data approach showed robust out-of-domain performance, making it a reliable baseline despite some information loss during translation.

## Method Summary
The study employs three methods to create Ukrainian toxicity classification datasets: translating an English corpus, filtering tweets using toxic keywords, and crowdsourcing annotations. It evaluates cross-lingual knowledge transfer approaches including Backtranslation, LLM prompting, and Adapter Training, comparing these with fine-tuning approaches. The experimental setup involves training models on different dataset combinations and evaluating their performance on both in-domain and out-of-domain test sets.

## Key Results
- Fine-tuning on crowdsourced data achieved 99% F1 score on in-domain test sets
- Backtranslation performed best among unsupervised baselines
- Translated data approach demonstrated robust out-of-domain performance despite information loss during translation

## Why This Works (Mechanism)
The study's effectiveness stems from leveraging cross-lingual transfer learning to overcome data scarcity in Ukrainian toxicity classification. By translating English toxicity datasets and applying techniques like backtranslation and adapter training, the models can learn toxicity patterns from high-resource languages and transfer them to Ukrainian. The crowdsourced annotations provide high-quality, culturally-relevant labels that capture Ukrainian-specific toxic language patterns.

## Foundational Learning
- **Cross-lingual transfer learning**: Why needed - enables leveraging toxicity classification knowledge from English to Ukrainian; Quick check - compare model performance with and without cross-lingual training
- **Backtranslation**: Why needed - generates synthetic data to improve model robustness; Quick check - evaluate performance improvement when using backtranslated data
- **Adapter training**: Why needed - allows efficient fine-tuning of pre-trained models for specific tasks; Quick check - measure parameter efficiency vs full fine-tuning
- **Crowdsourcing quality control**: Why needed - ensures culturally appropriate and accurate toxicity labels; Quick check - inter-annotator agreement scores

## Architecture Onboarding

**Component Map**
English Toxicity Corpus -> Translation Pipeline -> Ukrainian Translated Data
Keyword Filter -> Tweet Collection -> Keyword-filtered Ukrainian Data
Crowdsourcing Platform -> Annotator Pool -> Crowdsourced Ukrainian Data
Base Model -> Fine-tuning/Adapter Training -> Toxicity Classification Model

**Critical Path**
1. Data creation (crowdsourcing/translating/keyword filtering)
2. Model initialization (base pre-trained Ukrainian model)
3. Transfer learning approach selection (fine-tuning/adapters/backtranslation)
4. Model training and evaluation
5. Performance validation on test sets

**Design Tradeoffs**
- Translation vs. native data collection: Translation provides scale but loses cultural nuances; native collection is expensive but culturally accurate
- Supervised vs. unsupervised approaches: Supervised methods require labeled data but achieve higher performance; unsupervised approaches are cheaper but less accurate
- Fine-tuning vs. adapter training: Fine-tuning may achieve better performance but requires more parameters; adapter training is more parameter-efficient

**Failure Signatures**
- Over-reliance on explicit toxic keywords leading to false negatives
- Cultural misinterpretation of toxic language in translated data
- Model overfitting to training domain when tested on out-of-domain data

**3 First Experiments**
1. Compare performance of fine-tuned vs. adapter-trained models on in-domain test sets
2.