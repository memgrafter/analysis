---
ver: rpa2
title: 'Adversarial Examples: Generation Proposal in the Context of Facial Recognition
  Systems'
arxiv_id: '2404.17760'
source_url: https://arxiv.org/abs/2404.17760
tags:
- adversarial
- examples
- facial
- recognition
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores adversarial examples in facial recognition
  systems using a methodology that combines autoencoders and principal component analysis
  (PCA) to manipulate latent representations. The study uses a small dataset of 200
  manually processed images (100 per individual) and tests attacks on Amazon Rekognition.
---

# Adversarial Examples: Generation Proposal in the Context of Facial Recognition Systems

## Quick Facts
- arXiv ID: 2404.17760
- Source URL: https://arxiv.org/abs/2404.17760
- Authors: Marina Fuster; Ignacio Vidaurreta
- Reference count: 5
- Primary result: Generates adversarial examples for facial recognition using autoencoders and PCA on latent representations, achieving limited success with manual selection required

## Executive Summary
This paper explores adversarial examples in facial recognition systems using a methodology that combines autoencoders and principal component analysis (PCA) to manipulate latent representations. The study uses a small dataset of 200 manually processed images (100 per individual) and tests attacks on Amazon Rekognition. The authors hypothesize that PCA components can separate identity from facial expression, but results show only the first component correlates with identity. They generate adversarial examples for dodging and impersonation attacks, but quality remains inconsistent and requires manual selection. Baseline metrics show Rekognition assigns 98.95% similarity for same-person matches (Ignacio) and 99.73% for Marina. Despite limited success, the study provides insights into adversarial example generation and suggests broader datasets and alternative techniques (e.g., variational autoencoders) for future research.

## Method Summary
The study generates adversarial examples by first encoding facial images into a compressed latent space using an autoencoder, then applying PCA to organize the latent vectors and identify components controlling identity versus expression. Adversarial examples are created by modifying specific principal component values and decoding back to image space. The approach tests both dodging attacks (avoiding recognition) and impersonation attacks (fooling the system into recognizing one person as another) using Amazon Rekognition's similarity threshold. Quality is evaluated against baseline metrics for confidence, sharpness, and brightness.

## Key Results
- Only the first principal component showed clear separation between subjects for identity
- Generated adversarial examples required extensive manual selection due to inconsistent quality
- Dodging and impersonation attacks achieved limited success with similarity thresholds >80%
- Baseline Rekognition similarity: 98.95% for same-person matches (Ignacio) and 99.73% for Marina

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCA components can separate identity from facial expression features in the latent space of an autoencoder.
- Mechanism: By applying PCA to the autoencoder's latent representations, the first principal component captures identity-related variance while later components capture expression-related variance. Manipulating these components allows targeted changes to either identity or expression.
- Core assumption: The latent space of the autoencoder contains separable features for identity and expression that can be ordered by PCA.
- Evidence anchors:
  - [abstract] "The technique is based on the use of the autoencoder latent space, organized with principal component analysis."
  - [section] "Our initial hypothesis, which was not strongly favoured by the results, stated that it would be possible to separate between the 'identity' and 'facial expression' features to produce high-quality examples."
  - [corpus] Weak - corpus papers focus on adversarial attacks but don't specifically address PCA-based separation of identity and expression.
- Break condition: If the latent space does not contain separable identity and expression features, or if PCA cannot effectively order them by relevance to identity.

### Mechanism 2
- Claim: Modifying the first principal component value can change the identity label assigned by the facial recognition system.
- Mechanism: The first principal component correlates with identity, so increasing or decreasing its value transitions the reconstructed face toward a different identity. This creates adversarial examples for impersonation attacks.
- Core assumption: The first principal component has a monotonic relationship with identity classification probability.
- Evidence anchors:
  - [section] "The only component that displayed clear separation between subjects was the first component" and "it was verified the progression towards the identity of their counterpart."
  - [section] "The transition from average 'Ignacio' first principal component value towards 'Marina' first component value."
  - [corpus] Weak - corpus papers discuss adversarial examples but don't specifically validate PCA component manipulation for identity transfer.
- Break condition: If the first principal component doesn't consistently correlate with identity changes, or if the relationship is non-monotonic.

### Mechanism 3
- Claim: Modifying non-first principal components can change facial expressions while preserving identity.
- Mechanism: Components beyond the first capture expression variance. By replacing these components from a reference image while keeping the first component from the target identity, the expression changes while identity remains.
- Core assumption: Non-first principal components control expression features without affecting identity classification.
- Evidence anchors:
  - [section] "The remaining sixty-three components controlled the features of the face" and "to impose the reference's facial expression upon original's identity."
  - [section] "Further analysis upon the semantic meaning on individual principal components showed no promise of a subset of values controlling things such as orientation, smile, eye-opening expression, etc."
  - [corpus] Weak - corpus papers discuss adversarial examples but don't specifically address expression manipulation via PCA components.
- Break condition: If non-first components also affect identity or if the semantic meaning of components is too entangled to separate expression from identity.

## Foundational Learning

- Concept: Autoencoder latent space representation
  - Why needed here: The methodology relies on encoding faces into a compressed latent space where PCA can be applied to separate features.
  - Quick check question: What is the dimensionality of the latent space in the autoencoder used in this study?

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is used to organize the latent space vectors to identify which components correspond to identity versus expression.
  - Quick check question: How many principal components were computed in the study, and which one showed clear separation between subjects?

- Concept: Black box attack methodology
  - Why needed here: The study performs attacks without access to the internal workings of Amazon Rekognition, requiring indirect evaluation methods.
  - Quick check question: What similarity threshold was used to define successful dodging and impersonation attacks?

## Architecture Onboarding

- Component map:
  Dataset preparation pipeline -> Autoencoder training -> PCA on latent space -> Adversarial example generator -> Amazon Rekognition evaluation -> Quality assessment

- Critical path:
  1. Encode input image to latent space
  2. Apply PCA to obtain component values
  3. Modify target component(s) based on attack type
  4. Apply inverse PCA and decode to reconstructed image
  5. Submit to Amazon Rekognition for similarity assessment
  6. Evaluate against baseline metrics for quality

- Design tradeoffs:
  - Small dataset (200 images) vs. computational constraints
  - Manual image processing vs. automated pipeline robustness
  - Grayscale simplification vs. color information loss
  - Component-by-component modification vs. holistic optimization

- Failure signatures:
  - Low confidence scores from Amazon Rekognition
  - Significant quality degradation (sharpness/brightness below baseline)
  - Inconsistent attack success across similar images
  - Manual selection required for viable adversarial examples

- First 3 experiments:
  1. Baseline evaluation: Compare all dataset images against stored images to establish baseline similarity metrics
  2. Identity transfer test: Modify first principal component of average Ignacio image toward Marina's value and assess similarity changes
  3. Expression transfer test: Replace non-first components of Ignacio images with Marina's while keeping first component fixed, then evaluate identity preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the first principal component consistently act as a "signal" for identity across a broader array of subjects?
- Basis in paper: [explicit] The authors note that the first principal component displayed clear separation between subjects and hypothesize it could control identity assignment.
- Why unresolved: The study was limited to only two individuals, which may have skewed the results and prevented generalization.
- What evidence would resolve it: Testing the methodology on a larger, more diverse dataset with many individuals would show if PC1 consistently correlates with identity across different people.

### Open Question 2
- Question: What is the fundamental reason for the existence of adversarial examples in facial recognition systems?
- Basis in paper: [explicit] The authors state there is no consensus on the underlying reasons for adversarial examples and cite Ilyas et al. [2019] who suggest non-robust features may be a cause.
- Why unresolved: Despite various studies, the academic community has not reached agreement on whether adversarial examples stem from non-robust features, model limitations, or other factors.
- What evidence would resolve it: Systematic ablation studies that remove different types of features from training data and observe effects on adversarial example generation could identify causal factors.

### Open Question 3
- Question: How can the quality and consistency of generated adversarial examples be improved?
- Basis in paper: [explicit] The authors note their technique proved inconsistent, requiring extensive manual intervention, and that generated examples had questionable quality despite meeting benchmarks.
- Why unresolved: The study did not identify systematic patterns for why certain images made good adversarial examples versus others, and the methodology lacked automation.
- What evidence would resolve it: Developing automated metrics that can predict adversarial example quality before testing, and testing alternative generation techniques like variational autoencoders as suggested by the authors.

## Limitations
- Small dataset size (200 images total) limits statistical significance and generalizability
- Manual selection required for viable adversarial examples indicates inconsistent quality
- Grayscale simplification removes potentially important color-based identity features
- Weak evidence from related corpus papers specifically validating PCA-based identity manipulation

## Confidence

**High confidence**: The baseline similarity metrics from Amazon Rekognition (98.95% for Ignacio, 99.73% for Marina) and the general methodology of using autoencoders with PCA are well-established techniques.

**Medium confidence**: The claim that modifying the first principal component can successfully transfer identity shows partial support but requires manual selection of successful examples.

**Low confidence**: The initial hypothesis that PCA can effectively separate identity from facial expression features in the latent space is not strongly supported by the results.

## Next Checks

1. Expand dataset diversity: Test the methodology on a larger, more diverse dataset (minimum 1000 images per subject) to assess whether the small sample size was limiting factor for consistent adversarial example generation.

2. Color vs. grayscale comparison: Repeat the experiments using color images to determine if the grayscale simplification is reducing attack effectiveness, comparing success rates and quality metrics between both approaches.

3. Alternative latent space models: Validate whether variational autoencoders or other generative models produce more separable latent representations for identity and expression than standard autoencoders, potentially improving the PCA-based component separation.