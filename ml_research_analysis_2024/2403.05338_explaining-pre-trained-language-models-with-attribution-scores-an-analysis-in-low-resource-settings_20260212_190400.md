---
ver: rpa2
title: 'Explaining Pre-Trained Language Models with Attribution Scores: An Analysis
  in Low-Resource Settings'
arxiv_id: '2403.05338'
source_url: https://arxiv.org/abs/2403.05338
tags:
- scores
- attribution
- plausibility
- language
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the quality of attribution scores extracted
  from prompt-based models (PBMs) in low-resource settings compared to fine-tuned
  models (FTMs). It analyzes the plausibility and faithfulness of explanations from
  different attribution methods (attention, Integrated Gradients, and Shapley Value
  Sampling) across various training sizes.
---

# Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings

## Quick Facts
- **arXiv ID**: 2403.05338
- **Source URL**: https://arxiv.org/abs/2403.05338
- **Authors**: Wei Zhou; Heike Adel; Hendrik Schuff; Ngoc Thang Vu
- **Reference count**: 0
- **Primary result**: PBMs yield more plausible explanations than FTMs in low-resource settings, with Shapley Value Sampling outperforming other attribution methods

## Executive Summary
This paper investigates the quality of attribution scores from prompt-based models (PBMs) compared to fine-tuned models (FTMs) in low-resource settings. The study analyzes plausibility and faithfulness of explanations from attention, Integrated Gradients, and Shapley Value Sampling methods across various training sizes. The key finding is that PBMs consistently produce more plausible and faithful explanations than FTMs when training data is limited, with Shapley Value Sampling emerging as the most effective attribution method. The authors also demonstrate that these trends hold for decoder-based large language models, suggesting broader applicability of their findings.

## Method Summary
The paper compares prompt-based models (using manual prompts, BitFit, and BFF) with fine-tuned models on two datasets (TSE and e-SNLI) across training sizes from 8 to full dataset. Three attribution methods are evaluated: attention scores, Integrated Gradients, and Shapley Value Sampling. Plausibility is measured using average precision, while faithfulness is calculated using area under the threshold-performance curve normalized by maximum possible performance. The study uses BERT-base, BERT-large, and RoBERTa-large as base models, with hyperparameters tuned via 4-fold cross-validation.

## Key Results
- PBMs generate more plausible explanations than FTMs in low-resource settings (8-64 samples)
- Shapley Value Sampling consistently outperforms attention and Integrated Gradients in both plausibility and faithfulness
- The trend of PBMs producing better explanations holds for decoder-based large language models (tested with Vicuna)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PBMs generate more plausible explanations than FTMs in low-resource settings because they preserve more of the original pre-trained model's context understanding.
- Mechanism: When using prompts, the model retains its pre-trained knowledge and uses the prompt to direct attention to relevant parts of the input. This maintains richer contextual embeddings compared to fine-tuning which overwrites parameters with limited data.
- Core assumption: The pre-trained model's contextual understanding is superior to what can be learned from very limited task-specific data.
- Evidence anchors:
  - [abstract] "we find that using the prompting paradigm (with either encoder-based or decoder-based models) yields more plausible explanations than fine-tuning the models in low-resource settings"
  - [section] "we think this might be because PBMs pick up task information quicker than FTMs in the low-resource settings, so the explanations given by PBMs are more plausible"
  - [corpus] Weak - neighboring papers focus on prompting efficiency but not attribution quality comparison
- Break condition: If the pre-trained model's general knowledge is not relevant to the specific task domain, or if the prompt is poorly designed and misdirects attention.

### Mechanism 2
- Claim: Shapley Value Sampling consistently outperforms attention and Integrated Gradients in both plausibility and faithfulness because it accounts for feature interactions through permutation analysis.
- Mechanism: Shapley Value Sampling calculates attribution scores by considering every possible permutation of feature inclusion, capturing the marginal contribution of each feature in different contexts. This is particularly valuable for transformers where context significantly affects token importance.
- Core assumption: Feature importance in transformer models depends on the presence/absence of other features in the input context.
- Evidence anchors:
  - [abstract] "Shapley Value Sampling consistently outperforms attention and Integrated Gradients in terms of plausibility and faithfulness scores"
  - [section] "Shapley Value Sampling attribution scores are comparably faithful as gold annotations" and "taking each permutation to calculate feature importance is helpful in models like BERT, as context is of vital importance"
  - [corpus] Weak - neighboring papers don't specifically compare Shapley methods to attention/IG in low-resource settings
- Break condition: When computational cost becomes prohibitive for large inputs, or when the model architecture doesn't benefit significantly from context-aware attribution.

### Mechanism 3
- Claim: The trend of PBMs producing better explanations holds for decoder-based LLMs because they also benefit from preserving pre-trained knowledge in low-resource settings.
- Mechanism: Large language models, despite being decoder-only, still rely on extensive pre-training knowledge. When prompted with few examples, they maintain their general language understanding while focusing on the task-specific aspects provided in the prompt.
- Core assumption: The pre-training of LLMs provides sufficient general knowledge that few-shot prompting can effectively leverage, even for explanation quality.
- Evidence anchors:
  - [abstract] "The trends observed for PBMs also hold for decoder-based large language models (LLMs)"
  - [section] "our findings seem to be transferable to generative large language models" and "we get comparable results when extracting attribution scores from a large language model"
  - [corpus] Weak - the single LLM experiment (Vicuna) is limited and may not generalize to all LLMs
- Break condition: If the LLM's pre-training corpus doesn't align with the target task domain, or if the few-shot examples in the prompt dominate the model's behavior more than expected.

## Foundational Learning

- Concept: Pre-trained transformer models and their adaptation methods (fine-tuning vs prompting)
  - Why needed here: The entire comparison between PBMs and FTMs depends on understanding how these adaptation methods affect model behavior and attribution quality
  - Quick check question: What is the fundamental difference between how fine-tuning and prompting adapt a pre-trained model to a new task?

- Concept: Attribution methods (attention, Integrated Gradients, Shapley Value Sampling)
  - Why needed here: The paper directly compares these three methods for extracting explanations, requiring understanding of their computational approaches and theoretical properties
  - Quick check question: How does Shapley Value Sampling differ from Integrated Gradients in calculating feature importance?

- Concept: Evaluation metrics for explainability (plausibility and faithfulness)
  - Why needed here: The paper uses these specific metrics to assess attribution quality, requiring understanding of what they measure and how they're calculated
  - Quick check question: What is the key difference between plausibility and faithfulness as evaluation metrics for explanations?

## Architecture Onboarding

- Component map: Input → Tokenization → Prompt construction (PBM) or direct input (FTM) → Pre-trained transformer → Prediction head → Attribution extraction (attention/IG/Shapley) → Plausibility/Faithfulness evaluation
- Critical path: Input → Model prediction → Attribution extraction → Evaluation → Comparison across conditions
- Design tradeoffs:
  - PBM vs FTM: PBM preserves more pre-trained knowledge but requires prompt engineering; FTM adapts more to task but risks overfitting with limited data
  - Attribution method choice: Shapley provides best quality but highest computational cost; attention is fastest but least accurate; IG offers middle ground
  - Training size consideration: Very small datasets favor PBMs and Shapley; larger datasets may reduce the advantage gap
- Failure signatures:
  - PBM explanations becoming implausible: indicates poor prompt design or task mismatch with pre-training
  - Shapley computational failure: suggests input length too long for permutation calculations
  - Faithfulness scores not improving with better plausibility: suggests attribution method captures surface patterns rather than true decision factors
- First 3 experiments:
  1. Run PBM and FTM on a small subset (8 samples) of TSE with all three attribution methods, verify plausibility ordering matches paper
  2. Compare Shapley vs IG on a single instance with known important tokens to verify Shapley captures feature interactions
  3. Test attention-based attribution on a sentence with clear sentiment-bearing words to confirm attention's tendency to focus on functional words rather than content words

## Open Questions the Paper Calls Out

- How do soft prompts compare to discrete prompts in terms of plausibility and faithfulness of attribution scores in low-resource settings?
- Do the trends observed for encoder-based PBMs and FTMs also hold for encoder-decoder models when extracting attribution scores?
- How do the plausibility and faithfulness scores of attribution methods vary across different languages and domains in low-resource settings?
- How do the plausibility and faithfulness scores of attribution methods change when using larger or smaller pre-trained models in low-resource settings?

## Limitations
- Results are based on limited datasets (TSE and e-SNLI) and specific model architectures
- Attribution quality comparison relies on proxy metrics that may not capture real-world interpretability needs
- Shapley Value Sampling results are only validated against two datasets, raising generalizability questions
- Study does not address domain adaptation issues when pre-trained models encounter out-of-distribution data

## Confidence
- **High confidence** in the comparative ranking of attribution methods (Shapley > Integrated Gradients > Attention)
- **Medium confidence** in the PBM vs FTM comparison for explanation quality
- **Medium confidence** in the transferability to LLMs

## Next Checks
1. Cross-domain validation: Test PBM vs FTM attribution quality comparison on diverse domains (medical, legal, code) to assess generalizability
2. Ablation on prompt quality: Systematically vary prompt quality to determine whether PBMs maintain explanation advantage
3. Human evaluation extension: Conduct human studies to validate whether plausibility and faithfulness metrics align with human judgments of explanation quality