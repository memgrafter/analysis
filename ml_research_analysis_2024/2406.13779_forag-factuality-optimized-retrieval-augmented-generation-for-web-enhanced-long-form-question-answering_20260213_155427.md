---
ver: rpa2
title: 'FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced
  Long-form Question Answering'
arxiv_id: '2406.13779'
source_url: https://arxiv.org/abs/2406.13779
tags:
- answer
- which
- factuality
- evaluation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes FoRAG, a factuality-optimized retrieval augmented
  generation method for web-enhanced long-form question answering. The key contributions
  are: (1) an outline-enhanced generator that produces well-structured, multifaceted
  answers by first drafting an organizational pattern and outline; (2) a doubly fine-grained
  RLHF framework that optimizes factuality through automated evaluation and reward
  modeling at multiple granularities.'
---

# FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering

## Quick Facts
- arXiv ID: 2406.13779
- Source URL: https://arxiv.org/abs/2406.13779
- Reference count: 40
- Key outcome: FoRAG achieves SOTA performance on English and Chinese benchmarks, with FoRAG-L-7B outperforming WebGPT-175B using only 1/24 of the parameters

## Executive Summary
This paper introduces FoRAG, a method for improving factuality in web-enhanced long-form question answering. The approach combines an outline-enhanced generator with a doubly fine-grained RLHF framework. The outline stage provides structured planning before expansion, while the fine-grained RLHF optimizes factuality through automated evaluation and reward modeling at multiple granularities. Extensive experiments demonstrate that FoRAG achieves state-of-the-art results on both English and Chinese benchmarks.

## Method Summary
FoRAG consists of two main components: an outline-enhanced generator and a doubly fine-grained RLHF framework. The generator first creates an organizational pattern and outline, then systematically expands each point to produce well-structured answers. The RLHF framework decomposes answers into segments at multiple granularities (sentence-level and subclaim-level) and provides rewards at both sequence and token levels. This approach addresses the sparse feedback problem in conventional RLHF and optimizes factuality through automated evaluation and reward modeling.

## Key Results
- FoRAG achieves state-of-the-art performance on both WebCPM (Chinese) and WebGLM-QA (English) benchmarks
- The FoRAG-L-7B model fine-tuned on Llama2-7B-chat outperforms WebGPT-175B on coherence, helpfulness, and factuality
- Subclaim-level evaluation performs better than sentence-level evaluation for factuality optimization
- Factuality optimization significantly raises factual consistency scores while maintaining answer length

## Why This Works (Mechanism)

### Mechanism 1: Outline-Enhanced Generation
- **Claim**: The two-stage outline-then-expand approach improves coherence by providing structured planning before detailed response generation
- **Core assumption**: LLMs can follow structured prompts to generate coherent organizational patterns that guide subsequent expansion
- **Evidence anchors**: Abstract mentions "clear logic in the generation of multifaceted answers" and section 4.1 describes the two-stage process aligning with human reasoning patterns

### Mechanism 2: Doubly Fine-Grained RLHF
- **Claim**: Decomposing answers into segments at multiple granularities provides denser reward signals than holistic RLHF
- **Core assumption**: More granular evaluation and reward modeling leads to better alignment than sparse holistic rewards, especially for long-form generation
- **Evidence anchors**: Abstract mentions "automatic evaluation and reward modeling in different levels of granularity" and section 5.2 explains how L non-zero rewards alleviate sparse feedback problems

### Mechanism 3: Synergistic Improvement
- **Claim**: The combination of outline structure and fine-grained factuality optimization creates synergistic improvements
- **Core assumption**: Better structured outputs are easier to evaluate for factuality, and factuality optimization preserves structural benefits
- **Evidence anchors**: Section 6.2 shows factuality optimization significantly raises factual consistency scores while section 6.3 demonstrates subclaim-level evaluation performs best

## Foundational Learning

- **Concept**: Markov Decision Process formulation of text generation
  - Why needed here: The paper frames response generation as an MDP to justify using reinforcement learning for optimization
  - Quick check question: In the MDP formulation, what constitutes the state at each time step during generation?

- **Concept**: Reward modeling and preference learning
  - Why needed here: The factuality optimization relies on training reward models to estimate human preferences for factual consistency
  - Quick check question: What is the key difference between sequence-level and token-level reward modeling in the context of this paper?

- **Concept**: Granularity in evaluation and reward assignment
  - Why needed here: The doubly fine-grained framework depends on understanding how different levels of granularity affect training signals
  - Quick check question: How does subclaim-level evaluation differ from sentence-level evaluation in terms of decomposition and aggregation?

## Architecture Onboarding

- **Component map**: Query → Web search → Context retrieval → Outline stage → Expansion stage → Factuality evaluation → Reward modeling → RL optimization
- **Critical path**: The entire pipeline from query input through web search, outline generation, expansion, evaluation, and RL optimization
- **Design tradeoffs**: Outline stage adds inference latency but improves coherence; fine-grained RLHF increases training time but provides better factuality signals; multi-granularity evaluation requires more complex implementation but captures different error types
- **Failure signatures**: Outline stage producing irrelevant or repetitive points; factuality optimization causing mode collapse or overly conservative responses; computational costs becoming prohibitive for real-time deployment
- **First 3 experiments**:
  1. Compare coherence metrics with and without outline stage on a small dataset
  2. Test different granularities (sentence vs subclaim) for factuality evaluation
  3. Measure training time and factuality improvements across different reward modeling approaches

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the doubly fine-grained RLHF framework perform when applied to other language generation tasks beyond web-enhanced LFQA, such as summarization or dialogue generation?
- **Basis in paper**: The authors mention that their framework "can also be generalized to other RLHF tasks."
- **Why unresolved**: The paper focuses solely on evaluating the framework for web-enhanced LFQA and does not provide empirical evidence for its effectiveness on other tasks

### Open Question 2
- **Question**: What is the impact of using different aggregation functions (average, minimum, maximum) in the subclaim-level evaluation on the overall factuality of the generated answers?
- **Basis in paper**: The authors mention using different aggregation functions in the subclaim-level evaluation but do not compare their effects
- **Why unresolved**: The paper does not provide a detailed analysis of how the choice of aggregation function affects the final factuality scores

### Open Question 3
- **Question**: How does the performance of FoRAG vary with different levels of outline complexity in the outline-enhanced generator?
- **Basis in paper**: The paper discusses the use of outlines but does not explore how varying the complexity of outlines affects the generated answers
- **Why unresolved**: The paper does not provide an analysis of the relationship between outline complexity and the quality of the generated answers

## Limitations
- Automated evaluation reliability may be limited in detecting nuanced factual errors, particularly in cross-lingual contexts
- The effectiveness of outline-enhanced generation may depend on specific prompt engineering and may not generalize across different question types or domains
- The doubly fine-grained RLHF framework requires significant computational resources for both training and inference

## Confidence
- **High confidence**: The basic claim that FoRAG achieves state-of-the-art performance on the tested benchmarks, supported by direct experimental comparisons
- **Medium confidence**: The mechanism claims about why the doubly fine-grained RLHF framework works better than holistic approaches
- **Medium confidence**: The synergy claim between outline-enhanced generation and factuality optimization

## Next Checks
1. Conduct ablation study on granularity levels to isolate the contribution of different granularities
2. Evaluate FoRAG on a multilingual dataset to verify factuality optimization generalizes beyond English-Chinese
3. Perform human evaluation comparison to validate automated evaluation results, particularly for edge cases