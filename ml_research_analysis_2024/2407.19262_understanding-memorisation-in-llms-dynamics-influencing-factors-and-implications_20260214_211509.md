---
ver: rpa2
title: 'Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and Implications'
arxiv_id: '2407.19262'
source_url: https://arxiv.org/abs/2407.19262
tags:
- epoch
- accuracy
- strings
- random
- memorisation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the dynamics of memorization in large language
  models (LLMs) by training models to memorize random strings, which allows isolating
  memorization from other phenomena like in-context learning. The authors observe
  two distinct phases in memorization: a "Guessing-Phase" where models learn the token
  distribution and a "Memorization-Phase" where they recall tokens from memorized
  prefixes.'
---

# Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and Implications

## Quick Facts
- **arXiv ID:** 2407.19262
- **Source URL:** https://arxiv.org/abs/2407.19262
- **Reference count:** 40
- **Primary result:** Study reveals two-phase memorization dynamics in LLMs and shows higher entropy strings are easier to memorize, challenging existing assumptions about memorization difficulty.

## Executive Summary
This paper investigates memorization dynamics in large language models by training them on random strings, which isolates memorization from other phenomena like in-context learning. The authors discover a consistent two-phase memorization process: an initial Guessing-Phase where models learn token distributions, followed by a Memorization-Phase where they recall specific tokens from memorized prefixes. They find that higher entropy strings are memorized faster, that sequential memorization causes forgetting of earlier strings while accelerating later ones, and that models require both local prefixes and global context for accurate token recollection. These findings highlight limitations in current memorization measures and suggest new directions for quantifying memorization in LLMs.

## Method Summary
The study uses synthetic random strings generated from alphabets of varying sizes with controlled entropy levels. Models from different families (Pythia, Phi, Llama2) are trained to minimize cross-entropy loss over 100 epochs (300 for untrained models) using linear learning rate decay. Token-level recollection accuracy is measured using greedy decoding, and memorization dynamics are tracked epoch-by-epoch through metrics including aggregate probability over the alphabet, entropy of token distribution, and KL-divergence from the true distribution. The approach isolates memorization by using random strings that prevent in-context learning shortcuts.

## Key Results
- Models exhibit two distinct phases in memorization: a Guessing-Phase (learning token distributions) followed by a Memorization-Phase (recalling specific tokens)
- Higher entropy strings are memorized faster despite being harder to guess during the Guessing-Phase
- Sequential memorization causes forgetting of earlier strings but accelerates memorization of later strings
- Both local prefixes and global context are required for accurate token recollection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The two-phase memorization process emerges because the model first learns the token distribution before learning exact token sequences.
- **Mechanism:** During the Guessing-Phase, the model allocates probability mass across the alphabet to match the true distribution, maximizing entropy. Once distribution learning is complete, it enters the Memorization-Phase where it reduces entropy by memorizing exact tokens conditioned on local prefixes.
- **Core assumption:** The model prioritizes learning the distribution over memorization until distribution learning is complete.
- **Evidence anchors:**
  - [abstract] "we find consistent phases of the dynamics across families of models"
  - [section] "In the initial Guessing-Phase, the models are learning which tokens are in A and separate those from V (rise in aggregate probability)"
- **Break condition:** If the model could learn exact sequences without first learning the distribution, the two-phase pattern would disappear.

### Mechanism 2
- **Claim:** Higher entropy strings are easier to memorize because they require less precise token prediction per position.
- **Mechanism:** For strings with uniform PA (high entropy), each token prediction is less constrained, making it easier for the model to guess correctly during the Guessing-Phase. Once in the Memorization-Phase, the model can more easily encode the less predictable patterns.
- **Core assumption:** Memorization difficulty correlates inversely with per-token prediction uncertainty.
- **Evidence anchors:**
  - [abstract] "we find consistent phases of the dynamics across families of models (Pythia, Phi and Llama2)"
  - [section] "We find that the entropy of the distribution of tokens in the random strings affects the two-phase dynamics"
- **Break condition:** If compression-based storage mechanisms were dominant, lower entropy strings would be easier to memorize.

### Mechanism 3
- **Claim:** Sequential memorization causes forgetting of earlier strings but accelerates memorization of later strings through transfer learning.
- **Mechanism:** When the model memorizes new strings, it overwrites representations used for earlier strings (forgetting). However, the model also develops generalized memorization capabilities that make subsequent memorization faster.
- **Core assumption:** The model has limited capacity for maintaining multiple memorized sequences simultaneously.
- **Evidence anchors:**
  - [abstract] "We also show that sequential exposition to different random strings has a significant effect on memorisation."
  - [section] "Our results indicate that models forget old random strings when they get repeatedly exposed to new ones"
- **Break condition:** If the model could perfectly maintain all memorized strings, forgetting would not occur.

## Foundational Learning

- **Concept:** Two-phase learning dynamics (distribution learning → sequence memorization)
  - **Why needed here:** Understanding why models first learn token distributions before memorizing sequences is crucial for interpreting memorization metrics and developing better measurement approaches.
  - **Quick check question:** If a model could skip directly to memorizing sequences, would we still observe the Guessing-Phase in our accuracy plots?

- **Concept:** Entropy and memorability relationship
  - **Why needed here:** The counterintuitive finding that higher entropy strings are easier to memorize challenges common assumptions about memorization difficulty and has implications for privacy risk assessment.
  - **Quick check question:** Given two strings with the same alphabet size, one uniform and one skewed, which would be easier to memorize and why?

- **Concept:** Local vs. global context in token recollection
  - **Why needed here:** Understanding that both local prefixes and global context (token distribution) are required for accurate token recollection is essential for developing accurate memorization measures and understanding model behavior.
  - **Quick check question:** If you only preserve the local prefix but randomize the global context, how would this affect the model's ability to recall memorized tokens?

## Architecture Onboarding

- **Component map:** Random string generation → Model training (100 epochs) → Accuracy computation → Phase identification → Analysis of influencing factors
- **Critical path:** String generation → Model training → Accuracy computation at each epoch → Phase identification (Guessing vs Memorization) → Analysis of influencing factors
- **Design tradeoffs:** Using random strings ensures clean isolation of memorization but may not fully capture real-world data patterns. The choice of alphabet size and entropy levels balances experimental control with practical relevance.
- **Failure signatures:** If models show no clear two-phase pattern, if accuracy plateaus below the random guess baseline, or if memorization dynamics don't correlate with entropy levels.
- **First 3 experiments:**
  1. Train a model on a low entropy (skewed distribution) random string and plot accuracy over epochs to verify the two-phase pattern.
  2. Compare memorization speed for strings with different alphabet sizes but similar entropy to isolate the effect of entropy from alphabet size.
  3. Test token recollection accuracy with varying prefix lengths and global context conditions to verify the local prefix + global context requirement.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the exact mechanism by which higher entropy strings are easier to memorize, despite being harder to guess during the Guessing-Phase?
- **Basis in paper:** [explicit] The paper observes that strings with higher entropy are memorized faster, but does not provide a definitive explanation for this phenomenon.
- **Why unresolved:** The paper rules out compression as a potential mechanism and notes that creating strings where small prefixes are more predictive does not necessarily make them more memorable. The relationship between entropy and memorization remains an open question.
- **What evidence would resolve it:** Experiments testing different memorization techniques (e.g., varying model architectures, training procedures) on strings with controlled entropy levels, and analyzing the resulting memorization dynamics and patterns.

### Open Question 2
- **Question:** How can we develop a robust measure of memorization that accounts for the Guessing-Phase and the random order of token memorization observed in the paper?
- **Basis in paper:** [inferred] The paper highlights the limitations of existing memorization measures, which may overestimate memorization by not accounting for in-context learning during the Guessing-Phase and underestimate it by focusing on contiguous token sequences.
- **Why unresolved:** Current measures are based on detecting contiguous memorized sequences, but the paper shows that tokens are memorized in random order. A new measure is needed that can accurately quantify memorization at the token level while discounting for in-context learning.
- **What evidence would resolve it:** Development and validation of a new memorization measure that operates at the token level, discounts for in-context learning, and accurately reflects the degree of memorization observed in the paper's experiments.

### Open Question 3
- **Question:** How does the ability of models to infer the alphabet distribution via in-context learning affect the length of the Guessing-Phase and overall memorization dynamics?
- **Basis in paper:** [explicit] The paper observes that Llama2 models exhibit strong in-context learning abilities and effectively shorten the Guessing-Phase to zero, while other models go through a distinct Guessing-Phase.
- **Why unresolved:** The paper does not provide a detailed analysis of how in-context learning ability varies across models and how it specifically impacts the Guessing-Phase and overall memorization dynamics.
- **What evidence would resolve it:** Systematic experiments comparing the memorization dynamics of models with varying in-context learning abilities, controlling for other factors, and analyzing the relationship between in-context learning, Guessing-Phase length, and overall memorization speed.

## Limitations
- The use of synthetic random strings may not fully capture the complexity and structure of real-world data patterns
- Sequential memorization experiments only consider a limited number of strings (up to 10), which may not scale to real training scenarios
- The local prefix + global context requirement for token recollection is demonstrated empirically but lacks theoretical grounding

## Confidence
**High Confidence** (Supported by direct experimental evidence and consistent patterns):
- The existence of two distinct phases in memorization dynamics (Guessing-Phase and Memorization-Phase)
- The relationship between alphabet size and memorization speed
- Sequential memorization effects (forgetting of earlier strings and acceleration of later memorization)

**Medium Confidence** (Well-supported but with some gaps or limitations):
- The relationship between entropy and memorization difficulty
- The requirement for both local prefixes and global context in token recollection
- The general characterization of memorization dynamics across model families

**Low Confidence** (More speculative or limited by methodology):
- Claims about how these findings generalize to real-world data
- The specific mechanism by which higher entropy facilitates memorization
- Long-term forgetting effects with extensive sequential memorization

## Next Checks
1. **Cross-validate entropy findings with structured data**: Test whether the counterintuitive finding that higher entropy strings are easier to memorize holds when using structured synthetic data with patterns (e.g., repeating substrings, hierarchical structures) that more closely resemble natural language. This would validate whether the entropy-memorization relationship is a fundamental property or an artifact of random string generation.

2. **Measure retention with extended sequential memorization**: Extend the sequential memorization experiments to include 50-100 strings rather than 10, tracking both new string memorization speed and retention of earlier strings over time. This would test whether the observed forgetting is linear, saturating, or exhibits more complex dynamics at scale.

3. **Test alternative prefix/context combinations**: Systematically vary the availability of local prefix information (prefix length from 0 to full string) while controlling global context, and vice versa (full prefix with randomized context). This would more precisely quantify the contribution of each component to token recollection accuracy and test the robustness of the local prefix + global context requirement.