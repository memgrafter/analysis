---
ver: rpa2
title: The Effect of Similarity Measures on Accurate Stability Estimates for Local
  Surrogate Models in Text-based Explainable AI
arxiv_id: '2406.15839'
source_url: https://arxiv.org/abs/2406.15839
tags:
- similarity
- measures
- measure
- have
- perturbations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how different similarity measures affect
  the accuracy of stability estimates in text-based explainable AI (XAI). The authors
  examine various similarity measures designed for text-based ranked lists, including
  Kendall's Tau, Spearman's Footrule, and Rank-biased Overlap, to determine how changes
  in the type of measure or threshold of success affect the conclusions generated
  from common adversarial attack processes.
---

# The Effect of Similarity Measures on Accurate Stability Estimates for Local Surrogate Models in Text-based Explainable AI

## Quick Facts
- arXiv ID: 2406.15839
- Source URL: https://arxiv.org/abs/2406.15839
- Reference count: 19
- Key outcome: Certain similarity measures are overly sensitive, causing exaggerated instability estimates in adversarial XAI attacks.

## Executive Summary
This paper investigates how different similarity measures affect the accuracy of stability estimates in text-based explainable AI (XAI). The authors examine various similarity measures designed for text-based ranked lists, including Kendall's Tau, Spearman's Footrule, and Rank-biased Overlap, to determine how changes in the type of measure or threshold of success affect the conclusions generated from common adversarial attack processes. They find that certain measures are overly sensitive, resulting in erroneous estimates of stability. Specifically, Kendall's Tau is extremely sensitive, leading to nearly 100% attack success rates across all combinations of threshold and dataset, rendering it unsuitable for use in text-based adversarial XAI attacks. Jaccard, Spearman, and Spearmanw also show excessive sensitivity for higher similarity thresholds. The study concludes that practitioners and researchers should choose similarity measures judiciously, considering how sensitive a measure is to the inherent variation between explanations.

## Method Summary
The paper investigates how similarity measures affect stability estimates in text-based XAI by conducting adversarial attacks on LIME explanations. The method involves fine-tuning DistilBERT models on two datasets (Gender bias Twitter dataset and symptoms to diagnosis dataset), generating LIME explanations, and then running adversarial attacks using a greedy search procedure with various similarity measures (Kendall's Tau, Spearman's Footrule, Rank-biased Overlap, Jaccard). Attack success rates, perturbation quality metrics (USE similarity, perplexity), and minimal perturbations are calculated for each similarity measure and threshold combination.

## Key Results
- Kendall's Tau shows nearly 100% attack success rates across all threshold and dataset combinations, indicating excessive sensitivity.
- Jaccard, Spearman, and Spearmanw measures also exhibit high sensitivity at higher similarity thresholds.
- Weighted measures (Spearman-w, Jaccard-w) may offer a balanced alternative, depending on the quality of original weights.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Certain similarity measures are overly sensitive, causing exaggerated instability estimates in adversarial XAI attacks.
- Mechanism: The similarity measure determines how small perturbations in text are evaluated. Highly sensitive measures like Kendall's Tau detect minor reordering as large differences, leading to false positives for instability.
- Core assumption: The instability of LIME's sampling process introduces inherent variation that some similarity measures misinterpret as adversarial manipulation.
- Evidence anchors:
  - [abstract]: "Certain measures are found to be overly sensitive, resulting in erroneous estimates of stability."
  - [section]: "Kendall's tau is extremely sensitive with nearly 100% attack success across every combination of threshold and dataset."
  - [corpus]: The paper is surrounded by related work on adversarial attacks and stability estimation, supporting the relevance of measure sensitivity.
- Break Condition: If the base model's explanation is inherently stable or if the similarity measure is adjusted to be less sensitive, the exaggerated instability would not occur.

### Mechanism 2
- Claim: The choice of similarity measure affects the number of perturbations needed to achieve a successful adversarial attack.
- Mechanism: Measures that require substantial changes (like Jaccard or Spearman) force more perturbations to meet the threshold, while sensitive measures (like Kendall) need fewer.
- Core assumption: The greedy search algorithm selects the first viable perturbation, so measure sensitivity directly impacts perturbation count.
- Evidence anchors:
  - [section]: "Reducing the number of perturbations also helps maintain the quality of the perturbed text as ideal word replacements are difficult to generate."
  - [section]: "We see no average ending similarities appreciably below the success threshold... and there are no notable signs of consistently low perturbation rates."
  - [corpus]: The neighbor papers focus on robustness and attack efficiency, aligning with the mechanism of perturbation minimization.
- Break Condition: If a different search strategy is used (e.g., one that optimizes for minimal perturbations), the relationship between measure choice and perturbation count may change.

### Mechanism 3
- Claim: Weighted similarity measures can balance sensitivity and perturbation quality, offering a middle ground between coarse and overly sensitive measures.
- Mechanism: By applying weights from the original explanation, measures like Spearman-w and Jaccard-w emphasize changes to more important features, reducing false positives while maintaining attack efficacy.
- Core assumption: The importance weights from LIME accurately reflect feature significance, making weighted measures more appropriate.
- Evidence anchors:
  - [section]: "We apply the same weights to each measure, which are the weights associated with the original unperturbed explanation which are normalized with respect to the absolute value of the weight."
  - [section]: "Spearman w may prove particularly useful if the original explanatory weights are deemed important or the fine-tuning of RBO too cumbersome."
  - [corpus]: The corpus includes work on alignment and feature importance, supporting the use of weighted measures.
- Break Condition: If the original weights are unreliable or if the measure's weighting scheme is poorly tuned, the balance may be lost, leading to either excessive sensitivity or coarseness.

## Foundational Learning

- Concept: Adversarial attacks on XAI
  - Why needed here: Understanding how perturbations are used to test the stability of explanations is central to the paper's investigation.
  - Quick check question: What is the goal of an adversarial attack in the context of XAI?
- Concept: Similarity measures for ranked lists
  - Why needed here: The paper evaluates various measures (Kendall's Tau, Spearman's Footrule, etc.) to determine their impact on stability estimates.
  - Quick check question: How does Kendall's Tau differ from Spearman's Footrule in comparing ranked lists?
- Concept: Greedy search algorithms
  - Why needed here: The perturbation generation process uses a greedy approach, which affects how measures influence attack success.
  - Quick check question: Why might a greedy search algorithm be inefficient for finding minimal perturbations?

## Architecture Onboarding

- Component map: Original text document -> LIME explanation -> Greedy search perturbation generator -> Similarity measure evaluation -> Attack success evaluator -> Perturbation quality assessment
- Critical path: Perturbation -> Similarity Evaluation -> Success Check -> Quality Assessment
- Design tradeoffs:
  - Sensitivity vs. robustness: Highly sensitive measures detect small changes but may overestimate instability.
  - Computational cost vs. accuracy: More complex measures may provide better results but at higher computational expense.
  - Perturbation quality vs. attack success: Fewer perturbations preserve text quality but may reduce attack effectiveness.
- Failure signatures:
  - Excessive sensitivity: Nearly 100% attack success rates (e.g., Kendall's Tau).
  - Poor quality: High perplexity and low USE similarity in perturbed text.
  - Inefficient search: No consistent pattern in perturbation counts across measures.
- First 3 experiments:
  1. Compare attack success rates of Kendall's Tau vs. Spearman's Footrule at 50% threshold.
  2. Measure perplexity and USE similarity for Jaccard vs. RBO0.5 perturbed texts.
  3. Evaluate the effect of weighting parameters (0.5, 0.7, 0.9) on RBO attack success and perturbation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different similarity measures compare in terms of sensitivity to inherent variation between explanations, and which measures are most robust?
- Basis in paper: [explicit] The paper investigates how changes in the type of similarity measure or threshold of success affect the conclusions generated from common adversarial attack processes. It finds that certain measures are overly sensitive, resulting in erroneous estimates of stability.
- Why unresolved: While the paper identifies some measures as overly sensitive, it does not provide a comprehensive comparison of all measures in terms of their sensitivity to inherent variation.
- What evidence would resolve it: A systematic evaluation of various similarity measures on datasets with known levels of inherent variation in explanations would help determine which measures are most robust to such variation.

### Open Question 2
- Question: Does the choice of similarity measure impact the generation of minimum viable perturbations, and if so, how?
- Basis in paper: [explicit] The paper asks whether the choice of similarity measure affects the generation of minimum viable perturbations, defined as the fewest possible perturbations needed to reduce the similarity below a threshold while maintaining the structure and meaning of the original document.
- Why unresolved: The paper finds no strong indications that the choice of similarity measure significantly impacts the number of perturbations required, but it does not explore alternative search strategies that might be more effective in finding minimal perturbations.
- What evidence would resolve it: Experiments comparing the performance of different similarity measures in conjunction with alternative search strategies, such as those using a minimum reduction threshold or more comprehensive exploration of the search space, would provide insights into the impact of measure choice on minimal perturbation generation.

### Open Question 3
- Question: How transferable are the findings on similarity measures to other XAI methods beyond LIME?
- Basis in paper: [explicit] The paper acknowledges that its findings may not be directly applicable to other XAI methods, particularly those with less inherent instability than LIME.
- Why unresolved: The paper's focus on LIME limits the generalizability of its conclusions to other XAI methods. Different methods may exhibit varying levels of sensitivity to different similarity measures.
- What evidence would resolve it: Replicating the experiments with a diverse set of XAI methods, including those with different levels of inherent instability, would help determine the transferability of the findings to other methods.

## Limitations
- The study focuses on text-based explanations from LIME, limiting generalizability to other XAI methods.
- Results are based on two specific datasets (GB and S2D), which may not represent all text classification scenarios.
- The greedy search algorithm may not find globally optimal minimal perturbations, potentially overestimating measure sensitivity.

## Confidence

**High Confidence:** Kendall's Tau shows excessive sensitivity leading to nearly 100% attack success rates across all conditions.

**Medium Confidence:** Jaccard and Spearman measures exhibit similar sensitivity patterns at higher thresholds.

**Medium Confidence:** Weighted measures (Spearman-w, Jaccard-w) provide balanced alternatives, though their performance depends on the quality of original weights.

## Next Checks

1. Test the same similarity measures on non-LIME explanation methods (e.g., SHAP) to assess generalizability.
2. Validate findings across additional text classification datasets with different domains and class distributions.
3. Compare greedy search results with more sophisticated optimization algorithms to determine if minimal perturbations are truly achieved.