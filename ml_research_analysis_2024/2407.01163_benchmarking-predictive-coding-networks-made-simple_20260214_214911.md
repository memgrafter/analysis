---
ver: rpa2
title: Benchmarking Predictive Coding Networks -- Made Simple
arxiv_id: '2407.01163'
source_url: https://arxiv.org/abs/2407.01163
tags:
- learning
- energy
- state
- training
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PCX, a new library for training predictive
  coding networks (PCNs) with a focus on performance and simplicity. The authors implement
  a comprehensive set of benchmarks and compare multiple PC algorithms, including
  standard PC, incremental PC, Monte Carlo PC, and nudging methods, on tasks like
  image classification and generation.
---

# Benchmarking Predictive Coding Networks -- Made Simple

## Quick Facts
- arXiv ID: 2407.01163
- Source URL: https://arxiv.org/abs/2407.01163
- Reference count: 40
- Key outcome: PCX library achieves state-of-the-art results on CIFAR-100 and Tiny ImageNet while providing open-source benchmarks for predictive coding networks

## Executive Summary
This paper introduces PCX, a JAX-based library for training predictive coding networks (PCNs) with a focus on performance and simplicity. The authors implement comprehensive benchmarks comparing multiple PC algorithms (standard PC, incremental PC, Monte Carlo PC, and nudging methods) on tasks like image classification and generation. Their experiments achieve state-of-the-art results on CIFAR-100 and Tiny ImageNet, while also revealing key limitations of PCNs related to scalability and training stability. The work provides open-source code, benchmarks, and insights into the challenges of scaling PCNs.

## Method Summary
The paper introduces PCX, a library built on JAX that enables efficient training of predictive coding networks through JIT compilation and optimized primitives. The library implements multiple PC algorithms including standard PC, incremental PC (iPC), Monte Carlo PC (MCPC), and nudging variants (positive/negative/centered). Experiments span discriminative tasks (MNIST, FashionMNIST, CIFAR-10/100, Tiny ImageNet) and generative tasks (autoencoders, MCPC) using various architectures from simple MLPs to VGG models. The library provides extensive hyperparameter tuning and ablation studies to identify optimal configurations.

## Key Results
- PCX achieves state-of-the-art accuracy on CIFAR-100 and competitive results on Tiny ImageNet using nudging algorithms
- Nudging methods (PN/NN) consistently outperform standard PC and other variants, particularly on complex tasks
- Energy imbalance across layers identified as key bottleneck for scaling PCNs to deeper architectures
- JAX-based implementation achieves performance on par with traditional backpropagation methods

## Why This Works (Mechanism)

### Mechanism 1
PCX library enables scalable PCN training by leveraging JAX's JIT compilation, providing a user-friendly, deep-learning oriented interface with minimal learning curve. PCX wraps JAX primitives to optimize predictive coding computations, making them as efficient as traditional backpropagation methods. Core assumption: JAX's JIT compilation can effectively parallelize PCN computations and the interface is intuitive enough for deep learning practitioners.

### Mechanism 2
The nudging algorithms (positive, negative, and centered) improve PCN performance by guiding the output towards the target, addressing the instability of standard PC methods. By nudging the output towards the target, the algorithm stabilizes the learning process and improves convergence. Core assumption: The instability in standard PC methods is primarily due to the lack of guidance towards the target, and nudging effectively addresses this issue.

### Mechanism 3
The analysis of energy propagation and training stability reveals that the imbalance in energy distribution across layers is a key limitation for scaling PCNs. By studying the energy distribution, we can identify the root cause of instability and develop strategies to address it, such as improving initialization or optimization techniques. Core assumption: The energy imbalance is the primary cause of instability, and addressing it will enable scaling to deeper architectures.

## Foundational Learning

- **Predictive Coding Networks (PCNs)**: Understanding the core principles of PCNs is essential for using the PCX library and interpreting the results of the experiments. Why needed: Core algorithm for training and inference. Quick check: What is the main difference between PCNs and traditional backpropagation methods in terms of information flow?

- **JAX and JIT Compilation**: Familiarity with JAX and JIT compilation is necessary for understanding how PCX achieves its efficiency and for potentially contributing to the library. Why needed: Underlying framework for JIT compilation and parallelization. Quick check: How does JIT compilation improve the performance of PCN computations?

- **Energy-Based Models and Variational Free Energy**: Understanding energy-based models and variational free energy is crucial for interpreting the analysis of energy propagation and stability in PCNs. Why needed: Crucial for interpreting energy propagation analysis. Quick check: How is the variational free energy used in PCNs to optimize the model parameters and latent states?

## Architecture Onboarding

- **Component map**: PCX library -> JAX framework -> Predictive Coding Networks -> Datasets and models
- **Critical path**: 1. Install PCX library and its dependencies (JAX) 2. Familiarize with the PCX interface and tutorials 3. Choose a benchmark task and model architecture 4. Configure hyperparameters and training settings 5. Run experiments and analyze results
- **Design tradeoffs**: Performance vs. ease of use: PCX aims to provide a balance between efficiency and user-friendliness. Flexibility vs. simplicity: The library supports various PC algorithms and models, but may require more complex configurations for advanced use cases.
- **Failure signatures**: Poor performance: May indicate issues with hyperparameters, initialization, or model architecture. Instability: Could be caused by energy imbalance, inappropriate learning rates, or insufficient training steps. Scalability limitations: May arise from computational constraints or architectural bottlenecks.
- **First 3 experiments**: 1. Train a simple MLP on MNIST using standard PC and compare performance with backpropagation. 2. Evaluate the impact of nudging algorithms (PN, NN, CN) on a CIFAR-10 classification task. 3. Analyze the energy propagation in a deep VGG model and identify potential bottlenecks for scaling.

## Open Questions the Paper Calls Out

### Open Question 1
Why does PCN performance degrade with deeper architectures (VGG-7 vs VGG-5) on complex datasets like Tiny ImageNet? Basis: The authors observe that VGG-5 consistently outperforms VGG-7 on CIFAR-100 and Tiny ImageNet, despite the latter having more layers, and hypothesize this is related to energy propagation issues. Why unresolved: The paper identifies energy imbalance between layers as a potential bottleneck but does not provide a definitive explanation. What evidence would resolve it: Controlled experiments varying layer width, state learning rates, and initialization methods to quantify their impact on energy propagation and accuracy in deeper networks.

### Open Question 2
Can PCNs achieve competitive performance on large-scale datasets (e.g., ImageNet) with current training techniques? Basis: The authors highlight scalability as a major open problem and demonstrate limitations of PCNs on Tiny ImageNet, suggesting further research is needed to scale to more complex datasets. Why unresolved: The paper's experiments are limited to smaller datasets (CIFAR-100, Tiny ImageNet) and do not explore the feasibility of training PCNs on larger, more complex datasets like ImageNet. What evidence would resolve it: Benchmarking PCNs on ImageNet with various architectures, algorithms, and optimization techniques to determine if competitive performance can be achieved.

### Open Question 3
How can the training instability of PCNs with Adam optimizer be addressed, especially for wide layers? Basis: The authors observe that Adam becomes unstable for wide layers, leading to performance degradation, while SGD remains stable. Why unresolved: The paper identifies the issue but does not propose solutions or investigate the underlying causes of the instability. What evidence would resolve it: Developing and testing novel optimization techniques or regularization methods to stabilize Adam training in wide PCNs, and analyzing the interaction between layer width and optimizer performance.

### Open Question 4
Can PCNs be effectively used for out-of-distribution (OOD) detection without requiring specific training for that purpose? Basis: The authors demonstrate that PCNs can assess OOD samples based on energy scores without specific training, but observe a decorrelation between energy and softmax values after state optimization. Why unresolved: While the paper shows potential for OOD detection, the relationship between energy scores and OOD detection accuracy is not fully characterized, and the impact of state optimization on this relationship is unclear. What evidence would resolve it: Systematic evaluation of PCN energy scores for OOD detection across various datasets and comparison with established OOD detection methods, including analysis of the effect of state optimization on detection performance.

## Limitations

- Energy imbalance across layers identified as key bottleneck preventing scaling to deeper architectures
- Training instability with Adam optimizer for wide layers, requiring careful optimization
- Performance degradation on complex datasets (Tiny ImageNet) compared to simpler benchmarks

## Confidence

- JAX's JIT compilation providing "performance on par with traditional backpropagation": High confidence based on systematic benchmarks
- Nudging algorithms (PN/NN) consistently improving performance: Medium confidence, depends heavily on task complexity and network depth
- Energy imbalance being primary cause of instability in deeper networks: High confidence from extensive ablation studies, but mechanism explanation remains speculative
- State-of-the-art results on CIFAR-100 and Tiny ImageNet: Medium confidence due to limited comparison with recent non-PCN methods

## Next Checks

1. Test PCX scalability on architectures deeper than VGG-7 to verify if energy imbalance limitations persist
2. Compare nudging algorithm performance across a wider range of initialization schemes and optimizers
3. Benchmark against state-of-the-art non-PCN models from the same period to contextualize the claimed improvements