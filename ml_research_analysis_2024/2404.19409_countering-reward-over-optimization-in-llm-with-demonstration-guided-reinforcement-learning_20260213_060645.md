---
ver: rpa2
title: Countering Reward Over-optimization in LLM with Demonstration-Guided Reinforcement
  Learning
arxiv_id: '2404.19409'
source_url: https://arxiv.org/abs/2404.19409
tags:
- reward
- rcfd
- house
- demonstrations
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses reward over-optimization (ROO) in LLM finetuning,\
  \ where reinforcement learning can lead to unnatural text and loss of diversity.\
  \ The authors propose Reward Calibration from Demonstration (RCfD), which uses human\
  \ demonstrations and a reward model to recalibrate the reward objective by minimizing\
  \ the distance between the LLM\u2019s and demonstrations\u2019 rewards, rather than\
  \ directly maximizing the reward."
---

# Countering Reward Over-optimization in LLM with Demonstration-Guided Reinforcement Learning

## Quick Facts
- arXiv ID: 2404.19409
- Source URL: https://arxiv.org/abs/2404.19409
- Reference count: 39
- Key outcome: RCfD achieves comparable performance to tuned baselines while mitigating ROO by aligning reward distributions with demonstrations

## Executive Summary
This paper addresses reward over-optimization (ROO) in LLM finetuning, where reinforcement learning can lead to unnatural text and loss of diversity. The authors propose Reward Calibration from Demonstration (RCfD), which uses human demonstrations and a reward model to recalibrate the reward objective by minimizing the distance between the LLM's and demonstrations' rewards, rather than directly maximizing the reward. This approach avoids incentivizing exploitation of the reward model and promotes more natural generation. Experiments on three language tasks show that RCfD achieves comparable performance to carefully tuned baselines while mitigating ROO, and can handle multi-reward settings without hyperparameter tuning.

## Method Summary
RCfD addresses ROO by aligning the LLM's reward distribution with demonstration reward distribution. Instead of maximizing reward directly, it minimizes L2 distance between reward model outputs for LLM generations and demonstration generations. The method uses Proximal Policy Optimization (PPO) with LoRA for efficient fine-tuning, and handles multi-reward settings by independently calibrating and whitening each reward. The approach requires demonstration data paired with prompts and a pretrained reward model, and evaluates performance using average reward, reward distribution alignment (KL divergence), and model-based evaluations (success, factuality, naturalness, verbosity).

## Key Results
- RCfD achieves comparable performance to carefully tuned baselines while mitigating ROO across three language tasks
- The method handles multi-reward settings without hyperparameter tuning by independently calibrating and whitening each reward
- RCfD offers predictable behavior regardless of reward model quality, consistently converging towards the desired reward distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RCfD calibrates reward by aligning LLM reward distribution with demonstration reward distribution
- Mechanism: Instead of maximizing reward directly, RCfD minimizes L2 distance between reward model outputs for LLM generations and demonstration generations, preventing excessive optimization
- Core assumption: Demonstration reward distribution represents desired behavior and serves as stable calibration target
- Evidence anchors:
  - [abstract] "the RCfD objective minimizes the distance between the demonstrations' and LLM's rewards rather than directly maximizing the reward function"
  - [section 3.2] "the RCfD objective: RCf D(x, y) = −||RRM(x, y) − RRM(x, yd)||2 2"
  - [corpus] Found 25 related papers; evidence shows reward over-optimization is recognized problem but RCfD is distinct approach
- Break condition: If demonstration reward distribution doesn't match desired behavior, calibration will optimize toward wrong target

### Mechanism 2
- Claim: Demonstration-guided approach enables predictable behavior without hyperparameter tuning
- Mechanism: By targeting specific reward distribution from demonstrations rather than arbitrary reward maximization, model behavior converges to demonstration-level performance
- Core assumption: Demonstrations provide sufficient signal to guide reward distribution without extensive tuning
- Evidence anchors:
  - [abstract] "the resulting LLM behavior is far more predictable, making it an a priori regularization method"
  - [section 5.2] "RCfD offers predictable behavior regardless of the reward model's quality, consistently converging towards the desired reward distribution"
  - [section 5.3] "RCfD naturally tackles both of these challenges by aligning the policy reward distribution on the demonstrations"
- Break condition: If demonstration set is too small or unrepresentative, predictability breaks down

### Mechanism 3
- Claim: RCfD handles multi-reward settings by independently calibrating and whitening each reward
- Mechanism: Composite rewards are calibrated separately against demonstration distribution, then combined with proper weighting
- Core assumption: Individual reward calibration preserves demonstration behavior characteristics
- Evidence anchors:
  - [section 3.2] "we independently recalibrate and whiten each reward before summing them"
  - [section 5.3] "RCfD successfully handles multi-reward objectives by using demonstrations to guide LLMs toward the desired behavior"
  - [section 5.3] "The RCfD objective automatically recalibrates both rewards by using the demonstration and without tuning any α"
- Break condition: If reward components are highly correlated or dependent, independent calibration may produce suboptimal results

## Foundational Learning

- Concept: Reward over-optimization in RLHF
  - Why needed here: Understanding ROO is essential to grasp why RCfD is necessary and how it addresses the problem
  - Quick check question: What are the two main origins of ROO identified in the related works section?

- Concept: KL regularization in LLM finetuning
  - Why needed here: RCfD is positioned as alternative to KL regularization, so understanding its limitations is crucial
  - Quick check question: Why does KL regularization require computationally expensive hyperparameter tuning according to the abstract?

- Concept: Demonstration-guided reinforcement learning
  - Why needed here: RCfD builds on DGRL concepts, so understanding how demonstrations guide RL is foundational
  - Quick check question: How does DGRL differ from pure imitation learning according to the related works section?

## Architecture Onboarding

- Component map:
  Prompt input -> LLM policy πθ -> Generation y
  Reward model RRM -> Scores RRM(x,y) and RRM(x,yd)
  RCfD objective computation -> L2 distance minimization
  PPO optimizer -> Parameter updates
  Demonstration dataset D = {(xn, ynd)} for calibration target

- Critical path: Prompt → Generation → Reward scoring → RCfD objective → PPO update → Policy improvement
- Design tradeoffs: RCfD trades potential reward maximization for stability and predictability; requires demonstration data availability
- Failure signatures: 
  - Misaligned demonstration reward distribution
  - Poor demonstration quality or representativeness
  - Reward model miscalibration or bias
  - Insufficient demonstration diversity for task coverage

- First 3 experiments:
  1. Sequence-level log-likelihood calibration test with Wikipedia data to verify basic mechanism
  2. Single reward task (movie review sentiment) to compare against tuned baselines
  3. Multi-reward task (summarization with length penalty) to test composite reward handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RCfD's performance compare to methods that use dynamic reward reweighting, such as those proposed by Moskovitz et al. (2023), in terms of computational efficiency and final task performance?
- Basis in paper: [explicit] The paper mentions that Moskovitz et al. (2023) "identify proxy points where ROO occurs and retrain the LLM by dynamically reweighting the rewards not to exceed the proxy points, avoiding the ROO regime" and contrasts this with RCfD which "avoids computing the proxy points using demonstrations, requiring less compute and no gold-standard metrics."
- Why unresolved: The paper does not provide a direct comparison between RCfD and dynamic reward reweighting methods in terms of computational cost or performance on the same tasks.
- What evidence would resolve it: Empirical results comparing RCfD and dynamic reward reweighting on the same tasks, reporting both final performance and computational resources used.

### Open Question 2
- Question: Can RCfD be extended to handle settings where demonstrations are not available for all prompts, and if so, what is the most effective approach?
- Basis in paper: [inferred] The paper mentions in the discussion that "an intuitive extension to remove demonstrations would involve constructing a regressor to predict the reward of the demonstration, potentially using RLAIF methods (Lee et al., 2023)" but states this is left for future research.
- Why unresolved: The paper does not explore or evaluate methods for extending RCfD to settings with incomplete demonstration data.
- What evidence would resolve it: Experimental results comparing RCfD with and without the proposed regressor extension on tasks with incomplete demonstration data.

### Open Question 3
- Question: How does RCfD's ability to mitigate ROO compare when using reward models of varying quality or when the reward model is known to have specific biases?
- Basis in paper: [explicit] The paper states that "RCfD offers predictable behavior regardless of the reward model's quality, consistently converging towards the desired reward distribution observed in demonstrations" and discusses this as an advantage over classic RL objectives.
- Why unresolved: The paper does not empirically evaluate RCfD's performance with reward models of varying quality or with known biases.
- What evidence would resolve it: Experiments varying the quality of the reward model (e.g., by adding noise or using less accurate models) and introducing known biases, then measuring RCfD's performance and ability to mitigate ROO.

## Limitations
- Demonstration Quality Dependency: RCfD's effectiveness fundamentally depends on the quality and representativeness of demonstration data
- Computational Overhead: The paper claims efficiency by avoiding hyperparameter tuning but doesn't provide concrete runtime comparisons
- Reward Model Sensitivity: While RCfD claims to handle reward model quality variations, experiments primarily use a single reward model architecture

## Confidence
- RCfD effectively mitigates ROO: High confidence - Supported by quantitative metrics (KL divergence, reward distribution alignment) and qualitative evaluations across multiple tasks
- RCfD eliminates need for hyperparameter tuning: Medium confidence - Valid for reward scaling but doesn't address other PPO hyperparameters that may still require tuning
- RCfD handles multi-reward settings naturally: Medium confidence - Demonstrated on two specific multi-reward tasks, but general applicability across diverse reward combinations untested

## Next Checks
1. Demonstration Quality Ablation: Systematically vary demonstration dataset size and quality (e.g., using synthetic vs. human demonstrations) to quantify impact on RCfD performance and identify minimum viable demonstration requirements.

2. Reward Model Architecture Robustness: Test RCfD with different reward model architectures (e.g., DeBERTa-based pairwise reward models vs. BERT-based pointwise models) to verify claimed robustness to reward model quality variations.

3. Computational Efficiency Benchmark: Measure wall-clock training time, memory usage, and convergence speed of RCfD versus tuned PPO baselines across tasks of varying complexity to validate efficiency claims beyond hyperparameter tuning.