---
ver: rpa2
title: Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards
  Linear Complexity
arxiv_id: '2410.06846'
source_url: https://arxiv.org/abs/2410.06846
tags:
- target
- conference
- teacher
- https
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to convert large pre-trained transformers
  into efficient linear-complexity models (like Linformer, Mamba, or Mamba2) without
  re-doing expensive pretraining. It does this by combining parameter transfer with
  layerwise distillation, guiding the student model to mimic hidden states from the
  teacher during fine-tuning.
---

# Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity

## Quick Facts
- arXiv ID: 2410.06846
- Source URL: https://arxiv.org/abs/2410.06846
- Authors: Mutian He; Philip N. Garner
- Reference count: 40
- Primary result: Converts transformers to linear-complexity models via joint fine-tuning and layerwise distillation, recovering or surpassing original performance

## Executive Summary
This paper presents a method to convert large pretrained transformers into efficient linear-complexity models (Linformer, Mamba, Mamba2) without expensive re-pretraining. The approach combines parameter transfer with layerwise distillation, guiding the student model to mimic hidden states from the teacher during fine-tuning. Multiple guidance modes are explored, including direct target guidance, trajectory following, waypoint checkpoints, and hybrid strategies. Experiments across language tasks, language modeling, and speech applications demonstrate that converted models achieve performance comparable to or better than original transformers while using significantly less computation.

## Method Summary
The method, called Cross-Architecture Layerwise Distillation (CALD), converts pretrained transformers by replacing attention layers with linear alternatives while preserving embeddings, feed-forward networks, and layer normalization. The student model is then jointly fine-tuned with the teacher using a distillation loss that minimizes mean squared error between corresponding hidden states. Multiple guidance modes are supported: target-guided (from final fine-tuned teacher), trajectory-guided (along the teacher's fine-tuning path), waypoint-guided (from checkpoints), and hybrid approaches. The method is applied to various architectures including Linformer, Mamba, and Mamba2 across language tasks (QNLI, QQP, SST2, IMDB), speech tasks (TED-LIUM ASR, SLURP IC, VoxCeleb1 SID), and language modeling on Pile.

## Key Results
- Converted models recover or surpass original transformer performance across all tested tasks
- Trajectory-guided and waypoint-guided distillation are especially effective for speech tasks where hidden states undergo large shifts during fine-tuning
- Hybrid guidance modes combining early target guidance with later unguided training provide robust performance across domains
- Speech models converted to bidirectional Mamba2 with cross-attention achieve 13.3% WER on TED-LIUM without any fine-tuning
- Language models converted to Mamba show competitive performance on Pile with significant computational efficiency gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint fine-tuning with layerwise distillation enables efficient transfer of task-specific knowledge from transformer to linear models without re-pretraining.
- Mechanism: The method aligns hidden states layer-by-layer between the transformer teacher and linear student during fine-tuning, guiding the student to mimic the teacher's task-adapted behavior.
- Core assumption: Hidden state alignment is sufficient to transfer task-specific capabilities, and the student can learn to approximate the teacher's behavior despite architectural differences.
- Evidence anchors:
  - [abstract] "This paper introduces a method to convert large pre-trained transformers into efficient linear-complexity models (like Linformer, Mamba, or Mamba2) without re-doing expensive pretraining. It does this by combining parameter transfer with layerwise distillation, guiding the student model to mimic hidden states from the teacher during fine-tuning."
  - [section 3] "By posing loss on the difference of hidden states, we enforce constraints on the deviation of the student's hidden states from the teacher. In this way, we directly guide each layer in the student to reproduce the behavior of the corresponding teacher layer."

### Mechanism 2
- Claim: Different guidance modes optimize knowledge transfer based on how much hidden state information is preserved during fine-tuning.
- Mechanism: Trajectory and waypoint modes capture the full fine-tuning path from pretrained to task-adapted states, while target mode focuses only on final task-adapted states. Hybrid mode combines early guidance with later freedom to adapt.
- Core assumption: The trajectory of hidden states during fine-tuning contains valuable information about how pretrained knowledge is adapted to tasks.
- Evidence anchors:
  - [section 3] "We can guide the converted model by the sequence of models on the teacher's fine-tuning trajectory... Alternatively, we can use the distilled model as a good initialization for conversion, and allow deviation from the teacher's behavior in the later training phase."
  - [section 4.4] "Unlike NLP models, features produced by the fine-tuned speech models are far from the initial ones since the early phase of fine-tuning"

### Mechanism 3
- Claim: Parameter transfer combined with distillation achieves performance comparable to re-pretrained models while using far less computation.
- Mechanism: The method preserves as many pretrained parameters as possible (embeddings, feed-forward, layernorm) while only replacing attention layers with efficient alternatives, then fine-tunes with distillation to recover performance.
- Core assumption: Most of the pretrained model's capabilities reside in layers other than attention, making parameter transfer effective.
- Evidence anchors:
  - [abstract] "Experiments on language tasks, language modeling, and speech tasks (ASR, intent classification, speaker ID) show the converted models recover or even surpass original transformer performance while using far less computation."
  - [section 3] "The architecture may not be identical to the standard Linformer, Mamba, or Mamba2... In this way, all the models have roughly the same number of parameters after conversion."

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The method relies on transferring knowledge from teacher to student models through hidden state alignment.
  - Quick check question: What is the difference between output distillation and layerwise distillation?

- Concept: Linear Complexity Attention
  - Why needed here: The target architectures (Linformer, Mamba, Mamba2) replace quadratic attention with linear complexity alternatives.
  - Quick check question: How does Linformer achieve linear complexity in self-attention?

- Concept: State Space Models
  - Why needed here: Mamba and Mamba2 are based on state space models, which are the foundation of the speech processing experiments.
  - Quick check question: What is the relationship between state space models and recurrent neural networks?

## Architecture Onboarding

- Component map:
  - Source model (RoBERTa/Wav2Vec2/Pythia) -> Student model (same architecture with attention layers replaced) -> Teacher model (transformer fine-tuned on target task) -> Distillation loss (MSE between hidden states) -> Optimization (joint fine-tuning with AdamW)

- Critical path:
  1. Initialize student with source model parameters
  2. Replace attention layers with target architecture
  3. Fine-tune teacher on target task
  4. Jointly fine-tune student with distillation loss
  5. Evaluate performance against baseline transformer

- Design tradeoffs:
  - Parameter transfer vs. random initialization: Transfer preserves pretrained knowledge but may limit adaptation
  - Layerwise vs. output distillation: Layerwise provides more detailed guidance but requires more computation
  - Guidance modes: Target is simple but may miss early fine-tuning dynamics; trajectory captures full path but doubles computation

- Failure signatures:
  - NaN loss values during training (indicates instability in distillation)
  - Large performance gap between student and teacher on validation set
  - Training instability when replacing attention layers

- First 3 experiments:
  1. Implement unguided conversion (parameter transfer only) to establish baseline performance degradation
  2. Add target guided distillation to verify if hidden state alignment improves performance
  3. Compare trajectory vs waypoint guidance modes to understand which captures more useful information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the trajectory-guided distillation approach consistently outperform target-guided approaches across all speech processing tasks, or are there specific task characteristics that determine its effectiveness?
- Basis in paper: [explicit] The paper notes that trajectory-guided approaches work better when there is limited hidden state shift during fine-tuning, and when pretrained model features remain relevant for the target task. However, for speech tasks, the hidden states undergo significant shift during fine-tuning, making trajectory guidance less effective.
- Why unresolved: The paper only provides limited evidence for this claim, focusing primarily on speech tasks. More comprehensive experiments across different types of tasks (e.g., NLP, vision, multimodal) would be needed to determine if this pattern holds generally.
- What evidence would resolve it: Experiments comparing trajectory-guided and target-guided approaches across a wide range of tasks with varying amounts of hidden state shift during fine-tuning.

### Open Question 2
- Question: What is the optimal way to determine the switching point in the hybrid distillation approach, and how does this affect final model performance?
- Basis in paper: [inferred] The paper mentions using a switching point at around 30% of total training steps for the hybrid approach, but this seems arbitrary and may not be optimal for all tasks or model configurations.
- Why unresolved: The paper doesn't explore how different switching points affect performance, or whether there are better strategies for determining when to switch from guided to unguided training.
- What evidence would resolve it: Systematic experiments varying the switching point across different tasks and model architectures, along with analysis of how switching point affects convergence and final performance.

### Open Question 3
- Question: How does the effectiveness of CALD vary with the size of the target task dataset, and are there dataset size thresholds where different guidance strategies become optimal?
- Basis in paper: [inferred] While the paper demonstrates CALD's effectiveness across various tasks, it doesn't systematically explore how dataset size affects the relative performance of different guidance strategies.
- Why unresolved: Understanding the relationship between dataset size and optimal guidance strategy could help practitioners choose the most effective approach for their specific use case.
- What evidence would resolve it: Experiments training models with varying dataset sizes (both very small and very large) and comparing the effectiveness of different guidance strategies across this spectrum.

## Limitations

- The relative effectiveness of guidance modes varies significantly between domains, with trajectory guidance working better for speech tasks but not necessarily for NLP tasks
- The method's effectiveness for architectures beyond Linformer, Mamba, and Mamba2 remains unverified
- The paper doesn't analyze the quality of hidden state alignment or whether linear architectures can adequately represent teacher hidden states

## Confidence

**High confidence** (9/10): The core claim that joint fine-tuning with layerwise distillation enables efficient conversion of transformers to linear models without re-pretraining is well-supported by extensive experiments across multiple tasks, domains, and architectures.

**Medium confidence** (7/10): The claim about different guidance modes optimizing knowledge transfer based on hidden state preservation is supported by experiments but the theoretical understanding of why certain modes work better in specific domains is incomplete.

**Low confidence** (4/10): The claim that parameter transfer alone can preserve most pretrained capabilities while replacing attention layers is not thoroughly validated.

## Next Checks

1. **Hidden state similarity analysis**: Compute and compare hidden state similarity (e.g., centered kernel alignment or canonical correlation) between teacher and student models across different fine-tuning stages to quantify how well distillation aligns representations and whether certain architectural differences prevent effective alignment.

2. **Architecture ablation study**: Test the method on additional linear attention architectures (RWKV, RetNet, S4) and perform controlled experiments varying which components are transferred vs. randomly initialized to determine the contribution of each architectural element to final performance.

3. **Long sequence behavior evaluation**: Design experiments specifically targeting the intended use case of long sequences by evaluating model performance and efficiency on sequence lengths well beyond the training distribution (e.g., 32K+ tokens) to verify the practical benefits of linear complexity for the target application domain.