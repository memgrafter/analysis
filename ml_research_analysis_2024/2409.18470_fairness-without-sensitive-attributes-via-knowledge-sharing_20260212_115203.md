---
ver: rpa2
title: Fairness without Sensitive Attributes via Knowledge Sharing
arxiv_id: '2409.18470'
source_url: https://arxiv.org/abs/2409.18470
tags:
- fairness
- classifier
- confidence
- sensitive
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving fairness in machine
  learning models when sensitive attributes are unavailable due to privacy concerns.
  The core method, called "Reckoner," introduces a confidence-based hierarchical classifier
  that leverages a dual-model system.
---

# Fairness without Sensitive Attributes via Knowledge Sharing

## Quick Facts
- arXiv ID: 2409.18470
- Source URL: https://arxiv.org/abs/2409.18470
- Authors: Hongliang Ni; Lei Han; Tong Chen; Shazia Sadiq; Gianluca Demartini
- Reference count: 40
- One-line primary result: Confidence-based hierarchical classifier outperforms state-of-the-art baselines in fairness metrics (Equalised Odds and Demographic Parity) and accuracy without requiring sensitive attributes

## Executive Summary
The paper addresses the challenge of improving fairness in machine learning models when sensitive attributes are unavailable due to privacy concerns. The core method, called "Reckoner," introduces a confidence-based hierarchical classifier that leverages a dual-model system. It uses a high-confidence data subset to guide a low-confidence subset, avoiding biased predictions while maintaining accuracy. Learnable noise is incorporated to retain essential information for predictions. Experimental results on COMPAS, New Adult, and CelebA datasets show that Reckoner outperforms state-of-the-art baselines in both fairness metrics (Equalised Odds and Demographic Parity) and accuracy, with improvements of up to 5.54% in fairness and 1.45% in accuracy compared to the best baselines.

## Method Summary
Reckoner operates through a dual-model system that first trains a logistic regression classifier to obtain confidence scores, then splits the data at a threshold (0.6) into high-confidence and low-confidence subsets. Two three-layer MLPs are initialized with respective subsets, with learnable noise added to inputs via a noise wrapper. The low-confidence classifier learns from pseudo-labels generated by the high-confidence classifier, while the high-confidence classifier updates using both ground truth and knowledge transferred from the low-confidence classifier. This hierarchical approach enables the model to avoid biased predictions while maintaining accuracy, with the learnable noise component helping to retain only essential information for predictions.

## Key Results
- Reckoner achieves up to 5.54% improvement in fairness metrics (Equalised Odds and Demographic Parity) compared to best baselines
- Model shows 1.45% improvement in accuracy over top-performing baselines
- Performance validated across three datasets: COMPAS, New Adult, and CelebA
- Learnable noise component contributes to retaining essential prediction information while reducing bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence-based data splitting exposes biased patterns in high-confidence subsets while low-confidence subsets retain more fairness
- Mechanism: By training a logistic regression classifier and splitting data at a confidence threshold (0.6), the method isolates subsets where biased labels are most strongly learned. High-confidence subsets amplify demographic disparities, while low-confidence subsets retain smaller bias gaps
- Core assumption: Biased labels in training data cause models to amplify demographic disparities as confidence increases
- Evidence anchors:
  - [abstract] "We first present results showing that if the dataset contains biased labels or other hidden biases, classifiers significantly increase the bias gap across different demographic groups in the subset with higher prediction confidence."
  - [section] "We observe that as the model makes predictions with higher confidence scores, the bias gap between demographic groups increases as well."
  - [corpus] Weak correlation; corpus focuses on fairness-aware methods without direct confidence-thresholding evidence
- Break condition: If the dataset lacks biased labels or if confidence scores do not correlate with demographic disparities, the splitting strategy loses its effectiveness

### Mechanism 2
- Claim: Learnable noise applied to input data forces the model to focus on essential features while reducing reliance on biased proxies
- Mechanism: Learnable noise is added via a noise wrapper (two-layer MLP) that modifies inputs, encouraging the model to select features with higher cross-entropy to ground truth while discarding biased correlations
- Core assumption: Adding noise during training encourages feature selection that reduces bias while maintaining prediction accuracy
- Evidence anchors:
  - [section] "We add a noise wrapper to vectors of the same dimensions as the input... This is beneficial for reducing bias gaps in model predictions because, even if the dataset does not contain sensitive information, the associated non-sensitive information may still be biased."
  - [abstract] "We introduce learnable noise into the original data, aiming to retain only the necessary information for prediction."
  - [corpus] Weak evidence; corpus does not discuss learnable noise for fairness
- Break condition: If the noise wrapper fails to effectively filter biased features or if the model overfits to noise patterns, fairness improvements may not materialize

### Mechanism 3
- Claim: Knowledge transfer from a low-confidence (fairer) classifier to a high-confidence (accurate) classifier balances fairness and accuracy
- Mechanism: The low-confidence classifier generates pseudo-labels for the high-confidence classifier. The high-confidence classifier updates its parameters using both ground truth and the low-confidence classifier's knowledge, weighted by hyperparameter α
- Core assumption: The low-confidence classifier learns fairer decision boundaries despite lower accuracy, and this knowledge can be transferred to improve fairness in the high-confidence classifier
- Evidence anchors:
  - [section] "The motivation behind our proposed framework stems from the analytical findings... We observed the smallest bias gap in the results of the classifier on low-confidence subsets."
  - [abstract] "Inspired by these findings, we devised a dual-model system in which a version of the model initialised with a high-confidence data subset learns from a version of the model initialised with a low-confidence data subset, enabling it to avoid biased predictions."
  - [corpus] Weak correlation; corpus does not discuss dual-model knowledge transfer for fairness
- Break condition: If the low-confidence classifier fails to learn fairer patterns or if knowledge transfer destabilizes the high-confidence classifier, the approach may degrade performance

## Foundational Learning

- Concept: Logistic regression and confidence score calibration
  - Why needed here: Confidence-based data splitting relies on accurate confidence estimates from a simple classifier
  - Quick check question: How does logistic regression output confidence scores, and what do these scores represent?

- Concept: Binary cross-entropy loss and pseudo-label training
  - Why needed here: Both pseudo-learning and final classifier updates use binary cross-entropy, requiring understanding of supervised learning with and without ground truth
  - Quick check question: What is the difference between training with ground truth labels versus pseudo-labels in terms of loss computation?

- Concept: Learnable noise and feature selection
  - Why needed here: Learnable noise acts as a regularization mechanism, requiring understanding of how noise injection influences model training and feature importance
  - Quick check question: How does adding learnable noise to input data affect feature selection during model training?

## Architecture Onboarding

- Component map:
  Identification stage: Logistic regression classifier → Confidence-based data split → High-confidence and low-confidence subsets
  Refinement stage: Noise wrapper (learnable noise) → High-confidence classifier (three-layer MLP) → Low-confidence classifier (three-layer MLP) → Knowledge transfer mechanism → Final high-confidence classifier update

- Critical path:
  1. Train logistic regression on raw data
  2. Split data at confidence threshold (0.6)
  3. Initialize dual classifiers with respective subsets
  4. Apply learnable noise to inputs
  5. Train low-confidence classifier with pseudo-labels from high-confidence classifier
  6. Transfer knowledge from low-confidence to high-confidence classifier
  7. Update high-confidence classifier with ground truth and transferred knowledge

- Design tradeoffs:
  - Confidence threshold selection: Higher thresholds isolate more biased data but reduce subset size; lower thresholds increase subset size but retain more bias
  - Learnable noise strength: Stronger noise increases bias reduction but may degrade accuracy; weaker noise preserves accuracy but reduces fairness gains
  - Knowledge transfer weight (α): Higher α preserves more high-confidence classifier knowledge; lower α emphasizes fairness from low-confidence classifier

- Failure signatures:
  - High-confidence classifier accuracy drops significantly: Likely due to excessive noise or improper knowledge transfer
  - Fairness metrics do not improve: Possible causes include inappropriate confidence threshold or ineffective pseudo-label training
  - Model instability during training: Could indicate poor hyperparameter tuning or insufficient training iterations for pseudo-learning

- First 3 experiments:
  1. Validate confidence-based data splitting: Train logistic regression, split data at threshold 0.6, compare bias gaps between subsets
  2. Test learnable noise effectiveness: Train classifiers with and without learnable noise, measure bias reduction and accuracy changes
  3. Evaluate knowledge transfer impact: Train dual classifiers with pseudo-learning, compare fairness and accuracy against single classifier baseline

## Open Questions the Paper Calls Out

- Question: How can the Reckoner framework be extended to handle intersectional fairness across multiple sensitive attributes?
- Basis in paper: [explicit] The authors acknowledge that their framework can perform classification tasks with multiple sensitive attributes but do not discuss intersectional fairness due to data scarcity and lack of appropriate metrics
- Why unresolved: The challenge of data scarcity at intersections of minority groups and the lack of proper group fairness metrics for intersectional analysis are not addressed in the paper
- What evidence would resolve it: Developing a framework that can handle intersectional fairness, including methods to address data scarcity and appropriate metrics for evaluation

## Limitations
- Analysis limited to three benchmark datasets, which may restrict generalizability to other domains
- Confidence-based splitting assumes biased labels correlate with demographic disparities, but this relationship may not hold across all domains
- Learnable noise introduces additional complexity that could affect model stability and require careful tuning

## Confidence

- High: The core observation that confidence scores correlate with demographic bias gaps
- Medium: The effectiveness of learnable noise for reducing bias while maintaining accuracy  
- Medium: The knowledge transfer mechanism improving fairness without significant accuracy loss

## Next Checks

1. Test the confidence-based splitting strategy on datasets with known label bias to verify that higher confidence scores consistently correlate with larger demographic disparities

2. Evaluate the sensitivity of results to the confidence threshold parameter (currently fixed at 0.6) across multiple datasets

3. Conduct ablation studies removing the learnable noise component to quantify its contribution to fairness improvements versus potential accuracy degradation