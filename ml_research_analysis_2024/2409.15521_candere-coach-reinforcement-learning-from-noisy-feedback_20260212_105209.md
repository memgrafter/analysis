---
ver: rpa2
title: 'CANDERE-COACH: Reinforcement Learning from Noisy Feedback'
arxiv_id: '2409.15521'
source_url: https://arxiv.org/abs/2409.15521
tags:
- feedback
- noise
- learning
- candere-coach
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reinforcement learning (RL)
  from noisy human feedback. While methods like Deep COACH use human feedback as an
  advantage function, they assume perfect feedback, which is unrealistic in practice.
---

# CANDERE-COACH: Reinforcement Learning from Noisy Feedback

## Quick Facts
- arXiv ID: 2409.15521
- Source URL: https://arxiv.org/abs/2409.15521
- Reference count: 21
- Primary result: Successfully learns with up to 40% noisy feedback in RL from human input

## Executive Summary
This paper addresses the challenge of reinforcement learning from noisy human feedback, a critical issue since real-world human input is rarely perfect. The authors introduce CANDERE-COACH, which builds upon Deep COACH by adding a noise-filtering classifier that detects and corrects noisy feedback using loss-based uncertainty detection and active relabeling. The method demonstrates robust performance across three benchmark domains (Cart Pole, Lunar Lander, and Minigrid Doorkey) even with substantial noise levels up to 40%, significantly outperforming existing approaches that assume perfect feedback.

## Method Summary
CANDERE-COACH extends Deep COACH by incorporating a noise-filtering classifier that identifies potentially noisy data points based on their loss values. The algorithm employs active relabeling to flip suspected incorrect labels, effectively cleaning the training data. This modular approach can be integrated into other feedback-based RL algorithms like TAMER. The method is evaluated under various noise conditions, including up to 40% noise in feedback and noisy pretraining datasets under low noise levels, demonstrating superior performance compared to baseline approaches.

## Key Results
- Maintains learning performance with up to 40% noisy feedback across three benchmark domains
- Outperforms Deep COACH and its variants in noisy feedback scenarios
- Shows effectiveness with noisy pretraining datasets under low noise conditions

## Why This Works (Mechanism)
The method works by leveraging loss-based uncertainty detection to identify potentially noisy feedback data points. When the classifier detects high uncertainty (indicated by large loss values), it triggers active relabeling to correct suspected incorrect labels. This noise-filtering mechanism prevents the RL agent from learning incorrect behaviors from noisy human feedback, maintaining learning stability and performance even under substantial noise conditions.

## Foundational Learning

**Reinforcement Learning from Human Feedback** - Learning policies based on human-provided feedback signals rather than explicit reward functions. This is needed because real-world human feedback is often imperfect and noisy, making traditional RL approaches unreliable.

**Advantage Function** - Measures how much better an action is compared to the average action in a given state. This is crucial for understanding which actions the human feedback is trying to reinforce or discourage.

**Loss-based Uncertainty Detection** - Uses prediction loss values to identify potentially incorrect or noisy data points. This is needed because high loss often correlates with data that doesn't fit the learned model, suggesting possible noise.

**Active Relabeling** - A technique where suspected noisy data points are corrected or relabeled to improve dataset quality. This is essential for maintaining clean training data in the presence of noise.

**Noise Filtering Classifier** - A component that distinguishes between clean and noisy feedback data. This is needed to prevent the RL agent from learning from incorrect human feedback signals.

**Pretraining with Noisy Data** - Training on datasets that contain noise before fine-tuning. This is relevant for scenarios where initial training data may be imperfect but can be cleaned during learning.

## Architecture Onboarding

**Component Map:** Human Feedback -> Noise Filter Classifier -> Active Relabeler -> Clean Feedback -> RL Agent -> Policy

**Critical Path:** The noise filter classifier operates continuously during training, detecting uncertain data points (high loss values) and triggering active relabeling. Cleaned feedback is then used to update the RL agent's policy. This feedback loop runs iteratively throughout training.

**Design Tradeoffs:** The method trades computational overhead from active relabeling against improved learning robustness. The loss threshold for noise detection requires careful tuning - too low misses noisy data, too high wastes resources on clean data. The modular design allows extension to other feedback-based RL algorithms but adds complexity.

**Failure Signatures:** High false positive rates in noise detection can lead to over-correction and degradation of good feedback. If the noise level exceeds the classifier's detection capability (particularly in pretraining scenarios), learning performance deteriorates significantly. The method may struggle with non-uniform noise distributions or complex feedback patterns.

**First Experiments:** 1) Test noise detection accuracy on synthetic noisy datasets with known ground truth, 2) Evaluate active relabeling effectiveness by measuring label correction rates, 3) Assess performance degradation as noise levels increase beyond 40% to establish failure boundaries.

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness degrades significantly under high noise levels in pretraining scenarios
- Active relabeling introduces computational overhead that scales with dataset size
- Performance under different noise distribution patterns (burst vs. uniform noise) not fully explored

## Confidence
High - Systematic improvements demonstrated across multiple domains with varying noise levels, sound methodology using loss-based uncertainty detection and active relabeling, modular design enables extension to other feedback-based RL algorithms.

## Next Checks
1. Test the method on high-dimensional continuous control tasks like humanoid locomotion or robotic manipulation to assess scalability
2. Evaluate performance under different noise distribution patterns (clustered vs. uniform) to understand robustness to various noise characteristics
3. Conduct ablation studies to quantify the individual contributions of loss-based detection versus active relabeling components to overall performance improvement