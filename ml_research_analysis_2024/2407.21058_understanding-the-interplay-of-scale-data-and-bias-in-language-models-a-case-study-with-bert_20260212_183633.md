---
ver: rpa2
title: 'Understanding the Interplay of Scale, Data, and Bias in Language Models: A
  Case Study with BERT'
arxiv_id: '2407.21058'
source_url: https://arxiv.org/abs/2407.21058
tags:
- bias
- data
- biases
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how model scale and pre-training data influence
  social biases in BERT language models. Through pre-training four BERT sizes (Mini,
  Small, Medium, Base) on Wikipedia and Common Crawl datasets, researchers found that
  larger models amplify biases from their training data.
---

# Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT

## Quick Facts
- arXiv ID: 2407.21058
- Source URL: https://arxiv.org/abs/2407.21058
- Authors: Muhammad Ali; Swetasudha Panda; Qinlan Shen; Michael Wick; Ari Kobren
- Reference count: 15
- One-line primary result: Larger BERT models amplify biases from their training data, with Common Crawl increasing toxicity and Wikipedia increasing gender stereotypes at scale, while downstream classification biases decrease with model size.

## Executive Summary
This study systematically investigates how model scale and pre-training data influence social biases in BERT language models. Through controlled experiments pre-training four BERT sizes (Mini, Small, Medium, Base) on Wikipedia and Common Crawl datasets, researchers demonstrate that larger models amplify biases present in their training data while simultaneously showing reduced downstream classification biases. The findings reveal a complex relationship where scale both exacerbates certain bias types and mitigates others, depending on the task and data source.

## Method Summary
The researchers pre-trained four BERT architectures (Mini, Small, Medium, Base) on two distinct datasets - Wikipedia and Common Crawl - for 8,000 training steps each. They evaluated upstream biases using metrics like log probability bias scores for gender pronouns and sentiment analysis across identity groups. Downstream biases were measured through fine-tuning on toxicity classification tasks using the Dixon et al. (2018) dataset, with false positive rates calculated across different demographic groups. The study also analyzed dataset biases to understand how pre-training data composition influences model behavior.

## Key Results
- Larger models amplify biases present in their training data, with Common Crawl-trained models showing increased toxicity and Wikipedia-trained models exhibiting greater gender stereotypes at scale
- Downstream classification biases consistently decrease with model size, regardless of pre-training data source
- Pre-training data quality and moderation level determines the type of bias amplified - Wikipedia contains gender representation biases while Common Crawl contains more toxic content
- Wikipedia encodes more gender stereotypes while Common Crawl contains more toxic content, as measured by log odds ratio and sentiment analysis metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models amplify biases present in their training data because they fit the data more closely.
- Mechanism: As model size increases, parameter count grows, allowing the model to memorize and reproduce subtle patterns in training data, including biased associations between identities and sentiments.
- Core assumption: Bias is encoded in training data and larger models can better fit this data.
- Evidence anchors:
  - [abstract]: "With increasing scale, models pre-trained on large internet scrapes like Common Crawl exhibit higher toxicity, whereas models pre-trained on moderated data sources like Wikipedia show greater gender stereotypes."
  - [section]: "Models often learn harmful artifacts in the data, and since bigger models fit the data better, biases can aggravate as model size increases."
- Break condition: If model architecture changes to include explicit bias mitigation layers, or if training data is thoroughly cleaned of biases before training.

### Mechanism 2
- Claim: Downstream biases decrease with model size because larger models learn more robust task-specific rules rather than relying on shortcut heuristics.
- Mechanism: As model capacity increases, it can learn genuine semantic patterns for classification tasks instead of using superficial correlations like identity words as toxicity indicators.
- Core assumption: Scale helps models learn task-specific rules that are less biased than shortcut heuristics.
- Evidence anchors:
  - [abstract]: "However, downstream biases generally decrease with increasing model scale, irrespective of the pre-training data."
  - [section]: "scale helps the model learn more reliable task-specific rules in favor of biases like gender bias; and so for this reason, a model might actually get less biased with scale."
- Break condition: If fine-tuning data contains strong identity-toxicity correlations, or if model architecture limits learning of robust features.

### Mechanism 3
- Claim: Pre-training data quality and moderation level determines the type of bias amplified at scale.
- Mechanism: Different datasets contain different bias patterns - Wikipedia has gender representation biases while Common Crawl has toxicity content - and larger models amplify whichever patterns are present.
- Core assumption: Training data contains different types of biases that scale will amplify.
- Evidence anchors:
  - [abstract]: "models pre-trained on large internet scrapes like Common Crawl exhibit higher toxicity, whereas models pre-trained on moderated data sources like Wikipedia show greater gender stereotypes."
  - [section]: "Different data has different biases. For example, biographies about notable figures on Wikipedia are skewed towards men over women (Tripodi 2023), and large web scrapes like the common-crawl are likely more diverse in overall topics covered, but also much more toxic."
- Break condition: If data curation process removes specific bias types, or if dataset composition changes significantly.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: This is the pretraining objective used for BERT, and understanding how it works is crucial to understanding how biases are learned during pretraining.
  - Quick check question: In MLM, what percentage of tokens are typically masked during pretraining, and what is the model trying to predict?

- Concept: False Positive Rate (FPR) in classification
  - Why needed here: The paper measures downstream bias using FPR across different demographic groups, which is a standard fairness metric.
  - Quick check question: If a toxicity classifier has FPR of 20% for Muslims and 5% for Christians on non-toxic examples, which group experiences more allocative harm?

- Concept: Log probability bias score
  - Why needed here: This metric is used to measure upstream gender bias by comparing pronoun probabilities in occupation contexts.
  - Quick check question: What does a log probability gap of 0 indicate about gender associations in MLM predictions?

## Architecture Onboarding

- Component map: BERT architecture with four sizes (Mini, Small, Medium, Base) varying in layers and hidden dimensions; MLM pretraining objective; sequence classification head for downstream fine-tuning.
- Critical path: Pretrain → Measure upstream bias → Fine-tune for classification → Measure downstream bias → Analyze data sources → Draw conclusions.
- Design tradeoffs: Larger models give better performance but amplify biases more; moderated data reduces toxicity but may encode other biases; simpler architectures easier to analyze but may not capture complex patterns.
- Failure signatures: Unexpected bias patterns that don't correlate with scale or data type; metrics showing no improvement with scale; training instability at larger sizes.
- First 3 experiments:
  1. Pretrain BERT-Mini on both Wikipedia and CC-100, measure initial bias levels to establish baseline.
  2. Train BERT-Base on both datasets, compare upstream bias to Mini to verify scaling effect.
  3. Fine-tune both models on toxicity classification, measure downstream FPR across all identity groups to confirm scaling law for downstream bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the relationship between model scale and bias vary across different types of neural architectures beyond BERT (e.g., autoregressive vs. encoder-decoder models)?
- Basis in paper: [inferred] The paper acknowledges that BERT is only one model family and that their results cannot be generalized to all modern LLM architectures, explicitly stating the need to explore other architectures.
- Why unresolved: The study focuses specifically on BERT due to its popularity and to control for architectural variance, but does not investigate other model families like GPT, T5, or LLaMA.
- What evidence would resolve it: Systematic experiments pre-training and evaluating multiple model families (autoregressive, encoder-decoder, encoder-only) across varying scales on the same datasets with identical bias metrics.

### Open Question 2
- Question: What is the precise mechanism by which pre-training data influences the evolution of bias during training, and can this be predicted or modeled?
- Basis in paper: [explicit] The authors analyze dataset biases and find correlations between sentiment in pre-training data and model bias, but acknowledge they cannot fully explain the observed patterns or predict them.
- Why unresolved: While the paper demonstrates that pre-training data matters and shows some correlations, it doesn't establish causal mechanisms or develop predictive models for how specific data characteristics translate to model bias.
- What evidence would resolve it: Controlled experiments varying specific data characteristics (toxicity levels, gender representation, topic distributions) in isolation, combined with mechanistic interpretability studies of how models process these features during training.

### Open Question 3
- Question: At what point does increasing model scale stop providing benefits for bias mitigation, and could there be a scale threshold beyond which biases actually worsen?
- Basis in paper: [inferred] The study only examines models up to BERT-Base (110M parameters) and observes decreasing downstream bias with scale, but doesn't explore whether this trend continues at larger scales or reverses.
- Why unresolved: The authors note their analysis is limited to smaller models compared to state-of-the-art LLMs, leaving open whether the observed patterns hold at billion-parameter scales.
- What evidence would resolve it: Pre-training and evaluating the full range of BERT sizes (including BERT-Large and beyond) and comparing with modern large language models, measuring both upstream and downstream bias across all scales.

## Limitations
- The study only examines BERT architecture and cannot generalize findings to other model families like autoregressive or encoder-decoder models.
- The analysis is limited to models up to BERT-Base (110M parameters), leaving open whether scaling laws hold at larger scales.
- While the paper demonstrates correlations between data characteristics and model bias, it doesn't establish definitive causal mechanisms for how specific data patterns translate to model behavior.

## Confidence
- High confidence: The core finding that model scale amplifies upstream biases from training data is well-supported by systematic experiments across four model sizes and two datasets.
- Medium confidence: The downstream bias reduction mechanism is plausible but could be influenced by other factors beyond the proposed task-specific learning explanation.
- Medium confidence: The characterization of dataset-specific biases (Wikipedia gender stereotypes vs. Common Crawl toxicity) is based on reasonable analysis but could benefit from more granular investigation of data composition.

## Next Checks
1. Conduct ablation studies varying fine-tuning dataset composition to isolate whether downstream bias reduction is truly scale-driven or influenced by fine-tuning data properties.
2. Perform controlled experiments with synthetic training data containing known bias patterns to directly verify the amplification mechanism proposed.
3. Extend analysis to additional bias types beyond gender and toxicity to test whether the observed scaling laws generalize to other forms of social bias.