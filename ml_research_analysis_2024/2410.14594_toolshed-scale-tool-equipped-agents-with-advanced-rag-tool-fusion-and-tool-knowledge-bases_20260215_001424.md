---
ver: rpa2
title: 'Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and Tool
  Knowledge Bases'
arxiv_id: '2410.14594'
source_url: https://arxiv.org/abs/2410.14594
tags:
- tool
- tools
- retrieval
- advanced
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling tool-equipped agents
  beyond the 128-tool limit imposed by model providers like OpenAI and Anthropic.
  The authors introduce Toolshed Knowledge Bases, a vector database for storing enhanced
  tool representations, and Advanced RAG-Tool Fusion, an ensemble of retrieval-augmented
  generation techniques applied across pre-retrieval, intra-retrieval, and post-retrieval
  phases.
---

# Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and Tool Knowledge Bases

## Quick Facts
- arXiv ID: 2410.14594
- Source URL: https://arxiv.org/abs/2410.14594
- Reference count: 40
- Key outcome: Achieves 46%, 56%, and 47% absolute improvements in recall@5 on ToolE single-tool, ToolE multi-tool, and Seal-Tools benchmark datasets respectively compared to BM25 baseline

## Executive Summary
This paper addresses the challenge of scaling tool-equipped agents beyond the 128-tool limit imposed by model providers like OpenAI and Anthropic. The authors introduce Toolshed Knowledge Bases, a vector database for storing enhanced tool representations, and Advanced RAG-Tool Fusion, an ensemble of retrieval-augmented generation techniques applied across pre-retrieval, intra-retrieval, and post-retrieval phases. Their approach achieves significant improvements in recall@5 on multiple benchmark datasets while enabling agents to effectively use thousands of tools while optimizing token costs and latency through configurable tool selection thresholds.

## Method Summary
The methodology involves creating Toolshed Knowledge Bases as vector databases storing enhanced tool representations that include tool name, description, argument schema, synthetic queries, and key topics. Advanced RAG-Tool Fusion is implemented as an ensemble of techniques applied in three phases: pre-retrieval enhancement of tool documents with additional information, intra-retrieval query planning and transformation to increase accuracy, and post-retrieval reranking and self-reflection to refine results. The system enables agents to access thousands of tools while maintaining retrieval accuracy through configurable parameters for total tools (tool-M) and selection threshold (top-k).

## Key Results
- Achieves 46% absolute improvement in recall@5 on ToolE single-tool benchmark
- Achieves 56% absolute improvement in recall@5 on ToolE multi-tool benchmark
- Achieves 47% absolute improvement in recall@5 on Seal-Tools benchmark
- Demonstrates effective scaling to thousands of tools while optimizing token costs and latency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Advanced RAG-Tool Fusion improves retrieval accuracy by using multiple retrieval-augmented generation techniques across pre-retrieval, intra-retrieval, and post-retrieval phases.
- Mechanism: The system enhances tool documents with additional information during pre-retrieval, uses query planning and transformation during intra-retrieval, and applies reranking and self-reflection during post-retrieval to refine the retrieved tools.
- Core assumption: The quality of tool retrieval can be significantly improved by applying advanced RAG techniques beyond simple keyword matching.
- Evidence anchors: [abstract] "During pre-retrieval, tool documents are enhanced with key information and stored in the Toolshed Knowledge Base. Intra-retrieval focuses on query planning and transformation to increase retrieval accuracy. Post-retrieval refines the retrieved tool documents and enables self-reflection."

### Mechanism 2
- Claim: Toolshed Knowledge Bases enable scaling tool-equipped agents to thousands of tools without significant drop in accuracy.
- Mechanism: The system stores enhanced tool representations in a vector database, allowing for efficient retrieval of relevant tools even when the total number of tools is very large.
- Core assumption: Vector databases can efficiently store and retrieve tool representations, and the enhanced tool documents maintain semantic similarity with user queries.
- Evidence anchors: [abstract] "Our approach achieves 46%, 56%, and 47% absolute improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools benchmark datasets, respectively (Recall@5)."

### Mechanism 3
- Claim: Varying the number of tools (tool-M) and selection threshold (top-k) allows for optimization of token costs and latency while maintaining retrieval accuracy.
- Mechanism: By adjusting the number of tools an agent has access to (tool-M) and the number of tools retrieved (top-k), the system can balance retrieval accuracy with computational efficiency.
- Core assumption: There is a trade-off between retrieval accuracy and computational efficiency that can be optimized by adjusting tool-M and top-k.
- Evidence anchors: [abstract] "Furthermore, by varying both the total number of tools (tool-M) an Agent has access to and the tool selection threshold (top-k), we address trade-offs between retrieval accuracy, agent performance, and token cost."

## Foundational Learning

- Concept: Vector databases and embeddings
  - Why needed here: Toolshed Knowledge Bases use vector databases to store enhanced tool representations, which are created using embeddings.
  - Quick check question: What is the difference between a vector database and a traditional relational database, and why is it useful for tool retrieval?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Advanced RAG-Tool Fusion is an ensemble of RAG techniques applied to tool retrieval, selection, and planning.
  - Quick check question: How does RAG improve upon traditional retrieval methods, and what are some common RAG patterns used in this system?

- Concept: Query planning and decomposition
  - Why needed here: Intra-retrieval phase of Advanced RAG-Tool Fusion uses query planning and decomposition to increase retrieval accuracy.
  - Quick check question: Why is query planning and decomposition important for complex user queries, and how does it improve tool retrieval?

## Architecture Onboarding

- Component map: Toolshed Knowledge Bases -> Advanced RAG-Tool Fusion -> Simple Agent -> Evaluation Framework
- Critical path:
  1. Enhance tool documents with additional information (pre-retrieval)
  2. Store enhanced tool representations in Toolshed Knowledge Bases
  3. Receive user query and decompose it into sub-queries (intra-retrieval)
  4. Retrieve top-k relevant tools from Toolshed Knowledge Bases using the sub-queries
  5. Rerank and refine the retrieved tools (post-retrieval)
  6. Equip the Simple Agent with the final top-k tools
  7. Simple Agent uses the tools to answer the user query
  8. Evaluate tool calling accuracy and token costs
- Design tradeoffs:
  - Trade-off between retrieval accuracy and computational efficiency by varying tool-M and top-k
  - Trade-off between the complexity of enhanced tool representations and retrieval accuracy
  - Trade-off between the number of RAG techniques used and system complexity
- Failure signatures:
  - Low retrieval accuracy: Check the quality of enhanced tool representations, the effectiveness of query planning and decomposition, and the reranking algorithms
  - High token costs: Optimize the values of tool-M and top-k to reduce the number of tools retrieved
  - Slow latency: Optimize the vector database queries and reduce the number of RAG techniques used
- First 3 experiments:
  1. Test the retrieval accuracy of Advanced RAG-Tool Fusion on a small sample of the ToolE or Seal-Tools benchmark datasets with different tool document configurations
  2. Vary the values of tool-M and top-k to find the optimal balance between retrieval accuracy and computational efficiency
  3. Compare the performance of Advanced RAG-Tool Fusion with a simple BM25 baseline on a larger sample of the benchmark datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Advanced RAG-Tool Fusion system perform on tool datasets with high semantic overlap between tool descriptions, where multiple tools could plausibly answer the same query?
- Basis in paper: [explicit] The paper mentions that the Seal-Tools dataset has minimal tool overlap, and notes that other datasets may have queries answerable by multiple overlapping tools, causing artificial decreases in accuracy. It also suggests that future work could explore how the system handles complex tool datasets with overlapping tools.
- Why unresolved: The paper focuses on datasets with minimal overlap (Seal-Tools and ToolE) and doesn't provide experimental results for datasets with high tool overlap. The performance impact of tool overlap on retrieval accuracy is not quantified.
- What evidence would resolve it: Experiments comparing Advanced RAG-Tool Fusion performance on datasets with varying degrees of tool overlap, measuring recall@5 and agent accuracy as overlap increases. Analysis of how query decomposition and multi-query expansion handle ambiguous queries in high-overlap scenarios.

### Open Question 2
- Question: What is the optimal configuration for the Toolshed Knowledge Base (number and type of components to include in tool embeddings) for different types of tool domains?
- Basis in paper: [explicit] The paper suggests that different tool datasets may perform differently with different combinations of the 5 components (name, description, argument schema, hypothetical questions, key topics) and encourages researchers to benchmark their own datasets. It notes that for Seal-Tools, argument schema was critical while hypothetical queries added less value, but for ToolE the opposite was true.
- Why unresolved: The paper only tests a limited set of configurations on two datasets. The optimal configuration likely varies by domain (finance, healthcare, education, etc.) but this relationship is not systematically explored.
- What evidence would resolve it: Comparative experiments across multiple tool domains (finance, healthcare, general APIs, etc.) testing different configurations of the 5 components. Analysis showing which components are most valuable for each domain type and why.

### Open Question 3
- Question: How does the Advanced RAG-Tool Fusion system perform in production environments with multi-turn chat history where follow-up questions reference previous tool calls?
- Basis in paper: [explicit] Section 7 identifies this as a limitation, noting that the paper doesn't address scenarios where users ask follow-up questions like "what if the initial cost was $500 more?" after a previous tool call. It questions whether to reuse the initialized agent, use the single tool from the previous call, or re-run the modules.
- Why unresolved: The paper focuses on single-turn interactions and doesn't provide any experiments or analysis of multi-turn scenarios. The handling of chat history and tool state is not addressed.
- What evidence would resolve it: Experiments testing Advanced RAG-Tool Fusion in multi-turn chat scenarios, measuring accuracy and token efficiency when handling follow-up questions. Analysis of different strategies for maintaining context across turns (reuse previous tools, re-run retrieval, or hybrid approaches).

## Limitations
- System depends heavily on quality tool representations and effectiveness of RAG techniques employed
- Enhanced tool documents require careful construction with synthetic queries and key topics that may not generalize across domains
- Limited exploration of how retrieval improvements translate to end-task performance in real-world applications

## Confidence
- **High confidence** in the core claim that vector databases can effectively scale tool retrieval beyond 128 tools, supported by clear experimental results showing consistent improvements across multiple benchmarks
- **Medium confidence** in the specific RAG techniques' contribution to the improvements, as the paper doesn't provide ablation studies isolating the impact of individual techniques within Advanced RAG-Tool Fusion
- **Medium confidence** in the token cost optimization claims, as the paper focuses on retrieval accuracy metrics without comprehensive end-to-end latency measurements

## Next Checks
1. Conduct ablation studies isolating each component of Advanced RAG-Tool Fusion to quantify individual contributions to the 46-56% improvements
2. Test the Toolshed Knowledge Bases approach with tool collections from different domains (financial, healthcare, general web tools) to assess cross-domain generalization
3. Measure complete agent response times including tool retrieval, selection, and execution phases across different tool-M and top-k configurations to validate claimed token cost optimizations and identify bottlenecks