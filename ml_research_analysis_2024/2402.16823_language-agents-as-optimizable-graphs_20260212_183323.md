---
ver: rpa2
title: Language Agents as Optimizable Graphs
arxiv_id: '2402.16823'
source_url: https://arxiv.org/abs/2402.16823
tags:
- agents
- optimization
- node
- graph
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for building and optimizing LLM-based
  agents using computational graphs. Nodes represent operations like LLM queries or
  tool usage, edges define information flow, and composite graphs represent multi-agent
  swarms.
---

# Language Agents as Optimizable Graphs

## Quick Facts
- arXiv ID: 2402.16823
- Source URL: https://arxiv.org/abs/2402.16823
- Reference count: 40
- Key outcome: Graph-based framework enables automatic optimization of LLM-based agents at node and edge levels, improving performance on MMLU, Mini Crosswords, HumanEval, and GAIA benchmarks

## Executive Summary
This paper introduces GPTSwarm, a framework that models language agents as computational graphs where nodes represent operations like LLM queries and edges define information flow. The framework supports two levels of optimization: node optimization refines individual agent prompts through reinforcement learning, while edge optimization tunes inter-agent communication patterns via graph sampling and REINFORCE algorithm. Experiments demonstrate that edge optimization can filter adversarial agents, improve puzzle-solving accuracy, and enhance code generation through prompt refinement. The graph-based approach enables systematic construction and automatic improvement of complex multi-agent systems.

## Method Summary
The method involves constructing computational graphs where nodes represent operations (LLM queries, tool usage, function calls) and edges define information flow. Optimization occurs at two levels: node optimization uses reinforcement learning to iteratively refine prompts based on execution history, while edge optimization applies REINFORCE to tune agent communication patterns. The framework supports composite graphs representing swarms of agents, with experiments conducted on MMLU, Mini Crosswords, HumanEval, and GAIA benchmarks to evaluate performance improvements.

## Key Results
- Edge optimization successfully filters adversarial agents in MMLU experiments, improving accuracy from 33.3% to 50.0%
- GPTSwarm achieves 72.2% accuracy on Mini Crosswords, surpassing prior methods like Magicoder (66.7%)
- Node optimization improves HumanEval code generation accuracy from 48.4% to 59.7%
- GAIA benchmark results demonstrate the framework's ability to handle complex tasks requiring multiple tools and reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph representation enables systematic optimization of agent interactions by converting discrete communication patterns into a continuous optimization space.
- Mechanism: By parameterizing edge probabilities in composite graphs and applying REINFORCE algorithm, the framework transforms discrete agent communication decisions into a differentiable optimization problem.
- Core assumption: The utility function (task success) can be estimated from sampled graph configurations, even though the underlying LLM inference is non-differentiable.
- Evidence anchors: [abstract] "improve agent orchestration by changing graph connectivity (edge optimization)", [section] "we reformulate our edge optimization as a continuous optimization problem", [corpus] Weak - no direct comparison studies on REINFORCE for graph optimization found
- Break condition: If the utility estimation variance becomes too high, gradient estimates become unreliable and optimization fails to converge.

### Mechanism 2
- Claim: Node-level prompt optimization through iterative refinement based on execution history improves individual agent performance.
- Mechanism: Each node maintains a history of input-output pairs and uses an improvement function I to iteratively refine prompts based on past performance.
- Core assumption: Node prompts can be optimized independently when other prompts are fixed, despite potential interdependencies in complex graphs.
- Evidence anchors: [abstract] "refine node-level LLM prompts (node optimization)", [section] "our graph representation leads to a separation of concerns where each node has a specific purpose", [corpus] Weak - no direct studies on independent node prompt optimization in multi-agent graphs
- Break condition: If node interdependencies create cascading failures, independent optimization may degrade overall system performance.

### Mechanism 3
- Claim: Composite graph representation naturally captures hierarchical intelligence by allowing agent graphs to be composed into swarms.
- Mechanism: Individual agent graphs (representing specialized functions) can be recursively combined, with edges defining inter-agent communication channels.
- Core assumption: Intelligence emerges from the combination of modular components, following the "society of mind" principle.
- Evidence anchors: [abstract] "Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration", [section] "swarm, or composite graph, represents a complex system of agents where the collective capabilities of this system may exceed those of individual agents", [corpus] Weak - limited empirical validation of emergent intelligence in similar frameworks
- Break condition: If agent specialization becomes too narrow, the composite system may fail to handle tasks requiring cross-domain reasoning.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs) and topological ordering
  - Why needed here: The framework requires DAGs to ensure proper execution order without cycles, using topological sort for node execution
  - Quick check question: Can you explain why cycles in the execution graph would cause problems for this framework?

- Concept: Reinforcement Learning (specifically REINFORCE algorithm)
  - Why needed here: Edge optimization uses REINFORCE to handle the non-differentiable nature of LLM-based utility functions
  - Quick check question: How does REINFORCE handle the credit assignment problem in discrete action spaces?

- Concept: Multi-agent systems and communication protocols
  - Why needed here: Understanding how agents collaborate and communicate is fundamental to designing effective composite graphs
  - Quick check question: What are the key differences between centralized and decentralized communication patterns in multi-agent systems?

## Architecture Onboarding

- Component map: Nodes (LLM queries, tool usage, function calls) -> Graphs (individual agents) -> CompositeGraphs (swarms of agents) -> Optimizers (edge REINFORCE, node history-based refinement)

- Critical path: Input → Graph execution (topological order) → Output, with optimization loops for both nodes and edges

- Design tradeoffs:
  - Flexibility vs. performance: More modular nodes allow easier optimization but may increase communication overhead
  - Optimization granularity: Independent node optimization simplifies the problem but may miss global optima
  - Computational cost: REINFORCE requires sampling multiple graph configurations per iteration

- Failure signatures:
  - Edge optimization fails: Scores plateau below baseline, indicating inability to find better communication patterns
  - Node optimization fails: Prompts become overly specific to training data, causing overfitting
  - Graph execution fails: Cycles detected during topological sorting or nodes receiving insufficient context

- First 3 experiments:
  1. Implement a simple chain graph with 3 nodes and verify topological execution
  2. Create a composite graph with 2 agents and test basic inter-agent communication
  3. Run edge optimization on a small adversarial setting (1 truthful vs 1 adversarial agent) and verify filtering behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the GPTSwarm framework scale with the number of agents in the swarm, particularly when the number of agents exceeds 100?
- Basis in paper: [inferred] The paper discusses the modular nature of the framework and its ability to handle multiple agents, but does not provide experimental results or theoretical analysis on scaling beyond a certain number of agents.
- Why unresolved: The paper focuses on demonstrating the framework's capabilities with a moderate number of agents and does not explore the limits of scalability or the potential challenges that arise with a very large number of agents.
- What evidence would resolve it: Experimental results showing the performance of the GPTSwarm framework with swarms of varying sizes, particularly with more than 100 agents, would help determine how well the framework scales and identify any bottlenecks or limitations.

### Open Question 2
- Question: How does the internal node topology of each agent affect the overall performance of the swarm, and can dynamically changing the topology enhance task planning?
- Basis in paper: [inferred] The paper mentions that the current methods optimize edge connections between agents but does not address the internal structure of each agent or the potential benefits of dynamically changing the topology.
- Why unresolved: The paper focuses on optimizing the communication patterns between agents but does not explore the impact of the internal structure of each agent or the potential benefits of allowing agents to adapt their internal topology based on the task at hand.
- What evidence would resolve it: Experimental results comparing the performance of swarms with different internal node topologies and the ability to dynamically change the topology would help determine the importance of the internal structure and its impact on task planning and overall performance.

### Open Question 3
- Question: How can the GPTSwarm framework be extended to handle more complex tasks that require a combination of multiple tools and external resources, such as web browsing, file analysis, and code generation?
- Basis in paper: [explicit] The paper mentions that the GAIA benchmark tests for generality by including questions that require various tools and external resources, but the current implementation only uses a limited set of tools.
- Why unresolved: The paper demonstrates the framework's ability to handle tasks that require multiple tools but does not explore the limits of its capabilities or provide a comprehensive set of tools and external resources that can be integrated into the framework.
- What evidence would resolve it: A more extensive set of experiments using the GPTSwarm framework on tasks that require a wide range of tools and external resources, along with a discussion of the challenges and potential solutions for integrating these resources into the framework, would help determine its ability to handle complex tasks.

## Limitations

- The framework's scalability to very large swarms (100+ agents) remains untested, raising questions about computational feasibility
- The assumption of independent node optimization may break down in practice due to complex interdependencies between agents
- Empirical validation focuses on moderate complexity tasks, with limited evidence for emergent intelligence in composite graphs

## Confidence

- High confidence: The fundamental graph representation approach and basic optimization mechanisms are sound and well-founded in existing literature
- Medium confidence: The node optimization procedure and its claimed independence benefits, as the paper provides limited empirical evidence for complex scenarios
- Medium confidence: The edge optimization mechanism using REINFORCE, as the non-differentiable nature of LLM-based utility functions presents significant challenges that aren't fully addressed
- Low confidence: Claims about emergent intelligence from composite graphs, as the paper provides minimal evidence of such phenomena beyond basic demonstrations

## Next Checks

1. **Robustness Testing Under Node Interdependencies**: Design experiments where node prompts have known interdependencies (e.g., where output from node A directly affects the optimal prompt for node B). Test whether independent node optimization still improves overall performance or if it degrades due to cascading effects.

2. **Scalability Analysis of Edge Optimization**: Implement the edge optimization on progressively larger composite graphs (10, 50, 100 agents) and measure: (a) convergence time and stability, (b) variance in utility estimates across graph samples, and (c) whether the algorithm can still find meaningful improvements as the search space grows exponentially.

3. **Cross-Domain Transfer Validation**: Train optimized graphs on one task domain (e.g., MMLU general knowledge) and test performance on structurally similar but semantically different tasks (e.g., specialized domain quizzes). Measure whether the optimization generalizes or overfits to the specific training distribution.