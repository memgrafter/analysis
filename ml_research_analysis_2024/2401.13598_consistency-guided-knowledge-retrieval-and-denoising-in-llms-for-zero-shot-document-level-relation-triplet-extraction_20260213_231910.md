---
ver: rpa2
title: Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot
  Document-level Relation Triplet Extraction
arxiv_id: '2401.13598'
source_url: https://arxiv.org/abs/2401.13598
tags:
- relation
- data
- synthetic
- extraction
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of zero-shot document-level relation
  triplet extraction, where models must identify entities and their relationships
  in long documents without labeled training data for the target relations. The proposed
  GenRDK framework generates synthetic training data by prompting ChatGPT with a chain-of-retrieval
  approach that guides the model to create documents containing multiple complex relation
  triplets.
---

# Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction

## Quick Facts
- arXiv ID: 2401.13598
- Source URL: https://arxiv.org/abs/2401.13598
- Reference count: 40
- Zero-shot relation triplet extraction achieves up to 51.88 F1 score on DocRED

## Executive Summary
This paper tackles zero-shot document-level relation triplet extraction by generating synthetic training data using a chain-of-retrieval prompt to guide ChatGPT. The framework, called GenRDK, creates long documents containing multiple complex relation triplets through sequential generation steps. To improve data quality, a consistency-guided denoising strategy leverages cross-document knowledge graphs to identify and correct noisy labels. Experimental results demonstrate significant performance gains over competitive baselines on two public datasets.

## Method Summary
GenRDK generates synthetic training data by prompting ChatGPT with a chain-of-retrieval approach that breaks down document generation into sequential steps. A pre-denoising model is trained on seen relation data using LoRA fine-tuning to generate pseudo labels for synthetic data. Cross-document knowledge graphs are constructed from both synthetic and pseudo labels, and consistency scores are calculated to identify reliable relational facts. Unreliable facts are pruned and the synthetic data is relabeled before fine-tuning LLaMA2-13B-Chat on the denoised data for relation triplet extraction.

## Key Results
- Achieves 51.88 F1 score on DocRED dataset for zero-shot relation extraction
- Outperforms competitive baselines by substantial margins
- Shows consistent improvements across 5 and 10 unseen relation types
- Demonstrates effectiveness of consistency-guided denoising strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-retrieval prompt enables LLMs to generate long documents with multiple relation triplets by breaking down generation into sequential steps
- Mechanism: Sequential approach partitions complex generation into discrete steps: selecting related relations, generating document, extracting entities, extracting triplets, and providing reasoning
- Core assumption: LLMs can maintain coherence and follow step-by-step instructions when generating long documents
- Evidence anchors: [abstract] "chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step"; [section 3.2] "partitions the complex generation problem into a sequence of simple questions"
- Break condition: Fails if LLM loses context between steps or generates inconsistent entities

### Mechanism 2
- Claim: Consistency-guided cross-document knowledge denoising improves synthetic data quality by leveraging redundancy across multiple documents
- Mechanism: Multiple documents about same topic contain overlapping but different relational facts; consistency scores identify high-frequency facts as reliable
- Core assumption: Same relational facts appear in different forms across synthetic documents; consistent facts are more likely correct
- Evidence anchors: [abstract] "denoising strategy based on the consistency of cross-document knowledge"; [section 3.4] "calculate consistency scores to evaluate reliability of relational facts"
- Break condition: Fails if LLM consistently hallucinates same incorrect facts across documents

### Mechanism 3
- Claim: Pre-training on seen relations with LoRA fine-tuning enables reliable pseudo labels for denoising synthetic data
- Mechanism: Pre-denoising model trained on seen relations generates pseudo labels combined with synthetic labels for cross-document knowledge graphs
- Core assumption: Knowledge from seen relations transfers to help identify and correct errors in unseen relation triplets
- Evidence anchors: [abstract] "Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat"; [section 3.3] "train a pre-denoising model by data with seen relations to generate pseudo labels"
- Break condition: Fails if pseudo labels are systematically incorrect or LoRA adaptation insufficient

## Foundational Learning

- Concept: Zero-shot learning and few-shot learning paradigms
  - Why needed here: Framework operates in zero-shot setting requiring understanding of adapting models to unseen classes
  - Quick check question: What distinguishes zero-shot learning from few-shot learning in terms of training data availability and model adaptation?

- Concept: Knowledge graph construction and consistency scoring
  - Why needed here: Denoising strategy relies on building knowledge graphs from entity-relation pairs and computing consistency scores
  - Quick check question: How would you construct a knowledge graph from relation triplets, and what graph metrics could you use to measure fact consistency?

- Concept: Large language model prompting strategies (chain-of-thought, chain-of-retrieval)
  - Why needed here: Framework's effectiveness depends on designing prompts that guide LLMs to generate structured, multi-step outputs
  - Quick check question: What's the difference between chain-of-thought prompting and chain-of-retrieval prompting, and when would each be more appropriate?

## Architecture Onboarding

- Component map: Chain-of-retrieval prompt → ChatGPT generation → Seen relation training (LoRA) → Pre-denoising model → Pseudo labels → Cross-document knowledge graph construction → Consistency scoring → Pruning and relabeling → LLaMA2-13B-Chat fine-tuning → Relation triplet extractor
- Critical path: Most time-consuming steps are ChatGPT generation (slow and costly) and cross-document consistency calculation (scales with number of documents)
- Design tradeoffs: Trades computational cost for data quality; alternative could be simpler denoising with noisier data
- Failure signatures: Performance plateaus despite more synthetic data generation (suggesting denoising isn't working), or model overfits to synthetic data patterns (suggesting insufficient diversity)
- First 3 experiments:
  1. Run chain-of-retrieval prompt with single relation type and verify output contains at least 3-4 relation triplets across multiple sentences
  2. Generate 10 synthetic documents for same relation type and manually inspect for consistency vs. variation in facts
  3. Test pre-denoising model on small synthetic data sample to verify it generates pseudo labels matching ground truth on seen relations

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but several areas remain unexplored based on the experimental scope and methodology limitations.

## Limitations

- Effectiveness of consistency-guided denoising relies on assumption that correct facts appear consistently across documents while incorrect ones vary, without direct validation
- Framework's dependence on ChatGPT introduces potential costs and variability affecting reproducibility
- Transfer learning assumption from seen to unseen relations is empirically supported but lacks mechanistic explanation or ablation studies

## Confidence

**High Confidence**: Chain-of-retrieval prompting approach is well-established; overall framework architecture follows standard data-centric ML patterns

**Medium Confidence**: Empirical results show significant improvements but based on small test sets without statistical significance testing; denoising improvement demonstrated but pruning quality not verified

**Low Confidence**: Assumption that pseudo labels from seen relations help with unseen relation denoising lacks direct validation; paper shows approach works empirically but doesn't analyze whether pseudo labels are actually helpful

## Next Checks

1. **Manual verification of denoised facts**: Randomly sample 50 pruned and 50 retained triplets from denoising step, manually verify whether pruning decisions correctly identified noise versus valid but rare facts

2. **Ablation of pseudo label contribution**: Compare full denoising approach against version using only synthetic labels (no pseudo labels from seen relations) to isolate transfer learning component's contribution

3. **Robustness to generation variability**: Generate synthetic data for same relation types using different random seeds or modified prompts, measure consistency of generated documents and final model's performance