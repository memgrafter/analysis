---
ver: rpa2
title: 'FAMES: Fast Approximate Multiplier Substitution for Mixed-Precision Quantized
  DNNs--Down to 2 Bits!'
arxiv_id: '2411.18055'
source_url: https://arxiv.org/abs/2411.18055
tags:
- cientine
- appmuls
- energy
- approximate
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FAMES proposes a fast method for approximate multiplier substitution
  in mixed-precision DNNs, extending the use of approximate multipliers down to 2-bit
  precision. The key idea is to estimate the impact of approximate multipliers on
  DNN accuracy using a counting matrix and Taylor expansion, then select optimal multipliers
  for each layer via integer linear programming.
---

# FAMES: Fast Approximate Multiplier Substitution for Mixed-Precision Quantized DNNs--Down to 2 Bits!

## Quick Facts
- arXiv ID: 2411.18055
- Source URL: https://arxiv.org/abs/2411.18055
- Authors: Yi Ren; Ruge Xu; Xinfei Guo; Weikang Qian
- Reference count: 23
- Achieves 28.67% energy reduction over quantized models with bitwidths as low as 2 bits while maintaining less than 1% accuracy loss

## Executive Summary
FAMES introduces a fast method for approximate multiplier substitution in mixed-precision quantized deep neural networks, extending the use of approximate multipliers down to 2-bit precision. The key innovation is estimating the impact of approximate multipliers on DNN accuracy using a counting matrix and Taylor expansion, then selecting optimal multipliers for each layer via integer linear programming. This approach achieves significant energy reduction (28.67%) compared to quantized models of the same bitwidth while maintaining accuracy losses under 1%.

## Method Summary
FAMES addresses the challenge of integrating approximate multipliers into quantized DNNs by providing a fast estimation method that avoids costly retraining. The method operates in three stages: (1) perturbation estimation using a counting matrix to efficiently model error propagation through Taylor expansion, (2) integer linear programming-based selection of optimal approximate multipliers for each layer under energy constraints, and (3) calibration using learnable clipping weights to recover accuracy without full retraining. This approach enables the use of approximate multipliers with bitwidths as low as 2 bits, achieving substantial energy savings while maintaining high accuracy.

## Key Results
- Achieves 28.67% energy reduction over quantized models of the same bitwidth
- Maintains accuracy losses under 1% across tested models
- Runs up to 300Ã— faster than previous genetic algorithm-based methods
- Successfully extends approximate multiplier use down to 2-bit precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The counting matrix enables efficient error propagation modeling for approximate multipliers in quantized DNNs.
- Mechanism: By counting the number of occurrences of each possible input pair during convolution computation, the counting matrix transforms the complex error propagation into a linear operation between the error matrix and the counting matrix.
- Core assumption: The counting matrix is independent of the specific approximate multiplier being evaluated.
- Evidence anchors:
  - [section]: "To show the impact of the error matrix E(k,AMk) more directly, we transform the second term of Eq. (7) into the convolution of a counting matrix C (k,i,j) with the error matrix E(k,AMk), where C (k,i,j) is a matrix related to the output Y (k) i,j that tracks the number of occurrences of each possible multiplication input pairs during the calculation of Y (k) i,j."
  - [abstract]: "The key idea is to estimate the impact of approximate multipliers on DNN accuracy using a counting matrix and Taylor expansion"
- Break condition: If the input distributions change significantly across layers, the counting matrix may not accurately capture error propagation.

### Mechanism 2
- Claim: Taylor expansion enables efficient loss perturbation estimation without retraining.
- Mechanism: The method approximates the loss change caused by approximate multipliers using first and second-order derivatives of the loss with respect to error vectors, calculated once per layer and reused across different multipliers.
- Core assumption: The pre-trained model is converged enough that higher-order terms beyond second-order can be neglected.
- Evidence anchors:
  - [section]: "As we will reveal, ge(k) and He(k) are derived from the model parameters and the input distribution, which are irrelevant to the choice of AppMuls. Therefore, they only need be calculated once, which makes the estimation method efficient even for estimating many AppMuls."
  - [abstract]: "Our experiments demonstrate an average 28.67% energy reduction on state-of-the-art mixed-precision quantized models with bitwidths as low as 2 bits and accuracy losses kept under 1%."
- Break condition: If the loss landscape is highly non-convex or if the approximation error is too large, the Taylor expansion may become inaccurate.

### Mechanism 3
- Claim: Integer Linear Programming (ILP) enables optimal multiplier selection under energy constraints.
- Mechanism: The ILP formulation minimizes total loss perturbation while constraining total energy consumption, selecting the best approximate multiplier for each layer based on precomputed perturbation estimates.
- Core assumption: The energy consumption of each multiplier can be accurately modeled as a linear function of layer parameters.
- Evidence anchors:
  - [section]: "We propose an integer linear programming (ILP)-based method for selecting the optimal AppMul for each layer, leverages the above estimation of loss perturbation as the minimization target"
  - [abstract]: "Experiments show that FAMES achieves 28.67% energy reduction over quantized models of the same bitwidth with less than 1% accuracy loss"
- Break condition: If the energy model becomes non-linear due to complex circuit interactions, the ILP solution may be suboptimal.

## Foundational Learning

- Concept: Taylor series expansion and its application in loss approximation
  - Why needed here: The method uses Taylor expansion to estimate how errors from approximate multipliers propagate through the DNN to affect the final loss.
  - Quick check question: What mathematical condition must be satisfied for Taylor expansion to provide a good approximation of a function near a point?

- Concept: Integer Linear Programming (ILP) and its formulation
  - Why needed here: The method formulates the multiplier selection problem as an ILP to efficiently find the optimal configuration under energy constraints.
  - Quick check question: In an ILP formulation, what distinguishes the objective function from the constraints?

- Concept: Error propagation in neural networks
  - Why needed here: Understanding how errors from approximate computations propagate through layers is fundamental to the counting matrix approach.
  - Quick check question: How does the error introduced in one layer of a neural network typically affect subsequent layers?

## Architecture Onboarding

- Component map: Pre-trained quantized model -> Counting matrix generation -> Taylor expansion perturbation estimation -> ILP-based multiplier selection -> Calibration using learnable clipping weights -> Calibrated approximate model

- Critical path:
  1. Counting matrix generation for all layers
  2. Taylor expansion-based perturbation estimation
  3. ILP-based multiplier selection
  4. Calibration using learnable clipping weights
  The perturbation estimation and ILP selection are the most computationally intensive steps.

- Design tradeoffs:
  - Accuracy vs. energy consumption: The ILP formulation balances these competing objectives
  - Speed vs. precision: The Taylor expansion provides a fast but approximate solution compared to full retraining
  - Model complexity vs. implementation feasibility: The method supports down to 2-bit precision but requires careful calibration

- Failure signatures:
  - Accuracy degradation exceeding 1% threshold
  - ILP solver failing to find feasible solutions
  - Calibration process not converging
  - Energy estimation significantly diverging from actual hardware measurements

- First 3 experiments:
  1. Test counting matrix generation on a simple 2-layer network with known input patterns
  2. Verify Taylor expansion perturbation estimates against ground truth from full retraining on a small model
  3. Validate ILP-based selection by comparing against exhaustive search on a tiny model with few multiplier options

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of FAMES scale when applied to larger DNN models like ResNet-50 or deeper architectures?
  - Basis in paper: [inferred] The paper evaluates FAMES on ResNet-20, VGG-19, SqueezeNet, and ResNet-18, but does not explicitly test on larger models like ResNet-50 or deeper architectures.
  - Why unresolved: The paper focuses on smaller models, and the scalability to larger models is not discussed or tested.
  - What evidence would resolve it: Experimental results showing the performance of FAMES on larger models like ResNet-50 or deeper architectures, including accuracy, energy consumption, and runtime.

- Open Question 2: What is the impact of using different error metrics in ALSRAC on the performance of FAMES?
  - Basis in paper: [explicit] The paper mentions that ALSRAC supports various error metrics and chooses the mean relative error distance (MRED) as the metric with a threshold of 20%.
  - Why unresolved: The paper does not explore the impact of using different error metrics in ALSRAC on the performance of FAMES.
  - What evidence would resolve it: Comparative results showing the performance of FAMES using different error metrics in ALSRAC, including accuracy, energy consumption, and runtime.

- Open Question 3: How does the accuracy of FAMES compare when applied to other types of neural networks, such as recurrent neural networks (RNNs) or transformers?
  - Basis in paper: [inferred] The paper focuses on convolutional neural networks (CNNs) and does not explicitly test FAMES on other types of neural networks like RNNs or transformers.
  - Why unresolved: The paper does not explore the applicability of FAMES to other types of neural networks.
  - What evidence would resolve it: Experimental results showing the performance of FAMES on other types of neural networks like RNNs or transformers, including accuracy, energy consumption, and runtime.

## Limitations

- The counting matrix approach assumes stable input distributions across layers, which may not hold for all DNN architectures or datasets
- The Taylor expansion approximation relies on the pre-trained model being sufficiently converged, and the assumption that higher-order terms can be neglected may not hold for all cases
- The ILP formulation assumes linear energy consumption, which may not capture complex circuit interactions accurately

## Confidence

- Counting matrix error propagation: Medium - The method is theoretically sound but depends on stable input distributions
- Taylor expansion perturbation estimation: Medium - Valid for converged models but accuracy may degrade for complex loss landscapes
- ILP-based multiplier selection: High - The optimization framework is well-established, though energy modeling assumptions may introduce error

## Next Checks

1. Test the counting matrix approach on a network with varying input distributions across layers to assess its robustness
2. Compare Taylor expansion estimates against ground truth from full retraining on multiple network architectures to quantify approximation error
3. Validate the energy estimation model against actual hardware measurements across different approximate multiplier implementations