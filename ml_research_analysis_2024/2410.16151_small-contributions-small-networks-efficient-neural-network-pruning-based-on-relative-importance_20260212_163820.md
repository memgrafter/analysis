---
ver: rpa2
title: 'Small Contributions, Small Networks: Efficient Neural Network Pruning Based
  on Relative Importance'
arxiv_id: '2410.16151'
source_url: https://arxiv.org/abs/2410.16151
tags:
- pruning
- activation
- weight
- network
- wanda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying large-scale neural
  networks on resource-constrained devices by introducing an efficient pruning method
  based on relative importance. The core idea leverages statistical analysis and information
  theory to quantify the contribution of each weight to neuron outputs, using activation
  statistics to identify weights with minimal impact for removal.
---

# Small Contributions, Small Networks: Efficient Neural Network Pruning Based on Relative Importance

## Quick Facts
- arXiv ID: 2410.16151
- Source URL: https://arxiv.org/abs/2410.16151
- Reference count: 6
- One-line primary result: Achieves 97.63% accuracy on MNIST with 75% of weights pruned using statistical contribution analysis

## Executive Summary
This paper introduces a novel neural network pruning method that leverages statistical analysis and information theory to quantify the contribution of each weight to neuron outputs. The approach uses activation statistics to identify weights with minimal impact for removal, building a distribution of weight contributions across the dataset and utilizing its parameters to guide pruning decisions. The method demonstrates superior performance compared to baseline and state-of-the-art techniques, maintaining high accuracy even with aggressive pruning ratios.

## Method Summary
The proposed method calculates the relative contribution of each weight to neuron outputs using activation statistics across the training dataset. It models these contributions as a normal distribution, enabling statistical pruning decisions based on mean and standard deviation parameters. The approach introduces a blind range concept where weights causing neuron outputs to stay within regions of zero activation function derivative can be safely pruned. An optional Pruning-aware Training strategy incorporates L1 regularization on neuron outputs to enhance pruning effectiveness.

## Key Results
- Achieves 97.63% accuracy on MNIST even with 75% of weights pruned
- Outperforms baseline and state-of-the-art pruning techniques across different activation functions
- Maintains effectiveness across various pruning ratios (25-75%)
- Simple yet powerful solution for network compression suitable for resource-constrained devices

## Why This Works (Mechanism)

### Mechanism 1
Weights whose contributions fall within the activation function's "blind range" can be pruned without affecting model outputs. The activation function's derivative is zero in the blind range, meaning small perturbations in input do not change the output. Pruning weights that cause neuron outputs to stay within this range preserves model behavior. Core assumption: The blind range is wide enough and stable across the dataset to safely mask weight importance. Break condition: If the blind range is too narrow or weight pruning shifts outputs outside this range, accuracy degrades.

### Mechanism 2
Weight importance is modeled as a Gaussian distribution over dataset contributions, enabling statistical pruning decisions. By applying the Central Limit Theorem, the paper aggregates weight contributions across training examples into a normal distribution. The mean and standard deviation of this distribution are used to rank weights for pruning. Core assumption: The distribution of weight contributions across the dataset approximates a normal distribution due to the Central Limit Theorem. Break condition: If the contribution distribution is not Gaussian (e.g., heavy tails), the statistical assumptions fail.

### Mechanism 3
Mutual information between weights and neuron outputs quantifies pruning sensitivity. Mutual information measures how much knowing a weight reduces uncertainty about the neuron output. Weights with low mutual information are safe to prune. Core assumption: Mutual information can be reliably estimated from activation statistics without full probabilistic modeling. Break condition: If mutual information estimation is inaccurate or computationally infeasible, pruning decisions become unreliable.

## Foundational Learning

- Concept: Central Limit Theorem
  - Why needed here: Justifies modeling weight contribution distributions as Gaussian for statistical pruning
  - Quick check question: If you sum independent random variables, what distribution does their sum approach as the number of variables grows?

- Concept: Mutual Information
  - Why needed here: Provides a theoretical grounding for measuring how much a weight influences its neuron's output
  - Quick check question: If two variables are independent, what is their mutual information?

- Concept: Activation function derivatives and blind ranges
  - Why needed here: Determines safe pruning zones where weight changes do not affect outputs
  - Quick check question: For ReLU, over what input range is the derivative zero?

## Architecture Onboarding

- Component map: Contribution calculator -> Distribution estimator -> Importance scorer -> Pruning mask generator -> Pruning-aware trainer
- Critical path: Contribution calculation → Distribution fitting → Importance scoring → Thresholding → Mask application → Fine-tuning
- Design tradeoffs:
  - Statistical vs. deterministic pruning: Gaussian assumption simplifies computation but may miss edge cases
  - Layer decay factor: Balances pruning across layers but requires tuning
  - Data subset size: Smaller subsets speed computation but reduce accuracy
- Failure signatures:
  - Accuracy drop after pruning: Blind range too narrow or distribution assumptions invalid
  - Slow convergence: Layer decay too aggressive or regularization λrL1 too high
  - Memory overflow: Contribution matrix too large for full dataset
- First 3 experiments:
  1. Run ablation on β and layer decay s with 50% pruning target
  2. Compare ReLU vs. Sigmoid vs. Tanh blind range widths empirically
  3. Test pruning-aware training with varying λrL1 values on 25% pruning

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed pruning method perform on deeper architectures beyond simple fully connected networks, such as convolutional neural networks (CNNs) or transformers? The paper only tests the method on a simple 3-layer fully connected network for MNIST classification. The authors mention potential applicability to other architectures but do not provide experimental validation. Experiments applying the method to CNNs (e.g., ResNet, VGG) and transformers on diverse tasks (e.g., CIFAR-10, ImageNet, language modeling) would demonstrate the method's scalability and effectiveness across different network types and complexities.

### Open Question 2
What is the impact of the proposed pruning method on network generalization and robustness to adversarial attacks? While the paper reports high accuracy on MNIST, it does not address how pruning affects the network's ability to generalize to unseen data or its robustness to adversarial examples. Testing the pruned models on validation/test sets beyond the training data, and evaluating their performance under adversarial attacks (e.g., FGSM, PGD), would provide insights into the method's impact on generalization and robustness.

### Open Question 3
How does the proposed pruning method compare to other state-of-the-art pruning techniques in terms of inference speedup and energy efficiency on actual hardware? The paper demonstrates superior accuracy retention but does not measure practical benefits like inference time reduction or energy consumption on real devices. Benchmarking the pruned models on actual hardware (e.g., CPUs, GPUs, edge devices) to measure inference time, energy consumption, and memory usage would reveal the practical advantages of the method over other pruning techniques.

## Limitations

- The Gaussian distribution assumption for weight contributions may not hold for all network architectures or datasets
- The blind range concept depends heavily on activation function characteristics which vary across layers and training stages
- Mutual information estimation requires careful implementation to avoid computational bottlenecks

## Confidence

- High: The blind range mechanism for safe pruning is well-grounded in activation function theory
- Medium: The Gaussian distribution modeling of weight contributions shows empirical validity on MNIST but needs broader validation
- Low: The mutual information-based importance scoring requires further theoretical justification and computational validation

## Next Checks

1. Test the method on larger, more complex datasets (e.g., CIFAR-10) to evaluate scalability and robustness beyond MNIST
2. Conduct sensitivity analysis on the α and β parameters in the importance scoring function across different network architectures
3. Compare blind range widths empirically across activation functions and training stages to verify the stability assumption