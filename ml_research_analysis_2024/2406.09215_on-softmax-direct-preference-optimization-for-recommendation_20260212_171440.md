---
ver: rpa2
title: On Softmax Direct Preference Optimization for Recommendation
arxiv_id: '2406.09215'
source_url: https://arxiv.org/abs/2406.09215
tags:
- s-dpo
- preference
- language
- recommendation
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Softmax-DPO (S-DPO), an alternative version
  of Direct Preference Optimization (DPO) tailored for Language Model (LM)-based recommender
  systems. S-DPO addresses the limitation of existing LM-based recommenders that use
  language modeling loss, which fails to fully leverage user preference data and is
  not optimized for personalized ranking tasks.
---

# On Softmax Direct Preference Optimization for Recommendation

## Quick Facts
- arXiv ID: 2406.09215
- Source URL: https://arxiv.org/abs/2406.09215
- Authors: Yuxin Chen; Junfei Tan; An Zhang; Zhengyi Yang; Leheng Sheng; Enzhi Zhang; Xiang Wang; Tat-Seng Chua
- Reference count: 40
- Key outcome: S-DPO significantly outperforms traditional LM-based recommenders by instilling ranking information through multiple negative samples and hard negative mining.

## Executive Summary
This paper addresses the limitation of language model (LM)-based recommenders that rely solely on language modeling loss, which fails to fully leverage user preference data and optimize for personalized ranking tasks. The authors propose Softmax-DPO (S-DPO), an extension of Direct Preference Optimization (DPO) that incorporates multiple negative samples and extends DPO to the Plackett-Luce preference model. S-DPO bridges the gap between language modeling loss and preference alignment by instilling ranking information into LMs to distinguish preferred items from negatives, rather than solely focusing on positives. Theoretical analysis connects S-DPO with softmax loss over negative sampling, highlighting its inherent benefit of mining hard negatives for better optimization.

## Method Summary
The paper proposes S-DPO for LM-based recommender systems, addressing the limitation of language modeling loss that fails to leverage user preference data for personalized ranking. S-DPO extends DPO with the Plackett-Luce preference model to handle partial rankings in recommendation and incorporates multiple negative samples for better ranking gradients. The method involves supervised fine-tuning (SFT) of a pre-trained LM on recommendation-specific data, followed by preference alignment using the S-DPO loss function. The S-DPO loss is derived by substituting the reward function in DPO with a softmax-based formulation that considers multiple negative items, connecting it to contrastive learning paradigms that mine hard negative examples.

## Key Results
- S-DPO consistently outperforms all traditional recommenders and state-of-the-art LM-based recommenders on MovieLens, Goodreads, and LastFM datasets.
- The method demonstrates superiority in modeling user preference and boosting recommendation performance through effective ranking information instilled by multiple negative samples.
- S-DPO shows an inherent benefit of mining hard negative examples, leading to accelerated training and improved performance.

## Why This Works (Mechanism)

### Mechanism 1
S-DPO improves LM-based recommenders by instilling ranking information through multiple negative samples, bridging the gap between language modeling loss and personalized ranking tasks. By extending DPO with the Plackett-Luce preference model to handle partial rankings and connecting to softmax sampling strategies, S-DPO leverages multiple negatives for better ranking gradients. The core assumption is that incorporating multiple negative items in user preference data provides more effective gradients and better rewards for preferred items compared to traditional language modeling loss or pairwise DPO.

### Mechanism 2
S-DPO has an inherent benefit of mining hard negative examples, which boosts performance and accelerates training. The method assigns different weights to gradients of negative items based on their relative reward compared to other negatives, making it more discriminative in distinguishing preferred items. The core assumption is that hard negative items, which are more likely to be chosen by LM-based recommenders, provide more valuable gradients for optimization compared to easy negative items.

### Mechanism 3
S-DPO is more suitable for multi-negative user preference alignment than DPO due to its connection with softmax loss over negative sampling. As softmax loss works better than BPR loss in multi-negative scenarios, S-DPO is inferred to be more tailored for multi-negative user preference alignment. The core assumption is that softmax loss over negative sampling is more effective than pairwise loss (BPR) in multi-negative scenarios for recommendation tasks.

## Foundational Learning

- Concept: Language Modeling Loss vs. Preference Alignment
  - Why needed here: Understanding the difference between language modeling loss and preference alignment is crucial to grasp why S-DPO is necessary for LM-based recommenders.
  - Quick check question: What is the key difference between predicting the next token in a sequence and predicting a preferred item from a set of candidates in recommendation tasks?

- Concept: Direct Preference Optimization (DPO) and its extension to multi-negative scenarios
  - Why needed here: S-DPO is an extension of DPO, so understanding DPO's mechanism and limitations is essential to comprehend S-DPO's improvements.
  - Quick check question: How does DPO differ from traditional RLHF in aligning language models with human preferences?

- Concept: Plackett-Luce preference model and its application to partial rankings
  - Why needed here: S-DPO uses the Plackett-Luce model to handle partial rankings in recommendation, extending it from full rankings.
  - Quick check question: What is the main difference between full rankings and partial rankings in the context of recommendation tasks?

## Architecture Onboarding

- Component map: Pre-trained LM -> Supervised Fine-Tuning (SFT) -> Preference Alignment with S-DPO -> Evaluation
- Critical path: 1. Pre-train LM on large corpus 2. Supervised fine-tuning on recommendation-specific data 3. Preference alignment using S-DPO with multiple negatives 4. Evaluation using hit ratio and valid ratio metrics
- Design tradeoffs: Number of negative samples (more negatives provide better gradients but increase computational cost), value of β (controls balance between ranking signals and inherent LM knowledge), backbone LM choice (different LMs may have varying effectiveness when fine-tuned with S-DPO)
- Failure signatures: Degraded performance compared to traditional recommenders or LM-based baselines, inability to generate appropriate responses (low valid ratio), overfitting to training data leading to poor generalization
- First 3 experiments: 1. Ablation study comparing S-DPO with SFT and DPO to isolate the effect of explicit ranking optimization and multiple negative samples 2. Study on the number of negative samples to determine the optimal number for balancing performance and computational cost 3. Hyperparameter search for β to find the best value for balancing ranking signals and inherent LM knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of S-DPO scale with the number of negative samples beyond 15?
- Basis in paper: [explicit] The paper mentions that the number of negative samples is capped at 15 in experiments and states that the potential of multiple negative samples hasn't been fully explored.
- Why unresolved: Computational resources and time limitations prevented testing with more negative samples.
- What evidence would resolve it: Experiments showing performance improvements (or lack thereof) with varying numbers of negative samples, particularly beyond 15, would clarify the optimal number of negatives for S-DPO.

### Open Question 2
- Question: How does S-DPO perform on other recommendation tasks, such as those involving cold-start users or items?
- Basis in paper: [inferred] The paper focuses on sequential recommendation and evaluates on three datasets, but doesn't explicitly address cold-start scenarios.
- Why unresolved: The experiments only consider warm-start scenarios where sufficient interaction history exists.
- What evidence would resolve it: Experiments comparing S-DPO's performance on cold-start versus warm-start users/items would reveal its effectiveness in these challenging scenarios.

### Open Question 3
- Question: How does the choice of backbone language model architecture (e.g., encoder-decoder vs. decoder-only) affect S-DPO's performance?
- Basis in paper: [explicit] The paper mentions that experiments were conducted using models with varying architectures and sizes, including Llama1-7B, Pythia-2.8B, and Mistral-7B.
- Why unresolved: While different models were tested, the paper doesn't specifically analyze how the architecture type influences S-DPO's effectiveness.
- What evidence would resolve it: Comparative experiments using different backbone architectures (e.g., encoder-decoder like T5 vs. decoder-only like GPT) with S-DPO would clarify the impact of architecture on performance.

## Limitations
- Limited evaluation to only three datasets without comprehensive ablation studies isolating S-DPO's specific contributions.
- Lack of comparison with alternative preference optimization methods like PPO or SLiC to validate claimed superiority.
- No discussion of computational efficiency concerns or potential overfitting risks when using multiple negative samples.

## Confidence
- High Confidence: The theoretical foundation connecting S-DPO to softmax loss over negative sampling is well-established.
- Medium Confidence: The claim about S-DPO's effectiveness in mining hard negatives is supported by theoretical analysis but lacks extensive empirical validation.
- Low Confidence: The superiority of S-DPO over other preference optimization methods is claimed but not thoroughly validated through comprehensive ablation studies.

## Next Checks
1. Conduct ablation studies to isolate the contribution of each component in S-DPO by systematically removing or replacing each element.
2. Test S-DPO's performance across a broader range of datasets with different characteristics to assess generalizability beyond the three evaluated datasets.
3. Compare S-DPO against alternative preference optimization methods (PPO, SLiC, other DPO variants) in both ranking quality and computational efficiency to validate the claimed superiority.