---
ver: rpa2
title: 'What Matters in Explanations: Towards Explainable Fake Review Detection Focusing
  on Transformers'
arxiv_id: '2407.21056'
source_url: https://arxiv.org/abs/2407.21056
tags:
- fake
- review
- reviews
- detection
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes an explainable fake review detection framework
  using transformer models (DistilBERT and XLNet) combined with Layer-wise Relevance
  Propagation (LRP) for generating explanations. The authors compare their transformer-based
  models with deep learning baselines (BiLSTM, CNN variants) on two datasets: Fake
  Review Dataset (40K reviews) and Yelp Review Dataset (682K reviews).'
---

# What Matters in Explanations: Towards Explainable Fake Review Detection Focusing on Transformers

## Quick Facts
- arXiv ID: 2407.21056
- Source URL: https://arxiv.org/abs/2407.21056
- Reference count: 40
- Transformer-based models achieve state-of-the-art fake review detection performance (95.92% accuracy, 98.21% F1-score)

## Executive Summary
This paper introduces an explainable fake review detection framework using transformer models (DistilBERT and XLNet) combined with Layer-wise Relevance Propagation (LRP) for generating explanations. The authors evaluate their approach on two datasets: Fake Review Dataset (40K reviews) and Yelp Review Dataset (682K reviews), comparing transformer models against deep learning baselines. While transformer models achieve state-of-the-art performance, empirical user evaluation reveals that word-level explanations provide limited interpretability for fake review detection, as context and grammatical structure matter more than specific terms.

## Method Summary
The framework employs transformer models (DistilBERT and XLNet) for fake review detection, trained on two datasets with 70/15/15 train/validation/test splits. The models use pre-trained weights and are fine-tuned for classification. For explanations, Layer-wise Relevance Propagation (LRP) attributes relevance scores to individual words by backpropagating through network layers. The approach is compared against deep learning baselines including BiLSTM, CNN variants, and word embeddings (FastText). Performance is evaluated using accuracy, precision, F1-score, and AUC metrics.

## Key Results
- DistilBERT achieves 95.92% accuracy and 98.21% F1-score on the Fake Review Dataset, outperforming existing methods like OpenAI and fakeRoBERTa
- Transformer models significantly outperform deep learning baselines (BiLSTM, CNN variants) on both datasets
- Empirical user evaluation with 12 participants indicates that word-level explanations are insufficient for fake review detection interpretability
- Models perform better on machine-generated reviews (Fake Review Dataset) than human-written reviews (Yelp dataset)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based models achieve higher fake review detection performance because they capture contextual semantic relationships in text more effectively than traditional DL or ML models.
- Mechanism: Transformers use self-attention to weigh the importance of all words in a review relative to each other, allowing nuanced understanding of context, sentiment, and subtle cues in fake vs. real reviews.
- Core assumption: Fake review detection benefits from contextual understanding, not just keyword matching or bag-of-words representations.
- Evidence anchors:
  - [abstract]: "transformer models including XLNet and DistilBERT... experimental results... demonstrate that our predictive models achieve state-of-the-art performance and outperform several existing methods."
  - [section]: "In terms of all evaluation metrics, our proposed two transformer-based fake review detection models achieved significantly higher accuracy (0.9592), precision (0.9906), and F1-Score (0.98210) among all employed models."
  - [corpus]: Weak corpus evidence; neighbor papers mention multimodal detection but not direct transformer comparison.
- Break condition: If fake reviews are generated with very short, formulaic phrases where context is minimal, the advantage of transformers over simpler models may diminish.

### Mechanism 2
- Claim: Layer-wise Relevance Propagation (LRP) explains predictions by backpropagating relevance scores from output to input, attributing importance to individual words.
- Mechanism: LRP redistributes the prediction score through the network layers using a relevance conservation principle, assigning higher relevance scores to words that most influence the classification decision.
- Core assumption: Individual word relevance scores can meaningfully explain model predictions in fake review detection.
- Evidence anchors:
  - [abstract]: "We then introduce layer-wise relevance propagation (LRP) technique for generating explanations that can map the contributions of words toward the predicted class."
  - [section]: "LRP technique can unveil the black-box deep learning model by back-propagating the output from the output layer up to the input layers through re-distributing the weight in the previous layers."
  - [corpus]: No direct corpus evidence about LRP performance in fake review detection; neighbors discuss explainability but not LRP specifically.
- Break condition: If users find word-level explanations insufficient (as the empirical study suggests), the mechanism fails to meet interpretability goals.

### Mechanism 3
- Claim: User comprehension of explanations is limited when they focus on individual words rather than overall context, tone, and grammatical structure.
- Mechanism: Users rely on holistic understanding of text, so highlighting specific words does not align with their decision-making process in identifying fake reviews.
- Core assumption: Fake review detection by humans depends more on contextual cues and writing patterns than on specific keywords.
- Evidence anchors:
  - [abstract]: "empirical user evaluation with 12 participants suggests that highlighting individual words provides limited interpretability for fake review detection, as context and grammatical structure matter more than specific terms."
  - [section]: "The empirical evaluation with 12 human subjects... concludes which important information needs to be considered in generating explanations in the context of fake review identification."
  - [corpus]: Weak; neighbor papers discuss fake review detection but not user evaluation of explanations.
- Break condition: If future studies show users can effectively use word-level explanations when combined with contextual summaries, this mechanism's limitation may be mitigated.

## Foundational Learning

- Concept: Transformer architecture (self-attention, positional encoding, layer stacking)
  - Why needed here: Transformers process entire review sequences simultaneously, capturing dependencies between words regardless of distance, crucial for understanding nuanced fake review patterns.
  - Quick check question: What is the primary advantage of self-attention over recurrent layers in processing reviews?

- Concept: Layer-wise Relevance Propagation (LRP) algorithm and relevance conservation
  - Why needed here: LRP explains individual predictions by attributing relevance to input words, providing transparency for model decisions in fake review detection.
  - Quick check question: How does LRP ensure that total relevance is conserved when backpropagating through layers?

- Concept: Evaluation metrics for imbalanced datasets (AUC, precision, recall, F1-score)
  - Why needed here: Fake review datasets are often imbalanced; appropriate metrics ensure fair model assessment beyond simple accuracy.
  - Quick check question: Why is AUC preferred over accuracy when evaluating models on imbalanced fake review datasets?

## Architecture Onboarding

- Component map:
  Data preprocessing → Tokenizer (DistilBERT/XLNet) → Transformer layers → Pooler/Classification head → LRP explainer → Explanation visualization
  Alternative path: Data preprocessing → Embedding layer → BiLSTM/CNN layers → Dense layers → Classifier → LRP explainer

- Critical path: Input text → Tokenizer → Transformer → Classification → LRP relevance computation → Highlighted explanation
  - Bottleneck: LRP computation can be slow for long reviews; consider limiting input length or using faster approximation methods.

- Design tradeoffs:
  - Transformer vs. DL: Higher accuracy and contextual understanding vs. increased computational cost and complexity.
  - LRP vs. other XAI methods (SHAP, LIME): Direct relevance attribution vs. approximation-based methods; LRP may be faster but less flexible for non-linear interactions.
  - Word highlighting vs. contextual summaries: Granular explanations vs. user-friendly interpretability.

- Failure signatures:
  - Low LRP relevance scores across all words: Model may not be confident or LRP parameters need tuning.
  - High variance in user evaluations: Explanations may not align with human reasoning patterns.
  - Performance drop on human-written reviews vs. LLM-generated: Model may overfit to synthetic data patterns.

- First 3 experiments:
  1. Compare transformer model performance with and without LRP explanations on a held-out test set to ensure explanations don't degrade accuracy.
  2. Run LRP on a diverse set of reviews (fake and real) to visualize and validate that highlighted words align with expected fake review indicators.
  3. Conduct a small-scale user study (3-5 participants) to test if highlighted word explanations improve user trust or understanding compared to no explanations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do transformer-based models compare to deep learning models for fake review detection across different datasets and languages?
- Basis in paper: [explicit] The paper states that transformer-based models (DistilBERT and XLNet) significantly outperformed deep learning models (BiLSTM, CNN, CNN-LSTM, CNN-GRU) on both the Fake Review Dataset and Yelp Review Dataset, achieving state-of-the-art performance.
- Why unresolved: The study focused on English language datasets. Performance may vary across different languages, review types, or domain-specific vocabulary.
- What evidence would resolve it: Comparative studies across multilingual datasets, domain-specific review collections, and different language models would provide insights into model generalizability.

### Open Question 2
- Question: What are the most effective explanation methods for fake review detection that balance interpretability and accuracy?
- Basis in paper: [explicit] The paper conducted an empirical user evaluation that found LRP-generated word-level explanations had limited effectiveness, as users found context and grammatical structure more important than individual highlighted words.
- Why unresolved: The study tested only LRP technique and did not explore alternative explanation methods like SHAP, LIME, or attention visualization that might better capture contextual information.
- What evidence would resolve it: Comparative evaluation of multiple explanation methods using human subjects would identify which approaches best support user understanding of fake review predictions.

### Open Question 3
- Question: How do different review characteristics (length, sentiment, grammatical quality) impact fake review detection performance?
- Basis in paper: [inferred] The paper noted performance differences between human-written reviews (Yelp dataset) and machine-generated reviews (Fake Review Dataset), suggesting that review characteristics influence detection accuracy.
- Why unresolved: The study did not systematically analyze how specific review features affect model performance or whether different models excel at detecting different types of fake reviews.
- What evidence would resolve it: Controlled experiments varying review characteristics and analyzing model performance on different review types would reveal how features impact detection effectiveness.

## Limitations
- Small empirical user study (n=12) limits generalizability of conclusions about explanation effectiveness
- No hyperparameter settings or specific LRP implementation details provided for exact reproduction
- Focus on word-level explanations may miss more effective explanation formats like contextual summaries

## Confidence
- **High Confidence:** Transformer models (DistilBERT, XLNet) achieve state-of-the-art performance on fake review detection datasets (accuracy 95.92%, F1-score 98.21% on Fake Review Dataset)
- **Medium Confidence:** LRP successfully generates word-level explanations that align with model predictions
- **Low Confidence:** Word-level explanations are insufficient for user comprehension of fake review detection decisions

## Next Checks
1. Conduct a larger-scale user study (n≥50) with diverse participants to validate whether word-level explanations or alternative formats (contextual summaries, attention maps) better support user understanding of fake review detection
2. Test the robustness of transformer models on reviews generated by different LLM architectures beyond ULMFiT and GPT-2 to ensure generalizability
3. Implement and compare alternative XAI methods (SHAP, LIME, attention visualization) against LRP to determine which best balances computational efficiency with user interpretability for fake review detection