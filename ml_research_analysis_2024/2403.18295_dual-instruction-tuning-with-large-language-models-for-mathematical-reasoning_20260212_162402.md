---
ver: rpa2
title: Dual Instruction Tuning with Large Language Models for Mathematical Reasoning
arxiv_id: '2403.18295'
source_url: https://arxiv.org/abs/2403.18295
tags:
- instruction
- reasoning
- uni00000013
- task
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dual instruction tuning strategy for large
  language models to enhance mathematical reasoning. The method introduces two auxiliary
  tasks - Intermediate Reasoning State Prediction (IRSP) and Instruction Reconstruction
  (IR) - to model reasoning from both forward and reverse directions.
---

# Dual Instruction Tuning with Large Language Models for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2403.18295
- Source URL: https://arxiv.org/abs/2403.18295
- Reference count: 23
- Primary result: Dual instruction tuning with IRSP and IR tasks improves mathematical reasoning accuracy by 7-36% across various benchmarks

## Executive Summary
This paper introduces a dual instruction tuning strategy for large language models to enhance mathematical reasoning capabilities. The approach adds two auxiliary tasks - Intermediate Reasoning State Prediction (IRSP) and Instruction Reconstruction (IR) - that train models to understand instruction-thought mappings in both forward and reverse directions. By fine-tuning on existing mathematical instructions plus newly created IRSP/IR data, the method improves reasoning accuracy, reduces errors, and enhances domain generalization across multiple mathematical reasoning benchmarks.

## Method Summary
The method employs multi-task fine-tuning with CodeLlama models using three data sources: original mathematical instructions (CoT/PoT pairs), IRSP task data (masked intermediate reasoning steps), and IR task data (masked instruction clauses). IRSP masks 15% of reasoning steps and predicts them from instruction and partial thoughts, while IR masks 60% of instruction clauses and reconstructs them from thoughts. The model uses a hybrid CoT/PoT instruction tuning strategy, training on 20% IRSP data and 60% IR task data alongside the original dataset to improve both reasoning accuracy and instruction comprehension.

## Key Results
- Dual instruction tuning improves reasoning accuracy by 7-36% across GSM8K, MATH, NumGLUE, AQuA, SV AMP, and Mathematics benchmarks
- IRSP task with 15% masking ratio optimizes performance on complex reasoning tasks like MATH and AQuA
- IR task with 60% masking ratio enhances instruction comprehension while maintaining task performance
- Models show improved domain generalization compared to baseline instruction tuning approaches

## Why This Works (Mechanism)

### Mechanism 1
The dual instruction tuning strategy addresses reasoning errors by enhancing bidirectional understanding between instructions and thoughts. IRSP predicts masked reasoning steps from instruction context, enforcing accurate parsing of instruction clauses, while IR reconstructs masked instruction clauses from thoughts, ensuring reverse comprehension. This bidirectional mapping reduces errors, omissions, and redundancies in generated reasoning.

### Mechanism 2
Multi-task fine-tuning with original instructions plus IRSP/IR data improves domain generalization. By training on diverse data sources, the model learns to apply reasoning skills across varied problem formats rather than memorizing patterns. This exposure to different instruction styles and reasoning structures enables better transfer to unseen mathematical domains.

### Mechanism 3
The hybrid CoT/PoT instruction tuning combines abstract reasoning steps with executable programs. CoT provides transparent reasoning structure for understanding problem decomposition, while PoT offers computational precision for accurate calculations. This integration compensates for individual weaknesses, enabling both logical reasoning and numerical accuracy.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Enables transparent breakdown of complex problems into intermediate steps for correctable reasoning
  - Quick check question: What is the purpose of generating intermediate reasoning steps in CoT?

- Concept: Program-of-Thought (PoT) reasoning
  - Why needed here: Provides structured, executable reasoning that can be computationally verified to reduce logical errors
  - Quick check question: How does PoT differ from CoT in handling mathematical reasoning?

- Concept: Supervised fine-tuning with instruction data
  - Why needed here: Aligns model responses with desired instruction-following behavior for consistent mathematical task performance
  - Quick check question: What is the role of instruction data in adapting a pre-trained model to specific tasks?

## Architecture Onboarding

- Component map: System instruction -> User instruction -> Target response (CoT/PoT) -> Encoder -> Decoder -> Masked token prediction -> Loss computation
- Critical path: 1) Load instruction-thought pair, 2) Apply masking for IRSP/IR if applicable, 3) Concatenate system instruction, instruction, and partial thought, 4) Generate response step-by-step, 5) Compute loss on target tokens, 6) Update model parameters
- Design tradeoffs: Masking ratio affects difficulty vs performance; task ratio impacts training efficiency; model size trades generalization vs resource requirements
- Failure signatures: High training loss but low validation accuracy indicates overfitting or poor data quality; low training loss but poor performance suggests underfitting or insufficient capacity; disproportionate errors in certain tasks indicate imbalanced data or masking issues
- First 3 experiments: 1) Train baseline with original instruction data only; 2) Add IRSP task with 20% data ratio and 15% masking; 3) Add IR task with 60% data ratio and 60% masking

## Open Questions the Paper Calls Out

### Open Question 1
How does the dual instruction tuning strategy perform on mathematical reasoning tasks requiring advanced mathematical knowledge beyond basic arithmetic and algebra? The paper acknowledges limitations with complex mathematical domains and suggests pre-training on extensive mathematical data might be more effective.

### Open Question 2
How well does the strategy generalize to mathematical reasoning tasks in domains significantly different from training data? While the paper mentions improved domain generalization, it doesn't explore performance on completely unrelated domains like financial modeling or physics-based simulations.

### Open Question 3
How does the IRSP task masking ratio affect performance across different types of mathematical reasoning tasks? The paper found 15% optimal for MATH/AQuA but didn't analyze task-specific optimal ratios for numerical computation, logical reasoning, or spatial reasoning tasks.

## Limitations

- Implementation details for IRSP and IR tasks lack specification of exact data augmentation processes and prompt templates
- Experimental results based on single dataset (MathInstruct) may not fully represent generalization capabilities
- Limited analysis of model performance on individual tasks and masking ratio effects

## Confidence

- **High Confidence**: Dual instruction tuning strategy improves mathematical reasoning abilities (significant benchmark gains)
- **Medium Confidence**: IRSP and IR tasks enhance bidirectional instruction-thought understanding (reduces errors/redundancies)
- **Low Confidence**: Multi-task fine-tuning ensures domain generalization (insufficient evidence provided)

## Next Checks

1. Conduct detailed task implementation analysis with varying IRSP masking ratios (10%, 15%, 20%) and IR task ratios (50%, 60%, 70%) to determine optimal settings and validate impact on reasoning accuracy

2. Test dual instruction tuning strategy on additional mathematical reasoning datasets (GSM8K, MultiArith, ASDiv) and benchmarks (MATH, AQuA) to assess broader generalization capabilities

3. Perform ablation study training models with only IRSP, only IR, and both tasks combined to isolate individual and synergistic effects on mathematical reasoning performance