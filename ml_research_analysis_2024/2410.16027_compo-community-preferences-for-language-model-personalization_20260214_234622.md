---
ver: rpa2
title: 'ComPO: Community Preferences for Language Model Personalization'
arxiv_id: '2410.16027'
source_url: https://arxiv.org/abs/2410.16027
tags:
- subreddit
- preference
- preferences
- human
- community
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of personalizing language models
  to diverse human preferences by contextualizing model outputs with community identifiers
  (e.g., subreddit names) during preference optimization. The authors collect and
  release COMPR ED, a dataset of community-level preferences from Reddit spanning
  five domains, and propose Community Preference Optimization (COMPO), which incorporates
  subreddit context into supervised fine-tuning and direct preference optimization
  stages.
---

# ComPO: Community Preferences for Language Model Personalization

## Quick Facts
- **arXiv ID**: 2410.16027
- **Source URL**: https://arxiv.org/abs/2410.16027
- **Reference count**: 40
- **Primary result**: Conditioning language models on community identifiers during preference optimization substantially enhances model performance and generates responses preferred by human annotators and GPT-4 judges.

## Executive Summary
This paper addresses the challenge of personalizing language models to diverse human preferences by contextualizing model outputs with community identifiers (e.g., subreddit names) during preference optimization. The authors collect and release COMPR ED, a dataset of community-level preferences from Reddit spanning five domains, and propose Community Preference Optimization (COMPO), which incorporates subreddit context into supervised fine-tuning and direct preference optimization stages. Experiments demonstrate that COMPO significantly outperforms non-contextualized baselines in generating responses preferred by human annotators and GPT-4 judges, while random subreddit contexts degrade performance. The approach effectively tailors model outputs to community norms and values, highlighting the importance of community-specific personalization in language model alignment.

## Method Summary
The authors propose Community Preference Optimization (COMPO), a framework that conditions language models on community identifiers during preference optimization. The approach involves two stages: (1) supervised fine-tuning with subreddit context, where the model learns to associate community identifiers with preferred responses, and (2) direct preference optimization (DPO) that further refines the model using preference pairs with subreddit context. The method leverages a newly collected COMPR ED dataset containing 1.3M preference pairs from Reddit across five domains (science, finance, history, politics, gender/sexuality), where community preferences are inferred from upvote patterns. The model learns to generate responses that align with specific community norms and values by incorporating subreddit names as context during both training stages.

## Key Results
- COMPO significantly outperforms non-contextualized baselines (SFT-NC, DPO, SFT-C) in generating responses preferred by human annotators and GPT-4 judges
- Randomizing subreddit contexts during training (COMPO -RANDOMIZED) degrades performance, confirming the importance of community-specific conditioning
- The approach effectively captures community-specific preferences while avoiding privacy concerns associated with individual feedback aggregation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning on subreddit identifiers during preference optimization leads to more tailored model outputs that better reflect community-specific preferences.
- Mechanism: By incorporating subreddit names as context in both supervised fine-tuning and direct preference optimization stages, the model learns to associate specific community norms and values with generated responses, enabling personalization at the community level.
- Core assumption: Reddit subreddit communities have distinct preferences and norms that can be captured through upvote patterns and community context.
- Evidence anchors:
  - [abstract] "Experiments reveal that conditioning language models on a community identifier (i.e., subreddit name) during preference tuning substantially enhances model performance."
  - [section 4] "Reddit consists of thousands of subreddits often discussing similar topics but different participants and different upvoting patterns."
- Break condition: If community preferences are too heterogeneous or overlapping to be captured by subreddit identifiers alone, or if upvote patterns don't reliably reflect community preferences.

### Mechanism 2
- Claim: Direct Preference Optimization (DPO) with community context is more effective than standard preference tuning or supervised fine-tuning alone.
- Mechanism: DPO implicitly optimizes for the same objective as RLHF but offers higher stability. When combined with community context, it allows the model to learn community-specific reward functions that guide response generation toward community-preferred outputs.
- Core assumption: Community-specific reward functions learned through DPO can effectively guide response generation toward community-preferred outputs.
- Evidence anchors:
  - [abstract] "Our experiments reveal that conditioning language models on a community identifier (i.e., subreddit name) during preference tuning substantially enhances model performance."
  - [section 3] "We adopt this framework for all experiments in this work" (referring to DPO).
- Break condition: If the DPO framework fails to capture nuanced community preferences or if community-specific reward functions conflict with general language quality.

### Mechanism 3
- Claim: Community-level preference data from Reddit provides sufficient signal for effective personalization without requiring individual user data.
- Mechanism: By aggregating preferences at the community level through upvote patterns, the approach sidesteps privacy concerns associated with individual feedback while still capturing meaningful preference variations across communities.
- Core assumption: Community-level upvote patterns are a reliable proxy for community preferences and can be aggregated to train effective personalization models.
- Evidence anchors:
  - [abstract] "This dataset facilitates studying diversity in preferences without incurring privacy concerns associated with individual feedback."
  - [section 4] "We collect and release COMPR ED, a question answering dataset with community-level preferences from Reddit."
- Break condition: If community-level aggregation smooths out important individual preferences or if upvote patterns don't reliably reflect true community preferences.

## Foundational Learning

- Concept: Preference optimization in language models
  - Why needed here: Understanding how language models can be tuned to generate outputs that align with human preferences is fundamental to this work's approach.
  - Quick check question: What's the difference between supervised fine-tuning and preference optimization in language models?

- Concept: Community-based preference aggregation
  - Why needed here: The approach relies on aggregating preferences at the community level rather than individual level, which requires understanding how community preferences can be captured and modeled.
  - Quick check question: How can upvote patterns on Reddit be used as a proxy for community preferences?

- Concept: Contextual conditioning in language models
  - Why needed here: The core innovation involves conditioning model outputs on community identifiers, which requires understanding how context can be incorporated into language model training.
  - Quick check question: What are the different ways to incorporate contextual information into language model training pipelines?

## Architecture Onboarding

- Component map: Data collection → Preprocessing → SFT with community context → DPO with community context → Evaluation
- Critical path: Data collection → Preprocessing → SFT with community context → DPO with community context → Evaluation
- Design tradeoffs:
  - Community-level vs. individual-level personalization
  - Computational cost vs. personalization granularity
  - Data quality vs. quantity (upvote patterns vs. explicit preferences)
  - Generalizability vs. community-specific optimization
- Failure signatures:
  - Poor performance when subreddit context is randomized (COMPO -RANDOMIZED)
  - No improvement over baselines in predictable subreddits
  - High gibberish rates in human evaluation
  - Low agreement between GPT-4 and human judges
- First 3 experiments:
  1. Train reward models with and without subreddit context on the collected data to verify context importance
  2. Compare COMPO against baselines (SFT-NC, DPO, SFT-C) using GPT-4 evaluation
  3. Conduct human evaluation on a subset of subreddits to validate model preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much training data is required for effective personalization across different community sizes and characteristics?
- Basis in paper: [inferred] The authors mention that "how much data is needed for effective personalization" is an important future direction, and their analysis in Section 6.3 suggests subreddit predictability affects personalization effectiveness.
- Why unresolved: The paper doesn't systematically vary training dataset sizes to determine the minimum effective amount of preference data needed for different types of communities.
- What evidence would resolve it: Controlled experiments varying training dataset sizes across different community types (politics, science, finance, etc.) while measuring personalization effectiveness would establish data requirements.

### Open Question 2
- Question: Can models effectively handle individual-level personalization beyond community-level aggregation?
- Basis in paper: [explicit] The authors state "how to solve the cold start problem" and "can we build hybrid models that consider both explicit and implicit preferences" as future directions.
- Why unresolved: The current approach aggregates preferences at the community level, but individual preferences may diverge from community norms, and there's no mechanism for handling individual-specific preferences.
- What evidence would resolve it: Developing and testing a model that combines community-level context with individual preference signals, along with metrics showing improved personalization over community-level approaches alone.

### Open Question 3
- Question: How can models be designed to avoid echo chambers while maintaining effective personalization?
- Basis in paper: [explicit] The authors discuss this directly in their "Ethical Considerations" section, noting that "models that optimize towards the preferences of an individual or a community can be prone to learning specific undesired stereotypes."
- Why unresolved: The paper acknowledges this risk but doesn't propose concrete solutions for balancing personalization with exposure to diverse perspectives.
- What evidence would resolve it: Implementation and evaluation of hybrid approaches that incorporate both personalized responses and exposure to alternative viewpoints, with metrics measuring both personalization effectiveness and diversity of perspectives.

## Limitations

- The approach may not capture fine-grained individual preferences within communities, as it aggregates at the community level
- Performance depends on the reliability of upvote patterns as proxies for community preferences, which may not always reflect nuanced preferences
- The method requires substantial amounts of community preference data for effective personalization, raising scalability concerns

## Confidence

- **High confidence**: The mechanism of conditioning on subreddit identifiers during preference optimization works as described and improves performance over non-contextualized baselines
- **Medium confidence**: The COMPR ED dataset captures meaningful community-level preferences that generalize across evaluation settings
- **Medium confidence**: The performance improvements translate to genuinely more preferred responses from human perspectives

## Next Checks

1. Conduct large-scale human preference studies across all tested subreddits to validate GPT-4 judge results and measure true preference alignment
2. Test the approach on non-Reddit community data (Discord servers, specialized forums) to assess generalizability beyond the collected dataset
3. Implement ablation studies varying the strength of subreddit conditioning (different LoRA ranks, attention masking strategies) to identify optimal personalization granularity