---
ver: rpa2
title: Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains
arxiv_id: '2402.18747'
source_url: https://arxiv.org/abs/2402.18747
tags:
- metrics
- translation
- domain
- pre-trained
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the domain robustness of fine-tuned machine
  translation (MT) metrics. A new biomedical domain dataset with 25k MQM annotations
  across 11 language pairs is introduced to evaluate MT metrics trained on prior WMT
  data.
---

# Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains

## Quick Facts
- arXiv ID: 2402.18747
- Source URL: https://arxiv.org/abs/2402.18747
- Reference count: 26
- Fine-tuned MT metrics (e.g., COMET) show significantly lower correlation with human judgments in unseen biomedical domain compared to surface-form metrics (BLEU, CHRF, TER) and pre-trained but not fine-tuned metrics (BERTScore, PRISM)

## Executive Summary
This work investigates the domain robustness of fine-tuned machine translation (MT) metrics by introducing a new biomedical domain dataset with 25k MQM annotations across 11 language pairs. The study compares fine-tuned metrics like COMET against surface-form metrics (BLEU, CHRF, TER) and pre-trained but not fine-tuned metrics (BERTScore, PRISM) when evaluated on both WMT-style and biomedical data. Results show that fine-tuned metrics exhibit a substantial performance drop in the unseen biomedical domain, suggesting that fine-tuning on WMT-style annotations introduces domain-specific bias that doesn't generalize well. The paper demonstrates that this gap persists throughout the fine-tuning process and is not due to pre-trained model limitations, but can be mitigated by fine-tuning with in-domain biomedical data.

## Method Summary
The paper evaluates machine translation metrics by comparing their segment-level Kendall's τ correlation with human judgments on both WMT22 MQM dataset and a new biomedical MQM dataset containing 25k annotations across 11 language pairs. Metrics are categorized into three types: surface-form metrics (BLEU, CHRF, TER), pre-trained metrics not fine-tuned on MT quality judgments (BERTScore, PRISM), and fine-tuned metrics (COMET, BLEURT, UniTE). The evaluation pipeline involves loading pre-trained models, preparing fine-tuning data, training regression heads for fine-tuned metrics, and computing correlation scores against human annotations. The study systematically compares performance across these categories to identify domain robustness gaps.

## Key Results
- Fine-tuned metrics exhibit substantial performance drops in unseen biomedical domain compared to both surface-form and pre-trained metrics
- The performance gap persists throughout fine-tuning process and is not due to catastrophic forgetting
- Fine-tuning with in-domain biomedical data significantly improves COMET's performance in the biomedical domain

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned metrics degrade in unseen domains because their extra parameters are trained only on WMT-style annotations and lack generalization. The fine-tuning stage adds a lightweight regression head on top of a pre-trained model. When this head is trained on WMT human judgments, it learns domain-specific features that don't transfer well to biomedical data, leading to lower correlation. This gap suggests that fine-tuned metrics struggle with training/inference domain mismatch.

### Mechanism 2
Pre-trained models without fine-tuning retain more general linguistic features, which can be leveraged by algorithm-based metrics. Pre-trained models like XLM-RoBERTa or NLLB learn general cross-lingual representations. Metrics like BERTScore or PRISM use these representations directly without further adaptation, preserving their broad domain applicability. Pre-trained metrics which are not fine-tuned on MT quality judgments show less performance drop in unseen domains.

### Mechanism 3
Domain gap is not due to catastrophic forgetting but to the inherent mismatch between WMT and biomedical domain features. Experiments varying fine-tuning epochs show stable low performance on bio, not a drop from overfit, indicating the model never learned bio-relevant features. The model is a weak bio metric at all stages, as opposed to first learning and then forgetting.

## Foundational Learning

- **Domain adaptation in NLP**: Why needed - The paper hinges on how models behave under domain shift, a central concern in adaptation. Quick check - What is the difference between in-domain and out-of-domain performance, and why does it matter for metrics?
- **Fine-tuning vs. pre-training**: Why needed - The study compares metrics that fine-tune on human judgments versus those that do not. Quick check - What changes in a model when it is fine-tuned on a new task versus when it is used as-is from pre-training?
- **Meta-evaluation of MT metrics**: Why needed - Correlation with human judgments is the standard way to evaluate metric quality. Quick check - How is Kendall's τ used to measure the agreement between automatic metrics and human annotations?

## Architecture Onboarding

- **Component map**: Pre-trained LM (XLM-Roberta, NLLB) -> Fine-tuning head (feed-forward regressor for COMET) -> Input encoder (source, hypothesis, reference vectors) -> Algorithm-based wrappers (PRISM, BERTScore) -> Prompt-based LLMs (GEMBA, AUTOMQM)
- **Critical path**: 1. Load pre-trained model, 2. Prepare fine-tuning data (DA/MQM), 3. Train regression head (MSE loss), 4. Evaluate on target domain (bio), 5. Compare correlation to human judgments
- **Design tradeoffs**: Fine-tuning adds domain bias but may improve in-domain performance; pre-trained-only approaches retain generality but may miss fine-grained quality cues; algorithm-based metrics are faster but may lack sensitivity
- **Failure signatures**: Fine-tuned metrics show large drops in unseen domains; correlation plateaus early during fine-tuning on mismatched data; performance improves when fine-tuning on in-domain data
- **First 3 experiments**: 1. Train COMET on WMT DA+MQM, evaluate on bio; observe performance gap, 2. Train COMET on bio MQM, evaluate on bio; observe improvement, 3. Fine-tune XLM-Roberta on bio domain data, measure effect on BERTScore vs COMET

## Open Questions the Paper Calls Out

### Open Question 1
Does fine-tuning MT metrics on more diverse domains reduce the domain robustness gap? The authors state that "fine-tuning with in-domain biomedical data improves COMET performance" and suggest that "better generalization with more diverse training data" could help. This is unresolved because the paper only tests with biomedical domain data and doesn't explore training on multiple diverse domains simultaneously. Experiments showing COMET performance on biomedical domain after fine-tuning on a dataset containing biomedical plus other diverse domains would resolve this.

### Open Question 2
How does the size of the fine-tuning dataset affect domain robustness? The authors show that "just 1k judgements improves correlation to 0.313" when fine-tuning COMET on biomedical data, suggesting dataset size matters. This is unresolved because the paper only tests up to 6k biomedical judgments and doesn't explore the relationship between dataset size and domain robustness systematically. A comprehensive study testing COMET performance on biomedical domain with varying fine-tuning dataset sizes would resolve this.

### Open Question 3
Are there architectural modifications to fine-tuned metrics that could improve domain robustness? The authors rule out pre-trained model limitations as the cause of poor biomedical performance and show the gap persists throughout fine-tuning, suggesting the architecture itself might be a factor. This is unresolved because the paper only tests standard COMET architecture and doesn't explore modifications like domain adaptation layers or multi-task learning approaches. Experiments comparing standard COMET to variants with architectural modifications would resolve this.

## Limitations
- Findings are based on a single biomedical domain dataset, which may not fully represent the diversity of specialized domains (e.g., legal, technical)
- Evaluation focuses on segment-level Kendall's τ correlation, which may not capture all aspects of metric quality
- Fine-tuning procedures for different metrics are not fully specified, including hyperparameters and training data selection criteria

## Confidence

- **High Confidence**: Fine-tuned metrics show significantly lower correlation with human judgments in the biomedical domain compared to surface-form and pre-trained metrics
- **Medium Confidence**: Fine-tuning introduces domain-specific bias that doesn't generalize well to unseen domains
- **Low Confidence**: The performance gap is solely due to training data domain mismatch rather than architectural limitations

## Next Checks

1. **Domain Generalization Test**: Evaluate the same set of metrics on additional unseen domains (e.g., legal, technical) to determine if the performance gap is consistent across different domain shifts
2. **Fine-tuning Ablation Study**: Systematically vary fine-tuning hyperparameters (e.g., epochs, learning rate) and training data composition to isolate the impact of each factor on domain robustness
3. **Cross-Architecture Comparison**: Compare the performance of fine-tuned metrics with other adaptation strategies (e.g., domain-adaptive pre-training, multi-task learning) to assess whether fine-tuning is uniquely vulnerable to domain shifts