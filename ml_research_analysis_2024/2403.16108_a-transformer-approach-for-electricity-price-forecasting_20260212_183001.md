---
ver: rpa2
title: A Transformer approach for Electricity Price Forecasting
arxiv_id: '2403.16108'
source_url: https://arxiv.org/abs/2403.16108
tags:
- transformer
- forecasting
- electricity
- price
- adam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a pure Transformer model for electricity
  price forecasting, contrasting with previous approaches that combine Transformers
  with recurrent networks. The model processes past electricity prices using a Transformer
  encoder, enhanced by an embedding layer and positional encoding, and concatenates
  these features with exogenous variables processed through a separate embedding layer.
---

# A Transformer approach for Electricity Price Forecasting

## Quick Facts
- arXiv ID: 2403.16108
- Source URL: https://arxiv.org/abs/2403.16108
- Reference count: 40
- Introduces a pure Transformer model for electricity price forecasting, outperforming existing benchmarks on four of five open-access electricity markets.

## Executive Summary
This paper presents a novel pure Transformer model for day-ahead electricity price forecasting, departing from previous hybrid approaches that combine Transformers with recurrent networks. The model processes past electricity prices through a Transformer encoder enhanced by embedding and positional encoding, concatenates these features with exogenous variables, and uses an MLP to predict 24-hour prices. Evaluated on five open-access electricity markets using the EPF toolbox framework, the model achieves state-of-the-art performance, significantly outperforming both a naïve model and a DNN ensemble benchmark on four datasets. The results demonstrate the Transformer's capability to capture temporal patterns effectively without recurrent components.

## Method Summary
The method employs a pure Transformer encoder architecture to process sequences of past electricity prices, enhanced by embedding layers and positional encoding. Exogenous variables for the target day are processed through a separate embedding layer and concatenated with the Transformer's output for the most recent day. A final MLP predicts the 24-hour day-ahead prices. The model is trained on open-access electricity market datasets with hyperparameters tuned per market, and evaluated using MAE, RMSE, and sMAPE metrics. A renormalization step is applied to exogenous variables during testing without retraining.

## Key Results
- Achieved state-of-the-art performance on four of five open-access electricity markets.
- Significantly outperformed naïve and DNN ensemble benchmarks in MAE, RMSE, and sMAPE.
- Demonstrated that pure Transformer architecture can effectively capture temporal patterns in electricity prices without recurrent layers.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pure Transformer architecture captures temporal dependencies in electricity prices without recurrent layers.
- Mechanism: Self-attention layers allow the model to weigh the importance of past price values relative to each other, enabling the capture of long-range dependencies and complex temporal patterns.
- Core assumption: Attention mechanisms alone are sufficient to model the temporal dynamics of electricity price time series.
- Evidence anchors:
  - [abstract] "showing that the attention layer is enough for capturing the temporal patterns."
  - [section] "With this structure, the Transformer can capture temporal patterns and make a prediction focusing more attention on longer trends than other temporal models (e.g. LSTMs)."
  - [corpus] Weak. Neighboring papers discuss LSTM+Attention hybrids or alternative DL approaches but do not directly validate pure Transformer efficacy for EPF.
- Break condition: If attention weights converge to uniform distributions or fail to adapt to regime shifts in the price series.

### Mechanism 2
- Claim: Concatenating exogenous variables processed via embedding with Transformer-encoded price history preserves feature interactions.
- Mechanism: Exogenous variables (e.g., demand, renewable generation) are embedded and concatenated with the Transformer's output for the most recent day, allowing the MLP to learn joint representations for prediction.
- Core assumption: Exogenous variables are stationary enough during the test period that renormalization without retraining suffices.
- Evidence anchors:
  - [section] "the results of each flow are concatenated and served as an input to a multivariate Multilayer Perceptron for predicting day ahead prices."
  - [section] "a renormalization step was applied consisting on subtracting the mean and standard deviation from the data previous to the next day to predict."
  - [corpus] Missing. No neighboring papers explicitly discuss renormalization strategies for exogenous features in EPF.
- Break condition: If exogenous variables undergo abrupt structural changes that are not captured by mean/std normalization.

### Mechanism 3
- Claim: The embedding layer transforms raw price sequences into higher-dimensional representations that improve pattern learning.
- Mechanism: Feedforward + ReLU embeddings map each day's 24-hour vector to a richer n-dimensional space before positional encoding and attention, enhancing the Transformer's ability to distinguish subtle temporal patterns.
- Core assumption: Increasing dimensionality (n) improves the model's expressiveness without overfitting.
- Evidence anchors:
  - [section] "These layers transform each 24-valued vector of one day into a higher-order vector."
  - [section] "The two Embedding layers defined for preprocessing the inputs are projections into a higher dimensional space."
  - [corpus] Weak. No neighboring papers analyze embedding dimension effects for EPF.
- Break condition: If embedding size is too large relative to dataset size, causing overfitting and degraded generalization.

## Foundational Learning

- Concept: Self-attention mechanism
  - Why needed here: Enables the model to focus on relevant past price points without recurrence, crucial for capturing long-term dependencies in volatile electricity prices.
  - Quick check question: In a self-attention layer, how does the model determine which past time steps are most relevant for predicting the next day's prices?

- Concept: Positional encoding
  - Why needed here: Transformers lack inherent sequential ordering; positional encodings inject temporal order information so the model can distinguish earlier from later prices.
  - Quick check question: What happens to the model's predictions if you remove positional encoding but keep the same attention weights?

- Concept: Hyperparameter tuning and validation strategy
  - Why needed here: Optimal architecture (embedding size, number of heads, layers, etc.) varies by market; validation ensures robustness without relying solely on test performance.
  - Quick check question: Why does the paper use the last 42 weeks of training data for validation instead of a random split?

## Architecture Onboarding

- Component map: Exogenous variables → Embedding → Output A; Past prices → Embedding → Positional Encoding → Transformer → Output B; Concatenate A and B → MLP → 24-dimensional price forecast
- Critical path: Embedding → Positional Encoding → Transformer → Concatenate → MLP
- Design tradeoffs:
  - Larger embedding size (n) increases expressiveness but risks overfitting.
  - More Transformer layers improve pattern capture but increase training time and gradient issues.
  - Longer historical sequence (m) captures broader trends but adds noise and computational cost.
- Failure signatures:
  - Over-smoothing in attention weights → poor capture of regime shifts.
  - Large gap between validation and test performance → overfitting.
  - Consistently high error on specific hours → missing hour-specific patterns.
- First 3 experiments:
  1. Baseline: Train with n=128, NL=4, FD=512, m=14 days; evaluate MAE on validation set.
  2. Attention scaling: Increase NH from 4 to 8; observe change in attention distribution and validation MAE.
  3. Historical length: Vary m between 7 and 21 days; plot validation MAE vs. sequence length to identify optimal trade-off.

## Open Questions the Paper Calls Out

- Open Question 1: How does the pure Transformer architecture compare to LSTM-based models for electricity price forecasting in terms of accuracy and computational efficiency?
  - Basis in paper: [inferred] The paper mentions that future work could involve comparing LSTMs and Transformers, and that the presented model is not an ensemble.
  - Why unresolved: The paper does not provide a direct comparison between the pure Transformer and LSTM-based models, and the potential benefits of ensemble methods are not explored.
  - What evidence would resolve it: Conducting experiments comparing the pure Transformer model to LSTM-based models and Transformer ensembles on the same datasets and metrics.

- Open Question 2: How does the inclusion of past exogenous variables in the Transformer architecture affect the model's performance in electricity price forecasting?
  - Basis in paper: [inferred] The paper suggests that future work could involve exploring how to incorporate past exogenous variables into the Transformer architecture.
  - Why unresolved: The current model only uses past prices and current exogenous variables, leaving the potential impact of historical exogenous variables unexplored.
  - What evidence would resolve it: Modifying the model to include past exogenous variables and evaluating its performance against the current model on the same datasets and metrics.

- Open Question 3: What is the optimal sequence length (number of past days) for the Transformer model in electricity price forecasting across different markets?
  - Basis in paper: [explicit] The paper mentions that the sequence length is a hyperparameter that should be optimized, and different values are tested in the validation results.
  - Why unresolved: The paper does not provide a definitive answer on the optimal sequence length, as it varies across different datasets and configurations.
  - What evidence would resolve it: Conducting a comprehensive sensitivity analysis on the sequence length parameter across all datasets and reporting the optimal values for each market.

## Limitations

- Performance degradation in one of five markets suggests model may not generalize uniformly across all electricity markets.
- Reliance on a specific data normalization strategy without retraining raises questions about model robustness under distributional shifts.
- Lack of ablation studies to isolate the impact of the pure Transformer architecture versus alternative designs.

## Confidence

- High confidence in the architectural description and general methodology.
- Medium confidence in the claim of state-of-the-art performance, as results depend on hyperparameter choices and dataset-specific tuning.
- Low confidence in the generalizability of the model without further validation on out-of-distribution data or during extreme market events.

## Next Checks

1. Perform ablation studies comparing the pure Transformer against hybrid models (e.g., LSTM+Attention) on each market to quantify the benefit of the pure Transformer design.
2. Test model robustness by retraining with exogenous variables from a different year to simulate distributional shift.
3. Conduct an analysis of attention weight distributions to verify they capture meaningful temporal patterns rather than converging to uniform distributions.