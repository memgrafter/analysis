---
ver: rpa2
title: Cyclic Neural Network
arxiv_id: '2402.03332'
source_url: https://arxiv.org/abs/2402.03332
tags:
- graph
- neurons
- neuron
- neural
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cyclic Neural Networks (Cyclic NNs), a novel
  ANN architecture that allows neuron connections in any graph-like structure, including
  cycles. Unlike current ANNs that are built layer-by-layer to ensure a Directed Acyclic
  Graph (DAG) property, Cyclic NNs are more biologically plausible, resembling the
  flexible and dynamic graph nature of biological neural systems.
---

# Cyclic Neural Network

## Quick Facts
- arXiv ID: 2402.03332
- Source URL: https://arxiv.org/abs/2402.03332
- Authors: Liangwei Yang; Hengrui Zhang; Zihe Song; Jiawei Zhang; Weizhi Zhang; Jing Ma; Philip S. Yu
- Reference count: 19
- Key outcome: Introduces Cyclic Neural Networks (Cyclic NNs) with arbitrary graph structures and demonstrates superiority over backpropagation on standard benchmarks using forward-forward training

## Executive Summary
This paper presents Cyclic Neural Networks (Cyclic NNs), a novel neural network architecture that breaks from the traditional layer-by-layer design by allowing neuron connections in arbitrary graph-like structures, including cycles. Unlike current artificial neural networks that must be Directed Acyclic Graphs (DAGs) for backpropagation, Cyclic NNs are more biologically plausible, resembling the flexible and dynamic graph nature of biological neural systems. The authors develop the Graph Over Multi-layer Perceptron (GOMLP) as the first detailed model based on this new design paradigm, and demonstrate its superiority over backpropagation training methods through the use of a forward-forward (FF) training algorithm on widely tested datasets.

## Method Summary
The paper introduces GOMLP, a neural network architecture built by creating graph structures over multi-layer perceptrons, allowing arbitrary connections including cycles. The architecture consists of computational neurons that receive inputs from pre-synapse neurons, process them using local loss functions, and update parameters independently without global backpropagation. The forward-forward (FF) training algorithm is used, where each neuron optimizes locally by comparing positive and negative samples using goodness functions. The model is tested on MNIST, NewsGroup, and IMDB datasets, with performance compared against traditional backpropagation methods.

## Key Results
- Cyclic NNs with various graph structures (cycle, complete, WS, BA) outperform traditional backpropagation methods on MNIST, NewsGroup, and IMDB datasets
- Local optimization through forward-forward training provides greater parallelism and reduces communication overhead compared to global backpropagation
- The graph structure over multi-layer perceptrons enables richer information flow and more complex representations than sequential architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The local optimization approach enables computational neurons to adapt independently without waiting for global gradient propagation, improving parallelism and reducing communication overhead.
- Mechanism: Each neuron receives inputs from its pre-synapse neurons, processes them locally using its own loss function, and updates its parameters without backpropagating errors through the entire network. This allows multiple neurons to be updated simultaneously.
- Core assumption: Local loss functions can effectively guide each neuron to contribute meaningfully to the overall task without global coordination.
- Evidence anchors:
  - [abstract] "The proposed Cyclic NNs offer greater adaptability, parallelism, and privacy compared to current ANNs"
  - [section] "Each computational neuron can be optimized immediately when the data comes without the need to wait for the gradient to propagate back"
  - [corpus] Weak evidence - corpus neighbors focus on biological inspiration but don't directly address local optimization benefits
- Break condition: If local loss functions cannot coordinate neuron behavior effectively, the network may fail to learn coherent representations, leading to poor overall performance.

### Mechanism 2
- Claim: The graph structure over multi-layer perceptrons (GOMLP) allows for richer information flow between layers compared to traditional sequential architectures.
- Mechanism: By connecting neurons in arbitrary graph structures (cycles, complete graphs, etc.), information can flow through multiple paths simultaneously, enabling more complex representations and potentially avoiding information bottlenecks.
- Core assumption: More complex graph structures can capture dependencies and relationships that sequential layers cannot.
- Evidence anchors:
  - [abstract] "allowing neuron connections in any graph-like structure, including cycles"
  - [section] "GOMLP is designed by building a graph structure over the multi-layer perception"
  - [corpus] Weak evidence - corpus neighbors mention graph structures but focus on different applications
- Break condition: If the graph structure becomes too complex, it may lead to overparameterization or make optimization difficult, potentially degrading performance.

### Mechanism 3
- Claim: The forward-forward (FF) training algorithm provides localized training that is more biologically plausible than backpropagation while maintaining or improving performance.
- Mechanism: Instead of global backpropagation, FF uses local goodness functions to compare positive and negative samples at each neuron, mimicking how biological neurons might learn from local context.
- Core assumption: Local goodness functions can effectively guide learning without global error signals.
- Evidence anchors:
  - [abstract] "demonstrating its superiority over current BP training methods through the use of a forward-forward (FF) training algorithm"
  - [section] "Similar to Hebb's Rule and STDP learning, the FF algorithm is a localized learning method"
  - [corpus] Weak evidence - corpus neighbors discuss biological inspiration but don't specifically address FF algorithm
- Break condition: If the FF algorithm cannot effectively propagate useful information through the network, learning may stall or converge to suboptimal solutions.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs) in neural networks
  - Why needed here: Understanding why traditional ANNs are constrained to DAG structures helps explain the novelty of allowing cycles in Cyclic NNs
  - Quick check question: What property of backpropagation requires neural networks to be DAGs?

- Concept: Hebbian learning and spike-timing-dependent plasticity (STDP)
  - Why needed here: These biological learning rules inspire the localized training approach used in Cyclic NNs
  - Quick check question: How does Hebb's rule ("neurons that fire together wire together") relate to the local optimization in Cyclic NNs?

- Concept: Graph neural networks and message passing
  - Why needed here: Understanding how information flows through graph structures helps in grasping the GOMLP architecture
  - Quick check question: How does information propagation differ between traditional MLPs and graph-structured networks?

## Architecture Onboarding

- Component map: Input → Computational neurons (with local processing) → Synapses (information flow) → Readout layer (aggregation) → Output

- Critical path: Input → Computational neurons (with local processing) → Synapses (information flow) → Readout layer (aggregation) → Output

- Design tradeoffs:
  - Flexibility vs. stability: More complex graph structures offer greater expressiveness but may be harder to train
  - Local vs. global optimization: Local optimization improves parallelism but may sacrifice some coordination
  - Biological plausibility vs. engineering practicality: The design is inspired by biology but must still be computationally efficient

- Failure signatures:
  - Over-smoothing: Too much information propagation leads to neurons producing similar outputs
  - Local minima: Neurons may get stuck in suboptimal solutions without global coordination
  - Gradient vanishing/exploding: Despite local optimization, some neurons may fail to learn effectively

- First 3 experiments:
  1. Implement a simple cycle graph with 3-5 neurons and test on MNIST to verify basic functionality
  2. Compare performance of different graph structures (chain, cycle, complete) on a small dataset
  3. Test sensitivity to the goodness threshold parameter (θ) to understand its impact on learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance and computational efficiency of Cyclic Neural Networks compare to traditional ANN architectures when scaled to larger datasets and more complex tasks?
- Basis in paper: [explicit] The paper demonstrates the superiority of Cyclic NNs over current backpropagation methods using a forward-forward training algorithm on widely tested datasets, but does not explore scalability.
- Why unresolved: The experiments conducted are on widely tested datasets, but the scalability of Cyclic NNs to larger, more complex datasets is not addressed.
- What evidence would resolve it: Performance and efficiency metrics of Cyclic NNs on larger datasets and more complex tasks compared to traditional ANNs.

### Open Question 2
- Question: What are the specific advantages and potential drawbacks of using different graph structures (e.g., cycle, complete, WS, BA) in Cyclic NNs for various types of data and tasks?
- Basis in paper: [explicit] The paper tests multiple graph generators to build the graph in GOMLP, including chain, cycle, complete, Watts-Strogatz, and Barabási-Albert graphs, but does not provide a comprehensive analysis of their advantages and drawbacks.
- Why unresolved: While the paper demonstrates the effectiveness of different graph structures, it does not provide a detailed analysis of their advantages and drawbacks for different types of data and tasks.
- What evidence would resolve it: A comprehensive study comparing the performance of Cyclic NNs with different graph structures on various types of data and tasks.

### Open Question 3
- Question: How does the local optimization in Cyclic NNs affect the model's ability to learn and generalize compared to global optimization methods used in traditional ANNs?
- Basis in paper: [explicit] The paper highlights the use of local optimization in Cyclic NNs, which is different from the global optimization methods used in traditional ANNs, but does not provide a detailed comparison of their effects on learning and generalization.
- Why unresolved: The paper introduces the concept of local optimization but does not explore its effects on the model's learning and generalization capabilities compared to global optimization methods.
- What evidence would resolve it: A comparative study of the learning and generalization capabilities of Cyclic NNs with local optimization versus traditional ANNs with global optimization on various tasks and datasets.

## Limitations

- The paper provides limited empirical validation of whether local goodness functions are sufficient for global task performance, lacking ablation studies to isolate the contributions of graph structure versus local optimization
- Biological plausibility claims are compelling but remain largely theoretical without direct experimental evidence linking the architecture to biological neural systems
- Computational complexity of maintaining arbitrary graph structures versus traditional layer-based networks is not thoroughly analyzed

## Confidence

- High confidence: The architectural framework (GOMLP) is clearly defined and implementable
- Medium confidence: Local optimization benefits are theoretically sound but need broader empirical validation
- Low confidence: Biological plausibility claims exceed current experimental evidence

## Next Checks

1. Implement ablation studies comparing performance with and without cycles in the graph structure to isolate the contribution of cyclic connections
2. Test scalability by evaluating on larger, more complex datasets (ImageNet, COCO) to verify generalization beyond benchmark datasets
3. Analyze training dynamics through visualization of neuron activations and goodness scores to verify that local optimization effectively coordinates network behavior