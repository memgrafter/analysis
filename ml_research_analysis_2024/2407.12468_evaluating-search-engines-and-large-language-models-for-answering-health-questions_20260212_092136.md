---
ver: rpa2
title: Evaluating Search Engines and Large Language Models for Answering Health Questions
arxiv_id: '2407.12468'
source_url: https://arxiv.org/abs/2407.12468
tags:
- llms
- health
- search
- questions
- trec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the effectiveness of four search engines, seven
  large language models (LLMs), and retrieval-augmented generation (RAG) approaches
  in answering 150 health-related questions from the TREC Health Misinformation Track.
  Search engines correctly answered 50-70% of questions, while LLMs achieved about
  80% accuracy, though their performance varied with prompt design.
---

# Evaluating Search Engines and Large Language Models for Answering Health Questions

## Quick Facts
- arXiv ID: 2407.12468
- Source URL: https://arxiv.org/abs/2407.12468
- Authors: Marcos Fernández-Pichel; Juan C. Pichel; David E. Losada
- Reference count: 40
- Primary result: Search engines correctly answered 50-70% of health questions, while LLMs achieved about 80% accuracy, with RAG approaches improving smaller LLMs by up to 30%

## Executive Summary
This study compares the effectiveness of four search engines, seven large language models (LLMs), and retrieval-augmented generation (RAG) approaches in answering 150 health-related questions from the TREC Health Misinformation Track. Search engines correctly answered 50-70% of questions, while LLMs achieved about 80% accuracy, though their performance varied with prompt design. RAG methods significantly enhanced smaller LLMs, improving accuracy by up to 30% when integrating retrieval evidence. The study found that search engine results often did not directly answer health questions, and user behavior models showed that inspecting only the first relevant result was not inferior to more thorough approaches. These findings highlight the potential of combining search and language models for accurate health information retrieval.

## Method Summary
The study evaluates four search engines (Google, Bing, Yahoo, DuckDuckGo), seven LLMs (GPT-3 d-002, GPT-3 d-003, ChatGPT, GPT-4, FlanT5, Llama3, MedLlama3), and RAG approaches using 150 binary health questions from TREC Health Misinformation Track 2020-2022. For search engines, top 20 results were scraped, passages extracted with MonoT5, and GPT-3 used for reading comprehension to determine yes/no answers. LLMs were tested with zero-shot and few-shot experiments using three prompting strategies ("no context", "non-expert", "expert"). RAG experiments augmented LLMs with Google search result passages under both prompting strategies.

## Key Results
- Search engines correctly answered 50-70% of health questions through direct extraction
- LLMs achieved approximately 80% accuracy with expert prompting
- RAG methods improved smaller LLMs by up to 30% when integrating retrieval evidence
- Inspecting only the first relevant result was not inferior to more thorough inspection approaches
- Performance varied significantly with prompt design and number of in-context examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Search engines retrieve relevant results but many top results do not directly answer the binary health question.
- Mechanism: The retrieval process surfaces many topically relevant pages, but the content may not contain a clear yes/no answer to the specific question. This leads to high recall but lower precision in terms of actual answers.
- Core assumption: Topical relevance ≠ answer presence. A page can be about the topic but not provide a definitive answer.
- Evidence anchors:
  - By focusing on the retrieved pages that provide an unequivocal answer to the reference health questions, we obtained much higher precision.
  - This suggests that the problem is not so much with LLMs as with the questions themselves, as search engines also return incorrect information in the top results for these difficult questions.

### Mechanism 2
- Claim: LLMs outperform search engines in accuracy because they can synthesize answers from training data rather than extract them from individual documents.
- Mechanism: LLMs leverage their large training corpora and reasoning capabilities to generate answers, bypassing the need to find a single document with the answer. This allows them to answer questions even when no single retrieved page provides a clear answer.
- Core assumption: The training data contains sufficient medical knowledge to answer these questions directly.
- Evidence anchors:
  - Our results suggest that, in general, the most capable LLMs generate better answers compared with those extracted from top webpages ranked by SEs.
  - It seems that the extensive training data of the LLMs, coupled with their superior reasoning abilities, offer significant advantages over extracting responses from a handful of top-ranked search results.

### Mechanism 3
- Claim: Retrieval-augmented generation improves smaller LLMs by providing relevant evidence that compensates for their limited knowledge.
- Mechanism: By injecting passages from search engine results into the LLM prompt, smaller models gain access to specific evidence they might lack internally, allowing them to generate more accurate answers.
- Core assumption: The injected passages contain correct information relevant to the question.
- Evidence anchors:
  - We demonstrated that smaller LLMs reach the level of superior models when grounded in evidence provided by a search engine.
  - This opens up the debate on whether it is worthwhile to persist in generating massive and computationally demanding models, or alternatively, we can direct our efforts towards leveraging lighter models enriched with search evidence.

## Foundational Learning

- Concept: Binary question answering evaluation
  - Why needed here: The study measures effectiveness by comparing yes/no answers to ground truth, requiring understanding of precision/recall metrics for binary classification.
  - Quick check question: If a system answers 80 questions correctly out of 100, what is its precision?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The study compares baseline LLMs to RAG variants that incorporate search results, requiring understanding of how retrieved evidence is injected and used.
  - Quick check question: In RAG, where does the retrieved evidence typically get injected in the LLM's processing pipeline?

- Concept: User behavior modeling in information retrieval
  - Why needed here: The study simulates lazy vs diligent users to understand real-world effectiveness, requiring understanding of how inspection depth affects decision quality.
  - Quick check question: What is the key difference between lazy and diligent user models in this study?

## Architecture Onboarding

- Component map:
  - Question ingestion → Search engine API → Document retrieval → Passage extraction → Answer extraction (for SEs)
  - Question ingestion → LLM API → Answer generation (for LLMs)
  - Question ingestion → Search engine API → Passage extraction → LLM API (for RAG)

- Critical path: For RAG systems, the critical path is: question → search → passage extraction → prompt construction → LLM inference → answer extraction

- Design tradeoffs:
  - Search engine choice: Bing performed best but differences weren't statistically significant
  - Prompt engineering: "Expert" prompts improved accuracy but may not reflect real user behavior
  - Number of in-context examples: One example was sufficient; more didn't consistently improve performance
  - Number of injected passages: Three passages provided good balance; more could introduce noise

- Failure signatures:
  - Low precision at rank 1 suggests retrieval isn't finding answer-bearing documents
  - High variance across prompts suggests model sensitivity to input formatting
  - Performance drops with incorrect injected passages indicate RAG system fragility

- First 3 experiments:
  1. Run the same 150 questions through Google, Bing, Yahoo, and DuckDuckGo to establish baseline SE performance
  2. Test GPT-4, ChatGPT, and Llama3 with "no context" and "expert" prompts to establish baseline LLM performance
  3. Implement RAG with Google top-3 passages injected into the best-performing LLM to measure performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs vary across different health domains and question complexities?
- Basis in paper: The paper mentions that the difficulty levels of the three datasets vary, with the 2020 health questions centered on COVID-19 being easier for LLMs.
- Why unresolved: The paper does not provide a detailed analysis of how LLM performance varies across different health domains and question complexities.
- What evidence would resolve it: A comprehensive study evaluating LLM performance across a wide range of health domains and question complexities would be needed.

### Open Question 2
- Question: What is the impact of user personalization factors, such as geolocalization and user preferences, on the effectiveness of LLMs and search engines in answering health questions?
- Basis in paper: The paper mentions that retrieval results are known to be dependent on multiple user factors, such as geolocalization or user preferences.
- Why unresolved: The paper does not consider the effect of personalization in its experiments.
- What evidence would resolve it: Experiments comparing the performance of LLMs and search engines with and without personalization factors would be needed.

### Open Question 3
- Question: How does the readability and understandability of LLM responses and search engine results affect user experience and health-related decisions?
- Basis in paper: The paper mentions that Yan et al. argued that search results should be re-ranked by descending readability and that Zuccon and Koopman proposed an innovative method that integrates the notion of understandability into the evaluation of retrieval systems for consumer health search.
- Why unresolved: The paper does not specifically analyze the understandability or readability of search results or LLM completions.
- What evidence would resolve it: A study evaluating the readability and understandability of LLM responses and search engine results and their impact on user experience and health-related decisions would be needed.

## Limitations
- Study relies on binary health questions from TREC Health Misinformation Track, which may not represent full complexity of real-world health information needs
- Search engine results were scraped from Singapore IPs, potentially introducing geographic bias in retrieved content
- Limited evaluation of hallucination rates in LLM-generated answers, focusing primarily on accuracy against ground truth

## Confidence

- **High confidence**: Search engines correctly answered 50-70% of questions (measured through direct extraction from retrieved documents)
- **High confidence**: LLMs achieved approximately 80% accuracy with expert prompting (consistent across multiple models)
- **Medium confidence**: RAG methods improved smaller LLMs by up to 30% (based on limited comparison between select model pairs)
- **Medium confidence**: User behavior models showing inspection of first relevant result is not inferior (simulation-based, not user study validated)

## Next Checks

1. Conduct a human evaluation study to validate the user behavior model findings, comparing actual user performance when inspecting only the first relevant result versus more thorough approaches
2. Measure and report hallucination rates for LLM-generated answers, particularly for questions where no clear ground truth exists in the TREC dataset
3. Test the RAG approach with a broader range of passage injection strategies (varying number of passages, different ranking methods) to identify optimal configurations for different LLM sizes