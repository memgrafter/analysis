---
ver: rpa2
title: The Systems Engineering Approach in Times of Large Language Models
arxiv_id: '2411.09050'
source_url: https://arxiv.org/abs/2411.09050
tags:
- systems
- engineering
- system
- llms
- principles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies challenges in integrating Large Language
  Models (LLMs) into socio-technical systems, including alignment, interpretability,
  sustainability, and security. The authors survey 24 research papers that apply systems
  engineering principles to address similar challenges in AI-based systems.
---

# The Systems Engineering Approach in Times of Large Language Models

## Quick Facts
- arXiv ID: 2411.09050
- Source URL: https://arxiv.org/abs/2411.09050
- Authors: Christian Cabrera; Viviana Bastidas; Jennifer Schooling; Neil D. Lawrence
- Reference count: 29
- Primary result: Systems engineering principles can address LLM integration challenges, with Top-Down, Systems Views, and Problem-Solving Cycle principles most effective for alignment and reliability

## Executive Summary
This paper examines how systems engineering principles can address challenges in integrating Large Language Models into socio-technical systems. The authors survey 24 research papers that apply systems engineering to AI-based systems, identifying key principles and architectural patterns for managing LLM challenges including alignment, interpretability, sustainability, and security. The research highlights the Systems Views, Top-Down, and Problem-Solving Cycle principles as most effective for addressing these challenges, while identifying critical open research directions including inclusive requirements definition and organizational culture change.

## Method Summary
The authors conducted a literature survey using a semi-automated semantic filtering tool to identify 24 research papers from 2017-2024 that apply systems engineering principles to AI-based systems challenges. Papers were selected from academic databases using a specific search query and manually reviewed for relevance. The analysis focused on identifying which systems engineering principles were applied and how they addressed specific challenges in AI-based systems, with the goal of informing future approaches to LLM integration.

## Key Results
- Systems Views, Top-Down, and Problem-Solving Cycle principles are most commonly used for alignment and reliability challenges
- Validation architectural patterns (human-based, policy-based, model-based) can prevent security and privacy violations
- Iterative development with continuous testing addresses LLM reliability and alignment issues
- Top-down decomposition bridges the gap between high-level requirements and LLM-based implementations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Systems engineering principles can bridge the gap between high-level requirements and LLM-based system implementations.
- Mechanism: The top-down principle decomposes complex requirements into actionable components, while the Systems Views principle ensures all stakeholder perspectives are considered during design.
- Core assumption: LLM outputs can be integrated into systems if their uncertainty and interpretability issues are explicitly managed through systematic decomposition.
- Evidence anchors:
  - [section] "The top-down principle decomposes the problem and the solution to guarantee that the latter addresses the former. Meyer and Gruhn (2019) uses this principle to align RL solutions with high-level requirements."
  - [section] "The System Views principle defines systems from different perspectives and facilitates the design of reliable systems that align with their intents."

### Mechanism 2
- Claim: Problem-solving cycles with continuous testing and verification can address LLM reliability and alignment issues in safety-critical systems.
- Mechanism: Iterative development processes that include checkpoints for data integrity, model robustness, and system quality can catch misalignment early and enable course correction.
- Core assumption: Regular validation points throughout the system lifecycle can detect when LLM outputs deviate from requirements before causing system failures.
- Evidence anchors:
  - [section] "Zeller et al. (2024) introduce a workflow for continuous deployment and safety assurance of ML-based systems. This process includes testing and verification steps along the systems' lifecycle, considering three viewpoints to provide solutions that operate according to safety functional requirements."
  - [section] "Systems alignment and reliability rely on rich specifications at inception, rigorous testing at development, and auditing mechanisms and educational efforts at deployment."

### Mechanism 3
- Claim: Architectural patterns that explicitly validate LLM outputs can prevent security and privacy violations in sensitive applications.
- Mechanism: Implementing validation layers that check LLM outputs against predefined policies or human review before integration into critical systems.
- Core assumption: Even if LLM outputs are uncertain or potentially harmful, validation mechanisms can catch problematic content before system integration.
- Evidence anchors:
  - [section] "The architectural patterns propose three alternatives (i.e., the Variant Creation principle) to implement such validation: human-based, policy-based, or model-based."
  - [section] "Cai (2020) proposes a checklist for analysing AI systems safety from different perspectives. The paper proposes decomposing autonomous systems into parts to apply the safety checklist."

## Foundational Learning

- Concept: Systems engineering lifecycle phases
  - Why needed here: Understanding how requirements flow through design, development, deployment, and maintenance phases is crucial for integrating LLMs appropriately at each stage
  - Quick check question: What are the four main phases of systems engineering and what LLM-specific considerations apply to each?

- Concept: Systems views and stakeholder perspectives
  - Why needed here: LLMs affect multiple stakeholders differently; understanding diverse viewpoints ensures comprehensive requirement gathering and validation
  - Quick check question: How do governance audits, model audits, and application audits differ in their focus on LLM systems?

- Concept: Top-down vs bottom-up design approaches
  - Why needed here: The paper emphasizes top-down decomposition for aligning LLM capabilities with system requirements rather than building systems around available LLM features
  - Quick check question: What is the key difference between top-down and bottom-up approaches when integrating LLMs into existing systems?

## Architecture Onboarding

- Component map:
  - Requirement decomposition layer -> LLM integration layer -> Validation and verification layer -> Monitoring and adaptation layer

- Critical path:
  1. Requirements gathering and decomposition
  2. System architecture design with LLM components
  3. Implementation with continuous verification
  4. Deployment with monitoring infrastructure
  5. Maintenance and adaptation cycles

- Design tradeoffs:
  - Performance vs interpretability: More interpretable models may sacrifice some performance
  - Centralization vs decentralization: Cloud-based LLMs offer more power but raise privacy concerns
  - Validation overhead vs deployment speed: Rigorous validation slows deployment but increases safety

- Failure signatures:
  - Requirement misalignment: System meets technical specs but fails to satisfy user needs
  - Data drift: Model performance degrades as input data characteristics change
  - Validation bypass: Security checks are circumvented due to performance pressures

- First 3 experiments:
  1. Implement a simple requirement decomposition workflow for a small LLM-based system and measure alignment between decomposed requirements and final implementation
  2. Build a validation layer that checks LLM outputs against predefined policies and measure false positive/negative rates
  3. Create a monitoring dashboard that tracks key performance indicators and data drift metrics in a deployed LLM system

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can public engagement activities be effectively incorporated into systems engineering methodologies to define inclusive requirements for socio-technical systems?
- Basis in paper: [explicit] The paper identifies this as an open research direction, noting that "Systems engineering methodologies must consider public engagement activities to get an inclusive problem definition."
- Why unresolved: Public engagement is complex due to diverse stakeholders with different communication styles and interests. Existing systems engineering methods work well within single organizations but struggle with broader public involvement.
- What evidence would resolve it: Development and validation of frameworks that successfully integrate public engagement activities into systems engineering processes, demonstrated through case studies in diverse socio-technical domains.

### Open Question 2
- Question: What approaches can bridge the gap between high-level conceptual requirements (like sustainability and fairness) and their technical implementation in AI-based socio-technical systems?
- Basis in paper: [explicit] The paper identifies this as an open research direction, stating there is "a gap between high-level concepts like sustainability, robustness, and truthfulness and their technical implementations."
- Why unresolved: Current reductionist techniques fail to capture real-world complexity, and systems engineering lacks dynamic tools for managing interactions between multiple actors, subsystems, and data sources.
- What evidence would resolve it: Creation of validated methods that successfully translate high-level socio-technical requirements into implementable technical specifications, demonstrated through successful deployment in real-world systems.

### Open Question 3
- Question: How can organizations successfully shift from agile methodologies to approaches that prioritize planning phases when developing AI-based systems with components that designers don't fully understand?
- Basis in paper: [explicit] The paper identifies this as an open research direction, noting that organizations need "a shift towards methodologies that balance or even prioritise planning phases against production phases."
- Why unresolved: Current organizational cultures prioritize working software as the measure of progress, but AI components create new safety-critical considerations that require more rigorous planning and testing.
- What evidence would resolve it: Case studies of organizations that have successfully implemented such methodological shifts, with metrics showing improved safety and reliability outcomes in AI-based systems.

## Limitations
- Small sample size (24 papers) may not capture full range of systems engineering applications to LLMs
- Focus on academic literature may underrepresent industry practices and emerging operational approaches
- Analysis captures documented applications but cannot fully account for real-world implementation challenges
- Search methodology may have missed relevant papers using different terminology or appearing in non-indexed venues

## Confidence
- Claim: Systems engineering principles can effectively address LLM integration challenges -> Medium
- Claim: Architectural patterns for LLM validation and monitoring are effective -> Low-Medium
- Claim: Identified open research directions are valid and important -> High

## Next Checks
1. **Implementation Study**: Conduct a case study implementing the identified systems engineering principles (Top-Down decomposition, Systems Views) in an actual LLM integration project, measuring requirement alignment and system performance against baseline approaches.

2. **Validation Efficacy Testing**: Build and evaluate all three proposed validation architectural patterns (human-based, policy-based, model-based) in a controlled environment, measuring false positive/negative rates, processing latency, and scalability under realistic workloads.

3. **Stakeholder Feedback Collection**: Interview practitioners from different domains (healthcare, finance, government) who have attempted LLM integration to assess whether the identified principles and architectural patterns match their real-world experiences and challenges.