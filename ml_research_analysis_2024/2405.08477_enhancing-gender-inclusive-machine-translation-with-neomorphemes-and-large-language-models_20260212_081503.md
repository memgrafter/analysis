---
ver: rpa2
title: Enhancing Gender-Inclusive Machine Translation with Neomorphemes and Large
  Language Models
arxiv_id: '2405.08477'
source_url: https://arxiv.org/abs/2405.08477
tags:
- gender
- translation
- which
- neomorphemes
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of neomorphemes in machine translation
  to promote gender inclusivity, particularly for non-binary individuals. The authors
  release NEO-GATE, a benchmark for evaluating gender-inclusive English-to-Italian
  translation using neomorphemes, and explore prompting techniques with large language
  models (LLMs) to generate translations incorporating these neologistic elements.
---

# Enhancing Gender-Inclusive Machine Translation with Neomorphemes and Large Language Models

## Quick Facts
- **arXiv ID**: 2405.08477
- **Source URL**: https://arxiv.org/abs/2405.08477
- **Reference count**: 27
- **Primary result**: GPT-4 and Mixtral show promising performance for gender-inclusive MT with neomorphemes when properly prompted

## Executive Summary
This paper investigates the use of neomorphemes in machine translation to promote gender inclusivity, particularly for non-binary individuals. The authors release NEO-GATE, a benchmark for evaluating gender-inclusive English-to-Italian translation using neomorphemes, and explore prompting techniques with large language models (LLMs) to generate translations incorporating these neologistic elements. They assess four LLMs of different families and sizes, along with two popular Italian neomorpheme paradigms (Asterisk and Schwa), using various prompt formats. The results show that GPT-4 and Mixtral generally exhibit promising performance when properly prompted, while Llama 2 and Tower are less suitable for this task.

## Method Summary
The study evaluates four LLMs (GPT-4, Mixtral-8x7B, Llama 2 70B, TowerInstruct-7B) using zero-shot and few-shot prompting with three formats (Direct, Binary, Ternary) on the NEO-GATE benchmark containing 841 English-Italian sentence pairs. The evaluation uses four metrics: Coverage (proportion of gender-marked words correctly generated), Accuracy (proportion of neomorphemes correctly generated among matched words), Coverage-weighted accuracy (product of coverage and accuracy), and Mis-generation (inappropriate neomorpheme usage). The authors test 1, 4, and 8 demonstration examples for few-shot prompting and compare performance across two neomorpheme paradigms (Asterisk and Schwa).

## Key Results
- GPT-4 and Mixtral demonstrate superior performance for gender-inclusive MT with neomorphemes compared to Llama 2 and Tower
- Model performance is highly sensitive to prompt formatting, with angle-bracket notation causing issues for Llama 2
- Current models struggle with accurate neomorpheme placement, showing non-trivial mis-generation rates across all models

## Why This Works (Mechanism)
Large language models can leverage their understanding of morphological patterns and contextual cues to generate appropriate neomorphemes when properly prompted. The models' ability to recognize gender-neutral references and apply the correct neomorpheme paradigm depends on both their pre-training data exposure and the specificity of the prompting strategy.

## Foundational Learning
- **Neomorphemes**: Novel linguistic elements created to express non-binary gender identities in morphologically rich languages - needed to understand the core innovation being evaluated
- **Zero-shot vs few-shot prompting**: Techniques for guiding LLM outputs without (zero-shot) or with (few-shot) demonstration examples - needed to understand the experimental design
- **Coverage vs Accuracy metrics**: Distinct evaluation approaches measuring generation completeness vs correctness - needed to interpret the nuanced performance results
- **Mis-generation detection**: Identifying inappropriate neomorpheme usage on non-human referents - needed to assess model limitations
- **Prompt formatting sensitivity**: How different prompt structures affect model output - needed to understand reproducibility challenges
- **Language pair specificity**: English-to-Italian translation challenges - needed to contextualize the benchmark's scope

## Architecture Onboarding

**Component Map**: Input text -> Prompt formatting (Direct/Binary/Ternary) -> LLM inference -> Output parsing -> Metric calculation (Coverage/Accuracy/Mis-generation) -> Result aggregation

**Critical Path**: English sentence → Prompt formatting → LLM generation → Italian translation with neomorphemes → Evaluation against annotated references

**Design Tradeoffs**: Zero-shot prompting offers simplicity but lower performance; few-shot prompting improves results but requires careful demonstration selection; prompt formatting affects reproducibility across model families

**Failure Signatures**: Llama 2 omitting angle-bracket notation; models generating neomorphemes on inanimate objects; inconsistent neomorpheme application across similar contexts

**First Experiments**:
1. Test basic prompt formatting with each model using a small validation set
2. Compare zero-shot vs few-shot performance with 1 demonstration
3. Evaluate mis-generation rates on a subset of sentences with non-human referents

## Open Questions the Paper Calls Out
None

## Limitations
- The NEO-GATE benchmark contains only 841 sentence pairs, limiting evaluation scope
- Performance is highly sensitive to prompt formatting, affecting reproducibility
- Results are specific to English-to-Italian translation and may not generalize to other language pairs
- Only two neomorpheme paradigms (Asterisk and Schwa) are evaluated, potentially missing other linguistic innovations

## Confidence

**Major Claim**: GPT-4 and Mixtral demonstrate superior performance for gender-inclusive MT with neomorphemes (Medium-High confidence)
**Claim**: LLM performance varies significantly by model family and size (High confidence)
**Claim**: Current models struggle with accurate neomorpheme placement (High confidence)

## Next Checks
1. **Cross-linguistic generalization test**: Replicate the experiment with English-to-Spanish and English-to-German translation pairs to assess whether the observed model performance patterns hold across languages with different gender marking systems

2. **Fine-tuning effectiveness validation**: Conduct controlled experiments fine-tuning smaller models (Llama 2, Tower) on the NEO-GATE dataset to determine whether the performance gap with GPT-4/Mixtral can be closed through domain adaptation

3. **Neomorpheme paradigm expansion evaluation**: Test the models' ability to handle additional neomorpheme systems beyond Asterisk and Schwa, including the E-ending and U-ending paradigms mentioned in the literature