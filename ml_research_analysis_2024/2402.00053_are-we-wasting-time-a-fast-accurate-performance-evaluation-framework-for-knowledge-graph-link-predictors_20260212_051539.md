---
ver: rpa2
title: Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for
  Knowledge Graph Link Predictors
arxiv_id: '2402.00053'
source_url: https://arxiv.org/abs/2402.00053
tags:
- sampling
- knowledge
- evaluation
- graph
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of evaluating
  Knowledge Graph Completion (KGC) models, which typically require ranking all entities
  in a graph for each link prediction query. The authors propose a framework that
  uses relational recommenders to guide candidate selection for evaluation, significantly
  reducing the number of entities that need to be ranked while maintaining estimation
  accuracy.
---

# Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for Knowledge Graph Link Predictors

## Quick Facts
- arXiv ID: 2402.00053
- Source URL: https://arxiv.org/abs/2402.00053
- Reference count: 40
- This paper proposes a framework that uses relational recommenders to guide candidate selection for Knowledge Graph Completion evaluation, reducing evaluation time from 30 minutes to 20 seconds on large-scale datasets while maintaining accuracy.

## Executive Summary
Knowledge Graph Completion (KGC) evaluation typically requires ranking all entities in a graph for each link prediction query, which is computationally expensive. This paper introduces a framework that uses relational recommenders to identify a subset of entities to rank, significantly reducing evaluation time while maintaining accuracy. The method samples candidates from domain and range sets derived from relation recommender scores, focusing on more relevant entities rather than uniform random sampling. Experiments show a 90-fold improvement in speed on the ogbl-wikikg2 dataset with minimal loss in metric estimation accuracy.

## Method Summary
The framework proposes two sampling approaches for KGC evaluation: Static and Probabilistic. The Static method uses a threshold to discretize relation recommender scores into binary domain and range sets, then samples entities from these sets. The Probabilistic method treats scores as probabilities and samples accordingly. Both methods reduce the evaluation complexity from O(|E|²) to O(2·|R|) by sampling candidates once per relation type rather than for each individual query. The approach is model-agnostic and works with any KGC model, using relation recommenders like L-WD (Label-Wise Distribution) and PIE (Probabilistic Inductive Embeddings) to guide candidate selection.

## Key Results
- Reduces evaluation time from 30 minutes to 20 seconds on ogbl-wikikg2 dataset
- Achieves up to 90-fold speedup while maintaining accurate performance estimates
- Static sampling outperforms Probabilistic sampling on most datasets, though Probabilistic is more accurate on the large-scale ogbl-wikikg2 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uniform random sampling overestimates ranking performance because it disproportionately includes "easy negatives" that would be filtered out immediately by domain/range scoring.
- Mechanism: Relational recommenders assign low scores to entities that are clearly irrelevant to a relation's domain/range, allowing them to be excluded from evaluation. Random sampling includes many such entities, which artificially inflates metrics because these easy negatives don't challenge the model.
- Core assumption: The domain and range sets accurately represent which entities can participate in a relation, and there's a clear distinction between entities that are relevant versus irrelevant to a relation.
- Evidence anchors:
  - [abstract]: "sampling uniformly at random vastly overestimates the ranking performance of a method"
  - [section 4]: "the smaller the sample size, the more optimistic will our expected ranking metrics be"
  - [corpus]: Weak - corpus papers focus on sampling in other domains but don't directly address KG evaluation tradeoffs

### Mechanism 2
- Claim: Sampling from domain/range sets provides more representative evaluation by focusing on harder negative examples that actually challenge the model.
- Mechanism: By sampling only from entities that have non-zero scores in the relation recommender, evaluation concentrates on entities that are plausible candidates for the relation, making the ranking task more realistic and metrics more accurate.
- Core assumption: The relation recommender can distinguish between plausible and implausible candidates with reasonable accuracy, and the remaining candidates after filtering represent the true difficulty distribution.
- Evidence anchors:
  - [abstract]: "using relational recommenders to guide the selection of candidates for evaluation"
  - [section 4.1]: "we sample more difficult negatives with Relational Recommenders"
  - [corpus]: Moderate - Active evaluation papers discuss importance of representative sampling but don't address KG-specific structure

### Mechanism 3
- Claim: The computational complexity reduction from O(|E|²) to O(2·|R|) sampling makes large-scale evaluation feasible while maintaining accuracy.
- Mechanism: Instead of sampling for each individual query (h,r,?) or (?,r,t), relational recommenders allow sampling once per relation type, reducing the number of sampling operations from O(|E|·|K_G_test|) to O(2·|R|).
- Core assumption: The relation recommender scores are stable enough across different queries involving the same relation that sampling once per relation provides representative candidates.
- Evidence anchors:
  - [section 4]: "we can sample head- and tail candidates for each relation instead of for each query"
  - [table 3]: Shows reduction from millions of samples to thousands
  - [corpus]: Strong - Papers on efficient sampling for large-scale systems directly support this complexity argument

## Foundational Learning

- Concept: Hypergeometric distribution
  - Why needed here: Understanding why random sampling size affects metric estimation accuracy requires grasping how sampling without replacement affects the probability of including challenging examples
  - Quick check question: If you have 1000 entities and sample 10 uniformly at random, what's the probability of including a specific entity?

- Concept: Candidate Recall vs Reduction Rate tradeoff
  - Why needed here: Balancing how many candidates to filter out while ensuring you don't miss true positives is central to understanding relation recommender effectiveness
  - Quick check question: If a relation recommender reduces candidates by 90% but misses 5% of true positives, what's the Candidate Recall?

- Concept: Knowledge Graph completion evaluation metrics
  - Why needed here: Understanding MRR, Hits@X, and filtered vs unfiltered metrics is essential for interpreting the evaluation framework's effectiveness
  - Quick check question: What's the difference between filtered and unfiltered MRR in KG completion?

## Architecture Onboarding

- Component map: KG training data, relation recommender model -> Score matrix computation, thresholding/discretization, candidate sampling -> Sampled candidate sets for evaluation
- Critical path:
  1. Build score matrix X from KG structure
  2. Discretize scores to form domain/range sets (static) or use as probabilities (probabilistic)
  3. Sample candidates from these sets
  4. Use sampled candidates for model evaluation
- Design tradeoffs:
  - Static vs probabilistic sampling: Static provides consistency but may be too strict; probabilistic captures uncertainty but adds randomness
  - Score matrix complexity: Simple methods like L-WD are fast but less sophisticated than GNN-based PIE
  - Type usage: Incorporating types improves accuracy but requires additional data and processing
- Failure signatures:
  - Low Candidate Recall indicates too aggressive filtering
  - High MAPE relative to random sampling suggests poor relation recommender quality
  - Negative correlation with true metrics indicates fundamental methodological issues
- First 3 experiments:
  1. Implement L-WD on a small KG and compare Candidate Recall vs Reduction Rate across different threshold values
  2. Run evaluation with static sampling on FB15k-237 and measure correlation with full evaluation metrics
  3. Compare static vs probabilistic sampling on ogbl-wikikg2 and measure speed vs accuracy tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed framework change when using more advanced relation recommenders that incorporate entity information, rather than the simpler, entity-agnostic methods used in this paper?
- Basis in paper: [explicit] The authors state that their framework is model-agnostic and can work with any KGC model, but they only compare simple, fast methods like L-WD with more advanced neural approaches like PIE. They also mention that their point is not to create novel relational recommenders but to show that these can be used as a tool.
- Why unresolved: The paper only provides empirical comparisons between a limited set of relation recommenders, leaving open the question of how more sophisticated methods would perform within the framework.
- What evidence would resolve it: Systematic experiments comparing the proposed framework using various relation recommenders of increasing complexity, including those that consider both entities and relations, would provide insights into the trade-offs between computational cost and estimation accuracy.

### Open Question 2
- Question: Can the proposed framework be extended to other tasks beyond Knowledge Graph Completion, such as inductive link prediction or logical query answering?
- Basis in paper: [explicit] The authors conclude by stating that their framework is extendable to other settings where the task is to rank entities, such as inductive KG completion and logical query answering, but leave this for future work.
- Why unresolved: The paper focuses on Knowledge Graph Completion and does not explore the potential of the framework for other related tasks.
- What evidence would resolve it: Applying the proposed framework to tasks like inductive link prediction or logical query answering and evaluating its effectiveness in these contexts would demonstrate its generalizability and potential benefits for a wider range of applications.

### Open Question 3
- Question: How does the choice of sampling strategy (Static vs. Probabilistic) impact the estimation accuracy and computational efficiency of the framework across different datasets and KGC models?
- Basis in paper: [explicit] The authors propose two sampling approaches, Static and Probabilistic, and compare their performance in terms of correlation with true metrics and MAE. They find that Static sampling generally outperforms Probabilistic, but Probabilistic is more accurate on the large-scale dataset ogbl-wikikg2.
- Why unresolved: While the paper provides some insights into the relative performance of the two sampling strategies, it does not provide a comprehensive analysis of their behavior across various datasets and models.
- What evidence would resolve it: Conducting extensive experiments on a diverse set of datasets and with different KGC models, comparing the performance of Static and Probabilistic sampling in terms of both accuracy and efficiency, would help determine the optimal choice of sampling strategy for different scenarios.

## Limitations

- Performance depends on quality of relation recommenders, which may not capture complex KG semantics
- Framework assumes domain/range sets accurately represent valid entities for relations, which may not hold for KGs with incomplete type information
- Method's effectiveness may vary significantly across different KG structures and relation patterns

## Confidence

- High confidence: Computational complexity reduction from O(|E|²) to O(2·|R|) and corresponding speedup claims
- Medium confidence: Accuracy preservation claims, as these depend on dataset-specific properties and relation recommender quality
- Medium confidence: Mechanism 1 (easy negative filtering) - well-supported by theoretical analysis but relies on domain/range completeness

## Next Checks

1. Test framework performance on KGs with sparse type information to assess domain/range estimation reliability
2. Implement adversarial KG structures where domain/range sets would incorrectly filter valid candidates
3. Compare performance across different relation recommender architectures (shallow vs GNN-based) to quantify sensitivity to recommender quality