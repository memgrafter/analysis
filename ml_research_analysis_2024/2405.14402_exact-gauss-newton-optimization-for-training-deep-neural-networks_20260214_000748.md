---
ver: rpa2
title: Exact Gauss-Newton Optimization for Training Deep Neural Networks
arxiv_id: '2405.14402'
source_url: https://arxiv.org/abs/2405.14402
tags:
- cited
- learning
- page
- pages
- hessian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Exact Gauss-Newton (EGN), a stochastic second-order
  optimization algorithm that efficiently computes the descent direction for deep
  neural network training. EGN uses the Gauss-Newton Hessian approximation combined
  with the Duncan-Guttman matrix identity to solve the linear system exactly in a
  computationally efficient manner, avoiding the cubic complexity typically associated
  with matrix inversion.
---

# Exact Gauss-Newton Optimization for Training Deep Neural Networks

## Quick Facts
- arXiv ID: 2405.14402
- Source URL: https://arxiv.org/abs/2405.14402
- Reference count: 40
- This paper introduces EGN, a stochastic second-order optimizer that efficiently computes descent directions for deep neural network training using the Duncan-Guttman identity.

## Executive Summary
This paper introduces Exact Gauss-Newton (EGN), a stochastic second-order optimization algorithm that efficiently computes the descent direction for deep neural network training. EGN uses the Gauss-Newton Hessian approximation combined with the Duncan-Guttman matrix identity to solve the linear system exactly in a computationally efficient manner, avoiding the cubic complexity typically associated with matrix inversion. The method is particularly advantageous when the parameter dimension far exceeds the batch size. EGN incorporates several improvements including line search, adaptive regularization, and momentum to enhance convergence. Under mild assumptions, the authors prove that EGN converges in expectation to a stationary point. The algorithm is evaluated across multiple supervised learning and reinforcement learning tasks, demonstrating consistent performance that matches or exceeds well-tuned SGD, Adam, GAF, SQN, and SGN optimizers.

## Method Summary
EGN computes descent directions using the Gauss-Newton Hessian approximation H = (1/b)J⊤QJ + λI, where J is the Jacobian and Q is a diagonal matrix of squared residuals. The key innovation is using the Duncan-Guttman identity to solve the linear system exactly in O(bcd) time instead of O(d³), where d is the parameter dimension and bc is the batch size times number of classes. The method computes the full Jacobian via backpropagation, then applies a low-rank matrix factorization to obtain the descent direction. EGN includes optional enhancements: momentum for variance reduction, line search for step size selection, and adaptive regularization for λ tuning. The algorithm maintains convergence guarantees while incorporating these improvements.

## Key Results
- EGN consistently matches or exceeds well-tuned SGD, Adam, GAF, SQN, and SGN optimizers across supervised learning and reinforcement learning tasks
- EGN shows particular advantages in regression tasks and LQR control problems, with competitive performance in classification and reinforcement learning scenarios
- The algorithm achieves faster convergence and better generalization than first-order methods while maintaining theoretical convergence guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Duncan-Guttman identity enables exact solution of the Gauss-Newton linear system in O(bcd) time instead of O(d³).
- Mechanism: By recognizing that the Levenberg-Marquardt Hessian H = (1/b)J⊤QJ + λI has low rank (bc ≪ d), the Duncan-Guttman identity allows transferring the expensive matrix inversion from d-dimensional space to bc-dimensional space where it can be computed exactly.
- Core assumption: The parameter dimension d is much larger than bc (batch size times number of classes), making H low-rank by construction.
- Evidence anchors:
  - [abstract] "Leveraging the Duncan-Guttman matrix identity, the parameter update is obtained by factorizing a matrix which has the size of the mini-batch."
  - [section 4] "By observing that Equation (4.1) defining the direction d LM has the form of the left-hand side of the identity (4.2) we state the following."
  - [corpus] Weak evidence - related papers discuss Gauss-Newton but not specifically Duncan-Guttman identity applications.

### Mechanism 2
- Claim: Momentum and line search can be seamlessly integrated into EGN without breaking convergence guarantees.
- Mechanism: Temporal averaging (momentum) stabilizes noisy Hessian estimates by incorporating past descent directions, while line search ensures sufficient decrease in loss despite stochasticity. The adaptive regularization adjusts λ based on the ratio between actual and predicted loss decrease.
- Core assumption: The stochastic estimates remain bounded and the correlation structure between preconditioner and gradient allows maintaining descent properties.
- Evidence anchors:
  - [abstract] "Additionally, we show how improvements such as line search, adaptive regularization, and momentum can be seamlessly added to EGN to further accelerate the algorithm."
  - [section 4.2] "For Gauss-Newton methods we could alternatively reduce the variance of the Jacobian J... Another option is to apply momentum to the descent direction d t."
  - [corpus] No direct evidence found in related papers about momentum integration in exact Gauss-Newton methods.

### Mechanism 3
- Claim: The regularized Gauss-Newton approximation H GN = (1/b)J⊤QJ provides a good preconditioner for neural network training.
- Mechanism: For smooth loss functions and neural networks, the true Hessian is dominated by the J⊤QJ term, making the Gauss-Newton approximation accurate while avoiding second-order derivative computation.
- Core assumption: The loss function is smooth and the network architecture satisfies conditions where J⊤QJ captures the essential curvature information.
- Evidence anchors:
  - [section 3.2] "As pointed out in [70], approximation (3.6) is justified since the true Hessian is dominated by the term J⊤QJ."
  - [section 6] Experimental results show EGN consistently matches or exceeds first-order methods across diverse tasks.
  - [corpus] Related paper "Theoretical characterisation of the Gauss-Newton conditioning in Neural Networks" suggests theoretical support exists.

## Foundational Learning

- Concept: Matrix inversion and computational complexity (O(n³) for naive inversion)
  - Why needed here: Understanding why direct matrix inversion is prohibitive for large d and how EGN avoids this bottleneck
  - Quick check question: If d = 10⁶, what is the approximate computational cost of naive matrix inversion in terms of floating-point operations?

- Concept: Low-rank matrix properties and the Woodbury identity
  - Why needed here: EGN uses a similar but more efficient approach (Duncan-Guttman) than the Woodbury identity for low-rank systems
  - Quick check question: What is the key difference between the Woodbury identity and Duncan-Guttman identity in terms of computational efficiency?

- Concept: Automatic differentiation and Jacobian computation
  - Why needed here: EGN requires computing the full Jacobian matrix J, which is more expensive than computing gradients alone
  - Quick check question: How does the computational cost of computing the full Jacobian compare to computing just the gradient for a neural network?

## Architecture Onboarding

- Component map: Forward pass → Jacobian computation → Direction calculation → Line search (if enabled) → Parameter update
- Critical path: Forward pass → Jacobian computation → Direction calculation → Line search (if enabled) → Parameter update
- Design tradeoffs: EGN trades increased per-iteration computation (Jacobian calculation) for faster convergence and better conditioning. The batch size must be balanced between computational efficiency and variance reduction.
- Failure signatures: Poor convergence may indicate inappropriate λ selection, batch size too small for variance control, or numerical instability in the low-rank solve. Memory issues may arise from storing large Jacobians.
- First 3 experiments:
  1. Implement basic EGN without enhancements on a small regression dataset (California Housing) to verify correctness
  2. Compare EGN vs SGD on a classification task (IMDB Reviews) with varying batch sizes to study variance effects
  3. Test adaptive regularization by running EGN with and without the adaptive λ update to measure convergence differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EGN's performance scale with batch sizes significantly larger than 128, particularly for extremely high-dimensional parameter spaces?
- Basis in paper: [inferred] The paper notes that for large batch sizes (b > 128), computational times increase significantly, and the direction calculation stage begins to dominate the update time. This suggests limitations in scalability.
- Why unresolved: The paper's experiments only tested batch sizes up to 128, and the observed performance degradation at larger batch sizes was not systematically studied or addressed with mitigation strategies.
- What evidence would resolve it: Systematic experiments comparing EGN performance across a wide range of batch sizes (e.g., 32 to 2048) on various model architectures, along with benchmarking against alternative methods or proposed mitigations for large-batch scenarios.

### Open Question 2
- Question: Can EGN be effectively adapted for classification tasks with a very large number of classes (e.g., large language model pretraining), and if so, what modifications are necessary?
- Basis in paper: [explicit] The paper identifies this as a major limitation, noting that the softmax function introduces coupling between outputs, making the Jacobian dense and computationally expensive for high-dimensional classification problems.
- Why unresolved: While the paper mentions potential solutions (approximating the Hessian, using alternative loss functions like multi-class hinge loss, or the Gauss-Newton-Bartlett estimator), it does not provide empirical validation of these approaches for large-scale classification tasks.
- What evidence would resolve it: Empirical studies comparing EGN with these proposed modifications on datasets with thousands of classes, measuring both computational efficiency and final classification accuracy relative to first-order methods.

### Open Question 3
- Question: How does the convergence rate of EGN compare to other second-order methods (e.g., Newton's method, quasi-Newton) in terms of wall-clock time for reaching a given accuracy, particularly for ill-conditioned problems?
- Basis in paper: [inferred] The paper proves convergence to a stationary point but does not provide a detailed comparison of convergence rates with other second-order methods in terms of practical wall-clock time, especially for problems with challenging loss landscapes.
- Why unresolved: The paper focuses on comparing EGN to first-order methods and SGN in terms of final accuracy and generalization performance, but does not provide a detailed analysis of convergence speed relative to other second-order optimization techniques.
- What evidence would resolve it: Experiments measuring the wall-clock time for EGN, Newton's method, and quasi-Newton methods to reach a predefined accuracy threshold on a variety of problems, including those with known ill-conditioning.

## Limitations
- EGN requires computing and storing the full Jacobian matrix, which becomes prohibitive for large models with many classes or high-dimensional outputs
- The algorithm's efficiency relies heavily on the assumption that bc ≪ d, which may not hold for all architectures
- The Gauss-Newton approximation may break down for non-smooth loss functions or architectures with complex second-order derivative structures

## Confidence
- High confidence in the mathematical derivation of the Duncan-Guttman identity application and the exact solution of the linear system
- Medium confidence in the convergence proof under stated assumptions, though practical conditions may be more restrictive
- Medium confidence in empirical results given the comprehensive evaluation across multiple task types and the consistent performance improvements

## Next Checks
1. Test EGN on a large-scale multi-class classification problem (e.g., ImageNet) to verify scalability claims and measure memory usage with different batch sizes
2. Compare EGN against K-FAC and Shampoo optimizers on the same set of tasks to establish relative performance among second-order methods
3. Implement a variant using the Woodbury identity instead of Duncan-Guttman to quantify the claimed computational advantages in practice