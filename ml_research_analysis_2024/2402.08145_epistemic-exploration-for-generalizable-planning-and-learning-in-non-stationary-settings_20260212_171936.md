---
ver: rpa2
title: Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary
  Settings
arxiv_id: '2402.08145'
source_url: https://arxiv.org/abs/2402.08145
tags:
- learning
- transition
- task
- state
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for continual planning
  and model learning in relational, non-stationary stochastic environments, which
  is essential for deploying sequential decision-making systems in the uncertain and
  evolving real world. The proposed approach addresses the challenges of working in
  practical settings with unknown and non-stationary transition systems and changing
  tasks by modeling gaps in the agent's current state of knowledge and using them
  to conduct focused, investigative explorations.
---

# Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings

## Quick Facts
- arXiv ID: 2402.08145
- Source URL: https://arxiv.org/abs/2402.08145
- Authors: Rushang Karia; Pulkit Verma; Alberto Speranzon; Siddharth Srivastava
- Reference count: 4
- Key outcome: Introduces a framework for continual planning and model learning in relational, non-stationary stochastic environments, significantly outperforming planning and RL baselines in sample complexity

## Executive Summary
This paper presents a novel framework for continual planning and model learning in relational, non-stationary stochastic environments. The approach addresses the challenges of working in practical settings with unknown and non-stationary transition systems and changing tasks by modeling gaps in the agent's current state of knowledge and using them to conduct focused, investigative explorations. The data collected through these explorations is then used for learning generalizable probabilistic models to solve the current task despite continual changes in the environment dynamics. The empirical evaluations on several non-stationary benchmark domains demonstrate that this approach significantly outperforms planning and RL baselines in terms of sample complexity.

## Method Summary
The proposed framework combines active query-based model learning (AQML) with statistical goodness-of-fit testing to detect and adapt to non-stationarity in relational MDPs. When the agent encounters transitions inconsistent with its learned model (M-consistency violations), it identifies the inconsistent predicates and uses FOND planning to generate distinguishing queries that resolve these inconsistencies. Additionally, the framework employs Pearson's chi-square tests to detect distributional shifts in action effects even when M-consistency is preserved. This need-based learning approach selectively updates only the affected components of the model, enabling faster adaptation than comprehensive relearning while maintaining generalizability across tasks.

## Key Results
- The framework significantly outperforms planning and RL baselines in terms of sample complexity on non-stationary benchmark domains
- Theoretical results show the system exhibits desirable convergence properties when stationarity holds
- The approach demonstrates effective zero-shot transfer when only object names or quantities change in the environment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework detects non-stationarity by monitoring simulator transitions for M-consistency and initiates focused exploration when inconsistencies are found.
- Mechanism: When a transition τ = (s, a, s′) fails M-consistency (τ ⇌ M), the system identifies inconsistent predicates in action preconditions/effects and re-learns them via AQML, effectively targeting only the changed components.
- Core assumption: Changes in the transition system are localized and can be isolated by identifying inconsistent predicates.
- Evidence anchors:
  - [abstract] "models gaps in the agent's current state of knowledge and uses them to conduct focused, investigative explorations."
  - [section] "If a transition τ = (s, a, s′) is not consistent w.r.t. the learned model M, i.e., τ ⇌ M, then we simultaneously update the model-learning process since a new query now needs to be synthesized that can resolve the inconsistency."
- Break condition: If changes are global or affect many predicates simultaneously, the approach may require re-learning most of the model, reducing efficiency.

### Mechanism 2
- Claim: Goodness-of-fit tests with Pearson's chi-square detect distributional shifts in action effects even when M-consistency is preserved.
- Mechanism: After each M-consistent transition, the system records frequencies of effects and applies Pearson's chi-square test to check if the observed distribution matches the learned probabilities; if the test fails, probabilities are reset and re-estimated.
- Core assumption: Distributional changes in action effects can be detected through statistical hypothesis testing even when structural changes are not present.
- Evidence anchors:
  - [section] "We use Pearson's chi-square test (Pearson 1992) for detecting o.o.d. effects... If the confidence computed using χ2 is less than some threshold θ (0.05 in our experiments), the goodness-of-fit test is deemed to have failed..."
  - [section] "Such changes cannot be quickly reflected if only MLE estimates are used to compute probabilities since these estimates can be slow to adapt to the new distribution."
- Break condition: If changes are subtle or sample sizes are too small, statistical tests may lack power to detect shifts.

### Mechanism 3
- Claim: Need-based learning with AQML selectively updates only inconsistent parts of the model, enabling faster adaptation than comprehensive relearning.
- Mechanism: Instead of relearning all actions when non-stationarity is detected, the system only re-learns actions/actions components that fail M-consistency or goodness-of-fit tests, leveraging AQML's focused query synthesis.
- Core assumption: Non-stationarity typically affects a small subset of the model rather than the entire transition system.
- Evidence anchors:
  - [abstract] "Data collected using these explorations is used for learning generalizable probabilistic models for solving the current task despite continual changes in the environment dynamics."
  - [section] "We develop a paradigm that can work in the presence of non-stationarity. We now begin by describing the problem that we address, followed by a detailed overview of our approach."
- Break condition: If non-stationarity is widespread or occurs frequently, the overhead of detecting and updating small parts may exceed the cost of full relearning.

## Foundational Learning

- Concept: Relational Markov Decision Processes (RMDPs) and PPDDL modeling
  - Why needed here: The framework operates on lifted PPDDL models that generalize across tasks, enabling zero-shot transfer when only object names or quantities change.
  - Quick check question: Can you explain the difference between a lifted predicate p↑(x1, ..., xn) and a grounded predicate p(o1, ..., on)?

- Concept: Active Query-based Model Learning (AQML) and FOND planning
  - Why needed here: AQML uses FOND planning to generate distinguishing queries that isolate which predicates are missing or incorrect in the current model, enabling efficient exploration.
  - Quick check question: How does AQML reduce model learning to a FOND planning problem, and why does this guarantee p-distinguishing policies?

- Concept: Statistical hypothesis testing (Pearson's chi-square)
  - Why needed here: Goodness-of-fit tests detect when action effect distributions have changed even when M-consistency is preserved, enabling adaptation to subtle non-stationarity.
  - Quick check question: What is the null hypothesis in Pearson's chi-square test for goodness of fit, and what does rejecting it indicate about the learned model?

## Architecture Onboarding

- Component map: Simulator -> Learned PPDDL model -> Model learning engine (AQML) -> Goodness-of-fit tester -> Planner (LAO*) -> Execution controller

- Critical path:
  1. Compute policy using current PPDDL model
  2. Execute action in simulator
  3. Check M-consistency of resulting transition
  4. If inconsistent: identify predicates, add to AQML query queue
  5. If consistent: perform goodness-of-fit test
  6. If test fails: reset and re-estimate probabilities
  7. Update model as needed
  8. Repeat

- Design tradeoffs:
  - Comprehensive vs. need-based learning: Need-based reduces sample complexity but may miss interactions between components
  - Statistical vs. structural detection: Goodness-of-fit catches distributional changes but requires sufficient samples
  - Exploration strategy: Random walks vs. goal-directed exploration affects sample efficiency

- Failure signatures:
  - Policy consistently fails to reach goal despite updates → model learning not capturing true transition dynamics
  - High frequency of goodness-of-fit test failures → environment has rapidly changing distributions
  - AQML queries taking excessive time → large search space or complex inconsistencies

- First 3 experiments:
  1. Run on stationary Tireworld domain to verify convergence to Oracle performance
  2. Introduce single-action precondition change and verify need-based learning corrects only affected action
  3. Simulate distributional shift in action effects and verify goodness-of-fit detection and adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit on the number of tasks that can be accomplished in non-stationary settings using this framework, and how does this limit compare to other approaches?
- Basis in paper: [inferred] The paper mentions that the approach can accomplish tasks in non-stationary settings, but does not provide a theoretical limit on the number of tasks.
- Why unresolved: The paper does not provide a theoretical analysis of the maximum number of tasks that can be accomplished using this framework in non-stationary settings.
- What evidence would resolve it: A theoretical analysis comparing the maximum number of tasks that can be accomplished using this framework versus other approaches in non-stationary settings.

### Open Question 2
- Question: How does the performance of this framework scale with the size of the state space and action space?
- Basis in paper: [inferred] The paper mentions that the framework can handle relational domains, but does not provide an analysis of how performance scales with the size of the state space and action space.
- Why unresolved: The paper does not provide an empirical or theoretical analysis of how the performance of the framework scales with the size of the state space and action space.
- What evidence would resolve it: An empirical or theoretical analysis showing how the performance of the framework scales with the size of the state space and action space.

### Open Question 3
- Question: Can this framework be extended to handle continuous state spaces and actions?
- Basis in paper: [inferred] The paper mentions that the framework can handle relational domains, but does not provide an analysis of how it can be extended to handle continuous state spaces and actions.
- Why unresolved: The paper does not provide a discussion or analysis of how the framework can be extended to handle continuous state spaces and actions.
- What evidence would resolve it: A discussion or analysis showing how the framework can be extended to handle continuous state spaces and actions, along with empirical results demonstrating its effectiveness.

## Limitations
- The framework assumes non-stationarity is localized to specific predicates, which may not hold in domains with global or systemic changes
- Reliance on statistical hypothesis testing requires sufficient samples for each distribution, which may not be feasible in resource-constrained or high-dimensional settings
- The paper does not address catastrophic forgetting or interference between model updates in long-running systems with frequent changes

## Confidence
- **High Confidence**: The framework's overall architecture and integration of AQML with goodness-of-fit testing is well-specified and theoretically grounded. The empirical evaluation methodology is sound.
- **Medium Confidence**: The efficiency claims (sample complexity improvements) are supported by experiments but may be sensitive to domain-specific properties and hyperparameter choices not fully explored in the paper.
- **Low Confidence**: The assumption that non-stationarity is localized and the robustness of the approach to frequent, global changes are asserted but not rigorously tested across diverse scenarios.

## Next Checks
1. **Stress Test Localization Assumption**: Systematically evaluate the framework's performance when non-stationarity affects increasingly larger portions of the model (from single predicates to entire action sets) to quantify the degradation in efficiency.

2. **Statistical Test Robustness**: Conduct experiments with varying sample sizes and distribution change magnitudes to determine the detection thresholds and false positive/negative rates of the goodness-of-fit tests under resource constraints.

3. **Catastrophic Forgetting Analysis**: Implement a long-running simulation with frequent, interleaved non-stationarity events and monitor the degradation of previously learned components to assess the framework's stability over extended deployments.