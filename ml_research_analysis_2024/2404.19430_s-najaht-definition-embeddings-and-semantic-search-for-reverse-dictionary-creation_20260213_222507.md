---
ver: rpa2
title: "S\xF5najaht: Definition Embeddings and Semantic Search for Reverse Dictionary\
  \ Creation"
arxiv_id: '2404.19430'
source_url: https://arxiv.org/abs/2404.19430
tags:
- dictionary
- language
- word
- reverse
- definitions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose an information retrieval-based reverse dictionary system
  using pre-trained language models and approximate nearest neighbors search. The
  system was applied to an Estonian language lexicon resource to introduce cross-lingual
  reverse dictionary functionality.
---

# Sõnajaht: Definition Embeddings and Semantic Search for Reverse Dictionary Creation

## Quick Facts
- arXiv ID: 2404.19430
- Source URL: https://arxiv.org/abs/2404.19430
- Reference count: 14
- Primary result: Achieved median rank of 1 in monolingual and 2 in cross-lingual settings using unlabeled evaluation approach

## Executive Summary
This paper introduces Sõnajaht, an information retrieval-based reverse dictionary system that leverages pre-trained language models and approximate nearest neighbors search. The system was applied to the Estonian language lexicon resource Sõnaveeb to provide cross-lingual reverse dictionary functionality. The authors propose a novel evaluation approach using synonymy relations from the dictionary itself as ground truth, eliminating the need for external annotations. Results demonstrate that the approach without model training is feasible, with multilingual models trained on Estonian data showing superior performance.

## Method Summary
The method employs pre-trained language models to encode dictionary definitions into dense vector representations, which are stored in a vector database using the Qdrant implementation of the Hierarchical Navigable Small World (HNSW) approximate nearest neighbors algorithm. The system extracts words, definitions, and synonymy relations from the Sõnaveeb lexicon, then performs semantic search based on cosine similarity between query and stored definition vectors. The evaluation uses both an existing English dataset extended with Estonian and Russian translations and a novel unlabeled approach that leverages the dictionary's own synonymy relations as ground truth.

## Key Results
- Achieved median rank of 1 for monolingual and 2 for cross-lingual retrieval using unlabeled evaluation
- Approximate nearest neighbors search is approximately 60 times faster than brute-force search
- Models trained with Estonian data showed superior cross-lingual performance
- No model training required achieved feasible performance levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained transformer embeddings provide dense semantic representations that enable effective semantic search for reverse dictionary tasks.
- Mechanism: The model maps definitions to dense vectors in a continuous space where semantically similar definitions are close, allowing retrieval based on cosine similarity rather than exact term matching.
- Core assumption: Dense embeddings capture sufficient semantic nuance to distinguish between definitions while grouping related ones.
- Evidence anchors:
  - [abstract] "dense sentence representations of modern pre-trained transformer-based language models make these problems obsolete and provide suitable representations for implementing semantic search functionality"
  - [section] "We encode the definitions using a pre-trained language model and then store these definitions in a vector database"
- Break condition: If embeddings fail to capture fine-grained semantic differences between similar definitions, retrieval precision will degrade.

### Mechanism 2
- Claim: Approximate nearest neighbors algorithms enable scalable semantic search in large dictionary databases.
- Mechanism: HNSW creates a navigable small world graph that allows logarithmic-time nearest neighbor searches in high-dimensional spaces.
- Core assumption: The trade-off between exactness and speed is acceptable for real-world reverse dictionary applications.
- Evidence anchors:
  - [section] "We chose the Qdrant vector database that implements the Hierarchical Navigable Small World (HNSW) approximate nearest neighbors algorithm...estimated the nearest neighbors search to be approximately 60 times faster than the brute-force search"
- Break condition: If the approximation significantly degrades retrieval quality, the system may miss relevant words.

### Mechanism 3
- Claim: Synonymy relations from the dictionary itself can serve as ground truth for evaluation without requiring external annotations.
- Mechanism: By treating both target words and their synonyms as valid answers for a given definition, the evaluation leverages the dictionary's internal structure to assess retrieval quality.
- Core assumption: Synonymy is a reliable indicator of semantic equivalence for reverse dictionary evaluation purposes.
- Evidence anchors:
  - [section] "we propose a novel approach to defining the ground truth for reverse dictionary evaluation based on the synonymy relations of the underlying lexicon"
- Break condition: If synonymy relations are incomplete or inaccurate, evaluation metrics may be misleading.

## Foundational Learning

- Concept: Dense vector representations
  - Why needed here: Understanding how transformers create continuous embeddings that capture semantic meaning is crucial for grasping why this approach works better than keyword-based methods.
  - Quick check question: What is the key difference between sparse (tf-idf) and dense (transformer) representations in terms of semantic capture?

- Concept: Approximate nearest neighbor search
  - Why needed here: Knowing how HNSW and similar algorithms trade exactness for speed helps understand the scalability claims and when they might break down.
  - Quick check question: Why is brute-force search infeasible for large dictionary databases, and how does HNSW address this?

- Concept: Cross-lingual embeddings
  - Why needed here: Understanding how multilingual models handle different languages is essential for interpreting the cross-lingual results and limitations.
  - Quick check question: What properties must an embedding model have to work effectively for cross-lingual reverse dictionary search?

## Architecture Onboarding

- Component map: Lexicon API → Data extraction/filtering → Embedding model → Vector database (Qdrant) → SQLite for metadata → Search interface
- Critical path: User query → Embedding → ANN search → Result filtering → Display
- Design tradeoffs: Exact vs. approximate search (speed vs. precision), multilingual vs. monolingual models (coverage vs. quality), simple vs. complex evaluation (effort vs. insight)
- Failure signatures: Poor precision/recall → Check embeddings quality; Slow response → Check ANN parameters; Missing results → Check data filtering or synonymy extraction
- First 3 experiments:
  1. Run monolingual search with different embedding models on small subset to compare MAP scores
  2. Test cross-lingual search with Estonian definitions and non-Estonian queries
  3. Validate unlabeled evaluation approach by comparing with labeled dataset results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the unlabeled evaluation approach correlate with human judgments in reverse dictionary tasks?
- Basis in paper: [explicit] The paper mentions that the ultimate purpose is to enable reverse dictionary search functionality and that human evaluation would be developed in future work. It also notes that the correlation between automatic measures and human judgments needs to be established.
- Why unresolved: The paper does not include any human evaluation component, relying instead on automatic metrics like MAP, MRR, and accuracy@k.
- What evidence would resolve it: Conducting a user study where human participants rate the relevance of retrieved words for various definitions, then comparing these ratings to the automatic evaluation metrics.

### Open Question 2
- Question: What is the optimal visual representation for displaying reverse dictionary results to maximize user satisfaction?
- Basis in paper: [inferred] The paper mentions that the visual presentation of reverse dictionary output would play a significant role in user satisfaction, but this aspect was not evaluated.
- Why unresolved: The paper focused on the technical aspects of building the reverse dictionary system but did not investigate user interface design or visualization strategies.
- What evidence would resolve it: A/B testing different UI layouts for displaying search results, measuring user satisfaction and task completion rates for each design.

### Open Question 3
- Question: How does filtering non-informative definitions or correcting erroneous synonymy relations affect model rankings in the unlabeled evaluation?
- Basis in paper: [explicit] The paper mentions that the level of noise in the dictionary resource was not assessed and that filtering could result in more precise evaluation.
- Why unresolved: The authors chose to use the raw data without additional cleaning to minimize effort, assuming it wouldn't significantly change model rankings.
- What evidence would resolve it: Conducting the evaluation on multiple versions of the dataset - one with no filtering, one with basic noise filtering, and one with extensive cleaning - then comparing the model rankings across these versions.

## Limitations
- Relies heavily on quality and completeness of synonymy relations within the lexicon
- Cross-lingual performance shows degradation compared to monolingual retrieval
- Results focused on Estonian language may not generalize to morphologically different languages
- Unlabeled evaluation approach may not fully capture real-world usage patterns

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Monolingual retrieval performance | High |
| Cross-lingual performance | Medium |
| Unlabeled evaluation approach applicability | Low |

## Next Checks

1. Test the unlabeled evaluation approach on a dictionary with different synonymy density to assess robustness of this evaluation method
2. Evaluate performance on a morphologically complex language (e.g., Finnish or Turkish) to test cross-lingual generalization beyond Estonian
3. Compare results using exact nearest neighbor search versus approximate search on a smaller dataset to quantify the precision-speed tradeoff