---
ver: rpa2
title: 'MMInA: Benchmarking Multihop Multimodal Internet Agents'
arxiv_id: '2404.09992'
source_url: https://arxiv.org/abs/2404.09992
tags:
- tasks
- agents
- task
- agent
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMInA, a benchmark for evaluating multihop
  multimodal Internet agents on real-world websites. Unlike existing benchmarks focused
  on single-hop or text-only tasks, MMInA features 1,050 human-written tasks across
  14 diverse, evolving websites requiring agents to extract both visual and textual
  information through multiple browsing steps.
---

# MMInA: Benchmarking Multihop Multimodal Internet Agents

## Quick Facts
- arXiv ID: 2404.09992
- Source URL: https://arxiv.org/abs/2404.09992
- Authors: Shulin Tian; Ziniu Zhang; Liangyu Chen; Ziwei Liu
- Reference count: 36
- Introduces MMInA benchmark for evaluating multihop multimodal Internet agents on real-world websites

## Executive Summary
MMInA addresses a critical gap in evaluating AI agents for real-world web navigation by introducing a benchmark for multihop multimodal tasks. Unlike existing benchmarks focused on single-hop or text-only scenarios, MMInA features 1,050 human-written tasks across 14 diverse, evolving websites requiring agents to extract both visual and textual information through multiple browsing steps. The benchmark employs a novel hop-success-rate evaluation protocol to better assess performance on compositional tasks, revealing significant performance gaps in current state-of-the-art LLMs and LMMs, particularly in multihop scenarios where agents often fail early.

## Method Summary
The MMInA benchmark is constructed with 1,050 human-written tasks across 14 diverse websites, requiring agents to navigate through multiple browsing steps while extracting both visual and textual information. A novel evaluation protocol based on hop success rates is proposed to assess performance on compositional tasks, moving beyond simple task completion metrics. The authors introduce a memory augmentation method that replays past action trajectories to improve agent performance, particularly for multihop tasks where traditional approaches struggle with information retention across browsing steps.

## Key Results
- State-of-the-art LLMs and LMMs show significant performance gaps on MMInA, especially in multihop scenarios where agents often fail early
- Memory augmentation method substantially improves both single-hop and multihop task success rates
- Hop-success-rate evaluation protocol reveals that current agents struggle with compositional reasoning across multiple browsing steps

## Why This Works (Mechanism)
The effectiveness of MMInA stems from its realistic task design that mirrors actual web navigation scenarios requiring multimodal reasoning. The hop-success-rate evaluation captures the compositional nature of real-world web tasks where failure at any step prevents overall success. The memory augmentation works by maintaining and replaying action trajectories, addressing the fundamental challenge of information retention across multiple browsing steps that current LLMs struggle with.

## Foundational Learning
- **Multimodal reasoning**: Understanding and integrating visual and textual information simultaneously is essential for web navigation tasks where both modalities carry critical information.
- **Compositional task execution**: Breaking down complex objectives into sequential steps requires planning and memory across multiple interactions.
- **Web navigation dynamics**: Understanding website structures, form interactions, and navigation patterns is crucial for successful task completion.

## Architecture Onboarding
**Component Map**: Task Generator -> Environment Simulator -> Agent Module -> Memory Augmentation -> Evaluation Module
**Critical Path**: Task generation and environment setup -> agent interaction through browsing steps -> memory replay for information retention -> hop-success-rate evaluation
**Design Tradeoffs**: Scale vs. diversity (1,050 tasks across 14 websites balances coverage with manageability) and evaluation granularity (hop-level vs. task-level success metrics)
**Failure Signatures**: Early task abandonment in multihop scenarios, inability to integrate visual information with textual context, and loss of state information across browsing steps
**First 3 Experiments**: 1) Single-hop task performance comparison between baseline and memory-augmented agents, 2) Multi-hop task success rate analysis with and without memory replay, 3) Ablation study on memory augmentation components

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small scale of 1,050 tasks may not capture full diversity of real-world web interactions
- Evaluation protocol relies on predefined task structures that may not generalize to open-ended scenarios
- Memory augmentation introduces computational overhead not fully characterized in terms of efficiency trade-offs

## Confidence
- **High Confidence**: Benchmark design methodology and distinction from existing single-hop or text-only benchmarks
- **Medium Confidence**: Performance gaps observed between state-of-the-art models and benchmark requirements
- **Medium Confidence**: Effectiveness of proposed memory augmentation method, though generalization requires further validation

## Next Checks
1. Conduct stress tests on the benchmark with additional task variations and edge cases to assess robustness
2. Implement cross-validation across different website subsets to evaluate generalization of memory augmentation improvements
3. Measure and report computational overhead and latency introduced by memory augmentation across different task complexities