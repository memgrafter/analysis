---
ver: rpa2
title: 'BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic
  Neural Graph Filtering'
arxiv_id: '2404.03528'
source_url: https://arxiv.org/abs/2404.03528
tags:
- graph
- semantic
- bangla
- information
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BanglaAutoKG, the first automatic framework
  for constructing Bengali knowledge graphs (KGs) from unstructured Bangla text. The
  approach leverages multilingual LLMs for entity extraction and relation identification,
  translation dictionaries for semantic mapping, and BERT embeddings for node features.
---

# BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering

## Quick Facts
- arXiv ID: 2404.03528
- Source URL: https://arxiv.org/abs/2404.03528
- Reference count: 0
- BanglaAutoKG achieves semantic alignment scores of 0.892 for poems and 0.912 for Wikipedia articles

## Executive Summary
This paper introduces BanglaAutoKG, the first automatic framework for constructing Bengali knowledge graphs (KGs) from unstructured Bangla text. The approach leverages multilingual LLMs for entity extraction and relation identification, translation dictionaries for semantic mapping, and BERT embeddings for node features. A GNN-based feature denoising method and semantic filtering improve KG quality by removing noise and irrelevant edges. The model achieves high semantic alignment scores (0.892 for poems, 0.912 for Wikipedia articles) and successfully captures key entities and relationships from both structured and unstructured Bengali text.

## Method Summary
BanglaAutoKG automatically constructs Bengali knowledge graphs from unstructured Bangla text through a multi-stage pipeline. First, a multilingual LLM extracts entities and relations from the input text. These entities are then mapped to English equivalents using a translation dictionary, and BERT embeddings are generated for semantic representation. The resulting graph undergoes GNN-based feature denoising to clean embeddings and align them with graph topology, followed by semantic filtering to prune less significant edges based on semantic similarity thresholds. The framework operates without supervised training data, relying instead on the reasoning capabilities of LLMs and semantic similarity measures to construct and refine the KG structure.

## Key Results
- Achieved semantic alignment scores of 0.892 for Bangla poems and 0.912 for Wikipedia articles
- Successfully extracted and connected entities and relationships from both structured and unstructured Bengali text
- Demonstrated the viability of automatic KG construction for low-resource languages using multilingual LLMs and GNN-based refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual LLM prompting enables robust entity and relation extraction from low-resource Bangla text.
- Mechanism: LLMs trained on multilingual corpora can parse entity mentions and infer relationships even when specialized Bangla NER tools are absent, leveraging cross-linguistic semantic patterns.
- Core assumption: The multilingual LLM's training data includes sufficient Bangla examples and its reasoning capabilities generalize across languages.
- Evidence anchors:
  - [abstract] "We utilize multilingual LLMs to understand various languages and correlate entities and relations universally."
  - [section] "Given a paragraph, article or any text t, we process it using a multilingual LLM (ChatGPT or Bard) to extract different entities (V o) from the text to get entities and relations."
  - [corpus] No direct evidence in corpus; assumes LLM's multilingual competence.
- Break condition: If LLM lacks sufficient Bangla context or if entity structures are highly domain-specific beyond its reasoning scope.

### Mechanism 2
- Claim: BERT embeddings of translated Bangla words capture semantic similarity for KG construction.
- Mechanism: Words are translated to English, then BERT embeddings are extracted to represent semantic features, enabling cross-lingual alignment.
- Core assumption: Semantic meaning is preserved across translation and BERT's contextualized embeddings generalize to word-level representations.
- Evidence anchors:
  - [abstract] "By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG."
  - [section] "Using this information, we leverage a pre-trained BERT (Devlin et al., 2019) model to generate feature vectors for these entities, creating an initial feature matrix X o."
  - [corpus] No direct evidence in corpus; relies on general BERT translation studies.
- Break condition: If translations are inaccurate or if word-level BERT features lose contextual nuances needed for relation detection.

### Mechanism 3
- Claim: Graph Neural Network-based feature denoising and semantic filtering improve KG quality by aligning node embeddings and pruning irrelevant edges.
- Mechanism: Self-supervised attention filters denoise initial embeddings; topological and local neighborhood convolutions reweight edges based on semantic similarity.
- Core assumption: Graph structure can guide effective denoising, and semantic similarity computed in embedding space correlates with true KG relevance.
- Evidence anchors:
  - [abstract] "To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges."
  - [section] "To mitigate this noise, we adopt a Graph Neural Network (GNN)-based feature denoising method, utilizing self-supervised attention filtering. Subsequently, we design a GNN-based semantic filtering technique to identify and remove less significant connections."
  - [corpus] No direct evidence in corpus; relies on GNN denoising literature.
- Break condition: If the semantic similarity threshold is poorly tuned or if denoising oversmooths node features, causing loss of important distinctions.

## Foundational Learning

- Concept: Multilingual LLM prompting for entity extraction
  - Why needed here: Specialized Bangla NER models are scarce; LLMs provide a fallback for identifying entities and relations in low-resource languages.
  - Quick check question: What is the primary advantage of using a multilingual LLM over building a custom Bangla NER model in this context?

- Concept: Translation dictionary + BERT embeddings for semantic mapping
  - Why needed here: Direct Bangla encoders are limited; mapping to English via dictionary enables use of robust multilingual BERT embeddings.
  - Quick check question: Why does the approach translate Bangla words to English before extracting BERT embeddings?

- Concept: Graph Neural Networks for feature denoising and edge filtering
  - Why needed here: Raw embeddings are noisy and unstructured; GNNs align features to the graph topology and prune irrelevant edges based on semantic similarity.
  - Quick check question: How does the GNN-based semantic filter decide which edges to remove from the KG?

## Architecture Onboarding

- Component map: Input Bangla text -> Multilingual LLM (entity/relation extraction) -> Translation dictionary (English mapping) -> Pre-trained BERT (embeddings) -> GNN denoising -> GNN semantic filtering -> Output Bengali KG

- Critical path:
  1. Text → LLM → Entities + Relations
  2. Entities + Dictionary → English words → BERT embeddings
  3. Embeddings + Graph structure → GNN denoising
  4. Denoised graph → GNN semantic filtering → Final KG

- Design tradeoffs:
  - LLM accuracy vs. computational cost
  - Translation quality vs. embedding fidelity
  - GNN complexity vs. KG pruning precision
  - Semantic threshold γ vs. KG density

- Failure signatures:
  - Too many spurious entities → LLM extraction errors
  - Poor edge pruning → Incorrect semantic threshold
  - Loss of important nodes → Overaggressive denoising
  - Low semantic alignment → Embedding misalignment

- First 3 experiments:
  1. Run the pipeline on a short Bangla poem; verify extracted entities match manual annotation.
  2. Vary the semantic similarity threshold γ; observe impact on edge count and A-SFAS score.
  3. Compare A-SFAS with and without GNN feature denoising to quantify its contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BanglaAutoKG compare to potential supervised methods for knowledge graph construction in Bengali, given that no supervised baselines currently exist?
- Basis in paper: [explicit] The paper states "In the absence of automatic KG construction methods in the Bengali language, our research focuses on case study analysis" and mentions the need for future comparative studies.
- Why unresolved: The authors acknowledge that supervised methods could potentially perform better but have not been able to implement or compare against them due to the lack of annotated Bengali KG datasets.
- What evidence would resolve it: Direct comparison studies using supervised learning approaches with annotated Bengali text corpora to construct KGs and benchmark against BanglaAutoKG's performance.

### Open Question 2
- Question: How does BanglaAutoKG handle highly metaphorical or figurative Bengali text, and what is the extent of performance degradation in such cases?
- Basis in paper: [explicit] The discussion section mentions "Another problem is irregular output from LLMs" and "the complexity of deep metaphorical texts presents challenges for LLMs, potentially diminishing our model's performance."
- Why unresolved: While the authors acknowledge this limitation, they haven't provided quantitative data on how much metaphorical text impacts performance or proposed specific solutions for this challenge.
- What evidence would resolve it: Systematic testing of BanglaAutoKG on datasets containing varying degrees of metaphorical Bengali text with performance metrics showing degradation rates and effectiveness of potential mitigation strategies.

### Open Question 3
- Question: What is the optimal threshold γ value for edge pruning across different types of Bengali text, and how does it vary based on text characteristics?
- Basis in paper: [explicit] The paper mentions "γ is contingent upon the specific characteristics of the graph" and states it's configured to retain 90% of edges in their experimental framework.
- Why unresolved: The authors don't provide guidance on how to determine the optimal γ value for different text types or domains, nor do they explore the impact of varying this parameter systematically.
- What evidence would resolve it: Empirical studies showing performance metrics across different γ values for various Bengali text domains (news, poetry, technical documents, etc.) to establish guidelines for threshold selection.

## Limitations
- The approach relies heavily on multilingual LLM performance for entity extraction without validation of Bangla-specific accuracy
- Translation dictionary quality and coverage are unspecified, potentially introducing semantic drift
- GNN-based denoising and filtering mechanisms lack detailed architectural specifications, limiting reproducibility
- Evaluation focuses primarily on semantic alignment scores without comprehensive qualitative assessment of KG correctness

## Confidence

- High confidence: The overall pipeline architecture (LLM → translation → BERT → GNN) is clearly described and follows established methodologies.
- Medium confidence: The claimed semantic alignment scores (0.892 for poems, 0.912 for Wikipedia articles) are presented but lack detailed validation methodology and comparison baselines.
- Low confidence: The effectiveness of individual components (translation dictionary quality, GNN hyperparameter tuning, semantic threshold selection) is not empirically validated with ablation studies.

## Next Checks

1. Conduct an ablation study comparing A-SFAS scores with and without each major component (LLM extraction, translation dictionary, BERT embeddings, GNN denoising, semantic filtering) to quantify individual contributions.
2. Perform qualitative evaluation by manually inspecting 50 randomly selected edges from the final KG to assess correctness and relevance, comparing against gold-standard annotations.
3. Test the framework on a held-out Bangla text corpus not used in training or development to verify generalization and robustness to domain shifts.