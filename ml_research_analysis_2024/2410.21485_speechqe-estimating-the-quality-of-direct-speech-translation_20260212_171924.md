---
ver: rpa2
title: 'SpeechQE: Estimating the Quality of Direct Speech Translation'
arxiv_id: '2410.21485'
source_url: https://arxiv.org/abs/2410.21485
tags:
- speech
- speechqe
- quality
- translation
- cascaded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SpeechQE, a task for estimating the quality
  of direct speech translation. It proposes an end-to-end approach that combines a
  pre-trained speech encoder with a large language model (LLM) via a modality adapter.
---

# SpeechQE: Estimating the Quality of Direct Speech Translation

## Quick Facts
- arXiv ID: 2410.21485
- Source URL: https://arxiv.org/abs/2410.21485
- Reference count: 19
- End-to-end SpeechQE models outperform cascaded systems on CoV oST2 and IWSLT23-ACL datasets

## Executive Summary
This work introduces SpeechQE, a task for estimating the quality of direct speech translation. The authors propose an end-to-end approach that combines a pre-trained speech encoder with a large language model (LLM) via a modality adapter. Evaluated on CoV oST2 and IWSLT23-ACL datasets, the end-to-end models outperformed cascaded systems based on ASR and text-QE models, showing better correlation with both metric-based and human direct assessment scores. The best end-to-end model achieved Spearman correlations up to 0.925 with reference-based metrics and 0.509 with human judgments. Additionally, the end-to-end model demonstrated zero-shot error span detection capabilities, though cascaded systems with SOTA ASR still performed better in that task.

## Method Summary
The SpeechQE approach uses an end-to-end architecture combining a pre-trained speech encoder (Whisper-large-v2) with a pre-trained text LLM (TowerInstruct-7B) through a modality adapter. The adapter consists of three 1D convolutional layers with a 512-dimensional bottleneck. The model is trained using a multi-task approach incorporating ASR, ST, and SpeechQE tasks, with optional two-phase training (ASR+ST first, then SpeechQE). The system is evaluated on CoV oST2 and IWSLT23-ACL datasets using Spearman correlation with reference-based metrics and human direct assessment scores, as well as zero-shot error span detection capabilities.

## Key Results
- End-to-end SpeechQE models outperformed cascaded systems on CoV oST2 and IWSLT23-ACL datasets
- Best end-to-end model achieved Spearman correlation of 0.925 with reference-based metrics and 0.509 with human judgments
- End-to-end models demonstrated zero-shot error span detection capabilities, though cascaded systems with SOTA ASR performed better in this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end SpeechQE systems outperform cascaded systems because they avoid ASR-induced transcription errors.
- Mechanism: The end-to-end model directly maps speech input to quality scores without relying on an intermediate ASR transcription, which may contain errors that mislead downstream text-QE models.
- Core assumption: The direct mapping from speech to quality estimation is more accurate than the cascaded path of speech→ASR→text-QE.
- Evidence anchors:
  - [abstract]: "Results suggest that end-to-end approaches are better suited to estimating the quality of direct speech translation than using quality estimation systems designed for text in cascaded systems."
  - [section 5.1]: "The end-to-end SpeechQE systems consistently outperform the cascaded system which included the SOTA ASR system (whisper-large-v3)."
  - [corpus]: Weak. While there is evidence of ASR performance variance, no direct ablation of ASR-only errors in QE quality is provided.
- Break condition: If the end-to-end model's speech encoder introduces more noise or distortion than the ASR transcription, or if the LLM is not well-aligned with speech modality semantics.

### Mechanism 2
- Claim: The modality adapter enables effective transfer of translation quality estimation knowledge from text-LLM to speech domain.
- Mechanism: A lightweight adapter bridges the gap between speech embeddings and text embedding space, allowing a pre-trained text-LLM to handle speech-related tasks without retraining from scratch.
- Core assumption: The adapter can learn to map speech features to a compatible embedding space that the LLM can interpret for QE tasks.
- Evidence anchors:
  - [section 3.2]: "We adopt a popular configuration for integrating speech modality into text-LLM that trains a lightweight modality adapter..."
  - [section 5.1]: "Among four E2E models, LoRA training the text-LLM with a fixed pre-trained speech adapter (TowerInstruct-LoRA+Adapter-pt-Fixed) performs the best..."
  - [corpus]: No explicit quantitative evidence of adapter efficacy beyond correlation results; assumes adapter architecture is effective.
- Break condition: If the adapter architecture is insufficient to align speech and text modalities, or if the speech encoder produces features incompatible with the LLM.

### Mechanism 3
- Claim: Joint training with ASR, ST, and SpeechQE tasks enables the model to learn cross-modal mappings that improve QE accuracy.
- Mechanism: Multi-task learning aligns speech and text representations across ASR and ST tasks, creating a shared embedding space that supports QE without needing explicit SpeechQE data.
- Core assumption: Cross-task supervision helps the model learn robust speech-text alignment, improving downstream QE performance.
- Evidence anchors:
  - [section 3.2]: "We train the E2E model with the SpeechQE task, complemented with the ASR and ST tasks which provide supervision of mapping between text modality."
  - [section 5.1]: "All variants of our E2E system outperform BLASER2.0, perhaps due to its limited exposure to diverse translation quality at training time."
  - [corpus]: Implicit. The multi-task setup is described, but no ablation study isolating the effect of ASR/ST training is presented.
- Break condition: If the ASR/ST tasks introduce conflicting objectives or if the training data for these tasks is not representative of the QE domain.

## Foundational Learning

- Concept: Quality Estimation (QE) for machine translation
  - Why needed here: QE predicts translation quality without reference translations, crucial for assessing speech translation output reliability.
  - Quick check question: What is the main difference between reference-based and reference-free quality estimation?

- Concept: Modality adapters in multimodal learning
  - Why needed here: The adapter bridges speech and text modalities so a text-LLM can process speech input for QE.
  - Quick check question: How does a modality adapter differ from full fine-tuning in multimodal integration?

- Concept: End-to-end vs. cascaded architectures
  - Why needed here: Determines whether speech quality estimation goes directly through a unified model or through separate ASR and text-QE stages.
  - Quick check question: What are the primary trade-offs between end-to-end and cascaded approaches in terms of latency and error propagation?

## Architecture Onboarding

- Component map: Speech encoder (Whisper-large-v2) -> modality adapter (3 conv layers + bottleneck) -> text-LLM (TowerInstruct-7B) -> output score
- Critical path: Raw audio -> speech encoder -> adapter -> LLM -> QE score
- Design tradeoffs:
  - Adapter vs. full fine-tuning: adapter is parameter-efficient but may limit adaptation depth.
  - Fixed vs. LoRA-trained LLM: fixed is faster but LoRA may improve task-specific performance.
- Failure signatures:
  - Low correlation with metrics: possible modality misalignment or insufficient adapter capacity.
  - Erratic scores: potential issues in speech encoder or adapter instability.
  - High latency: overly complex adapter or inefficient LLM inference.
- First 3 experiments:
  1. Test adapter-only training with ASR and ST tasks before QE to confirm modality alignment.
  2. Compare fixed vs. LoRA text-LLM fine-tuning on a small QE dataset.
  3. Run ablation: speech encoder frozen vs. trainable to measure impact on QE performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different speech encoder architectures impact the performance of end-to-end SpeechQE models?
- Basis in paper: [inferred] The paper uses a pre-trained speech encoder (Whisper-large-v2) and freezes it, but notes that the optimal architecture for integrating speech modality into text language models remains an open question.
- Why unresolved: The paper does not explore different speech encoder architectures or their impact on SpeechQE performance.
- What evidence would resolve it: Comparing SpeechQE performance using various speech encoders (e.g., different Whisper models, HuBERT, Wav2Vec2) would clarify which architectures are most effective.

### Open Question 2
- Question: Can SpeechQE models generalize effectively across diverse language pairs and domains beyond the European languages tested?
- Basis in paper: [explicit] The paper acknowledges that experiments were limited to European languages (English-to-German and Spanish-to-English) and suggests expanding to non-European languages to assess generalizability.
- Why unresolved: The paper only tests on two European language pairs, leaving the model's performance on other language families unexplored.
- What evidence would resolve it: Evaluating SpeechQE models on a wider range of language pairs, including non-European and low-resource languages, would demonstrate their cross-linguistic robustness.

### Open Question 3
- Question: How does the inclusion of diverse speech instruction tuning tasks during training affect the zero-shot error span detection capabilities of SpeechQE models?
- Basis in paper: [inferred] The paper notes that the current training tasks (ASR, ST, SpeechQE) interfere with zero-shot ESD performance and suggests augmenting training with diverse speech instruction tuning and QA tasks.
- Why unresolved: The paper does not experiment with additional speech instruction tuning tasks to improve zero-shot ESD performance.
- What evidence would resolve it: Training SpeechQE models with a broader set of speech instruction tuning tasks and evaluating their zero-shot ESD performance would reveal the impact of diverse training on this capability.

### Open Question 4
- Question: What is the impact of model size and architecture choices on the efficiency and performance trade-offs in SpeechQE systems?
- Basis in paper: [explicit] The paper compares cascaded systems with larger text-QE models to end-to-end models, showing that E2E systems still outperform despite being smaller, but does not explore varying model sizes or architectures in depth.
- Why unresolved: While the paper touches on model size, it does not systematically investigate how different architectures or scaling affect SpeechQE performance and efficiency.
- What evidence would resolve it: Conducting experiments with various model sizes and architectures (e.g., smaller or larger speech encoders, different adapter configurations) would clarify the optimal balance between efficiency and performance.

## Limitations

- The modality adapter configuration lacks precise hyperparameters and architectural details
- Multi-task training benefits are inferred rather than explicitly validated through ablation studies
- Zero-shot error span detection results lack comparison to supervised baselines, making performance assessment difficult

## Confidence

- High confidence: Overall end-to-end approach outperforming cascaded systems (supported by consistent correlation improvements across multiple metrics and datasets)
- Medium confidence: Modality adapter's effectiveness (results show improvement but lack quantitative comparison to alternative integration methods)
- Low confidence: Specific contribution of multi-task training (no controlled ablation studies were performed)

## Next Checks

1. **Ablation study on modality adapter architecture**: Systematically test different adapter configurations (varying bottleneck dimensions, layer counts, and training strategies) to isolate the impact of architectural choices on QE performance.

2. **Error span detection benchmark**: Compare zero-shot error span detection performance against supervised baselines on the same datasets to establish whether the observed capability represents genuine generalization or dataset-specific artifacts.

3. **ASR system robustness analysis**: Conduct controlled experiments varying ASR system quality (different models and error rates) to quantify the exact contribution of ASR errors to cascaded system performance degradation.