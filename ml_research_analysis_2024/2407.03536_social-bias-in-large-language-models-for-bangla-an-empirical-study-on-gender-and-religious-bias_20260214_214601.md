---
ver: rpa2
title: 'Social Bias in Large Language Models For Bangla: An Empirical Study on Gender
  and Religious Bias'
arxiv_id: '2407.03536'
source_url: https://arxiv.org/abs/2407.03536
tags:
- bias
- gender
- language
- bangla
- religion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first empirical study on social bias in
  large language models (LLMs) for Bangla, focusing on gender and religious bias.
  The authors curated a dataset with template-based and naturally sourced sentences,
  and employed two probing techniques to measure bias using the Disparate Impact (DI)
  metric.
---

# Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias

## Quick Facts
- arXiv ID: 2407.03536
- Source URL: https://arxiv.org/abs/2407.03536
- Reference count: 13
- Primary result: First empirical study measuring gender and religious bias in Bangla LLMs using Disparate Impact metric

## Executive Summary
This paper presents the first empirical study on social bias in large language models for Bangla, focusing on gender and religious bias. The authors curated a dataset with template-based and naturally sourced sentences, and employed two probing techniques to measure bias using the Disparate Impact (DI) metric. Experiments on three LLMs (GPT-3.5, GPT-4o, and Llama-3) revealed significant gender and religious biases, with DI scores deviating from the fairness threshold of 1. GPT-3.5 showed a bias towards the feminine gender, while Llama-3 exhibited male gender bias. All models displayed religious bias, associating negative traits with the Muslim community and positive traits with the Hindu community.

## Method Summary
The authors developed a template-based approach combined with naturally sourced sentences to create a Bangla bias detection dataset. They employed two probing techniques to measure bias, calculating Disparate Impact scores for different gender and religious groups. The study tested three prominent LLMs (GPT-3.5, GPT-4o, and Llama-3) to assess their performance on Bangla-specific bias detection tasks.

## Key Results
- All tested LLMs showed significant gender bias with Disparate Impact scores deviating from fairness threshold of 1
- GPT-3.5 exhibited feminine gender bias while Llama-3 showed male gender bias
- All models displayed religious bias, associating negative traits with Muslim community and positive traits with Hindu community

## Why This Works (Mechanism)
The Disparate Impact metric effectively quantifies bias by comparing prediction rates across different social groups. By using template-based sentences combined with naturally sourced examples, the authors created a controlled environment to measure how LLMs respond to gender and religious prompts. The metric's threshold of 1 serves as a fairness benchmark, with deviations indicating bias.

## Foundational Learning
- **Disparate Impact (DI) Metric**: Measures bias by comparing prediction rates across groups; needed for quantitative bias assessment; quick check: DI = 1 indicates fairness
- **Template-based Dataset Generation**: Creates controlled test cases for bias detection; needed for systematic evaluation; quick check: templates should cover diverse scenarios
- **Bias Probing Techniques**: Methods to elicit and measure bias in model outputs; needed for identifying hidden biases; quick check: multiple techniques should converge on similar findings
- **Fairness Thresholds**: Predetermined values indicating acceptable bias levels; needed for objective evaluation; quick check: threshold should be justified by domain standards

## Architecture Onboarding

**Component Map**
Dataset Generator -> Bias Probing Module -> LLM Inference Engine -> Disparate Impact Calculator -> Bias Assessment

**Critical Path**
Dataset generation → bias elicitation through prompts → model inference → DI score calculation → bias interpretation

**Design Tradeoffs**
- Template-based vs fully natural sentences (control vs realism)
- Quantitative DI metric vs qualitative assessment
- Limited model selection vs comprehensive coverage
- Bangla-specific focus vs multilingual generalization

**Failure Signatures**
- DI scores consistently near 1 despite known biases
- High variance in DI scores across similar prompts
- Systematic errors favoring majority groups
- Inconsistent bias patterns across different probing techniques

**3 First Experiments**
1. Test with synthetic bias-free dataset to establish baseline DI scores
2. Run cross-lingual bias tests using English prompts translated to Bangla
3. Compare DI scores across different temperature settings in LLM inference

## Open Questions the Paper Calls Out
None

## Limitations
- Template-based dataset generation may not capture natural language complexity
- Focus only on gender and religious biases, excluding other social dimensions
- Limited testing to three LLMs, potentially missing broader patterns

## Confidence

**Major Claims Confidence:**
- **High confidence**: Detection of gender and religious bias in tested LLMs using DI metric
- **Medium confidence**: Specific directional biases (feminine vs masculine, Hindu vs Muslim) due to potential dataset limitations
- **Low confidence**: Comparative ranking of models' bias severity without more extensive testing

## Next Checks
1. Replicate experiments using a fully naturally-sourced dataset to validate template-based findings
2. Apply multiple bias metrics (e.g., WEAT, relative negative impact) to triangulate results
3. Test additional Bangla LLMs including open-source models fine-tuned specifically for Bangla