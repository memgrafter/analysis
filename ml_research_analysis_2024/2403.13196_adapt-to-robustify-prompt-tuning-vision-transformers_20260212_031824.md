---
ver: rpa2
title: ADAPT to Robustify Prompt Tuning Vision Transformers
arxiv_id: '2403.13196'
source_url: https://arxiv.org/abs/2403.13196
tags:
- prompt
- adversarial
- tuning
- training
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of Vision Transformers (ViTs)
  under the prompt tuning paradigm, revealing that existing adversarial defense methods
  suffer from gradient obfuscation when applied to prompt tuning. To address this,
  the authors propose ADAPT, a novel framework for adaptive adversarial training that
  conditions on prompts during both adversarial example generation and training.
---

# ADAPT to Robustify Prompt Tuning Vision Transformers

## Quick Facts
- **arXiv ID**: 2403.13196
- **Source URL**: https://arxiv.org/abs/2403.13196
- **Reference count**: 13
- **Primary result**: ADAPT achieves ~40% robust accuracy while tuning only ~1% of ViT parameters

## Executive Summary
This paper addresses the critical challenge of adversarial robustness in Vision Transformers (ViTs) when using parameter-efficient prompt tuning. The authors identify that existing adversarial defense methods suffer from gradient obfuscation when applied to prompt tuning, making them ineffective against adaptive attacks. To solve this, they propose ADAPT, a novel framework that conditions adversarial training on prompts during both adversarial example generation and training. The method employs two loss formulations—cross-entropy and KL divergence—and demonstrates competitive robust accuracy compared to full-model fine-tuning methods while tuning only ~1% of parameters.

## Method Summary
ADAPT introduces a conditioning mechanism that incorporates prompt information during both adversarial example generation and training phases. The framework uses Projected Gradient Descent (PGD) attacks that are conditioned on the current prompt, ensuring the adversarial examples are tailored to the specific prompt configuration. During training, the model optimizes two loss functions: standard cross-entropy loss and KL divergence between predictions on clean and adversarial examples. This dual-loss approach encourages the model to maintain consistent predictions across clean and perturbed inputs while being robust to adversarial attacks. The conditioning is implemented by concatenating prompt embeddings with intermediate features during the attack generation process and using them as additional inputs during training.

## Key Results
- Achieves ~40% robust accuracy on CIFAR-10, CIFAR-100, and Imagenette datasets
- Tunes only ~1% of ViT parameters compared to full-model fine-tuning
- Outperforms existing adversarial defenses against adaptive white-box and black-box attacks
- Demonstrates effectiveness across multiple dataset sizes and complexities

## Why This Works (Mechanism)
ADAPT addresses the fundamental issue of gradient obfuscation in prompt tuning by ensuring that adversarial examples are specifically tailored to the current prompt configuration. Traditional adversarial training methods generate perturbations based solely on image features, which can create gradients that are ineffective or misleading when applied to prompt-tuned models. By conditioning the attack generation process on prompts, ADAPT creates adversarial examples that directly target the prompt's influence on the model's decision boundaries. The KL divergence loss further stabilizes training by encouraging the model to maintain consistent probability distributions between clean and adversarial inputs, preventing catastrophic forgetting of clean data accuracy while building robustness.

## Foundational Learning
**Vision Transformers (ViTs)**: Why needed - Understanding the base architecture is crucial since prompt tuning modifies how ViTs process visual information. Quick check - Can you explain how self-attention differs from convolutional operations in image processing?

**Prompt Tuning**: Why needed - This parameter-efficient method is the foundation of the study, and understanding its mechanics is essential for grasping why standard defenses fail. Quick check - How does prompt tuning differ from full fine-tuning in terms of parameter updates?

**Adversarial Training**: Why needed - The paper builds upon and improves existing adversarial training methods, requiring understanding of their limitations. Quick check - What is the difference between standard training and adversarial training in terms of loss optimization?

**Gradient Obfuscation**: Why needed - This is the central problem ADAPT addresses, making it critical to understand why it occurs in prompt tuning. Quick check - Can you explain why gradient obfuscation makes adversarial defenses appear effective when they're not?

## Architecture Onboarding

**Component Map**: Input Images → Prompt Generator → Conditioned PGD Attack → ViT Backbone → Classification Head → Cross-Entropy + KL Loss → Parameter Updates

**Critical Path**: The critical path involves the conditioning mechanism where prompts are integrated into both the adversarial attack generation and the training process. This conditioning ensures that perturbations are specifically crafted to challenge the prompt-tuned model's decision boundaries.

**Design Tradeoffs**: The primary tradeoff is between computational efficiency and robustness. While ADAPT maintains the parameter efficiency of prompt tuning (~1% parameters), the conditioning mechanism adds computational overhead during adversarial example generation. The dual-loss formulation (cross-entropy + KL divergence) provides better stability but increases training complexity compared to single-loss approaches.

**Failure Signatures**: Models may fail when the conditioning mechanism is too weak, resulting in attacks that don't effectively challenge the prompt's influence. Another failure mode occurs when the KL divergence loss dominates, causing the model to prioritize consistency over robustness, leading to reduced adversarial accuracy.

**First Experiments**: 
1. Test ADAPT's performance with only cross-entropy loss versus only KL divergence loss to determine the optimal loss combination
2. Evaluate the impact of different conditioning strengths on attack effectiveness
3. Compare ADAPT's performance against standard adversarial training applied to prompt-tuned models

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the research raises several implicit questions about the generalizability of ADAPT to other parameter-efficient tuning methods, the theoretical foundations of why KL divergence loss works particularly well for robustness, and whether the conditioning mechanism can be extended to other transformer architectures beyond ViTs.

## Limitations
- Analysis of gradient obfuscation is limited to specific attack methods, potentially missing other adaptive attack vectors
- Effectiveness of KL divergence loss is demonstrated empirically but lacks theoretical justification
- Limited evaluation of robustness against natural distribution shifts and common corruptions beyond adversarial examples

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| ADAPT achieves competitive robust accuracy compared to full-model fine-tuning while tuning only ~1% of parameters | High |
| ADAPT effectively addresses gradient obfuscation issues in prompt tuning adversarial defenses | Medium |
| The two proposed loss formulations (cross-entropy and KL divergence) meaningfully improve robustness | Medium |

## Next Checks

1. Test ADAPT against adaptive attacks specifically designed to circumvent prompt-based defenses, such as attacks that jointly optimize both prompt and image perturbations

2. Evaluate ADAPT's robustness to natural distribution shifts and common corruptions (e.g., Gaussian noise, blur, weather effects) beyond adversarial examples

3. Conduct ablation studies isolating the contribution of conditioning on prompts during adversarial example generation versus conditioning during training to better understand which component drives robustness improvements