---
ver: rpa2
title: Generative vs. Discriminative modeling under the lens of uncertainty quantification
arxiv_id: '2406.09172'
source_url: https://arxiv.org/abs/2406.09172
tags:
- generative
- discriminative
- distribution
- which
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper undertakes a comparative analysis of generative and
  discriminative modeling approaches under the lens of Bayesian uncertainty quantification.
  The authors argue that while discriminative models are convenient for direct posterior
  approximation, they lack explicit priors and thus cannot leverage prior knowledge
  or handle imbalanced datasets effectively.
---

# Generative vs. Discriminative modeling under the lens of uncertainty quantification

## Quick Facts
- arXiv ID: 2406.09172
- Source URL: https://arxiv.org/abs/2406.09172
- Reference count: 40
- Primary result: Generative models can match or exceed discriminative accuracy while maintaining better uncertainty calibration, especially under data imbalance or limited labeled data

## Executive Summary
This paper provides a comparative analysis of generative and discriminative modeling approaches through the lens of Bayesian uncertainty quantification. The authors demonstrate that while discriminative models offer computational convenience for direct posterior approximation, they lack explicit priors and struggle with imbalanced datasets and limited labeled data. Generative models, by incorporating explicit prior distributions and leveraging both labeled and unlabeled data, can achieve superior performance and uncertainty calibration. The theoretical analysis is supported by simulations on regression and classification tasks using MNIST and FashionMNIST datasets.

## Method Summary
The authors compare generative and discriminative models using neural network architectures with similar parameter counts. Discriminative models use 4-layer fully connected networks, while generative models employ Normalizing Flows with stochastic layers. Both approaches use Gibbs sampling with T=10 steps for posterior inference, implemented via Stochastic Gradient Langevin Dynamics. Experiments are conducted across three scenarios: identical priors and sizes, imbalanced datasets with different priors but same sizes, and few labeled samples with same priors but different sizes. Performance is evaluated using classification accuracy percentages and standard deviations across 10 independent runs on MNIST and FashionMNIST datasets with dimensionality reduced via PCA to retain 95% explained variance.

## Key Results
- Generative models can match or exceed discriminative accuracy, particularly under data imbalance and limited labeled data
- Generative models maintain better uncertainty calibration compared to discriminative approaches
- Semi-supervised learning via Gibbs sampling is effective with generative models but not with discriminative ones
- The implicit prior in discriminative models shifts toward the empirical data distribution, causing bias under imbalance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative models provide epistemic uncertainty quantification by integrating over model parameters with respect to the posterior, while discriminative models use an implicit prior inferred from the data distribution.
- Mechanism: In the generative case, the posterior predictive distribution is constructed as an average over models with respect to the true posterior, p(θ|y₀, D), which incorporates prior knowledge and aleatoric uncertainty. In the discriminative case, the ppd uses an implicit prior inferred from D, leading to biased predictions under data imbalance.
- Core assumption: The prior distribution over parameters is explicit in the generative case but implicit and dataset-dependent in the discriminative case.
- Evidence anchors: [abstract] "discriminative models are convenient for direct posterior approximation, they lack explicit priors and thus cannot leverage prior knowledge or handle imbalanced datasets effectively"; [section 3.4] "While in the former setting this immutable marginal distribution describes prior knowledge and does not depend on D; in the latter setting, this marginal distribution is the result of an intricate interaction between the dataset D, the prior distribution Π_X0 and the DGP PY|X"
- Break condition: If the dataset D is large and representative of the true data distribution, the implicit prior in discriminative models may approximate the true prior, reducing the bias.

### Mechanism 2
- Claim: Generative models enable semi-supervised learning by leveraging unlabeled data to reduce epistemic uncertainty, while discriminative models cannot use unlabeled data effectively.
- Mechanism: In the generative case, the posterior over parameters p(θ|y₀, D, Y) depends on both labeled and unlabeled data Y through the likelihood of generating the unlabeled observations. This allows the model to infer probable parameters that explain both labeled and unlabeled data, reducing epistemic uncertainty. In the discriminative case, the posterior over parameters does not depend on unlabeled data Y, so the model cannot leverage this information.
- Core assumption: The generative model can generate both labeled and unlabeled data from the same set of parameters.
- Evidence anchors: [abstract] "generative models... can exploit both labeled and unlabeled data, enabling semi-supervised learning via Gibbs sampling"; [section 4.2] "the generative approach indeed allows for semi-supervised learning... the unlabeled data Y indeed carry information on model θ"
- Break condition: If the unlabeled data Y is not representative of the true data distribution, or if the generative model is misspecified, the semi-supervised learning may not reduce epistemic uncertainty effectively.

### Mechanism 3
- Claim: Generative models can extrapolate beyond the support of the training data, while discriminative models are limited to interpolating within the support of the training data.
- Mechanism: In the generative case, the prior distribution over parameters acts as a regularizer, constraining the predictions to a specific region of the space. This allows the model to make predictions outside the support of the training data. In the discriminative case, the implicit prior inferred from the training data shifts the predictions towards the regions well-represented in the training data, limiting the model to interpolating within the support.
- Core assumption: The prior distribution over parameters is informative and constrains the predictions in a meaningful way.
- Evidence anchors: [section 3.4] "the generative approach does not suffer from the same shortcoming and we observe that the affine generative model indeed produces a ppd which assigns high probability to the true value of x0 and approximates the true unknown posterior"; [section 3.4.1] "discriminative models indeed suffer from imbalanced dataset"
- Break condition: If the prior distribution over parameters is uninformative or misspecified, the generative model may not extrapolate effectively beyond the support of the training data.

## Foundational Learning

- Concept: Bayesian inference and posterior predictive distribution
  - Why needed here: The paper compares generative and discriminative models under the lens of Bayesian uncertainty quantification, which relies on computing the posterior predictive distribution.
  - Quick check question: What is the posterior predictive distribution, and how is it computed in the generative and discriminative cases?

- Concept: Generative and discriminative modeling approaches
  - Why needed here: The paper compares the two main approaches for learning conditional probability distributions, which differ in their construction and the structure of the underlying inference problem.
  - Quick check question: What are the key differences between generative and discriminative modeling approaches, and how do they affect the posterior predictive distribution?

- Concept: Semi-supervised learning and Gibbs sampling
  - Why needed here: The paper discusses the compatibility of generative and discriminative models with semi-supervised learning, which involves inferring a model from both labeled and unlabeled data using Gibbs sampling.
  - Quick check question: How does Gibbs sampling enable semi-supervised learning in the generative case, and why is it not effective in the discriminative case?

## Architecture Onboarding

- Component map: Labeled data + Unlabeled data -> Generative/Discriminative Model -> Posterior over parameters -> Posterior Predictive Distribution -> Predictions
- Critical path:
  1. Define generative or discriminative model with appropriate parameterization
  2. Specify prior distribution over parameters (explicit in generative, implicit in discriminative)
  3. Compute or approximate posterior over parameters given labeled data
  4. Sample from posterior predictive distribution using Gibbs sampling (generative) or direct sampling (discriminative)
  5. Evaluate performance on test data, especially under data imbalance or limited labeled data
- Design tradeoffs:
  - Generative models are more expressive and can handle data imbalance and semi-supervised learning, but are computationally more expensive and require more sophisticated structure
  - Discriminative models are simpler and more convenient, but are limited by the implicit prior and cannot handle data imbalance or semi-supervised learning effectively
- Failure signatures:
  - Generative models: Poor performance if the prior is misspecified or if the model is too complex for the data
  - Discriminative models: Poor performance under data imbalance or when extrapolating beyond the support of the training data
- First 3 experiments:
  1. Compare generative and discriminative models on a simple regression task with simulated data, evaluating performance under data imbalance and limited labeled data
  2. Implement Gibbs sampling for the generative model and compare to direct sampling for the discriminative model on a classification task with real data
  3. Extend the generative model to handle semi-supervised learning on a dataset with both labeled and unlabeled data, and compare to the discriminative model with only labeled data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the implicit prior in discriminative models quantitatively shift toward the empirical data distribution, and what is its mathematical formulation?
- Basis in paper: [explicit] The paper discusses the implicit prior in discriminative models and its shift toward the empirical data distribution.
- Why unresolved: The paper provides qualitative insights and illustrations but lacks a precise mathematical formulation for the implicit prior's shift.
- What evidence would resolve it: A mathematical model or formula that describes the shift of the implicit prior in discriminative models toward the empirical data distribution, supported by empirical validation.

### Open Question 2
- Question: Can generative models consistently outperform discriminative models in semi-supervised learning across diverse real-world datasets, and under what conditions?
- Basis in paper: [explicit] The paper suggests that generative models can leverage unlabeled data effectively, but does not provide extensive empirical validation across diverse datasets.
- Why unresolved: The paper provides limited empirical evidence, primarily using MNIST and FashionMNIST, which may not generalize to all real-world scenarios.
- What evidence would resolve it: Extensive empirical studies comparing generative and discriminative models in semi-supervised learning across a wide range of real-world datasets with varying characteristics.

### Open Question 3
- Question: What are the computational trade-offs between generative and discriminative models in terms of scalability and efficiency, especially in high-dimensional settings?
- Basis in paper: [inferred] The paper highlights the convenience of discriminative models in high-dimensional settings but does not delve into computational trade-offs.
- Why unresolved: The paper focuses on theoretical aspects and does not address practical computational considerations such as scalability and efficiency.
- What evidence would resolve it: A comparative analysis of the computational complexity, scalability, and efficiency of generative versus discriminative models in high-dimensional settings, supported by empirical benchmarks.

## Limitations
- Limited empirical validation across diverse real-world datasets, primarily focusing on MNIST and FashionMNIST
- Theoretical claims about semi-supervised learning and extrapolation lack direct experimental evidence
- Computational trade-offs and scalability considerations are not thoroughly explored

## Confidence

**High confidence**: The theoretical framework for Bayesian inference and posterior predictive distributions is well-established. The simulation results showing generative models' performance advantages under data imbalance are directly supported by experimental evidence.

**Medium confidence**: The claims about epistemic uncertainty quantification differences between generative and discriminative models are theoretically sound but rely on assumptions about model correctness and data representativeness that aren't fully validated.

**Low confidence**: The semi-supervised learning claims and extrapolation capabilities lack direct experimental validation. The corpus doesn't provide sufficient evidence that generative models can effectively leverage unlabeled data or extrapolate beyond training data support.

## Next Checks

1. **Empirical validation of prior effects**: Conduct controlled experiments comparing explicit vs. implicit prior effects on predictions under varying degrees of data imbalance, quantifying bias magnitude and uncertainty calibration differences.

2. **Semi-supervised learning performance**: Design experiments specifically testing generative models' ability to leverage unlabeled data for reducing epistemic uncertainty, comparing against discriminative baselines with limited labeled data.

3. **Extrapolation capability assessment**: Create synthetic datasets where ground truth values exist outside training data support, testing whether generative models can reliably extrapolate while discriminative models fail.