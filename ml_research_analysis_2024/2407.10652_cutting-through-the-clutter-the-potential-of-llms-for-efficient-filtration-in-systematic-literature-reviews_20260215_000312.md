---
ver: rpa2
title: 'Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration
  in Systematic Literature Reviews'
arxiv_id: '2407.10652'
source_url: https://arxiv.org/abs/2407.10652
tags:
- literature
- papers
- research
- llms
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates Large Language Models (LLMs) for automated
  filtration in systematic literature reviews, addressing the labor-intensive nature
  of manual paper screening. A structured pipeline uses multiple LLM agents to classify
  papers via title and abstract, with consensus voting ensuring high recall (98.8%)
  and reduced false negatives.
---

# Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews

## Quick Facts
- arXiv ID: 2407.10652
- Source URL: https://arxiv.org/abs/2407.10652
- Reference count: 27
- Primary result: LLM-based consensus voting achieved >98.8% recall, reducing manual screening time from weeks to minutes

## Executive Summary
This work evaluates Large Language Models (LLMs) for automated filtration in systematic literature reviews, addressing the labor-intensive nature of manual paper screening. A structured pipeline uses multiple LLM agents to classify papers via title and abstract, with consensus voting ensuring high recall (>98.8%) and reduced false negatives. Tested on 8,323 papers, commercial models like GPT-4o and Claude 3.5 Sonnet significantly accelerated filtering (weeks → minutes) while maintaining accuracy comparable to or exceeding human error thresholds. Open-source models like Llama3 showed lower precision but high recall. The approach reduces manual workload, enables scalable reviews, and highlights responsible AI-human collaboration for academic research, though challenges like bias and over-reliance remain.

## Method Summary
The authors developed a structured pipeline for LLM-based literature filtration using multiple classification agents operating on paper titles and abstracts. Papers were classified through consensus voting among different LLM models (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Flash, Llama3) to ensure high recall and minimize false negatives. The system processed an initial corpus of 8,323 papers related to "Graph Exploration in Immersive Settings," scraped from ACM Digital Library, IEEE Xplore, and Eurographics. The approach employed clear role definitions and exclusion/inclusion criteria in prompts, with results validated against ground truth to measure precision and recall.

## Key Results
- Consensus voting among multiple LLM agents achieved recall rates exceeding 98.8%, surpassing typical human error thresholds
- Filtering time reduced from weeks to minutes, processing 8,323 papers rapidly while maintaining accuracy
- Commercial models (GPT-4o, Claude 3.5 Sonnet) showed superior performance to open-source Llama3, which maintained high recall but lower precision
- Individual LLM errors were sufficiently diverse that consensus voting effectively filtered out false negatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consensus voting among multiple LLM agents reduces false negatives below human error thresholds.
- Mechanism: Different LLM models have complementary error patterns—when each model independently classifies papers, they often make different mistakes. By requiring unanimous agreement to reject a paper, most false negatives are filtered out because no single model can make the exclusion decision alone.
- Core assumption: Individual LLM errors are sufficiently diverse and uncorrelated so that consensus voting improves overall accuracy.
- Evidence anchors:
  - [abstract]: "A consensus scheme ensures recall rates >98.8%, surpassing typical human error thresholds"
  - [section]: "Regarding the erroneous inclusions (FP), for most papers, only one LLM... was responsible for the wrong classification... For extremely relevant incorrectly discarded papers (FN), where mostly individual LLMs... generated errors, but false exclusion by multiple LLMs were way lower"
  - [corpus]: Weak evidence - corpus contains related work but doesn't directly support this mechanism
- Break condition: If LLM models become too similar in architecture or training data, their errors may become correlated, reducing the effectiveness of consensus voting.

### Mechanism 2
- Claim: Using LLMs for classification prevents the fatigue and inconsistency that affect human reviewers.
- Mechanism: LLMs can process papers consistently at high speed without degradation in performance over time, whereas human reviewers experience fatigue and decreased accuracy during long screening sessions.
- Core assumption: LLMs maintain consistent performance regardless of workload volume.
- Evidence anchors:
  - [abstract]: "reducing filtering time from weeks to minutes"
  - [section]: "According to Wallace et al. [22], an experienced peer-reviewer can manually screen about two papers per minute... At this rate, a corpus of about 8,000 potentially relevant publications... requires approximately 66 person-hours... Effects like fatigue, loss of accuracy, inefficiencies... typically increase the required time frame significantly"
  - [corpus]: Weak evidence - corpus contains related work but doesn't directly support this mechanism
- Break condition: If LLMs encounter adversarial examples or edge cases they haven't been trained on, their consistency advantage may break down.

### Mechanism 3
- Claim: Structured prompts with clear role definitions and exclusion/inclusion criteria enable LLMs to make accurate relevance judgments.
- Mechanism: By explicitly defining the research direction, providing examples of relevant papers, and listing exclusion/inclusion exceptions, the LLM has sufficient context to make informed classification decisions rather than relying on ambiguous keyword matching.
- Core assumption: LLMs can understand and apply complex, nuanced criteria when properly prompted.
- Evidence anchors:
  - [abstract]: "Our open-source tool LLMSurver presents a visual interface to utilize LLMs for literature filtration, evaluate the results, and refine queries in an interactive way"
  - [section]: "In this schema, we clearly tell the LLM its context and role, its overall task, before concluding with a output format and the paper title and abstract... we also added further exclusion and inclusion criteria to the final prompt"
  - [corpus]: Weak evidence - corpus contains related work but doesn't directly support this mechanism
- Break condition: If the research domain is too complex or ambiguous for the LLM to parse from the prompt alone, accuracy will degrade.

## Foundational Learning

- Concept: Systematic Literature Review (SLR) methodology
  - Why needed here: Understanding the SLR process is essential to appreciate why automated filtration is valuable and what quality thresholds must be met
  - Quick check question: What are the key stages in a systematic literature review and why is the initial screening phase particularly labor-intensive?

- Concept: Recall vs Precision tradeoffs in information retrieval
  - Why needed here: The paper prioritizes high recall (avoiding false negatives) over perfect precision, which is critical for understanding the design choices
  - Quick check question: Why might a systematic review prioritize recall over precision, and what are the implications for manual review workload?

- Concept: Prompt engineering for LLMs
  - Why needed here: The paper's success depends heavily on how the classification task is framed and what context is provided to the LLM
  - Quick check question: What key elements should be included in a prompt to help an LLM make accurate relevance judgments for academic papers?

## Architecture Onboarding

- Component map: Initial keyword scraping -> Metadata parsing -> Duplicate removal -> Type filtering -> Multiple LLM classification agents -> Consensus voting -> Validation phase -> Output
- Critical path: The core workflow is: paper corpus -> LLM classification (multiple agents) -> consensus voting -> human validation of remaining papers
- Design tradeoffs: High recall vs high precision tradeoff (accepting more false positives to avoid false negatives), commercial vs open-source models (cost vs performance), single vs multiple LLM agents (simplicity vs robustness)
- Failure signatures: High false negative rate (missing relevant papers), inconsistent classifications across agents, LLM hallucinations introducing fake references, prompt misinterpretation leading to systematic classification errors
- First 3 experiments:
  1. Test individual LLM agents on a small validation set with known ground truth to establish baseline performance
  2. Implement and test consensus voting scheme with varying numbers of agents to find optimal balance
  3. Compare commercial vs open-source model performance on same dataset to inform cost-benefit decisions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limits of LLM-based filtration in terms of domain specificity and terminology complexity?
- Basis in paper: [inferred] The authors note that jargon and field-specific terminology may affect accuracy and suggest future work on prompt engineering and few-shot learning.
- Why unresolved: The study evaluated a single large corpus with a basic prompt format. Performance in other domains or with different prompt styles remains untested.
- What evidence would resolve it: Systematic testing of LLM filtration across multiple research domains with varying terminology complexity, using diverse prompt formats and few-shot learning approaches.

### Open Question 2
- Question: How do different LLM architectures (open-source vs. commercial) compare in terms of recall, precision, and cost-effectiveness for literature review filtration?
- Basis in paper: [explicit] The authors compare performance of Llama3 (open-source) and commercial models (GPT-4o, Claude 3.5 Sonnet) but note differences in cost and availability.
- Why unresolved: While the study provides a snapshot comparison, long-term cost-effectiveness and performance stability across different model versions and updates remain unclear.
- What evidence would resolve it: Longitudinal studies tracking model performance and costs over time, including newer model versions and architectural changes.

### Open Question 3
- Question: What are the optimal human-AI collaboration strategies for LLM-based literature review filtration?
- Basis in paper: [explicit] The authors propose a structured pipeline incorporating human oversight and consensus voting but acknowledge the need for further research on interactive feedback loops.
- Why unresolved: The study presents one approach to human-AI collaboration but does not explore alternative strategies or optimal balance between automation and human intervention.
- What evidence would resolve it: Comparative studies of different human-AI collaboration models, measuring efficiency, accuracy, and user satisfaction across various literature review tasks and team compositions.

## Limitations
- Limited domain diversity: Evaluation conducted on single SLR corpus focused on "Graph Exploration in Immersive Settings," generalizability to other domains remains uncertain
- Resource requirements for consensus voting: Using multiple LLM agents increases computational costs and API usage, potentially limiting scalability for researchers with budget constraints
- Prompt sensitivity: Paper demonstrates success with their prompt schema but doesn't extensively explore how variations in prompt engineering affect outcomes across different domains

## Confidence
- **High confidence**: The fundamental claim that LLMs can accelerate systematic literature review screening from weeks to minutes is well-supported by the timing comparison and the demonstrated ability to process 8,323 papers rapidly.
- **Medium confidence**: The claim that consensus voting achieves recall >98.8% is supported by their results, but replication across different domains and corpus sizes would strengthen this claim.
- **Medium confidence**: The assertion that open-source models like Llama3 show lower precision but high recall is supported by their comparison, though the performance gap and its implications for different use cases could be more thoroughly explored.

## Next Checks
1. **Cross-domain validation**: Test the pipeline on SLR corpora from different research domains (medical, social sciences, computer science) to assess generalizability of the >98.8% recall claim.
2. **Cost-benefit analysis**: Quantify the computational costs and API usage for consensus voting across multiple LLM agents versus single-agent approaches, including time and monetary resources.
3. **Prompt sensitivity analysis**: Systematically vary key elements of the prompt schema (role definition, exclusion criteria, inclusion examples) to determine which components are most critical for maintaining high recall.