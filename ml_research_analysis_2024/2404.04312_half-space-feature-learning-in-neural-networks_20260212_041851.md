---
ver: rpa2
title: Half-Space Feature Learning in Neural Networks
arxiv_id: '2404.04312'
source_url: https://arxiv.org/abs/2404.04312
tags:
- neural
- relu
- learning
- dlgn
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates feature learning in neural networks by
  proposing a novel framework that views networks as mixtures of simple experts. The
  authors introduce the Deep Linearly Gated Network (DLGN) as a model that sits between
  deep linear networks and ReLU networks.
---

# Half-Space Feature Learning in Neural Networks

## Quick Facts
- arXiv ID: 2404.04312
- Source URL: https://arxiv.org/abs/2404.04312
- Authors: Mahesh Lorik Yadav; Harish Guruprasad Ramaswamy; Chandrashekar Lakshminarayanan
- Reference count: 31
- Key outcome: This paper investigates feature learning in neural networks by proposing a novel framework that views networks as mixtures of simple experts.

## Executive Summary
This paper introduces Deep Linearly Gated Networks (DLGNs) as a model that bridges the gap between deep linear networks and ReLU networks. DLGNs use simpler gating functions where each feature corresponds to an indicator function for a region described as an intersection of half-spaces in the input space. The key contribution is demonstrating that DLGNs can learn non-linear features that are then linearly combined, unlike deep linear networks which can only represent linear functions. Through experiments on synthetic and real datasets, the authors show that DLGNs learn to allocate more "experts" (paths through the network) to simpler regions of the target function, supporting the idea that neural networks learn data-appropriate features.

## Method Summary
The method involves implementing DLGN and ReLU network architectures with identical 5-layer convolutional structures, global average pooling, and dense output layers. The approach uses Adam optimizer with learning rate 2e-4. Experiments include generating a synthetic dataset with points on a unit circle and frequency-varying labels, then training both networks and visualizing overlap kernels at initialization, middle, and final epochs. The method also runs CIFAR-10 experiments with ResNet-34 and ResNet-110 architectures, comparing test accuracy between DLGN and ReLU variants.

## Key Results
- DLGNs learn non-linear features through half-space intersections that are then linearly combined
- Networks allocate more paths to simpler regions of the target function during training
- DLGNs bridge the gap between NTK view (no feature learning) and hierarchical feature learning view
- The overlap kernel captures learned features and changes during training to become better suited for the task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DLGNs can learn non-linear features that are then linearly combined, unlike deep linear networks.
- Mechanism: DLGNs use a mixture of simple experts model where each expert corresponds to a path through the network. The gating function for each path is a product of indicator functions for half-spaces in the input space. During training, gradient descent allocates more experts to simpler regions of the target function, effectively learning data-appropriate features.
- Core assumption: The gating function's simpler structure compared to ReLU networks allows for clearer interpretation and analysis of feature learning dynamics.
- Evidence anchors:
  - [abstract]: "DLGNs can learn non-linear features (which are then linearly combined), and unlike ReLU networks these features are ultimately simple – each feature is effectively an indicator function for a region compactly described as an intersection of (number of layers) half-spaces in the input space."
  - [section]: "The gating model itself has a ReLU function in its definition and is thus hard to interpret or analyse. This gating model can represent complex shapes... These issues with individual gating models makes both the learning dynamics and the learned model hard to interpret."
  - [corpus]: Weak or missing evidence. The corpus neighbors discuss related topics like feature learning in ReLU networks and the NTK, but don't directly support the specific mechanism of DLGNs.

### Mechanism 2
- Claim: DLGNs bridge the gap between the NTK view (no feature learning) and the hierarchical feature learning view.
- Mechanism: DLGNs learn features during the early stages of SGD training when the loss is still high, and then linearly combine these features to learn a low-loss model in later stages. This is in contrast to the NTK view where no features are learned, and the hierarchical view where intricate hierarchical features are learned.
- Core assumption: The overlap kernel, defined as the number of paths active for both inputs, captures the learned features and changes during training to become better suited for the task.
- Evidence anchors:
  - [abstract]: "The inference that the neural tangent kernel (and hence the learned features) change during training to become better suited to the task at hand is well established."
  - [section]: "We provide a framework by means of which the dynamics of the learned features can be better studied... The overlap kernel gives us a plausible mechanism behind the change of NTK during training."
  - [corpus]: Weak or missing evidence. The corpus neighbors discuss the NTK and feature learning in ReLU networks, but don't directly support the specific mechanism of DLGNs bridging the gap between the NTK and hierarchical views.

### Mechanism 3
- Claim: The active path regions of experts/paths move preferentially to cover low-frequency/simpler regions of the target function in the input space.
- Mechanism: During training, gradient descent allocates more resources (paths) to simpler regions of the target function. This is because simpler regions are easier to fit, and allocating more paths to them reduces the overall loss faster.
- Core assumption: The target function has varying complexity across the input space, with some regions being simpler than others.
- Evidence anchors:
  - [abstract]: "We provide supporting evidence for a position that straddles the two extremes – Neural networks learn data-appropriate non-linear features in the early stages of SGD training while the loss is still high, and combine these features linearly to learn a low-loss model in the later stages."
  - [section]: "The patterns revealed by the overlap kernel show an interesting structure... In both cases, the number of paths active for the top half of the circle (corresponding to the lower frequency region for the target function) becomes much larger than the number of paths active for the other half."
  - [corpus]: Weak or missing evidence. The corpus neighbors discuss related topics like feature learning and the NTK, but don't directly support the specific mechanism of DLGNs allocating more resources to simpler regions of the target function.

## Foundational Learning

- Concept: Mixture of experts model
  - Why needed here: DLGNs are built upon the mixture of experts framework, where each expert corresponds to a path through the network. Understanding this model is crucial for grasping how DLGNs learn features.
  - Quick check question: What is the key difference between the gating model in a ReLU network and a DLGN?

- Concept: Half-spaces and convex polyhedra
  - Why needed here: In DLGNs, each feature corresponds to an indicator function for a region described as an intersection of half-spaces in the input space. Understanding half-spaces and their intersections is essential for interpreting the learned features.
  - Quick check question: How does the active path region in a DLGN differ from the activation pattern region in a ReLU network?

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: The paper discusses how DLGNs bridge the gap between the NTK view (no feature learning) and the hierarchical feature learning view. Understanding the NTK and its role in neural network training is important for appreciating the significance of DLGNs.
  - Quick check question: How does the NTK change during training, and what does this imply about feature learning in neural networks?

## Architecture Onboarding

- Component map: Input layer -> L-1 hidden layers with m nodes each -> Output layer
- Critical path: Initialize weight matrices W1, ..., WL and U1, ..., UL -> For each input x, compute the gating function f_π(x) for each path π -> Compute the expert prediction function g_π(x) for each path π -> Combine the experts: by(x) = Σ_π f_π(x)g_π(x) -> Compute the loss and update the weight matrices using gradient descent
- Design tradeoffs: Simpler gating function compared to ReLU networks allows for clearer interpretation and analysis of feature learning dynamics; Fixed number of experts (paths) may limit the model's ability to capture highly complex target functions; Linear combination of features may not be sufficient for all tasks, especially those requiring non-linear feature interactions
- Failure signatures: Poor performance on tasks requiring highly non-linear feature interactions; Inability to capture target functions with high-frequency components; Overfitting on small datasets due to the fixed number of experts
- First 3 experiments: 1. Train a DLGN on a simple 2D regression dataset with a target function that has varying complexity across the input space. Visualize the learned features using the overlap kernel and analyze how the active path regions change during training. 2. Compare the performance of a DLGN and a ReLU network on a small image classification dataset (e.g., CIFAR-10). Analyze the learned features and the dynamics of the NTK for both models. 3. Modify the gating function in a DLGN to use a more complex indicator function (e.g., a quadratic function of the input). Train the modified DLGN on the same datasets and compare its performance and feature learning dynamics with the original DLGN.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims about feature learning in DLGNs are built on simplified assumptions about the gating mechanism
- Empirical validation relies on small-scale experiments with synthetic data and standard image datasets
- Comparison between DLGNs and ReLU networks is primarily qualitative with limited quantitative analysis
- Paper does not provide detailed analysis of computational complexity and scalability of DLGNs

## Confidence

- **High**: The claim that DLGNs can learn non-linear features that are then linearly combined is supported by the mathematical framework and the experiments on synthetic data.
- **Medium**: The assertion that DLGNs bridge the gap between the NTK view and the hierarchical feature learning view is plausible based on the overlap kernel analysis, but requires further validation with more diverse datasets and architectures.
- **Low**: The specific mechanism by which DLGNs allocate more experts to simpler regions of the target function is inferred from the experiments but lacks a rigorous theoretical explanation.

## Next Checks

1. Conduct a more extensive empirical study comparing DLGNs and ReLU networks on a wider range of datasets, including those with more complex, high-dimensional input spaces and non-stationary distributions.

2. Develop a theoretical framework that provides a deeper understanding of the feature learning dynamics in DLGNs, including the conditions under which the half-space representation is sufficient and the implications of the overlap kernel for the NTK.

3. Investigate the scalability and computational efficiency of DLGNs by analyzing their performance on large-scale datasets and comparing their training and inference times with those of ReLU networks.