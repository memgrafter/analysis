---
ver: rpa2
title: 'Honest AI: Fine-Tuning "Small" Language Models to Say "I Don''t Know", and
  Reducing Hallucination in RAG'
arxiv_id: '2410.09699'
source_url: https://arxiv.org/abs/2410.09699
tags:
- question
- llms
- language
- answer
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of hallucinations in large language
  models (LLMs), which is a major obstacle for enterprise applications requiring accurate
  information. The authors propose a novel strategy called "Honest AI" to fine-tune
  small language models to say "I don't know" when uncertain, thereby reducing hallucinations.
---

# Honest AI: Fine-Tuning "Small" Language Models to Say "I Don't Know", and Reducing Hallucination in RAG

## Quick Facts
- arXiv ID: 2410.09699
- Source URL: https://arxiv.org/abs/2410.09699
- Reference count: 39
- The hybrid approach combining RAG and fine-tuning achieved the highest score in the CRAG benchmark

## Executive Summary
This paper addresses the critical problem of hallucinations in large language models, particularly for enterprise applications requiring accurate information. The authors propose "Honest AI," a novel strategy that fine-tunes small language models to say "I don't know" when uncertain, thereby reducing hallucinations. The approach combines this fine-tuning with retrieval-augmented generation (RAG) techniques, emphasizing the use of resource-efficient models with fewer than 10 billion parameters. Their solution ranked first in Task 2 for the false premise question in the CRAG competition.

## Method Summary
The method involves fine-tuning Llama-2-7B-chat using QLoRA with modified training data where challenging question types (comparison, false premise, yes/no) retain their original answers while others are replaced with "I don't know." This fine-tuned model is then combined with a hybrid RAG approach that uses Meta-Llama-3-8B-Instruct for initial domain and answer generation, with routing logic that handles movie domain questions differently due to their lower hallucination risk. The approach leverages web search and knowledge graphs for retrieval, and uses cosine similarity for document matching.

## Key Results
- RAG alone does not significantly improve performance without fine-tuning
- The hybrid approach combining RAG and fine-tuning achieved the highest score in the CRAG benchmark
- The solution ranked first in Task 2 for the false premise question in the competition
- The approach emphasizes resource efficiency by using models with fewer than 10 billion parameters

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with explicit "I don't know" examples improves model honesty in uncertain scenarios. By modifying training data to replace answers for challenging question types with "I don't know," the model learns to recognize when it lacks sufficient information. Core assumption: LLMs can learn to distinguish between easy and hard questions through pattern recognition. Break condition: If the model cannot reliably distinguish question difficulty.

### Mechanism 2
The hybrid approach combining RAG and fine-tuned model improves accuracy while reducing hallucination. The system uses RAG for domains with lower hallucination risk (like movies) and switches to the fine-tuned model for uncertain scenarios. Core assumption: Different domains have varying levels of hallucination risk. Break condition: If domain classification is inaccurate.

### Mechanism 3
Using smaller models (<10B parameters) with fine-tuning is more resource-efficient than large models for hallucination reduction. Smaller models require less computational resources while achieving comparable performance through targeted training. Core assumption: Model size and hallucination reduction are not directly proportional. Break condition: If the size limitation prevents adequate learning of uncertainty patterns.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) and its limitations
  - Why needed here: Understanding RAG is crucial for recognizing why it alone doesn't solve hallucination problems
  - Quick check question: Why does the paper claim RAG alone doesn't significantly improve performance?

- Concept: Fine-tuning techniques and QLoRA optimization
  - Why needed here: The core innovation involves fine-tuning smaller models efficiently using QLoRA
  - Quick check question: What is QLoRA and why was it chosen over full fine-tuning?

- Concept: Hallucination types and evaluation metrics
  - Why needed here: The scoring system penalizes hallucinations differently from missing answers
  - Quick check question: How does the CRAG scoring system distinguish between hallucinations and missing answers?

## Architecture Onboarding

- Component map: Question → Domain Classification → RAG Processing → Uncertainty Detection → Final Answer Generation
- Critical path: Question enters system, undergoes domain classification, RAG retrieves relevant documents, fine-tuned model detects uncertainty, hybrid decision engine generates final answer
- Design tradeoffs: Smaller model size vs. capability trade-offs, complexity of hybrid routing vs. simplicity of single approach
- Failure signatures: Domain misclassification leading to wrong routing, fine-tuned model failing to detect uncertainty, RAG retrieving irrelevant information
- First 3 experiments:
  1. Test fine-tuned model alone on simple vs. complex questions to verify uncertainty detection
  2. Test RAG system alone with different domain configurations to establish baseline performance
  3. Test hybrid approach with controlled domain inputs to verify routing logic

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of retrieved content impact the performance of RAG models across different domains? The paper discusses sensitivity to content quality but lacks comprehensive analysis across all domains.

### Open Question 2
What are the specific characteristics of questions that lead to high probabilities of hallucination in LLMs? The paper mentions specialized domains and complex reasoning but doesn't provide detailed classification.

### Open Question 3
How effective is the proposed hybrid approach in reducing hallucinations across different question types and domains? The paper describes the approach's success but lacks detailed evaluation across all scenarios.

### Open Question 4
What are the limitations of current fine-tuning techniques in addressing hallucinations, and how can they be improved? The paper discusses QLoRA's success but doesn't explore limitations or potential improvements.

### Open Question 5
How can the proposed adaptive methodology using divide-and-conquer be further developed to enhance hallucination detection and correction? The paper outlines the methodology but lacks a detailed development plan.

## Limitations

- The approach relies heavily on specific characteristics of the CRAG dataset and competition scoring system
- The fine-tuning strategy appears heuristic rather than grounded in deeper understanding of uncertainty mechanisms
- The hybrid approach's superiority is based on competition results rather than controlled experiments

## Confidence

- High confidence: RAG alone doesn't solve hallucination problems; QLoRA for efficient fine-tuning is a standard technique
- Medium confidence: Fine-tuning with "I don't know" examples improves honesty, though lacks rigorous ablation studies
- Low confidence: Hybrid approach's superiority based on competition results rather than controlled comparisons

## Next Checks

1. Conduct ablation studies comparing the fine-tuned model's performance when trained with different proportions of "I don't know" responses
2. Implement the hybrid approach with different domain classification methods to test routing logic robustness
3. Test the fine-tuned model and hybrid approach on datasets outside the CRAG competition to assess generalization capabilities