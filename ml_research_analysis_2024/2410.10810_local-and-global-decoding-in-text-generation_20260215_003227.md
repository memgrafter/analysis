---
ver: rpa2
title: Local and Global Decoding in Text Generation
arxiv_id: '2410.10810'
source_url: https://arxiv.org/abs/2410.10810
tags:
- decoding
- local
- global
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the effects of local versus global normalisation\
  \ in text generation decoding algorithms. While traditional methods like top-k and\
  \ top-\u03C0 apply local normalisation (renormalising each context independently),\
  \ this can distort the model's output distribution."
---

# Local and Global Decoding in Text Generation

## Quick Facts
- arXiv ID: 2410.10810
- Source URL: https://arxiv.org/abs/2410.10810
- Reference count: 40
- Key outcome: Global decoding performs worse than local decoding, producing shorter and more repetitive text despite preserving the model's true distribution

## Executive Summary
This paper investigates the effects of local versus global normalisation in text generation decoding algorithms. Traditional methods like top-k and top-π apply local normalisation (renormalising each context independently), which can distort the model's output distribution. The authors introduce globally-normalised versions of these algorithms and propose an independent Metropolis-Hastings method to sample from globally-normalised distributions. Experiments with Pythia language models (70m to 2.8b parameters) show that global decoding generally performs worse than local decoding as measured by MAUVE scores, and produces shorter, more repetitive text.

## Method Summary
The authors compare local and global normalisation approaches in text generation decoding. Local decoding algorithms (top-k, top-π) prune and renormalise independently at each step, while global variants prune but renormalise over the entire distribution. To sample from globally-normalised distributions without computing intractable normalisation constants, they propose an independent Metropolis-Hastings algorithm using the locally-normalised distribution as a proposal. Experiments were conducted on Pythia language models ranging from 70m to 2.8b parameters, evaluating generated text quality using MAUVE scores and diversity using self-BLEU scores.

## Key Results
- Global decoding performs worse than local decoding across all tested configurations (MAUVE scores)
- Global decoding produces shorter sequences on average compared to local decoding
- Sequences generated through global decoding show higher self-BLEU scores (more repetitive)
- The distortion introduced by local normalisation appears to be a beneficial feature rather than a bug

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local normalisation distorts the language model's distribution in a way that improves text quality
- Mechanism: By renormalising each context independently, local decoding assigns higher probabilities to longer sequences and discounts repetitive patterns, leading to more coherent and human-like text
- Core assumption: The distortion introduced by local normalisation compensates for the probability-quality paradox in language generation
- Evidence anchors:
  - [abstract] "Our results suggest that distortion is an important feature of local decoding algorithms"
  - [section 3.3] "Unlike local normalisation, global normalisation does not distort the distribution beyond the pruning process"
  - [corpus] Weak - no direct evidence in corpus papers about quality improvements from distortion

### Mechanism 2
- Claim: Global decoding produces shorter, less diverse text due to probability compounding over sequence length
- Mechanism: Since global normalisation preserves the original distribution, longer sequences have exponentially lower probabilities. This leads to shorter, more repetitive outputs
- Core assumption: Language models inherently assign lower probabilities to longer sequences, and global decoding preserves this bias
- Evidence anchors:
  - [section 7] "sequences generated through global decoding tend to be shorter on average"
  - [section 7] "As probabilities compound multiplicatively over the length of a sequence...the overall probability of longer sequences decreases exponentially"
  - [corpus] Weak - no direct evidence in corpus papers about sequence length differences

### Mechanism 3
- Claim: Independent Metropolis-Hastings (IMH) provides a practical way to approximate global decoding without computing intractable normalisation constants
- Mechanism: IMH uses the locally-normalised distribution as a proposal distribution and iteratively accepts/rejects samples to approximate the globally-normalised distribution
- Core assumption: The locally-normalised distribution is a reasonable approximation of the globally-normalised distribution for the IMH proposal
- Evidence anchors:
  - [section 4] "we propose an independent Metropolis-Hastings algorithm to approximate sampling from globally-normalised distributions"
  - [section 4] "we instead use its locally normalised version as the proposal distribution: pimh(w) = pα(w)"
  - [corpus] Weak - no direct evidence in corpus papers about IMH usage in text generation

## Foundational Learning

- Concept: Language model probability distributions and autoregressive structure
  - Why needed here: Understanding how language models define probabilities over strings is crucial for grasping the difference between local and global normalisation
  - Quick check question: How does an autoregressive language model define the probability of a string w = w1, w2, ..., w|w|?

- Concept: Normalisation in probability distributions
  - Why needed here: Normalisation ensures that probability distributions sum to 1, and understanding local vs. global normalisation is key to the paper's contribution
  - Quick check question: What is the difference between local normalisation (normalising each context independently) and global normalisation (normalising the entire distribution)?

- Concept: Markov chain Monte Carlo methods, specifically Independent Metropolis-Hastings
  - Why needed here: IMH is the proposed method for sampling from globally-normalised distributions without computing intractable normalisation constants
  - Quick check question: How does the Independent Metropolis-Hastings algorithm work, and why is it suitable for approximating global decoding?

## Architecture Onboarding

- Component map:
  - Language Model (Pythia) -> Local Decoding Algorithms (top-k, top-π) -> IMH Sampler -> Evaluation Metrics (MAUVE, self-BLEU)

- Critical path:
  1. Sample from language model using local decoding (pα)
  2. Use IMH with pα as proposal to approximate global decoding (pγ)
  3. Evaluate generated text using MAUVE and self-BLEU

- Design tradeoffs:
  - Local vs. Global Normalisation: Local provides better text quality but distorts the distribution; global preserves the distribution but may produce lower quality text
  - IMH Iterations: More iterations improve approximation of pγ but increase computational cost
  - Pruning Strategy: Top-k vs. top-π affects the level of distortion and the minimum probability retained

- Failure signatures:
  - If global decoding consistently outperforms local decoding, the core claim about distortion being beneficial may be incorrect
  - If IMH fails to converge or produces samples that don't match pγ, the approximation method may be flawed
  - If MAUVE scores don't correlate with human judgments of text quality, the evaluation metric may be inadequate

- First 3 experiments:
  1. Implement local and global versions of top-k and top-π decoding on a small Pythia model and compare MAUVE scores
  2. Measure sequence lengths and self-BLEU scores for local vs. global decoding to confirm the shorter, more repetitive nature of global outputs
  3. Run IMH with varying numbers of iterations and compare the resulting MAUVE scores to assess convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of local normalisation distortion improve text generation quality, and can these be isolated and enhanced?
- Basis in paper: [explicit] The authors find that local decoding performs better than global decoding despite global normalisation preserving distribution integrity, suggesting distortion is a beneficial feature
- Why unresolved: The paper demonstrates the effect but doesn't identify which specific distortion mechanisms are responsible for the improvement
- What evidence would resolve it: Systematic ablation studies comparing different types of local normalisation distortions (e.g., varying the degree of pruning, different renormalisation strategies) while controlling for other variables

### Open Question 2
- Question: How do the quality benefits of local normalisation distortion scale with model size and capability?
- Basis in paper: [inferred] Experiments were limited to models ranging from 70m to 2.8b parameters, leaving open whether larger models would show similar patterns
- Why unresolved: The authors note this as a limitation, having only tested on relatively small models due to computational constraints
- What evidence would resolve it: Testing the same decoding algorithms on frontier models (e.g., GPT-4 class) to determine if the observed benefits persist at larger scales

### Open Question 3
- Question: Can we design decoding algorithms that preserve the beneficial aspects of local normalisation distortion while mitigating its negative effects (shorter text, repetition)?
- Basis in paper: [explicit] The authors note that global decoding produces shorter, more repetitive text while local decoding produces longer, more coherent text, suggesting a trade-off
- Why unresolved: The paper identifies the trade-off but doesn't propose solutions to balance these competing effects
- What evidence would resolve it: Development and evaluation of hybrid decoding approaches that combine elements of both local and global normalisation to achieve optimal text quality across multiple dimensions

## Limitations

- The core claim relies on MAUVE scores which measure distributional similarity but don't directly assess semantic coherence or factual accuracy
- Experiments are limited to Pythia models and WebText dataset, limiting generalizability to other model architectures and domains
- IMH approximation introduces computational overhead and potential sampling bias that isn't thoroughly examined

## Confidence

- High confidence: Local decoding algorithms (top-k, top-π) consistently outperform global variants in MAUVE scores across all tested configurations
- Medium confidence: The relationship between sequence length, repetition, and decoding strategy is robust, though the underlying causes could be more deeply investigated
- Medium confidence: IMH provides a viable approximation for global decoding sampling, but the optimal number of iterations and convergence properties need further validation

## Next Checks

1. Conduct human evaluation studies to verify whether MAUVE score improvements correlate with perceived text quality, particularly for long-form generation tasks where global decoding's tendency toward shorter outputs might be advantageous

2. Test the decoding strategies across multiple model architectures (GPT, BERT, T5 families) and diverse datasets to assess the robustness of findings beyond the Pythia/WebText combination

3. Perform ablation studies on the IMH algorithm to determine optimal iteration counts and identify conditions where the approximation breaks down, including measuring actual acceptance rates and computational overhead compared to local decoding