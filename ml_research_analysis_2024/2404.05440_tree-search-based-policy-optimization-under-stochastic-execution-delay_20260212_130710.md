---
ver: rpa2
title: Tree Search-Based Policy Optimization under Stochastic Execution Delay
arxiv_id: '2404.05440'
source_url: https://arxiv.org/abs/2404.05440
tags:
- delay
- delays
- stochastic
- policy
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles reinforcement learning in environments with
  stochastic execution delays, where the agent's actions are executed after a random
  delay. The authors propose a new formalism called stochastic delayed execution MDPs
  (SED-MDPs) that models random delays as a separate process from the MDP dynamics.
---

# Tree Search-Based Policy Optimization under Stochastic Execution Delay

## Quick Facts
- arXiv ID: 2404.05440
- Source URL: https://arxiv.org/abs/2404.05440
- Reference count: 40
- Primary result: DEZ achieves higher average scores on 39/45 constant delay and 42/45 stochastic delay experiments compared to baselines

## Executive Summary
This paper addresses reinforcement learning in environments with stochastic execution delays, where agent actions are executed after random delays. The authors propose a new formalism called stochastic delayed execution MDPs (SED-MDPs) that models random delays as a separate process from the MDP dynamics. They prove that, given observed delay values, it suffices to restrict policy search to Markov policies to achieve optimal performance. Based on this theoretical insight, they devise Delayed EfficientZero (DEZ), a model-based algorithm that leverages Monte Carlo tree search to infer future states from the action queue and make decisions accordingly.

## Method Summary
The paper tackles reinforcement learning with stochastic execution delays by introducing SED-MDPs and DEZ. The key insight is that when delays are observable and independent of MDP dynamics, Markov policies suffice for optimality. DEZ extends EfficientZero by maintaining action and delay queues, resolving pending actions at each step, and using the learned forward model to predict future states based on expected pending actions. This allows MCTS to plan as if actions were executed immediately. The algorithm is evaluated on the Atari suite with both constant and stochastic delays, showing significant performance improvements over baselines while preserving EfficientZero's sample efficiency.

## Key Results
- DEZ achieves higher average scores on 39 out of 45 experiments with constant delays compared to Oblivious EfficientZero baseline
- DEZ achieves higher average scores on 42 out of 45 experiments with stochastic delays compared to Oblivious EfficientZero baseline
- DEZ demonstrates robustness to very large delay values in extreme scenarios
- The algorithm requires approximately 130K interactions, only slightly more than EfficientZero's 100K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Observing delay values allows restriction of policy search to Markov policies without loss of optimality.
- Mechanism: The delay process is independent of the MDP dynamics and observable in real time. Given the observed delays, the effective decision time at each step is deterministic given the history, so the policy only needs to depend on the current state, not the full history.
- Core assumption: Delay process is independent of MDP dynamics and observable; delays are known at each step.
- Evidence anchors:
  - [abstract] "We show that given observed delay values, it is sufficient to perform a policy search in the class of Markov policies in order to reach optimal performance"
  - [section 4] Theorem 4.2 proves existence of a Markov policy yielding the same process distribution as any history-dependent policy when delays are observed.
  - [corpus] No direct corpus evidence; claim is supported by the paper's own theorems.
- Break condition: If delays are not observable or the delay process depends on MDP states/actions, the Markov policy sufficiency may fail.

### Mechanism 2
- Claim: DEZ accurately infers future states from the action queue using the learned forward model, enabling planning under delay.
- Mechanism: DEZ maintains action and delay queues, resolves pending actions at each step, and uses the learned dynamics model to predict future states (ˆst+zt) based on expected pending actions. This allows MCTS to plan as if actions were executed immediately.
- Core assumption: The forward model learned by EfficientZero's dynamics network is accurate enough in the latent space to support reliable multi-step predictions under delay.
- Evidence anchors:
  - [section 5] "DEZ keeps track of past actions and their delays using two separate queues... It utilizes these queues to infer future states and make decisions accordingly."
  - [section 6] "To make accurate state predictions, we embrace the state representation network... and the dynamics network... The output of this search... also serves as the policy target in the loss function."
  - [corpus] Limited; the claim is supported by experimental results in the paper.
- Break condition: If the forward model becomes inaccurate due to compounding prediction errors over longer delays, state inference and planning will degrade.

### Mechanism 3
- Claim: DEZ preserves EfficientZero's sample efficiency while handling stochastic delays.
- Mechanism: DEZ leverages EfficientZero's architectural framework (representation learning, dynamics model, MCTS planning) but modifies the action selection and replay buffer to account for delays. This avoids the exponential complexity of state augmentation.
- Core assumption: EfficientZero's sample efficiency transfers to the delayed setting when the model is adapted to account for delays.
- Evidence anchors:
  - [abstract] "DEZ builds upon the sample efficiency of EfficientZero while handling delayed execution"
  - [section 5] "Drawing inspiration from the recent achievements of EfficientZero... we use its architectural framework to infer future states with high accuracy"
  - [section 6] "Although our approach significantly benefits from EfficientZero’s sample efficiency, the presence of delay adds complexity... requiring mildly more interactions – 130K in our case."
  - [corpus] No direct corpus evidence; claim is supported by the paper's own experimental results.
- Break condition: If the delay complexity overwhelms the sample efficiency gains, performance may degrade compared to state augmentation methods.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper extends the standard MDP framework to handle stochastic delays, so understanding MDPs is fundamental.
  - Quick check question: What are the key components of an MDP tuple (S, A, P, r, μ, γ)?

- Concept: State augmentation for handling delays
  - Why needed here: The paper contrasts its approach with state augmentation, which is a common method for handling delays but has exponential complexity.
  - Quick check question: How does state augmentation work for handling delays, and why does its complexity grow exponentially with delay?

- Concept: Monte Carlo Tree Search (MCTS) and model-based RL
  - Why needed here: DEZ uses MCTS similar to EfficientZero for planning, so understanding these concepts is crucial.
  - Quick check question: How does MCTS work in model-based RL algorithms like EfficientZero, and how does it differ from model-free approaches?

## Architecture Onboarding

- Component map:
  - Action queue: Stores past actions [at-M, ..., at-1]
  - Delay queue: Stores past delay values [zt-M, ..., zt-1]
  - Forward model (G): Predicts next state given current state and action
  - Representation network (H): Encodes observations into latent states
  - MCTS planner: Uses predicted future states for decision-making
  - Replay buffer: Stores transitions (st, aτt, rt) adjusted for effective decision times

- Critical path:
  1. Observe current state st and delay zt
  2. Resolve pending actions using queues and effective decision time formula
  3. Predict future state ˆst+zt using forward model and pending actions
  4. Run MCTS from ˆst+zt to select action at
  5. Update action and delay queues with at and zt
  6. Store transition (st, aτt, rt) in replay buffer (adjusted for effective decision time)

- Design tradeoffs:
  - Accuracy vs. computational cost: More accurate forward model predictions enable better planning but require more computation.
  - Memory vs. delay handling: Storing action and delay queues allows handling stochastic delays but increases memory usage.
  - Sample efficiency vs. robustness: EfficientZero's architecture provides sample efficiency but may be less robust to large prediction errors under long delays.

- Failure signatures:
  - Performance degrades with increasing delay values due to compounding prediction errors.
  - Training instability if the forward model cannot accurately predict future states.
  - Suboptimal performance if the effective decision time calculation is incorrect.

- First 3 experiments:
  1. Test DEZ on a simple environment with constant delay to verify basic functionality.
  2. Test DEZ on an environment with stochastic delays to validate delay handling and state inference.
  3. Compare DEZ's performance with and without delay observation to confirm the importance of delay observability.

## Open Questions the Paper Calls Out
None

## Limitations

- Theoretical Scope: The proof that Markov policies suffice under observed delays assumes delays are independent of MDP dynamics. The paper doesn't fully explore cases where this assumption breaks (e.g., delays correlated with agent actions or environmental states).
- Model Accuracy Dependence: DEZ's performance critically depends on the forward model's accuracy for multi-step predictions. The paper shows success but doesn't provide extensive ablation studies on model quality versus performance.
- Computational Overhead: While the paper mentions "mildly more interactions" than EfficientZero, it doesn't comprehensively quantify the computational overhead of maintaining action/delay queues and running MCTS with predicted states.

## Confidence

- High Confidence: The theoretical result that observed delays allow restriction to Markov policies (Theorem 4.2 is clearly stated and proven within the paper's assumptions).
- Medium Confidence: Experimental results showing DEZ outperforms baselines, though limited to 15 Atari games and specific delay configurations.
- Low Confidence: Claims about DEZ's sample efficiency preservation without direct comparison to state augmentation methods on identical metrics.

## Next Checks

1. **Model Robustness Test**: Systematically vary the forward model's prediction horizon and measure performance degradation to establish the maximum reliable delay DEZ can handle.

2. **Baseline Comparison**: Implement and compare against a state-augmentation baseline with varying levels of approximation to directly validate the claimed computational advantage.

3. **Delay Correlation Stress Test**: Modify the delay process to be correlated with agent actions or environmental states to test the robustness of the Markov policy sufficiency claim beyond the paper's assumptions.