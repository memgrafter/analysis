---
ver: rpa2
title: 'DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts'
arxiv_id: '2412.10510'
source_url: https://arxiv.org/abs/2412.10510
tags:
- defame
- claim
- fact-checking
- evidence
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DEFAME is a zero-shot, modular pipeline for multimodal fact-checking\
  \ that combines a multimodal LLM with dynamic retrieval tools\u2014web search, image\
  \ search, reverse image search, and geolocation\u2014to verify claims involving\
  \ both text and images. Unlike prior work limited to text-only claims or gold evidence,\
  \ DEFAME retrieves multimodal evidence in real time and generates structured, human-readable\
  \ reports."
---

# DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts

## Quick Facts
- **arXiv ID**: 2412.10510
- **Source URL**: https://arxiv.org/abs/2412.10510
- **Reference count**: 40
- **Key outcome**: New state-of-the-art accuracy (70.5%, 59.2%, 83.9%) on multimodal fact-checking benchmarks, outperforming GPT-4O baselines by up to 32.5 percentage points

## Executive Summary
DEFAME is a zero-shot, modular pipeline for multimodal fact-checking that combines a multimodal LLM with dynamic retrieval tools—web search, image search, reverse image search, and geolocation—to verify claims involving both text and images. Unlike prior work limited to text-only claims or gold evidence, DEFAME retrieves multimodal evidence in real time and generates structured, human-readable reports. Evaluated on AVERI TEC, MOCHEG, and VERITE, it establishes new state-of-the-art accuracy: 70.5%, 59.2%, and 83.9% respectively. A new benchmark, CLAIM REVIEW 2024+, contains post-GPT-4O claims and shows DEFAME outperforms GPT-4O baselines by up to 32.5 percentage points, demonstrating robustness to temporal knowledge gaps. Human evaluation confirms its reports are more complete and better justified than baseline explanations.

## Method Summary
DEFAME implements a six-stage iterative process using a multimodal LLM backbone (GPT-4O) with zero-shot in-context learning. The pipeline dynamically selects from four external tools—web search, image search, reverse image search, and geolocation—based on claim characteristics. Each claim proceeds through Plan Actions, Execute Actions, Summarize Results, Develop Fact-Check, Judge Verdict, and Justify Verdict stages. The system retrieves real-time multimodal evidence rather than relying on gold evidence, generating structured reports that integrate findings from multiple sources. The approach is fully modular, allowing tool interchangeability and adaptation to different LLM backbones.

## Key Results
- Achieves 70.5% accuracy on AVERI TEC, 59.2% on MOCHEG, and 83.9% on VERITE benchmarks
- Outperforms GPT-4O baselines by up to 32.5 percentage points on CLAIM REVIEW 2024+ benchmark
- Human evaluation shows DEFAME reports are more complete and better justified than baseline explanations
- Ablation study demonstrates significant contributions from individual components (web search, image search, reverse image search, geolocation)

## Why This Works (Mechanism)
The system's effectiveness stems from its dynamic evidence retrieval approach, which addresses the fundamental limitation of prior work that relied on static gold evidence. By combining multiple specialized tools—web search for textual context, image search for visual verification, reverse image search for source tracking, and geolocation for spatial validation—DEFAME can handle the full spectrum of multimodal claims. The zero-shot design eliminates the need for task-specific training data, while the modular architecture allows adaptation to different claim types and LLM capabilities.

## Foundational Learning
- **Multimodal LLM integration**: Understanding how GPT-4O processes combined text-image inputs is crucial for effective pipeline design and prompt engineering
- **Dynamic tool selection**: The system must learn when and how to invoke different retrieval tools based on claim characteristics and available evidence
- **Zero-shot prompting**: Success depends on crafting effective in-context examples and prompt templates without task-specific training data
- **Evidence synthesis**: The pipeline must integrate diverse evidence types (textual, visual, spatial) into coherent fact-checking reports
- **API orchestration**: Coordinating multiple external services (search engines, vision APIs, geolocation tools) requires robust error handling and rate limiting
- **Human evaluation metrics**: Understanding how completeness and justification quality are assessed provides insight into system requirements

## Architecture Onboarding

**Component Map**: Plan Actions -> Execute Actions -> Summarize Results -> Develop Fact-Check -> Judge Verdict -> Justify Verdict

**Critical Path**: The pipeline follows a sequential flow where each stage builds upon the previous one. Plan Actions determines which tools to use, Execute Actions retrieves evidence, Summarize Results consolidates findings, Develop Fact-Check formulates the verification approach, Judge Verdict makes the final determination, and Justify Verdict provides the explanatory report.

**Design Tradeoffs**: The zero-shot approach maximizes flexibility but may sacrifice accuracy on edge cases compared to fine-tuned models. Dynamic tool selection optimizes resource usage but introduces latency and potential API dependency issues. The modular design enables adaptation but requires careful coordination between components.

**Failure Signatures**: Common failures include tool execution timeouts, insufficient evidence retrieval leading to incorrect verdicts, and API rate limiting. The system may also struggle with claims requiring specialized domain knowledge not captured by general web search.

**First Experiments**:
1. Test the Plan Actions stage with diverse claim types to verify appropriate tool selection
2. Validate Execute Actions by checking API response quality and handling of edge cases
3. Evaluate Summarize Results integration of evidence from multiple sources

## Open Questions the Paper Calls Out

**Open Question 1**: How can DEFAME's reliance on external evidence sources be mitigated to reduce the risk of incorporating unreliable information? The paper acknowledges that search results may include leaked or biased content, and while some approaches assess source credibility using third-party ratings, DEFAME lacks a concrete solution for incorporating this into its framework.

**Open Question 2**: What are the specific limitations of open-source MLLMs like LLAVA-1V and LLAMA 4 SCOUT compared to GPT-4O in the context of DEFAME? While the paper shows GPT-4O outperformed open-source models, it doesn't provide detailed analysis of specific limitations such as action formatting or processing speed differences.

**Open Question 3**: How can DEFAME be scaled up to handle the massive volume of claims posted on social media platforms? The paper mentions potential use for social media platforms but acknowledges that DEFAME could become expensive depending on claim volume, without providing concrete scaling solutions.

## Limitations

- Complete absence of published implementation details, including prompt templates and API configurations
- Reliance on external APIs introduces potential brittleness and scalability constraints
- Zero-shot approach may limit handling of domain-specific verification challenges
- Unusually large performance improvements over baselines raise questions about implementation sensitivity

## Confidence

**High confidence**: Core architectural framework and evaluation methodology are clearly specified and internally consistent
**Medium confidence**: Performance numbers appear internally consistent but unusually large improvement over baselines is concerning
**Low confidence**: Complete absence of implementation details prevents verification of reproducibility

## Next Checks

1. **Reimplementation validation**: Attempt to recreate DEFAME pipeline using only high-level specifications, documenting missing critical details
2. **Baseline comparison refinement**: Re-run GPT-4O baseline on CLAIM REVIEW 2024+ with multiple prompt variants to verify 32.5 percentage point gap
3. **Tool dependency analysis**: Systematically vary configuration parameters for each external tool to assess performance sensitivity