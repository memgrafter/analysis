---
ver: rpa2
title: 'Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation
  through Simultaneous Decoding'
arxiv_id: '2404.14600'
source_url: https://arxiv.org/abs/2404.14600
tags:
- retrieval
- https
- decoding
- generative
- semanticscholar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the inefficiency of constrained beam search
  in generative retrieval models, which can lead to relevant document identifiers
  being pruned due to local optima. To mitigate this, the authors propose PAG, a novel
  approach that guides autoregressive generation through simultaneous decoding using
  two types of document identifiers: set-based (lexical tokens) and sequential (quantized
  relevance-based representations).'
---

# Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation through Simultaneous Decoding

## Quick Facts
- arXiv ID: 2404.14600
- Source URL: https://arxiv.org/abs/2404.14600
- Reference count: 40
- Authors: Hansi Zeng; Chen Luo; Hamed Zamani
- Key outcome: PAG achieves 15.6% MRR improvement on MSMARCO and 22x speedup in query latency with a 10x smaller beam size compared to state-of-the-art generative retrieval models.

## Executive Summary
This paper addresses the inefficiency of constrained beam search in generative retrieval models, where relevant document identifiers can be pruned due to local optima. The authors propose PAG (Planning-Ahead Generative Retrieval), which guides autoregressive generation through simultaneous decoding using two types of document identifiers: set-based (lexical tokens) and sequential (quantized relevance-based representations). PAG first performs simultaneous decoding on the set-based identifiers to approximate document-level scores, then conditions the autoregressive generation on these scores to reduce the likelihood of pruning relevant prefixes. Extensive experiments on MSMARCO and TREC Deep Learning Track datasets show that PAG significantly outperforms the state-of-the-art generative retrieval model while using a smaller beam size and achieving substantial speedup.

## Method Summary
PAG constructs set-based and sequential document identifiers to capture lexical and semantic information respectively. The set-based DocID is built from the top-m most important lexical tokens of each document, while the sequential DocID is obtained via quantization of relevance-based document representations. PAG performs simultaneous decoding on the set-based identifiers to approximate document-level scores, then uses these scores to guide autoregressive generation through planning-ahead constrained beam search. The model is trained using a three-stage pipeline: first training separate models for set-based and sequential DocIDs, then jointly training a unified model on both types of identifiers. The approach is implemented using a T5-base encoder-decoder backbone.

## Key Results
- Achieves 15.6% MRR improvement on MSMARCO passage retrieval benchmark compared to state-of-the-art generative retrieval models
- Provides 22x speedup in query latency while using a 10x smaller beam size (8 vs 64)
- Shows significant performance improvements on TREC Deep Learning Track 2019/2020 datasets with NDCG@10 gains of 4.6% and 5.3% respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Planning-ahead constrained beam search reduces the pruning of relevant prefixes by conditioning on document-level scores from simultaneous decoding.
- Mechanism: Instead of expanding prefixes based solely on the next token score, the scoring function now incorporates the maximum simultaneous relevance score achievable from any document sharing that prefix. This guides the search toward prefixes that can lead to high relevance scores once decoding is complete.
- Core assumption: Local greedy decisions in beam search cause many relevant document identifiers to be pruned early, and conditioning on document-level scores can mitigate this.
- Evidence anchors: [abstract] "motivated by the bag-of-words assumption in information retrieval, the set-based identifier is built on lexical tokens" and [section] "According to Equation (3), which is used in state-of-the-art generative retrieval models, the document ID prefixes are expanded solely based on the contribution by the next token score."
- Break condition: If the simultaneous relevance scores are poorly correlated with true relevance, the guidance may lead beam search astray.

### Mechanism 2
- Claim: Combining lexical (set-based) and semantic (sequential) document identifiers improves retrieval performance by capturing complementary information.
- Mechanism: The set-based DocID is constructed from the top-m most important lexical tokens of each document, while the sequential DocID is obtained via quantization of relevance-based document representations. The model jointly predicts both types of DocIDs and combines their relevance scores.
- Core assumption: Lexical information and semantic information are complementary for retrieval tasks.
- Evidence anchors: [abstract] "Motivated by the bag-of-words assumption in information retrieval, the set-based identifier is built on lexical tokens. The sequential identifier, on the other hand, is obtained via quantizing relevance-based representations of documents."
- Break condition: If the corpus contains many documents with similar lexical content but different semantics, the set-based DocID may not provide enough discriminative power.

### Mechanism 3
- Claim: The three-stage training pipeline gradually adapts the model to joint prediction of set-based and sequential DocIDs, improving final performance.
- Mechanism: First, two separate models are trained for set-based and sequential DocIDs respectively. Then, a unified model is initialized by averaging the weights of these two models and trained jointly on both types of DocIDs.
- Core assumption: Gradual adaptation from separate to joint modeling helps the model learn to effectively combine the two types of identifiers.
- Evidence anchors: [section] "The whole optimization process consists of three stages, the first two of which can be trained in parallel. The first two stages are applied to make generative retrieval model capable of predicting set-based DocIDs and sequential DocIDs, respectively."
- Break condition: If the two separate models have very different architectures or learning dynamics, simply averaging their weights may not provide a good initialization for joint training.

## Foundational Learning

- Concept: Bag-of-words assumption in information retrieval
  - Why needed here: The set-based DocID construction relies on selecting the most important lexical tokens from each document, which is based on the bag-of-words assumption.
  - Quick check question: What is the bag-of-words assumption, and how does it differ from the assumption made by semantic (dense) retrieval models?

- Concept: Autoregressive generation and constrained beam search
  - Why needed here: The paper proposes a new decoding method that guides autoregressive generation using simultaneous decoding scores.
  - Quick check question: How does constrained beam search work in the context of generative retrieval, and what are its limitations?

- Concept: Residual quantization for vector compression
  - Why needed here: The sequential DocID is constructed by applying residual quantization to the relevance-based document representations.
  - Quick check question: What is residual quantization, and how does it differ from other vector quantization methods?

## Architecture Onboarding

- Component map: Query → Encoder → Simultaneous decoding → Document-level scores → Autoregressive decoding (guided by simultaneous scores) → DocID generation → Document retrieval

- Critical path: Query → Encoder → Simultaneous decoding → Document-level scores → Autoregressive decoding (guided by simultaneous scores) → DocID generation → Document retrieval

- Design tradeoffs:
  - Set-based DocID size (m) vs. index memory and retrieval effectiveness
  - Beam size vs. query latency and retrieval effectiveness
  - Sequential DocID length (L) vs. expressiveness and computational cost

- Failure signatures:
  - Low recall even with large beam sizes may indicate ineffective simultaneous decoding
  - High latency may indicate inefficient implementation of the planning-ahead beam search
  - Performance degradation when removing either DocID type may indicate poor complementarity

- First 3 experiments:
  1. Compare retrieval effectiveness with and without the planning-ahead beam search on a small dataset
  2. Analyze the impact of set-based DocID size (m) on retrieval effectiveness and index memory
  3. Measure query latency with different beam sizes and compare against the theoretical computational cost analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PAG scale with increasing corpus size, particularly in billion-scale datasets, and what approximation techniques could be employed to maintain efficiency?
- Basis in paper: [inferred] The paper discusses the computational cost of PAG's decoding methods and mentions that for billion-scale collections, simultaneous decoding might become slower, suggesting the need for approximation techniques.
- Why unresolved: The experiments in the paper are conducted on a million-scale dataset (MSMARCO with 8.8 million passages), so the performance and efficiency of PAG on larger datasets remain unexplored.
- What evidence would resolve it: Experiments evaluating PAG's performance and efficiency on billion-scale datasets, along with an analysis of different approximation techniques to maintain its effectiveness and speed.

### Open Question 2
- Question: Can PAG be effectively integrated into other knowledge-intensive tasks beyond information retrieval, such as open-domain question answering or personalized generation, and what modifications would be necessary?
- Basis in paper: [explicit] The paper mentions the potential of integrating PAG into knowledge-intensive text generation and personalized generation tasks, but does not provide specific details or experiments.
- Why unresolved: The paper focuses on evaluating PAG's performance in information retrieval tasks, leaving the exploration of its applicability to other knowledge-intensive tasks for future work.
- What evidence would resolve it: Experiments demonstrating PAG's effectiveness in open-domain question answering, personalized generation, or other knowledge-intensive tasks, along with an analysis of the necessary modifications to adapt PAG for these applications.

### Open Question 3
- Question: How does the choice of set-based and sequential DocID construction methods impact PAG's performance, and are there alternative approaches that could lead to further improvements?
- Basis in paper: [inferred] The paper describes the specific methods used for constructing set-based and sequential DocIDs in PAG, but does not explore alternative approaches or conduct a comprehensive comparison of different methods.
- Why unresolved: The paper focuses on a specific approach for DocID construction, leaving the exploration of alternative methods and their impact on PAG's performance for future work.
- What evidence would resolve it: Experiments comparing the performance of PAG using different set-based and sequential DocID construction methods, along with an analysis of the strengths and weaknesses of each approach.

## Limitations

- The simultaneous decoding mechanism relies on the assumption that bag-of-words representations capture sufficient document-level relevance information, which may not hold for all query types or document collections.
- The computational cost analysis shows theoretical improvements, but practical implementation details and real-world performance characteristics remain unclear.
- The three-stage training pipeline introduces additional complexity, and the specific hyperparameters and optimization strategies used are not fully detailed in the paper.

## Confidence

- High Confidence: The core mechanism of using simultaneous decoding scores to guide autoregressive generation is well-defined and theoretically sound. The experimental results on MSMARCO and TREC datasets provide strong evidence for the effectiveness of PAG.
- Medium Confidence: The claim about 22x speedup in query latency is based on theoretical computational cost analysis rather than measured experimental results. The practical implementation details that would enable such speedups are not fully specified.
- Low Confidence: The generalization capability of PAG across different domains and languages is not thoroughly evaluated. The paper focuses primarily on English passage retrieval, and the effectiveness of the approach for other types of documents or languages remains uncertain.

## Next Checks

1. **Implementation Verification:** Implement the PAG framework with the specified hyperparameters (m=20, beam size=8, sequential DocID length=10) on the MSMARCO passage retrieval dataset. Compare the MRR@10 results with the reported 15.6% improvement over state-of-the-art generative retrieval models. Measure the actual query latency and compare it with the theoretical 22x speedup claim.

2. **Ablation Study:** Conduct a comprehensive ablation study to evaluate the individual contributions of the set-based and sequential DocIDs, as well as the planning-ahead constrained beam search. Remove each component systematically and measure the impact on retrieval effectiveness (MRR@10, NDCG@10) and efficiency (query latency).

3. **Cross-Dataset Evaluation:** Evaluate PAG on additional retrieval datasets beyond MSMARCO and TREC Deep Learning Track, such as TREC-COVID or Robust04. Assess the model's performance across different document types (news articles, web pages, scientific papers) and query characteristics (informational, navigational, transactional) to validate its generalization capability.