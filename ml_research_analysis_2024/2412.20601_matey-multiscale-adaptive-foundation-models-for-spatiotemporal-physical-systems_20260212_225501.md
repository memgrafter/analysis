---
ver: rpa2
title: 'MATEY: multiscale adaptive foundation models for spatiotemporal physical systems'
arxiv_id: '2412.20601'
source_url: https://arxiv.org/abs/2412.20601
tags:
- attention
- training
- fine-tuning
- patch
- svit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MATEY, a multiscale adaptive foundation model
  for spatiotemporal physical systems. The authors address the computational challenge
  of handling extremely long token sequences required for accurate representation
  of multiscale features in physical systems using vision transformers.
---

# MATEY: multiscale adaptive foundation models for spatiotemporal physical systems

## Quick Facts
- arXiv ID: 2412.20601
- Source URL: https://arxiv.org/abs/2412.20601
- Reference count: 32
- This paper introduces MATEY, a multiscale adaptive foundation model that reduces computational cost for spatiotemporal physical systems through dynamic patch size adjustment and efficient attention mechanisms.

## Executive Summary
MATEY addresses the computational challenge of handling extremely long token sequences required for accurate representation of multiscale features in physical systems using vision transformers. The authors introduce two adaptive tokenization schemes that dynamically adjust patch sizes based on local feature complexity, significantly reducing computational cost while maintaining or improving accuracy. The model also evaluates different spatiotemporal attention mechanisms, finding that fully decoupled axial attention requires more training time and model weights to achieve the same accuracy as other approaches. Through experiments on PDEBench data and two fine-tuning tasks (colliding thermals and magnetohydrodynamics), MATEY demonstrates superior performance, particularly in low-data regimes.

## Method Summary
MATEY is a multiscale adaptive foundation model for spatiotemporal physical systems that extends Vision Transformer architecture with adaptive tokenization and spatiotemporal attention mechanisms. The model employs two adaptive tokenization schemes that dynamically adjust patch sizes based on local feature complexity, starting with coarse patches and refining only those with high variance. Three attention schemes are evaluated: standard ViT with quadratic complexity, Spatiotemporal ViT (SViT) with decoupled temporal and spatial attention, and Axial ViT (A ViT) with further decoupling into axial directions. The model is pretrained on PDEBench datasets and fine-tuned on colliding thermals and MHD datasets, with performance measured by normalized root-mean-square error (NRMSE).

## Key Results
- Adaptive tokenization reduces computational cost by 2-5× while maintaining or improving accuracy across all tested physical systems
- Models pretrained on PDEBench data outperform those trained from scratch, especially in low-data regimes
- Fully decoupled axial attention requires 2-3× more training time and model weights to achieve comparable accuracy to other approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive tokenization reduces computational cost by dynamically adjusting patch sizes based on local feature complexity
- Mechanism: The model starts with coarse patches and refines only those with high variance (complexity), reducing the total number of tokens needed while maintaining accuracy in critical regions
- Core assumption: Local variance in physical solutions is a reliable proxy for where high resolution is needed
- Evidence anchors:
  - [abstract]: "two adaptive tokenization schemes that dynamically adjust patch sizes based on local features"
  - [section]: "adaptive tokenization method that dynamically adjusts patch sizes across the system based on local features"
  - [corpus]: Weak - no direct evidence in corpus papers about adaptive tokenization for physical systems
- Break condition: If local variance fails to correlate with regions requiring high resolution, or if the refinement threshold selection is poorly calibrated

### Mechanism 2
- Claim: Decoupled spatiotemporal attention reduces computational complexity while maintaining accuracy for most physical systems
- Mechanism: By factorizing attention operations along temporal and spatial dimensions (SViT) or further into axial directions (A ViT), the quadratic scaling is reduced from O(N²) to approximately O(N) operations per attention block
- Core assumption: Spatiotemporal correlations can be effectively captured through sequential factorization without significant information loss
- Evidence anchors:
  - [abstract]: "we present a set of spatiotemporal attention schemes, where the temporal or axial spatial dimensions are decoupled"
  - [section]: "the temporal or axial spatial dimensions are decoupled, and evaluate their computational and data efficiencies"
  - [corpus]: Weak - corpus papers focus on EEG, traffic, and climate data but not specifically on physical system attention mechanisms
- Break condition: When physical systems exhibit strong cross-scale correlations that cannot be captured through sequential factorization

### Mechanism 3
- Claim: Pretraining on PDEBench data improves fine-tuning performance, especially in low-data regimes
- Mechanism: Models pretrained on diverse physical systems develop transferable representations that can be fine-tuned to new physics with fewer samples than training from scratch
- Core assumption: Physical systems share common underlying patterns that can be learned during pretraining and transferred to related but distinct problems
- Evidence anchors:
  - [abstract]: "models pretrained on PDEBench data outperform the ones trained from scratch, especially in the low data regime"
  - [section]: "models pretrained on PDEBench data outperform the ones trained from scratch, especially in the low data regime with frozen attention"
  - [corpus]: Weak - corpus papers focus on different applications (EEG, traffic) without addressing physical system pretraining
- Break condition: When fine-tuning targets involve physics fundamentally different from pretraining data, reducing transfer benefits

## Foundational Learning

- Concept: Vision Transformer architecture and self-attention mechanisms
  - Why needed here: MATEY builds on ViT foundations with modifications for spatiotemporal physical systems
  - Quick check question: Can you explain how self-attention in ViTs differs from traditional convolutional approaches?

- Concept: Multiscale modeling and adaptive mesh refinement
  - Why needed here: The adaptive tokenization scheme is inspired by AMR techniques from computational physics
  - Quick check question: What is the primary computational advantage of adaptive mesh refinement over uniform high-resolution grids?

- Concept: Foundation models and transfer learning
  - Why needed here: The work explores pretraining on PDEBench and fine-tuning to new physical systems
  - Quick check question: How does pretraining on diverse datasets improve performance on specific downstream tasks?

## Architecture Onboarding

- Component map: Multi-physics preprocessor/postprocessor -> Adaptive tokenization module -> Spatiotemporal attention module -> Physics-guided training with system-based sampling

- Critical path:
  1. Preprocess input solutions to unified latent space
  2. Tokenize using adaptive scheme based on local variance
  3. Apply spatiotemporal attention (decoupled for efficiency)
  4. Decode tokens back to solution space
  5. Train with multi-GPU DDP and gradient accumulation

- Design tradeoffs:
  - Accuracy vs computational cost in adaptive tokenization
  - Expressiveness vs efficiency in attention mechanism choice
  - Model size vs performance in different physical regimes

- Failure signatures:
  - Poor accuracy with adaptive tokenization: likely threshold γsts is misconfigured
  - Slow training with ViT: sequence length too long for quadratic attention scaling
  - No transfer learning benefit: pretraining data too dissimilar from fine-tuning targets

- First 3 experiments:
  1. Compare constant patch sizes (32x32, 16x16) vs adaptive tokenization on simple PDE
  2. Benchmark ViT vs SViT vs A ViT on colliding thermals dataset
  3. Test pretrained vs random initialization on low-data fine-tuning task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adaptive tokenization approach perform on higher-dimensional spatiotemporal data (e.g., 3D or 4D) where the computational challenges become even more severe?
- Basis in paper: [explicit] The authors mention that "input sequences can easily exceed billions of tokens" for high-resolution data but only evaluate their approach on 2D data.
- Why unresolved: The experiments only demonstrate performance on 2D systems, leaving the scalability and effectiveness of the approach in higher dimensions untested.
- What evidence would resolve it: Testing MATEY on 3D or 4D spatiotemporal datasets with varying levels of resolution and complexity to compare against traditional approaches.

### Open Question 2
- Question: What is the theoretical limit of patch size reduction before information loss becomes prohibitive in adaptive tokenization schemes?
- Basis in paper: [inferred] The authors show that Adap Mix can achieve lower error than the fine patch case ps=pxsts ˆ pxsts despite using only half the sequence length, suggesting there's a threshold where further reduction becomes counterproductive.
- Why unresolved: The paper doesn't explore the minimum viable patch size or establish guidelines for when adaptive tokenization stops providing benefits.
- What evidence would resolve it: Systematic experiments varying patch sizes to identify the point where predictive accuracy plateaus or degrades, potentially establishing theoretical bounds.

### Open Question 3
- Question: How would MATEY perform on systems with more complex physics that include additional variables not present in PDEBench, such as multiphase flows or chemical reactions?
- Basis in paper: [explicit] The authors note that "assessing model performance in more realistic settings, where equations like Navier-Stokes are coupled with those from other domains of physics, is a more informative test of the effectiveness of pretraining" but only test on colliding thermals and MHD.
- Why unresolved: The fine-tuning experiments, while including out-of-distribution physics, are still relatively simple compared to many real-world multiscale physical systems.
- What evidence would resolve it: Fine-tuning MATEY on systems with more complex physics, including additional variables and coupling between different physical domains, to assess transferability and performance degradation.

## Limitations
- The adaptive tokenization approach relies on local variance as a proxy for feature complexity, which may not generalize to all physical systems
- Computational savings are based on specific PDEBench datasets and may vary significantly for different physical phenomena
- Decoupled attention mechanisms may sacrifice accuracy in capturing cross-scale correlations critical for certain physical systems

## Confidence

**High Confidence Claims:**
- Adaptive tokenization significantly reduces computational cost while maintaining or improving accuracy for the tested physical systems
- Pretrained models outperform training from scratch in low-data regimes for the specific fine-tuning tasks evaluated
- Fully decoupled axial attention requires more training time and model weights to achieve comparable accuracy

**Medium Confidence Claims:**
- The specific performance improvements (NRMSE values, computational speedups) are robust across the tested datasets
- The proposed attention mechanisms (ViT, SViT, A ViT) provide meaningful trade-offs between accuracy and efficiency
- The adaptive tokenization threshold parameters can be effectively tuned for different physical systems

**Low Confidence Claims:**
- The adaptive tokenization approach will generalize to all multiscale physical systems
- The computational complexity reduction scales favorably to extremely large-scale problems
- The pretraining benefits will transfer to physical systems outside the PDEBench domain

## Next Checks
1. **Cross-Validation on Diverse Physical Systems**: Test MATEY on physical systems with fundamentally different characteristics from PDEBench (e.g., fracture mechanics, combustion, or biological systems) to validate the generalizability of adaptive tokenization and pretraining benefits.

2. **Ablation Study on Attention Mechanisms**: Conduct systematic experiments varying the degree of spatiotemporal decoupling to identify the optimal balance between computational efficiency and accuracy capture for different physical regimes.

3. **Scalability Assessment**: Evaluate the performance and computational scaling of MATEY on larger problem sizes (both spatial and temporal) to verify that the claimed complexity reductions hold as system size increases.