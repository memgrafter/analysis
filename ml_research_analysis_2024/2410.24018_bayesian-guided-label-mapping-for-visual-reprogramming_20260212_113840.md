---
ver: rpa2
title: Bayesian-guided Label Mapping for Visual Reprogramming
arxiv_id: '2410.24018'
source_url: https://arxiv.org/abs/2410.24018
tags:
- pretrained
- downstream
- label
- labels
- photo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Bayesian-guided Label Mapping (BLM) and its
  extension BLM+ to address limitations in existing gradient-free label mapping methods
  for visual reprogramming. Current one-to-one mappings overlook complex relationships
  between pretrained and downstream labels, leading to suboptimal performance.
---

# Bayesian-guided Label Mapping for Visual Reprogramming

## Quick Facts
- arXiv ID: 2410.24018
- Source URL: https://arxiv.org/abs/2410.24018
- Reference count: 40
- Primary result: BLM achieves up to 6.1% accuracy improvement over one-to-one label mapping on ResNet-18 and 7.5% on vision-language models

## Executive Summary
This paper addresses limitations in gradient-free visual reprogramming by proposing Bayesian-guided Label Mapping (BLM) and its extension BLM+. Current one-to-one label mappings overlook complex relationships between pretrained and downstream labels, leading to suboptimal performance. BLM constructs a probabilistic label mapping matrix using Bayesian conditional probability, considering the joint distribution of downstream labels and pretrained model predictions. BLM+ extends this by aggregating top-K predicted probabilities. Experiments on 12 datasets with both pretrained vision models and vision-language models demonstrate that BLM and BLM+ outperform existing methods, achieving up to 6.1% accuracy improvement on ResNet-18 and 7.5% on vision-language models. The approach offers both performance gains and interpretability in understanding visual reprogramming effectiveness.

## Method Summary
The method constructs an iteratively-updated probabilistic label mapping matrix where each element quantifies pairwise relationships between pretrained and downstream labels. Values are assigned using Bayesian conditional probability based on the joint distribution of predicted pretrained labels and ground-truth downstream labels. The approach integrates with existing visual reprogramming pipelines by applying the probabilistic mapping to reweight logits before computing cross-entropy loss. BLM+ extends this by aggregating top-K predicted probabilities instead of using single predictions, improving robustness to prediction uncertainty. The mapping matrix is updated at each training epoch to align with evolving input visual reprogramming patterns.

## Key Results
- BLM achieves 6.1% accuracy improvement over one-to-one mapping on ResNet-18 across 12 datasets
- BLM+ provides additional 3.5% improvement, reaching 7.5% total gain on vision-language models
- The probabilistic mapping approach offers interpretability by revealing complex label relationships
- Iterative updates are crucial for maintaining alignment between mapping matrix and evolving input patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bayesian conditional probability estimation captures complex many-to-many relationships between pretrained and downstream labels better than one-to-one mappings.
- **Mechanism:** BLM constructs a probabilistic label mapping matrix using joint distribution of downstream labels and pretrained model predictions. Each element ωyS,yT quantifies the relationship between pretrained label yS and downstream label yT, updated iteratively based on Bayesian conditional probability p(YT=yT|YS=yS,XT).
- **Core assumption:** The relationship between pretrained and downstream labels can be modeled as a joint probability distribution that captures complex label correlations beyond simple one-to-one correspondence.
- **Evidence anchors:**
  - [abstract] "BLM constructs an iteratively-updated probabilistic label mapping matrix, with each element quantifying a pairwise relationship between pretrained and downstream labels. The assignment of values to the constructed matrix is guided by Bayesian conditional probability, considering the joint distribution of the downstream labels and the labels predicted by the pretrained model on downstream samples."
  - [section 4] "Specifically, we present Bayesian-guided label mapping (BLM) in Section 4, which assigns values to elements in the probabilistic LM matrix based on Bayesian conditional probabilities, derived from the joint distribution of the predicted pretrained labels on downstream tasks and the ground-truth downstream labels."
  - [corpus] Weak - corpus lacks direct evidence for Bayesian conditional probability capturing complex label relationships.
- **Break condition:** When pretrained and downstream label spaces are completely unrelated or when pretrained model predictions are uninformative (random or uniform).

### Mechanism 2
- **Claim:** Aggregating top-K predicted probabilities improves robustness over using single most likely prediction.
- **Mechanism:** BLM+ extends BLM by considering top-K predicted pretrained labels when estimating joint distribution p(YT=yT,YS=yS|XT), aggregating their probabilities rather than relying on the single highest probability label.
- **Core assumption:** The pretrained model's top-K predictions contain useful information about label relationships that single best prediction misses.
- **Evidence anchors:**
  - [abstract] "BLM+ extends this by aggregating top-K predicted probabilities instead of using a single predicted label when estimating the joint distribution, accounting for uncertainty in the predictions."
  - [section 4] "Let YS K,i ≡ {y′| arg maxy1,...yK f(xT i )y′} denote the set of the top-K predicted pretrained labels for input xT i , and ˆp(yS|xT i ) ≡ (softmax ◦ f)(xT i )yS denote the predicted probability for any yS ∈ Y S given xT i . Then, within the BLM+ strategy, the joint density is approximated as..."
  - [corpus] Weak - corpus does not contain evidence specifically about top-K aggregation improving robustness.
- **Break condition:** When pretrained model predictions are highly uncertain (uniform distribution) or when K is too large relative to label space size.

### Mechanism 3
- **Claim:** Iterative updates of probabilistic mapping matrix during training align LM with evolving input VR patterns.
- **Mechanism:** BLM and BLM+ matrices are updated at each epoch as input VR patterns θ evolve, ensuring the output LM remains matched with current input transformations rather than being computed once beforehand.
- **Core assumption:** The relationship between pretrained and downstream labels changes as input VR patterns evolve during training.
- **Evidence anchors:**
  - [abstract] "BLM constructs an iteratively-updated probabilistic label mapping matrix"
  - [section 4] "This allows predictions for each downstream sample to consider diverse contributions from all pretrained labels, enabling a flexible many-to-many mapping strategy."
  - [section 5] "Furthermore, for both BLM and BLM+, iterative updates are crucial as the initial input VR may deviate considerably from the final iteration."
  - [corpus] Weak - corpus lacks direct evidence about iterative LM updates aligning with input VR evolution.
- **Break condition:** When input VR converges quickly to stable patterns or when relationship between pretrained and downstream labels is static.

## Foundational Learning

- **Concept:** Bayesian conditional probability and joint distributions
  - Why needed here: BLM relies on Bayes' theorem to estimate p(YT|YS,XT) from joint distribution p(YT,YS|XT), which requires understanding conditional probability fundamentals.
  - Quick check question: If p(YT=dog, YS=Chihuahua|XT)=0.3 and p(YS=Chihuahua|XT)=0.6, what is p(YT=dog|YS=Chihuahua,XT)?

- **Concept:** Probability aggregation and top-K selection
  - Why needed here: BLM+ uses top-K predicted probabilities instead of single best prediction, requiring understanding of probability aggregation techniques.
  - Quick check question: Given softmax probabilities [0.5, 0.3, 0.2] for three labels, what are the top-2 aggregated probabilities?

- **Concept:** Iterative optimization and parameter updates
  - Why needed here: BLM/BLM+ update their mapping matrices iteratively during training, requiring understanding of optimization dynamics and parameter synchronization.
  - Quick check question: Why might a mapping matrix computed once at initialization become suboptimal as input VR patterns evolve during training?

## Architecture Onboarding

- **Component map:** Input VR module (fin(·;θ)) -> Pretrained model (fpre) -> BLM/BLM+ module -> Loss function -> Optimizer -> Input VR parameters

- **Critical path:**
  1. Generate predictions from pretrained model on downstream samples
  2. Compute/update probabilistic mapping matrix using Bayesian estimation
  3. Apply mapping matrix to reweight logits for downstream labels
  4. Compute loss and update input VR parameters
  5. Repeat until convergence

- **Design tradeoffs:**
  - One-to-one vs probabilistic mapping: Simplicity vs expressiveness
  - Single prediction vs top-K aggregation: Efficiency vs robustness
  - Static vs iterative updates: Speed vs alignment with evolving VR
  - Laplace smoothing λ: Noise robustness vs sensitivity to true distribution

- **Failure signatures:**
  - Poor performance on tasks with very few classes (e.g., SVHN with 10 classes)
  - Slow convergence when pretrained model predictions are uninformative
  - Suboptimal results when pretrained and downstream label spaces are completely dissimilar
  - Performance degradation with inappropriate top-K ratio α

- **First 3 experiments:**
  1. Compare BLM vs one-to-one mapping on CIFAR10 with varying Laplace smoothing λ values
  2. Test BLM+ with different top-K ratios α on Flowers102 to find optimal K
  3. Measure convergence speed of iterative vs static BLM on DTD dataset

## Open Questions the Paper Calls Out
- **Question:** How does the performance of BLM and BLM+ scale with increasingly larger pretrained and downstream label spaces?
  - **Basis in paper:** [explicit] The paper demonstrates performance improvements on datasets with varying label space sizes but does not explore the limits of scalability.
  - **Why unresolved:** The experiments cover a range of label space sizes but do not include extremely large-scale tasks or examine how performance degrades or plateaus as label spaces grow.
  - **What evidence would resolve it:** Extensive experiments on tasks with thousands or millions of classes, comparing BLM/BML+ performance against baselines across multiple orders of magnitude in label space size.

- **Question:** Can BLM and BLM+ be effectively extended to handle open-set or few-shot downstream tasks where some labels have minimal training data?
  - **Basis in paper:** [inferred] The paper focuses on fully supervised classification tasks and does not address scenarios with limited labeled data or unknown classes.
  - **Why unresolved:** The current formulation assumes sufficient training data for all downstream classes and does not account for distribution shifts or unseen classes.
  - **What evidence would resolve it:** Experiments showing BLM/BML+ performance on few-shot learning benchmarks or open-set recognition tasks, including metrics for handling unknown classes.

- **Question:** What is the theoretical relationship between the optimal probabilistic label mapping and the true underlying semantic relationships between pretrained and downstream labels?
  - **Basis in paper:** [explicit] The paper provides theoretical analysis showing probabilistic LM should outperform deterministic LM under certain conditions but does not establish connections to actual semantic similarity.
  - **Why unresolved:** The theoretical analysis uses simplified binary label spaces and does not map the probabilistic relationships to meaningful semantic interpretations.
  - **What evidence would resolve it:** Formal analysis linking the learned probabilistic mappings to measures of semantic similarity or feature space distances, potentially using techniques from metric learning or representation analysis.

## Limitations
- Performance may degrade when pretrained and downstream label spaces are semantically distant
- The method relies on informative predictions from pretrained models, which may not hold for all downstream tasks
- Theoretical justification for why Bayesian conditional probability better captures label relationships remains underdeveloped

## Confidence
- **High Confidence:** BLM outperforms one-to-one mapping methods (ILM) across all tested datasets, with consistent accuracy improvements (6.1% on ResNet-18, 7.5% on vision-language models).
- **Medium Confidence:** BLM+ provides additional gains over BLM through top-K aggregation, though the improvement is smaller (3.5% vs 6.1% on ResNet-18) and may be dataset-dependent.
- **Low Confidence:** The theoretical underpinnings of why Bayesian conditional probability better captures label relationships, and why iterative updates are crucial, remain underdeveloped.

## Next Checks
1. Test BLM on datasets with semantically distant label spaces (e.g., mapping ImageNet to medical imaging tasks) to validate robustness claims.
2. Conduct ablation studies on the importance of iterative updates by comparing BLM with static mapping matrices computed at initialization.
3. Evaluate BLM+ with varying K values on datasets with different label space sizes to determine optimal top-K ratio scaling.