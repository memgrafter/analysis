---
ver: rpa2
title: Multimodal Generalized Category Discovery
arxiv_id: '2409.11624'
source_url: https://arxiv.org/abs/2409.11624
tags:
- multimodal
- modalities
- data
- text
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends Generalized Category Discovery (GCD) to a multimodal
  setting, addressing the challenge of discovering both known and novel categories
  from multimodal data. The authors propose MM-GCD, a framework that aligns features
  and outputs across modalities using contrastive learning and distillation techniques.
---

# Multimodal Generalized Category Discovery

## Quick Facts
- arXiv ID: 2409.11624
- Source URL: https://arxiv.org/abs/2409.11624
- Reference count: 36
- Primary result: Extends GCD to multimodal setting, achieving 11.5% and 4.7% improvements on UPMC-Food101 and N24News datasets

## Executive Summary
This paper introduces MM-GCD, a framework that extends Generalized Category Discovery to the multimodal domain. The method addresses the challenge of discovering both known and novel categories from multimodal data by aligning features and outputs across different modalities. The authors demonstrate that their approach significantly outperforms previous methods on two multimodal datasets, establishing new state-of-the-art results and highlighting the importance of modality alignment in this setting.

## Method Summary
The authors propose MM-GCD as a solution for multimodal Generalized Category Discovery. The framework leverages contrastive learning and distillation techniques to align features and outputs across different modalities. By doing so, it can effectively discover both known and novel categories from multimodal data sources. The approach is designed to take advantage of complementary information present across different modalities while maintaining consistency in the learned representations.

## Key Results
- MM-GCD achieves 11.5% improvement over previous methods on UPMC-Food101 dataset
- MM-GCD achieves 4.7% improvement over previous methods on N24News dataset
- Results demonstrate the effectiveness of modality alignment in multimodal GCD settings

## Why This Works (Mechanism)
The method works by aligning features and outputs across different modalities through contrastive learning and distillation. This alignment helps the model leverage complementary information from different data sources while maintaining consistency in the learned representations. The approach effectively bridges the gap between different modalities, allowing for more robust category discovery in multimodal settings.

## Foundational Learning
- Contrastive Learning: Needed for learning modality-invariant representations; quick check: measure alignment between modalities in embedding space
- Knowledge Distillation: Needed for transferring knowledge between modalities; quick check: evaluate teacher-student loss dynamics
- Generalized Category Discovery: Needed as the base problem formulation; quick check: validate unimodal GCD performance before adding multimodal components

## Architecture Onboarding
**Component Map:** MM-GCD -> Modality-Specific Encoders -> Feature Alignment Module -> Contrastive Loss -> Distillation Loss -> Category Discovery Head

**Critical Path:** Input Modalities → Modality-Specific Encoders → Feature Alignment → Shared Category Discovery Head

**Design Tradeoffs:** 
- Balance between modality-specific and cross-modal objectives
- Computational overhead from alignment mechanisms
- Sensitivity to hyperparameter tuning

**Failure Signatures:** 
- Poor alignment between modalities in embedding space
- Degraded performance when one modality is noisy
- Overfitting to specific modality characteristics

**First Experiments:** 
1. Test unimodal GCD baselines on each modality
2. Evaluate modality alignment quality in embedding space
3. Measure computational overhead of alignment components

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to only two datasets (UPMC-Food101 and N24News)
- Results may not generalize to larger-scale datasets or different multimodal domains
- Computational efficiency and scalability concerns not addressed
- Method's sensitivity to hyperparameter tuning not thoroughly explored

## Confidence
**High confidence:** Experimental results showing SOTA performance on tested datasets, fundamental contribution to modality alignment in multimodal GCD

**Medium confidence:** Generalizability to other multimodal domains, scalability to larger datasets

**Low confidence:** Robustness to noise and missing modalities

## Next Checks
1. Evaluate MM-GCD on additional multimodal datasets (e.g., Caltech-UCSD Birds, Fashion-Gen) to assess generalizability across different domains
2. Conduct ablation studies on computational overhead introduced by modality alignment components
3. Test model performance under missing modality scenarios to evaluate robustness and practical deployment considerations