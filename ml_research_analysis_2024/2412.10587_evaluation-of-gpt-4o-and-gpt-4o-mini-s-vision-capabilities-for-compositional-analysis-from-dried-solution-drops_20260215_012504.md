---
ver: rpa2
title: Evaluation of GPT-4o and GPT-4o-mini's Vision Capabilities for Compositional
  Analysis from Dried Solution Drops
arxiv_id: '2412.10587'
source_url: https://arxiv.org/abs/2412.10587
tags: []
core_contribution: "This study evaluated OpenAI\u2019s GPT-4o and GPT-4o-mini vision\
  \ models for identifying salts from dried solution drop images. Using 200 images\
  \ per salt (12 types), GPT-4o achieved 57% accuracy and a 0.52 F1 score, significantly\
  \ outperforming GPT-4o-mini (11% accuracy) and random chance (8%)."
---

# Evaluation of GPT-4o and GPT-4o-mini's Vision Capabilities for Compositional Analysis from Dried Solution Drops

## Quick Facts
- arXiv ID: 2412.10587
- Source URL: https://arxiv.org/abs/2412.10587
- Reference count: 4
- Primary result: GPT-4o achieved 57% accuracy and 0.52 F1 score for salt identification from dried solution drop images

## Executive Summary
This study evaluated OpenAI's GPT-4o and GPT-4o-mini vision models for identifying salts from dried solution drop images. Using 200 images per salt type across 12 different salts, GPT-4o demonstrated significantly better performance than GPT-4o-mini, achieving 57% accuracy compared to 11% for the mini model. The full GPT-4o model also showed a 0.52 F1 score, while the mini model exhibited strong bias toward Na₃PO₄, reducing its reliability. Despite the performance gap, both models showed promise for quick identification of salts from images, with GPT-4o proving more capable of distinguishing between similar-looking salts like KCl, KBr, and NaCl.

## Method Summary
The study used batch processing with OpenAI's API to classify 200 images per salt type (12 total salts) through 1200 requests per trial. Each request included a system prompt, 12 training images per salt type, and a test image. The models used temperature=0 and seed=17 to ensure deterministic results. GPT-4o-2024-08-06 and GPT-4o-mini-2024-07-18 were compared using accuracy, F1 scores, and Cohen's Kappa for agreement between trials. Training images were sourced from Dr. Steinbock's website, and results were analyzed statistically to evaluate model performance across different salt types.

## Key Results
- GPT-4o achieved 57% accuracy and 0.52 F1 score, significantly outperforming GPT-4o-mini's 11% accuracy
- GPT-4o-mini showed strong bias toward Na₃PO₄, reducing its reliability for general salt identification
- Both models struggled with similar-looking salts, particularly confusing KCl, KBr, and NaCl

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4o's superior performance over GPT-4o-mini is due to its better visual learning capabilities for detailed morphological analysis.
- Mechanism: GPT-4o can process visual patterns more effectively due to its larger model capacity and training data, allowing it to recognize subtle differences in salt crystal morphology.
- Core assumption: Visual learning capabilities scale with model size and training data diversity.
- Evidence anchors:
  - [abstract] "GPT-4o achieved 57% accuracy and a 0.52 F1 score, significantly outperforming both random chance (8%) and GPT-4o mini (11% accuracy)"
  - [section] "The full model proves clearly superior" and "the mini model proves to be less worthwhile even when considering cost and computing efficiency"
  - [corpus] Weak evidence - no directly relevant papers found
- Break condition: If the model's visual learning capabilities are not actually related to size/complexity, but rather to specific training data or architectural differences.

### Mechanism 2
- Claim: Temperature setting of 0 with seed 17 makes results more deterministic, reducing variability between trials.
- Mechanism: Setting temperature to 0 forces the model to choose the highest probability output, while the seed ensures consistent sampling for stochastic elements.
- Core assumption: Model outputs can be made deterministic through parameter tuning even though the underlying architecture is probabilistic.
- Evidence anchors:
  - [section] "Each request also contained the specific model name (either gpt-4o-mini-2024-07-18 or gpt-4o-2024-08-06), a temperature of 0, and a seed of 17 (chosen arbitrarily) in order to push the results closer to being deterministic"
  - [section] "The value of each kappa being below 1, meaning the trials of the same model do not display total agreement, is to be expected"
  - [corpus] Weak evidence - no directly relevant papers found
- Break condition: If the underlying model architecture has inherent stochastic elements that cannot be fully controlled by these parameters.

### Mechanism 3
- Claim: Batch processing with stateless API calls ensures reproducibility by preventing memory carryover between requests.
- Mechanism: Each request is processed independently without any context from previous requests, making the results comparable across trials.
- Core assumption: API requests are truly stateless and independent when using batch processing.
- Evidence anchors:
  - [section] "Using the API also ensures every request is independent (stateless), with no memory carrying over between requests"
  - [section] "For batch processing, a JSONL file is required, in which every line contains a valid JSON object representing individual requests to the API"
  - [corpus] Weak evidence - no directly relevant papers found
- Break condition: If the API implementation has hidden state persistence or if the batch processing introduces subtle dependencies between requests.

## Foundational Learning

- Concept: Cohen's Kappa for inter-rater reliability
  - Why needed here: Used to measure consistency between different trials and models while accounting for chance agreement
  - Quick check question: If two models always agree on wrong answers, what would Cohen's Kappa indicate about their agreement?

- Concept: F1 score calculation and interpretation
  - Why needed here: Provides a balanced measure of precision and recall to evaluate model performance beyond simple accuracy
  - Quick check question: Why might F1 scores differ from accuracy scores when there's class imbalance in model predictions?

- Concept: JSONL file format for batch processing
  - Why needed here: Required format for OpenAI's batch API to process multiple requests simultaneously
  - Quick check question: What key information must each JSON object in the JSONL file contain for salt identification?

## Architecture Onboarding

- Component map: Image folders → JSONL generation → OpenAI API batch processing → Response parsing → Analysis (Cohen's Kappa, accuracy, F1) → Visualization
- Critical path: Image selection → Training data construction → API request formation → Model inference → Result extraction → Statistical analysis
- Design tradeoffs: Using batch processing reduces cost but requires more complex data management; temperature 0 reduces variability but may miss optimal solutions; including training images increases token count and cost
- Failure signatures: High Cohen's Kappa between trials but low accuracy indicates model consistency but poor performance; Na₃PO₄ bias in mini model shows specific failure mode; confusion between similar salts (KCl/KBr/NaCl) indicates granularity limitations
- First 3 experiments:
  1. Test single image classification with different temperature settings to observe effect on output consistency
  2. Compare token usage between mini and full models for identical image sets to verify cost claims
  3. Run classification with only training images (no test images) to establish baseline model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning GPT-4o on salt-evaporite images affect identification accuracy and F1 scores compared to the baseline results?
- Basis in paper: [explicit] The paper mentions that "potential improvement can be found in the fine-tuning feature (13), leaving an opportunity for better results."
- Why unresolved: The study only tested GPT-4o and GPT-4o-mini without any fine-tuning on salt-evaporite images. The authors explicitly note this as a potential avenue for improvement but did not explore it.
- What evidence would resolve it: A controlled experiment comparing GPT-4o's performance with and without fine-tuning on the same dataset of 12 salt types, measuring accuracy and F1 scores for each salt.

### Open Question 2
- Question: What specific visual features cause GPT-4o to confuse KCl, KBr, and NaCl, and how could model training be adjusted to address this?
- Basis in paper: [explicit] The paper shows that "KBr and KCl were very often misidentified as NaCl (98 and 99 times, respectively)" and notes this "massive confusion" between these three salts.
- Why unresolved: While the paper identifies the confusion, it doesn't analyze the underlying visual characteristics that cause it or propose specific training modifications to address the issue.
- What evidence would resolve it: Detailed visual feature analysis comparing the deposit patterns of KCl, KBr, and NaCl, followed by targeted training modifications (e.g., contrastive learning, feature augmentation) and evaluation of their impact on confusion rates.

### Open Question 3
- Question: Would increasing the temperature parameter above 0 improve identification accuracy for salts with similar deposit patterns, or would it introduce too much variability?
- Basis in paper: [inferred] The study used temperature=0 to achieve deterministic results, but the authors note that "such confusion is not unexpected and can be improved on by allowing for greater attention to detail in models."
- Why unresolved: The paper only tested with temperature=0, which prioritizes consistency over potential accuracy gains from model stochasticity. The optimal temperature setting for this specific task remains unknown.
- What evidence would resolve it: Systematic testing of GPT-4o with different temperature values (e.g., 0.1, 0.3, 0.5) on the same dataset, comparing accuracy and consistency trade-offs for each salt type.

## Limitations
- Study relies on a single data source for training images, potentially limiting generalizability
- Significant performance gap between models raises questions about whether differences stem from architecture or training data
- Temperature=0 and seed=17 settings may not fully eliminate inherent model stochasticity

## Confidence
- **High confidence**: GPT-4o outperforms GPT-4o-mini for salt identification from images
- **Medium confidence**: Temperature and seed settings sufficiently control model variability
- **Medium confidence**: Batch processing ensures true independence between requests
- **Low confidence**: F1 scores accurately reflect model performance across all salt types

## Next Checks
1. Run 10 additional trials with varying temperature settings (0, 0.3, 0.7) to quantify the impact of randomness on model outputs and validate the temperature=0 assumption.
2. Test the models on salt images from multiple independent sources beyond Dr. Steinbock's website to assess generalizability and identify potential dataset-specific biases.
3. Conduct ablation studies comparing GPT-4o and GPT-4o-mini with identical prompts and training data to isolate whether performance differences stem from model architecture or training data exposure.