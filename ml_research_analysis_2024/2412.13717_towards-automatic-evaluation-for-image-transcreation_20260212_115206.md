---
ver: rpa2
title: Towards Automatic Evaluation for Image Transcreation
arxiv_id: '2412.13717'
source_url: https://arxiv.org/abs/2412.13717
tags:
- image
- cultural
- transcreation
- visual
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes the first suite of automatic metrics for evaluating\
  \ image transcreation, a task that adapts visual content across cultures. The authors\
  \ identify three critical dimensions\u2014cultural relevance, semantic equivalence,\
  \ and visual similarity\u2014and design metrics in three categories: object-based,\
  \ embedding-based, and VLM-based."
---

# Towards Automatic Evaluation for Image Transcreation

## Quick Facts
- arXiv ID: 2412.13717
- Source URL: https://arxiv.org/abs/2412.13717
- Authors: Simran Khanuja; Vivek Iyer; Claire He; Graham Neubig
- Reference count: 8
- Key outcome: First suite of automatic metrics for image transcreation evaluation with strong correlations (0.55-0.87) to human ratings

## Executive Summary
This paper introduces the first comprehensive framework for automatic evaluation of image transcreation, a task that adapts visual content across cultures. The authors identify three critical dimensions—cultural relevance, semantic equivalence, and visual similarity—and design metrics in three categories: object-based, embedding-based, and VLM-based. Meta-evaluation on a 7-country dataset demonstrates strong segment-level correlations with human ratings, showing that proprietary VLMs excel at assessing cultural relevance and semantic equivalence, while embedding-based metrics best capture visual similarity.

## Method Summary
The paper proposes three categories of metrics for evaluating image transcreation. Object-based metrics identify culturally specific items (CSIs) and measure correct replacements, embedding-based metrics use vision-language encoders like SigLIP to assess semantic similarity in shared embedding spaces, and VLM-based metrics leverage models like GPT-4o and Gemini-1.5-Pro for reasoning-based evaluation. The meta-evaluation approach compares these metrics against human ratings using Kendall's Tau correlation on a dataset of 3,543 images across 7 countries and 17 semantic categories.

## Key Results
- Strong segment-level correlations (0.55-0.87) between automatic metrics and human ratings
- Proprietary VLMs (GPT-4o, Gemini-1.5-Pro) excel at assessing cultural relevance and semantic equivalence
- Embedding-based metrics using SigLIP best capture visual similarity with correlation of 0.87
- Object-based metrics provide interpretable scores but are limited to discrete object changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proprietary VLMs excel at evaluating cultural relevance and semantic equivalence because they are trained on large-scale multimodal data that includes culturally diverse content and fine-grained semantic distinctions.
- Mechanism: These models leverage transformer-based architectures with attention mechanisms that capture contextual relationships across modalities, enabling them to recognize culturally specific objects and map them to semantically equivalent concepts in target cultures.
- Core assumption: The training data contains sufficient cultural diversity and semantic nuance to enable accurate cross-cultural reasoning.
- Evidence anchors:
  - [abstract] "proprietary VLMs (e.g., GPT-4o, Gemini-1.5-Pro) excel at assessing cultural relevance and semantic equivalence"
  - [section 5.4] "For semantic equivalence, proprietary VLMs such as GPT-4o and Gemini-1.5-Pro show the highest performance"
  - [corpus] Weak evidence - no direct citation for this specific mechanism in related papers
- Break condition: If the target culture contains concepts not represented in the model's training data, or if cultural nuances require domain-specific knowledge not captured during pretraining.

### Mechanism 2
- Claim: Embedding-based metrics using vision-language encoders like SigLIP are superior for visual similarity because they optimize for image-text alignment through contrastive learning objectives.
- Mechanism: The dual-encoder architecture learns to project images and text into a shared embedding space where semantically similar pairs have high cosine similarity, enabling fine-grained visual comparisons.
- Core assumption: Visual similarity can be effectively captured through semantic embeddings in a shared multimodal space.
- Evidence anchors:
  - [abstract] "embedding-based metrics (e.g., SigLIP) best capture visual similarity"
  - [section 5.4] "For visual similarity, embedding-based metrics using the SigLIP model excel, achieving a strong correlation of 0.87"
  - [corpus] No direct evidence for SigLIP's specific effectiveness in this context
- Break condition: When visual similarity requires perception of low-level features (color, texture, shape details) that are not preserved in semantic embeddings.

### Mechanism 3
- Claim: Object-based metrics work by identifying culturally specific items (CSIs) and replacing them with culturally relevant alternatives, measuring the proportion of correct replacements.
- Mechanism: The metric uses a two-step process: object detection identifies all entities in source and target images, then a reasoning model determines valid replacements based on cultural context and intent.
- Core assumption: Cultural relevance can be reduced to a set of discrete object replacements that can be algorithmically determined.
- Evidence anchors:
  - [section 4.1] "Calculate the proportion of CSIs that have been correctly replaced in the model output"
  - [section 5.3] "For the first step of object identification, we prompt Gemini-1.5-Pro"
  - [corpus] No direct evidence for the effectiveness of this specific CSI replacement approach
- Break condition: When cultural relevance depends on contextual relationships between objects, spatial arrangements, or abstract cultural concepts that cannot be reduced to object replacement.

## Foundational Learning

- Concept: Cross-cultural communication theory
  - Why needed here: Understanding how cultural concepts transfer between societies is fundamental to designing metrics that capture cultural relevance
  - Quick check question: Can you explain the difference between "foreignization" and "domestication" in translation studies?

- Concept: Multimodal embedding spaces
  - Why needed here: The metrics rely on projecting images and text into shared spaces where similarity can be measured
  - Quick check question: How does contrastive learning enable vision-language models to align visual and textual representations?

- Concept: Translation quality metrics
  - Why needed here: The work draws direct inspiration from established text translation metrics like BLEU and COMET
  - Quick check question: What are the key limitations of n-gram based metrics like BLEU that might also apply to object-based image metrics?

## Architecture Onboarding

- Component map: Image → Object Detection (Gemini-1.5-Pro) → CSI Identification → Replacement Validation → Score Calculation (Object-based); Image → Vision Encoder (SigLIP) → Text Encoder → Cosine Similarity (Embedding-based); Image → VLM (GPT-4o/Gemini) → Prompt-based Scoring (VLM-based)

- Critical path: For evaluation pipeline: Load image → Run all three metric types → Aggregate scores → Compare with human ratings → Calculate correlations

- Design tradeoffs: Object-based metrics are interpretable but limited to discrete object changes; embedding-based metrics capture semantic similarity but miss fine-grained visual details; VLM-based metrics handle reasoning but are expensive and potentially inconsistent

- Failure signatures: Object-based metrics fail when cultural relevance depends on context; embedding-based metrics fail when visual details matter; VLM-based metrics fail when reasoning requires extensive cultural knowledge not in training data

- First 3 experiments:
  1. Run all three metrics on a small set of known transcreation examples with clear cultural adaptations
  2. Compare metric scores against human ratings for a diverse set of image categories
  3. Test each metric's sensitivity to varying degrees of cultural adaptation (minimal vs. extensive changes)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can current VLMs effectively capture nuanced differences between transcreation systems, or do they only rank systems that are significantly different in quality?
- Basis in paper: [explicit] The paper notes that all outputs are rated by only one annotator per target image and that cap-retrieve is overwhelmingly better than the other two systems, potentially making ranking easier.
- Why unresolved: The evaluation dataset only contains three systems with clear quality differences, limiting the ability to test whether VLMs can distinguish subtle variations between more closely matched systems.
- What evidence would resolve it: Testing the metrics on a dataset with multiple transcreation systems of similar quality levels and analyzing whether the metrics can still effectively differentiate between them.

### Open Question 2
- Question: How can cultural relevance be measured more accurately beyond using country names, to capture finer-grained cultural nuances?
- Basis in paper: [explicit] The paper acknowledges that relying on country or region names to capture cultural relevance does not capture finer-grained nuances across systems.
- Why unresolved: Current approaches use broad country labels, which may miss important cultural distinctions within regions or fail to account for subcultures and specific demographic preferences.
- What evidence would resolve it: Developing and testing more granular cultural descriptors or using additional contextual information beyond country names to improve cultural relevance measurement.

### Open Question 3
- Question: Would extending VLM-based metric prompts to languages native to the target culture improve evaluation accuracy compared to English-only prompts?
- Basis in paper: [explicit] The paper notes that all prompting for VLM-based metrics is in English and suggests it would be interesting to explore extensions to other languages, particularly those native to the target culture.
- Why unresolved: The current evaluation only uses English prompts, which may not capture cultural nuances as effectively as prompts in the native language of the target audience.
- What evidence would resolve it: Conducting experiments comparing metric performance when using native language prompts versus English prompts for the same transcreation tasks across different cultures.

## Limitations

- Study focuses on 7 countries and 17 semantic categories, potentially missing global cultural complexity
- Reliance on proprietary VLMs raises concerns about reproducibility and accessibility for research groups
- Meta-evaluation assumes human ratings are ground truth, though cultural assessment can be inherently subjective
- Evaluation only uses English prompts for VLM-based metrics, potentially missing cultural nuances

## Confidence

- **High confidence**: Embedding-based metrics effectively capture visual similarity (correlation 0.87), as this relies on well-established contrastive learning principles and doesn't require complex cultural reasoning.
- **Medium confidence**: VLM-based metrics perform well for cultural relevance and semantic equivalence, but their performance may vary significantly with model versions and prompts, and they remain expensive to deploy at scale.
- **Medium confidence**: Object-based metrics provide interpretable scores for CSI replacement, but their effectiveness is limited to cases where cultural relevance can be reduced to discrete object changes.

## Next Checks

1. Test metric robustness across additional cultural contexts beyond the 7 countries studied, particularly for cultures with minimal representation in training data for proprietary VLMs.
2. Conduct ablation studies on VLM-based metrics to quantify the impact of prompt engineering and model selection on evaluation performance.
3. Evaluate metric sensitivity to different levels of transcreation intensity (minimal vs. extensive cultural adaptation) to ensure consistent performance across the full spectrum of possible outputs.