---
ver: rpa2
title: Knowledge-Informed Auto-Penetration Testing Based on Reinforcement Learning
  with Reward Machine
arxiv_id: '2405.15908'
source_url: https://arxiv.org/abs/2405.15908
tags:
- agent
- reward
- vulnerability
- which
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a knowledge-informed automated penetration
  testing (AutoPT) framework that leverages reinforcement learning with reward machines
  (RM) to improve training efficiency and interpretability. The core method idea involves
  encoding cybersecurity domain knowledge from MITRE ATT&CK into RMs, which guide
  the agent through subtasks and provide structured rewards during training.
---

# Knowledge-Informed Auto-Penetration Testing Based on Reinforcement Learning with Reward Machine

## Quick Facts
- arXiv ID: 2405.15908
- Source URL: https://arxiv.org/abs/2405.15908
- Reference count: 22
- Primary result: DQRM agents outperformed standard DQN agents, achieving faster training convergence and requiring fewer steps to capture flags (DQRM-RM2 needed 21.32 steps vs 767.46 for DQN-RM2)

## Executive Summary
This paper presents a knowledge-informed automated penetration testing framework that leverages reinforcement learning with reward machines (RMs) to improve training efficiency and interpretability. The core innovation is encoding cybersecurity domain knowledge from MITRE ATT&CK into RMs, which guide the agent through subtasks and provide structured rewards during training. Two RMs were designed for lateral movement tasks: RM1 with three subtasks (discover credentials, connect to new nodes, elevate privileges) and RM2 with four subtasks (discover nodes, discover credentials, connect, elevate privileges). The deep Q-learning with RM (DQRM) algorithm was employed to train the agent on CyberBattleSim environments, demonstrating superior performance compared to standard DQN approaches.

## Method Summary
The approach uses reward machines to encode domain knowledge from MITRE ATT&CK into structured state machines that guide reinforcement learning agents through penetration testing tasks. The DQRM algorithm learns separate Q-functions for each RM state, decomposing the complex penetration testing problem into manageable subtasks. The framework includes an event detection mechanism that maps observations to symbolic events, triggering transitions between RM states and providing specific rewards. Two reward machine designs were tested: RM1 with three subtasks and RM2 with four subtasks, both focusing on lateral movement phases of penetration testing.

## Key Results
- DQRM agents achieved faster training convergence than standard DQN agents, requiring approximately half the steps during training
- In evaluation, DQRM-RM2 required only 21.32 steps to capture flags versus 767.46 steps for DQN-RM2
- RM2 with more detailed knowledge structure (4 states vs 3 states) demonstrated better performance than RM1 across different environments
- The DQRM approach showed consistent advantages in both training efficiency and final performance metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward machines decompose complex penetration testing tasks into manageable subtasks, improving training efficiency.
- Mechanism: The RM encodes domain knowledge from MITRE ATT&CK into a state machine structure where each state represents a subtask (e.g., discover credentials, connect to new nodes, elevate privileges). This decomposition allows the agent to learn simpler sub-policies for each subtask rather than a single complex policy for the entire task.
- Core assumption: The MITRE ATT&CK knowledge base accurately captures the essential sequential structure of lateral movement attacks.
- Evidence anchors:
  - [abstract]: "We propose a knowledge-informed AutoPT framework called DRLRM-PT, which leverages reward machines (RMs) to encode domain knowledge as guidelines for training a PT policy."
  - [section]: "RM can specify different reward functions for PT in different phases, expanding the flexibility of traditional reward functions in RL/DRL-based PT."

### Mechanism 2
- Claim: The event-based state transitions in RMs provide interpretable feedback about the agent's progress through the attack sequence.
- Mechanism: The RM uses events (e.g., 'discovered new nodes', 'privilege elevated') detected by a labeling function to transition between states. Each transition provides both a new subtask ID and a specific reward, giving the agent clear feedback about what aspect of its action was valuable.
- Core assumption: The defined events accurately represent meaningful milestones in the penetration testing process.
- Evidence anchors:
  - [section]: "The RM receives a set of events detected during PT as input, leading to a transition of its internal state from one state to another according to its transition rules."
  - [section]: "RM allows the agent to specify task-specific reward functions to enhance the flexibility of the single reward function used in traditional RL algorithms."

### Mechanism 3
- Claim: DQRM (Deep Q-learning with RM) achieves better performance than standard DQN by learning separate Q-functions for each RM state.
- Mechanism: DQRM decomposes the policy into sub-policies for each subtask, allowing the agent to learn specialized strategies for different phases of the attack. The experimental results show DQRM agents need approximately half the steps of DQN agents during training.
- Core assumption: The subtask decomposition by RMs creates more learnable sub-problems than the monolithic approach.
- Evidence anchors:
  - [abstract]: "The experimental results demonstrate that the DQRM agent exhibits higher training efficiency in PT compared to agents without knowledge embedding."
  - [section]: "Experimental results showed that DQRM agents outperformed standard DQN agents, achieving faster training convergence and requiring fewer steps to capture flags."

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: Penetration testing involves incomplete information about the target network, requiring the agent to maintain beliefs about hidden states.
  - Quick check question: What distinguishes a POMDP from a standard MDP in the context of network penetration testing?

- Concept: Reward Machines and Event-Based Rewards
  - Why needed here: Traditional reward functions in RL-based PT are complex and hard to specify; RMs provide structured, interpretable reward decomposition.
  - Quick check question: How does an RM differ from a standard reward function in terms of flexibility and interpretability?

- Concept: Deep Q-learning with Experience Replay
  - Why needed here: The high-dimensional observation space in network penetration testing requires function approximation, and experience replay stabilizes training.
  - Quick check question: Why is experience replay particularly important in the CyberBattleSim environment?

## Architecture Onboarding

- Component map: Observation → Event Detection → RM State Transition → Q-network Selection → Action → Environment Response
- Critical path: Observation → Event Detection → RM State Transition → Q-network Selection → Action → Environment Response
- Design tradeoffs:
  - Simpler RMs (RM1) vs. more detailed RMs (RM2): More detailed RMs provide better guidance but may be more brittle
  - Single Q-network vs. per-state Q-networks: Per-state networks allow specialization but increase model complexity
  - Frequent vs. infrequent RM updates: More frequent updates provide better guidance but may reduce stability
- Failure signatures:
  - Poor training efficiency: Could indicate misaligned RM structure or poorly defined events
  - High variance in evaluation performance: May suggest insufficient exploration or overfitting to specific network topologies
  - No improvement over baseline: Could indicate the RM knowledge doesn't match actual attack patterns
- First 3 experiments:
  1. Train DQRM-RM1 and DQN-RM1 on CyberBattleChain (env-1) to verify the efficiency claim (should see DQRM converging faster)
  2. Compare DQRM-RM1 vs DQRM-RM2 on both environments to validate that more detailed knowledge improves performance
  3. Test with corrupted or incomplete MITRE ATT&CK knowledge to understand robustness boundaries of the approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DQRM agents vary across different types of enterprise network topologies beyond the two tested environments?
- Basis in paper: [inferred] The paper tested only two specific network environments (CyberBattleChain and CyberBattleToyCtf) and concluded DQRM agents outperformed DQN agents. However, it's unclear if this advantage holds across more diverse network architectures.
- Why unresolved: The study's scope was limited to two environments, which may not represent the full diversity of enterprise network topologies where AutoPT would be applied.
- What evidence would resolve it: Systematic testing of DQRM agents across a wide range of network topologies (hierarchical, mesh, star, ring, etc.) with varying sizes, node types, and vulnerability distributions.

### Open Question 2
- Question: What is the impact of reward machine complexity on training time and resource consumption for the DQRM algorithm?
- Basis in paper: [explicit] The paper notes that RMs with more detailed knowledge (RM2 with 5 states vs RM1 with 4 states) performed better, but doesn't discuss the computational trade-offs of more complex RMs.
- Why unresolved: The paper focuses on performance metrics (steps to capture flags, accumulated rewards) but doesn't analyze how reward machine complexity affects computational efficiency.
- What evidence would resolve it: Comparative analysis of training time, memory usage, and computational resources required for different RM complexities across multiple network environments.

### Open Question 3
- Question: How robust are the learned PT policies when facing adversarial environments that actively attempt to counter the penetration testing?
- Basis in paper: [inferred] The current formulation assumes a static environment where vulnerabilities and network configurations remain constant, but real-world systems may dynamically respond to PT attempts.
- Why unresolved: The paper doesn't address scenarios where network defenders detect and respond to PT activities by patching vulnerabilities, changing configurations, or implementing deception techniques.
- What evidence would resolve it: Testing DQRM agents against dynamic environments that can modify their vulnerability landscape or implement defensive mechanisms in response to PT activities.

## Limitations

- The approach relies heavily on the accuracy and completeness of MITRE ATT&CK knowledge encoding, which may not capture all real-world attack variations
- The CyberBattleSim environment, while realistic, remains a simulation that may not fully represent actual network complexity and security controls
- The two RM designs (RM1 and RM2) were hand-crafted rather than automatically optimized, raising questions about whether these specific structures are optimal for general penetration testing scenarios

## Confidence

**High Confidence**: The experimental results showing DQRM outperforming DQN in training efficiency and final performance are well-supported by quantitative metrics across multiple environments and RM designs. The fundamental mechanism of using RMs to decompose tasks into subtasks is theoretically sound and demonstrated effectively.

**Medium Confidence**: The claim that RM2 with more detailed knowledge structure consistently outperforms RM1 across different environments is supported by the presented results, but the sample size of tested environments is limited. The generalizability of these findings to diverse real-world scenarios remains uncertain.

**Low Confidence**: The assertion that this approach represents a significant advance toward end-to-end automated penetration testing should be tempered by the current focus on lateral movement only, without addressing initial compromise or post-exploitation phases.

## Next Checks

1. Test the DQRM approach across a broader range of network topologies and sizes in CyberBattleSim to assess scalability and generalization beyond the two environments studied.

2. Implement an automated RM structure learning mechanism rather than hand-crafted designs to evaluate whether optimal RM structures can be discovered through data rather than expert knowledge.

3. Conduct ablation studies removing different components (event detection, RM guidance, per-state Q-networks) to quantify the contribution of each element to the observed performance improvements.