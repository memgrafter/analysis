---
ver: rpa2
title: Wikidata as a seed for Web Extraction
arxiv_id: '2401.07812'
source_url: https://arxiv.org/abs/2401.07812
tags:
- wikidata
- knowledge
- extraction
- extract
- property
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for extracting new facts from
  external websites using Wikidata as a seed. The core idea is to cast web extraction
  as a question-answering task, leveraging language models fine-tuned on SQuAD and
  further adapted to extract information from HTML content.
---

# Wikidata as a seed for Web Extraction

## Quick Facts
- arXiv ID: 2401.07812
- Source URL: https://arxiv.org/abs/2401.07812
- Reference count: 24
- Primary result: Framework achieves 84.07% average F1-score for relation extraction and 65.3% Hit@1 for end-to-end performance

## Executive Summary
This paper introduces a framework for extracting new facts from external websites using Wikidata as a seed. The core idea is to cast web extraction as a question-answering task, leveraging language models fine-tuned on SQuAD and further adapted to extract information from HTML content. The framework uses external identifiers from Wikidata to identify relevant entities and properties, then trains QA models to extract the missing information from associated websites. Experiments show strong performance in relation extraction but reveal object linking as a significant bottleneck, with end-to-end accuracy dropping substantially when linking entities to Wikidata.

## Method Summary
The framework uses a two-stage approach: first extracting relations from HTML content using a QA model, then linking extracted entities to Wikidata. It leverages Wikidata's external identifiers to connect websites to entities, generates HTML-embedded QA pairs from Wikidata triples, and trains RoBERTa-based models for extraction. The system also includes a learning-to-rank model for entity disambiguation and proposes validated facts to Wikidata through WikidataComplete integration.

## Key Results
- 84.07% average F1-score for relation extraction across multiple domains
- 65.3% Hit@1 accuracy for end-to-end performance after object linking
- Demonstrated ability to extract millions of potential new facts from 54 tested domains
- Object linking identified as primary bottleneck, causing significant performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question answering models can be adapted to extract facts from HTML by fine-tuning on HTML-embedded QA pairs derived from Wikidata.
- Mechanism: The approach casts HTML fact extraction as an extractive QA task, using Wikidata triples to generate question-context pairs. The model learns to locate and extract answer spans within HTML content.
- Core assumption: The textual evidence for missing facts in external websites can be mapped to QA-style questions using property names, and HTML can be processed similarly to plain text with minor adaptations.
- Evidence anchors:
  - [abstract] "We take inspiration from ideas that are used to extract facts from textual collections and adapt them to extract facts from Web pages."
  - [section 3.3] "We solve the Web extraction task by mapping it to an extractive question-answering task."
  - [corpus] Weak corpus signal; no direct mention of QA on HTML.
- Break condition: If the textual evidence is too ambiguous or embedded in complex HTML structures, the model may fail to locate the correct span.

### Mechanism 2
- Claim: Pre-training a QA model on HTML-specific content improves zero-shot and few-shot performance.
- Mechanism: A baseline QA model fine-tuned on SQuAD is further pre-trained on a large corpus of HTML-embedded QA pairs from Wikidata. This adaptation teaches the model HTML-specific patterns and improves generalization to unseen domains.
- Core assumption: Learning HTML-specific patterns (e.g., tag structures, text layouts) reduces the domain adaptation gap and improves performance on new websites.
- Evidence anchors:
  - [section 4.3] "Overall the language model needs to learn: (1) The task of Web extraction is far from the QA task... (2) Patterns that are specific to a domain... (3) Patterns that are specific to the property that is extracted."
  - [section 4.3] "To assess how pre-training the language model for Web extraction can affect our framework, we design an additional experiment."
  - [corpus] Weak corpus signal; no direct mention of pre-training on HTML.
- Break condition: If the HTML patterns are too diverse or if the pre-training data is not representative of the target domains, performance gains may be limited.

### Mechanism 3
- Claim: Object linking can resolve entity ambiguity by learning to rank candidate entities based on their contextual features in Wikidata.
- Mechanism: For each textual evidence, the model ranks candidate entities linked to the same label in Wikidata. It uses features derived from the entities' relationships (e.g., "public university" vs "football club") to predict the most likely match.
- Core assumption: The context of entities in Wikidata (their properties and relationships) provides discriminative features that can disambiguate between entities with the same label.
- Evidence anchors:
  - [section 3.4] "We construct a machine learning-based linker with the following training strategy: We extract from Wikidata a sample of the objects... We train a learning-to-rank model which ranks the features associated to..."
  - [section 4.3] "The end-to-end performance after linking drops to65.3 % Hit@1. This is due to the ambiguity of the linked terms."
  - [corpus] Weak corpus signal; no direct mention of learning-to-rank for entity linking.
- Break condition: If the textual evidence is too ambiguous or if the Wikidata context is insufficient to distinguish between candidates, the linking step may introduce errors.

## Foundational Learning

- Concept: Extractive Question Answering
  - Why needed here: The framework relies on extractive QA to locate and extract answer spans from HTML content, mapping fact extraction to a well-studied NLP task.
  - Quick check question: Can you explain the difference between extractive and generative QA, and why extractive QA is suitable for this task?

- Concept: Few-shot and Zero-shot Learning
  - Why needed here: The framework must perform well with limited or no training data for new domains, requiring the ability to adapt quickly to new tasks with minimal examples.
  - Quick check question: What are the key differences between few-shot and zero-shot learning, and how does pre-training help in both scenarios?

- Concept: Entity Linking and Disambiguation
  - Why needed here: Extracted textual evidence must be linked to the correct Wikidata entity, requiring disambiguation between entities with the same label.
  - Quick check question: How does the learning-to-rank approach help disambiguate entities, and what features are used to rank candidates?

## Architecture Onboarding

- Component map:
  - Knowledge Selection -> Data Cleaning -> Relation Extraction -> Object Linking -> WikidataComplete Integration

- Critical path: Knowledge Selection → Data Cleaning → Relation Extraction → Object Linking → WikidataComplete Integration.

- Design tradeoffs:
  - Using QA models allows fine-grained extraction but requires significant training data and computational resources.
  - Pre-training on HTML improves few-shot performance but may introduce domain bias.
  - Object linking reduces ambiguity but adds complexity and potential errors.

- Failure signatures:
  - Low F1-score in relation extraction: Model struggles to locate correct answer spans in HTML.
  - Low Hit@1 in object linking: Model fails to disambiguate between entities with the same label.
  - High variance across domains: Model performance depends heavily on the structure and quality of the target websites.

- First 3 experiments:
  1. Fine-tune RoBERTa-Base on SQuAD, then evaluate on a small set of HTML-embedded QA pairs to establish baseline performance.
  2. Pre-train RoBERTa-Base on a large corpus of HTML-embedded QA pairs from Wikidata, then evaluate few-shot and zero-shot performance on unseen domains.
  3. Implement and evaluate the learning-to-rank object linker on a sample of ambiguous entity mentions, measuring improvement in Hit@1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the WebExtractor framework vary across different HTML structures (e.g., tables vs. lists vs. free text) within the same domain?
- Basis in paper: [inferred] The paper discusses extracting facts from structured, semi-structured, and unstructured HTML content but does not provide detailed performance breakdowns by HTML structure type.
- Why unresolved: The experiments focus on domain and property-level performance without analyzing the impact of specific HTML structures on extraction accuracy.
- What evidence would resolve it: Conducting experiments that categorize test data by HTML structure type and measuring F1-scores for each category within domains.

### Open Question 2
- Question: What is the impact of website blocking mechanisms (e.g., CAPTCHAs, rate limiting) on the scalability of the WebExtractor framework across all 7,220 unique domains?
- Basis in paper: [explicit] The paper mentions that many domains were excluded during curation because they blocked requests, but does not quantify the extent of this limitation.
- Why unresolved: The study only reports on 54 out of 7,220 domains, leaving uncertainty about the framework's performance on the remaining domains.
- What evidence would resolve it: Testing the framework on a representative sample of the remaining 7,166 domains and measuring success rates, including handling of blocking mechanisms.

### Open Question 3
- Question: How does the framework's performance change when applied to multilingual websites, particularly for properties with names that vary across languages?
- Basis in paper: [inferred] The paper focuses on English-language extraction and does not address multilingual capabilities or property name variations.
- Why unresolved: The experiments are conducted on English-language data, and the question-answering model is trained on English datasets, leaving performance on non-English sites unexplored.
- What evidence would resolve it: Training and evaluating the framework on multilingual datasets and measuring F1-scores across different languages for the same properties.

## Limitations
- Object linking remains a significant bottleneck, with end-to-end performance dropping from 84.07% to 65.3% Hit@1 accuracy
- Framework performance varies substantially across different websites and domains, suggesting sensitivity to HTML structure and content quality
- Relies heavily on the quality and coverage of external identifiers in Wikidata, which may be incomplete for many domains

## Confidence

- High confidence in the core mechanism of casting web extraction as QA task
- Medium confidence in the pre-training strategy for HTML adaptation
- Medium confidence in the learning-to-rank approach for object linking
- Low confidence in generalizability across diverse web domains

## Next Checks

1. **Domain Transfer Validation**: Test the framework on 10-15 additional domains not included in the original 54, measuring performance degradation and identifying which domain characteristics most affect accuracy.

2. **HTML Structure Robustness Test**: Systematically vary HTML structures (nested tables, different tag hierarchies, dynamic content) to quantify how preprocessing and tokenization strategies impact extraction accuracy.

3. **Entity Linking Error Analysis**: Conduct detailed analysis of false positives in object linking to determine whether errors stem from Wikidata context insufficiency, textual ambiguity, or model limitations, then propose targeted improvements.