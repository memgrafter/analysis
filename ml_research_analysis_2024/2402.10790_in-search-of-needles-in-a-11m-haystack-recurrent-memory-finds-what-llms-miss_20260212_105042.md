---
ver: rpa2
title: 'In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss'
arxiv_id: '2402.10790'
source_url: https://arxiv.org/abs/2402.10790
tags:
- memory
- context
- facts
- tokens
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BABILong, a benchmark designed to evaluate
  generative models' ability to process extremely long documents containing distributed
  facts. The benchmark extends the bAbI dataset by embedding algorithmic tasks within
  background text from Project Gutenberg 19, allowing evaluation of models on sequences
  up to 11 million tokens.
---

# In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss

## Quick Facts
- arXiv ID: 2402.10790
- Source URL: https://arxiv.org/abs/2402.10790
- Authors: Yuri Kuratov; Aydar Bulatov; Petr Anokhin; Dmitry Sorokin; Artyom Sorokin; Mikhail Burtsev
- Reference count: 34
- One-line primary result: Recurrent memory transformers (RMT-R) can process sequences up to 11 million tokens, outperforming GPT-4 and Mistral on long-context question answering tasks.

## Executive Summary
This paper introduces BABILong, a benchmark designed to evaluate generative models' ability to process extremely long documents containing distributed facts. The benchmark extends the bAbI dataset by embedding algorithmic tasks within background text from Project Gutenberg, allowing evaluation of models on sequences up to 11 million tokens. The study demonstrates that while GPT-4 and Mistral LLMs fail when context exceeds 25% of their window, and retrieval-augmented generation shows poor scalability, a GPT-2 model augmented with recurrent memory and self-retrieval achieves state-of-the-art performance on tasks up to 11M tokens.

## Method Summary
The authors introduce the BABILong benchmark to evaluate long-context processing capabilities. They compare four approaches: GPT-4 and Mistral LLMs, retrieval-augmented generation (RAG) with FAISS embeddings, recurrent memory transformer (RMT), and recurrent memory transformer with self-retrieval (RMT-R). RMT and RMT-R are implemented with GPT-2 backbone and trained using curriculum learning with segment size 512 and memory size 16. The evaluation spans tasks with contexts ranging from 0 to 11 million tokens, measuring accuracy in identifying relevant facts embedded in extensive background text.

## Key Results
- GPT-4 and Mistral LLMs fail to solve tasks when context exceeds 25% of their window, despite having 128K and 32K token contexts respectively
- Retrieval-augmented generation with FAISS embeddings shows poor scalability with increasing context length
- RMT-R achieves state-of-the-art performance on tasks up to 11M tokens, particularly excelling at distinguishing relevant facts from background text

## Why This Works (Mechanism)

### Mechanism 1
Recurrent Memory Transformer with self-retrieval (RMT-R) can scale to sequences up to 11 million tokens by processing input segments sequentially while maintaining and updating memory states. The self-retrieval component allows each memory token to attend to all past memory states, overcoming the bottleneck of fixed-size recurrent states. This enables effective storage and retrieval of relevant facts from background text.

### Mechanism 2
GPT-4 and Mistral LLMs fail when context exceeds 25% of their window because the additive nature of self-attention makes it harder to locate the few supporting facts as distracting text grows. The models seem to effectively use only a fraction of their context window, unable to efficiently utilize their full capacity for identifying relevant facts.

### Mechanism 3
Retrieval-augmented generation with FAISS embeddings shows poor scalability because the retrieval component struggles to find relevant facts among increasing amounts of background text. The default retrieval algorithm lacks consideration for temporal dynamics inherent in the task, making it ineffective as context grows.

## Foundational Learning

- **Self-attention mechanism in transformers**: Understanding how transformers process input sequences and the limitations of self-attention when dealing with long contexts. *Quick check: How does computational complexity of self-attention scale with input length, and what are the implications for processing extremely long sequences?*

- **Recurrent neural networks (RNNs) and their variants**: Grasping the basics of recurrent architectures and how they can process sequential data by maintaining hidden states. *Quick check: What is the key difference between standard RNNs and transformers, and how do recurrent architectures overcome limitations of transformers for long sequences?*

- **Memory-augmented neural networks**: Understanding how external or internal memory can be integrated into neural networks to store and retrieve information over long sequences. *Quick check: How do memory-augmented neural networks differ from standard RNNs or transformers, and what are the benefits of using memory for processing long sequences?*

## Architecture Onboarding

- **Component map**: Input sequences (up to 11M tokens) -> Segmented processing -> RMT/RMT-R (Transformer layers + Memory tokens [+ Self-retrieval]) -> Output answers

- **Critical path**: 1. Segment the long input sequence 2. Process each segment with RMT/RMT-R 3. Update memory states based on current segment 4. Retrieve relevant information from memory states (RMT-R only) 5. Generate answer based on retrieved information

- **Design tradeoffs**: Memory size vs. model capacity (larger memory allows storing more information but increases complexity); Segment size vs. parallelism (smaller segments allow more parallel processing but may lose long-range dependencies); Self-retrieval vs. memory efficiency (RMT-R can retrieve more relevant information but requires storing all past memory states)

- **Failure signatures**: Degradation in performance when context length exceeds 25% of model window; Poor scalability of retrieval-augmented generation with increasing context length; Inability to distinguish relevant facts from background text in extremely long sequences

- **First 3 experiments**: 1. Evaluate GPT-4 and Mistral on BABILong tasks with varying context lengths to confirm failure at 25% of window 2. Implement RMT with different memory sizes and segment lengths to find optimal configuration 3. Compare RMT and RMT-R performance on BABILong tasks to assess impact of self-retrieval

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the impact of different background text sources on the performance of models in the BABILong benchmark? The authors mention using PG19 and Wiki as background text sources and note that other sources may have different effects, but do not provide experimental results comparing different sources.

- **Open Question 2**: How does the performance of RMT-R scale with extremely long sequences beyond 11 million tokens? The authors state that RMT-R was evaluated up to 10 million tokens and did not encounter a memory limit, suggesting potential for longer sequences, but do not provide experimental results for sequences longer than 10 million tokens.

- **Open Question 3**: What is the optimal strategy for chunking text when using retrieval-augmented generation (RAG) with extremely long contexts? The authors experimented with sentence and 512-token chunk sizes and found sentence chunks performed better, but note this may not generalize to real-world applications.

## Limitations

- Implementation details of RMT and RMT-R architectures are not fully specified, particularly regarding memory token handling and the self-retrieval mechanism
- Exact training hyperparameters (learning rates, batch sizes, optimizer settings) are not provided, creating uncertainty in reproducing results
- Evaluation focuses on algorithmic tasks embedded in Project Gutenberg background text, leaving generalizability to real-world long-document tasks untested

## Confidence

- **High Confidence**: GPT-4 and Mistral LLMs fail to solve tasks when context exceeds 25% of their window; RMT-R achieves state-of-the-art performance on BABILong tasks up to 11M tokens; Retrieval-augmented generation with FAISS embeddings shows poor scalability with increasing context length
- **Medium Confidence**: RMT-R particularly excels at distinguishing relevant facts from background text; Attention maps reveal distinct memory operations for fact storage and retrieval; The specific superiority of RMT-R over RMT is primarily demonstrated on task qa2

## Next Checks

1. **Cross-task generalization test**: Evaluate RMT-R performance on a subset of BABILong tasks (qa1, qa3-qa5) to verify that the reported superiority extends beyond the single task (qa2) where it was primarily demonstrated.

2. **Memory efficiency analysis**: Measure the memory utilization patterns during RMT-R processing to verify the claimed ability to distinguish relevant facts from background text, including quantitative analysis of memory token activation patterns and attention distributions.

3. **Scaling behavior validation**: Conduct controlled experiments varying both memory size and segment length to determine the precise scaling relationship between these parameters and performance on BABILong tasks, helping identify whether reported results represent optimal configurations or if there's headroom for improvement.