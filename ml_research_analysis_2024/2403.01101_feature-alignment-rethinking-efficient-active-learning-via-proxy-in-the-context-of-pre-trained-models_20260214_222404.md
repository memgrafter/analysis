---
ver: rpa2
title: 'Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context
  of Pre-trained Models'
arxiv_id: '2403.01101'
source_url: https://arxiv.org/abs/2403.01101
tags:
- learning
- active
- samples
- standard
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles computational inefficiency in active learning
  with pre-trained models by proposing aligned selection via proxy (ASVP). The core
  method updates pre-computed features when they underperform fine-tuned features
  and switches between linear probing then fine-tuning (LP-FT) and fine-tuning (FT)
  based on LogME-PED scores to preserve valuable pre-trained information.
---

# Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models

## Quick Facts
- arXiv ID: 2403.01101
- Source URL: https://arxiv.org/abs/2403.01101
- Authors: Ziting Wen; Oscar Pizarro; Stefan Williams
- Reference count: 40
- Primary result: ASVP achieves up to 273% sample savings on Pets dataset while reducing active learning time significantly compared to SVPp

## Executive Summary
This paper addresses computational inefficiency in active learning with pre-trained models by proposing Aligned Selection via Proxy (ASVP). The method updates pre-computed features when they underperform fine-tuned features and switches between linear probing then fine-tuning (LP-FT) and fine-tuning (FT) based on LogME-PED scores. ASVP maintains computational efficiency while achieving comparable or better performance than standard active learning methods, with significant time savings on large datasets like ImageNet.

## Method Summary
ASVP improves proxy-based active learning by addressing two key inefficiencies: outdated pre-computed features and improper training method selection. The method evaluates when fine-tuned features outperform pre-computed ones using LogME-PED scores, updates features accordingly, and selects between LP-FT and FT training methods based on proxy model confidence. This dual approach preserves valuable pre-trained information while improving sample selection quality, achieving up to 273% sample savings on Pets dataset.

## Key Results
- ASVP achieves comparable or better performance than standard active learning while maintaining computational efficiency
- Sample savings ratios reach up to 273% on Pets dataset compared to random sampling
- Active learning time reduced from ~70 hours to ~2 hours on ImageNet with 25% labeling budget
- Superior performance across multiple datasets including ImageNet, CIFAR-10/100, and Pets

## Why This Works (Mechanism)

### Mechanism 1
Updating pre-computed features based on fine-tuned model performance reduces redundant sample selection. When LogME-PED scores indicate fine-tuned features are better at distinguishing sample categories, replacing pre-computed features with fine-tuned ones improves proxy model discrimination. The core assumption is that proxy model selection quality degrades when pre-computed features underperform fine-tuned features on distinguishing sample categories.

### Mechanism 2
Switching between LP-FT and FT preserves valuable pre-trained information. The method uses LP-FT when proxy model confidently predicts samples that fine-tuned model misclassifies, minimizing fine-tuning-induced feature distortion. The core assumption is that samples the proxy model predicts correctly but fine-tuned model misclassifies are important for retaining pre-trained model information.

### Mechanism 3
Sample saving ratio metric accurately quantifies labeling cost savings from active learning. The metric compares samples needed by random baseline to achieve same accuracy as active learning method. The core assumption is that interpolation between accuracy and sample count for random baseline is accurate and representative.

## Foundational Learning

- Concept: Active learning with pre-trained models
  - Why needed here: Understanding how standard active learning differs from proxy-based methods is critical for grasping ASVP's improvements
  - Quick check question: What is the key computational bottleneck when applying standard active learning to large pre-trained models?

- Concept: Feature space analysis in deep learning
  - Why needed here: The paper's core insight depends on understanding how feature representations evolve during fine-tuning
  - Quick check question: How do pre-trained and fine-tuned features differ in their ability to distinguish between classes as more labeled data is added?

- Concept: Linear probing and fine-tuning training methods
  - Why needed here: ASVP's switching mechanism between LP-FT and FT requires understanding both approaches
  - Quick check question: What is the primary difference between linear probing and fine-tuning in terms of which model parameters are updated?

## Architecture Onboarding

- Component map:
  Pre-trained model backbone -> Proxy model (2-layer MLP) using pre-computed features -> LogME-PED evaluation module -> Training method selector (LP-FT/FT) -> Feature updater (synchronizes pre-computed features with fine-tuned ones)

- Critical path:
  1. Forward pass all samples through pre-trained model
  2. Train proxy model with pre-computed features
  3. Select samples using active learning strategy
  4. Evaluate LogME-PED difference
  5. If difference > threshold, fine-tune model and update features
  6. Choose training method (LP-FT/FT) based on proxy vs fine-tuned performance
  7. Train final model with selected method
  8. Repeat until convergence

- Design tradeoffs:
  - Update frequency vs. computational cost: More frequent updates improve selection quality but increase computation
  - Training method selection criteria: Balancing between preserving pre-trained knowledge and correcting misclassifications
  - Proxy model complexity: More complex proxies may better approximate fine-tuned model but reduce efficiency gains

- Failure signatures:
  - High LogME-PED differences with no feature updates: Threshold too conservative
  - Rapid oscillation between LP-FT and FT: Selection criteria too sensitive
  - Accuracy plateaus early: Insufficient feature updates or inappropriate training method selection

- First 3 experiments:
  1. Reproduce baseline SVPp performance on CIFAR-10 with margin sampling to establish comparison point
  2. Implement feature alignment only (ASVP without training method switching) to isolate its impact
  3. Implement full ASVP with LogME-PED threshold tuning to find optimal update frequency

## Open Questions the Paper Calls Out
None

## Limitations
- The LogME-PED threshold of 0.03 is empirically chosen but lacks theoretical justification for generalizability across different datasets and model architectures
- The switching mechanism between LP-FT and FT relies on a binary classification of samples into regions B1 and B2, which may oversimplify complex relationships
- Evaluation focuses primarily on standard image classification datasets without extensive testing on diverse data types

## Confidence
**High confidence** in computational efficiency claims due to direct wall-clock time measurements and sample savings ratio calculations

**Medium confidence** in feature alignment mechanism effectiveness based on observed LogME-PED trends

**Medium confidence** in training method switching based on binary region classification heuristic

## Next Checks
1. Test ASVP performance across multiple datasets with varying LogME-PED thresholds (0.01, 0.05, 0.1) to establish robustness

2. Compare ASVP performance with a fixed training method (always LP-FT or always FT) across different sample sizes to quantify the exact contribution of the switching mechanism

3. Track LogME-PED differences and selection quality metrics throughout the active learning process to identify optimal update frequencies and potential overfitting to early-stage feature distributions