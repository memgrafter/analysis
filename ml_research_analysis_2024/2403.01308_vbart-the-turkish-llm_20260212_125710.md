---
ver: rpa2
title: 'VBART: The Turkish LLM'
arxiv_id: '2403.01308'
source_url: https://arxiv.org/abs/2403.01308
tags:
- arxiv
- text
- turkish
- language
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VBART, the first Turkish sequence-to-sequence
  large language model (LLM) trained from scratch. VBART comes in two sizes, Large
  (387M parameters) and XLarge (740M parameters), and outperforms existing multilingual
  models in tasks like text summarization, title generation, text paraphrasing, question
  answering, and question generation.
---

# VBART: The Turkish LLM

## Quick Facts
- arXiv ID: 2403.01308
- Source URL: https://arxiv.org/abs/2403.01308
- Authors: Meliksah Turker; Mehmet Erdi Ari; Aydin Han
- Reference count: 0
- Key outcome: First Turkish sequence-to-sequence LLM trained from scratch, achieving up to 3x better performance than multilingual models with fewer parameters

## Executive Summary
This paper introduces VBART, the first Turkish sequence-to-sequence large language model trained from scratch. VBART comes in two sizes (387M and 740M parameters) and demonstrates state-of-the-art performance on Turkish text generation tasks including summarization, title generation, paraphrasing, and question answering. The model outperforms existing multilingual approaches by up to 3x while using fewer parameters, primarily due to its monolingual pre-training approach and efficient Turkish tokenizer.

## Method Summary
VBART is built using a BART-based encoder-decoder architecture with sinusoidal positional embeddings. The model is pre-trained on 25.33B Turkish subword tokens from a 135GB corpus using sentence permutation with span masking (30% tokens masked). A dynamic data generator creates varied training samples through stochastic text noising processes. The model includes an enlargement method that transfers weights from a smaller pre-trained model while initializing additional layers from scratch. A monolingual SentencePiece tokenizer (32k vocabulary) is trained specifically on Turkish text to improve efficiency.

## Key Results
- VBART outperforms multilingual mT5 and mBART models by up to 3x on Turkish text generation tasks
- Monolingual tokenizer is up to 11x more efficient than multilingual alternatives
- XLarge model (740M parameters) achieves state-of-the-art results across all tested Turkish benchmarks
- Model enlargement technique enables efficient scaling while preserving learned representations

## Why This Works (Mechanism)

### Mechanism 1
Monolingual pre-training on Turkish text yields better performance than multilingual models with fewer parameters. The model learns language-specific patterns without dilution from other languages, allowing efficient specialization for Turkish text generation tasks. This assumes language-specific tokenization provides optimal foundation for Turkish tasks. Evidence shows VBART outperforms mT5 and mBART models despite having significantly fewer parameters. Break condition: Insufficient Turkish corpus size or quality to capture full linguistic diversity.

### Mechanism 2
Dynamic data generation with stochastic text noising creates more robust training samples. Each text input generates different input-output pairs through randomization of masking and permutation, augmenting the dataset beyond simple repetition. This assumes the same text can generate varied examples through randomization. Evidence shows the dynamic generator creates different permutations even when sampling the same text. Break condition: Noising parameters too aggressive or conservative, creating trivial or impossible reconstruction tasks.

### Mechanism 3
Model enlargement using transferred weights from a pre-trained smaller model accelerates training convergence. By initializing odd-numbered layers from the pre-trained model and even-numbered layers from scratch, the enlarged model inherits useful representations while expanding capacity. This assumes layer-wise initialization preserves useful features while allowing new layers to learn complementary representations. Evidence shows the enlargement technique successfully scales from Large to XLarge models. Break condition: Layer-wise copying disrupts internal representation flow or requires significantly different hyperparameters.

## Foundational Learning

- **Sequence-to-sequence pre-training objectives**: Teaches the model to reconstruct corrupted text, essential for conditional text generation tasks like summarization and paraphrasing. Quick check: Can the model accurately reconstruct a sentence where 30% of tokens are masked and the sentence order is permuted?

- **Tokenizer vocabulary size and efficiency trade-offs**: Turkish has agglutinative morphology requiring careful tokenization to balance vocabulary size against tokenization efficiency. Quick check: Does the Turkish tokenizer represent common Turkish words as single tokens rather than splitting them unnecessarily?

- **Model scaling laws and parameter efficiency**: Understanding how model performance scales with parameters helps determine optimal model sizes for resource-constrained languages. Quick check: Does doubling model parameters result in proportionally better performance, or are there diminishing returns?

## Architecture Onboarding

- **Component map**: Corpus -> Tokenizer (SentencePiece Unigram, 32k vocab) -> BART encoder-decoder architecture -> Dynamic data generator -> Training pipeline -> Fine-tuning on downstream tasks -> Evaluation
- **Critical path**: Pre-training → Model enlargement (if needed) → Fine-tuning on downstream tasks → Evaluation
- **Design tradeoffs**: Monolingual vs multilingual tokenization (efficiency vs coverage), model size vs training cost, dropout scheduling for training stability
- **Failure signatures**: Training instability (loss spikes), poor downstream performance (metrics below baselines), inefficient tokenization (excessive token counts for Turkish text)
- **First 3 experiments**: 
  1. Verify tokenizer efficiency by comparing token counts on Turkish text vs multilingual tokenizers
  2. Test pre-training convergence by monitoring loss curves and checking for characteristic "steep drops"
  3. Validate fine-tuning on a simple task (e.g., title generation) before scaling to more complex tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Cross-lingual generalization limits: Monolingual approach may sacrifice multilingual flexibility for Turkish-specific optimization
- Scaling law validity: Claims about Chinchilla Scaling Law's applicability lack comprehensive empirical validation
- Data quality and representativeness: Corpus composition and potential biases not fully characterized

## Confidence

**High confidence**: Technical implementation (architecture, tokenizer training, pre-training methodology) is well-documented and follows established practices. Monolingual tokenizer efficiency gains are supported by direct comparisons.

**Medium confidence**: Downstream task performance improvements over multilingual baselines are demonstrated but evaluated on limited Turkish benchmarks. 3x performance claim requires validation across broader task diversity.

**Low confidence**: Theoretical claims about scaling laws and model enlargement technique applicability lack comprehensive empirical validation beyond specific cases presented.

## Next Checks

1. Conduct systematic experiments training VBART models at 4-5 different parameter scales to empirically test Chinchilla Scaling Law's applicability to sequence-to-sequence architectures.

2. Test VBART's performance on code-switching tasks and low-resource Turkish dialects to quantify trade-offs between monolingual optimization and multilingual flexibility.

3. Perform comprehensive bias audits across different demographic groups and evaluate model robustness to adversarial inputs and out-of-distribution Turkish text patterns.