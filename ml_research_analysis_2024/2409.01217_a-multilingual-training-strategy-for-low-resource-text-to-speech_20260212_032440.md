---
ver: rpa2
title: A multilingual training strategy for low resource Text to Speech
arxiv_id: '2409.01217'
source_url: https://arxiv.org/abs/2409.01217
tags:
- speech
- language
- data
- languages
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of developing high-quality Text-to-Speech
  (TTS) systems for low-resource languages, particularly Moroccan Darija. The authors
  propose a multilingual training strategy that leverages data from social media to
  construct a small TTS dataset and employs cross-lingual transfer learning.
---

# A multilingual training strategy for low resource Text to Speech

## Quick Facts
- arXiv ID: 2409.01217
- Source URL: https://arxiv.org/abs/2409.01217
- Reference count: 40
- Key outcome: Multilingual pre-training with selected source languages improves intelligibility (MOS 3.72) and naturalness (MOS 3.44) of low-resource Darija TTS

## Executive Summary
This paper addresses the challenge of developing high-quality Text-to-Speech (TTS) systems for low-resource languages, particularly Moroccan Darija. The authors propose a multilingual training strategy that leverages data from social media to construct a small TTS dataset and employs cross-lingual transfer learning. They developed a language similarity model to select source languages (Arabic, Hebrew, French, and Dutch) based on acoustic similarity to Darija. The TTS pipeline involves a three-stage process: pre-training on multilingual corpora, fine-tuning on the target language, and knowledge distillation using a non-autoregressive model. Results show that multilingual pre-training significantly improves the intelligibility and naturalness of the synthesized speech compared to monolingual pre-training.

## Method Summary
The approach involves scraping YouTube audio data to create a 6-hour Darija dataset (1.2 hours parallel), then using a three-stage fine-tuning pipeline. First, a language similarity model (Siamese network with Wav2Vec embeddings) selects source languages acoustically close to Darija. Next, TransformerTTS is pre-trained on concatenated multilingual corpora (Arabic, Hebrew, French, Dutch, and English), then fine-tuned on the target Darija data. Finally, knowledge distillation with FastSpeech2 refines the model using ground-truth durations from the teacher's attention weights. A custom Parallel WaveGAN vocoder is trained on Darija audio for waveform generation.

## Key Results
- Multilingual pre-training (MOS 3.72) outperforms monolingual pre-training for Darija TTS intelligibility
- Knowledge distillation with FastSpeech2 improves prosody and reduces word errors
- Social media data can construct viable TTS datasets when paired text-speech data is unavailable
- Custom Parallel WaveGAN achieves MOS 3.19-3.30 on Darija speech synthesis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual pre-training improves target language TTS performance by transferring shared acoustic and phonetic knowledge.
- Mechanism: A language similarity model (Siamese network with Wav2Vec embeddings) selects source languages acoustically close to Darija. These languages are pooled in a multilingual pre-training stage, allowing the model to learn shared phonetic structures before fine-tuning on the target language.
- Core assumption: Acoustic similarity between languages correlates with transferable phonetic features that benefit TTS synthesis.
- Evidence anchors:
  - [abstract] "Our findings show that multilingual pre-training is better than monolingual pre-training at increasing the intelligibility and naturalness of the generated speech."
  - [section] "We first present a deep learning method to accurately select source languages."
  - [corpus] Weak evidence: No direct comparison of selected vs. non-selected language performance.
- Break condition: If selected source languages are not acoustically similar enough, the multilingual transfer benefit disappears.

### Mechanism 2
- Claim: Using social media data for TTS dataset construction is viable when paired text-speech data is unavailable.
- Mechanism: The authors scrape audio from YouTube story-telling channels, filter by quality (SNR ≥ 20dB), denoise, split into chunks, and pseudo-transcribe with an ASR system. Manual correction ensures transcription accuracy.
- Core assumption: Story-telling content provides sufficiently clean, single-speaker, expressive speech data suitable for TTS.
- Evidence anchors:
  - [section] "We assess the quality of speech and filter out noisy utterances by computing signal to noise ratio (SNR) using the waveform amplitude distribution analysis (WADA)."
  - [section] "We constructed a total of 6 hours of speech only data where each audio file is a single-channel WAV with a sample rate of 22.05kHz."
  - [corpus] Weak evidence: No systematic comparison with professional TTS datasets.
- Break condition: If social media audio quality is too low or domain mismatch exists, synthesis quality degrades significantly.

### Mechanism 3
- Claim: Knowledge distillation with a non-autoregressive model reduces word errors and improves prosody in low-resource TTS.
- Mechanism: After autoregressive TransformerTTS pre-training and fine-tuning, a FastSpeech2 model is trained using ground-truth durations extracted from the teacher's attention weights. This stage reduces substitutions, insertions, and deletions in synthesis.
- Core assumption: Non-autoregressive generation with duration prediction can correct alignment and pronunciation errors from autoregressive training.
- Evidence anchors:
  - [section] "We present the first TTS system that handles the Darija Dialect."
  - [section] "The model is refine-tuned using extracted ground-truth duration of each input token (character in our case) from the attention weights of the autoregressive teacher model."
  - [corpus] Moderate evidence: MOS improvements and word error reductions reported in tables.
- Break condition: If duration prediction is inaccurate, the non-autoregressive model may introduce new errors.

## Foundational Learning

- Concept: Transfer learning in neural TTS
  - Why needed here: Low-resource languages lack sufficient paired data; transfer from related languages can bootstrap performance.
  - Quick check question: How does multilingual pre-training differ from monolingual pre-training in TTS?
- Concept: Language similarity modeling for acoustic transfer
  - Why needed here: Selecting appropriate source languages maximizes transfer benefit; arbitrary selection risks negative transfer.
  - Quick check question: What acoustic features are most relevant for measuring language similarity in TTS?
- Concept: Non-autoregressive speech synthesis
  - Why needed here: Autoregressive models are slow and prone to alignment errors; non-autoregressive models can be faster and more robust.
  - Quick check question: How does FastSpeech2 use duration prediction to replace attention-based alignment?

## Architecture Onboarding

- Component map: TransformerTTS (autoregressive) → FastSpeech2 (non-autoregressive) → Parallel WaveGAN (vocoder). Input: raw text → Output: waveform.
- Critical path: Text → Linguistic features (char-level) → Mel-spectrogram (TransformerTTS) → Fine-tuned mel-spectrogram (FastSpeech2) → Waveform (PWG).
- Design tradeoffs: Multilingual pre-training increases data diversity but risks negative transfer; non-autoregressive models reduce errors but require accurate duration prediction.
- Failure signatures: High MCD indicates poor spectral quality; high word error rate indicates pronunciation/alignment issues; low MOS indicates poor naturalness/intelligibility.
- First 3 experiments:
  1. MONO En → MONO Ar → MONO Dar1: Test monolingual transfer from Arabic.
  2. MONO En → MULTI Ar,Heb,Fr,Du → MONO Dar2: Test multilingual transfer with selected languages.
  3. MONO Dar1 vs MONO Dar2 with KD: Compare monolingual vs multilingual with knowledge distillation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multilingual transfer learning compare to monolingual transfer learning when the source language is not a dialect of the target language?
- Basis in paper: [explicit] The paper states "We compare the performances of the pre-training on monolingual and multilingual corpora" and "We consider pairs (targetLang,si) where si belongs to S" but does not directly compare multilingual transfer to monolingual transfer when the source language is not a dialect.
- Why unresolved: The paper focuses on a case where the target language (Moroccan Darija) is a dialect of one of the source languages (Arabic), limiting the generalizability of the findings to other low-resource language pairs.
- What evidence would resolve it: A controlled experiment comparing multilingual transfer learning with monolingual transfer learning when the source language is not a dialect of the target language, using the same pipeline and dataset sizes.

### Open Question 2
- Question: What is the impact of imbalanced datasets for source languages on the performance of multilingual transfer learning in low-resource TTS?
- Basis in paper: [inferred] The paper mentions "For future works, we intend to focus on experimenting with imbalanced datasets for source languages" suggesting this is an open area of research.
- Why unresolved: The paper does not investigate the effect of imbalanced datasets on multilingual transfer learning performance, leaving a gap in understanding how dataset characteristics affect the approach.
- What evidence would resolve it: An experiment systematically varying the size of source language datasets and measuring the impact on the target language TTS quality.

### Open Question 3
- Question: How does the performance of multilingual transfer learning scale with the number of source languages used?
- Basis in paper: [inferred] The paper uses four source languages (Arabic, Hebrew, French, and Dutch) but does not explore the effect of varying the number of source languages on performance.
- Why unresolved: The paper does not investigate the relationship between the number of source languages and the quality of the target language TTS, leaving uncertainty about the optimal number of languages to include.
- What evidence would resolve it: A series of experiments using different numbers of source languages (e.g., 2, 4, 6, 8) while keeping the total amount of source language data constant, and measuring the impact on target language TTS quality.

## Limitations

- Data representativeness and domain mismatch: The scraped Darija dataset from story-telling content may not cover the full range of phonetic or prosodic variation needed for general-purpose TTS.
- Language similarity model validation: No empirical validation showing that the selected languages (Arabic, Hebrew, French, Dutch) actually perform better than alternative selections.
- Knowledge distillation effectiveness: The paper claims FastSpeech2 refinement reduces word errors, but specific error rates before and after distillation are not reported.

## Confidence

**High confidence**: The general three-stage training pipeline (multilingual pre-training → fine-tuning → knowledge distillation) is technically sound and aligns with established TTS literature. The use of MOS and MCD metrics for evaluation is appropriate.

**Medium confidence**: The specific claim that multilingual pre-training outperforms monolingual pre-training for Darija TTS is supported by the reported MOS scores, but the comparison lacks statistical significance testing and doesn't account for potential confounding factors like training duration or hyperparameter tuning.

**Low confidence**: The assertion that social media data can effectively replace professional TTS datasets is weakly supported, as the paper provides no systematic comparison of synthesis quality using different data sources or professional vs. social media data.

## Next Checks

1. **Ablation study on language selection** - Train identical TTS pipelines using different combinations of source languages (including languages not selected by the similarity model) to empirically validate whether the proposed selection method actually improves performance over random or alternative selections.

2. **Domain transfer analysis** - Test the trained Darija TTS system on out-of-domain text (e.g., news articles, conversational speech) to quantify how well the story-telling-trained model generalizes beyond its training domain, and compare this against a system trained on more diverse data sources.

3. **Baseline comparison with monolingual transfer** - Implement a direct comparison where TransformerTTS is pre-trained on Arabic-only data (the closest language) versus the multilingual approach, keeping all other variables constant, to isolate the benefit of multilingual versus single-language transfer learning.