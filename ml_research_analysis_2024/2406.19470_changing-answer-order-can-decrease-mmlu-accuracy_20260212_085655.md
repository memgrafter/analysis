---
ver: rpa2
title: Changing Answer Order Can Decrease MMLU Accuracy
arxiv_id: '2406.19470'
source_url: https://arxiv.org/abs/2406.19470
tags:
- answer
- mmlu
- language
- association
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of MMLU, a widely-used multiple-choice
  question answering dataset, to answer order shuffling. The authors find that all
  ten tested state-of-the-art language models exhibit decreased accuracy when the
  answer label contents are shuffled, with some models showing drops of over 40%.
---

# Changing Answer Order Can Decrease MMLU Accuracy

## Quick Facts
- arXiv ID: 2406.19470
- Source URL: https://arxiv.org/abs/2406.19470
- Reference count: 25
- Primary result: All tested state-of-the-art language models show decreased accuracy on MMLU when answer order is shuffled

## Executive Summary
This paper investigates the robustness of MMLU, a widely-used multiple-choice question answering dataset, to answer order shuffling. The authors find that all ten tested state-of-the-art language models exhibit decreased accuracy when the answer label contents are shuffled, with some models showing drops of over 40%. To quantify this effect, they introduce a new metric that measures how often a model correctly answers the same question across multiple shuffled versions of MMLU. The authors argue that this metric provides a more reliable assessment of model capabilities by reducing the impact of random chance. They also analyze performance across different MMLU categories, finding that problem-solving categories like mathematics are particularly affected by answer order changes.

## Method Summary
The authors evaluate ten state-of-the-art language models (7B-70B parameters) on the MMLU dataset, testing their robustness to answer order shuffling. They create shuffled versions of MMLU by randomly permuting answer label contents while preserving the correct answer and labels. The models are evaluated on both the original and shuffled versions, and a new metric is introduced to measure consistency across shuffles. This metric calculates how often a model correctly answers the same question across multiple shuffled versions, enforcing that the model correctly identify the answer multiple times rather than just once by chance.

## Key Results
- All ten tested language models show decreased accuracy when answer labels are shuffled
- Accuracy drops range from 4% to over 40% depending on the model
- Problem-solving categories like mathematics are particularly affected by answer order changes
- The new consistency metric significantly lowers the chance of correct answers occurring by random chance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shuffling answer label contents breaks the alignment between answer labels and answer text, causing models to lose the learned association between specific labels (like "A") and correct answers.
- Mechanism: When answer labels are shuffled, the model's prior learned mappings from label to answer content are disrupted. The model may have learned that the correct answer is always under label "A" for certain question types, and when this association is broken, accuracy drops.
- Core assumption: The model has learned label-answer associations rather than understanding the question content deeply enough to identify correct answers regardless of label.
- Evidence anchors:
  - [abstract] "When shuffling the answer label contents, we find that all explored models decrease in accuracy on MMLU"
  - [section 2.1] "To test the robustness of models to answer choice ordering, we shuffle the answer label contents"
  - [corpus] Weak evidence - no direct mention of this specific mechanism in related papers
- Break condition: Models that have been trained or fine-tuned on datasets with shuffled answer labels, or models that explicitly attend to answer content rather than labels.

### Mechanism 2
- Claim: Models exhibit positional bias, preferentially selecting certain answer positions (like first or last) regardless of content.
- Mechanism: The model has learned to favor specific answer positions (A, B, C, D) based on training data patterns. When answer contents are shuffled, the correct answer may move to a position the model is less likely to select.
- Core assumption: Training data exhibits positional bias in correct answer placement that models learn to exploit.
- Evidence anchors:
  - [abstract] "we additionally consider the percentage of examples each model answers correctly by random chance"
  - [section 4] "Some works have also shown that models can perform surprisingly well above random chance even when question text is removed and only answer options are provided"
  - [corpus] "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards" - suggests models exploit dataset patterns
- Break condition: Models trained on balanced datasets where correct answers are uniformly distributed across positions, or models using techniques that eliminate positional bias.

### Mechanism 3
- Claim: Models rely on statistical patterns in answer content rather than semantic understanding, making them vulnerable when answer order changes.
- Mechanism: Models may have learned that certain phrases or word patterns are more likely to appear in correct answers, regardless of the question. When answers are shuffled, these learned patterns no longer align with the correct answers.
- Core assumption: The model has learned surface-level statistical patterns rather than deep semantic understanding of the questions.
- Evidence anchors:
  - [abstract] "they introduce a new metric that measures how often a model correctly answers the same question across multiple shuffled versions of MMLU"
  - [section 2.2] "enforcing that the model correctly identify the answer M times... noticeably lowers the chance of it happening across the correct answer by chance"
  - [corpus] "None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks" - suggests models rely on memorization
- Break condition: Models that have been trained with adversarial examples or that explicitly learn to focus on question semantics over answer patterns.

## Foundational Learning

- Concept: Test-retest reliability
  - Why needed here: The paper's metric is essentially measuring test-retest reliability - how consistently a model can answer the same questions correctly across different versions of the test.
  - Quick check question: If a model answers 80% of questions correctly on the original MMLU and 75% on a shuffled version, what is its test-retest reliability score using the paper's metric?

- Concept: Positional bias in multiple-choice questions
  - Why needed here: Understanding how answer positions (A, B, C, D) can influence model predictions is crucial for interpreting the results of shuffling experiments.
  - Quick check question: If correct answers are distributed as A:30%, B:25%, C:25%, D:20% in a dataset, what positional bias might a model learn?

- Concept: Statistical learning vs. semantic understanding
  - Why needed here: The paper's findings suggest models may be relying on statistical patterns rather than true understanding, which is important for evaluating model capabilities.
  - Quick check question: If a model correctly answers questions 60% of the time when only seeing answer choices (no questions), what does this suggest about its learning approach?

## Architecture Onboarding

- Component map:
  - Input preprocessing: Question text + 4 answer options with labels
  - Model: Large language model (7B-70B parameters)
  - Evaluation: MMLU dataset with 57 tasks
  - Shuffling mechanism: Random permutation of answer contents
  - Metric calculation: Our metric measuring consistency across shuffles

- Critical path:
  1. Load MMLU dataset
  2. Generate shuffled versions of answer contents
  3. Run model inference on original and shuffled versions
  4. Calculate accuracy for each version
  5. Compute consistency metric (Our Metric)
  6. Analyze results by category and model

- Design tradeoffs:
  - Shuffling only answer contents vs. shuffling both labels and contents
  - Number of shuffles to run (computational cost vs. metric reliability)
  - 5-shot in-context learning vs. fine-tuning on MMLU

- Failure signatures:
  - Large accuracy drops (>20%) when shuffling answers
  - High variance in performance across different shuffles
  - Performance on problem-solving categories (math) drops more than other categories
  - Consistency metric significantly lower than original accuracy

- First 3 experiments:
  1. Run original MMLU evaluation on a small model (e.g., Mistral-7B) and record accuracy
  2. Generate 2 shuffled versions of MMLU and evaluate the same model, calculating Our Metric
  3. Compare the original accuracy vs. Our Metric to quantify the robustness drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed metric correlate with other measures of model robustness to input perturbations?
- Basis in paper: [explicit] The paper mentions that robustness to answer order shuffling is one aspect of model robustness, and other works have studied robustness to various input alterations like paraphrasing and word order changes.
- Why unresolved: The paper only tests one specific type of input perturbation (answer order shuffling) and does not compare the proposed metric to other robustness measures.
- What evidence would resolve it: Conducting experiments where the proposed metric is computed for models that have also been evaluated on other robustness benchmarks, and analyzing the correlation between scores.

### Open Question 2
- Question: Is there a relationship between model size and robustness to answer order shuffling?
- Basis in paper: [inferred] The paper observes that smaller models (like Mistral-7B and Gemma-7B) were generally more impacted by answer shuffling, while larger models showed more robustness. However, there are exceptions (e.g., Llama-3-8B being more robust than larger models).
- Why unresolved: The paper only tests a limited number of models and doesn't establish a clear trend between model size and robustness.
- What evidence would resolve it: Evaluating a wider range of models with varying sizes on the proposed metric and analyzing the correlation between model size and robustness scores.

### Open Question 3
- Question: How do different answer shuffling strategies affect the proposed metric?
- Basis in paper: [explicit] The paper uses two random shuffles of the answer label contents, but acknowledges that there are many more possible shuffles that could be tested.
- Why unresolved: The paper only explores a limited number of shuffling strategies and doesn't investigate how different shuffling methods might impact the metric.
- What evidence would resolve it: Experimenting with various shuffling strategies (e.g., different numbers of shuffles, different shuffling algorithms) and analyzing how they affect the proposed metric.

## Limitations

- The study only tests ten models, all from a similar family of transformer-based architectures
- The paper doesn't explore whether the observed effects persist across different prompting strategies or fine-tuning approaches
- Limited exploration of different shuffling strategies and their impact on the proposed metric

## Confidence

- High confidence: The empirical finding that shuffling answer contents decreases accuracy across all tested models (p<0.01 in most cases)
- Medium confidence: The claim that the new metric provides a more reliable assessment of model capabilities, as this depends on untested assumptions about what constitutes "true" understanding
- Medium confidence: The assertion that problem-solving categories are particularly affected, as the analysis is based on aggregated results rather than category-specific statistical tests

## Next Checks

1. Replicate the experiment with additional model architectures (including smaller models and non-transformer approaches) to test generalizability
2. Test whether the observed effects persist when using different prompting strategies or when models are fine-tuned on shuffled versions of the data
3. Conduct statistical significance testing on category-level performance differences to verify that problem-solving categories are indeed more affected than others