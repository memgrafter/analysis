---
ver: rpa2
title: Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities
arxiv_id: '2407.20337'
source_url: https://arxiv.org/abs/2407.20337
tags:
- images
- fake
- real
- code
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoDE, a contrastive learning framework for
  deepfake detection. It trains a vision transformer from scratch to distinguish real
  and fake images using global-local similarity matching.
---

# Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities

## Quick Facts
- **arXiv ID**: 2407.20337
- **Source URL**: https://arxiv.org/abs/2407.20337
- **Reference count**: 40
- **Primary result**: Introduces CoDE, a contrastive learning framework for deepfake detection that outperforms CLIP-based models on a 9.2M image dataset of diffusion-generated images while using only 5M parameters.

## Executive Summary
This paper presents CoDE, a novel contrastive learning framework for deepfake detection that leverages global-local similarity matching. Unlike existing approaches that rely on large pre-trained models like CLIP, CoDE is trained from scratch on a carefully constructed dataset of real and AI-generated images. The method uses a Vision Transformer Tiny architecture and enforces alignment between local and global image crops while maintaining separation between real and fake embeddings. This approach enables CoDE to achieve state-of-the-art results on deepfake detection while being significantly more parameter-efficient than existing methods.

## Method Summary
CoDE uses contrastive learning with InfoNCE loss to train a Vision Transformer from scratch on paired real and fake images. The key innovation is the global-local similarity matching mechanism, where the model learns to align embeddings from local crops (96x96) with their corresponding global crops (224x224) while maintaining separation between real and fake image clusters. The training uses a multi-crop strategy with random transformations to enhance robustness. The model is trained on a newly created dataset (D3) containing 9.2M images generated by four different diffusion models paired with real images from LAION-400M. The architecture consists of a ViT-Tiny backbone followed by L2 normalization and classification layers.

## Key Results
- CoDE achieves state-of-the-art accuracy on the D3 test set while using only 5M parameters compared to 307M for CLIP ViT-L
- The model shows strong generalization to unseen diffusion generators, achieving high accuracy on an extended test set with 12 additional models
- CoDE outperforms CLIP-based models by significant margins on diffusion-generated images while being more parameter-efficient
- The global-local similarity matching component provides substantial performance gains over global-only contrastive learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoDE's contrastive learning with global-local similarity matching outperforms CLIP-based models on deepfake detection.
- Mechanism: CoDE enforces alignment between local and global image crops while maintaining separation between real and fake embeddings, enabling the model to learn fine-grained discriminative features that CLIP's global-only alignment misses.
- Core assumption: Local features contain critical discriminative information for distinguishing real from fake images that is not captured by global image-level features alone.
- Evidence anchors:
  - [abstract] "CoDE is trained via contrastive learning by additionally enforcing global-local similarities"
  - [section 3] "Detecting a generated image... is also a setting where local image features play an important role"
  - [corpus] Weak evidence - no direct comparison to CLIP's local feature performance
- Break condition: If local-to-global correspondences are not discriminative for the specific diffusion generators being evaluated, or if the model fails to learn meaningful local features.

### Mechanism 2
- Claim: Training from scratch on paired real-fake data enables better architectural efficiency than using pre-trained CLIP models.
- Mechanism: By training from scratch exclusively for deepfake detection, CoDE can use a smaller ViT-Tiny backbone (5M parameters) while achieving superior performance compared to larger pre-trained models like CLIP ViT-L (307M parameters).
- Core assumption: Pre-training on general image-text pairs is not optimal for the specific task of deepfake detection, and task-specific training allows for architectural simplification.
- Evidence anchors:
  - [abstract] "by training from scratch on the real-fake distribution, we also reduce the scale of the model and find that a ViT Tiny architecture can provide state-of-the-art results"
  - [section 5.2] "CoDE employs 5M parameters compared to RN50 (26M), ViT-B (86M), and ViT-L (307M)"
  - [corpus] Weak evidence - no ablation comparing pre-trained vs scratch on identical architectures
- Break condition: If the dataset size is insufficient for effective scratch training, or if pre-training on related tasks provides significant benefits not captured in this evaluation.

### Mechanism 3
- Claim: The multi-crop strategy with local and global views enables learning of multi-scale discriminative features.
- Mechanism: By extracting and comparing both global crops (covering large areas) and local crops (covering small areas) of real and fake images, CoDE learns features at multiple scales that capture both overall image characteristics and fine-grained details.
- Core assumption: Deepfake detection requires both global context understanding and local detail analysis, and these different scales provide complementary information.
- Evidence anchors:
  - [section 3] "we build a loss component that acts in a multi-scale manner... local scale and global scale views"
  - [section 5.2] "the performance gain achieved by the best version of CoDE is significantly higher... highlighting the appropriateness of employing global-local correspondences"
  - [corpus] Weak evidence - no ablation isolating the contribution of different scale combinations
- Break condition: If the optimal crop sizes or scale ratios change significantly with different image resolutions or generator types.

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: Enables learning of discriminative embeddings by pulling together similar samples (real-real, fake-fake) while pushing apart dissimilar ones (real-fake)
  - Quick check question: How does the temperature parameter τ in InfoNCE affect the separation between positive and negative pairs in the embedding space?

- Concept: Multi-scale feature learning through multi-crop strategies
  - Why needed here: Deepfake detection requires analysis at both global and local levels, which multi-crop enables by forcing the model to learn consistent representations across different spatial scales
  - Quick check question: What is the typical resolution ratio between global and local crops in multi-crop strategies, and why is this ratio important?

- Concept: Vision Transformer architecture and patch embeddings
  - Why needed here: ViT provides a flexible backbone that can be trained from scratch and easily modified for the task-specific requirements of deepfake detection
  - Quick check question: How does the patch size in ViT affect the model's ability to capture local features relevant for deepfake detection?

## Architecture Onboarding

- Component map: Input images → Transform operator (T) → Multi-crop extraction (global 224x224, local 96x96) → Vision Transformer backbone → L2 normalization → Global contrastive loss (Lglobal) + Multi-scale contrastive loss (Lmulti-scale) → Classification layer (linear/NN/SVM)
- Critical path: Transform → Multi-crop → Backbone → Loss computation → Parameter update
- Design tradeoffs: Smaller ViT-Tiny vs larger pre-trained models (parameter efficiency vs potential pre-training benefits), single backbone vs teacher-student architecture (simplicity vs potential performance), transform augmentation strength vs maintaining image quality
- Failure signatures: Poor separation in embedding space (real and fake clusters overlap), sensitivity to transformations (performance drops significantly with augmented test images), inability to generalize to unseen generators (high accuracy on training generators but poor on test generators)
- First 3 experiments:
  1. Train CoDE on D3 with only Lglobal loss to isolate the contribution of global vs multi-scale learning
  2. Compare CoDE with pre-trained ViT-Tiny fine-tuned on D3 to measure the benefit of scratch training
  3. Evaluate CoDE with different local-to-global crop size ratios to find the optimal scale combination for deepfake detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CoDE perform on deepfakes generated by non-diffusion models like GANs or autoregressive models?
- Basis in paper: [explicit] The paper mentions that CoDE was primarily evaluated on diffusion models and that its performance on GAN-generated images was significantly lower compared to diffusion-generated ones.
- Why unresolved: The paper focused on diffusion models as they represent the current state-of-the-art in image generation, but did not extensively test CoDE on other types of generative models.
- What evidence would resolve it: Testing CoDE on datasets containing images generated by various GANs and autoregressive models, and comparing its performance to other deepfake detection methods.

### Open Question 2
- Question: Can CoDE's architecture be further optimized for even faster inference without sacrificing accuracy?
- Basis in paper: [inferred] The paper highlights CoDE's efficiency compared to other models but also mentions that there is always room for optimization in real-time applications.
- Why unresolved: While CoDE is faster than many existing methods, there might be architectural modifications or training techniques that could further improve its speed.
- What evidence would resolve it: Experimenting with different backbone architectures, quantization techniques, or model compression methods to reduce CoDE's parameter count and improve inference speed while maintaining or improving its accuracy.

### Open Question 3
- Question: How does CoDE generalize to deepfakes in different domains, such as medical images or satellite imagery?
- Basis in paper: [inferred] The paper primarily evaluated CoDE on natural images and did not explore its performance on other types of images.
- Why unresolved: Different image domains might have distinct characteristics that could affect CoDE's ability to distinguish between real and fake images.
- What evidence would resolve it: Testing CoDE on datasets containing real and fake medical images, satellite imagery, or other specialized image domains, and analyzing its performance compared to domain-specific deepfake detection methods.

### Open Question 4
- Question: Can CoDE be adapted to detect deepfakes in video sequences, considering temporal information?
- Basis in paper: [inferred] The paper focuses on image-level deepfake detection and does not address the challenge of detecting deepfakes in videos.
- Why unresolved: Deepfakes in videos can involve temporal inconsistencies or artifacts that are not present in individual frames, requiring a different approach than image-level detection.
- What evidence would resolve it: Extending CoDE's architecture to incorporate temporal information from video frames, such as using 3D convolutions or recurrent neural networks, and evaluating its performance on video deepfake detection datasets.

## Limitations

- Evaluation focuses primarily on diffusion models with limited testing on other generator types like GANs
- Dataset construction relies on LAION-400M for real images, which may introduce biases or inconsistencies
- Performance under real-world conditions (compression, resolution changes) not extensively tested

## Confidence

- **High Confidence**: The core mechanism of using contrastive learning with global-local similarity matching for deepfake detection is well-supported by the theoretical framework and experimental results on the D3 dataset
- **Medium Confidence**: The claim that training from scratch with a smaller ViT-Tiny architecture is more parameter-efficient than using larger pre-trained models like CLIP ViT-L is supported but lacks direct ablation studies
- **Medium Confidence**: The effectiveness of the multi-scale feature learning through multi-crop strategies is supported by performance gains, but the exact contribution of different scale combinations is not isolated

## Next Checks

1. **Generalization Across Architectures**: Evaluate CoDE on a broader set of 20+ diverse diffusion models including GAN-based, VAE-based, and future diffusion architectures not covered in the current test set to assess true scalability of the approach

2. **Robustness Under Real-World Conditions**: Test CoDE's performance on images with various compression levels (JPEG quality factors 50-95), different resolutions (from 256x256 to 1024x1024), and common post-processing operations to evaluate practical deployment readiness

3. **Ablation of Key Components**: Conduct systematic ablation studies isolating the contributions of (a) scratch training vs pre-training, (b) local vs global features, and (c) different crop size ratios to quantify the exact impact of each design choice on the final performance