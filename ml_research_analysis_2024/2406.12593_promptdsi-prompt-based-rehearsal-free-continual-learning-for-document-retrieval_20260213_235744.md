---
ver: rpa2
title: 'PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document Retrieval'
arxiv_id: '2406.12593'
source_url: https://arxiv.org/abs/2406.12593
tags:
- learning
- promptdsi
- prompt
- retrieval
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PromptDSI, a rehearsal-free continual learning
  method for document retrieval using differentiable search index (DSI). Unlike existing
  DSI approaches that require memory buffers or generative models, PromptDSI employs
  learnable prompts to incrementally index new documents without accessing previous
  data.
---

# PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document Retrieval

## Quick Facts
- arXiv ID: 2406.12593
- Source URL: https://arxiv.org/abs/2406.12593
- Reference count: 40
- Key outcome: PromptDSI achieves state-of-the-art rehearsal-free continual learning for document retrieval using learnable prompts and topic-aware prompt pools, outperforming existing baselines on NQ320k and MSMARCO 300k datasets.

## Executive Summary
PromptDSI introduces a novel rehearsal-free continual learning method for document retrieval using differentiable search index (DSI). The approach employs learnable prompts to incrementally index new documents without accessing previous data, addressing catastrophic forgetting while maintaining retrieval performance. Two key innovations include single-pass PCL that eliminates the initial forward pass for improved efficiency, and a topic-aware prompt pool using neural topic embeddings as fixed keys for better stability and interpretability. Experiments demonstrate that PromptDSI variants outperform rehearsal-based baselines and match strong cache-based approaches while being more parameter-efficient.

## Method Summary
PromptDSI builds upon the DSI architecture by adding a prompt pool for continual learning. During training on new corpora, the system uses prefix-tuning with learnable prompts to adapt to new documents while keeping the DSI backbone frozen. The single-pass PCL variant removes the initial forward pass by using intermediate layer representations for prompt selection. The topic-aware variant replaces learnable prompt keys with fixed neural topic embeddings mined from the initial corpus using BERTopic. The model is trained for 10 epochs with AdamW optimizer, and prompt selection is performed using top-N query-key matching.

## Key Results
- PromptDSI variants outperform rehearsal-based baselines on NQ320k and MSMARCO 300k datasets
- Single-pass PCL achieves comparable performance to two-pass PCL with significant latency reduction
- Topic-aware prompt pool using neural topic embeddings provides stable performance and improved interpretability
- PromptDSI matches strong cache-based baselines in mitigating forgetting while being more parameter-efficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing the first forward pass in PCL methods reduces computational overhead with negligible performance loss.
- Mechanism: Uses average token embeddings from intermediate layer preceding the prompting layer instead of [CLS] token from first pass for prompt selection.
- Core assumption: Query embeddings from intermediate layers retain sufficient semantic information for effective prompt selection.
- Evidence anchors: [abstract] mentions negligible trade-off in performance; [section] describes using AVG from intermediate layer.
- Break condition: If query semantics require deeper contextual understanding, intermediate representations may lose critical information.

### Mechanism 2
- Claim: Using neural topic embeddings as fixed prompt keys stabilizes query-key matching and improves prompt utilization.
- Mechanism: Replaces learnable prompt keys with fixed neural topic embeddings from initial corpus mined using BERTopic.
- Core assumption: Neural topic embeddings from initial corpus provide stable, semantically meaningful keys across subsequent corpora.
- Evidence anchors: [abstract] states topic-aware prompt pool eliminates instability while maintaining competitive performance; [section] describes BERTopic clustering and TF-IDF generation.
- Break condition: If new corpora contain topics entirely absent from initial corpus, fixed topic keys may become less effective.

### Mechanism 3
- Claim: Single-layer prompting is sufficient for optimal performance when adapting PCL methods to document retrieval.
- Mechanism: Identifies through layer-wise analysis that prompting at lower layers (particularly layer 2) achieves best balance between efficiency and performance.
- Core assumption: Document retrieval queries are semantically simple enough that shallow layer representations are sufficient.
- Evidence anchors: [section] verifies single-layer prompting is sufficient for optimal performance; [section] shows layer 2 achieves comparable or lower forgetting than IncDSI.
- Break condition: If document retrieval queries become more complex or require deeper semantic understanding, single-layer prompting may be insufficient.

## Foundational Learning

- Concept: Continual Learning and Catastrophic Forgetting
  - Why needed here: Addresses challenge of incrementally indexing new documents without forgetting previously indexed information
  - Quick check question: What is catastrophic forgetting and why is it particularly problematic in document retrieval systems?

- Concept: Differentiable Search Index (DSI) Architecture
  - Why needed here: PromptDSI builds upon DSI, which maps documents to identifiers through end-to-end learning
  - Quick check question: How does classification-based DSI differ from traditional dense retrieval approaches in terms of document representation?

- Concept: Prompt-based Learning and Prefix Tuning
  - Why needed here: PromptDSI uses learnable prompts to guide frozen DSI backbone for indexing new documents
  - Quick check question: How does prefix tuning differ from traditional prompt tuning in terms of where prompts are inserted in transformer architecture?

## Architecture Onboarding

- Component map: Frozen DSI encoder (θe) -> Linear classifier (θl) -> Prompt pool (P) with learnable prompts -> Prompt keys (K)
- Critical path: Process queries through frozen encoder -> Select top-N prompts using query-key matching -> Apply prefix-tuning with selected prompts -> Predict document identifiers using linear classifier
- Design tradeoffs: Single-pass PCL trades minimal performance loss for significant latency reduction; Fixed topic keys trade some flexibility for training stability and interpretability; Single-layer prompting trades some potential performance for computational efficiency
- Failure signatures: Performance degradation on initial corpus indicates forgetting; Low prompt utilization indicates training instability; Poor retrieval accuracy on new corpora indicates insufficient plasticity
- First 3 experiments:
  1. Implement single-pass PCL variant and compare performance with naive two-pass PCL on small document corpus
  2. Test neural topic embedding generation using BERTopic on initial corpus and validate semantic coherence
  3. Conduct layer-wise prompting analysis to identify optimal prompting layer for document retrieval tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PromptDSI performance scale when applied to very large corpora (e.g., full MS MARCO dataset with 8.8M passages) compared to smaller 300k subset?
- Basis in paper: [explicit] Notes extending to full MS MARCO would require about 25GiB memory for caching, highlighting scalability challenges
- Why unresolved: Experiments conducted on MSMARCO 300k subset, not full dataset; authors acknowledge need for further scalability investigation
- What evidence would resolve it: Conducting experiments on full MS MARCO dataset comparing performance and memory usage with other baselines

### Open Question 2
- Question: Can LLMs for topic modeling in PromptDSITopic lead to further improvements in performance and interpretability compared to current BERTopic approach?
- Basis in paper: [inferred] Suggests topic modeling may benefit from LLMs due to superior representational and contextual capabilities, but doesn't explore this avenue
- Why unresolved: Uses BERTopic for topic modeling without investigating potential benefits of using LLMs
- What evidence would resolve it: Implementing PromptDSITopic with LLMs for topic modeling and comparing performance and interpretability with BERTopic-based approach

### Open Question 3
- Question: What is the impact of varying number of prompting layers (beyond single-layer approach studied) on PromptDSI performance and efficiency?
- Basis in paper: [explicit] Conducts layer-wise prompting study finding single-layer sufficient for optimal performance, but notes multi-layer prompting incurs higher computational costs
- Why unresolved: Study focuses on single-layer prompting without extensively exploring effects of using multiple prompting layers
- What evidence would resolve it: Experimenting with different numbers of prompting layers in PromptDSI and evaluating impact on performance, efficiency, and stability-plasticity tradeoff

## Limitations
- Performance stability: The "negligible trade-off" claim for single-pass PCL lacks specific quantification of performance degradation
- Topic embedding generalization: Fixed topic keys may become less effective if new corpora contain entirely novel topics absent from initial corpus
- Single-layer prompting sufficiency: May be dataset-dependent and could be insufficient for more complex document retrieval tasks

## Confidence
- PCL Latency Improvement: High - Straightforward mechanism with clearly demonstrated computational benefit
- Topic-Aware Prompt Pool Effectiveness: Medium - Competitive performance shown but stability benefits not thoroughly quantified
- Single-Layer Prompting Sufficiency: Medium - Empirical evidence supports this for tested datasets but broader validation needed

## Next Checks
1. Conduct detailed ablation study measuring exact performance difference between two-pass PCL and single-pass variant across multiple document retrieval datasets with varying semantic complexity
2. Test topic-aware prompt pool on corpora that significantly differ from initial corpus (e.g., different domains) to quantify fixed topic key performance when topic distributions shift
3. Evaluate PromptDSI performance using progressively deeper prompting layers (L2, L4, L8, L12) on benchmark suite of document retrieval tasks to determine if deeper prompting becomes necessary for optimal performance