---
ver: rpa2
title: Using Large Language Models to Generate Authentic Multi-agent Knowledge Work
  Datasets
arxiv_id: '2409.04286'
source_url: https://arxiv.org/abs/2409.04286
tags:
- documents
- knowledge
- work
- generated
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces KnoWoGen, a knowledge work dataset generator
  that uses Large Language Models to simulate collaborative work environments. Current
  knowledge work datasets lack diversity, extensive annotations, and contextual information,
  making them unsuitable for evaluating knowledge work assistance tools.
---

# Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets

## Quick Facts
- arXiv ID: 2409.04286
- Source URL: https://arxiv.org/abs/2409.04286
- Authors: Desiree Heim; Christian Jilek; Adrian Ulges; Andreas Dengel
- Reference count: 5
- Primary result: KnoWoGen generates multi-agent knowledge work datasets with 53% document authenticity rating in human evaluation vs 74% for real documents

## Executive Summary
This paper introduces KnoWoGen, a system that uses Large Language Models to generate synthetic knowledge work datasets with accompanying contextual information. Current knowledge work datasets lack diversity and rich annotations needed for evaluating knowledge work assistance tools. KnoWoGen addresses this by simulating multi-agent knowledge work environments and capturing the resulting documents and interactions in a knowledge graph. The approach enables the generation of diverse, realistic knowledge work documents with full contextual information for tool evaluation and training.

## Method Summary
KnoWoGen uses LLMs within a multi-agent simulation framework to generate knowledge work documents. The system loads configuration files defining agents, tasks, and document types, then simulates collaborative work environments where agents perform tasks and generate documents. Each document generation uses prompts informed by the current simulation context. All background information, agent actions, and generated content are stored in a knowledge graph. The system was evaluated through a human study where participants rated the authenticity of 25 documents (5 real, 20 generated) on a 7-point Likert scale without knowing which were real or generated.

## Key Results
- Generated documents achieved 53% realistic rating vs 74% for real documents in human evaluation
- 71% of participants reported uncertainty about whether documents were real or generated
- Common issues identified: lack of details, generic content, and unauthentic names in generated documents
- Knowledge graph successfully captured simulation context for future tool evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based document generation enables high diversity in synthetic knowledge work datasets compared to template-based approaches.
- Mechanism: Large Language Models can produce varied outputs for the same prompt without relying on fixed patterns or examples, allowing for a broader range of document types and styles.
- Core assumption: The LLM has sufficient training data to cover the target domain and can generalize beyond its training corpus.
- Evidence anchors:
  - [abstract]: "Recently, with the emergence of Large Language Models (LLMs), new document generators evolved based on them which, in contrast to earlier solutions, can generate more diverse documents without requiring explicit templates or examples."
  - [section]: "Moreover, several simulators based on LLM-operated Multi-agent Systems emerged."
- Break condition: The LLM's training data lacks sufficient examples from the target domain, leading to generic or irrelevant content.

### Mechanism 2
- Claim: The knowledge graph stores contextual information enabling realistic document generation and future tool evaluation.
- Mechanism: By capturing task dependencies, agent relationships, and document histories in a graph structure, the system can provide rich context for generating subsequent documents and evaluating knowledge work tools.
- Core assumption: The knowledge graph structure effectively represents the relationships between different elements of the simulation.
- Evidence anchors:
  - [abstract]: "Additionally, the generator captures all background information, given in its configuration or created during the simulation process, in a knowledge graph."
  - [section]: "The simulation context stored in the knowledge graph can later be used as input or ground truth depending on the tools that should be optimized or evaluated."
- Break condition: The knowledge graph becomes too complex to maintain or query efficiently, slowing down the simulation.

### Mechanism 3
- Claim: Human evaluation validates the authenticity of generated documents and identifies areas for improvement.
- Mechanism: By having human raters assess generated documents against real ones using a Turing test-inspired approach, the system can measure its effectiveness and gather specific feedback for refinement.
- Core assumption: Human raters can reliably distinguish between real and generated documents based on content quality and authenticity.
- Evidence anchors:
  - [abstract]: "Our study involving human raters who assessed 53% of the generated and 74% of the real documents as realistic demonstrates the potential of our approach."
  - [section]: "Similar to previous publications [Cl21; Ja23; JHN23], we examined how authentic generated documents, in this case knowledge work documents, are perceived by human evaluators compared to real ones."
- Break condition: Human raters become biased due to repeated exposure to generated documents or the evaluation criteria become unclear.

## Foundational Learning

- Concept: Multi-agent simulation
  - Why needed here: To create realistic collaborative knowledge work scenarios where multiple agents interact and produce documents.
  - Quick check question: What distinguishes this multi-agent approach from typical agent-based systems that use predefined rules?

- Concept: Knowledge graph representation
  - Why needed here: To capture and preserve the contextual relationships between tasks, agents, and documents for future evaluation and analysis.
  - Quick check question: How does storing simulation context in a knowledge graph benefit tool evaluation compared to traditional database approaches?

- Concept: Prompt engineering for LLMs
  - Why needed here: To guide the LLM in generating authentic documents by providing appropriate context and instructions.
  - Quick check question: What are the key components of an effective prompt for generating knowledge work documents?

## Architecture Onboarding

- Component map:
  Configuration module (TOML files) -> Multi-agent simulation engine -> LLM document generator -> Knowledge graph storage -> Human evaluation interface

- Critical path:
  1. Load configuration
  2. Initialize agents and tasks
  3. Execute simulation rounds
  4. Generate documents via LLM
  5. Store context in knowledge graph
  6. Output dataset for evaluation

- Design tradeoffs:
  - LLM choice: Higher context length vs. computational cost
  - Simulation complexity: More agents/tasks vs. runtime performance
  - Prompt detail: More specific instructions vs. document diversity

- Failure signatures:
  - Unrealistic documents: Check LLM configuration and prompt templates
  - Slow simulation: Profile knowledge graph operations and agent task assignment
  - Inconsistent outputs: Verify random seed usage and configuration parameters

- First 3 experiments:
  1. Generate a single document type (e.g., email) with varying prompt configurations
  2. Run a minimal simulation with 2 agents and 3 tasks to test system integration
  3. Compare generated documents against a small set of real documents using human raters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the authenticity of generated knowledge work documents change when using documents generated based on more previously generated documents?
- Basis in paper: [inferred] The paper mentions that documents generated for the experiment were built based on a maximum of one previously generated document, and suggests examining whether the situation improves when using documents generated based on more previously generated documents.
- Why unresolved: The experiment only tested documents generated based on a maximum of one previously generated document, and did not explore the impact of using more previously generated documents on the authenticity of the generated documents.
- What evidence would resolve it: A follow-up experiment comparing the authenticity of documents generated based on different numbers of previously generated documents, using the same human evaluation methodology as the original experiment.

### Open Question 2
- Question: How does the diversity of generated documents change when using more general instructions in the prompt?
- Basis in paper: [inferred] The paper mentions that the prompts can be enhanced to encourage a more specific focus and more details, which can be achieved by including more general instructions in the prompt. It also suggests that including more general instructions could solve multiple other issues, such as encouraging characteristics that the experiment revealed as being particularly authentic.
- Why unresolved: The experiment did not test the impact of using more general instructions in the prompt on the diversity of the generated documents.
- What evidence would resolve it: A follow-up experiment comparing the diversity of documents generated using different sets of instructions in the prompt, using quantitative measures of diversity such as lexical diversity or semantic similarity.

### Open Question 3
- Question: How does the authenticity of generated knowledge work documents change when using different types of actions and tasks in the simulation?
- Basis in paper: [inferred] The paper mentions that tasks are sequences of actions with logical or content dependencies, and that the KnoWoGen can be configured to simulate different types of actions and tasks. It also suggests that examining other document types, multiple related documents, and inter-document diversity would be interesting for future experiments.
- Why unresolved: The experiment only tested two types of documents (emails and meeting minutes) and did not explore the impact of using different types of actions and tasks in the simulation on the authenticity of the generated documents.
- What evidence would resolve it: A follow-up experiment comparing the authenticity of documents generated using different types of actions and tasks in the simulation, using the same human evaluation methodology as the original experiment.

## Limitations
- Generated documents achieved only 53% realistic rating vs 74% for real documents, indicating authenticity limitations
- Evaluation limited to only emails and meeting minutes, leaving generalization to other document types uncertain
- Study did not account for potential rater fatigue or learning effects across multiple document ratings

## Confidence

**High Confidence**: The mechanism for using LLMs to generate diverse documents is well-supported, with clear evidence from the abstract and methodology sections. The knowledge graph storage approach is also confidently described with concrete implementation details.

**Medium Confidence**: The human evaluation results are presented clearly, but the relatively low authenticity rating (53% vs 74% for real documents) suggests the approach has limitations that weren't fully explored. The generalizability to other document types remains uncertain.

**Low Confidence**: The scalability claims and performance characteristics under different configurations are not thoroughly validated. The impact of prompt engineering variations on output quality is not systematically explored.

## Next Checks

1. **Cross-domain validation**: Test KnoWoGen with additional document types (e.g., project proposals, technical reports, meeting minutes) to assess generalization beyond emails and meeting minutes.

2. **Longitudinal study**: Conduct a follow-up evaluation with new participants and varied document presentations to assess whether the 53% authenticity rating holds across different user groups and presentation formats.

3. **Parameter sensitivity analysis**: Systematically vary key parameters (LLM model choice, prompt complexity, agent count) to identify which configurations yield the most authentic outputs and optimal performance characteristics.