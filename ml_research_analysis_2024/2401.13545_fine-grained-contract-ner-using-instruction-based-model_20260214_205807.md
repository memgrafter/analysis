---
ver: rpa2
title: Fine-grained Contract NER using instruction based model
arxiv_id: '2401.13545'
source_url: https://arxiv.org/abs/2401.13545
tags:
- cause
- effect
- task
- cotprompt
- nancial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of extracting cause-and-effect
  relationships from financial documents, a task known as causality detection. Traditional
  approaches treat this as a span extraction or sequence labeling problem.
---

# Fine-grained Contract NER using instruction based model

## Quick Facts
- arXiv ID: 2401.13545
- Source URL: https://arxiv.org/abs/2401.13545
- Authors: Hiranmai Sri Adibhatla; Pavan Baswani; Manish Shrivastava
- Reference count: 16
- Primary result: Third place in FinCausal-2023 shared task with F1 score of 0.54

## Executive Summary
This paper addresses the challenge of extracting cause-and-effect relationships from financial documents by transforming causality detection from a traditional span extraction or sequence labeling problem into a text-generation task suitable for Large Language Models (LLMs). The authors experiment with various prompt engineering strategies for zero-shot inference using LLMs like ChatGPT, llama-2, and ocra mini v3 7b, alongside supervised fine-tuning of BERT and RoBERTa models. Their approach achieves an F1 score of 0.54 and exact match score of 0.08 on the FinCausal-2023 shared task, placing third overall in the competition.

## Method Summary
The authors approach causality detection by reformulating it as a text-generation problem rather than traditional span extraction or sequence labeling. They implement zero-shot inference with LLMs using various prompt engineering strategies, including Chain-of-Thought prompting, and compare these results with supervised fine-tuned BERT and RoBERTa models. The methodology involves testing multiple LLMs (ChatGPT, llama-2, ocra mini v3 7b) with different prompt formulations and evaluating their performance on the FinCausal-2023 dataset. The best performance was achieved using ChatGPT with Chain-of-Thought prompting, demonstrating the effectiveness of transforming the task format for LLM-based approaches.

## Key Results
- Achieved F1 score of 0.54 and exact match score of 0.08 on FinCausal-2023 shared task
- Third place ranking in the FinCausal-2023 competition
- ChatGPT with Chain-of-Thought prompting outperformed other LLMs and fine-tuned models
- Demonstrated effectiveness of transforming causality detection into text-generation task

## Why This Works (Mechanism)
The paper's approach works by leveraging the generative capabilities of LLMs to naturally capture cause-and-effect relationships through text generation rather than constrained extraction. By reformulating the task as generation rather than classification or span labeling, the models can utilize their broader contextual understanding and reasoning abilities. The Chain-of-Thought prompting strategy enables step-by-step reasoning, allowing the models to break down complex causal relationships into manageable components before generating the final output. This transformation from extractive to generative tasks aligns with LLMs' strengths in understanding and producing natural language sequences.

## Foundational Learning

**Large Language Models (LLMs)**: Deep learning models trained on vast text corpora that can generate human-like text and perform various language tasks. Why needed: LLMs provide the generative foundation for transforming causality detection into a text-generation problem. Quick check: Verify model architecture and training data scale.

**Chain-of-Thought Prompting**: A technique that guides models through intermediate reasoning steps before producing final answers. Why needed: Enables systematic breakdown of complex causal relationships for more accurate generation. Quick check: Confirm prompt structure includes explicit reasoning steps.

**Zero-shot Learning**: Model inference without task-specific training, relying solely on prompt engineering. Why needed: Allows evaluation of LLM capabilities without resource-intensive fine-tuning. Quick check: Ensure prompts contain sufficient task instructions and examples.

**Fine-tuning**: Process of adapting pre-trained models to specific downstream tasks using labeled data. Why needed: Provides baseline comparison against zero-shot LLM approaches. Quick check: Verify fine-tuning dataset size and training parameters.

## Architecture Onboarding

**Component Map**: Data -> Preprocessing -> Zero-shot LLM (ChatGPT, llama-2, ocra mini v3 7b) -> Chain-of-Thought Prompting -> Output Generation

**Critical Path**: Input financial document → Prompt engineering → LLM generation → Causality extraction → Evaluation metrics

**Design Tradeoffs**: Zero-shot approaches offer flexibility and lower computational cost but may lack task-specific optimization compared to fine-tuned models. The generative approach trades precision for broader contextual understanding, potentially capturing more nuanced causal relationships.

**Failure Signatures**: Poor performance on documents with implicit or complex causal relationships, generation of irrelevant or incomplete cause-effect pairs, over-reliance on prompt phrasing rather than actual understanding, and potential hallucination of causal relationships not present in source documents.

**First Experiments**: 1) Compare zero-shot vs fine-tuned model performance on same dataset. 2) Test different prompt engineering strategies (Chain-of-Thought vs direct prompting). 3) Evaluate model performance across different types of financial documents (annual reports vs news articles).

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Modest performance metrics with F1 score of 0.54 indicating significant room for improvement
- Single-task evaluation on FinCausal-2023 without broader domain validation
- Limited comparison framework between zero-shot and fine-tuned approaches
- Results may not generalize to other causality detection tasks or larger financial corpora

## Confidence

**High confidence**: Methodology of transforming span extraction to text generation for LLMs is technically sound and well-explained. Comparison framework between different approaches is methodologically rigorous.

**Medium confidence**: Reported performance metrics and task ranking are reliable within FinCausal-2023 context, but generalizability to other financial document types remains uncertain.

**Medium confidence**: Superiority of ChatGPT with Chain-of-Thought prompting is demonstrated, but margin of improvement and robustness across different prompt variations could be further validated.

## Next Checks

1. Test proposed approach on additional financial datasets beyond FinCausal-2023 to evaluate generalizability across different document types and causality detection tasks.

2. Conduct ablation studies to isolate impact of Chain-of-Thought prompting versus other prompt engineering strategies on performance.

3. Evaluate model performance on out-of-distribution examples and adversarial cases to assess robustness and real-world applicability.