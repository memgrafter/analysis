---
ver: rpa2
title: Multilingual Non-Factoid Question Answering with Answer Paragraph Selection
arxiv_id: '2408.10604'
source_url: https://arxiv.org/abs/2408.10604
tags:
- question
- answer
- questions
- munfquad
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MuNfQuAD, a large multilingual question-answering
  dataset with non-factoid questions spanning 38 languages. It is constructed by scraping
  BBC news articles and extracting interrogative subheadings as questions and their
  subsequent paragraphs as silver answers.
---

# Multilingual Non-Factoid Question Answering with Answer Paragraph Selection

## Quick Facts
- arXiv ID: 2408.10604
- Source URL: https://arxiv.org/abs/2408.10604
- Reference count: 40
- Key outcome: Introduces MuNfQuAD, a large multilingual QA dataset with 578K QA pairs across 38 languages, and demonstrates that APS models trained on silver labels generalize to golden annotations with 72% accuracy.

## Executive Summary
This paper introduces MuNfQuAD, a large multilingual question-answering dataset with non-factoid questions spanning 38 languages. The dataset is constructed by scraping BBC news articles and extracting interrogative subheadings as questions and their subsequent paragraphs as silver answers. The authors develop and evaluate an Answer Paragraph Selection (APS) model fine-tuned on MuNfQuAD using multilingual encoders, achieving 80% accuracy and 72% macro F1 on the test set. The APS model generalizes well to a manually annotated golden set and improves instruction-tuned LLM performance when used to reduce context size.

## Method Summary
The paper introduces MuNfQuAD, a large multilingual QA dataset constructed by scraping BBC news articles and extracting interrogative subheadings as questions and their subsequent paragraphs as silver answers. The APS model is fine-tuned on MuNfQuAD using multilingual encoders with weighted focal loss (Î³=2), batch size 12 per GPU, for 1 epoch. The model architecture consists of multilingual encoder layers followed by three linear layers with dropout, outputting a sigmoid probability for paragraph relevance.

## Key Results
- MuNfQuAD contains over 578K QA pairs across 38 languages, making it the largest multilingual QA dataset to date
- The APS model achieves 80% accuracy and 72% macro F1 on the MuNfQuAD test set, and 72% accuracy and 66% macro F1 on the golden set
- APS-filtered context improves instruction-tuned LLM performance, with Gemma-2B win ratio improving from 11% to 14% and Gemma-7B from 19% to 25% on TyDi dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual non-factoid QA is feasible by leveraging automatically generated silver labels from interrogative subheadings in BBC articles.
- Mechanism: The dataset construction process extracts interrogative subheadings as questions and their subsequent paragraphs as answers, creating a scalable silver-label dataset. The high success rate of silver labels (98%) on golden annotations validates the approach.
- Core assumption: Paragraphs succeeding interrogative subheadings in BBC news articles reliably contain answers to the corresponding questions.
- Evidence anchors:
  - [abstract]: "Based on the manual annotations of 790 QA-pairs from MuNfQuAD (golden set), we observe that 98% of questions can be answered using their corresponding silver answer."
  - [section 3.1]: "The dataset comprises over 578K QA pairs across 38 languages, encompassing several low-resource languages, and stands as the largest multilingual QA dataset to date."
- Break condition: If interrogative subheadings are used as summaries or topic transitions rather than actual questions, the silver label assumption fails.

### Mechanism 2
- Claim: Answer Paragraph Selection (APS) models trained on silver labels can generalize to golden annotations across multiple languages.
- Mechanism: Fine-tuned APS models using multilingual encoders (XLM-V achieves 80% accuracy on test set, 72% on golden set) demonstrate that training on silver labels transfers to human-annotated data despite the label quality gap.
- Core assumption: Multilingual encoders can learn language-agnostic patterns for answer paragraph identification from silver-labeled data.
- Evidence anchors:
  - [abstract]: "The APS model attained an accuracy of 80% and 72%, as well as a macro F1 of 72% and 66%, on the MuNfQuAD testset and the golden set, respectively."
  - [section 5.1]: "Notably, predictions generated by our XLM-V based APS model on Nepali golden set achieve a superior Macro-F1 and Label-1 F1 as compared to the silver labels"
- Break condition: If language-specific answer patterns differ significantly from what the multilingual encoder learns from silver labels.

### Mechanism 3
- Claim: Fine-tuned APS models improve instruction-tuned LLM performance by reducing context size for non-factoid QA.
- Mechanism: Using APS model scores to filter context paragraphs before feeding to instruction-tuned LLMs (Gemma-2B win ratio improves from 11% to 14%, Gemma-7B from 19% to 25% on TyDi dataset).
- Core assumption: Reducing context to relevant paragraphs improves LLM performance more than providing full article context.
- Evidence anchors:
  - [section 6]: "Our experiments demonstrate that when the context associated with a question is reduced using the scores predicted by the MuNfQuAD-fine-tuned APS model, the performance of instructional LLMs... is enhanced."
  - [abstract]: "We also observe that the fine-tuned APS model is beneficial for reducing the context of a question."
- Break condition: If LLMs can handle full context efficiently or if APS filtering removes contextually important information.

## Foundational Learning

- Concept: Silver vs gold label quality in machine learning
  - Why needed here: Understanding the trade-off between automatically generated silver labels and manually annotated gold labels is crucial for interpreting model performance and generalization claims
  - Quick check question: Why might a model trained on silver labels perform differently on golden annotations than on test data with silver labels?

- Concept: Multilingual text processing and tokenization
  - Why needed here: The dataset spans 38 languages with different writing systems, requiring understanding of multilingual tokenization approaches and their impact on model performance
  - Quick check question: How does whitespace-based tokenization differ from language-specific tokenizers like jieba for Chinese or MeCab for Japanese?

- Concept: Answer paragraph selection vs reading comprehension
  - Why needed here: The paper distinguishes between APS (selecting relevant paragraphs) and RC (extracting answers within paragraphs), requiring understanding of when each approach is appropriate
  - Quick check question: What are the computational advantages of APS over RC when dealing with long documents?

## Architecture Onboarding

- Component map: BBC web scraper -> interrogative subheading extraction -> paragraph grouping -> silver label assignment -> multilingual encoder -> three linear layers with dropout -> sigmoid output
- Critical path: Data collection -> silver label generation -> APS model fine-tuning -> golden set evaluation -> LLM integration testing
- Design tradeoffs:
  - Silver labels vs manual annotation: Scalability vs quality
  - Multilingual encoders vs monolingual: Coverage vs performance
  - APS vs RC: Efficiency vs answer precision
  - Token limit (512) vs document length (875 words avg): Speed vs completeness
- Failure signatures:
  - Low Label-1 F1 with high Label-0 F1: Model over-predicting negative class
  - High performance on silver test set but low on golden set: Overfitting to silver label patterns
  - Poor LLM integration results: APS model removing contextually important paragraphs
- First 3 experiments:
  1. Train APS model with XLM-R on 1% subset using weighted binary cross entropy, compare to focal loss
  2. Evaluate APS model performance across languages in golden set, identify underperforming languages
  3. Test LLM performance with full context vs APS-filtered context on TyDi dataset subset

## Open Questions the Paper Calls Out
- What are the specific challenges and limitations of using silver labels in multilingual QA datasets, particularly in low-resource languages, and how do they impact model performance and generalization?
- How does the political bias inherent in BBC news articles affect the quality and generalizability of the MuNfQuAD dataset, and what strategies can be employed to mitigate this bias?
- What are the computational challenges and limitations of fine-tuning APS models on large multilingual datasets like MuNfQuAD, and how can these challenges be addressed to enable more efficient and scalable training?

## Limitations
- Silver label quality validation performed on only 790 QA pairs (less than 0.2% of dataset), raising questions about generalizability across all 38 languages
- 512 token limit for processing documents averaging 875 words, with inadequate discussion of truncation impact on answer paragraph selection quality
- Limited per-language performance analysis, raising concerns about potential bias toward high-resource languages in the training data

## Confidence
- High confidence: The dataset construction methodology and basic APS model architecture are well-documented and reproducible
- Medium confidence: The silver label quality (98% success rate) and APS model performance metrics, given the limited golden set validation
- Low confidence: The LLM integration results and generalization across all 38 languages without detailed per-language analysis

## Next Checks
1. Perform comprehensive golden annotation validation across all 38 languages in the dataset, not just the 790 QA pairs tested, to verify the 98% silver label success rate holds across language diversity
2. Conduct ablation studies on the 512 token limit by testing model performance with and without document truncation across different language families
3. Implement per-language performance analysis of the APS model on the golden set to identify potential bias toward high-resource languages and understand generalization limitations