---
ver: rpa2
title: 'Synthesising Robust Controllers for Robot Collectives with Recurrent Tasks:
  A Case Study'
arxiv_id: '2411.14371'
source_url: https://arxiv.org/abs/2411.14371
tags:
- strategy
- robot
- contamination
- room
- strategies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for synthesising robust controllers
  for robot collectives with recurrent tasks, using a cleaning robot scenario as a
  case study. The approach involves modelling the problem as a partially observable
  Markov decision process (POMDP), synthesising a strategy using probabilistic model
  checking, and then verifying the strategy against linear-time correctness properties.
---

# Synthesising Robust Controllers for Robot Collectives with Recurrent Tasks: A Case Study

## Quick Facts
- arXiv ID: 2411.14371
- Source URL: https://arxiv.org/abs/2411.14371
- Authors: Till Schnittka; Mario Gleirscher
- Reference count: 18
- Key outcome: Presents a method for synthesising robust controllers for robot collectives with recurrent tasks using POMDP-based strategy synthesis and probabilistic model checking

## Executive Summary
This paper introduces a novel approach to synthesising robust controllers for robot collectives performing recurrent tasks under uncertainty. The method models the problem as a partially observable Markov decision process (POMDP), synthesises strategies using probabilistic model checking, and verifies them against temporal logic properties. Using a cleaning robot scenario as a case study, the authors demonstrate that their approach can find optimal strategies that balance energy consumption and cleanliness maintenance while ensuring robots return to their starting positions. The work addresses the challenge of coordinating multiple robots in environments with uncertain contamination levels and room utilisation constraints.

## Method Summary
The approach involves modelling the robot collective coordination problem as a POMDP where contamination levels are hidden states, allowing tractable synthesis under uncertainty. The model is implemented in PRISM's POMDP extension, with reward structures encoding energy optimisation goals and constraint violations as penalties. Strategy synthesis uses grid-based belief-space approximation to handle the partial observability, followed by verification of the synthesised strategy against linear-time properties using PRISM's PLTL model checking. The method automatically generates a deterministic model from the POMDP strategy, enabling formal verification of recurrence, safety, and room utilisation constraints.

## Key Results
- Synthesised reasonable strategies for multiple robots coordinating to clean rooms while managing battery levels and contamination thresholds
- Demonstrated trade-offs between contamination probability and energy consumption through parameter evaluation
- Verified that synthesised strategies satisfy recurrence properties (robots return to starting positions) and safety constraints
- Showed the approach generalises to other applications like firefighting and geriatric care robot collectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The POMDP-based strategy synthesis is robust under environmental uncertainty because partial observability hides stochastic contamination, enabling tractable synthesis.
- Mechanism: Contamination is modeled as hidden states in the POMDP; the belief space approximation allows PRISM to synthesize strategies without tracking full contamination values, reducing state space complexity while maintaining robustness guarantees.
- Core assumption: Hidden stochastic factors (e.g., contamination) do not change optimal high-level coordination decisions.
- Evidence anchors:
  - [abstract] "Robustness against environmental uncertainty is encoded via partial observability."
  - [section 2] Definition of POMDP with partial observability and PRISM's belief-MDP approximation.
- Break condition: If contamination states become observable or critical to decision-making, hiding them may degrade strategy optimality.

### Mechanism 2
- Claim: The reward structure encodes recurrence and safety constraints as soft penalties, enabling synthesis despite PRISM's limitations on hard PLTL constraints.
- Mechanism: Penalties are added for violating constraints (e.g., battery empty, contamination threshold exceeded), prioritizing strategies that avoid these states without preventing synthesis of any path.
- Core assumption: Penalty values are chosen large enough to discourage constraint violations but small enough to allow feasible strategies.
- Evidence anchors:
  - [section 3.3] "We apply lower penalties to the contamination flags... the penalty for constraints is chosen such that it is not possible to offset the penalty of an invalid state by the reduced penalty for a less energy-consuming strategy."
- Break condition: If penalties are too low, strategies may violate constraints; if too high, synthesis may fail to find feasible strategies.

### Mechanism 3
- Claim: Simultaneous composition of robot actions reduces model complexity by avoiding explicit state enumeration of all robot combinations.
- Mechanism: The CPN model defines valid robot moves; PRISM's implicit synchronization ensures no conflicts, reducing state space compared to explicit enumeration of all robot configurations.
- Core assumption: Robot coordination constraints can be enforced at the action level without explicit state tracking.
- Evidence anchors:
  - [section 3.1] "We can use synchronisation (via action labels)... the simultaneous movements (cf. Figure 4a)."
- Break condition: If coordination becomes too complex for implicit synchronization, explicit state tracking may be necessary, increasing model size.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: POMDPs model environments where the agent cannot fully observe the state, which is critical for handling uncertainty in robot collectives (e.g., contamination levels).
  - Quick check question: What is the key difference between a POMDP and an MDP in terms of state observation?

- Concept: Probabilistic Model Checking
  - Why needed here: Used to verify that synthesized strategies satisfy temporal logic properties (e.g., recurrence, safety) under uncertainty.
  - Quick check question: How does PRISM use belief-MDP approximation to analyze POMDPs?

- Concept: Reward Structures in Model Checking
  - Why needed here: Reward functions encode optimization goals (e.g., energy consumption) and constraint penalties, guiding strategy synthesis.
  - Quick check question: Why can't PLTL constraints be used directly for POMDP strategy synthesis in PRISM?

## Architecture Onboarding

- Component map: CPN model -> POMDP model -> PRISM synthesis -> Induced model -> PLTL verification
- Critical path: Model → POMDP Synthesis → Strategy Extraction → Induced Model → PLTL Verification
- Design tradeoffs:
  - Partial observability vs. strategy optimality
  - Penalty magnitude vs. synthesis feasibility
  - Grid resolution vs. computational tractability
- Failure signatures:
  - Synthesis timeouts → Increase penalties or simplify model
  - Verification failures → Adjust reward structure or check induced model correctness
  - Strategy non-recurrence → Refine recurrence area definition or increase grid resolution
- First 3 experiments:
  1. Run synthesis with single robot, low contamination probability, verify recurrence
  2. Increase robot count, observe state space growth, adjust penalties for energy optimization
  3. Vary grid resolution, measure synthesis time and strategy quality tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal set of model parameters (a_bit, contamination probability, grid resolution g) for synthesizing correct and efficient strategies across different robot collective scenarios?
- Basis in paper: [explicit] Section 4.2 discusses parameter evaluation and finding optimal values
- Why unresolved: The paper shows parameter sensitivity but doesn't provide a general method for selecting optimal parameters for arbitrary scenarios
- What evidence would resolve it: A systematic framework or algorithm for parameter selection that generalizes across different robot collective applications and layouts

### Open Question 2
- Question: How can the recurrence area ω be automatically determined rather than manually specified?
- Basis in paper: [explicit] Section 5 mentions future work on finding probable ωs from room layout
- Why unresolved: Current approach requires manual examination of strategies or iterative verification of probable ωs
- What evidence would resolve it: An automated method to identify recurrence areas based on model structure, task requirements, and environmental constraints

### Open Question 3
- Question: Can the POMDP-based approach scale to significantly larger scenarios with more rooms, robots, and complex task requirements?
- Basis in paper: [inferred] Section 5 discusses complexity and mentions a 12-hour schedule for 3 robots with 11 rooms as a test case
- Why unresolved: The paper only evaluates relatively simple scenarios and doesn't demonstrate scalability to industrial-size problems
- What evidence would resolve it: Experimental results showing successful synthesis and verification for larger scenarios, or formal complexity analysis proving scalability bounds

### Open Question 4
- Question: How can quantitative strategy correctness be formally verified, specifically checking that contamination flag probabilities in the POMDP are greater than or equal to corresponding counter threshold probabilities in the induced model?
- Basis in paper: [explicit] Section 3.7 mentions this as "a property of M′ preserving quantitative strategy correctness, that we left for future work"
- Why unresolved: The paper acknowledges this verification gap but doesn't provide a solution
- What evidence would resolve it: A formal method or tool support for verifying the probabilistic relationship between the POMDP contamination flags and the induced model's contamination counters

## Limitations
- Unknown contamination rate values Ri.pr and threshold values Ri.threshold for each room are not specified
- Optimal values for penalty constants in the reward structure are not specified
- Limited evaluation to relatively simple scenarios with 3 robots and 11 rooms

## Confidence

- **High Confidence**: The theoretical foundations of POMDPs and probabilistic model checking are well-established and correctly applied in the paper.
- **Medium Confidence**: The case study results demonstrate the approach's effectiveness, but the limited scope of the cleaning robot scenario may not fully represent the complexity of other applications.
- **Low Confidence**: The exact impact of contamination rate values and penalty constants on strategy synthesis is not fully explored, and their optimal selection may require further investigation.

## Next Checks

1. **Parameter Sensitivity Analysis**: Conduct a systematic study of how different contamination rate values and penalty constants affect the synthesised strategies. This will help identify the robustness of the approach to parameter variations and guide the selection of optimal values for different applications.

2. **Scalability Evaluation**: Test the approach with larger robot collectives and more complex room plans to assess its scalability. This will help identify potential bottlenecks in the belief-space approximation and inform strategies for handling larger state spaces.

3. **Generalisation to Other Applications**: Apply the approach to other domains, such as firefighting or geriatric care, to evaluate its effectiveness in handling different types of uncertainties and constraints. This will provide insights into the approach's versatility and identify any domain-specific challenges that may arise.