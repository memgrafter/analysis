---
ver: rpa2
title: 'Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks'
arxiv_id: '2410.01744'
source_url: https://arxiv.org/abs/2410.01744
tags:
- multi-image
- text-rich
- data
- images
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Leopard is a multimodal large language model designed for text-rich,
  multi-image tasks such as understanding presentation slides, scanned documents,
  and webpage snapshots. It addresses two main challenges: the scarcity of high-quality
  instruction-tuning datasets for text-rich multi-image scenarios and the difficulty
  of balancing image resolution with visual feature sequence length.'
---

# Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks

## Quick Facts
- **arXiv ID**: 2410.01744
- **Source URL**: https://arxiv.org/abs/2410.01744
- **Reference count**: 40
- **Primary result**: Outperforms Llama-3.2 and Qwen2-VL by 8.5 points on text-rich multi-image benchmarks

## Executive Summary
Leopard is a multimodal large language model specifically designed for text-rich, multi-image tasks such as analyzing presentation slides, scanned documents, and webpage snapshots. The model addresses two key challenges in this domain: the scarcity of high-quality instruction-tuning datasets for text-rich multi-image scenarios and the difficulty of balancing image resolution with visual feature sequence length. Leopard introduces both a novel large-scale dataset of 925K text-rich, multi-image instructions and an adaptive high-resolution multi-image encoding module that dynamically allocates visual sequence length based on image aspect ratios and resolutions.

## Method Summary
The model leverages a comprehensive approach to text-rich multi-image understanding by combining a large-scale instruction-tuning dataset with an adaptive encoding architecture. The core innovation is the adaptive high-resolution multi-image encoding module, which dynamically adjusts visual sequence length allocation based on each image's aspect ratio and resolution requirements. This allows the model to maintain high image resolution while managing computational constraints. The training process utilizes the newly created 925K instruction dataset specifically curated for text-rich multi-image scenarios, enabling effective learning of complex visual-textual relationships in documents, slides, and web content.

## Key Results
- Achieves 8.5-point average improvement over Llama-3.2 and Qwen2-VL on text-rich multi-image benchmarks
- Maintains competitive performance on single-image and general multimodal tasks
- Demonstrates high efficiency by achieving results with only 1.2M training instances

## Why This Works (Mechanism)
The adaptive high-resolution encoding module dynamically allocates visual sequence length based on image characteristics, allowing optimal resolution preservation while managing computational costs. This targeted approach is particularly effective for text-rich scenarios where fine-grained visual details matter. The large-scale 925K instruction dataset provides diverse, high-quality training examples specifically designed for text-rich multi-image understanding, enabling the model to learn nuanced relationships between text and visual elements across multiple images.

## Foundational Learning
- **Multimodal instruction tuning**: Why needed - to align visual understanding with language generation; Quick check - evaluate zero-shot performance on unseen instruction formats
- **Dynamic sequence length allocation**: Why needed - to balance resolution quality with computational efficiency; Quick check - measure token usage vs. image complexity correlation
- **Text-rich image encoding**: Why needed - standard vision models struggle with dense text in images; Quick check - compare OCR accuracy integration with baseline models
- **Multi-image context modeling**: Why needed - to understand relationships across multiple related images; Quick check - evaluate performance on document page sequence tasks

## Architecture Onboarding
- **Component map**: Input images -> Adaptive High-Resolution Encoder -> Visual Feature Sequences -> Cross-Attention with LLM -> Text Generation
- **Critical path**: Image preprocessing → Adaptive resolution allocation → Feature extraction → Cross-modal attention → Text generation
- **Design tradeoffs**: Resolution vs. sequence length vs. computational cost; Model size vs. inference speed
- **Failure signatures**: Blurry text recognition, lost context across images, inconsistent handling of varying aspect ratios
- **3 first experiments**: 1) Single image text extraction accuracy vs. baseline models; 2) Multi-image document comprehension on synthetic datasets; 3) Resolution sensitivity analysis across different aspect ratios

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to specific text-rich scenarios, raising questions about generalizability to broader multimodal tasks
- Efficiency claims relative to model size need independent verification
- Performance on single-image tasks not extensively benchmarked against specialized single-image models

## Confidence
- Multi-image text-rich performance claims: High
- Generalizability to non-text-rich scenarios: Medium
- Efficiency claims relative to model size: Medium
- Open-source implementation completeness: High

## Next Checks
1. Independent reproduction of the 8.5-point average improvement on text-rich multi-image benchmarks using the released dataset and model weights
2. Evaluation of model performance on text-poor or general visual reasoning tasks to assess scope limitations
3. Systematic comparison of training efficiency metrics (FLOPs, parameters, dataset size) against other state-of-the-art multimodal models