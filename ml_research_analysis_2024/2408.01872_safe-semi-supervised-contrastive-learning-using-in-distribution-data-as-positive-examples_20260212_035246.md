---
ver: rpa2
title: Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive
  Examples
arxiv_id: '2408.01872'
source_url: https://arxiv.org/abs/2408.01872
tags:
- data
- learning
- class
- proposed
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of class distribution mismatch
  in semi-supervised learning, where out-of-distribution (OOD) data exists in unlabeled
  data. The authors propose a safe semi-supervised contrastive learning method that
  leverages self-supervised contrastive learning (SSCL) to fully exploit unlabeled
  data while avoiding the negative impact of OOD data.
---

# Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples

## Quick Facts
- arXiv ID: 2408.01872
- Source URL: https://arxiv.org/abs/2408.01872
- Authors: Min Gu Kwak; Hyungu Kahng; Seoung Bum Kim
- Reference count: 40
- Achieved 82.58% accuracy on CIFAR-10 with 0% mismatch ratio, outperforming MoCo by 0.31%

## Executive Summary
This paper addresses the challenge of class distribution mismatch in semi-supervised learning, where out-of-distribution (OOD) data exists in unlabeled data. The authors propose a safe semi-supervised contrastive learning method that leverages self-supervised contrastive learning (SSCL) to fully exploit unlabeled data while avoiding the negative impact of OOD data. The core innovation is treating labeled negative examples of the same class as positive examples through a modified contrastive loss function with a coefficient schedule. This approach prevents the loss of basic information shared across all data, regardless of class, and significantly improves classification accuracy compared to existing methods across multiple benchmark datasets.

## Method Summary
The method builds upon MoCo-based self-supervised contrastive learning by introducing a modified loss function (LID) that treats labeled negative examples of the same class as additional positive examples. During pre-training, the model learns instance discrimination representations using a memory queue mechanism, while simultaneously incorporating class discrimination through the LID loss with a coefficient schedule that gradually reduces its influence to prevent overfitting. After pre-training, the model undergoes fine-tuning using only the labeled data. The approach uses weak data augmentations (random resizing, cropping, horizontal flipping, color jittering, grayscale conversion) and trains with SGD optimizer and cosine decay learning rate schedule.

## Key Results
- Achieved 82.58% accuracy on CIFAR-10 with 0% mismatch ratio, outperforming MoCo by 0.31%
- Demonstrated significant improvements across multiple mismatch ratios (0% to 100%) on CIFAR-10, CIFAR-100, and Tiny ImageNet
- Showed superior performance in cross-dataset scenario (CIFAR-100+Tiny ImageNet) with 86.5% mismatch ratio
- Maintained effectiveness while avoiding the need to filter OOD data, reducing data collection costs

## Why This Works (Mechanism)

### Mechanism 1
Treating labeled negative examples of the same class as positive examples improves class representation clustering during pre-training. The proposed contrastive loss function LID reassigns labeled negative examples that share the same class as the anchor to be treated as additional positive examples. This strengthens the pull between same-class instances and enhances intra-class feature learning. Core assumption: Labeled negative examples of the same class are guaranteed to be in-distribution (ID) data.

### Mechanism 2
Using self-supervised contrastive learning (SSCL) without filtering OOD data preserves shared semantic information across all data. MoCo-based SSCL learns instance discrimination representations without relying on class labels, allowing the model to capture general semantic patterns shared across both ID and OOD data. Core assumption: Instance discrimination can learn representations that benefit downstream classification even when class distributions differ between labeled and unlabeled data.

### Mechanism 3
The coefficient schedule gradually reducing LID's influence prevents overfitting to the limited labeled data. A linear decay function w(t) starts at 1 and decreases to 0 by epoch tend, balancing instance discrimination (LMoCo) and class discrimination (LID) throughout training. Core assumption: Early emphasis on class discrimination is beneficial, but excessive reliance on limited labeled data can cause overfitting.

## Foundational Learning

- **Concept**: Instance discrimination in contrastive learning
  - Why needed here: SSCL methods like MoCo rely on instance discrimination to learn representations without using labels, which is crucial for leveraging unlabeled data regardless of class distribution mismatch.
  - Quick check question: What is the difference between instance discrimination and class discrimination in contrastive learning?

- **Concept**: Memory queue mechanism in MoCo
  - Why needed here: The memory queue provides a large set of negative examples without requiring huge batch sizes, which is essential for maintaining sufficient labeled negative examples in class distribution mismatch scenarios.
  - Quick check question: How does the momentum update of the key network in MoCo help maintain consistency in representations?

- **Concept**: Data augmentation strategies
  - Why needed here: Different augmentation strategies are used for SSL versus SSCL to avoid issues like color histogram memorization in SSCL.
  - Quick check question: Why does random color jittering and grayscale conversion work better for SSCL compared to Gaussian noise?

## Architecture Onboarding

- **Component map**: Query network (fθq) -> Key network (fθk) -> Memory queue -> Loss functions (LMoCo, LID) -> Coefficient scheduler -> Updated query network -> Enqueue representations

- **Critical path**:
  1. Generate anchor and positive views through data augmentation
  2. Retrieve keys from memory queue and identify same-class labeled keys
  3. Compute LMoCo for all examples
  4. Compute LID for labeled examples with same-class keys
  5. Apply weighted sum with coefficient schedule
  6. Update query network, momentum-update key network
  7. Enqueue new representations to memory queue

- **Design tradeoffs**:
  - Memory queue size vs. computational efficiency
  - Strong vs. weak data augmentation for pre-training
  - Schedule parameters (α, tend) vs. overfitting risk
  - Base network architecture (ResNet-50 vs. alternatives)

- **Failure signatures**:
  - Overfitting: Poor performance on validation set, representations become too clustered
  - Underfitting: Representations remain scattered, poor k-NN/linear evaluation accuracy
  - Degenerate solutions: Representations collapse to similar vectors, loss plateaus early

- **First 3 experiments**:
  1. Compare MoCo baseline vs. proposed method on CIFAR-10 with 50% mismatch ratio using linear evaluation
  2. Vary α parameter (1, 2, 3) with fixed tend=200 to find optimal balance
  3. Test different tend values (100, 200, 300) with fixed α=2 to optimize schedule

## Open Questions the Paper Calls Out

1. What are the specific adaptive scheduling strategies that could replace the linear decay coefficient schedule, and how would they impact model performance?
   - Basis in paper: The paper mentions that an adaptive scheduling strategy is required and that the linear decay schedule is a limitation, as the learning curve of the model differs depending on the type of data.
   - Why unresolved: The paper only suggests that adaptive scheduling is needed but does not provide specific strategies or experimental results to validate their effectiveness.

2. How can candidates for positive examples be selected from unlabeled negative examples to further improve representation quality?
   - Basis in paper: The paper suggests that selecting candidates for positive examples from unlabeled negative examples could improve representation quality, especially in scenarios with rare label information.
   - Why unresolved: The paper does not provide a specific method or experimental results to demonstrate the effectiveness of this approach.

3. How can a model be developed to effectively detect out-of-distribution (OOD) data while improving classification performance?
   - Basis in paper: The paper mentions that developing a model adept at OOD detection while improving classification performance would be interesting, as it could reduce data collection costs.
   - Why unresolved: The paper does not provide a specific method or experimental results to demonstrate the effectiveness of this approach.

## Limitations

- Limited evaluation on real-world distribution shift scenarios, relying primarily on synthetic mismatch scenarios
- Assumes labeled data is always in-distribution, which may not hold in practical scenarios with noisy or partially mislabeled data
- Memory queue mechanism for storing class labels alongside representations lacks implementation details that could affect reproducibility

## Confidence

**High Confidence**: The core mechanism of treating same-class labeled negatives as positives (Mechanism 1) is well-founded theoretically and shows consistent improvements across multiple datasets. The improvement over MoCo baseline (+0.31% on CIFAR-10 at 0% mismatch) is statistically significant and reproducible.

**Medium Confidence**: The assumption that instance discrimination captures useful shared semantic information across ID and OOD data (Mechanism 2) is plausible but not thoroughly validated. The paper doesn't provide evidence that the learned representations actually transfer useful information between ID and OOD domains.

**Low Confidence**: The specific form of the coefficient schedule (linear decay from 1 to 0) appears somewhat arbitrary. While the paper argues it prevents overfitting, there's no ablation study showing why this particular schedule is optimal or how sensitive the results are to its parameters.

## Next Checks

1. **Ablation study on coefficient schedule**: Test alternative scheduling functions (exponential decay, step function, adaptive scheduling based on validation performance) to determine if linear decay is truly optimal for balancing LID and LMoCo losses.

2. **Robustness to labeled data noise**: Evaluate the method's performance when labeled data contains mislabeled examples or when some labeled examples are actually OOD data, to test the assumption that labeled data is always in-distribution.

3. **Real-world distribution shift evaluation**: Apply the method to datasets with naturally occurring distribution shifts (e.g., domain adaptation benchmarks like Office-31 or VisDA) rather than synthetic mismatch scenarios to assess practical applicability.