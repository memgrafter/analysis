---
ver: rpa2
title: 'VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception
  of Geometric Information'
arxiv_id: '2412.00947'
source_url: https://arxiv.org/abs/2412.00947
tags:
- answer
- figure
- triangle
- line
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Vision Language Models (LVLMs) struggle to accurately perceive
  basic geometric information in images, such as shapes, angles, and sizes, despite
  human performance being nearly perfect. We introduce VisOnlyQA, a dataset designed
  to evaluate this specific capability independently of other skills like reasoning.
---

# VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information

## Quick Facts
- arXiv ID: 2412.00947
- Source URL: https://arxiv.org/abs/2412.00947
- Reference count: 40
- Primary result: Large vision language models (LVLMs) achieve only 48.8% accuracy on geometric perception tasks, far below human performance of 93.5%.

## Executive Summary
Large Vision Language Models struggle significantly with basic geometric perception tasks such as identifying shapes, angles, and sizes in images, despite human performance being nearly perfect. The VisOnlyQA dataset was introduced to evaluate this capability independently of reasoning or knowledge skills, containing 12 tasks with geometric shapes, chemical structures, charts, and 3D objects. Experiments with 23 state-of-the-art LVLMs, including GPT-4o and Gemini 2.5 Pro, show poor performance (as low as 48.8% accuracy) even after additional training on VisOnlyQA data. Interestingly, LVLMs using stronger language models perform better, suggesting the bottleneck lies in visual processing rather than reasoning ability.

## Method Summary
The study evaluates 23 large vision language models on the VisOnlyQA dataset, which contains 12 geometric perception tasks across synthetic and real images. Models are tested using zero-shot prompts with and without chain-of-thought reasoning, and selected models are fine-tuned on 70,000 synthetic training instances. Human performance is established through annotator evaluations on 300 sampled questions. Error analysis is conducted on chain-of-thought responses to categorize failure modes. The dataset includes Eval-Real (900 instances), Eval-Synthetic (700 instances), and Train (70k synthetic instances) splits.

## Key Results
- LVLMs achieve only 48.8% accuracy on real geometric data, compared to human performance of 93.5%
- Fine-tuning on VisOnlyQA training data does not consistently improve performance, even for in-distribution tasks
- Larger LVLMs and those with stronger language models show better geometric perception, indicating visual processing is the bottleneck
- Performance varies significantly across tasks, with some showing near-random guessing despite the absence of complex reasoning requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LVLMs fail at geometric perception because visual encoders output insufficient or corrupted geometric information.
- Mechanism: Visual transformers compress spatial relationships and exact geometric measurements into high-dimensional embeddings, which lose fine-grained shape, angle, and size details needed for precise geometric reasoning.
- Core assumption: Visual encoders prioritize semantic features over exact geometric measurements.
- Evidence anchors:
  - [abstract] "the way LVLMs process information from visual encoders is a bottleneck"
  - [section] "almost all errors are visual perception errors" (chain-of-thought analysis)
  - [corpus] "Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens" suggests token-level fragility

### Mechanism 2
- Claim: LLMs cannot recover precise geometric information from ambiguous visual embeddings.
- Mechanism: The LLM component attempts to reason about geometry but receives noisy, underspecified visual tokens that prevent accurate geometric judgment.
- Core assumption: LLM reasoning requires precise geometric inputs; ambiguity leads to random guessing.
- Evidence anchors:
  - [abstract] "LVLMs using stronger LLMs exhibit better geometric perception" despite no complex reasoning needed
  - [section] "fine-tuning does not always improve their performance on VisOnlyQA, even for in-distribution tasks"
  - [corpus] "Visual Description Grounding Reduces Hallucinations and Boosts Reasoning" implies grounding is critical

### Mechanism 3
- Claim: Training data scarcity for geometric perception causes poor performance.
- Mechanism: Standard LVLM pretraining focuses on general vision-language tasks, not explicit geometric measurement, leading to undertraining on geometric perception.
- Core assumption: Scaling vision-language pretraining improves all visual perception tasks uniformly.
- Evidence anchors:
  - [abstract] "fine-tuning on the training set of VisOnlyQA is not always effective, even for in-distribution tasks"
  - [section] "larger models exhibit better capability in perceiving geometric information but still perform near-randomly on some tasks"
  - [corpus] "LVLM-COUNT: Enhancing the Counting Ability" suggests targeted datasets can improve specific capabilities

## Foundational Learning

- Concept: Visual Transformer Embeddings
  - Why needed here: Understanding how ViTs encode spatial relationships is key to diagnosing geometric perception failures.
  - Quick check question: What spatial information is preserved vs. lost in ViT patch embeddings?

- Concept: Geometric Measurement vs. Semantic Recognition
  - Why needed here: Differentiating between "there is a circle" vs. "what is the exact diameter" explains why LVLMs fail here but not in general object detection.
  - Quick check question: Can you name a task where semantic recognition succeeds but precise geometric measurement fails?

- Concept: Multimodal Fusion in LVLMs
  - Why needed here: Understanding how visual tokens are combined with LLM embeddings explains why stronger LLMs help geometric perception.
  - Quick check question: How does the attention mechanism in a multimodal transformer handle visual vs. text tokens during geometric reasoning?

## Architecture Onboarding

- Component map: Image → Visual Transformer (ViT) → Spatial Embeddings → LLM Attention → Text Output
- Critical path: Image → ViT → LLM → Answer (geometric perception bottleneck at ViT→LLM transition)
- Design tradeoffs:
  - ViT: Smaller patches preserve detail but increase compute; larger patches lose geometry
  - LLM: Stronger reasoning helps but cannot fix missing geometric data
  - Dataset: Synthetic data helps but may not generalize to real-world distributions
- Failure signatures:
  - Random guessing on geometric tasks despite strong reasoning ability
  - Inconsistent performance across tasks requiring similar geometric reasoning
  - Fine-tuning helps some tasks but not others unpredictably
- First 3 experiments:
  1. Replace ViT with explicit coordinate-based geometric encoder and compare performance.
  2. Fine-tune LLM only vs. all parameters to isolate visual vs. reasoning contributions.
  3. Train on synthetic geometric data only and test on real images to measure generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- The VisOnlyQA dataset contains only 1600 evaluation instances, which may be insufficient for definitive conclusions about LVLM capabilities
- Synthetic training data may not fully capture real-world geometric complexity, potentially limiting generalization
- Error analysis relies on GPT-4o for answer extraction, introducing an additional layer of potential error and bias

## Confidence

- **High Confidence**: The finding that LVLMs struggle with geometric perception is well-supported by experimental results showing consistent performance below human levels across multiple models and tasks.
- **Medium Confidence**: The hypothesis that visual encoders are the primary bottleneck is supported by the observation that stronger LLMs improve geometric perception, but this conclusion relies on indirect evidence.
- **Low Confidence**: The claim that fine-tuning does not consistently improve performance is based on limited fine-tuning experiments, and task-specific factors may play a larger role than currently acknowledged.

## Next Checks

1. **Ablation Study on Visual Encoders**: Replace the visual transformer with alternative geometric perception modules (e.g., coordinate-based systems or graph neural networks) in select LVLMs to directly test whether visual encoder architecture is the limiting factor.

2. **Cross-Dataset Generalization**: Test the same LVLMs on established geometric reasoning datasets (e.g., CLEVR, ShapeNet) to determine whether the geometric perception limitations generalize beyond VisOnlyQA.

3. **Fine-tuning Protocol Optimization**: Conduct systematic fine-tuning experiments varying learning rates, batch sizes, and training duration across multiple tasks to determine optimal fine-tuning strategies and isolate which components benefit most from geometric training data.