---
ver: rpa2
title: 'MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent Reinforcement
  Learning'
arxiv_id: '2407.16312'
source_url: https://arxiv.org/abs/2407.16312
tags:
- agents
- reward
- multi-objective
- learning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOMAland, the first standardized benchmark
  suite for multi-objective multi-agent reinforcement learning (MOMARL). The library
  provides over 10 diverse environments covering various configurations of observability,
  reward structures, and utility considerations.
---

# MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.16312
- Source URL: https://arxiv.org/abs/2407.16312
- Reference count: 40
- Primary result: Introduces MOMAland, the first standardized benchmark suite for multi-objective multi-agent reinforcement learning (MOMARL)

## Executive Summary
MOMAland addresses the critical need for standardized benchmarks in the emerging field of multi-objective multi-agent reinforcement learning (MOMARL). The library provides over 10 diverse environments with vectorial rewards, unified APIs, and baseline algorithms to enable systematic comparison and research in MOMARL. By extending existing MARL frameworks with multi-objective capabilities through wrappers and decomposition strategies, MOMAland enables researchers to explore Pareto-optimal policy sets in team and individual reward settings.

## Method Summary
MOMAland implements MOMARL environments using PettingZoo's parallel and AEC APIs modified to return vectorial rewards instead of scalars. The library provides wrapper utilities (NormaliseReward, LineariseReward, CentraliseAgent) that enable composition of existing MORL and MARL methods without rewriting core algorithms. Baseline algorithms include MOMAPPO (decomposition-based), GPI-LS (policy evaluation), PCN (policy gradient), and IQL (individual Q-learning). The evaluation framework uses Pareto analysis metrics including cardinality, hypervolume, and expected utility, supporting both known and unknown utility function scenarios.

## Key Results
- MOMAland provides the first standardized benchmark suite for MOMARL with 10+ diverse environments
- Baseline results demonstrate the library's capability to support MOMARL research across different reward structures
- MOMAPPO decomposition approach shows effective Pareto front approximation in team-reward cooperative settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MOMAland enables systematic comparison of MOMARL algorithms by providing standardized environments with vectorial rewards and unified APIs.
- Mechanism: The library implements parallel and AEC APIs that return reward vectors instead of scalars, allowing existing MORL and MARL methods to be composed via wrappers (e.g., LineariseReward, CentraliseAgent) without rewriting core algorithms.
- Core assumption: Algorithm designers can treat vectorial rewards as first-class signals and compose existing solvers through API wrappers.
- Evidence anchors:
  - [abstract] "MOMAland also includes algorithms capable of learning policies in such settings" and utilities enabling existing MORL/MARL methods via centralisation or scalarisation.
  - [section 4] "MOMAland extends both PettingZoo APIs by returning a vectorial reward (i.e., a NumPy array) instead of a scalar for each agent" and lists NormaliseReward, LineariseReward, and CentraliseAgent wrappers.
  - [corpus] Weak: No corpus papers explicitly mention API composition; inferred from the benchmark library description.
- Break condition: If reward vectors cannot be normalized or linearly combined without losing critical trade-off structure, the wrapper-based approach fails for non-linear utility functions.

### Mechanism 2
- Claim: The library supports exploration of Pareto-optimal policies in team-reward cooperative settings by decomposing MOMARL into scalarized single-objective subproblems.
- Mechanism: MOMAPPO uses a decomposition strategy where each subproblem is trained with a different weight vector via MAPPO, then policies are evaluated on the original vectorial reward to form a Pareto set.
- Core assumption: Solving multiple scalarized subproblems with known weight vectors can approximate the Pareto front in MOMARL team-reward settings.
- Evidence anchors:
  - [abstract] "MOMAland includes utilities and learning algorithms intended to establish baselines for future research in MOMARL" and references MOMAPPO.
  - [section 6.1.1] Algorithm 1 explicitly shows MOMAPPO with decomposition, normalization, and non-dominated policy selection.
  - [corpus] Weak: No direct corpus evidence of this specific decomposition in MOMARL; inferred from MORL literature.
- Break condition: If the weight vector sampling misses critical trade-offs or if scalarization destroys non-convex Pareto regions, the decomposition will under-cover the true Pareto front.

### Mechanism 3
- Claim: MOMAland enables evaluation of MOMARL algorithms under different utility regimes (unknown vs known) by providing both Pareto-front and scalarized performance metrics.
- Mechanism: When utilities are unknown, algorithms are evaluated via cardinality, hypervolume, and expected utility metrics over approximated Pareto fronts; when utilities are known, scalarized rewards and Nash equilibrium concepts are used.
- Core assumption: Multi-objective performance can be meaningfully compressed into scalar indicators without biasing comparisons.
- Evidence anchors:
  - [abstract] "To provide strong baselines for future research, MOMAland also includes algorithms capable of learning policies in such settings."
  - [section 3.3] Defines hypervolume, cardinality, and expected utility as performance indicators for unknown utilities and discusses utility-based evaluation for known utilities.
  - [corpus] Weak: No corpus evidence that these metrics are standard in MOMARL; inferred from MORL and game theory.
- Break condition: If hypervolume or cardinality are dominated by a single objective scale or if the reference point is poorly chosen, comparisons will be misleading.

## Foundational Learning

- Concept: Pareto dominance and Pareto set/front
  - Why needed here: MOMARL solutions are defined in terms of Pareto optimality; algorithms must identify non-dominated policies.
  - Quick check question: Given two value vectors v = [3, 5] and v' = [4, 4], does v Pareto dominate v'?

- Concept: Multi-objective utility functions and optimization criteria (SER vs ESR)
  - Why needed here: Determines whether to optimize utility over expected returns or expected utility; affects algorithm design and solution concept.
  - Quick check question: If u(x, y) = x + 2y and returns are stochastic, which criterion (SER or ESR) yields higher expected utility?

- Concept: Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs)
  - Why needed here: MOMAland environments are formalised as MOPOSGs/MODec-POMDPs; agents must reason about partial observability and history dependence.
  - Quick check question: In a MODec-POMDP, what memory structure must an agent maintain to act optimally?

## Architecture Onboarding

- Component map: Core library → Environment suite (10+ domains) → API layer (parallel/AEC) → Wrapper utilities (normalize, linearize, centralize) → Baseline algorithms (MOMAPPO, GPI-LS, PCN, IQL) → Evaluation metrics (hypervolume, cardinality, EU, scalarized reward)
- Critical path: Instantiate environment → Choose reward mode (team/individual) → Select wrapper strategy → Run baseline → Compute performance indicators → Compare Pareto sets
- Design tradeoffs: Parallel API favors speed but requires simultaneous actions; AEC API supports turn-based games but serializes execution. Wrappers enable reuse but may hide reward structure complexity.
- Failure signatures: (1) Algorithm converges to a single point instead of a Pareto front; (2) Hypervolume decreases over training; (3) Wrapper incorrectly collapses multi-agent dimension; (4) Evaluation metrics are inconsistent across runs
- First 3 experiments:
  1. Run MOMAPPO on mo-multiwalker-stability-v0 with 10 uniform weights; verify hypervolume increases and Pareto set cardinality grows.
  2. Apply CentraliseAgent wrapper to moitem_gathering_v0; run PCN and GPI-LS; compare Pareto front coverage.
  3. Use LineariseReward wrapper on mobeach_v0; run scalarized IQL; plot scalarized reward vs timesteps for team vs individual reward modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we extend existing solution concepts like Pareto-Nash equilibrium to general MOMARL settings where agents have individual rewards and unknown utility functions?
- Basis in paper: [explicit] The paper explicitly states that "there is little work so far on the individual reward setting with unknown utility functions" and that "to the best of our knowledge, there is a limited amount of methods capable of identifying a Pareto-Nash set of policies."
- Why unresolved: Current methods either assume team rewards (falling back to Pareto set) or known utility functions (falling back to Nash equilibrium). The general individual reward with unknown utility setting remains an open challenge.
- What evidence would resolve it: Development and empirical validation of algorithms that can identify Pareto-Nash equilibria in sequential MOMARL settings with individual rewards and unknown utilities.

### Open Question 2
- Question: What are the implications of mixed optimisation criteria (SER and ESR) within the same MOMARL system, and how does this affect the existence of stable outcomes like Nash equilibria?
- Basis in paper: [explicit] The paper mentions that "it may not be possible for agents to reach a stable outcome, e.g., Nash equilibria may not exist under SER" and that "it is also possible to have a mixture of optimisation criteria within the same system."
- Why unresolved: Research on mixed optimisation criteria in MOMARL is extremely limited, and the effects on solution concepts and stability are not well understood.
- What evidence would resolve it: Theoretical analysis and empirical studies demonstrating the effects of mixed SER/ESR criteria on the existence and stability of equilibria in MOMARL systems.

### Open Question 3
- Question: How can we develop interactive MOMARL algorithms that allow agents to concurrently learn their users' preferences and how to optimally act in the environment?
- Basis in paper: [explicit] The paper identifies this as an open challenge, stating that "interactive MOMARL, where agents have to concurrently learn their associated user's preferences, as well as how to optimally act in the environment, has not yet been explored."
- Why unresolved: The challenges include misalignment of preferences, the fact that agents may not want to share preferences openly, and the potential for agents to model opponents' utility functions to gain strategic advantages.
- What evidence would resolve it: Development and evaluation of interactive MOMARL algorithms that can learn both preferences and optimal policies, along with empirical results showing their effectiveness in various MOMARL settings.

## Limitations
- The benchmark suite currently lacks environments with continuous action spaces
- Does not include benchmarks for MOMARL with unknown utilities where agents must discover preferences during training
- Evaluation methodology assumes known utility functions for baseline comparisons

## Confidence
- High confidence in the library infrastructure and API design claims, as these are directly verifiable through the codebase
- Medium confidence in the baseline algorithm implementations, as they follow established MORL/MARL approaches but haven't been extensively validated across all environments
- Low confidence in the completeness of the benchmark suite for all MOMARL research directions, given the identified gaps in continuous actions and unknown utilities

## Next Checks
1. Verify the Pareto front coverage by running MOMAPPO with 50 weight vectors instead of 20 and comparing hypervolume improvements
2. Test the wrapper utilities by applying LineariseReward and CentraliseAgent to a simple environment and confirming that scalarized rewards match manual calculations
3. Validate the evaluation metrics by computing hypervolume and cardinality on known Pareto fronts (e.g., synthetic data) and comparing against analytical results