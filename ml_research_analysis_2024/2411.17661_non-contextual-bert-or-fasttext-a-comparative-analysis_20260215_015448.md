---
ver: rpa2
title: Non-Contextual BERT or FastText? A Comparative Analysis
arxiv_id: '2411.17661'
source_url: https://arxiv.org/abs/2411.17661
tags:
- embeddings
- bert
- fasttext
- tasks
- muril
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of non-contextual BERT embeddings
  versus FastText embeddings for Marathi NLP tasks, including sentiment classification,
  hate speech detection, and news classification. While prior work has focused on
  contextual BERT embeddings, this research investigates non-contextual embeddings,
  which rely only on table lookup without a full forward pass.
---

# Non-Contextual BERT or FastText? A Comparative Analysis

## Quick Facts
- arXiv ID: 2411.17661
- Source URL: https://arxiv.org/abs/2411.17661
- Authors: Abhay Shanbhag; Suramya Jadhav; Amogh Thakurdesai; Ridhima Sinare; Raviraj Joshi
- Reference count: 7
- Primary result: Non-contextual BERT embeddings outperform FastText embeddings for Marathi NLP tasks

## Executive Summary
This study evaluates non-contextual BERT embeddings against FastText embeddings for Marathi NLP tasks, including sentiment classification, hate speech detection, and news classification. While prior work has focused on contextual BERT embeddings, this research investigates non-contextual embeddings, which rely only on table lookup without a full forward pass. The analysis compares MahaBERT, Muril, IndicFT, and MahaFT embeddings, including their compressed and uncompressed variants. Results show that contextual embeddings outperform non-contextual ones, and non-contextual BERT embeddings generally outperform FastText embeddings, with some deviations across specific tasks. T-SNE visualizations reveal that BERT embeddings form denser clusters compared to FastText embeddings.

## Method Summary
The study compares four embeddings (MahaBERT, Muril, IndicFT, MahaFT) across three Marathi datasets (MahaSent, MahaHate, MahaNews) using a Multiple Logistic Regression classifier with 5-fold cross-validation. Both compressed (via SVD to 300 dimensions) and uncompressed variants are evaluated. Contextual embeddings are extracted from the last hidden layer, while non-contextual embeddings come from the first embedding layer. FastText embeddings are generated by averaging word vectors after building a custom vocabulary from concatenated training and validation data.

## Key Results
- Contextual embeddings consistently outperform non-contextual embeddings across all tasks
- Non-contextual BERT embeddings generally outperform FastText embeddings, with some task-specific deviations
- BERT-based embeddings form denser, more compact clusters in T-SNE visualizations compared to FastText embeddings
- Non-contextual embeddings perform better uncompressed, while compressed contextual embeddings show no consistent trend

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-contextual BERT embeddings outperform FastText embeddings on Marathi NLP tasks.
- Mechanism: Non-contextual BERT embeddings are derived from the first embedding layer of a pre-trained BERT model. This layer provides fixed, subword-aware vector representations without requiring full contextual processing, which reduces computational overhead while retaining semantic richness.
- Core assumption: The subword tokenization and training of BERT models captures language-specific morphology and semantics better than FastText for Marathi.
- Evidence anchors:
  - [abstract] "BERT-based non-contextual embeddings extracted from the first BERT embedding layer yield better results than FastText-based embeddings"
  - [section] "BERT-based non-contextual embeddings extracted from the first BERT embedding layer yield better results than FastText-based embeddings, suggesting a potential alternative to FastText embeddings"
  - [corpus] Weak. The corpus contains related studies on static embeddings, but none specifically compares Marathi non-contextual BERT to FastText.

### Mechanism 2
- Claim: Contextual embeddings outperform non-contextual embeddings across all tasks.
- Mechanism: Contextual embeddings capture word meaning conditioned on the surrounding sentence, allowing the model to disambiguate homonyms and model syntactic dependencies. This richer representation improves downstream classification performance.
- Core assumption: The tasks (sentiment, hate speech, news classification) benefit significantly from context-aware representations rather than static or first-layer embeddings.
- Evidence anchors:
  - [abstract] "Results show that contextual embeddings outperform non-contextual ones"
  - [section] "Our evaluation includes applying embeddings to a Multiple Logistic Regression (MLR) classifier for task performance assessment... The results demonstrate that contextual embeddings outperform non-contextual embeddings."
  - [corpus] Moderate. The corpus includes related studies (e.g., D'Sa et al., 2020; Ahmed et al., 2024) showing BERT outperforms FastText, but the explicit comparison between contextual vs. non-contextual within BERT is not present.

### Mechanism 3
- Claim: Compression of contextual embeddings shows no consistent trend, while non-contextual embeddings perform better uncompressed.
- Mechanism: SVD-based dimensionality reduction (to 300 from 768) for contextual embeddings may remove noise but can also discard task-relevant features. Non-contextual embeddings, being already fixed vectors from the embedding layer, lose important subword information when compressed.
- Core assumption: The first embedding layer of BERT contains dense, informative subword features that are diluted by aggressive compression.
- Evidence anchors:
  - [abstract] "Non-contextual embeddings perform better uncompressed, while compressed contextual embeddings show no consistent trend"
  - [section] "when evaluating the effect of compression, non-contextual embeddings tend to perform better in their uncompressed form, while no consistent trend was observed for compressed contextual embeddings"
  - [corpus] Weak. No direct corpus evidence on compression effects in Marathi or on non-contextual embeddings specifically.

## Foundational Learning

- Concept: Subword tokenization and embeddings
  - Why needed here: FastText and BERT both rely on subword units to handle morphology and out-of-vocabulary words in Marathi; understanding this helps explain their performance differences.
  - Quick check question: What is the difference between subword tokenization in FastText vs. BERT, and why does this matter for low-resource languages?

- Concept: Contextual vs. non-contextual representations
  - Why needed here: The paper explicitly compares contextual BERT embeddings (from last hidden layer) with non-contextual ones (from first embedding layer); knowing how these differ is key to interpreting results.
  - Quick check question: How does a contextual embedding differ from a non-contextual embedding in terms of information content and use cases?

- Concept: Dimensionality reduction via SVD
  - Why needed here: The paper compresses embeddings from 768 to 300 dimensions; understanding SVD and its effects is crucial for evaluating compression results.
  - Quick check question: What does SVD do to an embedding matrix, and what are the trade-offs of reducing dimensionality in NLP tasks?

## Architecture Onboarding

- Component map:
  Input text → Marathi tokenizer (BERT or custom for FastText) → Embedding extraction (contextual: last hidden layer average; non-contextual: first embedding layer; FastText: averaged word vectors) → Optional SVD compression (contextual/non-contextual only) → MLR classifier → Output labels.

- Critical path:
  Embedding extraction → MLR classification. The choice of embedding (contextual vs. non-contextual, compressed vs. uncompressed) directly determines downstream performance.

- Design tradeoffs:
  Speed vs. accuracy: Non-contextual embeddings are faster (table lookup only) but slightly less accurate than contextual ones. Compression vs. information loss: Compressing embeddings saves memory but may degrade performance, especially for non-contextual embeddings. Model choice: MahaBERT generally outperforms Muril; MahaFT often outperforms IndicFT for Marathi.

- Failure signatures:
  FastText embeddings perform poorly if vocabulary construction misses important subwords or if averaging loses too much signal. Compressed non-contextual embeddings degrade sharply, suggesting information loss. Inconsistent performance across tasks for non-contextual BERT may indicate task-specific embedding suitability.

- First 3 experiments:
  1. Run the same MLR classifier with MahaBERT contextual (uncompressed) vs. non-contextual (uncompressed) embeddings on the MahaSent 3-class sentiment task; compare accuracy.
  2. Repeat experiment 1 with SVD compression to 300 dimensions for both embedding types; observe performance drop.
  3. Swap MahaBERT with Muril for both contextual and non-contextual settings on MahaHate 2-class task; compare results to assess model-specific differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the superior performance of non-contextual BERT embeddings over FastText embeddings hold across other low-resource languages beyond Marathi?
- Basis in paper: [explicit] The paper compares non-contextual BERT and FastText embeddings for Marathi NLP tasks and finds non-contextual BERT embeddings generally outperform FastText embeddings, suggesting they "present a promising alternative for low-resource NLP."
- Why unresolved: The study is limited to Marathi. While the results are promising, it remains unclear whether this performance pattern generalizes to other low-resource languages with different linguistic characteristics and resource availability.
- What evidence would resolve it: Conducting similar comparative analyses across multiple low-resource languages (e.g., other Indian languages, African languages, indigenous languages) would establish whether this performance pattern is language-specific or a broader phenomenon in low-resource NLP.

### Open Question 2
- Question: What specific aspects of the BERT model architecture contribute to the denser cluster formations observed in T-SNE visualizations compared to FastText embeddings?
- Basis in paper: [explicit] The paper notes that "BERT-based embeddings (MahaBERT and Muril) tend to form more compact clusters compared to FastText-based embeddings" and observes "denser class formations" in visualizations.
- Why unresolved: While the paper observes this pattern, it does not investigate the underlying causes. The denser clusters could result from BERT's contextual understanding, its pre-training objectives, the attention mechanism, or other architectural features.
- What evidence would resolve it: Systematic ablation studies comparing different BERT variants (different attention mechanisms, pre-training objectives, layer configurations) and their effects on embedding geometry would identify which architectural components drive the observed clustering behavior.

### Open Question 3
- Question: Why do compressed non-contextual embeddings perform better than their uncompressed counterparts while compressed contextual embeddings show no consistent trend?
- Basis in paper: [explicit] The paper observes that "non-contextual embeddings tend to perform better in their uncompressed form, while no consistent trend was observed for compressed contextual embeddings."
- Why unresolved: The paper reports this phenomenon but does not explain the underlying reasons. The contradictory behavior between non-contextual and contextual embeddings under compression suggests different information retention properties that merit investigation.
- What evidence would resolve it: Analyzing the information content preserved after compression (e.g., through probing tasks, principal component analysis of retained variance, or feature importance analysis) for both embedding types would reveal why non-contextual embeddings benefit from compression while contextual embeddings do not show consistent improvements.

## Limitations

- Analysis is limited to Marathi language, restricting generalizability to other low-resource languages without further validation
- Only uses a single classifier (MLR) without exploring how other models or architectures might interact with different embedding types
- Fixed compression strategy (SVD to 300 dimensions) may not be optimal across all tasks and embedding types

## Confidence

- **High confidence**: Contextual embeddings outperform non-contextual embeddings; non-contextual BERT embeddings generally outperform FastText embeddings
- **Medium confidence**: Compression of non-contextual embeddings degrades performance; no consistent trend for compressed contextual embeddings
- **Low confidence**: The specific superiority of MahaBERT over Muril (or vice versa) across all tasks; the generalizability of results to other languages

## Next Checks

1. Cross-language validation: Replicate the analysis on another low-resource language (e.g., Hindi or Tamil) using the same methodology to assess generalizability of the observed trends
2. Statistical significance testing: Apply paired t-tests or bootstrapping to the classification accuracies to determine whether performance differences between embedding types are statistically significant
3. Alternative classifiers: Evaluate the embeddings using a different classifier (e.g., SVM or fine-tuned neural network) to check if the observed performance patterns hold across model architectures