---
ver: rpa2
title: 'AlignGuard: Scalable Safety Alignment for Text-to-Image Generation'
arxiv_id: '2412.10493'
source_url: https://arxiv.org/abs/2412.10493
tags:
- safety
- alignment
- alignguard
- unsafe
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scalable safety alignment
  for text-to-image (T2I) models to prevent harmful content generation. The authors
  propose AlignGuard, a method that trains safety experts (LoRA matrices) using Direct
  Preference Optimization (DPO) on synthetically generated safe/unsafe image-text
  pairs from their CoProV2 dataset.
---

# AlignGuard: Scalable Safety Alignment for Text-to-Image Generation

## Quick Facts
- arXiv ID: 2412.10493
- Source URL: https://arxiv.org/abs/2412.10493
- Authors: Runtao Liu; I Chieh Chen; Jindong Gu; Jipeng Zhang; Renjie Pi; Qifeng Chen; Philip Torr; Ashkan Khakzar; Fabio Pizzati
- Reference count: 40
- Primary result: Achieves 7x more concept removal than baselines while maintaining generation quality

## Executive Summary
AlignGuard addresses the challenge of scalable safety alignment for text-to-image models by training category-specific LoRA experts using Direct Preference Optimization (DPO) on synthetically generated safe/unsafe image-text pairs. The method employs a novel data-based merging strategy called Co-Merge that leverages neuron activation frequencies to combine multiple safety experts efficiently. Experiments demonstrate that AlignGuard significantly reduces inappropriate content generation (IP from 0.51 to 0.07 on Stable Diffusion v1.5) while maintaining high generation quality, outperforming existing safety alignment methods by 7x in concept removal capability.

## Method Summary
AlignGuard trains safety experts (LoRA matrices) using DPO on a synthetically generated dataset (CoProV2) containing paired safe/unsafe image-text pairs across 723 harmful concepts in 7 categories. The key innovation is Co-Merge, a data-based merging strategy that selects neurons based on activation frequency across experts rather than naive averaging or expensive optimization. This approach enables training on the full dataset scale while maintaining generation quality through the paired image training mechanism that guides the model toward safe outputs without catastrophic forgetting.

## Key Results
- Reduces Inappropriate Probability (IP) from 0.51 to 0.07 on Stable Diffusion v1.5
- Maintains generation quality with FID of 70.96 vs 69.77 and CLIPScore of 32.32 vs 33.52
- Achieves 7x more concept removal than baseline methods
- Generalizes across multiple T2I models (SD v1.5, v2.1, SDXL)
- Demonstrates robustness to adversarial attacks with lower IP scores than baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Safety alignment at scale is achieved by decomposing the problem into category-specific experts and merging them.
- **Mechanism:** Train separate LoRA experts for each safety category, then merge them using a data-driven neuron activation frequency strategy.
- **Core assumption:** Different safety categories activate distinct neurons in the LoRA architecture, allowing clean separation and merging.
- **Evidence anchors:** [abstract] "We enable the application of Direct Preference Optimization (DPO) for safety purposes in T2I models by synthetically generating a dataset of harmful and safe image-text pairs" [section 4.2] "Our intuition is that by decomposing safety alignment in multiple categories, trained LoRAs sharing the same architecture would encode information about category-specific alignment in different neurons"
- **Break condition:** If category concepts overlap significantly (e.g., "violence" and "self-harm" both involve blood), the same neurons may activate, causing interference and reducing effectiveness.

### Mechanism 2
- **Claim:** DPO-based training with paired safe/unsafe images effectively guides the model away from harmful content.
- **Mechanism:** For each unsafe prompt, a paired safe image is generated. DPO is applied so the model prefers generating the safe image over the unsafe one, while maintaining generation quality.
- **Core assumption:** The paired safe image is visually similar enough to the unsafe one to provide meaningful guidance without causing catastrophic forgetting.
- **Evidence anchors:** [section 4.1] "we use DPO to discourage the generation of unsafe outputs for an unsafe prompt pU, and promote the generation of the paired safe image xS instead" [section 5.2] "AlignGuard demonstrates superior generalization, achieving better IP scores on unseen I2P (0.08) and UD (0.16)"
- **Break condition:** If the paired safe image is too dissimilar, the model may learn to generate something completely different rather than removing harmful content. If too similar, the distinction may be too subtle for effective learning.

### Mechanism 3
- **Claim:** Co-Merge outperforms existing merging strategies by being data-aware and requiring no hyperparameter tuning.
- **Mechanism:** Co-Merge selects the most frequently activated neurons across all prompts for each neuron position, creating an optimal merged LoRA without expensive optimization.
- **Core assumption:** The activation frequency of neurons across multiple prompts correlates with their importance for safety alignment.
- **Evidence anchors:** [section 4.3] "we identify the neurons with the highest activation frequencies across experts. Specifically, for each neuron j in the merged expert Lmerged, we select the neuron from the original set of experts {L1, ..., LN} that has the highest activation frequency count" [section 5.4] "Co-Merge consistently outperform these baselines... by performing a data-aware merging using unsafe prompts, we are able to optimally balance the contributions of each expert"
- **Break condition:** If activation frequency doesn't correlate with safety importance (e.g., if high-frequency neurons correspond to background details rather than harmful concepts), the merged model will underperform.

## Foundational Learning

- **Concept: Diffusion models and denoising process**
  - Why needed here: AlignGuard builds on understanding how diffusion models generate images through iterative denoising
  - Quick check question: How does a diffusion model transform random noise into a coherent image?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: AlignGuard uses DPO to align the model with safety preferences
  - Quick check question: What distinguishes DPO from standard supervised learning in the context of model alignment?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Safety experts are implemented as LoRA matrices for efficient fine-tuning
  - Quick check question: Why would LoRA be preferred over full fine-tuning for safety alignment?

## Architecture Onboarding

- **Component map:** T2I model (Stable Diffusion) -> CoProV2 dataset generator -> Category-specific LoRA experts -> Co-Merge algorithm -> Merged safety model

- **Critical path:** 1) Generate CoProV2 dataset (unsafe/safe prompt pairs + images) 2) Train LoRA experts per category using DPO 3) Apply Co-Merge to combine experts 4) Evaluate merged model on safety benchmarks 5) Deploy merged LoRA with base model

- **Design tradeoffs:** Category granularity vs. computational cost (more categories = better safety but more training), Dataset size vs. training time (larger CoProV2 = better performance but longer training), LoRA rank vs. performance (higher rank = better safety but more parameters)

- **Failure signatures:** High IP but low FID/CLIPScore → Model refuses to generate anything (over-alignment), Low IP but high FID/CLIPScore → Safety alignment works but generation quality drops, Inconsistent performance across categories → Category overlap or merging issues

- **First 3 experiments:** 1) Train a single LoRA expert on one category (e.g., Violence) and evaluate IP reduction 2) Test Co-Merge with two experts (e.g., Violence + Sexual) to validate merging strategy 3) Compare IP/FID trade-off with baseline methods on a subset of CoProV2

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions arise from the research:

- How does AlignGuard's performance scale with even larger datasets beyond CoProV2, and what are the computational limits?
- Can AlignGuard's merging strategy (Co-Merge) be generalized to other model merging scenarios beyond safety alignment?
- How robust is AlignGuard to adaptive adversarial attacks that specifically target the safety alignment mechanisms?
- What is the minimum LoRA rank that maintains acceptable safety alignment performance while maximizing computational efficiency?

## Limitations
- Category overlap may cause neuron interference when safety concepts share significant conceptual overlap
- The assumption that neuron activation frequency correlates with safety importance lacks theoretical justification
- Scalability claims are based on category-level training rather than concept-level, with the paper acknowledging that existing baselines fail at the full 723 concept scale

## Confidence
**High Confidence:** The empirical results showing 7x improvement in concept removal and substantial IP reduction are well-supported by the presented data and consistent across multiple T2I models.

**Medium Confidence:** The Co-Merge algorithm's superiority over existing merging strategies is demonstrated through experiments, but the underlying assumption about neuron activation frequencies correlating with safety importance lacks theoretical justification.

**Low Confidence:** The scalability claims to 723 concepts are primarily based on category-level training rather than concept-level, with the paper acknowledging that existing baselines fail at this scale.

## Next Checks
1. **Category Overlap Validation:** Systematically test the method's performance when safety categories share significant conceptual overlap (e.g., Violence vs. Self-Harm). Measure neuron activation patterns to verify whether distinct neurons are truly activated for each category, and evaluate performance degradation when categories overlap.

2. **Adversarial Robustness Extension:** Test AlignGuard against more sophisticated adversarial attacks beyond those presented, including gradient-based optimization attacks, prompt injection techniques, and semantic-preserving harmful content. Compare robustness against state-of-the-art safety alignment methods.

3. **Long-term Stability Analysis:** Evaluate the method's effectiveness after multiple rounds of safety updates and concept additions. Measure catastrophic forgetting by tracking performance on previously aligned categories while adding new ones, and assess whether the Co-Merge strategy maintains balance over time.