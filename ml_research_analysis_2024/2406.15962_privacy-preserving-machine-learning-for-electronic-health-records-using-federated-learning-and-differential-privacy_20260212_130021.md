---
ver: rpa2
title: Privacy Preserving Machine Learning for Electronic Health Records using Federated
  Learning and Differential Privacy
arxiv_id: '2406.15962'
source_url: https://arxiv.org/abs/2406.15962
tags:
- data
- learning
- federated
- patient
- records
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses privacy-preserving machine learning for electronic
  health records by implementing federated learning and differential privacy techniques.
  The authors combined synthetic patient data with medical cost data to create a realistic
  dataset, then applied multiple regression models to predict medical charges.
---

# Privacy Preserving Machine Learning for Electronic Health Records using Federated Learning and Differential Privacy

## Quick Facts
- arXiv ID: 2406.15962
- Source URL: https://arxiv.org/abs/2406.15962
- Reference count: 0
- Primary result: Demonstrated privacy-preserving machine learning for EHR data using federated learning and differential privacy, achieving R-squared of 0.86 and RMSE of $4,224.49 with GradientBoostingRegressor

## Executive Summary
This study addresses privacy-preserving machine learning for electronic health records by implementing federated learning and differential privacy techniques. The authors combined synthetic patient data with medical cost data to create a realistic dataset, then applied multiple regression models to predict medical charges. They evaluated differential privacy by adding noise to charge values, implemented pseudonymization using SHA-256 encryption for social security numbers, and developed a federated learning system where three clients collaboratively trained a model while keeping data local. The federated learning implementation achieved convergence around 80-90 epochs, with mean absolute error decreasing as training progressed.

## Method Summary
The authors created a privacy-preserving machine learning pipeline for electronic health records by combining synthetic patient data from Synthea with medical cost data. They implemented pseudonymization using SHA-256 encryption for social security numbers, added differential privacy noise to charge values, and developed a federated learning system with three simulated clients. The pipeline included data preprocessing with StandardScaler and OneHotEncoder, federated training using TensorFlow Federated's build_federated_averaging_process, and evaluation using multiple regression models including GradientBoostingRegressor. The federated averaging algorithm aggregated model updates from clients without sharing raw data, while differential privacy added calibrated noise to protect individual patient information.

## Key Results
- GradientBoostingRegressor achieved R-squared of 0.86 and RMSE of $4,224.49 on the combined dataset
- Federated learning converged around 80-90 epochs with decreasing mean absolute error
- Differential privacy implementation maintained model utility while protecting sensitive charge values
- SHA-256 pseudonymization successfully protected social security numbers from direct re-identification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Federated averaging enables collaborative model training while keeping patient data local
- Mechanism: Each client trains locally on its private data, computes gradient updates, sends only gradients to server, server averages gradients and broadcasts updated model back to all clients
- Core assumption: Client updates can be aggregated meaningfully without sharing raw data
- Evidence anchors:
  - [section] "The server aggregates the updates from each client to compute an overall update. The simplest way to do this is to compute an average"
  - [section] "Each client computes an update to the model parameters based on its local data... Each client sends its computed update back to the server"
  - [corpus] No direct evidence found - corpus papers discuss federated learning but don't verify this specific averaging mechanism
- Break condition: If client data distributions are too heterogeneous, simple averaging may produce suboptimal global models

### Mechanism 2
- Claim: Differential privacy adds calibrated noise to protect individual data points while preserving statistical utility
- Mechanism: Noise sampled from Laplace or Gaussian distribution is added to sensitive values (charges) before model training, with noise magnitude controlled by privacy budget (ε)
- Core assumption: Added noise sufficiently masks individual records while maintaining aggregate patterns for model learning
- Evidence anchors:
  - [section] "To simulate DP, noise was added to the 'charges' column of the patient records"
  - [section] "The highlighted columns in Fig. 6 compare the medical charges before adding noise (green) and after (magenta)"
  - [corpus] No direct evidence found - corpus papers mention differential privacy but don't verify this specific implementation
- Break condition: If privacy budget is too low or noise too high, model performance degrades significantly

### Mechanism 3
- Claim: SHA-256 pseudonymization prevents direct re-identification of patients while maintaining data utility
- Mechanism: Each patient's SSN is replaced with its SHA-256 hash, creating a one-way mapping that cannot be reversed without the original value
- Core assumption: Hash collisions are negligible and original SSNs are not exposed in plaintext
- Evidence anchors:
  - [section] "a pseudonymization scheme was applied using a secure hash function. The hashlib library provide the functionality needed to replace each patients' SSN with a SHA-256 pseudonym"
  - [section] "As shown in Fig. 8, this provides protection against leaks or malicious use of the patient data because each SSN cannot be decrypted without a unique key"
  - [corpus] No direct evidence found - corpus papers discuss pseudonymization but don't verify this specific SHA-256 implementation
- Break condition: If attackers obtain plaintext SSNs through other means, pseudonymization provides no protection

## Foundational Learning

- Concept: Linear regression and gradient boosting fundamentals
  - Why needed here: The study evaluates multiple regression models (LinearRegression, GradientBoostingRegressor, etc.) to predict medical charges
  - Quick check question: What is the key difference between linear regression and gradient boosting in terms of how they model relationships between features and target?

- Concept: Federated learning architecture and TFF framework
  - Why needed here: The implementation uses TensorFlow Federated (TFF) to coordinate multi-client training, requiring understanding of client-server communication patterns
  - Quick check question: In the federated averaging algorithm, what information flows from clients to server and back?

- Concept: Differential privacy and privacy budgets
  - Why needed here: DP is applied to sensitive charge values, requiring understanding of how noise magnitude relates to privacy guarantees
  - Quick check question: How does decreasing the privacy budget (ε) affect both privacy protection and model utility?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Federated learning orchestration -> Model training and evaluation
  Synthetic data + medical costs -> SHA-256 pseudonymization -> DP noise addition

- Critical path:
  1. Data preparation and partitioning
  2. Model initialization and TFF wrapping
  3. Federated training loop (communication rounds)
  4. Loss/MAE monitoring and convergence checking
  5. Final model evaluation

- Design tradeoffs:
  - Simple averaging vs. weighted averaging based on client data size
  - Fixed learning rate vs. adaptive optimization
  - Fixed number of rounds vs. early stopping based on convergence
  - No client selection vs. dynamic client participation

- Failure signatures:
  - MAE plateaus early: Possible client data distribution mismatch or insufficient training rounds
  - MAE increases over time: Possible learning rate too high or client updates too noisy
  - Training fails to converge: Possible model architecture too simple or data not properly normalized

- First 3 experiments:
  1. Baseline: Run with current three-client setup, record convergence behavior and final MAE
  2. Privacy sensitivity: Vary noise scale in differential privacy, measure impact on MAE and privacy guarantees
  3. Client heterogeneity: Create clients with different data distributions, observe federated averaging performance

## Open Questions the Paper Calls Out

- How would federated learning performance be affected if clients had heterogeneous data distributions rather than the identical splits used in this study?
- What is the trade-off between differential privacy noise levels and model performance in federated learning for EHR data?
- How would secure aggregation mechanisms impact communication efficiency and model convergence in this federated learning setup?

## Limitations
- Synthetic data combined with medical costs may not fully represent real-world EHR complexity and heterogeneity
- Only three simulated clients used, insufficient for evaluating large-scale healthcare network performance
- Differential privacy implemented as simplified noise addition rather than formal DP-SGD with quantified privacy guarantees

## Confidence

- High Confidence: Federated averaging mechanism and SHA-256 pseudonymization are well-established techniques with clear implementation paths
- Medium Confidence: GradientBoostingRegressor performance metrics likely valid for specific dataset but may not generalize to real-world data
- Low Confidence: Differential privacy implementation's actual privacy guarantees, as noise addition method is simplified and privacy budget parameters not fully specified

## Next Checks

1. **Real Data Validation**: Replace synthetic data with real EHR datasets from multiple healthcare institutions to assess model performance under realistic data distributions and heterogeneity conditions

2. **Formal DP Implementation**: Implement true differential privacy using DP-SGD or similar mechanisms with formally specified privacy budgets (epsilon, delta) and conduct privacy accounting to quantify actual privacy guarantees

3. **Scalability Testing**: Scale the federated learning implementation to dozens or hundreds of clients using real-world healthcare network topologies to evaluate convergence behavior, communication efficiency, and model performance at scale