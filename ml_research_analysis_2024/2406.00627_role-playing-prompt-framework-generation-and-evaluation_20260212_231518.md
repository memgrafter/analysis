---
ver: rpa2
title: 'Role-playing Prompt Framework: Generation and Evaluation'
arxiv_id: '2406.00627'
source_url: https://arxiv.org/abs/2406.00627
tags:
- role-playing
- answer
- llms
- role
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a prompt-based framework to generate and evaluate
  role-playing datasets using GPT-4o. The framework decomposes the task into three
  sub-tasks: plot construction, question generation, and answer generation, employing
  various prompt engineering techniques.'
---

# Role-playing Prompt Framework: Generation and Evaluation

## Quick Facts
- arXiv ID: 2406.00627
- Source URL: https://arxiv.org/abs/2406.00627
- Authors: Xun Liu; Zhengwei Ni
- Reference count: 38
- Key outcome: Fine-tuned models achieve higher Rouge-L scores than their original counterparts, with the "Explain" prompt yielding the best performance across all three evaluation dimensions

## Executive Summary
This paper presents a prompt-based framework for generating and evaluating role-playing datasets using GPT-4o. The framework decomposes the task into three sub-tasks: plot construction, question generation, and answer generation, employing various prompt engineering techniques. Open-source LLMs (ChatGLM3-7B, LLaMA2-13B, Baichuan2-13B) are fine-tuned on the generated datasets using LoRA. Performance is evaluated using a GPT-4o-based evaluator across three dimensions (Characteristics, Task Response, Quality) and quantitatively via Rouge-L metrics. Results demonstrate that fine-tuned models outperform their original counterparts, with the dataset generated using the "Explain" prompt showing the best performance.

## Method Summary
The framework uses GPT-4o to generate role-playing datasets by decomposing the task into plot construction, question generation, and answer generation sub-tasks. Four different answer generation prompts (Base, Read, Emotion, Explain) are used to create four datasets. Open-source LLMs are fine-tuned using LoRA with these datasets (10 epochs, learning rate 2e-4, batch size 4). Performance is evaluated using both Rouge-L metrics for text similarity and a GPT-4o-based evaluator that ranks responses across three dimensions: Characteristics, Task Response, and Quality.

## Key Results
- Fine-tuned models achieve higher Rouge-L scores than their original counterparts
- The dataset generated using the "Explain" prompt yields the best performance across all three evaluation dimensions
- Adding explanation requirements to answer prompts improves role-playing dialog quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing role-playing into three sub-tasks improves GPT-4o generation quality
- Mechanism: Task decomposition reduces cognitive load and enables targeted prompt engineering for each sub-task
- Core assumption: GPT-4o performs better on simpler, more focused tasks than on complex end-to-end generation
- Evidence anchors: [abstract] "decomposes the task into three sub-tasks: plot construction, question generation, and answer generation"; [section] "Inspired by Khot et al. [10], the role-playing dialog generation task is decomposed into three sub-tasks, each carried out using distinct prompts with GPT-4o"
- Break condition: If sub-tasks become too interdependent, decomposition overhead may outweigh benefits

### Mechanism 2
- Claim: Adding explanation requirement to answer prompts improves role-playing dialog quality
- Mechanism: Requiring explanation forces the model to engage in deeper reasoning about character behavior before generating responses
- Core assumption: Chain-of-thought reasoning improves output quality in creative generation tasks
- Evidence anchors: [abstract] "the dataset generated using the 'Explain' prompt yields the best performance across all three evaluation dimensions"; [section] "prior research indicates that in reasoning application areas, asking the LLM to explain its output can improve response quality and accuracy"
- Break condition: If explanations become formulaic or repetitive, the quality benefit may diminish

### Mechanism 3
- Claim: LoRA fine-tuning on generated datasets improves open-source LLM role-playing capabilities
- Mechanism: Low-rank adaptation efficiently adapts pre-trained models to domain-specific patterns in role-playing dialogues
- Core assumption: Generated datasets capture sufficient domain-relevant patterns for effective fine-tuning
- Evidence anchors: [abstract] "fine-tuned models achieve higher Rouge-L scores than their original counterparts"; [section] "we employ the Low-Rank Adaptation (LoRA) method, a finetuning technique that strategically adjusts trainable model parameters while maintaining the pre-trained model weights"
- Break condition: If generated data contains systematic biases or lacks diversity, fine-tuning may degrade performance

## Foundational Learning

- Concept: Prompt Engineering Techniques
  - Why needed here: Different prompt structures (in-context learning, chain-of-thought, emotion prompts) are tested to optimize generation quality
  - Quick check question: What are the key differences between in-context learning and chain-of-thought prompting?

- Concept: Evaluation Metrics for Text Generation
  - Why needed here: Rouge-L measures n-gram overlap while GPT-4o evaluator provides multi-dimensional assessment
  - Quick check question: How does Rouge-L differ from other text generation evaluation metrics like BLEU or ROUGE-1?

- Concept: Fine-tuning Methods for LLMs
  - Why needed here: LoRA is used to efficiently adapt open-source models to role-playing task without full fine-tuning
  - Quick check question: What are the computational advantages of LoRA compared to full fine-tuning?

## Architecture Onboarding

- Component map: GPT-4o (data generation) → Datasets → LoRA fine-tuning → Open-source LLMs → GPT-4o evaluator → Performance ranking
- Critical path: 1. Generate questions using plot → question pipeline; 2. Generate answers using four different prompts; 3. Split into training/validation sets; 4. LoRA fine-tune models on training data; 5. Evaluate on validation data using both Rouge-L and GPT-4o evaluator
- Design tradeoffs: Generation quality vs. diversity; Evaluation comprehensiveness vs. efficiency; Model size vs. performance
- Failure signatures: Low Rouge-L scores indicate poor alignment; High variance in GPT-4o evaluator rankings suggests inconsistent performance; Validation loss not decreasing during LoRA fine-tuning indicates poor dataset quality or hyperparameters
- First 3 experiments: 1. Compare Rouge-L scores of fine-tuned vs. original models on validation set; 2. Run GPT-4o evaluator on predictions from all fine-tuned models; 3. Ablation study: Remove explanation requirement from Answer-Explain prompt and compare performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the framework generalize to other narrative domains beyond Journey to the West, such as modern fiction or historical texts?
- Basis in paper: [inferred] The paper only tests the framework on Journey to the West, raising questions about its broader applicability.
- Why unresolved: The evaluation is limited to a single literary work, preventing conclusions about cross-domain effectiveness.
- What evidence would resolve it: Experiments applying the framework to diverse narrative sources (e.g., modern novels, historical accounts) and comparing performance metrics across domains.

### Open Question 2
- Question: How do the evaluation prompts' design choices (e.g., three dimensions, ranking format) affect the reliability and consistency of GPT-4o-based assessments?
- Basis in paper: [explicit] The paper uses GPT-4o to evaluate performance across three dimensions (Characteristics, Task Response, Quality) through ranking, but does not analyze the robustness of this approach.
- Why unresolved: No validation of the evaluation prompts' effectiveness or potential biases in the GPT-4o evaluator's judgments is provided.
- What evidence would resolve it: A study comparing GPT-4o's rankings with human evaluations or other established metrics, and analyzing inter-rater reliability.

### Open Question 3
- Question: What is the optimal balance between prompt complexity (e.g., adding "Explain" or "Emotion" instructions) and dataset quality for different model sizes?
- Basis in paper: [explicit] The paper tests four prompt variants but does not systematically analyze how prompt complexity affects smaller vs. larger models.
- Why unresolved: The experiments focus on three models of varying sizes without exploring how prompt complexity scales with model capacity.
- What evidence would resolve it: A study varying prompt complexity across a wider range of model sizes and analyzing the trade-off between instruction detail and dataset quality.

## Limitations
- The framework relies heavily on GPT-4o as both generator and evaluator, creating potential circularity in evaluation
- The study uses a single source material (Journey to the West), limiting generalizability to other role-playing contexts
- The decomposition approach assumes task independence that may not hold for complex role-playing scenarios

## Confidence

- High Confidence: The basic premise that LoRA fine-tuning improves open-source LLM performance (supported by direct Rouge-L score comparisons)
- Medium Confidence: The decomposition approach improves generation quality (mechanistically sound but limited corpus validation)
- Medium Confidence: The explanation requirement enhances role-playing capabilities (best performer in evaluation but weak corpus support)

## Next Checks

1. Cross-Domain Validation: Test the framework on a different source material (e.g., modern fiction or historical text) to verify generalizability beyond Journey to the West. Compare whether the "Explain" prompt consistently outperforms other prompts across domains.

2. Human Evaluation Benchmark: Conduct human evaluation of the generated role-playing dialogues to validate the GPT-4o evaluator's rankings. Specifically, have human raters assess the three evaluation dimensions (Characteristics, Task Response, Quality) and compare with GPT-4o results.

3. Ablation Study on Decomposition: Run an end-to-end generation experiment without task decomposition to measure the actual performance cost/benefit of the three-step approach. This would help quantify whether the decomposition overhead is justified by quality improvements.