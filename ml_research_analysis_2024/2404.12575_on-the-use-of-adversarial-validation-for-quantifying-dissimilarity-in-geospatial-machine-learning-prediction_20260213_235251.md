---
ver: rpa2
title: On the use of adversarial validation for quantifying dissimilarity in geospatial
  machine learning prediction
arxiv_id: '2404.12575'
source_url: https://arxiv.org/abs/2404.12575
tags:
- prediction
- dissimilarity
- data
- methods
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating geospatial machine
  learning models when training data and prediction locations differ. The authors
  propose a novel method called dissimilarity quantification by adversarial validation
  (DAV) that uses a binary classifier to measure the dissimilarity between sample
  data and prediction locations on a scale from 0% to 100%.
---

# On the use of adversarial validation for quantifying dissimilarity in geospatial machine learning prediction

## Quick Facts
- arXiv ID: 2404.12575
- Source URL: https://arxiv.org/abs/2404.12575
- Reference count: 40
- Primary result: A novel method quantifies dissimilarity between sample data and prediction locations in geospatial ML, showing that random cross-validation works best when dissimilarity is low but geospatial methods outperform it as dissimilarity increases

## Executive Summary
This paper addresses a fundamental challenge in geospatial machine learning: how to evaluate models when training data and prediction locations differ. The authors propose dissimilarity quantification by adversarial validation (DAV), which uses a binary classifier to measure distributional differences between sample data and prediction locations on a 0-100% scale. Through synthetic and real dataset experiments, they demonstrate that DAV effectively quantifies dissimilarity and that cross-validation method performance varies predictably with dissimilarity levels. When dissimilarity is low (<30%), random cross-validation provides the most accurate evaluations, but as dissimilarity increases, geospatial cross-validation methods become more accurate and outperform random selection.

## Method Summary
The DAV method quantifies dissimilarity by treating sample data and prediction locations as separate categories in a binary classification problem. It randomly samples prediction locations to match the number of samples, assigns binary labels, trains a random forest classifier, and uses the AUC score to measure how well the classifier can distinguish between the two groups. This AUC is normalized to a 0-100% scale where low values indicate similar distributions and high values indicate dissimilarity. The method was tested by constructing prediction tasks with varying levels of sample clustering, calculating dissimilarities with DAV, and evaluating performance using random cross-validation (RDM-CV) and geospatial cross-validation methods (BLK-CV, SP-CV) based on RMSE differences.

## Key Results
- DAV successfully quantifies dissimilarity across the full 0-100% range in synthetic datasets with controlled clustering levels
- When dissimilarity is low (<30%), random CV provides the most accurate evaluation results
- As dissimilarity increases, geospatial CV methods, especially SP-CV, become increasingly accurate and outperform random CV
- When dissimilarity is very high (≥90%), no CV method provides accurate evaluation results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adversarial validation classifier effectively quantifies dissimilarity by measuring its ability to distinguish between sample data and prediction locations.
- Mechanism: DAV trains a binary classifier (random forest) to differentiate samples from prediction locations. When the classifier struggles to distinguish them (low AUC), the distributions are similar; when it easily separates them (high AUC), dissimilarity is high.
- Core assumption: The feature space structure that distinguishes samples from prediction locations reflects true distributional differences relevant to model performance.
- Evidence anchors:
  - [abstract] "AV is a technique proposed by FastML (2016) to detect and mitigate the problems of data distribution differences between test and training datasets"
  - [section] "the core idea of AV is treating test and training datasets as separate categories in a binary classification problem"
  - [corpus] Weak evidence - corpus neighbors discuss adversarial validation in different contexts (jailbreaking, ensemble clustering) but not geospatial dissimilarity quantification specifically.
- Break condition: If the classifier can achieve perfect separation purely based on location metadata rather than feature space differences, the metric becomes location-based rather than distributional.

### Mechanism 2
- Claim: Random selection of prediction locations equalizes class imbalance while maintaining unbiased representation of the full prediction space.
- Mechanism: DAV randomly samples prediction locations to match the number of samples, preventing the classifier from being overwhelmed by the majority class while preserving spatial coverage.
- Core assumption: Random sampling from prediction locations provides an unbiased representation of the entire prediction space.
- Evidence anchors:
  - [section] "we employ a random selection in step 1 to conduct such an unbiased representation"
  - [section] "Random sampling and regular sampling ensure that the sample data and prediction locations are similar from the perspective of data distribution"
  - [corpus] No direct evidence in corpus neighbors about random sampling strategies for imbalance in geospatial contexts.
- Break condition: If prediction locations exhibit strong spatial clustering or non-uniform distribution, random sampling may misrepresent the true prediction space.

### Mechanism 3
- Claim: The transition from random CV to geospatial CV methods as dissimilarity increases reflects the changing nature of the prediction task from interpolation to extrapolation.
- Mechanism: When dissimilarity is low (<30%), samples adequately represent the prediction space, making random CV appropriate. As dissimilarity increases, geospatial CV methods better account for spatial autocorrelation and feature space differences.
- Core assumption: The relationship between dissimilarity and CV method performance follows predictable patterns across different datasets and prediction tasks.
- Evidence anchors:
  - [abstract] "when dissimilarity is low (usually lower than 30%), RDM-CV provides the most accurate evaluation results"
  - [section] "As dissimilarity increases, geospatial CV methods, especially SP-CV, become more and more accurate"
  - [corpus] Weak evidence - corpus neighbors don't discuss cross-validation methods or their relationship to dissimilarity in geospatial contexts.
- Break condition: If the underlying data distribution changes dramatically or if the feature space relationships are non-stationary, the established patterns may not hold.

## Foundational Learning

- Concept: Adversarial validation technique
  - Why needed here: Provides a principled way to measure distributional differences between two datasets in feature space
  - Quick check question: What does a low AUC score indicate about the relationship between sample data and prediction locations?

- Concept: Cross-validation methods and their assumptions
  - Why needed here: Understanding when random CV versus geospatial CV methods are appropriate requires knowing their underlying assumptions about data similarity
  - Quick check question: Why does random CV tend to be over-optimistic when sample data and prediction locations differ?

- Concept: Feature space representation in geospatial data
  - Why needed here: DAV relies on comparing feature spaces, so understanding how geospatial covariates relate to target variables is crucial
  - Quick check question: How might spatial autocorrelation in covariates affect the interpretation of dissimilarity scores?

## Architecture Onboarding

- Component map: Sample data with target variable -> Prediction locations with covariates -> DAV module (random sampling, label assignment, classifier training, AUC calculation, normalization) -> CV comparison (RDM-CV, BLK-CV, SP-CV with RMSE) -> Output (dissimilarity scores and CV evaluation performance comparisons)

- Critical path: Construct prediction tasks → Calculate dissimilarities with DAV → Evaluate with multiple CV methods → Analyze relationship patterns

- Design tradeoffs:
  - Using random forest for both prediction and adversarial validation ensures consistency but may limit the method to problems where RF performs well
  - Random sampling of prediction locations addresses class imbalance but may not capture spatial heterogeneity in prediction locations
  - AUC normalization to 0-100% scale makes interpretation intuitive but assumes AUC below 0.5 is rare or non-informative

- Failure signatures:
  - Perfect classification by adversarial validation based solely on location metadata (not feature space)
  - Consistent disagreement between synthetic and real dataset results indicating method sensitivity to data characteristics
  - CV method performance patterns that don't align with established geospatial prediction principles

- First 3 experiments:
  1. Run DAV on a synthetic dataset with known clustering levels to verify the 0-100% scale correspondence
  2. Compare RDM-CV and geospatial CV methods on a dataset with controlled dissimilarity levels
  3. Test DAV sensitivity by varying the random forest hyperparameters and observing dissimilarity score stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of binary classifier in DAV affect the quantified dissimilarity scores across different geospatial datasets?
- Basis in paper: [explicit] The authors note that "any ML algorithm that can be used as a binary classifier is acceptable" but chose random forest for its stability, and suggest that "it is preferable that the ML algorithm (including hyperparameters) for the AV classifier and for the prediction task are the same."
- Why unresolved: The paper only tested DAV with random forest as the classifier, so the sensitivity of DAV to different classifier choices remains unexplored.
- What evidence would resolve it: Comparative experiments using multiple classifier types (e.g., logistic regression, gradient boosting, neural networks) across various geospatial datasets to assess consistency and variation in DAV dissimilarity scores.

### Open Question 2
- Question: What is the minimum number of samples required for DAV to produce reliable dissimilarity quantification, and how does this threshold vary with dataset complexity?
- Basis in paper: [inferred] The authors set the number of samples to 1000 following other studies, but acknowledge that "the number of prediction locations is typically much larger than the number of samples," suggesting this could be a limiting factor.
- Why unresolved: The paper does not systematically explore how varying sample sizes affect DAV's performance or establish guidelines for minimum sample requirements.
- What evidence would resolve it: Systematic experiments with varying sample sizes (e.g., 100, 500, 1000, 2000) across multiple datasets to identify the point at which DAV dissimilarity scores stabilize and become reliable.

### Open Question 3
- Question: Can DAV be effectively adapted for non-spatial machine learning problems where dissimilarity between training and test distributions is a concern?
- Basis in paper: [explicit] The authors state that "DAV has great potential in broader geoscience applications" and mention potential applications in "classification and semantic segmentation of remote sensing images," but do not explore non-spatial domains.
- Why unresolved: The paper focuses exclusively on geospatial applications and does not test DAV's performance on general machine learning problems where dataset shift is problematic.
- What evidence would resolve it: Application of DAV to non-spatial ML benchmarks (e.g., image classification datasets with domain adaptation challenges, tabular data with covariate shift) to evaluate its effectiveness outside geospatial contexts.

## Limitations

- Limited real-world validation with only one real dataset (Amazon AGB) despite examining multiple CV methods
- The assumption that feature space differences directly translate to prediction performance differences may not hold for all geospatial applications where spatial autocorrelation patterns dominate
- The relationship between dissimilarity and CV method performance may be context-dependent, varying across different geographic regions, covariate structures, and prediction targets

## Confidence

- DAV effectively quantifies dissimilarity across 0-100% scale: High - Supported by systematic synthetic dataset experiments and consistent results across different clustering levels
- CV method performance transitions from RDM-CV to geospatial CV as dissimilarity increases: Medium - Pattern observed but based on limited dataset variety and specific implementation details
- Random sampling of prediction locations provides unbiased representation: Medium - Mechanistically sound but not empirically validated across diverse spatial distributions

## Next Checks

1. Apply DAV and CV method comparison to at least 3 additional real geospatial datasets (e.g., soil properties, land cover, temperature) with varying spatial autocorrelation structures to verify the observed performance patterns

2. Test DAV's random sampling assumption by creating prediction location distributions with known clustering patterns and measuring how this affects dissimilarity quantification accuracy

3. Systematically vary random forest hyperparameters (ntree, mtry, depth) and different classifier algorithms to assess the stability of DAV dissimilarity scores across different model choices