---
ver: rpa2
title: 'BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger Visual
  Cues'
arxiv_id: '2407.20341'
source_url: https://arxiv.org/abs/2407.20341
tags:
- image
- captions
- bridge
- visual
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BRIDGE, a learnable and reference-free image
  captioning evaluation metric that addresses the limitations of existing metrics
  by incorporating fine-grained visual features. The key innovation is a mapping module
  that generates multimodal pseudo-captions by enriching template captions with dense
  visual features, enabling a more nuanced alignment between images and candidate
  captions.
---

# BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger Visual Cues

## Quick Facts
- **arXiv ID**: 2407.20341
- **Source URL**: https://arxiv.org/abs/2407.20341
- **Reference count**: 40
- **One-line primary result**: BRIDGE improves image captioning evaluation by enriching template captions with fine-grained visual features, achieving higher correlation with human judgments and better sensitivity to object hallucinations than existing reference-free metrics.

## Executive Summary
BRIDGE introduces a learnable, reference-free metric for image captioning evaluation that addresses the limitations of existing metrics by incorporating fine-grained visual features. The key innovation is a mapping module that generates multimodal pseudo-captions by enriching template captions with dense visual features, enabling a more nuanced alignment between images and candidate captions. BRIDGE outperforms existing reference-free metrics (CLIP-S, PAC-S) on multiple datasets, achieving higher correlation with human judgments (e.g., +3.6 Kendall τc on Flickr8k-Expert) and better sensitivity to object hallucinations. The method demonstrates robustness across different template qualities and backbone models, offering a significant improvement in evaluating image-text alignment.

## Method Summary
BRIDGE employs a dual-encoder architecture with a CLIP backbone to evaluate image captions by aligning candidate captions with visual content through multimodal pseudo-captions. The method generates template captions with masked noun chunks, then uses a mapping module (stack of Transformer encoder layers with cross-attention) to inject fine-grained grid-level visual features into the masked positions. A weighted contrastive loss aligns pseudo-captions with both the image and corresponding ground-truth captions, while a regularization branch promotes alignment with noun chunk prompts. The final BRIDGE score combines CLIP-Score with pseudo-caption similarity, enabling reference-free evaluation that captures fine-grained visual-textual alignment.

## Key Results
- BRIDGE achieves state-of-the-art correlation with human judgments, improving Kendall τc by +3.6 on Flickr8k-Expert compared to CLIP-Score
- The method demonstrates superior sensitivity to object hallucinations, achieving 83.5% accuracy on FOIL compared to 72.5% for CLIP-Score
- BRIDGE shows consistent performance across different template caption qualities (Transformer, BLIP, BLIP-2) and backbone models (ViT-B/32, ViT-L/14)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BRIDGE improves caption evaluation by injecting fine-grained visual features into template captions.
- Mechanism: The mapping module takes a templated caption with masked noun chunks and replaces each [MASK] with dense visual features extracted from the image's grid features, producing a multimodal pseudo-caption that combines text and visual embeddings.
- Core assumption: Template captions provide a structural scaffold that the mapping network can enrich with relevant visual details, improving fine-grained alignment over global image embeddings alone.
- Evidence anchors:
  - [abstract] "employs a novel module to map visual features into dense vectors and integrates them into multi-modal pseudo-captions"
  - [section 3.2] "The mapping network is implemented as a stack of Transformer encoder layers interleaved with cross-attention layers...to refine each template caption with visual information"
  - [corpus] No direct mention of grid-level visual features in related work, but CLIP-Score is cited as baseline using global features only
- Break condition: If the mapping network fails to align visual features with the correct masked noun chunks, or if the template caption does not accurately represent the image structure, the pseudo-caption will not meaningfully improve evaluation.

### Mechanism 2
- Claim: Weighted contrastive loss ensures pseudo-captions are aligned both with images and with corresponding ground-truth captions.
- Mechanism: The loss combines image-to-pseudo-caption similarity (L1), pseudo-caption-to-ground-truth similarity (L2), and regularization promoting pseudo-caption-to-noun-chunk alignment (L3), with weights tuned to balance these objectives.
- Core assumption: Aligning pseudo-captions with both the image and its ground-truth caption encourages the model to capture both visual and textual coherence.
- Evidence anchors:
  - [section 3.3] "The first loss, denoted as L1, tries to align the pseudo-captions...with the global visual features...In addition to this loss term, we define a second loss component L2 that promotes the alignment between pseudo-captions and the textual feature vector of the ground-truth caption"
  - [supplementary A] Full formulation of L1 and L2 with weighting by number of noun chunks
  - [corpus] Contrastive learning is standard in multimodal learning but the specific weighted formulation is not cited in neighbors
- Break condition: If the weighting or loss balance is off, the pseudo-captions may overfit to either visual or textual cues, harming overall correlation.

### Mechanism 3
- Claim: BRIDGE's reference-free design makes it robust to missing or imperfect reference captions.
- Mechanism: By internally constructing pseudo-captions from the image itself, BRIDGE does not rely on external reference captions for evaluation, avoiding the bias and coverage issues inherent in reference-based metrics.
- Core assumption: The mapping network can generate pseudo-captions that sufficiently represent the image content to serve as a proxy for reference captions.
- Evidence anchors:
  - [abstract] "properly incorporates information from the input image without relying on reference captions"
  - [section 1] "obtaining these reference captions can often be challenging and expensive"
  - [section 4.2] Evaluation on datasets without requiring reference captions (e.g., Pascal-50S pairwise comparisons)
  - [corpus] No neighbor explicitly discusses reference-free captioning evaluation; this is a distinguishing feature
- Break condition: If pseudo-captions fail to capture key aspects of the image, the metric will miss important caption quality signals.

## Foundational Learning

- Concept: Visual-textual embedding alignment via contrastive learning
  - Why needed here: BRIDGE's core mechanism relies on projecting image and caption features into a shared space where cosine similarity reflects semantic alignment
  - Quick check question: What is the role of the temperature parameter τ in the InfoNCE loss, and how does it affect the sharpness of the similarity distribution?

- Concept: Transformer-based cross-attention for multimodal fusion
  - Why needed here: The mapping network uses cross-attention layers to fuse grid-level visual features into the template caption sequence, enabling fine-grained visual enrichment
  - Quick check question: How does the cross-attention mechanism differ from self-attention in terms of information flow between modalities?

- Concept: Template-based caption generation with masked noun chunks
  - Why needed here: Templates provide a syntactic skeleton that guides the mapping network in placing visual features appropriately within the caption structure
  - Quick check question: Why does BRIDGE mask noun chunks rather than individual words, and how does this choice affect the granularity of visual injection?

## Architecture Onboarding

- Component map:
  - Input: Image I, candidate caption T
  - Visual encoder EV: Extracts grid features bv from image
  - Text encoder ET: Encodes template captions and pseudo-captions
  - Mapping module ψ: Stack of Transformer layers + cross-attention, takes template captions and grid features, outputs enriched tokens for [MASK] positions
  - Regularization branch C(x): MLP projection of pseudo-captions aligned with noun chunk prompts
  - Loss functions L1, L2, L3: Weighted contrastive losses for multimodal alignment
  - Output: BRIDGE score combining CLIP-Score and pseudo-caption similarity

- Critical path:
  1. Generate template caption with [MASK] for noun chunks
  2. Expand image features bv, replicate template captions per noun chunk
  3. Pass through mapping module ψ to get enriched tokens
  4. Replace [MASK] with enriched tokens → pseudo-captions
  5. Encode pseudo-captions with ET
  6. Compute similarity between pseudo-captions and candidate caption
  7. Combine with CLIP-Score for final BRIDGE score

- Design tradeoffs:
  - Using grid features vs global features: grid provides fine-grained detail but increases computational cost
  - Number of noun chunks masked: more chunks → more visual detail but higher complexity and potential overfitting
  - Backbone choice (CLIP ViT-B/32 vs ViT-L/14): larger backbone improves correlation but increases inference time

- Failure signatures:
  - Low correlation scores on datasets with clear visual-textual alignment but high noun chunk variance
  - Sensitivity to noun chunk extraction errors (e.g., missing adjectives or compound nouns)
  - Over-reliance on CLIP-Score if pseudo-caption similarity is not properly weighted

- First 3 experiments:
  1. Ablation: Remove mapping module, use template captions directly → expect large drop in correlation
  2. Ablation: Use global visual features instead of grid features → expect reduced fine-grained sensitivity
  3. Ablation: Remove regularization branch L3 → expect slightly lower correlation, especially on datasets with object hallucination detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of template caption quality affect the long-term robustness of the BRIDGE metric across different captioning datasets and styles?
- Basis in paper: [explicit] The paper analyzes the impact of different template caption sources (Transformer, BLIP, BLIP-2) and finds that BRIDGE performs well even with lower-quality captions, but does not explore long-term robustness across diverse datasets or evolving captioning styles.
- Why unresolved: The study focuses on a limited set of datasets and does not investigate how BRIDGE adapts to future datasets with different styles or distributions of captions.
- What evidence would resolve it: Conducting extensive experiments on a wide range of captioning datasets, including those with diverse styles and distributions, to assess the long-term robustness of BRIDGE.

### Open Question 2
- Question: What is the impact of using different visual feature extraction methods (e.g., grid features vs. global features) on the performance of BRIDGE in capturing fine-grained visual details?
- Basis in paper: [explicit] The paper compares the use of grid-level features and global features in BRIDGE, finding that grid features lead to better performance in capturing fine-grained details.
- Why unresolved: The study does not explore the impact of other visual feature extraction methods, such as object detection or segmentation, on BRIDGE's performance.
- What evidence would resolve it: Conducting experiments using different visual feature extraction methods and comparing their impact on BRIDGE's performance in capturing fine-grained visual details.

### Open Question 3
- Question: How does the number of pseudo-tokens (US) affect the performance of BRIDGE in different evaluation scenarios, such as object hallucination detection or caption ranking?
- Basis in paper: [explicit] The paper investigates the effect of varying the number of pseudo-tokens (US) on BRIDGE's performance and finds that US=3 generally leads to the best results.
- Why unresolved: The study does not explore the impact of US on specific evaluation scenarios, such as object hallucination detection or caption ranking, where different numbers of pseudo-tokens might be more effective.
- What evidence would resolve it: Conducting experiments using different values of US in specific evaluation scenarios to determine the optimal number of pseudo-tokens for each scenario.

## Limitations

- The method's performance depends on the quality of template captions generated by external captioners, introducing a potential propagation of errors if the base captioner performs poorly.
- BRIDGE's generalization to images with complex or ambiguous visual content remains untested, limiting confidence in its robustness across diverse image types.
- The specific weight choices for the contrastive loss components and temperature parameter are not thoroughly explored, potentially affecting optimal performance.

## Confidence

- **High Confidence**: The core mechanism of enriching template captions with grid-level visual features is well-supported by the experimental results, particularly the improved correlation scores across multiple datasets.
- **Medium Confidence**: The effectiveness of the weighted contrastive loss formulation is supported by ablation studies, but the specific weight choices (λ1=0.01, λ2=1.0, λ3=0.01) and temperature τ are not thoroughly explored.
- **Medium Confidence**: The claim that BRIDGE is more sensitive to object hallucinations is demonstrated on Pascal-50S and FOIL, but the mechanism by which pseudo-captions specifically capture these errors could be more rigorously explained.

## Next Checks

1. **Ablation on Grid Feature Resolution**: Test BRIDGE with different grid resolutions (e.g., 7x7 vs 14x14) to quantify the impact of visual granularity on correlation performance.
2. **Cross-Dataset Robustness**: Evaluate BRIDGE on datasets with varying image complexity (e.g., Conceptual Captions, VizWiz) to assess generalization beyond standard benchmarks.
3. **Human Evaluation of Pseudo-Captions**: Conduct a human study to rate the quality and relevance of generated pseudo-captions, ensuring they serve as valid proxies for reference captions.