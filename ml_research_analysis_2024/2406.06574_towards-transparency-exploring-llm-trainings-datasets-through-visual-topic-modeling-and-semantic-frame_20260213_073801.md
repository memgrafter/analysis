---
ver: rpa2
title: 'Towards Transparency: Exploring LLM Trainings Datasets through Visual Topic
  Modeling and Semantic Frame'
arxiv_id: '2406.06574'
source_url: https://arxiv.org/abs/2406.06574
tags:
- arxiv
- http
- data
- topic
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Bunka, a tool that leverages topic modeling
  and semantic frame analysis to improve transparency and quality of LLM training
  datasets. The authors demonstrate three use cases: (1) visual topic cartography
  to summarize fine-tuning prompts, (2) topic-based filtering to accelerate Direct
  Preference Optimization (DPO), and (3) semantic framing to detect dataset biases.'
---

# Towards Transparency: Exploring LLM Trainings Datasets through Visual Topic Modeling and Semantic Frame

## Quick Facts
- arXiv ID: 2406.06574
- Source URL: https://arxiv.org/abs/2406.06574
- Reference count: 32
- Primary result: Introduces Bunka tool using topic modeling and semantic frames to improve LLM dataset transparency

## Executive Summary
This paper introduces Bunka, a tool that leverages topic modeling and semantic frame analysis to improve transparency and quality of LLM training datasets. The authors demonstrate three use cases: visual topic cartography to summarize fine-tuning prompts, topic-based filtering to accelerate Direct Preference Optimization (DPO), and semantic framing to detect dataset biases. The tool aims to address the lack of transparency in LLM training data by providing efficient, visual exploration methods.

## Method Summary
The method combines topic modeling with 2D cartography visualization using UMAP/TSNE dimension reduction and clustering algorithms. It processes text data through embedding models, reduces dimensions for spatial visualization, identifies topics through clustering, and applies semantic framing for bias detection. The approach is demonstrated through three use cases: summarizing fine-tuning prompts, filtering DPO datasets to retain GPT-4-specific responses, and analyzing dataset biases using conceptual frame pairs.

## Key Results
- Visual topic cartography successfully summarizes fine-tuning prompts with coherent topic clusters
- Topic-based filtering reduces ChatML DPO Pairs dataset to 1/6 size while improving benchmark performance
- Semantic framing analysis reveals dataset biases toward future, work, and male-related content
- Topic Neural Hermes model outperforms base and fully fine-tuned models on most benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Topic modeling combined with 2D cartography improves dataset transparency by leveraging human spatial reasoning capabilities
- Mechanism: Transforms textual data into embeddings, reduces dimensions to 2D using UMAP/TSNE, clusters documents, and visualizes them on a map where spatial proximity indicates semantic similarity
- Core assumption: Humans process spatial relationships and visual information more efficiently than abstract topic distributions
- Evidence anchors:
  - [abstract] "We show how Topic Modeling coupled with 2-dimensional Cartography can increase the transparency of datasets"
  - [section] "Cognitive science shows that 2D maps and diagrams are the easiest way to represent the multiple dimensions of an information (distribution of topics, relationships between documents etc.) in a cognitively tractable way"
  - [corpus] Weak evidence - the corpus only shows related topic modeling papers without specific spatial reasoning studies
- Break condition: If users cannot interpret spatial relationships or if high-dimensional semantic relationships cannot be meaningfully projected to 2D space

### Mechanism 2
- Claim: Filtering DPO datasets based on GPT-4-specific topics accelerates fine-tuning while maintaining or improving model performance
- Mechanism: Identifies topics unique to GPT-4 responses, filters prompts to retain only those leading to GPT-4-specific answers, and uses this reduced dataset for DPO fine-tuning
- Core assumption: GPT-4-specific responses contain unique patterns that can be learned more efficiently when other redundant patterns are removed
- Evidence anchors:
  - [abstract] "For the DPO case, Bunka filters the ChatML DPO Pairs dataset by retaining only prompts leading to GPT-4-specific responses, reducing the dataset to 1/6 of its original size"
  - [section] "We hypothesize that focusing the DPO process only on prompts that lead to GPT-4 specific responses, we could accelerate the DPO fine-tuning process"
  - [corpus] Weak evidence - corpus contains topic modeling papers but no specific DPO efficiency studies
- Break condition: If GPT-4-specific topics are not actually unique or if the filtered dataset loses critical information needed for comprehensive learning

### Mechanism 3
- Claim: Semantic frame analysis using embeddings reveals dataset biases by mapping documents onto conceptual continuums
- Mechanism: Embeds pairs of contrasting terms, calculates document positions on these frames using cosine similarity, and identifies biases based on document distribution
- Core assumption: Embedding spaces capture meaningful semantic relationships that can be used to detect conceptual biases in datasets
- Evidence anchors:
  - [abstract] "Semantic framing analysis reveals dataset biases toward future, work, and male-related content"
  - [section] "We use the concept of frames and apply them to LLMs training datasets to visually understand the relationships between a content and dedicated frames"
  - [corpus] Weak evidence - corpus shows related framing analysis papers but no specific bias detection validation
- Break condition: If embedding spaces do not preserve semantic relationships accurately or if the chosen frame pairs do not capture meaningful biases

## Foundational Learning

- Concept: Topic modeling fundamentals
  - Why needed here: The system relies on identifying latent themes in text data through techniques like LDA, NMF, and embedding-based approaches
  - Quick check question: What is the difference between topic modeling and document classification, and why is topic modeling more suitable for exploratory dataset analysis?

- Concept: Dimensionality reduction techniques
  - Why needed here: The system uses UMAP and TSNE to project high-dimensional embeddings into 2D space for visualization
  - Quick check question: How do UMAP and TSNE differ in their approach to preserving local vs. global structure in high-dimensional data?

- Concept: Embedding model selection and evaluation
  - Why needed here: The system uses different embedding models and must choose appropriate models for specific tasks
  - Quick check question: What factors should be considered when selecting an embedding model for topic modeling vs. semantic framing tasks?

## Architecture Onboarding

- Component map: Input processing → Embedding layer → Dimension reduction → Clustering → Visualization → Semantic framing
- Critical path: Text → Embedding → Dimension Reduction → Clustering → Visualization → User Interaction
- Design tradeoffs:
  - Embedding model choice: Tradeoff between embedding quality (measured by MTEB benchmarks) and computational cost
  - Number of clusters: Balancing granularity of analysis against visual clutter and interpretability
  - Chunk size: Smaller chunks may capture more specific topics but lose broader context
  - Frame selection: Arbitrary frame choices may not capture all relevant biases
- Failure signatures:
  - Topic coherence issues: Topics contain semantically unrelated terms or are too broad/narrow
  - Visualization problems: Clusters overlap excessively or are too dispersed to be meaningful
  - Bias detection failures: Frame analysis shows no clear bias patterns or produces contradictory results
- First 3 experiments:
  1. Test different embedding models on the prompt-collective dataset and compare ARI scores to validate embedding quality
  2. Vary the number of clusters (k=10, 15, 20) on the same dataset to find optimal granularity
  3. Apply semantic framing with different frame pairs (future/past, work/leisure, etc.) to validate bias detection capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different embedding models affect the stability and interpretability of topic modeling results in BunkaTopics?
- Basis in paper: [explicit] The paper compares four embedding models (mxbai-embed-large-v1, all-MiniLM-L6-v2, bge-large-en-v1.5, UAE-Large-V1) and shows their Adjusted Rand Index (ARI) values, with bge-large-en-v1.5 and UAE-Large-V1 showing the highest similarity (ARI=0.42).
- Why unresolved: The paper only provides a limited comparison of ARI values. It doesn't explore the impact of embedding model choice on downstream tasks like dataset filtering or bias detection, nor does it investigate whether certain models are better suited for specific types of datasets or domains.
- What evidence would resolve it: Systematic experiments comparing BunkaTopics outputs across multiple embedding models on diverse datasets, measuring not just clustering similarity but also task-specific performance (e.g., how well filtered datasets improve model fine-tuning, or how accurately semantic framing identifies biases).

### Open Question 2
- Question: What is the optimal number of topics for BunkaTopics to balance interpretability and granularity across different dataset sizes and domains?
- Basis in paper: [inferred] The paper mentions that the number of topics was set to 15 for the prompt-collective dataset and 30 for the DPO datasets, but acknowledges that more research is needed to determine the right number of topics. It also notes that users can iterate over different numbers of topics until the quality seems good enough.
- Why unresolved: There is no established methodology for determining the optimal number of topics. The paper suggests comparing topics with LLM responses or human annotator ratings, but doesn't provide concrete guidelines or benchmarks for this process.
- What evidence would resolve it: Development of a standardized evaluation framework that correlates topic granularity with specific use cases (e.g., dataset summarization vs. bias detection), potentially incorporating human-in-the-loop methods to validate topic quality and relevance.

### Open Question 3
- Question: How does the choice of semantic frame pairs affect the accuracy and sensitivity of bias detection in Bunka?
- Basis in paper: [explicit] The paper uses two specific frame pairs (future/past and work/leisure) and shows their effectiveness in identifying dataset biases. It also mentions that the authors arbitrarily chose these frames.
- Why unresolved: The paper doesn't explore how different frame pairs might reveal different types of biases or whether certain frame pairs are more effective for specific domains. It also doesn't address how to systematically select or validate semantic frames.
- What evidence would resolve it: Comparative studies using multiple frame pairs on the same datasets to identify which frames best capture known biases, and experiments testing frame selection strategies (e.g., using domain-specific knowledge or automated frame discovery methods).

## Limitations
- Limited validation of visual cartography benefits without user studies
- Single dataset evaluation for DPO filtering method
- Arbitrary selection of semantic frame pairs without systematic validation
- Weak connections to existing literature with no citations in nearest neighbor papers

## Confidence
- **Medium confidence** in visual topic cartography mechanism
- **Medium confidence** in DPO filtering approach
- **Low confidence** in semantic framing for bias detection

## Next Checks
1. Conduct a user study comparing comprehension and analysis efficiency between traditional topic lists and the visual cartography approach to validate the claimed cognitive benefits
2. Test the DPO filtering method across multiple datasets and model architectures to determine if the 1/6 reduction ratio generalizes or if it's dataset-specific
3. Develop a systematic framework for selecting frame pairs in semantic analysis and validate that detected biases correspond to known issues in the source datasets or manifest in model behavior