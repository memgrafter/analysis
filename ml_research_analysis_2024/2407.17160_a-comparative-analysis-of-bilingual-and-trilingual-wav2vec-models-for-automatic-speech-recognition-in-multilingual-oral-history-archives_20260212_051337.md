---
ver: rpa2
title: A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic
  Speech Recognition in Multilingual Oral History Archives
arxiv_id: '2407.17160'
source_url: https://arxiv.org/abs/2407.17160
tags:
- speech
- hours
- czech
- languages
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares monolingual, bilingual, and trilingual Wav2Vec
  2.0 models for automatic speech recognition (ASR) on multilingual oral history archives,
  particularly the MALACH dataset. The authors evaluate these models on both the MALACH
  dataset and the CommonVoice dataset across English, German, and Czech.
---

# A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives

## Quick Facts
- arXiv ID: 2407.17160
- Source URL: https://arxiv.org/abs/2407.17160
- Authors: Jan Lehečka; Josef V. Psutka; Luboš Šmídl; Pavel Ircing; Josef Psutka
- Reference count: 0
- Primary result: Monolingual Wav2Vec models outperform bilingual and trilingual variants on multilingual oral history archives

## Executive Summary
This study compares monolingual, bilingual, and trilingual Wav2Vec 2.0 models for automatic speech recognition on multilingual oral history archives, specifically the MALACH dataset containing English, German, and Czech speech. The authors find that monolingual models consistently outperform multilingual models, even when processing mixed-language sentences from non-native speakers. While large-scale multilingual models like Whisper show better performance, they require significantly higher computational resources. The results suggest that adding more languages to pre-training does not improve performance and may actually degrade it.

## Method Summary
The researchers fine-tuned monolingual, bilingual, and trilingual Wav2Vec 2.0 models on the MALACH oral history archive and CommonVoice datasets. They evaluated model performance using Word Error Rate (WER) across different language combinations and tested both in-domain (MALACH) and out-of-domain (CommonVoice) scenarios. The study compared base Wav2Vec models with different language configurations and also benchmarked against large-scale multilingual models like Whisper. All models used the same fine-tuning procedure with CTC decoding, allowing for direct performance comparison across different pre-training strategies.

## Key Results
- Monolingual Wav2Vec models consistently achieved lower WER than bilingual and trilingual models on both MALACH and CommonVoice datasets
- Adding more languages to pre-training increased WER rather than decreasing it, suggesting representational interference
- Large-scale multilingual models like Whisper outperformed monolingual models but required significantly higher computational resources
- The monolingual advantage persisted even for mixed-language sentences containing German phrases within English and Czech speech

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monolingual Wav2Vec models outperform multilingual models even on mixed-language speech data
- Mechanism: Specialized language models trained on clean, monolingual data develop more robust acoustic and linguistic representations for their target language, while multilingual models suffer from interference between languages during pre-training
- Core assumption: Mixed-language speech contains sufficient monolingual segments for specialized models to leverage their superior representations
- Evidence anchors:
  - [abstract] "monolingual speech recognition models are, in most cases, superior to multilingual models, even when processing the oral history archive full of mixed-language sentences from non-native speakers"
  - [section] "Our results suggest that adding more languages into the pre-training phase while keeping the model at the same size did not bring any improvement for either dataset"
  - [corpus] Weak evidence - no direct corpus evidence for interference mechanism, though results support the claim
- Break condition: If the mixed-language segments become predominantly code-switched at phrase or word level, where no single language dominates long enough for monolingual models to establish context

### Mechanism 2
- Claim: Adding more languages to pre-training increases WER rather than decreasing it
- Mechanism: Parameter sharing in fixed-capacity models leads to representational interference when multiple languages compete for the same embedding space, degrading performance on all languages
- Core assumption: Wav2Vec-base model has insufficient capacity to maintain distinct, high-quality representations for multiple languages simultaneously
- Evidence anchors:
  - [section] "We plotted this interesting trend in Fig. 2, where for each dataset and each language, we compared the monolingual model (row 1 in Tab. 2), the average WER scored by bilingual models (rows 2, 4, and 6) and the trilingual model fine-tuned on a single language (row 8)"
  - [section] "We observed a trend in WER increasing when adding more languages"
  - [corpus] Weak evidence - no direct corpus evidence for representational interference, but results pattern supports this mechanism
- Break condition: If model capacity is increased proportionally with language count, or if languages share significant phonological/phonetic features that enable beneficial parameter sharing

### Mechanism 3
- Claim: Large-scale multilingual models (Whisper) can outperform monolingual models but at significantly higher computational cost
- Mechanism: Increased model capacity and training data volume in large-scale models overcomes representational interference, allowing them to learn language-specific features despite multilingual training
- Core assumption: Computational cost scales super-linearly with model size, making large-scale models impractical for many deployment scenarios
- Evidence anchors:
  - [abstract] "Large-scale multilingual models like Whisper show better performance but at significantly higher computational costs"
  - [section] "Only large-scale multilingual models, many times larger, can outperform monolingual Wav2Vec models, but only at the cost of much higher decoding complexity"
  - [corpus] Moderate evidence - comparison includes parameter counts (Whisper-large has 1,550M params vs Wav2Vec-base at 95M), supporting the computational cost claim
- Break condition: If computational constraints become the primary bottleneck, or if inference speed requirements cannot tolerate the increased latency of larger models

## Foundational Learning

- Concept: Self-supervised pre-training in speech models
  - Why needed here: Understanding how Wav2Vec models learn speech representations without labels is crucial for interpreting why multilingual pre-training may be harmful
  - Quick check question: What is the primary objective function used during Wav2Vec 2.0 pre-training?

- Concept: Connectionist Temporal Classification (CTC) for ASR
  - Why needed here: The final CTC layer enables sequence-to-sequence mapping from acoustic features to text, and understanding its limitations helps explain why multilingual models struggle
  - Quick check question: How does CTC handle the alignment problem between variable-length audio and text sequences?

- Concept: Code-switching and mixed-language speech patterns
  - Why needed here: The MALACH dataset contains German phrases within English and Czech speech, requiring understanding of how ASR models handle language boundaries
  - Quick check question: What linguistic features typically signal a language switch in continuous speech?

## Architecture Onboarding

- Component map: Raw waveform → Feature encoder → Context network → Quantized targets (pre-training) → Linear projection + CTC (fine-tuning) → Text output
- Critical path: Raw waveform → Feature encoder → Context network → Quantized targets (pre-training) → Linear projection + CTC (fine-tuning) → Text output
- Design tradeoffs: Fixed model capacity vs. multilingual coverage; pre-training data diversity vs. specialization; computational cost vs. performance
- Failure signatures: Increasing WER with additional languages in pre-training suggests representational interference; hallucination in Whisper models after fine-tuning indicates overfitting or confidence calibration issues
- First 3 experiments:
  1. Replicate monolingual vs. bilingual comparison on a balanced subset of CommonVoice to verify the core finding
  2. Test intermediate model sizes (e.g., 200M parameters) to find the capacity threshold where multilingual benefits emerge
  3. Evaluate on artificially mixed-language data with controlled switch points to isolate the effect of language boundaries on monolingual vs. multilingual performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of monolingual models over multilingual models extend to other language families beyond Indo-European languages?
- Basis in paper: [explicit] The paper compares monolingual, bilingual, and trilingual Wav2Vec models for English, German, and Czech, all Indo-European languages, but does not explore languages from other families.
- Why unresolved: The study is limited to three Indo-European languages, leaving open the question of whether the observed trend holds for non-Indo-European languages.
- What evidence would resolve it: Conducting similar experiments with languages from different families, such as Chinese, Arabic, or Swahili, and comparing their performance to multilingual models.

### Open Question 2
- Question: How do the computational costs of large-scale multilingual models compare to the potential improvements in accuracy when processing multilingual datasets?
- Basis in paper: [explicit] The paper notes that large-scale multilingual models like Whisper can outperform monolingual models but at significantly higher computational costs.
- Why unresolved: The study does not quantify the trade-off between computational cost and accuracy improvement, nor does it explore if the increased cost is justified by the performance gains.
- What evidence would resolve it: Detailed analysis of the computational resources required for large-scale models versus the accuracy improvements achieved, possibly including a cost-benefit analysis.

### Open Question 3
- Question: What is the impact of using language models in the CTC decoder on the performance of monolingual and multilingual ASR models?
- Basis in paper: [explicit] The paper mentions that significantly better results could be scored with monolingual models when using more training data and a language model in the CTC decoder.
- Why unresolved: The study does not explore the specific impact of incorporating language models in the CTC decoder on the performance of both monolingual and multilingual models.
- What evidence would resolve it: Experiments comparing the performance of ASR models with and without language models in the CTC decoder, across different languages and datasets.

## Limitations

- Experimental scope limited to three Indo-European languages within a specific oral history domain
- Does not investigate intermediate model sizes or capacity scaling effects
- Does not explore alternative multilingual training strategies that might mitigate interference effects
- Comparison with Whisper lacks detailed analysis of error types and confidence calibration

## Confidence

**High Confidence**: The core finding that monolingual models outperform bilingual and trilingual models on the tested datasets is well-supported by the experimental results.

**Medium Confidence**: The mechanistic explanation for why multilingual models underperform—specifically, representational interference due to parameter sharing in fixed-capacity models—is supported by the results but not directly measured.

**Low Confidence**: The generalization of these findings to other language families, different speech domains, or alternative multilingual training approaches remains speculative.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate the same monolingual vs. multilingual comparison on spontaneous conversational speech datasets like Fisher or Switchboard to determine if the monolingual advantage persists outside the oral history domain.

2. **Capacity Scaling Experiment**: Systematically vary model capacity (e.g., testing 100M, 200M, 400M parameter models) while maintaining multilingual training to identify the minimum size required for multilingual benefits to emerge.

3. **Alternative Multilingual Training Strategies**: Implement and evaluate language-specific adapters or mixture-of-experts approaches on the same datasets to determine if the monolingual advantage can be achieved through architectural modifications rather than pure monolingual training.