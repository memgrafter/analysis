---
ver: rpa2
title: 'WayveScenes101: A Dataset and Benchmark for Novel View Synthesis in Autonomous
  Driving'
arxiv_id: '2407.08280'
source_url: https://arxiv.org/abs/2407.08280
tags:
- scenes
- scene
- dataset
- driving
- camera
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WayveScenes101, a dataset for novel view
  synthesis in autonomous driving. It consists of 101 driving scenes with 101,000
  images from five synchronized cameras, capturing challenging real-world conditions
  like dynamic objects, weather variations, and occlusions.
---

# WayveScenes101: A Dataset and Benchmark for Novel View Synthesis in Autonomous Driving

## Quick Facts
- arXiv ID: 2407.08280
- Source URL: https://arxiv.org/abs/2407.08280
- Authors: Jannik Zürn; Paul Gladkov; Sofía Dudas; Fergal Cotter; Sofi Toteva; Jamie Shotton; Vasiliki Simaiaki; Nikhil Mohan
- Reference count: 32
- Key outcome: Introduces WayveScenes101, a dataset for novel view synthesis in autonomous driving with 101 driving scenes, 101,000 images from five synchronized cameras, and COLMAP-derived camera poses.

## Executive Summary
WayveScenes101 is a novel dataset designed to advance scene reconstruction methods for autonomous driving. It consists of 101 driving scenes captured from five synchronized cameras, providing 101,000 images across diverse real-world conditions. The dataset includes COLMAP-derived camera poses and detailed metadata, enabling targeted evaluation of novel view synthesis models. An evaluation protocol using a held-out front-forward camera tests model generalization capabilities, addressing limitations of prior datasets that focused on static or object-centric scenes.

## Method Summary
The WayveScenes101 dataset captures driving scenes using a five-camera rig (four wide-angle lateral cameras and one front-facing camera) with significant baseline distances (0.9-1.2m). Each scene is 20 seconds long at 10 fps, yielding 2,000 images per scene. COLMAP is used to estimate camera poses, which are provided alongside the raw images and scene metadata including weather, time of day, and traffic conditions. The evaluation protocol involves training models on four cameras while holding out the front-forward camera, testing the model's ability to generalize to novel viewpoints with complete frustum overlap but significant baseline distance.

## Key Results
- Dataset provides 101,000 images across 101 driving scenes with diverse conditions including dynamic objects, weather variations, and occlusions
- COLMAP-derived camera poses offer accurate geometric constraints for novel view synthesis in autonomous driving scenarios
- Five-camera rig with significant baseline enables robust testing of generalization capabilities through held-out front-forward camera evaluation
- Detailed metadata allows for targeted performance evaluation across specific driving conditions and environmental factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: COLMAP-derived camera poses provide accurate geometric constraints that improve novel view synthesis quality for autonomous driving.
- Mechanism: The dataset includes COLMAP-derived camera poses in standard formats, which supply accurate 3D geometric information that models can use to reason about scene geometry. This geometric prior is particularly important for autonomous driving scenes with complex occlusions and dynamic objects.
- Core assumption: COLMAP pose estimation is accurate enough for novel view synthesis in dynamic driving scenarios.
- Evidence anchors:
  - [section] "Along with the raw images, we include COLMAP-derived camera poses in standard data formats."
  - [corpus] Weak - no direct evidence about COLMAP accuracy in corpus.
- Break condition: If camera poses are inaccurate or not properly synchronized with images, model performance will degrade significantly.

### Mechanism 2
- Claim: The five-camera rig with a held-out evaluation camera enables robust testing of generalization capabilities.
- Mechanism: The dataset uses five synchronized cameras (four wide-angle and one front-facing) with a significant baseline (0.9-1.2m). The front-forward camera is held out during training, forcing models to learn generalizable scene representations rather than overfitting to specific viewpoints.
- Core assumption: The front-forward camera provides sufficient geometric diversity from training views while maintaining frustum overlap.
- Evidence anchors:
  - [section] "We propose an evaluation protocol for evaluating models on held-out camera views that are off-axis from the training views, specifically testing the generalisation capabilities of methods."
  - [section] "We propose to use the front-forward camera for model evaluation as it has a complete frustum overlap with the lateral forward-facing cameras but maintains a significant baseline distance from them."
- Break condition: If the held-out camera is too similar to training views or too dissimilar, the generalization test becomes meaningless.

### Mechanism 3
- Claim: Diverse scene metadata enables targeted evaluation and model improvement for specific driving conditions.
- Mechanism: The dataset includes detailed metadata (weather, time of day, traffic conditions, etc.) that allows researchers to evaluate model performance across specific scenarios and identify weaknesses in particular conditions.
- Core assumption: Scene metadata is accurate and representative of the actual scene characteristics.
- Evidence anchors:
  - [section] "Finally, we provide detailed metadata for all scenes, including weather, time of day, and traffic conditions, to allow for a detailed model performance breakdown across scene characteristics."
  - [appendix] Scene metadata distribution shows comprehensive coverage across multiple conditions.
- Break condition: If metadata is inaccurate or incomplete, targeted evaluation becomes unreliable.

## Foundational Learning

- Concept: 3D reconstruction and camera geometry
  - Why needed here: Understanding how camera poses and geometric constraints affect novel view synthesis is fundamental to using this dataset effectively.
  - Quick check question: What geometric information does COLMAP provide, and how does it differ from using camera intrinsics alone?

- Concept: Novel view synthesis evaluation metrics
  - Why needed here: The dataset proposes specific metrics (PSNR, SSIM, LPIPS, FID) and evaluation protocols that researchers must understand to properly benchmark models.
  - Quick check question: How does the FID metric differ from PSNR/SSIM when evaluating novel view synthesis models?

- Concept: Autonomous driving scene characteristics
  - Why needed here: The dataset focuses on challenging driving scenarios with dynamic objects and occlusions that require specialized approaches compared to object-centric or indoor scenes.
  - Quick check question: What unique challenges do dynamic pedestrians and vehicles present for novel view synthesis compared to static scenes?

## Architecture Onboarding

- Component map: Dataset → Images + Camera Poses + Metadata → Training pipeline (4 cameras) → Evaluation pipeline (5th camera) → Metrics calculation
- Critical path: Raw data collection → COLMAP pose estimation → Data formatting → Training pipeline → Held-out evaluation → Metric computation
- Design tradeoffs: Five cameras provide comprehensive coverage but increase data complexity; held-out camera enables generalization testing but reduces training data; COLMAP poses add accuracy but require additional processing.
- Failure signatures: Poor reconstruction quality on held-out camera indicates overfitting; inconsistent performance across metadata categories suggests dataset bias; high variance in metrics indicates unstable training.
- First 3 experiments:
  1. Train a baseline NeRF model on 4 cameras and evaluate on the 5th camera to establish baseline performance.
  2. Evaluate model performance across different weather conditions using provided metadata to identify environmental weaknesses.
  3. Compare performance on static vs. dynamic scenes to quantify the impact of moving objects on reconstruction quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of dynamic objects like pedestrians, cyclists, and vehicles affect the performance of novel view synthesis models compared to static scenes?
- Basis in paper: [explicit] The paper mentions that the dataset contains many dynamic and deformable elements, which are challenging for current methods.
- Why unresolved: The paper introduces the dataset but does not provide detailed experimental results comparing model performance on dynamic versus static scenes.
- What evidence would resolve it: Detailed performance metrics (PSNR, SSIM, LPIPS, FID) comparing model outputs on scenes with varying levels of dynamic content.

### Open Question 2
- Question: What is the impact of different weather conditions (sunny, overcast, rainy) on the accuracy of novel view synthesis models?
- Basis in paper: [explicit] The dataset includes diverse environmental conditions, and the paper suggests evaluating models for specific subsets of scenes, such as those with rain.
- Why unresolved: The paper does not provide specific results on how different weather conditions affect model performance.
- What evidence would resolve it: Performance metrics broken down by weather condition, showing how accuracy varies across sunny, overcast, and rainy scenes.

### Open Question 3
- Question: How well do novel view synthesis models generalize to off-axis views with large baseline distances, as tested by the front-forward camera?
- Basis in paper: [explicit] The paper proposes an evaluation protocol using a held-out front-forward camera to measure off-axis reconstruction quality and test generalization capabilities.
- Why unresolved: While the protocol is proposed, the paper does not provide results on how well models perform on these off-axis views.
- What evidence would resolve it: Comparative performance metrics for models trained with and without the front-forward camera, highlighting differences in off-axis view accuracy.

### Open Question 4
- Question: What is the effect of rapid exposure changes and lens flare on the reconstruction quality of novel view synthesis models?
- Basis in paper: [explicit] The paper mentions that driving scenes can exhibit dynamic lighting conditions such as lens flare and fast camera exposure changes, which are difficult to model.
- Why unresolved: The paper does not provide specific experimental results on how these lighting conditions affect model performance.
- What evidence would resolve it: Performance metrics for scenes with and without rapid exposure changes or lens flare, demonstrating the impact on reconstruction quality.

## Limitations

- The paper does not provide baseline model implementation details or training hyperparameters, making direct comparison with other methods difficult
- No quantitative evaluation of COLMAP pose accuracy in dynamic driving scenarios, which is critical for the dataset's utility
- Limited discussion of how the dataset handles temporal consistency across the 20-second scene sequences
- No analysis of potential dataset biases or limitations in scene coverage

## Confidence

- High confidence: The dataset's basic structure, camera setup, and metadata provision are well-specified
- Medium confidence: The evaluation protocol using a held-out camera is clearly defined but its effectiveness depends on unvalidated assumptions about pose accuracy
- Medium confidence: The claim about addressing limitations of prior datasets is supported by comparison but lacks quantitative validation

## Next Checks

1. Validate COLMAP pose accuracy by comparing reconstruction quality on static scenes (where poses should be highly accurate) versus dynamic scenes with moving objects
2. Test model generalization by training on subsets of metadata categories (e.g., only daytime scenes) and evaluating on held-out conditions (night scenes) to verify the dataset enables targeted evaluation
3. Conduct ablation studies removing COLMAP poses versus using only camera intrinsics to quantify the impact of geometric priors on novel view synthesis quality