---
ver: rpa2
title: Verifying Machine Unlearning with Explainable AI
arxiv_id: '2411.13332'
source_url: https://arxiv.org/abs/2411.13332
tags:
- unlearning
- methods
- data
- these
- patterns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of Explainable AI (XAI)
  in verifying Machine Unlearning (MU) within the context of harbor front monitoring,
  focusing on data privacy and regulatory compliance. The study explores various MU
  approaches, including data relabeling and model perturbation, and leverages attribution-based
  XAI to evaluate their effectiveness.
---

# Verifying Machine Unlearning with Explainable AI

## Quick Facts
- arXiv ID: 2411.13332
- Source URL: https://arxiv.org/abs/2411.13332
- Authors: Àlex Pujol Vidal; Anders S. Johansen; Mohammad N. S. Jahromi; Sergio Escalera; Kamal Nasrollahi; Thomas B. Moeslund
- Reference count: 24
- Key outcome: This paper investigates the effectiveness of Explainable AI (XAI) in verifying Machine Unlearning (MU) within the context of harbor front monitoring, focusing on data privacy and regulatory compliance.

## Executive Summary
This paper investigates the effectiveness of Explainable AI (XAI) in verifying Machine Unlearning (MU) within the context of harbor front monitoring, focusing on data privacy and regulatory compliance. The study explores various MU approaches, including data relabeling and model perturbation, and leverages attribution-based XAI to evaluate their effectiveness. Two novel XAI-based metrics, Heatmap Coverage (HC) and Attention Shift (AS), are proposed to quantify the impact of MU methods on model performance. The results demonstrate that MU methods, evaluated through XAI techniques, can efficiently unlearn undesired patterns, offering a more practical alternative to complete model retraining. Specifically, the unlearned models show improved ability to concentrate heatmap intensity on desired target objects, with HC and AS metrics indicating effective removal of human-related features. The study highlights the potential of XAI in verifying MU methods and sets the stage for future research to enhance their joint integration.

## Method Summary
The study employs the LTD dataset (1,069,428 thermal images) and focuses on removing human counting patterns while retaining other object counting capabilities. Four classes are used: Human, Bicycle, Motorcycle, and Vehicle. The method involves training baseline models, then applying unlearning techniques including fine-tuning on relabeled data, pruning and reinitialization, and a Confuse method. Attribution-based XAI using SIDU generates heatmaps from the last convolutional layer to evaluate model attention patterns. Two novel metrics are introduced: Heatmap Coverage (HC) measures spatial correctness of predicted heatmaps against ground truth object regions, while Attention Shift (AS) quantifies changes in model attention patterns. The evaluation compares unlearned models against baseline models using regression metrics (RMSE, MAE) and the new XAI-based metrics.

## Key Results
- MU methods evaluated through XAI techniques can efficiently unlearn undesired patterns, offering a more practical alternative to complete model retraining.
- Unlearned models show improved ability to concentrate heatmap intensity on desired target objects.
- HC and AS metrics indicate effective removal of human-related features from the models.
- Unlearning methods maintain model performance on retained counting tasks while removing human-related attention patterns.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SIDU-generated heatmaps reveal whether unlearning has successfully removed human-related attention patterns from the model.
- Mechanism: SIDU computes pixel-wise importance by extracting feature maps from the last convolutional layer and combining similarity difference (SD) and uniqueness (U) masks. This produces heatmaps that highlight influential input regions. Comparing heatmaps before and after unlearning shows shifts in attention away from human patterns.
- Core assumption: The regions of high attribution in SIDU heatmaps correspond to the model's actual reliance on those features for prediction.
- Evidence anchors:
  - [abstract] "We leverage attribution-based XAI to discuss the effects of unlearning on model performance."
  - [section] "We propose leveraging attribute-based XAI to evaluate how much MU succeeds qualitatively and quantitavely in forgetting learned patterns"
  - [corpus] Weak - neighboring papers focus on verification but do not validate SIDU specifically for unlearning assessment.
- Break condition: If the attribution method does not accurately reflect feature importance, or if the model has internal representations that are not captured by the last convolutional layer.

### Mechanism 2
- Claim: Heatmap Coverage (HC) metric quantifies spatial correctness of predicted heatmaps by measuring overlap between heatmap intensity and ground truth object regions.
- Mechanism: HC computes the average weighted element-wise overlap between SIDU heatmap and object detection bounding box annotations. Higher HC for non-human objects post-unlearning indicates successful removal of human attention.
- Core assumption: Ground truth bounding boxes accurately represent the regions the model should focus on for the task (object counting).
- Evidence anchors:
  - [section] "HC measures the spatial correctness of the predicted heatmaps, while AS quantifies the shift in attention to relevant areas"
  - [section] "HC quantifies the spatial correctness of the predicted heatmaps by computing the average of a weighted element-wise overlap between predicted SIDU heatmapH and its corresponding region of interest maskM"
- Break condition: If object detection annotations are inaccurate or incomplete, or if the model uses non-spatial features for prediction.

### Mechanism 3
- Claim: Attention Shift (AS) metric quantifies the degree of change in model attention patterns between original and unlearned models.
- Mechanism: AS computes the standard deviation of the difference between heatmaps from original and unlearned models. Higher AS values indicate more significant shifts away from human-related regions.
- Core assumption: Standard deviation of heatmap differences is a meaningful measure of attention shift magnitude.
- Evidence anchors:
  - [section] "AS quantifies the shift in attention to relevant areas, providing a comprehensive assessment of the unlearning process"
  - [section] "we measure the change in relative importance by computing the standard deviation of the difference between heatmaps"
- Break condition: If the standard deviation metric does not capture meaningful differences in attention patterns, or if noise in heatmap generation dominates the signal.

## Foundational Learning

- Concept: Attribution-based Explainable AI (XAI) methods
  - Why needed here: To visualize and quantify which input features the model relies on for predictions, enabling verification of unlearning effectiveness
  - Quick check question: How does Grad-CAM differ from perturbation-based methods like LIME in terms of computational requirements and applicability?

- Concept: Machine Unlearning (MU) techniques
  - Why needed here: To remove specific learned patterns (human counting) from models without full retraining, addressing privacy compliance requirements
  - Quick check question: What is the key difference between model-agnostic and model-intrinsic unlearning approaches in terms of applicability and guarantees?

- Concept: Regression evaluation metrics (MAE, RMSE)
  - Why needed here: To assess whether unlearning maintains model performance on the retained counting task while removing human-related patterns
  - Quick check question: Why might RMSE be more sensitive to outliers than MAE in regression tasks?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> Model training component -> XAI component -> Evaluation pipeline
- Critical path: Data preprocessing → Model training → SIDU heatmap generation → HC/AS metric computation → Results analysis
- Design tradeoffs:
  - SIDU vs gradient-based methods: SIDU doesn't require model gradients but may be less precise for some architectures
  - Fine-tuning vs complete retraining: Fine-tuning is faster but may retain residual human patterns
  - HC/AS metrics vs traditional accuracy: New metrics provide insight into attention patterns but may be noisier
- Failure signatures:
  - HC scores don't improve post-unlearning: Unlearning may not be effective or ground truth annotations may be inaccurate
  - AS values remain low: Model may not have shifted attention away from human regions
  - SIDU heatmaps appear noisy: Feature extraction or mask computation may have issues
- First 3 experiments:
  1. Train baseline model on full dataset, generate SIDU heatmaps, calculate HC/AS metrics
  2. Apply fine-tuning unlearning, compare heatmaps and metrics against baseline
  3. Apply Confuse method unlearning, analyze heatmap differences and metric improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can XAI methods be further optimized to provide more accurate verification of MU techniques, particularly in complex models?
- Basis in paper: [explicit] The paper discusses the use of attribution-based XAI methods like SIDU for verifying MU but acknowledges the lack of consensus on the efficacy of XAI techniques and metrics for evaluating model explanations.
- Why unresolved: The effectiveness of XAI methods in accurately reflecting important regions in the image is not fully established, and there is variability in the explanations generated by different XAI techniques.
- What evidence would resolve it: Comparative studies using multiple XAI methods on various models to establish consistent and reliable metrics for evaluating model explanations.

### Open Question 2
- Question: What are the long-term effects of MU on model performance and privacy compliance across different types of datasets and tasks?
- Basis in paper: [inferred] The paper suggests that MU can efficiently unlearn undesired patterns, but it does not explore the long-term impacts of these techniques on model performance or privacy compliance.
- Why unresolved: The study focuses on a specific use-case (harbor front monitoring) and does not investigate how MU techniques perform over time or across different datasets and tasks.
- What evidence would resolve it: Longitudinal studies and experiments across diverse datasets and tasks to assess the durability and adaptability of MU techniques.

### Open Question 3
- Question: How can XAI and MU be integrated into a joint end-to-end optimization framework to enhance their effectiveness?
- Basis in paper: [explicit] The paper suggests that the next step could be joint end-to-end optimization to ensure that XAI features are considered to directly target undesired attributes at training time.
- Why unresolved: The paper proposes this integration but does not provide a concrete framework or methodology for achieving it.
- What evidence would resolve it: Development and testing of a joint optimization framework that incorporates both XAI and MU techniques, demonstrating improved performance and compliance.

## Limitations

- The proposed verification approach relies heavily on the assumption that SIDU attribution accurately captures the model's true feature importance for unlearning assessment.
- The study's scope is limited to one specific dataset (LTD) and regression task, which constrains generalizability to other domains or classification problems.
- The unlearning methods evaluated are relatively basic, and their effectiveness compared to more sophisticated approaches remains unexplored.

## Confidence

- SIDU-based heatmap generation for unlearning verification: Medium
- HC and AS metrics as effective evaluation tools: Medium
- Unlearning methods' practical effectiveness compared to retraining: High

## Next Checks

1. Conduct ablation studies varying the SIDU attribution parameters to assess robustness of the HC and AS metrics
2. Test the verification framework on multiple datasets and model architectures to evaluate generalizability
3. Compare against alternative XAI methods (e.g., Grad-CAM, LIME) to validate SIDU's superiority for unlearning assessment