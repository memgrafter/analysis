---
ver: rpa2
title: Learning state and proposal dynamics in state-space models using differentiable
  particle filters and neural networks
arxiv_id: '2411.15638'
source_url: https://arxiv.org/abs/2411.15638
tags:
- distribution
- particle
- proposal
- parameters
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StateMixNN, a novel method that uses a pair
  of neural networks to learn both the proposal distribution and transition distribution
  of a particle filter. The approach parameterizes these distributions as multivariate
  Gaussian mixtures, with means and covariances learned via dense neural networks.
---

# Learning state and proposal dynamics in state-space models using differentiable particle filters and neural networks

## Quick Facts
- arXiv ID: 2411.15638
- Source URL: https://arxiv.org/abs/2411.15638
- Reference count: 40
- Primary result: StateMixNN outperforms state-of-the-art particle filtering methods in highly non-linear scenarios by learning both transition and proposal distributions as multivariate Gaussian mixtures.

## Executive Summary
This paper introduces StateMixNN, a novel method that uses a pair of neural networks to learn both the proposal distribution and transition distribution of a particle filter. The approach parameterizes these distributions as multivariate Gaussian mixtures, with means and covariances learned via dense neural networks. By optimizing the log-likelihood using differentiable particle filters, the method enables state-space model learning without requiring hidden state knowledge, relying only on observations. The proposed method outperforms state-of-the-art techniques, including the improved auxiliary particle filter and bootstrap particle filter, particularly in highly non-linear scenarios.

## Method Summary
StateMixNN learns state and proposal dynamics by parameterizing both the transition and proposal distributions as multivariate Gaussian mixtures, with component parameters learned through dense neural networks. The method uses differentiable particle filters to compute gradients of the log-likelihood with respect to network parameters, enabling end-to-end training. An alternating optimization scheme updates the transition and proposal networks sequentially to stabilize learning. The algorithm addresses likelihood concentration through observation batching, progressively incorporating more data during training.

## Key Results
- StateMixNN achieves 27% average improvement in relative MSE over the improved auxiliary particle filter on Lorenz 96 and Kuramoto oscillator models
- Performance gains increase with model complexity, reaching 59% improvement on the Kuramoto oscillator
- The method successfully learns highly non-linear transition and proposal dynamics without requiring hidden state knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The learned Gaussian mixture proposal captures multi-modal posterior structure better than unimodal proposals.
- Mechanism: By approximating the proposal distribution as a multivariate Gaussian mixture with learned component means and covariances, the model can represent complex, multi-modal posterior distributions that arise in highly non-linear systems. Each mixture component acts as a local mode in the state space.
- Core assumption: The true posterior distribution is sufficiently smooth and can be well-approximated by a finite mixture of Gaussians.
- Evidence anchors:
  - [abstract] "Both distributions are approximated using multivariate Gaussian mixtures"
  - [section] "Mixtures of multivariate Gaussians can represent a wide range of possible distributions, ofering flexibility and expressiveness"
  - [corpus] Weak - no direct comparison to unimodal proposals in neighbor papers

### Mechanism 2
- Claim: Alternating conditional updates stabilize learning of coupled transition and proposal distributions.
- Mechanism: The transition and proposal distributions are interdependent in particle filtering - changes in one affect the optimal form of the other. By learning each distribution conditional on the current estimate of the other, the algorithm allows them to adapt to each other's structure without divergence.
- Core assumption: The transition and proposal distributions can be learned sequentially without losing convergence guarantees.
- Evidence anchors:
  - [section] "In order to learn the network parameters θ(f) and θ(π), Alg. 3 uses an alternating scheme to learn each parameter conditional on the value of the other"
  - [section] "Learning one parameter conditional on the other stabilises inference, as the parameters heavily influence each other"
  - [corpus] Weak - no explicit discussion of alternating schemes in neighbor papers

### Mechanism 3
- Claim: Observation batching prevents likelihood concentration and enables learning from random initialization.
- Mechanism: By training on increasingly larger batches of observations (y(1) ⊆ y(2) ⊆ ... ⊆ y(B)), the algorithm starts with a diffuse likelihood surface that allows gradient-based optimization to make progress, then progressively incorporates more information to refine the estimate.
- Core assumption: The observation sequence contains sufficient information to distinguish between different parameter values.
- Evidence anchors:
  - [section] "State-space models in general suffer from likelihood concentration, where ℓ(θ|y1:T)... becomes increasingly concentrated around a single value of θ as T increases"
  - [section] "We address likelihood concentration... using observation batching"
  - [corpus] Weak - no discussion of likelihood concentration in neighbor papers

## Foundational Learning

- Concept: Particle filtering and sequential Monte Carlo methods
  - Why needed here: The entire algorithm is built on particle filtering framework, understanding importance weights, resampling, and proposal distributions is essential
  - Quick check question: What is the purpose of the proposal distribution in a particle filter, and why is it different from the transition distribution?

- Concept: Differentiable programming and gradient-based optimization
  - Why needed here: The method uses differentiable particle filters to enable gradient-based learning of network parameters through backpropagation
  - Quick check question: How does the stop-gradient differentiable particle filter make the resampling step differentiable?

- Concept: Neural network architecture and training
  - Why needed here: The transition and proposal distributions are parameterized by neural networks, requiring understanding of network design, activation functions, and optimization methods
  - Quick check question: Why does the paper use diagonal covariances instead of full covariance matrices in the Gaussian mixtures?

## Architecture Onboarding

- Component map: Observations → Particle filter → Weights → Log-likelihood → Gradients → Network parameter updates
- Critical path: StateMixNN (main algorithm) → Transition network NN(f) → Proposal network NN(π) → Differentiable particle filter (Alg. 2) → Alternating update loop (Alg. 3-5)
- Design tradeoffs:
  - Mixture components vs computational cost: More components increase expressiveness but require more parameters and computation
  - Diagonal vs full covariances: Diagonal is computationally efficient but may miss some dependencies
  - Alternating vs simultaneous updates: Alternating provides stability but may converge slower
- Failure signatures:
  - Numerical instability: Extremely small weights leading to NaN values
  - Poor convergence: Log-likelihood plateaus early or oscillates
  - Mode collapse: All particles collapse to a single region, effective sample size drops
- First 3 experiments:
  1. Test on simple linear Gaussian system where Kalman filter is optimal - should match KF performance
  2. Run with S=1 (single Gaussian) on Lorenz 96 - should still outperform BPF but less than multi-component version
  3. Test with very few particles (K=10) on Kuramoto oscillator - should show degradation but still work

## Open Questions the Paper Calls Out

- **Open Question 1**: How does StateMixNN's performance scale with the number of mixture components in highly non-linear systems?
  - Basis in paper: [explicit] The paper notes that StateMixNN's performance improves with more mixture components, particularly in highly non-linear scenarios.
  - Why unresolved: The paper only tests up to 10 mixture components, and the relationship between performance and component number in highly non-linear systems remains unclear.
  - What evidence would resolve it: Additional experiments testing StateMixNN with varying numbers of mixture components (e.g., 20, 50, 100) in highly non-linear systems would clarify this relationship.

- **Open Question 2**: How does StateMixNN handle systems with multimodal state distributions that are not well-separated?
  - Basis in paper: [inferred] The paper demonstrates StateMixNN's effectiveness in capturing multimodal distributions in the Lorenz 96 system, but does not address cases where modes are closely spaced.
  - Why unresolved: The paper focuses on systems with distinct, well-separated modes, leaving the performance in cases of closely spaced or overlapping modes unexplored.
  - What evidence would resolve it: Testing StateMixNN on systems with closely spaced multimodal distributions (e.g., overlapping Gaussian mixtures) would provide insights into its handling of such cases.

- **Open Question 3**: What is the impact of using non-diagonal covariance matrices in StateMixNN's Gaussian mixture components?
  - Basis in paper: [explicit] The paper restricts covariances to diagonal form to reduce parameters and computational complexity.
  - Why unresolved: The paper does not explore the potential benefits or drawbacks of using full covariance matrices, which could capture more complex relationships between state dimensions.
  - What evidence would resolve it: Comparing StateMixNN's performance with diagonal and full covariance matrices in various systems would clarify the trade-offs involved.

## Limitations
- The method assumes the true posterior can be well-approximated by a finite mixture of Gaussians, which may not hold for highly complex or multi-modal distributions.
- The alternating update scheme, while stabilizing, may converge slower than joint optimization approaches and could potentially get stuck in local optima.
- The use of diagonal covariances in the Gaussian mixtures may miss important correlations in the state space that could improve performance.

## Confidence

**High confidence**: The core algorithmic framework and its implementation details (Gaussian mixture parameterization, differentiable particle filtering, alternating updates)

**Medium confidence**: The effectiveness of the observation batching approach for mitigating likelihood concentration

**Medium confidence**: The superiority claims over existing methods, though performance gains are clearly demonstrated in the tested scenarios

## Next Checks

1. Test StateMixNN on a simple linear Gaussian system where Kalman filter is optimal - should match KF performance to validate the method doesn't degrade basic cases
2. Run with S=1 (single Gaussian) on Lorenz 96 - should still outperform BPF but less than multi-component version to verify the benefit of mixture components
3. Test with very few particles (K=10) on Kuramoto oscillator - should show degradation but still work to assess robustness to particle count