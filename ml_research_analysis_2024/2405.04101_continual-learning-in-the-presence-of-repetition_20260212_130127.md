---
ver: rpa2
title: Continual Learning in the Presence of Repetition
arxiv_id: '2405.04101'
source_url: https://arxiv.org/abs/2405.04101
tags:
- learning
- repetition
- each
- data
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reports on the CLVision challenge at CVPR 2023, which
  focused on continual learning (CL) with repeated concepts in data streams. Unlike
  standard CL benchmarks, this challenge emphasized exploiting naturally occurring
  repetition in streams rather than controlled rehearsal.
---

# Continual Learning in the Presence of Repetition

## Quick Facts
- arXiv ID: 2405.04101
- Source URL: https://arxiv.org/abs/2405.04101
- Reference count: 40
- Primary result: Ensemble-based strategies significantly outperform traditional CL baselines when repetition is present in data streams

## Executive Summary
This paper reports on the CLVision challenge at CVPR 2023, which focused on continual learning with repeated concepts in data streams. Unlike standard CL benchmarks, this challenge emphasized exploiting naturally occurring repetition in streams rather than controlled rehearsal. Three finalist teams developed ensemble-based solutions that significantly outperformed traditional CL baselines on streams with repetition. The findings demonstrate that repetition fundamentally changes which CL strategies are most effective, with ensemble approaches showing particular advantage when classes reappear across multiple experiences.

## Method Summary
The challenge employed a class-incremental learning setup with repetition (CIR) using the CIFAR-100 dataset. Three finalist approaches emerged: HAT-CIR used multiple network replicas with hard attention and momentum-based decision making; Horde combined frozen feature extractors with pseudo-feature projection for class alignment; DWGRNet employed gated branches with open-set recognition techniques. All approaches relied on ensemble-based strategies where each experience had dedicated model components trained on overlapping subsets of classes. The key innovation was leveraging natural repetition in streams rather than relying on rehearsal buffers.

## Key Results
- Ensemble-based strategies significantly outperformed traditional CL baselines (ER, A-GEM, GDumb) on CIR streams
- The HAT-CIR approach achieved highest accuracy by combining network replicas with hard attention mechanisms
- Performance advantage of ensemble methods disappeared when tested on streams without repetition
- CIFAR-100 without repetition experiments showed ER with buffer outperformed ensemble solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble-based strategies are more effective than single-model approaches when repetition is present in the data stream.
- Mechanism: Each experience has a dedicated model branch that is trained on that experience and then frozen, creating an ensemble of specialists. Repetition allows each class to be observed in multiple experiences with different class pairings, enabling richer feature extraction and more robust representations.
- Core assumption: The repetition in the data stream provides natural bagging that an ensemble can exploit effectively.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 2
- Claim: Hard attention-based partitioning can mitigate catastrophic forgetting by isolating parameters for each experience.
- Mechanism: HAT uses binary masks to partition the network based on experience identifiers, preventing interference between experiences. During training, masks are gradually made sparse to isolate experience-specific parameters.
- Core assumption: Experience identifiers are available during both training and testing phases.
- Evidence anchors: [section], [corpus]

### Mechanism 3
- Claim: Open-set recognition techniques can improve ensemble performance by weighting predictions based on entropy, feature norm, and experience class count.
- Mechanism: Each branch evaluates whether a test sample is within-distribution or out-of-distribution. Predictions are weighted by entropy (low for in-distribution), feature norm (higher suggests in-distribution), and number of classes seen by that branch (more classes = more reliable).
- Core assumption: Entropy of predicted probabilities correlates with confidence and OOD detection.
- Evidence anchors: [section], [corpus]

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: The core challenge in continual learning is preventing models from forgetting previously learned knowledge when training on new experiences
  - Quick check question: What happens to a model's performance on task A after it's trained on task B without any regularization?

- Concept: Ensemble learning
  - Why needed here: The winning strategies all employ ensemble-based approaches where each experience has its own dedicated model component
  - Quick check question: How does having multiple model components, each trained on different data subsets, help with continual learning?

- Concept: Open-set recognition
  - Why needed here: Used in DWGRNet to handle cases where test samples may not belong to any class seen by a particular branch
  - Quick check question: What distinguishes an open-set recognition problem from a standard classification problem?

## Architecture Onboarding

- Component map: Base Slim-ResNet-18 backbone with multiple specialized branches/ensembles. Each experience can have: (1) a dedicated feature extractor (HAT-CIR fragments), (2) multiple ensemble replicas trained on same experience, (3) gating units to control branch activation, (4) unified head for final classification.
- Critical path: During training - add new branch for current experience, train with appropriate loss (cross-entropy, contrastive, etc.), freeze branch. During inference - forward sample through all branches, collect outputs, apply weighting strategy, make final prediction.
- Design tradeoffs: More branches = better specialization but higher memory/compute. Using replay buffer vs natural repetition. Single model with attention vs multiple replicas.
- Failure signatures: Poor performance on classes that appear only in early experiences, high entropy predictions across all branches, overconfident predictions from branches that haven't seen certain classes.
- First 3 experiments:
  1. Implement basic ensemble with one branch per experience, train on CIR stream, measure accuracy vs single model baseline
  2. Add open-set recognition weighting (entropy-based) to ensemble predictions, measure improvement
  3. Test performance on stream without repetition to verify ensemble advantage disappears

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the amount and type of repetition in data streams affect the relative effectiveness of different CL strategies?
- Basis in paper: [explicit] The paper states "An exciting question for future work is to map out exactly how the amount and the type of repetition affect the effectiveness of each approach."
- Why unresolved: While the paper demonstrates that repetition fundamentally changes which CL strategies are most effective, it does not systematically investigate how varying the amount or type of repetition (e.g., different repetition probability distributions) would impact different strategies' performance.
- What evidence would resolve it: Controlled experiments varying the repetition probability distribution (ùëÉùëü) and first occurrence distribution (ùëÉùëì) parameters across multiple CL strategies to identify which strategies benefit most from different repetition patterns.

### Open Question 2
- Question: Why is the ensemble-based approach particularly effective for continual learning in the presence of repetition?
- Basis in paper: [explicit] The paper states "We speculate that one reason is that the repetition allows each class to be observed in multiple experiences and in different pairings with other classes" and mentions this could allow "development of a 'rich' set of feature extractors."
- Why unresolved: The paper only provides speculative explanations about why ensemble-based strategies work well with repetition, but does not provide rigorous empirical or theoretical justification for this effectiveness.
- What evidence would resolve it: Detailed analysis comparing feature representations learned by ensemble-based approaches versus single-model approaches across different repetition patterns, or theoretical analysis of how repetition provides natural bagging for ensemble methods.

### Open Question 3
- Question: What is the optimal balance between the number of fragments and ensembles in ensemble-based CL strategies?
- Basis in paper: [explicit] The HAT-CIR ablation study shows "both increases in ensemble replicas and increases in fragment replicas enhance performance" but notes "there is a sweet spot in balancing the number of fragments and ensembles for optimal performance."
- Why unresolved: While the paper explores different configurations of fragments and ensembles, it does not identify general principles for determining the optimal balance between these two types of network replicas.
- What evidence would resolve it: Systematic experiments identifying the relationship between dataset characteristics, repetition patterns, and the optimal fragment-to-ensemble ratio, or analytical models predicting computational trade-offs for different configurations.

## Limitations
- Findings are specific to data streams with guaranteed repetition and may not generalize to standard CL scenarios without repetition
- Reliance on CIFAR-100 and Tiny ImageNet datasets limits ecological validity for more complex real-world applications
- Challenge format constraints (hardware and time limits) may prevent development of more sophisticated models

## Confidence
- High confidence: Ensemble-based strategies are more effective than traditional CL baselines when repetition is present in streams
- Medium confidence: The specific mechanisms (hard attention, pseudo-feature projection, open-set recognition) are responsible for performance gains
- Low confidence: These findings will generalize to other datasets, stream generation methods, or real-world scenarios without repetition

## Next Checks
1. **Ablation on repetition frequency**: Systematically vary the repetition probability in generated streams and measure how ensemble performance degrades as repetition decreases, identifying the threshold where traditional CL methods become superior.

2. **Cross-dataset validation**: Test the three finalist strategies on larger, more complex datasets like ImageNet-100 or domain-specific image streams to evaluate whether ensemble advantages persist with increased visual complexity and intra-class variation.

3. **Real-world stream evaluation**: Deploy these strategies on actual production data streams (e.g., web traffic classification, sensor data) where repetition patterns are naturally occurring but unpredictable, measuring both accuracy and computational overhead compared to standard CL approaches.