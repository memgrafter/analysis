---
ver: rpa2
title: 'The Same But Different: Structural Similarities and Differences in Multilingual
  Language Modeling'
arxiv_id: '2410.09223'
source_url: https://arxiv.org/abs/2410.09223
tags:
- heads
- english
- chinese
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates whether multilingual language models (LLMs)\
  \ use shared or language-specific internal circuits when processing tasks with either\
  \ common or distinct morphosyntactic structures across languages. Using English\
  \ and Chinese, the authors analyze circuits for an indirect object identification\
  \ (IOI) task\u2014structurally identical in both languages\u2014and a past tense\
  \ generation task\u2014requiring morphological marking in English but not Chinese."
---

# The Same But Different: Structural Similarities and Differences in Multilingual Language Modeling

## Quick Facts
- **arXiv ID**: 2410.09223
- **Source URL**: https://arxiv.org/abs/2410.09223
- **Reference count**: 21
- **Primary result**: Multilingual language models flexibly balance shared mechanisms with language-specific components depending on structural requirements, using nearly identical circuits for syntactically identical tasks across languages.

## Executive Summary
This work investigates whether multilingual language models (LLMs) use shared or language-specific internal circuits when processing tasks with either common or distinct morphosyntactic structures across languages. Using English and Chinese, the authors analyze circuits for an indirect object identification (IOI) task—structurally identical in both languages—and a past tense generation task—requiring morphological marking in English but not Chinese. With path patching and information flow routes, they find that multilingual models use nearly identical circuits for IOI in both languages, and even monolingual models trained independently converge on similar circuitry. For past tense, multilingual models employ overlapping circuits but activate language-specific heads and feed-forward networks only where morphological marking is needed (English). These results show that LLMs flexibly balance shared mechanisms with language-specific components, depending on structural requirements, offering insights for cross-lingual transfer and model interpretability.

## Method Summary
The study employs path patching and information flow routes to identify causally important internal components (attention heads and feed-forward networks) for specific linguistic tasks across languages. Using BLOOM-560M, GPT2-small, CPM-distilled, and Qwen2-0.5B-instruct, the authors analyze indirect object identification and past tense generation tasks in English and Chinese. Minimal pair templates are created for IOI tasks, while past tense tasks use regular verbs. Path patching identifies important heads and circuits, while ablation studies validate functionality by removing specific components to observe performance changes.

## Key Results
- Multilingual models use nearly identical internal circuits for syntactically identical tasks (IOI) across English and Chinese, with high overlap in important heads between languages.
- Even monolingual models trained independently on different languages converge on similar circuits for the same linguistic task.
- For past tense generation, multilingual models employ overlapping circuits but activate language-specific heads and feed-forward networks only where morphological marking is needed (English), while bypassing them for languages without such marking (Chinese).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual models use nearly identical internal circuits for handling syntactically identical tasks across languages.
- Mechanism: The model employs a shared circuit with the same attention heads and feed-forward networks for both English and Chinese IOI tasks, performing the same algorithmic steps (duplicate token detection, inhibition, copying).
- Core assumption: Structural similarity in morphosyntactic processes translates to circuit-level similarity regardless of lexical differences.
- Evidence anchors:
  - [abstract] "we find evidence that models employ the same circuit to handle the same syntactic process independently of the language in which it occurs"
  - [section 3.4] "we observe a high overlap in the important heads used between languages"
- Break condition: When tasks require fundamentally different morphosyntactic processes (e.g., morphological marking in one language but not another), the circuit overlap decreases and language-specific components emerge.

### Mechanism 2
- Claim: Monolingual models trained independently on different languages converge on similar circuits for the same linguistic task.
- Mechanism: Despite different training data, monolingual English and Chinese models independently develop nearly identical IOI circuits with the same functional components and algorithmic steps.
- Core assumption: The distribution of linguistic patterns in different languages contains sufficient regularities that lead to convergent circuit development.
- Evidence anchors:
  - [abstract] "even for monolingual models trained completely independently"
  - [section 3.5] "we discover circuits with path patching which gives similar results and identifies the same components that are important for the circuits"
- Break condition: When linguistic structures are fundamentally different between languages, monolingual models may develop divergent circuits to handle language-specific requirements.

### Mechanism 3
- Claim: Multilingual models employ language-specific components only where necessary for language-specific morphosyntactic processes.
- Mechanism: For past tense generation, multilingual models use a shared circuit for core processing but activate language-specific heads and feed-forward networks only for morphological marking (English) while bypassing them for languages without such marking (Chinese).
- Core assumption: The model can selectively engage language-specific components based on the morphosyntactic requirements of the input language.
- Evidence anchors:
  - [abstract] "multilingual models employ language-specific components (attention heads and feed-forward networks) when needed to handle linguistic processes (e.g., morphological marking) that only exist in some languages"
  - [section 4.4] "we observe that almost no heads have high activation frequency in the later layers (Layer 19-20)" for Chinese past tense task
  - [section 4.6] "the accuracy on the English task drops from 97.44% to 47.44% and the zero-rank rate from 58.06% to 17.74%. However, for the Chinese task, the zero-rank rate is barely changed"
- Break condition: When both languages require similar morphosyntactic processing, the language-specific components may not be activated, leading to shared circuit usage.

## Foundational Learning

- Concept: Path patching and information flow routes for mechanistic interpretability
  - Why needed here: These techniques are essential for identifying which internal components (attention heads, feed-forward networks) are causally important for specific linguistic tasks across languages
  - Quick check question: What is the key difference between path patching and information flow routes in terms of how they identify important model components?

- Concept: Minimal pair template design for cross-lingual tasks
  - Why needed here: Creating appropriate minimal pairs is crucial for isolating the specific linguistic processes being studied and ensuring valid cross-lingual comparisons
  - Quick check question: Why were minimal pairs not used for the Chinese past tense task, and what limitation did this create?

- Concept: Circuit functionality validation through ablation studies
  - Why needed here: Ablating specific heads or layers helps confirm their functional role in the circuit and demonstrates whether they are essential for task completion in specific languages
  - Quick check question: What did the ablation of past tense heads reveal about their role in English versus Chinese past tense processing?

## Architecture Onboarding

- Component map: Embedding layer -> 24 transformer layers (attention heads, feed-forward networks) -> Layer normalization -> Final projection to vocabulary space
- Critical path: Input processing through embedding layer → Layer-wise attention and feed-forward operations → Residual connections and layer normalization → Final projection to vocabulary space → Output logits generation
- Design tradeoffs:
  - Using shared circuits across languages reduces parameter requirements but may limit language-specific optimization
  - Language-specific components provide necessary flexibility but increase model complexity
  - Minimal pair design must balance linguistic validity with technical feasibility
- Failure signatures:
  - Low accuracy/zero-rank rate in one language while maintaining performance in another
  - Circuit overlap that doesn't match linguistic structural similarity
  - Language-specific components activating when not needed or failing to activate when required
- First 3 experiments:
  1. Replicate IOI circuit analysis on a different multilingual model to verify circuit convergence
  2. Test past tense task with languages that have intermediate morphological complexity (e.g., Spanish)
  3. Perform systematic ablation of language-specific components to map their exact contribution to cross-lingual performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs learn to consistently converge on similar circuits for tasks with common structural patterns across languages, even when trained independently on different languages?
- Basis in paper: [explicit] The paper shows that monolingual models trained on English and Chinese each adopt nearly the same circuit for the IOI task, suggesting a surprising amount of consistency with how LLMs learn to handle this particular aspect of language modeling.
- Why unresolved: The paper does not investigate the training dynamics or mechanisms that lead to this convergence. It is unclear what specific aspects of the training data or optimization process cause models to learn similar circuits for similar tasks.
- What evidence would resolve it: Analyzing the training trajectories of monolingual models on similar tasks could reveal when and how specific circuit components emerge. Comparing the gradients and weight updates during training could provide insights into the optimization dynamics that lead to convergence.

### Open Question 2
- Question: To what extent can the observed circuit overlap between languages be exploited to improve cross-lingual transfer and parameter efficiency in multilingual models?
- Basis in paper: [inferred] The paper shows that multilingual models use shared circuits for tasks with common structures across languages, suggesting that these shared mechanisms could be leveraged for transfer learning.
- Why unresolved: The paper does not investigate whether the identified shared circuits can be explicitly used to improve cross-lingual transfer or reduce the number of parameters needed for multilingual models.
- What evidence would resolve it: Experiments that transfer the identified shared circuits from one language to another and measure the impact on performance and parameter efficiency would provide evidence for the practical utility of this approach.

### Open Question 3
- Question: How do LLMs handle tasks that require language-specific morphological processing, and what are the implications for modeling languages with diverse morphological systems?
- Basis in paper: [explicit] The paper shows that for the past tense task, multilingual models employ language-specific heads and feed-forward networks only where morphological marking is needed (English), while using largely overlapping circuits for both languages.
- Why unresolved: The paper focuses on a specific example of English tense morphology and does not investigate how LLMs handle other types of language-specific morphological processes or more complex morphological systems.
- What evidence would resolve it: Analyzing the circuits for tasks involving other types of morphological processing (e.g., agreement, case marking) in languages with diverse morphological systems would provide insights into the generalizability of the observed patterns and the challenges of modeling languages with different morphological structures.

## Limitations

- The study is constrained to two languages (English and Chinese) with a narrow set of linguistic tasks, limiting generalizability to other language pairs and morphosyntactic phenomena.
- The path patching methodology relies on indirect measurements of causal importance that may not fully capture the complete functional role of identified components.
- The monolingual convergence results are based on independent analysis rather than direct comparison of trained models, making it difficult to assess the degree of true circuit similarity.

## Confidence

- **High confidence**: The finding that multilingual models use shared circuits for structurally identical tasks across languages (IOI) is well-supported by path patching results and ablation studies showing consistent performance across both languages with nearly identical component usage.
- **Medium confidence**: The observation that monolingual models converge on similar circuits for the same task is supported by independent analyses but would benefit from direct comparison of trained models or more extensive language pairs to strengthen this claim.
- **Medium confidence**: The evidence for language-specific component activation in morphological tasks (past tense) is robust for English but limited for Chinese due to task design constraints, reducing confidence in the full characterization of language-specific circuit behavior.

## Next Checks

1. **Cross-Model Circuit Verification**: Replicate the IOI circuit analysis on a different multilingual architecture (e.g., mBERT or XLM-R) to determine whether the convergence on shared circuits is architecture-dependent or represents a general property of multilingual models.

2. **Intermediate Morphology Test**: Design a past tense task using a language with intermediate morphological complexity (such as Spanish or French) to map the continuum of shared versus language-specific component activation and test whether the model can flexibly engage different levels of morphological processing.

3. **Systematic Language-Specific Component Mapping**: Perform targeted ablation studies that progressively remove language-specific components to quantify their exact contribution to cross-lingual performance, particularly focusing on why Chinese past tense processing remains robust despite limited component activation.