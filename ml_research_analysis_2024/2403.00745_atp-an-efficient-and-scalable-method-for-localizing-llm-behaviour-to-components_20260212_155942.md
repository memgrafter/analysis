---
ver: rpa2
title: 'AtP*: An efficient and scalable method for localizing LLM behaviour to components'
arxiv_id: '2403.00745'
source_url: https://arxiv.org/abs/2403.00745
tags:
- clean
- nodes
- noise
- prompt
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates Activation Patching (AtP), a method for
  causally attributing model behavior to individual components, and proposes improvements
  to address its limitations. The authors identify two classes of failure modes in
  AtP: false negatives from attention saturation and cancellation between direct and
  indirect effects.'
---

# AtP*: An efficient and scalable method for localizing LLM behaviour to components

## Quick Facts
- arXiv ID: 2403.00745
- Source URL: https://arxiv.org/abs/2403.00745
- Reference count: 40
- Primary result: AtP* achieves 100% recall of top nodes with significantly lower computational cost than brute-force activation patching, especially for fine-grained components like MLP neurons and attention heads in large language models.

## Executive Summary
The paper introduces AtP*, an improved method for localizing transformer model behavior to individual components like attention heads and MLP neurons. AtP* addresses two key failure modes in the original AtP method: false negatives from attention saturation and cancellation between direct and indirect effects. Through systematic experiments across different model scales and tasks, the authors demonstrate that AtP* significantly outperforms alternative methods like Subsampling, Blocks, and Hierarchical approaches while providing theoretical guarantees on remaining false negatives.

## Method Summary
AtP* is a causal attribution method that uses first-order Taylor expansion to approximate the effect of intervening on individual nodes in a transformer model. The method introduces two key improvements over the original AtP: (1) recomputing attention softmax for queries and keys to avoid saturation issues, and (2) using dropout in the backward pass to mitigate cancellation between direct and indirect effects. The method is validated through comparison against ground truth obtained via exhaustive activation patching, measuring cost of verified recall (the number of forward passes needed to verify the top K nodes).

## Key Results
- AtP* achieves 100% recall of top nodes with significantly fewer forward passes than brute-force activation patching
- The QK fix (recomputing attention softmax) prevents false negatives in attention saturation regions
- GradDrop (dropout in backward pass) reduces cancellation between direct and indirect effects
- AtP significantly outperforms Subsampling, Blocks, and Hierarchical methods across all model scales tested

## Why This Works (Mechanism)

### Mechanism 1: QK Fix for Attention Saturation
- Claim: Recomputing attention softmax during patching prevents false negatives caused by attention saturation.
- Mechanism: When query/key patches occur in saturated softmax regions, gradients poorly approximate the true effect. Recomputing softmax using patched values gives a more accurate linear approximation.
- Core assumption: Attention saturation is a common failure mode in AtP that can be corrected by recomputing the softmax.
- Evidence anchors: [abstract], [section] discussion of normal activation patching and AtP's linear approximation, weak corpus support.

### Mechanism 2: GradDrop for Cancellation Reduction
- Claim: Using dropout on the backward pass reduces cancellation between direct and indirect effects.
- Mechanism: When direct and indirect effects nearly cancel, small approximation errors can make the total effect appear near zero. Zeroing gradients at intermediate layers breaks this cancellation pattern.
- Core assumption: Cancellation between direct and indirect effects is a significant failure mode that can be mitigated by disrupting the gradient flow.
- Evidence anchors: [abstract], [section] discussion of cancellation failure mode and proposed backpropagation modification, weak corpus support.

### Mechanism 3: Subsampling with Node Additivity
- Claim: Subsampling with node additivity assumption provides an unbiased estimator of node effects.
- Mechanism: By sampling subsets of nodes and comparing their effects with and without a target node, we can estimate the node's contribution while reducing computational cost.
- Core assumption: The effect of intervening on a set of nodes is approximately the sum of individual node effects (node additivity).
- Evidence anchors: [section] discussion of unbiased estimator construction and simple interaction model, weak corpus support.

## Foundational Learning

- **Activation Patching**: Baseline method that exhaustively patches individual nodes to measure their causal effect. Why needed: This is the ground truth method that AtP approximates. Quick check: What is the computational complexity of exhaustive activation patching versus AtP?

- **Gradient-based approximation**: AtP uses first-order Taylor expansion to approximate the effect of node interventions. Why needed: Understanding this approximation is crucial for implementing AtP and its variants. Quick check: Under what conditions does a first-order approximation fail to capture nonlinear effects?

- **Attention mechanisms in transformers**: Understanding how attention heads work is crucial for implementing the QK fix. Why needed: The QK fix specifically addresses failure modes in attention layers. Quick check: How does the softmax function in attention create saturation issues that affect gradient-based methods?

## Architecture Onboarding

- **Component map**: Input prompt pairs → Forward passes (clean and noise) → AtP/AtP* computation (with QK fix and/or GradDrop) → Ranked node importance estimates → Verification via activation patching

- **Critical path**: 
  1. Forward pass on clean prompt (cache activations)
  2. Forward pass on noise prompt (cache activations)
  3. Compute AtP estimates (with QK fix if applicable)
  4. Apply GradDrop if using AtP*
  5. Verify top nodes with activation patching

- **Design tradeoffs**:
  - AtP vs AtP*: AtP* has higher upfront cost but fewer false negatives
  - QK fix vs no QK fix: QK fix prevents saturation issues but adds computation
  - GradDrop vs no GradDrop: GradDrop prevents cancellation but requires multiple backward passes

- **Failure signatures**:
  - High proportion of false negatives in attention nodes suggests QK fix needed
  - Cancellation patterns in gradient estimates suggest GradDrop needed
  - Inconsistent results across prompt pairs suggest node additivity assumption may be violated

- **First 3 experiments**:
  1. Compare AtP vs AtP* on a simple IOI task with attention nodes
  2. Test QK fix effectiveness by measuring false negative reduction in attention heads
  3. Evaluate GradDrop impact by comparing cancellation rates with and without dropout

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does AtP* generalize to other transformer architectures beyond decoder-only models, such as encoder-decoder models or models with novel attention mechanisms?
- Basis in paper: [inferred] The paper focuses on decoder-only transformer language models and acknowledges the formalism is "straightforwardly applicable to other model classes" but does not empirically test this.
- Why unresolved: The authors only test on decoder-only transformers (Pythia models) and speculate about generalization without empirical validation.
- What evidence would resolve it: Systematic experiments applying AtP* to encoder-decoder models (like T5 or BART) and models with novel attention mechanisms (like Performer or RWKV) while comparing performance to ground truth.

### Open Question 2
- Question: What is the optimal block size and branching factor for the Blocks and Hierarchical methods across different model scales and task types?
- Basis in paper: [explicit] The authors sweep across block sizes {2, 6, 20, 60, 250} and branching factor B=3 for Hierarchical method, noting a tradeoff but not identifying optimal values.
- Why unresolved: The paper provides heuristic arguments for choosing B=3 but doesn't empirically determine optimal hyperparameters that balance computational cost and recall accuracy.
- What evidence would resolve it: Comprehensive ablation studies varying block sizes and branching factors across multiple model scales (including larger than 12B parameters) and diverse task types, measuring IRWRGM cost and recall at different thresholds.

### Open Question 3
- Question: How does the choice between noising and denoising affect the reliability of AtP* across different circuit types and model behaviors?
- Basis in paper: [explicit] The authors mention denoising is "also widely used in the literature" and provide preliminary evidence that AtP* performs worse with denoising on IOI, but leave thorough investigation to future work.
- Why unresolved: The paper only briefly explores denoising versus noising in Appendix B.4 without systematic comparison or explanation of why performance differs.
- What evidence would resolve it: Systematic comparison of AtP* performance using noising versus denoising across multiple circuit types (factual recall, multi-step reasoning, code generation) and model sizes, with analysis of when and why each approach succeeds or fails.

## Limitations

- The QK fix requires recomputing attention softmax, adding computational overhead that may be prohibitive for certain use cases
- The GradDrop approach assumes optimal dropout patterns that are not fully characterized
- The node additivity assumption underlying subsampling may break down for complex interaction effects between components
- Empirical evaluation is limited to next-token prediction tasks with Pythia model suite, limiting generalization claims

## Confidence

- **High confidence**: The identification of attention saturation as a failure mode in AtP, and the effectiveness of recomputing softmax for queries and keys. Well-supported by mathematical derivation and experimental results.
- **Medium confidence**: The GradDrop approach for reducing cancellation effects. The mechanism is sound but optimal implementation details and dropout patterns require further investigation.
- **Medium confidence**: The overall computational advantage of AtP* over brute-force activation patching, particularly for fine-grained components. Scaling behavior is demonstrated but may vary with different model architectures and tasks.

## Next Checks

1. **Cross-architecture validation**: Test AtP* on a diverse set of transformer architectures (e.g., BERT, GPT, T5) and tasks (e.g., classification, generation) to assess generalization beyond next-token prediction with Pythia models.

2. **Saturation prevalence analysis**: Systematically measure the frequency and impact of attention saturation across different model sizes, attention patterns, and prompt types to quantify when the QK fix provides the most benefit.

3. **Alternative cancellation mitigation strategies**: Compare GradDrop against other approaches for handling cancellation effects, such as adaptive learning rates or alternative gradient approximation methods, to identify the most robust solution.