---
ver: rpa2
title: 'The EarlyBird Gets the WORM: Heuristically Accelerating EarlyBird Convergence'
arxiv_id: '2406.11872'
source_url: https://arxiv.org/abs/2406.11872
tags:
- train
- worm
- pruning
- accuracy
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WORM is a pruning method that accelerates early-bird ticket search
  by truncating gradients of connections flagged for pruning, thereby forcing the
  model to rely on remaining connections earlier. It addresses the inefficiency of
  standard early-bird pruning, which delays pruning decisions until convergence.
---

# The EarlyBird Gets the WORM: Heuristically Accelerating EarlyBird Convergence

## Quick Facts
- arXiv ID: 2406.11872
- Source URL: https://arxiv.org/abs/2406.11872
- Authors: Adithya Vasudev
- Reference count: 21
- One-line primary result: WORM accelerates EarlyBird ticket search by truncating gradients of pruned connections, achieving faster convergence and better post-pruning accuracy on CNNs.

## Executive Summary
WORM is a method that accelerates EarlyBird ticket search by truncating gradients of connections flagged for pruning, forcing the network to rely on remaining connections earlier. It addresses the inefficiency of standard EarlyBird pruning, which delays pruning decisions until convergence. Experiments show WORM finds winning tickets faster and with better post-pruning accuracy on CNNs (ResNet-18 and VGG-11), reducing search epochs and improving robustness. On transformers (BERT), WORM achieves smaller accuracy drops but converges slower due to pre-trained information distortion. The method is generalizable across architectures but shows stronger benefits for CNNs than transformers.

## Method Summary
WORM modifies EarlyBird by applying gradient truncation when the average mask distance drops below a threshold (s + δ). It exploits the stability of pruning masks near convergence by zeroing gradients for weights flagged by the current mask, effectively removing them from training. This forces the network to embed learned information into surviving connections before actual pruning. The method uses representative layers (batch norm for CNNs, query/key layers for transformers) to compute mask distance and applies truncation only after detecting an "elbow" in the mask distance trajectory, balancing early stability exploitation with preserving learning dynamics.

## Key Results
- WORM reduces EarlyBird ticket search epochs on ResNet-18/CIFAR-10 and VGG-11 while maintaining or improving post-pruning accuracy.
- On BERT transformers, WORM achieves smaller accuracy drops post-pruning but converges slower due to pre-trained information distortion.
- The method shows stronger benefits for CNNs than transformers, with consistent speedup and robustness improvements.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Near convergence, EarlyBird masks stabilize, revealing static pruning patterns.
- Mechanism: The mask distance trajectory shows an elbow where the maximal mask distance plateaus close to the stopping threshold, indicating that a subset of masks within the sliding window becomes stable. This stability suggests consistent unimportant connections that can be identified early.
- Core assumption: Mask distance is a reliable proxy for subnetwork convergence and stability.
- Evidence anchors:
  - [section] "As shown above, a distinctive elbow occurs for a maximal mask distance value that is substantially close to the stopping value of 0.1... This elbow is matched by a similar phenomenon occurring for the trajectory of the average distance."
  - [abstract] "The EarlyBird hypothesis proposes an efficient algorithm to find these winning lottery tickets in convolutional neural networks, using the novel concept of distance between subnetworks to detect convergence in the subnetworks of a model."
- Break condition: If mask distance fluctuations remain high near the stopping threshold, the assumption of early stability fails.

### Mechanism 2
- Claim: Truncating gradients for pruned connections forces the network to rely on remaining connections earlier.
- Mechanism: By zeroing gradients for weights flagged by the current mask, WORM prevents these weights from updating, effectively removing them from training. This forces the network to embed learned information into the surviving connections before actual pruning.
- Core assumption: Gradient truncation mimics the effect of pruning without changing the architecture.
- Evidence anchors:
  - [section] "Formally, we can express gradient truncation using the following equation... ∇t(θ) = (r∇(θ) if m(θ) = 1, ∇(θ) if m(θ) = 0)."
  - [abstract] "We proposes WORM, a method that exploits these static groups by truncating their gradients, forcing the model to rely on other neurons."
- Break condition: If gradient truncation too early distorts essential learning, it may degrade convergence or final accuracy.

### Mechanism 3
- Claim: Delayed gradient truncation (after the elbow) balances early stability exploitation with preserving learning dynamics.
- Mechanism: WORM applies gradient truncation only after the average mask distance drops below a trigger point (s + δ), avoiding premature distortion while still leveraging stable masks.
- Core assumption: There exists a meaningful "elbow epoch" after which masks are stable enough to truncate safely.
- Evidence anchors:
  - [section] "Computing this elbow analytically is rather complex, and thus we take an experimental approach... we begin applying clipping when the average mask distance dips below a threshold."
  - [abstract] "WORM is a method that exploits these static groups by truncating their gradients, forcing the model to rely on other neurons."
- Break condition: If the elbow is misidentified, truncation may start too early (hurting accuracy) or too late (missing speedup).

## Foundational Learning

- Concept: Lottery Ticket Hypothesis
  - Why needed here: WORM builds on the premise that sparse subnetworks (winning tickets) exist in dense networks and can be identified without full training.
  - Quick check question: What does the Lottery Ticket Hypothesis claim about subnetworks in neural networks?

- Concept: EarlyBird Mask Distance Metric
  - Why needed here: WORM relies on EarlyBird's mask distance to detect convergence and stability of pruning masks.
  - Quick check question: How does EarlyBird use mask distance to determine when to stop ticket search?

- Concept: Gradient Truncation as a Pruning Proxy
  - Why needed here: WORM uses gradient truncation to simulate pruning effects during training, guiding the network to rely on important connections earlier.
  - Quick check question: What is the effect of setting the truncation parameter r to a very small value?

## Architecture Onboarding

- Component map:
  - EarlyBird ticket search (mask distance computation, sliding window)
  - Gradient truncation module (conditional application based on mask distance)
  - Pruning mask application (at convergence)
  - Retraining pipeline (fine-tuning after pruning)

- Critical path:
  1. Train and update weights.
  2. Prune representative layers (batch norm for CNNs, QK layers for transformers).
  3. Compute mask distance; update sliding window.
  4. If mask distance indicates stability (elbow), apply gradient truncation.
  5. Continue until convergence; output pruned mask.
  6. Retrain on pruned subnetwork.

- Design tradeoffs:
  - Early truncation risks distorting learning but gains speedup.
  - Late truncation preserves learning but may miss speedup.
  - Using BN layers for CNNs vs. QK layers for transformers affects representative layer choice.

- Failure signatures:
  - Slow convergence: truncation too aggressive or applied too early.
  - Accuracy drop: truncation distorts essential feature learning.
  - No speedup: truncation never triggered or ineffective.

- First 3 experiments:
  1. Run EarlyBird on ResNet-18/CIFAR-10; plot mask distance trajectory to identify elbow.
  2. Apply WORM with trigger point at elbow; compare ticket search epochs vs. EarlyBird.
  3. Evaluate post-pruning accuracy and retraining speed on VGG-11 and ResNet-18.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does WORM's gradient truncation strategy generalize to other transformer architectures beyond BERT, such as GPT or Vision Transformers?
- Basis in paper: [explicit] The paper states "WORM remains a promising first step for efficient transformer pruning methods" but notes it slowed convergence for BERT, suggesting architecture-specific considerations.
- Why unresolved: The paper only tested WORM on BERT with sentence classification heads, leaving generalization to other transformer variants unexplored.
- What evidence would resolve it: Experiments applying WORM to GPT, Vision Transformers, and other transformer architectures with varying pretraining schemes would demonstrate generalizability.

### Open Question 2
- Question: What is the optimal trigger point for gradient truncation in WORM, and can it be determined analytically rather than experimentally?
- Basis in paper: [explicit] The paper states "Computing this elbow analytically is rather complex" and uses an experimental approach with margin of error δ = 0.05.
- Why unresolved: The current method relies on empirical thresholds rather than theoretical determination of the optimal truncation start point.
- What evidence would resolve it: Mathematical analysis connecting mask distance dynamics to optimal truncation timing, validated through controlled experiments across architectures.

### Open Question 3
- Question: How does WORM interact with alternative pruning strategies like gradient pruning, structured pruning, or undecayed pruning?
- Basis in paper: [explicit] The conclusion mentions "investigating how WORM interacts with more complex pruning strategies" as future work.
- Why unresolved: The paper only tested WORM with unstructured L1-norm magnitude pruning, leaving interactions with other pruning methods unexplored.
- What evidence would resolve it: Comparative experiments applying WORM alongside different pruning algorithms to measure impacts on convergence speed, accuracy retention, and computational efficiency.

## Limitations

- WORM shows stronger speedup for CNNs than transformers, with slower convergence observed on BERT due to pre-trained information distortion.
- The exact trigger threshold (s + δ) and its sensitivity to model architecture remain underspecified, limiting reproducibility.
- Confidence in the claim that WORM universally improves retraining speed is low, as transformer results show slower convergence despite reduced search epochs.

## Confidence

- High: The core mechanism of WORM (gradient truncation based on mask distance stability) is well-supported and accelerates EarlyBird ticket search.
- Medium: The universality of WORM's benefits across architectures is uncertain, with mixed results between CNNs and transformers.
- Low: The claim of universal retraining speed improvement is not well-supported, particularly for transformers where convergence slows.

## Next Checks

1. Validate mask distance trajectory and elbow detection: Reproduce mask distance plots for ResNet-18/CIFAR-10 and BERT/GLUE to confirm the elbow occurs at a maximal distance close to the stopping threshold (0.1), ensuring the stability assumption holds.

2. Test gradient truncation sensitivity: Experiment with different trigger thresholds (δ values) and truncation parameters (r) to quantify their impact on convergence speed and accuracy, particularly for transformers where distortion effects are noted.

3. Benchmark retraining speed and robustness: Measure post-pruning retraining epochs and accuracy retention across diverse models (e.g., MobileNet, RoBERTa) to assess whether WORM's speedup generalizes beyond the tested CNNs and BERT.