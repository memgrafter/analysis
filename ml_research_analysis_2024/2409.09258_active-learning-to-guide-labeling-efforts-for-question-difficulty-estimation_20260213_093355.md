---
ver: rpa2
title: Active Learning to Guide Labeling Efforts for Question Difficulty Estimation
arxiv_id: '2409.09258'
source_url: https://arxiv.org/abs/2409.09258
tags:
- acquisition
- labeled
- level
- learning
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores active learning for Question Difficulty Estimation
  (QDE) to reduce labeling effort while maintaining predictive performance. It introduces
  PowerVariance, a novel acquisition function that extends the popular PowerBALD method
  to regression by leveraging Monte Carlo dropout to capture epistemic uncertainty.
---

# Active Learning to Guide Labeling Efforts for Question Difficulty Estimation

## Quick Facts
- arXiv ID: 2409.09258
- Source URL: https://arxiv.org/abs/2409.09258
- Reference count: 9
- Primary result: Novel PowerVariance acquisition function reduces labeling effort by 90% while maintaining QDE performance

## Executive Summary
This work introduces an active learning approach for Question Difficulty Estimation (QDE) that significantly reduces labeling costs while preserving predictive accuracy. The proposed PowerVariance acquisition function extends the PowerBALD method to regression tasks by combining Monte Carlo dropout uncertainty estimation with variance-based sampling. The approach iteratively selects the most informative unlabeled samples, minimizing redundancy and maximizing information gain during the labeling process.

## Method Summary
The study proposes PowerVariance, a novel acquisition function for active learning in QDE that builds upon the PowerBALD framework. The method employs Monte Carlo dropout to estimate epistemic uncertainty and combines it with a variance-based component to select informative samples for labeling. This iterative approach identifies samples that are both uncertain and diverse, aiming to reduce redundancy while maximizing information gain. The evaluation uses DistilBERT on the RACE++ dataset, comparing PowerVariance against uniform sampling and naive variance-based acquisition strategies.

## Key Results
- PowerVariance achieves a discrete RMSE score only 5% higher than fully supervised model after labeling just 10% of training data
- The proposed method significantly outperforms uniform and naive variance-based acquisition strategies
- Active learning approach reduces labeling effort by approximately 90% while maintaining competitive QDE performance

## Why This Works (Mechanism)
PowerVariance works by effectively capturing both epistemic uncertainty through Monte Carlo dropout and sample diversity through variance-based sampling. The acquisition function balances the exploration of uncertain regions with exploitation of diverse samples, preventing the model from repeatedly labeling similar, redundant examples. This dual focus on uncertainty and diversity enables more efficient use of labeling resources, allowing the model to learn more effectively from fewer labeled examples.

## Foundational Learning
- **Monte Carlo dropout**: Why needed - estimates model uncertainty by performing multiple forward passes with dropout enabled; Quick check - verify dropout is applied consistently during both training and inference
- **Epistemic uncertainty**: Why needed - captures model's lack of knowledge about its predictions; Quick check - confirm uncertainty estimates decrease as more data is labeled
- **Variance-based sampling**: Why needed - ensures selection of diverse samples rather than clustered ones; Quick check - measure pairwise distances between selected samples
- **Acquisition functions**: Why needed - guides the selection of informative samples for labeling; Quick check - validate that selected samples have higher uncertainty/variance than random samples
- **Active learning cycles**: Why needed - iterative process of labeling and retraining improves efficiency; Quick check - track performance improvement across labeling iterations
- **RACE++ dataset**: Why needed - provides standardized benchmark for QDE tasks; Quick check - verify dataset statistics match reported values

## Architecture Onboarding
Component map: RACE++ dataset -> DistilBERT model -> Monte Carlo dropout -> PowerVariance acquisition -> Oracle labeling -> Retrained model
Critical path: Data sampling -> Uncertainty estimation -> Acquisition function evaluation -> Oracle labeling -> Model retraining
Design tradeoffs: PowerVariance vs. simpler variance-only methods; Monte Carlo dropout vs. ensemble methods for uncertainty; computational cost of multiple forward passes vs. accuracy gains
Failure signatures: Poor performance if acquisition function selects redundant samples; failure to capture true epistemic uncertainty; computational bottleneck during multiple forward passes
First experiments: 1) Compare PowerVariance against uniform sampling baseline; 2) Evaluate Monte Carlo dropout uncertainty estimates on held-out data; 3) Test sensitivity to dropout rate hyperparameter

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation conducted on single dataset (RACE++) with specific model architecture (DistilBERT)
- Discrete RMSE metric requires further validation against established QDE evaluation standards
- Limited exploration of performance across different educational domains and question types

## Confidence
- High confidence: Monte Carlo dropout uncertainty estimation methodology and PowerVariance acquisition function design
- Medium confidence: Comparative performance results on RACE++, given single dataset evaluation
- Medium confidence: Claim of "state-of-the-art" performance, benchmarked only against uniform and naive variance baselines

## Next Checks
1. Test PowerVariance across multiple QDE datasets with varying characteristics (e.g., STEM vs humanities, multiple-choice vs open-ended questions)
2. Compare against additional active learning baselines including Bayesian Active Learning by Disagreement (BALD) and entropy-based methods
3. Conduct ablation studies to quantify contribution of Monte Carlo dropout uncertainty estimation versus variance-based component in PowerVariance