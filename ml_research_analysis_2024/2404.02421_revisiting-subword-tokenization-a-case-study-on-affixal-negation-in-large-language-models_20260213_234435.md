---
ver: rpa2
title: 'Revisiting subword tokenization: A case study on affixal negation in large
  language models'
arxiv_id: '2404.02421'
source_url: https://arxiv.org/abs/2404.02421
tags:
- negation
- affixal
- negative
- word
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how well modern large language models handle
  affixal negation, where negation is expressed through negative prefixes or suffixes.
  The authors design experiments to probe model awareness of negation in words like
  "uninteresting" or "effortless," where the negation is embedded within the word
  itself.
---

# Revisiting subword tokenization: A case study on affixal negation in large language models

## Quick Facts
- arXiv ID: 2404.02421
- Source URL: https://arxiv.org/abs/2404.02421
- Reference count: 21
- Large language models can infer negated meanings despite poor tokenization of negative affixes

## Executive Summary
This paper investigates how modern large language models handle affixal negation, where negation is expressed through negative prefixes or suffixes embedded within words. The authors find that despite current tokenization methods often failing to preserve negative affixes correctly, language models can still reliably infer the negated meaning through subword integration and contextual understanding. This reveals a disconnect between tokenization accuracy and model awareness of negation. The study also explores the impact on downstream sentiment analysis tasks, revealing that models tend to associate negation with negative sentiment, even when the overall sentiment may be positive.

## Method Summary
The authors design experiments to probe model awareness of affixal negation using a lexicon from van Son et al. (2016) containing 2089 affixal negations and 2055 non-negated words. They analyze the performance of various tokenization methods (BPE variants and Unigram LM) across multiple models including BERT, RoBERTa, XLNet, ALBERT, T5, LLaMA-2, GPT-2, and GPT-4. The study employs binary classification tasks for zero- and few-shot settings to test model awareness, and evaluates the impact on downstream sentiment analysis using datasets like SST-2 and Rotten Tomatoes. Token attribution analysis using the Integrated Gradient method is used to understand model predictions.

## Key Results
- T5 has the best performance in producing negative affix-preserving tokens, while models using Unigram LM method outperform those using BPE for most other models
- Model performance on affixal negation detection is significantly lower than on non-negated words, with the best models achieving near-perfect performance on non-negated counterparts
- Models consistently associate negative affixes with negative sentiment, introducing bias in sentiment analysis tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subword tokenization methods fail to preserve negative affixes morphologically, yet models can still infer negation meaning
- Mechanism: Even though tokenization breaks negative prefixes/suffixes into unrelated subwords, the model's internal representations and contextual understanding allow it to reconstruct the negated meaning
- Core assumption: Models can integrate subword embeddings and context to infer morphology despite poor tokenization
- Evidence anchors:
  - [abstract] "Despite some interesting mismatches between tokenization accuracy and negation detection performance, we show that models can, on the whole, reliably recognize the meaning of affixal negation."
  - [section 4.1] "T5 has the best performance in producing negative affix-preserving tokens, while for the remaining, models employing the unigram LM method outperform those using BPE."
- Break condition: If the model cannot integrate subword embeddings effectively, or if the affix is broken into tokens that are semantically unrelated to the root word, the model may fail to infer negation

### Mechanism 2
- Claim: Negative affixes are strong indicators of negative sentiment, but this creates a bias in models
- Mechanism: The presence of a negative affix leads models to predict negative sentiment, even when the overall sentiment may be positive
- Core assumption: Models have been exposed to sentiment analysis data where negation is conflated with negative sentiment
- Evidence anchors:
  - [section 4.3.1] "Previous work has shown that negation is a strong indicator of negative sentiment... This inspired us to extend our analysis to a downstream sentiment analysis task."
  - [section 5] "We observe high attribution scores from relevant tokens, such as the subword tokens of the target words... Negative affixes have flipping sentiment effect."
- Break condition: If the model is trained on a balanced dataset where negation is not always associated with negative sentiment, or if the model can effectively distinguish between negation and sentiment based on context

### Mechanism 3
- Claim: Correct tokenization leads to better model awareness of negation, but the effect is not significant
- Mechanism: When tokenization correctly preserves negative affixes, the model can more easily identify the negated meaning. However, even with incorrect tokenization, the model can still infer the meaning through subword integration
- Core assumption: The model's ability to infer meaning from subwords is robust enough to compensate for poor tokenization
- Evidence anchors:
  - [section 4.1] "We find that most [tokenization methods] do not effectively produce the correct negative affixes."
  - [section 4.2] "We see that the performance on Neg (the subset containing only affixal negation) is much lower compared to its Non-neg counterpart, where the best models achieve near-perfect performance."
- Break condition: If the model's subword integration is not robust enough, or if the affix is broken into tokens that are semantically unrelated to the root word, the model may fail to infer negation even with correct tokenization

## Foundational Learning

- Concept: Subword tokenization
  - Why needed here: Understanding how words are broken down into smaller units is crucial for analyzing the impact of tokenization on negation detection
  - Quick check question: What are the two main subword tokenization methods discussed in the paper, and how do they differ?

- Concept: Negation
  - Why needed here: Negation is the core linguistic phenomenon being studied, and understanding its various forms (e.g., syntactic negation vs. affixal negation) is essential for interpreting the results
  - Quick check question: What is affixal negation, and how does it differ from syntactic negation?

- Concept: Sentiment analysis
  - Why needed here: Sentiment analysis is used as a downstream task to measure the impact of affixal negation on model predictions
  - Quick check question: How does the presence of a negative affix affect sentiment prediction, and what bias does this introduce?

## Architecture Onboarding

- Component map: Tokenization Methods (BPE, Unigram LM) -> Language Models (BERT, RoBERTa, XLNet, ALBERT, T5, LLaMA-2, GPT-2, GPT-4) -> Tasks (Negation Prediction, Sentiment Analysis) -> Evaluation
- Critical path: Tokenization → Model → Task → Evaluation. The tokenization method affects the model's ability to understand negation, which in turn impacts performance on downstream tasks
- Design tradeoffs: Morphologically correct tokenization may improve model understanding of negation, but it may also increase vocabulary size and computational complexity. Incorrect tokenization may hinder understanding, but models can sometimes compensate through subword integration
- Failure signatures: Poor performance on negation prediction tasks, especially for words with negative affixes that are incorrectly tokenized. Bias in sentiment analysis, where words with negative affixes are consistently predicted as negative sentiment, even when the overall sentiment is positive
- First 3 experiments:
  1. Tokenization analysis: Analyze the performance of different tokenization methods in preserving negative affixes
  2. Negation prediction: Design a task to probe the model's awareness of affixal negation, both in a fine-tuned and zero/few-shot setting
  3. Sentiment analysis: Evaluate the impact of affixal negation on sentiment analysis tasks, both at the word and sentence level

## Open Questions the Paper Calls Out

- How do morphology-aware subword tokenization methods compare to standard methods (BPE, Unigram LM) in terms of handling affixal negation?
- What is the impact of affixal negation on tasks other than sentiment analysis, such as natural language inference or question answering?
- How does the presence of affixal negation in training data affect the performance of language models on negation-related tasks?

## Limitations

- The study focuses on a specific set of languages and tokenization methods, and findings may not generalize to all LLMs or linguistic phenomena
- The sentiment analysis experiments are based on limited datasets, and the observed bias towards negative sentiment for affixal negations may not hold across all sentiment analysis tasks
- The paper does not explore the impact of different training datasets on models' ability to handle affixal negation

## Confidence

- High confidence: LLMs can infer negated meanings despite poor tokenization of negative affixes
- Medium confidence: Negative affixes are strong indicators of negative sentiment, leading to a bias in sentiment analysis
- Low confidence: Correct tokenization leads to better model awareness of negation, but the effect is not significant

## Next Checks

1. Expand Language and Tokenization Method Coverage: Replicate the study with a broader range of languages and tokenization methods to assess the generalizability of the findings

2. Investigate Training Data Impact: Analyze the impact of different training datasets on the models' ability to handle affixal negation, including datasets with balanced sentiment annotations

3. Explore Alternative Sentiment Analysis Tasks: Evaluate the observed sentiment bias in affixal negations across a wider variety of sentiment analysis tasks and datasets to determine the extent of the bias