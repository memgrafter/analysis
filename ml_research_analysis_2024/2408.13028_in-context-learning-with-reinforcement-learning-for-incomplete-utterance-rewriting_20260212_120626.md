---
ver: rpa2
title: In-Context Learning with Reinforcement Learning for Incomplete Utterance Rewriting
arxiv_id: '2408.13028'
source_url: https://arxiv.org/abs/2408.13028
tags:
- examples
- utterance
- example
- task
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of incomplete utterance rewriting
  (IUR) in dialogue systems, where speakers often use incomplete utterances like co-reference
  or ellipsis. The authors propose a reinforcement learning-based framework (RLS)
  for selecting in-context examples to improve the performance of large language models
  (LLMs) on the IUR task.
---

# In-Context Learning with Reinforcement Learning for Incomplete Utterance Rewriting

## Quick Facts
- **arXiv ID**: 2408.13028
- **Source URL**: https://arxiv.org/abs/2408.13028
- **Reference count**: 7
- **Primary result**: RLS achieves 2.0-2.5 ROUGE and 1.3-2.5 BLEU improvements over baseline methods

## Executive Summary
This paper addresses the problem of incomplete utterance rewriting (IUR) in dialogue systems, where speakers often use incomplete utterances like co-reference or ellipsis. The authors propose a reinforcement learning-based framework (RLS) that selects optimal in-context examples to improve large language model (LLM) performance on the IUR task. RLS consists of a language model selector that encodes candidate examples and selects the top-k examples for the LLM, and an LLM generator that produces rewritten utterances. The outputs of the LLM are used to compute rewards and policy gradients to optimize the LM selector. Experiments on three benchmark datasets show that RLS significantly outperforms existing example selection methods, achieving up to 2.0-2.5 ROUGE score and 1.3-2.5 BLEU score improvements. The approach also shows advantages over supervised finetuning models in few-shot settings. Further analysis reveals that balancing the abundance and similarity of examples with the test case is important for improving LLM performance on the IUR task.

## Method Summary
The RLS framework uses a language model selector (BERT encoder) to encode candidate examples and select the top-k examples for the LLM generator. The LLM produces rewritten utterances, which are scored to compute rewards. These rewards are used to compute policy gradients and update the LM selector. The method operates on three benchmark datasets (CANARD, TASK, REWRITE) in English and Chinese, using ROUGE, BLEU, and F-score metrics for evaluation. The approach leverages policy gradient optimization to train the LM selector using rewards derived from LLM outputs without requiring labeled demonstration data.

## Key Results
- RLS achieves 2.0-2.5 ROUGE score improvements over baseline example selection methods
- RLS achieves 1.3-2.5 BLEU score improvements compared to existing approaches
- RLS shows advantages over supervised finetuning models in few-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Policy-based RL optimizes the LM selector by directly using LLM-generated rewards instead of proxy metrics.
- **Mechanism**: The LM selector encodes candidate examples into dense representations; selected examples form demonstrations for the LLM, whose outputs are scored to produce rewards. These rewards are used to compute policy gradients and update the LM selector.
- **Core assumption**: Rewards computed from LLM outputs correlate with improved analogy ability for the IUR task.
- **Evidence anchors**: Abstract states "The outputs of LLM are adopted to compute the reward and policy gradient to optimize the LM selector"; section 4.3 shows gradient computation explicitly uses LLM performance as reward.
- **Break condition**: If LLM outputs are noisy or rewards do not reflect true IUR quality, the selector optimization degrades.

### Mechanism 2
- **Claim**: Balancing example abundance (complexity) and similarity to test case improves LLM analogy ability.
- **Mechanism**: RLS selects examples with richer linguistic features (longer utterances, more POS types, more chunks) while maintaining semantic similarity, creating a more diverse and informative demonstration set.
- **Core assumption**: Demonstration diversity enhances LLM generalization for rewriting.
- **Evidence anchors**: Section 6.4 shows RLS examples have higher length, POS types, and chunk counts than baseline; selecting only by complexity or only by similarity hurts performance.
- **Break condition**: If complexity overwhelms similarity, demonstrations become irrelevant and performance drops.

### Mechanism 3
- **Claim**: Using an LM selector as a proxy for example selection is computationally cheaper than repeated LLM inference.
- **Mechanism**: BERT encodes examples once; the smaller LM selector is trained via RL, avoiding repeated large-scale LLM calls during selection.
- **Core assumption**: BERT-based encodings are sufficient to guide effective example selection.
- **Evidence anchors**: Section 5.4 uses SentenceBERT (English) or BERT-base-Chinese (Chinese) as LM selector; section 4.1 shows BERT encodes contexts and incomplete utterances into dense representations.
- **Break condition**: If BERT encodings poorly represent IUR-relevant features, proxy fails and selection quality suffers.

## Foundational Learning

- **Concept**: Reinforcement Learning Policy Gradient
  - **Why needed here**: To train the LM selector using rewards derived from LLM outputs without labeled demonstration data.
  - **Quick check question**: What is the role of the reward baseline in reducing variance in policy gradient updates?

- **Concept**: Dense Representation Retrieval
  - **Why needed here**: To compute semantic similarity between candidate examples and test cases for informed selection.
  - **Quick check question**: How does cosine similarity between BERT embeddings approximate semantic relevance?

- **Concept**: In-Context Learning (ICL)
  - **Why needed here**: The task requires the LLM to generate outputs based on demonstrations without weight updates.
  - **Quick check question**: Why does ICL performance depend heavily on the choice of demonstration examples?

## Architecture Onboarding

- **Component map**: BERT encoder → LM selector (RL policy) → LLM generator → reward scorer → gradient updater
- **Critical path**: Encode examples → select top-k → generate output → compute reward → update selector
- **Design tradeoffs**: Smaller LM selector reduces cost but may miss subtle semantic cues; larger demonstration sets improve diversity but increase input length limits
- **Failure signatures**: Low reward variance → selector not learning; LLM outputs off-topic → encoding or selection misaligned; overfitting to training examples → poor generalization
- **First 3 experiments**:
  1. Baseline: Random example selection and ICL performance
  2. Ablation: Fix selector, vary number of demonstrations, measure reward stability
  3. Complexity probe: Manually select high-complexity examples, compare to RLS performance

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the size of the candidate example set and training set impact the performance of RLS?
  - **Basis in paper**: The paper conducts experiments with different sizes of candidate and training sets (100-100, 100-500, 500-100, 500-500) and shows that RLS performance improves with more candidates and training samples.
  - **Why unresolved**: While the paper demonstrates the trend, it doesn't provide a detailed analysis of the relationship between set sizes and performance. It also doesn't explore the optimal size for different scenarios or datasets.
  - **What evidence would resolve it**: Conducting a more extensive range of experiments with varying set sizes and analyzing the performance curve to identify the point of diminishing returns. Additionally, investigating how the optimal set sizes might differ based on dataset characteristics or task complexity.

- **Open Question 2**: What is the impact of the order of examples in the demonstration on RLS performance?
  - **Basis in paper**: The paper conducts experiments comparing the performance of arranging examples in the demonstration by the order of sampling versus reverse order, showing comparable results.
  - **Why unresolved**: While the paper shows that the order doesn't significantly impact performance, it doesn't explore why this is the case or if there are any subtle effects that might be missed in the current analysis. It also doesn't investigate if certain types of examples benefit more from specific positions in the demonstration.
  - **What evidence would resolve it**: Conducting a more detailed analysis of the performance differences between different orderings, possibly using more sophisticated statistical methods. Investigating if certain types of examples (e.g., those with specific complexity or similarity metrics) benefit more from certain positions in the demonstration.

- **Open Question 3**: How does RLS compare to other example selection methods when applied to different types of LLM?
  - **Basis in paper**: The paper mentions that RLS is promising for deriving better results with stronger LLM, but only experiments with ChatGLM-7B and gpt3.5.
  - **Why unresolved**: The paper doesn't explore how RLS performs with other types of LLM, such as those with different architectures or training methods. It also doesn't investigate if RLS is particularly well-suited to certain types of LLM or if it has any limitations with specific architectures.
  - **What evidence would resolve it**: Conducting experiments with a diverse range of LLM, including those with different architectures (e.g., transformer-based, recurrent neural network-based) and training methods (e.g., supervised learning, reinforcement learning). Analyzing the performance of RLS across these different types of LLM to identify any patterns or limitations.

## Limitations
- Direct evidence of reward correlation with true IUR quality is weak; the link between policy gradient optimization and improved analogy ability is assumed rather than empirically validated
- Computational efficiency claims are based on model size assumptions rather than empirical measurement of actual inference time and resource usage
- The assumption that BERT-based encodings provide sufficient semantic representation for effective example selection is untested

## Confidence
- **High Confidence**: RLS achieves superior ROUGE/BLEU scores compared to baseline example selection methods on three benchmark datasets
- **Medium Confidence**: Balancing example complexity and similarity improves LLM performance, though causal mechanisms remain correlational
- **Low Confidence**: Policy-based RL optimization directly improves analogy ability for IUR; this connection is assumed but not empirically validated

## Next Checks
1. **Reward Quality Validation**: Correlate RLS-selected rewards with human judgments of IUR quality to verify that policy gradients optimize for task-relevant improvements rather than proxy metrics
2. **Computational Cost Analysis**: Measure actual inference time and resource usage comparing RLS against direct LLM example selection across different model sizes to validate claimed efficiency gains
3. **Robustness Testing**: Evaluate RLS performance when example diversity is artificially constrained (e.g., same utterance length, limited POS variety) to test the claimed importance of balancing abundance and similarity