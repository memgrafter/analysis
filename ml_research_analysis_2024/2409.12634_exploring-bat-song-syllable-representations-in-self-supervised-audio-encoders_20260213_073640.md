---
ver: rpa2
title: Exploring bat song syllable representations in self-supervised audio encoders
arxiv_id: '2409.12634'
source_url: https://arxiv.org/abs/2409.12634
tags:
- syllable
- speech
- audio
- self-supervised
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how well self-supervised audio encoders
  trained on human-generated sounds can distinguish between vocalization types of
  the Greater Sac-Winged Bat. The researchers analyzed bat song syllables from territorial
  recordings using four pre-trained models (two trained on human speech, one on animal
  vocalizations, and one on music) and two traditional feature sets (LFCC and MFCC).
---

# Exploring bat song syllable representations in self-supervised audio encoders

## Quick Facts
- arXiv ID: 2409.12634
- Source URL: https://arxiv.org/abs/2409.12634
- Reference count: 0
- Primary result: Self-supervised models trained on human speech generate the most distinctive representations of bat song syllables when vocalizations are slowed down to human hearing range

## Executive Summary
This study investigates how well self-supervised audio encoders trained on human-generated sounds can distinguish between vocalization types of the Greater Sac-Winged Bat. The researchers analyzed bat song syllables from territorial recordings using four pre-trained models (two trained on human speech, one on animal vocalizations, and one on music) and two traditional feature sets (LFCC and MFCC). They extracted 768-dimensional feature embeddings by passing slowed-down bat songs through each model and averaging frame representations within each syllable. Silhouette coefficients based on Mahalanobis distances showed that syllable type separability was highest in models trained on human speech (HuBERT: 0.86, Wav2Vec2: 0.84), followed by the animal vocalization model (0.81) and music model (0.75), while traditional features showed much lower separability (LFCC: 0.35, MFCC: 0.39).

## Method Summary
The study analyzed territorial songs of the Greater Sac-Winged Bat, extracting five syllable types from 20 recordings. Bat vocalizations were preprocessed by denoising, high-pass filtering at 10kHz, slowing down by factor 8 to move them into human hearing range, and downsampling to 16kHz. Four pre-trained self-supervised audio encoders (HuBERT-speech, Wav2Vec2-speech, Wav2Vec2-music, and AVES-animal) and two traditional feature sets (MFCC, LFCC) were used to extract 768-dimensional frame representations. These were averaged within each syllable, projected to 4 LDA dimensions, and evaluated using silhouette coefficients based on Mahalanobis distances between syllable type clusters.

## Key Results
- HuBERT and Wav2Vec2 models trained on human speech achieved the highest syllable type separability (0.86 and 0.84 silhouette coefficients)
- The animal vocalization model showed strong performance (0.81) despite being trained on fewer hours of audio
- Traditional MFCC and LFCC features performed substantially worse (0.39 and 0.35) than self-supervised representations
- The HuBERT architecture showed a slight advantage over Wav2Vec2, potentially due to its clustering objective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised models trained on human speech learn general acoustic representations that transfer effectively to bat vocalizations when the bat songs are slowed down into the human hearing range.
- Mechanism: The models' pre-training on human speech forces them to learn robust, hierarchical acoustic features (phonetic units, prosodic patterns) that are not species-specific but capture fundamental sound structure. When bat vocalizations are downsampled into this range, these learned representations can still discriminate syllable types because the underlying acoustic patterns share similarities with human speech.
- Core assumption: Acoustic features that are useful for human speech discrimination are also useful for bat syllable discrimination when the frequency range is aligned.
- Evidence anchors:
  - [abstract] "models pre-trained on human speech generate the most distinctive representations of different syllable types"
  - [section] "we move the songs into the human auditory range by slowing down all recordings... mean syllable duration is 235 ms... most energy is contained within the 1-8 kHz frequency band"
  - [corpus] Weak evidence - corpus contains unrelated music/lyrics papers, no direct support for cross-species transfer
- Break condition: If bat vocalizations contain frequency-dependent features that are lost or distorted when slowed down, the transfer would fail because the acoustic structure would no longer be preserved.

### Mechanism 2
- Claim: The HuBERT architecture's clustering objective creates more separable feature subspaces than Wav2Vec2, leading to better syllable discrimination.
- Mechanism: During HuBERT training, masked audio segments are predicted by matching them to cluster assignments from k-means applied to earlier model representations. This forces the model to organize its feature space into distinct clusters, which may generalize to creating more linearly separable representations for new acoustic categories like bat syllables.
- Core assumption: The clustering objective in HuBERT training induces a feature space structure that benefits out-of-distribution classification tasks.
- Evidence anchors:
  - [abstract] "HuBERT architecture showed a slight syllable separability advantage compared to the Wav2Vec2 architecture"
  - [section] "This could be due to the clustering objective that is part of the HuBERT training procedure, potentially driving the model's internal representations towards generally more separable subspaces"
  - [corpus] Weak evidence - corpus contains unrelated music/lyrics papers, no direct support for HuBERT's clustering advantage
- Break condition: If the clustering objective primarily benefits in-domain speech tasks but doesn't generalize to bioacoustic categories, the separability advantage would disappear for bat syllables.

### Mechanism 3
- Claim: Mean-pooling across time captures syllable-level information effectively despite ignoring temporal dynamics.
- Mechanism: Transformer-based embeddings have been shown to capture information across multiple timescales in human speech, including phoneme-level and word-level information. The mean-pooled representations from these layers can therefore encode sufficient information about syllable identity even without explicit temporal modeling.
- Core assumption: Mean-pooled Transformer embeddings contain enough temporal information to distinguish between different syllable types.
- Evidence anchors:
  - [section] "similar mean-pooled Transformer-based embeddings have been shown to successfully capture information across several timescales in human speech processing (for example on the phoneme- and word-level), and perform well on bioacoustic transfer learning tasks"
  - [abstract] No direct mention of mean-pooling effectiveness
  - [corpus] Weak evidence - corpus contains unrelated music/lyrics papers, no direct support for mean-pooling effectiveness
- Break condition: If bat syllables have distinctive temporal patterns (upsweeps vs. downsweeps) that are critical for classification, mean-pooling would lose this information and performance would degrade.

## Foundational Learning

- Concept: Self-supervised learning through masked prediction
  - Why needed here: The paper relies on models trained without bat vocalization labels, using self-supervised objectives to learn useful representations that can later be applied to bat syllables
  - Quick check question: What is the primary objective used to train HuBERT and Wav2Vec2 models on unlabeled audio data?

- Concept: Frequency domain vs. time domain representations
  - Why needed here: The study involves converting bat vocalizations (high-frequency) to human hearing range and extracting features like MFCC/LFCC that operate in the frequency domain
  - Quick check question: Why were the bat recordings slowed down by a factor of 8 before processing with the pre-trained models?

- Concept: Silhouette coefficient and cluster separability metrics
  - Why needed here: The paper uses silhouette coefficients based on Mahalanobis distances to quantify how well different syllable types are separated in the learned feature spaces
  - Quick check question: How is the silhouette coefficient calculated for each syllable type cluster in this study?

## Architecture Onboarding

- Component map: Audio preprocessing -> Feature extraction -> Mean-pooling -> LDA projection -> Silhouette coefficient calculation
- Critical path: Audio preprocessing → Feature extraction (model inference) → Mean-pooling across frames → LDA projection → Silhouette coefficient calculation
- Design tradeoffs:
  - Model choice: Human speech models perform best but are trained on different vocalizations; animal vocalization model is more domain-relevant but smaller
  - Feature aggregation: Mean-pooling is simple but loses temporal dynamics; more complex pooling might capture upsweeps/downsweeps better
  - Frequency conversion: Slowing down preserves some structure but may distort frequency-dependent features
- Failure signatures:
  - Low silhouette coefficients across all models suggest preprocessing issues or that syllable types are inherently not separable in the chosen feature space
  - One model performing significantly worse than others might indicate domain mismatch or insufficient training data for that model
  - Inconsistent results across repetitions might indicate sensitivity to preprocessing parameters
- First 3 experiments:
  1. Replicate the baseline results by running the four self-supervised models and two traditional features on the bat song dataset with identical preprocessing
  2. Test different downsampling factors (e.g., 4x, 6x, 8x, 10x) to find the optimal balance between preserving bat vocalization structure and matching human hearing range
  3. Compare mean-pooling with other temporal aggregation methods (max pooling, attention-based pooling, or keeping frame-level representations) to assess the impact of temporal information loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the superior performance of speech-trained models over the animal vocalization model result from dataset size differences rather than training domain?
- Basis in paper: [explicit] The authors note that the AVES model was pre-trained on a substantially smaller amount of audio than the speech and music models, and suggest a comparison against models pre-trained on fewer hours of speech would be needed to determine if dataset size explains the difference.
- Why unresolved: Direct comparison between models with matched training data sizes but different domains has not been conducted.
- What evidence would resolve it: Training a speech model on the same amount of data as AVES and comparing their bat syllable encoding performance.

### Open Question 2
- Question: Which specific acoustic features of bat syllables are most prominently encoded by different audio encoder architectures?
- Basis in paper: [inferred] The authors note that territorial songs encode singer identity and several other features, and plan to investigate what interpretable features contribute to distinctive syllable type representations across internal layers.
- Why unresolved: Current analysis only examines overall syllable type separability, not the specific acoustic dimensions being captured.
- What evidence would resolve it: Detailed feature importance analysis showing which acoustic parameters (fundamental frequency, spectral shape, temporal patterns) are prioritized by each model.

### Open Question 3
- Question: How do self-supervised audio encoders perform on other bioacoustic classification tasks beyond syllable type discrimination?
- Basis in paper: [explicit] The authors state they aim to test the applicability of their approach to other tasks such as syllable detection, species and dialect identification.
- Why unresolved: Current study only evaluates syllable type separability, not other downstream tasks.
- What evidence would resolve it: Comparative evaluation of encoder performance across multiple bioacoustic tasks including detection, classification, and speaker/dialect identification.

### Open Question 4
- Question: Does the HuBERT architecture's clustering objective specifically improve its ability to encode categorical vocalizations compared to Wav2Vec2?
- Basis in paper: [explicit] The authors observe that HuBERT showed a slight syllable separability advantage over Wav2Vec2 and suggest this could be due to HuBERT's clustering objective driving representations toward more separable subspaces.
- Why unresolved: No ablation studies have been conducted to isolate the effect of the clustering objective on vocalization encoding.
- What evidence would resolve it: Comparison of models with and without clustering objectives, or analysis of how clustering affects representations at different layers.

## Limitations

- The study relies on an untested assumption that slowing down bat vocalizations by factor 8 preserves the acoustic features necessary for syllable discrimination
- While the paper demonstrates quantitative separability, it does not validate whether these measures translate to actual classification performance on held-out data
- The superior performance of speech-trained models may be attributed to dataset size differences rather than the training domain

## Confidence

- **High confidence**: The experimental methodology is sound, with clear preprocessing steps and established metrics for measuring cluster separability
- **Medium confidence**: The observation that speech-trained models outperform music-trained models is well-supported, but the specific attribution to HuBERT's clustering objective versus Wav2Vec2's contrastive learning remains speculative
- **Low confidence**: The claim that these representations will generalize to other bioacoustic classification tasks across species is extrapolated without validation on additional vocalization types

## Next Checks

1. **Temporal structure validation**: Test whether preserving frame-level representations (instead of mean-pooling) improves silhouette coefficients, particularly for syllable types with distinctive upsweep/downsweep patterns
2. **Frequency conversion sensitivity**: Systematically vary the downsampling factor (4×, 6×, 8×, 10×) and measure how silhouette coefficients change to identify optimal frequency alignment for bat syllable discrimination
3. **Classification performance validation**: Train simple classifiers (SVM, logistic regression) on the extracted features to confirm that higher silhouette coefficients translate to improved out-of-sample syllable type classification accuracy