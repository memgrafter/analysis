---
ver: rpa2
title: How to Benchmark Vision Foundation Models for Semantic Segmentation?
arxiv_id: '2404.12172'
source_url: https://arxiv.org/abs/2404.12172
tags:
- performance
- semantic
- vfms
- segmentation
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to benchmark vision foundation models
  (VFMs) for semantic segmentation. The authors fine-tune various VFMs under different
  settings and assess the impact of individual settings on performance ranking and
  training time.
---

# How to Benchmark Vision Foundation Models for Semantic Segmentation?

## Quick Facts
- arXiv ID: 2404.12172
- Source URL: https://arxiv.org/abs/2404.12172
- Reference count: 38
- Key outcome: Recommends fine-tuning ViT-B variants with 16x16 patch size and linear decoder for efficient benchmarking, while using multiple datasets for training and evaluation due to varying performance rankings and domain shifts

## Executive Summary
This paper investigates how to effectively benchmark vision foundation models (VFMs) for semantic segmentation. The authors systematically evaluate different VFMs under various settings, including model architectures, pretraining objectives, fine-tuning strategies, and evaluation datasets. Their analysis reveals that pretraining with masked image modeling (MIM) is crucial for semantic segmentation performance, while promptable segmentation pretraining does not provide benefits. The study also demonstrates that linear probing is not representative of end-to-end fine-tuning, and that performance rankings can vary significantly across different datasets and domain shifts.

## Method Summary
The authors fine-tune 10 different VFMs on four semantic segmentation datasets (ADE20K, PASCAL VOC, Cityscapes, and GTA V) using Mask2Former architecture with both linear and transformer decoders. They evaluate models with ViT-B and ViT-L variants using 16x16 and 8x8 patch sizes, testing both end-to-end fine-tuning and linear probing strategies. Training is performed with AdamW optimizer, polynomial learning rate decay, and standard data augmentation techniques. Performance is measured using mIoU, and Kendall rank correlation coefficient is used to assess the consistency of performance rankings across different settings.

## Key Results
- Masked image modeling (MIM) pretraining with abstract representations is crucial for semantic segmentation, more important than the type of supervision used
- Promptable segmentation pretraining (SAM) does not benefit semantic segmentation downstream performance and can lead to negative transfer
- Linear probing is not representative of end-to-end fine-tuning for semantic segmentation, with low correlation coefficient (0.47) between rankings
- Performance rankings vary significantly across datasets, with different models excelling on different datasets
- ViT-B with 16x16 patch size and linear decoder is recommended for efficient benchmarking, reducing training time by over 13x

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked Image Modeling (MIM) with abstract representations is crucial for semantic segmentation performance, even more than the type of supervision used.
- Mechanism: MIM pretraining forces the model to reconstruct image content from masked patches, leading to rich, generalizable features that transfer well to semantic segmentation.
- Core assumption: Abstract representations (like those from MAE) capture meaningful semantic structure that benefits downstream segmentation.
- Evidence anchors:
  - [abstract] "masked image modeling (MIM) with abstract representations is crucial, even more important than the type of supervision used."
  - [section 4.2] "The highest performing models... have the pretraining objective of masked image modeling (MIM) with abstract representations in common... these findings suggest that MIM with abstract representations is a crucial pretraining objective for semantic segmentation, even more important than the type of supervision used."
  - [corpus] Weak: No direct neighbor papers discuss MIM vs supervision type comparison; corpus focuses on general VFM transfer.

### Mechanism 2
- Claim: Linear probing is not representative of end-to-end fine-tuning for semantic segmentation performance.
- Mechanism: Freezing the encoder during linear probing prevents adaptation of pretrained features to the specific characteristics of the segmentation task, causing a significant drop in performance and changing the performance ranking.
- Core assumption: Semantic segmentation requires fine-tuning of both encoder and decoder to adapt to pixel-level classification tasks.
- Evidence anchors:
  - [abstract] "Linear probing, a common practice for some VFMs, is not recommended, as it is not representative of end-to-end fine-tuning."
  - [section 4.1] "Current VFMs may not acquire enough spatial semantic understanding in pretraining to perform semantic segmentation effectively without end-to-end fine-tuning. Furthermore, a low correlation coefficient of 0.47 suggests freezing the encoder significantly changes the performance ranking."
  - [corpus] Weak: No neighbor papers explicitly compare linear probing vs end-to-end fine-tuning for VFMs in segmentation.

### Mechanism 3
- Claim: Promptable segmentation pretraining (SAM) does not benefit semantic segmentation downstream performance and can even lead to negative transfer.
- Mechanism: SAM's pretraining focuses on mask prediction without semantic labels, leading to models that lack semantic understanding required for semantic segmentation.
- Core assumption: Semantic consistency in pretraining masks is necessary for effective transfer to semantic segmentation.
- Evidence anchors:
  - [abstract] "The findings of such an analysis reveal that pretraining with promptable segmentation is not beneficial..."
  - [section 4.2] "SAM, despite being the only model pretrained with mask labels, performs poorly across most experiments... while mask labels arbitrarily belonging to, e.g., whole objects, parts or subparts, are beneficial for promptable segmentation, their semantic inconsistency may hinder the learning of effective features for semantic segmentation."
  - [corpus] Weak: No neighbor papers discuss SAM's transfer limitations for semantic segmentation.

## Foundational Learning

- Concept: Transfer learning effectiveness
  - Why needed here: The paper evaluates how well VFMs pretrained on one task transfer to semantic segmentation, requiring understanding of transfer learning principles.
  - Quick check question: Why might a model pretrained with mask labels but no semantic labels perform poorly on semantic segmentation?

- Concept: Benchmarking methodology
  - Why needed here: The paper develops recommendations for how to benchmark VFMs for semantic segmentation, requiring knowledge of experimental design and evaluation metrics.
  - Quick check question: What metrics and experimental controls are needed to fairly compare different VFMs for semantic segmentation?

- Concept: Domain adaptation and generalization
  - Why needed here: The paper investigates how performance rankings change across different datasets and domain shifts, requiring understanding of domain adaptation concepts.
  - Quick check question: Why might a model that performs well in-distribution perform poorly under domain shift?

## Architecture Onboarding

- Component map:
  - Encoder: Vision Transformer (ViT) backbone, various sizes (ViT-B, ViT-L), different pretraining objectives (MIM, supervised, etc.)
  - Decoder: Linear layer vs Mask2Former transformer decoder for semantic segmentation
  - Patch embedding: 16x16 vs 8x8 patch size
  - Fine-tuning strategy: End-to-end vs linear probing
  - Downstream datasets: ADE20K, PASCAL VOC, Cityscapes, domain shift with GTA V

- Critical path: Pretraining → Fine-tuning setup (encoder/decoder choice, patch size) → Evaluation (dataset choice, domain shift)

- Design tradeoffs:
  - Model size: Larger models (ViT-L) generally perform better but require more compute; ViT-B is recommended for efficient benchmarking
  - Patch size: Smaller patches (8x8) capture more detail but increase compute; 16x16 is recommended for efficient benchmarking
  - Decoder complexity: Mask2Former is more powerful but slower; linear decoder is recommended for efficient benchmarking
  - Fine-tuning strategy: End-to-end is more representative but slower; linear probing is faster but less representative

- Failure signatures:
  - Performance ranking changes significantly across datasets or domain shifts → dataset choice or domain generalization is critical
  - Linear probing shows large performance drops → end-to-end fine-tuning is necessary
  - Smaller patch sizes don't improve performance → model may not benefit from finer detail

- First 3 experiments:
  1. Fine-tune a ViT-B model with 16x16 patch size and linear decoder on ADE20K using end-to-end fine-tuning to establish baseline
  2. Compare end-to-end fine-tuning vs linear probing on the same model and dataset to validate linear probing is not representative
  3. Test the same model with Mask2Former decoder vs linear decoder to confirm linear decoder is sufficient for benchmarking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pretraining with promptable segmentation improve performance on semantic segmentation tasks?
- Basis in paper: [explicit] The paper explicitly states that pretraining with promptable segmentation is not beneficial for semantic segmentation.
- Why unresolved: While the paper shows SAM, pretrained with promptable segmentation, performs poorly, it does not explore why this is the case or whether different pretraining strategies within promptable segmentation could yield better results.
- What evidence would resolve it: Testing different pretraining strategies within promptable segmentation or comparing SAM's performance with other promptable segmentation models could provide insights into whether promptable segmentation pretraining can be beneficial for semantic segmentation.

### Open Question 2
- Question: How does the performance ranking of vision foundation models for semantic segmentation change when using more limited downstream data?
- Basis in paper: [inferred] The paper mentions that linear probing was investigated in a 1-shot setting, but the impact of more limited downstream data on the performance ranking is unclear.
- Why unresolved: The paper focuses on end-to-end fine-tuning and does not explore how the performance ranking changes when fine-tuning with fewer labeled examples.
- What evidence would resolve it: Fine-tuning vision foundation models with varying amounts of downstream data and comparing the performance rankings would provide insights into how the models perform under different data constraints.

### Open Question 3
- Question: What are the interactions between different benchmark settings and how do they impact the performance ranking of vision foundation models for semantic segmentation?
- Basis in paper: [explicit] The paper mentions that interactions between settings were not considered due to the combinatorial explosion of experiments.
- Why unresolved: The paper investigates the impact of individual benchmark settings on the performance ranking, but does not explore how these settings interact with each other.
- What evidence would resolve it: Conducting experiments that vary multiple benchmark settings simultaneously and analyzing the resulting performance rankings would provide insights into the interactions between settings and their combined impact on model performance.

## Limitations

- The study is based on experiments with 10 VFMs across 4 datasets, which may not capture all possible VFM architectures and downstream tasks
- The recommendation for linear decoder with ViT-B 16x16 may not generalize to all semantic segmentation scenarios, particularly those requiring fine-grained detail
- The claim that MIM is "even more important than the type of supervision used" is supported by strong evidence but remains a comparative statement rather than an absolute ranking

## Confidence

- **High**: Linear probing is not representative of end-to-end fine-tuning for semantic segmentation
- **High**: SAM pretraining does not benefit semantic segmentation downstream performance
- **Medium**: MIM with abstract representations is crucial for semantic segmentation performance
- **Medium**: ViT-B with 16x16 patch size and linear decoder is sufficient for efficient benchmarking

## Next Checks

1. Test the benchmarking recommendations on additional VFM architectures (e.g., CLIP-based models, newer MAE variants) to verify generalizability
2. Evaluate performance across a wider range of semantic segmentation tasks beyond the current datasets, particularly those with different resolution requirements and domain characteristics
3. Conduct ablation studies on different pretraining objectives while controlling for architecture to isolate the specific contribution of MIM to downstream performance