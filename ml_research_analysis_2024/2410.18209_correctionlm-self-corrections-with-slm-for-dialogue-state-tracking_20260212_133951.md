---
ver: rpa2
title: 'CorrectionLM: Self-Corrections with SLM for Dialogue State Tracking'
arxiv_id: '2410.18209'
source_url: https://arxiv.org/abs/2410.18209
tags:
- correction
- language
- dialogue
- in-context
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORRECTIONLM is a self-correction framework that enables small
  language models (SLMs) to correct their own outputs using in-context exemplars without
  relying on large language models (LLMs). Applied to dialogue state tracking (DST)
  in low-resource settings, the method uses a two-pass approach where an initial prediction
  is made and then corrected using examples of corrections during fine-tuning.
---

# CorrectionLM: Self-Corrections with SLM for Dialogue State Tracking

## Quick Facts
- arXiv ID: 2410.18209
- Source URL: https://arxiv.org/abs/2410.18209
- Authors: Chia-Hsuan Lee; Hao Cheng; Mari Ostendorf
- Reference count: 23
- Primary result: CORRECTIONLM achieves 56.20% Joint Goal Accuracy on MultiWOZ 2.4 with 5% training data, outperforming LLM baselines

## Executive Summary
CORRECTIONLM is a self-correction framework that enables small language models (SLMs) to correct their own outputs using in-context exemplars without relying on large language models (LLMs). Applied to dialogue state tracking (DST) in low-resource settings, the method uses a two-pass approach where an initial prediction is made and then corrected using examples of corrections during fine-tuning. This approach achieves results comparable to state-of-the-art LLMs while requiring significantly less computationâ€”approximately 1.7 million TeraFLOPs compared to 8.3 million for LLM-based approaches.

## Method Summary
CORRECTIONLM employs a two-pass approach for dialogue state tracking where an initial prediction is made using in-context exemplars with a frozen SLM, then corrected using a second SLM fine-tuned on correction demonstrations. The framework collects initial predictions across training data to identify common error patterns, then uses parameter-efficient QLoRA to fine-tune a second SLM specifically to correct these mistakes. The correction SLM receives augmented prompts with both original examples and correction demonstrations during inference. Applied to MultiWOZ 2.4 and SGD datasets with 5% training data, the method demonstrates strong performance in low-resource settings while reducing computational costs by approximately 80% compared to LLM-based approaches.

## Key Results
- Achieves 56.20% Joint Goal Accuracy on MultiWOZ 2.4 with 5% training data
- Outperforms single-pass LLM baselines while using only 1.7 million TeraFLOPs versus 8.3 million
- Shows particular effectiveness in out-of-domain scenarios with largest performance improvements in in-domain conversations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-correction improves accuracy by capturing and correcting SLM-specific error patterns
- Mechanism: The framework first generates initial predictions using a frozen SLM, then collects these predictions across the training set to identify common error modes. These error patterns are then used to fine-tune a second SLM specifically to correct these mistakes using in-context exemplars.
- Core assumption: SLMs have predictable error patterns that can be captured and corrected through exposure to their own mistakes
- Evidence anchors:
  - [abstract] "CORRECTIONLM, a novel correction framework that enables SLMs to self-correct using in-context exemplars without LLM involvement"
  - [section 2] "The primary objective of DST is to extract and convert task-relevant details from these user-system interactions into structured representations"
  - [corpus] Weak evidence - corpus shows related work on SLM correction but no direct evidence of error pattern capture

### Mechanism 2
- Claim: In-context learning with correction demonstrations improves SLM performance without full fine-tuning
- Mechanism: Instead of fine-tuning on all data, the framework uses parameter-efficient QLoRA to update only adapter weights, while incorporating correction demonstrations (initial prediction + gold label pairs) into the in-context examples during training
- Core assumption: In-context learning remains effective even when the model has been partially fine-tuned on correction patterns
- Evidence anchors:
  - [abstract] "CORRECTIONLM achieves results similar to a state-of-the-art LLM at a small fraction of the computation costs"
  - [section 3.2] "Building on these findings, we introduce a correction-augmented ICL tuning approach"
  - [corpus] Moderate evidence - corpus shows parameter-efficient fine-tuning works for SLMs but limited evidence for correction-specific ICL

### Mechanism 3
- Claim: Two-pass inference with correction SLM provides better results than single-pass approaches
- Mechanism: First pass generates initial predictions, second pass uses correction-tuned SLM with augmented in-context examples that include both original examples and correction demonstrations to refine the initial output
- Core assumption: The correction SLM can effectively distinguish between correct and incorrect predictions and make appropriate changes
- Evidence anchors:
  - [abstract] "CORRECTIONLM achieves results similar to a state-of-the-art LLM at a small fraction of the computation costs"
  - [section 3.1] "Unlike previous work (Yu et al., 2024b), no rationale is used in our second pass prompting"
  - [corpus] Strong evidence - corpus shows two-pass approaches improve accuracy but limited evidence for SLM-specific implementations

## Foundational Learning

- Concept: Dialogue State Tracking (DST)
  - Why needed here: The paper applies CORRECTIONLM to DST tasks, so understanding the task is fundamental
  - Quick check question: What is the difference between turn-level belief (TLB) and dialogue state (DST) in this context?

- Concept: In-Context Learning (ICL)
  - Why needed here: The framework relies heavily on ICL for both initial predictions and corrections
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of parameter updates?

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: The correction SLM uses QLoRA, a PEFT method, to minimize computational costs
  - Quick check question: What components of the model are actually updated during QLoRA fine-tuning?

## Architecture Onboarding

- Component map: Retriever (SenBERT) -> Inference SLM (Llama-3-8B) -> Correction SLM (QLoRA-fine-tuned)

- Critical path:
  1. Retriever selects k examples based on context similarity
  2. Inference SLM generates initial TLB predictions
  3. Correction SLM receives augmented prompt with correction demonstrations
  4. Correction SLM outputs refined predictions

- Design tradeoffs:
  - Computational efficiency vs. accuracy: Using 8B SLM instead of LLM reduces FLOPs by ~80%
  - Fine-tuning scope vs. generalization: QLoRA limits updates to adapters to preserve base capabilities
  - Example quantity vs. quality: k=10 for MultiWOZ, k=3 for SGD based on complexity

- Failure signatures:
  - Low correction acceptance rate: Model is either too conservative or predictions are already accurate
  - High correction acceptance with low accuracy gain: Model is making unnecessary changes
  - Retriever collapse: Examples become too similar, reducing diversity in in-context learning

- First 3 experiments:
  1. Verify retrieval quality by measuring semantic similarity between retrieved examples and test queries
  2. Test correction acceptance rate by comparing initial predictions to corrected outputs
  3. Validate computational efficiency by measuring actual FLOPs during inference versus claimed estimates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CORRECTIONLM vary with different sizes of in-context exemplars, and what is the optimal number of examples for maximizing correction effectiveness?
- Basis in paper: [explicit] The paper uses k=10 examples for MultiWOZ and k=3 for SGD, but does not explore the impact of varying these numbers.
- Why unresolved: The paper does not provide an analysis of how changing the number of in-context exemplars affects the model's correction performance.
- What evidence would resolve it: Conducting experiments with varying numbers of in-context exemplars (e.g., k=5, k=15) and comparing the correction effectiveness across these settings.

### Open Question 2
- Question: Can CORRECTIONLM's self-correction framework be effectively extended to tasks beyond dialogue state tracking, such as coding or mathematical reasoning?
- Basis in paper: [inferred] The paper focuses on dialogue state tracking but mentions the potential for extension to other tasks requiring multi-step reasoning.
- Why unresolved: The framework's effectiveness on other complex tasks is untested, and the paper does not provide evidence of its applicability beyond DST.
- What evidence would resolve it: Applying CORRECTIONLM to other complex tasks and evaluating its performance compared to existing methods.

### Open Question 3
- Question: What are the specific error types that CORRECTIONLM is most effective at correcting, and how does this vary between in-domain and out-of-domain scenarios?
- Basis in paper: [explicit] The paper discusses CORRECTIONLM's performance in out-of-domain evaluation but does not provide a detailed breakdown of error types corrected.
- Why unresolved: While the paper highlights improvements in out-of-domain scenarios, it does not specify which error types are most effectively addressed.
- What evidence would resolve it: Conducting a detailed error analysis to categorize and quantify the types of errors corrected by CORRECTIONLM in both in-domain and out-of-domain settings.

## Limitations

- Computational cost estimation uncertainty: The claimed 80% reduction in FLOPs may not account for fine-tuning overhead or varying inference batch sizes