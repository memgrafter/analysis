---
ver: rpa2
title: 'On the Similarity of Circuits across Languages: a Case Study on the Subject-verb
  Agreement Task'
arxiv_id: '2410.06496'
source_url: https://arxiv.org/abs/2410.06496
tags:
- gemma
- figure
- attention
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the circuits responsible for subject-verb
  agreement in multilingual language models, specifically comparing English and Spanish.
  The authors analyze Gemma 2B and find that both languages rely on a highly consistent
  circuit, primarily driven by attention head L13H7 writing a 'subject number' signal
  to the last residual stream.
---

# On the Similarity of Circuits across Languages: a Case Study on the Subject-verb Agreement Task

## Quick Facts
- arXiv ID: 2410.06496
- Source URL: https://arxiv.org/abs/2410.06496
- Reference count: 21
- Key outcome: English and Spanish subject-verb agreement in Gemma models rely on a consistent circuit driven by L13H7 attention head encoding subject number as a direction in the residual stream, read by specific MLP neurons.

## Executive Summary
This paper investigates the circuits responsible for subject-verb agreement in multilingual language models, specifically comparing English and Spanish. The authors analyze Gemma 2B and find that both languages rely on a highly consistent circuit, primarily driven by attention head L13H7 writing a 'subject number' signal to the last residual stream. This signal is represented as a direction in the residual stream space and is read by specific neurons in the final MLPs. The authors demonstrate that this direction has a causal effect on model predictions, successfully flipping Spanish verb number predictions by intervening with the English direction. They also provide evidence of similar behavior in other Gemma model sizes (2B, 7B, and 2 2B), suggesting this may be a fundamental architectural feature for handling grammatical number across languages.

## Method Summary
The authors use activation patching, direct logit attribution, attention pattern analysis, and activation steering to investigate subject-verb agreement circuits in Gemma 2B for English and Spanish. They identify key components through causal interventions and analyze the consistency of circuits across languages by comparing the attention head outputs and MLP neuron activations.

## Key Results
- Both English and Spanish rely on the same circuit, primarily driven by attention head L13H7 writing a 'subject number' signal to the last residual stream.
- The subject number signal is represented as a single direction (PC1) in the residual stream space, making it language-independent.
- Activation steering along this direction successfully flips verb number predictions across languages, demonstrating causal effects.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The subject number signal is encoded as a single direction in the residual stream, making it language-independent.
- Mechanism: Attention head L13H7 outputs a vector that, when projected onto a specific direction (PC1), distinguishes singular from plural subjects. This direction is consistent across English and Spanish datasets.
- Core assumption: The model represents grammatical number in a linear subspace that is shared across languages.
- Evidence anchors:
  - [abstract] "Notably, this subject number signal is represented as a direction in the residual stream space, and is language-independent."
  - [section 4] "We compute the dot product between the output of attention head L13H7 at the last position and column 2069 of Win (Win[:, 2069]) across the whole dataset and show the results in Figure 4."
  - [corpus] Weak evidence; related work focuses on syntactic probing but not this specific directional encoding.
- Break condition: If the dot product pattern breaks for non-English, non-Spanish languages or if PCA reveals multiple components instead of PC1 dominating.

### Mechanism 2
- Claim: A specific neuron in the final MLP reads the subject number direction and writes token predictions that match the grammatical number.
- Mechanism: Neuron 2069 in MLP13 activates based on the dot product of L13H7's output with its input weights. Positive activations promote plural verb forms; negative activations promote singular verb forms.
- Core assumption: The neuron's weights are aligned with the subject number direction such that its activation directly controls verb number prediction.
- Evidence anchors:
  - [section 4] "When the subject is singular, we get a negative dot product (activation) and promote singular verb forms (Table 1). When the subject is plural, we get positive dot product values and promote plural forms."
  - [abstract] "both circuits are highly consistent, being mainly driven by a particular attention head writing a ‘subject number’ signal to the last residual stream, which is read by a small set of neurons in the final MLPs."
  - [corpus] Weak evidence; related work on syntactic agreement neurons exists but not this specific neuron-direction interaction.
- Break condition: If other neurons in MLP13 dominate the logit difference or if the neuron's activation pattern becomes noisy across examples.

### Mechanism 3
- Claim: Activation steering along the subject number direction has a causal effect on verb number predictions across languages.
- Mechanism: By adding or subtracting the PC1 direction (found in English) to the attention head output in Spanish examples, the model's verb number prediction flips accordingly.
- Core assumption: The subject number direction is causally linked to the model's output and is not just a correlational artifact.
- Evidence anchors:
  - [abstract] "We demonstrate that this direction has a causal effect on the model predictions, effectively flipping the Spanish predicted verb number by intervening with the direction found in English."
  - [section 5] "Results show that adding PC1English successfully flips the Spanish verb number prediction to plural (Figure 7 (a)) on examples with singular subject, and that subtracting PC1English flips the Spanish plural number prediction to singular."
  - [corpus] Weak evidence; related work on activation steering exists but not for cross-lingual grammatical number.
- Break condition: If steering does not consistently flip predictions or if top non-verb tokens change significantly.

## Foundational Learning

- Concept: Residual stream and attention mechanisms in Transformers
  - Why needed here: The analysis relies on understanding how attention heads write to and read from the residual stream, and how this affects downstream predictions.
  - Quick check question: In a Transformer, what happens to the residual stream after an attention head processes it?

- Concept: Principal Component Analysis (PCA) for dimensionality reduction
  - Why needed here: PCA is used to identify the dominant direction (PC1) that encodes the subject number signal in the attention head outputs.
  - Quick check question: If a dataset has a strong linear trend, which principal component will capture most of the variance?

- Concept: Causal intervention and activation patching
  - Why needed here: These techniques are used to determine which components have a causal effect on the model's predictions by swapping activations between clean and corrupted inputs.
  - Quick check question: In activation patching, what does it mean if patching a component increases the logit difference between correct and incorrect predictions?

## Architecture Onboarding

- Component map: Embedding layer -> 14 attention blocks (each with multi-head attention and MLP) -> final unembedding layer. The critical circuit involves attention head L13H7, neuron 2069 in MLP13, and the residual stream at the last position.
- Critical path: Subject noun → L13H7 attention output → residual stream → neuron 2069 in MLP13 → logit difference favoring correct verb number.
- Design tradeoffs: Using a single direction for subject number allows for language independence but may limit the model's ability to represent more complex grammatical features. The gated MLP architecture enables neurons to take large positive and negative values, which is crucial for this mechanism.
- Failure signatures: If the circuit fails, you might see: (1) No clear PC1 direction in attention head outputs, (2) Multiple neurons in MLP13 contributing equally to logit difference instead of one dominant neuron, (3) Activation steering not flipping predictions consistently.
- First 3 experiments:
  1. Perform PCA on L13H7 outputs for both English and Spanish datasets and verify that PC1 separates singular and plural subjects.
  2. Compute the dot product of L13H7 outputs with Win[:, 2069] across examples to confirm the activation pattern matches subject number.
  3. Apply activation steering by adding/subtracting PC1English to L13H7 outputs in Spanish examples and measure the change in verb number predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do other transformer architectures beyond the Gemma family exhibit the same language-independent subject number direction in their subject-verb agreement circuits?
- Basis in paper: [explicit] The paper states "These findings suggest that the mechanism for handling subject-verb agreement is largely preserved across different scales and iterations of the Gemma model family" but acknowledges "we focused solely on Gemma models" and "Results obtained on these models do not guarantee they generalize to other model families."
- Why unresolved: The study only examined Gemma models (2B, 7B, and 2 2B). No testing was done on other architectures like GPT, Llama, or BERT families.
- What evidence would resolve it: Testing the same circuit analysis methodology on other transformer architectures would reveal whether the language-independent subject number direction is a general feature of transformer language models or specific to the Gemma architecture.

### Open Question 2
- Question: How do more linguistically distant languages (e.g., Chinese, Arabic, Finnish) interact with this subject-verb agreement circuit compared to English and Spanish?
- Basis in paper: [inferred] The paper states "our study is limited to two languages: English and Spanish" and "we cannot conclude that the same applies to all other languages, particularly those that are more linguistically distant."
- Why unresolved: The current study only tested languages with similar grammatical structures for subject-verb agreement (both have grammatical number and similar verb conjugation patterns).
- What evidence would resolve it: Applying the same circuit analysis to languages with different agreement systems (e.g., languages without grammatical number, languages with different word orders, or languages with complex agreement patterns) would reveal the circuit's universality.

### Open Question 3
- Question: What training data patterns lead to the emergence of this language-independent subject number direction in the residual stream?
- Basis in paper: [inferred] The paper demonstrates the existence of the direction but doesn't explore how it develops during training or what data patterns encourage its formation.
- Why unresolved: The study focuses on analyzing the trained model but doesn't investigate the training process or data distribution that leads to this specific circuit architecture.
- What evidence would resolve it: Training multiple models with different data distributions, data augmentations, or training curricula while monitoring the development of this circuit would reveal what patterns in the training data promote this language-independent representation.

## Limitations

- Generalizability Across Languages: The study only examines English and Spanish, both Indo-European languages with similar grammatical number systems. The circuit's applicability to languages with different agreement patterns remains unknown.
- Model Architecture Specificity: The findings are based on Gemma 2B and its variants. The circuit may be specific to this architecture or to models trained with similar objectives.
- Robustness to Input Variations: The analysis focuses on controlled subject-verb agreement tasks. The circuit's performance may degrade with more complex sentences or different word orders.

## Confidence

- **High Confidence**: The identification of L13H7 as the key attention head and neuron 2069 in MLP13 as the critical readout mechanism for subject-verb agreement in Gemma 2B.
- **Medium Confidence**: The claim that the subject number signal is encoded as a single direction (PC1) that is language-independent.
- **Low Confidence**: The assertion that this circuit is a "fundamental architectural feature" for handling grammatical number across languages.

## Next Checks

1. **Cross-Linguistic Validation**: Test the circuit's consistency in languages with different grammatical number systems (e.g., Japanese, which lacks grammatical number, or Arabic, which has dual number) to assess the generalizability of the findings.

2. **Architecture Comparison**: Investigate whether other Transformer-based models (e.g., GPT-2, BERT) use a similar circuit for subject-verb agreement, or if different architectures implement this task using distinct mechanisms.

3. **Robustness Testing**: Evaluate the circuit's performance on more complex sentences with intervening phrases, relative clauses, or non-standard word orders to determine its robustness to real-world linguistic variations.