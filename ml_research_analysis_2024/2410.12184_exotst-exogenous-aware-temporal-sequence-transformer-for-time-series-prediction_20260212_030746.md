---
ver: rpa2
title: 'ExoTST: Exogenous-Aware Temporal Sequence Transformer for Time Series Prediction'
arxiv_id: '2410.12184'
source_url: https://arxiv.org/abs/2410.12184
tags:
- exogenous
- series
- time
- past
- drivers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ExoTST introduces a transformer-based framework for time series
  prediction that effectively integrates past and current exogenous variables with
  historical endogenous data. The method employs two encoders for past and current
  exogenous series, a cross-temporal modality fusion module for efficient information
  exchange, and an endogenous decoder for prediction.
---

# ExoTST: Exogenous-Aware Temporal Sequence Transformer for Time Series Prediction

## Quick Facts
- arXiv ID: 2410.12184
- Source URL: https://arxiv.org/abs/2410.12184
- Reference count: 27
- Primary result: Up to 10% improvement in prediction accuracy compared to state-of-the-art baselines

## Executive Summary
ExoTST introduces a transformer-based framework for time series prediction that effectively integrates past and current exogenous variables with historical endogenous data. The method employs two encoders for past and current exogenous series, a cross-temporal modality fusion module for efficient information exchange, and an endogenous decoder for prediction. By treating past and current exogenous variables as distinct modalities and using cross-attention mechanisms, ExoTST captures complex temporal dependencies while maintaining robustness to data uncertainties.

## Method Summary
ExoTST processes time series data through a multi-stage pipeline: first, historical endogenous data and both past and current exogenous drivers are divided into overlapping patches. Two separate encoders process the past and current exogenous series using self-attention mechanisms, with each producing an aggregation token that summarizes patch-level information. A cross-temporal modality fusion module then uses these aggregation tokens in a cross-attention mechanism to exchange information between past and current exogenous representations. The endogenous decoder integrates this fused exogenous information with historical endogenous values through self-attention and cross-attention layers to generate predictions. The model is trained using MSE loss with Adam optimizer and early stopping based on validation performance.

## Key Results
- ExoTST achieves up to 10% improvement in MSE and MAE compared to state-of-the-art baselines
- The framework demonstrates superior performance on carbon flux datasets and standard time series benchmarks
- Shows consistent performance across both short-term and long-term forecasting horizons while handling missing values and noise effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ExoTST integrates past and current exogenous variables by treating them as separate modalities and using cross-temporal fusion.
- Mechanism: The model uses two encoders to process past and current exogenous series separately, then fuses their representations using a cross-attention-based module. This allows each modality to inform the other without losing temporal distinctions.
- Core assumption: Past and current exogenous variables have different distributions and should be modeled separately before fusion.
- Evidence anchors:
  - [abstract] "This module enables the model to jointly learn from both past and current exogenous series, treating them as distinct modalities."
  - [section] "By treating the past and projected exogenous series as different modalities, our approach offers flexibility and robustness in handling noise and uncertainties in exogenous data."
- Break condition: If the distribution shift between past and current exogenous variables is minimal, the separation may add unnecessary complexity.

### Mechanism 2
- Claim: Patch-wise self-attention enables efficient long-range temporal modeling while reducing computational overhead.
- Mechanism: The input time series is divided into overlapping patches, each mapped to a token. This reduces the sequence length from L to approximately L/S, lowering attention complexity from O(L²) to O((L/S)²).
- Core assumption: Local temporal patterns within patches are meaningful and can be aggregated for global understanding.
- Evidence anchors:
  - [section] "Utilizing patches significantly reduces the number of input tokens from L to approximately L/S... which simplifies the attention maps' computational complexity by a factor of S."
- Break condition: If temporal dependencies span patch boundaries extensively, important context may be lost.

### Mechanism 3
- Claim: The aggregation token (eagg) acts as a context carrier that efficiently exchanges information between past and current exogenous encodings.
- Mechanism: Each encoder produces an eagg token that attends to all patch tokens. During fusion, these eagg tokens serve as queries in cross-attention, reducing complexity from quadratic to linear.
- Core assumption: The eagg token can adequately summarize patch-level information for cross-modality communication.
- Evidence anchors:
  - [section] "By utilizing ep agg exclusively for queries, both computational and memory complexities of the attention map A are reduced to linear, as opposed to the quadratic complexity seen in full attention mechanisms."
- Break condition: If the eagg token cannot capture sufficient detail, important patch-level nuances may be lost in fusion.

## Foundational Learning

- Concept: Temporal sequence modeling
  - Why needed here: The task requires understanding and predicting future values based on past patterns in time series data.
  - Quick check question: What is the difference between autoregressive and forward modeling approaches in time series?

- Concept: Attention mechanisms in deep learning
  - Why needed here: Attention allows the model to focus on relevant parts of the input sequence when making predictions.
  - Quick check question: How does multi-head attention differ from single-head attention?

- Concept: Transformer architecture
  - Why needed here: The entire ExoTST framework is built on transformer blocks, requiring understanding of encoder-decoder structures.
  - Quick check question: What are the three main sub-layers in each transformer decoder layer?

## Architecture Onboarding

- Component map:
  Endogenous series + Past exogenous series + Current exogenous series -> Patching -> Two encoders -> Cross-temporal fusion module -> Endogenous decoder -> Predictions

- Critical path:
  1. Patch input series
  2. Encode past exogenous series
  3. Encode current exogenous series
  4. Fuse encodings via cross-temporal module
  5. Decode with fused exogenous info + past endogenous values
  6. Generate predictions

- Design tradeoffs:
  - Using separate encoders for past/current exogenous variables adds complexity but handles distribution shifts better
  - Patch-based approach reduces computation but may lose boundary-crossing dependencies
  - Aggregation tokens reduce fusion complexity but may oversimplify

- Failure signatures:
  - Poor performance on long horizons may indicate insufficient temporal context capture
  - Degradation with missing values suggests the fusion module isn't robust enough
  - Performance gap between short and long predictions may indicate issues with temporal generalization

- First 3 experiments:
  1. Test ablation of the cross-temporal fusion module (use simple concatenation instead)
  2. Vary patch size and stride to find optimal balance between efficiency and context
  3. Evaluate performance on synthetic data with known distribution shifts between past and current exogenous variables

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the limitations and scope of the work, several important open questions emerge:

1. How does ExoTST perform when the exogenous drivers are completely missing for extended periods, rather than just having random missing values or noise?
2. What is the theoretical explanation for why the cross-temporal modality fusion module improves prediction accuracy compared to simpler concatenation approaches?
3. How does ExoTST scale to extremely long time series (e.g., daily data spanning decades) where the number of patches becomes computationally prohibitive?

## Limitations

- The separation of past and current exogenous variables assumes significant distribution shifts, which may not hold for all applications
- The patch-based approach with fixed parameters (P=16, S=8) may not generalize optimally across domains with different temporal granularities
- The aggregation token mechanism could lose fine-grained patch-level information critical for certain forecasting tasks

## Confidence

- Claims about ExoTST's effectiveness are High confidence for the core methodological contributions (the two-encoder architecture and cross-temporal fusion module)
- Confidence is Medium for the performance claims due to the limited number of datasets and lack of comparison against more recent transformer-based methods

## Next Checks

1. Test ExoTST on datasets with known temporal misalignment between past and current exogenous variables to verify the robustness claim
2. Compare against recent state-of-the-art transformer methods on standard forecasting benchmarks
3. Conduct ablation studies on patch size (P) and stride (S) parameters to determine optimal configurations for different domain characteristics