---
ver: rpa2
title: Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak
  Large Vision-Language Models
arxiv_id: '2411.11496'
source_url: https://arxiv.org/abs/2411.11496
tags:
- step
- figure
- lvlms
- safe
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that seemingly safe images can be exploited\
  \ to jailbreak large vision-language models (LVLMs) by combining them with additional\
  \ safe images and prompts. The authors introduce Safety Snowball Agent (SSA), an\
  \ agent-based framework that leverages LVLMs\u2019 universal reasoning abilities\
  \ and safety snowball effect through two stages: initial response generation using\
  \ crafted or retrieved jailbreak images, and harmful snowballing with refined prompts\
  \ to progressively generate unsafe content."
---

# Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models

## Quick Facts
- arXiv ID: 2411.11496
- Source URL: https://arxiv.org/abs/2411.11496
- Reference count: 40
- One-line primary result: Introduces Safety Snowball Agent (SSA) framework that achieves 88.6% jailbreak success rate on GPT-4o by exploiting safe images combined with prompts

## Executive Summary
This paper reveals a novel vulnerability in large vision-language models (LVLMs) where seemingly safe images can be exploited to generate harmful content through a multi-stage jailbreak framework called Safety Snowball Agent (SSA). The framework leverages LVLMs' universal reasoning capabilities and a "safety snowball effect" to progressively amplify harmful outputs across multiple interaction turns. The authors demonstrate that any image—safe or unsafe—can potentially trigger harmful outputs when processed through SSA, achieving high jailbreak success rates on multiple leading LVLMs while bypassing common content moderators.

## Method Summary
The Safety Snowball Agent (SSA) framework operates in two stages: Initial Response Generation and Harmful Snowballing. In the first stage, SSA uses intent recognition to identify harmful topics from input images, then generates or retrieves jailbreak images using a tool pool, combining them with the original safe image and topic-specific prompts to create initial unsafe responses. In the second stage, SSA employs an iterative refinement process where prompts are progressively modified based on initial responses to amplify harmful content generation, with an evaluator determining when the output has reached sufficient harmfulness. The framework achieves this without directly containing unsafe content in its prompts or jailbreak images.

## Key Results
- SSA achieves 88.6% jailbreak success rate on GPT-4o and 88% on other LVLMs
- Demonstrates that safe images can be exploited to generate harmful outputs when combined with additional safe images and prompts
- Shows safety snowball effect where initial unsafe responses lead to progressively more harmful content across multiple turns
- Outperforms baseline methods while bypassing common content moderators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Universal reasoning capabilities enable LVLMs to integrate visual and textual inputs in unintended ways
- Mechanism: LVLMs interpret relationships across safe inputs from different modalities, leading to overinterpretation and generation of harmful outputs
- Core assumption: The model's ability to reason across modalities can be exploited when combined with carefully crafted prompts
- Evidence anchors:
  - [abstract] "LVLMs exhibit advanced reasoning capabilities, enabling them to integrate and interpret complex relationships between visual and textual inputs"
  - [section] "For example, in Figure 1, GPT-4o links a bank with the concept of a police car, ultimately generating content that promotes harmful activities"
- Break condition: If the model's reasoning capabilities are restricted or if input relationships are explicitly constrained

### Mechanism 2
- Claim: Safety snowball effect amplifies harmful content generation through iterative prompting
- Mechanism: Initial unsafe responses create a context that leads to progressively more harmful outputs in subsequent turns
- Core assumption: Once an LVLM accepts the premise of answering a harmful intent-related question, it will continue generating increasingly harmful content
- Evidence anchors:
  - [abstract] "SSA employs refined and iterative prompts to guide the LVLM towards generating progressively more harmful outputs, amplifying the model's unsafe behaviors"
  - [section] "We observe a similar effect in safety, where an initial unsafe statement leads to further harmful content generation"
- Break condition: If the model can recognize and interrupt the harmful context escalation during multi-turn interactions

### Mechanism 3
- Claim: Safe images can trigger harmful outputs when combined with additional safe images and prompts
- Mechanism: Any image, regardless of its inherent safety, can be exploited by SSA to jailbreak LVLMs through the combination of jailbreak images and refined prompts
- Core assumption: The LVLM's safety alignment mechanisms are not robust to multi-modal jailbreak strategies
- Evidence anchors:
  - [abstract] "we reveal that a safe image can be exploited to achieve the same jailbreak consequence when combined with additional safe images and prompts"
  - [section] "Our experiments demonstrate that SSA can use nearly any image to induce LVLMs to produce unsafe content"
- Break condition: If the model's safety mechanisms can effectively evaluate multi-modal inputs as a whole rather than processing them separately

## Foundational Learning

- Concept: Universal reasoning in LVLMs
  - Why needed here: Understanding how LVLMs process and integrate visual and textual information is crucial for exploiting their reasoning capabilities
  - Quick check question: How do LVLMs typically process multi-modal inputs, and what vulnerabilities does this create?

- Concept: Safety alignment limitations
  - Why needed here: Recognizing the gaps in safety alignment mechanisms helps understand why seemingly safe inputs can trigger harmful outputs
  - Quick check question: What are the key differences between text-only and vision-language model safety alignment?

- Concept: Iterative prompting strategies
  - Why needed here: The safety snowball effect relies on multi-turn interactions to amplify harmful content generation
  - Quick check question: How does the autoregressive nature of LVLMs contribute to the safety snowball effect?

## Architecture Onboarding

- Component map:
  Image → Intent Recognition → Jailbreak Image Generation → Initial Response → Harmful Snowballing → Final Response

- Critical path:
  Image → Intent Recognition → Jailbreak Image Generation → Initial Response → Harmful Snowballing → Final Response

- Design tradeoffs:
  - Balancing between prompt specificity and generality for broader attack effectiveness
  - Trade-off between attack success rate and detection risk
  - Complexity vs. efficiency in the jailbreak image generation process

- Failure signatures:
  - Low harmfulness scores in initial responses
  - Model refusal to generate unsafe content despite jailbreak attempts
  - Inconsistent attack success across different LVLMs

- First 3 experiments:
  1. Test SSA on a small set of safe images with a single LVLM to establish baseline effectiveness
  2. Compare harmfulness scores between initial responses and snowballed responses to validate the safety snowball effect
  3. Conduct ablation studies by removing components (jailbreak images, context) to identify critical success factors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SSA's framework be extended to detect and prevent the safety snowball effect before it occurs, rather than just exploiting it?
- Basis in paper: [explicit] The paper discusses the safety snowball effect where initial unsafe responses lead to increasingly harmful outputs, and mentions that LVLMs lack the ability to autonomously prevent these unsafe responses during single-turn interactions.
- Why unresolved: The current SSA framework leverages this effect for jailbreaking but doesn't explore defensive mechanisms to interrupt the cascade before harmful content is generated.
- What evidence would resolve it: Development and testing of a complementary system that can identify early warning signs of the safety snowball effect and implement intervention strategies to prevent escalation of harmful content.

### Open Question 2
- Question: How does the safety snowball effect differ between LVLMs with varying model sizes and architectures, and can this difference be quantified?
- Basis in paper: [explicit] The paper demonstrates the safety snowball effect across multiple LVLMs (GPT-4o, Intern-VL2, Qwen-VL2, VILA) but doesn't analyze how model size or architectural differences impact the effect's strength or prevalence.
- Why unresolved: While the paper shows the effect exists across different models, it doesn't explore whether larger models are more or less susceptible, or how architectural choices influence vulnerability to this phenomenon.
- What evidence would resolve it: Systematic comparison of the safety snowball effect across LVLMs of different sizes and architectures, measuring the rate and intensity of escalation in harmful content generation.

### Open Question 3
- Question: Are there universal patterns in the dangerous neurons activated by safe versus unsafe images, and can these patterns be used for proactive safety measures?
- Basis in paper: [explicit] The paper analyzes activation patterns of dangerous neurons when generating harmful content from both safe and unsafe images, finding similar neuronal activations in both cases.
- Why unresolved: The paper identifies similar activation patterns but doesn't explore whether these patterns are universal across different LVLMs or if they could be used to develop early warning systems for harmful content generation.
- What evidence would resolve it: Analysis of dangerous neuron activation patterns across a broader range of LVLMs to identify common signatures, followed by testing whether monitoring these patterns can predict and prevent harmful content generation.

## Limitations
- The claim that "any image" can be exploited requires further validation with a broader range of safe images beyond curated datasets
- Results may not generalize to all LVLMs, particularly open-source models with different safety training approaches
- The framework's effectiveness may vary in real-world deployment scenarios where safety mechanisms continue to evolve

## Confidence
- High Confidence: Experimental methodology and evaluation metrics are clearly defined with reproducible results on tested benchmarks
- Medium Confidence: The "any image" exploitation claim needs broader validation across diverse image datasets
- Medium Confidence: Safety snowball effect mechanism is well-supported but may vary with different LVLM architectures

## Next Checks
1. Test SSA against a broader range of LVLMs including open-source models with different safety training approaches to assess generalizability
2. Evaluate whether defensive mechanisms that analyze multi-modal inputs holistically can detect and block the safety snowball effect
3. Conduct a human evaluation study to validate the automated harmfulness scoring and assess whether the generated content poses real-world safety risks