---
ver: rpa2
title: Towards a Better Evaluation of Out-of-Domain Generalization
arxiv_id: '2405.19703'
source_url: https://arxiv.org/abs/2405.19703
tags:
- measure
- egiven
- worst
- groupdro
- ideal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating domain generalization
  (DG) algorithms, where models are trained to perform well on unseen test distributions.
  The authors argue that the commonly used average measure is insufficient and propose
  a new worst+gap measure that considers both predictive power and invariance.
---

# Towards a Better Evaluation of Out-of-Domain Generalization

## Quick Facts
- arXiv ID: 2405.19703
- Source URL: https://arxiv.org/abs/2405.19703
- Reference count: 40
- One-line primary result: The worst+gap measure improves domain generalization evaluation by capturing both predictive power and invariance, outperforming average measure in correlation with ideal measure across multiple datasets.

## Executive Summary
This paper addresses the problem of evaluating domain generalization (DG) algorithms, where models are trained to perform well on unseen test distributions. The authors argue that the commonly used average measure is insufficient and propose a new worst+gap measure that considers both predictive power and invariance. Theoretical justification is provided through two theorems derived from different assumptions, showing the necessity of including both worst and gap terms. Extensive experiments are conducted on modified datasets (SR-CMNIST, C-Cats&Dogs, L-CIFAR10, PACS-corrupted, and VLCS-corrupted) to compare the proposed measure with the average measure. The results demonstrate that the worst+gap measure correlates better with the ideal measure and is more effective in selecting the best-performing algorithm, especially when distributional discrepancies are significant.

## Method Summary
The paper proposes a new evaluation measure for domain generalization algorithms called worst+gap, which combines the worst leave-one-environment-out error (predictive power) with the gap between worst and best errors (invariance). The measure is theoretically justified through two theorems assuming uniform distribution and decreasing range, respectively. The authors conduct extensive experiments on five modified datasets with controlled spurious correlations, comparing the worst+gap measure against the average measure using Spearman's rank correlation and Kendall's tau with the ideal measure. The experiments involve 14 DomainBed algorithms with default hyperparameters and leave-one-environment-out validation.

## Key Results
- The worst+gap measure consistently shows higher correlation with the ideal measure than the average measure across all tested datasets and configurations
- Theoretical theorems provide justification for including both worst and gap terms under different distributional assumptions
- The worst+gap measure is more effective in selecting the best-performing domain generalization algorithm, particularly when distributional discrepancies are significant

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The worst+gap measure improves domain generalization evaluation by explicitly capturing both predictive power and invariance.
- Mechanism: By combining the worst leave-one-environment-out error (predictive power) with the gap between worst and best errors (invariance), the measure better reflects a model's ability to generalize across unseen domains.
- Core assumption: Both the worst performance and the gap between best and worst performances contain meaningful information about true domain generalization ability.
- Evidence anchors:
  - [abstract] "We propose worst+gap measure, which encompasses both predictive power and invariance."
  - [section 3.1] "The worst error and the gap can be considered as the predictive power and the environment invariance, respectively."
- Break condition: If worst performance is dominated by noise or if the gap does not correlate with distributional shift severity, the measure may mislead.

### Mechanism 2
- Claim: Theoretical grounding through two theorems from different assumptions justifies the inclusion of both worst and gap terms.
- Mechanism: Theorem 3.3 assumes uniform distribution and shows worst error relates to ideal measure; Theorem 3.4 assumes decreasing range and directly bounds ideal measure by worst+gap. Both converge on the same insight.
- Core assumption: The true domain generalization performance can be approximated by worst and gap terms under reasonable distributional assumptions.
- Evidence anchors:
  - [section 3.3.1] "According to Theorem 3.3, the worst error term is directly derived from the ideal measure."
  - [section 3.3.2] "Unlike Theorem 3.3, Theorem 3.4 directly demonstrates that the ideal measure max_e∈Eall Re(f) is upper-bounded by the weighted summation of the worst error and the gap."
- Break condition: If the assumptions (uniform distribution or decreasing range) are violated in real-world data, the theoretical justification weakens.

### Mechanism 3
- Claim: The proposed measure better correlates with the ideal measure across diverse dataset configurations than the average measure.
- Mechanism: Extensive experiments on SR-CMNIST, C-Cats&Dogs, L-CIFAR10, PACS-corrupted, and VLCS-corrupted datasets show consistent superiority of worst+gap in Spearman's ρ and Kendall's τ correlation with the ideal measure.
- Core assumption: Better correlation with the ideal measure implies better practical performance in selecting domain generalization algorithms.
- Evidence anchors:
  - [section 5.1] "First of all, it can be observed that RIDEAL_A has a stronger correlation with RW+G_A than RA_VG_A for every single scenario."
  - [section 5.3] "Similar to the previous experiments, RW+G_A exhibits a higher correlation with RIDEAL_A for the real-world datasets."
- Break condition: If the ideal measure itself is flawed or if the experimental setup does not capture real-world domain generalization challenges, correlation superiority may not translate to practical benefit.

## Foundational Learning

- Concept: Leave-one-environment-out (LOO) cross-validation
  - Why needed here: The proposed measure and the average measure both rely on LOO to estimate generalization performance when only limited environments are available.
  - Quick check question: How does LOO differ from standard k-fold cross-validation, and why is it appropriate for domain generalization evaluation?

- Concept: Spearman's rank correlation and Kendall's tau
  - Why needed here: These correlation metrics are used to compare how well different evaluation measures align with the ideal measure.
  - Quick check question: What is the key difference between Spearman's ρ and Kendall's τ, and when might one be preferred over the other?

- Concept: Spurious vs. invariant correlations in domain generalization
  - Why needed here: Understanding these concepts is crucial for interpreting why the worst+gap measure better captures true generalization performance.
  - Quick check question: In the CMNIST example, what constitutes the spurious correlation and what is the invariant feature that should be learned?

## Architecture Onboarding

- Component map: Dataset creation module -> Model training with 14 DomainBed algorithms -> LOO evaluation -> Measure calculation -> Correlation analysis -> Algorithm selection

- Critical path: Dataset creation → Model training with 14 DomainBed algorithms → LOO evaluation → Measure calculation → Correlation analysis → Algorithm selection

- Design tradeoffs:
  - Computational cost: Worst+gap requires computing both worst and best errors, increasing computation slightly over average measure
  - Interpretability: Average measure is simpler to understand but less informative; worst+gap requires understanding two components
  - Sensitivity: Worst+gap may be more sensitive to outliers in worst performance; average measure may be more stable but less discriminative

- Failure signatures:
  - Poor correlation with ideal measure despite theoretical justification
  - Inconsistent algorithm selection across different dataset configurations
  - Sensitivity to the number of environments (N) when N is small

- First 3 experiments:
  1. Verify measure calculation: Compute both average and worst+gap measures on a simple synthetic dataset with known spurious correlations
  2. Correlation analysis: Calculate Spearman's ρ and Kendall's τ between measures and ideal measure on SR-CMNIST with varying Scale and Ratio
  3. Algorithm selection comparison: Compare which algorithms are selected as best by average vs. worst+gap measures on C-Cats&Dogs dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the worst+gap measure perform when comparing more complex DG algorithms beyond those tested in DomainBed?
- Basis in paper: [inferred] The paper tests 14 DomainBed algorithms, but does not explore more advanced or recent DG methods.
- Why unresolved: The evaluation is limited to a fixed set of algorithms, leaving open whether the measure remains effective for a broader range of techniques.
- What evidence would resolve it: Testing the worst+gap measure on a wider variety of DG algorithms, including recent or more complex approaches, and comparing correlation with the ideal measure.

### Open Question 2
- Question: Does the worst+gap measure maintain its effectiveness as the number of environments (N) increases beyond the scales tested?
- Basis in paper: [explicit] The paper discusses asymptotic behavior and tests up to a certain scale, but does not explore very large N values.
- Why unresolved: The paper shows convergence for tested scales, but does not provide evidence for extremely large N.
- What evidence would resolve it: Conducting experiments with significantly larger numbers of environments and analyzing the measure's performance and correlation with the ideal measure.

### Open Question 3
- Question: How sensitive is the worst+gap measure to the choice of Ratio (majority vs. minority environments) in real-world datasets?
- Basis in paper: [explicit] The paper investigates Ratio in synthetic datasets but does not deeply explore its impact on real-world datasets.
- Why unresolved: The paper tests Ratio in controlled settings but does not fully explore its effects on more complex, real-world data.
- What evidence would resolve it: Analyzing the worst+gap measure's performance across different Ratio configurations in real-world datasets and comparing with the ideal measure.

## Limitations

- The theoretical justification relies on specific distributional assumptions (uniform distribution, decreasing range) that may not hold in all real-world scenarios
- The paper does not explore the measure's performance with more advanced or recent domain generalization algorithms beyond those in DomainBed
- Limited exploration of how the measure performs with very large numbers of environments (N)

## Confidence

- Theoretical justification: Medium (Theorems 3.3 and 3.4 provide independent derivations but rely on specific assumptions)
- Empirical findings: High (Extensive experiments across five datasets show consistent superiority of worst+gap)
- Practical superiority claims: Medium-Low (Correlation with ideal measure is demonstrated, but longer-term deployment validation is lacking)

## Next Checks

1. Test worst+gap measure on additional real-world datasets beyond PACS and VLCS to assess generalizability
2. Evaluate whether worst+gap-selected algorithms maintain their superiority after extended training or in production settings
3. Analyze sensitivity to the number of environments (N) to determine minimum requirements for reliable measure estimation