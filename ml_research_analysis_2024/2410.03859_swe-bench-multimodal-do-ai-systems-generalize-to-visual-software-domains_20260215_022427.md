---
ver: rpa2
title: 'SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?'
arxiv_id: '2410.03859'
source_url: https://arxiv.org/abs/2410.03859
tags:
- swe-bench
- task
- instances
- images
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SWE-bench Multimodal (SWE-bench M), a benchmark
  for evaluating AI systems on visual JavaScript software engineering tasks. SWE-bench
  M contains 617 task instances from 17 JavaScript libraries, with each instance including
  at least one image in its problem statement or unit tests.
---

# SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?

## Quick Facts
- arXiv ID: 2410.03859
- Source URL: https://arxiv.org/abs/2410.03859
- Reference count: 40
- Top-performing SWE-bench systems resolve only 12% of tasks on SWE-bench M compared to 43% on SWE-bench

## Executive Summary
This paper introduces SWE-bench Multimodal (SWE-bench M), a benchmark for evaluating AI systems on visual JavaScript software engineering tasks. The benchmark contains 617 task instances from 17 JavaScript libraries, with each instance including at least one image in its problem statement or unit tests. The authors found that top-performing SWE-bench systems struggle significantly on SWE-bench M, revealing limitations in visual problem-solving and cross-language generalization of current AI systems. SWE-agent, adapted with multimodal capabilities, substantially outperforms alternatives on SWE-bench M, highlighting the importance of flexible, language-agnostic features for software engineering agents.

## Method Summary
The authors created SWE-bench M by scraping GitHub PRs from 17 JavaScript repositories, filtering for instances with visual content, and validating task consistency through human annotation. They adapted three baseline systems (SWE-agent, Agentless, and RAG) for JavaScript by replacing Python-specific components with JavaScript equivalents, including a custom JavaScript parser. The systems were evaluated using GPT-4o and Claude 3.5 Sonnet on both SWE-bench M and SWE-bench, measuring % Resolved (proportion of successfully resolved task instances) and Avg. $ Cost (mean per-instance inference cost).

## Key Results
- SWE-agent M achieves 12.3% resolution rate on SWE-bench M versus 42.6% on SWE-bench
- SWE-agent JS outperforms Agentless JS by 11.2% on SWE-bench M, showing the importance of agent-based approaches
- JavaScript-specific adaptations are crucial, as vanilla systems fail completely on JavaScript tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SWE-bench M exposes fundamental limitations in visual problem-solving capabilities of current AI systems
- Mechanism: The benchmark introduces visual elements (images/videos) into software engineering tasks, revealing that top-performing systems struggle significantly when visual reasoning is required
- Core assumption: Visual comprehension is a distinct capability from text-based code understanding
- Evidence anchors:
  - [abstract] "Our analysis finds that top-performing SWE-bench systems struggle with SWE-bench M, revealing limitations in visual problem-solving"
  - [section 2.3] "Necessity of images. To gauge the importance of problem statements' images, we pose two questions to human annotators"
  - [corpus] Weak - neighboring papers discuss visual reasoning but don't directly validate this specific claim about SWE-bench M
- Break condition: If visual elements could be perfectly translated to text without information loss, the visual problem-solving limitation would disappear

### Mechanism 2
- Claim: JavaScript's diverse development practices expose limitations in language-agnostic system generalization
- Mechanism: JavaScript supports multiple programming paradigms (object-oriented, functional, procedural) creating codebases that differ significantly from Python, breaking assumptions in Python-specific tooling
- Core assumption: Existing systems rely heavily on Python-specific AST parsers and assumptions about code structure
- Evidence anchors:
  - [section 3.1] "The main adaptation needed is swapping out the Python ast module with a custom JavaScript parser written from scratch"
  - [section 3.1] "We found that most tools AutoCodeRover provides to the agent rely heavily on Python-specific program analysis features"
  - [corpus] Weak - neighboring papers mention language diversity but don't specifically address JavaScript paradigm diversity
- Break condition: If all programming languages had similar structural patterns or if language-agnostic tools were available

### Mechanism 3
- Claim: Multimodal tools can improve agent performance when properly integrated
- Mechanism: SWE-agent M's web-specific tools (browser, screenshot, image viewing) enable agents to iteratively reproduce issues and verify solutions visually
- Core assumption: Visual verification provides additional information that text-based approaches cannot capture
- Evidence anchors:
  - [section 4.1] "An analysis of the SWE-agent trajectories generated while resolving the development set"
  - [section 4.1] "SWE-agent M with GPT-4o builds websites and takes screenshots for 38.3% of the instances"
  - [corpus] Weak - neighboring papers discuss multimodal browsing but don't validate specific performance improvements on SWE-bench M
- Break condition: If text-based descriptions could fully capture all necessary visual information

## Foundational Learning

- Concept: Abstract Syntax Tree (AST) parsing
  - Why needed here: Existing systems rely on AST parsers for bug localization, which must be adapted for JavaScript
  - Quick check question: What is the primary difference between Python's AST module and JavaScript's tree-sitter library in terms of supported language features?

- Concept: Multimodal reasoning
  - Why needed here: SWE-bench M requires combining visual and textual information to solve problems
  - Quick check question: Why can't Optical Character Recognition (OCR) fully replace image understanding in software engineering tasks?

- Concept: Software engineering evaluation metrics
  - Why needed here: Understanding how % Resolved and Avg. $ Cost measure system performance
  - Quick check question: What is the difference between fail-to-pass (F2P) and pass-to-pass (P2P) tests in SWE-bench M evaluation?

## Architecture Onboarding

- Component map:
  Data Collection Pipeline -> Visual filtering -> Environment setup -> Inconsistency testing -> Human validation
  Evaluation Framework -> Task runner -> Test execution -> Result aggregation -> Performance metrics
  Agent Systems -> RAG baseline -> SWE-agent variants -> Agentless JS adaptation
  Tooling Layer -> JavaScript parser -> Web browser simulation -> Image handling utilities

- Critical path: Task instance -> Agent execution -> Test validation -> Performance measurement
  Each task instance requires: problem statement, codebase, gold patch, test cases
  Agent execution involves: environment setup, tool usage, solution generation, test running

- Design tradeoffs:
  - Python-specific vs. language-agnostic tooling: Python tools are more mature but limit cross-language generalization
  - Visual vs. text-only evaluation: Visual evaluation captures real-world complexity but increases evaluation complexity
  - Manual vs. automated validation: Human validation ensures quality but limits scalability

- Failure signatures:
  - Zero resolve rate on development split: Indicates fundamental incompatibility with JavaScript
  - Inconsistent test results: Suggests flaky tests that need removal
  - High inference costs: May indicate inefficient tool usage or complex problem-solving paths

- First 3 experiments:
  1. Run SWE-agent Base on a simple JavaScript task with text-only problem statement to establish baseline performance
  2. Test RAG system with 32K, 64K, and 100K context windows on development split to find optimal retrieval configuration
  3. Evaluate SWE-agent JS on a task with both text and image problem statement to measure impact of JavaScript-specific adaptations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AI systems on SWE-bench M vary when using more advanced multimodal models or improved agent environments?
- Basis in paper: [inferred] The paper mentions that current models and environments are limited, suggesting potential for improvement.
- Why unresolved: The paper focuses on evaluating existing systems and does not explore the impact of more advanced models or environments.
- What evidence would resolve it: Conducting experiments with state-of-the-art multimodal models and enhanced agent environments to compare performance on SWE-bench M.

### Open Question 2
- Question: What is the impact of visual content on the success rate of solving SWE-bench M tasks, and how can systems be improved to better utilize visual information?
- Basis in paper: [explicit] The paper highlights the importance of images in problem-solving and the challenges faced by systems in handling visual content.
- Why unresolved: The paper does not provide a detailed analysis of how visual content affects task success or propose methods to improve visual understanding.
- What evidence would resolve it: Analyzing task success rates with and without visual content, and developing techniques to enhance visual comprehension in AI systems.

### Open Question 3
- Question: How can AI systems be designed to handle the diverse programming paradigms and practices found in JavaScript, as seen in SWE-bench M?
- Basis in paper: [explicit] The paper discusses the challenges posed by JavaScript's diverse development practices and the limitations of current systems in handling them.
- Why unresolved: The paper does not propose specific solutions for improving AI systems' adaptability to different programming paradigms.
- What evidence would resolve it: Developing and testing AI systems that can effectively navigate and adapt to various JavaScript programming styles and practices.

## Limitations

- Benchmark focuses specifically on JavaScript, limiting generalizability to other languages
- Relatively small dataset of 617 instances may not provide sufficient statistical power
- Human annotation for validation introduces potential subjectivity and limits scalability

## Confidence

**High Confidence**: The finding that top-performing SWE-bench systems struggle significantly on SWE-bench M (12% resolution vs 43% on SWE-bench) is well-supported by empirical results across multiple systems and LLMs.

**Medium Confidence**: The claim that JavaScript's diverse development practices expose limitations in language-agnostic generalization is supported but requires further validation.

**Low Confidence**: The assertion that multimodal tools can substantially improve agent performance when properly integrated is primarily demonstrated through SWE-agent M's success, but lacks comparative ablation studies.

## Next Checks

1. **Cross-Language Generalization**: Evaluate SWE-bench M performance on a subset of tasks ported to Python or other languages to isolate whether limitations are language-specific or visual reasoning challenges.

2. **Controlled Visual Reasoning Tests**: Design synthetic tasks with varying levels of visual complexity to quantify the specific contribution of visual elements to system performance, independent of code complexity.

3. **Long-Horizon Visual Tasks**: Extend the benchmark to include multi-step visual software engineering tasks requiring sustained visual reasoning over extended development sessions to test system limitations under realistic conditions.