---
ver: rpa2
title: Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient
  visual classification
arxiv_id: '2403.18228'
source_url: https://arxiv.org/abs/2403.18228
tags:
- spiking
- neural
- training
- transform
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Fourier-or-Wavelet-based spikformer (FWformer),
  which replaces the Spiking Self-Attention (SSA) in the spikformer with spike-form
  Fourier Transform, Wavelet Transform, and their combinations. The authors hypothesize
  that both SSA and Fourier/Wavelet transforms use a set of basis functions for information
  transformation, but SSA calculates dynamic bases from Query and Key, while Fourier/Wavelet
  transforms use fixed triangular or wavelet bases.
---

# Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification

## Quick Facts
- arXiv ID: 2403.18228
- Source URL: https://arxiv.org/abs/2403.18228
- Authors: Qingyu Wang; Duzhen Zhang; Tilelin Zhang; Bo Xu
- Reference count: 40
- Primary result: FWformer achieves comparable or higher accuracies (0.4%-1.5%), higher running speeds (9%-51% for training and 19%-70% for inference), reduced theoretical energy consumption (20%-25%), and reduced GPU memory usage (4%-26%) compared to standard spikformer

## Executive Summary
This paper proposes the Fourier-or-Wavelet-based spikformer (FWformer), which replaces the Spiking Self-Attention (SSA) in spikformer with spike-form Fourier Transform, Wavelet Transform, and their combinations. The authors hypothesize that both SSA and Fourier/Wavelet transforms use basis functions for information transformation, but SSA calculates dynamic bases from Query and Key while Fourier/Wavelet transforms use fixed triangular or wavelet bases. The FWformer is evaluated on visual classification tasks including static image and event-based video datasets. Results show that FWformer achieves comparable or higher accuracies (0.4%-1.5%), higher running speeds (9%-51% for training and 19%-70% for inference), reduced theoretical energy consumption (20%-25%), and reduced GPU memory usage (4%-26%) compared to standard spikformer.

## Method Summary
The FWformer replaces SSA with spike-form Fourier Transform, Wavelet Transform, and their combinations. The core idea is that SSA uses dynamic basis functions computed from Query and Key with O(N²) complexity, while Fourier/Wavelet transforms use fixed basis functions with O(N log N) complexity via fast algorithms. The architecture maintains the same overall structure as spikformer but substitutes the self-attention mechanism with these fixed-basis transforms. The authors also analyze orthogonality changes in SSA basis functions during training and explore non-orthogonal combined wavelet bases, which further improves accuracy performance.

## Key Results
- FWformer achieves comparable or higher accuracies (0.4%-1.5%) compared to spikformer with SSA
- Higher running speeds: 9%-51% for training and 19%-70% for inference
- Reduced theoretical energy consumption: 20%-25%
- Reduced GPU memory usage: 4%-26%

## Why This Works (Mechanism)

### Mechanism 1
Replacing SSA with fixed-basis transforms (FFT/Wavelet) reduces computational complexity from O(N²) to O(N log N) while preserving or improving accuracy. SSA computes dynamic basis functions from Q and K, incurring O(N²) cost due to pairwise similarity. FFT/Wavelet use pre-defined fixed basis functions, enabling O(N log N) via fast algorithms. Core assumption: In spike-form inputs, dynamic bases in SSA add little benefit over fixed structured bases because sparsity weakens pairwise similarity.

### Mechanism 2
Fixed-basis transforms preserve sufficient representational power for spike-form visual classification, matching or exceeding SSA accuracy. Spike-form inputs are sparse and binary; fixed bases (Fourier/Wavelet) encode prior structured knowledge that captures dominant frequency or wavelet patterns, sufficient for classification without learning dynamic bases. Core assumption: The structured prior in fixed bases compensates for lack of learnable dynamic bases in the sparse spike context.

### Mechanism 3
The orthogonality of SSA basis functions decreases during training, motivating use of fixed non-orthogonal combined wavelet bases for improved accuracy. SSA basis functions become less orthogonal over training, suggesting some overlap is beneficial. Combining multiple fixed wavelet bases with learnable coefficients can emulate this learned non-orthogonality without incurring O(N²) cost. Core assumption: The trajectory of orthogonality loss in SSA reflects useful representational overlap; fixed non-orthogonal bases can capture this without dynamic computation.

## Foundational Learning

- Concept: Fourier and Wavelet Transforms as basis function representations
  - Why needed here: FWformer replaces SSA with these transforms, so understanding how they encode information via fixed bases is essential to grasp the design rationale
  - Quick check question: What is the key difference between Fourier Transform and Self-Attention in terms of basis functions?
    - Answer: Fourier uses fixed triangular (sinusoidal) bases; Self-Attention computes dynamic bases from input Query and Key

- Concept: Spike-form inputs and sparsity
  - Why needed here: FWformer operates on spike sequences; understanding sparsity's effect on similarity computation explains why SSA may be overkill
  - Quick check question: Why does sparsity in spike-form inputs weaken the benefit of dynamic bases in SSA?
    - Answer: Sparse binary spikes have weak pairwise correlations, so computing all pairwise similarities (O(N²)) adds little discriminative power compared to fixed bases

- Concept: Computational complexity and fast algorithms
  - Why needed here: FWformer's efficiency gain comes from replacing O(N²) with O(N log N) via FFT/Wavelet; knowing these algorithms is key to evaluating the speedup
  - Quick check question: How does FFT reduce complexity from O(N²) to O(N log N)?
    - Answer: FFT recursively decomposes the DFT into smaller DFTs, avoiding redundant computations and reducing total operations from quadratic to quasi-linear

## Architecture Onboarding

- Component map: Input -> SPS -> CPE -> FW Encoder (L layers) -> GAP -> SFC -> Output
- Critical path: Input → SPS → CPE → FW Encoder (L layers) → GAP → SFC → Output
- Design tradeoffs:
  - Fixed vs. dynamic bases: Fixed bases give O(N log N) and no learnable params, but may miss task-specific patterns; dynamic bases adapt to data but cost O(N²)
  - Orthogonality vs. non-orthogonality: Orthogonal bases are sparse and efficient; non-orthogonal bases can capture overlap but may need careful coefficient tuning
- Failure signatures:
  - Accuracy drops below SSA baseline: Fixed bases may be insufficient for the task
  - No speedup observed: Implementation may not be using FFT/Wavelet efficiently or data size too small for asymptotic gain
  - Training instability: Non-orthogonal combined bases may introduce redundancy or ill-conditioning
- First 3 experiments:
  1. Replace SSA with 1D-FFT in a minimal 2-layer FWformer on CIFAR10-DVS; measure accuracy and training time vs. SSA baseline
  2. Swap 1D-FFT with 2D-WT (Haar) in same setup; compare accuracy and GPU memory usage
  3. Implement combined non-orthogonal wavelet bases (e.g., Haar + Db1 with learnable coeffs); test if accuracy improves over single-basis variants

## Open Questions the Paper Calls Out

### Open Question 1
What is the relationship between the orthogonality of dynamic basis functions in self-attention and the performance of fixed non-orthogonal basis functions like those used in the FWformer? The paper shows that orthogonality decreases during training and that non-orthogonal combined bases improve accuracy, but a direct causal link between these observations is not established. Systematic experiments varying the orthogonality of combined bases and measuring their impact on accuracy and efficiency would clarify this relationship.

### Open Question 2
How do different wavelet basis functions and their combinations affect the accuracy and efficiency of the FWformer across various tasks? While some bases perform similarly to or better than others, the exploration is not exhaustive, and the reasons for these differences are not fully explained. A comprehensive study of a wider range of wavelet bases and their combinations, along with an analysis of their properties and impact on the network's learning process, would provide more insights.

### Open Question 3
In what scenarios are Fourier and Wavelet transforms more effective than self-attention in spiking neural networks? The paper suggests that Fourier and Wavelet transforms are effective in event-based video tasks but may not be suitable for complex tasks like NLP and ASR. Comparative studies of Fourier/Wavelet-based and self-attention-based spiking networks across a diverse set of tasks, analyzing their strengths and weaknesses in different contexts, would clarify this.

## Limitations
- The paper's claims about orthogonality loss in SSA and its implications for fixed non-orthogonal bases lack external validation
- Implementation details for the FW head, particularly wavelet basis combinations and their learnable coefficients, are underspecified
- The assertion that spike-form sparsity inherently weakens SSA's dynamic bases advantage is reasonable but not rigorously proven

## Confidence

**High Confidence**: Computational complexity improvements (O(N²) → O(N log N)) and GPU memory reduction claims, as these are directly measurable and the FFT/Wavelet algorithms are well-established

**Medium Confidence**: Accuracy improvements (0.4%-1.5%), as these are empirically demonstrated but may depend heavily on implementation details and dataset characteristics

**Low Confidence**: Theoretical energy consumption estimates and the orthogonality analysis of SSA basis functions, as these involve modeling assumptions and novel theoretical interpretations

## Next Checks

1. Verify the orthogonality trajectory of SSA basis functions during training through controlled experiments measuring basis function correlations across training epochs
2. Implement and compare multiple wavelet basis combinations (beyond Haar and Db1) to test the robustness of accuracy improvements from non-orthogonal bases
3. Conduct ablation studies on spike sparsity levels to quantify the threshold at which fixed bases outperform dynamic bases in SSA