---
ver: rpa2
title: Benign Overfitting for Regression with Trained Two-Layer ReLU Networks
arxiv_id: '2410.06191'
source_url: https://arxiv.org/abs/2410.06191
tags:
- have
- neural
- then
- gradient
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the generalization properties of two-layer ReLU
  neural networks trained by gradient flow for regression. The authors develop a novel
  approximation-estimation error decomposition to analyze the excess risk, avoiding
  traditional uniform convergence techniques.
---

# Benign Overfitting for Regression with Trained Two-Layer ReLU Networks

## Quick Facts
- arXiv ID: 2410.06191
- Source URL: https://arxiv.org/abs/2410.06191
- Reference count: 40
- Primary result: Establishes benign overfitting for finite-width ReLU networks in regression via NTK analysis without uniform convergence

## Executive Summary
This paper studies generalization properties of two-layer ReLU neural networks trained by gradient flow for regression. The authors develop a novel approximation-estimation error decomposition to analyze excess risk without traditional uniform convergence techniques. They establish that under appropriate conditions on sample size, network width, and input dimension, these networks can generalize well for arbitrary regression functions without assumptions on the regression function or noise distribution. The analysis operates in the neural tangent kernel regime and leverages real induction and advanced concentration inequalities.

## Method Summary
The method analyzes two-layer ReLU networks with m hidden neurons trained by gradient flow on square loss. Using antisymmetric initialization (half neurons from N(0,I_d), half negated), the paper operates in the NTK regime where network weights remain close to initialization. The key innovation is an approximation-estimation error decomposition that avoids uniform convergence by tracking the neural network trajectory in function space rather than parameter space. The analysis requires n samples from the unit sphere, labels bounded by 1, and sufficient network width relative to dimension.

## Key Results
- Shows exponential decay of approximation error in eigenspace where regression function concentrates
- Proves estimation error can be bounded without uniform convergence via NTK operator concentration
- Establishes first benign overfitting result for finite-width ReLU networks in regression settings
- Demonstrates exponential decay of empirical risk to zero under NTK regime assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposition of excess risk into approximation and estimation errors enables benign overfitting without uniform convergence.
- **Mechanism**: By tracking the neural network trajectory in the function space rather than parameter space, the paper avoids controlling the entire function class. The approximation error bounds how well the network fits the true regression function, while the estimation error bounds how well the empirical trajectory tracks the population trajectory.
- **Core assumption**: NTK regime holds where network weights remain close to initialization.
- **Break condition**: If NTK regime breaks down (weights move significantly from initialization), operator concentration bounds fail.

### Mechanism 2
- **Claim**: NTK operator concentration bounds ensure empirical trajectory tracks population trajectory.
- **Mechanism**: At initialization, NTK Gram matrix concentrates to its expectation. Along gradient flow, NTK operators remain close to initial values because weights move only slightly from initialization. This bounds estimation error without uniform convergence.
- **Core assumption**: Network width sufficiently large relative to input dimension and sample size.
- **Break condition**: If network width insufficient relative to sample size and dimension, operator concentration bounds fail.

### Mechanism 3
- **Claim**: Exponential decay of both approximation and estimation errors ensures empirical risk decays exponentially.
- **Mechanism**: Approximation error decays exponentially in eigenspace where regression function lives. Estimation error bounds show empirical trajectory tracks this exponentially decaying population trajectory. Together, empirical risk decays exponentially to zero.
- **Core assumption**: Eigenvalue structure of NTK operator ensures most regression function captured in finite-dimensional subspace.
- **Break condition**: If eigenvalue spectrum too flat, approximation error cannot decay exponentially and benign overfitting fails.

## Foundational Learning

- **Concept**: Neural Tangent Kernel (NTK) regime and its implications
  - Why needed here: Entire analysis relies on NTK approximation where network weights remain close to initialization, enabling operator concentration bounds.
  - Quick check question: What distinguishes the NTK regime from feature learning regimes in neural network training?

- **Concept**: Spectral theory of compact self-adjoint operators
  - Why needed here: NTK operator is compact and self-adjoint, with eigenvalues and eigenfunctions determining how well network can approximate functions in different subspaces.
  - Quick check question: How does eigenvalue decay rate of NTK operator affect approximation error bound?

- **Concept**: Real induction and its application to gradient flow trajectories
  - Why needed here: Paper uses real induction to prove properties hold for all time t ≥ 0 by showing they hold for t = 0, are preserved locally, and are preserved in the limit.
  - Quick check question: What are the three conditions that must be verified to apply real induction to a set of times?

## Architecture Onboarding

- **Component map**: Two-layer ReLU network -> Gradient flow optimization -> NTK analysis framework -> Approximation-estimation error decomposition -> High-probability event construction
- **Critical path**: Initialization -> NTK concentration -> Gradient flow -> Approximation error bound -> Estimation error bound -> Benign overfitting result
- **Design tradeoffs**: Larger network width improves NTK concentration but increases computational cost; smaller width may break NTK regime.
- **Failure signatures**: If network fails to generalize despite perfect training fit, check whether NTK regime assumption is violated or eigenvalue concentration assumption is violated.
- **First 3 experiments**:
  1. Verify NTK concentration by comparing empirical NTK Gram matrix to its expectation for increasing network widths.
  2. Test exponential decay of approximation error by measuring ∥ζt∥2 for different training times and network widths.
  3. Validate estimation error bounds by comparing empirical and population trajectories for different sample sizes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can benign overfitting occur for finite-width ReLU networks without the NTK regime assumptions?
- Basis in paper: [explicit] Paper explicitly states results operate in NTK regime and acknowledges analysis outside NTK regime is "extremely challenging."
- Why unresolved: Paper acknowledges that analysis outside NTK regime would require "more sophisticated ways of controlling the learning trajectory."
- What evidence would resolve it: Demonstrating generalization guarantees for ReLU networks trained with finite step sizes or different optimization algorithms while maintaining benign overfitting behavior.

### Open Question 2
- Question: What is the minimal network width required for benign overfitting to occur, as a function of problem parameters?
- Basis in paper: [explicit] Paper shows m ≫ d^5 is required and states that m and n must grow much larger with d when λ_ϵ ≤ o(1/d).
- Why unresolved: While paper provides sufficient conditions, it doesn't establish tight lower bounds on network width.
- What evidence would resolve it: Precise characterization of trade-off between network width and sample complexity needed to achieve given excess risk bound.

### Open Question 3
- Question: How does benign overfitting phenomenon change when moving beyond spherical data distribution assumption?
- Basis in paper: [explicit] Paper explicitly states "we do not require anything other than uniform distribution on the sphere for the data distribution."
- Why unresolved: Analysis relies heavily on properties specific to spherical data distributions, including explicit eigenvalue calculations.
- What evidence would resolve it: Extending approximation-estimation error decomposition framework to other data distributions and characterizing how NTK operator eigenvalues change.

## Limitations
- Results heavily depend on NTK regime assumption, which may not hold for practical learning rates or deeper architectures
- Scaling conditions between m, n, d, and ε are complex and may not translate to practical network design
- Antisymmetric initialization scheme and specific scaling conditions haven't been empirically validated across diverse regression tasks

## Confidence
- **High Confidence**: Mathematical derivation of approximation-estimation error decomposition and exponential decay rates under stated assumptions
- **Medium Confidence**: Claim of first benign overfitting result for finite-width ReLU networks in the specific NTK regime studied
- **Low Confidence**: Practical applicability of scaling conditions to real-world network architectures and datasets

## Next Checks
1. **Empirical NTK Concentration Test**: For networks of varying widths m with d fixed, measure operator norm ||H_0 - H||_2 as training progresses. Verify concentration scales as O(1/√m) and determine minimum m/d ratio required for benign overfitting.

2. **Eigenvalue Spectrum Analysis**: For different regression functions f* (including smooth functions, functions with varying regularity, and arbitrary bounded functions), compute eigenvalue spectrum of NTK operator and measure how much of ||f*||_L^2^2 concentrates in top k eigenspaces.

3. **Robustness to Initialization Scheme**: Repeat benign overfitting analysis with standard Gaussian initialization rather than antisymmetric scheme and test whether results hold.