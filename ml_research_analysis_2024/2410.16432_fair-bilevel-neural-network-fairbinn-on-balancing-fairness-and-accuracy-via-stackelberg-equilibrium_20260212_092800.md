---
ver: rpa2
title: 'Fair Bilevel Neural Network (FairBiNN): On Balancing fairness and accuracy
  via Stackelberg Equilibrium'
arxiv_id: '2410.16432'
source_url: https://arxiv.org/abs/2410.16432
tags:
- fairness
- accuracy
- loss
- optimization
- lipschitz
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of bias in machine learning
  models, specifically focusing on achieving fairness in classification tasks while
  maintaining high accuracy. The authors propose a novel bilevel optimization approach,
  FairBiNN, which formulates the fairness-accuracy trade-off as a Stackelberg game
  between two competing objectives.
---

# Fair Bilevel Neural Network (FairBiNN): On Balancing fairness and accuracy via Stackelberg Equilibrium

## Quick Facts
- arXiv ID: 2410.16432
- Source URL: https://arxiv.org/abs/2410.16432
- Authors: Mehdi Yazdani-Jahromi; Ali Khodabandeh Yalabadi; AmirArsalan Rajabi; Aida Tayebi; Ivan Garibay; Ozlem Ozmen Garibay
- Reference count: 40
- Primary result: Novel bilevel optimization approach for fairness-accuracy trade-off in neural networks, theoretically guaranteeing Pareto optimal solutions

## Executive Summary
This paper addresses the challenge of bias in machine learning models by proposing FairBiNN, a bilevel optimization framework that formulates the fairness-accuracy trade-off as a Stackelberg game. The method theoretically guarantees Pareto optimal solutions under certain assumptions and was evaluated on tabular datasets, showing significant improvements over state-of-the-art fairness techniques on the UCI Adult dataset while closely competing on the Heritage Health dataset. The framework offers flexibility in architecture and training algorithm choices while explicitly modeling the interaction between accuracy and fairness objectives.

## Method Summary
FairBiNN uses a bilevel optimization framework where accuracy is optimized in the upper level and fairness in the lower level. The method employs separate parameter sets and optimizers for each objective, with dynamic adjustment of the secondary objective based on the current solution. The framework leverages Stackelberg game theory to ensure Pareto optimality under assumptions of strict convexity of the primary loss and Lipschitz continuity of the fairness loss. The approach was evaluated on UCI Adult and Heritage Health datasets using demographic parity as the fairness metric.

## Key Results
- Achieved significantly better accuracy-fairness trade-offs than state-of-the-art methods on UCI Adult dataset
- Demonstrated competitive performance with recent models on Heritage Health dataset
- Theoretically guaranteed Pareto optimal solutions under strict convexity and Lipschitz continuity assumptions
- Showed superior performance in balancing demographic parity with classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bilevel optimization framework guarantees Pareto optimal solutions for balancing accuracy and fairness.
- Mechanism: By formulating fairness as a Stackelberg game between leader (accuracy) and follower (fairness) objectives, the framework ensures that improvements in one objective don't come at the expense of the other beyond the Pareto frontier.
- Core assumption: The primary loss function is strictly convex near local optima and the secondary loss function is smooth and Lipschitz continuous.
- Evidence anchors:
  - [abstract]: "Our deep learning-based approach concurrently optimizes for both accuracy and fairness objectives, and under certain assumptions, achieving proven Pareto optimal solutions"
  - [section 3.1.2]: "We leverage the theoretical results of [46], which investigates the relationship between Stackelberg equilibria and Pareto optimality in game theory"
  - [corpus]: Weak - only 1 of 8 related papers discusses bilevel optimization explicitly
- Break condition: If the strict convexity assumption fails or if the Lipschitz continuity requirement is violated by activation functions or loss components.

### Mechanism 2
- Claim: The bilevel approach provides more flexible control over the fairness-accuracy trade-off than traditional regularization methods.
- Mechanism: By separating the optimization into two distinct levels, the method allows dynamic adjustment of the secondary objective based on the current solution, enabling more nuanced trade-offs than fixed-weight regularization.
- Core assumption: The step sizes in the bilevel approach (αf for outer, αs for inner) can be independently tuned and the model is overparameterized.
- Evidence anchors:
  - [section 3.1.1]: "By formulating the problem as a hierarchical optimization task, we can explicitly model the interactions between the primary and secondary objectives"
  - [section 3.1.4]: "The leader problem remains a pure minimization of the primary loss, without any regularization terms that may slow or hinder its optimization"
  - [corpus]: Weak - only 1 of 8 related papers discusses Lagrangian methods in fairness context
- Break condition: If the independent tuning of step sizes becomes infeasible due to computational constraints or if the overparameterization assumption is violated.

### Mechanism 3
- Claim: The FairBiNN framework maintains Lipschitz continuity of the demographic parity loss function.
- Mechanism: Since neural network outputs are Lipschitz continuous with respect to their parameters, and the demographic parity loss is a difference of expectations of these outputs, the overall fairness loss remains Lipschitz continuous.
- Core assumption: The neural network layers use Lipschitz continuous activation functions (e.g., ReLU, sigmoid) and bounded outputs.
- Evidence anchors:
  - [section 3.2]: "We showed that the demographic parity loss function, when applied to the output of neural network layers, is also Lipschitz continuous (Theorem 3.10)"
  - [appendix A.5]: "Proof of Lipschitz continuity for sigmoid function and discussion of other common activation functions"
  - [corpus]: Weak - no related papers discuss Lipschitz continuity in fairness contexts
- Break condition: If non-Lipschitz activation functions like softmax are used in multiclass settings, breaking the continuity assumption.

## Foundational Learning

- Concept: Bilevel optimization and Stackelberg games
  - Why needed here: The core theoretical foundation relies on formulating the fairness-accuracy trade-off as a hierarchical optimization problem where one objective depends on the solution of another.
  - Quick check question: Can you explain how a Stackelberg game differs from a Nash equilibrium in terms of solution strategy and information structure?

- Concept: Pareto optimality and multi-objective optimization
  - Why needed here: The framework aims to find solutions that represent optimal trade-offs between accuracy and fairness, where improving one metric would worsen the other.
  - Quick check question: What is the geometric interpretation of Pareto optimality in the accuracy-fairness trade-off space?

- Concept: Lipschitz continuity and its implications for optimization
  - Why needed here: The theoretical guarantees depend on the loss functions being Lipschitz continuous, which ensures stable gradient-based optimization and bounded changes in output.
  - Quick check question: How does Lipschitz continuity of a function guarantee the existence of bounded gradients?

## Architecture Onboarding

- Component map: Base feature extraction layers -> Fairness layers (θs) -> Classification layers (θa)
- Critical path: During training, the accuracy player updates first using the current fairness parameters, then the fairness player updates while holding accuracy parameters fixed. This alternating optimization continues until convergence.
- Design tradeoffs: Using separate parameter sets for accuracy and fairness allows independent optimization but increases model complexity. Placing fairness layers near the output balances feature preservation with effective bias mitigation.
- Failure signatures: Poor fairness performance despite training may indicate violation of Lipschitz continuity assumptions (e.g., using softmax activations). Accuracy degradation with fairness improvement suggests η is set too high.
- First 3 experiments:
  1. Train with only accuracy layers (no fairness components) to establish baseline performance.
  2. Add fairness layers in different positions (early vs. late) to empirically determine optimal placement.
  3. Sweep η parameter values to find the sweet spot balancing accuracy and demographic parity on validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FairBiNN's bilevel optimization approach perform compared to regularization methods when the number of sensitive attributes increases beyond two?
- Basis in paper: [explicit] The paper focuses on binary sensitive attributes but mentions demographic parity can be extended to multiple groups.
- Why unresolved: The experiments and theoretical analysis primarily address binary sensitive attributes, leaving performance on multi-attribute scenarios unexplored.
- What evidence would resolve it: Comparative experiments with datasets containing multiple sensitive attributes and corresponding theoretical analysis of bilevel optimization convergence in multi-attribute scenarios.

### Open Question 2
- Question: What are the limitations of FairBiNN when applied to overparameterized models where θp may not be truly overparameterized?
- Basis in paper: [explicit] Theorem 3.9 assumes θp is overparameterized, but this may not hold for all real-world scenarios.
- Why unresolved: The theoretical analysis relies on overparameterization, but practical applications may involve models that are not fully overparameterized.
- What evidence would resolve it: Empirical studies comparing FairBiNN performance on models with varying degrees of overparameterization, and theoretical analysis extending the current framework to handle non-overparameterized cases.

### Open Question 3
- Question: How does FairBiNN perform when applied to time-series or sequential data compared to tabular data?
- Basis in paper: [inferred] The paper focuses on tabular datasets, suggesting potential limitations in handling other data types.
- Why unresolved: The current implementation and evaluation are limited to tabular datasets, leaving performance on other data structures unexplored.
- What evidence would resolve it: Experimental results on time-series or sequential datasets, along with architectural modifications needed to adapt FairBiNN for these data types.

## Limitations
- Theoretical guarantees rely on assumptions of strict convexity and Lipschitz continuity that may not hold in all practical scenarios
- Bilevel optimization introduces computational complexity that scales poorly with larger datasets and more complex models
- Focus on demographic parity as the fairness metric may not capture all aspects of fairness required in real-world applications

## Confidence
- Pareto optimality claims: Medium - theoretical framework is sound but empirical validation is limited to two tabular datasets
- Performance claims vs state-of-the-art: High for UCI Adult dataset, Medium for Heritage Health dataset
- Scalability and practical applicability: Low - computational complexity and overparameterization requirements may limit real-world deployment

## Next Checks
1. **Empirical Pareto Frontier Analysis**: Systematically sweep the η parameter across multiple orders of magnitude and plot the resulting accuracy-fairness trade-off curve to empirically verify Pareto optimality claims beyond the reported single operating point.

2. **Generalization Across Fairness Metrics**: Evaluate FairBiNN on additional fairness metrics beyond demographic parity (such as equalized odds and equal opportunity) to assess robustness of the approach to different fairness definitions.

3. **Computational Complexity Benchmarking**: Measure wall-clock training time and memory usage across different dataset sizes and model complexities to quantify the practical scalability limitations of the bilevel optimization approach.