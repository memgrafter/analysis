---
ver: rpa2
title: Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data
arxiv_id: '2404.14367'
source_url: https://arxiv.org/abs/2404.14367
tags:
- on-policy
- reward
- preference
- fine-tuning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates which fine-tuning approaches are most effective
  for aligning large language models (LLMs) with human preferences. It finds that
  methods employing on-policy sampling (e.g., PPO, REINFORCE) or explicit negative
  gradients (e.g., DPO, IPO) consistently outperform offline supervised methods (e.g.,
  Pref-FT, Binary FeedMe) in shifting probability mass toward high-reward responses.
---

# Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data

## Quick Facts
- arXiv ID: 2404.14367
- Source URL: https://arxiv.org/abs/2404.14367
- Reference count: 40
- Key outcome: On-policy sampling and negative gradients significantly improve LLM preference alignment, especially when optimal responses lie in low-likelihood regions

## Executive Summary
This paper investigates which fine-tuning approaches most effectively align large language models with human preferences. Through theoretical analysis and empirical experiments, the authors find that methods using on-policy sampling (PPO, REINFORCE) or explicit negative gradients (DPO, IPO) consistently outperform offline supervised methods (Pref-FT, Binary FeedMe). The key insight is that mode-seeking objectives, particularly when combined with on-policy sampling and negative gradients, can redistribute probability mass more efficiently toward high-reward responses. These findings provide actionable guidance: use on-policy sampling with negative gradients when the reward peak is misaligned with the reference policy, and consider sample reuse tradeoffs for computational efficiency.

## Method Summary
The paper compares multiple preference fine-tuning methods including supervised learning (Pref-FT, Binary FeedMe), on-policy reinforcement learning (PPO, REINFORCE), and contrastive learning (DPO, IPO). The experimental setup uses preference datasets like AlpacaFarm and UltraFeedback, with models trained to maximize reward under a learned reward model while staying close to a reference policy via KL regularization. The authors systematically vary factors including on/off-policy sampling, sample reuse, and presence of negative gradients to understand their impact on learning dynamics and final performance.

## Key Results
- On-policy sampling significantly outperforms offline methods when optimal responses lie in low-likelihood regions of the reference policy
- Methods with negative gradients (DPO, IPO) learn faster by directly suppressing dispreferred responses
- Mode-seeking objectives (reverse KL) redistribute probability mass more efficiently than mode-covering objectives (forward KL)
- The combination of on-policy sampling and negative gradients provides the strongest performance gains

## Why This Works (Mechanism)

### Mechanism 1: On-policy Sampling for Misaligned Rewards
On-policy sampling improves performance by exploring regions where the reward peak is misaligned with the reference policy distribution. When the optimal response lies in low-likelihood regions, on-policy sampling allows the policy to discover and learn from these responses that offline methods cannot effectively reach.

### Mechanism 2: Negative Gradients for Faster Learning
Negative gradients accelerate learning by directly suppressing dispreferred responses, creating a stronger learning signal than maximum likelihood approaches that only increase likelihood on preferred responses. This contrastive approach enables faster reorganization of probability mass.

### Mechanism 3: Mode-Seeking Efficiency
Mode-seeking objectives (reverse KL) concentrate probability mass more efficiently toward high-reward regions compared to mode-covering objectives (forward KL). This aggressive redistribution allows faster convergence by focusing on the most rewarding responses rather than attempting to cover all high-reward regions.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: Provides the framework for fine-tuning LLMs using preference data, including reward modeling and policy optimization
  - Quick check question: What are the three main steps in the typical RLHF pipeline, and how does this paper modify or analyze each step?

- **Concept: KL Divergence and its variants (forward vs reverse)**
  - Why needed here: The paper uses KL divergence as regularization and contrasts forward (mode-covering) and reverse (mode-seeking) KL to explain different optimization behaviors
  - Quick check question: How does reverse KL divergence differ from forward KL divergence in terms of probability mass reorganization, and why does this matter for preference fine-tuning?

- **Concept: Categorical Distribution Parameterization**
  - Why needed here: The theoretical analysis models LLM outputs as categorical distributions, requiring understanding of softmax parameterization and gradient dynamics
  - Quick check question: How does the softmax function convert logits to probabilities, and why is this representation important for analyzing mode-seeking behavior?

## Architecture Onboarding

- **Component map**: Reference policy (πref) -> Reward model (rφ) -> Policy (πθ) -> Preference dataset (Dpref)
- **Critical path**:
  1. Initialize reference policy via supervised fine-tuning
  2. Train reward model on preference dataset
  3. Optimize policy using chosen fine-tuning method (on/off-policy, with/without negative gradient)
  4. Evaluate policy against ground truth reward or human evaluation

- **Design tradeoffs**:
  - On-policy vs offline: On-policy provides better exploration but requires more computation; offline is cheaper but may underreach low-density reward regions
  - Negative gradient vs no negative gradient: Negative gradient accelerates learning but may require more diverse preference data
  - Sample reuse: More reuse improves sample efficiency but may reduce exploration and cause overfitting

- **Failure signatures**:
  - Poor performance despite on-policy sampling: Likely reward model inaccuracy in explored regions
  - Slow convergence with negative gradients: Preference data may lack sufficient negative examples
  - Instability with mode-seeking objectives: Multiple optimal modes in reward function may trap optimization

- **First 3 experiments**:
  1. Run on-policy DPO vs offline DPO on a simple synthetic task to observe convergence speed differences
  2. Compare mode-seeking (reverse KL) vs mode-covering (forward KL) objectives on a toy categorical distribution
  3. Test sample reuse effects by varying gradient steps per on-policy batch in a controlled bandit problem

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the quality of the reward model affect the performance of on-policy sampling methods?
  - Basis in paper: [inferred] The paper mentions on-policy sampling effectiveness but doesn't explicitly analyze reward model quality impact
  - Why unresolved: The analysis focuses on geometric alignment between reward function and reference policy, but doesn't account for reward model errors
  - What evidence would resolve it: A controlled experiment varying reward model quality and measuring impact on on-policy vs offline method performance

- **Open Question 2**: What is the optimal balance between on-policy sample reuse and sample freshness for different algorithms?
  - Basis in paper: [explicit] The paper discusses the tradeoff but doesn't provide clear guidelines for optimal balance
  - Why unresolved: The optimal balance likely depends on specific algorithm, reward function geometry, and preference data coverage
  - What evidence would resolve it: A systematic study varying sample reuse and on-policyness across different algorithms and problem settings

- **Open Question 3**: How does the composition of the preference data affect the performance of contrastive methods with negative gradients?
  - Basis in paper: [inferred] The paper shows contrastive methods perform well when reward peak is misaligned, but doesn't analyze preference data composition effects
  - Why unresolved: The effectiveness of negative gradient depends on identifying and suppressing dispreferred responses, which may be influenced by preference data distribution
  - What evidence would resolve it: An experiment varying preference data composition and measuring impact on contrastive method performance vs methods without negative gradients

## Limitations

- The paper's claims about on-policy sampling benefits assume reward models remain accurate in low-density regions explored by the policy, which may not hold in practice
- Empirical validation is constrained by available preference datasets, which may not capture the full diversity of human preferences across all domains
- The theoretical analysis focuses on simplified categorical distributions and may not fully capture the complexity of real LLM outputs

## Confidence

- **High Confidence**: The theoretical analysis of mode-seeking versus mode-covering objectives and the general framework for understanding different preference fine-tuning approaches
- **Medium Confidence**: The experimental results showing on-policy sampling benefits and negative gradient acceleration, though limited by dataset constraints and potential reward model inaccuracies
- **Low Confidence**: The universal applicability of these findings across all LLM sizes and domains, given the limited empirical scope

## Next Checks

1. **Reward Model Accuracy Test**: Systematically evaluate reward model accuracy in low-density regions explored by on-policy sampling versus regions covered by the reference policy to validate the core assumption about exploration benefits

2. **Cross-Domain Generalization**: Apply the same experimental framework to preference datasets from different domains (e.g., creative writing, technical reasoning) to assess whether the observed patterns generalize beyond the current datasets

3. **Multiple Optimal Mode Scenario**: Design experiments with reward functions containing multiple equally optimal modes to test whether mode-seeking objectives indeed converge to suboptimal local optima as hypothesized