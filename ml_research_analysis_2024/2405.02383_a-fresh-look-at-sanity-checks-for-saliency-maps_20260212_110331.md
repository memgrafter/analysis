---
ver: rpa2
title: A Fresh Look at Sanity Checks for Saliency Maps
arxiv_id: '2405.02383'
source_url: https://arxiv.org/abs/2405.02383
tags:
- mprt
- methods
- explanation
- smprt
- emprt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the methodological shortcomings of the Model
  Parameter Randomisation Test (MPRT) for evaluating saliency map explanations. The
  authors identify three key issues: sensitivity to preprocessing, layer-order effects,
  and noise in similarity measures.'
---

# A Fresh Look at Sanity Checks for Saliency Maps

## Quick Facts
- arXiv ID: 2405.02383
- Source URL: https://arxiv.org/abs/2405.02383
- Reference count: 40
- This paper addresses methodological shortcomings in the Model Parameter Randomisation Test (MPRT) for saliency map evaluation, proposing modifications to improve robustness and reduce noise sensitivity.

## Executive Summary
This paper critically examines the Model Parameter Randomisation Test (MPRT), a widely used sanity check for evaluating saliency map explanations in deep learning. The authors identify three key weaknesses: sensitivity to preprocessing variations, layer-order dependencies, and noise in similarity measures. To address these issues, they propose two modifications - Smooth MPRT (sMPRT) and Efficient MPRT (eMPRT) - which respectively denoise explanations through perturbation averaging and measure complexity changes instead of pairwise similarities. Extensive experiments across multiple datasets and models demonstrate that both variants outperform the original MPRT in meta-evaluation tests, with sMPRT reducing noise sensitivity and eMPRT eliminating layer-order dependencies while preserving the core principle that explanations should be sensitive to model parameters.

## Method Summary
The paper builds upon the existing MPRT framework by introducing two key modifications. Smooth MPRT (sMPRT) addresses noise sensitivity by averaging explanations over multiple perturbed versions of the input, effectively smoothing out stochastic variations. Efficient MPRT (eMPRT) circumvents layer-order dependencies by shifting from pairwise similarity measurements to tracking changes in explanation complexity as model parameters are randomized. Both approaches maintain the fundamental requirement that explanations should change when model parameters change, while improving robustness to preprocessing choices and eliminating problematic dependencies on layer ordering. The modifications are evaluated through systematic experiments on image classification tasks using various datasets and model architectures.

## Key Results
- sMPRT successfully reduces noise sensitivity in explanation evaluation, showing more stable results across different runs
- eMPRT eliminates layer-order dependencies that plagued the original MPRT, providing consistent evaluation regardless of parameter randomization order
- Both modified approaches outperform the original MPRT in meta-evaluation tests, demonstrating improved reliability for sanity checking saliency maps

## Why This Works (Mechanism)
The effectiveness of the proposed modifications stems from addressing specific failure modes in the original MPRT. sMPRT works by averaging over multiple perturbed inputs, which effectively filters out stochastic noise that can obscure meaningful changes in explanations. This smoothing operation preserves the signal of parameter sensitivity while reducing the impact of random fluctuations. eMPRT avoids layer-order dependencies by focusing on complexity metrics rather than direct similarity comparisons between explanation sets. This shift in measurement approach eliminates the sensitivity to the order in which parameters are randomized, a critical flaw in the original method. Both modifications maintain the core principle that explanations should be sensitive to model parameters while improving the robustness and consistency of the evaluation process.

## Foundational Learning

**Saliency Maps**: Visualizations highlighting which input features most influence a model's prediction. Why needed: Form the basis of explanation methods being evaluated. Quick check: Can you identify the most salient region in a simple image classification example?

**Model Parameter Randomisation Test (MPRT)**: A sanity check that verifies explanations change when model parameters are randomized. Why needed: Provides the baseline methodology being improved. Quick check: Can you explain why an explanation that doesn't change when parameters are randomized is problematic?

**Perturbation-based Denoising**: Technique of averaging results over multiple input variations to reduce noise. Why needed: Forms the core mechanism of sMPRT. Quick check: Can you describe how averaging over perturbations might reduce random fluctuations?

**Complexity Metrics**: Measures of explanation structure rather than direct similarity. Why needed: Alternative to similarity metrics in eMPRT. Quick check: Can you list two different ways to measure the complexity of a saliency map?

**Layer-order Dependencies**: Sensitivity of evaluation metrics to the sequence of parameter randomization. Why needed: Key weakness being addressed by eMPRT. Quick check: Can you explain why the order of randomization might affect similarity-based metrics?

## Architecture Onboarding

**Component Map**: Input Data -> Preprocessing -> Model Forward Pass -> Saliency Map Generation -> MPRT Evaluation -> Quality Assessment

**Critical Path**: The evaluation pipeline follows: input preprocessing → model inference → explanation generation → parameter randomization → explanation comparison → quality score calculation

**Design Tradeoffs**: The paper trades computational efficiency for improved robustness, with sMPRT requiring multiple perturbation passes and eMPRT shifting from direct similarity to complexity metrics

**Failure Signatures**: The original MPRT fails when: (1) preprocessing choices significantly alter explanations, (2) layer randomization order affects results, or (3) noise masks true parameter sensitivity

**First Experiments**: 1) Compare sMPRT vs original MPRT on noisy explanations 2) Test eMPRT's layer-order independence across different randomization sequences 3) Evaluate both methods on multiple datasets with varying preprocessing

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Both modifications preserve the core assumption that explanations should be sensitive to model parameters without critically examining this assumption's validity across all explanation types
- Experiments are limited to image classification datasets, leaving uncertainty about generalizability to text, tabular, or other data modalities
- The paper doesn't address potential computational overhead from sMPRT's multiple perturbation passes, particularly for large models or high-resolution inputs

## Confidence
- sMPRT noise reduction effectiveness: High - demonstrated through systematic experiments across multiple datasets and models
- eMPRT layer-order independence: Medium - successfully avoids dependencies but limited analysis of complexity metric interpretability
- Overall method improvement: High - both variants show clear advantages in meta-evaluation tests

## Next Checks
1. Conduct ablation studies on the number of perturbations used in sMPRT to determine the optimal trade-off between denoising effectiveness and computational cost
2. Test eMPRT's performance on non-image datasets (text, tabular data) to evaluate its generalizability across different data modalities
3. Perform user studies comparing explanations generated using the original MPRT versus the modified versions to assess real-world interpretability and usefulness