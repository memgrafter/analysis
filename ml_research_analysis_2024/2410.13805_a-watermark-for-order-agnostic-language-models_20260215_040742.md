---
ver: rpa2
title: A Watermark for Order-Agnostic Language Models
arxiv_id: '2410.13805'
source_url: https://arxiv.org/abs/2410.13805
tags:
- watermark
- pattern
- generation
- sequence
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PATTERN-MARK introduces a pattern-based watermarking framework
  specifically designed for order-agnostic language models (LMs), where content is
  not generated sequentially. The method employs a Markov-chain-based key sequence
  generation to produce high-frequency key patterns and a statistical pattern-based
  detection algorithm to recover these patterns during detection.
---

# A Watermark for Order-Agnostic Language Models

## Quick Facts
- arXiv ID: 2410.13805
- Source URL: https://arxiv.org/abs/2410.13805
- Authors: Ruibo Chen; Yihan Wu; Yihan Wu; Chenxi Liu; Junfeng Guo; Heng Huang
- Reference count: 40
- One-line primary result: PATTERN-MARK achieves up to 99.87% true positive rate on protein generation and 99.80% on machine translation at 0.1% false positive rate

## Executive Summary
PATTERN-MARK introduces a pattern-based watermarking framework specifically designed for order-agnostic language models (LMs), where content is not generated sequentially. The method employs a Markov-chain-based key sequence generation to produce high-frequency key patterns and a statistical pattern-based detection algorithm to recover these patterns during detection. Unlike traditional watermarking techniques that rely on sequential generation context, PATTERN-MARK generates watermark keys through a Markov chain, enabling the detection of key patterns regardless of generation order.

Experiments on ProteinMPNN and CMLM demonstrate that PATTERN-MARK achieves superior detection efficiency, generation quality, and robustness compared to baseline methods like Soft watermark, Unigram, and Multikey watermarking. The framework addresses the challenge of watermarking models that generate content without sequential dependencies, which is particularly relevant for tasks like protein generation and machine translation using non-autoregressive transformers.

## Method Summary
PATTERN-MARK uses a Markov-chain-based key sequence generator with transition matrix A=[[0,1],[1,0]] and initial distribution Q=[0.5,0.5] to create predictable alternating patterns (k1k2, k2k1). The vocabulary is partitioned into l parts, each associated with a key. During generation, token probabilities are modified based on the current key using a probability promotion strategy. Detection involves recovering the key sequence from generated tokens, counting pattern occurrences, and performing statistical hypothesis testing using precomputed probability distributions to determine if the content is watermarked.

## Key Results
- Achieves up to 99.87% true positive rate on protein generation and 99.80% on machine translation at 0.1% false positive rate
- Outperforms baseline methods (Soft watermark, Unigram, Multikey) on both detection efficiency and generation quality metrics
- Maintains comparable generation quality (pLDDT for proteins, BLEU for translation) to non-watermarked models
- Demonstrates robustness against random token modification attacks and paraphrase attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Markov-chain-based key sequence generation enables pattern-based detection independent of generation order
- Mechanism: PATTERN-MARK uses a Markov chain to generate a key sequence where the next key depends only on the current key, creating predictable high-frequency patterns (e.g., alternating k1k2, k2k1). During detection, the algorithm recovers this key sequence from the generated content by mapping tokens to keys based on vocabulary partitions, then tests for the presence of these high-frequency patterns.
- Core assumption: The key sequence can be reliably recovered during detection even though generation order is arbitrary
- Evidence anchors:
  - [abstract]: "We develop a Markov-chain-based watermark generator that produces watermark key sequences with high-frequency key patterns"
  - [section 3.2]: "During watermark detection, we do not have access to the entire key sequence used in watermarking. To address this, we propose a Markov-chain-based key sequence generation method, which generates a key sequence with high-frequency patterns"
  - [corpus]: Weak evidence - no direct corpus support for this specific mechanism
- Break condition: If the key sequence recovery fails due to incorrect mapping between tokens and keys, or if the Markov chain produces patterns that are not statistically distinguishable from random sequences

### Mechanism 2
- Claim: The pattern-based detection algorithm achieves controlled false positive rates through statistical hypothesis testing
- Mechanism: The detector counts occurrences of target patterns in the recovered key sequence and compares this count to the probability distribution under the null hypothesis (uniform random sequence). The p-value is calculated as the probability of observing at least as many pattern occurrences under the null hypothesis.
- Core assumption: The probability distribution PT,n of pattern occurrences under the null hypothesis can be accurately computed and used for hypothesis testing
- Evidence anchors:
  - [abstract]: "conducts statistical tests based on the count of high-frequency patterns"
  - [section 3.2]: "Given a set of patterns T ⊂ K^m, let ST(k) := Pn−m+1 j=1 1k[j:j+m−1]∈T ... the p-value (false positive rate, a.k.a. FPR) for this hypothesis test isP∞ i=c PT,n(i)"
  - [section 3.3]: Provides Algorithm 3 for computing PT,n using dynamic programming
  - [corpus]: Weak evidence - no direct corpus support for this specific statistical testing mechanism
- Break condition: If the computed PT,n is inaccurate due to computational errors or if the statistical test assumptions (e.g., independence) are violated

### Mechanism 3
- Claim: PATTERN-MARK outperforms existing watermarking methods on order-agnostic LMs while maintaining generation quality
- Mechanism: PATTERN-MARK's pattern-based approach avoids the context-dependency issues of existing methods like Soft watermark and Multikey watermark, which rely on previously generated tokens that may not exist in order-agnostic generation. It also avoids the quality degradation of Unigram watermark which uses fixed probability promotion across all generation steps.
- Core assumption: The pattern-based approach provides better detection efficiency and generation quality than existing methods when adapted to order-agnostic LMs
- Evidence anchors:
  - [abstract]: "PATTERN-MARK achieves superior detection efficiency, generation quality, and robustness compared to baseline methods like Soft watermark, Unigram, and Multikey watermarking"
  - [section 3.1]: Discusses why existing methods cannot be directly applied to order-agnostic LMs and why distortion-free schemes are not feasible
  - [section 4.2]: Presents experimental results showing PATTERN-MARK outperforms baselines on both protein generation and machine translation tasks
  - [corpus]: Weak evidence - no direct corpus support for this comparative claim
- Break condition: If the experimental setup is biased or if the baselines are not properly adapted for order-agnostic LMs, the comparative performance claims may not hold

## Foundational Learning

- Concept: Markov chains and their properties
  - Why needed here: PATTERN-MARK relies on generating key sequences using Markov chains where the next state depends only on the current state
  - Quick check question: What property of Markov chains makes them suitable for generating key sequences that can be recovered during detection without knowing generation order?

- Concept: Statistical hypothesis testing and p-values
  - Why needed here: The detection algorithm uses hypothesis testing to determine if a sequence is watermarked based on pattern occurrence counts
  - Quick check question: How is the p-value calculated in PATTERN-MARK's detection algorithm, and what does it represent?

- Concept: Order-agnostic language models
  - Why needed here: PATTERN-MARK is specifically designed for LMs that generate content without sequential order, unlike traditional left-to-right generation
  - Quick check question: How does order-agnostic generation differ from sequential generation, and why does this difference pose challenges for traditional watermarking methods?

## Architecture Onboarding

- Component map: Markov-chain-based key sequence generator -> Vocabulary partitioner -> Watermark embedder -> Key sequence recovery -> Pattern detector -> Statistical tester
- Critical path:
  1. Generate key sequence using Markov chain
  2. Partition vocabulary and associate keys with token subsets
  3. For each generation step, use current key to modify token probabilities
  4. Generate tokens using modified probabilities
  5. During detection, recover key sequence from generated tokens
  6. Count pattern occurrences in recovered key sequence
  7. Calculate p-value using precomputed PT,n distribution
  8. Make detection decision based on p-value threshold
- Design tradeoffs:
  - Pattern length m vs detection efficiency: Longer patterns are rarer and more distinctive but more sensitive to recovery errors
  - Transition matrix probabilities vs generation quality: Stronger watermarking (more predictable patterns) may reduce generation quality
  - Number of keys l vs detection accuracy: More keys provide finer-grained control but increase complexity
- Failure signatures:
  - High false positive rate: Indicates PT,n computation error or pattern selection issue
  - Low true positive rate: Suggests transition matrix not producing distinctive enough patterns
  - Generation quality degradation: May indicate probability promotion too aggressive
  - Key sequence recovery failure: Could be due to vocabulary partitioning not matching generation
- First 3 experiments:
  1. Verify Markov chain key sequence generation produces expected alternating patterns with given transition matrix
  2. Test key sequence recovery accuracy by generating known sequences and checking recovery
  3. Validate PT,n computation by comparing against empirical pattern occurrence frequencies in large random sequences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Markov chain transition matrix probability affect the generation quality and detection efficiency trade-off in PATTERN-MARK?
- Basis in paper: [explicit] The paper states "Generally, increasing the pattern length m leads to improved detection efficiency to some extent. However, excessively long patterns also introduce a higher sensitivity to errors during key sequence recovery. Consequently, while longer patterns enhance detection, they also risk increasing the false positive rate due to their sensitivity to minor errors. This trade-off results in the observed decline in detection efficiency for excessively long patterns." and "we evaluated the quality of content generated using various configurations of the transition matrix A. According to Figure 5, the quality of the generated content remains consistent across different values of a11. Consequently, we select a11 = 0, which corresponds to the strongest watermarking signal, as it does not compromise the quality while enhancing the watermark’s detectability."
- Why unresolved: The paper provides some analysis on the transition matrix, but does not provide a comprehensive study on how different transition matrix probabilities affect the trade-off between generation quality and detection efficiency.
- What evidence would resolve it: A comprehensive study on the effects of different transition matrix probabilities on the trade-off between generation quality and detection efficiency, including experiments with different values of a11 and a22, and analysis of the results.

### Open Question 2
- Question: How does the pattern length m affect the detection efficiency and false positive rate in PATTERN-MARK?
- Basis in paper: [explicit] The paper states "Generally, increasing the pattern length m leads to improved detection efficiency to some extent. However, excessively long patterns also introduce a higher sensitivity to errors during key sequence recovery. Consequently, while longer patterns enhance detection, they also risk increasing the false positive rate due to their sensitivity to minor errors. This trade-off results in the observed decline in detection efficiency for excessively long patterns."
- Why unresolved: The paper provides some analysis on the pattern length, but does not provide a comprehensive study on how different pattern lengths affect the detection efficiency and false positive rate.
- What evidence would resolve it: A comprehensive study on the effects of different pattern lengths on the detection efficiency and false positive rate, including experiments with different values of m, and analysis of the results.

### Open Question 3
- Question: How does PATTERN-MARK perform under different types of attacks, such as paraphrasing attacks on protein generation tasks?
- Basis in paper: [inferred] The paper mentions that "We compare the robustness of PATTERN-MARK against the baselines on both the protein generation and machine translation tasks. For the protein generation task, we employ a random token modification attack, while for the machine translation task, we use a paraphrase attack generated via ChatGPT. Unfortunately, we are unable to evaluate paraphrase attacks on protein generation due to the absence of a suitable rephrasing model."
- Why unresolved: The paper does not provide any results on how PATTERN-MARK performs under paraphrasing attacks on protein generation tasks.
- What evidence would resolve it: Experiments on how PATTERN-MARK performs under paraphrasing attacks on protein generation tasks, including comparison with other watermarking methods.

### Open Question 4
- Question: How does PATTERN-MARK perform under different types of order-agnostic language models, such as non-autoregressive transformers?
- Basis in paper: [inferred] The paper mentions that "In our watermarking framework, we will have a key set K, and the set of all key sequences is denoted by K∗. We denote k[1 : n] as a key sequence of length n." and "We use PM to denote the distribution of a language model and PW to denote the distribution of the watermarked language model. A sequentially decoded language model generates a token sequence conditioned on a given prompt. At any step in this process, the probability of generating the next token xn ∈ V, conditioned on the preceding tokens from x1 to xn−1, is denoted by PM (xn | x1, x2, . . . , xn−1). In order-agnostic LMs, the model does not have access to the full preceding sequence x1:n−1, but it may have access to tokens from other positions. Therefore, we use xoa n to represent all tokens that the order-agnostic LM can observe when generating the n-th token. Under this setting, the probability of generating the next token xn ∈ V for order-agnostic LMs is given by PM (xn | xoa n )."
- Why unresolved: The paper does not provide any results on how PATTERN-MARK performs under different types of order-agnostic language models.
- What evidence would resolve it: Experiments on how PATTERN-MARK performs under different types of order-agnostic language models, including comparison with other watermarking methods.

## Limitations

- Limited generalizability to other order-agnostic language models beyond ProteinMPNN and CMLM
- Uncertainty about the accuracy of the computed probability distribution PT,n for finite sequences
- No evaluation of PATTERN-MARK's performance under paraphrasing attacks on protein generation tasks

## Confidence

- **High Confidence**: The basic mechanism of using Markov chains to generate predictable key sequences for watermarking is sound and well-established in the literature. The pattern-based detection approach is theoretically valid for order-agnostic generation.
- **Medium Confidence**: The experimental results showing superior performance compared to baseline methods (Soft watermark, Unigram, Multikey) are supported by the reported metrics, but the experimental setup and implementation details are not fully specified, limiting reproducibility.
- **Low Confidence**: The claim that PATTERN-MARK maintains comparable generation quality to non-watermarked models while achieving high detection efficiency needs more extensive validation across different model types and tasks.

## Next Checks

- Validation Check 1: Implement a controlled experiment comparing PATTERN-MARK's detection accuracy when the key sequence recovery step is performed with varying levels of noise or partitioning errors. This would quantify the sensitivity of detection performance to implementation accuracy.
- Validation Check 2: Conduct ablation studies on different pattern lengths (m) and transition matrix configurations to determine the optimal tradeoff between detection efficiency and robustness to generation variability.
- Validation Check 3: Extend the evaluation to additional order-agnostic language models beyond ProteinMPNN and CMLM to assess generalizability, particularly focusing on models with different generation mechanisms and vocabulary sizes.