---
ver: rpa2
title: 'STAT: Shrinking Transformers After Training'
arxiv_id: '2406.00061'
source_url: https://arxiv.org/abs/2406.00061
tags:
- pruning
- layer
- network
- heads
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STAT, a structured pruning method for transformers
  that achieves state-of-the-art compression results without fine-tuning. The key
  innovation is using principled matrix factorizations to select and remove attention
  heads and neurons while calculating corrections to preserve accuracy.
---

# STAT: Shrinking Transformers After Training

## Quick Facts
- arXiv ID: 2406.00061
- Source URL: https://arxiv.org/abs/2406.00061
- Reference count: 34
- Primary result: Structured pruning method achieving state-of-the-art compression without fine-tuning

## Executive Summary
STAT introduces a structured pruning method for transformers that eliminates attention heads and neurons while preserving accuracy through principled matrix factorizations. The method leverages pivoted QR factorizations on activation outputs to identify which components to remove, then calculates corrections for subsequent layers to compensate for information loss. STAT can compress BERT in minutes and Llama-2 7B in under 3 hours on a single GPU, outperforming existing gradient-free pruning methods and matching results from approaches requiring extensive fine-tuning.

## Method Summary
STAT uses pivoted QR factorizations on activation outputs from unlabeled data to rank and select attention heads and neurons for pruning. After selecting components to eliminate, it solves a least-squares problem to find a dense interpolation matrix that optimally reconstructs pruned outputs from retained ones. This correction matrix is folded into the next layer's weights. To scale to large models, STAT employs CountSketch for randomized sketching and groups neurons to avoid expensive full-column pivoting. The method requires only a small set of unlabeled data and a pre-trained model, with no fine-tuning needed.

## Key Results
- STAT achieves state-of-the-art structured pruning results across BERT, DistilBERT, and Llama-2 models
- Compresses BERT in minutes and Llama-2 7B in under 3 hours on a single GPU
- Outperforms existing gradient-free pruning methods while matching or exceeding results from approaches requiring extensive fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pivoted QR factorization selects attention heads and neurons that are least critical for preserving network outputs.
- Mechanism: Column-pivoted QR ranks components by how much unique information they contribute based on activation outputs.
- Core assumption: Variance and structure in activation outputs directly reflect component importance.
- Break condition: If activation outputs are highly redundant or noisy, ranking may not reflect true importance.

### Mechanism 2
- Claim: The correction matrix compensates for information lost when pruning heads or neurons.
- Mechanism: Least-squares solve finds dense interpolation matrix that optimally reconstructs pruned outputs from retained ones.
- Core assumption: Relationship between retained and pruned components can be approximated by linear transformation.
- Break condition: If relationship is highly nonlinear or pruning data is unrepresentative, correction will be inaccurate.

### Mechanism 3
- Claim: Scaling to large models is achieved via randomized sketching and neuron grouping.
- Mechanism: CountSketch compresses activation matrix before factorization; neurons are grouped and pruned within each group.
- Core assumption: ID of sketched matrix preserves relative importance ranking of components.
- Break condition: If sketch fails to preserve important subspace structure, pruning decisions will be suboptimal.

## Foundational Learning

- Concept: Column-pivoted QR factorization and subset selection
  - Why needed here: Provides principled way to rank and select columns by how well they span column space
  - Quick check question: What property of diagonal of R in column-pivoted QR indicates most important columns?

- Concept: Interpolative decomposition and structured pruning
  - Why needed here: IDs express full activation matrix as subset plus correction, enabling pruning without retraining
  - Quick check question: How does an ID differ from an SVD in terms of basis vectors it produces?

- Concept: Randomized sketching (CountSketch) for dimensionality reduction
  - Why needed here: Enables STAT to handle very large activation matrices by projecting into lower-dimensional space
  - Quick check question: Why is it acceptable to compute ID on SZ instead of Z when using random sketching matrix S?

## Architecture Onboarding

- Component map: Input pipeline -> Pruning engine -> Postprocessing -> Output
- Critical path: 1) Forward pass on pruning data to collect activations 2) Column-pivoted QR factorization per layer 3) Least-squares solve for correction matrix 4) Update next layer weights 5) Remove pruned components
- Design tradeoffs: Memory vs. accuracy (more pruning data improves ID quality), Speed vs. accuracy (sketching reduces cost), Per-layer vs. global pruning (one-shot allocation)
- Failure signatures: Sudden accuracy drop (pruning too many in critical layer), Slow/failing factorization (try sketching or neuron grouping), Minimal speedup (low FLOP density in pruned regions)
- First 3 experiments: 1) Run STAT on BERT-base with 512 unlabeled samples 2) Vary pruning data size and plot accuracy 3) Enable/disable dense correction step and compare accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does layer normalization placement affect sensitivity of network outputs to pruning errors?
- Basis in paper: Different layer norm placements in BERT vs Llama lead to different error propagation behaviors
- Why unresolved: Paper uses different weighting schemes but lacks systematic theoretical analysis
- What evidence would resolve it: Rigorous mathematical analysis showing how layer norm placement influences error propagation

### Open Question 2
- Question: What is optimal strategy for selecting per-layer pruning ratios to minimize total error within FLOPS budget?
- Basis in paper: Uses one-shot technique with weighted error estimates but notes finding optimal allocation is challenging
- Why unresolved: Current approach uses empirical weighting functions without proving optimality
- What evidence would resolve it: Theoretical framework for optimal layer-wise pruning allocation

### Open Question 3
- Question: How does pruning data size affect trade-off between compression quality and computational cost?
- Basis in paper: Accuracy improves with more pruning data up to ~512 examples
- Why unresolved: Paper only examines limited range and doesn't analyze computational complexity implications
- What evidence would resolve it: Systematic experiments varying pruning data size across multiple model scales

### Open Question 4
- Question: Can two-step attention head pruning process be further optimized to reduce computational cost while maintaining accuracy?
- Basis in paper: Describes two-step process (head selection then dense correction) as necessary for full accuracy
- Why unresolved: Paper demonstrates necessity but doesn't explore alternatives or adaptive strategies
- What evidence would resolve it: Comparison of alternative pruning strategies and demonstration of computational speed-ups

## Limitations
- Relies on small set of unlabeled data that may not capture full input distribution
- Theoretical justification for pivoted QR rankings is primarily empirical rather than rigorously proven
- Does not address unstructured pruning or quantization, limiting deployment scenarios

## Confidence
- High Confidence: Core mechanism of using pivoted QR factorizations and correction matrices is well-supported
- Medium Confidence: Scalability claims through sketching and grouping are reasonable but implementation-dependent
- Medium Confidence: Comparisons to other pruning methods are robust but lack head-to-head testing with all approaches

## Next Checks
1. Ablation study on pruning data size: Vary unlabeled examples (128, 256, 512, 1024) and measure impact on accuracy and FLOP reduction
2. Correction matrix impact isolation: Implement variant skipping correction step entirely and compare accuracy
3. Layer-wise sensitivity analysis: Vary pruning ratio independently for each layer to identify most sensitive layers and test iterative allocation improvements