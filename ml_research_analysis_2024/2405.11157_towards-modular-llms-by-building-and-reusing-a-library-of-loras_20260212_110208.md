---
ver: rpa2
title: Towards Modular LLMs by Building and Reusing a Library of LoRAs
arxiv_id: '2405.11157'
source_url: https://arxiv.org/abs/2405.11157
tags:
- answer
- question
- routing
- tasks
- library
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors address the challenge of building modular large language
  models (LLMs) by reusing parameter-efficient adapters (LoRAs) trained on multiple
  tasks. They propose two main contributions: (1) Model-Based Clustering (MBC), a
  method to group tasks based on the similarity of their LoRA parameters and train
  one adapter per cluster, and (2) Arrow, a zero-shot routing mechanism that dynamically
  selects relevant adapters for new inputs by computing the alignment between hidden
  states and LoRA representations.'
---

# Towards Modular LLMs by Building and Reusing a Library of LoRAs

## Quick Facts
- arXiv ID: 2405.11157
- Source URL: https://arxiv.org/abs/2405.11157
- Authors: Oleksiy Ostapenko; Zhan Su; Edoardo Maria Ponti; Laurent Charlin; Nicolas Le Roux; Matheus Pereira; Lucas Caccia; Alessandro Sordoni
- Reference count: 40
- Primary result: Proposed MBC clustering and Arrow routing methods achieve superior generalization on held-out tasks compared to joint training and other adapter libraries.

## Executive Summary
This paper addresses the challenge of building modular large language models (LLMs) by reusing parameter-efficient adapters (LoRAs) trained on multiple tasks. The authors propose two main contributions: Model-Based Clustering (MBC), a method to group tasks based on LoRA parameter similarity, and Arrow, a zero-shot routing mechanism that dynamically selects relevant adapters for new inputs. Experiments with Phi-2 and Mistral on held-out tasks show that MBC-based adapters and Arrow routing lead to superior generalization compared to traditional joint training and other adapter libraries.

## Method Summary
The paper introduces a modular approach to LLM adaptation through LoRA clustering and dynamic routing. Model-Based Clustering (MBC) groups tasks by analyzing the similarity of their LoRA parameters, training one adapter per cluster to reduce redundancy. Arrow is a zero-shot routing mechanism that selects relevant adapters for new inputs by computing the alignment between hidden states and LoRA representations. This method aims to improve generalization and efficiency compared to traditional joint training or static adapter libraries.

## Key Results
- MBC clustering and Arrow routing outperform joint training and existing adapter libraries on held-out tasks with Phi-2 and Mistral.
- The modular approach demonstrates superior generalization by dynamically selecting relevant adapters for new inputs.
- LoRA parameter similarity correlates with task similarity, enabling effective clustering through MBC.

## Why This Works (Mechanism)
The proposed method leverages the parameter-efficient nature of LoRAs to create a modular system where tasks are grouped based on similarity, reducing redundancy. Arrow routing dynamically selects the most relevant adapters for new inputs by aligning hidden states with LoRA representations, enabling zero-shot adaptation. This approach improves generalization by focusing on task-specific parameters while maintaining modularity and reusability.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**: A parameter-efficient method for fine-tuning LLMs by adding low-rank matrices to the model's weights. *Why needed*: Reduces computational overhead while maintaining performance. *Quick check*: Verify that LoRA matrices are low-rank and sparse.
- **Model-Based Clustering (MBC)**: A method to group tasks based on the similarity of their LoRA parameters. *Why needed*: Identifies related tasks to reduce redundancy and improve modularity. *Quick check*: Confirm that clustered tasks share similar LoRA parameter distributions.
- **Zero-Shot Routing**: A mechanism to dynamically select relevant adapters for new inputs without task-specific training. *Why needed*: Enables adaptability to unseen tasks. *Quick check*: Ensure routing accuracy on held-out tasks.

## Architecture Onboarding
- **Component Map**: LoRA Training -> MBC Clustering -> Adapter Library -> Arrow Routing -> LLM Inference
- **Critical Path**: LoRA Training → MBC Clustering → Adapter Library → Arrow Routing → LLM Inference
- **Design Tradeoffs**: MBC clustering reduces redundancy but may oversimplify task relationships; Arrow routing adds computational overhead but improves adaptability.
- **Failure Signatures**: Poor clustering in MBC may lead to suboptimal adapter reuse; inaccurate routing in Arrow may result in irrelevant adapter selection.
- **First Experiments**: 1) Test MBC clustering on a small set of tasks to validate grouping accuracy. 2) Evaluate Arrow routing on held-out tasks to measure zero-shot performance. 3) Compare computational overhead of modular vs. joint training approaches.

## Open Questions the Paper Calls Out
None

## Limitations
- The clustering methodology (MBC) assumes LoRA parameter similarity correlates with task similarity, which may not hold for diverse task types and domains.
- The zero-shot routing mechanism (Arrow) relies on hidden-state alignment, which may not scale well to complex or heterogeneous tasks.
- Computational overhead of maintaining and routing through multiple LoRA adapters is not fully characterized.

## Confidence
- **High confidence**: Experimental results demonstrating performance improvements over joint training and existing adapter libraries on tested tasks (Phi-2 and Mistral with held-out tasks).
- **Medium confidence**: Effectiveness of MBC clustering in grouping related tasks and general applicability of Arrow routing to unseen tasks.
- **Low confidence**: Scalability to larger models and more diverse task sets, as well as long-term stability of the adapter library.

## Next Checks
1. Test the approach on a broader range of tasks, including multi-modal and domain-specific tasks, to evaluate the robustness of MBC clustering and Arrow routing mechanisms.
2. Analyze the computational overhead of maintaining and routing through multiple LoRA adapters, including memory usage and inference latency, to assess practical feasibility.
3. Validate the stability of the adapter library over time by retraining LoRAs on evolving datasets and measuring degradation in performance.