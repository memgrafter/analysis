---
ver: rpa2
title: Shapley Pruning for Neural Network Compression
arxiv_id: '2407.15875'
source_url: https://arxiv.org/abs/2407.15875
tags:
- shapley
- network
- value
- nodes
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a principled neural network pruning method based
  on Shapley values from coalitional game theory. It frames existing pruning approaches
  like leave-one-out and oracle pruning within a general Shapley value framework,
  then approximates the Shapley value for practical use in pruning convolutional neural
  networks.
---

# Shapley Pruning for Neural Network Compression

## Quick Facts
- arXiv ID: 2407.15875
- Source URL: https://arxiv.org/abs/2407.15875
- Authors: Kamil Adamczewski; Yawei Li; Luc van Gool
- Reference count: 40
- One-line primary result: Achieves state-of-the-art compression on VGG-16, LeNet-5, ResNet-56, and ResNet-50 with significant size reduction while maintaining competitive accuracy.

## Executive Summary
This paper introduces a principled neural network pruning method based on Shapley values from coalitional game theory. The authors frame existing pruning approaches within a general Shapley value framework and develop three approximation schemes to make the method computationally tractable for large networks. The method is evaluated on multiple architectures and datasets, demonstrating significant model compression while preserving performance.

## Method Summary
The method computes parameter importance using Shapley values, which measure marginal contribution across all parameter subsets. Three approximation schemes reduce computational complexity: partial k-greedy (limiting subset size), random permutation sampling, and weighted least-squares regression. For each layer, parameters are ranked by their Shapley values, with lowest-ranked parameters pruned. The pruned model is then retrained to recover accuracy. An Oracle rank benchmark is introduced to evaluate pruning quality by comparing against optimal subsets.

## Key Results
- VGG-16 compressed from 79MB to 3.4MB while maintaining competitive accuracy
- State-of-the-art compression results on VGG-16, LeNet-5, ResNet-56, and ResNet-50
- Outperforms existing methods like SNIP, GraSP, and SynFlow on CIFAR-10, MNIST, and ImageNet datasets
- Reduces model size and inference cost while preserving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shapley value provides a principled way to rank network parameters by their marginal contribution to overall accuracy.
- Mechanism: The Shapley value aggregates the marginal contribution of a parameter across all possible subsets of the network, capturing synergies between parameters that greedy methods miss.
- Core assumption: The accuracy function is additive over parameter subsets and can be reliably estimated via forward passes on validation data.
- Evidence anchors:
  - [abstract]: "connects existing pruning concepts such as leave-one-out pruning and oracle pruning and develop them into a more general Shapley value-based framework"
  - [section]: "The Shapley value is a measure that computes the marginal contribution over all subsets of parameters"
  - [corpus]: Weak evidence - neighboring papers focus on Shapley value for explainability but not specifically for pruning efficiency.
- Break condition: If parameter interactions are highly non-linear or accuracy is not additive, the Shapley approximation may misrepresent true importance.

### Mechanism 2
- Claim: Three approximation schemes (partial k-greedy, random permutation sampling, weighted least-squares regression) make Shapley computation tractable for large networks.
- Mechanism: Each scheme reduces the exponential number of subset evaluations by either restricting subset size, sampling permutations, or framing the problem as regression with sampled binary vectors.
- Core assumption: A limited number of samples is sufficient to approximate the true Shapley values for ranking purposes.
- Evidence anchors:
  - [section]: "three distinct ways to reduce the computational burden are adapted and reformulated to suit the task of node selection"
  - [section]: "approximations via partial k-greedy, random permutation sampling, and weighted least-squares regression"
  - [corpus]: Moderate evidence - sampling-based Shapley approximations are common in literature but specific schemes for pruning are less documented.
- Break condition: If the network has many highly correlated parameters, sampling may miss critical interaction effects.

### Mechanism 3
- Claim: Oracle ranking benchmark provides an upper bound for evaluating pruning rankings.
- Mechanism: For each subset size K, the oracle identifies the subset whose removal minimizes accuracy loss, and the Oracle rank is constructed to maximize overlap with these optimal subsets.
- Core assumption: The oracle subsets can be computed for small layers to provide ground truth, even if not for full networks.
- Evidence anchors:
  - [section]: "introduce the notion of the Oracle ranking, which is the rank created based on the oracle subsets"
  - [section]: "no rank exists that can select both one-element and two-element set which would coincide with the oracle sets"
  - [corpus]: Weak evidence - no neighboring papers discuss Oracle ranking for pruning.
- Break condition: If oracle subsets are not uniquely optimal or depend heavily on random initialization, the benchmark may be unstable.

## Foundational Learning

- Concept: Coalitional game theory and Shapley value definition
  - Why needed here: Provides the theoretical foundation for measuring parameter importance in a way that accounts for interactions
  - Quick check question: Can you write the formula for Shapley value of parameter i in terms of all subsets K not containing i?

- Concept: Neural network pruning and structured vs unstructured approaches
  - Why needed here: Determines what units (weights, channels, filters) can be pruned and how pruning affects network functionality
  - Quick check question: What is the difference between structured pruning of channels versus unstructured pruning of individual weights?

- Concept: Neural network forward pass and accuracy computation
  - Why needed here: The characteristic function Î½(K) requires evaluating network accuracy on validation data for different parameter subsets
  - Quick check question: How would you modify a network to evaluate accuracy when only a subset of channels is active?

## Architecture Onboarding

- Component map:
  - Pre-trained network model with identifiable parameter units (channels, filters, weights)
  - Validation dataset loader
  - Characteristic function evaluator (forward pass accuracy)
  - Shapley approximation modules (partial k-greedy, permutation sampling, regression)
  - Ranking and pruning logic
  - Retraining pipeline for pruned models

- Critical path:
  1. Load pre-trained model and validation data
  2. For each layer, generate subsets or permutations based on chosen approximation
  3. Evaluate characteristic function for each subset
  4. Compute/approximate Shapley values for all parameters
  5. Rank parameters by Shapley value
  6. Prune lowest-ranked parameters
  7. Retrain pruned model

- Design tradeoffs:
  - Accuracy vs compression ratio: More aggressive pruning yields smaller models but may reduce accuracy
  - Approximation quality vs computation time: Higher-quality approximations require more forward passes
  - Structured vs unstructured pruning: Structured pruning is more practical for deployment but may be less optimal

- Failure signatures:
  - Poor pruning results despite high Shapley values: May indicate non-additive accuracy function or insufficient sampling
  - Extremely long computation times: May need to reduce sample size or use simpler approximation
  - Network instability after pruning: May indicate critical parameters were incorrectly ranked

- First 3 experiments:
  1. Run leave-one-out (partial k=1) on a small layer (10-20 parameters) to verify basic functionality and compare with oracle
  2. Implement permutation sampling with S=5-10 samples per parameter and evaluate compression on LeNet-5
  3. Test weighted least-squares regression with L1 regularization on VGG-16 and compare accuracy vs baseline methods

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain implicit in the work.

## Limitations
- Core assumption of additive accuracy function may not hold for complex networks with strong parameter interactions
- Approximation quality verification is difficult without exact Shapley computation, which is intractable for large networks
- Oracle rank benchmark may provide an overly optimistic upper bound and is not practically computable for full networks

## Confidence
- Theoretical foundation: High - Shapley values are well-established in game theory
- Experimental results: Medium - Shows improvement over baselines but limited ablation studies
- Generalization: Medium - Results on multiple architectures but limited to image classification

## Next Checks
1. Implement exact Shapley computation for a small network layer (10-20 parameters) to compare with approximation schemes and establish approximation error bounds.

2. Conduct extensive ablation studies on the three approximation methods across different network architectures and pruning ratios to identify which schemes work best in which scenarios.

3. Test the method on additional datasets (e.g., CIFAR-100, Tiny ImageNet) and architectures (MobileNet, EfficientNet) to evaluate generalization beyond the reported results.