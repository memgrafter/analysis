---
ver: rpa2
title: 'Captions Are Worth a Thousand Words: Enhancing Product Retrieval with Pretrained
  Image-to-Text Models'
arxiv_id: '2402.08532'
source_url: https://arxiv.org/abs/2402.08532
tags:
- text
- product
- dataset
- search
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that pre-trained image-to-text models can
  enhance text-based product retrieval in eCommerce. The authors propose using models
  like instructBLIP and CLIP to generate product descriptions from images, which are
  combined with existing text descriptions.
---

# Captions Are Worth a Thousand Words: Enhancing Product Retrieval with Pretrained Image-to-Text Models

## Quick Facts
- arXiv ID: 2402.08532
- Source URL: https://arxiv.org/abs/2402.08532
- Reference count: 40
- Primary result: Image-to-text models enhance text-based product retrieval by generating descriptions from images that supplement or replace human-crafted text.

## Executive Summary
This paper demonstrates that pre-trained image-to-text models can enhance text-based product retrieval in eCommerce. The authors propose using models like instructBLIP and CLIP to generate product descriptions from images, which are combined with existing text descriptions. They evaluate their approach on the Amazon ESCI dataset, showing that their proposed models can enhance retrieval of existing text and generate highly-searchable standalone descriptions. Specifically, image-generated text marginally underperforms original human-crafted descriptions but outperforms baseline methods as the number of examples to rank increases. Augmenting original product descriptions with image-generated text further enhances overall performance.

## Method Summary
The authors use pre-trained image-to-text models (InstructBLIP, CLIP) to generate product descriptions from images in the Amazon ESCI dataset. These generated descriptions are combined with existing text descriptions when available. The resulting text is embedded and compared to user queries using text-query similarity scoring with fine-tuned cross encoders and transformers. The evaluation uses nDCG to measure item retrieval and ranking performance across text-only, image-only, and hybrid approaches.

## Key Results
- Image-generated text from instructBLIP and CLIP can supplement or replace human-written product descriptions in eCommerce search
- Augmenting existing product text with image-generated text improves overall retrieval performance
- Direct image-query similarity (bypassing text generation) yields marginal improvements over text-based methods

## Why This Works (Mechanism)

### Mechanism 1
Image-generated text from instructBLIP and CLIP can effectively supplement or replace existing human-written product descriptions in eCommerce search. Multimodal models generate detailed, contextually relevant text from product images, which is then embedded and compared to user queries using text-query similarity scoring. The core assumption is that images contain sufficient visual cues (color, material, usage, etc.) that can be reliably translated into searchable text features. Break condition: If generated captions omit critical distinguishing details (e.g., exact product version, dimensions, or technical specifications), retrieval accuracy for exact matches will degrade significantly.

### Mechanism 2
Augmenting existing product text with image-generated text improves overall retrieval performance beyond either source alone. Text-query similarity scoring incorporates both original descriptions and image-derived descriptions, increasing coverage of search-relevant attributes. The core assumption is that the image-generated text captures complementary information not present in the original human-crafted text. Break condition: If the image-generated text is redundant with existing text or introduces noise, the augmentation will not improve and may harm retrieval performance.

### Mechanism 3
Direct image-query similarity (bypassing text generation) yields marginal improvements over text-based methods. Image and query embeddings are directly compared using cosine similarity without intermediate text representation. The core assumption is that visual embeddings capture semantic relevance between images and queries sufficiently well for retrieval. Break condition: High latency or cost of vector database storage and inference outweighs the marginal performance gain, making the approach impractical for real-world deployment.

## Foundational Learning

- Concept: Contrastive learning for multimodal embeddings (CLIP architecture)
  - Why needed here: Enables zero-shot text-image matching without fine-tuning on domain-specific data.
  - Quick check question: How does CLIP's contrastive loss encourage relevant image-text pairs to be closer in embedding space than irrelevant pairs?

- Concept: Vision transformers for image encoding
  - Why needed here: Extracts rich visual features from product images that can be translated into descriptive text.
  - Quick check question: What is the role of patch embeddings and positional encodings in ViT's treatment of images?

- Concept: Text-query similarity scoring with cross-encoders
  - Why needed here: Computes fine-grained semantic similarity between product descriptions (original or generated) and user queries.
  - Quick check question: How does a cross-encoder differ from a bi-encoder in terms of computational efficiency and performance for retrieval tasks?

## Architecture Onboarding

- Component map: Product images → ViT encoder → Visual feature extraction → instructBLIP/CLIP → Text generation/tagging → Product descriptions → Cross-encoder/transformer → Text-query similarity scoring → Ranking scores

- Critical path: 1. Load product images and queries 2. Generate image-derived text using instructBLIP 3. Combine with original text if available 4. Compute similarity scores using cross-encoder 5. Rank products and evaluate with nDCG

- Design tradeoffs: instructBLIP vs CLIP (richer unstructured text vs structured tags), cross-encoder vs transformer (better accuracy vs faster inference), text-only vs image-only vs hybrid (coverage vs computation)

- Failure signatures: Low nDCG despite high-quality images (missing key attributes), slow inference (over-reliance on cross-encoders), high cost (excessive large model usage)

- First 3 experiments: 1. Generate captions for small product set using instructBLIP; manually inspect for coverage 2. Compare text-query similarity scores from instructBLIP-generated vs original text on held-out queries; compute nDCG 3. Test hybrid text (original + instructBLIP) vs original text only on same query set; measure improvement in nDCG

## Open Questions the Paper Calls Out

### Open Question 1
How would scaling the number of irrelevant items in the ESCI dataset affect the relative performance of image-generated text versus human-crafted descriptions? The paper states that with more irrelevant items, image-generated text performance degrades less than human-crafted descriptions, but the full extent of this effect is unknown. The paper only tests up to 20 padding items per query due to computational constraints. Real-world datasets likely have far more irrelevant items. Large-scale experiments with datasets containing hundreds or thousands of irrelevant items per query, comparing nDCG scores for both text types, would resolve this.

### Open Question 2
What is the optimal number and type of tags to extract from images using CLIP models to maximize product search performance? The paper notes that CLIP-based tagging underperforms InstructBLIP's unstructured outputs, suggesting the tag list may be too limited or not well-aligned with user search terms. The paper uses a fixed set of tags without systematically varying their number or type. Controlled experiments varying the number and type of CLIP tags, measuring resulting nDCG scores across multiple queries, would resolve this.

### Open Question 3
How do different large language models for query preprocessing affect the final search performance when combined with image-to-text models? The paper uses ChatGPT-3.5 for query preprocessing and observes consistent but modest improvements, but doesn't compare to other models. Different models may have varying abilities to handle misspellings, vagueness, and non-English terms. Head-to-head comparisons of multiple preprocessing models (GPT-4, Claude, etc.) with identical image-to-text pipelines, measuring final nDCG scores, would resolve this.

## Limitations

- Evaluation is constrained to a single dataset (Amazon ESCI), limiting generalizability to other eCommerce domains or product categories
- The paper claims that image-generated text "marginally underperforms" original human-crafted descriptions, but the specific performance gap is not quantified in the abstract
- Computational cost implications of using large multimodal models at scale are not discussed, which is critical for real-world deployment

## Confidence

- High confidence: The core claim that image-to-text models can generate product descriptions that enhance retrieval performance
- Medium confidence: The claim that augmenting original descriptions with image-generated text improves performance
- Low confidence: The claim about direct image-query similarity yielding "slight improvements"

## Next Checks

1. Extract and compare exact nDCG scores for instructBLIP-generated text versus original human-crafted descriptions across all tested scenarios (text-only, image-only, hybrid) to verify the "marginal underperformance" claim and determine the actual performance gap.

2. Replicate the core experiments with and without image-generated text augmentation on a small subset of the ESCI dataset to verify the claimed performance improvements from hybrid text sources and identify the contribution of each component.

3. Apply the same methodology to a different eCommerce product dataset (e.g., from another retailer or product category) to assess whether the retrieval performance gains observed on Amazon ESCI generalize beyond this specific dataset.