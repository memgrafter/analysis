---
ver: rpa2
title: Putting the Iterative Training of Decision Trees to the Test on a Real-World
  Robotic Task
arxiv_id: '2412.04974'
source_url: https://arxiv.org/abs/2412.04974
tags:
- samples
- episodes
- learning
- agent
- iteration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies an iterative decision tree training algorithm
  to a real-world robotic pendulum balancing task. The method uses samples from a
  trained DQN agent to iteratively build oblique decision trees that can match the
  DQN's performance while using fewer parameters.
---

# Putting the Iterative Training of Decision Trees to the Test on a Real-World Robotic Task

## Quick Facts
- arXiv ID: 2412.04974
- Source URL: https://arxiv.org/abs/2412.04974
- Reference count: 17
- Key outcome: Iterative oblique decision trees match DQN performance on real pendulum task while using 36% fewer parameters

## Executive Summary
This paper demonstrates the successful application of iterative decision tree training to a real-world robotic pendulum balancing task. The method uses samples from a trained DQN agent to build oblique decision trees that can match the DQN's performance while using fewer parameters. The approach involves starting with base samples from successful DQN episodes and progressively adding new samples from tree exploration, labeled by the DQN. After seven iterations, the best decision tree achieved performance on par with the DQN while reducing parameters by approximately 36%.

## Method Summary
The authors implement an iterative oblique decision tree training algorithm on a physical pendulum-cart system (CartPole Swing-up). The method begins with base samples collected from 92 successful DQN episodes, each containing 350 timesteps. Oblique decision trees are trained using the OPCT algorithm with depth 10. The iterative process involves using the best-performing tree to explore new states, labeling these states with the DQN's policy, and adding them to the training dataset for subsequent iterations. This process continues for 10 iterations with 10 trees trained per iteration, and performance is evaluated over 5 episodes for each tree.

## Key Results
- Best decision tree achieved performance on par with DQN (R = 7594.87 ± 826.85 vs 7138.83 ± 1517.47)
- Decision trees reduced parameters by approximately 36% compared to the DQN
- Iterative algorithm successfully handled real-world challenges including sensor noise and time delays
- Pendulum reached zenith state in 90% of runs with the best-performing decision tree

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative sample generation allows decision trees to progressively explore the state space beyond what the DQN covers in its training episodes.
- Mechanism: The algorithm starts with base samples from successful DQN episodes, then uses the best-performing DT to generate new states. These new states are labeled by the DQN and added to the training dataset for the next iteration. This process continues, gradually expanding the coverage of the state space.
- Core assumption: The DQN's policy provides sufficiently accurate action labels for the states generated by the DT, and the DT's exploration is effective at finding relevant but underrepresented states.
- Evidence anchors:
  - [abstract]: "The iterative algorithm starts with base samples from successful DQN episodes and progressively adds new samples from tree exploration, labeled by the DQN."
  - [section]: "Next, the states of the best-performing tree are labeled with the actions suggested by the DRL agent's policy. These new state-action-pairs are added to the previous samples and a new set of DTs is trained on this new, enriched, dataset (iteration 1)."
- Break condition: If the DQN's policy is not sufficiently accurate, or if the DT's exploration fails to find relevant states, the iterative process may not improve performance.

### Mechanism 2
- Claim: Oblique decision trees can match the performance of the DQN while using fewer parameters, making them more interpretable and lightweight.
- Mechanism: The iterative algorithm trains oblique DTs, which use linear combinations of features to partition the data. This allows them to capture complex relationships in the state-action space more efficiently than standard DTs. The best-performing DT after seven iterations achieves performance on par with the DQN while using fewer parameters.
- Core assumption: The state-action space can be effectively partitioned using oblique hyperplanes, and the trade-off between model complexity and performance is favorable for DTs.
- Evidence anchors:
  - [abstract]: "The method uses samples from a trained DQN agent to iteratively build oblique decision trees that can match the DQN's performance while using fewer parameters."
  - [section]: "While a binary DT of depth d = 10 generally has 1023 decision and 1024 leaf nodes, our best-performing DT consists of 599 decision nodes and 600 leaves... This ultimately corresponds to a reduction of parameters of about 36% compared to the DQN."
- Break condition: If the state-action space is too complex to be effectively captured by oblique DTs, or if the performance gap between DTs and DQN is too large, the approach may not be practical.

### Mechanism 3
- Claim: The iterative algorithm is applicable to real-world robotic tasks, which pose additional challenges compared to simulations, such as noise and delays.
- Mechanism: The algorithm is tested on a physical pendulum balancing task, where the pendulum is attached to a cart on a linear track. The real-world implementation involves a Raspberry Pi mini-computer and an STM microcontroller to control the motor, and sensors to measure the state of the system. The results demonstrate the feasibility of using iterative decision trees for real-world reinforcement learning tasks.
- Core assumption: The iterative algorithm can handle the additional challenges of real-world tasks, such as noise, delays, and mechanical factors, without significant degradation in performance.
- Evidence anchors:
  - [abstract]: "The real-world implementation involves a physical pendulum attached to a cart on a linear track... The results demonstrate the feasibility of using iterative decision trees for real-world reinforcement learning tasks."
  - [section]: "It should be noted, that the real-world implementation entails additional challenges for the DRL agent compared to the simulated environment. These include a time delay between issuing a prediction and actually executing the corresponding action, with consequences for the successful training of RL agents that are hard to anticipate. Additionally, sensor noise affects the agent's perception of the system's state, while other physical factors, such as wear and mechanical play, introduce further challenges."
- Break condition: If the real-world challenges are too severe, or if the iterative algorithm cannot adapt to them, the approach may not be applicable to real-world tasks.

## Foundational Learning

- Concept: Reinforcement Learning
  - Why needed here: The task involves training an agent to learn a policy for balancing a pendulum, which is a classic reinforcement learning problem.
  - Quick check question: What is the difference between model-based and model-free reinforcement learning?

- Concept: Decision Trees
  - Why needed here: The iterative algorithm uses decision trees to approximate the DQN's policy, making it more interpretable and lightweight.
  - Quick check question: What is the difference between oblique and non-oblique decision trees?

- Concept: Deep Reinforcement Learning
  - Why needed here: The DQN is a deep reinforcement learning agent that serves as the "teacher" for the decision trees.
  - Quick check question: What are the advantages and disadvantages of using deep neural networks for reinforcement learning compared to decision trees?

## Architecture Onboarding

- Component map:
  DQN agent -> Oblique decision trees -> Real-world pendulum-cart system -> Raspberry Pi/STM microcontroller -> Sensors

- Critical path:
  1. Train the DQN agent on the real-world pendulum balancing task
  2. Collect base samples from successful DQN episodes
  3. Train oblique decision trees on the base samples
  4. Evaluate the performance of the DTs in the real-world environment
  5. Use the best-performing DT to generate new states
  6. Label the new states with the DQN's policy
  7. Add the new state-action pairs to the training dataset
  8. Repeat steps 3-7 for a fixed number of iterations or until a stopping criterion is met

- Design tradeoffs:
  - Model complexity vs. performance: Using deeper or more complex DTs may improve performance but also increase the number of parameters and reduce interpretability
  - Exploration vs. exploitation: The iterative algorithm balances exploration of the state space by the DTs and exploitation of the DQN's policy for labeling the states
  - Real-world vs. simulated environments: The algorithm is tested on a real-world task, which poses additional challenges compared to simulations but also provides a more realistic test of its applicability

- Failure signatures:
  - Poor performance of the DTs compared to the DQN: This may indicate that the DTs are not effectively capturing the state-action space or that the iterative algorithm is not converging
  - Slow convergence or oscillation of the iterative algorithm: This may indicate that the step size or stopping criterion needs to be adjusted
  - High variance in the performance of the DTs across iterations: This may indicate that the exploration by the DTs is not effective or that the DQN's policy is not sufficiently accurate

- First 3 experiments:
  1. Train the DQN agent on the real-world pendulum balancing task and evaluate its performance in 100 episodes
  2. Collect base samples from successful DQN episodes and train oblique decision trees on these samples. Evaluate the performance of the DTs in the real-world environment
  3. Run the iterative algorithm for a few iterations and evaluate the performance of the DTs at each iteration. Analyze the convergence and the effectiveness of the exploration by the DTs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of decision trees compare to deep reinforcement learning agents when applied to more complex robotic tasks beyond the pendulum balancing problem?
- Basis in paper: [inferred] The paper demonstrates successful application of iterative decision trees to a real-world pendulum task but does not explore more complex scenarios.
- Why unresolved: The current study focuses on a relatively simple robotic task, leaving open questions about scalability to more complex environments.
- What evidence would resolve it: Testing the iterative decision tree algorithm on a variety of increasingly complex robotic tasks with different state spaces and action dimensions would provide direct evidence of its scalability.

### Open Question 2
- Question: What is the optimal ratio of base samples to newly generated samples during the iterative training process to achieve the best performance while minimizing computational resources?
- Basis in paper: [explicit] The authors note that the ratio of base samples to new samples is currently suboptimal and requires many iterations to achieve significant improvements.
- Why unresolved: The paper acknowledges the need for optimization of this ratio but does not provide specific guidance or experimental results.
- What evidence would resolve it: Systematic experiments varying the ratio of base to new samples across multiple iterations and tasks would identify optimal ratios for different scenarios.

### Open Question 3
- Question: How does the presence of noise and delays in real-world robotic systems affect the stability and performance of decision tree policies compared to deep neural network policies?
- Basis in paper: [explicit] The authors mention that real-world implementation entails additional challenges like time delays and sensor noise that affect the DRL agent's training and performance.
- Why unresolved: While the paper acknowledges these challenges, it does not specifically analyze how they differentially impact decision tree versus neural network policies.
- What evidence would resolve it: Comparative experiments measuring the robustness of decision tree and neural network policies under varying levels of noise and delay in controlled experiments would provide direct evidence.

## Limitations
- Minimal specification of DQN architecture (only "two dense layers with 64 neurons each")
- Limited evaluation episodes (5 per model) for performance comparison
- Hardware configuration details for physical pendulum-cart system not fully specified

## Confidence
- **High confidence**: The iterative algorithm framework and general methodology are well-described and internally consistent
- **Medium confidence**: The performance comparison between DTs and DQN (R = 7594.87 ± 826.85 vs DQN's 7138.83 ± 1517.47) is based on limited evaluation (5 episodes per model)
- **Low confidence**: The 36% parameter reduction claim depends on specific DT implementation details that are not fully specified

## Next Checks
1. Replicate the DQN training on the physical pendulum-cart system and verify the baseline performance (7138.83 ± 1517.47) with at least 100 evaluation episodes
2. Implement the OPCT algorithm with the specified parameters (depth 10, 10 trees per iteration) and verify convergence behavior across iterations
3. Conduct ablation studies by comparing iterative DT training against non-iterative approaches using the same sample budget and hardware configuration