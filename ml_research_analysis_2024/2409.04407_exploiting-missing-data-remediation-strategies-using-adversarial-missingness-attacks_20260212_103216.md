---
ver: rpa2
title: Exploiting Missing Data Remediation Strategies using Adversarial Missingness
  Attacks
arxiv_id: '2409.04407'
source_url: https://arxiv.org/abs/2409.04407
tags:
- missingness
- data
- imputation
- modeler
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adversarial Missingness (AM) attacks manipulate model learning
  by engineering missingness patterns in data, a novel threat model distinct from
  data poisoning. This work introduces a general bi-level optimization framework to
  launch AM attacks on widely-used missing data remediation strategies (complete case
  analysis, mean imputation, and regression-based imputation) for differentiable empirical
  risk minimization problems.
---

# Exploiting Missing Data Remediation Strategies using Adversarial Missingness Attacks

## Quick Facts
- arXiv ID: 2409.04407
- Source URL: https://arxiv.org/abs/2409.04407
- Reference count: 31
- Key outcome: AM attacks can manipulate model learning by engineering missingness patterns, suppressing feature significance and inflating ATE estimates with less than 20% missingness, even when adversaries modify only a subset of training data.

## Executive Summary
This paper introduces Adversarial Missingness (AM) attacks, a novel threat model where adversaries manipulate model learning by strategically engineering missingness patterns in data rather than injecting malicious data or perturbations. The authors develop a general bi-level optimization framework to launch AM attacks against widely-used missing data remediation strategies including complete case analysis, mean imputation, and regression-based imputation. Through experiments on real-world datasets, they demonstrate that AM attacks can effectively suppress feature significance and inflate average treatment effect (ATE) estimates, achieving manipulation with modest levels of missingness (under 20%). Notably, these attacks generalize across different model architectures and are effective even when adversaries can only modify a subset of the training data.

## Method Summary
The paper proposes a bi-level optimization framework where the adversary optimizes missingness mechanism parameters to minimize their objective while limiting missingness rate. The upper level optimizes the missingness mechanism parameters ϕ to minimize the adversarial objective g(θ̃;X) + λ·Ω(p_R|X;X), where θ̃ is the model learned in the lower level problem. The lower level simulates the modeler's training process using a differentiable proxy objective that approximates the modeler's objective function. This framework enables gradient-based optimization of missingness patterns against various missing data handling methods. The attacks are tested across multiple datasets including Twins, wine-quality, german-credit, ca-housing, and diabetes, demonstrating effectiveness in manipulating both feature significance and ATE estimates.

## Key Results
- AM attacks can suppress feature significance, making targeted features statistically insignificant in downstream models
- ATE estimates can be inflated from -1.61% to 10% with less than 20% missingness
- Attacks remain effective when adversaries can only modify a subset of training data
- Attacks generalize across different model architectures (TARnet, Tnet, Causal Forest) and imputation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adversary can manipulate missingness patterns to bias downstream model parameters even without injecting malicious data.
- Mechanism: The adversarial missingness (AM) attack modifies the missingness mask R while preserving all original data values. By carefully choosing which entries become missing, the adversary steers the model fitting process through a differentiable proxy objective that approximates the impact of the missingness on model training.
- Core assumption: The missingness mechanism p(R|X) is differentiable with respect to its parameters ϕ and the modeler uses a differentiable objective for handling missing data (CCA, mean imputation, or regression-based imputation).
- Break condition: If the modeler uses a non-differentiable missingness handling method or the missingness mechanism is not differentiable in its parameters.

### Mechanism 2
- Claim: The bi-level optimization framework enables the adversary to learn a missingness mechanism that minimizes their objective while limiting missingness rate.
- Mechanism: The upper level optimizes the missingness mechanism parameters ϕ to minimize the adversarial objective g(θ̃;X) + λ·Ω(p_R|X;X), where θ̃ is the model learned in the lower level problem. The lower level simulates the modeler's training process using the differentiable proxy objective.
- Core assumption: The modeler's training process can be approximated by a differentiable function of the missingness mechanism.
- Break condition: If the modeler's training process is highly non-convex and the differentiable proxy fails to capture its behavior, or if the adversary cannot solve the bi-level optimization efficiently.

### Mechanism 3
- Claim: Attacks generalize across different model architectures and imputation strategies even when optimized against a simpler proxy.
- Mechanism: The adversary optimizes the missingness mechanism against a logistic regression proxy, but the resulting mechanism still manipulates ATE estimates when computed using more complex models like TARnet, Tnet, or Causal Forest.
- Core assumption: The structural patterns of missingness that bias simpler models also transfer to more complex architectures.
- Break condition: If the target model's architecture or training procedure fundamentally changes how missingness affects parameter estimation.

## Foundational Learning

- Concept: Bi-level optimization
  - Why needed here: The adversary needs to optimize the missingness mechanism while accounting for how the modeler will respond to that missingness.
  - Quick check question: In the AM attack framework, what does the upper level optimize and what does the lower level compute?

- Concept: Differentiable approximation of missing data handling
  - Why needed here: The modeler's objective is not directly differentiable with respect to the missingness mechanism, so a proxy is needed for gradient-based optimization.
  - Quick check question: How does the differentiable proxy for mean imputation account for the dependence between the missingness mechanism and the imputed values?

- Concept: Average Treatment Effect (ATE) estimation
  - Why needed here: The paper demonstrates AM attacks on ATE estimation, showing how missingness can manipulate causal inference.
  - Quick check question: In the Twins dataset example, what is the ground truth ATE and what value does the adversary try to manipulate it to?

## Architecture Onboarding

- Component map: Data supplier -> Adversarial mechanism -> Modeler -> Evaluation
- Critical path:
  1. Initialize missingness mechanism parameters ϕ
  2. For each training iteration:
     - Sample missingness masks from p(R|X;ϕ)
     - Apply masks to create partially observed dataset X̄
     - Simulate modeler's training process using differentiable proxy
     - Update ϕ using gradients from upper-level objective
  3. Deploy learned missingness mechanism to poison dataset
- Design tradeoffs:
  - Differentiable proxy accuracy vs. computational efficiency
  - Missingness rate vs. attack effectiveness (controlled by λ)
  - Per-data-point parameterization vs. neural network parameterization of missingness mechanism
- Failure signatures:
  - Missingness rate remains high despite optimization (λ too small)
  - Attack fails to manipulate target coefficient significance (proxy inaccurate)
  - Convergence issues in bi-level optimization (non-convex lower-level problem)
- First 3 experiments:
  1. Test CCA attack on wine-quality dataset targeting alcohol feature - verify target becomes insignificant
  2. Test mean imputation attack on ca-housing dataset - verify manipulated coefficient and missingness rate
  3. Test ATE manipulation on Twins dataset with 100% data access - verify estimated ATE shifts from -1.61% to near 10%

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are AM attacks against deep learning models (e.g., neural networks) for missing data imputation?
- Basis in paper: [explicit] The paper states "Our empirical examples target tabular data" and "We defer extensions to other modalities (e.g., deep nets, images) and efficiency improvements to future work."
- Why unresolved: The paper only tested AM attacks on tabular data with traditional imputation methods. Deep learning models may have different vulnerabilities or robustness to adversarial missingness.
- What evidence would resolve it: Empirical studies comparing AM attack success rates on deep learning imputation methods versus traditional methods across various data modalities and architectures.

### Open Question 2
- Question: Can data valuation methods be effectively adapted as a defense against AM attacks?
- Basis in paper: [explicit] The paper tested data valuation defenses (KNN Shapley and LA V A) against AM attacks, finding partial success but noting that the ℓ₁ distance between coefficients remained significantly smaller than the distance to true coefficients.
- Why unresolved: While some data valuation methods showed limited effectiveness, the paper did not explore other variations or combinations of data valuation approaches that might provide stronger defense.
- What evidence would resolve it: Comparative analysis of multiple data valuation methods and their combinations, measuring both p-value recovery and coefficient distance metrics under various AM attack scenarios.

### Open Question 3
- Question: How do AM attacks perform against more sophisticated joint modeling approaches that assume conditional independence of missingness indicators?
- Basis in paper: [explicit] The paper mentions that "Recent work (Ipsen, Mattei, and Frellsen 2021; Ma and Zhang 2021; Ghalebikesabi et al. 2021) considers learning deep generative models to impute the missing entries" and notes these make specific assumptions about missingness mechanisms.
- Why unresolved: The paper only tested against traditional imputation methods and mentions these approaches might show robustness but doesn't empirically verify this claim.
- What evidence would resolve it: Empirical testing of AM attacks against various joint modeling approaches with different missingness mechanism assumptions, measuring attack success rates and comparing to traditional methods.

## Limitations

- The paper relies heavily on differentiable approximations that may not fully capture real-world missing data handling behaviors
- Claims about cross-architecture generalization are supported by limited experimental evidence
- No current defenses exist against AM attacks, and the paper does not explore potential mitigation strategies

## Confidence

- **High confidence**: The bi-level optimization framework is technically sound and the threat model is well-defined
- **Medium confidence**: The experimental results showing ATE manipulation and feature significance suppression appear reproducible
- **Low confidence**: Claims about cross-architecture generalization and effectiveness against complex models need more rigorous validation

## Next Checks

1. Reproduce the ATE manipulation experiments on the Twins dataset with varying missingness rates (5%, 10%, 20%) to verify the claimed inflation from -1.61% to 10% is consistently achievable
2. Test the AM attack framework against non-differentiable missing data handling methods (e.g., k-nearest neighbors imputation) to assess the limits of the differentiable proxy approach
3. Evaluate potential defense mechanisms such as robust missing data imputation or adversarial training to determine if the attacks can be mitigated in practice