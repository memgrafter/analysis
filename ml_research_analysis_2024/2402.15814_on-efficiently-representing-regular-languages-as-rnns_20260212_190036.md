---
ver: rpa2
title: On Efficiently Representing Regular Languages as RNNs
arxiv_id: '2402.15814'
source_url: https://arxiv.org/abs/2402.15814
tags:
- stack
- rnns
- languages
- function
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the question of which classes of language models
  (LMs) can be efficiently represented by recurrent neural networks (RNNs), extending
  previous work that focused on hierarchical languages. The authors introduce probabilistic
  bounded pushdown automata (BPDAs) as a general framework for studying RNNs' representational
  capacity.
---

# On Efficiently Representing Regular Languages as RNNs

## Quick Facts
- arXiv ID: 2402.15814
- Source URL: https://arxiv.org/abs/2402.15814
- Reference count: 34
- The paper shows RNNs can efficiently represent a broader class of language models than previously thought, specifically those defined by probabilistic bounded pushdown automata (BPDAs) with certain structural properties.

## Executive Summary
This paper addresses the question of which classes of language models (LMs) can be efficiently represented by recurrent neural networks (RNNs). The authors generalize previous work on hierarchical languages by introducing probabilistic bounded pushdown automata (BPDAs) as a framework for studying RNNs' representational capacity. They show that RNNs can efficiently represent LMs defined by BPDAs with specific stack update functions and representation compatibility, including n-gram and bounded Dyck languages. This suggests that RNNs' inductive biases are not inherently tied to hierarchical structures, expanding our understanding of their capabilities.

## Method Summary
The authors introduce probabilistic bounded pushdown automata (BPDAs) as a general framework for studying RNNs' representational capacity. They define three key properties for BPDAs: stack-affine functions, K-varied functions, and Σ-determined functions. The main result is a theorem providing sufficient conditions for efficient representation, which involves the BPDA's transition function conforming to a specific structure and being representation-compatible. The construction is demonstrated on n-gram and bounded Dyck languages, showing that RNNs can efficiently represent these language models under the stated conditions.

## Key Results
- RNNs can efficiently represent a broader class of language models than previously thought, specifically those defined by BPDAs with certain structural properties.
- The key is exploiting the bounded stack structure to encode relevant symbols, with the hidden state acting as a vectorial representation of this stack.
- The paper provides a theorem giving sufficient conditions for efficient representation, which generalizes prior work and offers new insights into RNNs' capabilities.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RNNs can efficiently represent a broader class of language models than previously thought, specifically those defined by probabilistic bounded pushdown automata (BPDAs) with certain structural properties.
- Mechanism: The key is exploiting the bounded stack structure to encode relevant symbols that have occurred in the string so far. The hidden state of the RNN acts as a vectorial representation of this stack, where different parts of the hidden state serve as placeholders for these symbols. The RNN updates this representation using affine transformations and a Σ-determined function that only depends on the input symbol.
- Core assumption: The transition function of the BPDA conforms to a specific structure (Eq. 16) involving a Σ-determined function α and a K-varied function ζ where all ζk are stack-affine. Additionally, the BPDA must be representation-compatible.
- Evidence anchors:
  - [abstract] "We generalize Hewitt et al.'s (2020) construction and show that RNNs can efficiently represent a larger class of LMs than previously claimed—specifically, those that can be represented by a pushdown automaton with a bounded stack and a specific stack update function."
  - [section 4] "The requirement for the BPDA to be representation compatible is crucial for Thm. 4.1; there exist BPDAs that fulfill all the criteria of Thm. 4.1 but the one on representation compatibility that are not efficiently representable by RNN LMs."
  - [corpus] Weak. The corpus contains papers on expressivity of RNNs but none directly discuss the BPDA framework or the specific structural properties required for efficient representation.
- Break condition: If the BPDA's transition function does not conform to the required structure, or if it is not representation-compatible, then the RNN cannot efficiently represent it. Additionally, if the stack size m becomes too large relative to the desired hidden state size D, efficient representation becomes impossible.

### Mechanism 2
- Claim: The specific structure of the BPDA's transition function allows for efficient representation by an RNN through parameter sharing and the use of the Heaviside activation function.
- Mechanism: The transition function is decomposed into K different stack-affine functions, each invariant to a subset of input symbols (K-varied). These functions are implemented by affine transformations of the hidden state (U and b parameters). The Σ-determined function α is implemented by the input matrix V and the Heaviside activation function, which selects and modifies the appropriate parts of the hidden state based on the input symbol.
- Core assumption: The stack-affine functions can be implemented by affine transformations of the vectorial stack representation, and the Σ-determined function can be implemented by the Heaviside activation function acting on the appropriately transformed hidden state.
- Evidence anchors:
  - [section 4.1] "Since each input symbol modifies the stack in a deterministic way—either it is discarded (in case of popping) or is added to the top position of the stack (in case of pushing)—the BPDA is Σ-determined."
  - [section C.1] "To perform the K different stack-affine functions, we define the parameters U and b... We also define the input matrix V... We now show that the parameters defined above simulate the stack update function correctly."
  - [corpus] Weak. The corpus mentions RNN generalization and expressivity but does not provide specific evidence for the parameter sharing or the use of the Heaviside activation function in this context.
- Break condition: If the stack-affine functions cannot be implemented by affine transformations, or if the Σ-determined function cannot be implemented by the Heaviside activation function acting on the transformed hidden state, then the RNN cannot efficiently represent the BPDA.

### Mechanism 3
- Claim: The representation-compatibility of the BPDA ensures that the next-symbol probabilities can be efficiently encoded by the output parameters of the RNN.
- Mechanism: The representation-compatibility condition (Def. 3.6) requires that the log-probabilities of the next symbol given the current stack configuration can be expressed as a softmax of an affine transformation of the vectorial stack representation. This allows the RNN to compute the same next-symbol probabilities as the BPDA using the output matrix E and bias vector u.
- Core assumption: The BPDA's next-symbol probability distributions are such that their log-probabilities can be expressed as a softmax of an affine transformation of the vectorial stack representation.
- Evidence anchors:
  - [section 3.1] "Definition 3.6. A deterministic BPDA is representation-compatible if there exists a matrix E P R|Σ|ˆmG and a vector u P R|Σ| such that, for every γ P Γďm, it holds for all y P Σ that log p py | γq " softmaxpEχpγq ` uqy."
  - [section C.1] "By definition, P then defines next-symbol probabilities p py | yq where log p py | φ pyqq " softmax`E1χpφ pyqq ` u1˘y for some matrix E1 P R|Σ|ˆmG and u1 P R|Σ|. The first part of the proof shows that h pyq contains exactly one copy of χpφ pyqq. We use that fact and define E and u, which will result in the softmax-normalized RNN computing identical next-symbol probabilities to those computed by P."
  - [corpus] Weak. The corpus mentions softmax bottleneck and language model expressivity but does not directly discuss the representation-compatibility condition or its role in efficient RNN representation.
- Break condition: If the BPDA's next-symbol probability distributions are not representation-compatible, then the RNN cannot efficiently encode them, and the weak equivalence between the RNN and the BPDA is lost.

## Foundational Learning

- Concept: Finite-state automata (FSAs) and their computational power.
  - Why needed here: The paper builds upon the classic result that RNNs are equivalent to FSAs, and then explores how efficiently RNNs can represent subclasses of FSA languages.
  - Quick check question: What is the computational power of deterministic finite-state automata compared to non-deterministic finite-state automata?

- Concept: Pushdown automata (PDAs) and their relationship to context-free languages.
  - Why needed here: BPDAs are a bounded version of PDAs, and understanding PDAs is crucial for grasping the generalization of Hewitt et al.'s (2020) result.
  - Quick check question: How does the addition of a stack to a finite-state automaton increase its computational power?

- Concept: Language models (LMs) and their formalization as probability distributions over strings.
  - Why needed here: The paper studies the representational capacity of RNNs as LMs, so understanding LMs as probabilistic models is essential.
  - Quick check question: What is the difference between a language and a language model in the context of formal language theory?

## Architecture Onboarding

- Component map:
  - Input (one-hot encoded symbols from Σ) -> Hidden state (vector of size D representing stack configuration) -> Output (logits of next-symbol probability distribution)

- Critical path:
  1. One-hot encode the input symbol.
  2. Update the hidden state using the recurrence ht " H pUht´1 ` Vrpytq ` bq.
  3. Compute the logits of the next-symbol probability distribution using Ehpytq ` u.
  4. Apply softmax to obtain the final probability distribution.

- Design tradeoffs:
  - Stack size m vs. hidden state size D: A larger stack size allows for more complex languages to be represented but requires a larger hidden state.
  - Number of partitions K vs. model complexity: A larger K allows for more diverse stack update functions but increases the complexity of the model.
  - Representation-compatibility: Restricting to representation-compatible BPDAs simplifies the output layer but limits the class of representable languages.

- Failure signatures:
  - If the model fails to learn the correct stack updates, the hidden state will not accurately represent the current stack configuration, leading to incorrect next-symbol predictions.
  - If the model fails to learn the correct next-symbol probabilities, the output layer will not accurately model the BPDA's probability distribution, leading to poor language modeling performance.
  - If the model is not representation-compatible, it will not be able to efficiently represent the BPDA, regardless of its training performance.

- First 3 experiments:
  1. Implement a simple BPDA that models an n-gram language model and verify that the RNN can efficiently represent it.
  2. Implement a BPDA that models a bounded Dyck language and compare the RNN's performance to the construction by Hewitt et al. (2020).
  3. Implement a BPDA that is not representation-compatible and demonstrate that the RNN cannot efficiently represent it.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between bounded stack languages and known subregular language classes?
- Basis in paper: [inferred] The paper mentions this as an open problem in the discussion section, noting that efficiently representable BPDAs do not naturally map to known subregular classes.
- Why unresolved: The authors state they plan to investigate this in future work, suggesting the relationship is not yet characterized.
- What evidence would resolve it: A formal proof showing either inclusion or separation between bounded stack languages and subregular classes, or a complete classification of bounded stack languages in terms of subregular hierarchies.

### Open Question 2
- Question: How does the learnability of bounded stack languages by RNNs compare to their representational efficiency?
- Basis in paper: [explicit] The authors discuss this in the context of inductive biases, noting that both theoretical and empirical insights are needed to understand how well RNNs can learn these representations during training.
- Why unresolved: The paper focuses on representational capacity, not learning dynamics. The authors explicitly state this is left for future work.
- What evidence would resolve it: Empirical studies comparing RNN training performance on bounded stack languages versus other language classes, or theoretical bounds on sample complexity for learning these representations.

### Open Question 3
- Question: Can the efficient representational capacity of RNNs be characterized with tight lower and upper bounds?
- Basis in paper: [explicit] The authors acknowledge in the limitations section that they do not provide tight bounds on the complexity of languages efficiently representable by RNNs.
- Why unresolved: The authors note this is a challenging open problem due to the complexity of the parameterization and the variety of possible language classes.
- What evidence would resolve it: A formal proof establishing both lower and upper bounds on the computational complexity of languages efficiently representable by RNNs, or a complete characterization of the class of languages in terms of known complexity hierarchies.

## Limitations

- The representation compatibility condition is crucial but may be challenging to verify for complex BPDAs beyond the provided examples.
- The paper focuses on representational capacity rather than learning dynamics, leaving questions about RNNs' ability to learn these representations during training.
- The bounds on efficient representation are not tight, and the exact class of languages efficiently representable by RNNs remains to be fully characterized.

## Confidence

- High: The theoretical framework is well-grounded and the construction for specific cases (n-gram and bounded Dyck languages) is clearly demonstrated.
- Medium: The generalizability to all BPDAs meeting the stated conditions remains somewhat theoretical without empirical validation on a broader set of examples.
- Low: The practical verifiability of weak equivalence between BPDAs and RNNs is computationally infeasible without further methodological developments.

## Next Checks

1. Empirical validation: Implement and test the construction on a diverse set of BPDAs beyond the provided examples, including non-trivial representation-compatible BPDAs, to verify the practical efficiency of the RNN representation.

2. Representation compatibility verification: Develop a systematic method or set of criteria to efficiently verify the representation compatibility condition for arbitrary BPDAs, as this is currently a potential bottleneck in applying the theorem.

3. Comparison with learning dynamics: Study how well standard RNN training algorithms (e.g., backpropagation through time) can learn to represent BPDAs that meet the conditions, as opposed to requiring explicit construction of the RNN parameters.