---
ver: rpa2
title: 'Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop
  Question Answering'
arxiv_id: '2404.14464'
source_url: https://arxiv.org/abs/2404.14464
tags:
- question
- answer
- retrieval
- paragraphs
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Tree of Reviews (ToR), a tree-based dynamic
  retrieval framework for multi-hop question answering. The method addresses limitations
  of chain-like iterative retrieval approaches by introducing a tree structure where
  the root is the question and other nodes are retrieved paragraphs.
---

# Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop Question Answering

## Quick Facts
- **arXiv ID**: 2404.14464
- **Source URL**: https://arxiv.org/abs/2404.14464
- **Reference count**: 18
- **Primary result**: ToR achieves state-of-the-art performance on HotpotQA, 2WikiMQA, and MuSiQue with improved recall@15 and EM/F1 scores

## Executive Summary
This paper introduces Tree of Reviews (ToR), a novel tree-based dynamic iterative retrieval framework for multi-hop question answering. ToR addresses limitations of chain-like iterative retrieval approaches by introducing a tree structure where each node contains a single paragraph evaluated independently by a paragraph review block. The framework dynamically decides whether to search further, accept, or reject based on relevance and sufficiency of information along each reasoning path. Experiments on three datasets demonstrate ToR achieves state-of-the-art performance in both retrieval quality and response generation.

## Method Summary
ToR constructs a tree with the question as the root and retrieved paragraphs as nodes. Each node is evaluated by a paragraph review block that decides whether to continue search, accept, or reject based on relevance and sufficiency of information. The framework explores multiple reasoning paths to mitigate error propagation. Two optimization strategies are proposed: pruning (relevance and repetitive) and effective expansion (CoT and missing paragraph completion). These strategies improve search efficiency and path diversity while maintaining or improving performance metrics.

## Key Results
- Achieves state-of-the-art performance on HotpotQA, 2WikiMQA, and MuSiQue datasets
- Improves recall@15 scores compared to chain-like iterative retrieval methods
- Demonstrates better EM/F1 scores in response generation through evidence pool fusion strategies
- Shows effective balance between search efficiency and answer quality through pruning strategies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Tree structure mitigates error propagation by isolating reasoning paths at the paragraph level
- **Mechanism**: Each node contains a single paragraph and is evaluated independently by the paragraph review block, preventing irrelevant information from affecting the entire reasoning path
- **Core assumption**: Separating paragraphs into individual nodes prevents cascading errors that occur when chain-like methods process multiple paragraphs together
- **Evidence anchors**:
  - [abstract]: "we introduce a tree structure to handle each retrieved paragraph separately, alleviating the misleading effect of irrelevant paragraphs on the reasoning path"
  - [section 3.2]: "Each node contains a single paragraph, mitigating the risk of diverging the reasoning process due to irrelevant information"
- **Break condition**: If paragraph review block cannot effectively judge relevance and sufficiency, irrelevant paragraphs may still propagate through the tree

### Mechanism 2
- **Claim**: Dynamic decision-making at each node enables targeted information gathering
- **Mechanism**: Paragraph review block decides whether to search further, accept, or reject based on current path information, allowing the model to identify missing information and generate targeted queries
- **Core assumption**: LLM can accurately assess whether current paragraphs contain sufficient information to answer the question
- **Evidence anchors**:
  - [abstract]: "Our framework dynamically decides to initiate a new search, reject, or accept based on the paragraphs on the reasoning paths"
  - [section 3.2]: "The block is designed to execute the following steps: (i) Judging whether the paragraphs are relevant to the question Q. (ii) Judging whether the paragraphs have enough information"
- **Break condition**: If paragraph review block makes incorrect judgments about relevance or sufficiency, the search may terminate prematurely or continue unnecessarily

### Mechanism 3
- **Claim**: Multiple reasoning paths increase the likelihood of finding correct evidence
- **Mechanism**: Tree structure explores diverse paths simultaneously, reducing the impact of a single reasoning error on the final answer
- **Core assumption**: Different reasoning paths are likely to contain complementary information
- **Evidence anchors**:
  - [abstract]: "the diversity of reasoning path extension reduces the impact of a single reasoning error on the whole"
  - [section 3.1]: "These blocks dynamically judge whether to stop or continue the search based on all paragraphs along the path from the root node to the current node"
- **Break condition**: If all reasoning paths converge on incorrect information or if the tree structure limits exploration of relevant paths

## Foundational Learning

- **Concept**: Chain-of-thought reasoning
  - Why needed here: To generate intermediate reasoning steps that guide iterative retrieval
  - Quick check question: Can the LLM generate step-by-step reasoning that identifies missing information in the current context?

- **Concept**: Iterative retrieval
  - Why needed here: To progressively gather information for multi-hop questions that cannot be answered in a single retrieval step
  - Quick check question: Does the system correctly generate new queries based on the information gap identified in previous retrieval steps?

- **Concept**: Tree traversal algorithms
  - Why needed here: To systematically explore multiple reasoning paths in the tree structure
  - Quick check question: Can the system correctly implement depth-first or breadth-first search through the reasoning tree?

## Architecture Onboarding

- **Component map**: Question → Dense retriever (Contriver) → Paragraph review block → Evidence pool → Reader → Final answer
- **Critical path**: Question → Dense retriever → Paragraph review block → Evidence pool → Reader → Final answer
- **Design tradeoffs**:
  - Tree depth vs. computational cost: deeper trees explore more paths but require more LLM calls
  - Breadth vs. efficiency: wider trees retrieve more paragraphs but may include more irrelevant information
  - Evidence fusion methods: analysis-based, paragraph-based, or combined approaches balance different information sources
- **Failure signatures**:
  - Low recall@15 with high EM/F1: indicates model may be overfitting to evidence pool without proper retrieval
  - High recall@15 with low EM/F1: suggests retrieved paragraphs contain relevant information but model cannot effectively synthesize it
  - Excessive LLM calls with minimal evidence: indicates pruning strategies are not effectively reducing redundant searches
- **First 3 experiments**:
  1. Baseline comparison: Run ToR against chain-like methods (ReAct, Self-Ask) on HotpotQA to verify improvement in recall@15
  2. Pruning effectiveness: Compare ToR with and without pruning strategies on 2WikiMQA to measure reduction in API calls and impact on performance
  3. Tree depth exploration: Vary tree depth from 2 to 4 on MuSiQue to identify optimal depth for balancing performance and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Tree of Reviews (ToR) framework perform on more complex reasoning tasks beyond multi-hop question answering, such as multi-step mathematical reasoning or logical inference problems?
- Basis in paper: [explicit] The authors acknowledge that the performance of ToR on other complex reasoning tasks "still requires further verification" and express hope that future work can leverage the tree structure to achieve favorable results in a broader array of complex reasoning tasks.
- Why unresolved: The paper only validates the effectiveness of ToR on multi-hop question answering tasks. There is no experimental evidence or analysis of its performance on other types of complex reasoning problems.
- What evidence would resolve it: Conducting experiments to evaluate ToR's performance on diverse complex reasoning tasks, such as mathematical problem solving, logical inference, or commonsense reasoning, and comparing the results with existing state-of-the-art methods for those tasks.

### Open Question 2
- Question: What is the impact of the tree depth and width on the performance and efficiency of the ToR framework, and how can the optimal configuration be determined for different types of questions or datasets?
- Basis in paper: [explicit] The authors discuss the impact of tree depth and width on performance in Section 4.5, noting that increasing depth improves retrieval and generation metrics but also increases the number of LLM calls. They suggest that a reasonable limit on tree depth is necessary to ensure search efficiency.
- Why unresolved: While the authors provide some insights into the trade-offs between depth, width, and performance, they do not offer a systematic approach for determining the optimal tree configuration for different scenarios. The choice of depth (3) and widths (5, 3, 3) in the main experiment appears to be based on empirical observation rather than a principled method.
- What evidence would resolve it: Conducting a comprehensive analysis of the relationship between tree depth, width, and performance across various question types and datasets, and developing a method for automatically determining the optimal tree configuration based on the characteristics of the input question and available resources.

### Open Question 3
- Question: How can the ToR framework be adapted to handle real-time, dynamic knowledge sources where the indexed documents are frequently updated, and how does this affect the performance and reliability of the system?
- Basis in paper: [inferred] The authors mention that in practical application scenarios like New Bing and Perplexity.AI, the indexed document scope is broader, and the retrieval source is updated in real-time. However, they do not discuss how ToR can be adapted to handle such dynamic knowledge sources or how the frequent updates might impact the system's performance.
- Why unresolved: The paper focuses on evaluating ToR using static knowledge sources (Wikipedia dumps from specific dates). There is no discussion of how the framework would perform when the underlying knowledge source is constantly changing or how it can be modified to efficiently handle real-time updates.
- What evidence would resolve it: Conducting experiments to evaluate ToR's performance on dynamically updated knowledge sources, comparing it with baseline methods, and analyzing the impact of update frequency on retrieval quality, response generation, and overall system efficiency. Additionally, exploring potential modifications to the ToR framework to better handle real-time knowledge updates, such as incorporating incremental indexing or adaptive pruning strategies.

## Limitations

- Tree structure may not be optimal for questions requiring sequential information gathering where later steps depend on earlier findings
- Paragraph review block relies heavily on LLM judgment capabilities, which may introduce errors for complex or ambiguous questions
- Effectiveness of pruning strategies depends on quality of relevance and repetitive detection, which may not generalize well across different domains

## Confidence

- **High confidence**: The tree structure effectively mitigates error propagation compared to chain-like methods. This is supported by strong empirical results showing improved recall@15 and EM/F1 scores across all three datasets.
- **Medium confidence**: The paragraph review block can accurately judge relevance and sufficiency of information. While the mechanism is clearly described, its effectiveness depends on LLM capabilities which may vary across implementations.
- **Medium confidence**: The pruning and effective expansion strategies significantly improve search efficiency without sacrificing performance. The paper provides ablation studies, but the generalizability of these strategies to other domains remains uncertain.

## Next Checks

1. **Error Propagation Analysis**: Implement ablation studies comparing ToR with and without the tree structure on questions where chain-like methods typically fail due to error propagation. Measure how often incorrect paragraphs in early steps lead to completely wrong final answers.

2. **Pruning Strategy Robustness**: Test ToR's pruning strategies across different domains (news articles, scientific papers, technical documentation) to evaluate whether the relevance and repetitive detection mechanisms generalize beyond Wikipedia-based multi-hop questions.

3. **Tree Depth Optimization**: Conduct experiments varying tree depth from 2 to 6 on each dataset to identify the optimal depth that balances computational cost (LLM API calls) with performance gains, and analyze at which depth diminishing returns set in.