---
ver: rpa2
title: Advancing Generalized Transfer Attack with Initialization Derived Bilevel Optimization
  and Dynamic Sequence Truncation
arxiv_id: '2406.02064'
source_url: https://arxiv.org/abs/2406.02064
tags:
- attack
- betak
- victim
- adversarial
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BETAK, a bilevel optimization framework for
  transfer attacks that improves adversarial example transferability by explicitly
  optimizing the nested relationship between pseudo-victim attackers and surrogate
  attackers. The method introduces Hyper-Gradient Response estimation and Dynamic
  Sequence Truncation to optimize perturbation initialization, achieving substantial
  improvements in attack success rates across multiple victim models.
---

# Advancing Generalized Transfer Attack with Initialization Derived Bilevel Optimization and Dynamic Sequence Truncation

## Quick Facts
- arXiv ID: 2406.02064
- Source URL: https://arxiv.org/abs/2406.02064
- Authors: Yaohua Liu; Jiaxin Gao; Xuan Liu; Xianghao Jiao; Xin Fan; Risheng Liu
- Reference count: 17
- One-line primary result: BETAK achieves 53.41% increase in attack success rates against IncRes-v2ens compared to baseline methods

## Executive Summary
This paper introduces BETAK, a novel bilevel optimization framework for transfer attacks that explicitly models the nested relationship between pseudo-victim and surrogate attackers to improve adversarial example transferability. By optimizing perturbation initialization through a bilevel optimization paradigm with Hyper-Gradient Response estimation and Dynamic Sequence Truncation, BETAK achieves substantial improvements in attack success rates across multiple victim models and defense strategies. The method demonstrates superior performance against both standardly trained and robustly trained victim models, with particular effectiveness in targeted attack scenarios.

## Method Summary
BETAK implements a bilevel optimization framework where the Upper-Level (UL) pseudo-victim attacker optimizes perturbation initialization while the Lower-Level (LL) surrogate attacker optimizes the perturbation itself. The framework incorporates Hyper-Gradient Response (HGR) estimation to incorporate second-order gradient information as feedback for transferability, and Dynamic Sequence Truncation (DST) to reduce computational overhead while maintaining performance. The method is evaluated on ImageNet using ResNet-50 as the surrogate model, Inception-v3 and IncRes-v2 as pseudo-victim models, and 9 victim models including both standardly trained and robustly trained variants.

## Key Results
- Achieves 53.41% increase in attack success rates against IncRes-v2ens compared to baseline methods
- Demonstrates superior performance across 9 victim models and 5 defense strategies
- Shows significant improvements in both targeted and untargeted attack scenarios
- Maintains effectiveness against various defense mechanisms including SGM, LinBP, Ghost, and BPA

## Why This Works (Mechanism)

### Mechanism 1
BETAK improves transferability by optimizing perturbation initialization through a bilevel optimization framework that explicitly models the nested relationship between pseudo-victim and surrogate attackers. The framework reformulates the nested constraint relationship, allowing for optimization of the perturbation initialization to achieve better generalization performance over unknown victim models.

### Mechanism 2
Hyper-Gradient Response estimation provides effective feedback for transferability by incorporating second-order gradient information. The hyper-gradient is explicitly calculated with respect to the perturbation initialization and used to update the initialization, providing more effective feedback than first-order methods.

### Mechanism 3
Dynamic Sequence Truncation improves theoretical properties and reduces computational burden while maintaining performance. DST dynamically adjusts the back-propagation path for HGR computation by truncating the historical sequence based on the change of the UL objective, reducing computational overhead while ensuring convergence.

## Foundational Learning

- Concept: Bilevel Optimization
  - Why needed here: BETAK explicitly models the nested relationship between pseudo-victim and surrogate attackers through bilevel optimization, allowing for optimization of perturbation initialization to improve transferability.
  - Quick check question: What is the key difference between single-level and bilevel optimization in the context of transfer attacks?

- Concept: Hyper-Gradient Estimation
  - Why needed here: HGR estimation incorporates second-order gradient information to provide effective feedback for transferability, which is crucial for optimizing the perturbation initialization.
  - Quick check question: How does hyper-gradient estimation differ from regular gradient estimation in terms of the information it captures?

- Concept: Dynamic Programming and Sequence Truncation
  - Why needed here: DST dynamically adjusts the back-propagation path for HGR computation by truncating the historical sequence, reducing computational overhead while maintaining performance.
  - Quick check question: What is the main advantage of dynamically truncating the historical sequence in HGR estimation?

## Architecture Onboarding

- Component map: UL pseudo-victim attacker -> HGR estimation -> DST -> LL surrogate attacker -> perturbation generation
- Critical path: UL optimization → HGR estimation → DST → LL optimization → perturbation generation
- Design tradeoffs:
  - Complexity vs. performance: BETAK is more complex than single-level methods but offers significantly improved transferability
  - Computational cost: HGR estimation and DST add computational overhead, but DST helps mitigate this
  - Model selection: Choice of pseudo-victim models affects performance
- Failure signatures:
  - Poor transferability: May indicate suboptimal pseudo-victim model selection or insufficient UL iterations
  - High computational cost: May indicate need for more aggressive DST or fewer LL iterations
  - Convergence issues: May indicate need for better initialization or hyperparameter tuning
- First 3 experiments:
  1. Implement BETAK with a single pseudo-victim model and compare transferability to baseline methods on a small dataset
  2. Evaluate the impact of different numbers of UL iterations on transferability performance
  3. Test the effectiveness of DST by comparing performance with and without DST at different LL iteration counts

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of pseudo-victim models affect the transferability performance across different victim architectures? The authors note that Inception-v3 and IncRes-v2 are chosen as pseudo-victim models, with IncRes-v2 showing higher performance improvement, but do not explore a wider range of combinations or investigate theoretical reasons behind effectiveness.

### Open Question 2
What is the theoretical relationship between the non-convexity of the lower-level surrogate attacker and the convergence guarantees of BETAK? While the authors state they provide convergence analysis for BETAK with non-convex lower-level attackers, they do not fully explore the implications of this non-convexity on practical convergence.

### Open Question 3
How does BETAK's performance scale with increasing complexity of victim models and defenses? The evaluation is limited to a fixed set of models and defenses without exploring the upper bounds of BETAK's effectiveness against more complex architectures or stronger defenses.

## Limitations

- Computational complexity of HGR estimation with second-order gradient information is not fully characterized
- Lack of ablation studies isolating individual contributions of HGR and DST components
- Limited exploration of sensitivity to pseudo-victim model selection and generalizability across different attack scenarios

## Confidence

- **High confidence**: BETAK improves transferability over baseline methods (53.41% increase against IncRes-v2ens)
- **Medium confidence**: Specific mechanisms (HGR and DST) driving improvements, lacking detailed ablation studies and computational complexity analysis
- **Low confidence**: Framework's robustness to different pseudo-victim model choices and performance on datasets beyond ImageNet

## Next Checks

1. **Ablation Study**: Implement BETAK without HGR and DST components separately to quantify their individual contributions to performance gains.
2. **Computational Profiling**: Measure wall-clock time and memory usage for BETAK versus baseline methods across different iteration counts to characterize computational overhead.
3. **Model Sensitivity Analysis**: Test BETAK with different combinations of pseudo-victim models to assess robustness to model selection.