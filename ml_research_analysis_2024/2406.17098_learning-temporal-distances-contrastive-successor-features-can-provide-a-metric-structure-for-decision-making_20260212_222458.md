---
ver: rpa2
title: 'Learning Temporal Distances: Contrastive Successor Features Can Provide a
  Metric Structure for Decision-Making'
arxiv_id: '2406.17098'
source_url: https://arxiv.org/abs/2406.17098
tags:
- learning
- distance
- contrastive
- temporal
- distances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new way to define distances between states
  in stochastic environments that satisfy the triangle inequality, enabling generalization
  and shortest-path finding. The authors propose successor distances based on contrastive
  learning of discounted state occupancy measures, which capture the difficulty of
  transitioning between states under stochastic dynamics.
---

# Learning Temporal Distances: Contrastive Successor Features Can Provide a Metric Structure for Decision-Making

## Quick Facts
- arXiv ID: 2406.17098
- Source URL: https://arxiv.org/abs/2406.17098
- Reference count: 39
- Primary result: Successor distances learned via contrastive methods satisfy triangle inequality and improve goal-reaching in stochastic environments

## Executive Summary
This paper introduces successor distances, a novel approach to learning temporal distances between states in stochastic environments that satisfy the triangle inequality. The method builds on contrastive learning of discounted state occupancy measures and demonstrates that a simple change of variables yields distances with metric properties. Empirically, the proposed temporal distance enables combinatorial generalization in controlled settings and achieves competitive performance on high-dimensional robotic control tasks when used as a value function for goal-conditioned reinforcement learning.

## Method Summary
The method learns temporal distances between states by first estimating discounted state occupancy measures through contrastive learning, then applying a change of variables to obtain successor distances that satisfy the triangle inequality. The approach can operate in two modes: directly learning the distance via contrastive learning with a specific parameterization, or distilling learned contrastive features into a quasimetric network architecture for improved generalization. The learned distances can then be used as value functions for goal-conditioned reinforcement learning.

## Key Results
- Successor distances satisfy triangle inequality in stochastic settings, enabling shortest-path finding
- Competitive performance on AntMaze benchmark compared to CRL and QRL methods
- Demonstrates combinatorial generalization on synthetic 2D navigation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The successor distance defined in Eq. (4) satisfies the triangle inequality even in stochastic settings.
- Mechanism: The successor distance is constructed as the difference between the log of discounted state occupancy measures for the goal state and the starting state. This construction, combined with the helper Lemma 3.3, ensures that the triangle inequality holds.
- Core assumption: The discounted state occupancy measure captures the difficulty of transitioning between states in a way that allows the triangle inequality to be proven.
- Evidence anchors:
  - [abstract]: "build on prior work in contrastive learning and quasimetrics to show how successor features learned by contrastive learning (after a change of variables) form a temporal distance that does satisfy the triangle inequality"
  - [section]: "Lemma 3.3... This lemma is the key to proving our main result: Theorem 3.4. dSD is a quasimetric over S"
  - [corpus]: Weak evidence - the corpus papers focus on related topics but do not directly address the triangle inequality in stochastic settings.
- Break condition: If the discounted state occupancy measure does not accurately capture the transition difficulty, or if Lemma 3.3 does not hold, the triangle inequality would be violated.

### Mechanism 2
- Claim: Contrastive learning with a specific parameterization (Eq. 12) can learn the successor distance.
- Mechanism: The contrastive learning objective learns an energy function that, when parameterized as the difference between a quasimetric network and another learned function, recovers the successor distance (Eq. 10).
- Core assumption: The quasimetric network architecture (e.g., MRN) is a universal approximator for the successor distance.
- Evidence anchors:
  - [abstract]: "a simple change of variables applied to contrastive features yields distances that obey metric properties"
  - [section]: "Lemma 4.1... For s ≠ g, the unique solution to the loss function in Eq. (8) with the parameterization in Eq. (12) is dϕ∗(s, a, g) = log pπγ(s+=g |s0=s, a0=a) / pπγ(s+=g |s0=g)"
  - [corpus]: Weak evidence - the corpus papers discuss contrastive learning and quasimetric representations but do not specifically address this parameterization.
- Break condition: If the quasimetric network cannot accurately approximate the successor distance, or if the contrastive learning objective does not converge to the optimal critic, the learned distance would be incorrect.

### Mechanism 3
- Claim: The learned successor distance can be used as a value function for goal-conditioned reinforcement learning.
- Mechanism: The successor distance, which satisfies the triangle inequality, can be used to select actions that minimize the distance to the goal. This is equivalent to maximizing the Q-function (Lemma 3.2).
- Core assumption: The goal-conditioned policy can be learned using advantage-weighted maximum likelihood (Eq. 22) based on the learned successor distance.
- Evidence anchors:
  - [abstract]: "when used as a value function for goal-conditioned reinforcement learning"
  - [section]: "Lemma 3.2... Selecting actions to minimize the successor distance is equivalent to selecting actions to maximize the (scaled and shifted) Q-function"
  - [corpus]: Weak evidence - the corpus papers discuss goal-conditioned RL but do not specifically address using a learned distance as a value function.
- Break condition: If the policy learning algorithm does not converge, or if the learned successor distance is not accurate enough, the goal-reaching performance would be suboptimal.

## Foundational Learning

- Concept: Markov decision processes (MDPs) and discounted state occupancy measures.
  - Why needed here: The successor distance is defined in terms of the discounted state occupancy measure, which captures the probability of reaching a goal state from a given state.
  - Quick check question: What is the relationship between the discounted state occupancy measure and the hitting time of a state?

- Concept: Quasimetric spaces and the triangle inequality.
  - Why needed here: The successor distance is a quasimetric, meaning it satisfies the triangle inequality. This property is crucial for enabling combinatorial generalization and finding shortest paths.
  - Quick check question: What is the difference between a metric and a quasimetric?

- Concept: Contrastive learning and infoNCE loss.
  - Why needed here: Contrastive learning is used to learn an energy function that approximates the optimal critic for the successor distance. The infoNCE loss is used to train this energy function.
  - Quick check question: How does the infoNCE loss encourage the learned representations to capture the temporal structure of the data?

## Architecture Onboarding

- Component map:
  Contrastive learning module -> Quasimetric network -> Goal-conditioned policy

- Critical path:
  1. Collect data from the MDP using a goal-reaching policy.
  2. Train the contrastive learning module to learn the energy function.
  3. Distill the learned energy function into a quasimetric network using the CMD-1 or CMD-2 method.
  4. Learn the goal-conditioned policy using advantage-weighted maximum likelihood based on the learned successor distance.

- Design tradeoffs:
  - One-step vs. two-step distillation: CMD-1 directly learns the successor distance using a specific parameterization, while CMD-2 first learns contrastive features and then distills them into a quasimetric network. CMD-1 is simpler but may be less flexible, while CMD-2 may be more robust but requires more computation.
  - Quasimetric network architecture: The choice of quasimetric network architecture (e.g., MRN) affects the expressiveness and generalization of the learned distance.

- Failure signatures:
  - Poor goal-reaching performance: indicates that the learned successor distance is not accurate enough or that the policy learning algorithm did not converge.
  - Violation of the triangle inequality: indicates that the learned distance does not satisfy the quasimetric properties.
  - Overfitting: indicates that the learned distance does not generalize well to unseen state pairs.

- First 3 experiments:
  1. Train the contrastive learning module on a simple MDP and visualize the learned energy function.
  2. Apply the CMD-1 or CMD-2 method to distill the learned energy function into a quasimetric network and verify that the learned distance satisfies the triangle inequality.
  3. Learn a goal-conditioned policy using the learned successor distance and evaluate its performance on a simple goal-reaching task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed temporal distance perform in non-ergodic settings where the state space contains absorbing states?
- Basis in paper: [inferred] The paper mentions the proposed distance may be infinite in non-ergodic settings as a limitation.
- Why unresolved: The paper does not provide empirical results or theoretical analysis of the distance behavior in non-ergodic environments.
- What evidence would resolve it: Experimental results showing performance degradation or failure cases in environments with absorbing states, or theoretical analysis proving boundedness properties under certain ergodicity conditions.

### Open Question 2
- Question: Can the successor distance metric be extended to partially observable Markov decision processes (POMDPs)?
- Basis in paper: [inferred] The paper focuses on Markov processes and controlled Markov processes but does not address partial observability.
- Why unresolved: The theoretical development relies on exact state observability for the discounted state occupancy measures and the triangle inequality proof.
- What evidence would resolve it: A formal extension of the distance definition to POMDPs with proofs of metric properties, or empirical results demonstrating successful application to partially observable environments.

### Open Question 3
- Question: What is the computational complexity of learning the temporal distance in continuous state spaces compared to discrete ones?
- Basis in paper: [explicit] The paper states that the temporal distance is computationally efficient to estimate "even in high-dimensional and stochastic settings" but provides no complexity analysis.
- Why unresolved: The paper demonstrates scalability through experiments but does not provide theoretical analysis of the computational requirements or compare complexity to alternative approaches.
- What evidence would resolve it: A formal analysis showing time and space complexity bounds for the contrastive learning approach, or empirical benchmarks comparing wall-clock time and memory usage against competing methods across varying state space dimensions.

## Limitations
- Limited empirical validation on complex, high-dimensional environments beyond the AntMaze benchmark
- Dependence on accurate estimation of discounted state occupancy measures through contrastive learning
- Sensitivity to hyperparameters like discount factor γ and contrastive learning temperature τ is not thoroughly investigated

## Confidence
- **High Confidence:** The theoretical framework establishing the successor distance as a quasimetric satisfying the triangle inequality (Theorem 3.4). This is well-supported by the proofs in the appendix.
- **Medium Confidence:** The effectiveness of contrastive learning for estimating the successor distance in practice. While the paper demonstrates this on specific benchmarks, more extensive empirical validation is needed.
- **Medium Confidence:** The claim that the learned successor distance enables combinatorial generalization. The paper provides some evidence for this, but a more rigorous analysis would strengthen the claim.

## Next Checks
1. Conduct ablation studies to assess the impact of hyperparameters (γ, τ) on the quality of the learned successor distance and the resulting control performance.
2. Test the method on more complex, high-dimensional environments (e.g., DeepMind Control Suite tasks) to evaluate its scalability and robustness.
3. Investigate the sensitivity of the method to the choice of quasimetric network architecture and the distillation procedure (CMD-1 vs. CMD-2).