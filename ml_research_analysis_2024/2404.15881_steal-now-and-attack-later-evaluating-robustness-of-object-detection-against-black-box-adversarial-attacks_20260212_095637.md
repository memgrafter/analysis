---
ver: rpa2
title: 'Steal Now and Attack Later: Evaluating Robustness of Object Detection against
  Black-box Adversarial Attacks'
arxiv_id: '2404.15881'
source_url: https://arxiv.org/abs/2404.15881
tags:
- objects
- attack
- adversarial
- attacks
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first latency attack against object detection
  under the black-box scenario, termed "steal now, attack later." The attack aims
  to inflate inference time by generating ghost objects in adversarial examples without
  requiring prior knowledge of the target model. The method collects candidate objects
  from public datasets, selects appropriate patches through a position-centric approach,
  and refines perturbations within an epsilon ball.
---

# Steal Now and Attack Later: Evaluating Robustness of Object Detection against Black-box Adversarial Attacks

## Quick Facts
- arXiv ID: 2404.15881
- Source URL: https://arxiv.org/abs/2404.15881
- Reference count: 40
- Presents first black-box latency attack against object detection systems

## Executive Summary
This paper introduces "steal now, attack later," a novel black-box adversarial attack methodology targeting object detection systems by generating ghost objects that inflate inference time. The attack operates without requiring prior knowledge of the target model, collecting candidate objects from public datasets and applying a position-centric approach with perturbation refinement. The method successfully attacks various object detection architectures including Faster-RCNN, RetinaNet, FCOS, DERT, YOLOv8, and Google Vision API with success rates ranging from 1% to 81% depending on the epsilon radius. The attack remains economically feasible at under $1 per image, posing significant security concerns for AI deployment.

## Method Summary
The "steal now, attack later" methodology collects candidate objects from public datasets without requiring knowledge of the target model. It employs a position-centric approach to select appropriate patches for ghost object insertion, then refines perturbations within an epsilon ball constraint. The attack generates adversarial examples that cause object detectors to hallucinate additional objects during inference, thereby increasing processing time and potentially degrading system performance. The method is designed to work across various object detection architectures and can target both local models and cloud-based APIs like Google Vision.

## Key Results
- Success rates range from 1% to 81% across different epsilon radii and target models
- Attacks successfully compromise Faster-RCNN, RetinaNet, FCOS, DERT, YOLOv8, and Google Vision API
- Attack generation cost remains under $1 per image
- Demonstrates first effective black-box latency attack on object detection systems

## Why This Works (Mechanism)
The attack exploits the computational overhead introduced when object detection models process additional hallucinated objects. By carefully crafting adversarial patches that resemble legitimate objects but are positioned to maximize inference burden, the method forces the detection pipeline to allocate resources to non-existent entities. The position-centric approach ensures these ghost objects are placed where they cause maximum disruption to the model's processing pipeline, while the perturbation refinement within epsilon bounds maintains the stealthiness of the attack by keeping modifications imperceptible to human observers.

## Foundational Learning
- **Object Detection Architecture**: Understanding how detectors process and classify objects in images
  - Why needed: Essential to identify where latency can be introduced through ghost objects
  - Quick check: Can explain the difference between anchor-based and anchor-free detectors

- **Adversarial Example Generation**: Techniques for creating inputs that fool machine learning models
  - Why needed: Core methodology for crafting ghost objects that evade detection
  - Quick check: Can describe the concept of epsilon balls in perturbation space

- **Black-box Attack Methodology**: Attacking systems without knowledge of internal model parameters
  - Why needed: Enables the attack to work on commercial APIs and proprietary systems
  - Quick check: Can explain how query-based black-box attacks differ from white-box approaches

- **Computational Cost Analysis**: Measuring the resource overhead introduced by adversarial examples
  - Why needed: To demonstrate the practical impact of latency attacks
  - Quick check: Can calculate cost per attack generation and compare to legitimate usage

## Architecture Onboarding

**Component Map**: Public Datasets -> Candidate Object Collection -> Position-Centric Selection -> Perturbation Refinement -> Adversarial Example Generation

**Critical Path**: The attack follows a sequential pipeline where candidate object collection feeds into position selection, which then undergoes perturbation refinement to generate the final adversarial example that targets the object detection system.

**Design Tradeoffs**: The method balances attack effectiveness (success rate) against stealthiness (imperceptibility) through epsilon ball constraints. Larger epsilon radii increase success rates but risk detection by human observers or defensive systems.

**Failure Signatures**: Attacks may fail when ghost objects are too dissimilar from legitimate objects in the dataset, when positioning doesn't align with the model's attention mechanisms, or when the perturbation exceeds the model's tolerance threshold.

**First Experiments**:
1. Test candidate object collection effectiveness across diverse public datasets
2. Evaluate position-centric selection performance on simple object detection models
3. Measure perturbation refinement success rates across different epsilon radii

## Open Questions the Paper Calls Out
None

## Limitations
- Success rates vary significantly (1% to 81%) across models and epsilon radii, indicating inconsistent effectiveness
- Claims of zero prior knowledge may be overstated given reliance on specific public dataset objects
- Cost analysis methodology for "under $1 per image" claim is not clearly specified
- Generalizability across diverse real-world object detection scenarios remains unproven

## Confidence

**High Confidence**: The paper successfully demonstrates that latency attacks are possible in black-box object detection scenarios

**Medium Confidence**: The effectiveness of the position-centric approach and perturbation refinement within epsilon bounds

**Low Confidence**: The claim of zero prior knowledge requirement and the generalizability across diverse object detection scenarios

## Next Checks

1. Test the attack's effectiveness across a broader range of public datasets and object categories to verify the "no prior knowledge" claim

2. Conduct cost analysis validation by independently measuring actual computational resources required for attack generation

3. Evaluate the attack's performance on real-world deployed systems (not just API endpoints) to assess practical security implications