---
ver: rpa2
title: Dataset Condensation Driven Machine Unlearning
arxiv_id: '2402.00195'
source_url: https://arxiv.org/abs/2402.00195
tags:
- unlearning
- dataset
- training
- machine
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a machine unlearning framework that combines
  dataset condensation and modular training to reduce computational cost while preserving
  privacy and utility. The approach condenses training data into smaller clusters,
  trains the model in three parts (beginning, intermediate, final) separately, and
  uses a reduced dataset for unlearning.
---

# Dataset Condensation Driven Machine Unlearning

## Quick Facts
- arXiv ID: 2402.00195
- Source URL: https://arxiv.org/abs/2402.00195
- Authors: Junaid Iqbal Khan
- Reference count: 40
- One-line primary result: Proposes a framework combining dataset condensation and modular training to achieve efficient machine unlearning with strong privacy-utility tradeoff

## Executive Summary
This paper introduces a novel machine unlearning framework that leverages dataset condensation techniques and modular training to reduce computational costs while preserving privacy and utility. The approach condenses training data into smaller representative clusters and trains the model in three separate parts, achieving effective unlearning with minimal epochs. Experiments on CIFAR-10 and SVHN demonstrate a good balance between unlearning privacy, utility, and efficiency compared to state-of-the-art methods, with the proposed unlearning metric showing strong correlation with membership inference attack accuracy.

## Method Summary
The proposed framework consists of an offline phase that condenses the entire training dataset into a smaller representative set using fast distribution matching and model inversion techniques, and an online phase that performs modular unlearning. The model is split into three parts (beginning, intermediate, final) which are trained separately on the reduced retain dataset and remembrance samples. This approach accelerates the unlearning process by focusing on catastrophic forgetting of the intermediate part while preserving utility through the beginning and final parts. The method also includes two new metrics for measuring unlearning effectiveness based on cosine similarity between gradients over retain and forget datasets.

## Key Results
- Achieves strong correlation (0.95) between proposed unlearning metric and membership inference attack accuracy
- Reduces unlearning time by condensing datasets while maintaining utility on CIFAR-10 and SVHN
- Demonstrates effective privacy protection through reduced membership inference attack success rates

## Why This Works (Mechanism)

### Mechanism 1
Combining dataset condensation with modular training reduces the size of the retain dataset while preserving the model's ability to unlearn effectively. Dataset condensation techniques reduce training data needed for unlearning by clustering images and condensing them into a smaller set of representative images. Modular training then accelerates the unlearning process by focusing on training specific parts of the model separately. The core assumption is that the condensed dataset can effectively represent the original data distribution, and modular training can achieve catastrophic forgetting with minimal epochs.

### Mechanism 2
Modular training reduces computational cost of unlearning by focusing on training specific parts of the model. The model is split into three parts (beginning, intermediate, final), which are trained separately. This allows for faster catastrophic forgetting by minimizing the number of epochs needed for the intermediate part. The core assumption is that deeper layers are more vulnerable to membership inference attacks while shallower layers contain more information about input data.

### Mechanism 3
The proposed unlearning metric correlates with membership inference attack accuracy, providing a measure of unlearning effectiveness. The metric is computed as cosine similarity between gradients of the loss function over retain and forget datasets. A lower value indicates better unlearning. The core assumption is that distance in parameter space between original and unlearned models is small, and gradients of loss function are orthogonal.

## Foundational Learning

- Concept: Dataset condensation
  - Why needed here: To reduce the size of the retain dataset, making the unlearning process more efficient
  - Quick check question: How does dataset condensation preserve the original data distribution while reducing the number of samples?

- Concept: Modular training
  - Why needed here: To accelerate the unlearning process by focusing on training specific parts of the model separately
  - Quick check question: Why is it beneficial to split the model into three parts for unlearning?

- Concept: Membership inference attacks
  - Why needed here: To measure the effectiveness of the unlearning process in protecting privacy
  - Quick check question: How does overfitting on the training data increase vulnerability to membership inference attacks?

## Architecture Onboarding

- Component map: Dataset condensation module -> Modular training module -> Unlearning metric module
- Critical path:
  1. Condense the training dataset into a smaller set of representative images
  2. Split the model into three parts (beginning, intermediate, final)
  3. Train the beginning part on the reduced retain dataset
  4. Train the final part on remembrance samples
  5. Compute the unlearning metric to measure effectiveness

- Design tradeoffs:
  - Condensing the dataset reduces unlearning time but may decrease accuracy
  - Modular training accelerates unlearning but requires careful tuning of the three parts
  - The unlearning metric provides a measure of effectiveness but may not correlate with all privacy metrics

- Failure signatures:
  - If the unlearning metric is high, the unlearning process is ineffective
  - If the model's accuracy on the retain dataset is low, the unlearning process has degraded utility
  - If the unlearning time is high, the process is not efficient

- First 3 experiments:
  1. Test the dataset condensation module with different values of K to find the optimal balance between dataset size and accuracy
  2. Test the modular training module with different numbers of iterations to find the optimal balance between unlearning time and effectiveness
  3. Test the unlearning metric module with different values of the forget dataset size to find the optimal balance between privacy and utility

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed unlearning metric (1 - cosine similarity between gradients over retain and forget datasets) correlate with other unlearning metrics like loss difference or parameter distance in practice? The authors propose this new unlearning metric and show it correlates strongly with MIA accuracy, but don't compare it to other potential metrics. What evidence would resolve it: Empirical comparison showing correlation coefficients between the proposed metric and metrics like L2 parameter distance or loss difference across various unlearning scenarios.

### Open Question 2
What is the theoretical bound on unlearning effectiveness when using dataset condensation techniques like fast distribution matching or model inversion? The authors acknowledge dataset condensation is heuristic and don't provide theoretical guarantees for unlearning effectiveness after condensation. What evidence would resolve it: Mathematical analysis proving conditions under which condensed representations preserve or enable unlearning of original samples, possibly using influence functions or other theoretical tools.

### Open Question 3
How does the performance of modular unlearning scale with dataset size and model architecture complexity beyond the tested CIFAR-10 and SVHN datasets? Experiments are limited to CIFAR-10 and SVHN with specific architectures (MLP, CNN, VGG16, ResNet18). What evidence would resolve it: Empirical results showing unlearning performance (RA, FA, MIA, UT) across various dataset sizes (e.g., ImageNet) and architectures (e.g., transformers, larger ResNets).

## Limitations

- Lack of detailed implementation specifics for dataset condensation techniques and modular training procedures
- Limited evaluation to only two datasets (CIFAR-10 and SVHN) without demonstrating robustness across different model architectures
- No theoretical guarantees provided for unlearning effectiveness after dataset condensation

## Confidence

- High confidence in the core concept of combining dataset condensation with modular training for unlearning
- Medium confidence in the effectiveness of the proposed unlearning metric due to limited experimental validation
- Low confidence in the generalizability of results across different datasets and model architectures

## Next Checks

1. Implement and validate the dataset condensation module using different values of K to quantify the tradeoff between dataset size reduction and utility preservation
2. Test the modular training approach across multiple model architectures (beyond ResNet) to verify its effectiveness and generalizability
3. Conduct extensive experiments comparing the proposed unlearning metric against established privacy metrics across various unlearning scenarios and attack types