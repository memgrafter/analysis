---
ver: rpa2
title: Introducing v0.5 of the AI Safety Benchmark from MLCommons
arxiv_id: '2404.12241'
source_url: https://arxiv.org/abs/2404.12241
tags:
- safety
- pages
- cited
- benchmark
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The MLCommons AI Safety Working Group released v0.5 of its AI Safety
  Benchmark, a tool to assess safety risks of chat-tuned language models. It includes
  43,090 test prompts across seven hazard categories like violence, hate, and child
  exploitation.
---

# Introducing v0.5 of the AI Safety Benchmark from MLCommons

## Quick Facts
- arXiv ID: 2404.12241
- Source URL: https://arxiv.org/abs/2404.12241
- Reference count: 40
- Primary result: MLCommons AI Safety Working Group released v0.5 of its AI Safety Benchmark, a tool to assess safety risks of chat-tuned language models.

## Executive Summary
The MLCommons AI Safety Working Group has released v0.5 of the AI Safety Benchmark, a comprehensive tool designed to evaluate safety risks in chat-tuned language models. The benchmark includes 43,090 test prompts across seven hazard categories including violence, hate, and child exploitation. Prompts were created using sentence fragments and templates to simulate different user personas and interactions. Responses were evaluated using LlamaGuard, achieving 70.4% accuracy on a human-verified subset. The benchmark demonstrates significant variation in model performance across personas and hazard types, and is intended as a proof-of-concept that should not be used to assess actual model safety. The team is seeking feedback for v1.0, planned for late 2024.

## Method Summary
The benchmark uses manually created prompts generated by combining 725 sentence fragments with 32 templates, creating 43,090 test items across seven hazard categories. These prompts are designed to simulate three user personas: typical user, malicious user, and vulnerable user. Model responses are collected at temperature 0.01 using the ModelBench platform and evaluated using LlamaGuard's safety filter. Human verification is performed on a stratified sample of 1,320 responses to validate LlamaGuard's accuracy, which achieves 70.4% on this subset. Results are aggregated into percentage scores and converted to a five-point risk grade (Low to High) based on comparison to reference models.

## Key Results
- LlamaGuard flagged 27,045 responses (4.8%) as unsafe across 43,090 prompts
- Accuracy of LlamaGuard was 70.4% on the human-verified subset of 1,320 responses
- Results showed high variation in model performance across different personas and hazard categories
- The benchmark is explicitly designed as a proof-of-concept and should not be used for actual safety assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AI Safety Benchmark leverages human-created prompts to simulate realistic user interactions and reduce model overfit to specific prompt patterns.
- Mechanism: By using templates and sentence fragments, the benchmark creates diverse, interpretable prompts across 13 interaction types, covering both activities and viewpoints. This allows evaluation of model responses under different user motivations.
- Core assumption: Diverse prompt generation based on linguistic interaction types can expose varied model safety weaknesses.
- Evidence anchors:
  - [section] "We create the test items by combining (1) sentence fragments with (2) templates. Each template is associated with one of 13 types of interaction that a user can have with a model"
  - [section] "The test items can be designed in several ways... We opted to use chat response testing as this is best suited to our Scope and SUTs"
- Break condition: If models learn to recognize prompt templates rather than underlying safety concepts, or if real-world interactions deviate significantly from these 13 types.

### Mechanism 2
- Claim: Automated evaluation using LlamaGuard provides a scalable and sufficiently accurate method for assessing model safety.
- Mechanism: LlamaGuard flags unsafe responses based on its training, and human verification confirms its accuracy (~70.4%) on a stratified sample. This allows grading of models across all 43,090 prompts.
- Core assumption: LlamaGuard's safety classification is accurate enough to serve as a reliable automated evaluator for benchmarking purposes.
- Evidence anchors:
  - [section] "LlamaGuard flagged 27,045 responses (4.8%) as unsafe. To verify LlamaGuard's accuracy, we selected a subset of the responses for human labelling... Accuracy is 70.4% on the human eval set"
  - [section] "Due to the relatively small size of the human eval set, we do not analyse more finegrained categories"
- Break condition: If LlamaGuard's accuracy degrades significantly for certain hazard categories or interaction types, or if adversarial prompts bypass its detection.

### Mechanism 3
- Claim: The five-point grading system with reference models provides interpretable and actionable safety assessments.
- Mechanism: Each test item is scored as unsafe/not unsafe, then aggregated into a percentage score. This is converted to a five-point risk grade (Low to High) based on comparison to reference models, allowing relative performance evaluation.
- Core assumption: Comparing models against reference models is more meaningful than absolute thresholds for safety assessment.
- Evidence anchors:
  - [section] "We take the lowest grade to minimize the risk that we overstate the safety of SUTs"
  - [section] "Choice of reference models... We are using a five-point grading scale, from 'Low' to 'High' risk, as described in Table 5"
- Break condition: If reference models themselves have safety weaknesses, or if the five-point scale oversimplifies complex safety risks.

## Foundational Learning

- Concept: Hazard categories and taxonomy
  - Why needed here: The benchmark requires a systematic way to classify and test for different types of safety risks. The taxonomy provides a hierarchical structure for organizing 13 hazard categories.
  - Quick check question: What are the seven hazard categories included in v0.5 of the benchmark?

- Concept: Interaction types and speech acts
  - Why needed here: Understanding how users can interact with models (requests, statements, viewpoints) is crucial for creating realistic test prompts that capture different safety scenarios.
  - Quick check question: How many interaction types are used to construct the test items, and what are the two main classes?

- Concept: Automated evaluation and human verification
  - Why needed here: The benchmark needs a scalable method to assess thousands of model responses, but also requires accuracy validation through human annotation.
  - Quick check question: What is the accuracy of LlamaGuard on the human-verified subset, and how many responses were in this evaluation set?

## Architecture Onboarding

- Component map: Prompt generation pipeline (sentence fragments + templates) → 43,090 test items → Model response collection → Automated evaluation (LlamaGuard) → Human verification (stratified sampling) → Grading system (percentage scores → five-point risk grades) → ModelBench platform (test execution, result aggregation, report generation)

- Critical path: Prompt generation → Model response collection → Automated evaluation → Human verification → Grading → Benchmark score calculation

- Design tradeoffs:
  - Simple vs. complex prompts: Simple prompts ensure interpretability but may miss nuanced safety scenarios
  - Automated vs. manual evaluation: Automation enables scalability but requires human verification for accuracy
  - Absolute vs. relative grading: Relative grading against reference models provides context but depends on reference model quality

- Failure signatures:
  - Low human verification accuracy indicates evaluator model issues
  - Inconsistent grading across similar test items suggests template problems
  - High false refusal rates indicate overly conservative safety filtering

- First 3 experiments:
  1. Test prompt generation with a small sample of sentence fragments and templates to validate interaction type coverage
  2. Run automated evaluation on a subset of model responses to assess LlamaGuard accuracy and identify categories needing improvement
  3. Compare grading results using different reference models to validate relative performance assessment methodology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different user personas (typical, malicious, vulnerable) impact the safety risks associated with AI models, and how can these risks be mitigated?
- Basis in paper: [explicit] The paper explicitly states that different personas pose different safety risks and that the v0.5 benchmark focuses on three personas: typical adult user, adult user intent on malicious activities, and adult user at risk of harm.
- Why unresolved: The paper acknowledges that the ecological validity of the prompts could be low and that true user intent cannot always be inferred from prompts alone. Additionally, the paper does not test for whether models falsely refuse benign prompts, which could be a significant safety concern for vulnerable users.
- What evidence would resolve it: A longitudinal study tracking real-world interactions with AI models across different user personas and analyzing the safety risks and effectiveness of mitigation strategies.

### Open Question 2
- Question: How can the AI Safety Benchmark be expanded to include a wider range of languages and cultural contexts beyond Western Europe and North America?
- Basis in paper: [explicit] The paper explicitly states that the v0.5 benchmark is limited to the English language and the cultural context of Western Europe and North America.
- Why unresolved: The paper acknowledges that the hazard categories may not apply equally well across other languages and cultural contexts, and that future versions of the benchmark will need to address this limitation.
- What evidence would resolve it: Testing the benchmark across a diverse range of languages and cultural contexts and analyzing the performance and relevance of the hazard categories in different settings.

### Open Question 3
- Question: How can the AI Safety Benchmark be improved to better assess the safety risks of AI models in real-world scenarios, beyond simple single-turn interactions?
- Basis in paper: [inferred] The paper acknowledges that the test items are designed to be simple and clear-cut, but this limits their relevance for testing more sophisticated users and real-world interactions.
- Why unresolved: The paper does not provide a clear solution for addressing the limitations of simple test items and improving the ecological validity of the benchmark.
- What evidence would resolve it: Developing more complex and realistic test scenarios that simulate real-world interactions with AI models, including multi-turn conversations and use of agents, and analyzing the performance of models in these scenarios.

## Limitations
- The benchmark relies on a single automated evaluator (LlamaGuard) with only 70.4% accuracy on human-verified subsets, raising concerns about misclassification rates
- The methodology uses templates and sentence fragments that may be vulnerable to model overfitting, potentially limiting real-world applicability
- The benchmark explicitly excludes important safety domains like privacy violations, model capabilities that could be misused, and truthfulness concerns

## Confidence
- **High confidence**: The benchmark's methodology is clearly described and the human verification process is transparent. The five-point grading system and reference model approach are well-specified.
- **Medium confidence**: The overall framework appears sound, but the low accuracy of the automated evaluator and limited human verification sample size reduce confidence in specific results.
- **Low confidence**: The benchmark's ability to generalize to real-world safety scenarios is uncertain given its limited scope and potential vulnerability to template recognition.

## Next Checks
1. **Evaluator accuracy validation**: Test LlamaGuard's performance across all seven hazard categories using a larger human-verified sample to identify categories where accuracy falls below acceptable thresholds.
2. **Template robustness test**: Generate alternative prompts using different linguistic structures to determine if models' safety performance varies significantly based on prompt format, indicating potential template overfitting.
3. **Reference model sensitivity analysis**: Evaluate how benchmark grades change when using different reference models to assess whether the relative grading approach is robust to reference model selection.