---
ver: rpa2
title: On the Reliability of Large Language Models for Causal Discovery
arxiv_id: '2407.19638'
source_url: https://arxiv.org/abs/2407.19638
tags:
- causal
- relations
- cause
- effect
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) are increasingly used for causal discovery,
  but their reliability is unclear. This study investigates the trustworthiness of
  LLMs in identifying causal relations using newly available open-source LLMs, OLMo
  and BLOOM, whose pre-training corpora are accessible.
---

# On the Reliability of Large Language Models for Causal Discovery

## Quick Facts
- **arXiv ID**: 2407.19638
- **Source URL**: https://arxiv.org/abs/2407.19638
- **Reference count**: 28
- **Primary result**: LLMs excel at recognizing frequently occurring causal relations but struggle with novel or rare causal relations due to memorization limitations

## Executive Summary
This study investigates the reliability of Large Language Models (LLMs) for causal discovery tasks by examining three key factors: the impact of memorization, the influence of incorrect causal relations in pre-training data, and the effect of contextual nuances. Using open-source LLMs (OLMo and BLOOM) with accessible pre-training corpora, the researchers conducted experiments on synthetic and real-world datasets to evaluate how well these models can identify causal relationships. The findings reveal that while LLMs perform well on causal relations that frequently appear in their training data, their ability to generalize to new or rare causal relations is limited. Additionally, the presence of incorrect causal relations significantly undermines LLMs' confidence in correct causal relations, and contextual information critically affects their performance in discerning causal connections.

## Method Summary
The study employed in-context learning with open-source LLMs (OLMo-7b-Instruct, BLOOM-7b1) using their accessible pre-training corpora (Dolma, ROOTS). Researchers created synthetic and real-world datasets for causal direction identification and full causal discovery tasks, using 0-4 demonstrations in prompts. They evaluated model performance using accuracy, F1 score, and Normalized Hamming Distance ratio metrics. The experimental design included querying pre-training corpora to analyze frequency distributions of causal relations, testing model responses across different contexts (positive, negative, neutral), and measuring confidence levels when varying frequencies of correct versus incorrect causal relations were present.

## Key Results
- LLMs achieve high F1 scores (0.88) on frequently occurring causal relations but drop to 0.2 on rare relations
- The occurrence of correct causal relations in pre-training corpora is approximately 12 times higher than incorrect causal relations
- Model confidence in correct causal relations decreases significantly when incorrect causal relations are present in pre-training data
- LLMs identify causal relations more accurately in positive contexts compared to negative or neutral contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform well on causal discovery tasks because pre-training corpora contain significantly more correct causal relations than incorrect ones.
- Mechanism: During pre-training, LLMs are exposed to correct causal relations more frequently than incorrect ones, leading to stronger memorization and pattern recognition for correct relations.
- Core assumption: The frequency distribution of correct versus incorrect causal relations in pre-training data directly influences LLM performance on causal discovery tasks.
- Evidence anchors:
  - [abstract] "Our findings indicate that while LLMs are effective in recognizing causal relations that occur frequently in pre-training data"
  - [section] "we observe that the occurrence of causal relations is, on average, 12 times higher than that of incorrect causal relations in Dolma and ROOTS corpora"
  - [corpus] "most incorrect causal relations do not exist in an affirmation context. They are usually in a question or negation context"
- Break condition: If pre-training corpora contain balanced or higher frequencies of incorrect causal relations, LLM performance on causal discovery would deteriorate significantly.

### Mechanism 2
- Claim: LLMs rely heavily on memorization rather than generalization for causal discovery, especially for frequently occurring relations.
- Mechanism: LLMs recognize causal relations through pattern matching and memorization of frequently occurring examples from pre-training data, rather than through abstract causal reasoning.
- Core assumption: The frequency of a causal relation in pre-training data correlates with the LLM's ability to correctly identify that relation.
- Evidence anchors:
  - [abstract] "while LLMs are effective in recognizing causal relations that occur frequently in pre-training data, their ability to generalize to new or rare causal relations is limited"
  - [section] "Fig. 1-5 show that both F1 and accuracy exhibit a strong positive correlation with occurrence in the pre-training corpora"
  - [corpus] "OLMo-7b-Instruct achieves an F1 score of 0.88 in the highest occurrence interval, but only 0.2 in the lowest occurrence interval"
- Break condition: When presented with causal relations that never appear in pre-training data, LLM performance drops to near-random levels.

### Mechanism 3
- Claim: Contextual information significantly affects LLM performance in causal discovery tasks.
- Mechanism: LLMs interpret causal relations differently based on surrounding context, with positive contexts enhancing recognition and negative contexts reducing it.
- Core assumption: The validity and strength of causal relations vary across different contexts, and LLMs are sensitive to these contextual variations.
- Evidence anchors:
  - [abstract] "the contextual information critically affects the outcomes of LLMs to discern causal connections between random variables"
  - [section] "we observe that all LLMs are more likely to identify causal relations in positive contexts compared to no context"
  - [corpus] "LLMs' ability to identify causal relations compared to no context. These results indicate that the validity and strength of causal relations can vary in different contexts"
- Break condition: If contextual information is removed or standardized, LLM performance variability across different causal relations would decrease.

## Foundational Learning

- Concept: Frequency-based memorization
  - Why needed here: Understanding how LLMs leverage frequency patterns in pre-training data to perform causal discovery tasks
  - Quick check question: If a causal relation appears 1000 times in pre-training data and another appears only once, which is the LLM more likely to correctly identify?

- Concept: Contextual interpretation in language models
  - Why needed here: Recognizing how surrounding text influences LLM interpretation of causal relationships
  - Quick check question: How might the LLM interpret "smoking causes lung cancer" differently in a medical journal versus a conspiracy theory forum?

- Concept: Confidence calibration in LLM outputs
  - Why needed here: Understanding how conflicting information in pre-training data affects LLM confidence in causal relation predictions
  - Quick check question: What happens to LLM confidence when both "smoking causes lung cancer" and "lung cancer causes smoking" appear frequently in pre-training data?

## Architecture Onboarding

- Component map: Pre-training corpus analysis -> In-context learning prompt engineering -> Causal relation evaluation pipeline
- Critical path: Causal relation query -> In-context learning generation -> Response aggregation -> Confidence scoring -> Final causal determination
- Design tradeoffs: Larger models provide better performance but at increased computational cost; synthetic data generation allows controlled experiments but may not capture real-world complexity
- Failure signatures: Low F1 scores on low-frequency relations, inconsistent responses across different contexts, reduced confidence when conflicting information is present
- First 3 experiments:
  1. Measure F1 scores across different frequency intervals of causal relations in pre-training data
  2. Test LLM confidence levels when varying frequencies of correct vs. incorrect causal relations are present
  3. Evaluate causal relation identification performance across positive, negative, and neutral contexts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be designed to better generalize to novel or rare causal relations that are not present in their pre-training data?
- Basis in paper: [explicit] The paper states that LLMs excel at recognizing causal relations that occur frequently in pre-training data but struggle with novel or rare causal relations.
- Why unresolved: The study focuses on evaluating current LLM performance but does not propose specific architectural or training modifications to improve generalization to unseen causal relations.
- What evidence would resolve it: Experiments comparing baseline LLMs with models incorporating techniques like few-shot learning, meta-learning, or hybrid approaches combining LLMs with traditional statistical causal discovery methods on datasets with controlled proportions of novel causal relations.

### Open Question 2
- Question: What specific mechanisms cause LLMs to lose confidence in correct causal relations when exposed to conflicting (incorrect) causal information during pre-training?
- Basis in paper: [explicit] The paper demonstrates that the presence of incorrect causal relations significantly undermines LLMs' confidence in correct causal relations.
- Why unresolved: While the paper identifies the negative impact of conflicting information, it does not investigate the underlying mechanisms (e.g., attention patterns, representation drift) that lead to this confidence reduction.
- What evidence would resolve it: Analysis of attention weights and internal representations when LLMs process correct vs. incorrect causal relations, combined with controlled experiments varying the ratio and type of conflicting information.

### Open Question 3
- Question: How does the strength and validity of causal relations vary across different contexts, and can this contextual variation be systematically modeled?
- Basis in paper: [explicit] The paper shows that LLMs' performance in causal discovery varies significantly across different contexts, with positive contexts enhancing and negative contexts diminishing their ability to identify causal relations.
- Why unresolved: The study demonstrates context sensitivity but does not explore the systematic relationship between contextual features and causal relation validity, nor does it propose methods to model this variation.
- What evidence would resolve it: A comprehensive analysis mapping specific contextual features (e.g., time, location, participant roles) to causal relation strength, combined with experiments testing LLM performance with systematically varied contextual information.

## Limitations

- Performance heavily dependent on frequency of causal relations in pre-training data, limiting generalization to novel relationships
- Study focuses primarily on binary positive/negative contexts, leaving nuanced contextual variations unexplored
- Exact magnitude of confidence reduction from conflicting information across different model sizes remains unclear

## Confidence

**High Confidence Claims:**
- LLMs demonstrate superior performance on frequently occurring causal relations compared to rare ones
- Contextual information significantly affects LLM interpretation of causal relations

**Medium Confidence Claims:**
- Memorization is the primary mechanism for LLM success in causal discovery
- Incorrect causal relations in pre-training data reduce confidence in correct causal relations

**Low Confidence Claims:**
- The 12:1 ratio of correct to incorrect causal relations is universally optimal
- In-context learning with 3 demonstrations is optimal

## Next Checks

1. **Cross-corpus validation**: Test the same LLMs on additional open-source pre-training corpora to verify whether the 12:1 ratio of correct to incorrect causal relations holds across different domains and sources.

2. **Zero-shot generalization test**: Evaluate model performance on entirely novel causal relations that have zero occurrences in any pre-training corpus to distinguish between memorization and true causal reasoning capabilities.

3. **Context granularity analysis**: Systematically vary contextual information from single-word cues to full paragraph contexts to identify the minimum context required for optimal causal relation identification.