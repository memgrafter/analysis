---
ver: rpa2
title: 'Learning to Beat ByteRL: Exploitability of Collectible Card Game Agents'
arxiv_id: '2404.16689'
source_url: https://arxiv.org/abs/2404.16689
tags:
- game
- learning
- byterl
- stage
- card
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the exploitability of ByteRL, the state-of-the-art
  agent in the collectible card game Legends of Code and Magic (LOCM) and Hearthstone.
  The authors use behavior cloning to train an agent that mimics ByteRL's gameplay
  and then fine-tune it using reinforcement learning.
---

# Learning to Beat ByteRL: Exploitability of Collectible Card Game Agents

## Quick Facts
- arXiv ID: 2404.16689
- Source URL: https://arxiv.org/abs/2404.16689
- Reference count: 40
- Primary result: Behavior cloning achieves 42.4% win rate against ByteRL in LOCM 1.5 battle stage

## Executive Summary
This paper investigates the exploitability of ByteRL, the state-of-the-art agent for the collectible card game Legends of Code and Magic (LOCM). The authors train an agent using behavior cloning to mimic ByteRL's gameplay, followed by reinforcement learning fine-tuning. Their results demonstrate that ByteRL's gameplay is highly exploitable, with the behavior-cloned agent achieving a 42.4% win rate against ByteRL in the battle stage of LOCM 1.5. Further RL fine-tuning allows the agent to surpass ByteRL's performance on hundreds of decks. An ablation study confirms that behavior cloning prior to RL fine-tuning is beneficial. The authors conclude that while ByteRL is strong, there is significant room for improvement, and their work represents an important step towards developing stronger agents for collectible card games.

## Method Summary
The authors use behavior cloning to train an agent that mimics ByteRL's gameplay in the collectible card game LOCM. They then fine-tune the behavior-cloned agent using reinforcement learning. The behavior cloning is performed on fixed deck pools, while the RL fine-tuning is done on procedurally generated deck pools. The authors conduct an ablation study to assess the benefits of behavior cloning prior to RL fine-tuning. They also experiment with different neural network architectures and training strategies to optimize the agent's performance against ByteRL.

## Key Results
- Behavior cloning achieves a 42.4% win rate against ByteRL in the battle stage of LOCM 1.5
- Reinforcement learning fine-tuning allows the agent to surpass ByteRL's performance on hundreds of decks
- Ablation study confirms that behavior cloning prior to RL fine-tuning is beneficial

## Why This Works (Mechanism)
The paper does not provide a detailed mechanism explaining why the proposed approach works. However, the authors hypothesize that behavior cloning allows the agent to learn a good initial policy that can be further improved through RL fine-tuning, leading to better performance against ByteRL.

## Foundational Learning
- Collectible card games: Understanding the rules and mechanics of LOCM is crucial for developing agents that can play the game effectively.
- Behavior cloning: This technique involves training an agent to mimic the actions of an expert, which is useful for learning from existing strong agents like ByteRL.
- Reinforcement learning: RL is used to fine-tune the behavior-cloned agent, allowing it to improve its policy and surpass ByteRL's performance.
- Ablation studies: These experiments help identify the importance of different components in the training pipeline, such as behavior cloning prior to RL fine-tuning.

## Architecture Onboarding

Component map:
ByteRL gameplay data -> Behavior cloning -> RL fine-tuning -> Final agent

Critical path:
Behavior cloning on fixed deck pools -> RL fine-tuning on procedurally generated deck pools -> Evaluation against ByteRL

Design tradeoffs:
- Using behavior cloning as an initialization vs. training from scratch
- Fixed deck pools vs. procedurally generated deck pools for training
- Network architecture choices (size, depth, etc.)

Failure signatures:
- Overfitting to the behavior cloning data, leading to poor generalization
- Insufficient exploration during RL fine-tuning, resulting in suboptimal policies
- Difficulty in scaling up to the full game with procedurally generated deck pools

First experiments:
1. Evaluate the behavior-cloned agent against ByteRL on a diverse set of decks
2. Conduct an ablation study comparing behavior cloning + RL fine-tuning vs. training from scratch
3. Test the final agent against other strong agents in LOCM to assess generalizability

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does increasing the training data size prevent overfitting and allow training larger networks, potentially increasing the win rate against ByteRL?
- Basis in paper: The authors mention this as a future plan, noting that they have yet to see whether increasing the training data size would prevent overfitting and allow training larger networks and whether it would increase the win rate.
- Why unresolved: The current experiments show overfitting in larger networks, and the authors have not yet conducted experiments with increased training data to test this hypothesis.
- What evidence would resolve it: Experiments demonstrating that larger networks trained on increased data do not overfit and achieve higher win rates against ByteRL compared to current models.

### Open Question 2
- Question: Can the authors develop a separate network for the draft stage that produces decks as strong as those generated by ByteRL?
- Basis in paper: The authors state that one of the most critical next steps is losing any dependence on ByteRL during the deck-building stage, which requires training a separate network for the draft stage that can produce similarly strong decks.
- Why unresolved: The current work focuses only on the battle stage, using ByteRL for the draft stage. Developing a competitive draft-stage network is a significant challenge that has not yet been addressed.
- What evidence would resolve it: A trained draft-stage network that, when used to build decks for the battle stage, allows the agent to consistently win against ByteRL in the battle stage.

### Open Question 3
- Question: Is behavior cloning prior to reinforcement learning fine-tuning beneficial when scaling up to the full game with procedurally generated deck pools?
- Basis in paper: The authors conducted an ablation study showing that behavior cloning prior to RL fine-tuning is beneficial for fixed deck pools, but they plan to experiment with automatic curriculum learning during the battle stage to scale up to the full game.
- Why unresolved: While the ablation study showed benefits for fixed deck pools, it is unclear if these benefits will translate to the full game with procedurally generated deck pools, which is more complex and requires generalization.
- What evidence would resolve it: Experiments demonstrating that behavior cloning followed by RL fine-tuning outperforms training from scratch in the full game with procedurally generated deck pools, particularly in terms of sample efficiency and final performance.

## Limitations
- The results are specific to LOCM 1.5 and may not generalize to other versions or collectible card games.
- The 42.4% win rate against ByteRL achieved through behavior cloning alone may not be representative of all strong agents in the game.
- The specific architecture and hyperparameters used may not be optimal, leaving room for further improvements.

## Confidence
- ByteRL's gameplay is highly exploitable: Medium
- Significant room for improvement over ByteRL: Medium

## Next Checks
1. Test the trained agent against other strong agents in LOCM beyond ByteRL to assess the generalizability of the exploitability findings.
2. Conduct experiments on different versions of LOCM and other collectible card games to evaluate the robustness of the results across various game variants.
3. Perform an ablation study on the specific architecture and hyperparameters used to identify the most critical factors contributing to the agent's performance improvement over ByteRL.