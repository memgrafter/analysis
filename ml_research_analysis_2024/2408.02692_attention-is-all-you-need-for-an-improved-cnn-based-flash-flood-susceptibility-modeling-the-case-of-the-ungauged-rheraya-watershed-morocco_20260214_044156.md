---
ver: rpa2
title: Attention is all you need for an improved CNN-based flash flood susceptibility
  modeling. The case of the ungauged Rheraya watershed, Morocco
arxiv_id: '2408.02692'
source_url: https://arxiv.org/abs/2408.02692
tags:
- flood
- flash
- susceptibility
- attention
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that integrating the convolutional block
  attention module (CBAM) into CNN architectures significantly improves flash flood
  susceptibility modeling. DenseNet121 with CBAM integrated into each convolutional
  block achieved the highest performance with accuracy of 0.95 and AUC of 0.98.
---

# Attention is all you need for an improved CNN-based flash flood susceptibility modeling. The case of the ungauged Rheraya watershed, Morocco

## Quick Facts
- arXiv ID: 2408.02692
- Source URL: https://arxiv.org/abs/2408.02692
- Authors: Akram Elghouat; Ahmed Algouti; Abdellah Algouti; Soukaina Baid
- Reference count: 23
- One-line result: CBAM-integrated CNNs, particularly DenseNet121, significantly improve flash flood susceptibility modeling in ungauged basins

## Executive Summary
This study introduces a novel approach to flash flood susceptibility modeling by integrating the Convolutional Block Attention Module (CBAM) into CNN architectures. The methodology was applied to the ungauged Rheraya watershed in Morocco, utilizing 16 conditioning factors derived from Sentinel-1 and Sentinel-2 satellite data. The DenseNet121 with CBAM integrated into each convolutional block achieved the highest performance, demonstrating the effectiveness of attention mechanisms in improving feature discrimination and model accuracy for flood risk assessment.

## Method Summary
The study employed three CNN architectures (ResNet18, DenseNet121, Xception) with CBAM attention modules integrated at different locations (each convolutional block, head, or tail). The models were trained on 522 flash flood inventory points and 16 conditioning factors (topographical, hydrological, meteorological, environmental, and anthropogenic) preprocessed into 12.5m resolution raster layers. Model performance was evaluated using accuracy, precision, recall, F1-score, and AUC metrics. The best-performing DenseNet+CBAM-In model was then used to generate flash flood susceptibility maps for the Rheraya watershed.

## Key Results
- DenseNet121 with CBAM integrated into each convolutional block achieved the highest accuracy of 0.95 and AUC of 0.98
- Distance to river and drainage density were identified as the most influential conditioning factors for flash flood occurrence
- The approach effectively identified high-risk areas, particularly in tourist zones of the Rheraya watershed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CBAM attention module improves feature discrimination in CNNs by adaptively recalibrating channel and spatial responses
- Mechanism: The module computes per-channel importance weights and spatial attention maps, then multiplies them back into the feature maps, forcing the network to emphasize informative features and suppress irrelevant background
- Core assumption: Flash flood susceptibility features have discriminative spatial and channel-wise patterns that can be learned and weighted
- Evidence anchors:
  - [abstract]: "integrating the convolutional block attention module (CBAM) into CNN architectures significantly improves flash flood susceptibility modeling."
  - [section]: "CBAM combines channel and spatial information through convolution operations, emphasizing key features in the convolution layers."
  - [corpus]: Weak match; no directly relevant paper found
- Break condition: If the data distribution lacks consistent spatial/channel patterns, attention weighting will not provide meaningful gains

### Mechanism 2
- Claim: Inserting CBAM into each convolutional block yields higher performance than head-only or tail-only placement
- Mechanism: Early and intermediate blocks capture fine-grained spatial details; applying attention at each block refines feature hierarchies progressively rather than only at input or output
- Core assumption: Flash flood conditioning factors contain hierarchical spatial patterns that benefit from iterative refinement
- Evidence anchors:
  - [section]: "we evaluated the impact of CBAM placement within CNN architectures, comparing its integration in each convolutional block, in the head, and in the tail of the CNN architectures."
  - [section]: "DenseNet+CBAM-In achieved the highest accuracy (1 and 0.95), followed by Xception+CBAM-In, ResNet+CBAM-In..."
  - [corpus]: Weak; no matching neighbor study
- Break condition: If CNN architecture depth is too shallow, block-level attention offers diminishing returns

### Mechanism 3
- Claim: Attention-based models outperform non-attention baselines on both training and test sets
- Mechanism: By focusing computation on relevant features, attention modules reduce overfitting and gradient issues common in deeper/wider CNNs
- Core assumption: Baseline CNNs without attention are prone to learning non-informative background features
- Evidence anchors:
  - [section]: "Results showed that adding the CBAM attention module significantly improved performance, demonstrating its effectiveness in flash flood susceptibility modeling."
  - [section]: "DenseNet+CBAM-In model also excelled in precision, recall, and F1-score metrics, indicating its ability for creating more accurate flash flood susceptibility maps."
  - [corpus]: Weak; no matching study
- Break condition: If the dataset is very small or highly noisy, attention mechanisms may overfit to spurious patterns

## Foundational Learning

- Concept: Convolutional Block Attention Module (CBAM)
  - Why needed here: CBAM provides a lightweight, architecture-agnostic way to enhance feature representation without heavy computational overhead
  - Quick check question: What are the two sequential sub-modules in CBAM, and what does each focus on?

- Concept: Multi-collinearity and Pearson correlation
  - Why needed here: Ensures that input conditioning factors are independent, improving model stability and interpretability
  - Quick check question: What VIF threshold is typically used to flag problematic multi-collinearity?

- Concept: Receiver Operating Characteristic (ROC) and Area Under Curve (AUC)
  - Why needed here: ROC/AUC provide threshold-independent evaluation of classification performance, critical for comparing models
  - Quick check question: What is the range of AUC values, and what does a value of 0.5 signify?

## Architecture Onboarding

- Component map: Conditioning factors (16 inputs) -> Preprocessing (standardization, alignment) -> CNN backbone (ResNet18/DenseNet121/Xception) -> CBAM attention module -> Output (binary susceptibility map)
- Critical path: Data → Preprocess (standardize, align) → Train/test split → CNN + CBAM → Evaluate (accuracy, precision, recall, F1, AUC) → Susceptibility map
- Design tradeoffs:
  - CBAM adds few parameters but increases inference time slightly
  - Block-level insertion improves accuracy but raises memory usage
  - Baseline vs. attention models: baseline is faster but less discriminative
- Failure signatures:
  - Overfitting: High training accuracy, low test accuracy
  - Vanishing gradients: Training stalls in deeper layers
  - Poor calibration: ROC curve near diagonal
- First 3 experiments:
  1. Baseline DenseNet121 without CBAM → establish performance floor
  2. DenseNet121 with CBAM only at the head → test coarse feature refinement
  3. DenseNet121 with CBAM in each conv block → validate incremental refinement gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the attention-based CNNs perform when applied to flood-prone regions with different geo-environmental settings?
- Basis in paper: [inferred] The authors acknowledge that their study's reliance on data from a single site limits the generalizability of their findings to other ungauged basins
- Why unresolved: The study only tested the models on the Rheraya watershed in Morocco, which may have unique characteristics not representative of other flood-prone areas
- What evidence would resolve it: Evaluating the accuracy and performance of the attention-based CNNs on flood-prone basins with different topographical, hydrological, and climatic conditions

### Open Question 2
- Question: What is the optimal number of conditioning factors to include in flash flood susceptibility modeling using attention-based CNNs?
- Basis in paper: [explicit] The authors state that using an optimal number of conditioning variables is vital, as an excessive number of features can significantly impact the model's performance
- Why unresolved: The study included 16 conditioning factors, but it is unclear if this is the optimal number or if fewer or more factors would yield better results
- What evidence would resolve it: Conducting sensitivity analyses or model comparisons using different numbers of conditioning factors to determine the optimal set for maximizing model performance

### Open Question 3
- Question: How do attention-based CNNs compare to other deep learning models like transformers and graph neural networks in flash flood susceptibility modeling?
- Basis in paper: [explicit] The authors mention that their analysis was limited to attention-based CNNs and baseline CNN architectures, and future work should include other deep learning models
- Why unresolved: The study only compared attention-based CNNs to traditional CNN architectures, without exploring other deep learning techniques that may offer superior performance
- What evidence would resolve it: Conducting a comprehensive comparison of attention-based CNNs, transformers, graph neural networks, and other deep learning models on the same dataset to evaluate their relative performance in flash flood susceptibility modeling

## Limitations
- Limited geographic validation - results are specific to the Rheraya watershed and may not generalize to other ungauged basins
- Lack of detailed preprocessing specifications and CBAM implementation details hinders reproducibility
- Absence of comparison with other attention mechanisms (SE blocks) makes CBAM's relative advantage unclear

## Confidence

- **High confidence** in the effectiveness of CBAM integration improving CNN performance (supported by multiple metrics and comparative results)
- **Medium confidence** in the generalizability of findings to other ungauged basins (limited geographic scope and lack of cross-validation)
- **Medium confidence** in the superiority of DenseNet121+CBAM-In architecture (based on single dataset results without external validation)

## Next Checks
1. Implement the exact preprocessing pipeline for all 16 conditioning factors to verify reproducibility of results
2. Test the DenseNet+CBAM-In architecture on a different ungauged watershed with distinct geographic and climatic conditions
3. Conduct ablation studies comparing CBAM with other attention mechanisms (SE blocks, channel attention) using identical CNN backbones and datasets