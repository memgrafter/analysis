---
ver: rpa2
title: Using Large Language Model for End-to-End Chinese ASR and NER
arxiv_id: '2401.11382'
source_url: https://arxiv.org/abs/2401.11382
tags:
- speech
- arxiv
- decoder-only
- tokens
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work compares two approaches for integrating speech modality
  into large language models (LLMs) for Chinese ASR and NER tasks: (1) decoder-only
  with adapter, and (2) encoder-decoder with cross-attention. The authors fine-tune
  Whisper encoder with ChatGLM3-6B on AISHELL dataset using LoRA adapters.'
---

# Using Large Language Model for End-to-End Chinese ASR and NER

## Quick Facts
- arXiv ID: 2401.11382
- Source URL: https://arxiv.org/abs/2401.11382
- Reference count: 0
- SOTA F1 score of 0.805 on AISHELL-NER test set

## Executive Summary
This paper explores two approaches for integrating speech modality into large language models (LLMs) for Chinese ASR and NER tasks: decoder-only with adapter and encoder-decoder with cross-attention. The authors fine-tune Whisper encoder with ChatGLM3-6B on AISHELL dataset using LoRA adapters, demonstrating that encoder-decoder architecture with cross-attention outperforms decoder-only for short-context ASR, while decoder-only benefits more from long-context due to better utilization of all LLM layers. For NER, both architectures achieve state-of-the-art F1 score of 0.805 using chain-of-thought approach, with decoder-only showing greater improvements with historical context.

## Method Summary
The method employs a two-architecture approach: (1) decoder-only with adapter layer and (2) encoder-decoder with cross-attention. Both use Whisper-large-v2 encoder frozen while fine-tuning ChatGLM3-6B with LoRA adapters (rank 32). Training occurs in three phases: short-form ASR (40 epochs), long-form ASR with historical context (20 epochs), and CoT NER (20 epochs). Special tokens indicate task type and separate utterances. The decoder-only model achieves 4.15% CER vs 4.39% for encoder-decoder on short-form ASR, while encoder-decoder performs better on long-form tasks. CoT NER achieves SOTA 0.805 F1 score.

## Key Results
- Encoder-decoder with cross-attention achieves 4.15% CER vs 4.39% for decoder-only on short-form ASR
- Decoder-only benefits more from long context (2 utterances) reducing omission errors by 7% vs Conformer baseline
- Both architectures achieve SOTA F1 score of 0.805 on AISHELL-NER using chain-of-thought NER
- Decoder-only model shows steady F1 improvement with historical context length, peaking at two utterances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoder-decoder architecture outperforms decoder-only when context is short due to better utilization of LLM deeper layers
- Mechanism: Cross-attention layers allow speech features to directly interact with deeper LLM layers, while decoder-only must propagate speech information through self-attention
- Core assumption: Deeper LLM layers provide more semantic abstraction useful for short-form ASR
- Evidence anchors:
  - [abstract] "encoder-decoder architecture with cross-attention leverages the deeper layers of the LLM and achieves superior performance on the short-form ASR task"
  - [section] "Figure 2 (a) illustrates the gate values across different layers for the cross-attention layer. The gate values remained roughly unchanged across different training phases and deep layers were assigned significantly larger gate values than shallow layers"
  - [corpus] Weak evidence - corpus neighbors focus on decoder-only approaches but don't directly compare architectural differences
- Break condition: If speech features require shallow-level acoustic information rather than semantic abstraction, the cross-attention advantage may diminish

### Mechanism 2
- Claim: Decoder-only architecture benefits more from long context due to dynamic adjustment of speech vs text token importance
- Mechanism: Self-attention allows decoder-only to redistribute attention weights based on context length, while cross-attention remains static
- Core assumption: Long context requires dynamic reweighting of speech and text modalities
- Evidence anchors:
  - [abstract] "decoder-only architecture benefits from a long context as it fully exploits all layers of the LLM"
  - [section] "Figure 2 (b)...The attention score on the speech tokens decreased with longer context as more attention was given to the text tokens"
  - [section] "the decoder-only model can dynamically adjust the importance of text tokens according to the context length"
- Break condition: If the task requires consistent speech feature utilization regardless of context length, the static cross-attention may be preferable

### Mechanism 3
- Claim: Chain-of-thought NER improves performance by first generating accurate ASR transcriptions before entity recognition
- Mechanism: Separating ASR generation from NER prediction allows LLM to focus on transcription accuracy before semantic labeling
- Core assumption: Clean ASR transcriptions are prerequisite for accurate NER
- Evidence anchors:
  - [abstract] "we obtained a state-of-the-art (SOTA) F1 score of 0.805 on the AISHELL-NER test set by using chain-of-thought (CoT) NER which first infers long-form ASR transcriptions and then predicts NER labels"
  - [section] "CoT NER...can be viewed as the combination of pipeline and E2E NER systems as the model accesses both ASR transcriptions and speech features"
- Break condition: If ASR errors are systematic and cannot be corrected through CoT, this approach may not improve NER accuracy

## Foundational Learning

- Concept: Cross-attention vs self-attention mechanisms
  - Why needed here: Understanding how speech features are incorporated differently in each architecture
  - Quick check question: How does cross-attention differ from self-attention in terms of query, key, and value relationships?

- Concept: Chain-of-thought reasoning in language models
  - Why needed here: CoT NER separates transcription from entity recognition, requiring understanding of intermediate reasoning steps
  - Quick check question: What are the benefits of having the model generate intermediate outputs before final predictions?

- Concept: LoRA parameter-efficient fine-tuning
  - Why needed here: The paper uses LoRA to adapt ChatGLM3 to ASR/NER tasks while keeping Whisper frozen
  - Quick check question: How does LoRA modify attention and feedforward layers differently from full fine-tuning?

## Architecture Onboarding

- Component map:
  - Whisper encoder (frozen) → Adapter layer or cross-attention → ChatGLM3-6B with LoRA
  - Input: Speech features (downsampled) + special tokens for task indication
  - Output: ASR transcriptions or NER-labeled text

- Critical path:
  1. Speech features extracted by Whisper encoder
  2. Feature transformation (Adapter or cross-attention projection)
  3. Integration with LLM through either adapter input or cross-attention layers
  4. Autoregressive generation of text output

- Design tradeoffs:
  - Decoder-only: Simpler implementation, better long-context performance, requires speech tokens as input
  - Encoder-decoder: More parameters (437M vs 43M), better short-context performance, static speech integration
  - CoT approach: Two-stage processing, higher accuracy but slower inference

- Failure signatures:
  - Decoder-only: Poor performance on short utterances, unstable attention to speech tokens
  - Encoder-decoder: Suboptimal long-context utilization, rigid speech integration
  - Both: Omission errors for rare entities, substitution errors for Chinese names

- First 3 experiments:
  1. Compare CER on short vs long utterances between decoder-only and encoder-decoder
  2. Visualize attention scores to speech tokens across layers and context lengths
  3. Test CoT NER vs direct NER on entity omission rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the decoder-only architecture's performance on NER tasks change when using longer historical contexts beyond two utterances?
- Basis in paper: [explicit] The paper mentions that the decoder-only model's F1 score increased steadily with historical context length, reaching its peak with two utterances, but does not explore beyond this point.
- Why unresolved: The paper only tested up to two historical utterances, leaving the impact of even longer contexts unexplored.
- What evidence would resolve it: Additional experiments with varying numbers of historical utterances (e.g., three or more) to determine if there is a point of diminishing returns or continued improvement in NER performance.

### Open Question 2
- Question: Can the encoder-decoder architecture be modified to dynamically adjust the importance of speech and text modalities, similar to the decoder-only model?
- Basis in paper: [inferred] The paper suggests that the encoder-decoder model treats speech modality in a static manner, while the decoder-only model can dynamically adjust modality importance.
- Why unresolved: The paper does not explore modifications to the encoder-decoder architecture to enable dynamic modality adjustment.
- What evidence would resolve it: Experiments comparing a modified encoder-decoder model with dynamic gates to the current static version and the decoder-only model, measuring performance on ASR and NER tasks.

### Open Question 3
- Question: How would the integration of the two architectures (decoder-only and encoder-decoder) affect performance on tasks requiring both high-level semantic and fine-grained acoustic information?
- Basis in paper: [explicit] The paper hypothesizes that combining the two approaches might achieve superior performance on a wide range of tasks requiring both semantic and acoustic information.
- Why unresolved: The paper does not implement or test a hybrid model combining the strengths of both architectures.
- What evidence would resolve it: Development and evaluation of a hybrid model on tasks such as audio event detection, comparing its performance to the individual decoder-only and encoder-decoder models.

## Limitations

- Architecture Generalization: The specific benefits observed for short-form vs long-form contexts with Chinese speech data require validation across different languages and domains
- CoT Implementation Details: The paper lacks specificity regarding the exact chain-of-thought format and instruction templates used for NER
- Hardware Requirements: Training these models requires substantial computational resources (8 NVIDIA A100 40GB GPUs)

## Confidence

**High Confidence**: The decoder-only architecture outperforming Conformer baseline by 0.68% absolute CER reduction and achieving SOTA F1 score of 0.805 on AISHELL-NER. These results are directly measurable and the experimental setup is clearly specified.

**Medium Confidence**: The mechanism explanations for why encoder-decoder performs better on short contexts and decoder-only performs better on long contexts. While the attention visualizations support these claims, the underlying assumptions about semantic abstraction layers and dynamic attention adjustment need further validation.

**Low Confidence**: The generalizability of the architectural tradeoffs beyond Chinese speech and the specific AISHELL datasets. The paper doesn't test on multiple languages or domains to establish broader applicability.

## Next Checks

1. **Cross-Architecture Validation**: Implement both architectures on a different language dataset (e.g., English Librispeech or Spanish CommonVoice) to verify if the short-form/long-form context performance patterns hold across languages.

2. **Attention Mechanism Analysis**: Conduct ablation studies removing cross-attention layers from the encoder-decoder model and adding them to the decoder-only model. Measure the impact on CER across different utterance lengths.

3. **CoT Format Sensitivity**: Systematically vary the chain-of-thought instruction format and intermediate reasoning steps while keeping the model architecture constant. Test different prompt structures, reasoning step granularities, and entity formatting approaches.