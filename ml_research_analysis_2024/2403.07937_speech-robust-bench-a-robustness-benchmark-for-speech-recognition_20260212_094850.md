---
ver: rpa2
title: 'Speech Robust Bench: A Robustness Benchmark For Speech Recognition'
arxiv_id: '2403.07937'
source_url: https://arxiv.org/abs/2403.07937
tags:
- speech
- robustness
- robust
- adversarial
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Speech Robust Bench (SRB), a comprehensive
  benchmark designed to evaluate the robustness of automatic speech recognition (ASR)
  models under diverse challenging scenarios. SRB includes 114 perturbations simulating
  environmental noise, special effects, spatial acoustics, and adversarial attacks,
  as well as datasets with accented speech and multi-speaker conversations.
---

# Speech Robust Bench: A Robustness Benchmark For Speech Recognition

## Quick Facts
- arXiv ID: 2403.07937
- Source URL: https://arxiv.org/abs/2403.07937
- Authors: Muhammad A. Shah; David Solans Noguero; Mikko A. Heikkila; Bhiksha Raj; Nicolas Kourtellis
- Reference count: 34
- Key outcome: Introduces SRB, a comprehensive benchmark with 114 perturbations evaluating ASR robustness across environmental noise, special effects, spatial acoustics, adversarial attacks, accented speech, and multi-speaker conversations.

## Executive Summary
Speech Robust Bench (SRB) is a comprehensive benchmark designed to evaluate the robustness of automatic speech recognition (ASR) models under diverse challenging scenarios. The benchmark includes 114 perturbations simulating environmental noise, special effects, spatial acoustics, and adversarial attacks, as well as datasets with accented speech and multi-speaker conversations. SRB evaluates state-of-the-art ASR models, including Whisper, Canary, and various Wav2Vec variants, across English and Spanish speech. Key findings reveal that larger models tend to be more robust, with Whisper (large-v2) outperforming other models on average. However, significant disparities in robustness were observed across demographic subgroups, such as gender and language, highlighting fairness concerns.

## Method Summary
SRB uses a structured taxonomy of 114 perturbations across 6 categories (clean speech, social gatherings, speech variations, environmental effects, digital augmentations, adversarial attacks). Each perturbation is parameterized and applied to clean speech datasets. ASR models transcribe the perturbed audio, and performance is measured using Word Error Rate (WER) and Word Error Rate Degradation (WERD), normalized by estimated speech quality degradation using DNSMOS and PESQ scores. The benchmark supports multi-granularity evaluation at overall, category, and individual perturbation levels, enabling detailed analysis of model weaknesses and fairness disparities.

## Key Results
- Whisper (large-v2) outperforms other models on average across all perturbations.
- Larger ASR models generally show better robustness, but significant disparities exist across demographic subgroups (e.g., gender, language).
- CTC-based models are more robust to adversarial attacks but less robust to non-adversarial perturbations compared to seq2seq and RNN-T models.
- SRB reveals specific vulnerabilities, such as Canary's weakness to special effects and spatial acoustics perturbations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SRB's structured perturbation taxonomy allows precise identification of model weaknesses.
- Mechanism: By categorizing perturbations into 6 high-level groups and defining specific parameters for each, SRB enables fine-grained analysis of which types of corruptions a model struggles with.
- Core assumption: Perturbations within each category are internally consistent and externally valid representations of real-world scenarios.
- Evidence anchors:
  - [abstract] "SRB is composed of 114 input perturbations which simulate an heterogeneous range of corruptions"
  - [section 3.1] Detailed taxonomy of scenarios and perturbation parameters
  - [corpus] Related work "Revisiting Acoustic Features for Robust ASR" supports need for comprehensive perturbation sets
- Break condition: If real-world corruption patterns don't align with SRB categories, the benchmark becomes less predictive of actual performance.

### Mechanism 2
- Claim: Normalized metrics with quality degradation weighting provide fair comparisons across perturbation difficulty.
- Mechanism: By dividing WER/WERD by estimated speech quality degradation (DNSMOS/PESQ scores), SRB ensures errors on "easy" scenarios count more than errors on "hard" scenarios when computing averages.
- Core assumption: DNSMOS and PESQ scores accurately reflect human perception of speech quality degradation.
- Evidence anchors:
  - [section 3.2] "we also estimate speech quality scores using appropriate models and use them to calculate normalized metrics"
  - [section 3.2] Explanation of DNSMOS and PESQ as models of human judgments
  - [corpus] Related work on perceptual evaluation supports using quality-weighted metrics
- Break condition: If DNSMOS/PESQ scores don't correlate with actual transcription difficulty, normalization becomes misleading.

### Mechanism 3
- Claim: Multi-granularity evaluation reveals both overall robustness and specific vulnerability patterns.
- Mechanism: SRB allows evaluation at multiple levels - overall average metrics, category-level breakdowns, individual perturbation analysis, and severity-level comparisons.
- Core assumption: Different granularities provide complementary insights rather than redundant information.
- Evidence anchors:
  - [section 4.2.1] "To identify the specific types of sFX and spatial acoustic perturbations against which cnry-1b lacks robustness, we plot the WERD on each perturbation"
  - [section 4.4] Analysis of robustness disparities across population subgroups
  - [corpus] Related work on auditing ASR fairness supports multi-level analysis approach
- Break condition: If high-level metrics don't correlate with low-level patterns, the multi-granularity approach loses value.

## Foundational Learning

- Concept: Automatic Speech Recognition (ASR) fundamentals
  - Why needed here: Understanding ASR model architecture and evaluation metrics is crucial for interpreting SRB results
  - Quick check question: What is the difference between CTC loss and cross-entropy loss in ASR training?

- Concept: Speech signal processing and perturbations
  - Why needed here: Understanding how environmental noise, room acoustics, and digital effects affect audio signals is essential for interpreting SRB scenarios
  - Quick check question: How does adding environmental noise at different SNR levels affect speech intelligibility?

- Concept: Robustness evaluation methodology
  - Why needed here: Understanding concepts like adversarial attacks, domain adaptation, and subgroup fairness is necessary for comprehensive SRB analysis
  - Quick check question: What's the difference between utterance-specific and utterance-agnostic adversarial attacks?

## Architecture Onboarding

- Component map: Perturbation generation module -> Transcription evaluation pipeline -> Metrics computation engine -> Quality estimation module -> Analysis visualization tools

- Critical path:
  1. Load clean speech dataset
  2. Apply selected perturbations with specified parameters
  3. Transcribe perturbed audio with target ASR model
  4. Compute quality scores for original and perturbed audio
  5. Calculate metrics and normalize by quality degradation
  6. Aggregate results across scenarios

- Design tradeoffs:
  - Comprehensive vs. efficient: Including all 114 perturbations provides thorough analysis but increases computational cost
  - Synthetic vs. real: Digital perturbations are controllable but may not capture all real-world complexities
  - Normalized vs. raw metrics: Quality-weighted metrics are fairer but may obscure absolute performance

- Failure signatures:
  - Inconsistent quality score degradation patterns across perturbations
  - Unexpected correlations between model size and robustness
  - Disproportionate subgroup performance disparities

- First 3 experiments:
  1. Baseline: Evaluate a simple ASR model on clean LibriSpeech and perturbed versions
  2. Category analysis: Compare model performance across SRB's 6 main categories
  3. Granularity test: Analyze individual perturbation performance to identify specific weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do larger ASR models consistently maintain their robustness advantage across all languages and demographic subgroups?
- Basis in paper: Explicit - The paper states "larger models tend to be more robust" but also observes significant disparities across English and Spanish speakers, and male and female speakers.
- Why unresolved: The paper only evaluates English and Spanish languages, and a limited set of demographic subgroups. The observed disparities suggest that model size may not be a universal solution.
- What evidence would resolve it: Systematic robustness evaluations of large ASR models across a diverse set of languages, accents, and demographic subgroups, controlling for other factors like training data composition.

### Open Question 2
- Question: What specific training data characteristics or augmentations lead to improved robustness against special effects and spatial acoustics?
- Basis in paper: Explicit - The paper notes that Whisper's robustness to special effects and spatial acoustics is "interesting" given its minimal data augmentation, suggesting it may have been trained on diverse digital media.
- Why unresolved: The paper does not have access to Whisper's training data or details about its composition. The speculation about diverse data sources is not confirmed.
- What evidence would resolve it: Analysis of the training data composition of robust models, particularly focusing on the presence of music, movie soundtracks, and diverse acoustic environments. Controlled experiments varying training data characteristics.

### Open Question 3
- Question: How do different ASR model architectures (seq2seq, CTC, RNN-T) perform in terms of robustness when controlling for model size and training data?
- Basis in paper: Explicit - The paper observes that CTC models are more robust to adversarial attacks, but less robust than seq2seq and RNN-T models on non-adversarial perturbations.
- Why unresolved: The paper compares models of different sizes and training data, making it difficult to isolate the effect of architecture. The observed trends may be confounded by these other factors.
- What evidence would resolve it: Direct comparison of ASR models with identical sizes and training data but different architectures (e.g., Wav2Vec with seq2seq vs. CTC head), evaluated on SRB.

## Limitations
- Some synthetic perturbations may not fully capture real-world complexity, particularly for spatial acoustics and adversarial attacks.
- DNSMOS and PESQ quality scores may not perfectly correlate with transcription difficulty across all perturbation types.
- Benchmark's ability to detect fairness issues depends on demographic representation in underlying datasets.

## Confidence
- High confidence: Overall benchmark methodology, perturbation taxonomy structure, and general findings about model robustness patterns.
- Medium confidence: Specific performance numbers, demographic subgroup disparities, and quality-normalized metrics, pending external validation.
- Low confidence: Exact severity thresholds and their impact on relative model rankings, as these depend on subjective quality assessments.

## Next Checks
1. Apply SRB to additional ASR models beyond the initial six tested, particularly emerging architectures like Whisper-improvements and SAM-based systems.
2. Conduct field studies comparing SRB predictions with actual performance in deployed systems under similar challenging conditions.
3. Evaluate SRB's subgroup analysis capabilities on more diverse speech datasets to verify fairness findings across broader population segments.