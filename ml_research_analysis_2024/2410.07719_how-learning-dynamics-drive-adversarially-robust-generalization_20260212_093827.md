---
ver: rpa2
title: How Learning Dynamics Drive Adversarially Robust Generalization?
arxiv_id: '2410.07719'
source_url: https://arxiv.org/abs/2410.07719
tags:
- robust
- adversarial
- learning
- covariance
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a PAC-Bayesian framework that links adversarial
  robustness to the posterior covariance of model parameters and the Hessian curvature
  of the adversarial loss landscape. The authors derive closed-form posterior covariances
  for both the stationary regime and the early phase of non-stationary transition
  during SGD with momentum, capturing how learning rate, gradient noise, and Hessian
  structure jointly shape robust generalization.
---

# How Learning Dynamics Drive Adversarially Robust Generalization?

## Quick Facts
- arXiv ID: 2410.07719
- Source URL: https://arxiv.org/abs/2410.07719
- Authors: Yuelin Xu; Xiao Zhang
- Reference count: 40
- Key outcome: PAC-Bayesian framework linking adversarial robustness to posterior covariance and Hessian curvature; controlled CIFAR experiments show curvature/noise spikes after learning rate decay explain robust overfitting.

## Executive Summary
This paper establishes a PAC-Bayesian framework that links adversarial robustness to the posterior covariance of model parameters and the Hessian curvature of the adversarial loss landscape. The authors derive closed-form posterior covariances for both the stationary regime and the early phase of non-stationary transition during SGD with momentum, capturing how learning rate, gradient noise, and Hessian structure jointly shape robust generalization. Through controlled empirical studies on CIFAR-10, CIFAR-100, and SVHN, they show that Hessian eigenvalues and gradient-noise eigenvalues increase sharply after learning rate decay, while posterior covariances shrink, leading to larger PAC-Bayesian bounds and explaining robust overfitting. The analysis demonstrates that flatness-promoting methods like adversarial weight perturbation (AWP) suppress curvature and noise, resulting in tighter bounds and better robustness. The findings highlight the central role of coupled curvature–posterior geometry in adversarially robust generalization.

## Method Summary
The paper uses a PAC-Bayesian framework to study how learning dynamics drive adversarially robust generalization. It models SGD with momentum as a linear dynamical system near a local optimum, deriving closed-form posterior covariances under quadratic loss approximation. The analysis covers both stationary regimes and non-stationary transitions after learning rate decay. The method involves spectral estimation of Hessian and gradient noise eigenvalues, computation of PAC-Bayesian bounds, and empirical validation on CIFAR-10/100 and SVHN using PreActResNet-18 trained with PGD adversarial training and AWP.

## Key Results
- Hessian and gradient-noise eigenvalues spike after learning rate decay, causing posterior covariance shrinkage and increased PAC-Bayesian bounds
- AWP reduces curvature and noise magnitudes, yielding tighter bounds and better robustness
- PAC-Bayesian terms λσ² and -lnσ² capture the tradeoff between flatness and noise in the loss landscape
- Empirical observations confirm theoretical predictions of posterior covariance dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The posterior covariance under SGD with momentum converges to a stationary distribution whose eigenvalues depend on the Hessian spectrum and gradient noise covariance.
- Mechanism: In a quadratic basin, the dynamics of SGD with Polyak momentum can be modeled as a linear dynamical system. The covariance of the parameter distribution evolves according to a Lyapunov equation whose solution depends on the eigenvalues of the Hessian and the noise covariance. When these matrices commute, their shared eigenvectors diagonalize the covariance update, yielding a closed-form solution for the stationary posterior covariance.
- Core assumption: The loss is approximately quadratic near a local optimum, the Hessian and noise covariance matrices commute, and the learning rate and momentum are stable.
- Evidence anchors:
  - [abstract]: "By characterizing discrete-time SGD dynamics near a local optimum under quadratic loss, we derive closed-form posterior covariances..."
  - [section]: "Assuming C commutes with H* under the stationary regime aligns with empirical observations that gradient noise covariance tends to align with the Hessian eigenspectrum..."
  - [corpus]: Weak - none of the neighboring papers directly address the commutativity of Hessian and noise covariance in the context of posterior covariance derivation.
- Break condition: If the Hessian and noise covariance do not commute, the posterior covariance will have non-diagonal cross-terms, breaking the closed-form solution and making the PAC-Bayesian bound less interpretable.

### Mechanism 2
- Claim: Learning rate decay triggers a non-stationary transition in the posterior covariance, temporarily reducing the PAC-Bayesian bound before it increases due to robust overfitting.
- Mechanism: When the learning rate drops, the SGD dynamics shift from one stationary regime to another. During the early transition phase, the posterior covariance interpolates between the old and new stationary covariances, with a decaying factor determined by the momentum and new learning rate. This transient behavior causes the bound to decrease initially, mimicking the empirical pattern of robust loss dropping briefly before rising.
- Core assumption: The system is near a local optimum, the noise covariance remains time-invariant after the decay, and the eigenvalues of the transition matrix are inside the unit disk.
- Evidence anchors:
  - [abstract]: "...the early phase of non-stationary transition during SGD with momentum..."
  - [section]: "The following theorem, whose formal version and proof are in Appendix B.3, shows how the posterior covariance ΣQ behaves in the early non-stationary transient phase."
  - [corpus]: Weak - neighboring papers focus on generalization bounds and Jacobian regularization but do not explicitly model the non-stationary transition of posterior covariance after learning rate decay.
- Break condition: If the noise covariance changes significantly during the transition, or if the learning rate drop is too large causing instability, the interpolation formula and the resulting bound may not hold.

### Mechanism 3
- Claim: Methods like Adversarial Weight Perturbation (AWP) reduce curvature and gradient noise, leading to larger posterior covariances and tighter PAC-Bayesian bounds.
- Mechanism: AWP explicitly regularizes the loss landscape to be flatter by perturbing weights during training. This reduces the magnitude of Hessian eigenvalues and the noise covariance along the principal curvature directions. With smaller λ and γ, the posterior covariance eigenvalues σ² increase (due to the inverse dependence on λ), and the dominating terms λσ² and -lnσ² in the bound decrease, yielding better robustness.
- Core assumption: The reduction in curvature and noise translates directly to changes in the posterior covariance structure, and the PAC-Bayesian bound remains tight under these changes.
- Evidence anchors:
  - [abstract]: "...flatness-promoting techniques like adversarial weight perturbation help to improve robustness."
  - [section]: "Compared to standard AT, both the Hessian eigenvalues {λi} and the noise eigenvalues {γi} are substantially smaller in magnitude... the two dominating terms λσ² and -lnσ² are significantly reduced, yielding a smaller overall bound."
  - [corpus]: Weak - while neighboring papers mention Jacobian regularization and sharpness-aware minimization, they do not specifically analyze the effect of these methods on posterior covariance structure and PAC-Bayesian bounds.
- Break condition: If AWP changes the noise covariance in a way that breaks the commutativity with the Hessian, or if the reduction in curvature is offset by increased noise elsewhere, the benefit to the bound may not materialize.

## Foundational Learning

- Concept: PAC-Bayesian framework
  - Why needed here: The paper uses PAC-Bayesian bounds to relate the generalization gap to the KL divergence between a prior and posterior over model parameters, with the posterior capturing the effect of SGD dynamics.
  - Quick check question: What is the role of the KL divergence term in the PAC-Bayesian bound, and how does it relate to the posterior covariance?
- Concept: Stochastic Gradient Descent (SGD) with momentum dynamics
  - Why needed here: The paper models SGD with momentum as a linear dynamical system to derive the evolution of the posterior covariance, requiring understanding of how gradients and noise propagate through the parameter updates.
  - Quick check question: How does the momentum term affect the stability and convergence of the posterior covariance in the stationary regime?
- Concept: Hessian matrix and its spectrum
  - Why needed here: The Hessian at a local optimum determines the curvature of the loss landscape, which directly influences the eigenvalues of the posterior covariance and the PAC-Bayesian bound.
  - Quick check question: Why is the commutativity between the Hessian and noise covariance important for obtaining a closed-form solution for the posterior covariance?

## Architecture Onboarding

- Component map: Quadratic loss approximation -> PAC-Bayesian bound derivation -> SGD with momentum dynamics model -> Stationary and non-stationary covariance analysis -> Empirical validation
- Critical path: Establish quadratic loss -> Derive PAC-Bayes bound -> Model SGD dynamics -> Solve for posterior covariance -> Analyze bound terms -> Validate empirically
- Design tradeoffs: Using a quadratic loss simplifies analysis but may not capture all aspects of deep learning loss landscapes; assuming commutativity between Hessian and noise covariance enables closed-form solutions but may not always hold.
- Failure signatures: If the commutativity assumption breaks, the posterior covariance will have cross-terms; if the quadratic approximation is poor, the derived covariances may not match empirical observations; if the noise covariance is not time-invariant, the non-stationary analysis may fail.
- First 3 experiments:
  1. Verify the quadratic approximation of the adversarial loss near a local optimum by comparing the empirical loss to a second-order Taylor expansion.
  2. Test the commutativity assumption by measuring the degree of alignment between the top eigenvectors of the Hessian and noise covariance matrices during training.
  3. Implement the SGD with momentum dynamics model and compare the predicted posterior covariance eigenvalues to those estimated from the training trajectory.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do posterior covariance dynamics generalize to adaptive optimizers like Adam or SGD with non-constant learning rate schedules?
- Basis in paper: [inferred] The analysis assumes fixed learning rate and Polyak momentum; the authors note extending to adaptive optimizers is a promising future direction.
- Why unresolved: The stationary and transient covariance formulas rely on closed-form eigenvalue tracking in fixed learning rate regimes; adaptive methods introduce per-parameter learning rates and moment estimates that break the shared eigenspace assumption.
- What evidence would resolve it: Empirical tracking of Hessian and gradient noise eigenvalues under Adam or learning rate schedules, combined with an extended PAC-Bayes bound incorporating the optimizer's update rules.

### Open Question 2
- Question: Can flatness-promoting methods like AWP be further optimized by explicitly targeting the bound's dominant terms (e.g., λσ² and −lnσ²)?
- Basis in paper: [explicit] The authors observe AWP reduces both curvature and noise magnitudes, yielding smaller bounds; they suggest future work on designing methods informed by the PAC-Bayes bound.
- Why unresolved: While AWP empirically improves robustness, the mechanism is not formally tied to minimizing specific PAC-Bayes terms; no systematic ablation of perturbation magnitude or frequency is provided.
- What evidence would resolve it: Controlled ablation studies varying AWP hyperparameters and correlating them with changes in the bound's dominant terms and test robustness.

### Open Question 3
- Question: What is the precise relationship between the commutativity/alignment of Hessian and gradient noise, and the tightness of the PAC-Bayes bound?
- Basis in paper: [explicit] The authors show strong commutativity and alignment under AT and AWP, and note that breaking commutativity would increase bound looseness.
- Why unresolved: The experiments demonstrate alignment empirically but do not quantify how much bound tightness degrades when alignment is artificially reduced.
- What evidence would resolve it: Synthetic experiments that deliberately decorrelate Hessian and noise directions, measuring the resulting increase in bound values and generalization gap.

## Limitations
- The analysis relies on quadratic approximation of the adversarial loss landscape, which may not hold in highly non-convex regions
- The commutativity assumption between Hessian and noise covariance matrices may not always be valid in deep neural networks
- Empirical validation is limited to PreActResNet-18 on standard image datasets, potentially limiting generalizability

## Confidence
- High: Mathematical derivation of stationary posterior covariance under quadratic loss and commuting Hessian/noise assumptions
- Medium: Non-stationary transition analysis after learning rate decay and its impact on PAC-Bayesian bounds
- Medium: Quantitative predictions of bound terms and their relationship to empirical robustness

## Next Checks
1. Test commutativity of Hessian and gradient noise covariance empirically across multiple layers and training epochs
2. Validate the quadratic approximation by comparing predicted versus observed robust loss evolution after learning rate decay
3. Implement AWP and verify whether reduced curvature/noise directly translates to improved PAC-Bayesian bound tightness