---
ver: rpa2
title: Joint enhancement of automatic chest X-ray diagnosis and radiological gaze
  prediction with multi-stage cooperative learning
arxiv_id: '2403.16970'
source_url: https://arxiv.org/abs/2403.16970
tags:
- visual
- proposed
- learning
- classification
- saliency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of enhancing both chest X-ray
  diagnosis and radiological gaze prediction using a deep learning framework. The
  core method idea involves a novel multi-stage cooperative learning strategy, utilizing
  a dual-encoder UNet with multi-scale feature-fusion and a multi-task learning approach.
---

# Joint enhancement of automatic chest X-ray diagnosis and radiological gaze prediction with multi-stage cooperative learning

## Quick Facts
- arXiv ID: 2403.16970
- Source URL: https://arxiv.org/abs/2403.16970
- Reference count: 31
- Achieves 0.93 AUC for chest X-ray diagnosis and 0.58 PCC for visual attention map prediction

## Executive Summary
This paper presents a novel multi-stage cooperative learning strategy to jointly enhance chest X-ray diagnosis and radiological gaze prediction. The approach uses a dual-encoder UNet with multi-scale feature fusion and a multi-task learning framework. By gradually introducing collaboration between tasks through staged training, the method addresses asynchronous convergence issues common in multi-task learning while improving both diagnostic accuracy and attention map prediction.

## Method Summary
The method employs a three-stage cooperative learning strategy. First, DenseNet-201 is pretrained with contrastive triplet loss and fine-tuned for CXR classification. Second, a Res SE-UNet is trained with DenseNet-201 frozen to predict visual saliency maps using KL divergence loss. Finally, a multi-scale feature-fusion classifier is trained with both encoders frozen to perform disease classification using cross-entropy loss. The approach integrates features from both encoders to improve classification performance while simultaneously predicting attention maps that align with radiologist gaze patterns.

## Key Results
- Achieved 0.93 AUC for chest X-ray diagnosis, outperforming baselines including MT-UNet and DenseNet-201
- Obtained 0.58 Pearson's correlation coefficient for visual attention map prediction
- Demonstrated improved performance over uncertainty-based training strategies while requiring fewer learnable parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-stage cooperative learning strategy prevents asynchronous convergence of task-specific losses.
- Mechanism: By training DenseNet-201 first (contrastive pretraining), then the Res SE-UNet, and finally the classifier, each stage leverages features from the previous one, reducing competition between tasks and stabilizing gradients.
- Core assumption: Gradual feature sharing between tasks is more effective than simultaneous joint training with a single loss.
- Evidence anchors:
  - [abstract] "To tackle the issue of asynchronous training schedules of individual tasks in multi-task learning, we proposed a multi-stage cooperative learning strategy"
  - [section] "we proposed a multi-stage cooperative learning strategy to best optimize the training of individual tasks, with a gradual introduction of collaboration"
  - [corpus] Weak evidence: no direct comparison of staged vs. simultaneous multi-task training in the neighbor papers.
- Break condition: If intermediate features from earlier stages are noisy or misaligned with later task needs, staged training could propagate errors instead of benefits.

### Mechanism 2
- Claim: The dual-encoder Res SE-UNet extracts richer, complementary features for saliency map prediction.
- Mechanism: DenseNet-201 provides high-level classification features; Res SE-UNet adds local spatial details and channel-wise recalibration via SE blocks, improving heatmap quality.
- Core assumption: Combining features from two distinct encoders yields more informative input than a single encoder alone.
- Evidence anchors:
  - [section] "we modified the UNet's encoder blocks with Residual and Squeeze-and-Excitation (SE) blocks... to enhance its robustness and training stability"
  - [section] "By leveraging features from two distinct image encoders to extract richer and more nuanced information from the CXR images, we intended to enhance the accuracy of visual saliency map prediction"
  - [corpus] Weak evidence: neighbor papers do not explicitly evaluate dual-encoder saliency prediction.
- Break condition: If the two encoders capture highly redundant features, the benefit diminishes; if their feature spaces are incompatible, fusion could degrade performance.

### Mechanism 3
- Claim: Multi-scale feature fusion in the classifier improves disease classification by integrating both classification-oriented and saliency-oriented features.
- Mechanism: Concatenating DenseNet-201 features with Res SE-UNet's upsampled features allows the classifier to use both abstract disease cues and spatial attention patterns.
- Core assumption: Saliency-relevant features contain discriminative information for disease classification that is not fully captured by DenseNet alone.
- Evidence anchors:
  - [abstract] "a multi-scale feature-fusion classifier to perform disease classification"
  - [section] "we concatenated the feature from the DenseNet-201 encoder and that from the last upsampling layer of the Res SE-UNet, and fed them into a simple CNN classifier"
  - [corpus] Weak evidence: no direct ablation on feature fusion in neighbor papers.
- Break condition: If saliency features are too noisy or unrelated to diagnosis, they could mislead the classifier and reduce accuracy.

## Foundational Learning

- Concept: Multi-task learning loss balancing
  - Why needed here: Without proper balancing, one task (e.g., classification) can dominate training, starving the other task (saliency prediction) of gradient updates.
  - Quick check question: How would you detect if one task is dominating training based on loss curves?
- Concept: Contrastive learning with triplet loss
  - Why needed here: Pretraining DenseNet-201 with contrastive learning improves feature discriminability, which benefits both classification and saliency prediction.
  - Quick check question: What is the effect of using a triplet margin that is too small or too large during pretraining?
- Concept: Attention mechanisms (SE blocks, self-attention)
  - Why needed here: SE blocks recalibrate channel-wise features, helping the model focus on clinically relevant image regions for both tasks.
  - Quick check question: What happens to model performance if SE blocks are removed from the Res SE-UNet?

## Architecture Onboarding

- Component map: DenseNet-201 encoder -> Res SE-UNet encoder-decoder -> multi-scale feature fusion classifier
- Critical path: DenseNet-201 → Res SE-UNet → classifier (staged pipeline)
- Design tradeoffs:
  - Staged training reduces asynchronous loss convergence but increases overall training time.
  - Dual encoders improve feature diversity but increase memory and compute cost.
  - SE blocks improve representation but add parameters and inference latency.
- Failure signatures:
  - Saliency maps become blurry or mislocalized → likely issue in Res SE-UNet training or feature fusion.
  - Classification accuracy stalls or drops → possible over-regularization from saliency features or misalignment in feature fusion.
  - Training instability in later stages → earlier stages may not produce useful features or loss weighting is unbalanced.
- First 3 experiments:
  1. Train only DenseNet-201 with contrastive pretraining and evaluate classification AUC.
  2. Train Res SE-UNet alone (no DenseNet) and measure saliency map KL divergence.
  3. Train the full pipeline in a single joint stage (no staged training) and compare AUC/heatmap quality against the staged approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed multi-stage cooperative learning strategy compare to uncertainty-based training strategies in terms of training efficiency and model performance?
- Basis in paper: [explicit] The paper states that uncertainty-based training strategies require more elaborate setups and additional learnable parameters, while the proposed method uses a simpler three-stage cooperative learning strategy.
- Why unresolved: The paper does not provide a direct comparison of training efficiency or performance metrics between the two approaches.
- What evidence would resolve it: Experimental results comparing training time, convergence rates, and performance metrics of both approaches under the same conditions.

### Open Question 2
- Question: Can the inclusion of additional gaze pattern representations, such as scanpaths, further improve the accuracy of radiological diagnosis beyond the current use of visual saliency maps?
- Basis in paper: [explicit] The paper mentions that scanpaths could enrich the understanding of the diagnostic procedure and potentially enhance diagnostic accuracy, but it has not been adopted for clinical scans.
- Why unresolved: The paper does not explore the use of scanpaths or provide experimental results on their impact.
- What evidence would resolve it: Experiments incorporating scanpath data into the model and comparing performance metrics with and without scanpaths.

### Open Question 3
- Question: How does the proposed method perform when applied to other medical imaging modalities, such as CT or MRI scans, beyond chest X-rays?
- Basis in paper: [inferred] The paper focuses on chest X-ray analysis and mentions the potential for extension to other applications, but does not provide evidence of performance on other modalities.
- Why unresolved: The paper does not include experiments or results on other medical imaging modalities.
- What evidence would resolve it: Application and validation of the proposed method on CT or MRI datasets, with performance metrics compared to existing techniques.

## Limitations

- The staged training approach's superiority over joint multi-task learning is asserted but not directly validated against a simultaneous joint-training baseline.
- Architectural details of the Res SE-UNet and the multi-scale fusion classifier are not fully specified, making exact reproduction challenging.
- The dataset size (983 training samples) is relatively small, raising questions about generalization and robustness of the reported improvements.

## Confidence

- **High confidence** in the core methodology (three-stage cooperative learning) as a plausible approach to multi-task training stability.
- **Medium confidence** in the dual-encoder architecture's contribution, as the paper provides a clear rationale but lacks direct ablation studies.
- **Low confidence** in the claimed performance gains without access to the exact model configurations and hyperparameters used.

## Next Checks

1. Implement and compare the three-stage cooperative learning pipeline against a single-stage joint training setup with a unified loss function, measuring differences in AUC and saliency map quality.
2. Train the classifier using only DenseNet-201 features (no saliency features) and compare classification accuracy to isolate the contribution of multi-scale feature fusion.
3. Reconstruct the Res SE-UNet based on the description and validate its performance on a standard saliency prediction benchmark (e.g., MIT300) to ensure it produces meaningful attention maps.