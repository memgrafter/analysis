---
ver: rpa2
title: 'TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent
  Generation'
arxiv_id: '2402.10178'
source_url: https://arxiv.org/abs/2402.10178
tags:
- task
- tasks
- agents
- arxiv
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of error propagation and limited
  adaptability in LLM-based agents for complex real-world tasks. The authors propose
  TDAG, a multi-agent framework that dynamically decomposes tasks and generates specialized
  subagents for each subtask.
---

# TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation

## Quick Facts
- arXiv ID: 2402.10178
- Source URL: https://arxiv.org/abs/2402.10178
- Reference count: 10
- Outperforms baselines on ItineraryBench with 49.08 average score

## Executive Summary
This paper addresses error propagation and limited adaptability in LLM-based agents for complex real-world tasks by proposing TDAG, a multi-agent framework that dynamically decomposes tasks and generates specialized subagents. The framework introduces ItineraryBench, a travel planning benchmark with fine-grained evaluation metrics. TDAG significantly outperforms baselines like ReAct and P&E, demonstrating superior adaptability and context awareness in complex task scenarios.

## Method Summary
TDAG is a multi-agent framework that dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent. The framework uses LLM prompting to create customized subagents with tailored tool documentation and incremental skill libraries. An incremental skill library stores past experiences as skills, which are retrieved using SentenceBERT similarity matching for similar subtasks. The system also includes a simulator for evaluating plan executability and constraint satisfaction.

## Key Results
- Achieves average score of 49.08 on ItineraryBench, outperforming ReAct (43.02) and P&E (42.85)
- Demonstrates superior adaptability through dynamic task decomposition and agent generation
- Shows effectiveness in preventing error propagation by updating subsequent subtasks based on earlier outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic task decomposition prevents error propagation by allowing later subtasks to be updated based on the success or failure of earlier ones.
- Mechanism: When a subtask fails, the main agent updates subsequent subtasks instead of blindly executing a fixed sequence, avoiding cascading failures.
- Core assumption: Real-time feedback from subtask execution is available and can be used to modify the task decomposition plan.
- Evidence anchors:
  - [abstract] "dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability"
  - [section] "Crucially, these decomposed subtasks are not static, but will be dynamically adjusted based on the outcomes of preceding tasks"
  - [corpus] Weak - no direct mention of error propagation prevention in corpus neighbors
- Break condition: If subtask outcomes cannot be accurately assessed in real-time, or if the main agent cannot generate meaningful updates to the task plan.

### Mechanism 2
- Claim: Generating specialized subagents for each subtask improves performance by reducing irrelevant context and focusing on task-specific requirements.
- Mechanism: Instead of using fixed subagents, TDAG creates customized subagents with tailored tool documentation and incremental skill libraries for each subtask.
- Core assumption: Subagent customization through LLM prompting can produce agents that are better suited to their specific subtasks than generic agents.
- Evidence anchors:
  - [abstract] "This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent"
  - [section] "Differing from previous methods... our framework allows for dynamic customization of subagents. This customization is achieved through LLM prompting"
  - [corpus] Moderate - some related work mentions modular task decomposition and dynamic collaboration
- Break condition: If LLM prompting cannot generate effective customizations, or if the overhead of generating new subagents outweighs the benefits.

### Mechanism 3
- Claim: The incremental skill library allows subagents to leverage past experiences and improve performance on similar subtasks.
- Mechanism: After completing a subtask, subagents summarize their process and store it as a skill. When facing similar subtasks, they retrieve relevant skills using SentenceBERT similarity matching.
- Core assumption: Similar subtasks will benefit from similar solution approaches, and the skill retrieval mechanism can effectively identify relevant past experiences.
- Evidence anchors:
  - [section] "Our framework generates skills for subagents based on the agent behavior during task execution. These skills are then stored in a skill library"
  - [section] "When employing a skill, we use a small SentenceBERT model to retrieve the most relevant skill from the library based on the subtask's content"
  - [corpus] Weak - no direct mention of skill libraries in corpus neighbors
- Break condition: If subtasks are too diverse for past experiences to be relevant, or if the skill retrieval mechanism fails to identify appropriate skills.

## Foundational Learning

- Concept: Task decomposition and its impact on problem complexity
  - Why needed here: Understanding how breaking complex tasks into smaller subtasks affects the overall problem-solving process and potential for error propagation
  - Quick check question: How does task decomposition help prevent the "cascading failure" problem in multi-step reasoning tasks?

- Concept: Multi-agent systems and coordination
  - Why needed here: The framework relies on multiple specialized agents working together, requiring understanding of agent communication and task allocation
  - Quick check question: What are the key challenges in coordinating multiple agents working on different parts of a complex task?

- Concept: Tool usage and external information integration
  - Why needed here: Agents need to effectively use databases and Python interpreters to access information and perform calculations during task execution
  - Quick check question: How can agents be designed to effectively integrate external tools and information sources into their decision-making process?

## Architecture Onboarding

- Component map:
  - Main Agent -> Task Decomposition -> Subagent Generator -> Subagents -> Skill Library -> Simulator -> Database

- Critical path: Task decomposition → Subagent generation → Subtask execution → Skill summarization → Plan evaluation

- Design tradeoffs:
  - Dynamic vs. static task decomposition: Flexibility vs. computational overhead
  - Custom vs. generic subagents: Performance vs. generation cost
  - Skill library size vs. retrieval accuracy: Memory usage vs. relevance

- Failure signatures:
  - Error propagation despite dynamic updates: Main agent cannot generate effective task modifications
  - Subagent customization failures: LLM prompting produces ineffective or irrelevant subagents
  - Skill library ineffectiveness: Retrieved skills are not relevant to current subtasks

- First 3 experiments:
  1. Test dynamic task decomposition by running a complex task with intentional early failures and observing if subsequent subtasks are updated
  2. Compare performance of custom vs. generic subagents on a representative subtask to measure the benefit of specialization
  3. Evaluate skill library effectiveness by running similar subtasks with and without skill retrieval and measuring performance differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic task decomposition in TDAG affect long-term planning and adaptability in scenarios with changing constraints or objectives?
- Basis in paper: [explicit] The paper discusses dynamic task decomposition and its role in adapting to evolving task requirements, but does not provide extensive analysis on long-term adaptability.
- Why unresolved: The paper focuses on immediate task adaptation but does not explore how the framework handles extended scenarios where constraints or objectives might change over time.
- What evidence would resolve it: Experimental results showing TDAG's performance in scenarios with evolving constraints or objectives over extended periods would clarify its long-term adaptability.

### Open Question 2
- Question: What is the impact of the incremental skill library on the efficiency and effectiveness of subagents in novel tasks that require unique skills not previously encountered?
- Basis in paper: [explicit] The paper mentions the incremental skill library and its role in skill refinement, but does not address its effectiveness in novel tasks.
- Why unresolved: The paper highlights the library's adaptability but does not explore its limitations or performance when encountering entirely new skills.
- What evidence would resolve it: Comparative studies showing TDAG's performance on novel tasks with unique skill requirements versus tasks with previously encountered skills would provide insights into the library's impact.

### Open Question 3
- Question: How does the fine-grained evaluation system in ItineraryBench influence the development of more robust and adaptable agents compared to traditional binary evaluation methods?
- Basis in paper: [explicit] The paper introduces a fine-grained evaluation system and contrasts it with binary scoring, but does not provide a comprehensive analysis of its impact on agent development.
- Why unresolved: While the paper suggests that fine-grained evaluation offers a more accurate reflection of agent capabilities, it does not explore how this influences the design and improvement of agents over time.
- What evidence would resolve it: Longitudinal studies comparing agent performance and development using fine-grained versus binary evaluation methods would elucidate the impact on agent robustness and adaptability.

## Limitations

- Specific task decomposition and agent generation prompts are not provided, making exact reproduction difficult
- Implementation details of the incremental skill library, including SentenceBERT model configuration, remain unclear
- Evaluation is conducted on a single benchmark (ItineraryBench), limiting generalizability to other complex task domains

## Confidence

- **High confidence**: The core architectural components of TDAG (dynamic task decomposition, specialized subagent generation, incremental skill library) are well-described and logically sound. The reported performance improvements on ItineraryBench are specific and measurable.
- **Medium confidence**: The mechanism of error propagation prevention through dynamic updates is theoretically plausible but lacks direct empirical validation in the paper. The benefits of subagent customization through LLM prompting are claimed but not extensively demonstrated across diverse task types.
- **Low confidence**: The effectiveness of the skill library for handling similar subtasks is weakly supported, as there is limited evidence showing how skill retrieval improves performance over multiple task executions.

## Next Checks

1. **Dynamic Decomposition Validation**: Conduct controlled experiments where intentional failures are introduced in early subtasks to verify that the main agent successfully updates subsequent subtasks and prevents cascading failures.

2. **Subagent Customization Evaluation**: Compare the performance of TDAG's custom subagents against generic subagents on a diverse set of subtasks to quantify the benefits of specialization across different task types.

3. **Skill Library Generalization**: Test the skill library on multiple instances of similar subtasks across different complex tasks to measure whether retrieved skills consistently improve performance and reduce execution time.