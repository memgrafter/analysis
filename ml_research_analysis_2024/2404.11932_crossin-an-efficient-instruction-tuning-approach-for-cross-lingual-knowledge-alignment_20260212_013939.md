---
ver: rpa2
title: 'CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge
  Alignment'
arxiv_id: '2404.11932'
source_url: https://arxiv.org/abs/2404.11932
tags:
- language
- languages
- cross-lingual
- multilingual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CrossIn, a method for cross-lingual instruction
  tuning that addresses the imbalance in multilingual training data for large language
  models (LLMs). CrossIn leverages a mixed composition of cross-lingual instruction
  tuning data, combining a Complex Task Dataset (CTD) rich in high-quality tasks with
  a Linguistic Uniformity Dataset (LUD) consisting of easier-to-translate tasks.
---

# CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment

## Quick Facts
- arXiv ID: 2404.11932
- Source URL: https://arxiv.org/abs/2404.11932
- Authors: Geyu Lin; Bin Wang; Zhengyuan Liu; Nancy F. Chen
- Reference count: 5
- Key outcome: Up to 40% relative gain in cross-lingual consistency and accuracy

## Executive Summary
CrossIn addresses the challenge of multilingual proficiency in large language models by introducing a mixed composition of cross-lingual instruction tuning data. The method combines Complex Task Dataset (CTD) with high-quality tasks and Linguistic Uniformity Dataset (LUD) with easier-to-translate tasks to enhance both task-solving capabilities and multilingual proficiency simultaneously. Experimental results show substantial improvements in performance across tasks and languages, with up to 40% relative gain in cross-lingual consistency and accuracy.

## Method Summary
CrossIn leverages a mixed composition of cross-lingual instruction tuning data to address the imbalance in multilingual training data for LLMs. The approach combines a Complex Task Dataset (CTD) rich in high-quality tasks with a Linguistic Uniformity Dataset (LUD) consisting of easier-to-translate tasks. This strategy allows for enhanced reasoning capabilities without compromising knowledge consistency across languages. The method involves fine-tuning with LoRA on models like Mistral-7B, Gemma-2B, and LLaMA-3-8B, using a multi-task and multi-faceted benchmark for evaluation.

## Key Results
- Up to 40% relative gain in cross-lingual consistency and accuracy
- Substantial improvements in performance across tasks and languages
- Effective bridging of the gap between different language capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual instruction tuning improves multilingual consistency by leveraging compressed representations shared across languages during instruction tuning.
- Mechanism: When models are trained with cross-lingual data, they learn to map similar concepts across languages into a shared representation space, enabling better generalization and consistency across languages.
- Core assumption: The compressed representation learned during pre-training is sufficiently robust to maintain cross-lingual alignment when fine-tuned with mixed-language instruction data.
- Evidence anchors:
  - [abstract] "Our method leverages the compressed representation shared by various languages to efficiently enhance the model's task-solving capabilities and multilingual proficiency within a single process."
  - [section] "In each iteration, LLMs are exposed to samples in several languages simultaneously, and the compressed representation encourages models to share linguistic features and generalize across different languages."
- Break condition: If the compressed representation is too shallow or the language pairs are too dissimilar, the cross-lingual alignment may fail, leading to degraded performance.

### Mechanism 2
- Claim: Using a mixed composition of cross-lingual instruction tuning data (CTD + LUD) balances task-solving capabilities and multilingual proficiency without compromising either.
- Mechanism: CTD provides complex, high-quality tasks that enhance reasoning capabilities, while LUD provides linguistically uniform tasks that are easier to translate and maintain knowledge consistency. The combination allows the model to learn both complex reasoning and cross-lingual alignment simultaneously.
- Core assumption: The two dataset types complement each other such that complex tasks from CTD don't overshadow the simpler, more consistent tasks from LUD, and vice versa.
- Evidence anchors:
  - [abstract] "CrossIn merges cross-lingual instruction data from CTD with machine-translated data from LUD to reinforce knowledge alignment during instruction tuning. This strategy allows us to enhance reasoning capabilities without compromising knowledge consistency across languages."
  - [section] "Tuning with CTD often leads to lower consistency due to model forgetting (Luo et al., 2024), while tuning with LUD may lead to lower reasoning capability due to its homogeneous nature."
- Break condition: If the ratio of CTD to LUD is imbalanced, the model may either lose consistency (too much CTD) or reasoning capability (too much LUD).

### Mechanism 3
- Claim: Cross-lingual instruction tuning with translation tasks improves knowledge transfer between languages more effectively than monolingual fine-tuning with translation data.
- Mechanism: By having the model generate outputs in different languages for the same instruction, it learns to map knowledge structures across languages directly, rather than learning to translate after the fact.
- Core assumption: Direct cross-lingual instruction generation creates stronger alignment between language representations than learning translation as a separate task after monolingual instruction tuning.
- Evidence anchors:
  - [abstract] "This strategy allows us to enhance reasoning capabilities without compromising knowledge consistency across languages."
  - [section] "We hypothesize that if the model concurrently learns these translation tasks, it could facilitate the transfer of knowledge between languages."
- Break condition: If the translation quality is poor or the language pairs are too dissimilar, the direct alignment may not form effectively, and the model may default to treating languages as separate modalities.

## Foundational Learning

- Concept: Compressed representation space in multilingual models
  - Why needed here: Understanding how languages share representations is crucial for grasping why cross-lingual instruction tuning works
  - Quick check question: What happens to the compressed representation when a model is fine-tuned only on English data after multilingual pre-training?

- Concept: Catastrophic forgetting in continual fine-tuning
  - Why needed here: The paper explicitly mentions that tuning with CTD can lead to lower consistency due to model forgetting
  - Quick check question: How does the CrossIn approach prevent catastrophic forgetting of cross-lingual knowledge?

- Concept: Cross-lingual consistency metrics (AC3, consistency score)
  - Why needed here: These metrics are central to evaluating the effectiveness of the CrossIn method
  - Quick check question: How does AC3 differ from simple accuracy, and why is this distinction important for multilingual evaluation?

## Architecture Onboarding

- Component map:
  Base model (e.g., Mistral-7B, Gemma-2B, LLaMA-3-8B) -> Instruction tuning datasets: CTD (Platypus) and LUD (Alpaca) -> CrossIn data generator (creates cross-lingual instruction pairs) -> Translation task generator (optional, for additional alignment) -> Evaluation framework (Cross-XQuAD, Cross-MMLU, Cross-LogiQA)

- Critical path:
  1. Load base model
  2. Prepare instruction tuning datasets (CTD + LUD)
  3. Generate CrossIn data (cross-lingual instruction pairs)
  4. Optionally generate translation tasks
  5. Fine-tune with LoRA using the combined dataset
  6. Evaluate on cross-lingual consistency benchmarks

- Design tradeoffs:
  - Using complex tasks (CTD) vs. simpler tasks (LUD) for different aspects of multilingual performance
  - Adding translation tasks vs. relying solely on cross-lingual instruction pairs
  - The number of cross-lingual samples to include (diminishing returns after ~5000 samples)

- Failure signatures:
  - Low consistency scores despite high accuracy (model learns to answer correctly but inconsistently across languages)
  - High consistency but low accuracy (model learns to give consistent but potentially incorrect answers)
  - No improvement over baseline (CrossIn data not effective for the specific base model or task type)

- First 3 experiments:
  1. Fine-tune the base model with only CTD (Platypus) and evaluate on all benchmarks to establish the "reasoning capability" baseline
  2. Fine-tune the base model with only LUD (Alpaca) and evaluate to establish the "consistency" baseline
  3. Fine-tune the base model with CrossInx2x and compare against both baselines to measure the improvement in balancing both aspects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal amount of cross-lingual instruction tuning data required to maximize the multilingual consistency and accuracy of LLMs?
- Basis in paper: [explicit] The paper discusses the influence of different quantities of cross-lingual alignment data on model performance, noting that a small amount of data can achieve satisfactory results.
- Why unresolved: While the paper provides insights into the effects of varying data quantities, it does not establish a definitive threshold for the optimal amount of data needed.
- What evidence would resolve it: Conducting further experiments with a wider range of data quantities and analyzing the performance metrics (accuracy, consistency, AC3) to identify the point of diminishing returns.

### Open Question 2
- Question: How does the integration of translation tasks affect the performance of cross-lingual instruction tuning in LLMs?
- Basis in paper: [explicit] The paper explores the efficacy of adding translation data alongside cross-lingual instruction tuning and finds that additional translation pairs do not bring performance gains.
- Why unresolved: The study suggests that translation tasks may not significantly enhance multilingual abilities, but it does not explore alternative methods of integrating translation or the potential benefits of more complex translation tasks.
- What evidence would resolve it: Designing and testing more sophisticated translation tasks or exploring different integration strategies to determine if they can enhance multilingual consistency and accuracy.

### Open Question 3
- Question: What are the implications of using cross-lingual instruction tuning on the pretraining stage of LLMs?
- Basis in paper: [inferred] The paper acknowledges the importance of pretraining in shaping the model's foundational knowledge but does not explore the impact of cross-lingual instruction tuning during this stage.
- Why unresolved: The paper focuses on instruction tuning but does not address how cross-lingual data formulation might affect pretraining efficiency and effectiveness.
- What evidence would resolve it: Conducting experiments to compare the performance of models trained with cross-lingual instruction tuning during pretraining versus traditional pretraining methods, analyzing changes in efficiency and effectiveness.

## Limitations

- Translation Quality Dependency: The CrossIn method's effectiveness heavily depends on the quality of machine translations used to create cross-lingual instruction pairs.
- Dataset Composition Sensitivity: The optimal ratio between CTD and LUD components is not explored, leaving open questions about whether the 2:1 ratio used is universally optimal across different base models or task domains.
- Base Model Limitations: The experiments focus on relatively small models (Mistral-7B, Gemma-2B, LLaMA-3-8B), limiting generalizability to the broader landscape of available LLMs.

## Confidence

**High Confidence Claims**:
- CrossIn improves multilingual consistency and task-solving capabilities simultaneously
- CrossIn outperforms monolingual fine-tuning approaches on cross-lingual benchmarks
- The mixed composition approach (CTD + LUD) provides better balance than using either dataset alone

**Medium Confidence Claims**:
- Cross-lingual instruction tuning leverages compressed representations for knowledge transfer
- CrossIn effectively bridges the gap between different language capabilities
- The approach is more reliable than alternative methods for improving multilingual performance

**Low Confidence Claims**:
- CrossIn is "more efficient" than alternative approaches (the paper doesn't provide runtime or resource usage comparisons)
- The 40% relative gain figure applies universally across all model sizes and language pairs
- CrossIn's benefits extend beyond the specific tasks and languages tested

## Next Checks

1. **Translation Quality Impact Study**: Conduct experiments varying the quality of translations used to create cross-lingual instruction pairs (e.g., using professional human translations vs. machine translations) to quantify how translation quality affects CrossIn's effectiveness.

2. **Cross-Lingual Generalization Test**: Evaluate CrossIn-tuned models on language pairs not present in the training data to test whether the method enables true cross-lingual knowledge transfer or merely improves performance within the specific language pairs seen during training.

3. **Efficiency Benchmarking**: Measure the computational resources (training time, memory usage, parameter count) required by CrossIn compared to alternative approaches like continued pre-training or sequential monolingual fine-tuning to validate the "efficient" claim in the paper's title.