---
ver: rpa2
title: Not All LLM Reasoners Are Created Equal
arxiv_id: '2410.01748'
source_url: https://arxiv.org/abs/2410.01748
tags:
- reasoning
- compositional
- arxiv
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the depth of reasoning capabilities of large
  language models (LLMs) on grade-school math (GSM) problems by evaluating their performance
  on compositional GSM tasks. Compositional GSM involves chaining two GSM problems
  together, where the answer to the first problem is used as a variable in the second.
---

# Not All LLM Reasoners Are Created Equal

## Quick Facts
- arXiv ID: 2410.01748
- Source URL: https://arxiv.org/abs/2410.01748
- Reference count: 29
- Primary result: LLM reasoning gaps between standard and compositional GSM tasks undermine model reliability

## Executive Summary
This paper investigates the depth of reasoning capabilities of large language models on grade-school math problems by evaluating their performance on compositional GSM tasks. The study benchmarks various open-weights and closed-source LLMs and finds that most models exhibit significant reasoning gaps between their performance on standard GSM8K and compositional GSM tasks. Smaller, more cost-efficient, and math-specialized models show larger reasoning gaps, reducing their practical utility. The research reveals that instruction-tuning impacts LLMs differently, finetuning can lead to task-specific overfitting, and code generation can be more effective than natural language for smaller models on compositional problems.

## Method Summary
The paper evaluates LLM reasoning depth by creating compositional GSM tasks through chaining two GSM problems together, where the answer to the first problem is used as a variable in the second. Using 8-shot prompting, the study tests various models (Gemini, Gemma2, LLAMA3, GPT, Phi, Qwen2.5, Mistral) on original GSM8K, modified GSM8K, and compositional GSM test sets with temperature 0 and pass@1 metric. The compositional reasoning gap score measures the difference between compositional GSM accuracy and expected accuracy based on individual question accuracy.

## Key Results
- Most models exhibit significant reasoning gaps between standard GSM8K and compositional GSM tasks
- Smaller, cost-efficient models show larger reasoning gaps, reducing their practical utility
- Instruction-tuning impacts smaller models more on standard GSM8K than compositional GSM
- Code generation is more effective than natural language for smaller models on compositional problems
- Finetuning on GSM8K can lead to task-specific overfitting, degrading performance on compositional tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller LLMs exhibit larger reasoning gaps due to limited capacity to handle multi-hop reasoning with additional context
- Mechanism: Smaller models struggle to maintain focus on the first question's answer while processing the second question, leading to distraction and errors in substitution
- Core assumption: Reasoning gaps are caused by inability to manage additional context rather than lack of mathematical understanding
- Evidence anchors: abstract gap findings, section on code generation benefits, GSM-Plus benchmark paper
- Break condition: If gaps are primarily due to test-set leakage or larger models show similar gaps

### Mechanism 2
- Claim: Instruction-tuning disproportionately impacts smaller LLMs, improving standard GSM8K but minimally compositional GSM
- Mechanism: Smaller LLMs are more sensitive to instruction-tuning but it optimizes for single-hop reasoning rather than multi-hop compositional thinking
- Core assumption: Instruction-tuning reinforces pattern matching for standard GSM8K without preparing for compositional reasoning
- Evidence anchors: abstract instruction-tuning findings, section on tuning impacts, GSM-Plus benchmark paper
- Break condition: If instruction-tuning improves compositional reasoning proportionally across all model sizes

### Mechanism 3
- Claim: Code generation is more effective than natural language for smaller LLMs due to structured format reducing cognitive load
- Mechanism: Code breaks down problems into executable steps, helping smaller models maintain logical flow between questions
- Core assumption: Code's structured format provides clearer framework for compositional reasoning compared to natural language
- Evidence anchors: abstract code generation findings, section on systematic differences, GSM-Infinite benchmark paper
- Break condition: If natural language proves equally or more effective for smaller models

## Foundational Learning

- Concept: Multi-hop reasoning
  - Why needed here: Understanding how models chain reasoning steps together is crucial for interpreting why compositional GSM tasks are more challenging than standard GSM8K problems
  - Quick check question: Can you explain the difference between single-hop and multi-hop reasoning in the context of math word problems?

- Concept: Context management in LLMs
  - Why needed here: The ability to handle and integrate additional context without being distracted is key to understanding the reasoning gaps observed in smaller models
  - Quick check question: How does the introduction of a second question in compositional GSM tasks affect the model's attention and reasoning process?

- Concept: Instruction-tuning effects on different model sizes
  - Why needed here: Recognizing how instruction-tuning impacts models of varying sizes helps explain the systematic differences in performance on compositional tasks
  - Quick check question: Why might instruction-tuning lead to larger improvements on standard GSM8K tasks compared to compositional GSM tasks, especially for smaller models?

## Architecture Onboarding

- Component map: LLM models -> Compositional GSM task generation -> Evaluation scripts (original GSM8K, modified GSM8K, compositional GSM) -> Analysis tools (reasoning gaps, performance differences)
- Critical path: 1) Generate compositional GSM tasks by chaining GSM8K questions 2) Evaluate models on original, modified, and compositional test sets 3) Analyze performance differences to identify reasoning gaps 4) Investigate factors contributing to gaps
- Design tradeoffs: Balancing model size and cost-efficiency against reasoning capabilities; choosing between natural language and code generation; deciding on extent of instruction-tuning and its impact on generalization
- Failure signatures: Large reasoning gaps between standard and compositional tasks; minimal improvement from instruction-tuning on compositional tasks for smaller models; overfitting to GSM8K after extensive fine-tuning; distraction by additional context
- First 3 experiments:
  1. Evaluate a small, cost-efficient model on compositional GSM tasks and compare its performance to standard GSM8K to measure the reasoning gap
  2. Test the impact of code generation versus natural language on a smaller model's performance on compositional GSM tasks
  3. Fine-tune a model on GSM8K with synthetic data and evaluate its performance on both standard and compositional test sets to assess for overfitting

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but the research raises several important areas for future investigation based on the observed limitations and unexpected findings.

## Limitations

- Focus on grade-school math problems may not generalize to more complex reasoning tasks
- Potential influence of test-set leakage despite authors' claims otherwise
- Compositional GSM generation relies on carefully constructed question pairs that may not capture all real-world reasoning scenarios
- Evaluation metrics primarily focus on final answer accuracy rather than intermediate reasoning steps

## Confidence

- High Confidence: Finding that most models show reasoning gaps between standard and compositional GSM tasks
- Medium Confidence: Conclusion about instruction-tuning impacts varying by model size
- Medium Confidence: Effectiveness of code generation for smaller models

## Next Checks

1. Cross-Domain Validation: Test the same compositional reasoning framework on other benchmark datasets (e.g., MMLU, BigBench) to assess whether observed reasoning gaps are specific to math problems or represent a more general limitation

2. Intermediate Step Analysis: Implement detailed analysis of intermediate reasoning steps in both natural language and code formats to identify exactly where smaller models fail in the compositional reasoning process

3. Fine-tuning Schedule Study: Conduct systematic study of different fine-tuning schedules and data mixtures to determine optimal training strategies that prevent overfitting while improving compositional reasoning capabilities