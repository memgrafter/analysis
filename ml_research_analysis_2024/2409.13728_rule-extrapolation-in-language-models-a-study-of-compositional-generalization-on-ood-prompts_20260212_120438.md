---
ver: rpa2
title: 'Rule Extrapolation in Language Models: A Study of Compositional Generalization
  on OOD Prompts'
arxiv_id: '2409.13728'
source_url: https://arxiv.org/abs/2409.13728
tags:
- rule
- language
- extrapolation
- should
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines how language models handle out-of-distribution\
  \ (OOD) prompts, focusing on a scenario called \"rule extrapolation\" where models\
  \ must complete prompts that violate one rule of a formal language while adhering\
  \ to others. The authors systematically compare five architectures\u2014linear models,\
  \ LSTMs, Transformers, Mamba (a state space model), and xLSTM\u2014across formal\
  \ languages with varying complexity, from regular to context-sensitive grammars."
---

# Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts

## Quick Facts
- **arXiv ID**: 2409.13728
- **Source URL**: https://arxiv.org/abs/2409.13728
- **Reference count**: 40
- **Primary result**: No single architecture dominates across all language types; Transformers excel on context-free/context-sensitive, LSTMs/Mamba on regular languages requiring parity

## Executive Summary
This paper systematically investigates rule extrapolation in language models, a scenario where models must complete prompts that violate one rule of a formal language while adhering to others. The authors compare five architectures (Linear, LSTM, Transformer, Mamba, xLSTM) across formal languages spanning the Chomsky hierarchy from regular to context-sensitive grammars. Through controlled experiments and analysis of training dynamics, they demonstrate that architectural biases significantly influence OOD generalization performance, with no single architecture universally superior. The work also proposes a normative framework inspired by Solomonoff induction for rational OOD prompt completion based on rule simplicity.

## Method Summary
The authors train five distinct architectures (Linear, LSTM, Transformer, Mamba, xLSTM) on formal languages generated according to specific grammar rules, with lengths up to 256 tokens. Each model is trained for 50,000 epochs using AdamW optimizer with inverse square root learning rate scheduling and batch size 128. They evaluate performance using test loss, in-distribution and out-of-distribution rule accuracy (R1, R2), and greedy decoding accuracy. OOD test prompts are specifically designed to violate one rule while preserving another, allowing systematic study of rule extrapolation. The study spans six formal languages (L1-L6) representing different complexity levels from regular to context-sensitive grammars.

## Key Results
- Transformers achieve the highest accuracy on context-free and context-sensitive languages (L3, L5, L6)
- LSTMs and Mamba architectures outperform others on regular languages requiring parity calculations (L1, L2)
- The xLSTM architecture shows intermediate performance between LSTMs and Transformers
- No single architecture dominates across all language types
- Training dynamics reveal a consistent simplicity bias, with models learning simpler rules before identifying the full language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers excel at rule extrapolation for context-free and context-sensitive grammars due to their ability to capture hierarchical structure and long-range dependencies.
- Mechanism: The self-attention mechanism in Transformers allows them to efficiently model global dependencies, which is crucial for understanding and extrapolating complex grammatical rules.
- Core assumption: The hierarchical nature of formal languages aligns well with the Transformer's architectural bias towards capturing such structures.
- Evidence anchors:
  - [abstract] "Transformers perform best on context-free and context-sensitive languages"
  - [section 4] "On L3, although all four models achieve perfect accuracy on (R2) both in- and out-of-distribution, and all models except the Linear, (near) perfectly obey (R1) in-distribution, the Transformer extrapolates (R1) to the largest extent (66%)"
  - [corpus] Weak evidence for this specific mechanism, but related work on compositional generalization in Transformers supports the general idea.
- Break condition: If the formal language lacks hierarchical structure or requires only local dependencies, the Transformer's advantage may diminish.

### Mechanism 2
- Claim: LSTMs and Mamba architectures perform better on regular languages due to their ability to efficiently compute parity.
- Mechanism: These architectures can maintain a hidden state that effectively tracks the parity of 'a' tokens, which is crucial for languages like L1 and L2.
- Core assumption: The ability to compute parity is a key factor in handling regular languages, and LSTMs/Mamba are better suited for this task than Transformers.
- Evidence anchors:
  - [abstract] "LSTMs and Mamba excel on regular languages that require parity calculations"
  - [section 4] "Perhaps surprisingly, modern architectures perform the worst on regular languages L1 and L2 : both Mamba and the Transformer are worse in- and out-of-distribution than the LSTM"
  - [corpus] Weak evidence for this specific mechanism, but Zhou et al. [2023] observed that Transformers struggle with addition or parity calculation.
- Break condition: If the regular language does not require parity computation or if the sequence length becomes too long for the LSTM/Mamba to maintain the hidden state effectively.

### Mechanism 3
- Claim: The xLSTM architecture, with its extended memory and gating mechanisms, provides a middle ground between LSTMs and Transformers.
- Mechanism: xLSTM combines the strengths of LSTMs (efficient parity computation) with some of the global modeling capabilities of Transformers, resulting in better performance on context-sensitive languages compared to standard LSTMs.
- Core assumption: The enhanced memory and gating mechanisms in xLSTM allow it to better capture complex grammatical rules while still maintaining some efficiency in parity computation.
- Evidence anchors:
  - [abstract] "The xLSTM generally lies somewhere between the LSTM and the Transformer"
  - [section 4] "For the context-sensitive Dyck language L6, the Transformer and LSTM perform similarly on both OOD (R1) and (R2)"
  - [corpus] Weak evidence for this specific mechanism, but the xLSTM architecture is designed to address some limitations of both LSTMs and Transformers.
- Break condition: If the language requires very long-range dependencies that neither LSTMs nor xLSTM can effectively model.

## Foundational Learning

- Concept: Formal languages and Chomsky hierarchy
  - Why needed here: Understanding the different types of formal languages (regular, context-free, context-sensitive) and their complexity is crucial for interpreting the results and the experimental setup.
  - Quick check question: Can you explain the difference between regular and context-free grammars using examples from the paper?

- Concept: Out-of-distribution (OOD) generalization
  - Why needed here: The paper focuses on OOD generalization, specifically rule extrapolation, where models are tested on prompts that violate one rule of a formal language while adhering to others.
  - Quick check question: How does rule extrapolation differ from standard OOD generalization, and why is it a useful concept for studying compositional generalization?

- Concept: Algorithmic Information Theory (AIT) and Solomonoff induction
  - Why needed here: The paper proposes a normative theory for OOD prompt completion inspired by Solomonoff induction, which is based on AIT principles.
  - Quick check question: How does the Solomonoff prior assign probabilities to sequences, and how does this relate to the concept of simplicity bias?

## Architecture Onboarding

- Component map:
  - Input token sequence -> Embedding layer -> Encoder/Decoder -> Output layer -> Next token prediction -> Loss calculation

- Critical path:
  - Input token sequence → Embedding layer → Encoder/Decoder → Output layer → Next token prediction → Loss calculation

- Design tradeoffs:
  - LSTM vs. Transformer: LSTMs are more efficient for parity computation but may struggle with long-range dependencies, while Transformers excel at capturing global structure but are less efficient for local computations.
  - Mamba vs. Transformer: Mamba replaces the attention mechanism with selective state spaces, potentially offering a middle ground between efficiency and global modeling capabilities.
  - xLSTM vs. LSTM: xLSTM extends the memory and gating mechanisms of LSTMs, potentially improving performance on complex languages at the cost of increased computational complexity.

- Failure signatures:
  - Transformer: Struggles with regular languages requiring parity computation (e.g., L1, L2)
  - LSTM/Mamba: May fail on context-free or context-sensitive languages with complex hierarchical structures
  - xLSTM: Performance may degrade if the language requires very long-range dependencies or if the sequence length exceeds the model's capacity

- First 3 experiments:
  1. Train and evaluate each architecture on the regular language L1, focusing on their ability to extrapolate rule R1 when rule R2 is violated.
  2. Train and evaluate each architecture on the context-free language L3, comparing their performance on extrapolating rule R1.
  3. Train and evaluate each architecture on the context-sensitive language L5, analyzing their ability to handle more complex grammatical rules and extrapolate rule R1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed simplicity bias in Transformer training dynamics (learning simpler rules first) persist across different hyperparameter settings and architectures?
- Basis in paper: [explicit] The authors note that their findings about training dynamics showing simplicity bias were observed specifically for Transformers on anbn language, and they tested multiple hyperparameter settings for Transformers but found consistent behavior
- Why unresolved: The authors only tested a limited set of hyperparameters and did not systematically explore how different architectures (LSTM, Mamba, xLSTM) behave under varying training conditions
- What evidence would resolve it: Comprehensive experiments varying learning rates, optimizer types, model sizes, and training durations across all architectures, comparing whether the same rule-learning ordering pattern emerges consistently

### Open Question 2
- Question: Can the proposed normative theory based on Solomonoff induction be effectively approximated in practical architectures to improve rule extrapolation performance?
- Basis in paper: [explicit] The authors state "We conjecture that approximating our normative algorithm similarly to the approach of Grau-Moya et al. [2024], will result in models with superior rule extrapolation properties. We leave this promising direction to future work."
- Why unresolved: The authors explicitly leave this as future work and do not provide any empirical validation of their theoretical framework
- What evidence would resolve it: Implementation of practical approximations of the proposed Solomonoff-inspired prior in actual language models, followed by systematic evaluation of rule extrapolation performance compared to standard architectures

### Open Question 3
- Question: Why do Transformers struggle specifically with regular languages requiring parity calculations, while performing well on context-free and context-sensitive grammars?
- Basis in paper: [explicit] The authors observe that "Transformers fare very well in most scenarios we investigated, they struggled on regular languages" and note that "these languages require calculating parity, in which the Transformer struggles"
- Why unresolved: The authors identify the phenomenon but do not provide a mechanistic explanation for why the Transformer architecture is particularly ill-suited for parity calculations despite its general success
- What evidence would resolve it: Detailed mechanistic interpretability analysis identifying specific architectural limitations or attention patterns that prevent effective parity computation in Transformers, potentially comparing with architectures that handle these tasks well

### Open Question 4
- Question: How does the rule extrapolation ability of language models scale with the number of rules and complexity of the formal language?
- Basis in paper: [inferred] The authors only test languages with exactly two rules and do not explore scenarios with more complex rule combinations or deeper hierarchies of language complexity
- Why unresolved: The experimental design limits investigation to binary rule intersections, leaving open questions about performance in more complex compositional scenarios
- What evidence would resolve it: Systematic experiments varying the number of rules (3, 4, or more) and testing languages with different types of rule interactions (nested dependencies, longer-range dependencies) across all architectures to map the scaling behavior of rule extrapolation ability

## Limitations

- The paper provides limited implementation details for Mamba and xLSTM architectures, which could impact reproducibility
- Training dynamics and convergence behavior across architectures are not fully explored, particularly for newer models
- Results from formal language experiments may not directly generalize to natural language tasks

## Confidence

**High Confidence Claims:**
- Transformers show superior performance on context-free and context-sensitive languages (based on direct experimental results)
- No single architecture dominates across all language types (clearly demonstrated in results)
- Simplicity bias is present in practical architectures (supported by training dynamics analysis)

**Medium Confidence Claims:**
- LSTMs and Mamba's advantage on regular languages stems from parity computation efficiency (mechanism plausible but not definitively proven)
- The proposed normative theory based on Solomonoff induction provides a useful framework for OOD generalization (conceptually sound but limited empirical validation)

**Low Confidence Claims:**
- The exact reasons for xLSTM's intermediate performance are well understood (insufficient analysis of this new architecture)
- The relationship between formal language complexity and natural language compositional generalization (speculative extension)

## Next Checks

1. **Architectural Ablation Study**: Systematically remove attention mechanisms from Transformers and state-space components from Mamba to isolate which architectural features contribute to performance differences on regular vs. complex languages.

2. **Training Dynamics Analysis**: Conduct a detailed study of loss landscapes and learning curves across all architectures, focusing on how each model converges to different rules and whether simpler rules are consistently learned first.

3. **Cross-Linguistic Validation**: Test the same architectures on a broader range of formal languages, including more complex context-sensitive languages and languages with different structural properties, to validate whether the observed architectural preferences generalize.