---
ver: rpa2
title: 'LMLPA: Language Model Linguistic Personality Assessment'
arxiv_id: '2410.17632'
source_url: https://arxiv.org/abs/2410.17632
tags:
- personality
- llms
- extent
- what
- traits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Language Model Linguistic Personality
  Assessment (LMLPA), a novel framework for evaluating the linguistic personalities
  of Large Language Models (LLMs). Unlike traditional human-centric psychometrics,
  LMLPA adapts the Big Five Inventory (BFI) into an open-ended questionnaire format
  and employs AI raters to transform textual responses into numerical personality
  scores.
---

# LMLPA: Language Model Linguistic Personality Assessment

## Quick Facts
- arXiv ID: 2410.17632
- Source URL: https://arxiv.org/abs/2410.17632
- Reference count: 23
- Primary result: Language Model Linguistic Personality Assessment (LMLPA) framework effectively measures LLM linguistic personalities using adapted Big Five Inventory with open-ended questions and AI raters

## Executive Summary
This paper introduces LMLPA, a novel framework for evaluating the linguistic personalities of Large Language Models (LLMs). The system adapts the Big Five Inventory (BFI) into an open-ended questionnaire format to mitigate LLMs' sensitivity to multiple-choice question order. AI raters are employed to transform textual responses into numerical personality scores, providing a robust benchmark for future research in human-AI interaction and personality assessment. The framework demonstrates high reliability and validity through statistical testing, successfully capturing distinct personality traits in LLMs.

## Method Summary
The LMLPA framework adapts the 44-item Big Five Inventory (BFI) into open-ended questions suitable for LLMs, replacing fixed multiple-choice responses with frequency adverbs (always, often, sometimes, rarely, never). AI raters (GPT-4-Turbo and Llama3-8B-Instruct) evaluate LLM responses and assign numerical personality scores. The system validates its structure through reliability tests (Cronbach's alpha, ICC), validity tests (PCA with KMO > 0.9, Bartlett's Test p < 0.001), and consistency checks (Cohen's Weighted Kappa). The methodology involves generating 250 unique responses from GPT-4-Turbo using distinct system prompts, then conducting comprehensive statistical analysis to verify the questionnaire captures meaningful personality dimensions.

## Key Results
- Reverse experiments show higher consistency (Cohen's Kappa 0.877) compared to traditional BFI questionnaires (0.401)
- Cronbach's alpha values exceed 0.8 for all personality dimensions, indicating strong internal consistency
- PCA validation reveals interpretable factors corresponding to Big Five personality traits with KMO > 0.9 and Bartlett's Test p < 0.001
- GPT-4-Turbo demonstrates high inter-rater reliability with ICC of 0.829 for single measures and 0.951 for average measures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-ended questionnaires reduce LLM sensitivity to option order
- Mechanism: Replacing fixed multiple-choice responses with open-ended answers using frequency adverbs avoids fixed option order bias
- Core assumption: LLMs are more robust to semantic interpretation of frequency adverbs than ordinal selection in MCQs
- Evidence anchors: Abstract confirms open-ended design mitigates option order sensitivity; Section 3.2 describes reverse experiment methodology
- Break condition: If LLMs still show order sensitivity in frequency adverb list or interpretation is inconsistent

### Mechanism 2
- Claim: AI raters provide consistent and objective personality scoring compared to human raters
- Mechanism: AI raters evaluate open-ended LLM responses and assign numerical personality scores, eliminating human bias
- Core assumption: AI raters can interpret nuanced open-ended responses and map them reliably to Big Five personality trait scores
- Evidence anchors: Section 4.2.2 shows GPT-4-Turbo ICC of 0.829 (single) and 0.951 (average); abstract confirms distinct personality traits can be quantified
- Break condition: If AI rater scores diverge significantly from human expert ratings or show low inter-rater reliability

### Mechanism 3
- Claim: PCA validates that the questionnaire captures distinct personality dimensions in LLM linguistic output
- Mechanism: Principal Component Analysis on LLM response data reveals interpretable factors corresponding to Big Five personality traits
- Core assumption: LLM linguistic patterns align with human personality trait structures and can be detected via PCA
- Evidence anchors: Abstract confirms PCA validates distinct personality traits; Section 5.2.2 shows unrotated PCA on 250 responses
- Break condition: If PCA does not yield interpretable factors or factors don't align with intended personality dimensions

## Foundational Learning

- Concept: Big Five personality model
  - Why needed here: The system adapts the Big Five Inventory (BFI) framework to measure LLM linguistic personalities
  - Quick check question: What are the five dimensions of the Big Five model and their general meanings?

- Concept: Reliability and validity testing (Cronbach's alpha, PCA, ICC)
  - Why needed here: These statistical methods validate that the questionnaire consistently measures personality traits and that extracted factors are meaningful
  - Quick check question: What does a Cronbach's alpha value above 0.8 indicate about a questionnaire?

- Concept: Zero-shot vs fine-tuned prompting for AI raters
  - Why needed here: The system uses zero-shot prompting for AI raters; understanding the difference is key for future improvements
  - Quick check question: What is the main difference between zero-shot and few-shot prompting in LLMs?

## Architecture Onboarding

- Component map:
  - Adapted-BFI questionnaire -> LLM responses -> AI Rater (GPT-4-Turbo/Llama3-8B-Instruct) -> Numerical personality scores -> PCA & Reliability Engine -> Validation metrics

- Critical path:
  1. Generate LLM responses to Adapted-BFI questions
  2. Feed responses to AI rater for scoring
  3. Aggregate scores across personality dimensions
  4. Validate structure via reliability and validity tests

- Design tradeoffs:
  - Open-ended vs MCQ: Reduces order bias but requires AI rater, increasing complexity
  - Single AI rater vs multiple raters: Simpler but may miss diverse interpretations
  - PCA-based validation vs confirmatory factor analysis: Exploratory but may conflate traits

- Failure signatures:
  - High inconsistency in reverse experiments (Cohen's Kappa << 0.8)
  - Low Cronbach's alpha (< 0.7) for any personality dimension
  - PCA yields fewer interpretable factors than expected
  - AI rater scores diverge significantly from human expert ratings

- First 3 experiments:
  1. Reverse experiment: Reverse frequency adverb order in prompts and measure consistency (Cohen's Kappa)
  2. AI rater validation: Compare AI rater scores against human expert ratings using ICC
  3. PCA validation: Run PCA on LLM responses and check for interpretable Big Five factors

## Open Questions the Paper Calls Out
None

## Limitations

- The validation against human raters uses a small sample size, limiting generalizability of AI rater objectivity claims
- The framework hasn't been tested across diverse LLM architectures, raising questions about generalizability beyond GPT-4-Turbo and Llama3-8B-Instruct
- Cross-validation and test-retest reliability metrics are not reported, leaving uncertainty about temporal stability and model transferability

## Confidence

- **High**: The reverse experiment methodology and statistical testing framework (Cronbach's alpha, ICC, PCA) are sound and well-documented
- **Medium**: The core claim that open-ended questionnaires reduce LLM option-order bias is supported by experimental evidence, but the mechanism could be more thoroughly tested
- **Medium**: The claim that AI raters can objectively score LLM personality traits is supported by high ICC values, but the validation against human raters is limited to a small sample size

## Next Checks

1. **Cross-Model Validation**: Test the LMLPA framework across diverse LLM architectures (Claude, Gemini, open-source models) to assess generalizability of the personality measurement beyond GPT-4-Turbo and Llama3-8B-Instruct.

2. **Human-AI Rater Comparison**: Conduct a large-scale study comparing AI rater scores against multiple human expert panels using established personality assessment protocols to validate the AI rater's objectivity and consistency.

3. **Longitudinal Stability Test**: Measure personality scores from the same LLM under identical conditions across multiple time points to assess test-retest reliability and temporal stability of the LMLPA measurements.