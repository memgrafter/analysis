---
ver: rpa2
title: Analyzing Quality, Bias, and Performance in Text-to-Image Generative Models
arxiv_id: '2407.00138'
source_url: https://arxiv.org/abs/2407.00138
tags:
- person
- image
- images
- face
- text-to-image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study quantitatively evaluates text-to-image models (Stable
  Diffusion, DALL-E Mini, LAFITE) using FID and R-Precision scores across COCO and
  Flickr30k datasets. Stable Diffusion consistently achieves the highest R-Precision
  scores and lower FID scores, indicating superior image quality and alignment with
  text prompts.
---

# Analyzing Quality, Bias, and Performance in Text-to-Image Generative Models

## Quick Facts
- **arXiv ID**: 2407.00138
- **Source URL**: https://arxiv.org/abs/2407.00138
- **Reference count**: 40
- **Primary result**: Stable Diffusion achieves highest R-Precision scores and lowest FID scores across COCO and Flickr30k datasets

## Executive Summary
This study provides a quantitative evaluation of three text-to-image generative models (Stable Diffusion, DALL-E Mini, LAFITE) using FID and R-Precision scores across COCO and Flickr30k datasets. The analysis reveals that Stable Diffusion consistently outperforms other models in both image quality and alignment with text prompts. The research also uncovers significant gender and racial biases in model outputs, with DALL-E Mini showing strong male preference even in female-biased scenarios, while Stable Diffusion displays bias toward white males in professional contexts. Face generation remains particularly challenging, with LAFITE producing unclear facial images and DALL-E Mini categorizing many images as "uncertain" for racial characteristics.

## Method Summary
The study evaluates text-to-image models using two datasets: COCO (10,000 faces, 10,000 motion images) and Flickr30k (10,000 faces, 5,000 motion images). Images are generated using each model with corresponding captions as prompts. FID scores measure image quality by comparing generated and real images, while R-Precision scores assess text-image alignment using DAMSM's pre-trained encoders. Bias analysis involves human evaluation of 88 gender prompts and 88 race prompts, categorizing outputs into predefined demographic groups. Face detection is performed using MTCNN, and images with insufficient facial details are excluded from analysis.

## Key Results
- Stable Diffusion achieves the highest R-Precision scores and lowest FID scores across both COCO and Flickr30k datasets
- DALL-E Mini exhibits gender bias by predominantly generating male figures even with female-biased scenarios
- LAFITE G produces unclear facial images and shows mixed performance in motion image generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stable Diffusion consistently achieves the highest R-Precision scores and lower FID scores across both COCO and Flickr30k datasets.
- Mechanism: Stable Diffusion uses a sequential transformation process that iteratively refines images, leading to higher alignment with text prompts and better visual fidelity compared to other models.
- Core assumption: The iterative refinement process in Stable Diffusion's architecture inherently produces more accurate and higher-quality images when evaluated against both R-Precision and FID metrics.
- Evidence anchors:
  - [abstract] "Stable Diffusion consistently achieves the highest R-Precision scores and lower FID scores, indicating superior image quality and alignment with text prompts."
  - [section] "Stable Diffusion’s sequential transformation process enhances image synthesis, Dall-E Mini benefits from the transformer architecture, and LAFITE G’s integration of a language model alongside StyleGAN2 leads to fluctuating outcomes."
  - [corpus] Strong evidence from multiple related papers on Stable Diffusion's architecture and performance advantages.
- Break condition: If the iterative refinement process is compromised or if the evaluation metrics are not representative of actual image quality and prompt alignment.

### Mechanism 2
- Claim: DALL-E Mini exhibits gender bias by predominantly generating male figures even with female-biased scenarios.
- Mechanism: The training data or model architecture of DALL-E Mini encodes gender stereotypes, leading to biased image generation when prompted with gender-neutral or female-biased terms.
- Core assumption: The model's training corpus contains implicit gender biases that are learned and reproduced during image generation.
- Evidence anchors:
  - [abstract] "DALL-E Mini predominantly generates male figures even with female-biased scenarios, while Stable Diffusion shows strong male and white individual preference in professional contexts."
  - [section] "Even when employing gender-neutral terms like 'person' or 'human,' Dall-E Mini tended to generate predominantly male figures, even in scenarios with a female bias."
  - [corpus] Moderate evidence from related papers discussing gender bias in text-to-image models, but specific evidence for DALL-E Mini's bias is limited.
- Break condition: If the training data is debiased or if the model architecture is modified to mitigate gender bias.

### Mechanism 3
- Claim: LAFITE G produces unclear facial images and shows mixed performance in motion image generation.
- Mechanism: The integration of a language model with StyleGAN2 in LAFITE G may not be optimized for generating clear facial features or consistent motion images, leading to variable quality.
- Core assumption: The combination of language modeling and generative adversarial networks in LAFITE G introduces complexities that affect the clarity and consistency of generated images.
- Evidence anchors:
  - [abstract] "LAFITE produces unclear facial images" and "LAFITE G displays weaker performance in the Face category and mixed results in Motion."
  - [section] "LAFITE G’s integration of a language model alongside StyleGAN2 leads to fluctuating outcomes."
  - [corpus] Limited evidence from related papers specifically addressing LAFITE G's performance issues; more research needed.
- Break condition: If the model architecture is refined or if the training process is adjusted to improve facial and motion image generation.

## Foundational Learning

- Concept: Text-to-image generative models
  - Why needed here: Understanding the architecture and functioning of text-to-image models is crucial to analyze their performance and biases.
  - Quick check question: What are the key components of a text-to-image generative model, and how do they contribute to image generation?

- Concept: Evaluation metrics (FID and R-Precision)
  - Why needed here: FID and R-Precision are used to quantitatively assess the quality and alignment of generated images with text prompts.
  - Quick check question: How do FID and R-Precision scores differ in evaluating text-to-image models, and what aspects of image generation do they measure?

- Concept: Bias detection and mitigation
  - Why needed here: Identifying and addressing biases in generated images is essential for ethical and fair use of text-to-image models.
  - Quick check question: What methods are used to detect and mitigate biases in text-to-image models, and how effective are they?

## Architecture Onboarding

- Component map: Text input → Text encoding → Image generation → Quality evaluation (FID/R-Precision) → Bias analysis → Output
- Critical path: Text input → Text encoding → Image generation → Quality evaluation (FID/R-Precision) → Bias analysis → Output
- Design tradeoffs: Balancing image quality with computational efficiency, addressing biases without compromising model performance, and choosing appropriate evaluation metrics
- Failure signatures: High FID scores indicating poor image quality, low R-Precision scores suggesting misalignment with text prompts, and biased outputs revealing social stereotypes
- First 3 experiments:
  1. Generate images using different models (Stable Diffusion, DALL-E Mini, LAFITE) with the same text prompts and compare FID and R-Precision scores
  2. Analyze the gender and racial biases in generated images by categorizing outputs and calculating representation percentages
  3. Modify prompts to test model responses to gender-neutral and race-neutral terms, observing any changes in bias patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different text-to-image models perform when generating human faces across diverse racial and ethnic groups?
- Basis in paper: [explicit] The paper highlights that face generation remains challenging due to diversity complexity, and that all face images produced by the LAFITE model were unclear, while a notable proportion of images generated by Dall-E Mini falls under the 'uncertain' category regarding racial characteristics.
- Why unresolved: The paper mentions the challenges in face generation but does not provide a detailed analysis of how different models perform across various racial and ethnic groups. The complexity of facial diversity impacts image quality, but specific performance metrics for different groups are not discussed.
- What evidence would resolve it: A comprehensive study comparing the performance of different text-to-image models in generating human faces across various racial and ethnic groups, using diverse datasets and detailed performance metrics.

### Open Question 2
- Question: What are the specific biases present in text-to-image models when generating images for professional contexts, and how do these biases vary across different models?
- Basis in paper: [explicit] The paper documents that Stable Diffusion and Dall-E Mini models display a pronounced inclination towards males and white individuals in professional contexts, such as "CEO" and "manager" prompts predominantly producing images biased toward white men.
- Why unresolved: While the paper identifies the presence of gender and racial biases in professional contexts, it does not delve into the specific biases present in each model or how these biases vary across different professional roles and scenarios.
- What evidence would resolve it: An in-depth analysis of the biases present in each text-to-image model when generating images for various professional contexts, including a comparison of biases across different models and professional roles.

### Open Question 3
- Question: How do the architectural differences between text-to-image models influence their performance in generating motion-based images and their susceptibility to biases?
- Basis in paper: [explicit] The paper notes that Stable Diffusion consistently achieves the highest R-Precision scores across all datasets and categories, while LAFITE G displays weaker performance in the Face category and mixed results in Motion. It also mentions that the performance in the Motion category displays more variability.
- Why unresolved: The paper highlights differences in performance between models but does not explore how specific architectural features contribute to these differences or their impact on bias susceptibility.
- What evidence would resolve it: A comparative study examining the architectural features of different text-to-image models and their influence on motion image generation performance and bias susceptibility, potentially through ablation studies or controlled experiments.

## Limitations

- The analysis relies on static datasets (COCO and Flickr30k) that may not represent current real-world diversity
- Human evaluation introduces subjectivity in bias assessment, with 19% of images categorized as "Uncertain" due to insufficient detail
- The analysis focuses on surface-level demographic representation without exploring deeper intersectional biases or cultural contexts

## Confidence

- **High Confidence**: Stable Diffusion's superior performance on FID and R-Precision metrics across both datasets is well-supported by quantitative evidence
- **Medium Confidence**: The documented gender and racial biases in DALL-E Mini and Stable Diffusion are supported by human evaluation, though the methodology's subjectivity introduces some uncertainty
- **Medium Confidence**: The performance differences between models on face and motion image generation are evidenced, but the analysis could benefit from more granular breakdown by image type and prompt complexity

## Next Checks

1. Conduct cross-validation using additional diverse datasets beyond COCO and Flickr30k to verify whether performance patterns hold across different image domains and cultural contexts
2. Implement automated bias detection tools (e.g., demographic classification models) alongside human evaluation to reduce subjectivity and quantify "Uncertain" classifications
3. Perform ablation studies on model architectures to isolate which components contribute most to performance differences and bias patterns, particularly for LAFITE's mixed results