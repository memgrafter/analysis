---
ver: rpa2
title: Disentangling shared and private latent factors in multimodal Variational Autoencoders
arxiv_id: '2403.06338'
source_url: https://arxiv.org/abs/2403.06338
tags:
- latent
- data
- shared
- multimodal
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the capability of multimodal Variational
  Autoencoders (VAEs) to disentangle shared and private latent factors in multi-omics
  data. It addresses the challenge of identifying latent factors that explain variation
  across modalities (shared) versus those specific to a single modality (private),
  particularly in scenarios where modality-specific variation dominates the shared
  signal.
---

# Disentangling shared and private latent factors in multimodal Variational Autoencoders

## Quick Facts
- arXiv ID: 2403.06338
- Source URL: https://arxiv.org/abs/2403.06338
- Reference count: 40
- Key outcome: MMV AE++ modification significantly improves shared latent factor learning in multimodal VAEs by restricting shared latent updates to cross-modal terms, achieving higher cross-modal prediction R² values than baselines across multiple real-world multi-omics datasets

## Executive Summary
This paper addresses the challenge of identifying shared versus private latent factors in multimodal Variational Autoencoders (VAEs) when modality-specific variation dominates the shared signal. The authors propose MMV AE++, a modification to the Mixture-of-Product-of-Experts (MoPoE) VAE that restricts shared latent space updates to only cross-modal terms using a stop-gradient operator. This prevents private variation from leaking into the shared space while maintaining the ability to learn meaningful shared structure. Through experiments on synthetic data and real multi-omics datasets (CLL, TCGA Breast Cancer, single-cell RNA-seq/ATAC-seq), MMV AE++ consistently outperforms existing methods like MV AE and MMV AE in cross-modal prediction tasks, demonstrating superior capability to disentangle shared and private latent factors.

## Method Summary
The method builds on multimodal VAE architectures (MVAE, MMVAE, MoPoE-VAE) with a structured latent space partitioned into private components for each modality (z_pr1, z_pr2) and a shared component (z_shared). The key innovation is MMV AE++, which modifies the training objective by applying a stop-gradient operator to z_shared during same-view predictions, allowing updates only from cross-modal reconstruction terms. This forces z_shared to encode only information useful for predicting other modalities. The approach is evaluated through cross-modal prediction R² scores and supervised classification AUC, with additional experiments incorporating label information through weighted loss terms.

## Key Results
- MMV AE++ achieves significantly higher R² values for cross-modal prediction of shared features compared to MV AE and MMV AE across multiple real-world datasets
- On the CLL dataset, MMV AE++ maintains cross-modal prediction R² of 0.1 even with 1000 "other" genes, while unsupervised methods fail when private variation dominates
- When incorporating supervised labels, MMV AE++ achieves AUC comparable to fully supervised models while retaining the ability to learn shared structure from unsupervised data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MMV AE++ modification improves cross-modal prediction by restricting shared latent updates to cross-modal terms, preventing private variation from leaking into the shared space.
- Mechanism: In the MMV AE++ objective, the gradient updates to zshared are restricted to cross-modal terms using a stop-gradient operator on zshared for same-view predictions. This forces zshared to only encode information useful for predicting other modalities, rather than absorbing private variation to improve same-view reconstruction.
- Core assumption: The dominant private variation in modalities creates a stronger signal for same-view reconstruction than the weaker shared signal, causing standard models to prioritize private over shared variation.
- Evidence anchors:
  - [abstract] "We propose a modification called MMV AE++ that is significantly more robust in its ability to infer shared latent factors."
  - [section] "In order to enforce this, we propose a modification to the MMV AE objective, where the gradient updates via zshared are restricted to only cross-modal terms."
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If the shared signal becomes even weaker relative to private variation, or if the stop-gradient operator prevents sufficient learning of shared structure.

### Mechanism 2
- Claim: The MMV AE objective's explicit cross-modal reconstruction terms create an inductive bias toward learning shared latent factors compared to MV AE.
- Mechanism: The MMV AE objective includes terms that directly encourage cross-modal prediction (Equation 3), whereas MV AE only optimizes within-modality reconstruction. This explicit cross-modal objective forces MMV AE to prioritize shared variation over private variation.
- Core assumption: Cross-modal prediction can only be achieved through shared latent factors, so optimizing for cross-modal reconstruction inherently promotes learning shared structure.
- Evidence anchors:
  - [section] "We hypothesise that this is an important distinction which may lead to MMV AE learning latent representations that are more amenable to cross-modal prediction, i.e. being potentially biased towards learning shared latent factors, more so than MV AE."
  - [section] "The fact that the last two cross-modal terms exist in MoPoE-V AE but not in MV AE, helps to shed light into the expected behaviours of MV AE and MoPoE-V AE, specifically on why we would expect MoPoE-V AE to be better suited for cross-modal reconstruction than MV AE."
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If cross-modal prediction becomes impossible due to lack of shared variation, or if the cross-modal terms dominate learning too strongly.

### Mechanism 3
- Claim: The mixture-of-experts (MoE) posterior in MMV AE creates a more flexible inference model than product-of-experts (PoE) in MV AE, better handling imbalanced modalities.
- Mechanism: MoE allows any single expert to have high posterior density, while PoE requires all experts to have high density. This makes MoE more robust when one modality has much higher dimensionality (more private variation) than the other.
- Core assumption: The modality with higher dimensionality will dominate the shared latent space in PoE due to its stronger signal, but MoE can better balance the influence of both modalities.
- Evidence anchors:
  - [section] "PoE will have high posterior density only as long as all experts have high posterior density, whereas in MoE it is sufficient that there exists at least one expert that has assigned high posterior density to this event."
  - [section] "We observe that both MoPoE-V AE and MMV AE consistently outperform MV AE" in synthetic experiments.
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If both modalities have similar dimensionality and variation, or if the MoE flexibility leads to overfitting.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs) and their multimodal extensions
  - Why needed here: The paper builds on multimodal VAE architectures (MVAE, MMVAE, MoPoE-VAE) and proposes a modification (MMVAE++), so understanding VAEs is fundamental.
  - Quick check question: What is the key difference between the encoder design in standard VAEs versus multimodal VAEs like MVAE and MMVAE?

- Concept: Cross-modal prediction and its relationship to shared latent factors
  - Why needed here: The paper uses cross-modal prediction performance (R²) as the primary metric for evaluating how well models learn shared latent factors, making this concept central to the methodology.
  - Quick check question: Why can cross-modal prediction only be achieved through shared latent factors in multimodal VAEs?

- Concept: Product-of-Experts (PoE) vs Mixture-of-Experts (MoE) in multimodal inference
  - Why needed here: The paper explicitly contrasts these two approaches and their inductive biases, showing how they affect the ability to learn shared versus private variation.
  - Quick check question: What is the key mathematical difference between PoE and MoE that leads to different behaviors in multimodal VAEs?

## Architecture Onboarding

- Component map: Data → modality-specific encoders q(z|x_m) → shared latent space z = [z_pr1, z_shared, z_pr2] → modality-specific decoders p(x|z) → reconstruction loss + KL divergence
- Critical path: The critical innovation is the modified training objective that only allows z_shared to be updated by cross-modal reconstruction terms through a stop-gradient operation.
- Design tradeoffs: The stop-gradient constraint improves shared factor learning but may limit same-view reconstruction quality. The approach trades some reconstruction accuracy for better interpretability of shared structure.
- Failure signatures: If R² values for shared features don't improve with MMVAE++ or if same-view prediction quality degrades significantly, the modification may be too restrictive. If cross-modal prediction remains poor, the shared signal may be too weak relative to private variation.
- First 3 experiments:
  1. Implement MMVAE++ and compare cross-modal prediction R² on synthetic data with varying ratios of shared to private features.
  2. Apply to a simple multi-omics dataset (like the CLL example) with a known biomarker to validate the approach.
  3. Test the impact of the stop-gradient constraint by comparing MMVAE++ with and without this modification on cross-modal prediction performance.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the discussion section highlights several areas for future work including theoretical analysis of the proposed method, exploration of alternative approaches to disentangle shared and private factors, and investigation of how these methods perform on larger-scale multi-omics datasets with more than two modalities.

## Limitations

- The analysis is primarily empirical with limited theoretical grounding for why the stop-gradient modification specifically improves shared factor learning
- The synthetic data experiments, while providing controlled conditions, may not fully capture the complexity of real multi-omics data
- The feature selection method for creating shared vs private feature sets relies on external biomarkers/labels, which may introduce bias

## Confidence

- High confidence: The comparative performance of MMV AE++ versus baselines on cross-modal prediction tasks is well-supported by experimental results across multiple datasets
- Medium confidence: The mechanism explanation for why the stop-gradient modification works is plausible but not definitively proven - it's an inference based on observed behavior
- Medium confidence: The claim that MoPoE-VAE is more robust to imbalanced modalities than MVAE is supported by synthetic experiments but could benefit from more extensive testing on real imbalanced datasets

## Next Checks

1. Conduct ablation studies removing the stop-gradient constraint to quantify its specific contribution to performance improvements
2. Test the method on additional multi-omics datasets with known shared structures to validate generalizability
3. Implement theoretical analysis of the modified objective function to formally prove convergence properties and inductive biases