---
ver: rpa2
title: Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging
arxiv_id: '2412.19512'
source_url: https://arxiv.org/abs/2412.19512
tags:
- safety
- merging
- tasks
- aligned
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Fine-tuning large language models for downstream tasks often degrades
  their safety alignment, a problem exacerbated by catastrophic forgetting. To address
  this without requiring additional safety data, we propose a simple method: merging
  the weights of pre- and post-fine-tuned models.'
---

# Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging

## Quick Facts
- arXiv ID: 2412.19512
- Source URL: https://arxiv.org/abs/2412.19512
- Authors: Hua Farn; Hsuan Su; Shachi H Kumar; Saurav Sahay; Shang-Tse Chen; Hung-yi Lee
- Reference count: 24
- One-line primary result: Merging pre- and post-fine-tuned model weights effectively mitigates safety degradation while improving downstream task performance.

## Executive Summary
Fine-tuning large language models for downstream tasks often degrades their safety alignment due to catastrophic forgetting. This paper proposes a simple yet effective solution: merging the weights of pre- and post-fine-tuned models through linear interpolation. The approach preserves safety while maintaining or improving task performance across multiple model families and benchmarks. The method is particularly valuable as it requires no additional safety data and can be applied to existing fine-tuned models.

## Method Summary
The method involves fine-tuning an aligned base model on a downstream task using LoRA with specified hyperparameters (r=8, α=16, lr=1e-4, 3 epochs), then creating merged models by linearly interpolating between the aligned model weights and the fine-tuned model weights using an interpolation coefficient λ. The merged models are evaluated on both downstream task performance and safety metrics, with λ tuned to find the optimal trade-off between task performance and safety preservation.

## Key Results
- Merging pre- and post-fine-tuned model weights reduces attack success rates while maintaining or improving downstream task performance
- The method preserves other aligned capabilities like instruction-following, not just safety alignment
- The interpolation coefficient λ provides controllable trade-offs between task performance and safety across multiple model families and tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merging pre- and post-fine-tuned model weights restores safety alignment by averaging out task-specific deviations that degrade safety.
- Mechanism: Linear interpolation of model weights recombines the aligned model's safety-preserving parameters with the fine-tuned model's task-specific parameters, effectively "averaging out" harmful shifts.
- Core assumption: Safety alignment is encoded in parameter space in a way that can be partially recovered through weighted averaging.
- Evidence anchors:
  - [abstract] "We show that simply merging the weights of pre- and post-fine-tuned models effectively mitigates safety degradation while enhancing performance."
  - [section] "After fine-tuning, we merge the parameters of the aligned model (θbase) with those of the fine-tuned model (θt) via linear interpolation."
- Break condition: If safety alignment is not linearly separable in parameter space, or if task-specific parameters dominate the merged model's behavior, the safety recovery effect may fail.

### Mechanism 2
- Claim: Merging preserves other aligned capabilities like instruction-following by restoring parameters that encode general alignment behaviors lost during task-specific fine-tuning.
- Mechanism: Catastrophic forgetting during fine-tuning affects not only safety but also general alignment behaviors; merging reintroduces these forgotten parameters.
- Core assumption: General alignment behaviors (e.g., instruction-following) are encoded in overlapping parameter regions with safety alignment.
- Evidence anchors:
  - [section] "Merging with the Aligned model helps recover this ability close to the original level" (referring to instruction-following recovery).
  - [section] "Merging can also preserve instruction adherence."
- Break condition: If task-specific fine-tuning affects non-overlapping parameter regions that don't contain general alignment information, merging won't recover these capabilities.

### Mechanism 3
- Claim: Merging provides controllable trade-offs between task performance and safety through interpolation coefficient λ.
- Mechanism: The λ parameter in linear interpolation allows tuning the balance between task-specific performance gains and safety preservation.
- Core assumption: There exists a continuous spectrum of behavior between the aligned and fine-tuned models that can be navigated through parameter interpolation.
- Evidence anchors:
  - [section] "λ ∈ [0, 1] controls the relative contribution of the fine-tuned model."
  - [section] "Eq. 2 is the formulation for the native linear merging method; other advanced merging methods can also be applied."
- Break condition: If the behavior space is not continuous or if there are discontinuous jumps in behavior at certain λ values, the trade-off may not be smooth or controllable.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper's safety degradation problem is fundamentally about catastrophic forgetting - the aligned model "forgets" its safety alignment when fine-tuned on downstream tasks.
  - Quick check question: What happens to previously learned knowledge when a neural network is fine-tuned on a new task without explicit preservation mechanisms?

- Concept: Model merging and parameter interpolation
  - Why needed here: The proposed solution relies on merging pre- and post-fine-tuned models through parameter interpolation, which requires understanding how model parameters encode behaviors.
  - Quick check question: How does linear interpolation of neural network weights combine the behaviors of two different models?

- Concept: Safety alignment in language models
  - Why needed here: Understanding what safety alignment means and how it's typically implemented helps explain why merging can preserve it.
  - Quick check question: What aspects of language model behavior constitute "safety alignment" and how are they typically encoded?

## Architecture Onboarding

- Component map:
  - Aligned base model (θbase) -> Fine-tuned model (θt) -> Merged model (θmerged)
  - Safety classifier: WildGuard for automated safety evaluation
  - Downstream task evaluators: Task-specific metrics (accuracy, BERTScore)

- Critical path:
  1. Load aligned model and downstream task data
  2. Fine-tune aligned model on task (LoRA with specific hyperparameters)
  3. Merge aligned and fine-tuned models using linear interpolation
  4. Evaluate merged model on both task performance and safety metrics
  5. Tune λ based on validation performance

- Design tradeoffs:
  - Simple linear merging vs. advanced methods (SLERP, DARE): Linear merging is simpler but may be less effective at preserving complex alignment behaviors
  - Safety classifier vs. human evaluation: WildGuard enables scalable evaluation but may miss nuanced safety issues
  - Interpolation coefficient selection: Need to balance task performance vs. safety preservation

- Failure signatures:
  - Safety degradation persists despite merging: May indicate that safety alignment is not linearly separable in parameter space
  - Task performance drops significantly after merging: λ may be too biased toward the aligned model
  - Inconsistent results across different merging methods: May indicate that the underlying parameter space structure is complex

- First 3 experiments:
  1. Run baseline SFT without merging to establish safety degradation baseline
  2. Apply linear merging with λ=0.5 and evaluate both safety and task performance
  3. Sweep λ from 0.1 to 0.9 to find optimal trade-off point for a specific task-model combination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does model merging effectively mitigate safety degradation across different model architectures beyond the three families tested (LLaMA-3, Gemma-2, Qwen2.5)?
- Basis in paper: [inferred] The paper tests only three model families and notes that effectiveness on different model architectures warrants further investigation
- Why unresolved: The paper explicitly states this as a limitation and only provides evidence for three specific model families
- What evidence would resolve it: Experiments showing consistent safety preservation through merging across diverse model architectures including transformer variants, different pretraining objectives, and models with varying parameter counts

### Open Question 2
- Question: Which specific safety categories are most vulnerable to degradation during fine-tuning, and can merging methods be optimized to address these categories more effectively?
- Basis in paper: [explicit] The paper analyzes safety category distributions and identifies categories 7 (Fraud/Deception) and 9 (Political Campaigning) as showing the most significant degradation
- Why unresolved: The paper observes category-specific degradation patterns but does not explore targeted optimization strategies for different merging methods
- What evidence would resolve it: Experiments testing category-specific merging weights or adaptive merging strategies that prioritize preservation of the most vulnerable safety categories

### Open Question 3
- Question: How does the interpolation factor λ in model merging affect the preservation of different capabilities (safety, instruction-following, task performance) across various downstream tasks?
- Basis in paper: [explicit] The paper varies λ from 0.1 to 0.9 but focuses on overall Pareto optimization rather than capability-specific analysis
- Why unresolved: The paper shows merging can preserve multiple capabilities but doesn't analyze how λ selection trades off between different preserved abilities
- What evidence would resolve it: Systematic analysis mapping different λ values to preservation levels of safety, instruction-following, and task performance across multiple tasks

### Open Question 4
- Question: Can model merging preserve safety when fine-tuning on datasets containing harmful content, or is it only effective for benign downstream tasks?
- Basis in paper: [inferred] The paper explicitly states it only evaluates on benign data and notes generalizability to datasets containing harmful content remains open
- Why unresolved: The paper's experimental scope is limited to benign tasks, leaving the method's effectiveness on harmful datasets untested
- What evidence would resolve it: Experiments fine-tuning on curated harmful datasets and measuring whether merging can recover safety alignment after exposure to harmful training data

## Limitations
- Safety recovery mechanism not fully understood - may be due to training data contamination rather than true parameter space averaging
- Limited evaluation on diverse safety threats - only a few attack types in AdvBench were tested
- Small sample size for safety evaluation may not generalize to real-world adversarial scenarios

## Confidence
**High Confidence**: The claim that model merging improves downstream task performance is well-supported, as this is a direct and measurable outcome with standard evaluation metrics. The mathematical formulation of linear interpolation is also clearly established.

**Medium Confidence**: The claim that merging effectively mitigates safety degradation is supported by automated safety classifier results, but the evaluation method (WildGuard) may miss nuanced safety issues that human evaluation would catch. The results are promising but not definitive across all attack types.

**Low Confidence**: The claim that merging "preserves other aligned capabilities" such as instruction-following is based on limited evidence - only mentioned in passing without comprehensive evaluation across multiple alignment dimensions.

## Next Checks
1. **Validation Check 1**: Conduct human evaluation of safety on a subset of AdvBench prompts to verify that automated classifier results align with human judgment, particularly for subtle safety violations that classifiers might miss.

2. **Validation Check 2**: Test the merging approach with safety datasets intentionally excluded from the fine-tuning process to ensure the safety recovery isn't simply due to training data contamination.

3. **Validation Check 3**: Evaluate the merged models on additional safety benchmarks beyond AdvBench and HEx-PHI, including real-world safety scenarios and multiple attack types to establish robustness across diverse safety threats.