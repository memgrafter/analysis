---
ver: rpa2
title: On the Robustness of Generative Information Retrieval Models
arxiv_id: '2412.18768'
source_url: https://arxiv.org/abs/2412.18768
tags:
- retrieval
- generative
- query
- robustness
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the out-of-distribution (OOD) robustness
  of generative information retrieval (IR) models compared to dense and sparse retrieval
  models. We analyze OOD scenarios across query variations, unseen query types, unseen
  tasks, and corpus expansion using the KILT benchmark.
---

# On the Robustness of Generative Information Retrieval Models

## Quick Facts
- arXiv ID: 2412.18768
- Source URL: https://arxiv.org/abs/2412.18768
- Reference count: 0
- Key outcome: Generative IR models exhibit poor OOD robustness, particularly for query variations and unseen tasks

## Executive Summary
This study systematically evaluates the out-of-distribution (OOD) robustness of generative information retrieval models compared to traditional dense and sparse retrieval approaches. Using the KILT benchmark, the research examines four key OOD scenarios: query variations, unseen query types, unseen tasks, and corpus expansion. The results reveal that generative models like BART and CorpusBrain consistently underperform their traditional counterparts in OOD settings, with particularly severe degradation in handling query variations and adapting to new tasks.

The findings challenge the assumption that generative models inherently possess better generalization capabilities in information retrieval. While generative IR models show some adaptability to corpus expansion, their vulnerability in handling distribution shifts during inference highlights a critical gap in current retrieval systems. This work provides important empirical evidence for the IR community about the current limitations of generative approaches and suggests directions for improving OOD robustness in future model development.

## Method Summary
The study employs a comprehensive experimental framework using the KILT benchmark to evaluate OOD robustness across multiple dimensions. The researchers systematically construct OOD scenarios by introducing query variations (paraphrasing, keyword substitution), testing on unseen query types (long-form, conversational), evaluating performance on unseen tasks beyond the training distribution, and examining behavior during corpus expansion. They compare three model families: generative models (BART, CorpusBrain), dense retrieval models (DPR), and sparse retrieval models (BM25). Performance is measured using standard IR metrics including R-precision, nDCG@5, and nDCG@10 across various KILT datasets.

## Key Results
- Generative IR models show significant performance drops in OOD scenarios, particularly for query variations and unseen tasks
- Dense retrieval models (DPR) demonstrate better OOD robustness than generative models in most tested scenarios
- Sparse models (BM25) maintain relatively stable performance across OOD settings but are outperformed by other approaches in in-distribution conditions
- Generative models exhibit better adaptability to corpus expansion compared to their performance in other OOD scenarios

## Why This Works (Mechanism)
Generative IR models rely on sequence-to-sequence architectures that learn complex mappings between queries and relevant documents through end-to-end training. Their poor OOD robustness stems from the fact that these models often overfit to specific distribution patterns in training data, making them sensitive to variations in query formulation or task structure. The generative approach, while powerful for in-distribution performance, lacks the explicit term-matching mechanisms that sparse models use or the dense representations that provide some semantic robustness.

## Foundational Learning
- **OOD Robustness**: The ability of models to maintain performance when faced with data distributions different from training; needed to understand real-world deployment scenarios where query distributions shift over time
- **Query Variations**: Different ways of expressing the same information need (paraphrasing, keyword substitution); critical for evaluating whether models capture semantic meaning beyond surface forms
- **Dense vs Sparse Representations**: Dense models use learned embeddings while sparse models rely on exact term matching; understanding this tradeoff is essential for interpreting model behavior
- **Sequence-to-Sequence IR**: Generative models that directly generate relevant document identifiers; important for grasping the architectural differences from traditional retrieval approaches
- **Retrieval Benchmarks**: Standardized evaluation frameworks like KILT that enable systematic comparison across different model types; necessary for establishing reliable performance measurements
- **Corpus Expansion**: The process of adding new documents to a collection and evaluating retrieval performance; relevant for understanding model scalability and adaptability

## Architecture Onboarding

Component map: Query -> Embedding/Generation -> Relevance Scoring -> Document Ranking -> Output

Critical path: Query processing → Model inference → Document selection → Ranking computation → Final result generation

Design tradeoffs: Generative models offer end-to-end learning and can capture complex query-document relationships but suffer from OOD sensitivity; dense models provide semantic understanding with better robustness but require separate indexing; sparse models offer reliability and interpretability but lack semantic generalization

Failure signatures: Significant performance drops on paraphrased queries, inability to handle conversational queries, catastrophic forgetting when new tasks are introduced, and inconsistent ranking when corpus structure changes

Three first experiments:
1. Test model performance on a held-out set of paraphrased queries from the training distribution to establish baseline sensitivity
2. Evaluate retrieval performance on a single unseen task to identify specific failure modes
3. Measure ranking stability when incrementally adding documents to the corpus to assess expansion robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow scope of evaluated generative IR models, focusing only on BART and CorpusBrain variants without exploring newer architectures
- Limited evaluation to the KILT benchmark, potentially constraining generalizability to other IR tasks or domains
- Lack of investigation into different training strategies or fine-tuning approaches that could impact OOD robustness

## Confidence

- High confidence: Generative IR models exhibit significant performance drops in query variation scenarios, consistently observed across multiple datasets and metrics
- Medium confidence: Poor adaptability to unseen tasks, given the study's limitation to KILT tasks without broader exploration of task types
- Medium confidence: Better adaptability to corpus expansion, based on limited expansion scenarios and corpus types

## Next Checks

1. Evaluate additional generative IR models (e.g., T5-based retrievers, ColBERTv2) across the same OOD scenarios to verify if observed performance patterns hold across different architectures

2. Conduct experiments with domain-specific datasets outside the KILT benchmark to assess generalizability of OOD robustness findings across different information needs and document collections

3. Investigate impact of various fine-tuning strategies and training data augmentation techniques on improving OOD robustness for generative IR models, particularly focusing on query variation scenarios