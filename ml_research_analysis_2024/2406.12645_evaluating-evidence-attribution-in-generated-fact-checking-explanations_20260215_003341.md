---
ver: rpa2
title: Evaluating Evidence Attribution in Generated Fact Checking Explanations
arxiv_id: '2406.12645'
source_url: https://arxiv.org/abs/2406.12645
tags:
- claim
- explanation
- evidence
- task
- reason
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates evidence attribution quality in LLM-generated
  fact-checking explanations. The authors introduce a novel "citation masking and
  recovery" protocol where annotators identify which explanation sentences cite specific
  evidence passages.
---

# Evaluating Evidence Attribution in Generated Fact Checking Explanations

## Quick Facts
- **arXiv ID:** 2406.12645
- **Source URL:** https://arxiv.org/abs/2406.12645
- **Reference count:** 40
- **Primary result:** GPT-4 achieves 0.74 F1 for citation accuracy in LLM-generated fact-checking explanations

## Executive Summary
This paper introduces a novel "citation masking and recovery" protocol to evaluate how well LLM-generated fact-checking explanations cite their evidence sources. The authors compare human-curated versus machine-selected evidence for explanation generation across three LLMs (GPT-4, GPT-3.5, LLaMA2-70B) using the PolitiHop dataset. Results show that while GPT-4 achieves the highest transparency scores, even it only reaches 0.74 F1 for citation accuracy. Surprisingly, machine-selected evidence produces explanations with similar or better transparency and utility than human-selected evidence, though it tends to be more comprehensive and redundant.

## Method Summary
The authors implement a citation masking and recovery protocol where annotators identify which explanation sentences cite specific evidence passages. Three LLMs (GPT-4, GPT-3.5, LLaMA2-70B) generate explanations with inline citations using both human-selected and machine-selected evidence. Transparency is measured through citation accuracy (precision, recall, F1, entropy) by masking citations and having annotators recover them. Utility is assessed through user helpfulness ratings on a 0-100 scale. The protocol is tested with both human annotators and automatic LLM annotation to evaluate correlation.

## Key Results
- GPT-4 achieves 0.74 F1 for citation accuracy, the best among tested models
- Machine-selected evidence produces explanations with similar or better transparency/utility than human-selected evidence
- High utility scores don't guarantee high transparency - explanations can appear helpful while citing sources incorrectly
- GPT-4 using machine-selected evidence achieves 0.74 F1 transparency and 76.34 utility score

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Machine-selected evidence produces explanations with similar or better transparency/utility than human-selected evidence
- **Mechanism:** LLMs select evidence that is more comprehensive and redundant, reinforcing key points in explanations
- **Core assumption:** Machine-selected evidence, though different from human-selected, contains relevant passages that enhance explanation quality
- **Evidence anchors:** [section] "Machine-selected evidence is more comprehensive... GPT-4 selected more evidence passages on average (5 vs. 3 per claim) and as such the evidence set is more comprehensive."
- **Break condition:** When machine-selected evidence lacks relevance or contains excessive noise that overwhelms the LLM's ability to generate coherent explanations

### Mechanism 2
- **Claim:** High utility scores don't guarantee high transparency in explanations
- **Mechanism:** Explanations can appear helpful in clarifying claims while containing incorrect citations or fabricated statements
- **Core assumption:** Utility (helpfulness in clarifying claims) and transparency (correct citation of sources) are distinct qualities that can be independently measured
- **Evidence anchors:** [section] "High utility doesn't necessarily imply high transparency... This demonstrates that utility and citation represent two distinct qualities and has an important implication: an explanation that appears helpful can actually still be misleading."
- **Break condition:** When the LLM's parametric knowledge generates plausible but inaccurate explanations that score high on utility despite poor citation accuracy

### Mechanism 3
- **Claim:** LLM annotation correlates with human annotation for transparency assessment
- **Mechanism:** The citation masking and recovery protocol can be automated using LLMs, reducing the need for human annotators
- **Core assumption:** LLMs can effectively identify which sentences should cite specific evidence passages when citations are masked
- **Evidence anchors:** [abstract] "We implement our protocol using both human annotators and automatic annotators, and find that LLM annotation correlates with human annotation, suggesting that attribution assessment can be automated."
- **Break condition:** When LLM annotation performance drops significantly compared to human annotation, indicating the protocol may not capture all aspects of proper citation

## Foundational Learning

- **Concept:** Evidence selection for fact-checking explanations
  - **Why needed here:** The quality of explanations depends heavily on selecting relevant evidence passages from a larger set
  - **Quick check question:** What is the primary difference between human-selected and machine-selected evidence in this study?

- **Concept:** Transparency vs. utility evaluation
  - **Why needed here:** The paper distinguishes between whether explanations cite sources correctly (transparency) and whether they help users understand claims (utility)
  - **Quick check question:** According to the findings, can an explanation be highly useful while having poor transparency?

- **Concept:** Citation masking and recovery protocol
  - **Why needed here:** This novel evaluation method tests whether explanations properly cite their sources by having annotators identify which sentences should cite specific evidence
  - **Quick check question:** What is the F1 score achieved by the best-performing LLM (GPT-4) for citation accuracy?

## Architecture Onboarding

- **Component map:** Claim → Evidence selection → Explanation generation → Transparency evaluation → Utility evaluation → Analysis
- **Critical path:** Claim → Evidence selection → Explanation generation → Transparency evaluation → Utility evaluation → Analysis
- **Design tradeoffs:** Human-selected evidence provides higher quality but is labor-intensive, while machine-selected evidence is more comprehensive but may contain redundancy and noise
- **Failure signatures:** Low F1 scores in citation accuracy, high entropy in annotator agreement, significant variance in transparency scores across different LLMs
- **First 3 experiments:**
  1. Compare human-selected vs. machine-selected evidence for GPT-4 explanation generation
  2. Test transparency and utility scores for different LLMs (GPT-4, GPT-3.5, LLaMA2-70B) using machine-selected evidence
  3. Analyze cases with lowest transparency scores to identify patterns of incorrect citations or fabricated statements

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the quality of explanations vary when using different sizes of LLMs for both evidence selection and explanation generation?
- **Basis in paper:** [explicit] The paper compares GPT-4, GPT-3.5, and LLaMA2-70B, noting that even the best model (GPT-4) achieves only 0.74 F1 for citation accuracy.
- **Why unresolved:** The paper does not explore the impact of varying model sizes beyond these three models, nor does it investigate whether smaller or larger models might yield different results.
- **What evidence would resolve it:** Experiments comparing a wider range of model sizes for both evidence selection and explanation generation, with corresponding transparency and utility metrics.

### Open Question 2
- **Question:** Can the transparency of LLM-generated fact-checking explanations be improved through advanced prompt engineering techniques?
- **Basis in paper:** [inferred] The paper mentions that despite encouraging performance, there is still room for improvement in explanation generation, and it does not explore advanced prompt engineering techniques.
- **Why unresolved:** The paper does not test different prompt engineering strategies to enhance transparency, such as using more detailed instructions or iterative refinement prompts.
- **What evidence would resolve it:** Comparative studies using various prompt engineering techniques to assess their impact on transparency scores.

### Open Question 3
- **Question:** How does the redundancy in machine-selected evidence affect the overall quality and utility of the generated explanations?
- **Basis in paper:** [explicit] The paper notes that machine-selected evidence tends to be more comprehensive but can contain redundancy, which can reinforce key points.
- **Why unresolved:** The paper does not quantify the impact of redundancy on explanation quality or determine an optimal balance between comprehensiveness and conciseness.
- **What evidence would resolve it:** Analysis correlating redundancy levels in evidence selection with transparency and utility scores, and experiments testing different redundancy thresholds.

## Limitations
- Missing implementation details for prompt templates and annotator quality control thresholds
- Results may be specific to the PolitiHop dataset and tested LLMs
- High utility doesn't guarantee high transparency, but this finding may not generalize across different domains
- GPT-4's 0.74 F1 citation accuracy still leaves room for improvement in evidence attribution

## Confidence
- **High confidence:** The distinction between utility and transparency as separate evaluation dimensions is well-supported by the data
- **Medium confidence:** Machine-selected evidence producing similar or better explanations than human-selected evidence, though this may be dataset-specific
- **Medium confidence:** LLM annotation correlating with human annotation for transparency assessment, pending verification of implementation details

## Next Checks
1. Replicate the evidence selection and explanation generation using the same prompt templates (once obtained) to verify the 0.74 F1 citation accuracy for GPT-4
2. Test the citation masking and recovery protocol with a different fact-checking dataset to assess generalizability of the utility-transparency decoupling finding
3. Implement automated LLM annotation for transparency assessment and measure correlation with human annotations using Krippendorff's alpha and MASI agreement metrics