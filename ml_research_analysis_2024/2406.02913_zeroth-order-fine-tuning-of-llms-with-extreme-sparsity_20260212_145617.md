---
ver: rpa2
title: Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity
arxiv_id: '2406.02913'
source_url: https://arxiv.org/abs/2406.02913
tags:
- fine-tuning
- sensitive
- parameters
- sparse
- static
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-tuning large language
  models (LLMs) under severe memory constraints, such as on mobile devices, where
  full precision forward passes are infeasible. The authors propose a method that
  combines sparsity and quantization with zeroth-order (ZO) optimization for efficient
  LLM fine-tuning.
---

# Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity

## Quick Facts
- arXiv ID: 2406.02913
- Source URL: https://arxiv.org/abs/2406.02913
- Authors: Wentao Guo; Jikai Long; Yimeng Zeng; Zirui Liu; Xinyu Yang; Yide Ran; Jacob R. Gardner; Osbert Bastani; Christopher De Sa; Xiaodong Yu; Beidi Chen; Zhaozhuo Xu
- Reference count: 40
- Primary result: Fine-tuning 0.1% sensitive parameters with ZO optimization achieves performance comparable to full ZO fine-tuning while enabling on-device LLM personalization within severe memory constraints.

## Executive Summary
This paper addresses the challenge of fine-tuning large language models under severe memory constraints, such as on mobile devices, where full precision forward passes are infeasible. The authors propose a method that combines sparsity and quantization with zeroth-order (ZO) optimization for efficient LLM fine-tuning. The core idea is to identify and fine-tune only a tiny fraction (0.1%) of "sensitive parameters" in the LLM, determined by their magnitude in the Fisher information matrix, while quantizing the remaining parameters to 4 bits. This approach allows for on-device personalization of LLMs without requiring full backpropagation or large memory budgets. Experiments demonstrate that fine-tuning 0.1% sensitive parameters with ZO optimization achieves performance comparable to full ZO fine-tuning and even surpasses it in some cases, while offering significant wall-clock time speedups. The method enables efficient fine-tuning of a Llama2-7B model on a GPU with less than 8 GiB of memory, making it suitable for deployment on memory-constrained devices.

## Method Summary
The method identifies sensitive parameters using Fisher information matrix magnitudes from pre-training data, selects the top 0.1% for fine-tuning, and applies zeroth-order optimization with SPSA to these parameters only. The remaining 99.9% of parameters are quantized to 4 bits using surrogate gradients from pre-training datasets. This decomposition into sparse (wsparse) and dense (wdense) parameter sets enables extreme memory efficiency while maintaining performance through targeted optimization of the most impactful parameters.

## Key Results
- Fine-tuning 0.1% sensitive parameters with ZO optimization achieves performance comparable to full ZO fine-tuning
- Combining 0.1% sparse fine-tuning with 4-bit quantization enables efficient fine-tuning of Llama2-7B on GPU with <8GB memory
- 0.1% sensitive parameter ZO fine-tuning outperforms full ZO fine-tuning on several downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning only 0.1% of sensitive parameters with ZO optimization can achieve performance comparable to full ZO fine-tuning.
- Mechanism: Pre-training identifies "sensitive parameters" that are critical for downstream tasks. By focusing optimization on these parameters, we maintain performance while drastically reducing computation and memory.
- Core assumption: The Fisher information matrix during pre-training can identify parameters that will be important for downstream tasks, and this pattern is transferable.
- Evidence anchors:
  - [abstract] "Our findings reveal that the pre-training process can identify a set of 'sensitive parameters' that can guide the ZO fine-tuning of LLMs on downstream tasks."
  - [section 3.1] "We observe an extreme sparsity pattern in LLM parameters: a subset, determined by selecting the top-k magnitude entries from the empirical Fisher information matrix, is effective for ZO fine-tuning."
  - [corpus] Weak evidence - only 1/8 related papers directly address parameter sensitivity in ZO fine-tuning.

### Mechanism 2
- Claim: Combining sparsity with quantization enables on-device LLM personalization within severe memory constraints.
- Mechanism: By identifying 0.1% sensitive parameters for ZO optimization and quantizing the remaining 99.9% to 4 bits, we reduce memory usage to fit within 8GB while maintaining performance.
- Core assumption: Quantizing non-sensitive parameters to 4 bits doesn't significantly impact performance since they're not being updated.
- Evidence anchors:
  - [abstract] "We show that ZO fine-tuning targeting these 0.1% sensitive parameters, combined with 4 bit quantization, enables efficient ZO fine-tuning of an Llama2-7B model on a GPU device with less than 8GiB of memory."
  - [section 3.5] "We would quantize the wdense to 4 bits, which reduces the model size of a Llama2-7B model from 13.5 to 3.4 GiB."
  - [corpus] Moderate evidence - several papers address quantization in ZO fine-tuning, but none combine it with extreme sparsity.

### Mechanism 3
- Claim: Using surrogate gradients from pre-training datasets can effectively identify sensitive parameters when downstream task gradients are unavailable.
- Mechanism: Gradients computed on pre-training datasets (like C4) can serve as a proxy for downstream task gradients to identify sensitive parameters before fine-tuning begins.
- Core assumption: The parameter sensitivity pattern is similar enough between pre-training and downstream tasks that C4 gradients can effectively identify sensitive parameters.
- Evidence anchors:
  - [section 3.3] "we can find that the sensitive parameters derived from pre-training datasets (C4) would still cover a large fraction of model sensitivity."
  - [figure 3] "the fact that 'task grad, static' does not vanish and still has a large ratio over 'task grad, dyn.' at the end of training demonstrate that we can select parameters before fine-tuning."
  - [corpus] Weak evidence - only 1/8 related papers mentions transferability of sparsity patterns.

## Foundational Learning

- Concept: Zeroth-order optimization
  - Why needed here: ZO optimization eliminates the need for backpropagation, saving memory by avoiding weight gradients and activation caching.
  - Quick check question: How does SPSA estimate gradients without backpropagation, and what are the memory savings compared to first-order methods?

- Concept: Fisher information matrix
  - Why needed here: The Fisher matrix identifies parameters with high sensitivity to task performance, guiding which parameters to fine-tune.
  - Quick check question: Why does the diagonal of the Fisher matrix correspond to coordinate-wise gradient squares, and how does this relate to parameter sensitivity?

- Concept: Quantization techniques
  - Why needed here: Post-training quantization reduces model size by compressing non-sensitive parameters to 4 bits while maintaining accuracy.
  - Quick check question: What are the trade-offs between different quantization methods (uniform vs non-uniform) when combined with sparse parameter updates?

## Architecture Onboarding

- Component map: Sensitive parameter identification -> Parameter decomposition -> Quantization -> On-device ZO fine-tuning -> Sparse inference
- Critical path: Parameter identification → Decomposition → Quantization → On-device ZO fine-tuning → Sparse inference
- Design tradeoffs: Extreme sparsity (0.1%) vs performance, 4-bit quantization vs accuracy, surrogate gradients vs task-specific gradients
- Failure signatures: Performance degradation when task distribution differs from pre-training, memory overflow if 4-bit quantization insufficient, slow convergence if sensitive parameters misidentified
- First 3 experiments:
  1. Validate that 0.1% sensitive parameters identified from C4 gradients maintain performance on RTE task
  2. Compare 4-bit vs 8-bit quantization impact on non-sensitive parameters for memory-constrained settings
  3. Benchmark inference speedup from sparse operations vs accuracy trade-off on mobile devices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sparsity level for ZO fine-tuning that balances performance and efficiency across different LLM architectures and tasks?
- Basis in paper: [explicit] The paper investigates sparsity levels from 0.1% to 99.9% and finds that 0.1% sensitive parameters outperform full ZO fine-tuning, but doesn't establish a universal optimal level.
- Why unresolved: The paper only tests a limited range of sparsity levels (0.1%, 10%, 99.9%) and doesn't systematically explore the entire spectrum to find a precise optimal point.
- What evidence would resolve it: A comprehensive ablation study testing sparsity levels at fine-grained intervals (e.g., 0.01%, 0.1%, 0.5%, 1%, 5%, 10%, 50%, 90%, 99%) across multiple architectures and tasks would reveal the optimal sparsity point.

### Open Question 2
- Question: How does the sensitive parameter mask derived from pre-training datasets transfer to tasks with significantly different data distributions or domains?
- Basis in paper: [explicit] The paper shows that sensitive parameters from C4 pre-training datasets work well across various downstream tasks, but doesn't test cross-domain transfer.
- Why unresolved: The experiments only test transfer within similar domains (text-based tasks) but don't examine how well the C4-derived mask performs on tasks from completely different domains (e.g., code, scientific text, low-resource languages).
- What evidence would resolve it: Testing the C4-derived sensitive mask on tasks from diverse domains (programming, biomedical, legal, low-resource languages) and measuring performance degradation compared to task-specific masks would quantify cross-domain transferability.

### Open Question 3
- Question: What is the theoretical explanation for why ZO fine-tuning of 0.1% sensitive parameters outperforms full ZO fine-tuning?
- Basis in paper: [explicit] The paper observes this phenomenon empirically but provides only limited theoretical justification through Fisher information matrix analysis.
- Why unresolved: While the paper offers some theoretical intuition about Fisher information and Hessian coverage, it doesn't provide a rigorous mathematical proof or explanation for why such extreme sparsity yields better results.
- What evidence would resolve it: Developing a formal theoretical framework that explains the optimization dynamics of ZO methods on sparse parameter subsets, potentially involving analysis of gradient noise, parameter redundancy, or optimization landscape properties.

## Limitations

- Extreme sparsity (0.1%) may not generalize to more complex tasks requiring broader parameter adaptation
- Reliance on Fisher information patterns from pre-training data assumes transferability that may fail for domains significantly different from C4
- 4-bit quantization may introduce unacceptable degradation in domains requiring high precision

## Confidence

**High Confidence**: The core mechanism of combining ZO optimization with extreme sparsity is technically sound and well-supported by experimental results on standard benchmarks. The memory efficiency claims (8GB GPU usage for 7B models) are verifiable and significant.

**Medium Confidence**: The transferability of Fisher-based sensitivity patterns from pre-training to downstream tasks shows promise but requires broader validation across diverse task distributions. The 4-bit quantization integration appears effective but may not generalize to all domains.

**Low Confidence**: The extreme 0.1% sparsity level's robustness across diverse, complex tasks remains unproven. Performance on tasks requiring substantial domain adaptation versus general language understanding is uncertain.

## Next Checks

1. **Cross-Domain Sensitivity Transfer**: Validate sensitive parameter identification on tasks from completely different domains (e.g., medical text analysis, code generation) compared to C4 pre-training data. Measure performance degradation when using C4-based sensitivity patterns versus task-specific patterns.

2. **Quantization Precision Scaling**: Systematically evaluate the trade-off between quantization bit-width (2-bit, 4-bit, 8-bit) and downstream task performance across diverse domains. Identify the minimum precision that maintains acceptable accuracy for different task types.

3. **Sparsity Threshold Robustness**: Test sensitivity identification at multiple sparsity levels (0.01%, 0.1%, 1%, 10%) across a diverse task suite to determine the optimal balance between memory efficiency and task performance for different complexity levels.