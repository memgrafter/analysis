---
ver: rpa2
title: Rethinking the Function of Neurons in KANs
arxiv_id: '2407.20667'
source_url: https://arxiv.org/abs/2407.20667
tags:
- arxiv
- function
- kolmogorov-arnold
- functions
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the efficacy of summation operation in
  Kolmogorov-Arnold Networks (KANs) neurons, motivated by the Kolmogorov-Arnold representation
  theorem. Through empirical research across ten benchmark machine learning datasets,
  the study evaluates nine multivariate functions (sum, min, max, multiply, mean,
  std, var, median, and norm) in two-layer KAN architectures.
---

# Rethinking the Function of Neurons in KANs

## Quick Facts
- arXiv ID: 2407.20667
- Source URL: https://arxiv.org/abs/2407.20667
- Reference count: 31
- This study demonstrates that using the mean function instead of sum in KAN neurons significantly improves performance and stability across ten benchmark datasets.

## Executive Summary
This study investigates the efficacy of different summation operations in Kolmogorov-Arnold Networks (KANs) neurons, motivated by the Kolmogorov-Arnold representation theorem. Through empirical research across ten benchmark machine learning datasets, the study evaluates nine multivariate functions (sum, min, max, multiply, mean, std, var, median, and norm) in two-layer KAN architectures. The results demonstrate that substituting the sum with the mean function in KAN neurons significantly improves performance compared to traditional KANs. The mean function maintains inputs within the effective range of spline activation functions, contributing to training stability. Across all datasets, KANs using mean function achieved higher average test accuracy with lower variability compared to standard KANs and KANs with layer normalization.

## Method Summary
The study evaluates nine multivariate functions in two-layer KAN architectures [nin, 10, 1] across ten UCI Machine Learning datasets. Models were trained for 2000 iterations using Adam optimizer (learning rate 0.01, batch size 32). Each experiment was repeated 20 times with 60/20/20 train/validation/test splits. Statistical significance was assessed using Wilcoxon signed-rank test. The study compares KAN with standard sum function, KAN with mean function (KAN-AVG), and KAN with layer normalization.

## Key Results
- Mean function outperforms sum function across all ten datasets with statistically significant improvements (p < 0.05)
- KAN-AVG achieved higher average test accuracy with lower variability compared to standard KANs
- Mean function maintains inputs within the effective range of spline activation functions, improving training stability
- Layer normalization was less effective than mean function for range confinement in high-dimensional data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using the mean function instead of the sum constrains neuron outputs to stay within the effective range of the spline activation functions.
- Mechanism: The mean reduces the magnitude of neuron outputs compared to the sum, keeping values closer to zero and within the [-1.0, +1.0] range where splines are defined.
- Core assumption: Spline activation functions behave predictably only within their defined range.
- Evidence anchors:
  - [abstract] "Our study demonstrates that this minor modification contributes to the stability of training by confining the input to the spline within the effective range of the activation function."
  - [section] "Our hypothesis posits that mean outperforms sum due to its ability to maintain input values within the effective range of the spline activation function. By default, the splines in KANs are defined with a range of [-1.0, +1.0], and values outside this range can lead to unpredictable activation behavior."
  - [corpus] No direct evidence in corpus about range confinement, weak support.
- Break condition: If the data distribution has very small variance, the difference between mean and sum becomes negligible, reducing this mechanism's effect.

### Mechanism 2
- Claim: The mean function is consistent with the Kolmogorov-Arnold representation theorem.
- Mechanism: The mean can be derived from the original theorem formulation by appropriately scaling the functions, showing mathematical compatibility.
- Core assumption: Mathematical consistency with the original theorem implies practical benefits.
- Evidence anchors:
  - [section] "It is worth noting that the mean function is consistent with the Kolmogorov-Arnold representation theorem. This can be observed by directly modifying Equation (1) using Φ′q(x) = Φq(x) / (2n+1) and ϕ′q,p(x) = ϕq,p(x) / n, resulting in: f(x1, . . . , xn) = 1/(2n + 1) Σ Φ′q(1/n Σ ϕ′q,p(xp))"
  - [abstract] "The study also shows that the mean function is consistent with the Kolmogorov-Arnold representation theorem, as it can be derived by appropriately scaling the functions in the original formulation."
  - [corpus] No corpus evidence supporting this mechanism, weak support.
- Break condition: If the mathematical scaling leads to numerical instability or if the theoretical consistency doesn't translate to practical gains.

### Mechanism 3
- Claim: Layer normalization does not effectively solve the range confinement problem for high-dimensional data.
- Mechanism: Layer normalization centers neuron outputs around zero but doesn't consistently ensure values fall within [-1.0, +1.0], especially as feature count increases.
- Core assumption: Layer normalization primarily addresses covariate shift, not range confinement.
- Evidence anchors:
  - [section] "We found that the suggestion of [4] to use Layer Normalization before the splines was effective in addressing covariate shift by centering neuron outputs around zero, but did not consistently ensure values fell within [-1.0, +1.0]."
  - [abstract] No direct mention of layer normalization's limitations.
  - [corpus] No corpus evidence about layer normalization limitations, weak support.
- Break condition: If layer normalization parameters are tuned specifically for range confinement, it might overcome this limitation.

## Foundational Learning

- Concept: Kolmogorov-Arnold representation theorem
  - Why needed here: Understanding the theorem is crucial because KANs are motivated by it, and the choice of neuron function must align with its principles.
  - Quick check question: Can you explain how the theorem states that any continuous multivariate function can be represented using univariate functions and addition?

- Concept: Spline activation functions and their effective range
  - Why needed here: The study hinges on keeping neuron outputs within the effective range of splines, so understanding how splines behave outside this range is essential.
  - Quick check question: What happens to spline activation functions when inputs fall outside their defined range of [-1.0, +1.0]?

- Concept: Statistical measures (mean, sum, variance, etc.)
  - Why needed here: The study compares different multivariate functions, so understanding their properties and effects on data distribution is necessary.
  - Quick check question: How does using the mean instead of the sum affect the scale and distribution of neuron outputs when the number of features increases?

## Architecture Onboarding

- Component map:
  - Input layer: Receives features
  - KAN layer 1: Applies element-wise functions ϕq,p to inputs, then aggregates using chosen multivariate function (mean/sum/etc.)
  - KAN layer 2: Applies functions Φq to aggregated outputs, then produces final outputs
  - Activation: B-spline functions defined on [-1.0, +1.0] range

- Critical path:
  1. Data preprocessing and splitting
  2. Forward pass through KAN layers using chosen multivariate function
  3. B-spline activation on neuron outputs
  4. Loss computation and backpropagation
  5. Parameter updates via Adam optimizer

- Design tradeoffs:
  - Mean vs sum: Mean provides better range confinement but may reduce signal magnitude
  - Layer normalization vs mean function: Layer normalization addresses covariate shift but not range issues
  - Function choice: Different functions (mean, std, var) offer different trade-offs in performance and consistency with theorem

- Failure signatures:
  - Training instability: Neuron outputs frequently exceed [-1.0, +1.0] range
  - Poor generalization: High variance in test accuracy across runs
  - Slow convergence: Learning rate too low or function choice inappropriate for data

- First 3 experiments:
  1. Compare mean vs sum on a small dataset (e.g., dermatology) to observe range confinement effects
  2. Test mean function on datasets with varying numbers of features to verify consistency across dimensions
  3. Evaluate layer normalization's effectiveness compared to mean function on high-dimensional data (e.g., semeion)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of KAN-AVG scale with increasing network depth beyond two layers?
- Basis in paper: [inferred] The study only tested a two-layer architecture [nin, 10, 1] and showed mean function improves stability and performance, but did not explore deeper architectures.
- Why unresolved: The paper focused on a fixed two-layer architecture and did not investigate how the mean function performs in deeper KANs where the benefits of input normalization might compound or change.
- What evidence would resolve it: Systematic experiments varying network depth (3+ layers) with both sum and mean functions across multiple datasets to compare stability and accuracy trends.

### Open Question 2
- Question: Does the mean function provide similar benefits when combined with alternative activation functions beyond B-splines, such as wavelets or radial basis functions?
- Basis in paper: [explicit] The paper notes that mean function could be extended to architectures using alternative activation functions (wavelets [24, 25], radial basis functions [4, 26], etc.) with minimal implementation overhead.
- Why unresolved: The experiments only used B-spline activations, so the generalizability of mean function benefits to other activation types remains untested.
- What evidence would resolve it: Comparative experiments using KAN-AVG with various activation functions (wavelets, RBFs, fractional functions) across the same benchmark datasets.

### Open Question 3
- Question: What is the theoretical relationship between the mean function in KAN neurons and the original Kolmogorov-Arnold representation theorem when extended to multi-layer networks?
- Basis in paper: [explicit] The paper shows mean function is consistent with the theorem through scaling of original functions, but this derivation only applies to the two-layer case described in Equation 1.
- Why unresolved: The paper demonstrates consistency for the basic two-layer formulation but doesn't explore how this extends to arbitrary-depth networks or whether the theoretical justification holds in deeper architectures.
- What evidence would resolve it: Mathematical proof or counterexample showing whether the mean-based multi-layer KAN can be derived from or remains consistent with the original representation theorem framework.

## Limitations
- Conclusions rely on empirical results from ten UCI datasets and two-layer architectures only
- Theoretical mechanism linking range confinement to performance lacks direct experimental validation
- Study doesn't explore deeper KAN architectures where sum-to-mean transformation might have different effects

## Confidence
- High confidence in empirical finding that mean outperforms sum across tested datasets (p < 0.05)
- Medium confidence in mechanism explanation about range confinement, as it's theoretically sound but not directly measured
- Low confidence in generalizability to non-UCI datasets or regression tasks not tested in this study

## Next Checks
1. Add instrumentation to track the percentage of neuron outputs that fall outside [-1.0, +1.0] during training for both sum and mean functions across multiple datasets

2. Test KAN-AVG with datasets that have vastly different feature scales (some normalized, others not) to verify the mean function's robustness to preprocessing choices

3. Evaluate KAN-AVG in three or four-layer architectures to determine if the sum-to-mean benefit persists as network depth increases, or if it diminishes due to signal attenuation