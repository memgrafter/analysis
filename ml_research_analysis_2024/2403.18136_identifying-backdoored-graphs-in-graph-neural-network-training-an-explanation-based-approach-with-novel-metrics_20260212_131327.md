---
ver: rpa2
title: 'Identifying Backdoored Graphs in Graph Neural Network Training: An Explanation-Based
  Approach with Novel Metrics'
arxiv_id: '2403.18136'
source_url: https://arxiv.org/abs/2403.18136
tags:
- trigger
- backdoor
- graph
- attacks
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the detection of backdoor attacks on Graph
  Neural Networks (GNNs), which can compromise their reliability in high-stakes domains.
  The authors propose a novel explanation-based detection method that leverages seven
  new metrics derived from GNN explanation processes to distinguish between clean
  and backdoored graphs.
---

# Identifying Backdoored Graphs in Graph Neural Network Training: An Explanation-Based Approach with Novel Metrics

## Quick Facts
- arXiv ID: 2403.18136
- Source URL: https://arxiv.org/abs/2403.18136
- Reference count: 40
- Authors: Jane Downer; Ren Wang; Binghui Wang

## Executive Summary
This paper addresses the critical challenge of detecting backdoor attacks on Graph Neural Networks (GNNs), which can compromise their reliability in high-stakes domains. The authors propose a novel explanation-based detection method that leverages seven new metrics derived from GNN explanation processes to distinguish between clean and backdooded graphs. Their approach overcomes limitations of relying solely on explanatory subgraphs, which often inconsistently reveal backdoor triggers. Tested across multiple datasets and attack models, including adaptive attacks, the method achieves high detection performance, with F1 scores up to 0.906 for random triggers and 0.842 for adaptive triggers.

## Method Summary
The method detects backdoor attacks by extracting and transforming secondary outputs from GNN explanation mechanisms, designing seven innovative metrics that capture backdoor patterns in these outputs. These metrics include Prediction Confidence, Explainability, Connectivity, SNDV, NDV, Elbow, and Curvature, which are computed from node/edge masks, loss curves, and prediction probabilities generated during the explanation process. The approach establishes detection thresholds from clean validation data using 25th/75th percentile ranges and applies composite metric voting to classify graphs as clean or backdoored, requiring at least k metrics to exceed their respective thresholds.

## Key Results
- Achieves F1 scores up to 0.906 for detecting random edge-removal (ER) triggers and 0.842 for adaptive triggers
- Outperforms traditional explanation-based methods that rely solely on explanatory subgraphs
- Demonstrates robustness against adaptive attacks specifically designed to evade GNN explanation processes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method detects backdoor triggers by leveraging graph-level explanations and seven novel metrics derived from the explanation process.
- Mechanism: GNN explanation mechanisms generate secondary outputs (masks, loss curves, prediction probabilities) that reveal backdoor patterns. The seven metrics capture these patterns consistently, allowing detection even when explanatory subgraphs alone fail.
- Core assumption: Backdoor triggers introduce consistent, detectable patterns in explanation outputs that differ from clean graphs.
- Evidence anchors:
  - [abstract]: "We extract and transform secondary outputs from GNN explanation mechanisms, designing seven innovative metrics for effective detection of backdoor attacks on GNNs."
  - [section]: "We introduce a set of novel metrics that leverage valuable insights from certain aspects of the GNN explanation process."
  - [corpus]: Weak - no directly comparable papers found in the corpus.
- Break condition: If backdoor triggers are designed to evade GNN explanation processes, the metrics derived from explanation outputs would fail to detect them.

### Mechanism 2
- Claim: The adaptive attack targets both the GNN classifier and the explanation process simultaneously.
- Mechanism: An edge generator GNN is trained to produce triggers that evade GNN explanation while the target GNN is trained to classify backdoored graphs correctly. This creates triggers that blend with clean graph structure.
- Core assumption: GNN explainers can be fooled by carefully crafted triggers that appear unimportant during the explanation process.
- Evidence anchors:
  - [section]: "The key idea is two-fold: (1) train a generator GNN to produce triggers that evade GNN explanation, and (2) simultaneously train the target GNN model that minimizes classification loss on the backdoored graphs with the trigger produced by this generator."
  - [corpus]: Weak - only one paper on adaptive attacks found, but not specifically targeting explanation processes.
- Break condition: If the explanation process can be made robust to adversarial trigger generation, or if the adaptive attack fails to produce truly stealthy triggers.

### Mechanism 3
- Claim: Composite detection using multiple metrics with voting outperforms single-metric detection.
- Mechanism: Each metric votes on whether a graph is clean or backdoored based on thresholds established from clean validation data. A graph is classified as backdoored if at least k metrics vote positively.
- Core assumption: Different metrics capture different aspects of backdoor behavior, and combining them provides more robust detection than any single metric.
- Evidence anchors:
  - [section]: "While no single metric is foolproof, considering multiple metrics at a time can boost our confidence in our detection."
  - [section]: "Our composite metric uses this notion of positive and negative metrics to make a final prediction of clean or backdoor."
  - [corpus]: Weak - no directly comparable composite metric approaches found in the corpus.
- Break condition: If backdoor triggers are designed to evade multiple metrics simultaneously, or if the voting threshold k is poorly chosen.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their vulnerability to backdoor attacks
  - Why needed here: The entire detection method is built around identifying backdoor attacks on GNNs, so understanding how GNNs work and how they can be attacked is fundamental.
  - Quick check question: What is the difference between a clean graph and a backdoored graph in the context of GNN backdoor attacks?

- Concept: GNN explanation methods (GNNExplainer, PGExplainer, etc.)
  - Why needed here: The detection method leverages outputs from GNN explanation processes, so understanding how these explainers work is crucial.
  - Quick check question: What is the difference between model explanations and phenomenon explanations in GNN explanation?

- Concept: Statistical metrics and threshold-based detection
  - Why needed here: The method establishes detection thresholds from clean validation data and uses statistical measures to distinguish between clean and backdoored graphs.
  - Quick check question: How would you establish a detection threshold for a metric that is expected to be higher for backdoored graphs?

## Architecture Onboarding

- Component map: GNN classifier (GIN, GCN, GAT variants) -> GNN explainer (GNNExplainer with configurable hyperparameters) -> Metric computation module (7 metrics) -> Clean validation data processor -> Composite metric voter -> Adaptive attack generator

- Critical path:
  1. Load graph data and split into training/validation sets
  2. Train GNN classifier on training data
  3. Apply GNN explainer to validation and training data
  4. Compute 7 metrics for each graph
  5. Establish detection thresholds from clean validation data
  6. Apply composite metric voting to training data
  7. Classify graphs as clean or backdoored

- Design tradeoffs:
  - Using GNN explanation adds computational overhead but provides rich information for detection
  - Choosing 7 metrics balances comprehensiveness with computational efficiency
  - The voting threshold (NPMR) affects precision-recall tradeoff
  - Clean validation data requirements vs. detection performance

- Failure signatures:
  - Low F1 scores indicate poor separation between clean and backdoored graphs
  - Metrics showing similar distributions for clean and backdoored graphs suggest attack evasion
  - High false positive rates suggest overly aggressive thresholds
  - High false negative rates suggest insufficient sensitivity

- First 3 experiments:
  1. Run on MUTAG dataset with random ER triggers (size 4, 20% poisoning) to establish baseline performance
  2. Test on AIDS dataset with PA triggers to evaluate cross-dataset generalization
  3. Apply adaptive attack generation to test robustness against evasion attempts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed detection metrics perform against adaptive attacks that specifically target the explanation process?
- Basis in paper: [explicit] The authors mention developing an adaptive attack to evaluate their approach rigorously and discuss the performance of their composite metric against adaptive triggers.
- Why unresolved: While the paper shows that the composite metric performs reasonably well against adaptive attacks, it does not provide a detailed analysis of the individual metric performance against such attacks or how the attack affects the explainer's ability to generate explanations.
- What evidence would resolve it: Detailed results showing the performance of each individual metric against adaptive attacks and an analysis of how the adaptive attack affects the explainer's explanation generation process.

### Open Question 2
- Question: What is the impact of different GNN architectures on the effectiveness of the proposed detection method?
- Basis in paper: [explicit] The paper mentions using GCN, GIN, and GAT in the construction of graph classifiers but does not provide a detailed comparison of how different architectures affect the detection performance.
- Why unresolved: The effectiveness of the detection method may vary depending on the GNN architecture used, but the paper does not explore this aspect in detail.
- What evidence would resolve it: Comparative results showing the detection performance using different GNN architectures on the same datasets and attack scenarios.

### Open Question 3
- Question: How does the choice of clean validation threshold percentiles affect the optimal Number of Positive Metrics Required (NPMR) for backdoor detection?
- Basis in paper: [explicit] The paper discusses using the 25th/75th percentile of clean validation data to establish thresholds for making predictions and mentions that the optimal NPMR changes under different thresholding settings.
- Why unresolved: The paper does not provide a comprehensive analysis of how varying the threshold percentiles affects the optimal NPMR and the overall detection performance.
- What evidence would resolve it: Results showing the optimal NPMR and detection performance across a range of threshold percentiles, allowing for a better understanding of the trade-offs involved.

## Limitations
- The method's effectiveness depends on the assumption that backdoor triggers consistently affect GNN explanation outputs in detectable ways
- The computational overhead of running GNN explanations on all training graphs may be prohibitive for large-scale applications
- The method requires clean validation data for threshold establishment, which may not always be available in practice

## Confidence
- High confidence: The core detection mechanism using multiple explanation-derived metrics is sound and well-validated across multiple datasets and attack types
- Medium confidence: The adaptive attack's effectiveness against the detection method is demonstrated, but the specific implementation details are sparse
- Medium confidence: The composite metric voting approach is effective but may require dataset-specific tuning of the NPMR threshold

## Next Checks
1. Test the detection method on additional graph datasets (particularly larger ones) to assess scalability and generalizability beyond the five tested datasets
2. Evaluate the method's performance when clean validation data is limited or when the validation data distribution differs from the training data
3. Investigate the computational cost and runtime performance when applying the method to graphs with significantly more nodes and edges than those tested (e.g., graphs with >10,000 nodes)