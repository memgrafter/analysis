---
ver: rpa2
title: 'UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large
  Language Models'
arxiv_id: '2402.10052'
source_url: https://arxiv.org/abs/2402.10052
tags:
- language
- unlearning
- data
- imagination
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces UnDIAL, a self-distillation method for LLM\
  \ unlearning that avoids the instability of gradient ascent approaches. Instead\
  \ of directly suppressing memorized tokens, UnDIAL adjusts logits by increasing\
  \ non-memorized tokens\u2019 probabilities, guiding the model to \"imagine\" alternative\
  \ responses."
---

# UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models

## Quick Facts
- arXiv ID: 2402.10052
- Source URL: https://arxiv.org/abs/2402.10052
- Reference count: 40
- Primary result: Achieves effective unlearning (EL3: 0.01-0.02) while maintaining high language quality (MAUVE > 0.7) on GPT-Neo models

## Executive Summary
This paper introduces UnDIAL, a self-distillation method for LLM unlearning that avoids the instability of gradient ascent approaches. Instead of directly suppressing memorized tokens, UnDIAL adjusts logits by increasing non-memorized tokens' probabilities, guiding the model to "imagine" alternative responses. Experiments on GPT-Neo models show that UnDIAL achieves effective unlearning with low memorization accuracy while maintaining high language generation quality and stable training dynamics. The method is scalable to larger models and compatible with parameter-efficient fine-tuning like LoRA, offering a robust solution for privacy-preserving LLM applications.

## Method Summary
UnDIAL is a self-distillation method that addresses the instability of gradient ascent approaches in LLM unlearning. Rather than directly suppressing memorized tokens, it adjusts logits by increasing non-memorized tokens' probabilities, guiding the model to "imagine" alternative responses. The method uses a teacher-student framework where the teacher model's logits are modified by adding a constant γ to all positions except the memorized token, effectively lowering the probability of the memorized token relative to others. The student model then learns from this adjusted distribution, internalizing the preference for non-memorized tokens. This approach ensures smooth convergence and avoids catastrophic forgetting, even in challenging unlearning tasks.

## Key Results
- UnDIAL achieves effective unlearning with memorization accuracy (EL3) as low as 0.01-0.02
- Maintains high language generation quality with MAUVE scores consistently above 0.7
- Provides stable training dynamics compared to gradient ascent methods
- Scales effectively to larger models and works with parameter-efficient fine-tuning (LoRA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-distillation with adjusted logits selectively reduces memorization by increasing non-memorized token logits.
- Mechanism: The method modifies the teacher model's logits by adding a constant γ to all positions except the memorized token, effectively lowering the probability of the memorized token relative to others. The student model then learns from this adjusted distribution, internalizing the preference for non-memorized tokens.
- Core assumption: Increasing non-memorized token logits in the teacher model biases the student model away from memorizing those tokens.
- Evidence anchors:
  - [abstract] "Instead of directly suppressing memorized tokens, UnDIAL adjusts logits by increasing non-memorized tokens' probabilities"
  - [section] "Given its memorization nature, the token it = argmaxzt[i] that has the highest logit output indicates the token that is memorized. Then, we increase all the pre-softmax logits by the so-called imagination strength coefficient γ except at coordinate it."
  - [corpus] Found 25 related papers. Average neighbor FMR=0.443. No citations found.
- Break condition: If γ is too large, the model may generate unnatural text; if too small, memorization may not be sufficiently reduced.

### Mechanism 2
- Claim: Selective imagination on specific token types (e.g., entities, nouns) preserves language generation quality better than uniform imagination.
- Mechanism: The method restricts imagination to a subset S of tokens (e.g., entities or nouns), avoiding functional words. This prevents disruption of grammatical structure while still reducing memorization of sensitive information.
- Core assumption: Functional words are necessary for grammatical coherence, so excluding them from imagination preserves generation quality.
- Evidence anchors:
  - [section] "We only enhance the imagination on a selected subset of tokens S. In this case, the loss function becomes: L = PT t=1 I(t ∈ S)H(zt + γ1it , zs)"
  - [abstract] "Our approach leverages self-distillation to adjust logits and selectively reduce the influence of targeted tokens."
  - [corpus] Weak corpus evidence; no direct citations on selective token type imagination.
- Break condition: If the subset S is too small or poorly chosen, unlearning effectiveness may be reduced.

### Mechanism 3
- Claim: The self-distillation framework ensures stable convergence and avoids catastrophic forgetting.
- Mechanism: The student model learns from the adjusted teacher logits, gradually approximating the teacher's distribution. This iterative process maintains language capabilities while reducing memorization.
- Core assumption: The teacher-student dynamic in self-distillation provides a stable training signal that balances unlearning and retention.
- Evidence anchors:
  - [abstract] "This technique ensures smooth convergence and avoids catastrophic forgetting, even in challenging unlearning tasks"
  - [section] "we allow the student model to learn from this modified teacher. The student model is trained to emulate the teacher's adjusted output probabilities"
  - [corpus] No direct corpus evidence on stability; assumption based on general self-distillation literature.
- Break condition: If the learning rate is too high, the student may diverge from the teacher; if too low, convergence may be too slow.

## Foundational Learning

- Concept: Cross-entropy loss and its role in language model training
  - Why needed here: Understanding how loss functions guide model behavior is essential for grasping why adjusting logits affects memorization.
  - Quick check question: What happens to the model's predictions when you minimize cross-entropy loss on a target distribution?

- Concept: Self-distillation and teacher-student training dynamics
  - Why needed here: The method relies on a teacher model to guide the student, so understanding this framework is crucial.
  - Quick check question: How does the student model's behavior change as it approximates the teacher's output distribution?

- Concept: Token-level probability manipulation in autoregressive models
  - Why needed here: The method adjusts logits at the token level, so understanding how this affects generation is key.
  - Quick check question: What is the effect of increasing a token's logit on its probability in the softmax output?

## Architecture Onboarding

- Component map:
  - Base model (teacher) → Logits adjustment (add γ to non-memorized tokens) → Student model (learns from adjusted logits)
  - Subset S selection module (optional, for selective imagination)
  - Loss computation module (cross-entropy between adjusted teacher and student logits)

- Critical path:
  1. Identify memorized tokens in the forget set.
  2. Adjust teacher logits by adding γ to non-memorized tokens.
  3. Compute cross-entropy loss between adjusted teacher and student logits.
  4. Update student model parameters via backpropagation.

- Design tradeoffs:
  - Larger γ improves unlearning but may degrade generation quality.
  - Selective imagination (subset S) preserves coherence but may reduce unlearning effectiveness.
  - Full fine-tuning vs. parameter-efficient fine-tuning (e.g., LoRA) affects computational cost and performance.

- Failure signatures:
  - High repetition rates in generated text (γ too large).
  - Persistent memorization (γ too small or subset S too restrictive).
  - Unstable training dynamics (learning rate too high).

- First 3 experiments:
  1. Test γ values (3, 10, 30) on a small model to find the sweet spot between unlearning and generation quality.
  2. Compare selective imagination (subset S) vs. uniform imagination on memorization reduction.
  3. Evaluate full fine-tuning vs. LoRA on a medium-sized model for computational efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does deliberate imagination scale effectively to larger models beyond the 2.7B parameter range tested?
- Basis in paper: [inferred] The authors note they were "constrained by computational limitations" and that "Future work could extend this exploration to other (and even larger) model families such as Llama 2 and Mistral."
- Why unresolved: The paper only tested up to 2.7B parameters and used GPT-Neo specifically. No experiments were conducted on models with hundreds of billions of parameters or other architectures.
- What evidence would resolve it: Experimental results showing DI performance on models like GPT-4, Llama 2-70B, or Claude on similar unlearning benchmarks.

### Open Question 2
- Question: How does deliberate imagination perform when unlearning multiple sequential requests or continuous unlearning scenarios?
- Basis in paper: [explicit] The authors claim DI "ensures smooth convergence and avoids catastrophic forgetting, even in challenging unlearning tasks with large datasets and sequential unlearning requests" but don't provide empirical evidence for sequential unlearning.
- Why unresolved: The experiments only show single-shot unlearning. No data is presented on how the model performs when asked to forget multiple different datasets sequentially.
- What evidence would resolve it: Experiments measuring DI performance when applying multiple, consecutive unlearning tasks and tracking degradation in language capabilities across the sequence.

### Open Question 3
- Question: What is the optimal strategy for selecting the subset S of tokens to apply deliberate imagination to?
- Basis in paper: [explicit] The authors note "we only enhance the imagination on a selected subset of tokens S" and suggest "A simple choice for the set S can be taking S as all the entities or nouns" but leave "further explorations on more sophisticated construction of S to future work."
- Why unresolved: The paper only compares two variants (nouns vs entities) without exploring other token selection strategies or providing theoretical justification for why certain tokens should be selected.
- What evidence would resolve it: Comparative experiments testing different token selection strategies (e.g., named entities, rare words, function words, or learned importance scores) and their impact on unlearning effectiveness and language preservation.

## Limitations
- Dataset dependency and reproducibility issues due to reliance on a specific forget set that is not publicly available in full detail
- Hyperparameter sensitivity with a narrow optimal γ range that may indicate brittleness across different model scales or domains
- Limited cross-model generalizability as all experiments use only GPT-Neo models

## Confidence

**High confidence**: The core mechanism of self-distillation with adjusted logits is theoretically sound and the experimental methodology is rigorous. The reported trends are internally consistent.

**Medium confidence**: The scalability claims to larger models and LoRA compatibility are supported by experiments but limited to three model sizes. The stability claims relative to gradient ascent methods are reasonable but could benefit from more extensive comparison.

**Low confidence**: Claims about optimal γ values and subset S selection lack systematic validation. The method's performance in real-world deployment scenarios is not evaluated.

## Next Checks

1. **Dataset generalization test**: Apply UnDIAL to a different unlearning dataset (e.g., Wikipedia sentences containing PII, medical records) and evaluate whether EL3 and MAUVE scores remain within the reported ranges. This would validate whether the method works beyond the specific extraction attack scenario.

2. **Hyperparameter sensitivity analysis**: Systematically vary γ (e.g., 1, 3, 10, 30, 50) and learning rates (1e-6 to 1e-4) across all three model sizes, measuring the full tradeoff surface between unlearning effectiveness (EL3) and generation quality (MAUVE, repetition). This would quantify the method's robustness to hyperparameter choices.

3. **Architecture transfer experiment**: Implement UnDIAL on a different LLM architecture (e.g., OPT or LLaMA) and evaluate whether the same γ ranges and training procedures work, or if architecture-specific tuning is required. This would test the method's generalizability beyond GPT-Neo.