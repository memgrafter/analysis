---
ver: rpa2
title: 'Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference'
arxiv_id: '2408.07802'
source_url: https://arxiv.org/abs/2408.07802
tags:
- kraken
- parallelism
- attention
- layer
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Kraken, a Transformer architecture variant
  designed to reduce latency in multi-device inference by introducing a fixed degree
  of intra-layer model parallelism. This allows collective operations to be overlapped
  with computation, improving hardware utilization.
---

# Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference

## Quick Facts
- arXiv ID: 2408.07802
- Source URL: https://arxiv.org/abs/2408.07802
- Reference count: 40
- Primary result: Kraken achieves a mean 35.6% speedup in Time To First Token during multi-device inference compared to standard Transformers.

## Executive Summary
This paper introduces Kraken, a Transformer architecture variant designed to reduce latency in multi-device inference by introducing a fixed degree of intra-layer model parallelism. This allows collective operations to be overlapped with computation, improving hardware utilization. Kraken models are trained on OpenWebText and evaluated on SuperGLUE, showing similar language modeling capabilities to standard Transformers. When tested on multi-GPU systems using TensorRT-LLM, Kraken achieves a mean 35.6% speedup in Time To First Token across various model sizes, context lengths, and parallelism degrees.

## Method Summary
Kraken introduces a fixed degree of intra-layer model parallelism by replacing each full Transformer layer with N parallel sub-layers. Each sub-layer performs independent Multi-Head Attention and FeedForward Network operations. The outputs are aggregated using a single AllReduce operation at the end of each layer, which is overlapped with the computation of the next layer's Multi-Head Attention. This design reduces the critical path latency by removing most inter-device communication from the latency bottleneck. Kraken models are pretrained on OpenWebText and evaluated on SuperGLUE to ensure language modeling capabilities are preserved while achieving significant latency reductions during inference on multi-GPU systems using TensorRT-LLM.

## Key Results
- Kraken models achieve similar perplexity and SuperGLUE benchmark performance to standard Transformers when trained on OpenWebText.
- Kraken achieves a mean 35.6% speedup in Time To First Token (TTFT) during multi-device inference compared to standard Transformers across various model sizes, context lengths, and parallelism degrees.
- The latency reduction is achieved by overlapping AllReduce operations with Multi-Head Attention computation, effectively removing inter-device communication from the critical path.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fixed intra-layer parallelism enables overlapping collective communication with computation, removing AllReduce from the critical path.
- Mechanism: By replacing each full Transformer layer with N parallel sub-layers, each sub-layer performs independent Multi-Head Attention and FeedForward Network operations. Only a single AllReduce is required at the end of each layer, and since the output is used in the FFN of the next layer, not the MHA, the MHA computation can proceed while the AllReduce executes.
- Core assumption: The AllReduce operation's latency is non-negligible and overlaps well with MHA compute.
- Evidence anchors:
  - [abstract] "By introducing a fixed degree of intra-layer model parallelism, the architecture allows collective operations to be overlapped with compute, decreasing latency and increasing hardware utilization."
  - [section 3.1] "As shown in Figure 3, this allows for overlapping the compute in the MHA block with the AllReduce, effectively removing most inter-device communication from the critical path."
  - [corpus] Weak: Corpus papers discuss general communication-efficient inference but none specifically validate the overlap pattern claimed here.
- Break condition: If AllReduce latency becomes negligible compared to MHA compute, or if hardware cannot overlap communication and computation, the speedup disappears.

### Mechanism 2
- Claim: Kraken preserves language modeling performance while reducing latency.
- Mechanism: The sub-layer structure maintains the same overall parameter budget as standard Transformers by reducing hidden state expansion and adjusting embedding dimensions. This ensures that model capacity is preserved while enabling parallelism.
- Core assumption: Adjusting hyperparameters to match parameter counts preserves representational power.
- Evidence anchors:
  - [abstract] "When trained on OpenWebText, Kraken models reach a similar perplexity as standard Transformers while also preserving their language modeling capabilities when evaluated on the SuperGLUE benchmark."
  - [section 3.2] "Using a configuration of a standard Transformer as the basis, we make the following two modifications to derive a Kraken configuration that has the same number of parameters."
  - [corpus] Weak: No corpus neighbor papers provide direct evidence that similar architectural changes preserve perplexity.
- Break condition: If the parameter budget adjustment fails to maintain sufficient model capacity, performance degrades.

### Mechanism 3
- Claim: Kraken's design complements multi-device hardware topologies.
- Mechanism: By fixing the degree of parallelism at training time, Kraken maps naturally onto nodes with predictable device counts (e.g., 4-way or 8-way GPU nodes), avoiding runtime partitioning decisions and reducing coordination overhead.
- Core assumption: Multi-device systems typically have homogeneous device counts per node.
- Evidence anchors:
  - [section 2.3] "A typical device adopts a wide, Single Instruction, Multiple Data (SIMD) architecture and consists of several compute cores/blocks along with on-chip, volatile memory and off-chip Dyanmic Random Access Memory (DRAM) or High Bandwidth Memory (HBM)."
  - [section 3.1] "The architecture is designed to complement the topology of multi-device setups such as nodes in typical datacenters and DGX (12) systems."
  - [corpus] Weak: Corpus papers discuss general multi-device inference but do not validate the topology-compatibility claim.
- Break condition: If deployed on systems with mismatched device counts, performance gains may be lost or negated by inefficient partitioning.

## Foundational Learning

- Concept: Tensor parallelism and AllReduce operations in Transformer inference.
  - Why needed here: Understanding how existing tensor parallelism introduces collective communication and its cost is essential to grasping why Kraken's approach reduces latency.
  - Quick check question: What are the two AllReduce operations introduced per layer in standard tensor parallelism schemes, and why are they expensive?

- Concept: Multi-Head Attention and its computational structure.
  - Why needed here: Kraken replaces full layers with parallel sub-layers containing MHA; knowing how MHA works and why it's parallelizable is critical.
  - Quick check question: In a standard Transformer, how is the output of Multi-Head Attention computed, and why can the heads be partitioned across devices?

- Concept: Model parallelism vs. pipeline parallelism.
  - Why needed here: Kraken uses model parallelism; distinguishing it from pipeline parallelism clarifies why it's suited for latency reduction rather than throughput.
  - Quick check question: How does model parallelism differ from pipeline parallelism in terms of data flow and latency characteristics during inference?

## Architecture Onboarding

- Component map: Input -> N parallel sub-layers (each with MHA and FFN) -> AllReduce -> LayerNorm -> Output
- Critical path: In Kraken, the critical path per layer is: sub-layer MHA → element-wise addition of AllReduce output → LayerNorm → FFN. The AllReduce is overlapped with MHA of the next layer.
- Design tradeoffs: Fixed parallelism degree simplifies mapping but limits flexibility; increased KV cache size per layer; requires pretraining; potential memory overhead mitigated by using Multi-Query Attention.
- Failure signatures: If latency does not improve, check: (1) whether AllReduce truly overlaps with MHA, (2) if the hardware supports concurrent communication/compute, (3) if the parallelism degree mismatches the deployment hardware.
- First 3 experiments:
  1. Compare Kraken vs. standard Transformer on a small 4-way GPU setup, measuring TTFT with TensorRT-LLM to confirm latency reduction.
  2. Profile operator runtimes (GEMM, AllReduce) to verify that AllReduce is no longer on the critical path.
  3. Test Kraken with Multi-Query Attention to evaluate KV cache memory savings and any impact on performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Kraken models compare to state-of-the-art Transformer training recipes or alternative architectures?
- Basis in paper: [inferred] The authors mention they do not compare with more state-of-the-art Transformer training recipes or alternatives due to limited access to GPU compute.
- Why unresolved: The authors did not have access to the necessary compute resources to conduct these comparisons.
- What evidence would resolve it: Training Kraken models using more advanced training techniques and comparing their performance to other state-of-the-art models on various benchmarks.

### Open Question 2
- Question: How does the performance of Kraken models scale with larger model sizes and longer training durations?
- Basis in paper: [explicit] The authors mention they did not exhaustively search for hyperparameters and stopped training at 150 billion tokens, in contrast to the typical 300 billion tokens for language models of such sizes.
- Why unresolved: The authors limited the training duration due to resource constraints.
- What evidence would resolve it: Training Kraken models for longer durations and on more diverse datasets to evaluate their performance on newer, more complex language modeling benchmarks.

### Open Question 3
- Question: How can the increased memory footprint of Kraken layers be mitigated, especially at long sequence lengths?
- Basis in paper: [explicit] The authors discuss the increased memory footprint of Kraken layers compared to standard Transformers and suggest using Multi-Query Attention or Grouped-Query Attention as potential solutions.
- Why unresolved: The authors only briefly mention these solutions without providing a comprehensive analysis of their effectiveness.
- What evidence would resolve it: Conducting experiments to compare the memory usage and performance of Kraken models using different attention mechanisms at various sequence lengths and batch sizes.

## Limitations
- Evaluation is limited to a single inference backend (TensorRT-LLM), leaving uncertainty about performance portability across frameworks.
- The paper does not explore dynamic parallelism or runtime load balancing, which may be necessary for heterogeneous device setups.
- KV cache growth (proportional to the number of sub-layers) is not thoroughly analyzed for memory-constrained scenarios, though Multi-Query Attention partially mitigates this.

## Confidence
- **High confidence**: The latency reduction mechanism (overlapping AllReduce with MHA compute) is technically sound and supported by the architectural description and TensorRT-MLM results.
- **Medium confidence**: The claimed speedup (35.6% mean reduction in TTFT) is plausible given the backend optimizations, but the lack of cross-framework validation limits generalizability.
- **Low confidence**: The language modeling equivalence is asserted based on perplexity and SuperGLUE metrics, but the absence of ablations or comparisons to alternative parallelism schemes weakens this claim.

## Next Checks
1. Implement Kraken in a second inference framework (e.g., FasterTransformer or vLLM) and measure TTFT speedup to confirm backend independence.
2. Profile memory usage for varying parallelism degrees and context lengths to quantify the trade-off between latency reduction and memory overhead.
3. Extend Kraken to support runtime parallelism degree adjustment and evaluate performance on heterogeneous device clusters.