---
ver: rpa2
title: Reconciling Kaplan and Chinchilla Scaling Laws
arxiv_id: '2406.12907'
source_url: https://arxiv.org/abs/2406.12907
tags:
- kaplan
- chinchilla
- scaling
- coefficients
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a discrepancy between the scaling laws for
  transformer language models reported by Kaplan et al. (2020) and Hoffmann et al.
---

# Reconciling Kaplan and Chinchilla Scaling Laws

## Quick Facts
- arXiv ID: 2406.12907
- Source URL: https://arxiv.org/abs/2406.12907
- Authors: Tim Pearce; Jinyeop Song
- Reference count: 2
- Primary result: Kaplan's overestimation of scaling coefficients (0.73) versus Chinchilla's (0.50) is explained by counting only non-embedding parameters and analyzing at small scales

## Executive Summary
This paper resolves a significant discrepancy between two landmark scaling law studies in language model research. Kaplan et al. (2020) found that optimal model parameters scale as N* ∝ C^0.73 with compute budget, while Hoffmann et al. (2022) reported N* ∝ C^0.50. The authors demonstrate that much of this difference stems from Kaplan counting only non-embedding parameters while studying at smaller scales where embeddings constitute a non-negligible fraction of total parameters. Through synthetic data generation and small-scale experiments, they show that using total parameter counts yields coefficients matching Chinchilla's findings, reaffirming the importance of including all parameters in scaling analyses.

## Method Summary
The authors employ a multi-pronged approach to investigate the scaling law discrepancy. First, they generate synthetic training curves using Chinchilla's parameters and an analytical loss function that incorporates the relationship between non-embedding and total parameters (NT = N_E + γN^(1/3)_E). Second, they conduct small-scale language model training experiments (0.8M-4.6M parameters) on BookCorpus data using GPT-2 tokenizer, varying learning rates and training durations to establish compute-optimal frontiers. Third, they fit power laws to these frontiers for both total and non-embedding parameter counts, comparing the resulting coefficients to Kaplan's 0.73 and Chinchilla's 0.50 values. The analysis reveals that local power law fitting at small scales produces coefficients close to Kaplan's findings when using non-embedding parameters, while total parameter counting consistently yields coefficients matching Chinchilla's.

## Key Results
- The discrepancy between Kaplan (0.73) and Chinchilla (0.50) scaling coefficients is primarily explained by parameter counting methodology
- Local power law fitting at small scales with non-embedding parameters yields coefficients of 0.78-0.74, close to Kaplan's 0.73
- Using total parameters in scaling studies produces coefficients matching Chinchilla's 0.50 value
- The relationship NT = N_E + γN^(1/3)_E between non-embedding and total parameters is sufficient to explain the observed scaling behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The discrepancy between Kaplan and Chinchilla scaling coefficients arises primarily from Kaplan counting only non-embedding parameters while studying at small scales.
- Mechanism: At small parameter counts, embedding parameters constitute a non-negligible fraction of total parameters. Kaplan's exclusion of these parameters biased their scaling analysis. When the same analysis is performed at larger scales where embeddings become negligible, the coefficients align.
- Core assumption: The relationship between non-embedding and total parameters follows the form NT = N_E + γN^(1/3)_E, where γ depends on vocabulary size and context length.
- Evidence anchors:
  - [abstract] "much of this discrepancy can be attributed to Kaplan counting non-embedding rather than total parameters, combined with their analysis being performed at small scale"
  - [section] "We find that much of this discrepancy can be attributed to Kaplan counting non-embedding rather than total parameters, combined with their analysis being performed at small scale"
  - [corpus] Weak evidence - related papers discuss compute-optimal scaling but don't directly address the parameter counting mechanism
- Break condition: If the relationship between embedding and non-embedding parameters doesn't follow the proposed form, or if embeddings contribute differently to scaling than assumed.

### Mechanism 2
- Claim: The local power law approximation at small scales produces coefficients close to Kaplan's findings.
- Mechanism: When analyzing the relationship between N* and C at small parameter counts, the theoretical relationship is not a pure power law due to the contribution of embedding parameters. Fitting a local power law to this curved relationship yields coefficients around 0.74-0.78, close to Kaplan's 0.73.
- Core assumption: A local power law approximation can accurately capture the scaling relationship in the small parameter regime.
- Evidence anchors:
  - [section] "fitting a 'local' power law at this small scale, produces a coefficient that is close to Kaplan's"
  - [section] "we find local scaling coefficients, Epoch AI specification: N* ∝∼ C^0.78, Chinchilla specification: N* ∝∼ C^0.74, which are close to the Kaplan coefficient of 0.73"
  - [corpus] Weak evidence - related papers discuss scaling laws but don't address the local power law approximation mechanism
- Break condition: If the true relationship between N* and C at small scales deviates significantly from a power law, or if the local approximation fails to capture the behavior accurately.

### Mechanism 3
- Claim: Using total parameters in scaling studies eliminates the bias and produces coefficients matching Chinchilla's findings.
- Mechanism: When scaling studies use total parameter counts (including embeddings) at any scale, the coefficients converge to those reported by Chinchilla (around 0.50), as embeddings become properly accounted for in the analysis.
- Core assumption: Total parameter count is the appropriate basis for scaling analysis across all scales.
- Evidence anchors:
  - [abstract] "Hence, this note reaffirms Chinchilla's scaling coefficients, by explaining the cause of Kaplan's original overestimation"
  - [section] "our Experiment 1 finding NT ∝ C^0.49 even for NT < 5M"
  - [corpus] Weak evidence - related papers discuss scaling laws but don't specifically address the impact of parameter counting methodology
- Break condition: If embeddings contribute differently to scaling than assumed, or if other factors dominate the scaling behavior.

## Foundational Learning

- Concept: Power laws and scaling relationships
  - Why needed here: The paper revolves around understanding and deriving power law relationships between model parameters, training tokens, and compute budget
  - Quick check question: What is the general form of a power law relationship between two variables X and Y?

- Concept: Parameter counting in neural networks
  - Why needed here: Understanding the distinction between total parameters and non-embedding parameters is crucial to grasping the paper's main argument
  - Quick check question: How do you calculate the total number of parameters in a transformer model?

- Concept: Compute-optimal training
  - Why needed here: The paper discusses how to optimally allocate compute between model size and training data
  - Quick check question: What is the relationship between compute budget, model parameters, and training tokens in language model training?

## Architecture Onboarding

- Component map:
  Transformer architecture (layers, attention mechanisms) -> Embedding layers (vocabulary and positional embeddings) -> Parameter counting methodology -> Scaling law analysis framework

- Critical path:
  1. Understanding the transformer architecture and parameter composition
  2. Grasping the distinction between total and non-embedding parameters
  3. Following the mathematical derivation of the scaling relationship
  4. Reproducing the synthetic data experiments
  5. Validating the findings through small-scale training experiments

- Design tradeoffs:
  - Parameter counting method (total vs non-embedding) affects scaling coefficients
  - Model size range studied influences the observed scaling behavior
  - Choice of loss function and dataset impacts scaling law estimation

- Failure signatures:
  - Incorrect parameter counting leading to biased scaling coefficients
  - Insufficient model size range causing inaccurate local power law fitting
  - Inappropriate loss function or dataset selection affecting scaling relationship

- First 3 experiments:
  1. Reproduce the synthetic data generation and local power law fitting from Figure 4
  2. Implement the small-scale training experiments comparing total vs non-embedding parameter counting
  3. Validate the analytical relationship between N* and C by computing the derivative and comparing to empirical results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise functional form of the relationship between non-embedding parameters (N_E) and total parameters (NT) that explains the scaling coefficient discrepancy?
- Basis in paper: [explicit] The paper proposes NT = N_E + γN^(1/3)_E as a suitable function and shows it can be motivated from both Kaplan and Chinchilla studies
- Why unresolved: The paper empirically fits this function but acknowledges that the exponent δ ≈ 0.34 (close to 1/3) is derived from Chinchilla model configurations, and the exact relationship may vary across different model architectures
- What evidence would resolve it: Direct measurements of parameter counts across a wide range of transformer models of different sizes and architectures to verify the functional form and determine if γ and δ are consistent across studies

### Open Question 2
- Question: How do embedding parameters contribute to scaling behavior compared to non-embedding parameters?
- Basis in paper: [inferred] The paper discusses that embedding parameters capture meaningful language properties like semantic factors and spatial/temporal representations, suggesting they contribute more than just translation
- Why unresolved: While the paper argues embedding parameters should be included in parameter counts, it doesn't quantify their relative importance or how they specifically affect the scaling relationship
- What evidence would resolve it: Experiments systematically varying embedding parameter counts while holding non-embedding parameters constant, and measuring the impact on loss scaling and compute efficiency

### Open Question 3
- Question: To what extent do other differences between Kaplan and Chinchilla studies (datasets, optimization schemes, computation counting) affect scaling coefficients?
- Basis in paper: [explicit] The authors acknowledge these differences but claim their preliminary work suggests these factors impact coefficients in a more minor way than the N_E vs NT distinction
- Why unresolved: The paper focuses primarily on the parameter counting issue and doesn't provide comprehensive analysis of how other methodological differences contribute to coefficient variation
- What evidence would resolve it: Controlled experiments varying one methodological aspect at a time (e.g., switching datasets while keeping all else constant) to isolate the contribution of each factor to scaling coefficient differences

## Limitations
- The analysis relies on an assumed relationship between non-embedding and total parameters that hasn't been independently verified across different model architectures
- The synthetic data experiments use Chinchilla's own parameters and loss function, creating potential circularity
- The small-scale experiments (0.8M-4.6M parameters) may not fully capture behavior at larger scales where embeddings become truly negligible

## Confidence

- High confidence: The mathematical derivation showing how counting non-embedding vs total parameters affects scaling coefficients is rigorous and well-supported by both theory and experiments.

- Medium confidence: The claim that Kaplan's overestimation was primarily due to parameter counting methodology and small-scale analysis, rather than other factors like optimization schemes or dataset choices.

- Medium confidence: The recommendation to use total parameters in future scaling studies, while reasonable, may oversimplify the complexity of parameter importance across different model components.

## Next Checks
1. **Architecture-specific validation**: Test whether the non-embedding to total parameter relationship holds across different transformer variants (e.g., GPT, BERT, OPT architectures) with varying embedding dimensions and context lengths.

2. **Scale extrapolation verification**: Validate the proposed mechanism at larger scales (10B+ parameters) where embeddings should become negligible, confirming whether the scaling coefficients converge to Chinchilla's values regardless of counting method.

3. **Optimization scheme impact**: Systematically vary optimization hyperparameters (learning rate schedules, batch sizes) in the small-scale experiments to determine if different optimization schemes produce materially different scaling coefficients beyond what parameter counting explains.