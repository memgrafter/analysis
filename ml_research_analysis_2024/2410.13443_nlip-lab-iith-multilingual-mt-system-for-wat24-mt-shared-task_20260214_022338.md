---
ver: rpa2
title: NLIP_Lab-IITH Multilingual MT System for WAT24 MT Shared Task
arxiv_id: '2410.13443'
source_url: https://arxiv.org/abs/2410.13443
tags:
- deva
- chrf
- languages
- language
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes NLIP Lab's multilingual machine translation
  system for the WAT24 shared task on multilingual Indic MT task for 22 scheduled
  languages belonging to 4 language families. We explore pre-training for Indic languages
  using alignment agreement objectives.
---

# NLIP_Lab-IITH Multilingual MT System for WAT24 MT Shared Task
## Quick Facts
- arXiv ID: 2410.13443
- Source URL: https://arxiv.org/abs/2410.13443
- Reference count: 7
- Achieved average chrF++ score of 46.80 and BLEU score of 18.19 for En-Indic direction in IN22-Gen benchmark

## Executive Summary
This paper presents NLIP Lab's multilingual machine translation system for the WAT24 shared task, focusing on 22 Indic languages across 4 language families. The system uses a 243M parameter multilingual translation model with alignment agreement pre-training and fine-tuning on high-quality seed data. The approach achieved competitive results with IndicTransv1, demonstrating effectiveness in handling the complexity of multilingual Indic translation.

## Method Summary
The system employs a Transformer big architecture (6 encoder/decoder layers, 1024 embedding size, 4096 FFN, 16 attention heads) with a two-stage training approach. First, pre-training is performed on a reduced BPCC corpus (282M to ~56M sentences) using alignment agreement objectives, where English words are substituted with equivalent Indic words from bilingual dictionaries. Second, the pre-trained model is fine-tuned on small, high-quality seed data (2.26M sentences) from curated sources. Temperature sampling (T=5) is used to handle data skew and ensure adequate representation of low-resource languages.

## Key Results
- In IN22-Gen benchmark: 46.80 chrF++ and 18.19 BLEU for En-Indic; 56.34 chrF++ and 30.82 BLEU for Indic-En
- In IN22-Conv benchmark: 43.43 chrF++ and 16.58 BLEU for En-Indic; 52.44 chrF++ and 29.77 BLEU for Indic-En
- Model is competitive with IndicTransv1 (474M parameters) despite having fewer parameters (243M)

## Why This Works (Mechanism)
### Mechanism 1: Alignment Agreement Pre-training
Claim: Alignment agreement pre-training with bilingual dictionary substitution improves multilingual model performance by bringing semantically similar embeddings closer together.
Mechanism: During pre-training, English words are randomly substituted with corresponding Indic language words from bilingual dictionaries, creating code-switched augmented sentences. This forces the model to learn aligned representations across languages.
Core assumption: Semantic alignment through dictionary-based substitution effectively reduces embedding space distances between translation equivalents.

### Mechanism 2: Seed Data Fine-tuning
Claim: Fine-tuning on small, high-quality seed data significantly improves translation quality compared to using only large but potentially noisy parallel corpora.
Mechanism: The model first pre-trains on a reduced but representative corpus from BPCC, then fine-tunes on curated datasets (ILCI, NLLB Seed, Massive, Daily, Wiki) totaling 2.26M sentences.
Core assumption: Small, high-quality datasets contain cleaner translations and better coverage of language-specific phenomena than larger but noisier corpora.

### Mechanism 3: Temperature Sampling for Data Balancing
Claim: Temperature sampling (T=5) effectively addresses data skew by oversampling low-resource languages while preventing excessive oversampling of very low-resource languages.
Mechanism: The heuristic-based temperature sampling creates a distribution that gives more representation to low-resource languages than uniform sampling would, while avoiding the extreme oversampling that would occur with direct proportional sampling.
Core assumption: The temperature sampling parameter (T=5) creates an optimal balance between representation and computational feasibility.

## Foundational Learning
- **Transformer architecture fundamentals**: Understanding self-attention, positional encoding, and encoder-decoder structure is essential since the paper uses a standard sequence-to-sequence Transformer big model with 6 encoder and 6 decoder layers.
  - Quick check: What is the primary difference between the encoder and decoder in a standard transformer architecture?

- **Multilingual NMT and zero-shot translation**: Understanding how multilingual models manage multiple language pairs and potential zero-shot translation scenarios is crucial since the system handles 22 Indic languages in a single model.
  - Quick check: How does a multilingual NMT model handle translation between language pairs it wasn't explicitly trained on?

- **Evaluation metrics for MT (BLEU, chrF++)**: Understanding these metrics is necessary since the paper reports results using both BLEU and chrF++ scores, which measure different aspects of translation quality.
  - Quick check: What is the key difference between BLEU and chrF++ metrics in terms of what linguistic units they evaluate?

## Architecture Onboarding
- **Component map**: Transformer big model (6 encoder layers, 6 decoder layers, 1024 embedding size, 4096 feed-forward size, 16 attention heads) → Alignment augmentation module (dictionary substitution) → Pre-training on 282M sentences → Fine-tuning on 2.26M seed data → Evaluation
- **Critical path**: Pre-training with alignment augmentation → Fine-tuning on seed data → Evaluation on benchmark datasets
- **Design tradeoffs**: Reduced corpus size (113.65M to ~56M) for computational feasibility vs. potential loss of coverage; 243M parameters vs. larger models like IndicTransv2 (1B) for efficiency vs. potential performance
- **Failure signatures**: Poor performance on specific language pairs despite good overall scores; significant degradation in Indic-En direction compared to En-Indic; inconsistent performance across different benchmarks
- **First 3 experiments**:
  1. Train IndicRASP without alignment augmentation to establish baseline performance
  2. Train IndicRASP with alignment augmentation but without fine-tuning to isolate pre-training effect
  3. Train a baseline multilingual model without temperature sampling to measure impact of data balancing

## Open Questions the Paper Calls Out
1. How does the alignment agreement-based pre-training objective compare to other pre-training methods (e.g., using monolingual data, large language models) for multilingual Indic MT?
2. What is the impact of substituting words from target sentences instead of source sentences in the alignment augmentation technique?
3. How does the model's performance vary with different sampling temperatures during data sampling for low-resource languages?

## Limitations
- No ablation studies isolating the individual contributions of alignment augmentation and seed data fine-tuning
- Temperature sampling parameter (T=5) appears to be a heuristic choice without systematic evaluation
- Comparison with IndicTransv1 (474M parameters) rather than the more recent IndicTransv2 (1B parameters) limits relative performance assessment

## Confidence
- **High Confidence**: Transformer architecture implementation, data preprocessing steps (temperature sampling, corpus reduction), and overall training methodology are clearly specified and follow established practices
- **Medium Confidence**: Reported benchmark results are verifiable through standard evaluation metrics, but lack of ablation studies and comparative analysis with larger state-of-the-art models reduces confidence in claimed improvements
- **Low Confidence**: Claims about specific effectiveness of alignment agreement pre-training and temperature sampling (T=5) are not well-supported by empirical evidence within the paper

## Next Checks
1. **Ablation Study**: Train three versions of the model - (a) baseline without alignment augmentation or seed fine-tuning, (b) with alignment augmentation only, and (c) with both alignment augmentation and seed fine-tuning. Compare results to isolate which component contributes most to performance gains.

2. **Temperature Sampling Sensitivity Analysis**: Train models with different temperature parameters (T=2, T=5, T=10) and analyze how varying the data balancing affects translation quality across low-resource vs. high-resource languages.

3. **Comparative Analysis with Larger Models**: Evaluate the proposed 243M parameter model against the more recent 1B parameter IndicTransv2 model on the same benchmarks to provide a more comprehensive assessment of relative performance and computational efficiency tradeoffs.