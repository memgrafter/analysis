---
ver: rpa2
title: Distilling Reasoning Ability from Large Language Models with Adaptive Thinking
arxiv_id: '2404.09170'
source_url: https://arxiv.org/abs/2404.09170
tags:
- post-thinking
- reasoning
- pre-thinking
- rationale
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an adaptive thinking mechanism (ATM) for distilling
  reasoning capabilities from large language models (LLMs) to small language models
  (SLMs). ATM leverages a perception module to adaptively choose between pre-thinking
  and post-thinking strategies based on question complexity.
---

# Distilling Reasoning Ability from Large Language Models with Adaptive Thinking

## Quick Facts
- arXiv ID: 2404.09170
- Source URL: https://arxiv.org/abs/2404.09170
- Reference count: 40
- Primary result: Adaptive thinking mechanism (ATM) outperforms pre-thinking and post-thinking by up to 7.33% on StrategyQA

## Executive Summary
This paper addresses the challenge of distilling reasoning capabilities from large language models (LLMs) to small language models (SLMs) by proposing an adaptive thinking mechanism (ATM). The key insight is that different reasoning questions benefit from different thinking strategies, and a perception module can dynamically select between pre-thinking and post-thinking approaches based on question complexity. ATM achieves significant performance improvements across 12 reasoning tasks while maintaining efficiency through a plug-and-play design that generalizes to different model architectures and languages.

## Method Summary
ATM uses a perception module with soft prompt tuning to evaluate question complexity and generate adaptive prompts that guide the SLM in choosing between pre-thinking (generate rationale then answer) and post-thinking (generate answer then rationale) strategies. The perception module employs a cross-attention mechanism with learnable query tokens to process input questions and additional features like word count. The model is trained using a 5-fold cross-validation approach where baselines determine complexity labels for each question, and the final ATM model learns to adaptively switch thinking strategies based on these labels.

## Key Results
- ATM outperforms both pre-thinking and post-thinking mechanisms on 12 reasoning tasks
- Achieves up to 7.33% accuracy improvement on StrategyQA dataset
- Generalizes effectively to other models (Qwen2.5-0.5B, LLama3.2-1B) and languages (Chinese datasets)
- Reduces inference time compared to pure pre-thinking while maintaining high accuracy

## Why This Works (Mechanism)

### Mechanism 1: Escaping Rationale-Sensitivity
- Claim: Post-thinking enables answers to escape the rationale-sensitive problem by generating answers before rationales
- Mechanism: By restructuring the rationale-answer generation sequence into an answer-rationale sequence, the model no longer relies on rationale for answer generation
- Core assumption: The rationale generated after the answer primarily serves as an explanation rather than a necessary reasoning path
- Evidence anchors:
  - [abstract]: "the answer can escape from the adverse effects caused by minor errors in the rationale"
  - [section]: "Since the answer is generated prior to the rationale, the rationale primarily serves as an explanation for the answer, enabling the student SLM to avoid the adverse effects of minor errors in the rationale"
  - [corpus]: No direct corpus evidence found for this specific mechanism

### Mechanism 2: Error Amplification for Hard Samples
- Claim: Post-thinking functions as an error amplifier that helps the model focus on learning hard samples
- Mechanism: When the model generates incorrect answers, it attempts to justify them with unusual rationales, creating greater loss on incorrect samples
- Core assumption: The model will generate rationales that deviate further from correct reasoning when trying to justify wrong answers
- Evidence anchors:
  - [abstract]: "the rationale serves as an error amplifier to the answer, which makes the SLM focus on learning hard samples"
  - [section]: "When the SLM produces an incorrect answer, it attempts to generate an unusual rationale to justify the error... leading to greater loss on incorrect samples"
  - [corpus]: No direct corpus evidence found for this specific mechanism

### Mechanism 3: Adaptive Strategy Selection
- Claim: Adaptive thinking mechanism dynamically selects between pre-thinking and post-thinking based on question complexity
- Mechanism: A perception module evaluates question complexity and generates soft prompt tokens to guide the model in choosing the appropriate thinking strategy
- Core assumption: The model can accurately perceive question complexity and that different question types benefit from different reasoning strategies
- Evidence anchors:
  - [abstract]: "a plug-and-play adaptive-thinking mechanism is proposed with the aid of the soft prompt tuning to integrate the merits of the pre-thinking mechanism and post-thinking mechanism, in which a perception module is introduced to adaptively prompt SLM answer or think first based on perceiving the complexity of the questions"
  - [section]: "we propose the adaptive thinking mechanism (ATM), which dynamically selects between pre-thinking and post-thinking based on the SLMs' capabilities and the complexity of a given question"
  - [corpus]: No direct corpus evidence found for this specific mechanism

## Foundational Learning

- **Chain of thought (CoT) distillation**: Why needed here - This work builds on CoT distillation as the foundational approach for transferring reasoning capabilities from LLMs to SLMs
  - Quick check question: What is the primary difference between standard fine-tuning and CoT distillation in the context of this paper?

- **Rationale-sensitive problem**: Why needed here - Understanding this problem is crucial for grasping why post-thinking was developed as an alternative to pre-thinking
  - Quick check question: How does the rationale-sensitive problem manifest in pre-thinking mechanisms according to the paper?

- **Soft prompt tuning**: Why needed here - The perception module in ATM relies on soft prompt tuning to generate adaptive prompts based on question complexity
  - Quick check question: What is the key distinction between soft prompt tuning and standard prompt engineering?

## Architecture Onboarding

- **Component map**: Input question and features -> Perception module (cross-attention with query tokens) -> Soft prompt tokens -> Generation module (SLM) -> Output (answer-rationale or rationale-answer sequence)

- **Critical path**:
  1. Input question and additional features enter the perception module
  2. Perception module generates soft prompt tokens through cross-attention
  3. Soft prompt tokens are prepended to input tokens
  4. Generation module processes the augmented input
  5. Model outputs either answer-rationale or rationale-answer sequence based on perceived complexity

- **Design tradeoffs**:
  - Pre-thinking vs Post-thinking: Pre-thinking enables complex reasoning decomposition but suffers from rationale sensitivity; post-thinking avoids rationale sensitivity but may lose explicit reasoning capabilities
  - Additional complexity vs performance: The perception module adds parameters and complexity but enables dynamic strategy selection
  - Training efficiency vs effectiveness: Using 5-fold cross-validation for labeling assignment increases training time but improves label quality

- **Failure signatures**:
  - If ATM performs worse than either pure pre-thinking or post-thinking, the perception module may be failing to accurately assess complexity
  - If performance degrades on arithmetic tasks specifically, the model may be over-relying on post-thinking for complex reasoning tasks
  - If inference speed doesn't improve significantly, the post-thinking component may not being selected often enough

- **First 3 experiments**:
  1. Implement the perception module independently and test its ability to classify question complexity on a held-out validation set
  2. Compare pure post-thinking performance against pre-thinking on a balanced set of easy and hard questions to verify the error amplification effect
  3. Run an ablation study removing the perception module to quantify its contribution to overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ATM scale when applied to language models with significantly different architectures or sizes beyond GPT2-Large and T5-Large?
- Basis in paper: [explicit] The paper mentions testing ATM on Qwen2.5-0.5B and LLama3.2-1B, but does not extensively explore performance across a wider range of architectures and sizes.
- Why unresolved: The current experiments focus on a limited set of models, and the paper does not provide a comprehensive analysis of ATM's performance across diverse model architectures or scales.
- What evidence would resolve it: Extensive experiments evaluating ATM on a broader range of language models with varying architectures, sizes, and training objectives would provide insights into its scalability and generalizability.

### Open Question 2
- Question: What is the impact of different features (beyond word count) fed into the perception module on the model's performance and complexity?
- Basis in paper: [explicit] The paper briefly mentions that readability and Mean Dependency Distance (MDD) could be used as features, but does not provide a detailed analysis of their impact on the model's performance.
- Why unresolved: The paper only provides a high-level overview of the potential benefits of incorporating different features, but does not explore their specific effects on the model's performance or complexity.
- What evidence would resolve it: A comprehensive study evaluating the impact of various features (e.g., readability, MDD, linguistic complexity) on the perception module's performance and the overall model's complexity would provide insights into the optimal feature selection.

### Open Question 3
- Question: How does the performance of ATM change when using different teacher LLM models with varying reasoning capabilities?
- Basis in paper: [explicit] The paper mentions testing ATM with different teacher LLM models (e.g., gpt-3.5-turbo, GLM4), but does not provide a detailed analysis of the impact of teacher LLM quality on the student SLM's performance.
- Why unresolved: The current experiments focus on a limited set of teacher LLM models, and the paper does not explore the relationship between teacher LLM quality and the effectiveness of ATM.
- What evidence would resolve it: Extensive experiments evaluating ATM with a diverse range of teacher LLM models, including those with varying reasoning capabilities, would provide insights into the importance of teacher LLM quality for effective knowledge distillation.

## Limitations

- Empirical validation scope is limited to English-language tasks, with only 3 Chinese datasets tested for cross-linguistic generalization
- Mechanism verification relies on indirect evidence rather than targeted ablation studies isolating each proposed mechanism
- Architecture constraints include added computational overhead without detailed analysis of performance-complexity trade-offs
- Generalization to other domains beyond reasoning tasks remains unexplored

## Confidence

**High confidence**: The core claim that ATM outperforms both pre-thinking and post-thinking mechanisms on reasoning tasks is well-supported by extensive experiments across 12 datasets and two SLM architectures

**Medium confidence**: The claim about ATM generalizing to other models and languages has some support but is based on limited evidence (3 Chinese datasets, 1 different SLM architecture)

**Medium confidence**: The theoretical mechanisms (rationale-sensitivity escape, error amplification) are plausible but not directly validated through targeted experiments or ablation studies

**Low confidence**: Claims about ATM being "plug-and-play" and easily applicable to other models lack detailed implementation guidelines or evidence of successful application to diverse architectures

## Next Checks

1. **Ablation study for mechanism isolation**: Run experiments that isolate each proposed mechanism by creating variants of ATM where only one mechanism is active at a time (e.g., a version that uses post-thinking but disables the error amplification effect, or a version that uses adaptive selection but keeps pre-thinking only). This would quantify each mechanism's individual contribution to performance gains.

2. **Cross-linguistic generalization analysis**: Extend the language generalization experiments beyond the current 3 Chinese datasets to include diverse language families (e.g., non-Latin scripts, agglutinative languages). Additionally, analyze which specific aspects of ATM transfer across languages versus which require language-specific adaptation.

3. **Complexity perception accuracy validation**: Evaluate the perception module's ability to accurately classify question complexity independently of the final ATM performance. This could involve creating a held-out validation set where human experts label question complexity, then comparing these labels against the perception module's predictions to quantify accuracy and identify systematic biases.