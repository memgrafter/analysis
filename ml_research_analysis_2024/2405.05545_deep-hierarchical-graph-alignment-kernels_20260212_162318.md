---
ver: rpa2
title: Deep Hierarchical Graph Alignment Kernels
arxiv_id: '2405.05545'
source_url: https://arxiv.org/abs/2405.05545
tags:
- graph
- dhgak
- kernels
- kernel
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deep Hierarchical Graph Alignment Kernels
  (DHGAK), a novel approach to graph similarity that addresses the limitations of
  existing R-convolution graph kernels. DHGAK captures the structural information
  of graphs by defining b-width h-hop slices around each node, embedding them into
  a deep vector space using Natural Language Models (NLM), and aligning them through
  clustering in the deep embedding space.
---

# Deep Hierarchical Graph Alignment Kernels

## Quick Facts
- arXiv ID: 2405.05545
- Source URL: https://arxiv.org/abs/2405.05545
- Reference count: 40
- Primary result: Outperforms state-of-the-art graph kernels on 16 real-world datasets using deep hierarchical alignment of graph substructures

## Executive Summary
This paper introduces Deep Hierarchical Graph Alignment Kernels (DHGAK), a novel approach to graph similarity that addresses limitations of R-convolution graph kernels by incorporating deep embeddings and alignment through clustering. DHGAK captures structural information by defining b-width h-hop slices around each node, embedding them into a deep vector space using Natural Language Models (NLM), and aligning them through clustering in the deep embedding space. The method is theoretically proven to be positive semi-definite and achieves superior classification accuracy compared to existing graph kernels on most tested datasets.

## Method Summary
DHGAK extracts hierarchical b-width h-hop slices from graphs, embeds them using Natural Language Models (BERT or word2vec), clusters slices in embedding space, constructs feature maps via kernel mean embedding, and combines across hierarchy levels. The method treats slices from the same cluster as aligned and assigns them the same feature map in the Reproducing Kernel Hilbert Space (RKHS). DHGAK is constructed by summing DGAK values across different hop depths, capturing multi-scale structural information. The approach is validated through 10-fold cross-validation on 16 real-world graph datasets, comparing against state-of-the-art graph kernels.

## Key Results
- Outperforms state-of-the-art graph kernels on most of 16 real-world datasets
- Achieves highest classification accuracy on 11 out of 16 tested datasets
- Provides theoretical guarantees of positive semi-definiteness and linear separability in RKHS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep alignment via clustering captures topological position information between substructures.
- Mechanism: By clustering deep embeddings of b-width h-hop slices and assigning aligned slices the same feature map, the method preserves the relative positions of substructures in the graph, unlike R-convolution kernels which ignore such topology.
- Core assumption: Similar slices from the same or different graphs will have similar deep embeddings, making them clusterable in the deep space.
- Evidence anchors: [abstract] "the relational substructures are hierarchically aligned to cluster distributions in their deep embedding space"; [section] "we propose a Deep Alignment Kernel (DAK) that aligns slices from the same or different graphs by computing the expectation of the cluster assignments of the adopted clustering methods"

### Mechanism 2
- Claim: Kernel mean embedding over DAK feature maps yields a positive semi-definite graph kernel.
- Mechanism: Treating feature maps of DAK in each graph as i.i.d. samples from an unknown distribution, kernel mean embedding computes the expectation of feature maps, ensuring positive semi-definiteness and enabling linear separability in RKHS.
- Core assumption: Slices within the same graph are i.i.d. samples of an underlying distribution.
- Evidence anchors: [abstract] "graph feature maps are derived by kernel mean embedding"; [section] "we adopt the kernel mean embedding [Muandet et al., 2017] to construct Deep Graph Alignment Kernel (DGAK) from a distributional perspective"

### Mechanism 3
- Claim: Summation of DGAK across hierarchical slices captures multi-scale structural information.
- Mechanism: For each hop h, DGAK captures slice-level alignment; summing over h aggregates hierarchical structural information, improving discrimination between graphs.
- Core assumption: Different hop depths encode complementary structural aspects of the graph.
- Evidence anchors: [abstract] "With the summation of DGAK on each hop of b-width slice, we exploit hierarchical graph structural information"; [section] "DHGAK is constructed by the summation of DGAK on slices of different hierarchies"

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: RKHS provides the mathematical framework where DHGAK's feature maps live, ensuring positive semi-definiteness and enabling linear separability.
  - Quick check question: What property of RKHS guarantees that the sum of two positive semi-definite kernels remains positive semi-definite?

- Concept: Kernel Mean Embedding
  - Why needed here: Allows constructing DGAK by embedding distributions of slice feature maps, turning alignment into a distributional comparison.
  - Quick check question: How does kernel mean embedding transform a set of feature maps into a single RKHS element?

- Concept: Weisfeiler-Lehman (WL) Isomorphism Test
  - Why needed here: Provides the baseline node labeling scheme used to encode slices before deep embedding.
  - Quick check question: Why is WL relabeling necessary before applying NLM to slice encodings?

## Architecture Onboarding

- Component map: Input graphs -> WL relabeling -> b-width h-hop slice encoding -> NLM deep embedding -> Clustering in embedding space -> DAK feature maps -> Kernel mean embedding -> DGAK -> Summation over hops -> DHGAK kernel matrix

- Critical path: 1. Slice encoding (O(n(n+e))) -> bottleneck for large graphs; 2. NLM inference per token (O(HN(n²+ne)TNLM)); 3. Clustering per hop (O(HT|Ψ|TC(Nn))); 4. Kernel matrix assembly (O(HNn))

- Design tradeoffs:
  - NLM choice: BERT gives richer embeddings but slower; word2vec faster but less expressive.
  - Clustering method: K-means deterministic but sensitive to init; DBSCAN automatic cluster count but slower.
  - Hop depth H: More hops -> richer structure but risk of noise and memory blowup.
  - Slice width b: Larger b -> more context but longer encodings may exceed BERT token limit.

- Failure signatures:
  - Memory error: Deep embeddings too large (e.g., hop=3, width=2, large graphs).
  - Slow runtime: NLM inference dominates; try word2vec or reduce H.
  - Poor accuracy: Clustering fails to align similar slices; check NLM quality or increase T.

- First 3 experiments:
  1. Reproduce Table 1 results on a small dataset (e.g., MUTAG) to validate correctness.
  2. Ablation study: Compare DHGAK-BERT vs HGAK-label to confirm deep embedding value.
  3. Parameter sweep: Vary H and cluster_factor on KKI to find stable operating region.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of clustering method affect the performance and interpretability of DHGAK, and are there optimal clustering strategies for specific graph types?
- Basis in paper: [explicit] The paper mentions that DHGAK allows for any clustering method to be used for alignment, and it tests K-means and DBSCAN as clustering methods.
- Why unresolved: While the paper demonstrates that different clustering methods can be used, it does not provide a comprehensive analysis of how the choice of clustering method affects performance across different graph types or whether certain clustering strategies are more suitable for specific graph structures.
- What evidence would resolve it: Systematic experiments comparing the performance of DHGAK with various clustering methods on a diverse set of graph types, along with an analysis of the alignment quality and interpretability of the resulting clusters.

### Open Question 2
- Question: Can the hierarchical slice structure in DHGAK be further optimized or generalized to capture more complex graph substructures, and how does this impact the model's performance and scalability?
- Basis in paper: [explicit] The paper introduces the concept of hierarchical b-width h-hop slices to capture graph substructures at different levels of granularity.
- Why unresolved: While the hierarchical slice structure is shown to improve performance, the paper does not explore whether there are more optimal ways to define or parameterize these slices, or how alternative substructures might affect the model's ability to capture complex graph patterns.
- What evidence would resolve it: Empirical studies comparing the performance of DHGAK with different slice definitions, such as varying the width, hop count, or incorporating alternative substructures like random walks or graph motifs.

### Open Question 3
- Question: How does DHGAK's performance scale with increasing graph size and complexity, and what are the computational bottlenecks limiting its scalability?
- Basis in paper: [explicit] The paper provides a time complexity analysis of DHGAK and compares its running time with other graph kernels on different dataset sizes.
- Why unresolved: While the paper demonstrates that DHGAK can handle large datasets, it does not provide a detailed analysis of how its performance scales with increasing graph size or identify the specific computational bottlenecks that limit its scalability.
- What evidence would resolve it: Experiments evaluating DHGAK's performance and running time on increasingly large and complex graphs, along with a detailed profiling of the computational bottlenecks and potential optimization strategies.

## Limitations
- Relies on strong i.i.d. assumption for slices within the same graph, which may not hold in practice
- Performance heavily depends on quality of NLM embeddings for capturing structural similarity
- Computational complexity may limit scalability to very large graphs (O(n(n+e)) slice encoding)

## Confidence
- **High** confidence in positive semi-definiteness proof and general framework
- **Medium** confidence in empirical performance claims due to implementation dependencies
- **Low** confidence in scalability claims for very large graphs given slice encoding complexity

## Next Checks
1. **Ablation of NLM dependency**: Run DHGAK with random embeddings (no NLM) on a small dataset to quantify how much performance gain comes from deep alignment versus the hierarchical structure alone.

2. **Stress test clustering robustness**: On KKI dataset, vary cluster_factor from 0.1 to 10.0 and plot accuracy to identify the stable operating region and potential overfitting to cluster parameters.

3. **Memory scalability analysis**: Profile memory usage on COLLAB dataset while varying hop depth H and slice width b to empirically verify the claimed memory bottlenecks and identify practical limits for real-world deployment.