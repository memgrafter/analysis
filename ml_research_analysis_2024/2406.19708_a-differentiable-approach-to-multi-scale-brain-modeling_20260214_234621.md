---
ver: rpa2
title: A Differentiable Approach to Multi-scale Brain Modeling
arxiv_id: '2406.19708'
source_url: https://arxiv.org/abs/2406.19708
tags:
- brain
- spiking
- neuron
- fitting
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a differentiable multi-scale brain modeling
  workflow using the BrainPy simulator, which enables gradient-based optimization
  across different brain scales. At the single-neuron level, differentiable neuron
  models are fitted to electrophysiological data using gradient methods.
---

# A Differentiable Approach to Multi-scale Brain Modeling

## Quick Facts
- arXiv ID: 2406.19708
- Source URL: https://arxiv.org/abs/2406.19708
- Reference count: 40
- Key outcome: Differentiable multi-scale brain modeling workflow using BrainPy simulator enables gradient-based optimization across electrophysiological, anatomical, and behavioral scales.

## Executive Summary
This paper introduces a differentiable multi-scale brain modeling workflow that leverages the BrainPy simulator to bridge neuroscience data across different scales. The approach uses differentiable neuron and synapse models with gradient-based optimization to fit electrophysiological data, incorporate connectomic constraints for network construction, and train models on cognitive tasks. The framework demonstrates superior performance and speed compared to conventional optimization algorithms while producing biologically plausible network dynamics and behavior.

## Method Summary
The workflow employs differentiable neuron models (generalized leaky integrate-and-fire and Hodgkin-Huxley) optimized using gradient methods like L-BFGS-B to fit electrophysiological data. Network construction incorporates connectomic data through event-driven differentiable synaptic operators using CSR format for efficient computation. The biologically-informed spiking neural networks are trained on cognitive tasks using BrainScale's online gradient-based learning framework, enabling gradient propagation through the entire multi-scale pipeline.

## Key Results
- Superior fitting performance and speed for generalized leaky integrate-and-fire and Hodgkin-Huxley neuron models compared to conventional optimization algorithms
- Successful training of biologically-informed excitatory-inhibitory spiking networks on working memory tasks that replicate observed neural activity patterns
- Demonstration of synaptic weight distribution changes that align with biological expectations following task training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differentiable brain models enable gradient-based optimization across scales, improving fitting performance and speed.
- Mechanism: By making neuron and synapse models differentiable, gradient descent methods can be used to optimize parameters, replacing slower heuristic or evolutionary algorithms.
- Core assumption: The surrogate gradient approach adequately approximates the non-differentiable spiking function for optimization purposes.
- Evidence anchors:
  - [abstract] "We leverage this capability of BrainPy across different brain scales. At the single-neuron level, we implement differentiable neuron models and employ gradient methods to optimize their fit to electrophysiological data."
  - [section] "We apply this approach in our workflow to enable gradient-based optimization."
  - [corpus] No direct evidence; assumption supported by cited literature on surrogate gradients.
- Break condition: If the surrogate gradient poorly approximates the true gradient, optimization may converge to suboptimal solutions or fail to converge.

### Mechanism 2
- Claim: Event-driven differentiable synaptic operators enable efficient gradient computation in large-scale spiking networks.
- Mechanism: CSR-formatted synaptic connections allow sparse, event-driven matrix-vector multiplication that is both forward and backward differentiable, reducing computation time and memory usage.
- Core assumption: CSR format efficiently represents synaptic connectivity and enables efficient event-driven computation.
- Evidence anchors:
  - [abstract] "We address this challenge by introducing differentiable event-driven synaptic operators compatible with autograd frameworks."
  - [section] "We utilize the compressed sparse row (CSR) format for storing synaptic connections and implement event-driven operations based on CSR arrays."
  - [corpus] No direct evidence; assumption supported by cited literature on CSR format and event-driven computation.
- Break condition: If the network connectivity is too dense or irregular, CSR format may not provide significant efficiency gains.

### Mechanism 3
- Claim: Incorporating connectomic data constrains network models, improving biological plausibility and generalization.
- Mechanism: Using connectome data to construct network connectivity ensures that the resulting models reflect the structural organization of real brains, leading to more realistic neural dynamics and behavior.
- Core assumption: Connectome data accurately represents the structural connectivity of the brain region of interest.
- Evidence anchors:
  - [abstract] "On the network level, we incorporate connectomic data to construct biologically constrained network models."
  - [section] "We incorporate brain structure and connectome information to construct realistic brain models."
  - [corpus] No direct evidence; assumption supported by cited literature on connectomics.
- Break condition: If the connectome data is noisy, incomplete, or not representative of the brain region of interest, the resulting network models may be inaccurate or misleading.

## Foundational Learning

- Concept: Surrogate gradients for spiking neural networks
  - Why needed here: To enable gradient-based optimization of spiking neural networks, which have non-differentiable activation functions.
  - Quick check question: How do surrogate gradients approximate the true gradient of a spiking function?

- Concept: Event-driven computation in neural networks
  - Why needed here: To efficiently process sparse, asynchronous spike events in large-scale spiking networks.
  - Quick check question: How does event-driven computation reduce the computational complexity of spike propagation in neural networks?

- Concept: Connectomics and brain structure
  - Why needed here: To incorporate structural constraints from real brain data into network models, improving biological plausibility.
  - Quick check question: How do different connectomic data sources (e.g., DTI, electron microscopy) capture different aspects of brain structure?

## Architecture Onboarding

- Component map: Differentiable neuron models -> Differentiable synapse models -> Event-driven synaptic operators (CSR format) -> Connectomic data integration -> Gradient-based optimization -> BrainPy simulator

- Critical path: 1. Fit single neuron models to electrophysiological data using gradient-based optimization. 2. Construct network models using connectomic data and differentiable synapse models. 3. Train network models on cognitive tasks using gradient-based learning rules. 4. Evaluate model performance and biological plausibility.

- Design tradeoffs: Model complexity vs. computational efficiency, Biological realism vs. generalizability, Data quality vs. model accuracy, Online learning vs. offline learning

- Failure signatures: Poor fitting of single neuron models to electrophysiological data, Inability to reproduce neural activity patterns observed in experiments, Lack of biological plausibility in network connectivity or dynamics, Slow convergence or poor performance of gradient-based optimization

- First 3 experiments: 1. Fit a simple integrate-and-fire neuron model to synthetic electrophysiological data using gradient descent. 2. Construct a small network model using random connectivity and train it on a simple cognitive task (e.g., pattern recognition). 3. Incorporate connectomic data from a publicly available source (e.g., mouse connectome) to construct a more realistic network model and evaluate its performance on the same cognitive task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed differentiable multi-scale brain modeling approach be scaled up to model brain-scale spiking neural networks with millions or billions of neurons?
- Basis in paper: [explicit] The authors mention that while they cannot provide a definitive answer, they have evaluated the scalability of their approach using the online learning framework BrainScale. They demonstrate that BrainScale achieves constant memory footprint and faster computation time compared to backpropagation through time (BPTT) for long sequences, suggesting potential scalability to larger networks.
- Why unresolved: Scaling up to brain-scale networks with millions or billions of neurons presents significant computational and memory challenges that require further research and optimization of the proposed approach.
- What evidence would resolve it: Experimental results demonstrating the successful scaling of the differentiable multi-scale brain modeling approach to brain-scale spiking neural networks with millions or billions of neurons, along with analysis of the computational and memory requirements, would provide evidence to resolve this open question.

### Open Question 2
- Question: How can the proposed approach handle the integration of information and constraints across different spatial and temporal scales in a seamless and efficient manner?
- Basis in paper: [explicit] The authors mention that seamlessly integrating information and constraints from different spatial and temporal scales is a non-trivial task and a potential limitation of their approach. They note that incorporating more biological details or constraints usually leads to more poor training performance due to the increased complexity and potential issues like vanishing/exploding gradients and slow convergence.
- Why unresolved: Developing effective strategies for integrating multi-scale information and constraints while maintaining computational efficiency and avoiding optimization issues remains an open challenge.
- What evidence would resolve it: Successful demonstrations of the proposed approach integrating multi-scale information and constraints from various spatial and temporal scales, along with analysis of the computational efficiency and optimization performance, would provide evidence to address this open question.

### Open Question 3
- Question: How can the interpretability and theoretical insights be derived from the complex, non-linear brain dynamics models obtained using the proposed gradient-based optimization methods?
- Basis in paper: [explicit] The authors acknowledge that while gradient-based optimization methods can produce models that fit the data, it may be challenging to derive theoretical insights or interpretable mechanisms from these models, especially when dealing with highly complex and non-linear brain dynamics.
- Why unresolved: Developing techniques to extract interpretable insights and theoretical understanding from the complex, data-driven models obtained through gradient-based optimization remains an open challenge in the field of computational neuroscience.
- What evidence would resolve it: Successful demonstrations of deriving interpretable insights and theoretical understanding from the complex, non-linear brain dynamics models obtained using the proposed approach, along with analysis of the interpretability and theoretical grounding of the derived models, would provide evidence to address this open question.

## Limitations

- Lack of specific details about connectomic data sources and exact hyperparameters for BrainScale training framework
- Limited empirical evidence of improved biological plausibility beyond synaptic weight distributions
- Evaluation focuses primarily on specific neuron models and a single working memory task, limiting generalizability

## Confidence

**High Confidence**: The fundamental mechanism of using differentiable neuron models with surrogate gradients for gradient-based optimization is well-established in the literature and supported by experimental results.

**Medium Confidence**: The claim that event-driven differentiable synaptic operators enable efficient gradient computation is supported by CSR format implementation, but lacks direct performance comparisons with alternative approaches.

**Medium Confidence**: The incorporation of connectomic data to construct biologically constrained networks is theoretically sound, but the paper provides limited empirical evidence of improved biological plausibility.

## Next Checks

1. **Surrogate Gradient Sensitivity**: Test multiple surrogate gradient functions (sigmoid, arctan, Gaussian) on the same neuron fitting task to quantify the impact on convergence speed and final fitting accuracy.

2. **Scalability Validation**: Implement the same workflow with progressively larger networks (10x, 100x, 1000x) and measure computational time and memory usage to verify the claimed efficiency gains.

3. **Cross-Task Generalization**: Apply the trained network models to a different cognitive task (e.g., decision-making or sensory processing) to evaluate whether the biologically-informed connectivity generalizes beyond the working memory task.