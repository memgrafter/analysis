---
ver: rpa2
title: 'Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving
  and Zero-Shot Instruction Following'
arxiv_id: '2402.06559'
source_url: https://arxiv.org/abs/2402.06559
tags:
- vehicle
- lane
- diffusion
- self
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion-ES combines evolutionary search with diffusion models
  to optimize black-box reward functions for trajectory planning. It samples and mutates
  trajectories using truncated diffusion-denoising, achieving state-of-the-art performance
  on the nuPlan autonomous driving benchmark.
---

# Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following

## Quick Facts
- arXiv ID: 2402.06559
- Source URL: https://arxiv.org/abs/2402.06559
- Reference count: 40
- Key outcome: Combines evolutionary search with diffusion models to optimize black-box reward functions for trajectory planning, achieving state-of-the-art performance on nuPlan autonomous driving benchmark.

## Executive Summary
Diffusion-ES is a novel approach for autonomous driving trajectory planning that combines evolutionary search with diffusion models. It samples and mutates trajectories using truncated diffusion-denoising, achieving state-of-the-art performance on the nuPlan autonomous driving benchmark. Unlike prior methods, it can optimize non-differentiable objectives like language-shaped rewards from LLM prompting, enabling novel behaviors such as aggressive lane weaving not present in training data.

## Method Summary
Diffusion-ES combines evolutionary strategies with trajectory denoising using truncated diffusion processes to optimize black-box reward functions. The method samples initial trajectories from a pre-trained diffusion model, evaluates them using the reward function, and then applies truncated denoising as mutation to generate new candidate trajectories. This process iterates over multiple generations, selecting the highest-scoring trajectories for further mutation until convergence. The approach is particularly effective for optimizing non-differentiable rewards, including those generated from natural language instructions via LLM prompting.

## Key Results
- Outperforms sampling-based planners, reactive policies, and reward-gradient guidance on nuPlan autonomous driving benchmark
- Successfully optimizes non-differentiable language-shaped rewards for zero-shot instruction following
- Enables novel driving behaviors (e.g., aggressive lane weaving) not present in training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion-ES outperforms reward-gradient guidance even when the reward function is differentiable because gradient guidance struggles with non-convex, noisy intermediate trajectories.
- Mechanism: Reward-gradient guidance requires fitting reward regressors to both clean and noisy samples, which is difficult when the reward function is non-differentiable or when intermediate noisy states are hard to score meaningfully. Diffusion-ES avoids this by optimizing only on clean, final denoised samples.
- Core assumption: The diffusion model can effectively project noisy samples back to the data manifold through denoising, making the final samples suitable for reward evaluation.
- Evidence anchors:
  - [abstract] "Reward-gradient guided denoising requires a differentiable reward function fitted to both clean and noised samples, limiting its applicability as a general trajectory optimizer."
  - [section 4.3] "We hypothesize that this is because although the ground truth reward function is available, it may not provide suitable guidance for intermediate noisy trajectories."
  - [corpus] Weak evidence - no direct comparison between diffusion-ES and reward-gradient guidance for differentiable rewards found in corpus.
- Break condition: If the diffusion model fails to project noisy samples back to the data manifold, the final samples may be unrealistic and unsuitable for reward evaluation.

### Mechanism 2
- Claim: Using an unconditional diffusion model with test-time reward optimization allows for better out-of-distribution generalization compared to conditional diffusion models.
- Mechanism: Conditional diffusion models narrow the sampling distribution to scene-specific trajectories, which speeds up search but limits exploration of novel behaviors not present in training data. Unconditional models with reward optimization can synthesize novel behaviors by exploring the full trajectory space.
- Core assumption: The diffusion model has learned a rich representation of the trajectory data manifold that includes diverse behaviors, even if they weren't paired with specific scene contexts during training.
- Evidence anchors:
  - [abstract] "The less conditioning information, the wider the distribution to draw trajectory samples from, the slower the search, but the better the generalization to OOD tasks and scenarios that require novel pairings of trajectories and scene contexts, not present in the training data."
  - [section 4.2] "Diffusion-ES performs significantly worse with a conditional diffusion model. The conditional diffusion model is much harder to guide since the scene context causes fewer samples to be in-distribution."
  - [corpus] Weak evidence - corpus does not directly address the trade-off between conditional and unconditional diffusion models for trajectory optimization.
- Break condition: If the unconditional diffusion model has not learned a sufficiently diverse trajectory distribution, it may fail to generate novel behaviors even with reward optimization.

### Mechanism 3
- Claim: Truncated diffusion-denoising for mutation is more efficient than Gaussian perturbations because it leverages the learned data manifold.
- Mechanism: Gaussian perturbations can push samples far from the data manifold, requiring many iterations to find valid trajectories. Truncated diffusion-denoising applies a small amount of noise and then denoises, which projects the samples back to the manifold and allows for more efficient exploration.
- Core assumption: The diffusion model has learned a good approximation of the trajectory data manifold that can be used for projection.
- Evidence anchors:
  - [abstract] "It mutates high-scoring trajectories using a truncated diffusion process that applies a small number of noising and denoising steps, allowing for much more efficient exploration of the solution space."
  - [section 3.2] "Prior evolutionary search methods resort to naive Gaussian perturbations which do not exploit any prior knowledge about the data manifold. Our key insight is to leverage a truncated diffusion-denoising process to mutate trajectories in a way the resulting mutations are part of the data manifold."
  - [corpus] Weak evidence - corpus does not directly compare truncated diffusion-denoising to Gaussian perturbations for mutation in evolutionary strategies.
- Break condition: If the truncation level is too high, the denoising process may not be able to project the samples back to the manifold, leading to invalid trajectories.

## Foundational Learning

- Concept: Diffusion models learn to denoise by reversing a gradual noising process, capturing the data distribution in the process.
  - Why needed here: Understanding how diffusion models work is crucial for understanding how Diffusion-ES leverages them for efficient trajectory sampling and mutation.
  - Quick check question: What is the purpose of the variance schedule in a diffusion model?

- Concept: Evolutionary strategies optimize black-box functions by iteratively updating a population of solutions based on their fitness.
  - Why needed here: Diffusion-ES combines evolutionary strategies with diffusion models, so understanding the basics of evolutionary strategies is necessary to grasp the overall approach.
  - Quick check question: How does the selection process in evolutionary strategies typically work?

- Concept: Language models can be prompted to generate code or other structured outputs, enabling zero-shot task completion.
  - Why needed here: Diffusion-ES uses language models to generate reward functions from natural language instructions, so understanding how language models can be prompted for code generation is important.
  - Quick check question: What is the key difference between few-shot prompting and finetuning a language model for a specific task?

## Architecture Onboarding

- Component map: Diffusion model -> Evolutionary search loop -> Reward function -> Language model (for instruction following)
- Critical path: Trajectory sampling → Reward evaluation → Elite selection → Truncated denoising mutation → Next generation
- Design tradeoffs:
  - Unconditional vs. conditional diffusion model: Unconditional models allow for better OOD generalization but require more search iterations. Conditional models are faster but may struggle with novel behaviors.
  - Mutation strength (truncation level): Higher truncation allows for more exploration but may lead to invalid trajectories. Lower truncation is safer but may get stuck in local optima.
  - Population size and search iterations: Larger populations and more iterations improve solution quality but increase computational cost.
- Failure signatures:
  - Poor driving performance: Could indicate issues with the diffusion model, reward function, or evolutionary search parameters.
  - Inability to follow instructions: Could indicate problems with the language model prompts or the reward shaping code generation.
  - Slow convergence: Could indicate the need for more search iterations, a larger population, or a different mutation strategy.
- First 3 experiments:
  1. Run Diffusion-ES with a simple, handcrafted reward function (e.g., lane following) and visualize the generated trajectories to verify basic functionality.
  2. Compare the performance of unconditional and conditional diffusion models on a simple instruction-following task to understand the trade-off between speed and generalization.
  3. Experiment with different mutation strengths (truncation levels) to find the optimal balance between exploration and exploitation.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations discussed, several key questions remain unresolved:
- How does the performance of Diffusion-ES scale with the dimensionality of the trajectory space, particularly for high-dimensional robotic control tasks beyond autonomous driving?
- What is the impact of different noise schedules (beyond the linear schedule used) on the quality of generated trajectories and optimization efficiency?
- How does Diffusion-ES perform when the reward function is non-stationary or changes during optimization, as might occur in dynamic environments?

## Limitations
- Limited direct empirical evidence comparing Diffusion-ES to reward-gradient guidance on differentiable rewards
- Effectiveness of truncated diffusion-denoising for mutation compared to Gaussian perturbations lacks direct empirical validation
- Performance scaling with trajectory space dimensionality not explored

## Confidence
- **High confidence**: The core mechanism of using diffusion models for efficient trajectory sampling and mutation is well-established and empirically validated. The superiority of Diffusion-ES over sampling-based planners and reactive policies on the nuPlan benchmark is strongly supported.
- **Medium confidence**: The claim that Diffusion-ES can effectively optimize non-differentiable rewards from LLM prompting is supported by the instruction-following experiments, but the specific prompt engineering and code generation details are not fully specified.
- **Low confidence**: The hypothesis that reward-gradient guidance underperforms even on differentiable rewards due to difficulties with noisy intermediate trajectories lacks direct empirical evidence in the corpus.

## Next Checks
1. Direct comparison of mutation strategies: Conduct a controlled experiment comparing truncated diffusion-denoising to Gaussian perturbations for mutation in evolutionary strategies, measuring convergence speed and solution quality on a benchmark task.
2. Reward-gradient guidance benchmark: Implement and evaluate reward-gradient guidance on the same differentiable reward functions used by Diffusion-ES, directly comparing performance and analyzing the impact of noisy intermediate trajectories.
3. Conditional vs. unconditional diffusion model ablation: Perform a systematic ablation study varying the conditioning information in the diffusion model, quantifying the trade-off between search speed and out-of-distribution generalization on a range of instruction-following tasks.