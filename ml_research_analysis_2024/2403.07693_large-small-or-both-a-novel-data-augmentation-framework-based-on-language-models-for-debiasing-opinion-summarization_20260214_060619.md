---
ver: rpa2
title: 'Large, Small or Both: A Novel Data Augmentation Framework Based on Language
  Models for Debiasing Opinion Summarization'
arxiv_id: '2403.07693'
source_url: https://arxiv.org/abs/2403.07693
tags:
- data
- sentiment
- text
- language
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses sentiment bias in opinion summarization, where
  models struggle to generate negative summaries for negative reviews due to dataset
  imbalance. To mitigate this, the authors propose LASS, a novel framework combining
  large and small language models.
---

# Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization

## Quick Facts
- **arXiv ID**: 2403.07693
- **Source URL**: https://arxiv.org/abs/2403.07693
- **Reference count**: 25
- **Primary result**: Proposed LASS framework significantly improves negative sentiment accuracy (up to 51.2% increase) in opinion summarization while maintaining ROUGE scores

## Executive Summary
This paper addresses sentiment bias in opinion summarization where models struggle to generate negative summaries due to dataset imbalance (over 70% positive reviews). The authors propose LASS, a novel framework combining large and small language models. LASS first uses a large language model to generate counterfactual negative reviews by minimally rewriting positive reviews, then trains a disentangled autoencoder (Dis-AE) on this data to learn sentiment and content representations. The framework generates additional synthetic negative reviews by combining representations, followed by filtering based on perplexity and sentiment classification. Experiments on Amazon and Yelp datasets show LASS significantly improves negative sentiment accuracy while maintaining comparable ROUGE scores, achieving similar results to using only large language models but at lower cost.

## Method Summary
The LASS framework addresses sentiment bias through a two-stage process. First, it generates counterfactual negative reviews from positive ones using a large language model with carefully designed prompts that enforce minimal edits while inverting sentiment. The framework then trains a disentangled autoencoder (Dis-AE) on these counterfactual pairs to learn separate sentiment and content representations. Using these learned representations, LASS generates synthetic negative reviews by combining sentiment representations from negative reviews with content representations from positive reviews. Finally, it filters the generated data using perplexity thresholds and sentiment classification to ensure quality. The augmented dataset is then used to train opinion summarization models.

## Key Results
- LASS significantly improves negative sentiment accuracy at both sentence (up to 51.2% increase) and review levels
- The framework achieves comparable ROUGE scores to using only large language models while generating 265,000 fewer synthetic data points
- LASS demonstrates effectiveness across multiple summarization models (Copycat, Coop, TRACE) on Amazon and Yelp datasets
- The combination of LLM-generated counterfactuals and Dis-AE-based synthesis provides a cost-effective alternative to using only large language models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Counterfactual generation via LLMs with optimized prompts can produce synthetic negative reviews that maintain the same content as positive reviews but with opposite sentiment.
- **Mechanism**: The framework uses a two-stage prompt optimization process. First, a basic prompt is used to generate initial counterfactuals. Then, the prompt is iteratively refined by incorporating manually annotated examples that fix specific rewriting issues (e.g., incomplete sentiment transformation, logical inconsistencies). This process improves the LLM's ability to generate coherent counterfactuals.
- **Core assumption**: LLMs can learn to follow the "minimal-edit principle" and generate counterfactuals that preserve content while inverting sentiment when provided with appropriate in-context examples.
- **Evidence anchors**: [abstract]: "We design prompts to ensure that the large pre-trained language model follows the minimal-edit principle when generating the counterfactual samples with opposite sentiments." [section]: "We first devised a foundational prompt to leverage the in-context learning capabilities of LLM for obtaining emotional opposite reviews. Then we enhance the prompt design by incorporating human-annotated samples..."

### Mechanism 2
- **Claim**: The Dis-AE model can disentangle sentiment and content representations from text, allowing for controlled generation of synthetic data.
- **Mechanism**: The Dis-AE model uses a bidirectional LSTM encoder to extract sentiment (ze) and content (zn) representations. These are constrained through reconstruction loss (Lrec), sentiment classification loss (Le), content neutrality loss (Ln), and distance loss (Ldis). By combining sentiment representations from negative reviews with content representations from positive reviews, new synthetic negative reviews can be generated.
- **Core assumption**: Sentiment and content information can be effectively disentangled in the latent space, and combining representations from different samples will produce coherent text.
- **Evidence anchors**: [abstract]: "Then, a disentangle reconstruction model is trained based on the generated data. After training, a large amount of synthetic data can be obtained by decoding the new representation obtained from the combination of different sample representations..." [section]: "The aim of Dis-AE is to reconstruct the input pairs... sentiment representation zp_e and zn_e are forced to distance themselves."

### Mechanism 3
- **Claim**: Data filtering based on perplexity and sentiment classification ensures the quality of generated synthetic reviews.
- **Mechanism**: After generating synthetic reviews using the Dis-AE model, the framework filters out low-quality samples by computing their perplexity (using GPT-2) and classifying their sentiment. Only samples with perplexity below a threshold (125) and classified as negative are retained.
- **Core assumption**: Perplexity is a reliable proxy for text fluency, and the sentiment classifier can accurately identify negative reviews.
- **Evidence anchors**: [abstract]: "Finally, a large amount of synthetic data can be obtained by decoding the new representation obtained from the combination of different sample representations and filtering based on confusion degree and sentiment classification." [section]: "Due to the limitation of small model generation ability, the generated text may be unreadable, or with incorrect sentiment polarity. Therefore, we add a data filtering process based on perplexity and sentiment classification..."

## Foundational Learning

- **Concept**: Prompt engineering and in-context learning
  - **Why needed here**: The framework relies on LLMs to generate counterfactual reviews, and the quality of these reviews depends heavily on the prompt design. Understanding how to craft effective prompts and leverage in-context learning is crucial for generating high-quality synthetic data.
  - **Quick check question**: What are the key elements of a well-designed prompt for counterfactual generation, and how can in-context examples improve the LLM's performance?

- **Concept**: Disentanglement learning and latent space manipulation
  - **Why needed here**: The Dis-AE model's ability to separate sentiment and content representations is central to the framework's data generation process. Understanding the principles of disentanglement learning and how to manipulate latent representations is essential for effectively using the model.
  - **Quick check question**: How does the Dis-AE model ensure that sentiment and content representations are disentangled, and what are the key constraints used during training?

- **Concept**: Data quality control and filtering
  - **Why needed here**: The framework generates a large volume of synthetic data, and ensuring its quality is critical for the downstream summarization task. Understanding how to use metrics like perplexity and sentiment classification for data filtering is important for maintaining the integrity of the augmented dataset.
  - **Quick check question**: How do perplexity and sentiment classification work together to filter out low-quality synthetic reviews, and what are the potential failure modes of this filtering process?

## Architecture Onboarding

- **Component map**: LLM-based counterfactual generator -> Dis-AE model (encoder, classifier, decoder) -> Data filtering module (perplexity computation, sentiment classification) -> Summarization models (Copycat, Coop, TRACE)

- **Critical path**: LLM-generated counterfactuals → Dis-AE training → Synthetic data generation → Filtering → Data augmentation for summarization models

- **Design tradeoffs**: Using both LLMs and a smaller Dis-AE model balances generation quality and computational cost. The prompt optimization process improves counterfactual generation but requires manual annotation effort. The filtering process ensures data quality but may reduce the volume of usable synthetic data.

- **Failure signatures**: Poor counterfactual generation (incoherent or incorrectly sentimented synthetic reviews), Disentanglement failure (mixed sentiment or content in generated reviews), Filtering issues (too many or too few reviews passing quality checks).

- **First 3 experiments**:
  1. Test the LLM's counterfactual generation with the basic prompt and evaluate the quality of the generated reviews.
  2. Train the Dis-AE model on the LLM-generated counterfactuals and assess its ability to reconstruct input pairs and generate new synthetic reviews.
  3. Apply the filtering process to the Dis-AE-generated reviews and measure the impact on data quality and volume.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the minimum amount of training data required for Dis-AE to achieve optimal performance?
- **Basis in paper**: [explicit] The paper mentions that determining the minimum data required for Dis-AE training is a critical issue, and the current approach based on perplexity and counterfactual reconstruction metrics only indirectly reflects the quality of generated counterfactual texts.
- **Why unresolved**: The paper does not provide a definitive answer to this question, as it only explores the impact of different synthetic data sizes on sentiment accuracies and uses perplexity and counterfactual reconstruction ROUGE score as metrics to evaluate the quality of generated text.
- **What evidence would resolve it**: Conducting extensive experiments with varying amounts of training data for Dis-AE and measuring the performance on downstream tasks would provide evidence to determine the minimum required data size.

### Open Question 2
- **Question**: How does the performance of LASS compare to using only large language models (LLMs) in terms of sentiment accuracy and ROUGE scores across different summarization models and datasets?
- **Basis in paper**: [explicit] The paper states that LASS achieved results comparable to LLMs only, with an average reduction of 265,000 synthetic data points. However, it does not provide a detailed comparison of sentiment accuracy and ROUGE scores between LASS and using only LLMs.
- **Why unresolved**: While the paper mentions the comparable results, it does not provide specific numbers or a comprehensive analysis of the performance differences between LASS and using only LLMs across various summarization models and datasets.
- **What evidence would resolve it**: Conducting experiments with different summarization models and datasets, comparing the sentiment accuracy and ROUGE scores of LASS against using only LLMs, would provide evidence to determine the relative performance of LASS.

### Open Question 3
- **Question**: How does the sensitivity of the counter-template parameter in the TRACE model affect its performance when using data augmentation methods based on GPT or LASS?
- **Basis in paper**: [explicit] The paper mentions that changes in data distribution significantly affect the performance of the TRACE model, as observed in preliminary experiments. It also states that the counter-template parameter in TRACE is sensitive to the training data.
- **Why unresolved**: The paper does not provide a detailed analysis of how the sensitivity of the counter-template parameter impacts the performance of TRACE when using data augmentation methods.
- **What evidence would resolve it**: Conducting experiments with varying levels of sensitivity in the counter-template parameter and measuring the performance of TRACE using data augmentation methods would provide evidence to understand the impact of this sensitivity on the model's performance.

## Limitations

- The framework's performance heavily depends on prompt engineering quality, with limited evidence about prompt robustness across different domains
- The Dis-AE model's disentanglement capabilities lack comprehensive ablation studies showing sensitivity to architectural choices or training parameters
- The evaluation focuses narrowly on sentiment accuracy and ROUGE scores without examining downstream task generalization or potential new biases
- The filtering process using perplexity and sentiment classification assumes these metrics are reliable quality indicators without exploring their potential failures

## Confidence

**High Confidence**: The core claim that dataset imbalance causes sentiment bias in opinion summarization is well-established and the proposed framework logically addresses this through counterfactual generation and disentangled representation learning.

**Medium Confidence**: The experimental results showing improved negative sentiment accuracy (up to 51.2%) are promising, but the evaluation setup has limitations - only three summarization models are tested, and the comparison against other data augmentation methods is limited.

**Low Confidence**: The generalizability of the prompt optimization process and the Dis-AE architecture to other domains or tasks remains uncertain without additional validation beyond opinion summarization.

## Next Checks

1. **Ablation Study on Filtering Thresholds**: Systematically vary the perplexity threshold and sentiment classifier confidence requirements to quantify their impact on synthetic data quality and downstream summarization performance.

2. **Cross-Domain Robustness Test**: Apply the LASS framework to a different domain (e.g., product reviews for electronics or movies) using the same prompt optimization procedure to assess whether the minimal-edit principle generalizes across different types of opinion text.

3. **Human Evaluation of Synthetic Data Quality**: Conduct blinded human assessments comparing LLM-generated counterfactuals versus Dis-AE generated synthetic reviews to verify that the disentanglement approach maintains content fidelity while accurately inverting sentiment.