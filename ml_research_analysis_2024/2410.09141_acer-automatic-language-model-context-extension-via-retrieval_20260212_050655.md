---
ver: rpa2
title: 'ACER: Automatic Language Model Context Extension via Retrieval'
arxiv_id: '2410.09141'
source_url: https://arxiv.org/abs/2410.09141
tags:
- context
- data
- wang
- long-context
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ACER, a method for automatically extending
  language model capabilities to longer contexts without human-generated supervision.
  The core idea is to synthesize training data by combining retrieval with a short-context
  language model, then fine-tune the long-context model using this synthetic data.
---

# ACER: Automatic Language Model Context Extension via Retrieval

## Quick Facts
- arXiv ID: 2410.09141
- Source URL: https://arxiv.org/abs/2410.09141
- Authors: Luyu Gao; Yunyi Zhang; Jamie Callan
- Reference count: 26
- Primary result: ACER outperforms baseline long-context models on QA tasks using synthetic data from retrieval and short-context models

## Executive Summary
This paper introduces ACER, a method that automatically extends language model capabilities to longer contexts without requiring human-generated supervision. The approach synthesizes training data by combining retrieval with a short-context language model, then fine-tunes the long-context model using this synthetic data. By mimicking human information processing through document chunk ranking and selective generation, ACER demonstrates superior performance on long-context retrieval-augmented generation and reading comprehension tasks compared to both contemporary generalist long-context models and the retrieval pipeline used to generate the training data.

## Method Summary
ACER's core innovation lies in its synthetic data generation pipeline that leverages retrieval and a short-context language model to create training examples for long-context models. The process begins by retrieving relevant document chunks for a given query, ranking them based on relevance, and having the short-context model generate answers using only the top-ranked candidates. This synthetic data is then used to fine-tune the long-context model, effectively teaching it to handle extended contexts without requiring expensive human annotations. The method is designed to scale to very long contexts while maintaining the reasoning capabilities of shorter-context models.

## Key Results
- ACER achieves 0.446 EM on Natural Questions, outperforming baseline models
- ACER achieves 0.648 EM on TriviaQA, demonstrating strong performance on factual recall
- ACER achieves 0.189 F1 on NarrativeQA, showing effectiveness on narrative understanding tasks

## Why This Works (Mechanism)
The method works by leveraging the reasoning capabilities of short-context models while extending their effective context length through retrieval-augmented training data. By ranking document chunks and having the short-context model generate answers using only the most relevant information, ACER teaches long-context models to prioritize and synthesize information effectively. This approach addresses the fundamental challenge of scaling language models to longer contexts without requiring proportional increases in human supervision.

## Foundational Learning

**Retrieval-based document ranking**: Understanding how to identify and prioritize relevant document chunks is crucial for ACER's effectiveness. Quick check: Verify retrieval system returns relevant chunks for diverse queries.

**Synthetic data generation**: The process of creating training examples from model outputs requires understanding prompt engineering and generation quality control. Quick check: Evaluate synthetic data quality using human or automated metrics.

**Long-context model fine-tuning**: Knowledge of how to adapt models for extended contexts while maintaining performance on shorter inputs. Quick check: Monitor perplexity and task-specific metrics during fine-tuning.

## Architecture Onboarding

**Component map**: Query -> Retrieval System -> Document Chunk Ranking -> Short-Context Model -> Synthetic Data Generation -> Long-Context Model Fine-tuning

**Critical path**: The retrieval and ranking components are critical as they determine which information the short-context model uses for generation, directly impacting the quality of synthetic training data.

**Design tradeoffs**: The balance between retrieval precision and the number of chunks used affects both computational cost and model performance. Using more chunks increases coverage but also computational requirements.

**Failure signatures**: Poor retrieval quality leads to irrelevant training data, causing the long-context model to learn incorrect associations. Insufficient chunk ranking can result in noise overwhelming signal in the synthetic data.

**First experiments**:
1. Test retrieval quality on a sample of queries using standard metrics (recall, precision)
2. Evaluate short-context model generation quality on ranked chunks
3. Validate synthetic data by checking if it produces coherent and relevant examples

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses narrowly on long-context QA tasks, leaving uncertainty about effectiveness on other applications like summarization or code generation
- Synthetic data generation may introduce biases from the short-context model's generation patterns that aren't fully characterized
- Comparison with generalist long-context models doesn't explore computational efficiency trade-offs during inference

## Confidence
High: Effectiveness of synthetic data generation on tested QA benchmarks
Medium: Generalizability to other long-context tasks beyond QA
Low: Claims about computational efficiency and real-world deployment considerations

## Next Checks
1. Test ACER on long-context tasks beyond QA, including document summarization, multi-hop reasoning, and code generation to assess generalizability
2. Conduct an ablation study varying the number of retrieved chunks, retrieval strategy parameters, and short-context model choices to identify which components most impact performance
3. Compare inference-time efficiency (latency, memory usage) of ACER-trained models versus baseline long-context models on representative long-context tasks