---
ver: rpa2
title: 'GenRec: Generative Sequential Recommendation with Large Language Models'
arxiv_id: '2407.21191'
source_url: https://arxiv.org/abs/2407.21191
tags:
- item
- user
- recommendation
- sequential
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of sequential recommendation, which
  aims to predict the next item a user is most likely to interact with based on their
  historical interaction sequence. The authors propose GenRec, a novel generative
  framework that formulates sequential recommendation as a sequence-to-sequence generation
  task.
---

# GenRec: Generative Sequential Recommendation with Large Language Models

## Quick Facts
- arXiv ID: 2407.21191
- Source URL: https://arxiv.org/abs/2407.21191
- Authors: Panfeng Cao; Pietro Lio
- Reference count: 38
- Key outcome: Achieves state-of-the-art performance on sequential recommendation using generative framework with masked item prediction

## Executive Summary
GenRec introduces a novel generative framework for sequential recommendation that formulates next-item prediction as a sequence-to-sequence generation task. Unlike existing methods that rely on manually designed hard prompts, GenRec leverages the sequence modeling capability of Transformer with a masked item prediction objective to learn bidirectional sequential patterns. The model achieves state-of-the-art performance across public real-world datasets while maintaining a lightweight architecture that can be trained effectively in low-resource settings, making it highly applicable to real-world scenarios.

## Method Summary
GenRec is a generative framework that treats sequential recommendation as a sequence-to-sequence generation problem. The input is a textual user item sequence, and the output is the top-ranked next items. The model utilizes a Transformer encoder-decoder architecture (BART base) with 6-layer encoder, 6-layer decoder, 12 attention heads, and 768 dimension (184M parameters). It employs a masked item prediction objective where a random item in the sequence is masked during pretraining, allowing the encoder to capture context from both left and right sides. The model is pretrained for about an hour and finetuned for 25 epochs on one NVIDIA RTX 3090 GPU using AdamW optimizer with learning rate 1e-5 and weight decay.

## Key Results
- Achieves state-of-the-art performance on Amazon Sports, Amazon Beauty, and Yelp datasets
- Demonstrates effective generalization across different domains
- Requires only a few hours to train effectively in low-resource settings
- Masked item prediction objective significantly improves model performance by a large margin

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GenRec achieves state-of-the-art performance by using masked item prediction to learn bidirectional sequential patterns.
- Mechanism: The model randomly masks an item in the user sequence and trains the decoder to predict it, allowing the encoder to capture context from both left and right sides of the sequence. This bidirectional encoding better captures non-linear user behavior patterns compared to left-to-right only methods.
- Core assumption: Bidirectional context improves sequential pattern learning compared to unidirectional approaches.
- Evidence anchors:
  - [abstract] "GenRec utilizes the sequence modeling capability of Transformer and adopts the masked item prediction objective to effectively learn the hidden bidirectional sequential patterns."
  - [section] "Our bidirectional masked sequence modeling task is based on the observation that the item sequence is not strictly ordered."
  - [corpus] Weak corpus support - the nearest neighbor papers focus on generative recommendation but don't explicitly validate masked item prediction for bidirectional learning.
- Break condition: If user behavior is strictly sequential (each item depends only on previous items), bidirectional masking adds noise rather than signal.

### Mechanism 2
- Claim: Pretraining with masked sequence modeling significantly improves downstream sequential recommendation performance.
- Mechanism: The model first learns general sequential patterns from large-scale pretraining data using the same masked prediction objective. This learned knowledge is then transferred to the specific recommendation task through finetuning, where the model already understands how to reconstruct masked items in context.
- Core assumption: Knowledge learned from pretraining on masked sequence modeling transfers effectively to the next-item prediction task.
- Evidence anchors:
  - [abstract] "Our experiments also validate the effectiveness of the the proposed masked item prediction objective that improves the model performance by a large margin."
  - [section] "To verify the effectiveness of the proposed masked sequence modeling task, we conduct the ablation study... the model performance drops across all metrics without the masked sequence modeling task."
  - [corpus] Weak corpus support - while pretraining is common in NLP, specific validation of masked sequence modeling for sequential recommendation is limited in the corpus.
- Break condition: If the pretraining corpus has different sequential patterns than the target domain, transfer learning may be ineffective.

### Mechanism 3
- Claim: GenRec's lightweight architecture enables effective training in low-resource settings while maintaining competitive performance.
- Mechanism: By using a relatively small model (184 million parameters) and requiring only a few hours of training time, GenRec democratizes access to generative recommendation. The architecture leverages pretrained BART weights, reducing the need for extensive task-specific pretraining.
- Core assumption: Model size and training efficiency can be balanced with performance for practical deployment.
- Evidence anchors:
  - [abstract] "Moreover, GenRec is lightweight and requires only a few hours to train effectively in low-resource settings, making it highly applicable to real-world scenarios and helping to democratize large language models in the sequential recommendation domain."
  - [section] "GenRec is pretrained for about an hour and finetuned for 25 epochs on one NVIDIA RTX 3090 GPU."
  - [corpus] Moderate corpus support - the neighbor papers mention "long-tail generative recommendation" and "plug-and-play frameworks" suggesting interest in efficient approaches, though not directly validating GenRec's specific architecture.
- Break condition: If the target domain requires complex long-range dependencies, the relatively small model size may limit performance.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: GenRec uses encoder-decoder Transformer to capture bidirectional sequential patterns through self-attention
  - Quick check question: How does multi-head self-attention allow the model to attend to different positions in the sequence simultaneously?

- Concept: Masked language modeling and pretraining objectives
  - Why needed here: The model uses masked item prediction during both pretraining and finetuning to learn bidirectional patterns
  - Quick check question: What's the difference between BERT's masked language modeling and GenRec's masked item prediction?

- Concept: Sequence-to-sequence generation framework
  - Why needed here: GenRec frames sequential recommendation as a generation task rather than classification, enabling auto-regressive item prediction
  - Quick check question: How does the decoder generate items one-by-one in the generation process?

## Architecture Onboarding

- Component map: Input sequence → Tokenizer → Cross-modal embedding (token + positional + ID embeddings) → Encoder (6-layer Transformer) → Masked item prediction task → Decoder (6-layer Transformer) → Generated items
- Critical path: Data preprocessing → Tokenization → Embedding creation → Encoder forward pass → Masked item prediction → Decoder generation → Loss computation → Parameter updates
- Design tradeoffs: Smaller model size (184M params) vs. performance potential, bidirectional learning vs. computational overhead, pretraining efficiency vs. domain specificity
- Failure signatures: Poor performance on longer sequences (attention limitation), degraded results when pretraining data differs significantly from target domain, sensitivity to tokenization choices for user/item identifiers
- First 3 experiments:
  1. Compare GenRec performance with and without pretraining on a small dataset to validate transfer learning effectiveness
  2. Test different masking rates (1 item vs. multiple items) to find optimal pretraining configuration
  3. Evaluate performance on varying sequence lengths to identify attention window limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GenRec's performance compare to other methods when scaled to larger datasets or with longer user interaction sequences?
- Basis in paper: [inferred] The paper mentions that GenRec is lightweight and requires only a few hours to train effectively in low-resource settings, but does not provide results on larger datasets or longer sequences.
- Why unresolved: The experiments were conducted on three public real-world datasets with limited user interaction sequences, and the paper does not explore the model's performance on larger datasets or with longer sequences.
- What evidence would resolve it: Experimental results comparing GenRec's performance to other methods on larger datasets or with longer user interaction sequences would resolve this question.

### Open Question 2
- Question: How does GenRec's performance compare to other methods when incorporating additional contextual information such as item attributes or user demographics?
- Basis in paper: [inferred] The paper mentions that contextual information such as item attributes can be incorporated as self-supervised signals to learn user and item representations, but does not explore the impact of incorporating such information on GenRec's performance.
- Why unresolved: The experiments were conducted without incorporating additional contextual information, and the paper does not explore the potential benefits or drawbacks of incorporating such information on GenRec's performance.
- What evidence would resolve it: Experimental results comparing GenRec's performance to other methods when incorporating additional contextual information would resolve this question.

### Open Question 3
- Question: How does GenRec's performance compare to other methods when using different masking mechanisms or objectives during pretraining?
- Basis in paper: [explicit] The paper mentions that different masking mechanisms are used in pretraining, finetuning, and inference, but does not explore the impact of using different masking mechanisms or objectives on GenRec's performance.
- Why unresolved: The experiments were conducted using a specific masking mechanism and objective, and the paper does not explore the potential benefits or drawbacks of using different masking mechanisms or objectives on GenRec's performance.
- What evidence would resolve it: Experimental results comparing GenRec's performance to other methods when using different masking mechanisms or objectives during pretraining would resolve this question.

## Limitations
- Performance may degrade on longer sequences beyond the 512 token limit
- Effectiveness depends on user behavior not being strictly sequential
- Model size (184M parameters) may limit performance on domains requiring complex long-range dependencies

## Confidence
**High Confidence:** The claim that GenRec achieves state-of-the-art performance on public datasets (Amazon Sports, Amazon Beauty, Yelp) with specific metrics (HR@{5,10}, NDCG@{5,10}) is well-supported by the experimental results section and multiple comparisons against established baselines.

**Medium Confidence:** The assertion that the masked item prediction objective significantly improves performance is supported by ablation studies, but the magnitude of improvement and its generalizability across different dataset characteristics warrants further investigation.

**Medium Confidence:** The claim about GenRec being lightweight and training effectively in low-resource settings is supported by training time specifications, but real-world deployment scenarios may present additional computational challenges not captured in the controlled experimental setup.

## Next Checks
1. **Domain Transferability Test:** Evaluate GenRec's performance when trained on one domain (e.g., Amazon Sports) and tested on a different domain (e.g., Yelp) to validate the cross-domain generalization claims and assess the true value of pretraining versus domain-specific finetuning.

2. **Sequence Length Sensitivity Analysis:** Systematically evaluate GenRec's performance across varying sequence lengths (short: 5-10 items, medium: 20-50 items, long: 100+ items) to identify potential attention window limitations and determine practical sequence length constraints for deployment.

3. **Ablation on Pretraining Duration:** Conduct experiments varying pretraining time (15 minutes, 1 hour, 2 hours) to quantify the relationship between pretraining investment and downstream performance, helping to establish optimal resource allocation for different use cases.