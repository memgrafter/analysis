---
ver: rpa2
title: Robust Claim Verification Through Fact Detection
arxiv_id: '2407.18367'
source_url: https://arxiv.org/abs/2407.18367
tags:
- claim
- evidence
- verification
- factdetect
- short
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FactDetect, a method to enhance claim verification
  by generating and labeling concise factual statements from evidence. The approach
  uses an LLM to extract matching phrases, generate questions, and produce short fact
  sentences, which are then labeled based on their relevance to the claim and evidence.
---

# Robust Claim Verification Through Fact Detection
## Quick Facts
- arXiv ID: 2407.18367
- Source URL: https://arxiv.org/abs/2407.18367
- Reference count: 27
- FactDetect improves claim verification performance by 15% F1 in supervised models and achieves 17.3% average performance gain in zero-shot LLM prompting

## Executive Summary
This paper introduces FactDetect, a method that enhances claim verification by generating and labeling concise factual statements from evidence. The approach uses an LLM to extract matching phrases, generate questions, and produce short fact sentences, which are then labeled based on their relevance to the claim and evidence. FactDetect is integrated into both supervised multitask learning models and zero-shot LLM prompting strategies, demonstrating significant improvements in scientific claim verification across multiple datasets.

## Method Summary
FactDetect employs a three-step process: (1) Extract matching phrases between claims and evidence using an LLM, (2) Generate concise questions and corresponding short fact sentences from these phrases, and (3) Label each fact statement as relevant or not based on its connection to the claim and evidence. This structured fact extraction pipeline is integrated into supervised multitask learning frameworks and zero-shot LLM prompting approaches, providing a novel way to represent evidence for claim verification tasks.

## Key Results
- FactDetect improves supervised multitask learning models by 15% F1 score
- Achieves 17.3% average performance gain in zero-shot LLM prompting across three scientific claim verification datasets
- Demonstrates effectiveness across multiple experimental conditions and model architectures

## Why This Works (Mechanism)
The method works by transforming unstructured evidence into structured, verifiable fact statements that directly connect claims to supporting information. By generating concise factual sentences rather than using raw evidence text, the approach reduces noise and focuses the verification process on directly relevant information. The labeling mechanism ensures that only pertinent facts are considered, while the question-generation component helps clarify the relationship between claims and evidence.

## Foundational Learning
- **LLM-based fact extraction**: Needed to automatically identify relevant information from evidence; Quick check: Verify extraction quality through manual inspection of generated facts
- **Fact statement generation**: Required to create concise, verifiable representations; Quick check: Measure fact statement brevity and relevance scores
- **Multitask learning integration**: Essential for combining fact detection with verification; Quick check: Compare single-task vs multitask performance
- **Zero-shot prompting**: Important for evaluation without training data; Quick check: Test different prompt templates and their impact
- **Scientific claim verification**: Domain-specific context; Quick check: Ensure dataset alignment with scientific literature standards

## Architecture Onboarding
**Component Map**: Claim -> FactExtract (LLM) -> FactLabel (LLM) -> VerificationModel -> Output
**Critical Path**: Evidence processing through fact extraction, labeling, and verification determines overall system performance
**Design Tradeoffs**: FactDetect prioritizes precision over recall in fact extraction, trading potential information loss for reduced noise in verification
**Failure Signatures**: Poor fact extraction leads to missing verification evidence; incorrect labeling causes false positives/negatives; hallucination in LLM-generated facts propagates downstream
**First Experiments**: (1) Compare fact extraction quality with baseline evidence representation; (2) Test fact labeling accuracy against human annotations; (3) Measure verification performance with varying numbers of extracted facts

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Limited evaluation to scientific claim verification datasets restricts generalizability conclusions
- Reliance on LLM-generated fact statements introduces potential hallucination risks without quantified error propagation
- Performance improvements may be influenced by differing training data distributions and preprocessing pipelines

## Confidence
- High confidence in core observation that structured fact extraction improves claim verification performance
- Medium confidence in the specific 15% F1 improvement in supervised models due to limited dataset sizes
- Low confidence in zero-shot LLM prompting results due to heavy dependence on prompt engineering quality

## Next Checks
1. Evaluate FactDetect on non-scientific domains like political claims or product reviews to assess domain transferability
2. Implement error analysis to quantify hallucination rates in LLM-generated facts and measure downstream impact
3. Conduct ablation studies removing individual components (phrase matching, question generation, fact labeling) to determine which contributes most to performance gains