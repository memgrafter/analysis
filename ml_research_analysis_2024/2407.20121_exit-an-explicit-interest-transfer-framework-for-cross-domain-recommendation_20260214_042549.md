---
ver: rpa2
title: 'EXIT: An EXplicit Interest Transfer Framework for Cross-Domain Recommendation'
arxiv_id: '2407.20121'
source_url: https://arxiv.org/abs/2407.20121
tags:
- domain
- interest
- user
- transfer
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EXIT introduces an explicit interest transfer framework for cross-domain
  recommendation that addresses negative transfer issues by using supervised learning
  to identify and transfer only beneficial source domain interests. The method employs
  a novel interest combination label to supervise the transfer process and a scene
  selector network to model transfer intensity under different contexts.
---

# EXIT: An EXplicit Interest Transfer Framework for Cross-Domain Recommendation

## Quick Facts
- arXiv ID: 2407.20121
- Source URL: https://arxiv.org/abs/2407.20121
- Authors: Lei Huang; Weitao Li; Chenrui Zhang; Jinpeng Wang; Xianchun Yi; Sheng Chen
- Reference count: 40
- Primary result: 1.23% CTCVR and 3.65% GTV improvements in online deployment at Meituan

## Executive Summary
EXIT introduces an explicit interest transfer framework for cross-domain recommendation that addresses negative transfer issues by using supervised learning to identify and transfer only beneficial source domain interests. The method employs a novel interest combination label to supervise the transfer process and a scene selector network to model transfer intensity under different contexts. The framework achieves 1.23% CTCVR and 3.65% GTV improvements in online deployment at Meituan, demonstrating both effectiveness and practical viability for industrial recommendation systems.

## Method Summary
EXIT is a multi-task framework that transfers interests from source to target domains using supervised learning with Interest Combination Labels (ICL) and a Scene Selector Network (SSN). The framework combines cross-entropy losses for target and source domain predictions with L1 loss on ICL supervision, weighted by Œª parameters. The model uses shared user/item embeddings across domains while maintaining domain-specific towers for interest prediction. The final prediction is computed as Pwhole = Ptarget + Psource √ó Ptrans, where Ptrans is the transfer probability determined by SSN.

## Key Results
- Achieves 1.23% CTCVR and 3.65% GTV improvements in online deployment at Meituan
- Effectively prevents negative transfer by identifying and excluding inappropriate interest signals
- Demonstrates practical viability for industrial recommendation systems with minimal complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised learning with Interest Combination Labels (ICL) directly identifies which source domain interests are beneficial for transfer, preventing negative transfer.
- Mechanism: ICL encodes three scenarios: (1) no transfer when interests don't overlap, (2) full transfer when both domains have purchase, (3) partial transfer using group consistency interest when interests differ. This supervision guides the model to learn when and what to transfer.
- Core assumption: Group consistency interest (Equation 5) is a reliable proxy for whether source domain interest should transfer when user-level signals conflict.
- Evidence anchors:
  - [abstract] "we propose a novel label combination approach that enables the model to directly learn beneficial source domain interests through supervised learning, while excluding inappropriate interest signals."
  - [section 3.4] "Through the supervision of ICL, the interest transfer probability learned by the model will tend to 0 when training loss is minimized" and detailed ICL construction logic.
  - [corpus] Weak: No direct corpus evidence found for group consistency interest methodology, but CDR literature shows supervised approaches are less common.
- Break condition: Group consistency interest calculation becomes unreliable when item overlap between domains is very sparse, or when user groups are too small to establish meaningful consistency patterns.

### Mechanism 2
- Claim: Scene Selector Network (SSN) models context-dependent transfer intensity, ensuring interests transfer only under appropriate conditions.
- Mechanism: SSN takes user, item, and context embeddings plus fine-grained scene features (age, category, time, location) and outputs a transfer probability Ptrans that modulates source domain influence based on contextual relevance.
- Core assumption: User interests vary predictably across contexts (e.g., food delivery on weekdays vs entertainment on weekends), and these patterns can be learned from scene features.
- Evidence anchors:
  - [abstract] "we introduce a scene selector network to model the interest transfer intensity under fine-grained scenes."
  - [section 3.5] "Scene information plays a significant role in predicting user interests and is also an important factor affecting cross-domain interest transfer" with SSN architecture details.
  - [corpus] Weak: No corpus evidence found for scene-aware transfer intensity modeling in CDR literature.
- Break condition: When scene features don't capture the actual contextual factors driving interest differences, or when the relationship between scenes and transfer suitability is too complex for the SSN architecture.

### Mechanism 3
- Claim: Multi-interest joint loss training balances target domain accuracy with source domain transfer learning, preventing collapse to either domain.
- Mechanism: The loss function combines cross-entropy losses for target and source domain predictions with L1 loss on ICL supervision, weighted by Œª parameters. This ensures both domains are learned while transfer probability is optimized.
- Core assumption: Joint training with multiple loss components can learn complementary domain representations without one dominating the other.
- Evidence anchors:
  - [abstract] "Without complex network structures or training processes, EXIT can be easily deployed in the industrial recommendation system."
  - [section 3.6] "the multi-interest joint loss is represented as follows" with detailed equation showing balance between target loss, source loss, and ICL loss.
  - [corpus] Weak: No corpus evidence found for this specific multi-loss formulation in CDR literature.
- Break condition: When Œª parameter tuning is difficult and leads to one domain dominating training, or when the L1 loss on continuous ICL values doesn't provide sufficient gradient signal.

## Foundational Learning

- Concept: Cross-Domain Recommendation (CDR) fundamentals
  - Why needed here: Understanding why CDR is necessary (data sparsity, cold start) and how implicit methods fail (negative transfer) is crucial for grasping EXIT's explicit approach.
  - Quick check question: What is the key difference between implicit and explicit CDR paradigms, and why does this distinction matter for negative transfer?

- Concept: Supervised learning with custom labels
  - Why needed here: ICL is a novel label construction that requires understanding how supervised learning can be applied to transfer learning problems.
  - Quick check question: How does the Interest Combination Label encode different transfer scenarios, and what supervised signal does each scenario provide?

- Concept: Multi-task learning with shared representations
  - Why needed here: EXIT uses a multi-task framework where user/item embeddings are shared across domains while domain-specific towers predict interests.
  - Quick check question: How does the multi-task architecture balance learning domain-specific interests while sharing representations, and why is this important for transfer?

## Architecture Onboarding

- Component map: Raw features ‚Üí Embedding layer ‚Üí Multi-task Network (Target Tower + Source Tower) ‚Üí Scene Selector Network ‚Üí Output (Pwhole = Ptarget + Psource √ó Ptrans) ‚Üí Multi-interest Joint Loss
- Critical path: Raw features ‚Üí Embeddings ‚Üí Multi-task towers ‚Üí SSN ‚Üí Final prediction ‚Üí Loss calculation
- Design tradeoffs: EXIT trades off model complexity for explicit transfer control; simpler than complex embedding-sharing methods but requires careful label engineering and scene feature design
- Failure signatures: Poor performance on either domain indicates loss balance issues; high NFR suggests negative transfer; low AUC suggests scene features aren't capturing transfer patterns
- First 3 experiments:
  1. Ablation: Remove SSN to test if scene context is truly beneficial for transfer intensity modeling
  2. Parameter sweep: Test different Œª1, Œª2, Œª3 values to find optimal loss balance
  3. Label variation: Test ICL with and without group consistency interest to quantify its impact on negative transfer prevention

## Open Questions the Paper Calls Out
- How does the performance of EXIT scale when applied to domains with no user or item overlap compared to the current setup where users and items are shared?
- What is the optimal frequency for updating the group consistency interest (ùúÇ) in real-world deployments, and how does this frequency affect model performance?
- How sensitive is EXIT's performance to the selection of scene features used in the Scene Selector Network, and which scene features contribute most to preventing negative transfer?

## Limitations
- No direct empirical validation of the key mechanisms, particularly the effectiveness of group consistency interest and scene selector network in preventing negative transfer
- Evaluation primarily relies on aggregate performance metrics rather than detailed analysis of transfer patterns and failure cases
- Lack of ablation studies to isolate the contribution of each component to overall performance

## Confidence
- **High Confidence**: The multi-task framework architecture and overall training procedure are well-specified and reproducible
- **Medium Confidence**: The Interest Combination Label methodology and its role in preventing negative transfer, though implementation details are partially missing
- **Low Confidence**: The effectiveness of the Scene Selector Network and the specific impact of different scene features on transfer intensity modeling

## Next Checks
1. Conduct ablation studies to isolate the contribution of each component (ICL, SSN, multi-task loss) to overall performance
2. Perform detailed error analysis to identify specific scenarios where negative transfer occurs despite the framework
3. Test the framework on additional domains and datasets to evaluate generalizability beyond the Meituan deployment environment