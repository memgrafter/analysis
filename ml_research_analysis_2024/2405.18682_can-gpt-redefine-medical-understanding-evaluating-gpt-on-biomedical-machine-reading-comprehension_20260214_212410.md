---
ver: rpa2
title: Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine
  Reading Comprehension
arxiv_id: '2405.18682'
source_url: https://arxiv.org/abs/2405.18682
tags:
- context
- prompting
- answer
- implicit
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates GPT's performance on four closed-book biomedical
  machine reading comprehension benchmarks using various prompting strategies. The
  authors introduce a novel Implicit Retrieval Augmented Generation (RAG) technique
  that asks the LLM to first retrieve relevant text extracts from the context before
  answering the query, eliminating the need for vector databases.
---

# Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine Reading Comprehension

## Quick Facts
- arXiv ID: 2405.18682
- Source URL: https://arxiv.org/abs/2405.18682
- Authors: Shubham Vatsal; Ayush Singh
- Reference count: 6
- Primary result: GPT-4 in zero-shot settings outperforms supervised models on biomedical MRC tasks, achieving state-of-the-art results on two benchmarks

## Executive Summary
This paper evaluates GPT-4's performance on four biomedical machine reading comprehension benchmarks using various prompting strategies, including a novel Implicit Retrieval Augmented Generation (RAG) technique. The authors demonstrate that GPT-4 in zero-shot settings can outperform supervised models, achieving new state-of-the-art results on two benchmarks. Their Implicit RAG method, which has the model directly identify relevant text extracts from context rather than using vector databases, ranks first on two datasets and second on the others. Qualitative analysis confirms that the retrieved sections are mostly relevant to the questions, validating the approach's effectiveness.

## Method Summary
The study evaluates GPT-4 on four closed-book biomedical MRC benchmarks using zero-shot prompting with multiple techniques: Basic, Chain-of-Thought (CoT), Analogical Reasoning (AR), and Implicit RAG. Implicit RAG prompts the LLM to first locate and extract the most relevant sections from the context before answering, eliminating the need for vector database retrieval. Experiments compare these methods across datasets with different MRC formats (cloze, extractive, multiple-choice), using accuracy and F1 metrics for evaluation. Human experts qualitatively assess the relevance of retrieved sections.

## Key Results
- GPT-4 in zero-shot settings outperforms supervised models, achieving new state-of-the-art results on two benchmarks
- Implicit RAG ranks first on two datasets and second on the others, eliminating the need for vector databases
- Qualitative analysis by human experts confirms that retrieved sections are mostly relevant to the questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit RAG outperforms traditional RAG by having the LLM directly identify relevant text extracts rather than relying on vector database retrieval.
- Mechanism: Instead of computing and storing embeddings in a vector database, the model is prompted to first locate and extract the most relevant sections from the context before answering. This reduces retrieval overhead and leverages the LLM's inherent ability to understand context.
- Core assumption: The LLM can reliably identify relevant sections without external retrieval tools, and these sections are sufficient for answering the query.
- Evidence anchors:
  - [abstract] "propose a prompting strategy named Implicit Retrieval Augmented Generation (RAG) that alleviates the need for using vector databases to retrieve important chunks in traditional RAG setups."
  - [section] "In our proposed novel prompting technique Implicit RAG, we completely ignore the overhead involved in getting the embeddings of the entire corpus and storing them in a vector database. Instead, we ask the LLM itself to find the most relevant text extracts or sections in the given context..."
  - [corpus] Weak - the corpus analysis shows no direct evidence of embedding or retrieval methods; this is inferred from the paper's description.
- Break condition: If the context is too long to fit in the model's context window, or if the LLM's ability to identify relevant sections is unreliable for certain question types.

### Mechanism 2
- Claim: Prompting strategies like Chain-of-Thought (CoT) and Analogical Reasoning (AR) improve performance by structuring the reasoning process and leveraging global knowledge.
- Mechanism: CoT breaks down complex questions into simpler sub-questions, guiding the model through logical steps. AR uses analogies to connect the query to known patterns, forcing the model to draw on its embedded knowledge.
- Core assumption: The model's internal knowledge is sufficiently rich and relevant to the biomedical domain to benefit from these reasoning strategies.
- Evidence anchors:
  - [abstract] "We experiment with different conventional prompting techniques as well as introduce our own novel prompting method."
  - [section] "The rationale behind using the CoT technique is that there may be multiple smaller questions that need to be answered first in order to conclude the answer of the final asked question."
  - [corpus] Weak - corpus analysis does not provide direct evidence of CoT or AR effectiveness; inferred from paper methodology.
- Break condition: If the biomedical domain knowledge in the model is insufficient or misaligned with the task, these strategies may not yield benefits.

### Mechanism 3
- Claim: Zero-shot GPT performance can surpass supervised models on biomedical MRC tasks due to its broad pre-training and reasoning capabilities.
- Mechanism: The model uses its pre-trained knowledge to understand and reason about biomedical text without needing task-specific fine-tuning, effectively generalizing from its training data.
- Core assumption: The model's pre-training data includes sufficient biomedical and reasoning-relevant information to perform well on these tasks without additional training.
- Evidence anchors:
  - [abstract] "Experiments show that modern-day LLMs like GPT even in a zero-shot setting can outperform supervised models, leading to new state-of-the-art (SoTA) results on two of the benchmarks."
  - [section] "Experiments show that modern-day LLMs like GPT even in a zero-shot setting can outperform supervised models, leading to new state-of-the-art (SoTA) results on two of the benchmarks."
  - [corpus] Weak - corpus analysis shows no direct evidence of zero-shot performance; inferred from the paper's experimental results.
- Break condition: If the biomedical domain is too specialized or the questions require up-to-date knowledge not present in the model's pre-training data.

## Foundational Learning

- Concept: Understanding MRC task variations (cloze, extractive, multiple-choice)
  - Why needed here: Different datasets use different MRC formats, requiring tailored prompting strategies.
  - Quick check question: Can you identify which MRC format each of the four datasets (ProcessBank, BioMRC, MASH-QA, CliCR) uses?

- Concept: Prompt engineering techniques (CoT, AR, RAG)
  - Why needed here: The study evaluates multiple prompting strategies to find the most effective one for biomedical MRC.
  - Quick check question: What is the key difference between Chain-of-Thought and Analogical Reasoning prompting?

- Concept: Zero-shot vs. supervised learning
  - Why needed here: The study compares zero-shot GPT performance against supervised models to demonstrate the effectiveness of pre-trained LLMs.
  - Quick check question: Why might a zero-shot model outperform a supervised model on certain tasks?

## Architecture Onboarding

- Component map: LLM (GPT-4) -> prompting templates -> biomedical MRC datasets -> evaluation metrics (accuracy, F1, EM) -> qualitative assessment by human experts
- Critical path: Prompt LLM -> Extract relevant sections (Implicit RAG) or reason through steps (CoT/AR) -> Generate answer -> Evaluate against ground truth
- Design tradeoffs: Implicit RAG reduces retrieval overhead but may struggle with very long contexts; CoT/AR improve reasoning but add computational steps; zero-shot avoids fine-tuning but relies on pre-training quality
- Failure signatures: Poor performance on questions requiring deep domain knowledge not in pre-training; failure to identify relevant sections in long contexts; overfitting to specific dataset patterns
- First 3 experiments:
  1. Run Basic prompting on a small subset of each dataset to establish baseline performance
  2. Implement and test Implicit RAG on the same subset to compare retrieval efficiency and accuracy
  3. Evaluate CoT and AR prompting on a diverse set of question types to assess reasoning improvements

## Open Questions the Paper Calls Out
None

## Limitations

- Dataset Size and Diversity: The total number of test examples across all datasets is relatively small, raising questions about generalizability to the broader biomedical domain.
- Prompt Template Specificity: Limited detail on exact prompt formulations creates uncertainty about replicability and potential overfitting to specific datasets.
- Human Evaluation Subjectivity: Qualitative analysis relies on human expert judgment, introducing potential subjectivity and bias.

## Confidence

- High Confidence: The claim that GPT-4 in zero-shot settings outperforms supervised models on biomedical MRC tasks.
- Medium Confidence: The assertion that Implicit RAG is superior to traditional RAG methods.
- Medium Confidence: The generalizability of prompting strategies across different biomedical MRC tasks.

## Next Checks

- Next Check 1: Conduct cross-dataset validation by applying the best-performing prompting strategy from one dataset to the other three datasets.
- Next Check 2: Perform ablation studies on the Implicit RAG technique by systematically varying the number of retrieved sections and comparing performance against traditional RAG.
- Next Check 3: Test the prompting strategies on a held-out biomedical dataset not seen during method development to assess real-world applicability.