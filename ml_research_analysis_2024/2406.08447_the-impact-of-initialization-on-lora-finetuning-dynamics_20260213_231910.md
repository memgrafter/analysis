---
ver: rpa2
title: The Impact of Initialization on LoRA Finetuning Dynamics
arxiv_id: '2406.08447'
source_url: https://arxiv.org/abs/2406.08447
tags:
- learning
- init
- lora
- initialization
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the impact of initialization schemes in
  Low Rank Adaptation (LoRA) for efficient fine-tuning of large language models. It
  demonstrates that initializing matrix A with random values and matrix B with zeros
  (Init[A]) leads to better performance compared to the opposite initialization (Init[B]).
---

# The Impact of Initialization on LoRA Finetuning Dynamics

## Quick Facts
- arXiv ID: 2406.08447
- Source URL: https://arxiv.org/abs/2406.08447
- Authors: Soufiane Hayou; Nikhil Ghosh; Bin Yu
- Reference count: 40
- Key outcome: Initializing matrix A with random values and matrix B with zeros (Init[A]) leads to better performance than the opposite initialization (Init[B]) in LoRA fine-tuning

## Executive Summary
This paper investigates how different initialization schemes for Low-Rank Adaptation (LoRA) affect the fine-tuning dynamics of large language models. The authors demonstrate that initializing matrix A with random values while keeping matrix B at zero (Init[A]) leads to superior performance compared to the opposite approach (Init[B]). Through theoretical analysis and empirical validation across multiple tasks and model architectures, they show that Init[A] enables the use of larger learning rates without causing output instability, resulting in more efficient feature learning despite some internal instability in intermediate features.

## Method Summary
The paper employs a combination of theoretical analysis and empirical experiments to compare two LoRA initialization schemes. The theoretical component uses asymptotic analysis with a γ-operator framework to track how quantities scale with width n, deriving bounds on learning rates that maintain stability. Empirically, the authors conduct experiments on synthetic data from teacher models, GLUE benchmark tasks using RoBERTa-large, TinyLlama on WikiText-2, and Llama-7b on Flan-v2 and GSM8k datasets. They perform grid searches over learning rates for both initialization schemes and compare performance metrics including accuracy, perplexity, and MMLU scores.

## Key Results
- Init[A] consistently outperforms Init[B] across multiple language tasks and model architectures
- The optimal learning rate with Init[A] scales as Θ(n^-1/2) while Init[B] is limited to Θ(n^-1)
- With Init[A], internal features ZA grow as Θ(n^1/2) but output features ZB remain stable at Θ(1)
- Both initialization schemes exhibit suboptimal feature learning in the infinite-width limit

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Initializing matrix A with random values and matrix B with zeros (Init[A]) leads to better performance than the opposite initialization (Init[B]).
- **Mechanism:** The key difference lies in how the two initialization schemes affect the asymptotic behavior of LoRA feature updates. With Init[A], the learning rate can be scaled as Θ(n^-1/2) without causing output instability, while with Init[B] the maximal learning rate is limited to Θ(n^-1). This allows Init[A] to use larger learning rates, which leads to more efficient feature learning at the cost of some internal instability in the ZA features.
- **Core assumption:** The analysis assumes a large-width limit where the width n is the only scaling dimension, and all other parameters (rank r, depth L, etc.) are fixed.
- **Evidence anchors:**
  - [abstract] "The theoretical analysis reveals that Init[A] allows the use of larger learning rates without causing output instability, resulting in more efficient feature learning."
  - [section 3.5] "With Init[A], the maximal learning rate that does not lead to instability in ZB scales as Θ(n^-1/2). This can be seen as an asymptotic form of the edge of stability phenomenon..."
  - [corpus] Weak - no direct citations to initialization studies found.
- **Break condition:** The advantage of Init[A] diminishes at small widths where the constants in the Θ notation dominate, or if the optimal learning rate with Init[B] happens to be larger due to task-specific constants.

### Mechanism 2
- **Claim:** Init[A] causes "internal instability" where ZA features grow as Θ(n^1/2) but ZB remains Θ(1).
- **Mechanism:** With Init[A], initializing B to zero and A to random values means that as training progresses, the product AtZ grows with width, while Bt remains small (Θ(n^-1/2)). This creates a trade-off where internal features (ZA) can become large, but the output features (ZB) remain stable.
- **Core assumption:** The processed gradients satisfy gt_AZ = Θ(n), which is justified by the structure of gradient updates in Adam-like optimizers.
- **Evidence anchors:**
  - [section 3.5] "the features are efficiently updated... However, this comes with caveat: the features Z^A grow as Θ(n^1/2) which can potentially cause numerical instabilities."
  - [section 3.6] "Because of this bound on the maximal learning rate, no internal instability occurs with Init[B]."
  - [corpus] Weak - no direct citations to internal instability studies found.
- **Break condition:** If the width is not large enough for the asymptotic behavior to dominate, or if the training dynamics deviate significantly from the assumed setup.

### Mechanism 3
- **Claim:** Init[B] leads to suboptimal feature learning because matrix B is undertrained in the large width limit.
- **Mechanism:** With Init[B], initializing A to zero and B to random values means that the update to B becomes negligible (δ2_t → 0) in the large width limit. This results in B being undertrained, which limits the effectiveness of the adaptation.
- **Core assumption:** The update to B is dominated by the gradient term involving Z t^-1_A, which vanishes as width grows.
- **Evidence anchors:**
  - [section 3.6] "With Init[B], the maximal learning rate (that does not violate stability) scales as Θ(n^-1)... Because of this bound on the maximal learning rate, no internal instability occurs with Init[B]."
  - [section 3.6] "Feature Learning: ∆Z^B = Θ(1) if and only if γ[η] = -1. Moreover, efficient feature learning cannot be achieved with Init[B] for any choice of learning rate scaling γ[η]."
  - [corpus] Weak - no direct citations to B undertraining studies found.
- **Break condition:** If the rank r grows with width, or if the training setup deviates significantly from the assumed single-LoRA-layer scenario.

## Foundational Learning

- **Concept:** Asymptotic analysis and the γ-operator
  - **Why needed here:** The paper relies heavily on tracking how quantities scale with width n using the γ-operator, which captures polynomial dependencies. Understanding this framework is crucial for following the theoretical arguments.
  - **Quick check question:** If v = n^2 and w = n^3, what is γ[v + w] according to the paper's rules?

- **Concept:** LoRA (Low-Rank Adaptation) mechanism
  - **Why needed here:** The entire paper is about comparing different initialization schemes for LoRA. Understanding how LoRA works - the BA update to pretrained weights - is fundamental to following the analysis.
  - **Quick check question:** In standard LoRA, what constraint ensures that finetuning starts from the pretrained model?

- **Concept:** Feature learning vs. stability trade-off
  - **Why needed here:** The paper's main theoretical contribution is identifying this trade-off in LoRA training. Understanding what constitutes "efficient feature learning" and "stability" in the context of the paper is essential.
  - **Quick check question:** According to Definition 5, what two conditions must be satisfied for LoRA fine-tuning to be considered "efficient"?

## Architecture Onboarding

- **Component map:** LoRA adapter -> Low-rank matrices A (r×n) and B (n×r) added to pretrained weights -> Initialization schemes: Init[A] (A random, B zero) vs Init[B] (B random, A zero) -> Training: Single LoRA layer assumed for theoretical analysis, multiple layers in practice -> Hyperparameters: Learning rate η (critical), rank r, α scaling factor

- **Critical path:**
  1. Initialize LoRA matrices (Init[A] or Init[B])
  2. Forward pass: Z = W^∗Z + α/r BA Z
  3. Backward pass: Compute gradients ∂L/∂A and ∂L/∂B
  4. Update: A_t = A_t-1 - ηg_A, B_t = B_t-1 - ηg_B
  5. Check stability: Z_B = O(1) as n → ∞
  6. Measure feature learning efficiency: δ1_t, δ2_t = Θ(1)

- **Design tradeoffs:**
  - Init[A]: Larger optimal learning rate (Θ(n^-1/2)) but internal instability in ZA
  - Init[B]: Smaller optimal learning rate (Θ(n^-1)) but stable training
  - Choice depends on width n and tolerance for internal instability

- **Failure signatures:**
  - Instability: Z_B grows unbounded as width increases
  - Undertraining: One of the LoRA matrices (typically B with Init[B]) doesn't contribute to updates
  - Vanishing updates: Feature updates δ1_t or δ2_t become negligible in the large width limit

- **First 3 experiments:**
  1. Replicate the synthetic teacher-student experiment with varying widths (n = 128, 256, 512, 1024, 2048) to observe the stability/feature learning trade-off
  2. Implement the γ-operator tracking to verify the recursive formulas for Z^A and B_t scaling
  3. Run GLUE benchmark experiments with both Init[A] and Init[B] on RoBERTa-large to compare performance and optimal learning rates

## Open Questions the Paper Calls Out
- Does the internal instability observed with Init[A] (growing magnitude of ZA features) persist or diminish in larger models beyond the widths tested in this paper?
- Can the suboptimal feature learning observed with both Init[A] and Init[B] in the infinite-width limit be effectively mitigated by approaches like LoRA+ or other methods?
- Is there a principled way to determine the optimal learning rate for Init[A] and Init[B] beyond empirical grid search, perhaps based on model width or other architectural factors?

## Limitations
- Theoretical analysis relies on asymptotic behavior in the large-width limit which may not fully capture finite-width effects
- Analysis assumes a single LoRA layer while practical applications typically use multiple layers
- The internal instability mechanism could lead to numerical issues in finite-precision implementations

## Confidence
- **High Confidence**: The empirical validation showing Init[A] consistently outperforming Init[B] across multiple tasks and models is robust and well-documented.
- **Medium Confidence**: The asymptotic analysis showing the Θ(n^-1/2) vs Θ(n^-1) scaling of maximal learning rates is mathematically sound, but finite-width corrections are not fully characterized.
- **Low Confidence**: The assumption that the optimal rank remains fixed as width scales is not thoroughly validated.

## Next Checks
1. Conduct experiments with varying widths (n = 128, 256, 512, 1024, 2048) to verify the predicted scaling of optimal learning rates and identify the point where asymptotic behavior dominates.
2. Extend the theoretical analysis to multi-layer LoRA setups where multiple LoRA adapters are applied.
3. Implement high-precision training to systematically investigate the practical limits of internal instability with Init[A].