---
ver: rpa2
title: 'High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization
  and Scaling Laws'
arxiv_id: '2410.18837'
source_url: https://arxiv.org/abs/2410.18837
tags:
- risk
- surrogate
- where
- target
- proposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a sharp characterization of knowledge distillation
  for high-dimensional ridgeless regression under model shift and distribution shift.
  It precisely describes the optimal surrogate model in terms of feature covariance
  and sample size, revealing an amplify-to-shrink phase transition across the eigenspectrum.
---

# High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws

## Quick Facts
- arXiv ID: 2410.18837
- Source URL: https://arxiv.org/abs/2410.18837
- Reference count: 40
- One-line primary result: Optimal surrogate models in knowledge distillation amplify certain features and shrink others based on eigenspectrum position, provably outperforming strong labels while maintaining the same scaling law.

## Executive Summary
This work provides a sharp characterization of knowledge distillation for high-dimensional ridgeless regression under model shift and distribution shift. The analysis precisely describes the optimal surrogate model in terms of feature covariance and sample size, revealing an amplify-to-shrink phase transition across the eigenspectrum. The optimal surrogate can strictly outperform strong labels but cannot improve the scaling law. The paper also establishes a non-asymptotic risk characterization for the two-stage process where the surrogate is itself learned via ERM.

## Method Summary
The method analyzes a two-stage knowledge distillation process in high-dimensional linear regression. In Stage 1, a surrogate model is trained on surrogate data to obtain optimal parameters. In Stage 2, a target model is trained on synthetic labels generated by the optimal surrogate. The analysis uses asymptotic random matrix theory to characterize the optimal surrogate parameters and derive risk bounds. The key insight is that the optimal surrogate selectively amplifies or shrinks different features based on their covariance structure, creating a phase transition in the per-feature gain.

## Key Results
- Optimal surrogate models exhibit a phase transition where features are amplified or shrunk based on their position in the eigenspectrum
- Weak-to-strong generalization can outperform training with strong labels under the same data budget
- The scaling law for excess risk remains unchanged between surrogate-to-target and standard target models
- Feature selection through mask operations can significantly improve surrogate-to-target performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal surrogate model in knowledge distillation for high-dimensional ridgeless regression amplifies certain features and shrinks others depending on their eigenspectrum position.
- Mechanism: For each feature, the optimal surrogate gain is determined by a threshold behavior based on covariance statistics ζi and sample size. Features before the transition point ζi = 1 - Ω are amplified (gain > 1), while those after are shrunk (gain < 1).
- Core assumption: The covariance matrix eigenvalues are not constant, and the problem operates in the over-parameterized regime (p > n).
- Evidence anchors:
  - [abstract] "This unveils a remarkable phenomenology in the process of knowledge distillation, that can be described as follows. Define the per-feature 'gain' of the optimal surrogate as gain = βs/β⋆ where the division is entrywise... There is a well-defined transition point, ζi = 1 − Ω in Fig. 1a, where the gain passes from strict amplification (gain > 1) to strict shrinkage (gain < 1)"
  - [section 3] "Corollary 1. Without loss of generality, suppose that Σt is diagonal... The first part shows that the optimal surrogate parameter is fully characterized by (ζi)p i=1, which only depends on the covariance spectrum (via λi) and the sample size n (via τt)"
- Break condition: If the covariance matrix is a multiple of the identity (Σ = cI), then the optimal surrogate equals the ground truth β⋆, eliminating any improvement opportunity.

### Mechanism 2
- Claim: The optimal surrogate model can outperform training with strong labels (ground truth) but cannot improve the data scaling law.
- Mechanism: By carefully selecting which features to amplify or shrink based on the eigenspectrum, the optimal surrogate reduces bias while maintaining variance control. This creates a strict improvement in test risk but doesn't change the fundamental scaling behavior with respect to sample size.
- Core assumption: Both eigenvalues and signal coefficients follow power-law decay structures.
- Evidence anchors:
  - [abstract] "In the context of weak-to-strong (W2S) generalization, this has the interpretation that (i) W2S training, with the surrogate as the weak model, can provably outperform training with strong labels under the same data budget, but (ii) it is unable to improve the data scaling law"
  - [section 4] "Proposition 6 (Scaling law). Let the covariance matrix Σt be diagonal with eigenvalues λi, and let the ground-truth parameter β⋆ have components βi... The excess test risk of the surrogate-to-target model with an optimal surrogate parameter scales the same as that of the standard target model"
- Break condition: If the noise variance doesn't decay with sample size (σ² = Θ(1)), it can overshadow the signal improvement and negate the scaling law benefit.

### Mechanism 3
- Claim: Weak-to-strong generalization succeeds when the strong model can avoid fitting the mistakes of the weak teacher by feature selection.
- Mechanism: The optimal surrogate acts as a weak supervisor by discarding features whose contribution would increase risk. This creates a mask operation M that selects only beneficial features, allowing the strong model to learn from cleaner supervision.
- Core assumption: The surrogate has access to fewer features than the target (ps < p), creating a natural weak-to-strong hierarchy.
- Evidence anchors:
  - [abstract] "We validate our results on numerical experiments both on ridgeless regression and on neural network architectures"
  - [section 3.1] "Proposition 3. Consider the target model in (6), assume that Σt is diagonal... If the mask operation M selects all the features that satisfy 1 − ζ²i > Ω, then the surrogate-to-target model outperforms the standard target model"
- Break condition: If the surrogate model cannot follow the optimal feature selection mechanism (as in the CIFAR-10 experiment), the surrogate-to-target model will underperform the standard target model.

## Foundational Learning

- Concept: High-dimensional linear regression with ridgeless minimum norm interpolation
  - Why needed here: The analysis fundamentally relies on understanding how over-parameterized models behave when fitting data exactly, particularly the implicit regularization effects that arise
  - Quick check question: In high-dimensional linear regression with p > n, what is the form of the minimum norm interpolator and how does it depend on the covariance structure?

- Concept: Random matrix theory and asymptotic analysis of linear estimators
  - Why needed here: The precise characterization of risk requires tools from random matrix theory to handle the complex interactions between feature covariance, sample size, and model parameters
  - Quick check question: How does the asymptotic risk of a linear estimator depend on the sample size ratio κ = p/n and the covariance eigenvalues?

- Concept: Distribution shift and transfer learning theory
  - Why needed here: The knowledge distillation problem inherently involves distribution shift between surrogate and target data, requiring understanding of how models generalize under such shifts
  - Quick check question: What is the relationship between model shift (different ground truth parameters) and covariance shift in transfer learning?

## Architecture Onboarding

- Component map: Surrogate Data Generation -> Optimal Surrogate Parameter Computation (βs*) -> Synthetic Label Generation -> Target Model Training -> Risk Evaluation
- Critical path: The critical path is: sample surrogate data → compute optimal surrogate parameter βs* → generate synthetic labels → train target model on synthetic labels → evaluate target risk. The bottleneck is computing βs* efficiently for large-scale problems.
- Design tradeoffs: The main tradeoff is between the benefit of using surrogate labels (strict risk improvement) versus the complexity of computing the optimal surrogate. Another tradeoff is between using a sparse surrogate (feature selection) versus a dense one - sparse surrogates can be better but require feature selection capability.
- Failure signatures: The surrogate-to-target model will fail if the covariance matrices are too similar (Σs ≈ Σt), if the feature selection mechanism is not implemented correctly, or if the problem is under-parameterized (n > p). In these cases, the surrogate model cannot outperform the standard target model.
- First 3 experiments:
  1. Verify the amplify-to-shrink phase transition by plotting the optimal surrogate gain versus eigenvalue index for synthetic data with power-law covariance structure
  2. Test weak-to-strong generalization by implementing the optimal mask operation M and comparing performance with and without feature selection
  3. Validate the scaling law by varying sample size n and confirming that the excess risk scales as Θ(n−(β−1)) for β < 2α + 1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal surrogate model behave in the limit of extremely large p relative to n?
- Basis in paper: Explicit - discussed in Section 4 with asymptotic analysis of τt and Ω
- Why unresolved: The paper provides asymptotic expressions but doesn't fully characterize the behavior as p → ∞
- What evidence would resolve it: Mathematical proofs showing the limiting behavior of the optimal surrogate parameters and risks as p grows much larger than n

### Open Question 2
- Question: Can the two-stage distillation process be extended to multiple stages, and would this improve performance?
- Basis in paper: Explicit - mentioned as an open direction in the concluding remarks
- Why unresolved: The paper only analyzes two-stage processes
- What evidence would resolve it: Theoretical analysis of multi-stage distillation processes and experimental validation showing performance improvements

### Open Question 3
- Question: How does knowledge distillation perform in neural network architectures beyond linear regression?
- Basis in paper: Explicit - mentioned as an open direction in the concluding remarks
- Why unresolved: The paper focuses on linear regression models
- What evidence would resolve it: Non-asymptotic risk characterizations for neural networks under knowledge distillation and empirical studies comparing distillation performance across different architectures

### Open Question 4
- Question: What is the exact relationship between the scaling law exponents for standard target models versus optimal surrogate-to-target models?
- Basis in paper: Explicit - discussed in Section 4 but with some open aspects regarding the exact relationship
- Why unresolved: The paper shows the exponents are the same but doesn't provide a tight characterization of the constant factors
- What evidence would resolve it: Mathematical proofs characterizing the exact relationship between scaling law constants for different model types under various data distributions

## Limitations
- Analysis relies heavily on specific asymptotic assumptions including power-law eigenvalue distributions and specific sample size scalings
- Optimal surrogate parameter computation requires numerical methods that may not scale efficiently to very high dimensions
- Theoretical results are primarily validated through synthetic experiments rather than real-world datasets

## Confidence

**High confidence:** The amplify-to-shrink phase transition characterization - this follows directly from the analytical expressions in Proposition 1 and Corollary 1

**Medium confidence:** The scaling law results - while the theoretical framework is sound, the constant factors depend on specific parameter choices that require careful numerical validation

**Medium confidence:** The weak-to-strong generalization mechanism - the theoretical conditions are well-established but practical implementation of optimal feature selection may be challenging

## Next Checks
1. Implement numerical experiments to verify the phase transition boundary ζᵢ = 1 - Ω across different power-law exponents α and sample sizes n
2. Test the optimal mask operation M on synthetic data with varying feature dimensions p and sample sizes n to confirm Proposition 3 conditions
3. Conduct ablation studies on the noise variance scaling σ² = O(n⁻ᵞ) to determine the minimum decay rate required for the scaling law to hold