---
ver: rpa2
title: 'Towards Explainable Goal Recognition Using Weight of Evidence (WoE): A Human-Centered
  Approach'
arxiv_id: '2409.11675'
source_url: https://arxiv.org/abs/2409.11675
tags:
- goal
- explanations
- recognition
- explanation
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a human-centered approach to explainable goal
  recognition (GR) using Weight of Evidence (WoE). It introduces a conceptual framework
  derived from human studies on how people explain others' behavior, identifying key
  concepts like observational markers.
---

# Towards Explainable Goal Recognition Using Weight of Evidence (WoE): A Human-Centered Approach

## Quick Facts
- arXiv ID: 2409.11675
- Source URL: https://arxiv.org/abs/2409.11675
- Authors: Abeer Alshehri; Amal Abdulrahman; Hajar Alamri; Tim Miller; Mor Vered
- Reference count: 17
- Primary result: Proposes XGR model using WoE to generate human-understandable explanations for goal recognition, validated through computational benchmarks and user studies

## Executive Summary
This paper introduces a human-centered approach to explainable goal recognition (GR) using Weight of Evidence (WoE). The authors develop a conceptual framework based on human studies of how people explain others' behavior, identifying key concepts like observational markers. The eXplainable Goal Recognition (XGR) model leverages WoE to generate explanations for both why and why not questions about goal predictions. The approach is validated through computational evaluation on eight GR benchmarks and three user studies, demonstrating significant improvements in user understanding, trust, and decision-making compared to baseline models.

## Method Summary
The XGR model integrates WoE with goal recognition to provide explanations for both why a particular goal was recognized and why other goals were not. The approach begins with a conceptual framework derived from human studies on behavioral explanations, identifying observational markers as key concepts. The model then applies WoE to compute and present these explanations in a human-understandable format. Computational evaluation across eight benchmarks demonstrates the model's effectiveness, while user studies validate improvements in user comprehension and trust.

## Key Results
- XGR achieves added computation cost of only 0.2%-45% across eight GR benchmarks while providing explainability
- Three user studies show XGR significantly enhances user understanding, trust, and decision-making compared to baseline models
- The approach successfully addresses the need for transparent and understandable goal predictions in AI systems

## Why This Works (Mechanism)
The approach works by grounding explanations in how humans naturally explain behavior to each other. By identifying observational markers through human studies and applying WoE, the model generates explanations that align with human reasoning patterns. This human-centered design makes the explanations more intuitive and actionable for users.

## Foundational Learning
- Weight of Evidence (WoE): A statistical measure that quantifies the evidence supporting a hypothesis; needed to compute the strength of evidence for goal predictions; quick check: verify WoE calculations match theoretical expectations
- Observational markers: Key indicators humans use to infer goals from behavior; needed to ground explanations in human reasoning patterns; quick check: validate markers through human studies
- Explainable AI frameworks: Systematic approaches to making AI decisions interpretable; needed to structure the explanation generation process; quick check: ensure explanations address user needs
- Goal recognition benchmarks: Standardized datasets for evaluating GR performance; needed to provide objective computational validation; quick check: verify benchmark selection covers diverse scenarios
- User study methodology: Systematic approaches to evaluating human-AI interaction; needed to validate the effectiveness of explanations; quick check: ensure appropriate sample sizes and controls

## Architecture Onboarding
- Component map: User query -> Observational marker identification -> WoE computation -> Explanation generation -> User interface
- Critical path: The user provides a "why" or "why not" question -> The system identifies relevant observational markers -> WoE scores are computed for competing goals -> Explanations are generated and presented to the user
- Design tradeoffs: Computational overhead (0.2%-45%) vs. explainability benefits; more detailed explanations provide better understanding but increase computation time
- Failure signatures: Incorrect observational marker identification can lead to misleading explanations; WoE misinterpretation can result in overconfidence or underconfidence in explanations
- First experiments: 1) Run computational benchmarks to measure performance overhead; 2) Conduct pilot user study with simple scenarios; 3) Test explanation generation with edge cases and ambiguous observations

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample sizes (20-30 participants) in user studies limit generalizability
- Computational overhead varies significantly (0.2%-45%) without deep exploration of driving factors
- Focus on only "why" and "why not" questions addresses limited subset of explanatory needs
- No exploration of potential biases in observational markers derived from human studies

## Confidence
- High: The core methodology of using WoE for generating goal recognition explanations is technically sound and well-validated through computational benchmarks
- Medium: The effectiveness of explanations for improving user understanding, trust, and decision-making, while supported by user studies, would benefit from larger-scale validation across more diverse user populations and real-world scenarios
- Low: The scalability of the approach to significantly larger or more complex goal recognition problems, and the robustness of explanations when dealing with noisy or incomplete observational data

## Next Checks
1. Conduct a longitudinal study tracking user performance and trust over extended periods to assess the durability of explanation benefits
2. Test the approach on significantly larger goal recognition benchmarks with hundreds of possible goals to evaluate scalability
3. Perform cross-cultural validation to ensure the observational markers and explanations are equally effective across different cultural contexts