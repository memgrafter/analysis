---
ver: rpa2
title: Multi-Style Facial Sketch Synthesis through Masked Generative Modeling
arxiv_id: '2408.12400'
source_url: https://arxiv.org/abs/2408.12400
tags:
- sketch
- facial
- ieee
- masked
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of facial sketch synthesis (FSS),
  aiming to generate high-quality sketch portraits from facial photographs. The authors
  propose a lightweight end-to-end model based on masked generative modeling to tackle
  challenges such as data scarcity, limited style types, and deficiencies in processing
  input information.
---

# Multi-Style Facial Sketch Synthesis through Masked Generative Modeling

## Quick Facts
- arXiv ID: 2408.12400
- Source URL: https://arxiv.org/abs/2408.12400
- Authors: Bowen Sun; Guo Lu; Shibao Zheng
- Reference count: 40
- Outperforms state-of-the-art methods with LPIPS 0.337, FID 26.79, SSIM 0.909, FSIM 0.377, and SCOOT 0.533

## Executive Summary
This paper introduces a lightweight end-to-end model for facial sketch synthesis that addresses key challenges in the field including data scarcity, limited style types, and processing inefficiencies. The approach leverages masked generative modeling with semi-supervised learning, using pre-training on synthetic sketches followed by fine-tuning on limited real data. The model employs a transformer-based architecture with style embeddings that enable continuous multi-style synthesis without requiring separate training for each style. Experimental results demonstrate significant improvements over existing methods across multiple benchmarks, with the model achieving state-of-the-art performance in all evaluated metrics.

## Method Summary
The proposed method uses a two-stage training process: first pre-training a transformer on synthesized sketches using masked image modeling, then fine-tuning on real hand-drawn sketches. The architecture consists of a CLIP-L/14 feature encoder, a modified U-ViT transformer that predicts masked image tokens, and a VQ-GAN decoder that converts tokens back to images. Style embeddings are integrated through cross-attention and normalization layers to enable multi-style generation. The model processes images in a compressed 16×16 token space for computational efficiency. Training uses a cosine mask schedule and combines perceptual and pixel losses during fine-tuning.

## Key Results
- Achieves LPIPS score of 0.337, surpassing previous state-of-the-art methods
- Obtains FID score of 26.79 with significant margin over competing approaches
- Demonstrates superior SSIM (0.909), FSIM (0.377), and SCOOT (0.533) scores
- Successfully generates continuous intermediate styles through smooth interpolation of style parameters

## Why This Works (Mechanism)

### Mechanism 1
Masked generative modeling overcomes data scarcity by allowing semi-supervised training on synthesized sketches plus limited real data. The model learns to reconstruct fully masked image tokens conditioned on facial features and style embeddings. Pre-training on synthetic sketches provides a rich source of data without requiring expensive hand-drawn pairs. Core assumption: Synthetic sketches generated by [12] are sufficiently similar to real sketches in distribution for effective pre-training.

### Mechanism 2
Style parameters enable continuous multi-style synthesis without needing separate training per style. Style embeddings are injected into the transformer's cross-attention and normalization layers, allowing smooth interpolation between styles during inference. Core assumption: The transformer can generalize style representations from limited discrete styles to continuous intermediate styles.

### Mechanism 3
VQ-tokenizer/decoder with latent space manipulation reduces computational load while maintaining quality. Images are quantized into 16×16 tokens (256 total), reducing dimensionality for transformer processing compared to full-resolution images. Core assumption: The VQ-GAN latent space preserves sufficient information for high-quality sketch reconstruction.

## Foundational Learning

- Concept: Masked Image Modeling (MIM)
  - Why needed here: Enables self-supervised pre-training on unlabeled data by predicting masked image tokens
  - Quick check question: What is the masking ratio schedule used in this paper, and why is it important for training stability?

- Concept: Style Embedding Conditioning
  - Why needed here: Allows generation of multiple sketch styles from a single model without retraining
  - Quick check question: How are style embeddings integrated into the transformer architecture in this work?

- Concept: VQ-GAN Tokenization
  - Why needed here: Provides compressed latent representation for efficient transformer processing
  - Quick check question: What is the vocabulary size of the VQ-GAN used, and how does it affect reconstruction quality?

## Architecture Onboarding

- Component map: Feature Encoder (CLIP-L/14) → Transformer (U-ViT) → Decoder (VQ-GAN) → Output Image
- Critical path: Feature extraction → masked token prediction → token decoding → final image
- Design tradeoffs:
  - Latent space compression (16×16) trades off some detail for computational efficiency
  - Pre-training on synthetic data reduces need for real sketch pairs but may introduce domain gap
  - Continuous style parameters enable interpolation but require smooth embedding space
- Failure signatures:
  - Background artifacts suggest issues with feature encoder or token reconstruction
  - Style inconsistency indicates problems with style embedding conditioning
  - Low quality in fine details points to limitations in VQ-GAN reconstruction
- First 3 experiments:
  1. Train transformer alone with MIM loss on synthetic data; evaluate reconstruction quality
  2. Add feature encoder conditioning; test if facial features improve reconstruction
  3. Integrate style embeddings; verify multi-style capability with different parameter values

## Open Questions the Paper Calls Out

### Open Question 1
How does the semi-supervised learning approach perform when trained on datasets with varying ratios of labeled to unlabeled data? The authors mention incorporating semi-supervised learning into the training process to overcome data insufficiency, but do not provide experimental results or analysis on the performance of the model when trained with different ratios of labeled to unlabeled data.

### Open Question 2
What is the impact of using different feature extraction models on the quality of the synthesized sketches? The authors use CLIP-L/14 as the feature encoder, stating it has exceptional face recognition capabilities, but do not explore or compare the performance of different feature extraction models in the context of facial sketch synthesis.

### Open Question 3
How does the model's performance scale with larger and more diverse datasets? The authors highlight the challenges of data scarcity and limited style types, suggesting that larger and more diverse datasets could improve performance, but do not provide experiments or analysis on the model's performance with larger and more diverse datasets.

## Limitations

- The critical assumption that synthetic sketches are sufficiently similar to real hand-drawn sketches for effective pre-training is not empirically validated
- Architecture details, particularly the exact transformer configuration and integration of CLIP features, remain underspecified
- The claim that masked generative modeling specifically addresses data insufficiency lacks ablation studies isolating the contribution of pre-training on synthetic data

## Confidence

- High Confidence: Quantitative results showing superior performance on standard metrics (LPIPS, FID, SSIM, FSIM, SCOOT)
- Medium Confidence: Mechanism of style embedding conditioning for multi-style synthesis
- Low Confidence: Claim that masked generative modeling specifically addresses data insufficiency

## Next Checks

1. Conduct a statistical comparison between synthetic and real sketch distributions using domain adaptation metrics to verify the effectiveness of pre-training on synthesized data

2. Generate interpolated styles between discrete styles and perform user studies or perceptual metrics to assess the consistency and quality of intermediate styles

3. Request or reconstruct the exact transformer architecture details, including layer configurations and attention mechanisms, to enable precise replication of the results