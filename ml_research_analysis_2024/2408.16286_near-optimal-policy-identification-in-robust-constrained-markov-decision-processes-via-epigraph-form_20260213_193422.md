---
ver: rpa2
title: Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes
  via Epigraph Form
arxiv_id: '2408.16286'
source_url: https://arxiv.org/abs/2408.16286
tags:
- policy
- where
- lemma
- equation
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of identifying near-optimal policies
  in robust constrained Markov decision processes (RCMDPs), where the goal is to minimize
  worst-case cost while satisfying constraints across uncertain environments. It identifies
  a key issue with the conventional Lagrangian approach, where policy gradients can
  conflict and trap the algorithm in suboptimal solutions.
---

# Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes via Epigraph Form

## Quick Facts
- arXiv ID: 2408.16286
- Source URL: https://arxiv.org/abs/2408.16286
- Reference count: 40
- Key outcome: First theoretical guarantee for near-optimal policy identification in RCMDPs with Õ(ε^{-4}) robust policy evaluations

## Executive Summary
This paper addresses the challenge of finding near-optimal policies in robust constrained Markov decision processes (RCMDPs), where the goal is to minimize worst-case cost while satisfying constraints across uncertain environments. The authors identify a critical flaw in the conventional Lagrangian approach: gradient conflicts between objective and constraint functions can trap the algorithm in suboptimal solutions. To overcome this, they propose leveraging the epigraph form of RCMDPs, which avoids these conflicts by selecting between objective or constraint gradients. They develop EpiRC-PGS, a new algorithm combining bisection search with a policy gradient subroutine, and prove it can identify ε-optimal policies with Õ(ε^{-4}) robust policy evaluations.

## Method Summary
The paper tackles RCMDPs by reformulating them using epigraph form, introducing an auxiliary variable y and optimizing minπ max{f(π) - y, h(π)} instead of the standard Lagrangian sum. The EpiRC-PGS algorithm uses a double-loop structure: an outer bisection search on threshold b0 to find the optimal objective value J*, and an inner projected policy gradient subroutine to solve the auxiliary problem minπ∈Π maxn∈J0,N K Jcn,U(π) - bn. The algorithm iteratively refines the search space using monotonicity of the constraint violation function, ultimately returning a policy that is ε-optimal for both objective and constraint satisfaction.

## Key Results
- Proves that Lagrangian policy gradients suffer from gradient conflicts that trap algorithms in suboptimal stationary points
- Shows epigraph form avoids gradient conflicts by selecting between objective or constraint gradients
- Guarantees finding ε-optimal policy with Õ(ε^{-4}) robust policy evaluations
- Demonstrates empirically that EpiRC-PGS outperforms Lagrangian approaches on tabular RCMDP benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The epigraph form avoids gradient conflicts that trap the Lagrangian approach in suboptimal solutions.
- Mechanism: By introducing an auxiliary variable `y` and using `max{f(π) - y, h(π)}` instead of summing gradients, the epigraph formulation selects only one gradient (either objective or constraint) rather than adding conflicting gradients together.
- Core assumption: The policy gradient method can efficiently optimize the auxiliary problem without getting stuck in local minima.
- Evidence anchors:
  - [abstract]: "This occurs when its inner minimization encounters a sum of conflicting gradients from the objective and constraint functions."
  - [section]: Theorem 1 demonstrates gradient conflict in Lagrangian formulation with Example 1 showing π2 is a stationary point but not optimal.
  - [corpus]: Weak - no direct citations about epigraph form avoiding gradient conflicts in RCMDPs.

### Mechanism 2
- Claim: The bisection search on the threshold variable `b0` guarantees finding the optimal objective value.
- Mechanism: The algorithm exploits monotonicity of `∆*b0` (minimum constraint violation as a function of `b0`) to perform binary search, ensuring convergence to `b0 = J*`.
- Core assumption: We can accurately evaluate `∆b0(π)` and its subgradients for any policy π and threshold `b0`.
- Evidence anchors:
  - [section]: Lemma 2 proves `∆*b0` is monotonically decreasing and `∆*J* = 0`.
  - [section]: Theorem 3 guarantees finding near-optimal policy after K iterations with specific accuracy bounds.
  - [corpus]: Weak - no direct citations about bisection search in RCMDPs.

### Mechanism 3
- Claim: The projected policy gradient subroutine finds stationary points that are actually optimal for the auxiliary problem.
- Mechanism: Using subgradients from `Gb0(π)` (worst-case cost functions) and projection onto policy space, the algorithm converges to policies where gradient conflicts don't exist.
- Core assumption: Under Assumption 5 (initial distribution coverage), all stationary points of `∆b0(π)` are globally optimal.
- Evidence anchors:
  - [section]: Theorem 4 proves optimality of stationary points under coverage assumption.
  - [section]: Theorem 5 shows convergence with specific iteration complexity `O(ε^-4)`.
  - [corpus]: Weak - no direct citations about optimality of stationary points in RCMDP epigraph form.

## Foundational Learning

- Concept: Subgradient calculus for weakly convex functions
  - Why needed here: The auxiliary problem `∆b0(π)` is weakly convex, requiring subgradient methods rather than standard gradients
  - Quick check question: What's the difference between a subgradient and a gradient, and when do we use each?

- Concept: Robust MDP evaluation with uncertainty sets
  - Why needed here: The algorithm needs to compute worst-case returns `Jcn,U(π)` for arbitrary uncertainty sets
  - Quick check question: How does the (s,a)-rectangular assumption simplify robust evaluation compared to general uncertainty sets?

- Concept: Moreau envelope and its gradient properties
  - Why needed here: The convergence proof uses Moreau envelope of the weakly convex objective to establish gradient bounds
  - Quick check question: What's the relationship between the Moreau envelope gradient and the original function's subgradient?

## Architecture Onboarding

- Component map:
  - Outer loop: Bisection search on threshold `b0`
  - Inner loop: Projected policy gradient on auxiliary problem
  - Evaluators: Robust return calculator and subgradient estimator
  - Data flow: `b0` → policy update → constraint violation check → search space update

- Critical path:
  1. Initialize search space `[0, H]`
  2. Compute midpoint `b0 = (i + j)/2`
  3. Run policy gradient subroutine to find π minimizing `∆b0(π)`
  4. Evaluate constraint violation `b∆ = maxn(bJn(π) - bn)`
  5. Update search bounds based on violation
  6. Repeat until convergence

- Design tradeoffs:
  - Double-loop vs single-loop: Double-loop is simpler but potentially slower; single-loop would require solving min-max saddle point problem
  - Policy gradient vs other methods: Policy gradient works with direct parameterization but may have slow convergence
  - Accuracy vs computation: Higher accuracy in evaluators enables fewer iterations but increases per-iteration cost

- Failure signatures:
  - Stuck in local minima: Algorithm converges but to suboptimal policy (gradient conflict issue)
  - Oscillating search: Bisection bounds don't converge (evaluation noise too high)
  - Slow convergence: Policy gradient subroutine takes too many iterations (learning rate too small or α too large)

- First 3 experiments:
  1. Verify gradient conflict exists: Run Lagrangian method on simple RCMDP and check if it gets stuck
  2. Test bisection search: Run with perfect evaluators on known RCMDP to verify convergence
  3. Validate policy gradient subroutine: Check that stationary points found are actually optimal under coverage assumption

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the epigraph form approach generalize to settings with non-rectangular uncertainty sets, such as s-rectangular or unstructured uncertainty?
- Basis in paper: [explicit] The paper states that the epigraph form avoids the rectangularity requirement for policy optimization, unlike existing robust policy gradient methods. It suggests this is a key advantage, but does not formally prove generalization to non-rectangular sets.
- Why unresolved: The paper only discusses theoretical guarantees for near-optimal policy identification in the tabular setting and does not extend the analysis to non-rectangular uncertainty sets. The robust policy evaluation step, which may require rectangularity, is abstracted away via assumptions.
- What evidence would resolve it: A theoretical analysis or empirical study showing that the epigraph form-based policy gradient method maintains convergence guarantees when applied to non-rectangular uncertainty sets.

### Open Question 2
- Question: Can the iteration complexity of EpiRC-PGS be improved below O(ε^-4), and is this bound tight?
- Basis in paper: [explicit] The paper notes that the eO(ε^-4) iteration complexity "may not be tight" and contrasts it with natural policy gradient methods for RMDPs, which achieve eO(ε^-2). It suggests this as a direction for future work.
- Why unresolved: The paper does not provide a lower bound on the iteration complexity or explore algorithmic modifications to reduce it. The analysis relies on weakly convex optimization techniques, which may not be optimal.
- What evidence would resolve it: A lower bound proof or an improved algorithm with a better iteration complexity analysis for RCMDPs.

### Open Question 3
- Question: Is the coverage assumption on the initial distribution (Assumption 5) necessary for the epigraph form approach, or can it be removed?
- Basis in paper: [explicit] The paper states that the coverage assumption is "not necessary in CMDPs" but is used in RCMDPs to ensure global convergence of the policy gradient method. It lists this as a limitation and future work direction.
- Why unresolved: The paper does not explore whether the epigraph form's structure inherently relaxes this assumption or if alternative analysis techniques could remove it.
- What evidence would resolve it: A theoretical proof showing that the epigraph form-based policy gradient method converges globally without the coverage assumption, or an empirical demonstration of its effectiveness in settings where the assumption is violated.

## Limitations

- Theoretical guarantees rely heavily on Assumption 5 (initial distribution coverage), which may not hold in practical applications
- Empirical validation is limited to tabular environments with small state/action spaces, leaving uncertainty about scalability
- Algorithm performance in continuous or high-dimensional settings remains unexplored
- Method depends on accurate subgradient evaluations, making it sensitive to estimation errors in complex uncertainty sets

## Confidence

- **High confidence**: The existence of gradient conflicts in Lagrangian formulations (supported by Theorem 1 and Example 1) and the monotonicity of the optimal value function ∆*b0 (proven in Lemma 2)
- **Medium confidence**: The convergence guarantees for the policy gradient subroutine and overall algorithm complexity (O(ε^-4)), which rely on problem-dependent constants that may vary significantly across instances
- **Low confidence**: The practical performance and scalability beyond the simple tabular experiments, particularly in continuous or large-scale settings

## Next Checks

1. **Verify coverage assumption necessity**: Test the algorithm on RCMDPs where the initial distribution doesn't satisfy the coverage assumption, and measure whether stationary points found are actually suboptimal. This would validate whether Assumption 5 is critical for the theoretical guarantees.

2. **Evaluate gradient conflict empirically**: Construct a simple RCMDP where the Lagrangian method demonstrably gets trapped in a suboptimal stationary point due to gradient conflicts, while the epigraph approach successfully finds the optimal solution. This would provide direct empirical evidence for the core motivation.

3. **Scale to larger problems**: Implement the algorithm on RCMDPs with larger state/action spaces or continuous action spaces, and measure how the O(ε^-4) iteration complexity manifests in practice. This would reveal whether the theoretical guarantees translate to practical scalability.