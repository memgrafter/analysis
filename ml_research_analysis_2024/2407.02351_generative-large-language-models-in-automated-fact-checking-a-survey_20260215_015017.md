---
ver: rpa2
title: 'Generative Large Language Models in Automated Fact-Checking: A Survey'
arxiv_id: '2407.02351'
source_url: https://arxiv.org/abs/2407.02351
tags:
- arxiv
- llms
- fact-checking
- classification
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents a comprehensive survey of generative large
  language models (LLMs) in automated fact-checking, examining 70 relevant studies
  across four main tasks: check-worthy claim detection, previously fact-checked claims
  detection, evidence retrieval, and fact verification/fake news detection. The survey
  categorizes methods into classification & regression, generation, and synthetic
  data generation, with further distinctions between prompting, fine-tuning, and augmentation
  with external knowledge.'
---

# Generative Large Language Models in Automated Fact-Checking: A Survey

## Quick Facts
- arXiv ID: 2407.02351
- Source URL: https://arxiv.org/abs/2407.02351
- Reference count: 40
- Primary result: Comprehensive survey of 70 studies on generative LLMs in automated fact-checking, identifying key tasks, methods, and future research directions.

## Executive Summary
This survey provides a systematic review of how generative large language models (LLMs) are applied to automated fact-checking tasks. The authors analyze 70 relevant studies across four main tasks: check-worthy claim detection, previously fact-checked claims detection, evidence retrieval, and fact verification/fake news detection. The survey categorizes methods into classification & regression, generation, and synthetic data generation, with further distinctions between prompting, fine-tuning, and augmentation with external knowledge. Key findings highlight the dominance of GPT-3.5, T5, and GPT-4 models, the focus on English-language fact-checking, and the need for standardized benchmarks and multilingual approaches.

## Method Summary
The survey methodology involves a comprehensive review of 70 relevant studies on generative LLMs in fact-checking. The analysis covers task categorization (check-worthy claim detection, previously fact-checked claims detection, evidence retrieval, and fact verification), methodological approaches (classification & regression, generation, synthetic data generation), and evaluation techniques. The survey identifies key models used (GPT-3.5, T5, GPT-4), highlights the focus on English-language fact-checking, and outlines future research directions including multilingual approaches, knowledge-augmented strategies, real-time applications, and interactive tools.

## Key Results
- Generative LLMs are applied across four main fact-checking tasks with classification, generation, and synthetic data generation approaches
- GPT-3.5, T5, and GPT-4 dominate the landscape, with most studies focusing on English and limited multilingual approaches
- Key challenges include multilingual fact-checking, knowledge-augmented strategies, real-time applications, and the need for standardized benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting techniques like Chain-of-Thought (CoT) and role specification improve LLM performance in fact-checking by guiding reasoning and task understanding.
- Mechanism: CoT prompts induce intermediate reasoning steps before final classification, enabling better logical assessment of claims. Role specification instructs the LLM to act as a fact-checker, improving context alignment and reducing ambiguity.
- Core assumption: LLMs can follow structured reasoning instructions and maintain role consistency during inference.
- Evidence anchors:
  - [abstract] "generative LLMs in the fact-checking process... highlighting various approaches and techniques for prompting or fine-tuning these models."
  - [section] "The Chain-of-Thought (CoT) technique harnesses the power of LLMs to provide logical reasoning steps. This reasoning is performed before the actual class is provided..."
  - [corpus] Weak; no direct corpus evidence for CoT effectiveness in fact-checking specifically.
- Break condition: If the prompt instructions are unclear or the reasoning steps are misaligned with the task, LLM outputs may become inconsistent or biased.

### Mechanism 2
- Claim: Augmenting LLMs with external knowledge (RAG) mitigates hallucinations and outdated information, improving fact-checking accuracy.
- Mechanism: RAG integrates retrieved evidence from trusted sources (e.g., scientific papers, Wikipedia) into the prompt context, enabling the LLM to base decisions on up-to-date, factual data rather than parametric memory alone.
- Core assumption: Retrieved evidence is relevant and credible, and the LLM can effectively integrate this into its reasoning process.
- Evidence anchors:
  - [abstract] "generative LLMs in the fact-checking process... highlighting various approaches and techniques for prompting or fine-tuning these models."
  - [section] "Retrieval Augmented Generation (RAG), which integrates information retrieval with the LLM, leveraging external sources to enhance accuracy."
  - [corpus] Weak; no direct corpus evidence for RAG effectiveness in fact-checking.
- Break condition: If the retrieval system returns irrelevant or low-quality evidence, the augmented responses may be misleading or incorrect.

### Mechanism 3
- Claim: Synthetic data generation using LLMs can address data scarcity in low-resource languages or domains, enabling broader fact-checking deployment.
- Mechanism: LLMs generate synthetic examples of claims, evidence pairs, or entire datasets to supplement training data, improving model robustness and coverage across diverse linguistic contexts.
- Core assumption: Generated synthetic data preserves the characteristics and quality of real fact-checking data, and the LLM can generate diverse, plausible examples.
- Evidence anchors:
  - [abstract] "Synthetic Data Generation aims to generate an entire dataset or its part with LLMs... in fact-checking, it can be used for generating datasets with disinformation..."
  - [section] "Synthetic data generation can augment datasets by filling gaps in sparse categories or enhancing variability in the data."
  - [corpus] Weak; no direct corpus evidence for synthetic data generation effectiveness in fact-checking.
- Break condition: If synthetic data is of poor quality or lacks diversity, it may introduce bias or degrade model performance.

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: Fact-checking requires multi-step reasoning to evaluate claims against evidence; CoT prompts guide LLMs through these steps systematically.
  - Quick check question: Can you explain how CoT differs from direct classification prompting in terms of LLM output structure?

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: LLMs may lack up-to-date information; RAG supplements their knowledge with retrieved evidence, reducing hallucinations and improving accuracy.
  - Quick check question: What are the risks of using RAG in fact-checking if the retrieval system returns irrelevant evidence?

- Concept: Synthetic data generation
  - Why needed here: Fact-checking datasets are often limited in size or language coverage; synthetic data can expand training sets and improve model generalization.
  - Quick check question: How might synthetic data introduce bias into fact-checking models, and what safeguards could mitigate this?

## Architecture Onboarding

- Component map:
  - Claim → Claim preprocessing (normalization, span detection, decomposition) → Evidence retrieval (search/query generation, rationale selection, RAG integration) → Fact verification (classification/ranking, explanation generation, CoT reasoning) → Output (veracity label, explanation, confidence score) → Knowledge sources (Wikipedia, scientific papers, fact-checking databases)

- Critical path:
  - Claim → Evidence retrieval → Fact verification → Output
  - Bottleneck: Evidence retrieval speed and relevance
  - Optimization focus: Efficient retrieval, prompt engineering, model fine-tuning

- Design tradeoffs:
  - Open-book vs. closed-book: Open-book allows up-to-date verification but increases latency; closed-book is faster but may be less accurate.
  - Prompt-only vs. fine-tuning: Prompting is faster and more flexible; fine-tuning can improve task-specific performance but requires labeled data.
  - Monolingual vs. multilingual: Multilingual models increase coverage but may sacrifice accuracy in low-resource languages.

- Failure signatures:
  - High false positive rate: Indicates poor claim detection or over-reliance on noisy evidence.
  - Hallucinations in explanations: Suggests insufficient grounding in retrieved evidence or weak RAG integration.
  - Slow inference times: Points to inefficient retrieval or overly complex prompt chains.

- First 3 experiments:
  1. Compare zero-shot prompting vs. few-shot CoT for fact verification on a small dataset; measure accuracy and explanation quality.
  2. Implement RAG with Wikipedia and scientific papers; evaluate hallucination reduction vs. baseline LLM.
  3. Generate synthetic claims in a low-resource language; fine-tune a multilingual model and assess cross-lingual transfer performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompting techniques (zero-shot, few-shot, Chain-of-Thought) compare in effectiveness for fact-checking tasks across various languages?
- Basis in paper: [explicit] The paper mentions that most research focuses on English, with only 12 out of 70 papers analyzing more than one language, and highlights the need for multilingual benchmarks.
- Why unresolved: There is a lack of comparative studies on the effectiveness of different prompting techniques across multiple languages, as most work focuses on single-language assessments.
- What evidence would resolve it: Comparative experiments evaluating the performance of zero-shot, few-shot, and Chain-of-Thought prompting techniques on fact-checking tasks across a diverse set of languages would provide insights into their relative effectiveness.

### Open Question 2
- Question: What are the limitations and potential biases introduced by using different external knowledge sources (e.g., Wikipedia, scientific repositories, open web) in Retrieval Augmented Generation (RAG) for fact-checking?
- Basis in paper: [explicit] The paper discusses the use of RAG to enhance accuracy by integrating external sources but notes that selecting the knowledge source is crucial and highlights weaknesses in Wikipedia and the open web.
- Why unresolved: While the paper acknowledges the importance of the knowledge source, it does not provide a detailed analysis of the limitations and biases introduced by different sources.
- What evidence would resolve it: Systematic studies comparing the impact of different external knowledge sources on the accuracy and bias of RAG-based fact-checking systems would clarify their strengths and weaknesses.

### Open Question 3
- Question: How can interactive fact-checking tools leveraging LLMs be designed to effectively empower fact-checkers and the general public in combating misinformation?
- Basis in paper: [explicit] The paper identifies interactive fact-checking tools as a promising future direction, allowing users to engage in dialogue with LLMs to explore the accuracy of claims.
- Why unresolved: While the potential of interactive tools is recognized, there is no detailed exploration of their design, implementation, or effectiveness in real-world scenarios.
- What evidence would resolve it: Development and evaluation of prototype interactive fact-checking tools, including user studies and assessments of their impact on misinformation mitigation, would provide practical insights into their design and effectiveness.

## Limitations

- The effectiveness of prompting techniques like Chain-of-Thought lacks direct corpus evidence from fact-checking studies
- RAG effectiveness for hallucination reduction and accuracy improvement is not empirically validated for fact-checking specifically
- Synthetic data generation impact on fact-checking performance is speculative with no evidence of quality control or bias mitigation strategies

## Confidence

- **High confidence**: The categorization framework (classification, generation, synthetic data) and the identification of major tasks (claim detection, evidence retrieval, fact verification) are well-supported by the surveyed literature.
- **Medium confidence**: The effectiveness of prompting techniques and RAG integration is plausible but lacks direct empirical validation from fact-checking studies.
- **Low confidence**: The impact of synthetic data generation on fact-checking performance is speculative, with no evidence of quality control or bias mitigation strategies.

## Next Checks

1. **Empirical validation of prompting techniques**: Conduct a controlled experiment comparing zero-shot, few-shot, and Chain-of-Thought prompting on a standardized fact-checking dataset (e.g., FEVER), measuring accuracy, explanation quality, and hallucination rates.

2. **RAG effectiveness in fact-checking**: Implement RAG with Wikipedia and scientific papers, then evaluate hallucination reduction and accuracy improvement compared to baseline LLMs on a multilingual fact-checking dataset.

3. **Synthetic data quality assessment**: Generate synthetic claims for a low-resource language using GPT-4, then fine-tune a multilingual model and evaluate cross-lingual transfer performance while measuring bias introduction and data diversity.