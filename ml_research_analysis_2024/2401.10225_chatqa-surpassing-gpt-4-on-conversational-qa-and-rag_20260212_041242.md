---
ver: rpa2
title: 'ChatQA: Surpassing GPT-4 on Conversational QA and RAG'
arxiv_id: '2401.10225'
source_url: https://arxiv.org/abs/2401.10225
tags:
- context
- conversational
- question
- answer
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ChatQA, a family of models that outperform
  GPT-4 on retrieval-augmented generation (RAG) and conversational question answering
  (QA). ChatQA employs a two-stage instruction tuning method, starting with supervised
  fine-tuning followed by context-enhanced instruction tuning, to enhance its capability
  for contextualized or RAG-based QA.
---

# ChatQA: Surpassing GPT-4 on Conversational QA and RAG

## Quick Facts
- arXiv ID: 2401.10225
- Source URL: https://arxiv.org/abs/2401.10225
- Authors: Zihan Liu; Wei Ping; Rajarshi Roy; Peng Xu; Chankyu Lee; Mohammad Shoeybi; Bryan Catanzaro
- Reference count: 36
- Primary result: ChatQA-1.5-70B built on Llama3 slightly outperforms GPT-4-Turbo by 4.4% on CHATRAG BENCH benchmark

## Executive Summary
ChatQA introduces a family of conversational QA and RAG models that outperform GPT-4 on comprehensive benchmarks. The key innovation is a two-stage instruction tuning method that significantly boosts RAG performance, combined with a dense retriever optimized for conversational queries. By incorporating a small amount of "unanswerable" samples, ChatQA reduces hallucination and improves robustness. The Llama3-ChatQA-1.5-70B model surpasses GPT-4-Turbo by 4.4%, demonstrating the effectiveness of the approach.

## Method Summary
ChatQA employs a two-stage instruction tuning approach: stage-1 supervised fine-tuning builds general instruction-following capability, while stage-2 context-enhanced instruction tuning teaches the model to effectively use retrieved context for RAG tasks. A dense retriever is fine-tuned on conversational query-context pairs, achieving results comparable to query rewriting while reducing deployment costs. The method incorporates 1.5k unanswerable samples to enhance robustness and reduce hallucination.

## Key Results
- Llama3-ChatQA-1.5-70B surpasses GPT-4-Turbo by 4.4% on CHATRAG BENCH benchmark
- ChatQA-1.0-70B slightly outperforms GPT-4-0613 and GPT-4-Turbo-2024-04-09
- Fine-tuned retriever achieves comparable performance to query rewriting while reducing computational costs
- Incorporating 1.5k unanswerable samples significantly improves model robustness

## Why This Works (Mechanism)

### Mechanism 1
The two-stage instruction tuning method improves RAG and conversational QA performance by first building instruction-following capability (stage-1 SFT) then teaching context-aware reasoning (stage-2). This bridges the gap between general instruction following and task-specific RAG performance. Evidence shows incorporating single-turn QA datasets in stage-2 improves scores, though direct neighbor studies are weak.

### Mechanism 2
Fine-tuning a single-turn retriever on conversational query-context pairs achieves comparable results to query rewriting while reducing computational costs. The retriever learns to handle multi-turn queries directly from human-annotated or high-quality synthetic conversational data. Evidence shows fine-tuning performs marginally worse on average top-1 recall but better on average top-5 recall compared to rewriting.

### Mechanism 3
Incorporating a small amount of "unanswerable" samples reduces hallucination and improves robustness by teaching the model to recognize when answers cannot be found. Evidence shows using 1.5k unanswerable samples achieves remarkable results in handling unanswerable scenarios, though neighbor studies on unanswerable sample impact are weak.

## Foundational Learning

- **Instruction tuning for language models**
  - Why needed: ChatQA relies on instruction tuning to teach models conversational QA and RAG tasks
  - Quick check: What is the difference between supervised fine-tuning and instruction tuning?

- **Retrieval-augmented generation (RAG)**
  - Why needed: ChatQA's core capability is answering questions using retrieved context
  - Quick check: How does RAG differ from standard generation in terms of input/output?

- **Conversational query rewriting vs. fine-tuning retrievers**
  - Why needed: ChatQA compares these two approaches for handling multi-turn queries
  - Quick check: What are the computational tradeoffs between query rewriting and retriever fine-tuning?

## Architecture Onboarding

- **Component map**: Foundation LLM (Llama2/3) -> Stage-1 SFT training -> Stage-2 context-enhanced instruction tuning -> Multi-turn query retriever (fine-tuned Dragon/E5) -> CHATRAG BENCH evaluation

- **Critical path**: 1. Train foundation LLM with stage-1 SFT, 2. Fine-tune retriever on conversational data, 3. Train with stage-2 context-enhanced instruction tuning, 4. Evaluate on CHATRAG BENCH

- **Design tradeoffs**: Two-stage tuning adds training complexity but improves performance; fine-tuned retriever vs. query rewriting trades computation for comparable accuracy; unanswerable samples needed in small amounts but finding right quantity is key

- **Failure signatures**: Poor stage-1 SFT → weak instruction following in stage-2; low-quality conversational training data → retriever fails on multi-turn queries; too few/no unanswerable samples → increased hallucination; wrong context ordering → "lost in the middle" phenomenon

- **First 3 experiments**: 1. Train Llama2-7B with stage-1 SFT only, evaluate on CHATRAG BENCH, 2. Fine-tune Dragon retriever on HumanAnnotatedConvQA, test on QReCC, 3. Train Llama2-7B with stage-1 + stage-2 (no unanswerable samples), evaluate unanswerable capability

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of synthetic data generated by GPT-3.5-Turbo compare to human-annotated data in terms of improving model performance on conversational QA tasks? The paper shows human-annotated data achieved significant improvements on QuAC and DoQA compared to synthetic data, but lacks detailed quality comparison and exploration of reasons for differences.

### Open Question 2
How does the performance of ChatQA models vary across different types of documents (text-only, tables, arithmetic calculations) compared to other models like GPT-4? The paper mentions ChatQA-1.0-70B performs better on text-only while GPT-4-0613 excels in tabular data, but lacks detailed analysis of performance variations across document types.

### Open Question 3
How does ChatQA's performance on unanswerable questions compare to GPT-4, and what are the implications for reducing hallucination in language models? The paper shows ChatQA-1.0-70B outperforms GPT-3.5-Turbo on unanswerable questions, but lacks comprehensive comparison with GPT-4 and exploration of implications for hallucination reduction.

## Limitations

- Data scale uncertainty: Exact sizes of training datasets and proportions of unanswerable samples remain unspecified, making it difficult to assess robustness of improvements
- Generalization concerns: CHATRAG BENCH benchmark is self-introduced by authors, creating potential evaluation bias without external validation
- Resource requirements: Two-stage instruction tuning and dual model fine-tuning likely require substantial computational resources without detailed cost analysis

## Confidence

- **High confidence**: Effectiveness of incorporating unanswerable samples to reduce hallucination is well-supported by theoretical reasoning and ablation study
- **Medium confidence**: Claim that ChatQA slightly outperforms GPT-4 on CHATRAG BENCH is supported by presented results but limited by self-introduced benchmark
- **Low confidence**: Assertion that fine-tuning retrievers is "more cost-effective" than query rewriting lacks quantitative cost analysis

## Next Checks

1. Evaluate ChatQA models on established, independently created conversational QA benchmarks (QuAC, CoQA, or NaturalQuestions) to verify generalization beyond CHATRAG BENCH

2. Systematically vary the proportion of unanswerable samples (0%, 0.5%, 1.5%, 3%, 5%) in training data to identify optimal amount and confirm 1.5k sufficiency for robustness improvements

3. Measure and compare total computational cost (training time, inference latency, memory usage) of ChatQA's fine-tuned retriever approach versus query rewriting across multiple deployment scenarios to validate claimed cost advantages