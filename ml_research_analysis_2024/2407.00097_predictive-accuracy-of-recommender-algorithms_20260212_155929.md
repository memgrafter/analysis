---
ver: rpa2
title: Predictive accuracy of recommender algorithms
arxiv_id: '2407.00097'
source_url: https://arxiv.org/abs/2407.00097
tags:
- recommender
- algorithms
- accuracy
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This dissertation addresses concerns about reproducibility and\
  \ methodological rigor in recommender systems research, particularly for deep learning\
  \ models. It compares five algorithms\u2014three conventional (KNN, Matrix Factorization,\
  \ SVD) and two deep learning (RBM, Autoencoder)\u2014on MovieLens data using RMSE\
  \ and MAE."
---

# Predictive accuracy of recommender algorithms

## Quick Facts
- arXiv ID: 2407.00097
- Source URL: https://arxiv.org/abs/2407.00097
- Reference count: 11
- Key outcome: Conventional recommender algorithms outperform deep learning models on MovieLens data, with DL models showing higher error rates and overfitting risks.

## Executive Summary
This dissertation investigates the predictive accuracy of recommender algorithms, comparing three conventional models (KNN, Matrix Factorization, SVD) with two deep learning models (RBM, Autoencoder) on MovieLens datasets. The study finds that conventional models match published benchmarks and improve with larger datasets, while deep learning models exhibit higher error rates and inconsistent scaling, likely due to overfitting. The research highlights methodological challenges in DL recommender systems and suggests regularization strategies to improve performance. These findings justify further research to refine deep learning approaches for recommender systems.

## Method Summary
The study evaluates five recommender algorithms on MovieLens datasets ranging from 100K to 1.25M ratings. Three conventional models (KNN User-User, Matrix Factorization, SVD) and two deep learning models (RBM, Autoencoder) are implemented using the Surprise framework and TensorFlow. Predictive accuracy is measured using RMSE and MAE, with cross-validation and hold-one-out evaluation. Hyperparameter tuning via grid search is applied to SVD, RBM, and Autoencoder. The study also records training and prediction times across dataset sizes.

## Key Results
- Conventional models (KNN, Matrix Factorization, SVD) matched published benchmarks and improved with larger datasets.
- Deep learning models (RBM, Autoencoder) showed higher error rates and inconsistent scaling, suggesting overfitting.
- Overfitting is identified as a key challenge for deep learning recommender systems, with regularization strategies discussed as potential solutions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KNN, Matrix Factorization, and SVD produce lower RMSE/MAE because they are convex, low-complexity linear models that generalize well.
- Mechanism: Convex objective functions allow gradient descent to converge to a global optimum without getting trapped in local minima, ensuring consistent low generalization error across varied datasets.
- Core assumption: The rating matrix is approximately low-rank and the training set is representative of the population.
- Evidence anchors:
  - [abstract] "The conventional models matched published benchmarks and improved with larger datasets, while the deep learning models showed higher error rates and inconsistent scaling."
  - [section] "As we observed with the previous models, it follows that increasing sample sizes will be associated with decreasing error."
  - [corpus] No direct citations, but the claim aligns with known theory that convex models (e.g., ALS, SVD) converge reliably and generalize better than highly parameterized deep models.
- Break condition: When the true data structure is highly non-linear or sparse beyond low-rank assumptions, or when the dataset contains strong outliers that violate the convexity assumptions.

### Mechanism 2
- Claim: Deep learning models (RBM, Autoencoder) show higher error due to overfitting caused by high model capacity relative to training data size.
- Mechanism: Large hidden layers and many parameters allow the model to fit noise in training data, leading to high variance and poor generalization on unseen data, especially when training data is sparse.
- Core assumption: The number of parameters in the DL model exceeds the effective degrees of freedom supported by the dataset, and regularization is insufficient to counter this.
- Evidence anchors:
  - [abstract] "The two DL algorithms did not perform as well and illuminated known challenges implementing DL recommender algorithms... Model overfitting is discussed as a potential explanation."
  - [section] "Model overfitting is a likely contributor to the increased values of RMSE observed for the deep learning models... Large hidden layers and many parameters allow the model to fit noise."
  - [corpus] No direct citations, but overfitting in deep models on sparse recommender data is a well-documented phenomenon.
- Break condition: When dataset size grows substantially (e.g., >10M ratings) or when strong regularization (L1/L2 penalties, dropout) is applied to reduce effective model complexity.

### Mechanism 3
- Claim: Deep learning models show inconsistent scaling because their training dynamics are sensitive to initialization, learning rate, and batch size, leading to local minima or divergent behavior as dataset size changes.
- Mechanism: Unlike convex models with stable convergence, DL training is non-convex; small changes in hyperparameters or data size can drastically alter convergence paths, resulting in erratic error scaling.
- Core assumption: The training algorithm (SGD/backpropagation) does not guarantee convergence to a global optimum, and hyperparameter settings that work for small data may fail for larger data.
- Evidence anchors:
  - [abstract] "The two DL algorithms did not perform as well and illuminated known challenges implementing DL recommender algorithms... Overfitting is discussed as a potential explanation."
  - [section] "Execution of the code exceeded the 35GB Colab memory size... DL models failed on datasets over 1M in size."
  - [corpus] No direct citations, but sensitivity to hyperparameters in DL is well known; no evidence of systematic scaling studies.
- Break condition: When training is run with systematic hyperparameter search (grid/random) and sufficient resources, or when dataset size is large enough to smooth out stochastic effects.

## Foundational Learning

- Concept: Convex optimization and global convergence
  - Why needed here: Understanding why KNN, NMF, and SVD yield stable, low error across dataset sizes; convex objectives guarantee convergence to global optimum.
  - Quick check question: Why does a convex loss function guarantee a unique global minimum, and how does this affect generalization?

- Concept: Bias-variance tradeoff and overfitting
  - Why needed here: Explains why deep models (RBM, Autoencoder) with many parameters can achieve low training error but high test error; high capacity leads to low bias but high variance.
  - Quick check question: How does increasing model complexity affect bias and variance, and why does this lead to overfitting in sparse recommender data?

- Concept: Non-convex optimization and local minima
  - Why needed here: Deep learning models use non-convex objectives; training can get stuck in suboptimal local minima, causing inconsistent scaling and error rates.
  - Quick check question: Why does non-convex optimization in deep networks make hyperparameter tuning critical for achieving good generalization?

## Architecture Onboarding

- Component map:
  - MovieLens ratings -> preprocessing -> train/test split (cross-validation) -> metrics (RMSE/MAE)
  - KNN (user-user similarity), Matrix Factorization (ALS), SVD, RBM (binary hidden, softmax visible), Autoencoder (encoder-decoder, lower-dim latent space)
  - Surprise (Python) for conventional models; custom TensorFlow/Keras code for RBM and Autoencoder
  - Grid search for hyperparameter tuning (SVD, RBM, Autoencoder), accuracy metrics, runtime profiling

- Critical path:
  1. Load and partition ratings dataset (size selection).
  2. Train algorithm on training fold(s).
  3. Evaluate on test fold(s), record RMSE/MAE.
  4. Repeat for all hyperparameter configurations (grid search where applicable).
  5. Aggregate results, compare algorithms.

- Design tradeoffs:
  - Memory vs. dataset size: Deep models fail when >35GB memory used; smaller datasets or more memory needed.
  - Complexity vs. accuracy: DL models can model non-linearities but risk overfitting; conventional models generalize better with less data.
  - Hyperparameter search cost: Grid search improves accuracy but increases runtime exponentially.

- Failure signatures:
  - Deep models: RMSE/MAE higher than conventional, non-monotonic scaling with dataset size, training failures due to memory limits.
  - Conventional models: RMSE/MAE consistent with benchmarks, monotonic decrease with dataset size, stable training.

- First 3 experiments:
  1. KNN with cosine similarity on 100K ratings dataset (baseline accuracy check).
  2. Matrix Factorization (ALS) on 1M ratings dataset (benchmark against published results).
  3. SVD without grid search on 1M ratings dataset (compare with SVD++ and with grid search tuned version).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does model overfitting in deep learning recommender systems become severe enough to produce worse performance than simpler models?
- Basis in paper: [explicit] The paper explicitly discusses model overfitting as a potential explanation for the weaker performance of the deep learning algorithms compared to the conventional ones, and cites the bias-variance tradeoff as a theoretical framework.
- Why unresolved: The paper provides evidence of overfitting in the specific experiments conducted, but does not establish clear thresholds or conditions under which overfitting consistently outweighs the potential benefits of deep learning.
- What evidence would resolve it: Systematic experiments varying model complexity (number of layers, nodes, parameters) and dataset size across multiple domains, measuring the point where overfitting leads to worse performance than simpler models.

### Open Question 2
- Question: What regularization strategies are most effective at mitigating overfitting in deep learning recommender systems?
- Basis in paper: [explicit] The paper discusses model overfitting as a potential explanation for the weaker performance of the deep learning algorithms and reviews several regularization strategies (L1, L2, early stopping) as possible approaches to improve predictive error.
- Why unresolved: The paper does not test these regularization strategies in the experiments, so their effectiveness in the context of recommender systems remains an open question.
- What evidence would resolve it: Experiments applying different regularization techniques (L1, L2, early stopping, dropout, etc.) to deep learning recommender models and comparing their performance against both unregularized deep learning models and conventional models.

### Open Question 3
- Question: How do the computational resource requirements of deep learning recommender systems impact their practical applicability compared to conventional methods?
- Basis in paper: [inferred] The paper mentions that deep learning models are computationally demanding and that the experiments were limited by the available resources (35GB memory, TPU), which prevented the use of larger datasets.
- Why unresolved: The paper does not quantify the trade-off between the potential accuracy gains of deep learning and the increased computational cost, nor does it explore the impact of resource limitations on model performance.
- What evidence would resolve it: Comparative studies measuring the accuracy, training time, and resource consumption of deep learning and conventional recommender models across different dataset sizes and computational environments.

## Limitations
- Study scope limited to MovieLens data and specific algorithms, may not generalize to other domains or advanced DL architectures.
- Hyperparameter tuning only applied to DL models, potentially underestimating conventional model performance.
- Memory constraints restricted dataset sizes for DL training, introducing bias toward smaller datasets.
- Does not address scalability or runtime efficiency in production settings.

## Confidence
- **High**: Confidence in convex, low-complexity models generalizing better due to consistent benchmarking and monotonic error reduction.
- **Medium**: Confidence in DL overfitting due to high capacity, but lacks detailed per-epoch training curves or regularization ablation studies.
- **Low**: Confidence in DL scaling inconsistency due to hyperparameter sensitivity, as no systematic hyperparameter sweep or convergence diagnostics were performed.

## Next Checks
1. Conduct a systematic hyperparameter sweep for SVD and DL models to isolate the effect of initialization and learning rate on scaling behavior.
2. Increase dataset size beyond 1.25M ratings (if feasible) to test whether DL models' performance improves with more data, controlling for regularization.
3. Implement and compare strong regularization strategies (L1/L2 penalties, dropout) across all models to quantify their impact on overfitting and generalization.