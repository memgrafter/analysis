---
ver: rpa2
title: Exploring the Correlation between Human and Machine Evaluation of Simultaneous
  Speech Translation
arxiv_id: '2406.10091'
source_url: https://arxiv.org/abs/2406.10091
tags:
- translation
- evaluation
- human
- language
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the correlation between human and machine evaluation
  of simultaneous speech translation, focusing on translation accuracy. The research
  uses semantic similarity metrics based on sentence embeddings and large language
  models (GPT-3.5) to assess the faithfulness of human and machine-generated translations
  of English speeches into Spanish.
---

# Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation

## Quick Facts
- arXiv ID: 2406.10091
- Source URL: https://arxiv.org/abs/2406.10091
- Authors: Xiaoman Wang; Claudio Fantinuoli
- Reference count: 14
- Primary result: GPT-3.5 with direct prompting shows strongest correlation with human judgment for simultaneous speech translation evaluation

## Executive Summary
This study investigates the correlation between human and machine evaluation of simultaneous speech translation, focusing on translation accuracy and semantic similarity. Using 12 English speeches translated into Spanish by both professional interpreters and machine translation systems, the researchers compared human evaluations (conducted by professional interpreters and bilingual individuals using a six-point Likert scale) with machine evaluations based on sentence embeddings and large language models. The results demonstrate that GPT-3.5 with direct prompting exhibits the strongest correlation with human judgment across different context window sizes, outperforming other models like all-MiniLM-L6-v2 and USEM. Notably, GPT-3.5's correlation is slightly higher for human-generated translations than machine-generated ones, suggesting it may be more attuned to the nuanced linguistic adjustments made by professional interpreters.

## Method Summary
The study collected 12 English speeches (approximately 5 minutes each) that were simultaneously interpreted into Spanish by professional interpreters and by KUDO AI Speech Translator. Human evaluations were conducted by 9 professional interpreters and 9 bilingual individuals who rated the faithfulness of translations on a six-point Likert scale. For machine evaluation, semantic similarity was calculated using sentence embeddings from all-MiniLM-L6-v2, GPT-Ada, and USEM models, as well as through direct prompting of GPT-3.5 to rate semantic similarity between source and target segments. Pearson correlation coefficients were computed between human and machine evaluations across different context window sizes (1-5 segments) at the segment level.

## Key Results
- GPT-3.5 with direct prompting demonstrates the strongest correlation with human judgment in evaluating simultaneous speech translation
- Correlation is higher for human-generated translations than machine-generated ones
- Context window size significantly impacts correlation, with larger windows generally improving semantic similarity assessments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5 with direct prompting correlates best with human evaluation of simultaneous speech translation.
- Mechanism: The direct prompting method allows GPT-3.5 to directly assess semantic similarity between source and target segments using a Likert scale, leveraging its contextual understanding and large-scale training data.
- Core assumption: GPT-3.5's training on diverse linguistic data enables it to capture nuances in simultaneous interpreting, including deliberate omissions and structural changes.
- Evidence anchors:
  - [abstract] "The results suggest GPT models, particularly GPT-3.5 with direct prompting, demonstrate the strongest correlation with human judgment in terms of semantic similarity between source and target texts."
  - [section] "GPT-3.5’s correlation with human judgment is somewhat stronger for translations produced by professional interpreters than for machine-generated ones."
  - [corpus] Found 25 related papers; average neighbor FMR=0.518. Weak direct evidence for this specific mechanism in corpus.
- Break condition: If GPT-3.5's training data doesn't adequately represent simultaneous interpreting strategies or if the prompting method doesn't capture context effectively.

### Mechanism 2
- Claim: Larger context windows improve semantic similarity assessment in simultaneous speech translation.
- Mechanism: Increasing the number of segments combined into a single context window allows models to capture cross-segment dependencies and better evaluate the overall faithfulness of the translation.
- Core assumption: Simultaneous interpreting involves complex dependencies across segments that require broader context for accurate evaluation.
- Evidence anchors:
  - [abstract] "Additionally, the study reveals that the size of the context window has a notable impact on this correlation."
  - [section] "The systematic variation in window size aimed to shed light on how semantic similarity between human and machine evaluations could potentially be influenced by the availability of cross-segment context."
  - [corpus] Weak direct evidence in corpus for context window impact on simultaneous interpreting evaluation.
- Break condition: If the relationship between window size and correlation plateaus or becomes negative beyond a certain threshold.

### Mechanism 3
- Claim: GPT-3.5 performs better on human-generated translations than machine-generated ones due to alignment with nuanced linguistic adjustments.
- Mechanism: Human interpreters often make subtle contextual, tonal, and idiomatic adjustments that align more closely with GPT-3.5's training on diverse linguistic data, whereas machine translations may adhere more strictly to literal equivalences.
- Core assumption: GPT-3.5's training data includes a wide range of human linguistic variations that match the types of adjustments made by professional interpreters.
- Evidence anchors:
  - [abstract] "Notably, GPT-3.5's correlation is slightly higher for human-generated translations than machine-generated ones."
  - [section] "This implies that GPT might be subtly more attuned to the linguistic nuances of human translation, even though it remains adept at evaluating speech translation."
  - [corpus] No direct evidence in corpus for this specific mechanism.
- Break condition: If machine translation systems evolve to produce more nuanced outputs or if GPT-3.5's training becomes more focused on machine translation patterns.

## Foundational Learning

- Concept: Semantic similarity metrics
  - Why needed here: To evaluate translation accuracy without relying on reference translations, which is crucial for simultaneous interpreting where the output often diverges from the source structure.
  - Quick check question: How do semantic similarity metrics differ from traditional n-gram based metrics like BLEU in evaluating simultaneous interpreting?

- Concept: Context window in sequential processing
  - Why needed here: Understanding how the amount of context affects model performance is critical for evaluating consecutive segments in simultaneous interpreting.
  - Quick check question: What happens to semantic similarity assessment when the context window is too small or too large for simultaneous interpreting evaluation?

- Concept: Large Language Model prompting techniques
  - Why needed here: Different prompting strategies (direct vs. embedding-based) can significantly impact the correlation with human judgment in translation evaluation.
  - Quick check question: How does direct prompting of GPT-3.5 differ from using its embeddings for semantic similarity calculation?

## Architecture Onboarding

- Component map: Data collection -> Human evaluation (professional interpreters and bilingual individuals using Likert scale) -> Machine evaluation (sentence embeddings from all-MiniLM-L6-v2, GPT-Ada, USEM, and direct GPT-3.5 prompting) -> Correlation analysis -> Results interpretation
- Critical path: Data collection → Human evaluation → Machine evaluation (multiple models) → Correlation analysis → Results interpretation
- Design tradeoffs: The choice between embedding-based methods and direct prompting involves a tradeoff between computational efficiency and correlation with human judgment. Embedding methods are faster but may miss nuanced context that direct prompting captures.
- Failure signatures: Low interrater agreement among human evaluators indicates potential inconsistencies in defining translation accuracy. Poor correlation between machine and human evaluations suggests the metric may not capture the aspects of simultaneous interpreting that humans value.
- First 3 experiments:
  1. Test GPT-3.5 direct prompting on a small subset of translations with varying context window sizes to observe correlation trends.
  2. Compare all-MiniLM-L6-v2 and GPT-Ada embeddings on the same subset to establish baseline correlation values.
  3. Analyze the impact of professional vs. machine translations on correlation values by creating balanced test sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do large language models perform in distinguishing nuanced errors in simultaneous interpretation, such as omissions or contextual shifts, compared to their ability to assess overall semantic similarity?
- Basis in paper: [explicit] The authors suggest future research could explore LLMs' ability to delineate nuances of typologies of errors rather than merely providing aggregate scores.
- Why unresolved: The study primarily focused on semantic similarity as a proxy for translation quality, without investigating the models' ability to identify specific types of errors or their severity.
- What evidence would resolve it: Experiments where LLMs are prompted to classify and score different error types in simultaneous interpretation, compared against human error analyses, would clarify their discriminative capabilities.

### Open Question 2
- Question: Does the context window size affect machine evaluation performance differently for human versus machine-generated translations, and what underlying factors drive these differences?
- Basis in paper: [explicit] The authors found that context window size impacts correlation, with GPT-3.5 performing better with larger windows, and noted that correlation was higher for human-generated translations.
- Why unresolved: While the study identified a correlation between context size and evaluation performance, it did not investigate the underlying reasons for differences in performance between human and machine translations or how specific linguistic features interact with context size.
- What evidence would resolve it: Comparative analyses of evaluation accuracy across different context window sizes for human and machine translations, combined with linguistic feature analysis (e.g., syntactic complexity, discourse coherence), would reveal the factors driving performance differences.

### Open Question 3
- Question: To what extent can automatic evaluation metrics be reliably adapted for real-time simultaneous interpretation assessment, and what technical and practical challenges must be addressed?
- Basis in paper: [inferred] The study's focus on simultaneous interpretation and the authors' suggestion that metrics could provide instant feedback to practitioners implies interest in real-time application, though this was not directly tested.
- Why unresolved: The experiments used post-hoc evaluation of transcriptions, leaving open questions about latency, computational feasibility, and accuracy in live settings where speech segmentation and translation occur dynamically.
- What evidence would resolve it: Pilot implementations of automatic evaluation metrics in live simultaneous interpretation environments, measuring evaluation speed, accuracy degradation over time, and practitioner acceptance, would address the feasibility of real-time use.

## Limitations
- Small dataset size (12 speeches) and single language pair (English-Spanish) limit generalizability
- Human evaluation sample size of 18 raters may limit statistical power
- Study focuses primarily on semantic similarity rather than other quality dimensions like fluency or latency

## Confidence
- Medium: The correlation patterns between GPT-3.5 and human judgments are robust across different context windows, but the exact magnitude may vary with different evaluation protocols or language pairs.

## Next Checks
1. Replicate the study with a larger dataset including multiple language pairs to assess generalizability of the correlation patterns.
2. Conduct ablation studies varying the prompt engineering for GPT-3.5 to determine which specific prompt components drive the correlation improvements.
3. Compare the semantic similarity approach with human evaluation protocols that include fluency and latency considerations to determine if the correlation holds across broader quality dimensions.