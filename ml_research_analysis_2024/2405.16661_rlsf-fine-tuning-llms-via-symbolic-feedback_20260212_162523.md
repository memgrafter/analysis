---
ver: rpa2
title: 'RLSF: Fine-tuning LLMs via Symbolic Feedback'
arxiv_id: '2405.16661'
source_url: https://arxiv.org/abs/2405.16661
tags:
- feedback
- symbolic
- rlsf
- llms
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reinforcement Learning via Symbolic Feedback
  (RLSF), a fine-tuning paradigm that integrates symbolic reasoning tools with LLMs
  to provide fine-grained, token-level feedback during training. Unlike traditional
  RLHF, which relies on sparse scalar rewards, RLSF leverages poly-sized certificates
  from symbolic tools (e.g., compilers, solvers, chemistry toolkits) to pinpoint errors
  at the token level, enabling more precise corrections.
---

# RLSF: Fine-tuning LLMs via Symbolic Feedback

## Quick Facts
- arXiv ID: 2405.16661
- Source URL: https://arxiv.org/abs/2405.16661
- Reference count: 40
- Fine-tuning LLMs with symbolic reasoning tools achieves significant performance gains, enabling smaller models to outperform much larger ones

## Executive Summary
RLSF (Reinforcement Learning via Symbolic Feedback) introduces a fine-tuning paradigm that integrates symbolic reasoning tools with large language models to provide fine-grained, token-level feedback during training. Unlike traditional RLHF which relies on sparse scalar rewards, RLSF leverages poly-sized certificates from symbolic tools (compilers, solvers, chemistry toolkits) to pinpoint errors at the token level, enabling more precise corrections. The approach significantly outperforms traditional methods across five tasks including code generation, chemistry problems, and the Game of 24, with smaller models achieving performance comparable to or exceeding much larger models like GPT-3.5 and GPT-4.

## Method Summary
RLSF fine-tunes pre-trained LLMs using Proximal Policy Optimization (PPO) with symbolic feedback from domain-specific reasoning tools. For each generated response, symbolic tools analyze outputs and produce detailed certificates identifying errors at the token level. These certificates are converted into reward vectors that guide fine-tuning, replacing the manual preference data collection required by RLHF. The method is evaluated across five tasks: natural language to C++ code translation using a compiler, three chemistry tasks (molecule generation, forward synthesis, retrosynthesis) using RDKit, and the Game of 24 using SymPy. Models are fine-tuned using datasets including SPoC for code, Mol-Instructions/USPTO-full/ChEBI-20 for chemistry, and 4nums.com for the Game of 24.

## Key Results
- RLSF-tuned models achieve +31.43% functional correctness in code generation compared to SFT, outperforming GPT-3.5 despite being 100× smaller
- In chemistry tasks, RLSF-tuned models improve exact match by up to +33.7% and validity by +58% compared to GPT-4, which is 1000× larger
- For the Game of 24, RLSF-tuned Llama2-7b-chat improves success rate by +25% and outperforms GPT-3.5 (25× larger)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLSF provides token-level feedback using poly-sized certificates from symbolic tools, enabling fine-grained correction of LLM outputs
- Mechanism: When an LLM generates a response, symbolic tools analyze it and produce detailed certificates identifying errors at the token level, which are then converted into reward vectors that guide fine-tuning
- Core assumption: Symbolic tools can generate interpretable, actionable feedback for each token in the LLM's output
- Evidence anchors:
  - [abstract] "RLSF leverages poly-sized certificates from symbolic tools...to pinpoint errors at the token level"
  - [section] "In the RLSF setting, the LLM is considered as the RL agent...the environment leverages reasoning tools that can generate poly-sized certificates"
  - [corpus] Weak - related papers focus on RL from self-feedback or symbolic execution but don't specifically address token-level feedback from poly-sized certificates
- Break condition: If symbolic tools cannot produce actionable token-level feedback or if the certificates are too large/complex to process efficiently

### Mechanism 2
- Claim: RLSF eliminates the need for manual preference data collection by using automated symbolic feedback
- Mechanism: Instead of relying on human-labeled preference pairs as in RLHF, RLSF automatically generates feedback through symbolic tools that verify outputs against domain-specific constraints
- Core assumption: Symbolic tools can reliably verify whether LLM outputs meet domain-specific constraints without human intervention
- Evidence anchors:
  - [abstract] "This paradigm bridges the gap between symbolic reasoning and LLM fine-tuning, enabling precise alignment with domain-specific constraints"
  - [section] "RLSF uses poly-sized certificates...to identify and correct errors in model outputs, offering token-level guidance without requiring differentiable reasoning systems"
  - [corpus] Weak - while related papers discuss symbolic execution and RL, they don't emphasize the elimination of manual preference data collection
- Break condition: If symbolic tools cannot fully capture the nuances of domain-specific correctness that humans would provide

### Mechanism 3
- Claim: RLSF enables smaller LLMs to outperform much larger models on domain-specific tasks by providing more precise feedback
- Mechanism: By giving detailed, token-level corrections, RLSF helps smaller models learn more efficiently and accurately than larger models trained with sparse scalar rewards
- Core assumption: The quality of feedback matters more than model size for domain-specific reasoning tasks
- Evidence anchors:
  - [abstract] "A key takeaway is that fine-tuning via RLSF enables relatively smaller LLMs to significantly outperform closed-source models that are orders of magnitude larger"
  - [section] "For example, RLSF-tuned models achieve +31.43% functional correctness in code generation...and outperform GPT-3.5 despite being 100× smaller"
  - [corpus] Weak - related papers don't specifically demonstrate smaller models outperforming larger ones through symbolic feedback
- Break condition: If the precision advantage of token-level feedback doesn't translate to significant performance gains over larger models

## Foundational Learning

- Concept: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - Why needed here: RLSF uses PPO as the optimization algorithm to fine-tune LLMs based on symbolic feedback
  - Quick check question: What is the key advantage of PPO over standard policy gradient methods in the context of LLM fine-tuning?

- Concept: Symbolic reasoning and verification tools
  - Why needed here: These tools (compilers, solvers, RDKit) provide the poly-sized certificates that form the basis of RLSF's feedback
  - Quick check question: How does RDKit verify the chemical validity of SMILES strings in the context of molecule generation?

- Concept: Token-level reward vector generation
  - Why needed here: The conversion of symbolic certificates into token-level rewards is the core mechanism that distinguishes RLSF from traditional RLHF
  - Quick check question: In the code generation task, how is the reward vector constructed based on compiler feedback?

## Architecture Onboarding

- Component map: Pre-trained LLM -> Symbolic reasoning tool environment -> Certificate processing module -> PPO optimizer -> Updated LLM parameters
- Critical path:
  1. Input prompt → LLM generates response
  2. Response → Symbolic tool generates certificate
  3. Certificate → Reward vector construction
  4. Reward vector + prompt/response → PPO update
  5. Updated LLM parameters
- Design tradeoffs:
  - Symbolic tool overhead vs. feedback quality: Using multiple symbolic tools increases feedback precision but also computational cost
  - Token-level vs. scalar rewards: Token-level rewards provide more granular feedback but require more complex processing
  - Domain-specific vs. general tools: Specialized tools provide better domain-specific feedback but reduce generality
- Failure signatures:
  - Slow training due to symbolic tool latency
  - Poor convergence if symbolic tools provide inconsistent feedback
  - Model collapse if reward vectors are incorrectly scaled
  - Limited generalization if symbolic feedback is too task-specific
- First 3 experiments:
  1. Implement a simple version with just compiler feedback for code generation, verify that CompAcc improves over SFT
  2. Add RDKit feedback for chemistry tasks, measure improvement in validity and exact match
  3. Compare token-level vs scalar rewards on Game of 24 to quantify the benefit of fine-grained feedback

## Open Questions the Paper Calls Out
- Does RLSF improve general reasoning capabilities beyond the five specific tasks evaluated? The authors note their focus is on domain-specific tasks and future work could explore general reasoning benchmarks.
- How does RLSF performance scale with larger language models? The authors suggest future investigations could explore applying RLSF to larger open-source LLMs beyond the 1.3B-7B parameter models tested.
- Can RLSF be effectively combined with multi-step symbolic feedback during inference? The authors believe combining RLSF with multi-step symbolic feedback during inference could further boost performance but haven't tested this approach.

## Limitations
- The method's dependence on specific symbolic tools raises questions about generalizability to other domains
- Computational overhead of generating and processing poly-sized certificates for each token is not fully characterized
- The paper doesn't address how the approach performs when symbolic tools provide conflicting feedback or when multiple valid solutions exist

## Confidence

- **High Confidence:** The core mechanism of using symbolic tools for token-level feedback and the PPO optimization framework are well-established concepts. The quantitative results showing improvements over SFT and Boolean feedback baselines are reproducible.
- **Medium Confidence:** The claim that RLSF enables smaller models to outperform much larger models is supported by the presented results but may depend on specific task characteristics and symbolic tool quality.
- **Low Confidence:** The scalability of RLSF to complex domains requiring multiple specialized symbolic tools simultaneously, and the method's robustness to noisy or inconsistent symbolic feedback, remain uncertain without further experimentation.

## Next Checks

1. **Scalability Test:** Implement RLSF with three or more concurrent symbolic tools on a single task to measure computational overhead and verify whether token-level feedback quality degrades with tool complexity.
2. **Feedback Consistency Analysis:** Design a synthetic benchmark where symbolic tools intentionally provide conflicting feedback on the same token, then measure how RLSF-trained models handle such contradictions compared to RLHF-trained models.
3. **Cross-Domain Transferability:** Apply RLSF to a new domain (e.g., mathematical proof generation using proof assistants) and evaluate whether the same token-level feedback mechanism generalizes or requires significant adaptation.