---
ver: rpa2
title: 'Emotion Detection in Reddit: Comparative Study of Machine Learning and Deep
  Learning Techniques'
arxiv_id: '2411.10328'
source_url: https://arxiv.org/abs/2411.10328
tags:
- emotion
- emotions
- learning
- detection
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comparative analysis of machine learning
  and deep learning techniques for emotion detection using Reddit comments. The GoEmotions
  dataset was mapped to Ekman's six basic emotions and preprocessed using different
  strategies for machine learning and deep learning models.
---

# Emotion Detection in Reddit: Comparative Study of Machine Learning and Deep Learning Techniques

## Quick Facts
- arXiv ID: 2411.10328
- Source URL: https://arxiv.org/abs/2411.10328
- Reference count: 0
- Primary result: Stacking classifier ensemble achieved highest accuracy (64%) and F1-score (0.53) for emotion detection

## Executive Summary
This study presents a comparative analysis of machine learning and deep learning techniques for emotion detection using Reddit comments from the GoEmotions dataset. The research maps 27 emotion categories to Ekman's six basic emotions plus neutral, then evaluates six machine learning models, three ensemble methods, and an LSTM model. The stacking classifier ensemble outperformed all other approaches, achieving 64% accuracy and F1-score of 0.53, while also demonstrating superior performance compared to the pre-trained EmoBERTa model. The study also developed an interactive Streamlit web application to demonstrate practical real-world application of the emotion detection system.

## Method Summary
The study utilized the GoEmotions dataset (approximately 58,000 Reddit comments) and mapped the 27 emotion categories to Ekman's six basic emotions (joy, anger, fear, sadness, disgust, surprise) plus neutral. Two preprocessing approaches were employed: raw text for deep learning models and cleaned/normalized text for machine learning models. Six machine learning models (Decision Tree, Random Forest, SVM, Naive Bayes, Logistic Regression, XGBoost) were trained using TF-IDF features, along with three ensemble methods (Voting, Bagging, Stacking) and an LSTM model with embedding layers. Performance was evaluated using accuracy and F1-score metrics, with results compared against a pre-trained EmoBERTa baseline.

## Key Results
- Stacking classifier achieved highest accuracy (64%) and F1-score (0.53) among all models tested
- The stacking ensemble outperformed individual machine learning models and other ensemble techniques (Voting, Bagging)
- TF-IDF feature extraction performed well for machine learning models on this Reddit emotion detection task
- The system was deployed as an interactive Streamlit web application for practical demonstration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mapping 27 GoEmotions labels to Ekman's six basic emotions simplifies the classification task and improves model performance.
- Mechanism: By reducing the number of classes from 27 to 6, the model can focus on distinguishing broader emotional patterns rather than fine-grained distinctions, which is particularly effective for machine learning models that rely on clear feature boundaries.
- Core assumption: The six basic emotions capture the essential emotional content of Reddit comments, and finer distinctions within these categories are less critical for detection accuracy.
- Evidence anchors:
  - [abstract] "These emotions are subsequently mapped to Ekman's six basic categories: joy, anger, fear, sadness, disgust, and surprise."
  - [section] "To simplify classification, the 27 original categories are mapped to Ekman's six basic emotions, along with a neutral category (Ekman, 1999)."

### Mechanism 2
- Claim: Ensemble methods, particularly stacking classifiers, outperform individual models by combining complementary strengths.
- Mechanism: The stacking classifier uses predictions from multiple base models (Random Forest, XGBoost, SVM) as features for a meta-classifier (Logistic Regression), allowing it to learn from the strengths and correct the weaknesses of individual models.
- Core assumption: The base models make different types of errors, and the meta-classifier can effectively combine their outputs to improve overall accuracy.
- Evidence anchors:
  - [abstract] "Results indicate that the Stacking classifier outperforms other models in accuracy and performance."
  - [section] "The Stacking Classifier delivered the highest accuracy and F1 score among all models, outperforming individual models and other ensemble techniques."

### Mechanism 3
- Claim: TF-IDF feature extraction is more effective than embeddings for this emotion detection task on Reddit comments.
- Mechanism: TF-IDF captures term relevance by combining term frequency with inverse document frequency, which works well for the relatively short and informal Reddit comments where certain emotional words appear frequently in specific contexts.
- Core assumption: The emotional content of Reddit comments can be effectively captured through term frequency patterns rather than semantic relationships that embeddings capture.
- Evidence anchors:
  - [section] "For machine learning models, TF-IDF was selected due to its effectiveness in transforming text data into structured features."
  - [section] "TF-IDF captures term relevance by combining term frequency with inverse document frequency, assigning higher weights to important terms that appear frequently in a document but are rare across the dataset."

## Foundational Learning

- Concept: Class imbalance and its impact on machine learning models
  - Why needed here: The dataset shows significant class imbalance with emotions like "joy" and "neutral" being much more prevalent than others like "anger" and "fear."
  - Quick check question: How does class imbalance typically affect precision and recall for minority classes in multi-class classification?

- Concept: Ensemble learning techniques and their tradeoffs
  - Why needed here: The study compares multiple ensemble methods (Voting, Bagging, Stacking) and needs to understand when each approach is most effective.
  - Quick check question: What is the key difference between hard voting and soft voting in ensemble classifiers?

- Concept: TF-IDF vs. word embeddings for text feature extraction
  - Why needed here: The paper uses TF-IDF for machine learning models but doesn't explore embeddings, requiring understanding of when each approach is appropriate.
  - Quick check question: When would word embeddings typically outperform TF-IDF for text classification tasks?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline (text normalization, emotion mapping, class handling) → Feature extraction (TF-IDF for ML models, raw text for LSTM) → Base models (Decision Tree, Random Forest, SVM, Naive Bayes, Logistic Regression, XGBoost) → Ensemble methods (Voting, Bagging, Stacking) → Deep learning component (LSTM with embedding layers) → Deployment interface (Streamlit web application)

- Critical path:
  Data preprocessing → Feature extraction → Model training → Ensemble combination → Evaluation → Deployment

- Design tradeoffs:
  - Simplicity vs. performance: Using only TF-IDF vs. exploring word embeddings
  - Model complexity vs. interpretability: Ensemble methods vs. single models
  - Domain specificity vs. generalizability: Custom models vs. pre-trained EmoBERTa

- Failure signatures:
  - Low recall on minority classes indicates class imbalance issues
  - Poor performance on sarcasm or irony suggests need for contextual understanding
  - Degradation on informal language indicates domain adaptation needs

- First 3 experiments:
  1. Train a simple Logistic Regression with TF-IDF on the balanced subset of the data to establish a baseline
  2. Implement a Stacking classifier with XGBoost, SVM, and Logistic Regression as base models to test ensemble benefits
  3. Fine-tune EmoBERTa on the GoEmotions dataset to compare with the custom stacking approach

## Open Questions the Paper Calls Out
None

## Limitations
- The emotion mapping from 27 categories to 6 Ekman basic emotions may result in significant information loss, as evidenced by the 36% error rate
- Only TF-IDF was explored for machine learning models without testing word embeddings, potentially missing performance improvements from semantic feature extraction
- The study does not explore fine-tuning the pre-trained EmoBERTa model on the specific dataset, limiting the comparison with custom approaches

## Confidence
- **High Confidence**: The superiority of the stacking classifier ensemble (64% accuracy) over individual models and the Voting and Bagging ensembles is well-supported by the experimental results presented.
- **Medium Confidence**: The claim that TF-IDF outperforms embeddings for this specific task is plausible but not empirically validated within the study, as embeddings were not tested for the machine learning models.
- **Low Confidence**: The generalizability of the 64% accuracy to real-world applications is uncertain without cross-validation on different datasets or testing on out-of-domain Reddit comments.

## Next Checks
1. Conduct ablation studies by removing individual base models from the stacking classifier to identify which components contribute most to performance improvements.
2. Implement and compare word embeddings (Word2Vec, GloVe) as features for the machine learning models to validate whether TF-IDF is indeed the optimal choice for this dataset.
3. Perform fine-tuning of the EmoBERTa model on the GoEmotions dataset and compare its performance with the stacking classifier to establish a stronger baseline for pre-trained transformer models.