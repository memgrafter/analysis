---
ver: rpa2
title: Measuring Sound Symbolism in Audio-visual Models
arxiv_id: '2409.12306'
source_url: https://arxiv.org/abs/2409.12306
tags:
- sound
- audio-visual
- symbolism
- learning
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether pre-trained audio-visual models
  exhibit sound symbolism, the non-arbitrary association between sounds and visual
  representations. The authors develop a specialized dataset of synthesized images
  and audio samples, categorized as "sharp" or "round" based on human preferences.
---

# Measuring Sound Symbolism in Audio-visual Models

## Quick Facts
- arXiv ID: 2409.12306
- Source URL: https://arxiv.org/abs/2409.12306
- Reference count: 0
- Primary result: Pre-trained audio-visual models exhibit sound symbolism patterns, particularly those trained on spoken image captions

## Executive Summary
This paper investigates whether pre-trained audio-visual models exhibit sound symbolism - the non-arbitrary association between sounds and visual representations. The authors develop a specialized synthetic dataset and use a non-parametric zero-shot approach to probe models' inherent knowledge. Their findings reveal that models trained on spoken image captions can significantly distinguish between round and sharp sounds and images, demonstrating correlations with established sound symbolism patterns. In contrast, models trained on general audio events show random or reversed tendencies, suggesting that the type of pre-training data critically influences the capture of sound-meaning connections.

## Method Summary
The researchers created a synthetic dataset of 500 images and 3888 audio samples categorized as "sharp" or "round" based on human preferences. They extracted embeddings from various pre-trained audio-visual models and calculated geometric and phonetic scores to evaluate the models' ability to distinguish between sharp and round attributes. The evaluation used ROC-AUC and Kendall's rank correlation coefficient in a zero-shot setting, meaning the models were tested without any fine-tuning on the sound symbolism task. This approach aimed to measure the models' inherent cross-modal understanding rather than task-specific adaptations.

## Key Results
- Models trained on spoken image captions significantly outperform chance in distinguishing sharp vs round sounds and images
- Models trained on general audio events show random or reversed tendencies in sound symbolism classification
- Coordinated representation models achieve better performance than joint representation models for sound symbolism capture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio-visual models trained on spoken image captions learn cross-modal mappings that align with human sound-meaning associations.
- Mechanism: During pre-training on spoken captions paired with images, the model's contrastive learning objective forces alignment between audio embeddings and visual embeddings. This creates a shared embedding space where phonetically "sharp" sounds are geometrically closer to visually "sharp" images, mirroring the kiki-bouba effect.
- Core assumption: The pre-training dataset contains enough diverse spoken captions and images that the model must learn generalizable cross-modal patterns rather than memorizing specific pairs.
- Evidence anchors:
  - [abstract] "Our findings reveal a significant correlation between the models' outputs and established patterns of sound symbolism, particularly in models trained on speech data."
  - [section] "Specifically, audio-visual models trained on spoken image captions are able to group visual stimuli and audio with different appearances significantly better than chance."
  - [corpus] Weak evidence - related papers mention sound symbolism in language models but don't directly confirm the cross-modal mechanism.

### Mechanism 2
- Claim: Models with coordinated representations capture sound symbolism better than those with joint representations.
- Mechanism: Coordinated models learn separate audio and visual encoders but enforce similarity between their embeddings through contrastive loss. This allows each modality to maintain its intrinsic structure while learning cross-modal alignment, which preserves phonetic characteristics needed for sound symbolism.
- Core assumption: Joint representations that combine modalities into a single space may lose modality-specific features critical for capturing sound symbolism.
- Evidence anchors:
  - [section] "models that learn coordinated representations achieve better classification performance than those with joint representations (or both)."
  - [section] "SpeechCLIP, FaST-VGS, and ImageBind can significantly distinguish sounds and image classes better than chance."
  - [corpus] No direct corpus evidence supporting this specific architectural claim.

### Mechanism 3
- Claim: The kiki-bouba effect in audio-visual models is stronger when tested with speech input compared to text forms of the same pseudowords.
- Mechanism: Speech contains phonetic and articulatory information that directly corresponds to the physical production of sounds, which is more closely tied to the visual shape associations than the abstract text representation.
- Core assumption: Text representations lack the phonetic features that carry the sound symbolism effect.
- Evidence anchors:
  - [section] "using speech input demonstrates a more pronounced sound symbolism effect than using text forms."
  - [section] "models that learn coordinated representations achieve better classification performance than those with joint representations."
  - [corpus] Weak evidence - related papers discuss multimodal models but don't specifically address the speech vs text input distinction for sound symbolism.

## Foundational Learning

- Concept: Cross-modal contrastive learning
  - Why needed here: The model needs to learn associations between audio and visual representations through a training objective that brings similar pairs closer in embedding space.
  - Quick check question: What loss function is typically used in contrastive learning to align audio and visual embeddings?

- Concept: Zero-shot probing with non-parametric evaluation
  - Why needed here: The study tests pre-trained models without fine-tuning, using geometric and phonetic scores to measure sound symbolism without introducing task-specific training bias.
  - Quick check question: How do geometric and phonetic scores differ in what they measure about the model's cross-modal understanding?

- Concept: Sound symbolism and the kiki-bouba effect
  - Why needed here: Understanding this psychological phenomenon provides the theoretical foundation for what the model is expected to capture and how to evaluate it.
  - Quick check question: What makes the kiki-bouba effect a robust phenomenon across cultures and age groups?

## Architecture Onboarding

- Component map: Audio → Fa → Embedding → Shared Space → Geometric/Phonetic Score Calculation → Evaluation
- Critical path: Audio → Fa → Embedding → Shared Space → Geometric/Phonetic Score Calculation → Evaluation
- Design tradeoffs:
  - Joint vs coordinated representations: Joint representations may lose modality-specific features but are computationally simpler; coordinated representations preserve more information but require careful similarity constraint design.
  - Pre-training data domain: Speech-focused data better captures sound symbolism but may limit general audio understanding; general audio data provides broader coverage but may miss phonetic associations.
- Failure signatures:
  - Random ROC-AUC scores (~0.5) indicate no sound symbolism capture
  - Reversed trends (negative correlation) suggest the model has learned opposite associations
  - Domain mismatch between pre-training data and evaluation data leads to poor performance
- First 3 experiments:
  1. Test the same model with speech vs text inputs to confirm the modality effect on sound symbolism
  2. Compare coordinated vs joint representation models on the same evaluation dataset
  3. Vary the pre-training data domain (speech captions vs general audio) while keeping architecture constant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can sound symbolism effects in audio-visual models transfer to practical applications like designing more effective audio-visual training objectives or improving model interpretability?
- Basis in paper: [explicit] The authors mention that future work could explore linking sound symbolism to other audio-visual tasks and designing sound-symbolism related training objectives.
- Why unresolved: The paper primarily investigates whether sound symbolism exists in models, not how to leverage it for practical improvements.
- What evidence would resolve it: Empirical studies showing improved performance on audio-visual tasks when incorporating sound symbolism-aware objectives, or experiments demonstrating enhanced model interpretability through sound symbolism analysis.

### Open Question 2
- Question: Do sound symbolism patterns in audio-visual models vary across languages, and if so, what does this reveal about the language-agnostic properties of sound symbolism?
- Basis in paper: [explicit] The authors acknowledge that their current analysis is limited to English-language corpora and suggest future research should evaluate models trained on diverse languages.
- Why unresolved: The study only examined English-language models, leaving open whether similar patterns emerge in other linguistic contexts.
- What evidence would resolve it: Comparative experiments showing whether sound symbolism patterns are consistent or differ across models trained on multiple languages, and correlation with linguistic universals.

### Open Question 3
- Question: Why do models trained on spoken image captions show stronger sound symbolism effects compared to those trained on general audio events, and what specific aspects of the pre-training data drive this difference?
- Basis in paper: [explicit] The authors observe that models pre-trained on spoken image captions generally achieve better classification performance than those pre-trained on general audio events, but the underlying reasons remain unclear.
- Why unresolved: The paper identifies the pattern but doesn't investigate the mechanisms or specific features of the training data that create this difference.
- What evidence would resolve it: Ablation studies comparing different types of audio-visual data, analysis of what specific semantic or acoustic features are captured by each training approach, and experiments isolating the contribution of speech versus non-speech audio.

## Limitations

- The synthetic evaluation dataset may not fully capture the complexity of real-world sound symbolism patterns
- Domain-specific pre-training appears critical but the exact dataset compositions and training procedures are not fully specified
- The zero-shot probing approach may miss task-specific adaptations that fine-tuning could reveal

## Confidence

- **High confidence**: Models trained on spoken image captions show significant correlation with sound symbolism patterns (ROC-AUC >> 0.5)
- **Medium confidence**: Coordinated representations outperform joint representations for sound symbolism capture
- **Low confidence**: Speech input demonstrates stronger sound symbolism effects than text input

## Next Checks

1. Test the same evaluation methodology across multiple model versions and training checkpoints to verify consistency of the sound symbolism effect over time
2. Conduct ablation studies varying the phonetic diversity and visual complexity in the synthetic dataset to determine which features most strongly drive the observed correlations
3. Implement controlled fine-tuning experiments to distinguish between zero-shot learned patterns versus task-specific adaptations in the models' sound symbolism capabilities