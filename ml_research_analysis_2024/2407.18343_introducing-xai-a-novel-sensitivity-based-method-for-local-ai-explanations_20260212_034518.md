---
ver: rpa2
title: "Introducing \u03B4-XAI: a novel sensitivity-based method for local AI explanations"
arxiv_id: '2407.18343'
source_url: https://arxiv.org/abs/2407.18343
tags:
- feature
- values
- shapley
- index
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces \u03B4-XAI, a novel sensitivity-based method\
  \ for local explanations of machine learning model predictions. \u03B4-XAI extends\
  \ the \u03B4 index from global sensitivity analysis to assess how individual feature\
  \ values impact predictions for specific instances."
---

# Introducing Î´-XAI: a novel sensitivity-based method for local AI explanations

## Quick Facts
- **arXiv ID:** 2407.18343
- **Source URL:** https://arxiv.org/abs/2407.18343
- **Reference count:** 35
- **Primary result:** Î´-XAI is a sensitivity-based method for local explanations that better captures extreme feature values compared to Shapley values

## Executive Summary
This paper introduces Î´-XAI, a novel method for local explainability in machine learning that extends the Î´ index from global sensitivity analysis to assess how individual feature values impact predictions for specific instances. Î´-XAI computes how much each feature value shifts the probability density of observing a particular model output, enabling ranking of features by their local importance. The method was evaluated on simulated linear regression scenarios and compared with Shapley values, demonstrating higher sensitivity in detecting dominant features and handling extreme feature values.

## Method Summary
Î´-XAI extends the Î´ index from global sensitivity analysis to the local explainability context by computing how much each feature value shifts the probability density function of model outputs. The method uses bootstrap sampling from the training data to estimate unconditional and conditional probability density functions of the model output, then calculates the Î´ index for each feature by measuring the distance between these distributions. Features are ranked by their normalized Î´ index values, with uncertainty estimates provided through median and interquartile range across bootstrap samples.

## Key Results
- Î´-XAI demonstrated higher sensitivity than Shapley values in detecting dominant features when there is a large disparity in feature importance
- Î´-XAI better captures the influence of extreme feature values on predictions by measuring shifts in output probability density
- The method provides intuitive explanations through probability density function visualizations that show how feature values affect prediction likelihood
- Î´-XAI handles correlated features robustly and produces feature rankings consistent with ground truth in linear regression scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Î´-XAI detects dominant features more sensitively than Shapley values when there is a large disparity in feature importance.
- Mechanism: By leveraging the probability density function of the model output, Î´-XAI quantifies how much a feature value shifts the likelihood of observing a particular prediction. When one feature has a much larger coefficient (e.g., Î² = 1000 vs Î² = 50), setting it to zero dramatically alters the output distribution, which Î´-XAI captures more effectively than Shapley values.
- Core assumption: The joint distribution of features and the model's response to extreme values are representative of the true domain.
- Evidence anchors:
  - [abstract] "Î´-XAI index demonstrated higher sensitivity in detecting dominant features and handling extreme feature values."
  - [section] "When Eq.10 is used to predict ð’™âˆ—, the Î´ index-ranking returns ð‘¥1 = 0 as the most impacting feature due to its dramatic impact on the probability of observing an output equal to 100."
- Break condition: If the model output is not well-approximated by its probability density function, or if feature distributions are not representative of real-world scenarios.

### Mechanism 2
- Claim: Î´-XAI better captures the influence of extreme feature values on predictions compared to Shapley values.
- Mechanism: Î´-XAI uses conditional probability density functions to measure the impact of each feature value on the output distribution. When a feature value is extreme (e.g., far from the mean), Î´-XAI detects a significant shift in the output's probability density, whereas Shapley values may underweight this effect.
- Core assumption: Extreme values in the feature space correspond to meaningful changes in the model's predictions.
- Evidence anchors:
  - [abstract] "Î´-XAI better captures their impact on predictions" in the context of extreme feature values.
  - [section] "ð‘¥2 and ð‘¥3 were considered equal to 2, hence they assumed 'unlikely' or 'extreme' values... By looking at Figure 6, panel C, it is possible to see that the output value ð‘¦ = 300 is as well a relatively 'unlikely' value... Î´-XAI method captures this characteristic of the model behaviour and distribution of the features."
- Break condition: If the model is not sensitive to extreme values or if extreme values are not well-represented in the training data.

### Mechanism 3
- Claim: Î´-XAI provides more intuitive explanations by leveraging probability density function visualizations.
- Mechanism: Î´-XAI ranks features based on how much their values shift the probability density of the model output. This is visualized using conditional vs. unconditional probability density functions, making the feature importance ranking clearer and more explainable to practitioners.
- Core assumption: Practitioners can intuitively understand and interpret changes in probability density functions.
- Evidence anchors:
  - [abstract] "Î´-XAI provides intuitive explanations by leveraging probability density functions, making feature rankings clearer and more explainable for practitioners."
  - [section] "From a qualitative perspective, it is our opinion that the Î´-XAI provides more intuitive explanations than Shapley values. Indeed, for each instance, the effect of a feature's value on observing a certain model prediction is described by leveraging the basic concepts of probability density functions."
- Break condition: If the target audience lacks statistical literacy or if visualizations are not effectively communicated.

## Foundational Learning

- Concept: Sensitivity Analysis (SA) and Global Sensitivity Analysis (GSA)
  - Why needed here: Î´-XAI extends GSA concepts to local explanations, so understanding SA/GSA is foundational to grasping how Î´-XAI works.
  - Quick check question: What is the difference between variance-based GSA and moment-independent GSA like the Î´ index?

- Concept: Probability Density Functions (PDFs)
  - Why needed here: Î´-XAI quantifies the impact of feature values by measuring shifts in the PDF of model outputs.
  - Quick check question: How does conditioning on a feature value change the PDF of the model output?

- Concept: Shapley Values
  - Why needed here: Î´-XAI is benchmarked against Shapley values, so understanding how Shapley values assign feature importance is essential for comparison.
  - Quick check question: What is the main assumption behind Shapley values in assigning feature importance?

## Architecture Onboarding

- Component map: Training data -> ML model -> Instance to explain -> Î´-XAI algorithm (bootstrap sampling, KDE, Î´ index) -> Feature importance ranking
- Critical path: Compute unconditional PDF of model output â†’ For each feature, compute conditional PDF â†’ Calculate Î´ index for each feature â†’ Normalize and rank features
- Design tradeoffs:
  - Bootstrap sampling adds robustness but increases computation time.
  - Kernel density estimation is flexible but sensitive to bandwidth choice.
  - Î´-XAI is more sensitive to extreme values but may overemphasize outliers.
- Failure signatures:
  - High variance in Î´ index across bootstrap samples â†’ unstable explanations
  - Î´ index close to zero for all features â†’ model output insensitive to input variations
  - Î´ index rankings inconsistent with domain knowledge â†’ potential model bias or data issues
- First 3 experiments:
  1. Implement Î´-XAI on a simple linear regression model and verify feature rankings match known coefficients.
  2. Compare Î´-XAI and Shapley values on a model with one dominant feature; check if Î´-XAI better detects the dominant feature.
  3. Test Î´-XAI on a model with correlated features; verify that it correctly identifies both the most important feature and irrelevant ones.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Î´-XAI method perform when applied to deep neural networks compared to simpler models like linear regression?
- Basis in paper: [inferred] The paper mentions that high-performing AI/ML models like deep neural networks often lack interpretability and that further investigations on real case studies will be performed.
- Why unresolved: The evaluation was conducted only on simulated linear regression scenarios. The method's performance on more complex models is unknown.
- What evidence would resolve it: Testing Î´-XAI on deep neural networks and comparing its performance and feature ranking consistency with Shapley values and other XAI methods.

### Open Question 2
- Question: Can Î´-XAI effectively detect and explain model behavior when multiple features are simultaneously at extreme values?
- Basis in paper: [explicit] The paper notes that Î´-XAI is more sensitive to extreme values of individual features but doesn't address scenarios where multiple features are extreme simultaneously.
- Why unresolved: The evaluation focused on single extreme features or correlated features, but not combinations of extreme values across multiple features.
- What evidence would resolve it: Testing Î´-XAI on datasets where multiple features take extreme values simultaneously and analyzing whether it correctly identifies their combined impact.

### Open Question 3
- Question: How does Î´-XAI handle feature interactions compared to Shapley values?
- Basis in paper: [inferred] While the paper mentions handling correlated features, it doesn't explicitly compare how Î´-XAI handles feature interactions versus Shapley values.
- Why unresolved: The paper demonstrates Î´-XAI works with correlated features but doesn't specifically analyze interaction effects between features.
- What evidence would resolve it: Testing both methods on datasets with known feature interactions and comparing their ability to correctly attribute importance to interaction effects versus individual feature effects.

## Limitations

- The evaluation is based entirely on synthetic linear regression data, leaving real-world performance on complex, non-linear models uncertain
- Specific implementation details of kernel density estimation (bandwidth selection method, kernel type) and bootstrap sampling parameters are not specified, which could significantly impact Î´-XAI performance
- Computational complexity of bootstrap sampling and kernel density estimation for high-dimensional or large datasets is not addressed, potentially limiting practical applicability

## Confidence

- **High confidence**: Î´-XAI provides intuitive explanations through probability density function visualizations (supported by theoretical grounding in GSA literature)
- **Medium confidence**: Î´-XAI better captures extreme feature values compared to Shapley values (supported by synthetic data but requires real-world validation)
- **Medium confidence**: Î´-XAI is generally consistent with Shapley values while showing higher sensitivity to dominant features (limited to linear regression scenarios)

## Next Checks

1. **Real-world dataset validation**: Apply Î´-XAI to a non-linear model (e.g., random forest or neural network) on a real-world dataset to assess performance beyond synthetic linear regression scenarios.

2. **Computational efficiency benchmarking**: Measure runtime and memory usage of Î´-XAI compared to Shapley values on datasets with varying dimensions and sample sizes to establish practical scalability limits.

3. **Sensitivity analysis to KDE parameters**: Systematically vary kernel density estimation parameters (bandwidth selection method, kernel type) and evaluate impact on Î´-XAI rankings to determine robustness to hyperparameter choices.