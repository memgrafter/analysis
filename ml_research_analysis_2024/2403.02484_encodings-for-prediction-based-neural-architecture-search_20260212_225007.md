---
ver: rpa2
title: Encodings for Prediction-based Neural Architecture Search
arxiv_id: '2403.02484'
source_url: https://arxiv.org/abs/2403.02484
tags:
- search
- encodings
- neural
- architecture
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies neural network encodings for neural architecture
  search (NAS) predictors, categorizing them into structural, score-based, unsupervised
  learned, and supervised learned encodings. It introduces "unified encodings" that
  enable transfer of predictors across multiple search spaces.
---

# Encodings for Prediction-based Neural Architecture Search

## Quick Facts
- arXiv ID: 2403.02484
- Source URL: https://arxiv.org/abs/2403.02484
- Authors: Yash Akhauri; Mohamed S. Abdelfattah
- Reference count: 40
- This paper introduces FLAN, a hybrid GNN predictor that improves sample efficiency of NAS accuracy predictors by over 8× through cross-domain transfer learning and 2.12× in end-to-end NAS.

## Executive Summary
This paper presents FLAN (Flow Attention for NAS), a novel predictor architecture that combines dual graph flows with graph attention mechanisms to improve neural architecture search (NAS) predictor performance. The authors evaluate 1.5 million architectures across 13 NAS benchmarks, demonstrating that their unified encoding approach enables transfer learning across different search spaces. FLAN achieves up to 15% improvement in Kendall-τ correlation compared to previous state-of-the-art methods, significantly reducing the number of architectures needed to train accurate predictors.

## Method Summary
FLAN employs a hybrid GNN architecture combining Dense Graph Flow (DGF) with residual connections and graph attention (GAT) mechanisms. The predictor uses multiple encoding types including structural (adjacency matrices), score-based (ZCP vectors), unsupervised learned (Arch2Vec), and supervised learned (CATE) encodings. A key innovation is the "unified encoding" approach that concatenates unique search space indices to operations, enabling cross-space transfer learning. The predictor is trained on subsets of NAS spaces and tested on entire spaces, with supplemental encodings concatenated to the MLP prediction head.

## Key Results
- FLAN improves sample efficiency of NAS accuracy predictors by over 8× through cross-domain transfer learning
- Delivers up to 15% improvement in Kendall-τ correlation compared to previous state-of-the-art TA-GATES
- Improves end-to-end NAS by 2.12× through better sample efficiency
- Achieves 46× improvement in sample efficiency across three NAS spaces with unified encodings

## Why This Works (Mechanism)

### Mechanism 1
FLAN's hybrid GNN with dual graph flows (DGF+GAT) improves predictor sample efficiency by over 8× compared to GCN-only encoders. The residual connections in DGF mitigate over-smoothing, preserving discriminative localized information. GAT adds pairwise attention for inter-node interaction. Ensemble of both graph flows captures complementary structural signals.

### Mechanism 2
Supplemental encodings (ZCP, Arch2Vec, CATE) concatenated to the MLP prediction head yield up to 15% improvement in Kendall-τ correlation. These encodings capture complementary architectural properties - ZCP proxies correlate with accuracy, Arch2Vec learns structural patterns, and CATE captures computational similarity. Their concatenation provides multi-view embeddings that guide prediction better than single encodings.

### Mechanism 3
Unified encodings enable transfer of predictors across NAS spaces, improving sample efficiency by over 8× when transferring from ENAS to Amoeba. By appending a unique search-space index to each operation, the predictor learns a common embedding space. During transfer, this shared space allows the predictor to adapt to the target space with very few samples, reusing learned patterns from the source space.

## Foundational Learning

- **Graph Neural Networks (GNN) fundamentals**: FLAN's core is a hybrid GNN (DGF+GAT). Understanding message passing, node aggregation, and over-smoothing is essential to grasp why residual connections help. *Quick check*: What happens to node representations if you stack too many GNN layers without residual connections? (Answer: over-smoothing, loss of discriminative power)

- **Zero-Cost Proxies (ZCP) and their correlation with accuracy**: ZCP vectors are one of the key supplemental encodings. Knowing how they are computed and why they correlate with accuracy explains their predictive value. *Quick check*: Why might training-free measures like gradient flow correlate with trained accuracy? (Answer: Because architectures that train well tend to have favorable gradient flow patterns even before training)

- **Transfer learning in predictor space**: The unified encoding + transfer mechanism is central to cross-space adaptation. Understanding source-target alignment and fine-tuning is crucial. *Quick check*: What is the difference between zero-shot transfer and few-shot fine-tuning in this context? (Answer: Zero-shot uses pre-trained predictor directly; few-shot retrains on small target samples)

## Architecture Onboarding

- **Component map**: Architecture encoding -> Adjacency + Op matrix -> Unified embedding (if cross-space) -> DGF layer -> GAT layer -> Ensemble -> Node aggregation -> Concatenate supplemental encodings -> MLP -> Kendall-τ accuracy prediction

- **Critical path**: 
  1. Parse architecture → adjacency + op matrix
  2. Embed ops via operation table (unified if cross-space)
  3. Feed into DGF → GAT → ensemble → aggregate
  4. Concatenate supplemental encodings (ZCP, Arch2Vec, CATE)
  5. MLP → accuracy prediction
  6. During transfer: load pre-trained weights → re-embed ops with unified index → fine-tune

- **Design tradeoffs**: 
  - GNN depth vs. over-smoothing (hence residual connections)
  - Number of supplemental encodings vs. MLP capacity
  - Timesteps T for operation updates vs. training time
  - Unified index granularity vs. embedding table size

- **Failure signatures**: 
  - Low Kendall-τ on held-out architectures → likely overfit or poor encoding choice
  - Sudden drop after transfer → source/target space too dissimilar
  - High variance across trials → insufficient training samples or unstable embeddings

- **First 3 experiments**:
  1. Train FLAN on NASBench-101 with adjacency only → measure Kendall-τ on 10% held-out set.
  2. Add ZCP as supplemental encoding → retrain → measure improvement.
  3. Transfer pre-trained FLAN from ENAS to Amoeba (16 samples) → compare to scratch training.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important research directions emerge from the work:
- How does the choice of additional supplementary encodings affect the transferability of FLAN across different NAS search spaces?
- What is the optimal number of time steps for updating operation embeddings in FLAN, and how does it vary across different NAS benchmarks?
- How does the dual graph flow mechanism in FLAN compare to other advanced GNN architectures in terms of prediction accuracy and sample efficiency?

## Limitations
- The unified encoding approach assumes semantic similarity between operations across spaces, but this isn't validated for completely disjoint operation sets
- The computational overhead of combining multiple GNN flows and supplemental encodings may limit scalability to larger architecture spaces
- Transfer learning claims depend heavily on the similarity between source and target NAS spaces, with limited analysis of transfer failure modes

## Confidence
- **High confidence**: FLAN architecture design (DGF+GAT hybrid) and its implementation details are well-specified and reproducible
- **Medium confidence**: Cross-domain transfer learning results showing 8× sample efficiency gains are promising but depend on source-target space similarity
- **Low confidence**: Scalability claims to 1.5M architectures across 13 NAS benchmarks are asserted but not thoroughly validated with computational complexity analysis

## Next Checks
1. **Transfer robustness test**: Systematically evaluate FLAN transfer performance when source and target spaces have increasingly dissimilar operation sets, documenting the point where transfer breaks down.
2. **Ablation study on GNN components**: Remove either DGF or GAT components individually and measure the impact on Kendall-τ correlation to quantify their individual contributions versus the ensemble effect.
3. **Scalability benchmark**: Measure training and inference times for FLAN on progressively larger NAS spaces (starting from 10K to 1M architectures) to verify computational feasibility claims.