---
ver: rpa2
title: More complex environments may be required to discover benefits of lifetime
  learning in evolving robots
arxiv_id: '2412.16184'
source_url: https://arxiv.org/abs/2412.16184
tags:
- learning
- environment
- robot
- more
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates how lifetime learning impacts robot performance
  across different environments. Two environments were tested: a flat terrain and
  a hilly terrain with obstacles.'
---

# More complex environments may be required to discover benefits of lifetime learning in evolving robots

## Quick Facts
- arXiv ID: 2412.16184
- Source URL: https://arxiv.org/abs/2412.16184
- Authors: Ege de Bruin; Kyrre Glette; Kai Olav Ellefsen
- Reference count: 2
- Primary result: Lifetime learning significantly improves robot performance in complex hilly environments but shows minimal impact in flat environments

## Executive Summary
This study investigates how lifetime learning affects robot performance across different environmental complexities. The research tested two environments - flat terrain and hilly terrain with obstacles - using an evolutionary algorithm to optimize robot morphologies while employing Bayesian Optimization to fine-tune control parameters with varying learning budgets. The key finding is that lifetime learning provides significant performance benefits in complex environments but minimal advantages in simple environments when comparing function evaluations. This suggests that environmental complexity is a crucial factor in determining when lifetime learning becomes beneficial for evolving robots.

## Method Summary
The study used a modular robot framework where morphologies were evolved using a standard evolutionary algorithm while control parameters were optimized using Bayesian Optimization during each robot's lifetime. Three learning budgets were tested (1, 30, and 50 iterations), with performance evaluated in both flat and hilly environments. The fitness function measured distance traveled in a fixed simulation time. Robots consisted of core, brick, and hinge modules with decentralized sine wave controllers that could be tuned based on touch sensor feedback. The experiments compared performance across different numbers of controller sets (1, 4, or 8) and learning budgets.

## Key Results
- Lifetime learning significantly improved performance in hilly environments compared to flat environments
- After 100,000 function evaluations, robots with learning budgets of 30 and 50 significantly outperformed those with budget of 1 in the hilly environment (p < 0.05 and p < 0.01 respectively)
- In the flat environment, differences between learning budgets were minimal after sufficient function evaluations
- Performance comparison across morphologies showed less pronounced differences between learning budgets than function evaluation comparisons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lifetime learning becomes essential in complex environments because the evolutionary process alone cannot optimize controller parameters for navigating varied terrain.
- Mechanism: In flat environments, morphology evolution alone can discover effective gaits without needing parameter tuning. However, in hilly environments, the interaction between morphology and control becomes more complex - robots must adapt their gait to overcome obstacles. Learning allows individual robots to optimize their control parameters within their lifetime, compensating for the inability of the evolutionary algorithm to perfectly match morphology to control.
- Core assumption: The evolutionary algorithm cannot efficiently explore the joint space of morphology and control parameters, especially when control parameters need to be context-dependent for different terrain features.
- Evidence anchors:
  - [abstract] "learning is significantly more beneficial in a hilly environment than in a flat environment"
  - [section] "when comparing on function evaluations there is a clear difference between the runs with a learning budget of 1 and the other learning budgets" in the hills environment
  - [corpus] Weak evidence - no corpus papers directly address the morphology-control co-optimization challenge in this context
- Break condition: If the evolutionary algorithm could efficiently explore the full morphology-control space, or if the environment complexity was reduced to a level where morphology alone could handle navigation.

### Mechanism 2
- Claim: Bayesian Optimization with learning budgets allows robots to escape local optima in the control parameter space that evolution cannot reach.
- Mechanism: The evolutionary algorithm optimizes morphology based on performance with random initial controllers. In complex environments, random controllers may perform poorly regardless of morphology, causing evolution to discard potentially good morphologies. Learning provides multiple optimization steps per morphology, allowing exploration of the control parameter space that random initialization cannot reach.
- Core assumption: The fitness landscape for control parameters has local optima that are difficult to escape from with single evaluations, but can be overcome with iterative optimization.
- Evidence anchors:
  - [section] "experiments with a higher learning budget have more function evaluations per morphology to find better control parameters"
  - [section] "Initially, the runs with a learning budget of 1 do perform better" but "after 100.000 function evaluations, the differences are minimal" in flat environment, suggesting learning helps overcome initial random parameter limitations
  - [corpus] No direct corpus evidence for this specific Bayesian Optimization mechanism
- Break condition: If the control parameter space were convex or had a single global optimum easily reachable from random initialization.

### Mechanism 3
- Claim: The learning budget provides a form of local adaptation that complements global evolution, with benefits scaling with environmental complexity.
- Mechanism: Evolution provides global optimization across the population for morphology, while learning provides local optimization within each robot's lifetime for control. In complex environments, the interaction between morphology and control creates a more rugged fitness landscape where local adaptation becomes more valuable. The 30 and 50 iteration learning budgets allow sufficient exploration to find good control parameters that random initialization (1 iteration) cannot discover.
- Core assumption: Environmental complexity creates fitness landscapes where global evolution alone is insufficient, and local learning provides complementary optimization that scales with difficulty.
- Evidence anchors:
  - [section] "there is a clear difference between the runs with a learning budget of 1 and the other learning budgets" in the hills environment when comparing function evaluations
  - [section] "A Wilcoxon test shows a significant difference between a learning budget of 1 and 50 on the hills environment after 100.000 function evaluations (p < 0.05)"
  - [corpus] No corpus papers directly support this specific hierarchical evolution-learning mechanism
- Break condition: If the environment were simple enough that global evolution could discover both good morphology and good control parameters without local learning.

## Foundational Learning

- Concept: Bayesian Optimization
  - Why needed here: Bayesian Optimization is used to efficiently search the continuous control parameter space (amplitude, phase offset, weights, etc.) without requiring gradient information, which is crucial for optimizing robot gaits.
  - Quick check question: What are the key components of Bayesian Optimization used in this paper, and why is the Matern 5/2 kernel chosen?

- Concept: Morphology-Control Co-evolution
  - Why needed here: Understanding that morphology and control are interdependent - a good morphology with poor control may perform worse than a mediocre morphology with good control, and vice versa.
  - Quick check question: Why might evolution alone struggle to co-optimize both morphology and control parameters simultaneously?

- Concept: Function Evaluation vs. Morphology Evaluation
  - Why needed here: Distinguishing between evaluating a robot's performance (function evaluation) and evaluating a robot's morphology (morphology evaluation), which have different computational costs and implications for algorithm design.
  - Quick check question: How does comparing fitness over morphologies differ from comparing fitness over function evaluations, and what does each metric tell us?

## Architecture Onboarding

- Component map:
  - Revolve2 framework -> Modular robot construction
  - MuJoCo simulator -> Physics simulation
  - Evolutionary Algorithm -> Morphology optimization
  - Bayesian Optimization -> Control parameter optimization
  - Flat and Hilly environments -> Fitness evaluation
  - Core, brick, and hinge modules -> Robot construction
  - Sine wave controllers -> Control system

- Critical path:
  1. Initialize population of robot morphologies
  2. For each morphology, run learning algorithm to optimize control parameters
  3. Evaluate fitness in environment (distance traveled)
  4. Select parents based on fitness
  5. Apply crossover and mutation to create offspring
  6. Repeat for specified generations

- Design tradeoffs:
  - Learning budget vs. computation time: Higher learning budgets (30, 50) provide better control optimization but require more function evaluations per morphology
  - Number of controllers (1, 4, 8) vs. parameter space complexity: More controllers allow specialization but increase the search space
  - Simulation time per evaluation (30 seconds) vs. evolutionary progress: Longer evaluations provide better fitness estimates but slow evolution

- Failure signatures:
  - Poor performance in both environments suggests issues with morphology evolution or control parameterization
  - Good performance in flat but poor in hilly suggests insufficient learning budget or control complexity
  - Large standard deviation across runs suggests stochastic elements need tuning (mutation rates, learning parameters)

- First 3 experiments:
  1. Run the flat environment with 1 learning iteration to establish baseline morphology-only performance
  2. Run the hilly environment with 1 learning iteration to verify the paper's claim that learning is more beneficial in complex environments
  3. Run the hilly environment with 30 learning iterations to observe the transition from morphology-dominated to learning-dominated optimization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the results change if controller parameters were evolved directly rather than learned through Bayesian Optimization?
- Basis in paper: [explicit] "For future work, we would like to compare the results to an evolutionary search with no learning, as currently there is no evolutionary search on controller parameters and this might improve performance in the no-learning case significantly."
- Why unresolved: The study only compared learning budgets (1, 30, 50 iterations) but did not include a condition where controller parameters were evolved through evolutionary algorithms rather than learned during the robot's lifetime.
- What evidence would resolve it: Experimental results comparing evolved controllers versus lifetime-learned controllers in both flat and hilly environments, showing whether direct evolution of controllers could match or exceed the performance of lifetime learning.

### Open Question 2
- Question: What is the optimal number of controller parameter sets (1, 4, or 8) for robots in different environmental complexities?
- Basis in paper: [explicit] "There are three learning budgets (1, 30, and 50), three different numbers of controllers (1, 4, and 8), and two environments (flat and hills), giving 18 experiments in total."
- Why unresolved: While the paper tested different numbers of controllers, it did not provide a systematic analysis of how the number of controller sets affects performance across different environmental complexities.
- What evidence would resolve it: Comparative results showing performance metrics for each controller set size (1, 4, 8) across both environments, identifying if there's an optimal number that balances complexity and reusability.

### Open Question 3
- Question: How does Lamarckian inheritance of learned controller parameters affect performance compared to learning from scratch?
- Basis in paper: [explicit] "This can also be extended by adding it to the learning experiments, adding Lamarckian inheritance to the robots' control."
- Why unresolved: The study used learning from scratch for all experiments, with no controller inheritance, making it unclear whether passing learned parameters to offspring would improve performance.
- What evidence would resolve it: Experimental results comparing lifetime learning with and without Lamarckian inheritance, showing whether inherited parameters lead to faster convergence or better final performance in both environments.

## Limitations
- Narrow experimental scope with only two environments tested
- Small population size (10 morphologies) and limited generations (100) may constrain search space exploration
- Specific sinusoidal control parameterization may not generalize to other control architectures

## Confidence
- High confidence: The observation that learning benefits are more pronounced in the hilly environment compared to the flat environment
- Medium confidence: The specific statistical significance of learning budgets of 30 and 50 outperforming budget of 1 in the hilly environment
- Low confidence: The generalizability of these findings to other types of environmental complexity or control architectures

## Next Checks
1. Test additional environment complexities (e.g., varying terrain height, obstacles, friction coefficients) to map the threshold at which learning becomes beneficial and determine if there's a gradual progression rather than a binary switch between flat and hilly environments.

2. Vary the population size and number of evolutionary generations to determine if the observed learning benefits persist with more extensive evolutionary search, which could reveal whether learning compensates for evolutionary limitations or provides genuinely complementary optimization.

3. Implement alternative control architectures (e.g., CPGs, neural networks) to verify whether the learning benefits observed with sinusoidal controllers generalize across different control parameterizations and whether certain architectures make learning more or less necessary.