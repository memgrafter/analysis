---
ver: rpa2
title: 'ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs'
arxiv_id: '2402.06334'
source_url: https://arxiv.org/abs/2402.06334
tags:
- dataset
- explanations
- datasets
- exaranker-v1
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study extends the ExaRanker method by using open-source LLMs
  (Llama-2-7B and Llama-2-70B) to generate explanations for augmenting training data
  in information retrieval. The method was evaluated using datasets of different sizes
  (15k and 50k relevant query-passage pairs) and compared against monoT5 baselines
  trained without explanations.
---

# ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs

## Quick Facts
- **arXiv ID**: 2402.06334
- **Source URL**: https://arxiv.org/abs/2402.06334
- **Reference count**: 15
- **Primary result**: Using open-source LLMs to generate explanations improves neural ranker performance, outperforming monoT5 trained on larger datasets by up to 0.8 nDCG@10 points

## Executive Summary
This study extends the ExaRanker method by employing open-source LLMs (Llama-2-7B and Llama-2-70B) to generate explanations for augmenting training data in information retrieval. The method was evaluated on MS MARCO passage ranking datasets of varying sizes (15k and 50k relevant query-passage pairs) and compared against monoT5 baselines trained without explanations. Results demonstrate that incorporating explanations consistently improves neural ranker performance, with larger LLMs providing greater benefits. ExaRanker-Open outperforms monoT5 trained on larger datasets (up to 400k samples) by up to 0.8 nDCG@10 points, confirming that data augmentation with explanations remains advantageous even with large datasets.

## Method Summary
The ExaRanker-Open method uses open-source LLMs (Llama-2-7B and Llama-2-70B) to generate explanations for query-passage pairs in the MS MARCO dataset. These explanations are used to augment training data for a T5-base model, which is fine-tuned for up to 30 epochs using AdamW optimizer with learning rate 3e-5, weight decay 0.01, and batch size 128. The model is evaluated using nDCG@10 metric on the BEIR benchmark datasets and compared against a monoT5 baseline trained without explanations. The study tests dataset sizes of 15k and 50k relevant query-passage pairs, plus equal non-relevant pairs.

## Key Results
- Incorporating LLM-generated explanations consistently improves neural ranker performance across all dataset sizes tested
- Larger LLMs (Llama-2-70B) provide greater performance benefits compared to smaller models (Llama-2-7B)
- ExaRanker-Open outperforms monoT5 trained on datasets up to 400k samples by up to 0.8 nDCG@10 points
- Benefits of explanation-based augmentation persist even with large datasets, addressing concerns from earlier work

## Why This Works (Mechanism)
Explanation-augmented training helps neural rankers better understand the relevance relationships between queries and passages by providing additional context about why certain pairs are relevant. This additional semantic information enables the model to learn more robust representations that generalize better to unseen queries and domains.

## Foundational Learning
- **Information Retrieval Fundamentals**: Understanding of ranking metrics like nDCG@10 and relevance judgment concepts
  - Why needed: To evaluate and interpret the performance improvements
  - Quick check: Verify understanding of how nDCG@10 is calculated and what it measures

- **LLM-based Data Augmentation**: How large language models can generate synthetic data to improve model training
  - Why needed: The core innovation involves using LLMs to create explanations that augment training data
  - Quick check: Confirm understanding of how explanations are generated and incorporated into training

- **Transformer-based Ranking Models**: Knowledge of how models like T5 process and rank text pairs
  - Why needed: To understand the architecture being trained and evaluated
  - Quick check: Verify understanding of T5's input/output format for ranking tasks

## Architecture Onboarding

**Component Map**: LLM Explanation Generator -> Augmented Dataset -> T5-base Ranker -> BEIR Evaluation

**Critical Path**: The most critical path is from LLM explanation generation through T5 training to final evaluation. The quality of explanations directly impacts the augmented dataset quality, which in turn affects ranker performance.

**Design Tradeoffs**: The study uses open-source LLMs instead of proprietary models to ensure reproducibility and accessibility. This tradeoff accepts potentially lower explanation quality compared to larger proprietary models in exchange for open availability.

**Failure Signatures**: Poor explanation quality from LLMs would manifest as degraded performance compared to monoT5 baselines. Overfitting during T5 training would show as performance drops on validation sets despite improvements on training data.

**First Experiments**:
1. Generate explanations using Llama-2-7B for a small subset of MS MARCO data and verify their relevance and coherence
2. Train T5-base on the augmented dataset with explanations and monitor validation performance on TREC-DL 2020
3. Evaluate the trained model on a single BEIR dataset (e.g., Robust04) to confirm zero-shot transfer capabilities

## Open Questions the Paper Calls Out
- How does the performance of ExaRanker-Open scale with even larger datasets beyond 300k samples, and is there an optimal dataset size where the benefits of explanation-based augmentation plateau?
- How sensitive is ExaRanker-Open's performance to the quality and diversity of the explanations generated by open-source LLMs, and can fine-tuning the explanation generation model improve retrieval effectiveness?
- Does the effectiveness of ExaRanker-Open transfer equally well across different retrieval tasks beyond the BEIR benchmark, such as specialized domains (e.g., legal, biomedical) or different ranking paradigms (e.g., re-ranking vs. first-stage retrieval)?

## Limitations
- The study does not provide the exact few-shot prompting template used for LLM explanation generation, limiting reproducibility
- Performance improvements are evaluated primarily on the BEIR benchmark, with limited testing on specialized domains
- The study does not explore fine-tuning the explanation generation LLMs themselves for potentially better performance

## Confidence
- **High confidence**: The general methodology of using LLM-generated explanations for data augmentation in IR tasks, and the overall trend of improved performance
- **Medium confidence**: The specific performance improvements (e.g., 0.8 nDCG@10 points) and the comparative advantage over monoT5 baselines
- **Low confidence**: The exact reproducibility of results without the few-shot prompting template

## Next Checks
1. Implement the explanation generation process using the same Llama-2 models with different prompting templates to assess sensitivity to prompting variations
2. Verify the T5-base fine-tuning results on the 15k and 50k datasets with explanations, comparing against the reported nDCG@10 scores
3. Test the zero-shot performance on BEIR benchmark datasets to confirm the generalization capabilities reported in the paper