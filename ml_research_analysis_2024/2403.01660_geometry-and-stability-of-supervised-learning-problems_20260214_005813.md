---
ver: rpa2
title: Geometry and Stability of Supervised Learning Problems
arxiv_id: '2403.01660'
source_url: https://arxiv.org/abs/2403.01660
tags:
- distance
- problem
- problems
- risk
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a geometric framework to compare supervised
  learning problems by introducing the Risk distance. This distance, inspired by optimal
  transport, allows quantifying how problem changes (e.g., noise, sampling bias, loss
  approximations) affect the original problem.
---

# Geometry and Stability of Supervised Learning Problems

## Quick Facts
- arXiv ID: 2403.01660
- Source URL: https://arxiv.org/abs/2403.01660
- Authors: Facundo Mémoli; Brantley Vose; Robert C. Williamson
- Reference count: 40
- Key outcome: This paper proposes a geometric framework to compare supervised learning problems by introducing the Risk distance, which facilitates stability results and enables answering key questions about the impact of compromises in machine learning problems.

## Executive Summary
This paper introduces a geometric framework for comparing supervised learning problems through a distance measure called Risk distance, inspired by optimal transport theory. The framework allows quantification of how changes to learning problems (such as noise, sampling bias, or loss approximations) affect the original problem, providing a systematic way to understand the geometry of the problem space and the stability of learning problems under modifications.

The authors establish theoretical foundations for this geometric approach, proving that important problem descriptors like constrained Bayes risk and loss profile are stable under problem modifications. They introduce two variants of the Risk distance (Lp-Risk and Connected Risk) and explore the geometry of problem spaces, providing explicit geodesics and proving the density of classification problems. This framework enables answering fundamental questions about the impact of compromises in machine learning and facilitates stability guarantees.

## Method Summary
The paper proposes a geometric framework for comparing supervised learning problems using the Risk distance, which is inspired by optimal transport theory. The framework involves defining a distance between probability distributions over labeled examples, allowing quantification of how problem changes affect the original problem. The authors introduce the Lp-Risk distance (incorporating predictor weights) and Connected Risk distance (more sensitive to risk landscape contours) as variants of the basic Risk distance. They establish theoretical foundations including stability results, explicit geodesics, and density of classification problems, while also exploring the structural connections between different types of learning problems (classification and regression).

## Key Results
- Introduces the Risk distance framework for comparing supervised learning problems, enabling quantification of problem changes
- Proves stability of problem descriptors (constrained Bayes risk and loss profile) under problem modifications
- Establishes explicit geodesics and proves density of classification problems in the problem space
- Introduces two variants of Risk distance: Lp-Risk (incorporating predictor weights) and Connected Risk (sensitive to risk landscape contours)

## Why This Works (Mechanism)
The geometric framework works by leveraging optimal transport theory to create a meaningful distance between probability distributions that represent different supervised learning problems. This distance captures both the marginal distributions over features and the joint distributions over features and labels, allowing for a comprehensive comparison that accounts for both input space structure and label relationships.

## Foundational Learning
- **Optimal Transport Theory**: Why needed - provides the mathematical foundation for defining meaningful distances between probability distributions; Quick check - understanding the Wasserstein metric and its properties
- **Supervised Learning Problem Formulation**: Why needed - establishes the basic setup of learning problems as probability distributions over labeled examples; Quick check - familiarity with joint distributions P(X,Y) and their properties
- **Bayes Risk and Loss Profiles**: Why needed - these are key descriptors that the framework proves to be stable under problem modifications; Quick check - understanding how Bayes risk depends on the problem distribution and loss function
- **Probability Measure Theory**: Why needed - provides the mathematical language for working with distributions and defining convergence; Quick check - understanding convergence in probability measures and weak convergence
- **Geodesic Geometry**: Why needed - allows characterization of shortest paths between problems in the problem space; Quick check - understanding how geodesics are computed in metric spaces
- **Classification vs Regression Problem Structure**: Why needed - framework explores connections between different problem types; Quick check - understanding how classification problems can be embedded in regression problem spaces

## Architecture Onboarding

Component Map: Problem Space -> Risk Distance -> Stability Results -> Problem Descriptors (Bayes Risk, Loss Profile) -> Problem Modifications (noise, sampling, loss approximations)

Critical Path: Define problem space → Compute Risk distance → Establish stability → Analyze problem descriptors → Apply to modifications

Design Tradeoffs:
- Computational complexity vs. theoretical precision in computing Risk distances
- Sensitivity to noise in Connected Risk distance vs. robustness in basic Risk distance
- Generality of framework vs. specificity of application to particular problem types

Failure Signatures:
- Numerical instability in computing optimal transport plans for high-dimensional problems
- Sensitivity to choice of loss function when comparing problems with different label structures
- Computational intractability for large-scale problems with complex feature spaces

First Experiments:
1. Compute Risk distances between simple synthetic problems with known modifications (adding noise, changing sampling distribution)
2. Verify stability of Bayes risk and loss profile under controlled problem modifications
3. Compute geodesics between pairs of classification problems and visualize the intermediate problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the rate of convergence in Theorem 69 depend on the properties of the problem P and the empirical problem Pn?
- Basis in paper: Explicit question 71 in the discussion section.
- Why unresolved: The paper states that the convergence rate depends on the properties of P and P1, but does not provide explicit bounds or formulas.
- What evidence would resolve it: Quantitative convergence rate bounds for specific classes of problems, such as linear regression or classification problems, would help resolve this question.

### Open Question 2
- Question: Can the framework be leveraged to design broader, practical reductions between learning problems, allowing more components of ML tasks to vary?
- Basis in paper: Discussion section mentioning the structural connection between classification and regression problems, and the possibility of reducing classification problems with severe label noise to regression problems with biased sampling.
- Why unresolved: The paper does not provide concrete examples or algorithms for such reductions.
- What evidence would resolve it: Concrete examples of reductions between learning problems, such as reducing a classification problem with label noise to a regression problem, would help resolve this question.

### Open Question 3
- Question: How does the empirical value of the optimal couplings and correspondences produced by the theory compare to other methods for relating two problems?
- Basis in paper: Discussion section mentioning the potential for insights from optimal couplings and correspondences between problems.
- Why unresolved: The paper does not provide empirical comparisons or case studies.
- What evidence would resolve it: Empirical studies comparing the performance of optimal couplings and correspondences to other methods for relating problems, such as using a common feature space, would help resolve this question.

## Limitations
- The theoretical framework assumes access to exact problem formulations and loss functions, which may not be realistic in many practical scenarios
- Computational tractability for large-scale problems with complex feature spaces remains unclear
- Empirical validation of the theoretical stability results across diverse real-world datasets and problem settings is lacking

## Confidence
- Theoretical consistency of the framework: High
- Practical utility across diverse domains: Medium
- Computational scalability: Medium
- Empirical validation of stability results: Low

## Next Checks
1. Implement empirical benchmarks comparing Risk distance-based problem comparisons against traditional metrics across multiple datasets and problem types to validate practical utility
2. Conduct computational complexity analysis for computing geodesics and Risk distances in high-dimensional problems to assess scalability
3. Design experiments testing the framework's sensitivity to different types of problem modifications (noise levels, sampling strategies, loss approximations) to quantify its practical stability guarantees