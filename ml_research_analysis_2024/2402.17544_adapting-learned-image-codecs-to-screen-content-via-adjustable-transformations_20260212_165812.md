---
ver: rpa2
title: Adapting Learned Image Codecs to Screen Content via Adjustable Transformations
arxiv_id: '2402.17544'
source_url: https://arxiv.org/abs/2402.17544
tags:
- image
- codec
- coding
- images
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving learned image codecs
  (LICs) performance on screen content (SC) images without retraining the codec itself.
  The core method involves introducing parameterized and invertible linear transformations,
  along with two neural networks (prefilter and postfilter), into the coding pipeline
  around the existing codec.
---

# Adapting Learned Image Codecs to Screen Content via Adjustable Transformations

## Quick Facts
- arXiv ID: 2402.17544
- Source URL: https://arxiv.org/abs/2402.17544
- Authors: H. Burak Dogaroglu; A. Burakhan Koyuncu; Atanas Boev; Elena Alshina; Eckehard Steinbach
- Reference count: 0
- Primary result: Up to 10% bitrate savings on screen content compression using prefilter/postfilter neural networks around frozen codec weights

## Executive Summary
This paper addresses the performance gap of learned image codecs on screen content (SC) images by introducing parameterized and invertible linear transformations along with neural network modules (prefilter and postfilter) around existing codecs. The approach maintains backward compatibility while achieving up to 10% bitrate savings compared to baseline LICs. The core insight is that screen content images can be transformed into more compressible representations through spatial modulations and desaturation, with neural networks learning to compensate for information loss and coding artifacts.

## Method Summary
The method introduces a prefilter neural network that creates more compressible images through spatial modulations and desaturation transformation, followed by a frozen baseline codec, then a postfilter neural network that removes modulations and reduces artifacts. The system is trained end-to-end while keeping codec weights frozen, using the JPEGAI-SC dataset. The desaturation transformation linearly interpolates between RGB images and grayscale versions, and the neural networks use MBConv architectures with 8 layers and 32 channels. The pipeline achieves improved compression efficiency for screen content while maintaining compatibility with existing codec implementations.

## Key Results
- Up to 10% bitrate savings on screen content compression compared to baseline learned image codecs
- Only 1% additional parameters introduced while maintaining backward compatibility
- Desaturation transformation with saturation level α=0.8 provides optimal balance between information loss and compressibility
- Performance gains are consistent across multiple baseline codecs (Ballé et al., Minnen et al., Cheng et al.)

## Why This Works (Mechanism)

### Mechanism 1
The prefilter and postfilter neural networks effectively compensate for information loss introduced by the desaturation transformation. The prefilter adds spatial modulations to create more compressible representations, while the postfilter learns to reverse these modulations and reduce artifacts. Core assumption: neural networks can learn effective encoding/decoding of spatial modulations. Evidence: end-to-end training shows improved compression efficiency. Break condition: if neural networks cannot learn effective transformations.

### Mechanism 2
The desaturation transformation reduces bitrate by destroying information in a beneficial way for compression. By linearly interpolating between RGB and grayscale versions, it reduces color information that may be redundant in screen content. Core assumption: screen content has color information that can be reduced without significant perceptual quality loss. Evidence: lower bitrate requirements with maintained quality. Break condition: if desaturation destroys critical color information.

### Mechanism 3
End-to-end training allows modules to learn efficient information utilization through the codec. Training prefilter, postfilter, and transformations together (with frozen codec weights) creates a coordinated system where each component complements the others. Core assumption: differentiable components can be effectively trained together. Evidence: improved overall pipeline performance. Break condition: if non-differentiable codec blocks effective gradient flow.

## Foundational Learning

- **Differentiable entropy bottleneck layers and end-to-end trainable autoencoder codecs**: Understanding how learned image codecs work is fundamental to grasping why adding prefilter/postfilter modules can improve performance without retraining the codec itself. Quick check: What is the role of the entropy bottleneck in learned image compression, and why does it make the codec differentiable?

- **Context modeling and hyperpriors in learned compression**: The paper builds on context modeling techniques from prior work, and understanding these concepts helps explain how information flows through the codec and how the prefilter/postfilter can add value. Quick check: How do hyperpriors improve compression efficiency in learned codecs, and what does this imply about information redundancy in latent representations?

- **Backward compatibility in compression standards**: The paper's approach specifically addresses maintaining compatibility with existing codec implementations, which is crucial for practical deployment. Quick check: Why is backward compatibility important in compression standards, and what challenges does it pose for adapting codecs to new data types?

## Architecture Onboarding

- **Component map**: Input image → Desaturation transformation → Prefilter (neural network) → Baseline codec → Postfilter (neural network) → Inverse desaturation transformation → Output image

- **Critical path**: Image → Desaturation → Prefilter → Codec (compression/decompression) → Postfilter → Inverse desaturation → Output

- **Design tradeoffs**: Using frozen codec weights ensures backward compatibility but limits optimization of the core compression algorithm. Neural network modules increase computational complexity (only ~1% more parameters but additional MAC operations). Desaturation transformation provides bitrate savings but introduces trade-off between information loss and compressibility.

- **Failure signatures**: Performance worse than baseline codec suggests harmful artifacts or destroyed critical information. Training instability indicates issues with end-to-end training or gradient flow. Excessive computational overhead relative to performance gains suggests insufficient value from additional modules.

- **First 3 experiments**:
  1. Test different desaturation levels (α values) with only transformation active to find optimal balance
  2. Train full pipeline with simple codec and evaluate on screen content vs natural images
  3. Compare performance with different neural network architectures (varying L and C parameters)

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal transformation coefficient α for each image type or quality point in learned image codecs? The paper tested limited α values (0.1 to 0.9) and found α = 0.9 performed slightly better than α = 0.8, but did not explore full optimization space or per-image adaptation. Systematic testing across different image types, quality points, and codecs would resolve this.

### Open Question 2
How do other types of parameterized transformations (beyond desaturation, PCA downscaling, and PCA quantization) affect compressibility of screen content images? The paper focused on three specific transformations and concluded desaturation was most effective, but noted other transformations might be useful for different domains. Comprehensive evaluation of various transformation types would identify those that improve compressibility.

### Open Question 3
What is the impact of different neural network architectures and sizes for prefilter and postfilter modules on compression performance? The paper tested limited network configurations (L=5,8,10 layers and C=32,64 channels) and found deeper models with 32 channels performed worse, but did not explore other architectures like transformers or attention mechanisms. Comparative evaluation would identify optimal architectures.

### Open Question 4
How well does the proposed method generalize to other out-of-distribution domains beyond screen content, such as medical imaging, satellite imagery, or computer-generated graphics? The authors suggest adapting this approach to other special domains is promising and that different domains may require specialized transformations and architectures. Application to other domains would measure performance gains and identify domain-specific requirements.

## Limitations

- Unknown architectural details of MBConv layers used in CR and RS modules, which could significantly impact performance
- Limited dataset scope (3K training images) raises questions about generalization to broader screen content distributions
- Computational overhead considerations not fully quantified, potentially impacting real-world deployment feasibility

## Confidence

**High confidence**: The core observation that learned codecs perform suboptimally on screen content due to out-of-distribution training data is well-supported by literature and empirical results showing up to 10% bitrate savings.

**Medium confidence**: The effectiveness of desaturation transformation in reducing bitrate is demonstrated, but optimal saturation level and robustness across different screen content types could vary.

**Medium confidence**: The end-to-end training approach with frozen codec weights is validated, but specific optimization strategy and hyperparameter choices may require tuning for different codec architectures.

## Next Checks

1. **Architecture ablation study**: Test different neural network configurations (varying L and C parameters) to identify optimal trade-off between performance gains and computational overhead, and determine whether proposed MBConv architecture is essential or if simpler alternatives could suffice.

2. **Cross-dataset generalization**: Evaluate trained models on additional screen content datasets beyond JPEGAI-SC to assess generalization to different screen content distributions and verify that performance gains are not dataset-specific.

3. **Computational complexity analysis**: Quantify full inference-time computational overhead (including MAC operations and memory usage) of prefilter and postfilter modules to provide complete picture of cost-benefit trade-off and identify potential optimization opportunities.