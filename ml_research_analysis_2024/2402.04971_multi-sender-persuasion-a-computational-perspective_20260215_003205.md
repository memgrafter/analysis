---
ver: rpa2
title: 'Multi-Sender Persuasion: A Computational Perspective'
arxiv_id: '2402.04971'
source_url: https://arxiv.org/abs/2402.04971
tags:
- utility
- receiver
- sender
- persuasion
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the computational complexity of finding Nash
  equilibria in multi-sender Bayesian persuasion games, where multiple senders with
  private information attempt to influence a single receiver's actions. The authors
  prove that computing best responses and finding equilibria are both computationally
  hard problems (NP-Hard and PPAD-Hard respectively), even in simple settings.
---

# Multi-Sender Persuasion: A Computational Perspective

## Quick Facts
- arXiv ID: 2402.04971
- Source URL: https://arxiv.org/abs/2402.04971
- Reference count: 40
- Multi-sender persuasion games are computationally hard (NP-Hard and PPAD-Hard)

## Executive Summary
This paper tackles the computational complexity of finding Nash equilibria in multi-sender Bayesian persuasion games, where multiple senders with private information attempt to influence a single receiver's actions. The authors prove that computing best responses and finding equilibria are both computationally hard problems (NP-Hard and PPAD-Hard respectively), even in simple settings. To address this, they propose a novel deep learning approach using a differentiable neural network architecture called DNL (Discontinuous and Non-Linear) to approximate the discontinuous utility functions in the game. The DNL network uses a lower part with ReLU activation and a higher part whose weights and biases are generated by a hypernetwork conditioned on the lower part's activation pattern, enabling it to represent both discontinuities and non-linearities. They combine this with the extra-gradient algorithm to find local Nash equilibria. Experiments on synthetic problems show that their method finds better local equilibria than baseline networks (ReLU and DeLU) and full-revelation equilibria, with higher sender utilities and social welfare.

## Method Summary
The authors propose a deep learning approach to find local Nash equilibria in multi-sender Bayesian persuasion games. They use a novel DNL (Discontinuous and Non-Linear) neural network architecture to approximate the discontinuous utility functions, with a lower part using ReLU activation and a higher part whose weights are generated by a hypernetwork conditioned on the lower part's activation pattern. The DNL network is trained on randomly sampled joint policies and corresponding ex-ante utilities using the Adam optimizer. Once trained, the DNL network serves as a differentiable proxy for the true utility functions, and the extra-gradient algorithm is used to find local Nash equilibria by performing gradient updates on the parameterized signaling policies. The found equilibria are then evaluated as local Nash equilibria by checking if any deviations within a neighborhood result in increased utility.

## Key Results
- Computing best responses and finding equilibria in multi-sender persuasion games are NP-Hard and PPAD-Hard respectively
- DNL network achieves lower approximation errors compared to ReLU and DeLU baseline networks
- DNL+extra-gradient finds local equilibria with higher sender utilities and social welfare than full-revelation equilibria
- DNL outperforms baseline networks in finding better local equilibria on synthetic problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The discontinuous utility functions in multi-sender persuasion games create a computational hardness barrier that standard differentiable methods cannot overcome.
- Mechanism: The utility of each sender is piecewise linear with discontinuities occurring when the receiver's optimal action changes. This happens because the receiver's action depends on posterior beliefs induced by joint signals, which are discontinuous in the senders' policies.
- Core assumption: The receiver is Bayesian and takes the action maximizing expected utility given their posterior belief.
- Evidence anchors:
  - [abstract]: "the non-differentiable and indeed discontinuous nature of the utility functions (Proposition 1) also pose hurdles"
  - [section]: "Proposition 1 [Discontinuous Utility]. The sender's utility function ui(π) is discontinuous and piecewise non-linear in (π1, . . . , πn)"
- Break condition: If the receiver's utility function becomes continuous in the induced posteriors, the hardness result may not hold.

### Mechanism 2
- Claim: The DNL network architecture can approximate discontinuous functions by combining lower ReLU layers with a hypernetwork that generates weights for higher layers.
- Mechanism: The lower part with ReLU activation creates piecewise linear regions. The hypernetwork conditions on the activation pattern of the lower part to generate different weights for the higher part, creating discontinuities at boundaries between regions.
- Core assumption: The hypernetwork can effectively map activation patterns to appropriate higher-layer parameters.
- Evidence anchors:
  - [section]: "We enable a fully-connected network to be piecewise Discontinuous and Non-Linear (DNL) by dividing the network into a lower part and a higher part"
  - [section]: "inputs in D(≤K)(x) share a non-linear higher network. Therefore, within this piece, the utility approximation can be non-linear"
- Break condition: If the hypernetwork cannot capture the complex mapping from activation patterns to higher-layer parameters, approximation quality will degrade.

### Mechanism 3
- Claim: The extra-gradient algorithm can find local Nash equilibria by using the DNL network as a differentiable proxy for the true utility functions.
- Mechanism: Extra-gradient performs alternating gradient steps with a look-ahead, using the DNL network to compute gradients. This navigates the approximated utility landscape to find points where no sender can unilaterally improve.
- Core assumption: The DNL network provides a sufficiently accurate approximation of the true discontinuous utilities.
- Evidence anchors:
  - [section]: "Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria"
  - [section]: "With fj as a differentiable representation of the senders' ex-ante utility, we can run extra-gradients to find local NE"
- Break condition: If the approximation error is too large, the extra-gradient may converge to points that are not local equilibria of the true game.

## Foundational Learning

- Concept: Bayesian updating and posterior beliefs
  - Why needed here: The receiver forms posterior beliefs about states after observing joint signals, and these beliefs determine their optimal action.
  - Quick check question: Given a prior belief µ0 and likelihood π(s|ω), what is the formula for the posterior belief µs(ω) after observing signal s?

- Concept: Nash equilibrium in simultaneous games
  - Why needed here: The senders choose signaling policies simultaneously, making their interaction a simultaneous game where Nash equilibrium is the solution concept.
  - Quick check question: In a 2-player game, what condition must hold for a strategy profile to be a Nash equilibrium?

- Concept: Computational complexity classes (NP-Hard, PPAD-Hard)
  - Why needed here: The paper proves that computing best responses and finding equilibria are computationally hard problems, which motivates the deep learning approach.
  - Quick check question: What is the difference between NP-Hard and PPAD-Hard problems in terms of the types of solutions they seek?

## Architecture Onboarding

- Component map:
  Input -> Lower part (ReLU) -> Hypernetwork -> Higher part -> Output
  (Joint policy π) -> (Activation pattern) -> (Weights/biases) -> (Estimated utility)

- Critical path:
  1. Sample joint policies π
  2. Compute true utilities using game simulator
  3. Forward pass through DNL network
  4. Compute loss and backpropagate to update DNL parameters
  5. Once trained, fix DNL parameters
  6. Use extra-gradient with DNL as utility proxy to find local NE

- Design tradeoffs:
  - Depth of lower vs higher parts: Deeper lower part may capture more complex patterns but reduce discontinuities; deeper higher part increases representational power but may be harder to train
  - Hypernetwork complexity: More complex hypernetworks can capture finer-grained discontinuities but increase parameter count and training difficulty
  - Neighborhood size ϵ for local NE: Smaller ϵ gives stricter local equilibria but may make finding them harder; larger ϵ is easier to find but may not be meaningful local equilibria

- Failure signatures:
  - High approximation error indicates DNL cannot capture utility discontinuities
  - Extra-gradient converging to policies that violate Bayesian plausibility (rows not summing to 1)
  - Found "local equilibria" where deviations within neighborhood actually improve utility
  - Training instability or mode collapse in hypernetwork

- First 3 experiments:
  1. Train DNL on synthetic game with known discontinuities, visualize approximation quality
  2. Compare DNL vs ReLU vs DeLU on finding local equilibria in small games
  3. Test sensitivity of extra-gradient to approximation error by adding noise to DNL outputs

## Open Questions the Paper Calls Out

- Can the DNL network architecture be extended to handle continuous action spaces beyond discrete sets?
- What is the computational complexity of finding a local Nash equilibrium in multi-sender persuasion games?
- How does the performance of the DNL architecture compare to other discontinuous function approximation methods beyond DeLU?
- Can the theoretical hardness results for best response computation be circumvented under specific utility structures?

## Limitations

- The empirical validation is limited to synthetic problems with relatively small state and signal spaces (up to 10 states/signals), leaving scalability to larger problems unexplored.
- The paper does not provide theoretical guarantees on the quality of local equilibria found relative to global optima.
- Implementation details around ensuring Bayesian plausibility constraints during optimization and specific tie-breaking rules for receiver action selection are not fully specified.

## Confidence

- **High Confidence**: Computational hardness results (NP-Hard and PPAD-Hard) for computing best responses and finding equilibria, respectively. These are supported by formal proofs in the appendix.
- **Medium Confidence**: DNL architecture's ability to approximate discontinuous utility functions. While the mechanism is sound and experiments show lower approximation error than baselines, the theoretical guarantees on approximation quality are limited.
- **Medium Confidence**: Extra-gradient algorithm finding local Nash equilibria using DNL as proxy. The algorithm is standard, but the paper doesn't provide convergence guarantees or bounds on approximation error propagation.

## Next Checks

1. **Scalability Test**: Evaluate the DNL+extra-gradient approach on larger synthetic problems with state and signal spaces of 20+ elements to assess computational scalability and approximation quality degradation.

2. **Constraint Satisfaction Analysis**: Implement and test explicit Bayesian plausibility constraint projection during extra-gradient optimization to verify that found equilibria satisfy the constraint that each sender's signaling policy forms a valid probability distribution over signals for each state.

3. **Global Optimality Bounds**: Develop and empirically validate bounds on how close local equilibria found by extra-gradient are to the global optimum, particularly in relation to the approximation error introduced by the DNL network.