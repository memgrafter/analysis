---
ver: rpa2
title: Answering real-world clinical questions using large language model based systems
arxiv_id: '2407.00541'
source_url: https://arxiv.org/abs/2407.00541
tags:
- page
- chatrwd
- claude
- gemini
- openevidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated five large language model (LLM)-based systems'
  ability to answer clinical questions, comparing general-purpose LLMs (ChatGPT-4,
  Claude 3 Opus, Gemini Pro 1.5) with purpose-built systems (OpenEvidence and ChatRWD).
  Nine physicians reviewed responses for relevance, reliability, and actionability
  across 50 clinical questions.
---

# Answering real-world clinical questions using large language model based systems

## Quick Facts
- arXiv ID: 2407.00541
- Source URL: https://arxiv.org/abs/2407.00541
- Reference count: 0
- Five LLM-based systems evaluated for answering clinical questions

## Executive Summary
This study evaluates five large language model (LLM)-based systems for answering clinical questions, comparing general-purpose LLMs (ChatGPT-4, Claude 3 Opus, Gemini Pro 1.5) with purpose-built systems (OpenEvidence and ChatRWD). Nine physicians reviewed responses for relevance, reliability, and actionability across 50 clinical questions. While general-purpose LLMs rarely produced relevant and evidence-based answers (2-10%), OpenEvidence and ChatRWD performed significantly better at 24% and 58% respectively. ChatRWD uniquely answered novel questions (65%) that other systems couldn't address, demonstrating the complementary value of RAG and agentic approaches for evidence-based medicine.

## Method Summary
The study evaluated five LLM-based systems on 50 clinical questions using a physician review framework. Questions were classified by novelty through literature search, then submitted to all five systems (general LLMs via prompts, purpose-built systems via APIs). Nine physicians independently reviewed responses using a standardized rubric across three dimensions: relevance, reliability, and actionability, with ratings aggregated by majority vote and inter-rater reliability calculated.

## Key Results
- General-purpose LLMs produced relevant, evidence-based answers for only 2-10% of questions
- OpenEvidence (RAG-based) achieved 24% relevant, evidence-based answers
- ChatRWD (agentic) achieved 58% relevant, evidence-based answers and uniquely addressed 65% of novel questions
- Combined use of OpenEvidence and ChatRWD increased actionable responses from 30% and 44% individually to 60% when used together

## Why This Works (Mechanism)

### Mechanism 1
- Claim: General-purpose LLMs produce hallucinated citations in clinical QA
- Mechanism: LLMs generate plausible but non-existent references by combining learned patterns of author names, journal titles, and article structures
- Core assumption: The model has sufficient training on biomedical literature to convincingly fabricate citations
- Evidence anchors:
  - [abstract] "general-purpose LLMs... rarely produced answers that were deemed relevant and evidence-based (2% - 10%)" and "Over 40% of all citations from Claude and Gemini could not be located on PubMed"
  - [section] "When the answers by LLMs were deemed less relevant, it was often due to minor study mis-specifications... or when the citation hallucination rate was over 40%"
  - [corpus] "weak or missing" - no direct evidence in corpus
- Break condition: If citation verification becomes standard practice before clinical use, the impact of hallucinations would be reduced

### Mechanism 2
- Claim: RAG-based systems produce more reliable evidence-based answers
- Mechanism: Retrieval-augmented generation grounds responses in existing literature by retrieving relevant studies before generating answers
- Core assumption: The retrieved literature corpus contains relevant evidence for most clinical questions
- Evidence anchors:
  - [abstract] "retrieval augmented generation (RAG)-based... LLM systems produced relevant and evidence-based answers for 24% (OpenEvidence)"
  - [section] "OpenEvidence rarely misinterpreted the questions and did not hallucinate citations"
  - [corpus] "weak or missing" - no direct evidence in corpus
- Break condition: If the literature corpus is incomplete or contains conflicting evidence, RAG performance would degrade

### Mechanism 3
- Claim: Agentic systems can generate novel evidence for questions without existing literature
- Mechanism: LLM-driven user interface translates clinical queries into structured study designs (PICO) and executes cohort selection and statistical analysis on real-world data
- Core assumption: Sufficient real-world patient data exists to answer the clinical question with appropriate statistical power
- Evidence anchors:
  - [abstract] "agentic ChatRWD was able to answer novel questions compared to other LLMs (65% vs. 0-9%)"
  - [section] "ChatRWD, specifically designed to conduct comparative studies on demand, outperformed the general LLMs and the RAG-based OpenEvidence particularly for novel questions"
  - [corpus] "weak or missing" - no direct evidence in corpus
- Break condition: If the underlying data lacks sufficient patient volume or the appropriate phenotypes for cohort selection

## Foundational Learning

- Concept: PICO framework (Population, Intervention, Comparison, Outcome)
  - Why needed here: Clinical questions need structured representation for both literature retrieval and study generation
  - Quick check question: Can you break down "What is the effect of Estrogen vs Progesterone on DVT incidence after lower extremity surgery?" into PICO components?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Grounds LLM responses in existing evidence rather than relying solely on model training
  - Quick check question: How does RAG reduce hallucination risk compared to standard LLM generation?

- Concept: Temporal Query Language (TQL) for cohort selection
  - Why needed here: Enables structured querying of longitudinal EHR data for on-demand study generation
  - Quick check question: What patient inclusion/exclusion criteria would you code for "patients with migraine who received eptinezumab vs botox therapy"?

## Architecture Onboarding

- Component map: User interface -> PICO extraction -> Phenotype retrieval -> TQL generation -> Cohort selection -> Statistical analysis -> Evidence summarization -> Clinical review
- Critical path: Clinical question → PICO extraction → Phenotype retrieval → TQL generation → Cohort selection → Statistical analysis → Evidence summarization → Clinical review
- Design tradeoffs:
  - RAG vs agentic: Literature retrieval provides higher quality evidence but cannot address novel questions; agentic generation can address novel questions but depends on data availability
  - Phenotype granularity: More specific phenotypes improve study accuracy but reduce the number of answerable questions
  - Citation verification: Manual verification ensures accuracy but doesn't scale; automated verification could introduce false negatives
- Failure signatures:
  - Hallucinated citations: Model generates plausible but non-existent references
  - Phenotype mis-specification: Incorrect inclusion/exclusion criteria leading to inappropriate cohorts
  - Insufficient power: Generated cohorts too small to provide meaningful statistical results
  - Study design mismatch: PICO extraction doesn't match the clinical question intent
- First 3 experiments:
  1. Test citation verification process by submitting LLM responses to PubMed search API and measuring hallucination rate
  2. Evaluate phenotype library coverage by attempting to answer a diverse set of clinical questions and measuring phenotype match rate
  3. Compare RAG vs agentic performance on a mixed set of novel and non-novel questions to quantify complementary value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different real-world data sources affect the performance of LLM-based clinical question answering systems?
- Basis in paper: [explicit] The paper mentions that ChatRWD was limited to a single data source due to data rights considerations and notes that "results may be different if run against other RWD sources"
- Why unresolved: The study only tested ChatRWD on one real-world data source (Eversana's Electronic Health Record Integrated Database), limiting generalizability to other datasets
- What evidence would resolve it: Systematic comparison of ChatRWD performance across multiple RWD sources with varying characteristics (e.g., different EHR systems, claims databases, registries) using the same 50 clinical questions

### Open Question 2
- Question: What is the optimal balance between RAG-based and agentic LLM systems for clinical decision support?
- Basis in paper: [explicit] The paper demonstrates that OpenEvidence (RAG-based) and ChatRWD (agentic) are complementary tools, with combined actionability increasing from 30% and 44% individually to 60% when used together
- Why unresolved: While the paper shows complementarity, it doesn't determine the optimal integration strategy, response routing logic, or whether a hybrid system would outperform using them separately
- What evidence would resolve it: A randomized trial comparing standalone OpenEvidence, standalone ChatRWD, and various integrated architectures for clinical question answering across diverse clinical scenarios

### Open Question 3
- Question: How can hallucination detection and prevention be systematically improved in LLM-based clinical systems?
- Basis in paper: [explicit] The paper identifies hallucination as a major failure mode, with 40-80% of general LLM citations being unverifiable and reviewers noting that "Claude, Gemini, and ChatGPT often produce hallucinations and factually incorrect information"
- Why unresolved: While the paper quantifies the hallucination problem, it doesn't explore systematic approaches to detect hallucinations or evaluate methods to prevent them
- What evidence would resolve it: Development and validation of hallucination detection algorithms that could be integrated into LLM pipelines, followed by comparison of their effectiveness against human reviewer detection rates across multiple LLM systems

## Limitations
- Evaluation relies on physician review with potential subjectivity, though nine reviewers provide redundancy
- Fixed set of 50 clinical questions with specific selection criteria may bias results toward questions amenable to agentic approaches
- No detailed error analysis provided for specific failure modes of each system

## Confidence
- High Confidence: General-purpose LLMs produce significantly fewer relevant and evidence-based answers compared to purpose-built systems
- Medium Confidence: The complementary value of RAG and agentic systems is demonstrated, but novelty assessment methodology is not fully specified
- Low Confidence: Claims about specific mechanisms of system failures/successes are weakly supported with limited diagnostic detail

## Next Checks
1. Conduct inter-rater reliability analysis on a subset of question-answer pairs to quantify and address potential subjectivity in physician assessments
2. Perform error categorization analysis on all system responses to identify common failure modes and their root causes
3. Test system performance on a broader and more diverse set of clinical questions, including those outside the comparative cohort study domain