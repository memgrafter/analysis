---
ver: rpa2
title: Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge
  Graphs
arxiv_id: '2402.11199'
source_url: https://arxiv.org/abs/2402.11199
tags:
- reasoning
- answer
- path
- llms
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates the reasoning capability of large language
  models (LLMs) by analyzing their chain-of-thought (CoT) explanations using knowledge
  graphs (KGs). The authors propose a discriminative evaluation to assess whether
  LLMs possess knowledge of valid reasoning, and a generative evaluation to measure
  the faithfulness of generated CoT reasoning.
---

# Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2402.11199
- **Source URL**: https://arxiv.org/abs/2402.11199
- **Reference count**: 40
- **Key outcome**: The paper evaluates the reasoning capability of large language models (LLMs) by analyzing their chain-of-thought (CoT) explanations using knowledge graphs (KGs).

## Executive Summary
This paper presents a novel framework for evaluating the reasoning capabilities of large language models (LLMs) by analyzing their chain-of-thought (CoT) explanations against knowledge graphs (KGs). The authors propose two complementary evaluation approaches: a discriminative evaluation to assess whether LLMs possess knowledge of valid reasoning paths, and a generative evaluation to measure the faithfulness of the generated CoT reasoning. Through experiments on five LLMs across two datasets, the study reveals that while LLMs demonstrate sufficient knowledge to perform reasoning, there is a significant gap between answer accuracy and the faithfulness of generated CoT reasoning, suggesting that models often arrive at correct answers through incorrect reasoning paths.

## Method Summary
The authors introduce a KG-based evaluation framework that treats CoT reasoning as a sequence of knowledge graph paths. The discriminative evaluation component tests whether LLMs can distinguish valid from invalid reasoning paths when presented with explicit path options, while the generative evaluation component measures how closely generated CoT explanations align with ground truth reasoning paths extracted from KGs. The framework uses two datasets: a multi-hop reasoning dataset with annotated reasoning paths and a question-answering dataset where reasoning paths are inferred from KGs. The evaluation metrics include path validity scores, faithfulness measurements, and comparison between answer accuracy and reasoning faithfulness.

## Key Results
- LLMs possess sufficient knowledge to perform reasoning, as demonstrated by discriminative evaluation performance
- There is a significant gap between answer accuracy and reasoning faithfulness in generative evaluation
- The gap between answer accuracy and reasoning faithfulness increases with model size
- Enhanced prompting strategies like planning and self-consistency improve both answer accuracy and reasoning faithfulness

## Why This Works (Mechanism)
The framework works by leveraging knowledge graphs as a ground truth representation of valid reasoning paths, allowing for systematic evaluation of both the presence of reasoning knowledge and the quality of generated reasoning explanations. By treating CoT reasoning as traversals through KGs, the evaluation can quantify both whether models know valid reasoning paths and whether they actually use them in their explanations.

## Foundational Learning
- **Knowledge Graphs (KGs)**: Structured representations of facts and relationships - needed to provide ground truth for valid reasoning paths, quick check: understand triple representation (head, relation, tail)
- **Chain-of-Thought (CoT)**: Step-by-step reasoning explanations generated by LLMs - needed to evaluate the reasoning process, quick check: can identify intermediate reasoning steps in generated text
- **Multi-hop Reasoning**: Reasoning that requires multiple inference steps - needed as the target task for evaluation, quick check: can trace reasoning through multiple entities
- **Discriminative vs Generative Evaluation**: Two complementary evaluation approaches - needed to separately assess knowledge possession vs reasoning faithfulness, quick check: understand difference between selecting vs generating reasoning paths
- **Faithfulness Metrics**: Measures of alignment between generated and ground truth reasoning - needed to quantify reasoning quality, quick check: can compute path similarity scores

## Architecture Onboarding

**Component Map**
Knowledge Graph -> Reasoning Path Extraction -> Discriminative Evaluation -> Generative Evaluation -> Faithfulness Metrics

**Critical Path**
KG construction/selection -> Reasoning path extraction from KG -> LLM CoT generation -> Path validity assessment -> Faithfulness scoring

**Design Tradeoffs**
The framework trades evaluation complexity for accuracy by using KG-based ground truth rather than human annotations alone, but this introduces dependency on KG completeness and quality.

**Failure Signatures**
- Low discriminative performance suggests LLMs lack knowledge of valid reasoning
- High answer accuracy with low faithfulness indicates reliance on direct knowledge
- Increasing gap with model size suggests shift from reasoning to memorization

**3 First Experiments**
1. Run discriminative evaluation on a simple KG with known valid/invalid paths
2. Generate CoT explanations for a multi-hop question with manually verified reasoning
3. Compare faithfulness scores across different prompting strategies on the same questions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation accuracy depends heavily on KG completeness and correctness
- Human-annotated reasoning paths may contain biases or alternative valid interpretations
- The causal mechanism for why larger models show increased gaps remains unclear
- Alternative explanations for the observed gap (like sophisticated heuristics) are not fully explored

## Confidence
- **High confidence**: Discriminative evaluation results showing LLMs possess knowledge of valid reasoning paths
- **Medium confidence**: Generative evaluation findings of gap between answer accuracy and reasoning faithfulness
- **Medium confidence**: Claim that gap increases with model size

## Next Checks
1. **KG Completeness Validation**: Conduct systematic experiments to measure how KG incompleteness affects evaluation scores by comparing results across KGs of varying coverage and precision, particularly focusing on whether missing edges in KGs incorrectly penalize valid alternative reasoning paths.

2. **Alternative Reasoning Path Analysis**: Implement a systematic comparison between human-annotated reasoning paths and algorithmically generated alternative valid paths to quantify how often the current evaluation framework incorrectly marks valid reasoning as unfaithful due to KG limitations or annotation biases.

3. **Model Behavior Dissection**: Design controlled experiments that isolate whether larger models are truly relying on direct knowledge versus sophisticated reasoning by testing on questions where direct knowledge would be insufficient but reasoning should still succeed, while controlling for other variables like training data overlap.