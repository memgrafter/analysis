---
ver: rpa2
title: Overview of the EHRSQL 2024 Shared Task on Reliable Text-to-SQL Modeling on
  Electronic Health Records
arxiv_id: '2405.06673'
source_url: https://arxiv.org/abs/2405.06673
tags:
- task
- questions
- shared
- ehrsql
- unanswerable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The EHRSQL 2024 shared task focused on building reliable text-to-SQL
  models for electronic health records (EHRs). Eight teams participated, using methods
  ranging from fine-tuning large language models like ChatGPT to pipeline approaches
  combining SQL generation with answerability detection.
---

# Overview of the EHRSQL 2024 Shared Task on Reliable Text-to-SQL Modeling on Electronic Health Records

## Quick Facts
- arXiv ID: 2405.06673
- Source URL: https://arxiv.org/abs/2405.06673
- Reference count: 8
- Eight teams participated in building reliable text-to-SQL models for EHRs

## Executive Summary
The EHRSQL 2024 shared task focused on building reliable text-to-SQL models for electronic health records (EHRs), emphasizing both correct SQL generation and abstention from unanswerable questions. Eight teams participated using various approaches including fine-tuning large language models like ChatGPT and pipeline methods combining SQL generation with answerability detection. The primary evaluation metric was the Reliability Score (RS), with RS(10) penalizing one incorrect prediction as much as ten correct ones. The top-performing team achieved an RS(10) of 81.32 using self-training with pseudo-labeled unanswerable questions, while no team achieved positive scores in the most rigorous RS(N) setting, highlighting the ongoing challenge of building highly reliable clinical query systems.

## Method Summary
Teams employed diverse approaches to the EHRSQL task, primarily using large language models (LLMs) like ChatGPT, GPT-4, and Claude Opus. Methods ranged from zero-shot prompting to fine-tuning these models on the training data. Some teams used pipeline approaches separating SQL generation and answerability detection, while others implemented unified approaches. The top-performing team utilized self-training with pseudo-labeled unanswerable questions, demonstrating the effectiveness of domain-specific fine-tuning and reliability-aware learning strategies.

## Key Results
- Top team achieved RS(10) score of 81.32 using self-training with pseudo-labeled unanswerable questions
- Unified approaches generally outperformed pipeline-based methods, even with the same LLM backbone
- No team achieved positive scores in RS(N) setting, where one incorrect prediction outweighs all correct ones
- RS(10) was the main evaluation metric, balancing correctness and abstention through penalty c = 10

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Penalizing incorrect predictions with c = 10 balances precision and abstention better than a softer penalty.
- Mechanism: The Reliability Score (RS) assigns -c for each wrong SQL prediction or generation attempt on unanswerable questions. With c = 10, ten correct predictions are needed to offset a single error, discouraging risky guesses.
- Core assumption: Model utility improves when harm from errors is weighted more heavily than gains from correct answers.
- Evidence anchors:
  - [abstract] states that RS(10) penalizes one incorrect prediction as much as ten correct ones, and teams with smaller gaps between RS(0) and RS(10) ranked higher.
  - [section 4.1] explains the RS formula and that RS(10) was the main evaluation metric.
  - [corpus] includes the LG AI Research & KAIST team achieving the best RS(10) score using self-training with pseudo-labeled unanswerable questions, supporting the idea that penalty-guided learning works.
- Break condition: If the cost of abstention outweighs the risk of incorrect answers in the deployment setting, or if the model's confidence estimation is unreliable.

### Mechanism 2
- Claim: Fine-tuning large language models on task-specific data improves performance more than zero-shot prompting alone.
- Mechanism: Teams that fine-tuned ChatGPT or other LLMs on the training data achieved higher RS scores, indicating adaptation to EHR domain and SQL structure.
- Core assumption: Pre-trained LLMs have general language and code knowledge, but require domain-specific tuning for reliable EHR query generation.
- Evidence anchors:
  - [section 5.2] notes that teams using fine-tuned LLMs performed well, highlighting the importance of domain-specific fine-tuning.
  - [abstract] mentions methods ranging from fine-tuning to prompting, with top performers using fine-tuning.
  - [corpus] shows the LG AI Research & KAIST team used self-training with pseudo-labeled unanswerable questions, a form of fine-tuning, and achieved the best RS(10) score.
- Break condition: If the training data is too small or unrepresentative, fine-tuning may lead to overfitting or poor generalization.

### Mechanism 3
- Claim: Combining answerability detection with SQL generation in a unified approach outperforms separate pipeline steps.
- Mechanism: Unified approaches use a single model to both generate SQL and decide whether to abstain, leveraging internal consistency and shared representations.
- Core assumption: The same model can learn to detect unanswerable questions while generating SQL, reducing error propagation compared to separate models.
- Evidence anchors:
  - [section 5.2] observes that unified approaches tended to outperform pipeline-based ones, even with the same backbone LLM.
  - [abstract] describes reliability as requiring both correct SQL generation and abstention from unanswerable questions.
  - [corpus] lists five teams using unified approaches, with the top two employing this method.
- Break condition: If the model architecture cannot effectively learn the dual task or if training data lacks clear answerability labels.

## Foundational Learning

- Concept: Reliability in question answering (QA) systems
  - Why needed here: The task emphasizes reliability, requiring models to abstain from incorrect or unanswerable questions to minimize harm.
  - Quick check question: What is the difference between a model that always answers and one that sometimes abstains? Why is abstention preferred in safety-critical domains?

- Concept: Text-to-SQL modeling
  - Why needed here: The shared task focuses on translating natural language questions into SQL queries for EHR databases.
  - Quick check question: What are the key challenges in text-to-SQL modeling, such as handling complex SQL structures and time expressions in questions?

- Concept: Large language models (LLMs) and fine-tuning
  - Why needed here: Most teams used LLMs like ChatGPT, and fine-tuning on task-specific data was crucial for high performance.
  - Quick check question: How does fine-tuning an LLM differ from zero-shot or few-shot prompting? What are the benefits and risks?

## Architecture Onboarding

- Component map:
  Input: Natural language question -> LLM-based SQL generation with abstention -> Output: SQL query or abstention decision -> Evaluation: Reliability Score (RS) with penalty c = 10 -> External: EHR database (MIMIC-IV demo)

- Critical path:
  1. Receive question
  2. Generate SQL or decide to abstain
  3. If generating SQL, execute against EHR database
  4. Compare execution result to ground truth
  5. Calculate RS score

- Design tradeoffs:
  - Unified vs. pipeline approaches: Unified offers shared representations and reduced error propagation, while pipelines allow specialized models but risk compounding errors.
  - Penalty choice: Higher penalties (e.g., RS(N)) enforce stricter reliability but may lead to excessive abstention; lower penalties (e.g., RS(0)) encourage answering but risk harm.
  - Fine-tuning vs. prompting: Fine-tuning adapts to domain but requires data and risks overfitting; prompting is faster but may lack reliability.

- Failure signatures:
  - Low RS scores with high abstention rates: Model is too conservative, abstaining from answerable questions.
  - Low RS scores with low abstention: Model is overconfident, answering incorrectly or attempting unanswerable questions.
  - Large gap between RS(0) and RS(10): Model struggles to balance correctness and abstention.

- First 3 experiments:
  1. Implement ABSTAIN-ALL baseline to establish minimum RS(10) score (20%) and understand abstention-only performance.
  2. Fine-tune a base LLM (e.g., ChatGPT) on the training data and evaluate RS(10) to assess domain adaptation impact.
  3. Implement a unified approach with self-training on pseudo-labeled unanswerable questions and compare RS(10) to baseline and fine-tuned models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific self-training methods and pseudo-labeling strategies did the top-performing team use for unanswerable questions?
- Basis in paper: [explicit] The paper mentions the LG AI Research & KAIST team achieved the best results using "self-training LLMs (Amini et al., 2022; Yuan et al., 2024) with pseudo-labeling for unanswerable questions" but doesn't provide implementation details.
- Why unresolved: The paper only briefly mentions the use of self-training and pseudo-labeling without explaining the specific algorithms, confidence thresholds, or iteration strategies used.
- What evidence would resolve it: Detailed description of the self-training loop, pseudo-labeling criteria, confidence thresholds, and number of iterations used by the top team.

### Open Question 2
- Question: How do the performance results change when using more advanced LLMs like GPT-4 or Claude Opus compared to ChatGPT across different reliability settings?
- Basis in paper: [inferred] The paper shows varying performance between teams using different LLMs (ChatGPT, GPT-4, Claude Opus) but doesn't systematically compare their performance across reliability settings.
- Why unresolved: While individual team results are presented, there's no direct comparison of different LLMs' performance under various reliability settings (RS(0), RS(10), RS(N)).
- What evidence would resolve it: A controlled experiment comparing the same method using different LLMs across all reliability settings with statistical significance testing.

### Open Question 3
- Question: What are the specific error patterns and failure modes of current text-to-SQL models in the EHR domain that prevent them from achieving positive scores in RS(N)?
- Basis in paper: [explicit] The paper states "none of the proposed models have yet achieved positive scores in RS(N) (where even one incorrect prediction outweighs the rest of the predictions being correct)" but doesn't analyze specific error patterns.
- Why unresolved: The paper identifies that current models fail to achieve positive RS(N) scores but doesn't provide a detailed error analysis of what types of questions or SQL patterns cause these failures.
- What evidence would resolve it: A comprehensive error analysis categorizing failures by question type, SQL complexity, and specific model architectures to identify systematic weaknesses.

## Limitations
- Training hyperparameters and specific implementation details for top-performing methods are not specified
- Dataset size and composition are not provided, limiting generalizability assessment
- No systematic comparison of different LLM performance across reliability settings

## Confidence
- High Confidence: The effectiveness of the Reliability Score (RS) in evaluating the reliability of text-to-SQL models
- Medium Confidence: The importance of fine-tuning large language models on task-specific data for improving performance
- Medium Confidence: The advantage of unified approaches over pipeline-based ones in terms of reliability

## Next Checks
1. Implement ABSTAIN-ALL baseline to establish minimum RS(10) score (20%) and understand abstention-only performance
2. Fine-tune a base LLM (e.g., ChatGPT, CodeLlama) on the EHRSQL 2024 training set and evaluate its RS(10) score to assess the impact of domain adaptation
3. Implement a unified approach that combines SQL generation and answerability detection, and fine-tune it using self-training on pseudo-labeled unanswerable questions. Compare its RS(10) score to the baseline and fine-tuned models to validate the effectiveness of this method.