---
ver: rpa2
title: 'Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial Retrieval
  with Neural Rankers and Large Language Models'
arxiv_id: '2401.01566'
source_url: https://arxiv.org/abs/2401.01566
tags:
- trial
- clinical
- training
- patient
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the IELAB team's approach to the TREC Clinical
  Trials Track 2023, which involved using neural rankers and Large Language Models
  (LLMs) to enhance clinical trial retrieval. The main challenges addressed were the
  limited availability of training data and the need to handle semi-structured patient
  descriptions.
---

# Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial Retrieval with Neural Rankers and Large Language Models

## Quick Facts
- arXiv ID: 2401.01566
- Source URL: https://arxiv.org/abs/2401.01566
- Reference count: 2
- Primary result: Achieved best NDCG@10 and P@10 scores in TREC Clinical Trials Track 2023 using hybrid neural retrievers and LLM-based re-ranking

## Executive Summary
This paper presents the IELAB team's approach to the TREC Clinical Trials Track 2023, addressing the challenge of retrieving relevant clinical trials from semi-structured patient descriptions. The team employed a multi-stage pipeline that combines neural rankers with Large Language Models (LLMs) to enhance retrieval effectiveness. By generating synthetic training data using ChatGPT, they overcame the limitation of scarce human-labeled data while leveraging the power of PubmedBERT-based dense and sparse retrievers. The system achieved state-of-the-art performance with the best NDCG@10 and P@10 scores in the competition.

## Method Summary
The IELAB team's approach involved three main stages: first, they generated synthetic patient descriptions for randomly selected clinical trials using ChatGPT to address the limited training data problem. Second, they trained both dense and sparse retrievers (PubmedBERT-base initialized) using contrastive learning with hard negative mining, combining them into a hybrid retriever through score interpolation. Finally, they employed a cross-encoder re-ranker (PubmedBERT-large) to re-rank the top 1000 results, followed by GPT-4 judgments on the top 20 results to provide final relevance scores for re-ranking. The system effectively handled semi-structured XML patient descriptions by converting them to free text using GPT-4.

## Key Results
- Achieved best NDCG@10 and P@10 scores among all participants in TREC Clinical Trials Track 2023
- Hybrid retriever combining SPLADEv2 and dense retriever outperformed individual components and BM25 baseline
- Cross-encoder re-ranker with score interpolation (0.9 weight) significantly improved retrieval effectiveness
- GPT-4 judgments on top 20 results provided additional ranking signal that enhanced final results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generation via LLMs can effectively substitute for scarce human-labeled training data in clinical trial retrieval tasks.
- Mechanism: ChatGPT is prompted to generate patient descriptions for randomly sampled clinical trials, creating a large synthetic dataset (20k examples) that supplements the small human-labeled dataset (5k examples).
- Core assumption: Generated synthetic patient descriptions maintain semantic and structural fidelity to real patient descriptions and do not introduce harmful noise that outweighs the benefit of increased training data volume.
- Evidence anchors:
  - [abstract] "Specifically, we employ ChatGPT to generate relevant patient descriptions for randomly selected clinical trials from the corpus."
  - [section] "With this prompt we could generate synthetic patient descriptions that potentially match the given randomly sampled clinical trials."
  - [corpus] Weak - no direct corpus evidence provided for this mechanism; paper does not report ablation or noise analysis.
- Break condition: If generated descriptions are systematically incorrect or introduce significant noise, model performance degrades despite increased data volume.

### Mechanism 2
- Claim: Hybrid retrieval combining dense and sparse representations improves clinical trial retrieval effectiveness over either approach alone.
- Mechanism: A SPLADEv2 sparse retriever and a dense retriever (initialized with PubmedBERT-base) are trained separately with hard negative mining, then their normalized scores are interpolated with equal weights (0.5 each) to form a hybrid retriever.
- Core assumption: Dense and sparse retrievers capture complementary information patterns in the clinical trial corpus, and simple score interpolation preserves these complementary strengths.
- Evidence anchors:
  - [section] "We utilize SPLADEv2... for the learned sparse retriever and a standard bi-encoder dense retriever... Both retrievers were initialized with the PubmedBERT-base checkpoint."
  - [section] "After the two stages of training for both SPLADEv2 and DR, we then created the hybrid retriever by interpolating the min-max normalized scores given by SPLADEv2 and DR with weights set to 0.5 for each methods."
  - [corpus] Weak - no corpus evidence provided for this mechanism's effectiveness; results are only shown against baseline BM25.
- Break condition: If dense and sparse retrievers capture overlapping information patterns, interpolation provides no benefit and may even harm performance.

### Mechanism 3
- Claim: Cross-encoder re-rankers with score interpolation can significantly improve retrieval effectiveness beyond first-stage hybrid retrieval.
- Mechanism: A PubmedBERT-large cross-encoder re-ranker is trained to re-rank the top 1000 results from the hybrid retriever, with its scores interpolated (0.9 weight) with the hybrid retriever scores.
- Core assumption: Cross-encoders can capture complex interactions between queries and documents that bi-encoders miss, and score interpolation allows leveraging both dense representation strengths and cross-encoder fine-grained matching.
- Evidence anchors:
  - [section] "We trained a cross-encoder (CE) re-ranker to re-rank the top 1000 results retrieved by our first-stage hybrid retriever. Our CE was initialized with PubmedBERT-large and trained with the LCE loss..."
  - [section] "We also applied score interpolation between the re-ranker score and the hybrid retriever score with the weight set to 0.9 for the re-ranker and 0.1 for the retriever."
  - [section] "The cross-encoder re-ranker can also significantly improve by re-ranking the top 1000 results from the hybrid first stage retriever (line k)."
- Break condition: If cross-encoder training data is insufficient or noisy, re-ranking may not improve and could potentially degrade results.

## Foundational Learning

- Concept: Contrastive learning with hard negative mining
  - Why needed here: Essential for training effective dense and sparse retrievers when only positive examples are available in training data
  - Quick check question: What is the purpose of using hard negatives sampled from top retrieved results rather than random negatives in contrastive training?

- Concept: Cross-encoder vs bi-encoder architectures
  - Why needed here: Understanding the architectural differences helps explain why cross-encoders can capture more complex query-document interactions despite higher computational cost
  - Quick check question: How does a cross-encoder architecture differ from a bi-encoder in terms of input processing and computational complexity?

- Concept: Prompt engineering for LLMs in synthetic data generation
  - Why needed here: Critical for generating high-quality synthetic training data that maintains semantic fidelity to real examples
  - Quick check question: What are the key components of an effective prompt for generating synthetic patient descriptions from clinical trial documents?

## Architecture Onboarding

- Component map: Input → XML-to-text conversion → Hybrid retriever → Cross-encoder re-ranker → GPT-4 judgment → Final ranking
- Critical path: Input → XML-to-text conversion → Hybrid retriever → Cross-encoder re-ranker → GPT-4 judgment → Final ranking
- Design tradeoffs: 
  - Synthetic data generation trades potential noise for increased training volume
  - Score interpolation balances computational efficiency (bi-encoders) with effectiveness (cross-encoders)
  - GPT-4 judgments provide additional ranking signal but introduce dependency on external API
- Failure signatures: 
  - Poor retrieval recall indicates issues with first-stage retriever training or XML-to-text conversion
  - Degradation after re-ranking suggests cross-encoder training problems or ineffective interpolation
  - Inconsistent results across runs may indicate instability in synthetic data generation
- First 3 experiments:
  1. Train and evaluate SPLADEv2 and dense retriever separately with hard negative mining to verify individual effectiveness
  2. Test hybrid retriever with different interpolation weights (0.3/0.7, 0.5/0.5, 0.7/0.3) to find optimal balance
  3. Evaluate cross-encoder re-ranker with and without score interpolation to measure impact on NDCG@10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the synthetic data generation quality compare to human-annotated data in terms of improving retrieval performance?
- Basis in paper: [explicit] The paper mentions using ChatGPT to generate synthetic patient descriptions for training, but does not provide a direct comparison of the quality of synthetic data versus human-annotated data.
- Why unresolved: The paper focuses on the overall system performance without isolating the impact of synthetic data quality.
- What evidence would resolve it: A study comparing retrieval performance using only synthetic data, only human-annotated data, and a combination of both would clarify the contribution of synthetic data.

### Open Question 2
- Question: What is the impact of using different LLMs (e.g., ChatGPT vs. GPT-4) on the effectiveness of the retrieval system?
- Basis in paper: [inferred] The paper uses ChatGPT for generating synthetic data and GPT-4 for re-ranking, but does not explore the impact of using different LLMs at each stage.
- Why unresolved: The choice of LLM might affect the quality of synthetic data and re-ranking, but this is not explored in the paper.
- What evidence would resolve it: An experiment comparing the performance of the system when using different LLMs for synthetic data generation and re-ranking would provide insights.

### Open Question 3
- Question: How does the system handle the variability in XML format patient descriptions in terms of data distribution mismatch?
- Basis in paper: [explicit] The paper mentions the challenge of semi-structured XML data and uses GPT-4 to convert it into natural language text, but does not evaluate the effectiveness of this conversion.
- Why unresolved: The conversion process might introduce errors or inconsistencies that affect retrieval performance, but this is not assessed.
- What evidence would resolve it: An analysis of the conversion accuracy and its impact on retrieval performance would clarify the effectiveness of the XML-to-text conversion.

## Limitations

- Lack of ablation studies to quantify the actual contribution of synthetic data versus human-annotated data
- No comparison with other strong retrieval baselines beyond BM25 to establish relative effectiveness
- Absence of sensitivity analysis for critical hyperparameters like interpolation weights and top-k cutoffs

## Confidence

- **High confidence**: The paper successfully demonstrates that the proposed multi-stage pipeline (hybrid retrieval + cross-encoder re-ranking + GPT-4 judgment) achieves strong performance on TREC Clinical Trials Track 2023 metrics (NDCG@10, P@10). The architectural components are clearly described and the results show consistent improvement across multiple runs.
- **Medium confidence**: The effectiveness of synthetic data generation for addressing limited training data. While the approach is well-motivated and implemented, the paper doesn't provide evidence about data quality or analyze whether synthetic descriptions introduce harmful noise.
- **Low confidence**: The claim that dense and sparse retrievers capture complementary information patterns. The paper asserts this but provides no corpus analysis or ablation studies to support that score interpolation provides benefits beyond either retriever alone.

## Next Checks

1. Conduct ablation studies comparing retrieval performance with and without synthetic data to quantify the actual benefit and measure any potential noise introduced by generated descriptions.

2. Test hybrid retriever with multiple interpolation weight configurations (e.g., 0.3/0.7, 0.7/0.3) and analyze the contribution of each retriever component to determine if the equal weighting assumption holds.

3. Evaluate cross-encoder re-ranker performance with different top-k cutoffs (e.g., top 100, top 500, top 1000) to determine the optimal range for effective re-ranking and assess whether the 1000 cutoff is justified.