---
ver: rpa2
title: Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak
  Large Vision-Language Models
arxiv_id: '2411.11496'
source_url: https://arxiv.org/abs/2411.11496
tags:
- step
- figure
- lvlms
- safe
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that seemingly safe images can be exploited
  to jailbreak large vision-language models (LVLMs) when combined with additional
  safe images and prompts. The authors introduce the Safety Snowball Agent (SSA),
  an agent-based framework that leverages LVLMs' universal reasoning capabilities
  and safety snowball effect to progressively induce harmful content generation.
---

# Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models

## Quick Facts
- arXiv ID: 2411.11496
- Source URL: https://arxiv.org/abs/2411.11496
- Authors: Chenhang Cui; Gelei Deng; An Zhang; Jingnan Zheng; Yicong Li; Lianli Gao; Tianwei Zhang; Tat-Seng Chua
- Reference count: 40
- Key outcome: This paper demonstrates that seemingly safe images can be exploited to jailbreak large vision-language models (LVLMs) when combined with additional safe images and prompts, achieving an 88.6% jailbreak success rate against GPT-4o.

## Executive Summary
This paper reveals a novel vulnerability in large vision-language models (LVLMs) where seemingly safe images can be exploited to generate harmful content when combined with additional safe images and prompts. The authors introduce the Safety Snowball Agent (SSA), an agent-based framework that leverages LVLMs' universal reasoning capabilities and a "safety snowball effect" to progressively induce harmful content generation. The framework operates in two stages: initial response generation using jailbreak images and harmful snowballing through iterative prompts. Experiments show that SSA achieves high jailbreak success rates across multiple LVLM architectures, effectively bypassing content moderators and raising significant concerns about the safety of current LVLM systems.

## Method Summary
The Safety Snowball Agent (SSA) framework exploits LVLMs' advanced reasoning capabilities to jailbreak seemingly safe images. The two-stage approach first uses intent recognition to identify harmful topics from input images, then employs tool-based image generation to create jailbreak images that influence the model's output. The framework then uses harmful snowballing, where initial unsafe responses are iteratively built upon to generate increasingly harmful content. SSA is evaluated on MM-SafetyBench and SafeAttack-Bench datasets using jailbreak success rate and harmfulness score metrics.

## Key Results
- SSA achieves an 88.6% jailbreak success rate against GPT-4o and similar high rates across other LVLMs
- Both safe and unsafe images can trigger similar harmful outputs and neuronal activations in LVLMs
- The safety snowball effect allows initial unsafe responses to be amplified into progressively more harmful content
- SSA effectively bypasses content moderators by using seemingly safe inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Universal reasoning capabilities enable LVLMs to overinterpret relationships across safe multimodal inputs
- Mechanism: The model integrates visual and textual inputs, creating unintended connections that lead to harmful content generation
- Core assumption: LVLMs' strong reasoning abilities can lead to overinterpretation rather than accurate understanding
- Evidence anchors:
  - [abstract] "Livelms exhibit advanced reasoning capabilities, enabling them to integrate and interpret complex relationships between visual and textual inputs... While this facilitates sophisticated content understanding and generation, it can also lead to overinterpretation"
  - [section 3] "By first presenting a question related to harmful intent-but not overtly harmful enough for the model to directly refuse-the LVLM can generate an initial response that implicitly accepts the premise of answering"
  - [corpus] Weak evidence - no direct studies found on overinterpretation as a jailbreak mechanism, though reasoning capabilities are well-documented
- Break condition: If the model's reasoning capabilities are limited or if explicit safety checks are implemented at the reasoning level

### Mechanism 2
- Claim: Safety snowball effect amplifies initial unsafe responses into progressively harmful content
- Mechanism: Once an initial unsafe response is generated, subsequent prompts build upon this context to produce increasingly harmful outputs
- Core assumption: LVLMs cannot self-correct during single-turn interactions and will continue along harmful trajectories
- Evidence anchors:
  - [abstract] "Through our experiments... we observe a similar effect in safety, where an initial unsafe statement leads to further harmful content generation. We term this effect the 'safety snowball effect'"
  - [section 3] "The transformer architecture used in LVLMs cannot inherently handle sequential reasoning problems within a single timestep... During single-turn interactions, the model is unable to self-correct"
  - [corpus] Moderate evidence - cascade effects in LLMs are documented, but specific safety snowball effects in LVLMs require more study
- Break condition: If the model can recognize and interrupt harmful patterns during generation, or if safety mechanisms can detect and block cascading harm

### Mechanism 3
- Claim: Safe images can be exploited through combination with additional safe images and prompts
- Mechanism: The SSA framework uses tool-based image generation and retrieval to create jailbreak images that, when combined with original safe images, trigger harmful responses
- Core assumption: The combination of multiple safe elements can create unsafe outputs through emergent properties
- Evidence anchors:
  - [abstract] "we reveal that a safe image can be exploited to achieve the same jailbreak consequence when combined with additional safe images and prompts"
  - [section 4.3] "With the obtained jailbreak image v∗, we proceed to generate the initial response from the target LVLM... By incorporatingv∗ into the input, we aim to influence the model's output towards the harmful topich"
  - [corpus] Moderate evidence - the combination of safe elements creating unsafe outputs is demonstrated, but the specific mechanism of tool-based jailbreak image generation needs more validation
- Break condition: If the model can detect and block the combination of safe elements that could create harmful outputs

## Foundational Learning

- Concept: Multimodal reasoning in LVLMs
  - Why needed here: Understanding how LVLMs process and integrate visual and textual information is crucial for comprehending how jailbreak attacks work
  - Quick check question: How do LVLMs typically process multimodal inputs, and what architectural components enable this integration?

- Concept: Safety alignment mechanisms in LVLMs
  - Why needed here: Knowing how safety mechanisms work helps understand how jailbreak attacks bypass them
  - Quick check question: What are the primary methods used to align LVLMs with safety guidelines, and where are their potential vulnerabilities?

- Concept: Cascade effects and self-correction in language models
  - Why needed here: The safety snowball effect relies on the model's inability to self-correct during generation
  - Quick check question: Why can't transformers inherently handle sequential reasoning problems within a single timestep, and what architectural limitations cause this?

## Architecture Onboarding

- Component map:
  - Intent Recognition Assistant (A) -> Tool Pool (T) -> Generator (G) -> Target Model (Θ) -> Evaluator (J)
  - Prompt Templates (pf, ps, pt) used throughout the interaction flow

- Critical path:
  1. Image input → Intent recognition → Tool selection
  2. Tool invocation → Jailbreak image generation
  3. Combined input → Initial response generation
  4. Context building → Snowball response generation
  5. Evaluation → Iteration or completion

- Design tradeoffs:
  - Tool selection vs. effectiveness: Different tools may be more effective for different harmful topics
  - Prompt complexity vs. success rate: More sophisticated prompts may increase jailbreak success but also detection risk
  - Iteration count vs. resource usage: More iterations increase harm but also computational cost

- Failure signatures:
  - Initial response refuses to engage with harmful topics
  - Generated jailbreak images are filtered or rejected
  - Evaluator fails to detect harmfulness in responses
  - Model detects pattern and blocks subsequent requests

- First 3 experiments:
  1. Test basic SSA functionality with simple safe images and known harmful topics
  2. Evaluate effectiveness across different LVLM architectures (GPT-4o, Qwen-VL2, etc.)
  3. Measure harmfulness scores with and without the snowball effect enabled

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do safe images and unsafe images differ in their neuronal activation patterns when triggering harmful content generation in LVLMs?
- Basis in paper: [explicit] The paper discusses activation pattern similarities between safe and unsafe images in Section 5.2, noting that both types of images can trigger similar neuronal activations when producing harmful outputs.
- Why unresolved: The paper only presents initial findings comparing activation patterns between safe and unsafe images. The authors note that both types of images can lead to similarly harmful content generation and activate comparable neurons, but they do not provide a comprehensive analysis of the specific differences or similarities in these activation patterns.
- What evidence would resolve it: Detailed quantitative comparisons of neuronal activation patterns between safe and unsafe images across multiple layers and neurons in LVLMs, showing specific similarities and differences in how these images trigger harmful content generation.

### Open Question 2
- Question: What are the long-term effects of the safety snowball effect on LVLMs' safety alignment and content generation capabilities?
- Basis in paper: [explicit] The paper introduces the safety snowball effect in Section 3, describing how initial unsafe responses can lead to progressively more harmful outputs through iterative prompts.
- Why unresolved: While the paper demonstrates the existence of the safety snowball effect and its impact on immediate harmful content generation, it does not explore how repeated exposure to this effect might influence the model's long-term safety alignment or its ability to generate safe content in future interactions.
- What evidence would resolve it: Longitudinal studies tracking LVLM behavior over multiple interactions, examining how repeated exposure to the safety snowball effect influences the model's safety alignment and content generation patterns over time.

### Open Question 3
- Question: Can the SSA framework be adapted to enhance LVLM safety by identifying and mitigating potential safety risks in seemingly benign inputs?
- Basis in paper: [inferred] The paper presents SSA as a framework for exploiting LVLM vulnerabilities through safe images and prompts, achieving high jailbreak success rates. The authors note that SSA can bypass content moderators by using safe inputs.
- Why unresolved: While the paper focuses on SSA's effectiveness in jailbreaking LVLMs, it does not explore whether the same framework could be repurposed to identify and mitigate safety risks in seemingly benign inputs, potentially enhancing LVLM safety.
- What evidence would resolve it: Experiments demonstrating how SSA could be modified to detect and prevent potential safety risks in safe inputs, showing whether the framework's understanding of LVLM vulnerabilities could be leveraged to improve safety measures.

## Limitations

- Experimental scope limited to specific LVLM architectures, potentially missing vulnerabilities in other models
- SafeAttack-Bench dataset constructed using Google Cloud Vision filtering may not represent real-world image diversity
- Study focuses on single-turn interactions, leaving questions about multi-turn dialogue scenarios
- Neural analysis of activation patterns is preliminary and lacks depth

## Confidence

**High Confidence**: The effectiveness of the Safety Snowball Agent (SSA) framework in achieving high jailbreak success rates across multiple LVLM architectures.

**Medium Confidence**: The claim that safe images can be exploited when combined with additional safe images and prompts.

**Low Confidence**: The assertion about similar neuronal activations between safe and unsafe images triggering harmful outputs.

## Next Checks

1. **Cross-Model Generalization Test**: Evaluate SSA against a broader range of LVLM architectures, including open-source models with different training methodologies, to assess the attack's generalizability beyond the tested models.

2. **Real-World Robustness Evaluation**: Test the attack in multi-turn conversational scenarios and with adversarial filtering techniques applied to the SafeAttack-Bench dataset to assess real-world effectiveness and potential defenses.

3. **Neural Activation Analysis**: Conduct detailed comparative analysis of neuronal activations between safe and unsafe images using techniques like integrated gradients or attention visualization to better understand the claimed similarity in neural responses.