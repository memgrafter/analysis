---
ver: rpa2
title: What Do Machine Learning Researchers Mean by "Reproducible"?
arxiv_id: '2412.03854'
source_url: https://arxiv.org/abs/2412.03854
tags:
- learning
- reproducibility
- machine
- conference
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper clarifies the scope of "reproducibility" in ML research
  by classifying recent papers into eight distinct topic areas: repeatability, reproducibility,
  replicability, adaptability, model selection, label/data quality, meta/incentives,
  and maintainability. Through a survey of 101 papers, the authors show that each
  topic contains many works predating the current "reproducibility crisis" discourse.'
---

# What Do Machine Learning Researchers Mean by "Reproducible"?

## Quick Facts
- arXiv ID: 2412.03854
- Source URL: https://arxiv.org/abs/2412.03854
- Reference count: 40
- Primary result: This paper clarifies the scope of "reproducibility" in ML research by classifying recent papers into eight distinct topic areas: repeatability, reproducibility, replicability, adaptability, model selection, label/data quality, meta/incentives, and maintainability.

## Executive Summary
This paper addresses the ambiguity around "reproducibility" in machine learning research by proposing a refined taxonomy of eight distinct rigor types. Through a survey of 101 self-identified reproducibility papers, the authors show that researchers actually study diverse concerns—not just replication—including adaptability, model selection, data quality, and incentives. The paper demonstrates that many rigor-focused works existed before the current "crisis" narrative, revealing continuity in these concerns. Additionally, it maps direct and indirect relationships between rigor types, showing how issues in one area can influence others, while highlighting that adaptability remains notably understudied despite its practical importance.

## Method Summary
The authors conducted a literature survey of 101 ML papers published since 2017 that self-identified as being about "reproducibility," "repeatability," or "replicability." They manually categorized each paper into one or more of eight proposed rigor types, then analyzed the distribution across categories and identified direct and indirect relationships between them. The methodology involved classifying papers based on their most important theme when spanning multiple categories, though the exact search criteria and classification methodology are not fully specified in the reproduction notes.

## Key Results
- Identified eight distinct rigor types in ML reproducibility literature: repeatability, reproducibility, replicability, adaptability, model selection, label/data quality, meta/incentives, and maintainability
- Demonstrated that many historical works addressed reproducibility concerns before the current "crisis" narrative emerged
- Mapped both direct (repeatability → reproducibility) and indirect (model selection → repeatability) dependencies between rigor types
- Found that adaptability—how methods perform on different data—is notably understudied despite its practical importance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The eight-part taxonomy captures the actual breadth of ML reproducibility research.
- **Mechanism:** By surveying 101 self-identified reproducibility papers and mapping them to eight distinct rigor types, the framework exposes that researchers actually study diverse concerns—not just replication. This allows clearer discussion of what is being measured and why.
- **Core assumption:** The surveyed papers are representative of the broader ML reproducibility literature.
- **Evidence anchors:**
  - [abstract] "we propose to refine the research to eight general topic areas."
  - [section] "Our literature survey identifies at least eight primary aspects of scientific rigor..."
  - [corpus] Found 25 related papers; average neighbor FMR=0.48. Evidence is moderate; corpus is small but topically aligned.
- **Break condition:** If most ML reproducibility work falls outside these eight categories, the taxonomy will misrepresent the field.

### Mechanism 2
- **Claim:** Historical work on reproducibility predates the current "crisis" narrative, revealing continuity in rigor concerns.
- **Mechanism:** The paper shows that many rigor-focused papers existed before 2017, suggesting reproducibility is not a new crisis but a longstanding technical challenge gaining new visibility.
- **Core assumption:** Papers published before 2017 address similar problems, even without the reproducibility label.
- **Evidence anchors:**
  - [section] "many historical works tackled these very issues... because they go back decades before the matter came to broader attention."
  - [section] "we note the field of programming languages has also proposed..."
  - [corpus] Related papers include meta-reviews and historical perspectives; moderate evidence for continuity.
- **Break condition:** If pre-2017 rigor work is methodologically distinct from current reproducibility concerns, continuity breaks down.

### Mechanism 3
- **Claim:** Indirect rigor relationships (e.g., model selection influencing replicability) reveal hidden dependencies in research practice.
- **Mechanism:** The paper maps both direct (repeatability → reproducibility) and indirect (model selection → repeatability) dependencies, showing how choices in one rigor type cascade to others, affecting overall reproducibility outcomes.
- **Core assumption:** Rigor types are not independent and their interactions meaningfully affect reproducibility outcomes.
- **Evidence anchors:**
  - [section] "we can further draw other connections that are of particular note... model selection on repeatability, reproducibility, and replicability..."
  - [section] "we note that a method that is adaptable is more likely to be maintainable."
  - [corpus] Limited evidence; corpus neighbors do not explicitly address these dependencies.
- **Break condition:** If rigor types are actually independent in practice, mapping their dependencies is misleading.

## Foundational Learning

- **Concept:** The distinction between repeatability, reproducibility, and replicability.
  - **Why needed here:** These three terms are often used interchangeably but have precise ACM definitions; misunderstanding them undermines rigor discussions.
  - **Quick check question:** Which of the following requires different people, same code, and same data? (Answer: reproducibility)

- **Concept:** Adaptability vs. generalization.
  - **Why needed here:** Adaptability concerns applying a method to entirely different data distributions, not just train/test splits; conflating them misguides research focus.
  - **Quick check question:** Does adaptability assume an IID relationship between training and test data? (Answer: No)

- **Concept:** The role of meta-analysis and incentives in reproducibility.
  - **Why needed here:** Without incentives and meta-evaluation, reproducibility efforts may not be sustained; this concept grounds practical adoption.
  - **Quick check question:** Why might reproducible work receive more citations? (Answer: transparency and verifiability increase impact)

## Architecture Onboarding

- **Component map:**
  - Literature Survey Engine → Eight-Rigor Classifier → Dependency Graph Generator → Meta-Analysis Layer
  - Inputs: 101 self-identified reproducibility papers, historical literature corpus
  - Outputs: Rigor taxonomy, dependency maps, recommendation matrix

- **Critical path:**
  1. Ingest paper metadata and abstracts.
  2. Apply manual or semi-automated classification to rigor types.
  3. Construct direct and indirect dependency links.
  4. Generate actionable recommendations per rigor type.

- **Design tradeoffs:**
  - Manual classification ensures nuance but limits scale.
  - Broad category definitions risk overlap but aid communication.
  - Dependency mapping is heuristic; evidence may be anecdotal.

- **Failure signatures:**
  - High inter-category confusion (>30% overlap in paper assignments).
  - Sparse historical literature mapping for pre-2017 papers.
  - Over-reliance on self-identification biases taxonomy toward "crisis" framing.

- **First 3 experiments:**
  1. Reclassify 20 randomly selected papers using only abstract text to test robustness.
  2. Validate dependency claims by surveying 10 researchers on perceived rigor interdependencies.
  3. Extend taxonomy to non-ML fields and measure overlap with existing reproducibility frameworks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the most effective methods for measuring and improving the adaptability of ML models to new, statistically different datasets?
- **Basis in paper:** [explicit] The paper identifies adaptability as notably understudied despite its practical importance, noting that generalization assumes intrinsic relationships between training and testing distributions while adaptability requires effectiveness on entirely different statistical distributions.
- **Why unresolved:** The paper states that "very little work on scientific rigor in AI/ML focuses on Adaptability" and that "considerable room for researchers to define and develop new ways of studying the problem" exists.
- **What evidence would resolve it:** Empirical studies comparing different approaches to measuring and improving model adaptability across diverse datasets, establishing standardized benchmarks and metrics for adaptability assessment.

### Open Question 2
- **Question:** How can the relationships between different rigor types (repeatability, reproducibility, replicability, etc.) be formally modeled to predict the impact of improvements in one area on others?
- **Basis in paper:** [explicit] The paper discusses direct and indirect relationships between rigor types in Figure 1 and section 3, noting connections like how adaptability influences maintainability, and how model selection affects all other aspects.
- **Why unresolved:** While the paper identifies these relationships, it presents them as observational findings rather than formal, predictive models of how changes in one rigor type propagate to others.
- **What evidence would resolve it:** Mathematical models or simulation frameworks that quantify how interventions in specific rigor types cascade through the interconnected system, validated through controlled experiments.

### Open Question 3
- **Question:** What are the optimal incentive structures that would encourage researchers to prioritize reproducibility and other rigor aspects without compromising innovation?
- **Basis in paper:** [explicit] The paper identifies "Meta & Incentive" as one of the eight rigor types but notes "very few papers have studied incentives for scientific rigor" and that this area is understudied.
- **Why unresolved:** The paper mentions that incentives influence all other parts of scientific rigor but provides no concrete evidence about which incentive structures are most effective or how to balance rigor with innovation.
- **What evidence would resolve it:** Comparative studies of different incentive structures across research communities, longitudinal data on how incentive changes affect rigor practices, and controlled experiments testing various reward mechanisms.

## Limitations

- The taxonomy's comprehensiveness depends heavily on the representativeness of the 101 surveyed papers, which were self-identified as reproducibility-focused, potentially biasing the taxonomy toward crisis-oriented framing.
- The classification process appears manual and may lack clear operational definitions for assigning papers to multiple overlapping categories, introducing subjectivity and inconsistency.
- Dependency relationships between rigor types are described but not empirically validated; the evidence appears largely heuristic and may not reflect actual practice dependencies.

## Confidence

- **High confidence**: The existence of eight distinct rigor types in ML reproducibility literature is well-supported by the survey data and aligns with ACM definitions.
- **Medium confidence**: Historical continuity claims are plausible given the identified pre-2017 works, but the selection process and alignment criteria are unclear.
- **Low confidence**: Dependency relationships between rigor types are proposed but lack empirical validation beyond heuristic reasoning.

## Next Checks

1. **Reclassification robustness test**: Randomly sample 20 papers from the original corpus and reclassify them using only abstract text, measuring inter-rater reliability and category confusion rates.
2. **Dependency validation survey**: Survey 10 active ML researchers to empirically assess whether they perceive the proposed dependency relationships between rigor types as accurate reflections of their research practice.
3. **Historical alignment verification**: Systematically review 10 pre-2017 rigor-focused papers to verify whether their methodological concerns and solutions align with the modern taxonomy's eight categories.