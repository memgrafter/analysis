---
ver: rpa2
title: Do Vision and Language Encoders Represent the World Similarly?
arxiv_id: '2401.05224'
source_url: https://arxiv.org/abs/2401.05224
tags:
- matching
- retrieval
- vision
- encoders
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether vision and language encoders inherently
  represent the world similarly, despite being trained separately. It employs Centered
  Kernel Alignment (CKA) to measure representation similarity between unaligned and
  aligned encoders on image-caption benchmarks, finding that unaligned encoders exhibit
  comparable semantic similarity to aligned ones like CLIP.
---

# Do Vision and Language Encoders Represent the World Similarly?

## Quick Facts
- arXiv ID: 2401.05224
- Source URL: https://arxiv.org/abs/2401.05224
- Reference count: 40
- Key outcome: Unaligned vision and language encoders exhibit comparable semantic similarity to aligned models like CLIP, enabling zero-shot caption matching and retrieval.

## Executive Summary
This paper investigates whether vision and language encoders trained separately can inherently represent the world similarly. Using Centered Kernel Alignment (CKA) as a similarity metric, the authors find that unaligned encoders (e.g., DINOv2 paired with All-Roberta-large-v1) show semantic alignment comparable to aligned models like CLIP. They propose two zero-shot methods—a seeded Quadratic Assignment Problem (QAP) optimization and a localized CKA metric—to match and retrieve captions without training. These approaches achieve strong performance on tasks like cross-lingual, cross-domain caption matching, and image classification, demonstrating effective zero-shot communication between unaligned encoders.

## Method Summary
The paper employs Centered Kernel Alignment (CKA) to measure representation similarity between vision and language encoders on image-caption benchmarks. Two methods are proposed: a seeded QAP optimization that finds optimal caption permutations maximizing CKA, and a localized CKA metric for retrieval tasks. Both methods leverage semantic similarities in cross-modal spaces without requiring training. Experiments are conducted on COCO and NoCaps datasets for caption matching, XTD-10 for cross-lingual tasks, and ImageNet-100 for classification.

## Key Results
- Unaligned vision and language encoders exhibit comparable CKA scores and matching accuracy to aligned models like CLIP.
- The QAP-based seeded matching method achieves strong performance in caption matching tasks.
- Local CKA-based retrieval exceeds CLIP's performance by over 17% on COCO captions.
- The proposed methods demonstrate effective zero-shot cross-lingual and cross-domain caption matching.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The representation spaces of unaligned vision and language encoders are semantically similar.
- **Mechanism**: CKA measures similarity between kernel matrices derived from embeddings. High CKA scores indicate similar semantic relationships in latent spaces.
- **Core assumption**: Vision and language encoders trained on different modalities but representing the same physical world develop similar internal representations of concepts.
- **Evidence anchors**: [abstract] states "representation spaces of unaligned and aligned encoders are semantically similar"; [section 4] shows "maximum CKA score is obtained on ground-truth ordering."
- **Break condition**: If encoders are trained on fundamentally different data distributions or tasks, CKA scores would be low, indicating poor semantic alignment.

### Mechanism 2
- **Claim**: QAP-based seeded matching can find optimal permutation of captions that maximizes CKA.
- **Mechanism**: Frames problem as seeded graph-matching using base aligned samples to guide query matching. QAP optimization seeks permutation maximizing CKA between permuted image and text representations.
- **Core assumption**: Exists permutation of captions aligning with image representations such that kernel matrices are maximally similar.
- **Evidence anchors**: [section 4.1] states "solution to this problem seeks to realign permuted set of images in way that maximizes CKA"; [section 5.3] shows "simple k-means clustering on image embeddings works best."
- **Break condition**: If semantic similarities between cross-modal spaces are too weak or noisy, QAP may not find meaningful permutation.

### Mechanism 3
- **Claim**: Local CKA-based retrieval can effectively match images and captions without training.
- **Mechanism**: Local CKA computes CKA score between query image-text pair and base set of aligned samples. Enables retrieval by finding caption with highest local CKA for given image.
- **Core assumption**: Correctly matched image-caption pair exhibits higher degree of alignment with base set in terms of CKA score.
- **Evidence anchors**: [section 4.2] provides formula "localCKA(zq, hq) = CKA(K[Z,zq], K[H,hq])"; [section 5.4] states "local CKA-based method exceeds CLIP's performance by over 17%."
- **Break condition**: If base set is not diverse enough or local CKA metric is not robust to noise, retrieval performance may degrade significantly.

## Foundational Learning

- **Concept: Centered Kernel Alignment (CKA)**
  - Why needed here: CKA is the core metric used to measure similarity between representation spaces of vision and language encoders.
  - Quick check question: How does CKA differ from Canonical Correlation Analysis (CCA) in measuring representation similarity?

- **Concept: Quadratic Assignment Problem (QAP)**
  - Why needed here: QAP solves the graph-matching problem for finding optimal permutation of captions that maximizes CKA.
  - Quick check question: Why is QAP considered NP-hard, and how does the seeded version help in practice?

- **Concept: Local CKA**
  - Why needed here: Local CKA extends global CKA metric to enable retrieval by computing CKA score between query pair and base set of aligned samples.
  - Quick check question: How does local CKA differ from global CKA, and why is it suitable for retrieval tasks?

## Architecture Onboarding

- **Component map**: Vision encoders (DINOv2, CLIP, ConvNeXt) -> Language encoders (All-Roberta-large-v1, paraphrase-multilingual-mpnet-base-v1) -> Base set of aligned image-caption pairs -> QAP optimization module -> Local CKA computation module

- **Critical path**: 1. Extract embeddings from vision and language encoders. 2. Compute CKA between embedding spaces to assess semantic similarity. 3. If CKA is sufficiently high, proceed to matching/retrieval. 4. Use QAP or Local CKA to match or retrieve captions for images.

- **Design tradeoffs**: Using CKA vs. other similarity metrics: CKA is more robust to invertible linear transformations but may be computationally expensive. QAP vs. other matching algorithms: QAP is NP-hard but can find global optima; heuristic methods may be faster but suboptimal. Local CKA vs. global CKA for retrieval: Local CKA is more granular but requires base set of aligned samples.

- **Failure signatures**: Low CKA scores: Indicates poor semantic alignment between encoders, possibly due to different training data or tasks. High QAP computation time: Suggests need for more efficient graph-matching algorithms or smaller base sets. Poor retrieval performance: May indicate noise in embeddings or insufficiently diverse base set.

- **First 3 experiments**: 1. Compute CKA between DINOv2 and All-Roberta-large-v1 on COCO captions to verify semantic similarity. 2. Implement QAP-based matching on small set of image-caption pairs and measure accuracy. 3. Compare Local CKA retrieval performance against relative representations on COCO captions.

## Open Questions the Paper Calls Out
- **Open Question 1**: What is the theoretical explanation for why well-trained vision encoders trained without language supervision (e.g., DINOv2) still exhibit high semantic similarity with language encoders comparable to aligned models like CLIP? Basis in paper: Explicit - The paper demonstrates that DINOv2 achieves comparable CKA and matching accuracy to CLIP despite lacking language supervision during training. Why unresolved: The paper observes this phenomenon but does not provide theoretical explanation for why self-supervised vision encoders can learn representations that align so well with language encoders' semantic spaces. What evidence would resolve it: Theoretical analysis showing how self-supervised objectives implicitly learn to capture semantic concepts that align with language-based representations, or empirical studies varying self-supervised objective to test impact on cross-modal alignment.

- **Open Question 2**: What is the impact of varying number of base samples on robustness of QAP matching and local CKA retrieval methods across different vision-language encoder combinations? Basis in paper: Explicit - The paper varies number of base samples in experiments and shows that accuracy improves with more base samples, but does not systematically study trade-off between robustness and computational cost. Why unresolved: While paper demonstrates importance of base samples, it doesn't explore how different base sample sizes affect methods' robustness to noise, domain shift, or encoder quality variations. What evidence would resolve it: Systematic ablation studies varying base sample size while testing on datasets with increasing levels of noise, domain shift, or using encoders of varying quality to determine optimal base sample size for robustness.

- **Open Question 3**: Can the proposed methods be extended to handle more than two modalities (e.g., vision, language, and audio) while maintaining zero-shot performance? Basis in paper: Inferred - The paper focuses on aligning vision and language encoders but doesn't explore extensions to multi-modal scenarios, though underlying principles (semantic similarity, CKA maximization) could theoretically apply. Why unresolved: The paper's methods are designed for two modalities, and extending them to three or more would require new formulations for CKA computation and matching across multiple embedding spaces. What evidence would resolve it: Experiments demonstrating successful zero-shot alignment of vision, language, and audio encoders using extended version of proposed methods, with performance comparisons to existing multi-modal alignment techniques.

## Limitations
- CKA analysis relies on assumption that high scores indicate meaningful semantic alignment, though sensitivity to embedding dimensions and kernel function choice could affect results.
- QAP optimization is computationally expensive and may not scale well to larger datasets.
- Base sample selection process using k-means clustering is crucial for performance but optimal number of clusters and sampling strategy are not thoroughly explored.
- Study focuses on specific encoder architectures and datasets, which may limit generalizability to other vision-language models or domains.

## Confidence
- **High Confidence**: Finding that unaligned encoders show comparable CKA scores to aligned models like CLIP is well-supported by empirical evidence across multiple encoder pairs and datasets.
- **Medium Confidence**: Effectiveness of QAP-based matching and local CKA retrieval methods is demonstrated, but performance could vary with different base sample selections and clustering strategies.
- **Medium Confidence**: Zero-shot capability claim is supported by strong performance metrics, but extent of this capability across diverse languages and domains requires further validation.

## Next Checks
1. **CKA Robustness Analysis**: Conduct sensitivity analysis of CKA scores to different kernel functions (linear, RBF) and embedding dimensions to ensure semantic similarity findings are not artifacts of specific choices.
2. **Base Sample Ablation Study**: Systematically vary number of base samples and clustering parameters (k in k-means) to determine optimal configuration for QAP matching and local CKA retrieval performance.
3. **Cross-Domain Generalization Test**: Evaluate proposed methods on out-of-domain image-caption pairs (e.g., from Conceptual Captions or niche domains) to assess robustness of semantic alignment and zero-shot matching capabilities.