---
ver: rpa2
title: 'Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional
  Adaptation'
arxiv_id: '2403.01467'
source_url: https://arxiv.org/abs/2403.01467
tags:
- graph
- adaptation
- domain
- node
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel source-free unsupervised graph domain
  adaptation method called GraphCTA that collaboratively performs model adaptation
  and graph adaptation. The key idea is to learn domain-shift invariant node representations
  via neighborhood-aware pseudo labeling and local-global consistency, while simultaneously
  refining the graph structure and node features using self-training with neighborhood
  contrastive learning.
---

# Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation

## Quick Facts
- arXiv ID: 2403.01467
- Source URL: https://arxiv.org/abs/2403.01467
- Authors: Zhen Zhang; Meihan Liu; Anhui Wang; Hongyang Chen; Zhao Li; Jiajun Bu; Bingsheng He
- Reference count: 40
- Key outcome: Proposed GraphCTA method significantly outperforms recent source-free baselines by an average of 2.14% in node classification accuracy, and even surpasses source-needed methods in some scenarios.

## Executive Summary
This paper addresses the challenge of source-free unsupervised graph domain adaptation (SF-GDA) for node classification. The authors propose GraphCTA, a novel framework that collaboratively performs model adaptation and graph adaptation in a bi-directional manner. By leveraging neighborhood-aware pseudo labeling and local-global consistency, GraphCTA learns domain-shift invariant node representations while simultaneously refining the graph structure and node features using self-training with neighborhood contrastive learning. Extensive experiments on multiple datasets demonstrate the effectiveness of GraphCTA in addressing domain shifts in graph-structured data.

## Method Summary
GraphCTA is a source-free graph domain adaptation method that addresses domain shifts in graph-structured data through collaborative bi-directional adaptation. The framework consists of two main components: model adaptation and graph adaptation. Model adaptation refines the source-pretrained GNN parameters using neighborhood-aware pseudo labeling weighted by global class prototypes, while graph adaptation refines node features and adjacency matrix via self-training with neighborhood contrastive learning. The method utilizes memory banks with momentum updating to stabilize prototype and prediction distributions across iterations, and optimizes a combination of weighted cross-entropy loss and InfoNCE contrastive loss to balance neighborhood prediction consistency and instance-prototype alignment.

## Key Results
- GraphCTA significantly outperforms recent source-free baselines by an average of 2.14% in node classification accuracy.
- The method even surpasses source-needed methods in some scenarios, demonstrating its effectiveness in addressing domain shifts without access to source data.
- GraphCTA is model-agnostic and effective in handling domain shifts in graph-structured data across multiple datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Collaborative bi-directional adaptation (model adaptation + graph adaptation) jointly addresses both source hypothesis bias and domain shift.
- Mechanism: Model adaptation refines the source-pretrained GNN parameters using neighborhood-aware pseudo labeling weighted by global class prototypes; graph adaptation refines node features and adjacency matrix via self-training with neighborhood contrastive learning.
- Core assumption: The source-pretrained model already produces reasonable semantic clusters in the target domain; neighborhood structures in the target graph contain reliable information for self-supervision.
- Evidence anchors:
  - [abstract] "learn domain-shift invariant node representations via neighborhood-aware pseudo labeling and local-global consistency, while simultaneously refining the graph structure and node features using self-training with neighborhood contrastive learning"
  - [section 3.2] "we propose to achieve model adaptation by encouraging neighborhood prediction consistency. The pseudo labels are generated by aggregating the predicted neighborhood class distributions"
  - [corpus] Weak: no explicit confirmation of the joint effectiveness from cited papers.
- Break condition: If neighborhood structures are too corrupted by domain shift, pseudo labels become unreliable and model adaptation degrades.

### Mechanism 2
- Claim: Memory banks with momentum updating stabilize prototype and prediction distributions across iterations.
- Mechanism: Two memory banks store node representations and their predictions; momentum updates (Î³=0.9) ensure consistent class prototypes and prediction distributions during training.
- Core assumption: Momentum updates prevent noisy fluctuations and allow consistent estimation of class centroids and prediction confidences.
- Evidence anchors:
  - [section 3.2] "Memory banks [16] are used to store all target representations and their corresponding predictions through momentum updating, which generates robust class prototypes and ensures consistent predictions during the training stage"
  - [section 3.2] "záµ¢áµ = (1 âˆ’ Î³)záµ¢áµ + Î³záµ¢, where Î³ is the momentum coefficient"
  - [corpus] Weak: memory banks are common in contrastive learning, but the specific use for graph domain adaptation is not confirmed by neighbors.
- Break condition: If momentum coefficient is too high, updates become too slow; too low, noisy fluctuations dominate.

### Mechanism 3
- Claim: Local-global consistency optimization balances neighborhood prediction consistency and instance-prototype alignment.
- Mechanism: Weighted cross-entropy loss between pseudo labels and predictions (LCE) plus InfoNCE contrastive loss (LCO) are combined to form the final model adaptation loss LM.
- Core assumption: Neighborhood consistency captures local structure, while instance-prototype alignment enforces global semantic coherence.
- Evidence anchors:
  - [section 3.2] "We fine-tune the model's parameters by optimizing the weighted cross-entropy loss between the pseudo label distribution and the predicted class distribution"
  - [section 3.2] "Additionally, we further consider instance-prototype alignment inspired by recent contrastive learning [4, 16, 42] to regularize the learned representations"
  - [corpus] Weak: contrastive learning is common, but its integration with neighborhood pseudo labeling in this context is not directly supported.
- Break condition: If Î» is mis-tuned, the balance between local and global consistency is lost, hurting adaptation.

## Foundational Learning

- Concept: GNN message passing and aggregation functions.
  - Why needed here: Model adaptation and graph adaptation both rely on propagating and refining node representations through neighborhood aggregation.
  - Quick check question: What is the role of the aggregation function Agg(Â·) in the update rule záµ¢Ë¡ = UpdateË¡(záµ¢Ë¡â»Â¹, AggË¡({záµ¤Ë¡â»Â¹ | u âˆˆ Ïˆ(Aáµ¢)}))?
- Concept: Pseudo-labeling and confidence weighting.
  - Why needed here: Neighborhood-aware pseudo labeling generates supervision signals in the absence of true labels, weighted by similarity to global prototypes to reduce noise.
  - Quick check question: How does the weight score sim(záµ¢, Î¼áµá¶œ) modify the contribution of each pseudo-labeled node in the loss?
- Concept: Neighborhood contrastive learning and positive/negative pair construction.
  - Why needed here: Graph adaptation uses contrastive learning to refine the graph by pulling together similar samples and pushing apart dissimilar ones based on memory bank representations.
  - Quick check question: What defines the positive and negative sample sets ðœ’áµ¢ and Î¨áµ¢ in the neighborhood contrastive loss?

## Architecture Onboarding

- Component map:
  - Source-pretrained GNN model (feature extractor fÎ¸ + classifier gÏ†) -> Memory banks (F for representations, P for predictions) -> Model adaptation module (neighborhood-aware pseudo labeling + local-global consistency loss) -> Graph adaptation module (node feature refinement Î”X + structure refinement Î”A + self-training with contrastive loss)
- Critical path:
  1. Initialize memory banks and graph transformations.
  2. For each epoch: update model parameters using LM while freezing Î”X, Î”A.
  3. Update Î”X using LG while freezing model parameters.
  4. Update Î”A using LG via projected gradient descent while freezing model parameters.
  5. Repeat until convergence.
- Design tradeoffs:
  - Graph adaptation via simple additive/masking vs. more complex structure learning (e.g., SLAPS, SUBLIME).
  - Neighborhood selection Ïˆ(Aáµ¢) and number of K-nearest neighbors in contrastive loss.
  - Momentum coefficient Î³ in memory banks and temperature Ï„ in contrastive loss.
- Failure signatures:
  - If pseudo labels are noisy (low confidence or incorrect neighborhoods), model adaptation degrades.
  - If Î”A modifications are too aggressive, graph structure loses meaningful topology.
  - If memory banks are not updated consistently, prototypes become unstable.
- First 3 experiments:
  1. Verify that neighborhood-aware pseudo labeling produces reasonable class assignments compared to vanilla pseudo labeling.
  2. Test the effect of momentum coefficient Î³ on memory bank stability and adaptation performance.
  3. Compare performance when using only model adaptation (LM) vs. only graph adaptation (LG) vs. both together.

## Open Questions the Paper Calls Out
- How does the proposed GraphCTA framework perform in scenarios with multi-source or open-set graph domain adaptation?
- What are the computational and memory requirements of GraphCTA compared to other source-free graph domain adaptation methods?
- How sensitive is GraphCTA to the choice of hyper-parameters, and are there any strategies for automatic hyper-parameter tuning?

## Limitations
- The effectiveness of the collaborative bi-directional adaptation relies heavily on the assumption that the source-pretrained model produces reasonable semantic clusters in the target domain, which is not explicitly validated.
- The choice of hyperparameters, particularly the trade-off parameters Î» in the loss functions and the momentum coefficient Î³ in memory banks, is not thoroughly explored or justified.
- The paper does not provide a detailed analysis of how the graph adaptation module affects the underlying graph structure, raising questions about potential information loss or distortion.

## Confidence
- High Confidence: The overall framework design and the integration of model adaptation and graph adaptation are well-motivated and logically sound.
- Medium Confidence: The experimental results demonstrate superior performance compared to recent source-free baselines, but the extent of improvement and its consistency across different scenarios require further validation.
- Low Confidence: The specific implementation details of the graph adaptation module, particularly the feature transformation and structure refinement functions, are not fully specified, making it difficult to reproduce the exact results.

## Next Checks
1. Conduct a thorough analysis of the target graph's neighborhood structures to verify the assumption that they contain reliable information for self-supervision. This could involve visualizing the graph topology and comparing it to the source domain.
2. Perform an extensive sensitivity analysis of the key hyperparameters (Î», Î³, Ï„, K) to understand their impact on the adaptation performance and identify optimal values.
3. Design an ablation study to isolate the effects of the graph adaptation module. Compare the performance of GraphCTA with and without graph adaptation to quantify its contribution to the overall improvement.