---
ver: rpa2
title: Whose Preferences? Differences in Fairness Preferences and Their Impact on
  the Fairness of AI Utilizing Human Feedback
arxiv_id: '2406.05902'
source_url: https://arxiv.org/abs/2406.05902
tags:
- sentence
- data
- demographic
- pairs
- annotators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates differences in fairness preferences among
  annotators based on demographic factors (race, age, political stance, education,
  LGBTQ+ identity) and their impact on fairness of AI systems trained on human feedback.
  Using a novel dataset of 1500 sentence pairs collected from MTurk and Prolific,
  significant differences in fairness judgments were found depending on annotator
  demographics and the sensitive attributes referenced in sentences.
---

# Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback

## Quick Facts
- **arXiv ID**: 2406.05902
- **Source URL**: https://arxiv.org/abs/2406.05902
- **Reference count**: 40
- **Key outcome**: Significant differences in fairness preferences among annotators based on demographics (race, age, political stance, education, LGBTQ+ identity), affecting AI system performance trained on human feedback

## Executive Summary
This paper investigates how demographic differences in annotators' fairness preferences impact AI systems trained on human feedback. The authors collect a novel dataset of 1500 sentence pairs annotated by crowdworkers, examining how race, age, political stance, education, and LGBTQ+ identity affect fairness judgments. They find significant gaps in fairness preferences across demographics and demonstrate that these differences propagate to downstream classifiers trained to predict human preferences. An ensemble classifier approach, giving equal weight to models trained on different demographics, outperforms single classifiers on various demographic intersections.

## Method Summary
The authors collected 1500 sentence pairs from MTurk and Prolific, annotated by 1000 crowdworkers who provided demographic information and rated both personal opinions and predictions of average American opinions on fairness. They binarized the 4-category labels to "unfair" (categories 0-2) and "not unfair" (category 3), then trained logistic regression models to analyze demographic effects on fairness judgments. For downstream modeling, they used BERT-based classifiers trained on annotations from different demographic groups, evaluating performance using balanced accuracy. Finally, they implemented an ensemble classifier that aggregates predictions from models trained on different demographics to improve performance across demographic intersections.

## Key Results
- Logistic regression revealed significant effects of age, politics, and education on annotators' personal fairness judgments, and all demographics except gender on predicted average American opinions
- Downstream models trained on different demographic groups' annotations performed differently on test data from various demographics, with no clear trend towards models trained on a specific demographic performing best on that demographic
- An ensemble classifier approach, giving equal weight to classifiers trained on different demographics, significantly improved performance for 9 out of 24 demographic intersections compared to a single classifier

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demographic identities of annotators significantly influence their fairness preferences.
- Mechanism: Logistic regression models show that variables like age, politics, education, and LGBTQ+ identity have significant effects on how annotators judge fairness in sentence pairs.
- Core assumption: Annotator demographics are good predictors of individual fairness judgments.
- Evidence anchors:
  - [abstract] "We find significant gaps in fairness preferences depending on the race, age, political stance, educational level, and LGBTQ+ identity of annotators."
  - [section 4] Logistic regression results in Table 1 show significant odds ratios for age, politics, education, and LGBTQ+ identity.
- Break condition: If demographic variables are not good predictors of fairness judgments, this mechanism would not hold.

### Mechanism 2
- Claim: Differences in annotator fairness preferences affect the performance of downstream models trained to predict these preferences.
- Mechanism: Models trained on annotations from different demographic groups perform differently when evaluated on test data from various demographics, with no clear trend towards models trained on a specific demographic performing best on that demographic.
- Core assumption: Models can learn and replicate the fairness preferences of the annotators they are trained on.
- Evidence anchors:
  - [abstract] "Further, we find that differences also exist in downstream classifiers trained to predict human preferences."
  - [section 5] Table 3 shows varying performance of models trained on different demographic groups.
- Break condition: If models do not learn or replicate the fairness preferences of their training data, this mechanism would not hold.

### Mechanism 3
- Claim: An ensemble classifier approach, giving equal weight to classifiers trained on different demographics, performs better for different demographic intersections compared to a single classifier.
- Mechanism: The ensemble approach reduces the impact of skewness in the number of labels provided by different groups, providing more representation to marginalized groups.
- Core assumption: Aggregating predictions from models trained on diverse demographics improves performance across demographic intersections.
- Evidence anchors:
  - [abstract] "Finally, we observe that an ensemble, giving equal weight to classifiers trained on annotations from different demographics, performs better for different demographic intersections; compared to a single classifier that gives equal weight to each annotation."
  - [section 6] Table 4 shows the ensemble classifier providing significantly better results for 9 out of 24 demographic intersections.
- Break condition: If the ensemble approach does not improve performance across demographic intersections, this mechanism would not hold.

## Foundational Learning

- Concept: Logistic regression for predicting categorical outcomes
  - Why needed here: To analyze the effect of annotator demographics on their fairness judgments
  - Quick check question: What type of regression model is used to predict fairness judgments based on annotator demographics?

- Concept: Balanced accuracy as a performance metric
  - Why needed here: To evaluate the performance of classifiers trained to predict human fairness preferences, especially in imbalanced datasets
  - Quick check question: Why is balanced accuracy used instead of standard accuracy in this study?

- Concept: Ensemble learning for combining multiple models
  - Why needed here: To improve performance across demographic intersections by aggregating predictions from models trained on different demographics
  - Quick check question: How does the ensemble approach in this study differ from a simple majority vote of all annotations?

## Architecture Onboarding

- Component map: Data collection module (MTurk and Prolific) -> Preprocessing and filtering module -> Logistic regression analysis module -> Neural classifier training module (BERT-based) -> Ensemble classifier module -> Evaluation and analysis module
- Critical path: Data collection → Preprocessing → Logistic regression analysis → Neural classifier training → Ensemble classifier → Evaluation
- Design tradeoffs:
  - Using binarized labels vs. original 4-category labels for analysis
  - Focusing on annotators' predictions of average American opinion vs. their personal opinions due to data availability
  - Supplementing new dataset with data from prior work to improve classifier performance
- Failure signatures:
  - Low variability in annotator responses (low unalikeability coefficients)
  - Classifier performance not significantly better than random baseline
  - Lack of significant differences in fairness preferences across demographics
- First 3 experiments:
  1. Train logistic regression models to predict annotators' fairness judgments based on their demographics
  2. Train neural classifiers (BERT-based) on annotations from different demographic groups and evaluate their performance
  3. Implement ensemble classifier approach and compare its performance to single classifier on demographic intersections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors cause classifiers to perform better on test data from specific demographic groups, regardless of training data demographics?
- Basis in paper: [inferred] from observation that models perform better when evaluated on data from certain groups like age 38+ or White people, but no clear directional effect from training demographics
- Why unresolved: The paper notes this is an interesting observation but doesn't provide a definitive explanation. Possible contributing factors like dataset sizes and levels of disagreement within groups are mentioned but not thoroughly investigated.
- What evidence would resolve it: A detailed analysis examining the relationship between model performance and factors like dataset size, label distribution, and disagreement levels within each demographic group. This could include statistical tests to determine which factors have the strongest correlation with performance.

### Open Question 2
- Question: How would using different terminology than "average American" in the survey affect the complexity of the survey, participants' experience, and the results?
- Basis in paper: [explicit] The paper acknowledges this limitation and notes they haven't explored how a different term could affect these aspects
- Why unresolved: The authors chose "average American" for simplicity but recognize it may not be the most precise term. They haven't tested alternative phrasings or measured their impact.
- What evidence would resolve it: Running the same survey with different terms (e.g., "other US-based annotators with similar qualifications") and comparing completion rates, response quality, and results across versions.

### Open Question 3
- Question: What would be the effect on downstream models if they were trained to predict annotators' personal fairness judgments rather than their perception of the average American's judgments?
- Basis in paper: [explicit] The authors note they had to focus on average American predictions due to data availability, but state it would be interesting to investigate effects on downstream models trained to respect learnt fairness judgments
- Why unresolved: The dataset used for training downstream models (Do+n) only contains average American predictions, not personal opinions, limiting the scope of the analysis
- What evidence would resolve it: Training and evaluating downstream models using both personal opinions and average American predictions, then comparing their performance and fairness properties across different demographic groups.

## Limitations

- The study relies on self-reported demographic information and subjective fairness judgments from crowdworkers, which may not fully capture the complexity of fairness perceptions across different groups
- The use of binarized labels (unfair/not unfair) simplifies the original 4-category responses, potentially losing nuanced information about fairness judgments
- The ensemble approach's improvement in performance may be partially attributed to the specific demographic composition of the dataset rather than being universally generalizable

## Confidence

- **High Confidence**: The existence of significant differences in fairness preferences across demographics is well-supported by the logistic regression analysis and consistent with prior research on demographic differences in moral judgments
- **Medium Confidence**: The performance improvements of ensemble classifiers over single classifiers are demonstrated but may be dataset-specific and require further validation on different datasets
- **Low Confidence**: The generalizability of findings to other fairness-related tasks or different cultural contexts remains uncertain given the US-centric dataset and limited exploration of cross-cultural differences

## Next Checks

1. **Cross-dataset validation**: Evaluate the ensemble classifier approach on a separate dataset with different sentence pairs and annotators to assess generalizability
2. **Sensitivity analysis**: Test how robust the findings are to different label binarization thresholds and whether using the full 4-category labels changes the observed patterns
3. **Cultural extension**: Replicate the study with annotators from different countries to examine whether demographic differences in fairness preferences are universal or culturally specific