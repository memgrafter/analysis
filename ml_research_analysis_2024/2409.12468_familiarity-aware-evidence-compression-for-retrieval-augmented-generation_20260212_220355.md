---
ver: rpa2
title: Familiarity-Aware Evidence Compression for Retrieval-Augmented Generation
arxiv_id: '2409.12468'
source_url: https://arxiv.org/abs/2409.12468
tags:
- evidence
- compression
- favicomp
- knowledge
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses a key limitation in retrieval-augmented generation
  (RAG) where compressed evidence may be unfamiliar to the target model, leading to
  suboptimal performance. The authors propose FAVICOMP, a training-free inference-time
  evidence compression technique that improves downstream performance by lowering
  the target model's perplexity during compression.
---

# Familiarity-Aware Evidence Compression for Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2409.12468
- Source URL: https://arxiv.org/abs/2409.12468
- Authors: Dongwon Jung; Qin Liu; Tenghao Huang; Ben Zhou; Muhao Chen
- Reference count: 20
- Primary result: FAVICOMP improves accuracy by up to 28.1% on open-domain QA while achieving high compression rates

## Executive Summary
FAVICOMP addresses a key limitation in retrieval-augmented generation (RAG) where compressed evidence may be unfamiliar to the target model, leading to suboptimal performance. The authors propose a training-free inference-time evidence compression technique that improves downstream performance by lowering the target model's perplexity during compression. FAVICOMP uses ensemble decoding to combine token probabilities from both the compression and target models, making the compressed evidence more familiar while integrating parametric knowledge from the target model.

## Method Summary
FAVICOMP is a training-free evidence compression technique for RAG systems that uses ensemble decoding to combine token probabilities from both a compression model and the target model. During compression, the method selects tokens by maximizing a weighted sum of log probabilities from both models, with an ensemble coefficient α typically set to 0.5. This approach constrains the compression model's token search space to tokens with lower perplexity for the target model, making the compressed evidence more familiar while potentially integrating the target model's parametric knowledge. The method is inference-time only and does not require training on target models.

## Key Results
- FAVICOMP consistently outperforms recent evidence compression baselines across five open-domain QA datasets
- Improves accuracy by up to 28.1% while maintaining high compression rates
- Uses Llama3-8B-Instruct and Mixtral-8x7B-Instruct as target models with Llama3.2-3B-Instruct and Mistral-7B-Instruct as compression models
- Evaluated on Natural Questions, TriviaQA, HotpotQA, 2WikiMultiHopQA, and MuSiQue datasets

## Why This Works (Mechanism)

### Mechanism 1
FAVICOMP lowers the target model's perplexity by constraining the compression model's token search space to tokens with lower perplexity for the target model. At each decoding step, FAVICOMP selects tokens by maximizing a weighted sum of log probabilities from both models, upweighting tokens that are more familiar to the target model.

### Mechanism 2
The method integrates parametric knowledge from the target model by selecting arg max tokens from the target model when they have higher probability than compression model tokens. This incorporates the target model's knowledge into the compressed evidence during ensemble decoding.

### Mechanism 3
FAVICOMP handles cases where retrieved evidence is incomplete by supplementing missing information with parametric knowledge from the target model. When evidence sets lack complete information, the target model generates missing pieces during ensemble decoding, filling knowledge gaps.

## Foundational Learning

- Concept: Perplexity as familiarity metric
  - Why needed here: The paper uses perplexity to quantify how familiar the target model is with the compressed evidence, guiding the ensemble decoding process.
  - Quick check question: If a compressed context has lower perplexity for a target model compared to another context, what does this indicate about the model's familiarity with each context?

- Concept: Ensemble decoding in language models
  - Why needed here: FAVICOMP uses ensemble decoding to combine probabilities from compression and target models, which is the core technical innovation.
  - Quick check question: In ensemble decoding, if we set the ensemble coefficient α to 0.5, what proportion of influence does each model have on the final token selection?

- Concept: Retrieval-augmented generation (RAG) pipeline
  - Why needed here: Understanding how FAVICOMP fits into the broader RAG framework is crucial for proper implementation and evaluation.
  - Quick check question: In a standard RAG pipeline, what are the two main components that work together to generate responses?

## Architecture Onboarding

- Component map: Retrieval system -> Compression model -> Target model -> FAVICOMP orchestrator -> Prompt templates
- Critical path: 1) Retrieve documents for question, 2) Run evidence compression with ensemble decoding, 3) Generate context using target model, 4) Combine probabilities from both models, 5) Select tokens to form compressed evidence, 6) Use compressed evidence for downstream task
- Design tradeoffs: Higher α values prioritize target model knowledge but may lose evidential content; lower α values preserve more evidential content but may be less familiar to target model; ensemble decoding doubles computation cost compared to single-model compression
- Failure signatures: Performance degradation with α=0.5 suggests perplexity may not correlate with downstream performance; low compression rates indicate ensemble decoding may introduce unnecessary tokens; accuracy drops on evidence-relevant subsets suggest target model knowledge conflicts with retrieved evidence
- First 3 experiments: 1) Run FAVICOMP with α=0.5 on a small subset and measure both perplexity and accuracy, 2) Compare FAVICOMP output with Zero-shot Summarization (α=0) and Generated Context (α=1) on same inputs, 3) Test FAVICOMP on both evidence-relevant and evidence-irrelevant subsets to verify knowledge integration

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal ensemble coefficient α for FAVICOMP across different dataset characteristics (e.g., multi-hop vs. single-hop questions)? While the paper tests α values showing α=0.5 generally performs best, it doesn't comprehensively analyze how dataset characteristics affect the optimal α or explore the relationship between dataset properties and optimal α.

### Open Question 2
How does FAVICOMP's performance compare to evidence compression methods that use parametric knowledge from the target model during training rather than just inference? The paper emphasizes FAVICOMP is training-free but doesn't explore whether incorporating target model knowledge during training could yield better results or compare against methods that train compression models with target model knowledge.

### Open Question 3
How does FAVICOMP scale to larger target models (e.g., GPT-4, Claude) and what are the computational trade-offs? The experiments only test FAVICOMP with relatively small models and don't explore performance scaling with larger models or analyze computational trade-offs like latency, memory usage, and cost per query at larger scales.

## Limitations
- Reliance on perplexity as a proxy for familiarity without proving correlation with actual performance
- Computational overhead from ensemble decoding that doubles inference time compared to single-model approaches
- No comprehensive analysis of how optimal ensemble coefficient varies across different dataset characteristics

## Confidence

**High Confidence Claims:**
- FAVICOMP is an inference-time, training-free technique for evidence compression
- The method uses ensemble decoding to combine token probabilities from compression and target models
- FAVICOMP outperforms baselines on the five tested QA datasets

**Medium Confidence Claims:**
- Lower perplexity correlates with improved downstream performance
- The target model's parametric knowledge meaningfully supplements incomplete retrieved evidence
- α=0.5 is the optimal ensemble coefficient across all datasets and tasks

**Low Confidence Claims:**
- FAVICOMP's improvements would generalize to non-QA tasks
- The mechanism handles all types of knowledge gaps in retrieved evidence
- The method scales effectively to much larger document collections

## Next Checks

1. **Perplexity-Performance Correlation Test**: Run controlled experiments varying the ensemble coefficient α while measuring both perplexity and downstream accuracy to establish whether perplexity is indeed a reliable proxy for familiarity. Test on at least two different target models to verify generalizability.

2. **Knowledge Conflict Detection**: Design experiments specifically targeting cases where the target model's parametric knowledge contradicts retrieved evidence. Measure hallucination rates and contradictory outputs to quantify the risks of knowledge integration, particularly for fact-based questions.

3. **Computational Efficiency Benchmark**: Compare FAVICOMP's inference time and resource usage against baseline methods across different document set sizes (e.g., 5, 10, 20 documents). Establish whether the double-model computation creates practical bottlenecks for real-world deployment.