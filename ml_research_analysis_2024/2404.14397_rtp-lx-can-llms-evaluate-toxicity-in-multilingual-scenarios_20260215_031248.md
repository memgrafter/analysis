---
ver: rpa2
title: 'RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?'
arxiv_id: '2404.14397'
source_url: https://arxiv.org/abs/2404.14397
tags:
- language
- llms
- content
- rtp-lx
- toxicity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) and small language models (SLMs) face
  challenges in evaluating toxicity in multilingual scenarios. To address this, the
  researchers created RTP-LX, a human-transcreated and human-annotated corpus of toxic
  prompts and outputs in 28 languages.
---

# RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?

## Quick Facts
- **arXiv ID:** 2404.14397
- **Source URL:** https://arxiv.org/abs/2404.14397
- **Reference count:** 23
- **Key outcome:** LLMs and SLMs struggle to evaluate toxicity in multilingual contexts, showing low agreement with human judges on holistic toxicity scoring and difficulty detecting subtle harm like microaggressions and bias.

## Executive Summary
This paper addresses the critical challenge of evaluating toxicity in multilingual contexts by introducing RTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and outputs in 28 languages. The researchers evaluated seven S/LLMs (GPT-4, Llama-2 variants, Llama Guard, Gemma variants, and Mistral) on their ability to detect toxic content. While the models achieved high accuracy scores, they showed low agreement with human judges when judging holistically and struggled particularly with subtle-yet-harmful content such as microaggressions, bias, and identity attacks. The study highlights the limitations of current models in culturally diverse contexts and provides a dataset to support further research in reducing harmful uses of these models.

## Method Summary
The researchers created RTP-LX by professionally transcreating the seed RTP corpus into 28 languages while maintaining cultural equivalence and removing US-centric references. They annotated the corpus for various harm categories including toxicity, bias, identity attack, microaggression, self-harm, sexual content, and violence. Seven S/LLMs were evaluated using a modified annotation rubric as prompts, with their outputs compared against human annotations using accuracy and weighted Cohen's kappa (κw) metrics. The evaluation considered both exact label matches and agreement with human judgments, with particular attention to the models' ability to detect subtle forms of toxicity.

## Key Results
- S/LLMs achieved high accuracy scores but showed low agreement with human judges on holistic toxicity scoring
- Models struggled to detect subtle-yet-harmful content like microaggressions, bias, and identity attacks
- The models tended to output binary labels (no harm vs explicit harm) and preferred higher-valued labels, leading to false positives
- Human-transcreation improved cultural relevance but did not fully resolve the models' difficulties with nuanced toxicity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models struggle to detect subtle forms of toxicity such as microaggressions and bias in multilingual contexts.
- Mechanism: The models tend to output binary labels (no harm or explicit harm) and overlook contextually harmful sentences, especially in nuanced cultural scenarios.
- Core assumption: The training data for LLMs is primarily in English and lacks sufficient representation of diverse cultural contexts.
- Evidence anchors:
  - [abstract] "they have low agreement with human judges when scoring holistically the toxicity of a prompt; and have difficulty discerning harm in context-dependent scenarios, particularly with subtle-yet-harmful content (e.g. microaggressions, bias)."
  - [section] "the models failed to correctly classify typically subtle-yet-harmful discourse such as microaggressions, bias, and identity attack."
- Break condition: If the models are fine-tuned with culturally diverse and context-rich datasets, their ability to detect subtle toxicity may improve.

### Mechanism 2
- Claim: Human-transcreated datasets improve cultural relevance and reduce data contamination in multilingual toxicity evaluation.
- Mechanism: Professional translators adapt prompts to preserve original intent while removing US-centric cultural references, ensuring the dataset reflects local sensitivities.
- Core assumption: Translators can accurately convey cultural nuances and offensive content across languages.
- Evidence anchors:
  - [section] "To address this and maintain a trustworthy dataset with equivalence across locales, the seed RTP corpus was professionally transcreated into the languages supported by RTP-LX."
  - [section] "The translators were encouraged to try out multiple dialects if they were familiar with them, and specify them in the corpus."
- Break condition: If translators fail to capture cultural nuances or if the transcreation process introduces significant noise, the dataset's effectiveness may diminish.

### Mechanism 3
- Claim: Large language models have low agreement with human judges on holistic toxicity scoring, leading to potential over- or underestimation of harmful content.
- Mechanism: Models prefer to output higher-valued labels, which in RTP-LX means "extreme harm," resulting in false positives and disagreements with human annotators.
- Core assumption: The models' training data and evaluation metrics are not aligned with human perceptions of toxicity.
- Evidence anchors:
  - [abstract] "they have low agreement with human judges when judging holistically the toxicity of a prompt."
  - [section] "When looking at the class breakdown we found that the models tended to not agree with human judgments."
- Break condition: If the models are fine-tuned with human-annotated datasets and evaluated using metrics that align with human perceptions, their agreement with human judges may improve.

## Foundational Learning

- Concept: Cultural sensitivity in language processing
  - Why needed here: Understanding cultural nuances is crucial for detecting subtle forms of toxicity across different languages and regions.
  - Quick check question: How can cultural context influence the perception of toxic language in multilingual scenarios?

- Concept: Data transcreation and localization
  - Why needed here: Adapting content to different languages and cultures is essential for creating effective multilingual toxicity datasets.
  - Quick check question: What are the key considerations when transcreating toxic language datasets to ensure cultural relevance?

- Concept: Inter-annotator agreement and evaluation metrics
  - Why needed here: Measuring the agreement between human judges and models is critical for assessing the effectiveness of toxicity detection systems.
  - Quick check question: How do different evaluation metrics, such as accuracy and Cohen's kappa, impact the assessment of multilingual toxicity detection models?

## Architecture Onboarding

- Component map: RTP-LX dataset → S/LLM models (GPT-4, Llama-2, Gemma, Mistral, Llama Guard) → Evaluation metrics (accuracy, Cohen's kappa) → Human annotators
- Critical path: Data transcreation → Annotation → Model evaluation → Analysis of results
- Design tradeoffs: Balancing cultural relevance with data contamination, choosing between different model architectures, and selecting appropriate evaluation metrics
- Failure signatures: Low inter-annotator agreement, binary labeling by models, and poor detection of subtle toxicity
- First 3 experiments:
  1. Evaluate the models on the transcreated subset to assess their performance on culturally adapted prompts.
  2. Test the models on the manual subset to measure their ability to detect culturally specific toxic language.
  3. Analyze the models' label distributions to identify patterns in their misclassification of subtle toxicity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do S/LLMs show different performance patterns when evaluating toxic content in languages outside the Indo-European family compared to Indo-European languages?
- Basis in paper: [explicit] The paper notes that RTP-LX primarily covers Indo-European languages and mentions this as a limitation, suggesting further work should expand to other language families.
- Why unresolved: The current corpus composition limits the ability to test this hypothesis across diverse linguistic structures.
- What evidence would resolve it: Testing the same S/LLMs on an expanded corpus containing languages from non-Indo-European families like Turkic, Sino-Tibetan, and Niger-Congo, then comparing performance metrics across language families.

### Open Question 2
- Question: Does the tendency of S/LLMs to output binary labels (no presence vs explicit presence of harm) persist when models are fine-tuned on culturally-specific toxic language datasets?
- Basis in paper: [inferred] The paper identifies this binary-label pathology in S/LLMs and suggests fine-tuning could improve performance, but does not test this directly.
- Why unresolved: The paper only evaluates unfinetuned S/LLMs and observes this pathology without testing whether fine-tuning mitigates it.
- What evidence would resolve it: Fine-tuning S/LLMs on culturally-specific toxic language datasets like RTP-LX and measuring whether the binary-label tendency decreases and IAA with human judges improves.

### Open Question 3
- Question: How does the performance of S/LLMs as toxicity evaluators compare to human annotators when evaluating culturally-specific toxic content that is not explicitly harmful?
- Basis in paper: [explicit] The paper finds that S/LLMs struggle with subtle-yet-harmful content like microaggressions and bias, particularly in culturally-sensitive scenarios.
- Why unresolved: While the paper identifies this weakness, it doesn't directly compare S/LLM performance to human annotators on these specific types of content.
- What evidence would resolve it: Conducting a direct comparison study where both S/LLMs and human annotators evaluate the same set of culturally-specific toxic content, then analyzing agreement rates and error patterns.

## Limitations
- The study covers only 28 languages, primarily Indo-European, limiting generalizability to other language families
- Reliance on human transcreation introduces potential subjectivity and cultural bias
- Focus on Western-centric toxic categories may not capture all culturally-specific forms of harm
- API-based model evaluation raises questions about reproducibility and potential hidden prompt engineering

## Confidence
**High Confidence**: The finding that LLMs show low agreement with human judges on holistic toxicity scoring is well-supported by quantitative metrics and systematic evaluation across multiple models.

**Medium Confidence**: The claim about models struggling with subtle toxicity forms like microaggressions has strong empirical support but may be influenced by the specific annotation scheme used.

**Low Confidence**: The assertion that training data primarily in English is the root cause of detection failures is speculative without direct analysis of model training data.

## Next Checks
1. **Cross-cultural validation study**: Conduct a blind evaluation where native speakers from different cultural backgrounds independently assess the same toxic prompts to measure the degree of cultural variation in toxicity perception.

2. **Fine-tuning experiment**: Take one of the struggling models (e.g., Llama Guard) and fine-tune it on a culturally diverse subset of RTP-LX, then re-evaluate its performance on both the original and transcreated subsets.

3. **Prompt engineering ablation**: Systematically test different prompt formulations with the API-based models (GPT-4, Llama-2) to determine whether the observed low agreement with human judges is due to the models' inherent limitations or could be improved through better prompting strategies.