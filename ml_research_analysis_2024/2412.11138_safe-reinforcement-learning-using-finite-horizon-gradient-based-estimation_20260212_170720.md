---
ver: rpa2
title: Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation
arxiv_id: '2412.11138'
source_url: https://arxiv.org/abs/2412.11138
tags:
- constraint
- safe
- estimation
- function
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Gradient-based Estimation (GBE), a method
  to estimate constraint changes in finite-horizon non-discounted reinforcement learning
  problems. Unlike existing methods that rely on infinite-horizon assumptions, GBE
  uses analytic gradients along trajectories for precise constraint prediction.
---

# Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation

## Quick Facts
- arXiv ID: 2412.11138
- Source URL: https://arxiv.org/abs/2412.11138
- Reference count: 40
- Primary result: Introduces Gradient-based Estimation (GBE) for accurate constraint prediction in finite-horizon RL, demonstrating superior safety performance compared to baseline algorithms.

## Executive Summary
This paper addresses the challenge of safe reinforcement learning in finite-horizon non-discounted settings, where traditional constraint estimation methods fail due to infinite-horizon assumptions. The authors introduce Gradient-based Estimation (GBE), which computes analytic gradients along trajectories for precise constraint prediction, and Constrained Gradient-based Policy Optimization (CGPO), a novel algorithm that iteratively solves surrogate problems within trust regions. Through experiments on differentiable environments and extensions to non-differentiable tasks using world models, CGPO demonstrates better constraint satisfaction and sample efficiency compared to baseline methods like PDO, APPO, CPO, CUP, BPTT-Lag, and SHAC-Lag.

## Method Summary
CGPO uses GBE to estimate constraint changes in finite-horizon settings by computing analytic gradients along trajectories rather than relying on discounted advantage functions. The algorithm iteratively solves a constrained surrogate problem within a trust region, using three update strategies depending on feasibility: steepest ascent for feasible regions, steepest descent for infeasible regions, or a convex QP with KKT conditions for partially feasible regions. The trust region radius is adaptively adjusted based on performance and constraint consistency. For non-differentiable environments, the method extends to MB-CGPO using a world model to provide gradients.

## Key Results
- GBE provides more accurate constraint estimation than existing methods in finite-horizon non-discounted settings
- CGPO achieves better constraint satisfaction while maintaining competitive performance compared to baseline algorithms
- The adaptive trust region mechanism improves both performance and constraint satisfaction across multiple environments
- MB-CGPO successfully extends the approach to non-differentiable tasks using world models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GBE avoids catastrophic estimation errors that ABE makes in finite-horizon non-discounted scenarios.
- Mechanism: GBE directly computes analytic gradients of cumulative costs along trajectories instead of relying on discounted advantage functions. This matches the actual finite-horizon structure of the constraints.
- Core assumption: The system dynamics are differentiable or can be approximated by a differentiable world model.
- Evidence anchors:
  - [abstract] "Unlike existing methods that rely on infinite-horizon assumptions, GBE uses analytic gradients along trajectories for precise constraint prediction."
  - [section 4.1] Derivation of GBE estimates via Taylor expansion and gradient back-propagation.
  - [corpus] Weak - neighboring papers focus on constraint satisfaction but don't directly validate GBE vs ABE in finite-horizon settings.
- Break condition: If the environment is non-differentiable and no accurate world model can be trained, gradient computation fails.

### Mechanism 2
- Claim: Trust region enforcement bounds estimation errors to maintain feasibility.
- Mechanism: Policy updates are constrained within a radius $\hat{\delta}$ such that the first-order Taylor approximation error (proportional to $\|\delta\|^2$) stays below a tolerable threshold. The trust region is adaptively resized based on performance/constraint consistency.
- Core assumption: Higher-order terms in the Taylor expansion are negligible for small $\|\delta\|$.
- Evidence anchors:
  - [section 4.1] Lemma 4.1 bounding estimation error by $\frac{1}{2}\epsilon\|\delta\|^2$.
  - [section 5.2] Theorem 5.3 showing worst-case bounds on performance drop and constraint violation.
  - [corpus] Weak - no explicit trust-region analysis in neighboring papers.
- Break condition: If the system is highly non-linear, higher-order terms dominate even for small updates, breaking the first-order approximation.

### Mechanism 3
- Claim: The constrained surrogate problem formulation ensures each update is both performance-improving and constraint-satisfying.
- Mechanism: The optimization solves $\max_\delta g_k^\top\delta$ subject to $c_k + q_k^\top\delta \leq 0$ and $\|\delta\|^2 \leq \hat{\delta}$. Depending on feasibility, it either follows the steepest ascent of the objective, steepest descent of the constraint, or solves a convex QP with KKT conditions.
- Core assumption: Slater's condition holds (existence of strictly feasible points), guaranteeing strong duality and convex solution.
- Evidence anchors:
  - [section 5.1] Three-case update strategy and dual variable computation via Algorithm 2.
  - [section 5.1] "The sub-problem (10) is characterized as convex. When the trust region is partially feasible, the existence of at least one strictly feasible point is guaranteed."
  - [corpus] Weak - neighboring papers don't detail this specific KKT-based solution method.
- Break condition: If the trust region is entirely infeasible or the dual problem becomes ill-conditioned, updates default to constraint descent, potentially slowing learning.

## Foundational Learning

- Concept: Markov Decision Process (MDP) and Constrained MDP (CMDP)
  - Why needed here: CGPO operates within CMDP framework where policies must satisfy cumulative cost constraints while maximizing return.
  - Quick check question: In a CMDP, how does the feasible policy set $\Pi_C$ differ from the unconstrained policy set $\Pi$?

- Concept: First-order Taylor expansion and error bounds
  - Why needed here: GBE relies on linear approximation of objective/constraint functions; error bounds justify trust region sizing.
  - Quick check question: If $\epsilon = \max_t \|\nabla^2 J_f(\theta_0 + t\delta)\|$, what is the upper bound on the estimation error for update $\delta$?

- Concept: KKT conditions and strong duality in convex optimization
  - Why needed here: The surrogate sub-problem is convex; solving it requires finding dual variables satisfying KKT.
  - Quick check question: Under what condition does Slater's theorem guarantee strong duality for the constrained QP in CGPO?

## Architecture Onboarding

- Component map:
  - Policy network $\pi_\theta$ -> Reward critic $V_R^\phi$ and cost critic $V_C^\psi$ -> Differentiable simulator or world model $F$ -> Trust region manager -> Dual variable solver

- Critical path:
  1. Sample trajectories from current policy
  2. Compute gradients $g_k, q_k$ via BPTT/SHAC through differentiable dynamics
  3. Solve constrained QP (trust region + constraint) to get $\delta$
  4. Update policy parameters $\theta_{k+1} = \theta_k + \delta$
  5. Update critics via finite-horizon TD($\lambda$)
  6. Adapt trust region radius

- Design tradeoffs:
  - Accuracy vs. differentiability: GBE needs differentiable systems; world models can approximate but add training overhead.
  - Trust region size: Larger $\hat{\delta}$ speeds learning but risks estimation error; smaller $\hat{\delta}$ is safer but slower.
  - Critic horizon: Short horizons (SHAC) reduce variance but may under-estimate long-term effects.

- Failure signatures:
  - Persistent constraint violations → trust region too large or gradient estimation poor
  - Oscillating performance → dual variables not converging or critic under-fitting
  - Very slow learning → trust region shrinking too aggressively or world model inaccurate

- First 3 experiments:
  1. Run CGPO on the provided differentiable FunctionEnv and compare ABE vs GBE error curves.
  2. Test adaptive vs fixed trust region on CartPole to see constraint satisfaction vs sample efficiency.
  3. Apply MB-CGPO to the non-differentiable Swimmer and measure performance degradation from world model error.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CGPO's performance scale with increasingly complex non-differentiable environments?
- Basis in paper: [inferred] The paper discusses using world models to provide gradients in non-differentiable tasks, but acknowledges this introduces additional computational overhead and may not provide as precise gradients as differentiable simulators.
- Why unresolved: The paper only tests CGPO with a simple world model (two-layer MLP) on two non-differentiable tasks. It's unclear how well CGPO would perform with more complex environments or more sophisticated world models.
- What evidence would resolve it: Experiments comparing CGPO's performance on various non-differentiable tasks with different world model architectures and training methods, measuring both policy performance and computational efficiency.

### Open Question 2
- Question: What is the theoretical relationship between the adaptive trust region radius and the convergence rate of CGPO?
- Basis in paper: [explicit] The paper introduces an adaptive trust region radius method to mitigate estimation errors, but only provides empirical evidence of its effectiveness in improving performance and constraint satisfaction.
- Why unresolved: While the paper demonstrates the practical benefits of the adaptive method, it doesn't provide a theoretical analysis of how the trust region radius affects the convergence rate or stability of the algorithm.
- What evidence would resolve it: A formal proof or rigorous empirical study demonstrating how the adaptive trust region radius influences the convergence rate, sample efficiency, and overall stability of CGPO compared to fixed-radius methods.

### Open Question 3
- Question: How does CGPO's performance compare to other methods that combine differentiable RL with Lagrangian approaches?
- Basis in paper: [explicit] The paper compares CGPO to BPTT-Lag and SHAC-Lag, which combine differentiable RL methods with Lagrangian approaches, and shows CGPO has better performance and constraint satisfaction.
- Why unresolved: While the paper shows CGPO outperforms these specific baselines, it doesn't provide a comprehensive comparison with all existing methods that combine differentiable RL with Lagrangian approaches, nor does it explore why CGPO performs better.
- What evidence would resolve it: A thorough comparison of CGPO with a wider range of differentiable RL + Lagrangian methods, including ablation studies to identify the key factors contributing to CGPO's superior performance.

## Limitations
- The method requires differentiable systems or accurate world models, limiting applicability to many real-world problems
- Validation is restricted to simulated environments, leaving questions about real-world performance
- The adaptive trust region mechanism may be sensitive to hyperparameter choices that aren't fully explored
- Comparison with some state-of-the-art safe RL methods is missing from the evaluation

## Confidence

- **High confidence**: The theoretical framework of GBE and its advantage over infinite-horizon methods in finite-horizon settings is well-established through derivation and supported by the error bound lemma.
- **Medium confidence**: The empirical performance improvements shown in differentiable environments are convincing, but the extension to non-differentiable environments via world models introduces additional uncertainty about real-world applicability.
- **Low confidence**: The specific hyperparameter choices (trust region thresholds, learning rates, etc.) and their sensitivity across different environments are not thoroughly explored.

## Next Checks
1. **Robustness test**: Systematically vary the trust region parameters (η₊, η₋, initial δ̂) across all environments to quantify sensitivity and identify optimal ranges.
2. **Real-world applicability**: Implement CGPO on a physical robot with differentiable dynamics (e.g., a simulated robot arm with known kinematics) and compare constraint satisfaction against CPO and other baselines.
3. **Scalability assessment**: Test CGPO on environments with longer horizons (e.g., 500+ steps) to evaluate whether the computational advantage of GBE over BPTT-Lag scales with horizon length.