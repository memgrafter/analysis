---
ver: rpa2
title: 'RealCQA-V2 : Visual Premise Proving A Manual COT Dataset for Charts'
arxiv_id: '2410.22492'
source_url: https://arxiv.org/abs/2410.22492
tags:
- chart
- reasoning
- value
- data
- axis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Visual Premise Proving (VPP), a structured
  approach for evaluating visual reasoning in chart question answering (CQA) by decomposing
  reasoning into logical premises. The RealCQA-V2 dataset comprises 10 million annotated
  question-answer pairs derived from scientific charts, with premises representing
  structural, data retrieval, and mathematical reasoning steps.
---

# RealCQA-V2 : Visual Premise Proving A Manual COT Dataset for Charts

## Quick Facts
- arXiv ID: 2410.22492
- Source URL: https://arxiv.org/abs/2410.22492
- Authors: Saleem Ahmed; Ranga Setlur; Venu Govindaraju
- Reference count: 40
- Key outcome: Visual Premise Proving framework improves reasoning accuracy in chart QA by decomposing reasoning into logical premises, with premise-based training showing gains particularly for string answers and unranked lists.

## Executive Summary
Visual Premise Proving (VPP) introduces a structured approach to chart question answering by decomposing reasoning into First-Order Logic premises. The RealCQA-V2 dataset contains 10 million annotated question-answer pairs from scientific charts, with premises representing structural, data retrieval, and mathematical reasoning steps. Two novel metrics, ACCVPP and DCP, assess reasoning correctness and depth beyond final-answer accuracy. Experiments show models achieve higher reasoning accuracy (27%) compared to chart structure (19%) and data retrieval (14%), indicating stronger generalization in reasoning despite visual complexity. The premise-based training approach improves performance on standard chart QA tasks, particularly for string answers and unranked lists.

## Method Summary
The VPP framework decomposes chart reasoning into logical premises using First-Order Logic, creating structured reasoning chains for chart question answering. The method involves generating premise-conclusion pairs from chart annotations, fine-tuning vision-language models on these pairs, and evaluating using ACCVPP and DCP metrics. The ChartPrem model, trained on premise conclusions, demonstrates improved performance on standard chart QA tasks compared to direct fine-tuning approaches.

## Key Results
- Models achieve 27% accuracy on reasoning premises, outperforming structure (19%) and data retrieval (14%) premises
- ChartPrem model trained with premises shows highest gains in string type answers and unranked lists
- ACCVPP value of 0.2 indicates only 20% of reasoning sequences are completely correct
- DCP value of 0.395 reveals models correctly validate approximately 39.5% of premises in incomplete sequences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing chart reasoning into FOL premises enables granular validation of reasoning steps, improving model interpretability and correctness.
- **Mechanism:** The Visual Premise Proving (VPP) framework breaks down chart question answering into structured premises (structural, data, reasoning, mathematical) that represent necessary steps for chart comprehension. Each premise is validated as true/false, allowing assessment of reasoning depth beyond final-answer accuracy.
- **Core assumption:** Models can be trained to validate individual logical premises in sequence, and this validation correlates with genuine reasoning capability rather than pattern matching.
- **Evidence anchors:**
  - [abstract] "VPP, we decompose reasoning into distinct logical premises, each capturing a necessary step for understanding and analyzing visual chart data."
  - [section 3.3.1] "The multiplicative approach reflects the logical 'AND' operation in formal proofs, where the truth value of the conclusion depends on all components being true."
  - [corpus] Weak evidence - no direct comparison studies between premise-based training and standard approaches in the related papers.

### Mechanism 2
- **Claim:** Training models on premise conclusions improves performance on standard chart question answering tasks, especially for string answers and unranked lists.
- **Mechanism:** The ChartPrem model, trained on premise-conclusion pairs, learns to parse chart structure and data relationships more effectively than models trained only on question-answer pairs. This structured reasoning approach transfers to better performance on various answer types.
- **Core assumption:** The structured reasoning patterns learned through premise validation generalize to standard QA tasks.
- **Evidence anchors:**
  - [section 4.2] "The ChartPrem version trained with the premises shows the most gains in string type answers and also unranked lists, suggesting the considerable importance of the chart structure and step wise reasoning."
  - [table 1] "ChartPrem(Ours) 44.62" accuracy compared to "Matcha(FT) 32.10" on total accuracy.
  - [corpus] Weak evidence - related papers focus on different approaches (visual instruction-tuning, code-driven reasoning) without direct comparison to premise-based training.

### Mechanism 3
- **Claim:** The ACCVPP and DCP metrics provide more nuanced evaluation of reasoning capabilities than standard accuracy metrics.
- **Mechanism:** ACCVPP measures the proportion of completely correct reasoning sequences (all premises true), while DCP measures the average depth of correct premises in incomplete sequences. Together, they reveal both overall reasoning success and partial reasoning capability.
- **Core assumption:** These metrics capture meaningful aspects of reasoning quality that standard accuracy metrics miss.
- **Evidence anchors:**
  - [section 3.3.1] "The AccVPP value of 0.2 indicates that only 20% of the sequences in the dataset were completely correct, where every premise within these sequences was validated accurately."
  - [section 3.3.2] "The DCP value of 0.395 reveals that, on average, approximately 39.5% of the premises are correctly validated in sequences where not all premises are correct."
  - [corpus] Weak evidence - no direct validation studies comparing these metrics to alternative evaluation methods in the related papers.

## Foundational Learning

- **Concept:** First-Order Logic (FOL) and its application to visual reasoning
  - **Why needed here:** The VPP framework relies on FOL premises with quantifiers (∀, ∃) and predicates to structure reasoning about charts. Understanding FOL is essential for creating valid premise sequences and evaluating reasoning chains.
  - **Quick check question:** Can you explain how a statement like "∀x∃y(P(x) ∧ Q(y))" would be used to validate a chart premise about data relationships?

- **Concept:** Chart structure and component identification
  - **Why needed here:** Premise creation requires understanding chart elements (titles, axes, legends, data series) and their relationships. This knowledge is fundamental for generating valid structural and data premises.
  - **Quick check question:** Given a bar chart with multiple data series, can you identify all the structural elements that would be needed to answer a question about comparing values across series?

- **Concept:** Chain of Thought (COT) prompting and multimodal reasoning
  - **Why needed here:** The VPP framework extends COT prompting from text to visual domains. Understanding how intermediate reasoning steps improve performance in NLP tasks helps grasp the motivation for structured visual reasoning.
  - **Quick check question:** How does the concept of "reasoning chains" in text-based COT relate to the premise sequences in VPP for chart reasoning?

## Architecture Onboarding

- **Component map:** Chart image → Premise generation → FOL premise validation → ACCVPP/DCP calculation → Model fine-tuning on premise conclusions → Improved QA performance
- **Critical path:** Chart image → Premise generation → FOL premise validation → ACCVPP/DCP calculation → Model fine-tuning on premise conclusions → Improved QA performance
- **Design tradeoffs:**
  - Template-based premises provide structure but may limit reasoning flexibility
  - Binary FOL premises simplify evaluation but may not capture all reasoning nuances
  - Large dataset (10M pairs) enables comprehensive training but requires significant computational resources
  - Focus on scientific charts provides real-world complexity but may limit generalizability to other chart types
- **Failure signatures:**
  - Low ACCVPP but high DCP: Models can partially reason but struggle with complete chains
  - High ACCVPP but poor QA performance: Premise validation doesn't transfer to actual question answering
  - Performance degradation on unrepresented chart types: Limited generalizability beyond training distribution
- **First 3 experiments:**
  1. **Premise Validation Accuracy:** Measure FOL solver performance on each premise category (structural, data, reasoning, mathematical) to identify which types models struggle with most.
  2. **Metric Correlation Analysis:** Compare ACCVPP and DCP scores with standard QA accuracy to validate that premise-based metrics capture meaningful reasoning differences.
  3. **Generalization Test:** Evaluate ChartPrem model on ChartArXiv dataset (unseen charts) to assess how well premise-based training generalizes to new visual domains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Visual Premise Proving framework be extended to other visual reasoning domains beyond charts, such as diagrams or real-world scenes?
- Basis in paper: [inferred] The paper mentions future work extending from charts to general visual question answering (VQA) and more abstract visual reasoning, indicating this is an open direction.
- Why unresolved: The current framework is specifically designed for structured chart data with known variables and predicates. Extending to unstructured visual domains would require identifying and formalizing visual premises in unconstrained environments, which is a significant challenge.
- What evidence would resolve it: Successful application of VPP to VQA datasets (e.g., A-OKVQA, ScienceQA) with comparable performance metrics (ACCVPP, DCP) would demonstrate generalizability.

### Open Question 2
- Question: What is the impact of increasing the depth and complexity of reasoning chains on model performance?
- Basis in paper: [explicit] The paper notes that models struggle with consistency in extended reasoning chains and that DCP values less than 0.5 indicate models tend to fail before reaching the halfway point of premise sequences.
- Why unresolved: The study only evaluates models on existing premise sequences of varying lengths. Systematic experiments varying chain length and complexity (e.g., number of premises, types of predicates) would reveal performance trends and limitations.
- What evidence would resolve it: Controlled experiments varying chain depth and measuring ACCVPP and DCP across different chain lengths would identify thresholds where performance degrades significantly.

### Open Question 3
- Question: How can premise generation be automated to reduce reliance on manual annotation?
- Basis in paper: [explicit] The paper acknowledges that Manual-COT datasets are expensive and painstaking to curate, while Auto-COT suffers from hallucination.
- Why unresolved: Current premise creation requires human annotators to identify chart components and generate logical sequences. Automating this process while maintaining accuracy and avoiding hallucination remains an open challenge.
- What evidence would resolve it: A method that automatically generates premises with accuracy comparable to human annotation (verified by ACCVPP metrics) would demonstrate feasibility.

## Limitations

- The dataset's heavy focus on scientific charts may limit generalizability to other chart types (business charts, infographics, etc.)
- The premise-based evaluation assumes that valid premise sequences necessarily indicate genuine reasoning capability, but models might exploit spurious correlations
- The binary true/false evaluation of FOL premises may oversimplify complex reasoning patterns

## Confidence

**High confidence**: The dataset creation methodology and basic evaluation metrics (ACCVPP, DCP) are well-defined and reproducible.

**Medium confidence**: The claim that premise-based training improves standard chart QA performance, based on limited comparison with existing models.

**Low confidence**: The assertion that higher reasoning accuracy compared to structure/data retrieval indicates superior generalization, without direct comparison studies.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate models trained on RealCQA-V2 on completely different chart datasets (e.g., ChartQA, InfoVQA) to assess whether premise-based reasoning generalizes beyond scientific charts.

2. **Ablation study on premise types**: Systematically remove different premise categories (structural, data, reasoning, mathematical) from training to determine which types contribute most to performance improvements.

3. **Human evaluation of reasoning quality**: Conduct human studies to validate whether ACCVPP and DCP scores correlate with actual reasoning quality and interpretability, beyond automated metrics.