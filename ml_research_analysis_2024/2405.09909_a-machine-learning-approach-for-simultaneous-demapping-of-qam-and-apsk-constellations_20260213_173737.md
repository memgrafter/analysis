---
ver: rpa2
title: A Machine Learning Approach for Simultaneous Demapping of QAM and APSK Constellations
arxiv_id: '2405.09909'
source_url: https://arxiv.org/abs/2405.09909
tags:
- constellations
- apsk
- representation
- constellation
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework enabling a single DNN demapper
  to handle multiple QAM and APSK constellations. The method introduces a hierarchical
  representation of constellation points that captures shared structure (like quadrant
  information and circular groupings) while maintaining mapping independence, allowing
  flexible bit-to-symbol mappings without hardcoding.
---

# A Machine Learning Approach for Simultaneous Demapping of QAM and APSK Constellations

## Quick Facts
- arXiv ID: 2405.09909
- Source URL: https://arxiv.org/abs/2405.09909
- Reference count: 17
- Primary result: A single DNN demapper jointly handles QAM and APSK constellations with hierarchical representation, matching optimal hard-decision bit error rates under AWGN.

## Executive Summary
This paper introduces a machine learning framework for a unified demapper that simultaneously processes both QAM and APSK constellations used in DVB-S2x satellite communications. The core innovation is a hierarchical neural network architecture that encodes constellation points in a way that captures shared geometric structure—such as quadrant and circular groupings—while remaining agnostic to the specific bit-to-symbol mapping. This approach avoids the need for separate demappers per constellation and reduces the number of required neural network outputs compared to direct symbol prediction. The framework is demonstrated through simulations under additive white Gaussian noise, where the joint DNN achieves bit error rates equivalent to the optimal hard-decision demapper for each constellation tested.

## Method Summary
The authors propose a hierarchical representation of constellation points that preserves shared structure across QAM and APSK families while enabling mapping independence. Instead of predicting the symbol index directly, the DNN predicts a sequence of hierarchical codes (e.g., quadrant, sector, subsector) that uniquely identify each constellation point. This reduces the output layer size and facilitates parameter sharing among constellations with similar geometric properties. The network is trained jointly on a set of constellations from DVB-S2x, and simulations under AWGN validate that the learned demapper matches the bit error rate of the optimal hard-decision demapper for each constellation.

## Key Results
- Jointly trained DNN demapper matches optimal hard-decision bit error rates for all tested QAM and APSK constellations under AWGN.
- Hierarchical representation reduces required neural network outputs compared to direct symbol prediction.
- Parameter sharing across constellations is enabled by the shared geometric structure captured in the hierarchical encoding.

## Why This Works (Mechanism)
The hierarchical encoding leverages the common geometric properties of QAM and APSK constellations—such as symmetry, radial and angular structure—to represent symbols in a compact, structured form. By predicting hierarchical codes rather than raw symbol indices, the network learns to exploit shared features across constellations, which reduces output complexity and enables efficient parameter sharing. This mapping independence ensures the approach is robust to different bit-to-symbol mappings, a practical requirement in real-world systems.

## Foundational Learning
- QAM (Quadrature Amplitude Modulation): A modulation scheme using both amplitude and phase variations; needed for understanding the modulation schemes the demapper targets.
- APSK (Amplitude and Phase Shift Keying): A modulation scheme with symbols arranged in concentric rings, important for distinguishing from QAM and leveraging shared circular structure.
- Hard-decision demapper: A classical demapping method that outputs the most likely transmitted bits; used as the performance baseline.
- Hierarchical encoding: A structured representation of symbols that captures shared geometric features; reduces output complexity and enables parameter sharing.
- DVB-S2x: A satellite communications standard specifying various QAM and APSK constellations; provides the testbed for the proposed method.

## Architecture Onboarding
- Component map: Input -> Hierarchical Encoder -> Shared Feature Extractor -> Hierarchical Output Decoder -> Bit Estimates
- Critical path: Constellation symbols are encoded hierarchically, processed through shared layers, then decoded to hierarchical outputs that map to bits.
- Design tradeoffs: Hierarchical encoding reduces output size and enables parameter sharing, but may introduce complexity in training and require careful design of the hierarchy.
- Failure signatures: Poor performance on constellations with very different structures from the training set; sensitivity to noise in early hierarchical stages; potential overfitting if hierarchy is too deep.
- First experiments:
  1. Train the DNN on a single constellation and compare BER to optimal hard-decision demapper.
  2. Jointly train on multiple constellations and evaluate generalization to unseen mappings.
  3. Test the effect of varying hierarchical depth on BER and parameter efficiency.

## Open Questions the Paper Calls Out
- Whether the hierarchical DNN architecture's performance gains translate to real-world hardware constraints and practical implementation in terms of computational complexity, latency, and power consumption.
- The robustness of the approach under fading channels, phase noise, and other impairments common in satellite communications, as the study relies on AWGN only.
- Generalization to constellations not included in training (e.g., larger sizes or non-standard APSK) is not demonstrated.
- The assumption that parameter sharing via hierarchical relationships is beneficial assumes the network can effectively learn shared features without overfitting or underfitting to specific constellations.

## Limitations
- Simulations only consider AWGN channel; real-world robustness to fading, phase noise, and nonlinearities is unverified.
- No experimental validation on actual hardware (FPGA/ASIC) for computational complexity, latency, or power consumption.
- Generalization to unseen constellation sizes or non-standard APSK mappings is not demonstrated.
- The benefit of parameter sharing is assumed but not experimentally compared to non-shared baselines.

## Confidence
- Core claims (joint demapping performance matching optimal hard-decision bounds and bit error rate preservation): High
- Architectural novelty (hierarchical representation): Medium
- Practical applicability (hardware feasibility, real-world robustness): Low

## Next Checks
1. Evaluate the DNN demapper's performance under realistic satellite channel models including fading, phase noise, and nonlinearities.
2. Implement the architecture on target hardware platforms (e.g., FPGA or ASIC) to assess computational complexity, latency, and power consumption.
3. Test the demapper's generalization capability on unseen constellation sizes and mappings outside the DVB-S2x set.