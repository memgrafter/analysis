---
ver: rpa2
title: Enhancing the Expressivity of Temporal Graph Networks through Source-Target
  Identification
arxiv_id: '2411.03596'
source_url: https://arxiv.org/abs/2411.03596
tags:
- node
- moving
- messages
- graph
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of Temporal Graph Networks
  (TGNs) in dynamic node affinity prediction tasks, where TGNs struggle to predict
  future interaction strengths between nodes. The authors prove that TGNs cannot represent
  moving averages, persistent forecasting, or autoregressive models over messages
  due to their permutation-invariant design.
---

# Enhancing the Expressivity of Temporal Graph Networks through Source-Target Identification

## Quick Facts
- arXiv ID: 2411.03596
- Source URL: https://arxiv.org/abs/2411.03596
- Reference count: 40
- Key outcome: TGNv2 significantly outperforms TGN and all current TG models on dynamic node affinity prediction tasks across all TGB datasets

## Executive Summary
This paper addresses a fundamental limitation in Temporal Graph Networks (TGNs) - their inability to predict future interaction strengths between nodes in dynamic node affinity prediction tasks. The authors prove that TGNs cannot represent moving averages, persistent forecasting, or autoregressive models over messages due to their permutation-invariant design. They propose TGNv2, which adds source-target identification to each message, allowing the model to distinguish between senders and receivers. TGNv2 is proven to be strictly more expressive than TGN and achieves significant performance improvements on the Temporal Graph Benchmark datasets.

## Method Summary
The authors identify that TGNs' permutation invariance prevents them from representing functions that depend on message order or specific source-target relationships. To address this, they propose TGNv2, which modifies the message function to include source-target identification by adding node embeddings for both the sender and receiver to each message. The memory module is reformulated using block matrices and permutation operations to maintain computational efficiency while enabling the model to distinguish between messages sent to different nodes. The model is trained with the same hyperparameters as TGN for fair comparison, using the Adam optimizer and evaluating performance using NDCG@10 on validation and test sets.

## Key Results
- TGNv2 outperforms TGN and all current TG models on dynamic node affinity prediction tasks across all TGB datasets
- The improvement is significant, with TGNv2 achieving better NDCG@10 scores on both validation and test sets
- TGNv2 is proven to be strictly more expressive than TGN, capable of representing moving averages, persistent forecasting, and autoregressive models over messages

## Why This Works (Mechanism)
TGNv2 works by breaking the permutation invariance of TGN through source-target identification. By adding node embeddings for both the sender and receiver to each message, the model can now distinguish between messages sent to different nodes and track the history of interactions more effectively. This allows TGNv2 to represent functions that depend on the specific relationships between nodes over time, rather than just the aggregate information about all messages.

## Foundational Learning
- **Permutation invariance in TGNs**: Why needed - Understanding this limitation is crucial for grasping why TGNs fail at dynamic node affinity prediction. Quick check - Verify that the model's output doesn't change when the order of messages is shuffled.
- **Source-target identification**: Why needed - This is the core innovation that enables TGNv2 to overcome TGNs' limitations. Quick check - Confirm that each message now contains embeddings for both the sender and receiver nodes.
- **Memory module reformulation**: Why needed - Ensures computational efficiency while enabling the model to distinguish between messages sent to different nodes. Quick check - Verify that the memory update operation correctly handles the block matrix structure.

## Architecture Onboarding
- **Component map**: Input events -> Message function (with source-target ID) -> Memory module (block matrices) -> Node embeddings -> Prediction
- **Critical path**: The message function with source-target identification is the critical innovation, as it enables the model to distinguish between messages sent to different nodes.
- **Design tradeoffs**: The authors chose to maintain the same hyperparameters as TGN for fair comparison, but this may limit TGNv2's performance relative to moving average heuristics. Exploring more expressive aggregation functions could close this gap.
- **Failure signatures**: If TGNv2 performs similarly to baseline TGN, it likely indicates incorrect implementation of source-target identification in the message functions.
- **First experiments**: 1) Verify that TGNv2 correctly distinguishes between messages sent to different nodes by checking memory updates. 2) Compare NDCG@10 scores on a small subset of data to ensure improvements. 3) Check computational overhead by measuring memory usage and training time.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How would TGNv2 perform if a more expressive aggregation function (e.g., attention-based) were used instead of the last-message aggregator?
- Basis in paper: The authors mention they formulated their message aggregator to output the last message "to compare our results fairly with prior TGN experiments" and suggest exploring more expressive aggregation functions could close the gap to heuristics.
- Why unresolved: The current implementation uses a simple last-message aggregator for fair comparison, but the authors acknowledge this choice may limit TGNv2's performance relative to moving average heuristics.
- What evidence would resolve it: Experimental results comparing TGNv2 with different aggregation functions (mean, attention, etc.) on the same datasets, measuring performance against moving average heuristics.

### Open Question 2
- Question: Can the source-target identification approach be extended to other temporal graph tasks like dynamic link prediction or dynamic node classification?
- Basis in paper: The authors conclude by stating "we would like to further develop our work by studying TGNv2 on other TG tasks, such as dynamic link prediction."
- Why unresolved: The paper only evaluates TGNv2 on dynamic node affinity prediction tasks, leaving open questions about its effectiveness on other temporal graph tasks.
- What evidence would resolve it: Experimental results showing TGNv2 performance on dynamic link prediction and dynamic node classification tasks compared to existing methods.

### Open Question 3
- Question: What is the computational overhead of adding source-target identification to TGN in terms of memory and training time?
- Basis in paper: The authors propose modifying TGN by adding source-target identification to each message, which would require additional parameters and computations, but don't discuss the computational cost.
- Why unresolved: The paper focuses on theoretical expressivity and empirical performance improvements but doesn't address the practical trade-offs in terms of computational resources.
- What evidence would resolve it: Detailed analysis comparing memory usage, training time, and inference time between TGN and TGNv2 on the same hardware, along with scaling behavior as graph size increases.

## Limitations
- The experimental validation relies heavily on the TGB datasets, which may not represent all dynamic node affinity prediction scenarios
- The paper does not explore alternative solutions for enhancing TGN expressivity beyond source-target identification
- The computational overhead of the proposed modifications is not discussed, limiting understanding of practical applicability

## Confidence
- **High Confidence**: The theoretical proof that TGNs cannot represent moving averages, persistent forecasting, or autoregressive models due to permutation invariance is mathematically rigorous and well-founded.
- **Medium Confidence**: The experimental results showing TGNv2's superiority on TGB datasets are convincing but may be dataset-specific. The claim of strict expressiveness improvement over TGN is supported by both theory and experiments but requires validation on additional benchmarks.
- **Low Confidence**: The assertion that source-target identification is the optimal solution for enhancing TGN expressivity, as other approaches are not thoroughly explored or compared.

## Next Checks
1. Implement and compare TGNv2 against other potential solutions for enhancing TGN expressivity, such as temporal attention mechanisms or adaptive memory updates, to confirm that source-target identification is indeed optimal.
2. Validate TGNv2's performance on additional temporal graph datasets beyond the TGB benchmark, including those with different characteristics (e.g., different edge densities, node feature distributions) to assess generalizability.
3. Conduct a detailed analysis of TGNv2's memory usage and computational overhead compared to baseline TGN, particularly for large-scale graphs, to ensure practical applicability.