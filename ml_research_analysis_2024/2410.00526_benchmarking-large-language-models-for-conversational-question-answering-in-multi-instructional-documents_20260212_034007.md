---
ver: rpa2
title: Benchmarking Large Language Models for Conversational Question Answering in
  Multi-instructional Documents
arxiv_id: '2410.00526'
source_url: https://arxiv.org/abs/2410.00526
tags:
- question
- instructions
- documents
- procedural
- instructional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InsCoQA, a novel benchmark designed to evaluate
  large language models (LLMs) in conversational question answering (CQA) using instructional
  documents. Unlike existing CQA datasets that focus on basic factual questions from
  single narrative documents, InsCoQA addresses the unique challenges of comprehending
  complex real-world instructional documents and providing accurate step-by-step guidance.
---

# Benchmarking Large Language Models for Conversational Question Answering in Multi-instructional Documents

## Quick Facts
- arXiv ID: 2410.00526
- Source URL: https://arxiv.org/abs/2410.00526
- Reference count: 36
- Key outcome: GPT-4 outperforms other state-of-the-art LLMs on InsCoQA benchmark for conversational QA in instructional documents

## Executive Summary
This paper introduces InsCoQA, a novel benchmark designed to evaluate large language models on conversational question answering using instructional documents. Unlike existing CQA datasets that focus on basic factual questions from single narrative documents, InsCoQA addresses the unique challenges of comprehending complex real-world instructional documents and providing accurate step-by-step guidance. The benchmark consists of 13.9k instructional conversations sourced from the Xiaohongshu platform across 13 diverse domains. To comprehensively assess LLMs on InsCoQA, the authors propose INSEVAL, an LLM-assisted evaluator that measures the integrity and accuracy of generated responses and procedural instructions.

## Method Summary
The paper collects 13.9k instructional conversations from the Xiaohongshu platform across 13 domains. The dataset construction pipeline involves query filtering from user data, document retrieval using an internal information retrieval engine, conversation generation using GPT-4, and human verification. For evaluation, INSEVAL uses Qwen2-7B-Instruct to assess Judge Score (1-10) and Task Completion Rate (0-100%) for generated responses and procedural instructions, supplemented with ROUGE-L metrics at both character and word levels.

## Key Results
- GPT-4 achieves superior performance on InsCoQA compared to other state-of-the-art LLMs
- INSEVAL effectively evaluates procedural instruction quality beyond traditional text-matching metrics
- Models struggle with multi-document synthesis, showing lower task completion rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: InsCoQA addresses real-world instructional complexity by requiring models to synthesize procedural guidance from multiple documents
- Mechanism: By collecting conversational QA pairs grounded in multiple instructional documents, the benchmark forces models to identify common steps across sources, resolve contradictions, and produce coherent step-by-step procedures
- Core assumption: Real-world instructional tasks require synthesizing information from multiple sources rather than retrieving from single documents
- Evidence anchors:
  - [abstract] "Unlike existing CQA datasets that focus on basic factual questions from single narrative documents, InsCoQA addresses the unique challenges of comprehending complex real-world instructional documents and providing accurate step-by-step guidance"
  - [section 3.1] "we collect multiple documents for each conversational QA sample... requires LLMs to identify important commonalities among different documents"

### Mechanism 2
- Claim: INSEVAL provides more accurate evaluation than traditional text-matching metrics for procedural instructions
- Mechanism: Uses an LLM evaluator to assess both the plain-text response quality and task completion rate of procedural instructions, capturing semantic correctness beyond exact text matching
- Core assumption: Procedural instructions require semantic understanding of task completion rather than surface-level text similarity
- Evidence anchors:
  - [section 5.1] "Evaluating whether the generated instructions accurately address the question... is challenging... when relying solely on exact text matching"
  - [section 5.2] "INSEVAL further quantifies the alignment between the Plain-text response and the ground truth using both character-level and word-level ROUGE-L metrics"

### Mechanism 3
- Claim: The dataset construction pipeline creates high-quality conversational data that reflects real user behavior
- Mechanism: Starts with popular user queries, retrieves multiple relevant documents, uses GPT-4 to generate conversational history and responses, then applies human verification to filter issues
- Core assumption: Popular user queries from real platforms reflect genuine instructional needs that can be addressed through conversational QA
- Evidence anchors:
  - [section 3.1] "To gather high-quality, instruction-focused queries, we extracted the most frequently asked user queries from the Xiaohongshu platform"
  - [section 3.2] "we use GPT-4... to generate multi-turn conversational history and the final-turn question"

## Foundational Learning

- Concept: Multi-document information synthesis
  - Why needed here: The core challenge of InsCoQA requires models to combine information from multiple instructional documents into coherent procedures
  - Quick check question: Given two documents describing how to change a tire (one focusing on removing the wheel, another on proper jack placement), what are the complete steps?

- Concept: Conversational context understanding
  - Why needed here: Questions reference previous conversation turns, requiring models to track context and resolve coreferences
  - Quick check question: If Q1 asks "How do I start cooking this recipe?" and Q2 asks "How long should I cook it?", what does "it" refer to?

- Concept: Procedural instruction generation
  - Why needed here: Unlike fact retrieval, this task requires generating ordered, step-by-step guidance that users can follow
  - Quick check question: How would you structure instructions for "how to bake a cake" differently from answering "what temperature should I bake a cake at"?

## Architecture Onboarding

- Component map: Query filtering → Document retrieval → Conversation generation → Human verification → Dataset construction → LLM evaluation → Benchmark release
- Critical path: Query filtering → Document retrieval → Conversation generation → Human verification → Dataset construction → LLM evaluation → Benchmark release
- Design tradeoffs:
  - Multiple documents increase realism but add complexity to both model training and evaluation
  - Human verification ensures quality but limits scalability of dataset construction
  - LLM-assisted evaluation provides semantic assessment but may introduce evaluator bias
- Failure signatures:
  - Low task completion rates indicate models struggle with multi-document synthesis
  - High ROUGE scores but low judge scores suggest surface-level matching without semantic understanding
  - High variance across domains indicates domain-specific knowledge gaps
- First 3 experiments:
  1. Evaluate a simple RAG baseline on InsCoQA to establish baseline performance and identify failure modes
  2. Test whether providing document summaries instead of full documents improves model performance
  3. Compare performance across domains to identify which instructional domains are most challenging for current LLMs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do smaller LLMs like Qwen2-7B-Instruct perform compared to larger models like GPT-4 in evaluating the quality of procedural instructions?
- Basis in paper: [explicit] The paper mentions that smaller LLMs like Qwen2-7B-Instruct are sufficiently competent for evaluating answer quality when compared to larger models like GPT-4, but the absolute values of Judge Score and Task Completion Rate may differ due to inherent biases in the LLMs.
- Why unresolved: The paper does not provide a detailed comparison of the performance of smaller LLMs against larger models like GPT-4 in evaluating the quality of procedural instructions.
- What evidence would resolve it: Conducting a detailed comparison of the performance of smaller LLMs like Qwen2-7B-Instruct against larger models like GPT-4 in evaluating the quality of procedural instructions, including metrics like Judge Score and Task Completion Rate.

### Open Question 2
- Question: How does the InsCoQA dataset perform on different types of instructional documents, such as those with varying levels of complexity or from different domains?
- Basis in paper: [inferred] The paper mentions that the InsCoQA dataset consists of 13.9k instructional conversations from Xiaohongshu across 13 diverse domains, but it does not provide a detailed analysis of how the dataset performs on different types of instructional documents.
- Why unresolved: The paper does not provide a detailed analysis of how the InsCoQA dataset performs on different types of instructional documents, such as those with varying levels of complexity or from different domains.
- What evidence would resolve it: Conducting a detailed analysis of how the InsCoQA dataset performs on different types of instructional documents, such as those with varying levels of complexity or from different domains, including metrics like Judge Score and Task Completion Rate.

### Open Question 3
- Question: How does the InsCoQA dataset perform on different types of conversational questions, such as those with varying levels of complexity or from different domains?
- Basis in paper: [inferred] The paper mentions that the InsCoQA dataset consists of 13.9k instructional conversations from Xiaohongshu across 13 diverse domains, but it does not provide a detailed analysis of how the dataset performs on different types of conversational questions.
- Why unresolved: The paper does not provide a detailed analysis of how the InsCoQA dataset performs on different types of conversational questions, such as those with varying levels of complexity or from different domains.
- What evidence would resolve it: Conducting a detailed analysis of how the InsCoQA dataset performs on different types of conversational questions, such as those with varying levels of complexity or from different domains, including metrics like Judge Score and Task Completion Rate.

## Limitations

- Dataset construction bias from GPT-4 generation may systematically favor certain conversational patterns
- Evaluator reliability concerns due to potential circularity in LLM-based evaluation
- Limited generalizability analysis across domains to distinguish domain-specific versus general conversational QA capabilities

## Confidence

**High Confidence**: The benchmark construction methodology and evaluation framework are well-documented and reproducible. The superiority of GPT-4 on this benchmark is consistently demonstrated across multiple metrics.

**Medium Confidence**: The claim that InsCoQA addresses unique real-world challenges is supported by the multi-document setup and diverse domains, but the actual representation of real-world complexity could be higher. The paper doesn't provide user studies validating that the generated conversations reflect authentic user needs.

**Low Confidence**: The assertion that existing CQA datasets are inadequate for instructional tasks is presented without comprehensive empirical comparison showing systematic failures of current models on real-world instructional scenarios.

## Next Checks

1. **Human Evaluation Study**: Conduct a user study with real instructional task practitioners to validate whether the conversational QA pairs in InsCoQA reflect genuine user needs and whether the multi-document setup represents typical real-world usage patterns.

2. **Cross-Domain Transfer Analysis**: Design experiments to test whether models trained on one domain can effectively transfer skills to other domains, helping distinguish between domain-specific knowledge acquisition versus general conversational QA capability improvements.

3. **Evaluator Consistency Validation**: Perform inter-rater reliability studies comparing INSEVAL's judgments against multiple human evaluators across a diverse sample of responses to quantify potential evaluator bias and establish confidence intervals for the reported metrics.