---
ver: rpa2
title: 'ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting'
arxiv_id: '2410.17856'
source_url: https://arxiv.org/abs/2410.17856
tags:
- interaction
- tasks
- arxiv
- object
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ROCKET-1, a novel method for enabling vision-language
  models (VLMs) to perform embodied decision-making in open-world environments. The
  key challenge addressed is the difficulty of communicating spatial information between
  VLMs and low-level policies, which is crucial for effective task planning.
---

# ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting

## Quick Facts
- arXiv ID: 2410.17856
- Source URL: https://arxiv.org/abs/2410.17856
- Reference count: 11
- Primary result: Achieves 76% absolute improvement in task success rates for open-world embodied decision-making

## Executive Summary
This paper introduces ROCKET-1, a novel method that enables vision-language models (VLMs) to perform embodied decision-making in open-world environments. The key challenge addressed is the difficulty of communicating spatial information between VLMs and low-level policies, which is crucial for effective task planning. To solve this, the authors propose visual-temporal context prompting, a communication protocol that leverages object segmentation from past and present observations to guide policy-environment interactions. ROCKET-1 is trained as a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, supported by real-time object tracking from SAM-2. Experiments in Minecraft demonstrate that this approach significantly improves open-world interaction performance, achieving a 76% absolute improvement in task success rates. The method also shows strong zero-shot generalization capabilities, enabling agents to complete previously unattainable tasks.

## Method Summary
ROCKET-1 addresses the spatial communication gap between VLMs and low-level policies by introducing visual-temporal context prompting. The method uses SAM-2 for object tracking and segmentation, generating masks that capture both current observations and historical context. A TransformerXL-based policy is trained to predict actions from concatenated visual observations and segmentation masks, with random segmentation dropping during training to encourage reliance on temporal context. The training data is automatically generated using backward trajectory relabeling, where SAM-2 segments objects by tracking backward from interaction events. During inference, ROCKET-1 operates at higher frequencies than VLMs, using the visual-temporal context to make decisions while VLMs provide high-level task decomposition.

## Key Results
- Achieves 76% absolute improvement in task success rates compared to baseline methods
- Demonstrates strong zero-shot generalization, completing tasks that were previously unattainable
- Outperforms methods using language instructions for spatial communication in embodied decision-making tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual-temporal context prompting enables effective spatial communication between VLMs and low-level policies
- Mechanism: By using object segmentation masks from past and present observations as task prompts, the policy can maintain spatial awareness even when objects are temporarily occluded
- Core assumption: Segmentation masks provide sufficient spatial information to disambiguate objects in the environment
- Evidence anchors:
  - [abstract] "leverages object segmentation from past observations to guide policy-environment interactions"
  - [section 1] "language often fails to effectively convey spatial information"
  - [section 3] "concatenate the observation and object segmentation pixel-wise into a 4-channel image"
- Break condition: If segmentation models fail to track objects across frames or if the environment has too many similar-looking objects

### Mechanism 2
- Claim: Backward trajectory relabeling enables efficient training data generation for segmentation-conditioned policies
- Mechanism: SAM-2 tracks objects backward in time from interaction events, automatically generating segmentation masks for training trajectories
- Core assumption: Interaction events can be reliably detected and the object involved is centered in the previous frame
- Evidence anchors:
  - [section 3] "traverses the trajectory in reverse order, segmenting interacting objects in frame t via an open-vocabulary grounding model"
  - [section 3] "SAM-2 is used to track and generate segmentations for frames t-1, t-2, ..., t-k"
  - [section 4.4] "backward trajectory relabeling method that can automatically detect and segment desired objects"
- Break condition: If interaction events are missed or objects are not centered in previous frames

### Mechanism 3
- Claim: Random segmentation dropping during training forces the policy to rely on visual-temporal context rather than direct segmentation cues
- Mechanism: By randomly masking segmentation inputs during training, the policy learns to infer spatial relationships from temporal context
- Core assumption: The policy can learn to reconstruct spatial information from temporal patterns even when direct segmentation is unavailable
- Evidence anchors:
  - [section 3] "randomly dropping segmentations with a certain probability, forcing the model to infer user intent from past inputs"
  - [section 3] "L = -|τ|∑log π(a_t|o_1:t, m_1:t ⊙w_1:t, c_1:t ⊙w_1:t)"
  - [section 3] "This allows us to discard the rewards and learn a conditioned policy π(a_t|o_1:t, m_1:t, c_1:t) directly using behavior cloning"
- Break condition: If the dropping probability is too high and the policy cannot recover spatial information from context

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The paper frames embodied decision-making as an MDP where the agent must maximize cumulative reward
  - Quick check question: What are the five components of an MDP as defined in the preliminaries section?

- Concept: Vision-Language Models (VLMs)
  - Why needed here: VLMs provide the high-level reasoning and task decomposition capabilities that guide the low-level policy
  - Quick check question: What are the key limitations of VLMs when applied directly to embodied decision-making?

- Concept: Object Tracking and Segmentation
  - Why needed here: SAM-2's ability to track objects across frames is crucial for maintaining spatial awareness in partially observable environments
  - Quick check question: According to the paper, what makes SAM-2 particularly suitable for partially observable environments?

## Architecture Onboarding

- Component map:
  High-level reasoner (GPT-4o) -> Molmo -> SAM-2 -> ROCKET-1 -> Minecraft environment

- Critical path:
  1. High-level reasoner decomposes task into steps
  2. Molmo identifies object locations via points
  3. SAM-2 generates segmentation masks and tracks objects
  4. ROCKET-1 uses segmentation masks and interaction types to predict actions
  5. Actions are executed in Minecraft environment

- Design tradeoffs:
  - Using segmentation masks vs language instructions for spatial communication
  - Running VLMs at lower frequencies vs real-time operation
  - Training with random segmentation dropping vs always available segmentation
  - Backward trajectory relabeling vs manual annotation

- Failure signatures:
  - If SAM-2 fails to track objects, ROCKET-1 loses spatial context
  - If segmentation masks are inaccurate, the policy may interact with wrong objects
  - If dropping probability is too high, policy cannot learn effective temporal reasoning
  - If Molmo fails to identify correct objects, the entire pipeline breaks down

- First 3 experiments:
  1. Test ROCKET-1 on simple tasks (hunt, mine) with perfect segmentation masks to establish baseline capability
  2. Evaluate impact of different SAM-2 model sizes on tracking performance and inference speed
  3. Test performance with varying segmentation dropping probabilities during training to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dropping probability p in Equation 4 affect ROCKET-1's performance on tasks requiring precise object localization versus those relying more on temporal reasoning?
- Basis in paper: [explicit] The paper mentions "randomly dropping segmentations with a certain probability, forcing the model to infer user intent from past inputs (visual-temporal context)" and provides Equation 4 showing the optimization objective with dropping probability p.
- Why unresolved: The paper does not provide empirical results on how different values of p affect performance across different task types or discuss the optimal range for p.
- What evidence would resolve it: Systematic experiments varying p and measuring performance on tasks with different spatial reasoning requirements, along with analysis of how this affects the model's reliance on segmentation masks versus temporal context.

### Open Question 2
- Question: What is the impact of ROCKET-1's 128-frame temporal window on its ability to solve tasks requiring longer-term memory or object tracking beyond this limit?
- Basis in paper: [explicit] The paper states "During inference, ROCKET-1 can access up to 128 frames of past observations" and discusses its use of visual-temporal context.
- Why unresolved: The paper does not evaluate tasks that require tracking objects or remembering information beyond 128 frames, nor does it discuss the limitations this imposes on long-horizon tasks.
- What evidence would resolve it: Experiments testing ROCKET-1 on tasks requiring object tracking or memory beyond 128 frames, and analysis of how performance degrades with increasing temporal requirements.

### Open Question 3
- Question: How does the performance of ROCKET-1 change when using different sizes of SAM-2 models, and what is the trade-off between tracking accuracy and inference speed in real-world applications?
- Basis in paper: [explicit] Table 4 in the paper compares different SAM-2 variants (tiny, small, base_plus, large) showing their impact on success rates and FPS, but only for specific tasks.
- Why unresolved: The paper does not provide comprehensive analysis of how SAM-2 model size affects overall ROCKET-1 performance across the entire benchmark, nor does it discuss the practical implications of the speed-accuracy trade-off.
- What evidence would resolve it: Comprehensive benchmarking of ROCKET-1 with different SAM-2 sizes across all tasks, along with analysis of how the speed-accuracy trade-off affects real-world deployment scenarios.

## Limitations

- The approach relies heavily on the quality of object tracking from SAM-2, which may struggle in complex environments with many similar objects or rapid motion
- Segmentation masks may not capture all relevant information for task completion, particularly for tasks requiring fine-grained manipulation
- The backward trajectory relabeling method assumes reliable detection of interaction events and centered objects in previous frames, which may not always hold

## Confidence

- High confidence: The mechanism of using visual-temporal context prompting for spatial communication between VLMs and low-level policies is well-supported by experimental results showing 76% absolute improvement in task success rates
- Medium confidence: The backward trajectory relabeling method for automatic segmentation mask generation appears sound, but details about detection reliability and parameter choices could affect reproducibility
- Medium confidence: The claim of strong zero-shot generalization capabilities is supported by results, but the evaluation across different task types could be more comprehensive

## Next Checks

1. Evaluate ROCKET-1's performance on tasks requiring fine-grained manipulation where segmentation masks may not provide sufficient spatial detail, comparing against methods that use alternative spatial communication protocols

2. Test the backward trajectory relabeling method's robustness by intentionally introducing noise into interaction event detection and measuring the impact on segmentation mask quality and downstream policy performance

3. Assess the computational overhead and real-time performance implications of the visual-temporal context prompting protocol by measuring VLM invocation frequency and inference latency across different dropping probability settings during inference