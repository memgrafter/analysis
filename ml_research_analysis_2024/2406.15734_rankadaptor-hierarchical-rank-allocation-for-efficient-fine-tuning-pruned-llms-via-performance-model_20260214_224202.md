---
ver: rpa2
title: 'RankAdaptor: Hierarchical Rank Allocation for Efficient Fine-Tuning Pruned
  LLMs via Performance Model'
arxiv_id: '2406.15734'
source_url: https://arxiv.org/abs/2406.15734
tags:
- performance
- lora
- pruned
- pruning
- rankadaptor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RankAdaptor addresses the challenge of recovering performance in
  structurally pruned large language models (LLMs) by introducing a hierarchical dynamic
  rank allocation method during fine-tuning. The core idea is to use a lightweight
  performance model that conducts offline meta-learning and online incremental learning
  to determine optimal rank values for each layer of the pruned LLM, adapting to the
  specific recovery needs of layers pruned to varying degrees.
---

# RankAdaptor: Hierarchical Rank Allocation for Efficient Fine-Tuning Pruned LLMs via Performance Model

## Quick Facts
- arXiv ID: 2406.15734
- Source URL: https://arxiv.org/abs/2406.15734
- Authors: Changhai Zhou, Shijie Han, Lining Yang, Yuhua Zhou, Xu Cheng, Yibin Wang, Hongguang Li
- Reference count: 40
- Key outcome: Hierarchical rank allocation recovers 92.13% accuracy on BoolQ task for 20% pruned LLaMA-7B vs 86.6% with standard LoRA

## Executive Summary
RankAdaptor addresses the challenge of recovering performance in structurally pruned large language models by introducing a hierarchical dynamic rank allocation method during fine-tuning. The core innovation is using a lightweight performance model that conducts offline meta-learning and online incremental learning to determine optimal rank values for each layer of the pruned LLM. This approach adapts to the specific recovery needs of layers pruned to varying degrees, outperforming standard LoRA's uniform rank allocation across different pruning settings and tasks.

## Method Summary
RankAdaptor introduces a hierarchical dynamic rank allocation method that uses a lightweight performance model to predict downstream task accuracy from different rank configurations without full fine-tuning. The performance model is first meta-learned on a small set of rank allocations with observed task accuracies, then incrementally updated as more fine-tuning results are observed. This enables efficient search through the huge solution space of possible rank combinations to find optimal per-layer rank values that best recover the pruned LLM's performance.

## Key Results
- Achieves 92.13% recovery of original accuracy on BoolQ task for 20% pruned LLaMA-7B, compared to 86.6% with LoRA
- Consistently outperforms standard LoRA across different pruning settings (5-40%) and tasks
- Demonstrates improvements ranging from 0.7% to 5.5% in accuracy on popular benchmarks with LLaMA-7B, LLaMA-13B, and Vicuna-7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RankAdaptor improves recovery by allocating different ranks per layer, reflecting the uneven importance distribution caused by structural pruning.
- Mechanism: After pruning, layers are pruned to varying degrees, so a uniform LoRA rank fails to provide sufficient adaptation for layers that are more damaged. RankAdaptor uses a performance model to predict how different rank allocations affect downstream task accuracy and optimizes each layer's rank individually.
- Core assumption: The downstream task performance of a pruned LLM is sensitive to rank allocation differences per layer.
- Evidence anchors:
  - [abstract] "structural pruning's uneven modification of model architecture, coupled with standard LoRA's fixed configuration allocation across layers in an online pipeline, leads to suboptimal performance"
  - [section] "Although applying standard LoRA with a uniform rank value across all layers can achieve a certain degree of recovery, it fails to adequately meet the unique recovery needs of layers pruned to varying degrees"
  - [corpus] "Reassessing Layer Pruning in LLMs: New Insights and Methods" - focuses on pruning layer importance, which aligns with layer-specific rank adaptation
- Break condition: If all layers are pruned equally, or if pruning is minimal enough that uniform rank is sufficient, the hierarchical benefit disappears.

### Mechanism 2
- Claim: The lightweight performance model predicts downstream accuracy from rank configurations without full fine-tuning, enabling efficient search in a huge solution space.
- Mechanism: The performance model is trained via meta-learning on a small set of rank allocations with observed task accuracies. Once trained, it can predict the performance of any rank combination quickly, so RankAdaptor can search for optimal ranks without running expensive fine-tuning for each candidate.
- Core assumption: Task performance is a smooth function of rank allocation that can be learned by a small MLP given a few labeled examples.
- Evidence anchors:
  - [section] "the performance model can predict the downstream task performance of billions of possible combinations in less than an hour"
  - [section] "we employ a performance model that conducts offline meta-learning and online incremental learning to explore optimal rank values for each layer"
  - [corpus] "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs" - shows feasibility of low-rank adaptations guided by learned models
- Break condition: If the performance model cannot learn a reliable mapping from rank configs to accuracy, predictions will be inaccurate and the search will fail.

### Mechanism 3
- Claim: Incremental learning of the performance model allows it to refine its predictions as more fine-tuning results are observed, improving convergence to good rank sets.
- Mechanism: After each rank configuration is tried and the real performance is measured, this data point is fed back to the performance model as training data, enabling it to correct its predictions and focus the search toward better regions of the rank space.
- Core assumption: Real performance data can be integrated efficiently into the performance model without destabilizing its predictions.
- Evidence anchors:
  - [section] "Subsequently, the recovery model is obtained via fine-tuning with the R'HD and P(recover(PL, R'HD)) are then evaluated on the downstream tasks. The actual performance data (R'HD, P(recover(PL, R'HD))) is fed back into the performance model for incremental learning"
  - [section] "The actual performance data ... is fed back into the performance model for incremental learning, enabling the model to continuously enhance its prediction accuracy over successive iterations"
  - [corpus] "EPSD: Early Pruning with Self-Distillation for Efficient Model Compression" - shows that iterative refinement based on observed outcomes can improve efficiency
- Break condition: If fine-tuning each candidate is too expensive or noisy, incremental learning will not converge effectively.

## Foundational Learning

- Concept: Structural pruning and its effect on layer importance
  - Why needed here: RankAdaptor's design is based on the fact that pruning unevenly damages layers, so recovery must adapt per layer.
  - Quick check question: What happens to a model's internal importance distribution after pruning certain neuron groups?

- Concept: Low-Rank Adaptation (LoRA) mechanics
  - Why needed here: RankAdaptor extends LoRA by making the rank a per-layer hyperparameter instead of fixed.
  - Quick check question: In LoRA, how does changing the rank affect the number of trainable parameters and the expressiveness of the update matrix?

- Concept: Meta-learning and incremental learning
  - Why needed here: The performance model is meta-learned on a small set of rank-performance pairs and then incrementally updated as more data is collected.
  - Quick check question: How does incremental learning differ from standard supervised learning in terms of data flow and model adaptation?

## Architecture Onboarding

- Component map:
  Pruned LLM -> Performance Model (MLP) -> Fine-tuning Module (LoRA with dynamic ranks) -> Incremental Learning Feedback Loop

- Critical path:
  1. Initialize performance model with random rank configs and measured accuracies.
  2. Use performance model to propose a rank set R'HD.
  3. Fine-tune pruned LLM with R'HD and measure accuracy.
  4. Update performance model with (R'HD, measured accuracy).
  5. Repeat until convergence.

- Design tradeoffs:
  - Small vs large sample size in estimation stage: more samples improve pruning quality but increase cost.
  - Inner MLP dimension: more neurons improve prediction accuracy but increase model size and training time.
  - Micro-batch size: larger sizes speed up training but may require more memory.

- Failure signatures:
  - Performance model predictions are inaccurate → rank search fails.
  - Incremental updates destabilize predictions → oscillation or divergence.
  - Fine-tuning with proposed ranks yields no improvement → model capacity or rank range too low.

- First 3 experiments:
  1. Run pruning at 20% with N=10 samples and standard LoRA, record accuracy drop.
  2. Apply RankAdaptor with the same pruning, compare recovery vs LoRA.
  3. Vary micro-batch size (4 vs 16) and measure impact on final accuracy and convergence speed.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability across architectures: Effectiveness on transformer variants with different attention mechanisms remains untested
- Computational overhead during deployment: Iterative approach may be prohibitive for resource-constrained scenarios
- Sensitivity to initialization: Poor initialization could lead to suboptimal rank allocation that incremental learning struggles to correct

## Confidence
**High Confidence**: Core claim of hierarchical rank allocation improving recovery is well-supported by consistent experimental improvements across multiple benchmarks and pruning levels.

**Medium Confidence**: Efficiency claims regarding performance model prediction speed are reasonable but lack independent verification.

**Low Confidence**: Incremental learning mechanism's effectiveness in practice is difficult to evaluate without detailed analysis of convergence speed and sensitivity to noisy measurements.

## Next Checks
1. Apply RankAdaptor to diverse LLM architectures (OPT, BLOOM, Falcon) to test cross-architecture effectiveness.

2. Systematically vary initial sample size and quality in meta-learning phase to measure impact on final rank allocation quality and convergence speed.

3. Implement realistic deployment scenario with strict time/memory constraints to evaluate practical feasibility compared to static compression methods.